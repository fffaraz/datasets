Pattern matching is far more analogous to a switch statement than function overloading.
I think you're focusing too much on the syntax (it "looks like" you're defining multiple functions, therefore it's overloading) and not on the semantics (which are not overloading and why everyone here is disagreeing with you). One other way that might help you see that it is a single function is to observe that there's no way to refer to one branch of the pattern match -- there's no way in code to refer to "the case of head when given an empty list". While in languages like C++ that do have overloading, you can take the address of the version of a function that accepts an int. In fact, post-compilation the overloaded versions of function are actually given different names.
Yes, you can never have too much documentation. There are only two real disadvantages to documentation: * It's very difficult to maintain, especially if the library is changing rapidly. API encapsulation, though, helps a lot in this regard. * There is no way to automatically refactor or test documentation for correctness. I can't use GHC to catch bugs in my documentation.
related: "System F, how to embed polymorphic domain specific languages in Haskell and OCaml." (Lindley, Univ Edinburgh) http://homepages.inf.ed.ac.uk/slindley/papers/embedding-f-draft-june2012.pdf 
Simon Peyton-Jones states, near the bottom of ticket 4359: &gt; Simon and I are deeply under water with other aspects of GHC, which makes it hard to devote attention to new features. That's likely true of everyone else working on GHC. So I'm with you in spirit, but the only way it will get done is if someone just does it. (And by someone, I mean you.)
&gt; Your browser may not be supported. Are you using a modern browser? Runtime Error in Main module: TypeError: Object # has no method 'clickedOn' This happens on Light Box demo. I'm using Chrome. 
&gt; And if I am to believe your argument, the only way to Haskell is with emacs as well. No, no, certainly not. (I said I use vim) I just disproved the reasoning based on habits. And this kinda includes saying "everyone uses that so I should use that too" (as in that case, you follow the habits of the group). Be careful, you're close to blaming me of what I condemned. Should jimenezrick have said "I prefer the way vim works" (a bit like you did) then it would've been fine with me. The problem was the "vim is an old friend", which suggested he haven't maybe even looked at what emacs can provide to him and that then he was following only his habits (that's maybe not the case, but that's how it felt). Thanks for your argumentation. Actually, using emacs with a mod like Evil might make everyone agree. &gt; This all smacks of the "Eclipse is the only way" religion in the Java world. The problem is that a lot of stuff is designed with the assumption that developers will use Eclipse.
Your points are completely correct, but doctest can help mitigate the problem a bit by making sure your code samples are correct. From experience writing the Yesod book, code snippets being out-of-date is usually a very good indication that you need to check the prose as well.
I didn't spend much time thinking about it tbh, but I don't think there's a chance for data loss. The calls to `await` should only return `Nothing` if the socket is closed, in which case losing the data isn't an issue.
Doesn't haskell have a numerical library that does *somepart* of matlab, like scipy (AFAIR it's just imports from C) I really like the way numerical algorithms are represented in functional paradigm, and with an interpreter, it get's so easy to test/compare results.
Except when it isn't. bmiTell :: (RealFloat a) =&gt; a -&gt; String bmiTell bmi | bmi &lt;= 18.5 = "You're underweight, you emo, you!" | bmi &lt;= 25.0 = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= 30.0 = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" 
 bmiTell :: (RealFloat a) =&gt; a -&gt; String bmiTell bmi | bmi &lt;= 18.5 = "You're underweight, you emo, you!" | bmi &lt;= 25.0 = "You're supposedly normal. Pffft, I bet you're ugly!" | bmi &lt;= 30.0 = "You're fat! Lose some weight, fatty!" | otherwise = "You're a whale, congratulations!" I was convinced, but now I am not. I might be hammering a dead point, but I think there might be a needless inconsistency that inhibits understanding for new learners.
But in the text editor and the visual pattern, it is not. It is identical. Why trip up new users? This is my point.
The guards there should be read as qualifying the specific situations in which `bmiTell bmi` is equal to each of those string expressions. There's still an asserting of equality going on there. Contrast this with `case`, where you can't tell me what two expressions you're saying are equal. I can type a `case` statement in GHCi and get an answer, without ever asserting any kind of equality at all.
This sort of pattern matching definition is extremely common in mathematics. It depends on your background.
I've thought about something like this when realizing I couldn't use "trhsx" for XML literals together with "she" for idiom brackets. At least not without a custom build system. Another issue with using preprocessors via `ghc -F` is that the preprocessor executable must be in `$PATH` which isn't guaranteed by `cabal install` and particularly troublesome with `cabal-dev`. If SugarHaskell works via Haskell packages and modules, that issue would be no more. Yet another issue is that things like HSX don't work from `ghci`. I've sometimes thought that HSX might do better as a GHC extension, for this reason and the ones above, but perhaps the better solution is to make SugarHaskell a GHC extension? In the paper they mention that "SugarHaskell is not integrated in the Cabal build system or the ghci interactive Haskell interpreter." I think it could make sense as a GHC extension, to complement TemplateHaskell/QuasiQuotes. Edit: the paper mentioned a HSX that is different from the one I'm talking about. I'm talking about the one from HSP for XML literals inside Haskell source files. I'll also add that some people might be opposed to coupling SugarHaskell to one compiler (in being a GHC extension), but it could additionally be provided as a decoupled preprocessor the same way there's one for the arrow syntax. Edit: As a GHC extension, SugarHaskell might also be able to yield better error messages for incorrect syntax, or incorrect code involving correct syntax. For example with HSX, line numbers are often wrong and although work is being done to improve that case (by using the exact-printer of `haskell-src-exts` instead of the pretty-printer), there's still the issue that error messages from GHC will show the generated code rather than what you actually have in the source file.
I was specifically talking about binding it to a name. 
It seems indeed. Last release is quite recent.
Going the way of [plai](http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/), I'd call it "type-case"ing. The function takes only one type of input, but you can still provide separate bodies for the different constructors for that type. Or just call it function definition with "pattern matching".
FTFY: (State **s** :++: []) a
Guards have nothing to do with functions, either. blorf :: String blorf | 5 &gt; 10 = "Hello" | 10 &lt; 5 = "Goodbye" | otherwise = "Salutations"
The two points that were made by others were = signifying the left side being equivalent to the right, and = being used to bind a function definition to a name. There are surely enough symbols for there to be uniqueness rather than having meaning switch on context.
Excellent question! As a subject of consistency there are many ways to look at it . First, I can't see a good reason for repeating names. You might even get rid of patterns that don't change. Pattern matching with the function name included must still be valid if you want to be able to omit the type declaration and as a matter of choice and I guess that's why chose to indent there. (-&gt;) is more interesting. One way to look at it is typing. ghci says its kind is (-&gt;) :: * -&gt; * -&gt; *, relating two types to another type whereas in pattern matching you are relating constructors and values. You could of course choose to have (-&gt;) act on everything but if you want polymorphism in your language you just have to fix some parts to avoid ambiguity. Now, it looks like (-&gt;) is acting at value level in lambda expressions which means that it refers to at least two different things depending on where it appears and ghci only told me about the type level thing (the function type). The notion of (=) is something very special. It states that both sides are equal for some vague definition of equal and at the same opposite (for some sufficiently broken definition of opposite). You are always deconstructing on the left and constructing on the right. Its type level buddy is (::). On the other hand, (-&gt;) has a very constructive feel and thus a clear preference to either be matched against on the left or used to construct on the right.
[Composing Monads Using Coproducts [pdf]](http://isi.uni-bremen.de/~cxl/habil/papers/icfp02.pdf) gives some details, although they use a different construction. The OP is using the universal property of the coproduct to give the datatype: any monad morphism from `m :++: n` to `r` consists of a morphism from `m` to `r` and a morphism from `n` to `r`. An element of the coproduct can then be represented by a function that takes a monad morphism out of the coproduct (= a morphism from each component) and gives the image of that element. The linked paper defines the coproduct (which they call `Plus`) explicitly as data, and then uses a function `coprod f g` to combine monad morphisms out of the components into morhpisms out of the sum. Their representation had some efficiency problems; the OP's might work better.
Very insightfull, the type signature makes everything stand firmly.
Hmm... I hadn't tried to run something as complex as State... I'll have to think about this. Edit: I guess it can't be run without a State monad transformer...
The advantage that monad transformers has is that monad coproducts don't exist in general! Monad products always exist, but you can't define monad coproducts once and for all. Monad transformers each exist for a different reason. ReaderT e exists because (-&gt;) e can distribute out over any other monad. WriterT e exists because (,) e can distribute 'in' over any other monad. StateT exists because you can sandwich a monad between an adjunction and there is an adjunction such that (,) e -| (-&gt;) e. ContT exists because (_ -&gt; r) is self adjoint, and you can choose to make r into m r without change. (Also, because ContT is a codensity monad/right Kan extension). All of these are based on different principles. Monad coproducts on the other hand are ill-defined for many monads. What you've tried to define is something where you have a monad homomorphism to embed effects from two other monads. When you go to compose effects from two non-ideal monads you'll fail. [Lüth and Ghani](http://delivery.acm.org/10.1145/590000/581492/p133-luth.pdf?ip=67.186.132.205&amp;acc=ACTIVE%20SERVICE&amp;CFID=81355505&amp;CFTOKEN=35332427&amp;__acm__=1341128947_8406a316685aadb38ac6fb4502b66b4b) took a stab at general monad coproducts, but later Ghani went off to do things other ways. There *do* exist monads for which you can freely take their products with other monads in the same family, we call these 'ideal monads'. They were explored by [Ghani and Uustalu](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.2698). I had them packaged in category-extras, but haven't repackaged them since.
Check out the github page for examples: www.github.com/liamoc/wizards
Can you be more specific with where I failed?
Sure. Consider `MonadWriter`. A number of the actions for that monad transformer take the monad in negative position and many for other monads take multiple monadic values and combine them in non-trivial ways, in the case of `MonadWriter` the particular `pass`, `listen` and `censor` operations are a problem. You can only `listen`, `pass` or `censor` pure `Writer` actions, not their coproducts with some other monad under your attempted construction. The same issue arises with `local` in `MonadReader`, `callCC` in `MonadCont`, `catch` in `MonadError`. A related example to the latter is when you try mixing the non-determinism that Dan Doel suggested with other effects. How do you execute `mplus (get &gt;&gt; mzero) get` in the coproduct of `[]` and `State s`? All of these constructions fail for coproducts, but are not a problem for the appropriate monad transformers. Pretty much the only things that actually work are the sort of well-behaved core of monadic actions that trivially distribute over one another, ask/asks, get/gets/put/modify, and tell. These work because they have all have linear positive occurrences of the monad. Almost any other monadic action fails to compose properly. The coproduct by a pair of monad homomorphisms yields a series of interleaved actions from each monad taken separately. Lüth and Ghani mention this briefly in the conclusion of their paper, but they are a bit overly optimistic about how bad it is in practice. A sign that all is not well with monad coproducts is that there exist constructions that should admit multiple semantics. `StateT s (ErrorT e Identity b)` and `ErrorT e (StateT s Identity b)` mean very very different things when errors happen, but the the coproduct of `State s` and `Either e` commuted either way should be indistinguishable -- not that you can recover from that error in the monad coproduct in the first place. On the surface, the lure of monad coproducts is strong, and you aren't the first person to make mistake that they might be nicer to work with in practice. I am only so glib with my supply of examples because I used to believe I could make it work too. =/
I assume the problem lies in the fact that the more advanced monads have additional structure, while monad coproducts only guarantee, well, monad coproducts? I guess there are no obvious solutions to that problem... But monad coproducts still exist in general, they just don't necessarily have the same structure as their elements.
That part isn't problematic. If you have a monad homomorphisms then it doesn't matter if you use &gt;&gt;= before or after embedding. The issues with this construction arise elsewhere.
Well the original question was what advantage do monad transformers have over monad coproducts. ;) Being able to actually use all of those operations I named makes a pretty compelling list so far. Also the construction you provided is may not technically be the coproduct, as I recall it the coproduct is a quotient (not sure in your case and too tired to check right now, but that definitely holds in Lüth and Ghani's construction) and the use of this construction also has certain technical restrictions on the types of monads it applies to (finitary layered monads). This was a recurring theme in the two papers of Ghani that I mentioned above.
I gave it a try and while it worked as far as I could tell it still is far from a decent editor, in particular the choice of black on black text for the all three themes provided by default for the "import" and "import qualified" parts of the Haskell syntax indicates some bad quality control.
It's vim embedded into a tmux section! For the tmux stuff i recommend the book [tmux: Productive Mouse-Free Development](http://pragprog.com/book/bhtmux/tmux), while my vim setup is mostly taken from [this blog post](http://www.drbunsen.org/text-triumvirate.html). I also used to have a public repo with all my dotfiles and plugins, but now it's private because it contains some private stuff (e.g, the license for the Viable plugin for Eclipse), but you shouldn't have problem to create a good environment from the aforementioned references. As a plus, I use syntastic for the "on-the-fly" syntax checking :) HTH!
He's a Haskeller (who doesn't yet realize it..) stuck in Python! :)
Sure! Here you are: [tmux.conf](https://gist.github.com/3028721)
I've tried Evil: it's handy but it would involve too much work customizing it to my needs: sometimes (for me at least) it's much more pragmatic vim + a tmux pane with a ghci session: I can easily swap via keybindings through panes, edit a bunch of files in Vim, then jump back to ghci and reload the file :)
To make my point more explicit, a value of type: (State s :++: []) a roughly corresponds to: forall m. (MonadState s m, MonadPlus m) =&gt; m a I suppose it's not even that good, for reasons Ed has mentioned. Anyhow, you can write these all day, but they're useless unless you put together some concrete type that is an instance of both of those classes. That is what monad transformers do automatically. So, if you use monad coproducts, you then either still need monad transformers, or you need to manually implement the monads that can be built with transformers. It's also kind of obvious if you think about it that coproducts can't work out something fully determined for this. There are two (at least) ways to combine state and nondeterminism. But, A + B is isomorphic to B + A, so coproducts can't be choosing between them for you. But transformers do. This also shows up if you look at Dan Piponi's blog article on Lawvere theories. Taking the product theory just gives you a theory that has both operations, but you need to manually write an interpreter of some sort that defines the interaction between them. You can figure it out by rote based on the way you want things to commute, but it doesn't happen automatically. With transformers, the order you plug them together makes these decisions, and the result is already an instance of most/all the classes you want.
It Is [tomorrow theme](https://github.com/chriskempson/tomorrow-theme), to be precise tomorrow night bright :)
You might have better luck with something like: forall r. Monad r =&gt; (forall a. m (r a) -&gt; r a) -&gt; (forall a. n (r a) -&gt; r a) -&gt; r x Where `r a` is supposed to be both an `m` and `n` algebra for all `a`.
Is there anything you can do with tmux I can't do with xmonad?
Yep :)
The uppercase converter does *not* assume complete utf8 sequences. I actually considered going into some detail on that, but decided it would be too confusing. Basically: the `decode` `Conduit` will retain any incomplete sequences from one chunk and use them next time around. If instead you used `CL.map Data.Text.Encoding.decodeUtf8`, you would have alignment issues. As for everything else: you're correct that there are alignment assumptions in the examples. In real life code, this would not be an appropriate approach, you would want to set up some kind of proper buffering as we do in `warp`. I kept it simple for demonstration purposes only. That said, pairing up with `attoparsec-conduit` is pretty easy, and then the buffering *is* completely handled for you automatically.
I'm curious: what does Tmux give you over running shells within Emacs? I've been using M-x shell and some related stuff to manage a bunch of shells, including remote ones, all within Emacs. Moreover, this fits well with the various comint-based modes like inferior Haskell. Some of my friends have suggested Screen and Tmux, but I'm not sure what I would get out of them past just using Emacs.
Speaking of those examples. I am currently reading through the paper version of the Yesod book and I noticed that most of the examples do use handwritten forms. I was wondering if there was any particular reason for that.
Long live the Haskell!
Do your vim and ghci sessions communicate?
Wow, and CtrlP seems even more laxist in the way it searches. (foobar will actually match "foo-anythingelse---bar") Nice ;)
No, they don't. But :r it's simply too handy, so I don't perceive this lack of communication like a problem :) If you want something more interactive, try VimShell.
Looks very nice. I'm not sure about using single letters as part of an exported function name though. Single letters almost always refer to a local variable, for me.
&gt; You only receive a Nothing once when you use awaitF. Any attempt to request more input after you receive the first Nothing will terminate the current Frame using the upstream return value. This is still a horrible hack in my eyes. It introduces a stateful protocol into an otherwise elegant system.
This is one of the reasons that I'm switching to indexed monads. With them I can eventually get rid of this behavior. However, the scope of this release was to simply translate v2.0 into indexed monads.
Any suggestions for a longer name? Another option is that I move `U` and `D` to a separate module, i.e. `Control.IMonad.Restrict.Interop` or something like that, so that you don't get them by default or you can import that module qualified.
How (if at all) does this relate to [monatron](http://hackage.haskell.org/package/Monatron)? (Just out of curiosity)
I'm not sure of the specific cases. Sometimes I did so for illustrative purposes, sometimes to reduce the cognitive load. But I think most of the time it was when there *wasn't* a user-facing form being generated, just grabbing some query-string parameters.
You can do this using the `WriterT` monad transformer, and then just changing the interpreter: logDebug x = tell (x ++ "\n") runPure = fmap fst . runWriterT runIO m = do (a, w) &lt;- runWriterT m liftIO $ putStr w return a Edit: oops. I just realized that this does not do the same thing because the feedback is not immediate. In that case you want to use the Pipe monad transformer: logDebug x = yield x runPure p = runPipe $ forever await &lt;+&lt; p runIO p = runPipe $ forever (await &gt;&gt;= lift . putStrLn) &lt;+&lt; p
Yeah, I tried it, but I didn't like the way VimShell worked (seemed a bit hacky). Yes, I forgot about :reload in GHCi.
Yeah, but as said in the post, your code is stuck to run in WriterT, can't it impact the perfs if you want to run without the debug log?
Monadtron is a library for monad transformers. Monad coproducts can be used in some of the same areas that monad transformers can.
That makes sense. I'm finally starting to learn attoparsec, but I still have plenty of nasty hand-rolled code to handle partial packets, so lack of that handling jumps out at me :)
The advantage of one over the other depends on what you want them for. I think the main problem with monad coproducts is that they can be *too big* for the usual uses. For example if you take the coproduct of State s and Maybe you get both the behaviour of StateT s Maybe, **and** MaybeT (State s), as well as many other behaviours. The layered presentation of monad coproducts of Lüth and Ghani shows this quite clearly. Hence, if what you want is, say, exactly StateT s Maybe, you can't get that with coproducts. 
While we're at it, which font is that?
Yes, it would impact performance.
How would the indexed applicative in index-core compare to what bifunctors provides: http://hackage.haskell.org/packages/archive/bifunctors/0.1.3.3/doc/html/Data-Bifunctor-Apply.html I have not looked at either in detail, so I am asking out of ignorance. We were hoping to switch reform to use bifunctors (instead of our own homebrew IndexedApplicative), but since I want to uses pipes in the future, perhaps it makes more sense to use index-core? (once it exports an indexed Applicative..)
Yeah, I just read about the patched fonts. Thanks, though. You inspired me to go on a hipster-spree, modernizing my themes/setup/plugins/life.
It's pretty much going to be cut and paste from Conor McBride's paper, and notationally it looks the same as the ordinary Applicative style. Keep in mind that it would only work for restricted monads, though.
With a slight generalization, you have a simple but flexible logging framework: data Priority = ... syslog priorities ... class Monad m =&gt; MonadLog m where log :: Priority -&gt; String -&gt; m () This lets you have logging "anywhere", but lets your application decide how log messages are handled. Unfortunately, I didn't see any packages on Hackage that do this. In general, "MonadFoo" classes can introduce functionality into your application code without cluttering it with monad stacking details. You still have to implement monad stacks somewhere, but now you can do it separately.
That's correct. joinR is compose and returnR is id.
A slightly stronger version of this is what I use in trifecta. The main difference is I include source locations and rendering hints.
I for one don't think that Frame and Pipe discussion should be separated just yet; it is useful for the interested learner to walk through the enhancement from Pipe to Frame. But for the sake of disambiguating yourself from Paolo's poor choice of package naming, it might indeed be wise to create a `frames` package (not to be confused with the existing, probably abandoned [frame package](http://hackage.haskell.org/package/frame)).
Have you worked through the details of that?
&gt; Paolo and I still aim to merge our packages in the future Ah, of this I was unaware. Roughly when in "the future" do you think the packages will merge? What discrepancies remain unresolved as of this Frame reformulation?
Glad to see all of the work going on with HAppStack :) One note: the link to the elm-happstack example currently points to an out of date version (a branch of the central repository). [This version](https://github.com/evancz/Elm/blob/master/Examples/elm-happstack/Main.hs) should be up to date. edit: both links are good now! See child comment.
There is still a lot to do, so my guess is probably half a year to go at least. Mainly, we need to merge his exception handling into pipes and figure out how to do parsing. I'm about to release a parsing implementation, though, layered on top of frames, and if he agrees with it then we'll be that much closer to merging. The exception handling is trickier to merge in, but I think it is still doable.
Yeah, I will try that out. In my own code I just desugar the Monad in performance critical sections (since I don't know the overhead of U yet), and use U otherwise.
It's not internal, it's enriched.
He was saying that indexed monads are categories _in_ the category of endofunctors, which means internal to me. Perhaps it's just a misunderstanding :)
The way `tailcalled` meant it was that you treat each functor as a morphism between its initial and final index, where `joinR` composes those morphisms: joinR :: m i j (m j k r) -&gt; m i k r ... and `returnR` creates the identity morphism: returnR :: r -&gt; m i i r When viewed in that light, the three laws I gave are the two identities and associativity of composition.
Yeah, it is ambiguous, but an internal category in the category of endofunctors would be a really strange beast (do endofunctor categories even have pullbacks, in general?). To see that it's indeed an enriched category with the same objects as `Hask`, you can define the `hom` bifunctor as: hom :: Hask -&gt; Hask -&gt; Func(Hask, Hask) hom a b = \r -&gt; m a b r where `m` is the indexed monad. Then, as Tekmo said, `return` is the identity and `join` is composition: return :: Id -&gt; hom a a join :: hom b c . hom a b -&gt; hom a c and the generalized monad laws correspond to identity and associativity for the enriched category.
No no, don't get me wrong, I understand why it is the way it is. It's just unfortunate is all. I didn't mean to imply that the people working on Agda disapproved (I've gotten no indication that they do). Rather that the language itself does. The heavy use of Unicode means you need some kind of special entry method in order to code quickly. Even more damning, the interactive structure with sheds etc really requires a good integration between the human's text-editing and the compiler's type-checking--- more integration than for the standard REPL style of interaction. Personally, I consider this interactivity to be a core part of what "Agda" is. (Compare to Coq which is 'usable' via REPL since it type-checks things sequentially, one declaration at a time.) And I know some people feel the same way about the Unicode in Agda. While both of these features *could* be implemented in any sensible editor, so far they're only in emacs. Thus, if you're not using emacs you're really not getting the whole experience of programming in Agda. That's what I meant by saying that "[Agda] actively disapproves of non-emacs usage."
You should check out the `&lt;~&lt;` operator from the internals, which is basically the primitive category of indexed pipes that just adds a `close` constructor without any notion of folding or finalization. It's very simple and very close to the implementation of composition for pipes and can help you reason about how indexed types mesh with the category of ordinary pipes. Frame composition is layered strictly on top of `&lt;~&lt;`.
It's true that Emacs already provides a lot of the benefit itself. Sometimes, the shell itself is not functional enough within emacs to run programs like less for example or more complicated programs like htop or what have you. Yea there's eshell, but when I have shells, eshells, erlang consoles, etc. I sometimes get tired of all that and just want one god damn normal shell *somewhere* :)
Yeah, I see that. I actually like the Emacs shell more in a bunch of cases--for example, I find navigating around a bunch of output easier in the Emacs shell than in less. I also find it much easier to navigate within Emacs than switching to an external program, although that's obviously configurable. I did have some problems with Git, but happily I managed to turn paging off. Now Git output is navigable within Emacs, which I like. For things top, man and a bunch of others (Erlang consoles, no doubt, but I've never used them) Emacs tends to have special modes. So you can do M-x top or M-x manual-entry rather than running them in a shell. I personally find this more convenient, but I definitely see how having a normal shell would be preferable to some. I guess the fundamental difference is that, for most things, I *don't* like the normal shell. I've nothing against Bash, I just don't like standard terminal emulators. So I just use M-x shell a lot. I'm suspecting that Tmux would not offer anything too interesting for me in this case, at least based on your description.
Yes, this is pretty much my attitude. When I program in CL or Clojure or any other Lisp dialect, the temptation of using Emacs it's simply too strong (more than a temptation, I would call it an urge :D )
As far as navigating around in less goes, I use Vim as my [pager](https://github.com/rkitover/vimpager/).
After signing up and trying to reply to your topic, I get "replyToTopicId is required".
Great set of slides. Regarding cloud haskell, you mark "Global Consistency" as "Not feasible" in distributed programming. Global consistency is feasible, it is just a library issue - someone needs to write the libraries for cloud haskell. Two libraries that are on my wish-list: [Transactors: A Programming Model for Maintaining Globally Consistent Distributed State in Unreliable Environments](http://wcl.cs.rpi.edu/papers/popl2005.pdf) which is apparently implemented in [Scala](http://doc.akka.io/docs/akka/2.0/scala/transactors.html) [LibPaxos](http://libpaxos.sourceforge.net/) 
It is because I install a incorrect dependency.. Has been fixed.
I actually went to a lot of trouble to get PDFs with the intermediate animations, because I wanted to write the slides in PP but use PDF when giving the talks. But I have the originals so I could make PDFs without the animations steps too.
hrm, I've gotten quite tired of websites recently that want me to give them my email and a new password in order to create an account. Any chance you could work [authenticate](http://hackage.haskell.org/package/authenticate) into your app? 
Looks awesome! Thanks for posting this. I'm excited to try the i18n snaplet. How many hours do you think you spent working on the site?
Neat! Let me know of any features you'd like to see.
Oh I see. I didn't understand what `runIdentity` was doing. I still have no clue about Monads but at least I begin to get it. Thanks :)
A few things I want came to mind as I converted my files: * A way to automatically generate a link to the github source version of the article, and provide the git command for acquiring that specific file. I suppose this might be too specific to be worked into BlogLiterately, but I still want it. * Access to the `it` variable in sequences of ghci interactions. This is sort of a corner case, but I did have to adjust one of my blog posts to take into account the fact that `it` is apparently not available. * A way to assert that a given ghci command will produce a given value. I want this for two reasons: 1) so that if someone is looking at the source code they can still see the expected result, and 2) as a sanity check to make sure the command produces the result you want it to. Presumably a textual comparison between expected output and actual ghci output would be sufficient. I really do love the ghci integration, though. BlogLiterately came with all of the really important features that I needed to painlessly turn my pre-existing .lhs files into some beautiful html (if I do say so myself).
In terms of git, have you given the magit package in emacs a try? It's not bad for day to day use. Occasionally I have to go to the shell to do more complex things though. I have a fairly customized urxvt shell which I like a lot. I'm also very comfortable piping output and using awk + sed so I never find myself needing to scroll through output anymore. One thing tmux could do for you is let you attach to it remotely, which is nice for me too. I leave emacs running in a tmux session, then when I go home, I can attach to the same thing and continue where I left off.
Side note about the post saying: "For fun, we can simplify preincrement further using a common helper." import Control.Applicative preInc = modify succ *&gt;get postInc = get &lt;* modify succ 
Thanks a lot, I was eagerly awaiting a blog post on this topic. I am happy you took the time to do it!
Nice tying and nice blog! I also wanted to jump in and say a word about `mfix`. You can tie as you go along with a function like this: tangle :: Monoid k =&gt; Knot k s a -&gt; Knot k s (a,k) tangle knot = mfix $ \ ~(a,k) -&gt; local (const k) (listen knot) Then if you `tangle` at the start, you don't need to supply tie any initial environment (an error/undefined will do). Cheers, danr 
I did exactly this when writing the [compiler for my programming language Sindre](https://github.com/Athas/Sindre/blob/master/Sindre/Compiler.hs), where the symbol table of functions output by the compiler was also used as its input. The only (as yet unresolved) problem was error handling, which I have so far been forced to do with `error`. I had great trouble making the compilation function return a proper error value, without losing the ability to tie the knot.
urls are down for me :(
The latest version of pandoc is 1.9.4.2, not 1.9.2. It seems it has a flag `blaze_html_0_5` to enable the use of `blaze-html 0.5.*`, but it is off by default. I'll look into this a bit more, honestly I'm not sure why it works for me!
Would work better if the second image was visible...
Yes, your question is a good one. My [initial approach](http://stackoverflow.com/questions/11060565/tying-the-knot-with-a-state-monad) used only `State`, and although it worked it was more brittle. It was too easy to mistakenly feed something from the "future" back into the "past" which, of course, changes the future... Picture Doc Brown lecturing Marty McFly about temporal paradoxes. This isn't a shortcoming of `State` per se, really just an API design issue. Regarding your edit: have another look at the signature for `runRWS` (ignoring anything to do with knot-tying for the moment). The first argument it takes is the monad, the second argument it takes is the reader input, and the third it takes is the initial state. The tricky bit is that, in my specific case here, the thing I'm feeding it as reader input is the eventual output of its own writer. One thing that might or might not be a point of confusion for you is that `runRWS` *doesn't* take an initial writer as one of its arguments. Since the writer is a monoid, `runRWS` (and of course `runWriter`) just start you off with the monoid's `mempty`. For completeness I should also clarify that when you `tell`, it's using the monoid's `mappend`.
Really interesting talk. Malheureusement, slides are not readable in the vid and I couldn't find them online. Also audio is echo-y and fatiguing to concentrate on.
I used this exact trick in my [acme-hq9plus package][1], to provide the "program's source code" to code that is generating it. Also, here's a monadic version of `tie`: type KnotT k s = RWST k k s tie :: MonadFix m =&gt; KnotT k s m a -&gt; s -&gt; m (a, s, k) tie knot s = mfix $ \ ~(_, _, k) -&gt; runRWST knot k s Another interesting knot-tying gem for IO is the [procrastinating-structure package][2]. [1]: http://hackage.haskell.org/packages/archive/acme-hq9plus/latest/doc/html/Acme-HQ9Plus.html [2]: http://hackage.haskell.org/packages/archive/procrastinating-structure/latest/doc/html/Data-PVar-Structure.html
Sometimes when I'm editing commit logs, it's nice to do it in an emacs that has all my files in open buffers. Also, I can refer back to the status when I'm writing the log so I don't have to remember what I've changed. Yea you can connect to a remote emacs server, but this doesn't have all my other shells open with various consoles and all that. I have yet to try remote pair programming (haven't needed it yet) but it sounds neat.
I believe these are the slides. http://www-fp.cs.st-andrews.ac.uk/tifp/TFP2012/TFP_2012/Turner.pdf
I didn't really understand Conor's point about distinguishing lists as values and as computations. Does that mean he thinks the `join` operation is somehow unsound?
I am just getting convinced of Emacs power (interestingly enough it is just so far C-[#] C-x TAB that is doing it so far... So how do I get into this other thing...
I'd like to hear more about why he thinks type classes were a bad idea.
I'm not fond of these talks where someones describes "The History of &lt;this domain&gt;", and it more or less becomes "The History of what I did in &lt;this domain&gt;". Of course, people have an unique personal perspective that is interesting, and David Turner is entitled to pose as a crucial person in the development on lazy functional languages, but sometimes I think we should ask the young ones, that have only learned from other people and have had to cross-check their facts to get a wide perspective, to do those retrospective talks. Frankly, the Haskell slide (I did not watch the video) is a bit preposterous. The main innovation is the switch of guards to the left-hand side of the equation. Was that meant to be a joke? Some of the real features of Haskell (type classes, monads, ... no, not even the type system) are mentioned in a one-liner at the end of the slide. A question on the historical side, page 4 in [the slides](http://www-fp.cs.st-andrews.ac.uk/tifp/TFP2012/TFP_2012/Turner.pdf). Is it correct to say that graph reduction techniques "showed that the efficiency disadvantage of normal order reduction can be overcome"? That is the only mentioned implementation device. I was under the general impression that those ideas had been abandoned because they're actually not so efficient, and GHC uses good native code generation to get reasonable performances, plus excellent optimizations to truly overcome the overhead of lazy evaluation. Granted, the STG is still present in some form, but it is more an intermediate compilation language than a bytecode.
Ghc uses compiled graph reduction. Of course, with enough optimization different underlying techniques start looking the same. Graph reduction can be implemented by some kind of interpreter (like David Turner did) or by compiled code (like Thomas Johnsson and I did).
No, he means that we ought to have two different datatypes - one for lists of things, one for non-deterministic computations (what he calls "prioriatized choice"). Or that's what I get anyway. While that answer offers interesting insights, I think it is an horrible answer there, given that the question was clearly posed by a beginner, who will probably be much more confused now.
Not at all: `join` makes perfect sense (see my other recent scribbling on SO). Values embed in computations by `return`: a value `v` becomes the computation which does nothing but return `v`. Computations embed in values silently in terms of types and syntax, but with a change of sense from "doing something to get a value" to "being a way to get a value". That is, Haskell blurs the distinction, which is great for writing funky control operators, but rotten for ordinary programming with values which happen to arise by effects. Haskell needs punctuation to distinguish these two modes of use: whether you want to work with the value of the computation done or the computation itself undone. It is syntax not typing which tells a cake arising by baking apart from a cake recipe. Both are edible but one tastes better than the other. I'm working on an alternative, embodied in the Frank prototype. The idea is to spend punctuation differently, so that values embed silently in computations but computations must be suspended explicitly to yield values. The key is to be clear about the administrative separation of that part of the type which describes permitted effects and that which describes values. Once you can see which are the effect permissions demanded and granted, it's easy to check if the latter include the former. Ordinary programs are cleaner and you don't need any special syntax for working in a monad (because you're always working in some monad), but control operators cost a little more (because thunking and forcing are explicit). Haskell makes a dog's breakfast analysing types as effects-versus-value, with different conventions depending on whether the permitted effects are just partiality (so the whole type is the value) or something more drastic (so the type is an application whose argument is the value). As a result, ordinary applicative programming with effectful components (as in ML) is made artificially ugly and monad transformer stacks require us to do explicit lifting. I think it's worth trying to do better.
I didn't say what I meant, just that it was another story. I was trying not to let my discomfort with an orthodoxy of which I disapprove prevent the explanation of a useful coping skill. The alternative I have in mind is not at all what you say that I mean, and I won't elaborate on that in this particular comment. The main point was to try to teach how to distinguish as-value and as-computation uses just from reading the do-notation, without desugaring it. That's a useful skill, I hope. I also wouldn't presume to speak for beginners. It's possible that my answer will make less sense rather than more, in which case the interactive nature of the site will provide the means of remedy: voting, commenting and follow-up questions allow for indication of what works as an explanation and where there is room for improvement. But it is a horrible answer, because it's intended to persuade anyone who gets its message to be a little dissatisfied with the Haskell status quo. What can I say? I'm a horrible person.
Have a look in ~/.cabal/config. Mine looks like this with the new defaults on OS X... install-dirs user prefix: /Users/hamish/Library/Haskell/$compiler/lib/$pkgid -- bindir: $prefix/bin It also creates symlinks to these at symlink-bindir: /Users/hamish/Library/Haskell/bin 
Stackoverflow is also a good place to ask questions like this
This guy invented Miranda, which has some history back at the beginning of Haskell. That is probably why.
Don't get me wrong: I love your work and your papers have provided endless entertainment. That answer itself was interesting. However, I think that this kind of answer perpetuates the aura of impenetrability that is around Haskell. Personally If I had read that 2 years ago while I was starting, that answer would have discouraged me and make me feel stupid. It has nothing to do with the fact that you say that [] should not be overused like that (I actually tend to agree with that). I think an explanation on how the list monad works would have made things clearer, while the first two answers just assume that the OP knows what the list monad does. If he really knew, he would not be confused!
I think there's something nice about also programming in the Identity ambient monad (or no ambient monad) where you get all these nice guarantees about (a + b) == (b + a) and many mechanical refactorings like that are valid. Once you add an ambient monad to most computations, you take away useful guarantees such as these. You have to spend more time thinking about the applicative-style-ordering... It loses some of the benefits of pure functional programming. I think an ambient monad could be nice, but maybe it should be an explicit invocation of applicative style?
&gt; Note: this technique may be dated. What I dislike so much about many FOSS compilers and frameworks is that support for Windows is often pretty much shit. It's true that Microsoft is making it (deliberately?) hard, but on the other hand, developers aren't making it easy for anyone either. We need a unified library-and-compiler system for Windows. /rant ^(-edit- or preferably, I need to be able to play all my games on a FOSS system with reasonable reliability and performance, which isn't possible right now)
It's a pity Haskell doesn't let you program in the Identity ambient monad, then, isn't it? The applicative (ie ordinary application, not &lt;*&gt;) syntax is often nice even when there are effects about. You get to choose the ambient monad, and it's documented in types. The present situation is that monadic-style code is *both* ugly and hard to reason about. Maybe that's lipstick on a pig, but I remember it fondly from ML days.
So, indeed, Windows makes it exceptionally wearisome to develop in, it's outright unfriendly towards programmers. So you can't expect anyone to bother working on it or going out of their way to support it for an open source project unless they like Windows, or are being paid to like it. There are few Windows users in the Haskell community, and so the subset of people willing to put effort in to add the necessary special cases and patches to make things work on it is minuscule. I use Linux and OS X at home and at work, I couldn't give two monads about Windows, but I do my `withSocketsDo` and use the portable filepath library with `&lt;/&gt;`’s instead of `"/"`’s. I won't go out of my way, though; in the most sincere way: patches welcome. Windows users need to provide support for their preferred OS just as FreeBSD or Solaris users are expected to, no one else is going to do it for you (unless you pay them).
&gt; Python's list comprehension syntax is taken (with trivial keyword/symbol modifications) directly from Haskell. The idea was just too good to pass up. Honestly I'm still not convinced about list comprehensions. Whenever I use one I end up changing it to normal functions later anyway. They're not composable and the syntax is awkward. If I had to name one feature of Haskell that I could easily have removed, this one springs to mind first. For me this is like taking `n+k` patterns from Haskell. 
You're very quick to assume the worst. It would be annoying if shifting the ambient monad did not involve some explicit syntax, so that the context for each expression was always clear.
&gt; For the same reason, I also have no way to try out any GUI programming. You may want to try working in the opposite direction: expose certain Haskell functions as a dynamic library, construct the GUI in the environment's native tools, and call the exposed Haskell functions as regular C functions. I haven't tried this in Windows, but so far it works fine in OS X.
Well, nothing, but I find it less clear than the list comprehension.
Ah, good idea, I'll add that.
Perhaps it should then say "function overloading".
The ONLY reason I am beginning to understand Haskell is by using Scala. Haskell uses so much math terminology, its difficult to understand. Coming from Scala, I see the practicality of certain structures first, then I can finally understand Haskell. It would be nice if Haskell had a 'plain language version'. "Its really cool, you can do this, and this solves as common problem" and then you dump Applicative functors on their lap. I didn't begin to understand functors and applicative functors till I began to use Option and Either and flatmap in scala.
A matter of taste, surely. I'm a Lisper, so I'm pretty syntax-phobic.
&gt; The comprehension seems needlessly recondite to me. I’m not taejo, but Python’s comprehension syntax ties in rather well with the set-builder notation familiar to most. It’s usually more concise than mapping and not at all abstruse after only a moment’s familiarity. It’s also a bit more general, e.g. [(x**2 + y**2)**0.5 for x in xs for y in ys if x != y] returns the distance between all unequal points in xs and ys; that sort of nesting gets ugly with maps.
What yatima said. But also, `map (1+)` says exactly what I want to do, while the lambda adds extra syntax and requires me to make up an extra name so I might as well do the comprehension.
Oh, now I get it -- if you install BlogLiterately from scratch it works fine because cabal's constraint solver figures out that it needs to turn on the blaze_html_0_5 flag to get everything to work out. But if you already had pandoc installed against an older version of blaze-html then you will run into problems. I'll add something about this to the BlogLiterately documentation.
In general, Python list comprehensions do not desugar to `map`. Compare &gt;&gt;&gt; x = map(lambda i: lambda: i, range(10)) &gt;&gt;&gt; [f() for f in x] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] vs. &gt;&gt;&gt; x = [lambda: i for i in range(10)] &gt;&gt;&gt; [f() for f in x] [9, 9, 9, 9, 9, 9, 9, 9, 9, 9] 
My understanding is that doing return [1,2] when in the List monad is the same as doing func :: Maybe (Maybe Int) func = return (Just 1) which is why you end up with wrapped lists, as [] are just syntactic sugar right? When really he wanted to do func :: Maybe Int func = return 5 or func = Just 5 Its a bit easier to see whats going on with the Maybe monad I think. 
I believe gasche is the correct one here about Guido not liking functional aspects of Python much. I believe he gave a speech about it last year where he was a keynote speaker. Ill see if the YouTube video's still up. 
In Haskell you probably already have a function that you can pass to map or filter, or you can write one concisely like (,) or (+1). So often "(map f . filter g) xs" is nicer than "[f x | x &lt;- xs, g x]" and it's certainly nicer than "(\xs -&gt; [f x | x &lt;- xs, g x])" if you're going the point-less route.
That's the impression I got from reading [this 2005 article from Guido](http://www.artima.com/weblogs/viewpost.jsp?thread=98196) on the removal of reduce, lambda, map and filter (lambda, map and filter where finally kept) in Python 3. The header ends with "I expect tons of disagreement in the feedback, all from ex-Lisp-or-Scheme folks. :-)", which I took at that time as a form of deliberate (if respectful) provocation. I was also quite disappointed by his [grossly misinformed 2009 article](http://neopythonic.blogspot.fr/2009/04/tail-recursion-elimination.html) on tail call optimization. He's later corrected his view (he issued [an update](http://neopythonic.blogspot.fr/2009/04/final-words-on-tail-calls.html) after just a few days), but there is still no trace of TCO in Python and it's still a pain -- while, contrarily to what he says, there are various reasonable way to handle this, for example by using an explicit `tailcall` keyword with an explicit backtrace-hurting semantics, and/or storing the backtrace outside the call stack. I have to say that, while looking for those references back (I didn't remember exactly where they were), I stumbled upon [this 2009 article](http://python-history.blogspot.fr/2009/04/origins-of-pythons-functional-features.html) article, "Origins of Python's 'Functional' Features", which is much more nuanced and reasonable. Finally, I don't believe that python is "not well designed" as a language; in fact I think that Guido has handled its evolution quite well, and the PEP mechanism is, I think, a quality reference to match in the process of evolving a language. It is simply that he is interested in vastly different things than functional programmers are, and has therefore different priorities (type system clearly was not, clean semantics maybe was not either) which led him to take different choices. I don't use Python, but have some respect for this language. And both Python and Haskell have an excellent syntactical taste. That said, I still internally cringe when I think that the whole [Sage](http://www.sagemath.org/) project is implemented in Python. A truly excellent repository of mathematical knowledge encoded in a oh-so-not-exciting language. Oh well, it would probably not have worked that well in Agda²... . ¹: Haskell's guards and use of `(a,b)` to denote product *types* excepted :) ²: or [Aldor](http://www.aldor.org/).
&gt; And Haskell's comprehension syntax doesn’t? Uh, where did I mention Haskell’s syntax? My response was in regards to commonslip’s explicit reference to Python, and thus rendered in those terms: &gt; What is wrong with map(lambda x: x.p, xs) or map(lambda x: x+1, xs) in python?
Well, for me "visitorBeanSingletonFactory" is just as confusing and people are not saying that OOP is hard because of that.
Lambda syntax is crippled in Python. There's a PEP that suggests GvR supported it to deter lambda abuse. (And lambda abuse is a near synonym of functional programming).
Right, it's the operator part that's confusing and out of left field there. That said, the word "overloading" in the object-oriented language world carries an implication of "resolved at compile time". For example, you'd be told you are wrong if you say to a Java programmer that overriding a base class method is an example of overloading. Haskell, since it doesn't have subtypes, doesn't really have such a compile-time versus run-time distinction in its semantics, but certain language features can conflict with a completely compile-time resolution of type class instances, and in practice they are implemented by dictionary passing in the general case, very like the vtable passing of object oriented languages for subtype polymorphism. So be careful telling a programmer who comes from a static OO experience that type classes are akin to overloading.
Do send complaints about gtk2hs to the gtk2hs mailing list. I keep a Windows VM lying around just to make sure it builds and works on Windows.
[Oh the irony](http://www.aldor.org/wiki/). Seriously, who develops Aldor? Seems like a fun language.
Thanks for the hint, I think I'll switch from MingW to Clang in that case. MVSC has never satisfied my needs, and I only download it to get access to the official header files that are up-to-date with NT6 (a.k.a. Windows Vista and Windows 7). I don't care much for GNU anymore, in fact, I'm starting to loathe it. So many flagship GNU projects reek of decennia worth of spaghetti code and technical debt, while the developers are proud of that smell because they made it. Meanwhile, they tout having the most free licensing scheme of all, while it's actually pretty limiting (ISC is as far as I know the cleanest license) and completely unreadable (the GPL is a wall of indecipherable legalese to me). But then again, I hate everything that isn't perfect.
Definitely check out Clang. The compiler is pretty robust, but the setup on windows is not quite there yet. You have to download the sources, use a tool to generate a VS project, and compile the code. [There are detailed instructions on it](http://clang.llvm.org/get_started.html), so it's not hard, but it's not just downloading an installer. The experience is completely worth it though. The diagnostics and error messages are amazing.
&gt; cabal's constraint solver figures out that it needs to turn on the blaze_html_0_5 flag to get everything to work out It can do that? Cool!
It's weird that the audio is so bad, while he is wearing a headset. It seems the headset was not used to record the audio, but only to amplify his voice in the room.
I hope you don't take this as mean-spirited, but can you convince me why I should start learning LiveScript instead of Roy? Are there any advantages to LiveScript over Roy? http://www.reddit.com/r/haskell/comments/vhkgf/roy_a_statically_typed_functional_language_for/ http://roy.brianmckenna.org/
This actually makes me want to take a peek at the GHC source code. Maybe that was the intent of the article.
I believe it was meant to be parsed: haskell `isA` modern (functional language `like` lisp) Or if you prefer: (is-a haskell (modern (like (functional language) lisp)) 
Looks like a strict functional language.
I have a few bones to pick with your bone-picking: &gt; The only POV where Haskell['s parametric polymorphism] is more powerful [than C++ templating] is the power of the type system to enforce properties. Oh, so in other words, the POV that pretty much every Haskeller cares about above almost anything else? ;) &gt; &gt; and ad-hoc polymorphism ( operator overloading) &gt; Those two things are not the same Not the same, but operator overloading can be achieved through ad-hoc polymorphism. Not a huge discrepancy there. &gt; &gt; through type classes (ala Java Interfaces) &gt; Neither are those. ala doesn't mean "exactly the same", it just indicates similar fashion, and in this case I tend to agree (though not whole-heartedly). Now for the part I most strongly disagree with: &gt; Static type systems are actual type systems, that prove properties about programs. So-called "dynamic type systems" are more of a value tag checking mechanism for detecting runtime errors. They are not comparable on any axis. From a type theory perspective, a "dynamically typed programming language" is actually just an untyped programming language. I don't believe this to be the case at all. The entire point of a type system is to reject invalid programs and accept valid ones. However, due to inherent limitations in type systems, there are many valid programs which are rejected, and many invalid programs which are not. A "dynamic type system" merely foregoes attempting to check before running the program whether or not it is well-typed, deferring the reporting of type errors until one is actually encountered during the execution of the program. In doing this, it avoids rejecting valid programs due to the practical limitations of typechecking beforehand. Typed Racket is an attempt to provide a full type system that is capable of correctly typing all Racket programs. (Well, to be honest, its goal is to correctly type all Scheme programs, and now they are adding some features for typing Racket programs.) So riddle me this: if all Typed Racket does is add type annotations, then is Racket an untyped language, or a typed language, given a typing scheme* exists that can type it? (OK so the truth is TR does more than just add type annotations, but make the assumption for the sake of argument. *no pun intended)
That's exactly why it is called LiveScript, it's an inside joke for those who know JavaScript well. There has been absolutely no confusion considering that last time "LiveScript" was used as a name for JavaScript was 17 years ago. Also, http://gkz.github.com/LiveScript/ is the first result on Google for 'livescript'. 
Yeah this is definitely quite a handy feature.
You could read what Bob Harper says, don't know about Turner http://existentialtype.wordpress.com/2011/04/16/modules-matter-most/
Livescript looks cool, but none of those one-liners are particularly impressive. I wonder if some of the Haskell one-liners would transfer over (like the powerset one).
&gt; Uh... Functional programming is hard, not only because you have to think 'differently', but because you have to be open to ideas that would be ridiculous stupid to implement in traditional languages. Keep in mind that the built in list type is analogous to a singly linked list from C. It's not an array. Whatever function you use to index it, whether `!!` or `last` needs to traverse it in sequence. Like a linked list, it's a poor data structure to use when random access is required.
Yes, the post was in response to one about CoffeeScript one liners. Thus some things that perhaps look more impressive as CoffeeScript one liners look really simple as LS ones.
 I lately went through the following course and found the general introduction to functional programming (on the basis of Scheme) very instructive. There is a tiny bit of haskell in the very last lecture. The course (e.g. on Itunes U): Programming Paradigms CS107 from Stanford 
Wow, this is great! Such strategies and suggestions may also be useful for non-novices, when specialized to complicated libraries. This could allow libraries to encode suggestions for the user about code that looks like it could be written more idiomatically or efficiently.
Well, `const id` is a "more primitive" expression from a combinator/lambda calculus point of view. flip const = (\f x y -&gt; f y x) (\a b -&gt; a) = (\x y -&gt; (\a b -&gt; a) y x) -- and this may get reduced every time the outer lambda -- gets applied. In general, reducing under lambda requires -- more sophisticated analysis. -- whereas const id is just KI in combinators: const id = (\x y -&gt; x) (\a -&gt; a) = \y -&gt; (\a -&gt; a) = \y a -&gt; a -- no reduction here, just simplification 
Links for the lazy: [cereal](http://hackage.haskell.org/package/cereal) [binary](http://hackage.haskell.org/package/binary)
Hmm. I was wondering if there was an "optimization" slant on it. Looks like I should bite the bullet and actually study lambda calc. Thanks for the insight!
&gt; At this point, I decided to go back to start and try to solve the problems without using the built in functions. They're too helpful, and I'm not learning enough using them too be able to do this. That's right! Those exercises aren't very useful if you simply use the standard library functions. Just pretend you're writing parts of the standard library.
Problem 1 is itself a problem. They have asked you to implement a partial function. The function domain is [a]. A total function would be able to handle any element of that set, but the given one fails on the empty list: []. The provided solution has type [a] -&gt; a, but there is no way to provide an element of type a to return if you are given the empty list. So they've started you off on the wrong foot. The Prelude will steer you wrong as well, 'cause Prelude.last has the same problem. So let's start on the right foot. The type of the co-domain (the function output) must be able to represent a single value, but also the possible lack of a value. data LastValue a = LastValue a | NoValue Here's the new version of myLast: myLast :: [a] -&gt; LastValue a myLast [] = NoValue myLast (x:[]) = LastValue x myLast (_:xs) = myLast xs You may have run into the type LastValue by now under another name: Maybe a. The standard prelude definition is: data Maybe a = Just a | Nothing The practice of using a null pointer to represent a missing value is shunned, because you cannot represent the null value itself. Instead, we use the Maybe type, which cleanly represents either a value or it's absence. I've introduced more of the language than the others have, which might be more confusing, but the point is not to sacrifice correctness at any time. It is easy to see from the equations of myLast that all values of [a] are handled without error. At least, it is for an experienced Haskeller. When you can read the code as a proof, you will start to get a handle on Haskell. 
it'll basically get compiled into something akin to: while(list-&gt;next != NULL) list = list-&gt;next; return list-&gt;value; (notice both these pieces of code fail with an empty list as input)
One bit of advice I think I can add to what's already been said is that you make sure you really understand patten matching. Getting to the point where I understood the syntax well enough to focus on problems at higher a level was probably one of my first stumbling blocks. I found this resource helpful. http://en.wikibooks.org/wiki/Haskell/Pattern_matching For example: lastElem (xs:p) = p Looks like it might work, but it's not actually going to do what you expect. The (:) operator means &lt;first element&gt; : &lt;rest of the list&gt; so your lastElem is going to return the input list minus the first element. lastElem (xs:p) = p &gt; I also made the assumption that main would be called at soon as the script were loaded in ghci. I actually consider this a nice feature. It means you can things like this (from some code I wrote recently): main = LS.getContents &gt;&gt;= (return.prettyPrintNBT.readNBTFile) &gt;&gt;= putStr mainTest = LS.readFile "level.dat" &gt;&gt;= (return.prettyPrintNBT.readNBTFile) &gt;&gt;= putStr main and mainTest are both IO actions. main reads something from the standard input, does stuff and prints the result out. That's what I want the compiled code to do, but piping input from inside gchi is a awkward, so I can specify a test file and test with an alternate main. You can also start a test run from any IO action, which makes debuging much more flexible than in, say, C, where you have to always start from your main function. Also, it almost goes without saying you can debug any function directly in ghci without having having to set up a context because functions are pure. In C if you want to debug a function deep within the call stack you have no choice but to work out how to run your program so execution reaches that function, which can be a pain to do. 
&gt; I end up reading about Fold (http://www.haskell.org/haskellwiki/Fold (dead link to identity element in section 2 btw)) and can feel my mind bend when trying to make sense of the definitions of foldr and foldl. No mention of foldr1 though. And there are two stackoverflow questions for const and id respectively which asks what's the point of them. Oh, on fold. Probably best to explain with an example. Say you have a list of numbers [1,2,3,4,5], and you want to add them up with some function like "+". What fold does for you is puts the operation between the elements of the list, so something like this [1,2,3,4,5] --goes to---&gt; 1 + 2 + 3 + 4 + 5 But actually, what if your list had zero element? So we provide a starting value. In our case if we want to sum elements, zero would be a good starting value [1,2,3,4,5] --goes to---&gt; 0 + 1 + 2 + 3 + 4 + 5 And [] --goes to---&gt; 0 Hopefully this will help your intuition when you get to the full technical definition of a fold. 
I recommend not skipping problems just because you know of solutions in the standard library. Reimplementing Prelude and List library functions for yourself is a great way to learn. Also, (!!) in particular is something to generally try to stay away from. There are cases where it's appropriate, but it's an unnatural operation on lists. Lists are the data structure you use when you want to iterate over a bunch of things in a particular order -- random access is slow, and in the case of (!!) it's even dangerous, because indices can be out of bounds. The same is also mostly true of length -- there are cases where you really want to know the length of a list, but there are many places where it's not really essential to compute and so should be avoided. For something like problem 2, where the operation you're implementing is already unnatural, then perhaps it's not so bad to use something like (!!), but if you're going to use Prelude functions, last . init is a nicer way.
I just started learnin Haskell yesterday and I'm loving it. For once it seems like my discrete math is useful and all of the solutions are so elegant it's frickin cool
Very interesting. I'd heard about the Reduceron a while back and it was the first I'd heard of computer architecture that was design to be more friendly to functional languages. It's all a bit opaque to me but I love skimming for the general idea.
This is a very important point. Show and Read are useful for debugging and quick-n-dirty scripts, but for real programs you should really use cereal or binary as Tekmo points out. 
Nope. The stack isn't used like you are used to: When forced myLast [1..1000] will compute myLast (1:2:3:...:[1000]) = myLast (2:3:...:[1000]) = myLast (3:...:[1000]) = ... myLast ([1000]) You aren't building up a stack frame as you go here. The stack works very differently in a lazy language. It is a stack of things you are currently forcing to evaluate, it is not directly related to the surface structure of your program.
One more language (indirectly) defined atop Coffeescript but removing the [horrible scoping rules](https://github.com/jashkenas/coffee-script/issues/712). If that means people insist on having sane scoping rules (allowing explicit shadowing), that's a good thing.
Good, i'll try it asap :)
and without the security holes!
On the problem with types: Write the type signature first. It helps you with thinking about what you want, and whether what you want makes sense. It also helps when defining multiple patterns for a function.
&gt; I’m often quite bad at starting something I find fun, and losing interest before I get to something I’d call complete. I am hoping to change this… I really hope a future potential employers don’t read this bit… Most of my personal projects go like this too. I think it's completely reasonable. The only reason I write code on my spare time anymore is to challenge myself, and once the challenge is gone I just don't see any point in keeping at it. Potential employers who are scared away by that mindset probably aren't worth working for.
How does binary compare to Erlang's bit-packing/unpacking syntax?
Hey Bartosz, 1) the listing seems to not be loading (correction, its loading really slowly) 2) did you guys get my emails from a week or several ago? :-) -Carter
It is lazy magic. Due to non-strictness, the Haskell "stack" doesn't work quite like the stack in strict languages. Further details: http://www.haskell.org/haskellwiki/Tail_recursion
Even `ghci` does tail-call elimination: &gt; let myLast ys = case ys of [x] -&gt; x; _:xs -&gt; myLast xs &gt; myLast [1..100000000] 100000000 
Oh, I thought the call instruction did specify the save register. If it doesn't then it's better to have branch. Many ISAs allow you to specify the save register.
Making bad hardware is easy. :) It's making good hardware that's difficult. And multipliers are tricky to make efficient. For 64x64 you probably want a multicycle pipelined multiplier. 
&gt; Uh... Functional programming is hard, not only because you have to think 'differently', but because you have to be open to ideas that would be ridiculous stupid to implement in traditional languages. Well, there is really one fundamental thing that you have to master - *declarative thinking*. Look at the solution to the Problem 1. What it really does is to **define** something. What's the last element of the singleton list? Of course it is its only element. What's the last element of the non-singleton list? Well, we can say that it is the **same** as the last element of the tail of that list, can't we? I've bolded the word 'same' and the reason is that in functional languages you are constantly exchanging one expression to another but equivalent. That's fundamental. You substitute again and again until you have what you want (this is sometimes called *normal form*). That's why the solution looks like it looks. That's why Haskell is called "mathy". And that's why many times you have to think indirectly about your task. In the solution to the Problem 1 it isn't said directly what the last element is. But after many substitutions of that expression you get the answer. After you'll become proficient with declarative thinking all of these ideas should not be so scary, I think.
Re: foldr1 Is it really too much to ask the people interested in Haskell do a quick bit of study to read up on catamorphisms, hylomorphisms, anamorphisms, paramorphisms, apomorphisms, zygomorphisms, histomorphisms, futumorphisms, dynamorphisms, and chronomorphisms? I mean, when you learn Python , you're expected to learn about "for" loops.. (I'd love to make a joke here about how "foldr1" is just the "foomorphism in the category of barfunctors", but I have no idea what I'm talking about. I'm pretty sure Category Theory is the study of how to play that game where you name things that start with a particular letter..)
The main advantages from this approach is that you can compose normal lenses with lenses with polymorphic updates with more limited 'pseudo-lenses' for 'getters' and 'setters' that can only be used for their respective functions, using only (.) and id from the Prelude. Moreover, to define a lens family you don't need to have any package dependencies at all. Since the simplest type for a (polymorphic) lens can be defined using just Prelude types. This means that library authors can create lenses into their types without incurring any dependency overhead whatsoever, unlike with other lens libraries. So if you don't want to choose a lens library, just make polymorphic "lens families" sndL :: Functor f =&gt; (a -&gt; f b) -&gt; (c,a) -&gt; f (c,b) sndL f (c,a) = (,) c &lt;$&gt; f a which can be composed with other lenses or specialized getters and setters without even importing a package.
I've just posted a [question on Stack Overflow](http://stackoverflow.com/questions/11372076/debugging-unwanted-strictness) and a [blog post](http://mergeconflict.com/archives/69) about this. You're probably thinking in the right direction, at any rate...
Haskell is ok for this purpose (at least a lot better than Python), as long as you use the modern text processing libraries - bytestrings, 'text', iteratees etc. By all means avoid Parsec - it's cool but was dog slow last time I checked.
"Just OK", or good? I have found a few articles already echoing your advice, re bytestrings.
I wonder why you don't mention attoparsec.
Excellent. The database bindings don't necessarily all have the right features in terms of bulk vs. row-by-row insert, so if you need to get that straight, you should be careful about what package you pick, and you may need to bind a bit yourself. Frankly, depending on your application, parsec isn't the fastest of the pack, but it may be more than enough, depending on what other operations are how expensive. And a simple parser in parsec is still way faster than a complex parser in parsec, so lots of the performance cost only comes from doing complicated things.
&gt; The major drawback right now is probably that the TH support is a bit primitive The conceptual complexity is probably the far greater drawback. Unlike pretty much any other lens library out there, it's difficult to just know what lenses are, read the documentation, and understand what's going on here. By comparison, last of comprehensive TH support is a minor detail.
There are lots of csv libs. This is one of the more flexible and configurable ones that I know of: http://hackage.haskell.org/package/csv-conduit
Wait, what? In my experience, Python *is* the most absurd as compared to Scheme or JavaScript. That is, in Python, variables from scopes beyond your own do not behave the same way as local variables. That is, you cannot reassign a variable declared outside of your current scope--if you have a mutable data structure, you can still mutate it, but you can't use something like a boolean for a flag for example. That is, this code will not work properly in Python: def f(x): flag = True def helper(y): # Do stuff if someCondition: flag = False I think in new versions of Python you can overcome this shortcoming by using the nonlocal keyword, looking something like this: def f(x): flag = True def helper(y): nonlocal flag # Do stuff if someCondition: flag = False In Scheme, on the other hand, you are free to use define to create local variables and set! to reassign nonlocal variables. I think this is a *much* more reasonable system than Python's! JavaScript (and, probably, by extension, CoffeeScript) follows Scheme's system except define becomes `var x = ...` and set! becomes `x = ...`. Not quite as neat and elegant, but fundamentally the same. This model lets you take full advantage of mutable variables and closures without having to bend over backwards. Idioms in both languages take full advantage of this, which is much less neat in Python. Also, some thoughts on lambdas: especially in sufficiently dynamic languages, lambdas are useful for adding control structures to the language. In JavaScript, patterns using functions like blocks of code are common--turning: $(function () { ... }) into function foo() { ... } $(foo) just makes the code less readable for no gain. This continues with other things like event handlers--if your event handler is only used in one place, it's simpler to write it inline than to give it a name and add an extra level of indirection. Having extra identifiers that are just used to get around your lambda's shortcomings is not a virtue. Anyhow, sorry for the slightly long rant. I've had to use too much Python and now I'm jaded. Don't even get me started on nonsense like mutable structures combined with default arguments!
I would love to be able to use haskell professionally after cs degree is done.
I like it a lot, as someone who has only dabbled (mostly unsuccessfully) in Haskell but experienced in a lot of other languages, including functional ones. The practical approach, focussing on how you actually engage a simple, realistic problem is very enlightening and (in my experience) quite rare. Rather than ranting on about how cool functional programming is and introducing lots of obscure operators, or solving toy problems in the REPL, you motivate use of hackage and hlint, deal with (straightforward) types directly instead of getting all pedagogical (/condescending) on the reader and build up the program in a way people actually would in practice. Well done! I've often thought of putting together a tutorial like this for a small project and distributing with it a git(/darcs/hg) repository that people could use to interact with the development process. Whether that would add value to the tutorial isn't clear, but it's an idea that seems particularly relevant to your style of presentation. I look forward to more!
lens-family-core provides first class(†) functional references. In addition to the usual operations of getting, setting and composition, plus integration with the state monad, lens families provide some unique features: * Polymorphic updating * Cast projection functions to read-only lenses * Cast semantic editor combinators to modify-only lenses (†) For optimal first-class support use the lens-family package with rank 2 / rank N polymorphism. Lens.Family.Clone allows for first-class support of lenses for those who require Haskell 98. 
&gt; the expression a + b in some dynamic language basically desugars to [D &lt;- int] (([int &lt;- D]a) :+: ([int &lt;-D]b)) I think where we disagree is that I would call that static desugaring type checking, not the run-time tag checking itself. To me, it feels like much the same thing that Haskell's type checker does, inserting coercions in place of newtypes, dictionaries in place of type classes. In "duck-typed" languages like Smalltalk or Ruby though, this desugaring phase wouldn't be possible. a + b is, semantically, just calling the `+` method in `a` dynamically. There's no static knowledge of what types are acceptable inputs to a method. Hell, you've even got `eval`. With `eval` in the Universe interpretation, you've got a decent type: `eval :: Universe -&gt; Universe`. With casts, I'm not sure how you're supposed to deal with that. &gt; The concrete interpretation does something very similar, so why not call it dynamic type checking? So, I would say type checkers are a subset of *abstract* interpreters. It's usually part of most common definitions of a "type system". E.g, Pierce's TAPL, if I recall correctly, defines type systems as being an abstract interpreter. Calling concrete interpretation dynamic type checking is like calling quickCheck partial model checking. I think it's a stretch of language that doesn't serve any productive purpose, aside from further muddying the waters of type system terminology (we already have that problem with "strong" and "weak" typing, or "strict" and "relaxed" typing, and other terms that effectively have no meaning anymore). 
Perhaps you mean literate haskell?
From personal experience: if you don't understand well enough how laziness works (I did not and still don't), it's too easy to make your program load all the data into memory before it starts to parse. Just something to be aware of.
I tought initially of writing the article in literate Haskell (and Pandoc which I use for the blog as well is quite capable of literate haskell), but I think that works well when you have a single program in the page scattered through; not more.
If you could post a snippet refactoring the if's cascade from main into a Maybe monad, that would be awesome.
I see a hell of a lot of whining from windows users about this. If Windows want things to work, why do they figure out how to fix it and submit patches? 
As we now have lenses with polymorphic update there is no reason to use the clumsy record update syntax anymore, right? So is there any chance that ghc might generate lenses instead of functions for record fields anytime soon? Or am I missing some drawbacks?
The derived polymorphic fields do not quite replicate the current behaviour of record updateing data Complicated a b = Complicated {_field1 :: a, _field2 :: a, _field3 :: b} field1 :: Lens (Complicated a b) a field1 f (Complicated x y z) = (\x' -&gt; Complicated x' y z) `fmap` f x field2 :: Lens (Complicated a b) a field2 f (Complicated x y z) = (\y' -&gt; Complicated x y' z) `fmap` f y In this case the `field1` and `field2` lenses must be monomorphic since each alone cannot change the type of `Complicated a b. And in general there is no way to take the product of two lenses on the same source since in general they may not be disjoint. The best solution I've been able to imagine is some sort of curly brace notation that takes a sequence of field names and produces a lens on the fly. That or we can simply punt on "complex polymorphic record updates" and say to anyone who needs this that they should hand-craft their own lens to do so. handmadeLens :: LensFamily (a, a) (a', a') (Complicated a b) (Complicated a' b) handmadeLens f (Complicated x y z) = (\(x', y') -&gt; Complicated x' y' z) &lt;$&gt; f (x, y)
The cascading ifs put me off as soon as I saw it .. not being a Haskeller I'm not familiar with the usual method of refactoring this sort of mess: in an imperative language, early returns work well; in Scheme you either get used to dealing with `cond`'s ordering or come to love deep indentation (though there's usually a refactoring that makes it shallower, it's just a mater of juggling `let`s until you're happy with the brevity and clarity of your result). As OP has said, an example of how you might refactor this in Haskell would be fantastic.
I submit all patches I can come up with. Maintainers don't respond.
In your specific case you have an error string associated with each `if` check, so if it were pure code you would want the `Either String` monad. I think the best way to demonstrate it how it works is to show a code example: safeHead :: [a] -&gt; Either String a safeHead [] = Left "Oops! Empty list!" safeHead (x:_) = Right x safeDivide :: Double -&gt; Double -&gt; Either String Double safeDivide x 0 = Left "Divide by zero!" safeDivide x y = Right (x / y) Each of those functions wraps an error in a `Left` with a descriptive string of why it failed, and wraps the successful result in a `Right`. Now, I'll create a contrived function to illustrate the `Either String` monad: headAndDivideByZero :: [Double] -&gt; Double -&gt; Either String Double headAndDivideByZero xs y = do x &lt;- safeHead xs z &lt;- safeDivide x y return z -- Gratuitous return The above `return` is gratuitous, because I could have shortened it to: headAndDivideByZero xs y = do x &lt;- safeHead xs safeDivide x y Now, the way the above function works is that if **any** of the actions in the monad fail (i.e. return a `Left`), the entire monad fails and returns the first `Left` it encountered. However, if **all** of the actions succeed (i.e. return a `Right`), the entire monad succeeds and returns the final result. So, for example: &gt; headAndDivideByZero [] 4 Left "Oops! Empty list!" &gt; headAndDivideByZero [1] 0 Left "Divide by zero!" &gt; headAndDivideByZero [1] 2 Right 0.5 The advantage of the `Either` monad is that you don't have to check for `Left`s and exit when you encounter one. The monad does that for you. You just bind the raw `Right` results as if there were no possibility of error. Now, you can even mix the `Either` monad with `IO` code using monad transformers. You just use `EitherT String IO` (with `EitherT` coming from the `eitherT` package). Again, a code example might help (I've truncated your error messages for this example): checkedMain :: EitherT String IO () checkedMain = do arguments &lt;- lift getArgs (sf, df) &lt;- case arguments of sf:df:[] -&gt; return (sf, df) _ -&gt; left "Two arguments expected" is_valid_source &lt;- lift $ isValid Source sf when (not is_valid_source) $ left "Invalid source" is_valid_destination &lt;- lift $ isValid Destination df when (not is_valid_destination) $ left "Invalid destination" lift $ readSource sf &gt;&gt;= writeDestination df However, the idiomatic way to do it would to be to actually write your `isValid` function in the `EitherT String IO` monad and put the check there. It's name and type would then be: checkFile :: FileType -&gt; FilePath -&gt; EitherT String IO () It would call `left "some error"` when the check failed and `return ()` to proceed normally. Notice that the type of the `checkedMain` function is `EitherT String IO ()`. To convert it back to an ordinary `IO` action so that you can run it in `main`, you would use `runEitherT`: runEitherT :: EitherT String IO a -&gt; IO (Either String a) runMain :: EitherT String IO () -&gt; IO () runMain main = do e &lt;- runEitherT main case e of Left str -&gt; putStrLn e Right () -&gt; return () main = runMain checkedMain Notice that this concentrates your error-handling into a single location rather than tightly integrating it into your application code, which is nice because it decomposes your error handling from your application logic. So if you later decided you wanted to do something else with the error string rather than printing it (such as logging it to a file), you would just make a single change to your `runMain` function, rather than refactoring every single if statement in your original code. It's actually funny that this came up because I'm actually working on a library to streamline the use of `EitherT String IO` (among other things) today and I'm planning on releasing it soon, so you will soon have this kind of functionality right out of the box. If the whole monad transformers thing is new to you, then I highly recommend you read [Monad Transformers - Step by Step](http://www.grabmueller.de/martin/www/pub/Transformers.en.html). Also, I recommend you study the following definition of the `Either` monad: instance Monad (Either a) where return = Right (Left a) &gt;&gt;= _ = Left a (Right b) &gt;&gt;= f = f b Also, if you are new to monads in general, then I recommend the best monad tutorial ever written: [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html). I think after reading that and studying the above monad definition for `Either`, you can figure out how `Either` basically does all the error checking for you.
I do a ton of parsing in my line of work. I recommend you start with Chapter 16 of Real World Haskell which is a great introduction for parsing. I have a lot of experience using `parsec` and `attoparsec`, so I'll discuss them a bit. `attoparsec` is blazing fast (about as fast as C, if you use its efficient primitives), but it is very difficult to get it to work correctly and the errors are completely unhelpful. When it works, though, it works great! `Parsec` is slower but it is very robust, intuitive and easy to use, and gives incredibly good error messages, which you can further customize yourself. There is also `uu-parsinglib`, which I haven't used yet, but it has some very advanced features. Unfortunately, it is not very well documented or marketed. If I were you, I'd recommend you start with `Parsec` to learn how parser combinators work, and if you need speed and low memory consumption then switch to `attoparsec` or try out `uu-parsinglib` if you want really advanced features. Most importantly, if you have trouble getting `attoparsec` to work, just PM me or ask on Stack Overflow. It's very brittle, so you have been warned! However, I think that between `Parsec`'s robustness and `attoparsec`'s speed, Haskell pretty much blows other languages out of the water when it comes to parsing, especially when combined with monad syntactic sugar.
Because I haven't used it :)
I'm sure a Schemer would use call/cc to do early returns, no?
I have been using haskell, as well as python, for ETL type tasks. I think haskell is great language, and when I get things right performance has been great. That requires finding and learning the right libraries which compared to python are less numerous, less obvious which is the right one for the job, and sometimes less well documented. Haskell's lazyness and built in memory management means it's easy to end up keeping all your data in RAM. The way round that is one of the iteratee-style frameworks. conduit seems to be the best for current library support and active development and has worked well for me. I have used a combination of csv-conduit and attoparsec. I have been loading data to noSQL and graph DBs not RDBMs so I don't have experience of any of the SQL access libraries.
Well, for me personally, just OK - honestly I think I wouldn't use Haskell for ETL tasks *that do not really highlight Haskell's strengths* - I'd probably use Java instead because it has more predictable performance (I'm confident that I can very quickly put together something dumb and boring and make it work fast). Depends on the task, though. Re predictable performance - I'm speaking from experience - I was recently developing some tools (in Haskell) that needed to do rather complex processing of unbounded amounts of data, and I was suffering heavily from memory leaks due to laziness. Took me a couple of days to figure that out, and I thought I was good at Haskell performance analysis! However, the end result was great, but if you cannot afford a risk of spending a couple of days...
This was fun. I'm fairly happy with what I came up with in 2 hours. My solution doesn't shy away from fractions like so many others do. http://pastebin.com/MU4BYLKk $ time ./Arith.exe 926 75 2 8 5 10 10 (926 % 1,(* (- (+ 8 10) (- (/ 2 5) 75)) 10)) (926 % 1,(* (- 75 (- (/ 2 5) (+ 8 10))) 10)) (926 % 1,(* (+ (- (+ 8 10) (/ 2 5)) 75) 10)) (926 % 1,(* (- (+ 8 10 75) (/ 2 5)) 10)) (926 % 1,(* (- (+ 8 10 75) (/ 2 5)) 10)) (926 % 1,(- (* (+ (- 75 5) 8) (+ 2 10)) 10)) (926 % 1,(- (* (+ (- 8 5) 75) (+ 2 10)) 10)) (926 % 1,(- (* (+ 2 10) (- 8 (- 5 75))) 10)) (926 % 1,(- (* (+ 2 10) (- 75 (- 5 8))) 10)) (926 % 1,(- (* (+ 2 10) (- (+ 8 75) 5)) 10)) real 0m35.137s user 0m0.000s sys 0m0.015s 
A simpler option would be to use when/unless exitWithMsg err = hPutStrLn strerr err &gt;&gt; exitFailure main = do v &lt;- a unless (test v) $ exitWithMsg "Oh no, not again." u &lt;- b v ...
Yes, however that specific code only works for the `IO` monad and I want to teach an approach that works with `EitherT` over any base monad. Also, I'm not a big fan of `Control.Exception`. I would modify yours to instead use: exitWithMsg = left ... and let the interpreter decide what to do with the error message. 
Thank you for the detailed response, it was very helpful. I've played around with EitherT, and I assume the idiomatic nature of the validation (you've mentioned) would take the following form main = do ret &lt;- runEitherT $ (lift getArgs &gt;&gt;= validateArguments) &gt;&gt;= validFile &gt;&gt;= conversion case ret of Left str -&gt; putStrLn str Right _ -&gt; return () Where the functions have th signature: validateArguments :: [String] -&gt; EitherT String IO (FilePath, FilePath) validFile :: (FilePath, FilePath) -&gt; EitherT String IO (FilePath, FilePath) conversion :: (FilePath, FilePath) -&gt; EitherT String IO (FilePath, FilePath) edit: formatting
Great work. Really glad that the library has primitives for authenticated encryption as well. For people in need of crypto using authenticated encryption should always be the default choice and it would be great if the documentation said so in big bold letters as well. I've been a bit afraid of implementing crypto primitives in Haskell myself, mainly since it's really hard to reason about timing attacks when using a high level language, especially one with lazy evaluation. Edit: took a look at the API and it seems that the decryptGCM function returns a plaintext and a tag. I think it would be better if it returned a Maybe Bytestring if the tag validation fails. It's better to not let the user do tag comparisons, to avoid mistakes.
Yes, that is exactly correct, except that I'd probably use `do` notation if I were teaching beginners in a tutorial. Also, I'm probably going to announce my error-handling package on reddit today which would simplify a lot of your tutorial code.
Yeah I'll stick with `do` notation for most of the tutorials I'll write and only squeeze in concepts slowly towards the end where I attach my personal style to the code. I hope that way throughout multiple tutorials they'll get an intuition for the desugaring of `do`.
2010 eh, that's ancient by Haskell standards. ;) Some updates to dons' answer: * Parsec is fine for text/bytestring these days * For json there's also the aeson package * For XML you probably want hxt over haxml, or hexpat for speed (possibly via hxt-expat) * polyparse and haxml are lgpl while most other packages are bsd/mit
Usually there are immediate instructions that can be used for increment and decrement. 
Isn't associativty + an identity and a binary operation on a set the definition of a monoid, not a category?
That would actually be a really cool tutorial idea. You can tag each step of the tutorial as a different commit. I think you should do it. The community needs more examples of actual application code that beginners can refer to, otherwise they are forced to examine library code which is less useful when learning how to write applications.
parsec is actually reasonably fast these days. The performance bugaboo has been dead since shortly after parsec 3 was released. Its maybe a factor of 2 slower than attoparsec, but it does a lot more, like actually showing you errors. That said, trifecta is basically a faster parsec with nicer error reporting, but a lot more installation requirements.
I was drawn to Haskell, and functional programming in general, by the robustness of the code. FP allows you to render illegal states unrepresentable. On a separate note, what types of projects have you used Haskell for? I'd be interested knowing where functional programming really shines.
That's certainly an option, though I consider `call/cc` a pretty big hammer .. and even though Scheme isn't terribly functional, most prefer to write in a functional style so anything that exposes control flow is best avoided. Hence my comment about "dealing with `cond`'s ordering".
Excellent post. Since you've already done so much I'm hesitant to ask for more .. but could you briefly contrast this technique with how one might use `Maybe`?
I wouldn't take Java's dominance in any particular niche as an argument for its superiority for that purpose. As somebody who is learning a non-mainstream language, I would assume you'd alread moved beyond the mindset that Java's ubiquity is tied to its technical merits. Java gets used because everyone uses Java.
Keep in mind that there are burritos flowing through the pipes.
If I might indulge in some meta-discourse activity, consider the above use of "just". The word "just" has several senses. In the above, it is being used as a short way of saying "precisely". But "just" can also mean "merely", as in "It's just a scratch.". Sometimes, the two collide. Categorists are fond of sentences "&lt;simple-thing&gt; is just &lt;specific-instance-of-deeper-abstraction&gt;", intending to offer a compact precise definition and perhaps hint at some value in the deeper abstraction. However, the pattern of the sentence is easily mistaken for a simplifying "merely" explanation: "The Wizard of Oz is just an old man with a big echo.". The categorical sentence is often, quite deliberately but without malice, the very opposite of a simplifying explanation. The trouble is that for some readers, even those aware of the intention, the resonance is enough to render the sentence ridiculous. The larger the increase in complexity between definiendum and definiens, the more comical it becomes, as in James Iry's classic about monads (with a "what's the problem?" for sauce). My advice to all concerned is to use "just" precisely when "precisely" and "merely" are both meant sincerely. What a patronising windbag I've become in my old age!
To a small extent I have already done so. There is now a 'parsers' package that contains most of the combinators from trifecta in a parser-neutral fashion, so I can release mixin packages for parsec and attoparsec. I've updated trifecta in github to use this package, but I haven't released the new version yet as I have a few things left to port (some diagnostic combinators, literate parsing and layout, and the old language parser transformer needs to move to 'parsers'). Along the way I've already shed a number of dependencies, but really they were only the easy ones. Sadly, I've also shed a bit of performance, but part of that was a conscious decision to make it easier to write new parser types that complied with my classes. There isn't a good way to break out, say, the terminfo dependency without either duplicating a bunch of code or orphaning a ton of instances. That said, I should be able to break out a lot of the highlighting functionality into another package, which will cut more dependencies. My concern during initial development was to maximize productivity. Speaking from experience, breaking up one big package into a lot of tiny pieces KILLS your productivity. category-extras allowed a vastly more efficient update process than my current distribution of packages. I know that if I make even a minor change in one of the low level packages that I'll be spending the next 4 hours to a day, mostly fiddling with .cabal files. This seriously reduces the number of iterations I can make on a design.
Yeah, I realized that in retrospect, which is why I included the code example to bring it more down to earth. Thanks for the constructive advice!
No problem at all! I enjoy teaching these kinds of things! The only difference is that with `Maybe` there is no error message or exceptional value and instead you use `Nothing` to lump together all exceptions. For example, instead of using `EitherT String IO`, you would just use `MaybeT IO`. If you wanted to throw an error, instead of using `left "some error"`, you would just use `nothing`, defined as: MaybeT . return . Nothing I also just announced a new error handling library which provides convenience functions for things like the above so you could write: liftMaybe :: (Monad m) =&gt; Maybe a -&gt; MaybeT m a liftMaybe Nothing Or you could use the `MonadPlus` instance of `MaybeT m` and just call `mzero`, which is equivalent. Talking about this makes me realize that it might be convenient to include a `nothing` function for `MaybeT` just for syntactic sweetness that does the same thing. You would also change your `run` function, so `runMain` would look something like: runMain :: MaybeT IO () -&gt; IO () runMain main = do e &lt;- runMaybeT main case e of Nothing -&gt; putStrLn "Some error occurred" Just () -&gt; return () For pure code, (i.e. just `Maybe`), it's also the same thing, with `Nothing` instead of `Left someError`. So if I were to rewrite my example functions, they would be: safeHead :: [a] -&gt; Maybe a safeHead [] = Nothing safeHead (x:_) = Just x safeDivide :: Double -&gt; Double -&gt; Maybe Double safeDivide x 0 = Nothing safeDivide x y = Just (x / y) ... and now I bind them within the `Maybe` monad instead of the `Either` monad: headAndDivideByZero xs y = do x &lt;- safeHead xs safeDivide x y Notice that there is no change to that function at all, except that now it returns a `Maybe` instead of an `Either`. There is no hard and fast rule for when to use `Maybe` or `Either`. Generally, people use `Maybe` when they are writing a simple one-off function or when there is only way for the function to fail (since you can always annotate that failure later using the `note` function from the `errors` library that [I just announced](http://www.reddit.com/r/haskell/comments/w7xrv/errors10_simplified_error_handling/)). If there are multiple ways it can fail or you want to attach some additional information that the caller might need to recover from the failure, then you use an `Either`.
At least on the Australian show Letters and Numbers, you can only do whole divisions, there can't be a remainder.
Reminds me of [this post](https://plus.google.com/116312471061608346570/posts/Dw985YH3qhi) by Chris: &gt; I spent about about a day doing deep refactoring work on gloss-web to clean up the design, and expose a REST interface separating the logic from the user interface. I probably rewrote, all things considered, two thirds of all the Haskell code in the application. Much of the code that I rewrote does arcane things like use the GHC API. To top it all off, I was lazy and decided to just do it on the production server instead of building a test system... &gt; &gt; Total number of "oh crap" moments where I realized I just broke the production server? Zero. Yep, I made it through an entire day of a significant rewrite of the code base, and never once compiled code and then had it not work correctly. Not that I'm saying that's a good habit (it's not), but it's definitely a testament to the power of static typing. Also, there is nothing more satisfying than code that has exactly one solution and you get to play `djinn`. :)
&gt; The first reason is that ErrorT comes with the additional baggage of the Error class, which really has no place in a proper EitherT implementation. I don't know how many times I've tried using that type, got hit by the Error class constraint, and threw up my hands in disgust and went back to EitherT (provided by the appropriately-named EitherT package and re-exported by Control.Error). The Error constraint is one way to have a proper `fail` implementation, though it's probably not the best way.
I haven't used any Haskell AES packages, but I did have to use php's mcrypt the other day, and for me the number one way to build a better AES package than mcrypt is to have proper padding. Not just for security, but to make decryption the inverse of encryption (mcrypt appends nulls when encrypting in a non-reversible way).
Thank you! I have been floundering trying to make sense of all this stuff in bits and it is so nice that you have crystalized it into this little package. EitherR is esp. cool and something I probably would have never discovered otherwise. I hope this (and the depended libraries) can make it into the HP soon.
I like static type systems where there's only one type, `T`, and you have inference rules like: G, x : T |- y : T ------------------- G |- \x.y : T and G |- f : T G |- x : T ------------------------------ G |- f x : T That type system is pretty cool. Really easy to type check, too!
It used to read "A monoid is just ...". The edited version removes the unfortunate spin.
That doesn't seem to satisfy the property that functions are right adjoint to products.
Did you consider [`failure`](http://hackage.haskell.org/package/failure)? class Monad f =&gt; Failure e f where failure :: e -&gt; f v instance Exception e =&gt; Failure e IO instance Failure e (Either e) 
He was writing to an existing interface (data-lens), so there's been a lot of client code written to test the interface already.
Ah, in that case my comment is irrelevant. 
Most libraries in Haskell are released to scratch a specific itch, so at first that itch serves as the initial test case and then they can fill it in with proper tests. Haskell actually has great frameworks for unit testing, including HUnit and QuickCheck (which is really amazing!), which you can include with your library on Hackage so that cabal can automatically run unit tests for you. However, in Haskell there is a significantly lower emphasis on unit testing and a greater emphasis on using the type system and equational reasoning to prove your program is correct. Unit tests are a good starting point until you can prove your program is correct, but once you do that they are pretty much a formality. Proving a program correct sounds hard at first until you realize that smaller Haskell applications can be easily assembled from automatically correct components. However, for medium to large applications you rely more on tests for the "business-logic" of your application until you spend the time to encode more of your program in the type system (or the type-class system, with associated laws). That seems lame, until you realize that a surprisingly small part of your application is actually business logic. Things that are hard in other languages, like concurrency, are very simple and easy to use libraries in Haskell, and they are automatically correct for you.
I've looked at it, and I think I'm just too impatient. I found it very verbose and slow going, spending a lot of time covering ground that is either familiar or trivial to me (remember, I have some functional experience as well). The cutesy style I also found quite offputting: I want a technical manual, not something dressed up as a kids' book. Related to this, it doesn't seem to serve well as a reference (I'm alluding of course to what I consider a gold standard of programming books, K&amp;R). Nor did it point me to where references might be found. Finally, again harking back to K&amp;R, while demonstrations abound through the book, playing around in the REPL just doesn't give the same sense of accomplishment as being able to compile even a trivial program, which you don't introduce until chapter 9! While the text is imo very well written, it really feels (to me) like you don't trust the reader. Here are some list functions .. but don't look for any more. Here's how to create some functions .. but you're not ready to put those in a module yet. Or use any others for that matter. As for creating an executable, finish most of these toy exercises first. Oh, and we're going to put childish pictures and stupid titles everywhere just to remind you that you're being treated like an idiot. LYAH is actually exactly what I had in mind when I wrote "condescending" in my post above. I think the writing and the visual presentation are excellent, but it's definitely not written for me. I expect that I'm a fairly unusual audience, which is why I've accepted there's nothing really aimed at me thus far .. but then, I'm not even sure LYAH knows who its audience is supposed to be. I hope this doesn't come across too negative -- yes, I'm critical, but you asked for criticism so I didn't feel the need to pull any punches :-).
Have you taken a look at [Real World Haskell](http://book.realworldhaskell.org/read/)? Also, the criticisms are very good. I obviously won't take offense since I didn't write it. ;) However, I like to write tutorial blog posts and I try to find niches that other tutorials don't cover.
Beautiful. I really hope that this package becomes the standard for error handling in Haskell. This seems like the best way to address the ancient concern of having to deal with [so many ways](http://www.randomhacks.net/articles/2007/03/10/haskell-8-ways-to-report-errors/) of handling errors in Haskell.
I think you misunderstood my post :-). I don't give a hoot for unit testing - this is what strong static properties understood by the compiler are for. What shocked me was the lack of any mention of "the itch" (which is what I'm solidifying when I write the use case) in the article, which gave the impression he had written a library without even considering how it would be used. Someone else already pointed out that the API is specified by a pre-existing library, which kind of makes this moot. I still don't know though; I tend to like some automated validation. Visually checking that all the functions are present with the correct type signatures doesn't sound like my mind of fun. But I should probably *look* at the library and what it does before I make any judgements. If it's less than a handful of functions, and he has copied explicit signatures into the code, that's sufficient IMO (and I'd actually argue those signatures act as a sort of test).
Posting this here, hoping that somebody might find it slightly interesting. If you have any corrections, suggestions or questions, please let me know!
RWH looks quite good, I don't recall coming across it before. Skimming the first few chapters and the ToC it seems pretty close to ideal. Though it lacks the real-world iterative development examples of your tutorial (hlint + refactorings), which really clicked for me.
[Obligatory relevant XKCD](http://xkcd.com/927/).
I don't agree with your point of view of unit tests as "a starting point until you can prove your program is correct". I consider unit tests as weak static guarantees that allow to gain insurances in properties that are impractical to check through the type system. - weak because they only exercise a finite part of the behavior space of the program (extremely tiny for explicit unit tests, and unpredictably large for random tests à la Quickcheck), instead of providing total guarantees like you would with a correctnes proofs (total over a given specification space, which may not encompass the whole behavior space, eg. memory consumption) - static because, if systematically run after compilation, they are essentially indistinguishable from a compile-time program analysis pass. In fact they are a form of user-defined program analysis. They're weak, but expressive (they can check things that would be impractical in the type system) and low-cost, low-learning-curve (they are expressed in the source language). Formal proofs can supplement type systems if they are mechanically verified (eg. written in Coq, Agda etc.); and you need to verify the program source directly, not some port of it inside the proof system. Otherwise the proofs won't help you when changing the code, and increase the cost of change rather than decrease it. Manual "equational reasoning" proofs have the same issue: they have to be redone when changing the program, instead of bringing you confidence that your changes have not broken the specification. Please don't fall into the fallacy that "Typed programs don't need tests.". It's too easy to debunk.
Oh, that wasn't my tutorial! :) I don't want to take credit for it. My blog is actually haskellforall.com.
&gt; I would assume you'd alread moved beyond the mindset that Java's ubiquity is tied to its technical merits. Never had that mindset. Most Java based software I have used is overly resource hungry, and often opaque when exceptions occur. Recent example: a JDBC driver issue resulted in two 1000+ line stack traces, which contained little information as to the cause. The Java based ETL platforms I have used suffer from another common problem: The runtimes work well, but the associated development tools are problematic, and manually creating the jobs involved a morass of dependencies, or worse, "coding in XML". This all leads me to build what is needed from scratch, and since the non-mainstream tools have not fulfilled my project requirements, I see no problem in solving them with non-mainstream languages. I am no language expert, but Haskell _seems_ to fit in a unique place: A fast language (due in part to native compilation), a powerful REPL (like Python), a large selection of libraries with a package manager (cabal), and a syntax that seems very readable after spending some time learning it (whitespace!).
What is the point of the Either monad? Isn't it exactly the same as Either? All you would need is a function `flipEither`. I also think the name 'Script' is unintuitive. In my mind a 'script' is something that can be run step by step.
May I suggest depending on the [either](http://hackage.haskell.org/package/either) package instead of EitherT? It requires fewer language extensions, and exports the useful `hoistEither` function.
Wow, how did I miss that gem? Yeah, good call. I'm definitely switching to that one. I love pretty much every library Edward writes.
Lenses can equivalently be represented as * coalgrabras of the store comonad * monoidal natural transformations of coalgbras data-lens uses the first represenation, van Laarhoven lenses use the second. To clarify the van Laarhoven representation: consider a fixed type `a`. we can think of `Coalgebra a f := a -&gt; f a` as a (higher-order) functor from functors to types (aka kind `(* -&gt; *) -&gt; *`). The category of functors is monoidal with the composition operation, and types are monoidal with the Cartesian product. `Coalgebra a` is a monoidal functor via idCoalg :: () -&gt; Coalgebra a Identity idCoalg () = Identity composeCoalg :: Functor f g =&gt; (Coalgebra a f, Coalgebra a g) -&gt; Coalgebra a (Compose f g) composeCoalg (c1, c2) = Compose . fmap c2 . c1 So finally we can define a monoidal natural transformation from `Coalgebra b` to `Coalgebra a` as lens :: forall f. Functor f =&gt; Coalgebra b f -&gt; Coalgebra a f that satisfies lens (idCoalg ()) = idCoalg () lens (composeCoalg (c1,c2)) = composeCoalg (lens c1, lens c2) which is exactly the type and laws for the van Laarhoven lens representation. So far everything I've said is covered in "[Functor is to Lens as Applicative is to Biplate: Introducing Multiplate](http://arxiv.org/abs/1103.2841)". The idea behind the generalization to lens families is that they can be either represented as: * coalgebroids of the indexed store comonad * monoidal natural transformations of coalgebroids The first representation appears to be very hard to do in Haskell, where as the second magically embeds into Haskell function space very nicely. `Coalgebroid a b f := a -&gt; f b` is a "typed" version of `Coalgebra`. It is again monoidal: idCoalgoid :: () -&gt; Coalgebroid a a Identity idCoalgoid () = Identity composeCoalgoid :: Functor f g =&gt; (Coalgebra a b f, Coalgebra b c g) -&gt; Coalgebra a c (Compose f g) composeCoalgoid (c1, c2) = Compose . fmap c2 . c1 And again we can define a monoidal natural transformation from `Coalgebroid b b'` to `Coalgebroid a a'` as lensFamily :: forall f. Functor f =&gt; Coalgebroid b b' f -&gt; Coalgebroid a a' f such that lensFamily (idCoalgoid ()) = idCoalgoid () lensFamily (composeCoalgoid (c1,c2)) = composeCoalgoid (lensFamily c1, lensFamily c2) HTH. Edit: as you can see, if I went in depth about the motivation behind the lens-family library, people would stay even further away from it. :)
**UPDATE** I just released a quick patch changing a dependency from the `EitherT` package to the `either` package. What this means is that if you already installed version 1.0 and upgrade to 1.1, you will have two packages exporting conflicting modules for Control.Monad.Trans.Either. The fix is very simple. Just type: ghc-pkg hide EitherT Which will hide the EitherT package so it does not conflict. I tried to make this fix as rapidly as possible to mitigate the damage. I apologize, but `ehird` notified me that either is a higher quality dependency for the EitherT type and I figured the earlier I make the fix and notify people, the better the damage mitigation.
Before someone thinks we have found the holy grail in computer programming, please read the author's important comment: "Now to be fair, this is a simple library and, more importantly, it is an extremely generic library. The types are so polymorphic that I conjecture that there is only one way to write functions matching the required types such that all parameters are used non-trivially and recursion is not used." 
Do you use focus?
Bananas (sliced or reactive) used to be all the hype. Now it's tomatoes and [lenses](http://en.wikipedia.org/wiki/Lens_%28genus%29).
FYI: http://news.ycombinator.com/item?id=4214589
Nope. I rarely use StateT directly, but rather some MonadState instance. I used to use `access` and even wrote my own variant for MonadReader but I realized now I can just write code like, `gets (^.foo.bar)` which is then consistent with the same using `asks`. I think I didn't realize this with data-lens due to the broken `^.` operator and it's uglier with `getL` or `^$`... One thing I'm wondering is how these read or write-only lenses relate to the lens laws; it seems they simply don't apply any more, at least not the two first laws? Also, is it impossible (FSVO; assuming no insane hacks) to write lens-illegal code with `getting` and `setting` by virtue of them not being part of the "unchecked" API? Sorry for "hijacking" this thread. ;)
I think this package's reference AES implementation is unfortunately vulnerable to a timing attack due to the nature of S-boxes - it should be changed to a fully constant time implementation as in something like [NaCl](http://nacl.cace-project.eu) (and sorry to self promote but I have a library that's a complete [binding to it as well](http://github.com/thoughtpolice/salt) which I think you've seen `dnaq`!) The AESNI implementation is timing attack resistant and I feel the reference one should be as well. I'll file a bug. (for the details see [this paper](http://cr.yp.to/antiforgery/cachetiming-20050414.pdf) by Dan Bernstein.) &gt; I've been a bit afraid of implementing crypto primitives in Haskell myself, mainly since it's really hard to reason about timing attacks when using a high level language, especially one with lazy evaluation. I've thought about this before and in my library I haven't yet gone through the process of somehow verifying this. Right now the underlying primitives are all constant time and I just strictify in most of the relevant parts I think, but I don't know if that's enough. There are other concerns like paging leaving encryption keys on disk. To fix this you would need to allocate a page and `mlock` (`mlock` marks a full page at minimum) it and carve secret keys out of this like an allocator. I haven't done this in my package either but it's something I would like to do in the future.
I think when LYAH was written, most Haskell tutorials/learning materials seemed to be targeted at academics with lots of barely-explained terminology. Or at least, the stereotype was that Haskell is "heavy" stuff targeted at PhD's. I'm guessing part of the cutesy style in LYAH is trying to combat that perception and show Haskell can be accessible to anyone, not just academics.
I was sort of assuming that the read-only and write-only lenses are lawless. However now I am wondering if the write-only lens (aka a sementic editor combinator) has some law?
Doesn't a semantic editor combinator `write` follow the law `write f . write g ≡ write (f . g)`?
That helps so much. Thanks for taking the time to write that up! I'm now significantly more excited about `lens-family`.
I don't think Shelly is a very good "starting point" for learning Haskell. I believe it's geared towards those who are already familiar with Haskell, and provides them with a convenient way to write shell scripts using the power of Haskell, rather than, say, Bash. The Haskell Beginner path I recommend is LYAH -&gt; 99 problems -&gt; RWH -&gt; 20 intermediate problems -&gt; typeclassopedia. If you've already gotten that far, then sure, Shelly could be a great way to improve your Haskell fu. I'm not saying this to scare you away from Shelly by any means; go ahead and play with it even if you *are* a complete beginner. Just beware; you may run into trouble if you haven't taken the time to learn monads yet.
Well fwiw it's mostly the same as the [polylens package](https://github.com/DanBurton/polylens) I threw together for his and edward's earlier blog posts, and I did test to at least make sure that the blog examples worked as described.
Thanks for the tip and I will try that out. That would make things a lot simpler.
Yeah, 1 might be more clear; I'll change that. However, I don't believe it's the case that \varnothing (which is the symbol I used) denotes the empty type; that's typically done with ⊥.
That system is very expressive, too. I hear there's even a way to write a fixpoint combinator in it.
Reddit seems worse: http://www.reddit.com/r/programming/comments/w8bg2/confession_of_a_haskell_hacker/
Yep. I very much appreciate your work! I should add an acknowledgements section somewhere to add you and edward and twan to it.
&gt; Also, is it impossible (FSVO; assuming no insane hacks) to write lens-illegal code with getting and setting by virtue of them not being part of the "unchecked" API? After this discussion, I'm thinking that I may have to move `setting` to the unchecked API, once I'm convinced I understand exactly what the precondition is. Oh well. There is a reason this is version 0.0.0. :)
I think incremental API's might be very important for certain stream processing applications. However I prefer that a crypto library never expose the ECB-primitive for anything other than single blocks since it should never be used to encrypt anything more than one block with a single key. Also for the authenticated cipher modes it's important to know that there is no way of verifying a ciphertext without having seen all of it. It would be dangerous for an incremental cipher api to return chunks of the decrypted text when it's not yet know if the ciphertext has been tampered with.
I don't really understand why you want to generalise `Free` to `IFree`, which isn't that general. That may allow you to derive vectors, and that's about it. Maybe you could also derive trees of a given height, but I don't really see how that would be useful. I have to say that reminds me of [Jean-Yves Girard's joke about mustard watches](http://iml.univ-mrs.fr/~girard/mustard/article.html).
Ahah, I do the same sometimes, too. Though it is a rather frightening experience. Also I did make errors a few times, and I tend to write tests at least when I don't fully understand my own code... 
For authenticated encryption, i agree everyone should prefer them nowadays, but the package is not about evangelizing it, but just provides it. Also i think that users most of time would use whatever a protocol dictates them to use, which they wouldn't have a choice anyway. The main problem of crypto primitive in haskell is, performance. Hence why this implementation is in C, and the haskell is just the bindings part. For decryptGCM, i originally did exactly this (providing a maybe ciphertext at the output), but change it because i preferred the API to look symmetric, and i think it's useful that the user is given the tag (debugging purpose, alternative use). I agree though about the user/tag comparaison/mistake problem, and for this it would make sense to newtype the tag (which i originally did too, but changed it during debugging), and provide a constant time Eq instance.
For ECB, there's a lots of use to expose a multi block operations. Of course it shouldn't be used as is, as it would be *very* *insecure*, but for example CTR mode is just a ECB of a ever increasing number xor'ed with the input. You could obviously use a single block function to do that, but performance wise, it would be sensible to generate the input by multi blocks. For GCM, It could be indeed dangerous to handle the plaintext without having the tag checked (which only happens at the end), however there's lots of good safe use too that would benefits for seeing the plaintext early (which you would have to discard if the tag doesn't check). Overall i don't want to hold the user's hand at the code level if that's going to block some legitimate uses. The documentation should states everything clearly though (which at the moment is probably too lean, patch welcome :))
&gt; Usually for library code that is well-written there will often only be exactly one way to write it correctly That's why I mentioned naming and argument order. These things are still up to the developer, and deserve at least a minimum of thought (which for me means writing use-cases and asking myself if they're readable, or any of the degrees of freedom are worth investigating).
There's no general purpose padding that exists to do that (at the block level, which would be expected transparently by the block cipher). It always have to be explicit: using CTS mode, TLS style padding, SSH style padding, etc. Currently in cipher-aes, with mode that expect a block-size, it will exception-out if you try to feed it an invalid bytestring.
Explicit is absolutely fine. What I'm against is: * implicitly doing the wrong thing, and * claiming to be a general-use encryption library without exposing the necessary functions to do the right thing
If you enjoy reading this paper, then check out Pearls of Functional Algorithm Design by Richard Bird.
See also: http://blog.fogus.me/2012/05/02/a-functional-programming-influence-graph/
I discuss that in the post. The two main problems were the `Error` class constraint along with the less general `throwError` and `catchError`.
Oops, sorry. I went straight to the code. :)
To err is human. To go straight to the code is divine. :)
Also a lot of "languages" spawned or somehow linked to haskell are obscure research projects (Isabelle ? Felix ?, Timber ?, WTF!) While haskell is indeed a very fertile ground for programming research it hardly represents a huge influence in industry. Maybe 30 years later, but not now. 
It does if you have G |- a : T G |- b : T ------------------------ G |- a x b : T There's no problem in writing curry and uncurry in a unityped language.
Nice! [Mine took two hours, too.](http://hpaste.org/71147) I chose to print out answers in improving order (until I found a correct answer), since the problem statement asks you to find the closest one if there isn't an exact match. sorghum:~/programming% time ./test 926 75 2 8 5 10 10 75 (- 75 (+ 2 (- 8 (+ 5 10)))) (- 75 (+ 2 (- 8 (+ 5 (* 10 10))))) (- 75 (+ 2 (- 8 (* (+ 5 10) 10)))) (- 75 (+ 2 (- 8 (* (* 5 10) 10)))) (- 75 (+ 2 (- 5 (* (* 8 10) 10)))) (- 75 (* (* 2 10) (- 8 (* 5 10)))) (+ 2 (* 8 (+ (* 5 10) (- 75 10)))) (+ 2 (- (* 10 (+ (+ 8 10) 75)) 5)) (* 10 (+ 8 (- (+ 10 75) (/ 2 5)))) best answer evaluates to 926 % 1 ./test 926 75 2 8 5 10 10 6.37s user 0.19s system 99% cpu 6.577 total There's rather a lot of room for clever pruning when doing the search! I wasn't expecting that. Biggest remaining optimization would be to write a bag-aware `choose` function (i.e. one that is more careful about splitting duplicate elements).
The problem explicitly states that division is real division.
&gt; I also wonder how "Java Generics" were influenced by Haskell. Phillip Wadler, one of the designers of the Haskell programming language, was part of the team that developed [Generic Java][1]. His talk [Faith, Evolution, and Programming Languages][2] makes the connection to type classes pretty clear. [1]: http://homepages.inf.ed.ac.uk/wadler/gj/ [2]: http://www.youtube.com/watch?v=2t9q-936yxw
Thanks for that, Conor, that's fascinating. I must admit that I thought of indexing free monads by numbers entirely as a way to derive vectors, and hadn't yet thought through whether there were more interesting instantiations. Your VF and (:*) are vastly superior and more interesting.
Basically, everyone wants to be "like Haskell" but no one wants to actually use Haskell itself.
Grrrr, that's really annoying and leads beginners in the wrong direction since transformers is so commonly used. Your library goes a long way to making this clearer, but it sure would be nice if we could get transformers fixed and eliminate another dependency.
But to be an adjunction we require that `curry` and `uncurry` be inverses. But in untyped languages, passing a pair into `curry . uncurry` doesn't usually give you the pair back. Though it would be awesome if it did.
Ok, I'll talk to Ross about merging either into transformers.
I'm sorry, I just assumed time travel was involved.
I've heard that VB.NET has syntax for XML literals that were influenced by HSX/HSP from Haskell.
it's not *that* bad... http://gruntthepeon.free.fr/ssemath/
If you're wondering why this is getting downvoted, it's only because this was already [posted recently](http://www.reddit.com/r/haskell/comments/w17ea/using_return_vs_not_using_return_in_the_list_monad/) to this reddit. I didn't downvote you, but I just wanted to let you know.
I would have thought a quick call to sudo apt-get install ghc would do the job, I'm 90% certain that's how I did it. On 12.04 this will install version 7.4.1 of GHC. 
Ah I didn't know you could get the haskell platform straight from apt. My method of just installing GHC will just download the GHC 7.4.1 binaries and install them (no compiling)
I was thinking of code like this: https://gist.github.com/3078744
I wouldn't say F# is "heavily influenced by Haskell" (the obvious influence is rather OCaml), but I'm not too familiar with F# so I may be missing something. Besides the "computation expressions" syntax which can be considered a generalization of the do-notation (but that's already a bit far-fetched; the examples have used Monads quite a lot, though), what do you have in mind? Maybe the use of a lazy enumeration type (`Seq`) as a central data structure?
The people doing a lot of the development of C#, F#, and Haskell all work at the same company. I'd say that's a pretty clear link.
Well indeed Wadler participed in both, but that doesn't necessarily mean one influenced the other in a significant way. Wadler also designed Links which seems (from far away) closer to SML, or maybe OCaml, than Haskell (SMLy syntax, polymorphic variants, row types...). I am surprised by the idea of a clear connection between Generic Java and type classes, as they seem quite different in nature (eg. Generics are erased at runtime, while type classes precisely are not); I didn't want to spend one hour to look at the video, but I looked at the slides, and there seem to be no obvious relation (except that he uses the example of building a comparison on lists from a comparison on elements in both cases). Could you elaborate on what this connection is?
Oh yeah, and by the way Haskell is influenced by Excel which is influenced by the Kinect. Oh wait...
You can also use `:{` in ghci to write things with newlines: Prelude&gt; :{ Prelude| let null [] = True Prelude| null _ = False Prelude| :} Prelude&gt; null [] True 
And since ghci interprets the code the debugger is often unusable for larger apps.
Huh? I've never heard of this being the case. (I've also never tried loading a "large app" into ghci, so take my view with a grain of salt.) Can you give a specific example of a large app that renders ghci unusable? Thankfully, "newcomers" and "large apps" are typically orthogonal, so at least this isn't a concern for the former.
I'm quite confident you can use the software searcher thing of ubuntu to download ghc (searching for haskell, probably) I'm ( obviously) not a linux guy and it has been some time I tried that(11.x) but it was rather painless. I think that all I did was downloading the haskell platform. Maybe the easiest way to do it, with the full toolchain http://hackage.haskell.org/platform/linux.html
I usually use Debug.Trace when I really really need to know how the values are doing.
As of a while ago Num doesn't require Eq (or Show).
`apt-get install haskell-platform` is one way, as everyone suggests. You can also install via source using the excellent Debian `alternatives` system [1](http://wiki.debian.org/DebianAlternatives), [2](http://www.debian-administration.org/articles/91). I wrote a [guide and the requisite scripts for this at my github repo](https://github.com/byrongibson/scripts/tree/master/install/haskell). Installing from source is not too bad, works perfectly on 12.04 x64 (and I assume x32 but I haven't tested that), and there are several benefits: 1. Debian/Ubuntu/etc repos seem to consistently lag the current version of GHC and Haskell platform. (Though that might be getting better, judging from how quickly http://packages.ubuntu.com/haskell-platform and http://packages.debian.org/haskell-platform were updated with Platform 2012.2.0.0) 2. Manage multiple GHC and Haskell Platform versions, easily toggle between them with `update-alternatives --config`. 3. Upgrade to new versions of GHC and Haskell Platform without overwriting or deleting the previous. If this causes regressions in your apps, easily roll back to the prior working version with a simple `update-alternatives --config`. 4. Keep all files of GHC and Haskell Platform together in a single location like `/opt/haskell/ghc` and `/opt/haskell/platform`, instead of spread out over `/usr/bin`, `/usr/lib`, and `/usr/share`. 5. Easily uninstall with `update-alternatives --remove-all` (script included), and `rm -rf /opt/haskell`. 6. Run haskell via system PATH instead of user PATH (eg, no need to add `/opt/haskell/ghc/bin` to your PATH in .profile) 7. Get used to using update-alternatives, it's a great tool that makes managing manually installed, multi-version software painless. Java, Scala, Ant, and Maven all work equally well with it. The Debian Alternatives system is so good it's one of the main things keeping me on Ubuntu/Debian, instead of experimenting with the RedHat family, SUSE, or Arch.
One reason might be if you're interested in using IFree to explain/derive Free. Using the conventional notation in mathematics, the free monad generated by `f` is just `f^*(x)` where the star is precisely the Kleene star. The IFree version is just `f^n(x)`, and we can define `f^*(x) = \union_{n \in Nat} f^n(x)`.
Just to walk through why you get "No instance for (Show (a0 -&gt; a0))": Let's say you define a function like this: addUp :: Int -&gt; Int -&gt; Int -&gt; Int addUp x y z = x + y + z and you use it like this: ghci&gt; addUp 3 7 There's actually *nothing wrong* with the expression `addUp 3 7`. It's a perfectly valid Haskell expression. It evaluates to a function, with type `Int -&gt; Int`. Give it one more number, and it'll give you the sum. You can test this with `ghci&gt; let testFunc = addUp 3 7`. Works fine. (It's called "currying," and you may not have gotten to it yet.) The trouble comes when GHCi tries to show you what `addUp 3 7` *looks* like. When you type an expression at the prompt, GHCi evaluates it and shows you the result. It can only show you types that are members of the `Show` class, and in Haskell, functions can't be shown. So you get an error, and not quite the error you expect. There's a pattern here, actually. GHC tries manfully to find a valid program in your code. When it finally gives up, it's often a conceptual step or two beyond the source of the problem. The same thing is happening in your first example. When it sees `++` applied to `x`, GHC thinks, "Well, maybe x is a list, then, and xs is a list of lists. That'd work. Let's see...Nope! It's just Integers! Integers!" Error. Hope that helps!
map (case _ of {...}) xs map (if _ &gt; 2 then ... else ...) xs map (_*3+7) xs This is the syntax I want, modulo a different symbol than "_" perhaps since that seems like it might be problematic. Scope of the lambda is indicated by innermost containing parens. No special case needed for case expressions!
I was thinking `alert ((3,5).curry().uncurry());`
(_ * _ + _) ==&gt; \x y z -&gt; x*y+z (_ * (_ + _)) ==&gt; \x -&gt; x * (\y z -&gt; y+z) So the second one is nonsense and wouldn't type check, unless you've gone and made a Num instance for functions. An alternative would be to denote the scope with some kind of special wildcard-delimiting brackets, such as "\\{e}" for any expression e, possibly containing wildcards, e.g. \\{_ * (_ + _)} ==&gt; \x y z -&gt; x * (y + z)
As Bernard Hepton's character in I CLAVDIVS says, "One can recommend with less enthusiasm than one marries." so here's what I recommend, if anything at all. \ where p11 .. p1n -&gt; e1 .. pmn .. pmn -&gt; em I think it reads meaningfully, uses an existing layout indicator, and does not conflict with prior uses of that indicator. One could even imagine \ x y where .. meaning that the x and y are common to all the branches, with the "where" governing the subsequent arguments. Doubtless, there's something I'm not seeing. I haven't had my coffee yet.
That's a fair criticism of the innermost-containing-parens idea and I think I agree with you... What about special delimiting brackets for wildcards?
Looks like a kirby face raising the roof.
It may not conflict at the language level, but to the Haskeller's eyes it conflicts with what we expect `where` to mean.
Well, if you neglect lifting, the object (a,a,c) is isomorphic to ((a,a,),c), so you don't even need to discuss how tuples are built in Haskell.
Ah, I see what you mean. Updated the code, but I don't think it's watertight.
Isabelle is one of the most popular theorem provers in modern use. It's hardly an obscure research project.
Thanks for this, I was completely unaware of it.
Given how rarely I want lambdacase et al, I cannot work up any entusiasm on this subject.
If anyone can do it, GHC can do it. :-)
The influence wasn't evaluated until after Haskell existed, so it all worked out in the end. 
You're right. It should be `:: Integral a =&gt;`
Yes, it needs directions unless we're assuming that any two linked languages influenced each other, which I seriously doubt.
&gt; As Bernard Hepton's character in I CLAVDIVS says, "One can recommend with less enthusiasm than one marries." so here's what I recommend, if anything at all. lmfao Damnit, Conor, why are you so good at writing.
That seems like a reasonable answer if your perspective is that it's important to have the feature, and it's worth living with an ugly syntax for now. But I think most people's view here is more like "oh, that would be nice from time to time", and it's actually better to have nothing than a new wart.
&gt;&gt; Python's primary implementation is an interpreter. &gt; &gt;CPython compiles to bytecode. This insistence on a black-or-white &gt;"compiled vs interpreted" situation is getting ridiculous. You are just arguing semantics. When people talk about "compiled vs interpreted", they are referring to if it compiles to *native machine code*, or is interpreted at runtime. When you look at it like this, it is very clear. 
I don't see the point of such a comparison. What should it mean to the end user? I can compile to "native machine code" by producing an executable bundling the bytecode and the native interpreter. If you're trying to say something about performances, just say it, instead of making falsificating claims about implementation details.
Would a new keyword cause the pitchforks to come out? caseOf or lambdaCase [] -&gt; ... [x:xs] -&gt; ....
I wish I could see the code! Unless I am mistaken, the IEEE standard for floating point numbers stipulates that NAN == NAN evaluates to false. Could it be something so silly?
I don't mind lambdacase being added, I just don't think it has any real practical consequences.
So what would be the term for that? Breaking "optimization transparency"?
I don't know. I was going to kneejerk respond that it's not breaking referential transparency, at most it's breaking some intuitive "Eq laws". But compiling with or without optimization definitely shouldn't change the behaviour of a program, so it does seem like there's a bug here. I haven't looked into it beyond trying the example code so I don't know if it's in the optimizer or something else (which the optimizer is just interacting with).
This is a rather well known infelicity with GHC's native code gen, as Simon Marlow points out: http://www.haskell.org/pipermail/glasgow-haskell-users/2012-July/022572.html
What is (==) contracted to correspond to?
Which isn't a breakage of RT in and of itself.
And lambdas in general, IMHO. Sectioned operators or currying, otherwise Name That Function.
(==) does whatever you define it to do. It might correspond to semantic equality, it might be an equivalence relation, it might be a congruence relation, ..., or it might have no particular meaning.
As far as I can tell, either '==' and/or RT must be broken in this case. I'm aware that Floats can't be trusted to respect all the numeric invariants that hold for the Reals, but I suspect that equality should hold for identical Float values right? If equality does hold hold for identical values, then the values must differ in order for this test to fail. If the values differ, then that isn't that a break in RT?
== is not well defined on floats, in all of computing. If it could be defined, there would be no reason to have a "floating" point at all. The bug is that Haskell's type system canot cleanly express that most numeric types have equality but Floats do not. And I am not sure if Haskell can express that "f x == g x" is ill defined when x is not in an Eq type.
&gt; If so, that's the only place in the language where they are Records also require braces.
Yes, it does break RT.
Think of it like this 2.5 * 2.0 might not equal 5.0 That's fundamental to floats (Imagine a hypothetical implementation where 5.0 can be represented exactly, but 2.5 ends up being 2.499999 or 2.0 is 2.0000002. Order of operations can matter.
I was pondering this because of the general issues with floating point equality, not this specific issue. Removing the Eq instance would prevent people from comparing floats which is generally a good idea. Sadly Ord is defined to require Eq I think so this would be hard to do.
I just reread Simon's reply in the email thread and it looks like it was an x86 codegen bug. It all makes sense now. Thanks for your reply though :)
I think the correct term is "not reflexive".
&gt; 2.5 * 2.0 might not equal 5.0 That's not true for any even remotely sane floats. If you break arithmetic on numbers you can represent exactly in a few bits, you might as well say that representing everything as zero is a valid implementation of floats. &gt; That's fundamental to floats Not really. In a perfect world the IEEE standard on floats would be dead simple and everyone would respect it to the letter, allowing you to rely on operations on floats working properly just like you can rely on integers working properly. Alas, we do not live in a perfect world. However, the situation is not quite as dire as you depict. 2.5 \* 2.0 does equal 5.0, and 1.0+1.0 does equal 2.0.
They are indeed required. But braces are used in Agda rather specifically for certain things. Everywhere they're used, they're required.
Note that equality for FP types tests bits and shouldn't be assumed to represent equality of the real numbers modeled by FP. Numerical analysis applications always use a tolerance check rather than an equality check.
Internally FPUs often use excess precision for internal storage and truncate/extend the values on load/store. Naming a value probably causes the value to be spilled to a thirty two bit float while internally it was calculated with sixty four or eighty bits of storage.
It could also work for arrow procs: proc -&gt; case 'a of Pat1 -&gt; ... Pat2 -&gt; ...
As I’ve played around with Haskell, I’ve wondered — without having sufficient knowledge to conclude one way or the other — if it would be possible to provide a Show instance for functions at the GHCi prompt that displayed the type. Say, then, Prelude&gt; (+ 3) "Num a =&gt; a -&gt; a” Not a *huge* deal by any means, hardly even a minor one, but it’d be nice for beginners. And it’d more closely mirror the repls for e.g. OCaml.
That's an oversimplification.
Does disabling excess precision cause [exponentiation to lose accuracy](https://en.wikipedia.org/wiki/Extended_precision#Need_for_the_80_bit_format)? That is, when performing $x^y$ which is implemented as $2^{x \lb y}$ does disabling excess precision cause the intermediate values to be narrowed or only the final result? I don't have a 32-bit machine around to do an easy test.
Then perhaps you may want to check out my `Frame` type: http://hackage.haskell.org/packages/archive/pipes/2.1.0/doc/html/Control-Frame.html#t:Frame Although, to be fair, I think I've found an elegant way to convert it back into an ordinary monad, but it's still an instructive additional example of how one might used indexed free monads (it's actually an indexed free monad transformer).
No, it's supposed to be called referentially transparent. The issue is that it's not returning the same result when given the same arguments.
SPJ suggested this, but Simon Marlow and a few others expressed dislike for explicit-layout-only.
It's probably just that with `-O2` it's using extended precision.
Mandatory curly braces and semicolons: no using newlines and indentation to insert them implicitly.
How do you implement case/pattern-matching as Just Functions? e.g: zipWith f (x:xs) (y:ys) = f x y : zipWith f xs ys zipWith _ _ _ = [] 
I think Haskell needs to get a ShowDebug class, that like Typeable, should probably have automatic instances for all types (of kind *) without explicit instantiation. Then ShowDebug could be somewhat magical (use show when possible, (show . typeOf) when not possible) and really helpful.
Although this particular lambdacase is already wrapped into `maybe`.
How about `[wild|` and `|]` for brackets? ;)
i wrote some notes for myself, to give a short derivation of fixed point combinators "from scratch." in case they are of interest, here they are : **note 1** "self-application ties the knot in open recursion" given any recursive definition F = M[x:=F] (has occurances of F in M) we can try to eliminate instances of F. to do this we make the recursive call a variable ("open recursion.") try 1 : set G = \x.M . then GF = M[x:=F] = F. but of course this isn't good enough, it just unwinds one step of recursion. (can repeat GGF = F...) we would rather replace F with G to fully eliminate recursion. try 2 : G has an additional argument x, so GG = M[x:=G] has an open term. so instead set H = \h.M[x:=hh] then HH = M[x:=HH], so HH = F. **note 2** "define fixed point combinators from recursive definitions using open recursion" the best reference here is Klop, J "New fixed point combinators from old." 1) Curry's Y-combinator. let X_F be a fixed point of F. by definition X_F = FX_F this is a recursive definition, so now use note 1 : set X_F = WW, and try solve WW = FWW for W. W is a function of one argument, so we can write this as Wx = Fxx, or W = \x.Fxx then X_F = WW = (\x.Fxx)(\x.Fxx) is a fixed point of F. we can make this into a combinator Y = \F.(\x.Fxx)(\x.Fxx) 2) Turing's combinator. Suppose T is a fixed point combinator. then for any F TF = F(TF) this is a recursive definition, by note 1 set T = WW. then WWF = F(WWF) WxF = F(xxF) Wx = \F.F(xxF) W = \xF.F(xxF) thus T = WW = (\xF.F(xxF))(\yG.(GyyG)) 
The relevant syntactic extension here is [ViewPatterns](http://hackage.haskell.org/trac/ghc/wiki/ViewPatterns). Rather than "being" Just Functions, it permits the use of Just Functions in pattern matching. Plus there are base cases in the syntax, of course. The infinitely configurable language says nothing. But I tend to want to minimize my hard-coded syntactic primitives as much as possible. They subtly lock in design decisions in such a way that it's hard to consciously see them to realize they're wrong. (Deeper answer: Basic Hindley-Milner doesn't give you the primitives to pull apart the data such that you can pattern match it, it's structured to seal you off from that level. There's probably some more precise words to express this idea. I have in fact implemented pattern matching as a function in Perl, because you can get two Blobs O' Stuff and proceed to match on them. This produces, ah, "unidiomatic" perl code, but it so happened to be a big win in this one case....)
Can you describe how this package differs from the existing kmeans package on Hackage? http://hackage.haskell.org/package/kmeans
Please keep this going! I've been looking for something like this since I started learning Haskell.
If this was the case, floats in Haskell should not have an Eq instance.
I second the request to keep this up. I learned BASIC from working through game books in the 80s, and Land of Lisp to learn Lisp last year. The same thing for Haskell would be awesome. 
&gt; Or will the newly attached ReceivePort only get the new messages? I assumed this was the case, given the existing explanations. SendPorts are unique, so presumably, attaching a new ReceivePort to a SendPort means that any threads that have access to that same SendPort need to agree that a new ReceivePort is attached: any sends that occur prior to this agreement are not sent to the receive port, and any sends that occur afterwards are. What I find odd is that there is no way that I can see for a ReceivePort to be de-registered, other than `split`, which moves its registration to a new SendPort, but also registers a new ReceivePort to the old SendPort. Is there any way to take a SendPort with N receivers, and unregister the receivers to the point that it has 0 receivers?
Have you looked into zeromq for this kind of use case? This kind of sounds like an inproc pub/sub set of sockets to me.
[http://www.youtube.com/watch?v=JcniyQYFU6M](http://www.youtube.com/watch?v=JcniyQYFU6M)
Do you actually need a 32-bit machine? I thought using a 32-bit GHC was enough.
Hey neat. I recently decided to do the exact same thing, minus blogging about it; maybe I'll write up a comparison once we're both a little further along.
Instead of coding in parallel, why not join forces and code up a Most Excellent Roguelike? =D
Perhaps finalizers are transparently taking care of that for you?
The idea actually started with a text-adventure similar to the first project in Land of Lisp, but after seeing the Sokoban coding I wanted to do something that had a console component and a graphic component.
`fix` is a function that does what Y does, and what other recursion combinators do. But it does it by taking advantage of the fact that Haskell has lazy variable assignment, which the lambda calculus does not have. The definition of fix is fix f = let x = f x in x
I believe that the effect is the same, however the beauty of the Y combinator lies in its implementation, which is not the same as the implementation of fix. I don't think that it is possible to implement the Y combinator directly in Haskell due to the issue with creating infinite types. You have to avoid this by introducing a newtype: Prelude&gt; let y = \f -&gt; (\x -&gt; f (x x)) (\x -&gt; f (x x)) &lt;interactive&gt;:3:43: Occurs check: cannot construct the infinite type: t1 = t1 -&gt; t0 From http://stackoverflow.com/questions/4273413/y-combinator-in-haskell: newtype Mu a = Mu (Mu a -&gt; a) y f = (\h -&gt; h $ Mu h) (\x -&gt; f . (\(Mu g) -&gt; g) x $ x)
Ah but be fair, the clever bit in that sentence was written by Jack Pulman adapting Robert Graves making free with Suetonius. My involvement was purely editorial.
but what about let c = a * b in (c == c, c == a * b) ? Surely the fact that this returns two distinct variables, means referential transparency is broken? 
To be perfectly honest, ZeroMQ's inproc transport is not a good match for GHC. And I'm not sure exactly how their inproc pub/sub is implemented, but Control.Concurrent.Chan could rather easily be more efficient. In fact, I'd go a bit further than that and say the libzmq implementation as it stands isn't a very good match for GHC, as it's also seems to be trying to be a concurrency/event framework for more impoverished language implementations such as PHP and Python and C, which is something GHC doesn't really need. And since, in my experience, you have to use a given ZeroMQ socket from a single particular OS thread (I'm using 2.1.11 where you are supposed to be able to use a socket from multiple threads, but this doesn't really work yet.), this really clashes with GHC's implementation of explicit concurrency. And, ironically, I'm using split-channel to work around this problem: basically by representing a ZeroMQ socket as a Haskell thread that is bound to a particular OS thread, and communicating with the thread using split-channel. Which really is ok, as it fits in well with the structure of the application: a bunch of concurrent services in a vaguely Erlang/OTP-like fashion. And, the overhead of OS-bound threads and having one ZeroMQ context per socket when I should only need one context per process really is completely irrelevant for my particular application. Don't get me wrong, I think ZeroMQ is interesting, and I am using it. But this is much simpler and a better match for in-process Haskell communications. And I think there are compelling reasons why a native Haskell implementation of ZeroMQ would be better for GHC when dealing with interprocess and network communications.
&gt; Or will the newly attached ReceivePort only get the new messages? What other semantics would possibly be reasonable? The best I can come up with is keeping all messages around since the creation of the SendPort, which means not GC'ing anything until the SendPort also gets GC'd. You can also duplicate a ReceivePort in split-channel, just as you can using Control.Concurrent.Chan. This means you get a copy of every message already sitting in that ReceivePort.
&gt; What I find odd is that there is no way that I can see for a ReceivePort to be de-registered There is no registration. Just let the ReceivePort(s) get garbage collected, and you can reduce the number of ReceivePorts attached to a SendPort, possibly even to zero. The queue is basically just an mutable, append-only linked list, with a pointer to the next message. So for a message to be retained after a garbage collection, something has to be retaining a ReceivePort. Control.Concurrent.Chan is very similar to a tconc queue from the Lisp world, if anybody here is familiar with that.
No need for finalizers, but yes, it's all transparent. Just let the ReceivePorts get garbage collected if you want, as retaining them is what also causes the messages to be retained.
As far as I know there is no way to directly do what you want. Ghc only uses the instance head to pick the instance. But depending on what you want to do there's most likely some way to reformulate it to get what you want. 
Yes, as SimonM explained, it's a code generator bug. 
I know it's a bad idea, however my alternative is to add an extra parameter [in my case, that of a monad constructed using transformers] and its by far the preferable of two evils. Jason Reich from York helpfully led me onto this, however: it's bad voodoo but it's a round-the-houses way of getting roughly the idea: http://www.haskell.org/haskellwiki/GHC/AdvancedOverlap I entered GHC madness a long time ago, I'm afraid.
You want the `data-lens` package, specifically `Data.Lens.Lazy` which provides an easy way to access portions of a complex state object. I wrote a [tutorial](http://www.haskellforall.com/2012/01/haskell-for-mainstream-programmers_28.html) on using lenses both with and without state. The big utility of lenses is that you can compose them to access and mutate varying levels of complicated nested records and they permit much nicer syntax than the built-in record syntax. The only specific issue you mentioned that lenses do not address is namespacing. The only good way to namespace things in Haskell is on a per-module basis. It's pretty lame, but it's the only tool we have at the moment. So for larger projects where namespacing becomes an issue I will usually split data types with conflicting field names into separate modules. However, I know some research is going into developing a first-class namespacing system.
Thanks for the link, I'll be sure to have a crack at it. I'm not comfortable hacking around with type class inference in the first place, ergo coming to Reddit! A large portion of this work relates to catamorphisms - building on my previous paper at http://red.cs.nott.ac.uk/~led/papers/led_towards_tfp11.pdf - so anything I can do to numb the pain somewhat helps. Much appreciated! :) 
To qualify, I told him this was a way to do it. I didn't say I liked it.
I've been working on something similar using free monads, except using data types instead of type classes. I'll write a blog post describing it and link you to it.
good tutorial, though :)
Yes, sure! There was another k-Means package on Hackage - besides kmeans - that seems to have been retired (or I can't find it), I will comment only on the main differences with kmeans. * Data Structures hps-kmeans use the Vector package while kmeans uses Data.List. "Standard Lists" where my first approach, but switching to Vectors led to an impressive increase of performance (one order of magnitude). Vectors have not implemented yet all the functions of Data.List (like nub) and pattern matching (no "x:xs"), but they are a hell faster. Boxed Vectors can contain pretty much everything, so if implementing the remaining methods in Data.List, it should be the "same" (I don't know about internals yet, but I think some things about the Vector implementation can be incorporated to Lists - but it is just an impression). If you have your data in a list (probably), then "fromList" should do the trick and you won't have to deal with other data types / heavily modify your code. This is one part of the intention of "simplicity". * Centers In kmeans centers are chosen arbitrarily by the package, while in hps-kmeans they must be explicitly chosen by the user. I find this not a minor aspect, as it is recommended to chose the initial centers (somehow the seed of the algorithm) at random. Also, you may want to run the algorithm (and you should) from different starting points (different centers) and compare if the results are the same (this can be done by comparing the common elements of the clusters among different runs, because the centers are returned in no particular order - one starting point may produce the same cluster as "1" and other as "2" - I shall implement this in future versions / releases). * Readability "Readable" is really a subjective term, it Just True for me but Nothing to you. It is for me as I wrote it, so I am more familiar with it, but I find it more "close" to the way I studied / learned the problem. There's a plenty of room for improvement in this topic, as I am not happy enough with it. * Statistics Approach It's a different approach from kmeans, I'm a Statistician, and in Statistics the procedure is framed in a stochastic process generating the data for which you want to infer some of it parameters. You can see it also like an algorithm for Voronoi Tessellation, which is not bad, it's just different. "Different" does not means "Better", it may be more suitable for some cases while not for others. I don't think the packages are exclusive (one or another, but not both), because depending on the problem and your approach, you may find the kmeans implementation easier to use than hps-kmeans. * Safe Haskell I began to read &lt;http://www.haskell.org/ghc/docs/7.2.2/html/users_guide/safe-haskell.html&gt; but I don't understanding completely yet, but fore some reason, hps-kmeans is tagged as Safe Haskell in Hackage and kmeans isn't. I tried to stick to the Perfect / Pure world, avoiding monads in the implementation and leaving it to the user. kmeans does the same, but for some reason it is not tagged as "Safe Haskell". Don't know why, but, hey, it's nice for hps-kmeans!!! :D hehehe! Those are the main differences I find between the packages. Also, hps-kmeans aims to provide a "framework" for implementing the algorithm on a metric space, and more decoupling from R^d must be done. This is addressed also by kmeans, but it needs more decoupling also - IMO. This has taking too long (in terms of chars), I hope I have answered your question. There was another package, which used Vectors. It didn't run when I tested and used folds a lot. I found it not readable at all, as I couldn't "follow" the algorithm. Mostly for that package is the term "readable". Cheers, Rodrigo
Thank you very much. I really appreciated your tutorial.
Err wait, I was mistaken. `dupChan` is basically equivalent to `listen`. `duplicate`, believe it or not, is brand new.
Oh, and one other thought, one reason why you might use libzmq in conjunction with Haskell is that you wait on events from multiple sources, whereas in Haskell you do need a thread for each event source and then maybe feed them into a Chan or split-channel or whatever... though the concurrent Haskell paper does demonstrate a few combinators to do this more automatically for you. It'd be interesting to benchmark the two approaches in Haskell. I think tibbe has talked about maybe providings primitives so you can wait on several file descriptors. I don't really need that so I haven't investigated it; I more often find myself wanting to wait on multiple concurrent primitives, such as an MVar and a Chan, or two Chans, or whatever, and the only solution for that at the moment is to use multiple threads, or a combinator that will use multiple threads for you, or restructure your program a bit to avoid the need on waiting on multiple sources.
Why is the blog article dated october, 5th 2012? :D
This would be interesting to see done using a FRP framework for comparison. I might muck around and try it a little.
Nice. I've been wanting receiver and sender to have different types for some time.
Because yesod is from the future!
I'm all about forces being joined!
For that, you would visit http://halvm.org 
I would recommend bumping up to aws-1.* for this change, and reserve aws-0.* for bug fixes etc for people who are stuck with old GHCs. That way 0.* can just die out whenever demand for it dies out.
[Erik Meijer](http://research.microsoft.com/en-us/um/people/emeijer/) is the link where Haskell influenced C#. [Conor McBride](http://strictlypositive.org/) is the link where Haskell influenced Epigram. I once knew exactly how Haskell list comps got into Python, but I don't remember off the top of my head.
That issue is a very interesting point. I can't of the top of my head think of a good solution, but I'll think about it. :-)
Just drop the "case". What is wrong with lining up several lambdas together? We are already writing something like: fooMaybe &gt;&gt;= \x -&gt; .... A simple extension would be: fooMaybe &gt;&gt;= \ (Just x) -&gt; ... \ Nothing -&gt; ... Or if you prefer { and }: fooMaybe &gt;&gt;= { \ (Just x) -&gt; ... ; \ Nothing -&gt; ... } 
Not yet - most of my code is still pretty messy and experimental, although the FOV code is actually quite simple and self contained. I've uploaded it [here](https://gist.github.com/3094852) if you are interested. I do plan on putting everything up on github pretty soon though.
Not to ruin the joke, but your nickname is excellent!! http://www.youtube.com/watch?v=ISy0Hl0SBfg
I should also point out that DataKinds was not an "officially advertised" feature of GHC 7.4.x, and in particular the implementation had lots of bugs. I don't know whether any of them would affect you; it may be that your use case is simple enough not to run into trouble. In any case, the bugs have been mostly (all?) fixed in the HEAD, and DataKinds will be an official feature of 7.6.
tl;dr Hastily written Haskell beats the socks off hastily code in Ruby or C++ for academic/scientific software development 6.5x parallel speedup with 12 cores is nice! Is there a good tutorial on writing your own Arbitrary instances, including shrink? Viterbi slowdown of 7x versus C++ is more than I would have expected, but as the paper said, doesn't matter much for actual runtime of this program. Wow, three months to writ MRFy versus 1.5 years for SMURF, and MRFy does way more? PS. the bit of verse at the end is hysterical :-)
Thanks for posting this! Has anyone else in this subreddit taken a look at the [BioHaskell](http://biohaskell.org/) project? It seems like there is a ton of opportunity for development, but there don't seem to be many people contributing to the project.
LINQ is already noted as influenced by Haskell, and indeed I think that is definitely correct. But outside LINQ, what would you point out as definite influence of Haskell on the C# language design, besides "oh, they knew Haskell well" (sure, they have educated people at MS...) -- and VB.Net? I think you could also say that the work on Rx is influenced by Haskell, but it's more library-level; finally, the research on STM was probably prompted by Haskell successes, but it didn't make it. [SHE](http://strictlypositive.org/she/) certainly influenced Haskell, but what precisely in Epigram would you point out as "influenced by Haskell"? I don't know Epigram well, so I may be missing something obvious to the actual practitioners -- like typeclasses in Isabelle.
See also https://github.com/technogeeky/icfp12-paper-links
Seems like their git server is down, are there any mirrors to download it?
https://github.com/GaloisInc/HaLVM
Using type-classes for name-spacing is a bad idea. Here are the disadvantages, some of which have already been mentioned in this thread: * They have no associated laws to define correct behavior other than they "sort of" do the same thing. The best (worst?) example of this problem is the `filter` class, where filters on pipes/conduits only vaguely resemble the equivalent operation on lists. As Edward Yang pointed out, the point of these laws is to define how they interact with other components so that you can equationally reason about code, not just to group a bunch of similarly-named things together. * Type-classes are not first class. I'd prefer to see something like [first class modules](http://research.microsoft.com/en-us/um/people/simonpj/Papers/first-class-modules/first_class_modules.pdf) used to solve this problem. * The proposal involves a bunch of questionable extensions, all of which are very brittle (because type classes are not first class), even more so than the existing module system. * Without any algebraic laws, things will now silently fail if applied to the wrong argument, as Dan mentioned. I hate this, a lot, and it's very contrary to the spirit of Haskell and type-safety. I'm speaking from experience where I actually used this principle on a large library of mine for parsing Protein Data Bank (PDB) files, where there are tons of records that have overlapping names. I tried the type-class approach of type-classing shared names that sort of had the same meaning and that was a disaster (again, things silently failing) and then I switched to simply namespacing them using modules, which while not perfect was significantly better than the type-class approach because it didn't lead to bugs and was no more difficult to implement and use. I think the correct way going forward is some form of first-class namespacing. We should make more things accessible to our ordinary functions, not less, otherwise we will end up with the type-class situation where we have to introduce a new extension just to recapitulate functionality at the type-class level that we already trivially had at the functional level.
&gt; Mainstream languages require extended syntax to let you break out of multiple nested loops. On the flipside, they don't require you to say `liftIO` in those loops.
A nit: Why extend `++` to be `mappend`, instead of exporting `&lt;&gt;` (the new alias for `mappend`)?
You can click "Edit this file" here: https://github.com/technogeeky/icfp12-paper-links/blob/master/README.md
On the flip-flip-side, they don't provide a referentially transparent way to return a value upon breaking from a block, nor can they type-check that you provide the correct type of value depending on how many levels out you break.
I recently released the [control-monad-loop][1] package that lets you define loops you can break out of. Sample usage: import Control.Monad import Control.Monad.Trans.Loop main = foreach [0..] $ \i -&gt; foreach [0..] $ \j -&gt; do when (j == 0) $ continue -- skip to next iteration when (j &gt;= 5) $ exit -- exit the loop when (i &gt;= 5) $ lift exit -- exit the outer loop by calling 'exit' -- in the parent monad liftIO $ print (i, j) It's similar in principle to the EitherT approach in that it uses an early exit monad. However, the monad provides two ways to exit (`continue` and `exit`) rather than just one. The loop driver (e.g. `foreach`) fills in how `continue` and `exit` are to behave. [1]: http://hackage.haskell.org/package/control-monad-loop
In addition to the serious drawbacks mentioned by others here... The problems mentioned in the post are one of two things: import bloat or a much more complicated design question that can't be solved by throwing type classes at it. Every language I've ever used (and maybe every language period) suffers from import bloat for large projects. The Haskell module system, even though it is often criticized as being inadequate, actually gives us a pretty nice way to mitigate this problem. (See http://hackage.haskell.org/packages/archive/snap/0.9.0.1/doc/html/Snap.html for one example.) When this facility is inadequate, we find ourselves smack in the middle of something called essential complexity. But that essential import complexity is already a solved problem. The solution: IDE's. I've written far larger Java projects than anything I've ever done in Haskell, and Eclipse's import allowed me to pretty much forget that Java even had an import keyword. Haskell's IDE support isn't quite to that level yet. So let's stop this typeclass madness and put the effort into something that has been proven to work in the past and needs much less wholesale community buy-in.
I'm not sure that you need `MonadBaseControl` magic for generalizing `withFile`. It's just a glorified `bracket`, which is a glorified `catchE`, which can be implemented in any `EitherT e` monad.
+1 for IDE support. I'd love to have emacs auto-suggest adding import statements based on compiler errors. I suspect chrisdone could hack this up in no time :-)
I think you're cheating here. It is *not* easier to reason about the semantics of `forever $ lift getLine` in a referentially transparent language than it is to reason about `while (true) { readline(); }` in an imperative language. The difficulty, in both cases, is to understand how the effects are combined by the looping construct. In C, you have to know the semantics of your language, and in Haskell you have to know what the `$` actually does. It is not much simpler because they are equivalent, so the difficulties will be substantially the same. In a more general way, there are two ways to look at Haskell's monadic code: - you read the "high level" meaning, using the `do`-notation or forgetting the `$`, `&gt;&gt;` and stuff as white noise. This allow you to concentrate on what you're actually doing and I suppose that's how most people think in practice. This actually means writing in *imperative style* (that's why we say that Haskell is one of the best imperative languages), and you have the exact same reasoning issues as with a hardcoded-to-be-imperative language; but it usually works well just like imperative programs usually tend to work. - you read the low-level meaning, actually desugaring the `do`-notation in your head or thinking about the definition of the monadic operators for the particular monad at hand. Things actually behave in a referentially transparent way. But at the same time, it's harder and you can fit less code in your head at this level of details, exactly like it's not practical to actually think in detail about the fine-grained operational semantics of your imperative language when writing simple stuff. Monads are useful for allowing one person to use *different* effects in the different place of a program, instead of being tied to one fixed combination of effects planned in advance by the language designer; and you can write generic effect combinators (such as `forever`) that work for all of them. You have more flexibility as a programmer. But you still have to choose between the convenience of forgetting about effects and the simple reasoning of a detailed compositional semantics. Yes, you have a "referentially transparent way" to write "loops as conveniently as in an imperative language", but you have to change your glasses in between.
If you want; but my point was that before desugaring, "referential transparency" doesn't bring you much locally, and after desugaring you have code that is harder to read (because it's more low-level). I'm sorry because it looks like the scope of my remark was not clear enough: it was a comment about your reply above emphasizing referential transparency, *not* about your blog post itself -- that I think is reasonable, if maybe a bit delusional in the use of the terms "incredibly simple" and "ridiculously simple". I would be interested in seeing the `ContT` equivalent of the multiple nested loops with non-local break, because if you can bind the break continuation at the loop level you could have more readable code (no need for lifting).
I think most people can understand how the `Either` monad works. It stop because it receives a `Left`. I consider that pretty simple semantics. However, I do get your point about referential transparency. It's like how `State` is referentially transparent in principle, but from a cognitive standpoint it incurs greater over-head because you still have to mentally keep track of state. It is much more difficult to equationally work with stateful expressions even if you desugar them since other than the special case of Kleisli composition stateful expressions don't have any nice properties.
No. If there must be a herald, it can be \
I have looked a bit around on the site and I can't find anything about what kind of tasks it is anywhere. The only programming contest I have ever been in was a local qualifier to the IOI. Are these tasks of similar nature(algorithmic) or is it something different?
You may have a point. I mean, it's not a good idea to make \ a layout herald, but it may be the case that no layout is required at all. Are there ever situations where \ p1 -&gt; e1 \ p2 -&gt; e2 \ p3 -&gt; e3 would be a meaningful expression? Much though I wish I could write foo \ x -&gt; blah to apply a higher-order function `foo`, I currently must write foo (\x -&gt; blah) or foo $ \ x -&gt; blah Perhaps we can just write a bunch of lambdas in a row. Is there a counterexample which would make this ambiguous? Why not make \ a layout herald? Lots of people write code like foo $ \ x -&gt; goo $ \ y -&gt; boo and it would be a real nuisance to be forced to write foo $ \ x -&gt; goo $ \ y -&gt; boo to make sure everything subordinate to the \ was to the right of it. Otherwise, what's the proposal?
A random suggestion: you could have `($) = value`, turning `value f x` into `f $ x`.
But I still haven't finished all the Euler problems
As far as I can tell (correct me if I'm wrong), Dijkstra's criticism does not apply because we can distinguish how we reached the exit point of the loop and the proof that both exit paths are equivalent is that `fmap (either id id)` type-checks. The only way his point makes sense in this example is if there is another monad in the stack that is stateful, but that's not the fault of the `EitherT` part of the stack at all, nor the fault of `break`.
While laws are definitely important, I think that useful and sensible typeclasses can be made without equational reasoning. But in the context of classes in a core standard library, you definitely want this. When previously discussing the practice of superclasses in typeclasses, this is often cited as the reason to have superclass constraints (along with providing defaults). The problem with this is that, as you observe about typeclasses in general, superclasses are very brittle (the Show/Eq/Ord splitup, for example). There is of course more point to this than namespacing - polymorphic functions! I agree, though, an eventual consolidation of ADTs / modules / and typeclasses, and a syntactic homogenization of functions on these different things would be ideal. While at times it may feel like a kitchen sink, consolidation can happen once it becomes clear what works and what doesn't. I've put together a proposal that is in the same vein as Superclass Instance Defaults, but really quite different, and have a WIP Template Haskell based prototype (which isn't ideal, but I'm not familiar with GHC's code, and this will let people play with the idea without much trouble). It essentially gives you ML-style functors, but on typeclasses rather than modules. I'll write a blog about it as soon as it's roughly feature complete, hopefully in the next few days. https://github.com/mgsloan/instance-templates [Here's](https://github.com/mgsloan/instance-templates/blob/master/Deltas.md) how it takes the flexibility of the module system to the next step. With this, we could express most of the deltas you'd reasonably want to apply to an API while still retaining compatibility. This means that we can both export old APIs in terms of the new, and write down (or generate) statically checked deltas for upgrading code to the new API. The best thing about this is that these deltas are simply written in the language itself. I've read your post "Scrap Your Typeclasses", and I thoroughly agree that something's missing in the typeclass system. I don't think that passing them around explicitly is the solution, though, as that really gets us away from the declarative ideal of only writing down the things that matter, and not the auxiliary plumbing. Anyway, nearly all of your concerns are addressed by my proposed extension. Except perhaps for "Instances parametrized by run-time values" - we can already do that with ekmett's reflection package, but this extension does not provide it for *run-time values*. However, you do get instances parameterized by *values*.
This looks horrible, I won't be using this in any of my projects. I also won't be using any library that carries a dependency on it, to hopefully expedite its eventual death. 1. Typeclasses like this mean that my error messages will become nigh-on inscrutable, and more type signatures will need to be provided. 2. In my opinion, namespacing problems should be solved with a module system like Agda's, not abusing the typeclass mechanism. 3. This feels horribly unrigorous. You're just going to arbitrarily generalise every function in the prelude? With no hard algebraic laws to go along with those classes? I would be much more comfortable doing something like this in Isabelle, where at least you can demand proofs of properties you want to hold. E.g, if an item is insertable and it has a length, then inserting an item should increase the length.
Just compiled Clang, after a few failed attempts. I love it already. Clang (invoked as `clang a.c`) rejected a simple Hello World-ish example due to a type error and various printf-related warnings, giving spot-on suggestions for improvement in a very clear, colourful terminal output. GCC silently accepted the same code (invoked as `gcc a.c`). With Haskell being my language of choice, I'm happy to see that Clang is being pretty strict in type-checking C code. So thanks again for the hint :)
&gt; However, there is still some weird culture of teaching ContT for exiting from loops, which is incredibly over-kill and bad practice because it makes beginners think it's complicated when it's not. Well, how could I resist? [Breaking From a Loop with ContT](http://unknownparallel.wordpress.com/2012/07/12/breaking-from-a-loop-with-contt/) No, it's not quite as beginner friendly. But ContT really isn't that scary; just like Monads, ContT is actually a Lovable Warm Fuzzy Thing.
It's fine - you can start with ICFP as long as you finish them. It's always an honor to compete with someone who has finished all the Euler problems.
But convenient syntax overloading is what he is going for, isn't it?
Yeah. I don't know how often they update the print copy to match the fixes suggested by the comments, though.
Yeah, the reason typeclasses often come with "laws" is that they would severely hinder equational reasoning otherwise. If a method can mean anything you want it to mean, depending on the type context around it, then rewriting expressions is pretty much hopeless. Even with modules, you probably want some sort of specification of what you want things that share the same name to mean. Generalizing without thinking hard about what the generalization _means_, and how it interacts with other functions is the wrong way to go about this, in my opinion.
So, just to sum up the feedback: * Typeclass driven polymorphism has no place in broadly used Prelude code, except for Monads and Numeric Types. * If an API enables library authors to abuse it, it is bad. Notable exceptions include every haskell library. * All library functions should be based on algebraic laws. Also, these laws should ~~never~~ be occasionally broken (see IO monad) but infrequently enough that people assume they are safe to follow. Furthermore, reasonable superclassing (Monad a =&gt; Functor a) should be made impossible for silly, algebraically undefined error handling. * If a library happens to help with name spacing issues but is primarily for removing unnecessarily concrete types in the Prelude, its bad because typeclasses weren't designed for namespacing. * Classy Prelude is bad because typeclasses dont solve the record namespacing issue in a general way. * Using rather ~~old and tested extensions from GHC 6.8 I believe~~ new extensions is bad but... * Classy Prelude should be based on its own custom GHC extensions. As always, thanks for the constructive, actionable feedback! Much appreciated.
I will. It may be over a year before I ever release it because right now I'm trying to complete my thesis and it is low on my list of priorities. I only release libraries that I commit to maintaining actively, which means I need to be reasonably sure they are nearly complete.
I appreciate what you're doing here as I'm a novice Haskell developer, and I also like the ~~cut of your jib~~ ~~your snark~~ your retort to the people in this thread. But I am not persuaded that Classy Prelude solves my problems, and I can only say that from trying to develop against Haskell for a little while, my problem is that there are literally a million ways to do things, and every library has chosen one of those ways. Case in point: strings. No other programming language that I know of has the epic fail that is Haskell's string situation, and having to interface with multiple libraries is almost an absurdist form of programming art. A `String`/`[Char]` here, a `Text` there, a `ByteString` there... A lazy `ByteString` there (with the same name, no less!) This is absurd. The next version of Haskell should standardize strings, period. My other problem has to do with dependency hell, but string handling is probably my #1 complaint.
"simple-sendfile now sends headers in the same system call as the sendfile itself, resulting in much better performance for static file serving." Seems to be not entirely true (I wouldn't know how to do this either at syscall level?!). Looks like the headers are sent using send(2) with the MSG_MORE flag set, after which sendfile(2) is called, so you'll have 2 syscalls (which is about the best you can get in Linux I think anyway).
As the author of either I have no objection. I'd happily distribute the more exotic of my instances to my other packages as they all depend on transformers.
Very cool. Though I think for practical uses, the `lift` technique you chose is better. If you are breaking around in more than 2 levels of nested loops, you're probably doing something very, very wrong.
I don't think so. The error constraint is a pretty long standing feature of that library. When the topic came up during the move of the Monad (Either e) instance into base it was decided that ErrorT should be left with the Error constraint intact and any such changes should be on a new type. Keep in mind that losing the Error constraint would change the semantics of existing programs. They'd go from being able to handle 'fail' to hard crashes -- hence the desire for a new type. We almost didn't get to move the instance for Either for that same reason.
No problem. I added left and right to either. Pull 3.0.1 from hackage. 
Alas when you start adding the requisite signatures, it is a lot less convenient than it seems at first. Take for comparison our standard but nigh unusable regular expression library. =/
Note: many of these classes are actually present in my keys package, where they arise because about half of them are definable for representable functors. The main difference is that I do have a set of laws. -- I should probably add them to the documentation though. ;)
Iit was a summer school in France, I don't think it has been recorded.
I'll preface this by saying this is not how I would normally do it, but I imposed some constraints on myself: onLines :: (String -&gt; String) -&gt; (String -&gt; String) onLines = (unlines .) . (. lines) wordWrap :: Int -&gt; String -&gt; String wordWrap = onLines . wordWrapParagraph wordWrapParagraph :: Int -&gt; String -&gt; String wordWrapParagraph n = unlines . accWrap "" [] . words where accWrap accl accp [] = accp ++ [accl] accWrap [] [] (x::xs) = accWrap (accl ++ x) accp xs accWrap accl accp (x::xs) | length accl + length x &lt;= n = accWrap (accl ++ " " ++ x) accp xs | otherwise = accWrap x (accp ++ [accl]) xs I tried not to change your variable names (but I did use `x` and `xs` instead of `head t` and `tail t`), and I did not change the algorithm. However, the most important change (which I did not do due to time constraints) is use a different algorithm entirely, one which builds up the result more lazily. I think a bit of time spent to think about it very hard might result in something spectacularly smaller. It may be possible to even use `Data.List` functions instead of explicit recursion at all.
simple-sendfile requires conduit ≥0.4.1 &amp; &lt;0.5, and warp requires conduit == 0.5, so I don't see how warp can be installed?
That onLines function looks like a neat little trick. Is that a construction that is common in Haskell? I don't think I have ever seen it. Took me some time to work out what it does. :-)
you wainkers :)
Lambda calculus alone has unspecified evaluation order. 
Okay, so I admit that documentation is somewhat lacking... The haddocks generated by the code on the [github](https://github.com/AccelerateHS/accelerate) repo are a bit better laid out than the version on hackage. Otherwise there are several examples to be found in the accelerate-examples package; [hackage](http://hackage.haskell.org/package/accelerate-examples), [github](https://github.com/AccelerateHS/accelerate-examples). There is also a mailing list: accelerate-haskell@googlegroups.com, which you can join [here](http://groups.google.com/group/accelerate-haskell). There is the start of a wiki attached to the github repo as well. It, again, is a bit sparse, but feel free to edit it, even if just to list all the things that didn't make sense (that's actually very helpful).
I wish there were a way that you could tie version dependencies in a more fine-grained way to specific functions in a library so that if the ABI of those functions doesn't change then it is still compatible.
Yes, it's a combination of Haskell's guaranteed laziness and Haskell's variables.
What are some good methods for seeing whether the optimizations you are trying to achieve actually happened? (Besides benchmarking, that is.)
Check the core that is output by `ghc-core`. Unfortunately, it is very difficult to read at first until you get used to it.
I like having a shortcut in Emacs which will popup the core-that-would-be or generated ASM of the given function so that I can keep changing the source and looking again to see how that affects the output. You need to grok ASM and the ASM is quite long and optimized, but it can be illuminating. Core is generally legible too. Using the profiler goes without saying.
I wonder if hlint can suggest some of the performance optimizations (maybe with optional argument). These look like easy enough to spot and are mostly the same things (strict fields and thunks)
Function application has a higher precedent than operator application. So if you see (foo a) &lt;operator&gt; (baz b) you can remove parens: foo a &lt;operator&gt; baz b Also, use hlint. It will suggest you what unnecessary things to remove. 
That *is* more pleasing on the eyes...
This project started from the "lets get shit done" side of the community as opposed to the "lets write papers" side. Yes, CanFoo does not correspond to an existing category theory term that 25 people in the world know about, but most of the classes do have invariant laws that describe their expected behavior, we just have not written them yet. I think I can define almost all of them off the top of my head but haven't checked each one. Please note that this is only version 0.2 so they are not fully documented or described. I really do not understand the vitriol that this has generated. 
Thanks for the feedback. I dont know why you haven't received my upvotes, this was actually constructive and highly appreciated!
Fun tutorial. I've not had this much fun in haskell for ages! Looking forward to the next one.
That's all nice and well, but the baseless-assertion package has several well-known flaws. Fortunately, they are easily corrected by my upcoming evidence-less-statement package. The key difference is the use of a partiality monad `NotSure` to express undecidable statements.
Since your package hasn't yet been released, my suggestion is not too late: "evidence-free-statement" is a better package name, as it both minimizes confusion with already released packages and suggests that your package upholds the ideals of [point] free-dom.
I'm not really sure "let's get shit done" is the right description: while managing imports from a variety of modules is a chore, especially for beginners, it is by no means the biggest problem most people face on a daily basis. So really, what is being encouraged here is a different *aesthetic*, with the claim that it makes it easier to use text/unordered-containers--and a large portion of the community who dislike this aesthetic, partially because it is the aesthetic of the programming world at large (which they have rejected) and partially because it solves a problem that never really bothered them in the first place.
Of course it does.
Nobody suggest that you must absolutely use lists to represent the search space; there are various search monads that have more efficient implementations. And in the other direction, there are various situations where Prolog hardcoded DFS behavior makes for bad performances (unless you go into the hell of the cut rule) and possible non-termination for programs that have perfectly reasonable behavior when using a different search strategy.
Hmm, I must have introduced that bug myself at some stage. GHC seems not to complain so long as all the lines in the definition start at a larger column than the name of the thing being defined. So this monstrosity ugly s | s == "foo" = "foo" | s == "bar" = "bar" | s == "baz" = "baz" is accepted, for better or for worse. 
Yeah you can. You just have to tie the knot.
The interesting thing about baseless-assertion is that you can never apply it to itself. I even proved it in IdriCoqDa # baseless-assertion : Assertion -&gt; Assertion # represents the assertion that the input assertion is baseless # left as exercise to reader Theorem cannot-ba-ba : forall (alice : Assertion), let bob : Assertion = baseless-assertion alice not (baseless-assertion bob) Proof. intros alice bob. eauto. 
Regardling [slide 6](http://johantibell.com/files/haskell-performance-patterns.html#%286%29): data Tree a = Leaf | Bin a !(Tree a) !(Tree a) data IntTree = IntLeaf | IntBin {-# UNPACK #-} !Int !IntTree !IntTree I wish there were a nicer way to do this sort of thing automatically. Desired usage: data IntTree = specialize Tree Int Perhaps this could be achieved with TemplateHaskell. something like this: $(mkSpecialized ''Tree ''IntTree [''Int] [Strict, Unpack]) Of course, it would be nice if GHC could automagically create the unpacked IntTree for me, and replace any uses of `Tree Int` with `IntTree` for me. Of course, I wouldn't want it to do this without my permission: I should *ask* it to perform this specific optimization somehow: {-# specialize structure Tree Int #-} Do any of these ideas sound like they are any good? Does ghc provide anything similar to these already?
I would suggest an Ubuntu distro (possibly also Fedora). Have haskell-platform installed, obviously. Then, imho, it should contain *all* of the following (covering most of the bases for what various people like to use): Editors * emacs and emacs-mode * vim and ??? * eclipse and EclipseFP * SublimeText and ??? * Leksah All editors should be configured by default to sane syntax highlighting, and easily detect compile errors without having to leave the editor. Hooking into ghci is nice; preconfigure all plugins so that advanced features Just Work. I doubt it would be wise to limit selection to just one of these, since the community hasn't decided on One Blessed Editor (though emacs and vim probably make up the vast majority). Revision control * darcs * git * I think that covers most haskell projects. Possibly cvs, svn, hg, bz &amp; whatever else, just in case. irc clients * irssi * whatever other people use Preconfigure irc clients to automatically connect to #haskell on irc.freenode.net, with sane defaults for hiding irc cruft messages (#haskell is a busy place, so this is somewhat important). Browsers * All the usuals. Firefox &amp; Chrome, for sure. I can't think of any Haskell-specific plugins for that would be important here. Maybe have things like pentadactyl and reddit enhancement suite installed but turned off by default. Browser bookmarks * http://haskell.org * http://haskell.org/hoogle * http://reddit.com/r/haskell * http://stackoverflow.com/questions/tagged/haskell * http://patch-tag.com These, I think, are the essentials. Maybe throw in links to RWH and LYAH as well. Cabal packages * um... Here I'm not quite sure. There are *tons* of executables here that can be useful to Haskellers. Anything that is for linting Haskell code, generating graphs, monitoring Haskell processes, turning Haskell source files into html, should probably be included. And then there's libraries. It would be nice if the spin came with a bundle of commonly used libraries: a sort of haskell platform plus, if you will. The pain point will be trying to get versions of all these libraries that work together nicely. Popular web frameworks (Yesod, Snap, HappStack) should be included. I'm not sure if there are published metrics for # of downloads, but pretty much anything that is relatively stable and popular should ideally be included by defualt. The more you add, though, the more of a pain it will be to keep everything updated and happily working together without any dependency issues. The fewer you add, the more annoying it will be for users that have to cabal install their favorite libraries every time they spin up. Additionally, light support for Agda, Idris, and even Coq would probably be useful. Xmonad is also a plus, though perhaps don't enable this by default. :)
But `blah g = f . g . h` is SHORTER than `blah = (f .) . (. h)` Why would you write it the second way? Even the lambda is only two characters longer: `\g -&gt; f . g . h` `(f .) . (. h)`
&gt;I think the issue is that f is inlined, not map. This actually makes sense. Thanks.
As far as I know it's unknown if it's possible to run any pure Prolog (backtracking, cuts and logic variables) in pure Haskell (laziness) with a constant overhead, nor the other way around (and people have tried to do this, apparently). The implemenation probably would have to just implement whole Prolog in ST.
I feel the same. It's not matter of taste for me, it's just *way* harder to read.
Unfortunately not. You could create a modified version of it, though. There are a few projects to do syntactic extension of Haskell: http://hackage.haskell.org/package/BNFC-meta/ http://www.cse.chalmers.se/~patrikj/papers/MetaDSL_preprint_20110325.pdf And the awesome looking, unreleased SugarHaskell (paper draft) http://www.informatik.uni-marburg.de/~seba/projects/sugarj/sugarhaskell.pdf 
Yeah, it's messy to get to a database of modules/functions. Hoogle would be nice, but the built-in nature of using GHC interface files seems hard to beat for general use. Also, how would you disambiguate which module to import when the same function exists in many places? A few obvious ideas would be: - Cycle through possibilities - Keep a database/log of previous selections and prioritize by frequency 
I only defined nodes to emphasize how it could be applied to a general graph - i.e. nodes is used both in adjacent and as the parmeter to cliques. Let me know how the foldr approach works out - I'm not sure how efficient the use of ++ is here. My intent was model cliques as an infinite stream, and new elements are added by iterating over what's been generated up to that point. For example, that's how I see O'Neill's prime number sieve [1] operates, but in that case it's easy to determine the stopping point - i.e. you stop reconsuming the sieve when you exceed the sqrt of the number you are considering adding. In the case of cliques, I could see how to do this by keeping track of how long the cliques list is, and then addnode would know how much of the initial segment of cliques it needs to traverse to generate more elements. Seems kind of kludgey though. [1 - http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf](http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf) 
I'm not sure I understand: you can perfectly write completely strict programs in Haskell (with a bit of syntactical inconvenience, but if you have an automated translator you won't feel any pain). Are you ruling that out by saying that you want "pure" Haskell (is `seq` non-pure?). It's probably not terribly difficult to implement a prolog-like search monad; implementing logic variables and hoare clause resolution would be more work, but not necessarily needed to implement programs in practice (the OP link has examples of just using the search). There are efficiency questions, but I'm not sure that would be much of a problem. I remember there was a prolog implementation that worked by [compiling programs into OCaml](https://groups.google.com/forum/?fromgroups#!msg/comp.lang.prolog/N8fRQqcgRJY/uGX4tRs2xCgJ) (and using the OCaml native compiler and GC from there), that was competitive wrt. native optimized prolog implementaions. 
Hello! I'm the author of the please use 64 bit directions for Cairo gtk2hs etc on the Mac :-) 1) the recommendations for 32 bit date to when 64 bit os x support was first added, and it looks like the default suggestions on some tutorials etc are a bit antiquated owing to still having those 32 bit recs. 2) open a new tab in terminal and type "which ghc". If its usr/bin, then you should look at what other versions you had installed in that frameworks directory. If its user local, you have a 32 install from Mac homebrew, which would be odd because they fixed the default to 64 bit a. Few months ago 3) depending on the info from the above step, I may have more action steps, but in the mean time dl the latest Haskell platform release for 64 bit Mac so once I finish helping you life gets good again :-) Cheers
To uninstall HP, run /Library/Haskell/bin/uninstall-hs Follow the output from running that carefully: Running it this way just tells you what it could uninstall, and tells you what options to add to actually uninstall. Also, you might want to bookmark the HP start page that is installed on Mac OS X: file://localhost/Library/Haskell/doc/start.html - That has the uninstall info as well as links to all the other doc. The HP page doesn't recommend 32-bit *always*, just preferred over 64-bit. The [Gtk2Hs/Mac](http://www.haskell.org/haskellwiki/Gtk2Hs/Mac) page on the wiki seems to imply success with the 64-bit platform - but I have no idea if that is a restriction of homebrew, or just what the author of that page tested. Installing Cairo and gtk2hs on Mac has always been a pain - in part because GTK itself is hard to install. It seems to have gotten better recently, but still no walk in the park. And yes, I agree that getting GTK &amp; Cairo up on Mac really should be "unpack the tarball" simple.
What I'm talking about is merely a matter of performance. You want O(f(n)) Prolog programs as O(f(n)) programs in Haskell. It's not easy to say if it's possible. Either way the most you are fighting for is a log(f(n)) factor.
Twice the memory? Perhaps if your program is made up of nothing but pointers :). If you run on 64 bit and have ints that fit in 32 bit and memory shortage, you can always use Data.Int.Int32.
Int32 takes two 64-bit words on a 64-bit machine, or two 32-bit words on a 32-bit machine. Even if it gets unboxed, it still takes a machine word (32 or 64 bits). Haskell programs really do use approximately double the memory on 64 bits. The exception is unboxed arrays of fixed-size types (e.g. UArray Int32 and ByteString).
I spent a few hours trying to set up haskell-mode and ghc-mod, but it was pretty flaky. Random error messages that took me too long to figure out, and/or incorrect behavior (e.g: flymake disabling still distracts me with incorrect errors marked). I gave up on most of the advanced emacs integration features.
I can see two immediately obvious problems. First is that you've got all your code parameterised over `MonadRandom`. Many of your functions look too big for the compiler to automatically inline them, which means it will have to do dictionary-based dispatch to determine which version of a parameterised function to use. This can easily slow your code down by an order of magnitude. Second is that you've got all your code parameterised over `MonadRandom`. Wait, what, I'm saying the same thing again!? No. This time, I'll note that `Control.Monad.Random` seems to use `System.Random` to generate random numbers. The performance of `System.Random` is famously awful; alternative PRNGs can easily be [100 times faster](http://www.serpentine.com/blog/2009/09/19/a-new-pseudo-random-number-generator-for-haskell/). The "statistics" PRNG from that article now lives in a package of its own, [mwc-random](http://hackage.haskell.org/package/mwc-random). (Fair warning: if you're using mwc-random with GHC 7.2 or 7.4, you'll need to tweak your compilation options due to [a performance regression in GHC](http://hackage.haskell.org/trac/ghc/ticket/6166).) So there! In some Platonic reality, I have made your program 1000 times faster using only the power of my mind! Of course, the real speedup will be much smaller, but a 5-20 times improvement would not astonish me.
Parentheses aren't mandatory for that lens application. Use (((height ^= 180) . (width ^= 260)) window) Interestingly, you didn't even realize this was a possibility, despite having strong opinions about its readability.
This program links to a GPL library (pandoc) but claims to be MIT itself.
do-notation is best used when you need to locally save "state" in a monad action. For example, do t &lt;- getTime doAction t' &lt;- getTime return $ t' - t You can do this with lambdas and the monad combinators, but it turns into lumpy combinator soup. On the other hand, if all you are doing is naming values and applying a function to them, you are better off with the applicative interface: do x &lt;- getX y &lt;- getY z &lt;- getZ return $ f x y z versus f &lt;$&gt; getX &lt;*&gt; getY &lt;*&gt; getZ Why is the latter preferred by experienced Haskell programmers? Two main reasons: the applicative interface is less constraining than the monadic interface, meaning the same code works with more types. Second: function application, and computer code in general, "joins" values into a new value. The combinators you use represent the plumbing that puts them together. It is easier to mentally parse (that is, *ignore*) simpler combinators and focus on the things being put together. From this perspective, the monadic interface has the same problems a language like C or Ruby does -- you need to read the entire block/action definition/program to make sure your invariants are preserved. It is an unnecessary distraction. That said, sometimes the monadic interface *is* necessary.
I can do this already with: pandoc -f markdown+lhs What is the advantage of pandoc-unlit? 
Aaaah! Thanks. I should read things more carefully. It's probably why it's taking me so much effort to get this done.
Your factory/pipe-works analogy is a good start, but it is inadequately incomplete. People mean two distinct things by "OO" -- they mean the system which uses specific operators (-&gt; or .) to dispatch flow control, using classes or prototypes and the like. And they mean this *plus* a procedural language for sequencing operations. You appear to be using the latter (which is fine, but let's get it straight). But let's focus on the former, for a moment. Haskell record syntax embodies a prototype based OO dispatch system right out of the box. Even if the computer is "really" copying data, there is no point in treating: foo { x = "x" } as anything but changing foo's x field to "x" in the context in which this expression is used. If we use lenses (basically, composable record syntax), we can do things which are conceptually equivalent to: (( foo { x = "x"} ) { y = "y" } ) { z = "z" } to recover a sequencing language based on the (-&gt; a) monad. (Don't worry about the (-&gt; a) monad if you don't understand what this means yet) This would be like using a bunch of chained setter methods in an OO language to *generate* (by any means, copying or changing) a value with the appropriate fields set in the appropriate way. The "real" power of Haskell, and other typed functional languages, is that the "rules" allow us to use "equational reasoning" and other kinds of algebraic transformations which are guaranteed to preserve our invariants, once they are suitable encoded in the type system and value system. Your plumbing analogy is getting to this point, but is incomplete. In particular, we can plumb "pipe-works" using higher order types and functions -- for example, the Functor, Applicative, and Monad type classes are made specifically to encode different types of plumbing. In fact, the Monad type class encodes plumbing "your" *factories* together. By pushing flow control into these type classes, and tagging values with instances of the classes, we can statically ensure that the semantics of our plumbing always work, assuming the flow control implementation is correct. That is an important assumption, but we move the complexity of ensuring correct flow control into a single place, as opposed to having it spread out over the entire project.
Ok.. I'm following this: https://gist.github.com/2420144, and this time I started from scratch and followed it exactly. It was fine until step 4, where I get this: &gt; cabal install gtk Resolving dependencies... [1 of 2] Compiling SetupWrapper ( /var/folders/7s/6fn_d5px0hs2204xfztw88v40000gp/T/cairo-0.12.3.1-20203/cairo-0.12.3.1/SetupWrapper.hs, /var/folders/7s/6fn_d5px0hs2204xfztw88v40000gp/T/cairo-0.12.3.1-20203/cairo-0.12.3.1/dist/setup/SetupWrapper.o ) [2 of 2] Compiling Main ( /var/folders/7s/6fn_d5px0hs2204xfztw88v40000gp/T/cairo-0.12.3.1-20203/cairo-0.12.3.1/Setup.hs, /var/folders/7s/6fn_d5px0hs2204xfztw88v40000gp/T/cairo-0.12.3.1-20203/cairo-0.12.3.1/dist/setup/Main.o ) Linking /var/folders/7s/6fn_d5px0hs2204xfztw88v40000gp/T/cairo-0.12.3.1-20203/cairo-0.12.3.1/dist/setup/setup ... [1 of 2] Compiling Gtk2HsSetup ( Gtk2HsSetup.hs, dist/setup-wrapper/Gtk2HsSetup.o ) [2 of 2] Compiling Main ( SetupMain.hs, dist/setup-wrapper/Main.o ) Linking dist/setup-wrapper/setup ... Configuring cairo-0.12.3.1... setup: Package libpng was not found in the pkg-config search path. Perhaps you should add the directory containing `libpng.pc' to the PKG_CONFIG_PATH environment variable Package 'libpng', required by 'cairo', not found cabal: Error: some packages failed to install: cairo-0.12.3.1 failed during the configure step. The exception was: ExitFailure 1 gtk-0.12.3.1 depends on cairo-0.12.3.1 which failed to install. pango-0.12.3 depends on cairo-0.12.3.1 which failed to install. which I was able to get past with: export PKG_CONFIG_PATH=/usr/X11/lib/pkgconfig And it all seems to work, except.. the suggested test that it all works, fails with: &gt;./plott plott(29684) malloc: *** error for object 0x4024000000000000: pointer being freed was not allocated *** set a breakpoint in malloc_error_break to debug Abort trap: 6 Not sure where to start with this one - or even if it's going to be a problem when I do anything else. It's one for tomorrow, but at least I feel like I've got a lot further than yesterday, and the steps weren't so complicated. 
Strict fields have different sematics tough (you can't put undefined stuff into them) so the compiler can't do it for you without asking first.
I want to start with a question: if strict laws were defined for each of the typeclasses in CP, how would that affect your analysis of it? As far as your points: &gt; Wrong. In every library I release I specifically use the type system to prevent incorrect usage of it and the vast majority of standard libraries exercise the same level of caution. That's kind of the point of using Haskell in the first place: type safety. Explain to me how we are breaking type safety. There are type-safe ways to abuse a library. &gt; Correct. Haskell's module system was designed for general-purpose namespacing without laws, not type-classes. So any library which has incidental benefits from features that were not intended is bad by your logic. &gt; I think you are taking this a little bit too personally. No, I just dont like seeing someone elses work shot down with such vitriol when its only a version 0.2, early proof of concept. &gt; it's generally agreed upon that they don't work well. Please elaborate. Which extensions used in CP are "a bad thing"? For the record, I'm not in love with Fun Deps, but I didn't know that they were evil. &gt; I haven't seen that feedback in this thread. here it is: &gt; Type-classes are not first class. I'd prefer to see something like first class modules used to solve this problem. &gt; Here's your constructive feedback: continue to use the module system which works and gets stuff done and requires no community buy-in. I judge all namespacing proposals by how they compare to the current best-in-class solution, which is the module system, and the Classy Prelude doesn't measure up to it. This doesn't require community buy-in. Anyone can use it as a drop in replacement with just a NoImplicitPrelude. Lastly, its not a namespacing proposal! Its about removing unnecessary concrete types from the Prelude. Honestly, if you think this is so terrible, you should not use any of the numeric/math typeclasses from the prelude and use only functions that take concrete types. 
Thanks for the tips, Bryan! Simply replacing MonadRandom for mwc-random's gen as an additional function parameter already brought down the running time for 100,000 trials from ~8s to ~1s in GHC 7.4 without the additional compilation options (-fno-state-hack). Now it's time to take the first advice and make functions shorter. My first attempt at the selection function was split into many other functions, defined in terms of foldM1, but it made the program slower. I think it's time to learn how to read some core too.
In the first version, ~75% of the time and allocation were spent in uctValue, which simply does math, but I thought some evaluation was being deferred to it and didn't realize it was the RNG's fault. The biggest cost centre now is still in the RNG (System.Random.MWC.uniform2, ~60%), but it's much faster. I haven't measured Java's default RNG performance, but if it's comparable to mwc-random, then zipping through the tree is almost as fast as using the mutable tree in Java ;-)
See page 26 of: http://www.scribd.com/doc/19637022/Haskell-Arrays-Accelerated-with-GPUs and some info here: http://parfunk.blogspot.com/2012/05/how-to-write-hybrid-cpugpu-programs.html 
&gt; I want to start with a question: if strict laws were defined for each of the typeclasses in CP, how would that affect your analysis of it? So the point of laws is to equationally reason about code, so the actual criterion is whether or not I can more easily equationally reason about code I write. An example of a simple law for a type-classed `length` and `take` would be: length (take n xs) = n ... except that's not true for common definitions of take, but let's assume that it is true just for the sake of discussion. If I saw that in my code and the library writer told me it's true for all instances of the type-classed `length` and `take` functions, then I could safely rewrite my code and replace the left-hand side with the right-hand side with no change to the behavior of my program. That's a pretty trivial example, but the reason laws are so important is that when we define type-classes we essentially give up type-safety because now there is no restriction on what can implement that type-class. This is why we insist that when we do make that sacrifice that we at least require some general properties that instances must abide by to qualify as legitimate instances. The laws act as a substitute for type safety. So the concrete answer to your question is that a good law-based implementation should: * Be rigorous (i.e. be stated in terms of actual equations that are valid code substitutions) * Capture the semantic purpose of that type-class in a meaningful way, so that I feel confident that all instances "do what I expect" no matter which one I use. If Classy Prelude did that, it would be a much stronger library. However, it seems unlikely that this would happen because generally the only kinds of laws that ever successfully achieve the above criteria are theoretically-grounded ones. We don't advocate a theoretical approach because we love theory, but because historically it's the only approach to defining rigorous and intuitive laws that has ever succeeded. The Classy Prelude type-classes don't lend themselves to nice equational reasoning. &gt; Explain to me how we are breaking type safety. There are type-safe ways to abuse a library. All "lawless" type-classes essentially break type-safety because there is no restriction on what can be made an instance of that type-class. For example, given a type-class like: class CanTake xs where take :: Int -&gt; xs -&gt; xs I could just write: instance CanTake (Maybe a) where take _ _ = Nothing ... which would be wrong by most definitions of `take`, and if I accidentally used it on a value of type `Maybe a`, it would type-check and do the wrong thing. Now obviously nothing prevents a library writer from violating type-class laws, but usually if this happens the community will just stop using that library because we don't like depending on buggy code. &gt; Please elaborate. Which extensions used in CP are "a bad thing"? For the record, I'm not in love with Fun Deps, but I didn't know that they were evil. Multi-parameter type-classes and functional dependencies, and perhaps equality constraints, too, but I haven't had time to try them out. Also, typically type-class-based approaches eventually run into `UndecidableInstances` and `OverlappingInstances`, which are really ugly. &gt; This doesn't require community buy-in. Anyone can use it as a drop in replacement with just a NoImplicitPrelude. Lastly, its not a namespacing proposal! Its about removing unnecessary concrete types from the Prelude. Honestly, if you think this is so terrible, you should not use any of the numeric/math typeclasses from the prelude and use only functions that take concrete types. It does require buy-in because it creates friction between libraries that build on different preludes as they will generally not be compatible and forcing them to pick one ecosystem or the other. The way I see it, there is no compelling reason to switch to an alternate Prelude and fragment the community unless it is clearly superior and the classy prelude in its current form is actually a step backward. I can't prove that it won't get better, but I won't hold my breath.
Wait! The only reason I mentioned the inliner was because you were using a typeclassed monad instead of IO. In that situation, if you had small functions, there was a (quite small) chance that GHC could inline them and hence specialize them to the IO monad. Now that you are using IO, that motivation disappears. It's still _possible_ that you might want to manually inline a couple of functions for performance reasons, but don't do anything before you use the profiler.
To my modestly-trained eyes, the presence of non-strict foldl in Data.Tree.Zipper gives me pause, though I can't say if it is actually causing any trouble.
that "space" function in Fig. 2 on pg. 6 doesn't seem to implement BFS at all as the author claims, more like iterative deepening DFS: space s = step ++ expand step where step = [ ([m],t) | (m,t) &lt;- trans s ] expand ss = [ (ms++ns,t) | (ms,s) &lt;- ss, (ns,t) &lt;- space s ] space s = [ ([m],t) | (m,t) &lt;- trans s ] ++ [ (ms++ns,t) | (m,t) &lt;- trans s, let (ms,s) = ([m],t), (ns,t) &lt;- space s ] space s = [ ([m],s2) | (m,s2) &lt;- trans s ] ++ [ (m:ns,t) | (m,s2) &lt;- trans s, (ns,t) &lt;- space s2 ] Imagining search space with binary bifurcations, for one level this generates "[a],[b]". For two levels it generates "[a],[b], [aa],[ab],[ba],[bb]". For three levels - "[a],[b], [aa],[ab],[aaa],[aab],[aba],[abb], [ba],[bb],[baa],[bab],[bba],[bbb]".
I guess it was probably more OCaml than Agda.
I just installed it from debian repositories and it worked without problems. But manual installation could be daunting I agree
To be able to expose the constituent parsers in a way that you don't have to worry about which extension you enable first, you really want something like what elkhound/elsa gives you for c++, where they parse it as a context-free grammar, and then do resolution of the remaining ambiguities afterwards.
I agree with a number of Michael's points here. In particular, I wasn't particularly convinced by "I'd rather solve this problem with first class modules" as an answer. There are problems that are solved with type classes right now in Haskell, and rejecting those solutions based on a preference for some future feature that isn't designed yet is a poor idea. That said, I find myself ideologically more aligned with the approach of taking small steps and justifying them more solidly. Using a type class is syntactically pretty cheap in Haskell, but conceptually it adds a *lot* of complexity... even more so than, say, passing an explicit dictionary. I just find myself wanting to see that complexity justified with a really solid idea I can sink my teeth into that's proven itself to be important as an abstraction.
I think if Michael in original post explicitly stated that no yesod libraries will use new prelude, there would have been much less outcry and fears. There are thousands weird libraries on hackage. No one raises a fuss. People are only concerned when it affects them. 
I don't think Micheal is trying to "rewrite the core of our language". Rather, as I understand it the classy prelude is meant as a convenience for writing some kinds of code. And you are free to not use it if you don't want to. &gt; However, I will fill in the last gap, assuming that he is asking in good faith. Functional dependencies are very brittle. I think *very* brittle is an overstatement. Functional dependencies work well enough in most cases, as evidenced by the many users of 'mtl' for example. &gt; The Classy prelude is maintained by an author who has historically shown an utter disregard for the correctness of laws or the importance of equational reasoning, qualities I do not appreciate in somebody trying to rewrite the core of our language. Throwing ad-hominem attacks around is not going to help argue your case. Micheal's is a proposal to solve a real issue with the current state of Haskell. Whether or not the author has previously written a library that do not conform to a particular theoretical model that you like does not invalidate that fact. Giving a more full quote: &gt; &gt; provide name overloading, thereby reducing the number of import statements made and decreasing the syntactic overhead of qualified imports. ... In today's Haskell, the only approach possible to achieve this goal is typeclasses &gt; baseless_assertion.jpg I don't think your use of baseless-assertion is warranted here. Michael is right in that typeclasses provide the only means of name overloading. Using modules as you suggest would incur the overhead of many import statements and qualified names. Note that I am not saying that either approach is better, just that the assertion is not baseless.
Oh, *snap*. (Pun intended)
&gt; Throwing ad-hominem attacks around is not going to help argue your case. &gt; Whether or not the author has previously written a library that do not conform to a particular theoretical model that you like does not invalidate that fact. I disagree about dismissing this as an ad hominem argument or particularly with willfully ignoring the relevance of this point by pretending that it's about one specific past instance. There's clearly a divide in the Haskell community right now on some very fundamental, very real issues of design philosophy, and Michael is pretty much the mascot of one side of that divide. It would be silly to completely discount that fact when looking at the state of libraries. It's not as if the whole of the statement was just that Gabriel doesn't like conduit not following the category laws. In fact, this package is pretty much an expression of the hypothetical extreme of introducing weak abstractions with little concern for their behavior or properties.
&gt;which I will find once I can use my desktop instead of my phone. You typed ALL THAT on the phone ?! I need to look into those new awesome phones.
&gt; Micheal's is a proposal to solve a real issue with the current state of Haskell. Whether or not the author has previously written a library that do not conform to a particular theoretical model that you like does not invalidate that fact. As a layman, I am pretty sure Tekmo and others think not conforming to some mathematical model causes problems in the long run. Both sides are concerned with what they see as a problem first and aesthetics second. It is not just a matter of like or dislike on either side.
I have a long bus commute in the morning.
I'm sorry, you are right. My post was very aggressive and distasteful. I was just angry because I went to great lengths in my first comment on the previous post to keep it civil and then Michael just trashes my argument as being baseless. However, it still doesn't excuse my behavior. I just want to make clear, though, that you mischaracterize my overall position on type safety. I don't mind when people write bad libraries that are unsound, particularly if is for their own personal use, however I do mind when they try to make a case that the community should build on their library and then proceed to categorically reject every criticism of their library as being a flame or unconstructive.
&gt; Except that he hasn't demonstrated a real issue he is solving, other than sometimes removing two characters from the prefixes of functions. Using qualified imports everywhere can get quite annoying at times. Especially when you compare haskell to many other languages that do not have this problem. In most OO languages for example name resolution is local to a class, two classes can happily define a method with the same name. You can write `myMap.length + mySet.length` without problems, and without having to specify which `length` you mean in each case. I personally think that a nice solution would be to let the compiler figure out this overloading in a more local manner, without type classes. Something like: given an declaration with overloaded names imported from different modules, the names will be resolved if there is exactly one resolution that makes the declaration well-typed. This is weaker than type-classes, because the overloading is limited to a single declaration. I am also not sure how well it would work in practice, since obviously it is not implemented yet, although Ghc does provide something similar for records. Having two ways of overloading is also confusing. It is just an idea so far.
Regarding [slide 9](http://johantibell.com/files/haskell-performance-patterns.html#%289%29): Why can't we use unboxed tuples instead? Wouldn't it be even better? I tried: mean :: [Double] -&gt; Double mean xs = case foldl' (\ (# !s, !n #) x -&gt; (# s+x, n+1 #) ) (# 0, 0 #) xs of (# s, n #) -&gt; s/n But it won't compile: Couldn't match kind `*' against `(#)' Kind incompatibility when matching types: a0 :: * (# t0, t1 #) :: (#) In the pattern: (# s, n #) In the first argument of foldl', namely `(\ (# s, n #) x -&gt; (# s + x, n + 1 #))' I don't get the error message... **EDIT:** Okay, I got it. Unboxed tuples don't have the * kind that function parameters should have (and my lambda takes a unboxed tuple). But (-&gt;) has kind * -&gt; * -&gt; *, so why does it allow a function to return an unboxed tuple ? It kind should be * -&gt; ? -&gt; *, right? There should be "strict tuples" too, so that we wouldn't have to make a custom type (data SP a = SP !a !a), like: type SP a = (! a, a !)
&gt;Your tone is always unpleasant to read, you come of as preachy and condescending, and seem to have a vendetta against Michael Are we reading the same post? I don't see any of that, despite the fact that Michael is using this subreddit as his personal soapbox to throw logical fallacies at anyone who dares to suggest his vision of haskell isn't as good as actual haskell. Specifically dismissing Tekmo's arguments as baseless, rather than actually support his side of the argument.
OCaml approaches this problem with [local opens](http://caml.inria.fr/pub/docs/manual-ocaml/manual021.html#toc77), an expression-level construct that brings names from a module into scope (possibly shadowing existing names). There's a lightweight version and a more explicit one: Hashtable.(lookup x (insert x y z)) let open Hashtable in lookup x (insert x y z) Compared to classy prelude, this has the advantage of not complicating the types at all. The inferred types are the same as the longhand version, and there's no need to write type signatures to avoid inscrutable (or ambiguous) levels of polymorphism. Nor does it require any fancy module-level machinery. I'm no TH expert, but I very much suspect you could implement a declaration splice or a quasiquoter to have this in Haskell right now: foo x y z = [open|Hashtable| lookup x $ insert x y z |] foo x y z = lookup x $ insert x y z where $(open ''Hashtable) I'll be the first to admit that neither of these is as nice as OCaml's built-in syntax, but I do think that local opens (local dequalifications?) in some form are a better tradeoff between term-level repetition and type-level complication than either the current state of affairs or the classy prelude. 
As a member of the crowd, my reaction was largely prompted by dislike of the API design and its inability to meet your goals. You try to make far too many of the signatures far too general, and then make instances that would overlap with any attempted instantiation. I was going to leave my response as it stands, but you seem to have taken umbrage at the general brevity of peoples' responses, so I will respond to tone in kind. Skimming the list of classes, lets take a class from the classy prelude to hold up as an example: class CanMapM_ f i o m where mapM_ :: (i -&gt; m o) -&gt; f is nigh unusable. You'll get no type inference on the 'f' component of the result, there are no fundeps to guide inference whatsoever, and the only instance you have is CanMapM_Func ci i o m =&gt; CanMapM_ (ci -&gt; m ()) i o m which due to the lack of fundeps precludes *any* other instance from being defined without overlap, since the 'i' 'o' and 'm' parameters are used unquantified at the top level. The only way to make any other instances is to vary the outermost constructor on the first parameter and use ~'s on the remaining parameters. Of course this is exactly why we have fundeps! This is simple, actionable, and over the top. Now, a restricted version of Foldable that deals with monomorphic contains is probably a reasonable class, and lo and behold such a thing can (and has) been defined in http://hackage.haskell.org/packages/archive/reducers/0.1.8/doc/html/Data-Generator-Combinators.html#v:mapM_ mapM_ :: (Generator c, Monad m) =&gt; (Elem c -&gt; m b) -&gt; c -&gt; m () gives type inference enough to work with. API design is more than just throwing an MPTC and a million type annotations at every probem until you can overload the same name for use in every scenario. In general any time I see a class with 4 type arguments, and no fundeps, it stands out to me as a bad design that is going to be impossible to use. * CanMap has this problem * CanMapFunc has this problem * CanConcatMap has this problem * CanFilter has this problem * CanFilterFunc is actually pretty much okay * as is CanLength, but it could probably pick up an Integral constraint on the size. * CanMapM, CanMapMFunc, CanMapM_ and CanMapM_Func, CanFold all suffer the problem. * CanInsert makes me actually physically ill at the number of type errors and annotations it will create. * CanReadFile and CanWriteFile have the same awesomely unhelpful type errors as CanInsert to a somewhat lesser degree. * CanWriteFileFunc, CanStripPrefix are fine So in the end of a cursory inspection, I'm left with only a handful of type classes that are usable or even instantiable, or able to be called in practice, and to use them I have to litter my code with type signatures. I'm afraid I'm going to let my kneejerk reaction stand. Without fixing those issues, the lack of laws is largely a secondary fault. There is always a tension in making the types specific enough to give useful errors and to assist type inference and general enough to encompass more use cases. I don't dislike this library because it uses typeclasses to address namespacing. I dislike it because it uses them to address namespacing poorly. 
I have no way to test them, but there are some notes about gtk2hs installs [on the wiki](http://www.haskell.org/haskellwiki/Gtk2Hs/Mac).
I might have a couple of comments with respect to the foundness of the "OMG WTF IT HAZ NO PROOFS U BIG n00b!!" rant: First, what does actually prevent ClassyPrelude from coming with a set of QuickCheck properties? It's weaker than proofs in the Coq/Agda sense (if that's what you mean), but that would be awesome for new collections developers, who would already have their work done regarding the properties testing. And second, why don't you apply the same rant to, well... every typeclass available on Hackage? And in package 'base' (incidentally our already existing core) to start with? Because now, it appears you don't only condemn ClassyPrelude but the whole typeclass system. (And you would not do that without a complete blog post on the subject due to the ghastly specter of the baseless-assertion which as a Sword of Damocles now impends over you, would you?) To speak frankly, I've always been quite uncomfortable, after giving a screed about the might and wonders of what Haskell has to propose wrt code generalization to the good-but-yet-to-be-enlightened-and-raptured-to-higher-heavens® common Java folk, at the thought of telling them "well... yeah... to get the size of a List it's 'length', but for a Set or a Map it's not the same function, it's 'size'. Oh, and also we don't have a standard collection generalization [1]". [1] Okay we have Foldable/Traversable but it's weaker in the sense it generalizes less. (inb4 baseless-assertion.jpg)
&gt; Except that he hasn't demonstrated a real issue he is solving, other than sometimes removing two characters from the prefixes of functions. I can't speak for Michael, but this is a much bigger deal than saving two characters. When it's easier to use the wrong data structure (a list of chars) instead of the right one (Data.Text or whatever), people _do_. Lists are over-privileged in the syntax, and that has real negative consequences in real programming. It may not be a programming language theorem that's going to have rigid laws and proofs, but as a software engineering principle, "it should be easier to do the right thing than the wrong thing" (or at least _as_ easy) is very solid, and has disproportionately strong effects, _especially_ if you are trying to cost it out as "it's just two characters". It's much more expensive than that in cognitive overhead. It's also much more expensive than that when you realize "Oh crap, two weeks ago I spec'ed this as a list and now I have to change it to a ByteString and it's going to be hours sitting there and just rewriting all the ++s to something else... eh, screw it, I'll leave it as a list." I sort of understand your distaste, but against that I find myself balancing the fact that I consider the _current_ Prelude an absolute horror show. It is neither theoretically sound (partial functions ho! and that's just one example, I also dislike the map/fmap/mapM/etc garbage brought on by an excessively specific map &amp; friends), nor practically sound (do you like something other than lists? Too bad!). It teaches beginners a number of things that they will subsequently have to unlearn. How many times have we seen the "I thought Haskell was fast but I benchmarked loading some files and looking for matching lines and it was dog slow" because they used the functions that returns Strings? I don't know that this is it but at the moment I'll take something that's an improvement even if it isn't perfect. I'm also not convinced that you've fingered the correct location of the problems. I think a lot of the issues you've raised with the Classy Prelude are actually located A: in the underlying libraries it is trying to harmonize, which have little inconsistencies for no good reason and B: the problem space itself. To the extent that Classy is just a skin pulled over the common libraries, I actually agree that it tends to expose the already-existing problems, but it did not necessarily create them.
I have the same qualms about the Prelude as you do, but I don't think type-classing everything is the right solution. I think the right way forward is to pick small steps that are clear improvements. I think almost every point you raised doesn't require a type-class, with the exception of the `++`, which will be fixed once `Data.Monoid` has `&lt;+&gt;`. For things that don't make sense to type-class, just have the important libraries standardize on a naming convention for those functions so that switching between them is as simple as just changing one import statement.
Herein lies your problem. You're basically saying that the only type of comments that you accept as constructive are those that promote the existence of this library. But it just so happens that the feedback indicates that this library is not a positive thing for Haskell in general, and that's a perfectly valid and constructive opinion to have. There are six top-level comments criticizing CP and all but one of them outlines good reasons for their criticism. The remaining one, by sclv, is stated as an opinion. And if you knew him, you'd know that he's been around Haskell for a long time and his opinions are intelligent and worth taking note of. It looks like you're getting too attached to an idea. I'd suggest taking a step back and considering the possibility that the detractors aren't completely stupid or trolling.
He never specifically targetted *Tekmo's* arguments as baseless, other than mentioning the "brittle extensions" thing. [edit] to clarify: Tekmo himself complained that Michael did not even mention his real arguments. The brittle extensions thing was basically a comment in passing compared to the brunt of Tekmo's real objections.
Uh oh... instance Num a =&gt; Num [a] where (+) = zipWith (+) (*) = zipWith (*) fromInteger a = repeat $ fromInteger a main = do print $ 3 * [1,2,3] print $ 5 * [[1,2,3],[4,5,6]] print $ [10,10,10] * [[1,2,3],[4,5,6]] print $ [[10,10,10],[20,20,20]] * [[1,2,3],[4,5,6]] ;-)
Hm, what's the church encoding of Mu a? newtype Mu a = Mu (Mu a -&gt; a) The Scott encoding of this type is type Mu a = forall r. ((Mu a -&gt; a) -&gt; r) -&gt; r Inlining Mu a we get type Mu a = forall r. ((...( ... -&gt; a) -&gt; a) -&gt; a)... -&gt; a) -&gt; r) -&gt; r System F without recursion is strongly normalizing (every program terminates), so therefore `fix id` can't be implemented in System F, so therefore `fix` can't be implemented in System F.
At least it's principled :-) : http://hackage.haskell.org/packages/archive/dimensional/0.10.2/doc/html/Numeric-Units-Dimensional.html
I've `brew uninstall`ed everything and then gone over the steps again, and it STILL is launching X11 when I run the plott example. Any ideas how to get rid of that?
I really don't think we should be making decisions around what Java programmers will like. In order to like Haskell a deep appreciation of the problems in computer science and how Haskell addressees them is usually required. There are lots of good reasons to use haskell i think very well outlined by Tim Sweeney a few years ago.
&gt; Something like: given an declaration with overloaded names imported from different modules, the names will be resolved if there is exactly one resolution that makes the declaration well-typed. So, TDNR?
&gt; once Data.Monoid has &lt;+&gt; Did you mean `&lt;&gt;` (which is already in base 4.5) or is `&lt;+&gt;` something else?
To be fair, making decisions on what programmers would like would be a nice change from making decisions based on what people who write exclusively in pseudo-code in LaTeX would like :) Seriously though, you do have a good point here.
I agree with that as well. Haskell does require considerable mind expansion when coming from a more traditional language like java. To me the problem is usually best solved with documentation. 
I do most of my dayjob in kdb+, which supports numerical operations very much like this.... and is actually quite productive.
There does seem to be a bit of friction and when it takes this form it does reflect poorly on the community. Just an observation from being around open source projects for a while. That being said I have enjoyed the discussions and I think that Haskell benefits from both pragmatic implementations as well as having other people think about their theoretical soundness. This is a discussion that many communities lack. The influence that pipes has had on conduit has been great, specifically the unified Pipe type. I agree with other people who don't believe that this impacts the core language and as such this library will live or die on its own merits. If it solves peoples problems then great. If there is a better way of solving them (First Class Modules I believe were mentioned) then people wanting a solution to this problem should help that work get prioritized. If this approach shows that having a solution is valuable, then building compiler support should be that much more valuable. I would like to be able to write code that processes collections that is not bound to the implementation. I have noticed that this has led to both copy and paste/replace as well as the slow development of my own typeclass that would be something like the List interface from .net or java (in spirit, not implementation). I didn't see classy-prelude as solving that but it did provide the primitives. I typically an not that interested in as fine grained an approach. Writing collection agnostic code is something that I wish haskell had better support for. As far as new point number three it seems to be a continuum and most people are probably spread out somewhere along it. From my loose following of the pipes/conduit debate there were valid points on both sides. There is absolutely value in formally reasoning about core APIs. There is also absolutely value in squeezing out perf and shipping things to customers. We are in a better place today for the discussions that have taken place regardless of who said what about who. I tend to fall somewhat toward seeking pragmatic solutions although I have a healthy respect for employing formal reasoning when the situation is right. I don't think this needs to be as much as an either/or and can be more of an and with people from across the spectrum providing feedback. Building production code is usually a series of tradeoffs. It would be great if these tradeoffs were documented, but they often need to be made. There seems to be ample room for all parties to make their contributions.
&gt; People are only concerned when it affects them. That's not a fair accusation. Of course it's like that with Yesod, the thousands of weird libraries don't affect anybody who doesn't want to be affected. They're there because people experiment, you can't be against that. It should be encouraged. It is, however, rational to be against weird experiments in big libraries which everybody uses, have been using for a long time, and sometimes are even forced to use because of work. The Apache Java libraries aren't popular because they have amazing, weird supercode in them. They're popular because they're extensive, have boring but readable code, and just work.
That looks quite nice. Perhaps it could also work for modules already imported with a name: import qualified Data.Text.Lazy as Text noop = Text.(unpack . pack)
&gt;it appears you don't only condemn ClassyPrelude but the whole typeclass system. I fail to see how our opinions on the things that have been in haskell for a long time have anything to do with discussing a NEW library. No matter how much we are frustrated with type classes inconsistencies in mtl/transformers/prelude etc. these are the parts we have to live with. But do we have to live with ClassyPrelude ? Not if we can help it. 
&gt; Michael, Greg, and the rest of the Yesod team have done more for bringing type-safety to web application programming than any other project, ever. I don't want to downplay their contributions, but, *baseless-assertion.jpg*. To counter with my own baseless assertion, I'll say that Yesod's selling point is the "rapid application development", not type safety, and that the toolkit providing the strongest static safety guarantees is likely Happstack.
I'm just about to start diving into J (waiting for a book to arrive). Are you subscribed to [/r/apljk](http://www.reddit.com/r/apljk)?
Yes, but again, it will be the case for every library providing typeclasses. How do you want to solve this issue. Avoid all typeclasses? Would it be easier to prove code once typeclasses are gone? (It's a genuine question)
Nope, type-safety is usually the first strength of Yesod mentioned: http://www.yesodweb.com/ 
Libraries typically provide type classes that either define something which has provable laws, or is only used to represent some API specific to that library.
Yeah, that's exactly what I was imagining. With that import line you can say `Text.length` but not `Data.Text.Lazy.length`, and `open` would work the same way. What I'm not sure about is if it should try prepending already-qualified names with new qualifiers, as in import qualified Data.Text.Lazy import qualified Data.Text.Strict silly = Data.Text.(Lazy.length, Strict.length) I'm leaning towards "shouldn't work" on this one, since Haskell modules don't really "contain" their hierarchical children the way they contain values. (Incidentally, I'm not sure giving yet another meaning to `.` is a good idea, though it seems unambiguous enough to use for examples.)
Just because they say so themselves doesn't make it so, although it does provide somewhat good safety, certainly better than e.g. Snap anyway. However: * The routing DSL isn't DRY which can lead to undetected typos * The javascript templates are just string interpolation with no syntax verification and in particular no AST-level splices * The templates in general use different syntaxes for different kinds of interpolation rather than types, which means a typo can pass undetected * Persistent provides a limited set of types that values must be serialized to and from, thus disallowing fine-grained expressions of your data model, and encourages use of database systems with weak types or ACID properties, especially in development which is where you want to catch bugs By comparison: * Happstack's preferred routing can be generated automatically from the types (completely DRY) or defined with powerful and expressive combinators (compositional and allows for reuse, thus DRY) * The preferred solution for javascript is jmacro which guarantees syntactically valid javascript at compile-time and allows interpolation at the AST-level * Both jmacro and hsx select interpolation by type so you can't mistype the interpolation and pass compilation * Using acid-state you can use arbitrary Haskell data structures to model your data exactly right while you also get really strong ACID guarantees, and you'll use the same system in development and production And this is just from the top of my head. You might argue that some of these points aren't directly about *type* safety but rather other kinds of safety, but that would be like calling a newtype wrapper around Text "type safe routing".
In fact, I tried uninstalling everything and starting again. When I get to the gtk-demo in step 1, it still launches in X11. I can see in my $PATH contains `/usr/X11/bin` but I don't know where this comes from.
It sounds like the intention was for functions using it to nail the type down. If I tell GHC it's an actual list, is it not smart enough to pick out the correct implementation statically? It's already accepted to be good practice to annotate nearly every function with the desired type signature, because typeclasses are hardly the only way to end up with too much polymorphism. Why not take advantage of the information that's basically already there?
I find the whole, 'we're the people actually getting shit done,' line quite amusing. Because the recurring theme of Haskell (and its family) is that those LaTeX paper/pseudocode writing academics keep inventing language features and abstractions that are better for getting shit done than the people just settling for something ad-hoc so that they can get shit done. It's the refrain you hear from Java and Ruby and whatever programmers. That Haskell is designed by academics and I need to get real shit done. Except now we're using Haskell, but can't come up with elegant abstractions we can reason about because we need to get real shit done. Well, no thanks. I need to get shit done, so I'm going to gravitate toward the elegant theoretical designs; that keeps working out better.
I completely envy you, in the best possible way :)
That's a perfectly legitimate question. I can't claim to have the answer to that, but I can give my own perspective on the question of alternate preludes. My general impression is that all of them have been half-baked and most people attempting them are not willing to take on the responsibility that entails because that is an absolutely huge responsibility. Equally important, most people feel uncomfortable giving a single person control over such a fundamental part of the community. Haskell up until now has been a very distributed ecosystem and that gives it a very democratic flavor that it has benefited from tremendously, and many people don't want that to change. I think the best migration path to fixing the Prelude is not going to be alternate preludes at all, but rather extensions that make it easy to extend the Prelude in backwards-compatible ways so that it can remain an incremental and democratic process. For example, I'm a big fan of proposals like class aliases or instance templates, which are very well thought out and are good candidates for achieving just that. However, there's another more important reason that the Prelude hasn't changed very much, which is that it hasn't really had to. The ecosystem has done remarkably well with this Prelude and I think there are only two really huge issues with it that people have clearly identified as needing fixing, the first being the String type and the second being the inflexibility of type-class hierarchies. To be honest, though, even those two problems have been fairly minor in the big picture and don't really prevent intermediate Haskell programmers from writing excellent code. Right now there is still the issue of maintaining an "oral tradition" for which libraries to use, but that issue could be solved by switching to Hackage 2.0 and having package ratings, for example. So I guess my long-winded answer to that is that perhaps the reason that people haven't fixed it is because the problem isn't that bad. If it was really as bad as some people make it out to be I expect we would have already fixed these things a long time ago.
The toolkit providing the strongest static safety guarantees is likely ~~Yesod~~ ~~Happstack~~ ~~[Ur/Web](http://www.impredicative.com/ur/)~~ [Ynot[pdf]](http://dash.harvard.edu/bitstream/handle/1/3980868/Wisnesky_Certified.pdf?sequence=2). `blend 0.5` *baseless-assertion.jpg* *trollface.jpg*
Thank you for summarising so well why that "get shit done" line of reasoning is so irksome to me. 
&gt; To be fair, making decisions on what programmers would like would be a nice change from making decisions based on what people who write exclusively in pseudo-code in LaTeX would like :) I come to this subreddit to get away from stupid stereotypes and false dichotomies between "programmers in the trenches" and "ivory-tower academics". Can we please refrain from bringing up stuff like that in this discussion? Some of the biggest critics of the classy prelude here write plenty of pure, functional code daily for large corporations, without sacrificing sound principles (using plenty of academic-sounding jargon in the process!).
Well, they actually get "shit" done...
Some comments on your code: You use the IO monad unnecessarily. Just remove all your return statements and convert those functions into pure functions. You can still use pure functions within the IO monad by using 'let y = f x' instead of 'y &lt;- f x'. Second, avoid partial functions that ignore possible errors. Haskell makes it really easy to deal with errors without any boilerplate once you learn about the Maybe/Either monad and monad transformers. If this interests you, I will expand on this more.
Please do! I've encountered plenty of Maybe/Either, but monad transformers are foreign to me. This is sort of my first foray into actual Haskell programming -- I've done plenty of playing around with it, but this is the first real project I've done. So, needless to say, any tips on how to clean this up would be great. Thanks for your feedback!
&gt; Can't GHC be flagged to use a similar approach? At least for definitions tagged some way? Are you looking for the [SPECIALIZE pragma](http://www.haskell.org/ghc/docs/latest/html/users_guide/pragmas.html#specialize-pragma)?
You're right. But the install/uninstall issue is actually improving, with Mac OS X in the lead. The maintainer of the Mac OS X installer for Haskell Platform, [MtnViewMark](http://www.reddit.com/user/MtnViewMark), has been doing a great job towards getting a good user experience with a native Mac OS X feel. In short - please [report this as a bug](http://trac.haskell.org/haskell-platform/newticket?summary=%3CProblem%3E&amp;description=%3CDescribe%20the%20problem%3E&amp;component=Platform).
Nope, I won't embrace the idea that practicality and an affinity for theory are at odds :) If someone insists on making a dichotomy out of it, I'm going to call them out on it.
One potential solution within Haskell could be [pattern wildcards and rank-2 types](http://hpaste.org/71626) (I don't think using them presents any problems). Made a [separate post here](http://www.reddit.com/r/haskell/comments/wp229/recordwildcards_for_localised_module_imports_what/) to discuss this style of solution.
SPECIALIZE is too specific. Something more like a TEMPLATE pragma (specialize this for all the types it is used in, and specialize everything else needed to do this). This becomes super-compilation very quickly, but with a bit of control over what gets specialized.
Is that a "Yes" as in "yes or no = yes" or as in "no or yes = yes"?
&gt; categorically reject every criticism &gt; categorically iseewhatyoudidthere.jpg
While it's true that `f` is being passed in each call, that's not really the concern here. Often, passing a single parameter is far cheaper than allocating a thunk to avoid such passing. Allocating a closure is only a benefit as you have more and more parameters being closed over. Rather, the problem is that, in virtue of being a HOF, the actual function `f` is not known until runtime. Thus, we cannot perform a static dispatch to a known address when invoking `f`. Instead, we must perform an indirect call--- which actually involves multiple indirections, two or three IIRC. Bouncing out to memory/cache takes forever, and it will ruin the instruction pipelining as well. This is also part of the reason why type-class polymorphic functions can be so slow when they are not used at statically known type instantiations. The worker/wrapper trick here serves to inline the entire recursive definition of the function (rather than merely one level of unrolling that recursion), which in turn means that when that (inline) definition is compiled, we will in fact know what function `f` is--- and therefore can compile the definition such that it makes a direct call. Moreover, since we know what `f` is, we may be able to inline `f` as well. Though, for a simple function like `map`, the direct call is surely more important.
Also there's [ghc-core](http://hackage.haskell.org/package/ghc-core) which cleans things up for you with regards to z-encoding and full-resolution of names.
Also, I haven't really looked at the code, but since you mention that you're doing `Double` math, the `-fexcess-precision` flag often speeds things up, and I didn't see it in the file. You should also check out the other compiler flags wrt floating point operations.
This has been suggested/discussed before but I can't remember what the downsides were. It seems quite nice.
Okay, so there exists some sort of threshold in the libraries' size/pervasion which -- when those libraries come without proofs -- makes some go from "meh" to "wholehearted wrath". (It being rightful or not is another debate) The problem is about drawing that line precisely. I think it's the burden of every API provider to prove that her/his concrete types hold some set of laws, it's not the work of the generalization. (As you rightfully said donri, CanPack laws are different depending on the concrete type) Because a better module system for instance won't be better regarding proofs than a typeclass approach a la ClassyPrelude. (At least I don't see how, if someone can enlighten me...) So why declaring war on typeclasses in that matter? Actually I think you're tearing yourselves apart while arguing onto different things. Proofs have nothing to do in that discussion (unless you can show), we're only talking about the efficiency of the generalization. And finally, the most important Michael's point that everyone seem to overlook: IT IS **NOT** INTENDED TO REPLACE THE PRELUDE and especially not for the education of new haskellers!!!. It's only an alternative for **experienced** haskellers. So arguments saying "something as central or core-ish as the Prelude should hold such and such" are missing the point.
Okay (using 7.4 here), I stand corrected. That's just not what :k (-&gt;) says.
Very clever; I'd forgotten about record field disambiguation. Making the library choose its own short name (i.e. record constructor) means you can no longer swap out a data structure by changing the import, and as you note in the comments it doesn't work for types or constructors, but this seems like a decent way to package up common libraries.
MLton (a whole-program optimizing compiler for SML) does this kind of aggressive monomorphization, with generally good results. Like C++ templates, the tradeoff is increased code size and a loss of separate compilation. This isn't really supercompilation, as it's driven by types rather than values.
No, you don't understand my point: I'm not saying we should design Haskell the way that would disturb Java people the less (Hell no!). I'm saying that we (and other FP, esp. Scala, users) often criticize Java for sucking hard when it comes to generalization, but Java provides generalization for collections, while Haskell (yet advocated to be so good at abstraction) does not! I completely agree that stopping on that statement is stupid, since it's not a universal truth than generalized collections are better (still, they are handy) and since Haskell has not an "all in the box" philosophy, but I just say we kind of have what we could call *the Haskell paradox.* After all, even if the containers and array packages are not part of the standard, they are part of the haskell-platform, which is now the advisable way to use the language. We do have the 'collections-api' package, that would be the spirit, but it's not enough that such a package exists, it also has to be part of the platform, advisable and widespread. Maybe I'm barking up the wrong tree, maybe such an idea conflicts directly with all or part of the Haskell spirit (bad abstraction is worse than no abstraction, concise first abstract later (typeclasses are good at it), this is not the correct haskellish way to do, etc.) and I don't realize why a package like 'collections-api' should never make it to the "mainstream Haskell", still that bugs me. As a good picture is better than a long speech, let's emphasize this paradox [the Reddit way](http://i.imgur.com/CxjNP.jpg).
That's a good point, as a library author you have problems choosing a name.
Technically, I believe you mean "yes or x = yes" or "x or yes = yes" seeing as it is possible that both are true.
&gt; Some people actually want to get some real work done, without having to spend a year writing a paper to show what they've done is completely sound I resent your implication that if I write a paper on soundness of some work, that it's not "real work". I further reject the false dichotomy between those that have to get "real work done" and those that write papers. This distinction doesn't hold water, and should be especially obvious in the Haskell community, where there are plenty of people who both get shit done and do so in an academic-y, theoretically sound way. &gt; Not everything needs to have a rigorous proof that it is correct. I would argue that if something is designed to replace the most commonly used part of the standard library, then it absolutely should.
 {-# LANGUAGE Rank2Types #-} could be replaced by the weaker {-# LANGUAGE PolymorphicComponents #-} which no one seems to know about or use.
Never heard of it! Thanks!
&gt;Michael, Greg, and the rest of the Yesod team have done more for bringing type-safety to web application programming than any other project, ever. It is pretty sad that people actually think this. You seriously do not realize that there have been much more type safe web frameworks pre-dating yesod by many years? Or that if you want to go by popularity, both lift and play have had far more impact than any haskell framework? &gt;In fact, this zealous application of type safe principles engenders hatred from the another side of the community for using too much Template Haskell. No, the zealous application of template haskell causes that. It has nothing to do with type safety at all, and the fact that Michael has convinced noobs that type safety requires template haskell is one of the reasons for the complaints.
This is approaching a [translation of ML modules into System F[pdf]](http://research.microsoft.com/en-us/um/people/crusso/papers/fingmodules.pdf) that turns signatures and the types they contain into datatypes and their type parameters. Users of a module respect its abstract types by being polymorphic in those type parameters. Whether the abstraction benefits of this style are worth it is unclear to me, though it's true that it becomes more appealing if you're already packaging modules in datatypes for local import convenience. 
I think this is too much work for a *very* special case. The workaround isn't that bad, it doesn't even need any evil extensions. It is more important to get a good solution for implementing TypeEq and similar functions without using OverlappingInstances. The extension that I actually want is closed instances. The claim is that: &gt; In combination with total type families (possibly implemented via NewAxioms), this sugar could succinctly supplant much of the functionality of OverlappingInstances — stare deep into the Oleg. But there is no example of how that would work. Oleg's solution that is linked to requires three lines of boilerplate per type constructor!
INLINABLE does this.
One option is to have a standard name (M for module?) and import each module qualified. Then you can use both `Set.M{..}` and `SSH.M{..}` if you want. The problem with this is that you can't bundle a bunch of modules into one import the way classy prelude does: there's no way to export submodules without flattening them together, nor a way to rename the record constructors not to clash without duplicating the entire type.
I was thinking of an earlier proposal to name it `&lt;+&gt;`, but I didn't realize it had been added in base 4.5 under the name `&lt;&gt;`.
You can already generate instances at runtime!
I was just trying to answer why it can't come with quickcheck tests. But anyway, I think the reluctance for type classes like this is that type classes are used to write polymorphic code, and then you need some laws or expectations to reason about said polymorphisms; however, it was specifically mentioned as a non-intention of classy-prelude to be used to write polymorphic code, so, duno. I suppose it means you'll have to take care not to write "accidentally polymorphic code" with it, as type classes are now used for multiple unrelated purposes.
Maybe this is two separate problems: * Authors of libraries exporting helpful things to fix this problem. That's what you mentioned above, and that I replied to. * Authors of a prelude/clients of the libraries making a namespace solution for two or more conflicting libraries. The “Classy Prelude” only attempts the latter and it succeeds. The record solution, when attempting the latter, can also succeed because you as the author of a prelude or a client of the libraries get to decide the constructor names — and because it's only constructors, not classes, it's self-contained/doesn't spill out of the codebase making use of this solution.
Yeah, or just ad-hoc (C-style) polymorphism.
&gt; Happstack doesn't give you a default does it? Well, it supports blaze-html out of the box, but the best supported choice via integration packages is hsx. If you want to use web-routes in snap, you have to integrate it yourself both into the snaplet system and into heist. In happstack it is more or less a matter of adding a few dependencies. &gt; It doesn't really seem accurate to try to paint happstack as being at any particular degree of type safety, given that it is almost entirely in the users hands. Shrug, you could make the same claim for yesod-core/snap-core.
Here's some quick code I threw together: http://hpaste.org/71641 The basic idea is that `instance Foo a =&gt; Bar (X b)` is a function from instances to instances. You just need to combine bits and pieces at runtime to construct arbitrary instances. Note that you could always just make a type whose instance depended on some of its own fields, so you could generate custom instances that way too.
But that directly contradicts Michael's statement "The purpose of classy prelude is _not_ to encourage writing polymorphic code based on the typeclasses provided." So one of the two has to give.
This is related to the post on RecordWildCards for modules. The approach detailed above seems to both subsume and extend what is available through that approach, and would have given Haskell a module system that rivaled ML's. With the functorial lens packages floating around, this could have lead to an incredibly powerful system. And yet I can't find recent on the subject. The proposal certainly never made it into Haskell, and all discussion of it seems to have dropped off. A post similar to this one was made a year ago, and it received no discussion at all.
I think the dictionary approach is incredibly powerful because it is first class, and I think Chris's approach is a great step towards making it syntactically sweeter so it will have more widespread adoption.
Also, for people wondering what this syntax is useful for, it is targeted to people who prefer to use case statements when defining functions. For example, if I were to write safeHead, I would use: safeHead xs' = case xs' of [] -&gt; Nothing x:_ -&gt; Just x This extension allows me to do it without bringing a gratuitous variable name into scope just so I can do an anonymous pattern match. I find myself wanting this extension all the time because I write in a case-heavy style.
As a counterpoint, Tekmo comments are well founded and his contributions to the Haskell ecosystem are largely considered good and recommendable. Like it or not, there is often a problem with packages coming from Yesod and/or Michael : there are poorly thought out. A language community is more than just a compiler, it is a set of practices and quality standards. Haskell community has a history of carefully designing software and practices so that they rise the standards of programming. Yesod / Michael don't follow this culture, and rather keep throwing tons of poor software and poor practice in the face of the community. And because this team pretend to target real-world programmers and novices, newcomers to Haskell usually trust "the yesod/michael" way : tons of non-composable template haskell code, ignorance of laws associated to classes, tightly coupled packages and lots of extensions. People following this subreddit and stackoverflow might have noticed how *weird* some beginners questions have become lately. And you know what ? That irritates me way more than Tekmo regular efforts to keep Haskell standards high.
At least for that paper (and a similar one by Ken Shan), I think the point is less to actually write code that way, and more to show how System F already has all you need in a core language to compile a surface ML module system. So the hurdle to adding them to a Haskell compiler is writing the elaborator (and figuring out exactly how module elaboration interacts with class elaboration), not some fundamental features lacking in the core language (for instance, a paper by Harper I can think of spells out all the logic of a calculus with modules, but it uses some kinds of effects for correct behavior, and it's not too obvious that that can be elaborated to plain System F). I suspect the authors agree that implementing the elaboration manually is cumbersome. :)
It's purpose is to supply convenient overly generic, overly polymorphic functions for extremely wide range of use cases. You can call it what you want, the intent is clear, and so is the potential for abuse. 
Well said. Excellent point about the current Prelude being littered with partial functions. 
Right, I was trying to find discussions on his proposal and other ideas people had had, when I stumbled across this. This is an incredibly detailed proposal, and even if the dictionary approach is favored by the community, there are other extensions in this paper that should be looked at to extend the power of any module system. I really like the take on existentials and the (!) type of operator, type level projection, etc.. I don't necessarily like the exact syntax proposed in paper, but there's enough there that I'm surprised I can't find a recent discussion on the subject. This proposal has obviously been axed, and I'm not sure what it's deficiencies were. EDIT: Stupid typo.
So all this machinery to replace zot = (a,b) where a = L.pack [1,2,3] b = B.pack [1,2,3] with zot = (a,b) where a = pack [1,2,3] where L{..} = def b = pack [1,2,3] where B{..} = def What is it that i'm gaining with it ? Brevity ? no, clarity ? no. What am i missing ? 
&gt; sumMaybes = sum . map (fromMaybe 0) or &gt; sumMaybes = sum . catMaybes is shorter still, unfortunately `fromMaybe` and `catMaybes` are in Data.Maybe and not in the prelude.
Well then we will agree to disagree. I also think that while I agree with you in theory people tend to self organize into groups that are leaning one way or the other. And what is true is often less important than the view a particular camp has (not pointing at anyone here). This happens all the time across arbitrary groupings of humans around issues. Nothing going to change here. There will always be tension (not the bad kind) between people who want a tool that solves their problem now and others who want to think and reason through the problem to find the correct solution. While number 2 gets the world to a better place it may also be on a different timescale. Thinking out loud I wonder if this thread would be so big if the package had a different name that was more boring.
What is your reasoning for avoiding pattern matching right away?
No, I won't even do that. It's intellectually lazy to make a claim that they're at odds without backing it up. Feel free to stereotype those who like theory and strive for theoretically sound approaches to programming as ivory-tower academics, but I'm going to call you out on it. And when it comes down to it, why should I give up theoretical soundness to "get shit done"? I'm quite happy getting shit done at my EvilMegaCorp job while still maintaining a high degree of theoretical soundness and using stuff that "people who write pseudo-code in LaTeX" devised. If you don't like doing that, that's fine, but it's shady to make a false dichotomy of it and to use it to portray people on the "other side" in a negative light. Edit: after your edit to your post, my response to it sounds a lot harsher than warranted. I still disagree that you need to make the choice, though.
There are several small touch-feely reasons. First, I think the pattern-matching style is too visually congested and requires typing the function name over and again and makes it slightly inconvenient to refactor the name for functions that have lots of cases. I like to use pattern-matching for teaching beginners, but in my own code I just use ordinary case statements. Second, it works anywhere without the use of `let`/`where`. This is nice for mental consistency so that I don't have to carry any cognitive overhead from deciding whether to use pattern matching or case statements. If I use case statements everywhere then I don't think twice about it. Third, sometimes I like to inline the case statement to keep the definition as local as possible so I don't have to refer back and forth between the definition and the call site.
It's a win when you use names from the same module frequently in the same scope. If the names clash with the Prelude or other imported modules, it's a win for brevity: you don't have to keep repeating the prefix. If the names are used never or infrequently outside that scope, it's a win for clarity: you say what you're using near where you use it rather than importing into the global namespace. Use cases that come to mind are algorithms involving heavy use of Sets or Maps, and local Parsec parsers. 
That's not the only way you can bring the name into scope: zot = (a,b) where a = pack L [1,2,3] b = pack B [1,2,3]
ok, so what's the final syntax? (that trac entry is quite a monster...)
what version of os x do you have? what are the variables you're setting in your bashrc/profile file? have you run brew doctor at all? which install approach for haskell platform have you done? via brew or the packaged installer? have you removed the contents of ~/.ghc/ ? do you have the .cabal/config setup in the default way? 
I still owe you a Torsor. =)
I wrote tons of this sort of code recently. I tried `conduit` seriously for the first time, and was *constantly* doing this: await &gt;&gt;= \mx -&gt; case mx of Just x -&gt; ... Nothing -&gt; ... Or try implementing this: unChunker :: GInfConduit (Either a [a]) m a How did I do it? Big surprise... unChunker = awaitForever $ \ea -&gt; case ea of Left a -&gt; yield a Right as -&gt; CL.sourceList as Now, upon reflection, I *could* have written: unChunker = awaitForever $ either yield CL.sourceList but this style of programming doesn't generalize well when you have to case on data types with more than 2 constructors.
&gt; I find myself wanting this extension all the time because I write in a case-heavy style. Same here! I occasionally answer Scala questions on StackOverflow, and cannot understand the Scala tendency to stay as far away from case statements as possible. It's bizarre; case statements are the simplest and most straightforward way to program (though a bit verbose) that there is.
&gt; Furthermore, how would one create systems which can live in a mixed ecosystem with applications developed in non-haskell languages? By not using acid-state for those things, obviously. Probably not persistent, either, since persistent is sort of oriented towards "treat relational tables as lookup dictionaries". You are now talking about something completely unrelated to the safety claims I was countering, though. &gt; non-DRY /blog BlogR GET POST /blog/#BlogId BlogPostR GET POST We have to write `/blog` for each route beginning with that, and if we get it wrong, the application will work but we don't have the URLs we wanted and the only way to safely fix it is to set up redirections.
I think I understand some of that. Lenses almost allow you to compose signatures. For example, let's say you had two module types `A`, and `B`, with lenses: lens1 :: Lens A x lens2 :: Lens A y lens3 :: Lens B z Then you can just define a product module `C`: newtype C = C { unC :: (A, B) } c :: Lens C (A, B) c = iso C unC ... and use `fstLens` and `sndLens` (from `data-lens`) to derive the lenses to each lower module: lens1' = lens1 . fstLens . c lens2' = lens2 . fstLens . c lens3' = lens3 . sndLens . c However, that doesn't quite achieve what we want because there is no way to automatically unpack the tuple using that extension. I need to think about it some more.
Perhaps I should write a follow up post to [The Long and Epic Journey of LambdaCase](http://unknownparallel.wordpress.com/2012/07/09/the-long-and-epic-journey-of-lambdacase-2/); it could be just as long, just to cover the past week! I think it's safe to assume the follow-up arrow ticket represents the final syntax: http://hackage.haskell.org/trac/ghc/ticket/7081 Specifically, \case pat1 -&gt; e1 ... pat2 -&gt; e2 and if | cond1 -&gt; e1 ... condn -&gt; e2 Presumably, "`\case`" and "`if |`" are new "layout heralds", meaning that whatever comes next sets the indentation level for that block. I'm sure it'll all be in the ghc 7.6.1 release notes.
Hmm. The "case" one looks pretty weird to my eyes. The "if" one I love, though it is inconsistent (arrow vs. equation sign) with the old guard syntax: fun x y z = tmp where tmp | x&lt;5 = 10 | y&lt;5 = 20 | z&lt;5 = 30 | otherwise = 40 (edit: ok, this is a stupid example, what I wanted to show how to emulate the "if" syntax using a temporary variable) A great usage of the new "if" syntax is that it can be nested if | cond1 -&gt; if | cond2 -&gt; ... | cond3 -&gt; ... | cond4 -&gt; ... 
I think a better idea than wrappers all over the place would be an extension that adds automatic reification of modules. For instance: {-# LANGUAGE ModulesReification, RecordWildCards #-} import Data.Map -- due to the activation of the extension, in addition to the names being loaded -- in the current namespace, the data constructor Map is available import Data.Set as S -- idem, but the constructor will be called S And in both cases, the constructor's fields will be everything that's exported by the module. But they'd be normal values. Something equivalent for classes (turning an instance automatically into a value) might be awesome too: then classes and modules would be generalized and to some extent unified. Consequences: no old code, no packages would have to be altered, and you would continue to create modules the way you always did. It would be backwards-compatible.
It's consistent with guards in case expressions, though.
I'm afraid that's an ambiguous reference to `pack`.
I wrote this with the intention of both solidifying my own understanding of the correspondence, as well as making it easy for people to understand where the correspondence comes from. It's still a WIP, so comments are appreciated.
Both `L.pack` and the `L{..} = def` might be complementary, I think! When one doesn't suit your needs you can switch to the other. So you'd have: foo = L.pack "abc" but maybe elsewhere you'd have this: foo = pack "foo" `append` pack "bar" where L{..} = def
That's true. But otherwise the syntax is exactly the same as the above guarded "function" with zero arguments (except that we have a keyword instead of a normal word), and quite unlike the case syntax. (I haven't before thought of the trick of emulating this with case, as seen in the trac ticket: case () of _ | cond1 -&gt; ... | cond2 -&gt; ... ) 
Actually that is wrong. If L and B represent the lazy and the strict version of the "same" module (the same module signature in ML terms), the point is to be able to do: zot = (a,b) where a = pack [1,2,3] where BS{..} = lazyBS b = pack [1,2,3] where BS{..} = strictBS (I don't see the point of *def*) Then if you have a large code in 'a' or 'b' computation you don't have to type prefixes, plus if you want to change the flavour of bytestring used on that specific part of code you just have to change the RHS of the where clause. There is the point, that you can have the granularity that you want (except it cannot scale over several functions) and modify the fewest things when you refactor. So you control the scoping of namespaces using the regular Haskell scoping constructs: where or let.
The idea is that `=` is for the definition, not part of the guard. No name is being defined by a lambda or case, so they use an arrow instead. You could pronounce them as "is" and "then" (or maybe "goes to"?) if you like. foo x | side condition = definition "`foo x`, when `side condition`, is `definition`" case x of Just y -&gt; alt1 Nothing -&gt; alt2 "consider `x`: when `Just y` then `alt1`, or when `Nothing` then `alt2`"
Oops, I misread your post because I was in a hurry. I thought the gist was that you would define something like: data ByteString = B { pack :: [Word8] -&gt; L.ByteString } b = B { ... } l = B { ... } Then you would select the instance you unpack by using: zot = (a,b) where a = pack [1,2,3] where B{..} = b b = pack [1,2,3] where B{..} = l It seems like that would be a lot simpler than using the `Default` type-class. Then it would work both for the RecordWildCards extension and also for just picking out a specific function. Then `b` and `l` would coexist within the same module and would not need to be namespaced.
&gt; Michael is using this subreddit as his personal soapbox to throw logical fallacies at anyone who dares to suggest his vision of haskell isn't as good as actual haskell. It's worth noting that it was not Michael who posted this to /r/haskell. This claim is very mean spirited.
Writing a direct response to posts on this subreddit, and letting someone else submit it isn't any better than writing the direct response and submitting it himself.
&gt; We have to write /blog for each route beginning with that, and if we get it wrong, the application will work but we don't have the URLs we wanted and the only way to safely fix it is to set up redirections. That's a feature, you have complete control of how your routes will look like instead of relying on some function that translates constructor names to routes. It also allows you to change the names of the constructors without breaking your public API.
There's a difference between * being *aware* of criticism on a particular forum and addressing it * using a particular forum as one's soapbox Michael may be up on a soapbox, but it is unfair to imply that he is using this forum for his own nefarious purposes.
This, as I mentioned in the [classy prelude thread](http://www.reddit.com/r/haskell/comments/wn882/clarification_classy_prelude/c5ev5rs) is I think the cleanest solution namespace solution. Replicating modules as records just so you can pun the fields out of them happens to work with the extensions we have now, but it's a bit baroque. I'm not sure you should be able to omit importing the module at the top of the file, though. 
No it doesn't. See my recent submission: http://www.reddit.com/r/haskell/comments/wpx97/a_simpler_approach_to_firstclass_dictionaries/ Using that approach you can keep all the first-class modules within the same namespace and they can all be collected with a single import. Then you simply choose which dictionary to bring into scope.
Hm, not sure that would be great because the type of `pack` would/should differ for each “module”. It should be `[Word8] -&gt; B.ByteString` for strict and `[Word8] -&gt; L.ByteString` for lazy. How would you resolve that? **EDIT:** Nevermind, the answer is in your other reply.
Even better is Scott encoding everything. That way case analysis takes constant time rather than linear time.
Yeah, I pretty much knew what you meant ;). When I read baseless assertion post it came across to me as "screw you Tekmo," which is why I felt compelled to snark.
TIL about Scott encoding. I'll have to read some more, but I guess my only question at the moment is how it compares to Church encoding in terms of elegance and flexibility.
Yeah, I don't know why I didn't see it at first, but baseless assertion was a bad idea. A cute meme with amusing self-deprecating examples, but a bad idea nonetheless.
They may not have been implications you *intended* to make, but I certainly interpreted your comment that way, so the implication was certainly made. &gt; stated that this was supposed to be the one and only replacement for the standard prelude. It doesn't matter if it's the one-and-only replacement or an entirely optional replacement - it's still a replacement for the prelude, and as a replacement to the prelude, it should be measured up against the existing, flawed one. In my view, the flaws in the existing prelude are still preferable to the nightmare of improper reasoning that awaits me using Classy Prelude. &gt; It is designed to make some code easier to write, that's it. As I said elsewhere, this is at the expense of reasoning about it. I'll keep the slightly-harder-to-write but far-easier-to-reason-about code.
Tekmo actually explained what he considered brittle about `FunctionalDependencies`, so this point has been addressed - hence the silence.
Yes, we have this power with web-routes-boomerang too, but without the duplication.
Well said.
I much prefer \case
pattern match failure, I imagine. You should probably have an `otherwise` there.
This has potential, but having to constantly open the records would be annoying. 
This is why it would be nice if you could unpack them into the global namespace, i.e.: -- unpack all declarations at the top-level ByteStringLike {..} = strict Edit: Actually, that DOES work! HOLY COW!
Yeah, I wasn't too happy with baseless-assertion either. Really, it was just a bunch of newtype wrappers around defensive-kneejerk-reaction anyway.
It relies on RecordWildCards and NamedFieldPuns. See http://www.haskell.org/ghc/docs/7.4.2/html/users_guide/syntax-extns.html and ctrl+f for those two things. See [Scrap your type classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) for further details on the other tricks that are being used here.
Right, it's not intended to replace the prelude... yet it is a replacement prelude. I'm scratching my head.
But that's the thing: using this approach you have qualification for free: zot = (a, b) where a = pack strict [1, 2, 3] b = pack lazy [1, 2, 3] You can opt to not use `RecordWildCards` at all, in which case you qualify which version you want simply by choosing which dictionary you pass, without having to unpack anything. And of course, renaming the qualification is as simple as: s = strict l = lazy
Now if only we had a way to overload the same name based on context... ;)
&gt; Also, can somebody explain to me how Chris split his hpaste into two sections? Click "annotate".
Can we please just stop talking about this stuff? Do people not have anything better to do with their time? Focus on the content, construct your argument logically, be an adult and stop performing meta analysis on how people are reacting with an inappropriate/weird/unconstructive/.... style. You're not their mother. I feel like I'm watching a soap opera from the 90s. Edit: I'm not disagreeing with anything the OP has said. I guess I'm mostly reacting to the drama in that other thread that was supposed to clarify things.
Well, I'm glad people are discovering one of the intended uses of RecordWildcards. :)
Amusing that it took a mere 8 hours for one hit of syntactic sugar to lead to [a request for more](http://hackage.haskell.org/trac/ghc/ticket/7081). Still, I'm glad to have both of these additions; I use `function` quite religiously when doing monadic programming in OCaml and it's nice to have an analog in GHC.
For the case widget, I didn't want to use `left x` and `right y` because I want to treat the whole thing as a variable binder like a lambda. So I want to avoid implying the the `left` in the case expression is really the constructor, because it's not, it's just a part of the syncategorematic object `case ... of { left ... -&gt; ... ; right ... -&gt; ... }`. It makes sense, I think, when you consider that `vE_{x,y}` only has the variables, not the constructors, and all we're doing in that part is trying to give a notational variant of `vE_{x,y}`. So we don't want to make use of substantive things like `left` because there no real pattern matching going on. I'm going to see what people think tho, see how confusing it is this way. If it's too confusing, I'm going to just use the constructors. BTW, what I like about writing this is that it's made me realize that "finding a term language" is almost a non-thing: you could literally write all the sequent stuff using proof trees if you wanted, it would be just as good, but it would be bulky. The term language for natural deduction just is natural deduction. I think. I'm going to go through Wadler's paper that provides a language of the linear lambda calculus and see if the naive "translation" holds there as well.
It looks like `pandoc` is also written by the author of Pandoc (which is also GPL) - so claims that it isn't his original work might be difficult to argue.
I am really not a fan of the new syntax. I would much rather have seen something like: \ (Just n) -&gt; foo n | Nothing -&gt; bar Using \case makes it look like we're binding to a variable named 'case'. I can just imagine all the newbs asking the same question. Truly, it only buys us ten keystrokes over \x -&gt; case x of ... at the cost of some serious cognitive overhead.
I almost always use pattern matching style, but the use of case expressions tastefully doesn't bother me. One of the code-bases I've seen (DDC) uses case expressions almost exclusively and it's fine.
That syntax has parsing problems, I believe, something like let f x | bar = \(Just n) -&gt; foo n | Nothing -&gt; bar | baz = quap You can see how that might confuse a parser. 
I don't intend to.
Is this really the kind of stuff we want on the Haskell reddit? It seems to have a decent number of upvotes, and it feels heavy-handed to remove it, but can we refrain from stuff like this in future? We already downvote/remove meme pictures and other off-topic posts. We don't have strict rules right now, but that's mostly just because this community is really good at regulating itself. I realize you address the memeified cesspool in your post, but it sounds a bit like "I know this isn't what should be here, but I'm posting it anyway".
I hate the guard syntax anyway (`something = expr` should mean that `something` is definitionally equal to `expr`).
Neither SPJ nor I like the \of version despite the fact that it is easier to parse. Besides, Haskell does not have a history of choosing syntax based on its ease of implementation.
&gt; I'm not sure you should be able to omit importing the module at the top of the file, though. Agreed, that's not normal, even a fully qualified `Data.ByteString.pack` use won't work without top-level, and that seems to make sense for reading purposes.
Wow, cool. I could use that in my type-class-less Haskell subset. Nice!
It's intermittent for me too.
The arrow syntax considerations were thrown around before the lambda-case thing was merged, I think the poster was just holding back the ticket until the other one was resolved.
Is that ambiguous?
Flexible, Undecidable, Overlapping, Incoherent Instances of MultiParam Typeclasses that use Functional Dependencies and Flexible Contexts! Try saying that ten times fast.
Like bigstumpy said, install the 32-bit version of GHC. I was stuck with this problem for a long time. It was kind of intermittent, so I could still actually get some stuff done, but it was a huge pain. Switching to 32-bit fixed it for me.
I don't see why that should confuse Haskell's layout-based parser.
Just that guards aren't parsed using layout at all.
&gt;If his posts make the front page, it is because they are upvoted to that point. If his posts make the front page, it is because they were submitted. This is a small subreddit, *everything* makes the front page. Talk about disingenuous. &gt;The reason he even wrote this particular blog post is because he felt like discussion of Classy Prelude got off on the wrong foot Which is why it was a direct rebuttal to someone's comments here, taken out of the context of where people can see both sides, and deliberately ignoring the points he has no argument for, right?
I guess you're right. These encodings won't work here. The fact that system F is strongly normalizing is a strong argument.
We get it. You can't pattern match over multiple patterns in a lambda. But there is already a solution: using a case statement, as in \x -&gt; case x of (Just x') -&gt; foo x' Nothing -&gt; bar This gets noisy in the context of a monad action, specifically because of the unnecessary variable and indentation. But -- and this is the big issue -- Haskell does not mix well with ad hoc rules. There are perfectly sensible syntaxes for this feature which do not introduce any conceptual ambiguity.
Pandoc is a piece of software, `pandoc` the Haskell library exports to Haskell bindings for that piece of software. They're both authored by the same person and both GPL. So claiming that it doesn't qualify for copyright due to the threshold of originality won't work. It's not as if a third party produced this thin layer of bindings, but actually the person who originally conceived of them did, and the whole API of Pandoc. I'm guessing `pandoc` is GPL-contaminating on Hackage no matter how you look at it.
The problem *is* the intermediate variable, as in your example, when you want instead to do direct pattern matching. With a direct pattern match, which is the more general problem, both monadic binding and branching are made a little nicer. Haskell syntax is full of ad hoc rules.
That site's terrible to read with that bad stylesheet with long lines, it gives me headaches. A shame, relly. EDIT: Better wording.
You might be interested in EverNote "Clearly", which I use on Chrome and I think also works in Firefox.
I'm not sure how much long lines matter. I've been running an a/b test on my own website http://www.gwern.net for the past month or so for 1400/1300/900px max-width, and with 20,300 visitors for each version, the time-on-page is close to identical: 900px is winning with an observed improvement of... 0.31%.
Couldn't time-on-page decrease if the content is easier to read?
So, a genuine attempt to answer this question... first of all, I'm not sure really what is meant by "to prove code". You prove propositions, which are sometimes *about* code. But assuming this means prove some property about the code... At face value, *yes*, it certainly would be easier to prove properties about code that doesn't use type classes. That's because when you encounter a type class, there's nothing in the language itself that guarantees you basically anything at all about the instances of the type class. All you get is the type signature, and that's rarely enough to conclude anything interesting. *However*, the Haskell community has answered this concern by tending to associate type classes with laws that they expect of their instances. So if you're willing to accept a proof that *assumes* the instances of a type class will obey the relevant laws, then we can dodge that problem. This works, if the laws are chosen wisely (not a trivial task!), and if you are serious about expecting them to be upheld. If the laws are difficult to prove, this can even simplify reasoning about code by using type classes as a kind of modularity in proofs. However, undisciplined use of type classes still gets you right back where you started, which is having nothing but the type signatures to go on. As this relates to classy-prelude, there's certainly an undisciplined use of type classes. The only reasonable defense here seems to be is that these are just a notational shorthand, that there's really no expectation that people will abstract over these classes, and that really any use of them should be at a call site that fixes the instance with some generally top-level type annotation somewhere. So actually there is no real abstraction going on at all; just a hack to be able to say "filter" and mean different things in different places. But there's no guarantee this is really true, and aside from doing type inference in your head, no easy way to tell for sure whether and which implementations were used. This at least presents less of a problem than if abstraction over poorly specified type classes were being encouraged... but it appears to have the potential to be rather annoying.
I wonder if there is a way to know for sure. A look at the distributions of time-on-page for each width might be revealing. I would guess that the following categories or reader exist - * Readers that give up if they aren't enjoying reading (a medium time-on-page correlated with enjoyment) * Readers that will read the entire page regardless (a high time-on-page inversely correlated with ease-of-reading) * Scanners looking for a particular piece of text (a low time-on-page inversely correlated with ease-of-reading) This would lead to a distribution something like: Scanners Givers Up Complete Readers # # # Users ### #### ### | ################################# ---- Time on Page Assuming that enjoyment and ease of reading are correlated, I'd expect to see the first and third humps move left and the center hump move right when ease-of-reading improves. However, there are probably more effective methods of classifying these reader categories than reasoning about the behaviour of statistical distributions. Such as including page-behaviour detection with javascript. If you're working with mean-times only, depending on the number of users falling into each category, the opposing motion of the different distribution clusters could be hiding the real impact of the changes. I'd be interested if there is a way to know what's really going on with a high certainty without having to invest too much time. (edit - monospace fix)
&gt; I'd be interested if there is a way to know what's really going on with a high certainty without having to invest too much time. I don't think there is, at least not with Google's a/b tester.
Fair. Though the equation sign comes after the condition, so it's definitely equal
I'm only a hobbiest when it comes to haskell so I'm not sure I totally understand the advantage of pulling them into global scope like that vs: import Prelude hiding (lookup) import Data.Map Could you explain?
&gt; 15:30-16:00: Cake Break Killer.
Your understanding of `split` is correct. I'll try improving the documentation again. Did you see the diagram I posted in the comments? Was that helpful, unclear, or something else?
what do you mean by 'long lines'? I use 1024x768 and there are still margins. maybe you prefer blog themes that have boxes with links/widgets on the side, but I don't. I prefer 100% of space to be used for content.
Basically Scott encoding is "shallow" while Church encoding is "deep". If you look at the deconstructor functions like either, maybe, foldr, then foldr uses the Church encoding of lists, while the counterpart using Scott encoding would be list :: r -&gt; (a -&gt; List a -&gt; r) -&gt; [a] -&gt; r. Instead of recursing fully on the structure, it only unwraps one layer of constructors. You pass a non-recursive function to it, and add recursion from the "outside" (say by using 'fix'). Elegance is in the eye of the beholder, but Scott encoding is more flexible (easier to write functions like 'pred' or 'tail'). I think I also prefer it on elegance because it separates the recursion from the CPS transformation.
newtype Nat = Nat { nat :: forall r. r -&gt; (Nat -&gt; r) -&gt; r } toInteger :: Nat -&gt; Integer toInteger n = nat n 0 ((+1) . toInteger) Seems straightforward to me, but maybe I'm missing something?
I know how to encode types with it (you're hiding the infinite type behind a newtype, in this case), but I wanted to know where the linear/constant distinction came into play. Edit: now that I think about it, I guess it probably boils down to the kind of recursion that Church encoding rolls in, which means that writing e.g., `tail` on a list takes `O(n)` time. The direct access to the recursive case in Scott encoding means you have more flexibility when consuming it, I think.
See also: http://hackage.haskell.org/package/reflection https://github.com/ekmett/reflection/blob/master/examples/Constraints.hs
Right! But I was wondering about Church, not Scott. :)
&gt; Haskell syntax is full of ad hoc rules. Not infrequently, I find myself absentmindedly typing = when I should have written -&gt;, or more often writing -&gt; when I should have written =. 
&gt; It's not a problem of keystrokes I personally think that it's evil to have the compiler warn about shadowed names. If you turn off those warnings off this is a somewhat lesser problem. But yeah, I still think lambda-case is a significant syntactic improvement.
This was a misunderstanding on my part, my apologies. I thought you were discussing the pandoc package (pandoc-unlit didn't show up in my search). I thought pandoc on haskell might be a bunch of bindings to a different library written in another language, which if it were by different authors, might lend credence to the threshold of originality argument - so I checked it out and they weren't. Pandoc-unlit most likely does not qualify for copyright on its own but inherits the licensing of its libraries.
My solution to long lines: snap my browser to 1/2 of the screen. Lines are now 50% shorter. xD In windows it's usually windows button + right arrow; in xmonad, I just open a new terminal. The websites that *really* bug me are the ones that look awful at 1/2 width. Like reddit, for example. &gt;,&lt;
Strings don't use twice the memory and I would wager most programs are composed of more strings (in bytes, not necessarily in number of variables) than integers or pointers.
So you just whipped up an adhoc first class modules, using standard haskell tools (records and maps). Does that warrant a new name for a new language ? Perhaps it can be even implemented in TH. 
I fail to see how the type classes are not adhoc ? They do not enforce the laws. Programmer still has to do it himself. And in that case he might as well enforce those same laws with records/maps. 
I like to call it ... Haskell. :) There is nothing magic about it and it is completely backwards compatible with existing code (you can always interconvert between a typeclass/module and their corresponding dictionary). It is purely a matter of ecosystem and two extensions and I think the RecordWildCards extension is good enough sugar that it does not warrant using Template Haskell. I will write up a fuller blog post describing how to translate between ordinary module syntax and the corresponding dictionary trick and I already have a specific demonstrative use case lined up for my next pipe release.
Wouldn't it be better to have a scope regions where you can define many functions instead of reimporting same modules again and again for every function ? scope A where import Data.Map func1 a b = .... func2 c d = .... scope B where import Data.IntMap func1 a b = .... func2 c d = .... Or import Data.Map as M ... scope with M func1 a b = ... func2 c d = .... 
The sole purpose of all these suggestions seems to be getting rid of qualifiers in code. In Delphi (pascal like language) there's an operator "with". So when you have an object you need to work a lot with, you can write with File Open; Read; Close; I think it would be easy to use the same kind of syntax to bring into scope qualified modules: import qualified Data.Map as M import qualified Data.Text as T .... with M, T foo a b = .... bar c d = .... 
why don't idris, agda and other dependently-typed languages (with dependently typed records) use the same first-class module system that Cayenne did?
I also works, but as I told, I prefer to be consistent. 'let' and 'where' are already there in Haskell to define scopes (mostly at the level of a function), so to me it's preferable to reuse the constructs we have instead of inventing new ones. Your proposal would be useful if you could define scopes in order to reuse them afterwards, like: scope A where import Data.Map import Data.Vector.Unboxed func1 :: .... func1 a b = .... scope B where func2 a = ... func3 b = ... where import scope A -- IMO this is necessary to be able to locally open modules or scopes, -- so that you can have the granularity you want It resembles then what C++ or Clojure's namespaces provide. You'd then treat a 'scope' as a special kind of module, that can be declared inside another module. However it complicates my use case: I just wanted to separe what my functions can use, but not _declare them in separate namespaces_. You'd then need anonymous scopes: module Something where scope where import Data.Set f1 = ... scope where import Data.Map f2 = ... And then Something re-exports f1 and f2, they're not in sub-namespaces. It's just than f1 only accesses Data.Set and f2 Data.Map. *EDIT:* And then, scopes should have the same capacities as modules, e.g. control of what's exported: scope A (f1, f2) where f1 x = ... f2 x = ... f3 x = ... And to follow the logic all the way down, you should be able to declare nested scopes.
No, you're right. I'll clarify that bit. Thank you. :)
I don't like the naming of `T` over `Text`. I don't like that convention with qualified imports either, but it's even worse here because it simply isn't needed for terseness. Simply calling it `Text` would make user code more readable and scale better as more modules are added. Similarly all the other record modules in this package. For lazy variants, a prefix `L` might be sufficient, though (e.g. `LText`).
I was actually thinking the same thing. I kept it as T, V, &amp; so forth for now, just to pun off of the typical single-letter qualified imports, but given that the whole point of this library is to ease the burden that causes people to shorten qualified names in the first place, it might be wise to switch the convention to use a fully spelled-out name instead.
This looks like [bug 199](http://trac.haskell.org/haskell-platform/ticket/199). The workaround is either upgrade to lion with newer xcode or use 32-bit version.
&gt; you avoid the effect of "lumping all effects into one big IO-like monad". I agree, but that does not necessitate a "free monad", only the availability of appropriately domain-specific monads, that you wrote yourself or provided by a third-party library. Whether the monad functions actually contain their semantic denotation, or are just pieces of AST that will be interpreted later, is largely an implementation detail. &gt; I'm not sure what you mean. In my post Teletype was a monad. The point was that free monads let you keep the monad but let you factor out the effects you don't need. My point is: if you start using the `do`-notation with `Teletype`, you are writing impure code (only with a more restricted quiver of side-effects). You may in fact already be reasoning after the present code in an impure meaning, by which I mean thinking of returning an `ExitSuccess` constructor as "I tell the program to stop" rather than "I build a piece of program that represents the fact of stopping"; I don't know, purity is in the eye of the beholder. So yes, you have only the effect you needs, and this is better than having all in IO. But no, you are not necessarily fundamentally more "pure", and in fact that is not particularly related to the choice of a free monad: I could export a module with an abstract monad `Teletype` and the operations you use (and similar behavioral guarantees), that would in fact only be an alias for `IO`. PS: Truth be told, the latter point on testing is different and makes the distinction between different implementations of `Teletype`; it is true that ASTs are more easier to construct and inspect than sequence of effects, which makes your test easier. It may be possible to do something equivalent in a non-free monad by adding "logging" to the monad, essentially recording the trace of the effects performed; then you could formulate testing predicates as inspecting the trace.
Been thinking about this and I've come to the conclusion that all we need is a point-free case and the ability to pass this point-free version around. Then '\case foo -&gt; ...' could easily be written as 'case of foo -&gt; ...'. Now consider pattern matching on two arguments without nesting case expressions. Currently to do this we would tuple up the arguments and pattern match on the tuple. You could still do this with a point-free case. Better yet you can compose partially applied cases. Finally the multiway if could be written as 'case | foo -&gt; ...'. It shouldn't be too hard to implement. Detect 'case of' and replace it with '\useOnceName -&gt; case useOnceName', detect 'case |' and replace it with 'case () of _ |'. So really the proposal is just some sugar.
I edited my previous comment to add some extra points, so check those out if you haven't seen them already, especially the note about `pipes`. &gt; I agree, but that does not necessitate a "free monad", only the availability of appropriately domain-specific monads, that you wrote yourself or provided by a third-party library. Whether the monad functions actually contain their semantic denotation, or are just pieces of AST that will be interpreted later, is largely an implementation detail. I'm presenting free monads as a generic programming technique for structuring domain specific languages, so I'm not entirely sure we are in disagreement on this point. Free monads are pretty much the quintessential way to implement a domain-specific language, where the denotation is contained entirely within the free monad's base functor. &gt; My point is: if you start using the do-notation with Teletype, you are writing impure code (only with a more restricted quiver of side-effects). You may in fact already be reasoning after the present code in an impure meaning, by which I mean thinking of returning an ExitSuccess constructor as "I tell the program to stop" rather than "I build a piece of program that represents the fact of stopping"; I don't know, purity is in the eye of the beholder. Like I said in my post, purity's purpose is to equationally reason about programs. The motivating use case for what I'm presenting is where you are forced to use a monad that does not have a semantic denotation (either because the author of the package refuses to confer one or because the source code to the library is hidden). The free monad lets you factor out the parts that have no semantic denotation into the interpreter, allowing you to confer a semantic denotation to the rest of your program. Obviously, if you are working with some domain-specific monad that does have a semantic denotation, then this technique is not necessary. This is why I presented it within the context of `IO`, since `IO` is the quintessential example of a monad with no good denotation, but it's really supposed to be a technique for factoring out **any** monad you must work with that has a poor denotation. &gt; So yes, you have only the effect you needs, and this is better than having all in IO. But no, you are not necessarily fundamentally more "pure", and in fact that is not particularly related to the choice of a free monad: I could export a module with an abstract monad Teletype and the operations you use (and similar behavioral guarantees), that would in fact only be an alias for IO. No, that's contrary to what the post is advocating. The point is that the free monad is something you control and make so that you have control over the denotation for every part of your component outside of your interpreter. If you were to take my `Teletype` monad away from me and abstract it away and remove the denotation, I would just create a new free monad `Teletype2` to factor out the `Teletype` monad that you ruined. The point is that no matter how hard somebody tries to screw you by forcing you to use a monad, any monad, that has no denotation (including one that you yourself made), you can always fix your program by factoring that ugly monad into the interpreter. Edit: I don't know who downvoted you. You made a perfectly legitimate point.
Also, using the coproduct you can combine free monads, at least according to "Data Types a la carte". See sections 6 and 7: http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf So, you can add capabilities not in Teletype by making additional free monads and combining them. 
That's not true. A value of type `IO ()` is not "impure", it respects referential transparency just as any other (... when the `IO` implementation has no bugs). Code is code; in a reasonably well-defined language, it can always be given a semantics that is compositional (that's what semantics are for). When using an effect-like monad (eg. `State`), "giving a semantics" just means looking at the definition of the monad operators. When using the `do`-notation, you also perform a de-sugaring step first that explains how the semantics of each line is composed with the other ones. As Landin (iirc) remarked for C: if you interpret C statements as state-transforming functions, the semicolon is a silent notation for function composition, and this view allows for a referentially transparent interpretation of those statements; does it mean that C code is pure? "Impure" code is code that has "side-effects" that do magic stuff besides computing a value, this magic stuff happening, in the mental view of the programmer, alongside the value computation. If this is the way you think about Haskell code (for example code using the `do` notation, or code using explicit bind and return but where you've trained yourself to just ignore them), then this way to understand this code is "impure". The code is not "pure" and "impure" by itself (though the author or the language may make some view of it easier), the way you think about it is.
The point was not to encourage a monad-heavy style. I was only trying to say that, when forced to use a monad that has no denotation (like `IO`), you can factor out the denotation-less part into the interpreter and add a denotation to the rest of your program.
Guys, please stop downvoting `gasche`. If you think he's wrong, just correct him. Downvoting should be reserved for posts that are offensive and don't contribute to the discussion.
Well, I have a post I've been sitting on about implementing denotationally-safe concurrency using free monads, but I didn't want to overwhelm /r/haskell with posts on free monads, so I'm pacing them.
Or just write them using symbols from [SBV](http://hackage.haskell.org/package/sbv), doing it the haskell-way.
There is actually some code in happstack-server that adds a ToMessage instance for blaze-html. And there is a section in the Happstack Crash Course on how to use blaze-html with Happstack. So, in that sense, it is supported 'out of the box'. You do not need to cabal install any additional packages after installing happstack-server or include any extra imports in your code. For other templating solutions, you do have to install an extra Happstack specific library from hackage and add some imports. So those are supported, but not out of the box. I think it is fair to say that Happstack 'supports' blaze-html (and other templating libraries) because we do provide some code (even though it is not much) and documentation. It would different to claim that Happstack 'supports' any of the various SQL libraries on hackage. While they all work just fine with Happstack, we do not do anything at all to integrate them into Happstack or document their usage. 
Correct, I missed that. Of course, in principle the author of `exitSuccess` could have used: exitSuccess = void ... And then what little denotation `exitSuccess` might have had would have been completely lost.
Agreed — I'm actually alright with bigger names, in fact in my initial experiments I did `DataMap` rather than `M`, but used the latter to be more familiar to the Haskell audience.
This is quite cool, you did a lot of modules. I'll give it a try in a project, see how it feels. Regarding the big import list, can't you just replace it with BlahPrelude and export those modules? module BlahPrelude (module ModularPrelude.ByteString ,module ModularPrelude.LByteString ,module ModularPrelude.Text ,module ModularPrelude.LText ,module ModularPrelude.Vector) where Or are you refering to the S/T/L import list?
Don't overwhelm us, but do please keep them coming :)
Actually, Strings do take twice the memory (although ByteString and Text do not).
This is very nice. Did you know you can derive `run :: Teletype r -&gt; IO r` from a simpler function `runF :: TeletypeF (IO r) -&gt; IO r`? runF :: TeletypeF (IO r) -&gt; IO r runF t = case t of PutStrLn s c -&gt; putStrLn s &gt;&gt; c GetLine cs -&gt; getLine &gt;&gt;= cs ExitSuccess -&gt; exitSuccess liftRun :: (Monad m, Functor f) =&gt; (f (m a) -&gt; m a) -&gt; Free f a -&gt; m a liftRun f = lift where lift (Pure a) = return a lift (Free t) = f (fmap lift t) Then `run = liftRun runF`. (You can in fact write `liftRun` in terms of `Control.Monad.Free.iter`.) 
How about we just arrogantly call it "first class modules", as if it were THE approach. :)
Regarding populating the dictionary, could use punning: {-# LANGUAGE NamedFieldPuns, DisambiguateRecordFields #-} import Prelude () import Data.ByteString import qualified String bytestring = String.String { pack, length, map } Could save some redundancy, not sure (is the extra module is worth it?). Just an idea.
That's a very nice trick. Thanks! That would come in handy if you were defining a very large interface or multiple backends.
Going IO or adding IO to your transformer stack is just too seductive for application development, I suspect, but free monads are probably great for libraries where there is a clearer definition of what the monad should do.
I don't think you missed the point completely. I was just highlighting an advantage of free monads, mainly you can make many small free monads with different effects, and combine them. In the paper I linked to the author shows how to use free monads and type classes to compose new capabilities together. So, using that technique, you would not have to change the run function. 
Scott encodings do require that the language has fixpoint operators (both at the type- and term-levels), which is the big downside compared with Church encodings and the reason why I think they're not as popular. The big difference is that Church encoding of `T` uses the inductive elimination of `T`, whereas Scott encoding uses the (non-inductive) case elimination of `T`. Thus, if you want to take apart a single layer of `t:T`: with Scott encoding this is immediate and takes only the *O(1)* time of unwrapping the newtype and applying the resulting case-elimination function to `t`; whereas with Church encoding, case elimination on `t:T` takes *O(t)* because you have to recurse through the whole of `t` in order to rebuild its subterms. This is why it's such a pain to define `pred` on Church natural numbers, or to define `tail` on Church lists. Of course, laziness can save you from having to pay *O(t)* up front--- but you'll still have to pay it eventually, and even if all garbage is collected immediately you'll still have to pay for the loss of sharing.
While this is a nice approach to have in your toolbox—namely because you can have multiple interpreters for additional flexibility—I think that most of the time you want something much simpler. In particular, I think having limited versions of `IO` (or `State` or what have you) is extremely useful, but that the equational reasoning you get out of the approach in the article is generally too weak bother with for that reason alone. The performance of this approach is also going to be a problem in some cases. What I normally would do instead is something like the following: {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE NoImplicitPrelude #-} module Teletype ( T (), putStrLn, getLine, exitSuccess ) where import qualified Prelude as P import qualified System.Exit as SE import Prelude ((.)) newtype T a = T (P.IO a) deriving (P.Functor, P.Monad) putStrLn = T . P.putStrLn getLine = T P.getLine exitSuccess = T SE.exitSuccess This is a simple way of providing a monad that offers just the functionality you need. It also should impart exactly zero overhead (assuming basic inlining). This approach works even better in OCaml because you have actual type abstraction. Accordingly, you don't need to declare a `newtype` and write wrappers for every function; you simply would do `type 'a t = 'a io` and make the type abstract in the interface. **Edit:** I should note that I wouldn't advise doing this too often for things that would otherwise be in `IO`. I do, however, do it constantly for `State` as you can guarantee that external code can only alter the state through a limited interface. This is vital to preserving invariants. With `IO`, the "state" is huge and already being altered by other programs (and even other parts of the same program that are in `IO`), so I generally don't bother making limited interfaces. Also, I claim no novelty here; I assume this is a pretty common technique.
As illissius says, elegance is in the eye of the beholder. Though, personally, I think the Scott encoding is more elegant. The biggest reason is that by using Scott encodings we can completely remove the need for case analysis as part of the core language, without any (direct) performance loss. Cf., [Jan Jansen et al., IFL 2005](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.76.5557) and [Jan Jansen et al., TFP 2007](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.9841). (There may be performance loss due to the inability to syntactically recognize and analyze case expressions for optimization purposes, but this is more of an "in practice" loss than an "in principle" loss AFAIK.) Contrariwise, by using Church encodings we can eliminate the need for fixpoint operators. However, this is only of much concern to total functional programmers, since otherwise we'll still want fixpoint operators for other tasks. Moreover, because of the performance loss, even when totality is a concern, it's arguably better to allow standard case analysis and to use a dedicated totality checker to ensure that recursion is not used in a bad way. Unfortunately, totality checking is still rather ad hoc at the moment, so this approach is inelegant; but then having *O(n)* case analysis is also quite inelegant IMO.
The post is not so much about restricting effects as it is about building a core that does have a denotation out of a program that has no denotation. Even if I use your approach there is no denotation for what `putStrLn` actually does (it could fire missiles for all we know), but if I factor it through the free monad I can at least say something about how the free monad behaves, even if I still can't say anything about what will finally happen when it gets interpreted.
I agree, and I do think it's a good post that demonstrates a potentially useful technique. My point though was that I think limited interfaces are far more important than wringing out some weak properties from existing code and that there are easier ways to go about achieving said interfaces (not that you ever claimed otherwise). While I have you though, I will complain about your "tests for impure code don't really scale to large and complicated programs" comment. There are plenty of counter examples to this claim. For example, SQLite is fundamentally all about stateful operations (as, well, it's a database), yet it's also a large program that has an extremely comprehensive test suite. I also felt like you were referring to "test-dependent programming languages" as if Haskell was not itself test-dependent. While you can certainly get away with fewer tests in Haskell, I don't think you can actually get away with significantly fewer tests than you would need in any other language for most programs. (Ideally though, there would at least be less program to test in the first place.) Haskell is still a partial language where there's plenty to go wrong, and you still want (near) complete code coverage when you can manage it. In your final test at the end, you're demonstrating that `runPure echo`—which is not the interpreter you'd use in the real program—has the same semantics as `take 1`—which is not the semantics the real program should have. Yes, you've justified this through an equational proof (that you've not given), but this is still an extremely weak way of providing additional confidence in your program in my opinion. A simple unit test that shoves some input into the actual program and tests that the same input comes back out, however, would give me confidence.
There are a few reasons that I hesitate to fully embrace this approach for ModularPrelude. Firstly, consider [ModularPrelude.LByteString](https://github.com/DanBurton/modular-prelude/blob/master/ModularPrelude/LByteString.hs), specifically, these two types: data LByteStringModule = L { ... , length :: LByteString -&gt; Int64 , splitAt :: Int64 -&gt; LByteString -&gt; (LByteString, LByteString) ... } For some mysterious reason, Data.ByteString.Lazy uses `Int64`, whereas Data.ByteString (and also, obviously, List) uses `Int`. One could parameterize on that: data StringModule s i = String { ... , length :: s -&gt; i , splitAt :: i -&gt; s -&gt; (s, s) ... } But wait, now contrast the filter functions of Text and ByteString. Text is based on Char, while ByteString is based on Word8. I suppose we could parameterize yet again... data StringModule s c i = String { ... , filter :: (c -&gt; Bool) -&gt; s -&gt; s , length :: s -&gt; i ... } But should we be parameterizing these, or should we decide on a "canonical" `i` and `c`, and perform conversions? The original problem we were trying to solve was a more powerful namespacing mechanism. I feel that attempting to provide a common interface for various modules is unwise, not because it's not a good idea, but because it's just plain hard to decide on what the right interfaces are, and I've gone through that dilemma before with [sexy](https://github.com/DanBurton/sexy) so I'd rather not be the one to try. Plus, it is adding something to the Haskell ecosystem that isn't there yet, whereas all I had to do for ModularPrelude was translate "real" modules into first-class modules. In fact, that's probably another reason I lean away from the interface enhancement: because I'm simply translating modules, and the current module system is not that powerful, so in the process of translation, I don't *need* to use the enhancement. So tl;dr, I think it's a fine idea, but it adds too much work to the already daunting task of replacing Prelude. I'd love to see *someone* go through the pains of writing some good interfaces and first-class modules to go along with them, but that someone will not be me, and that project will not be modular-prelude (though feel free to borrow as much of the code as you like! It's BSD3, I'll get a proper license file up soon). If someone actually decides to do this, I highly recommend that you contribute to and base yourself off of BasicPrelude.hs, that way our two libraries could be easily used together: mine for simply mimicking imports, and yours for writing interface-based code.
Yeah, that's good reason to be hesitant to call them "modules". Maybe "module records" (a kind of record used to bundle names like a module)?
Hmm, yeah. I think you might be able to get something similar using parametricity there. The idea is that if `x` is a fully-defined value of type `forall a. M a` then `x &gt;&gt;= f` can never actually apply `f` to anything.* That means it can't tell whether `f` is `return`, so to obey the monad laws `x &gt;&gt;= f` needs to be `x` regardless of what `f` is. Just a sketch, though; I should take a look at the free theorems papers again. (* Unless bind always passes bottom to the function it gets, which would probably cause problems for the monad laws.)
If you are trying to learn Haskell, please try installing Haskell Platform instead: http://hackage.haskell.org/platform// This will get you going faster, have cabal all set up, and on Mac OS X give you full docs locally.
I can't help but feel that it is a bit silly to talk about these things as "first class modules". As Haskell programmers, we're certainly supposed to be taking types seriously, and yet these "modules" cannot contain types at all! Perhaps I'm missing something here, but this seems like nothing more than a record of functions. Records as modules certainly can work—but [as I mentioned recently](http://www.reddit.com/r/haskell/comments/wpeam/pdf_firstclass_modules_what_happened/c5ffk4v), you really do need dependent types to pull it off. Also Tekmo, I noticed that you cited your [Scrap Your Type Classes](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) post. Are you familiar with [First-Class Type Classes](http://mattam.org/research/publications/First-Class_Type_Classes.pdf)? It's more or less what you're suggesting, except more flexible (as you can do things like associated types) and doesn't lose the nice implicit syntax that type classes provide. Again, you of course need dependent types to make it work.
[OT] Can you, or someone, give a definition for 'pun' as it is commonly used in the Haskellverse?
Punning is when you are binding a variable with records, e.g. { foo = bar } either in a pattern match or record update, but if you have { foo = foo }, you can instead just write { foo }, that's a pun on foo. Saves some redundancy.
Ah, of course.
You can also use it to implement `runPure`: type Channel = State ([String], [String]) -- input and output channels read :: Channel String read = do (in_:ins, out) &lt;- get put (ins, out) return in_ write :: String -&gt; Channel () write s = do (in_, out) &lt;- get put (in_, out ++ [s]) runListF :: TeletypeF (Channel r) -&gt; Channel r runListF t = case t of PutStrLn s c -&gt; write s &gt;&gt; c GetLine cs -&gt; read &gt;&gt;= cs ExitSuccess -&gt; return undefined -- yes, this is a cheat! runPure :: Teletype r -&gt; [String] -&gt; [String] runPure t in_ = finalOut where (finalIn, finalOut) = execState interpreted (in_, []) interpreted = liftRun runListF t (I'm cheekily implementing Channel in terms of State just for the sake of being able to write it quickly. It is not a good implementation, especially because State doesn't have any arity-0 operations so we can't interpret ExitSuccess properly! But it serves the pedagogical purpose.)
Nice work. This is a simple and explicit design, which is very important in my opinion. Little nitpick : In my opinion, interfaces should be specified in their own module, and instances should be defined along with the data type. In other word, data types should depend on the interface, and not the other way round. Therefore, the "Data.String" module should not be actually published.
Why do you think it's 'evil'? It has saved me on a couple of occasions already.
Thank you. I will try that out. It will take me a while because right now I am working on a major release to `pipes`, but I will definitely work through those.
Isn't it what the extension DisambiguateRecordFields is for?
Would using a convention: module Thing.Module (Module (..)) where data Module = M { thing :: Int -&gt; Int } module Main (main) where import qualified Thing.Module as T import ThingTwo main = print (thing 1) where T.M {..} = thingTwo approximate data constructor aliasing well enough? Where every dictionary uses M as its data constructor name, then qualified imports get you a lot of the way there.
I love your approach, but don't throw defaults away! Actually you can still get something useful out of them: data StringLike s = S { pack :: String -&gt; s, ......} lazyBS :: StringLike L.ByteString lazyBS = S { ..... } strictBS :: StringLike S.ByteString strictBS = S { ...... } instance Default (StringLike L.ByteString) where def = lazyBS instance Default (StringLike S.ByteString) where def = strictBS Which enables you when type inference can do it (i.e. most of the time), just to say: stuff :: (L.ByteString, S.ByteString) stuff = (mk, mk) where mk = pack def [1,2,3] Even more concise! And you then have the choice between the two approaches! In all cases, still we're left with the problem of translating super-classes to that model...
No, you're still left with the problem of types which cannot be contained and still have to follow the regular Haskell modules system, and also with the module re-exporting stuff from other modules. (You can end up with a module type containing one/several fields which are themselves modules types, but then even with lenses I'm not sure of how much it'll be convenient)
You know who'd be entitled to comment on that? Michael Snoyberg.
LLVM 3 allows us to annotate memory loads with alias information and define our own type hierarchy that can capture our alias needs. We have an experimental implementation in the LLVM backend.
I haven't tried using this, but the documentation for DisambiguateRecordFields does not indicate that it will allow you to *export* multiple definitions of the same name. You can *have* multiple definitions of the same name brought into the same namespace, and with DisambiguateRecordFields you can even *use* them under certain situations, but you still cannot *export* them. Also note that RecordWildCards implies DisambiguateRecordFields. http://www.haskell.org/ghc/docs/7.4.2/html/users_guide/syntax-extns.html
It's because his reddit username is `snoyberg`, which is a bit confusing. I think I've made the same mistake before.
Oh, I know. He's `snoyberg` on GitHub as well. The joke of course is that it sounds like Zoidberg of Futurama fame. And in case someone had managed to escape it, there's a meme "Why not Zoidberg?". ;-)
[Lenses](http://www.haskellforall.com/2012/01/haskell-for-mainstream-programmers_28.html) make it easy to manipulate nested data types. You don't always have to use them, but it would significantly clean up your `handleDir` function.
&gt; Lastly, we’ll tell glpk to make each variable an Integer (since we can’t reserve fractional instances). Um, did we just make a leap from linear programming to the (undecidable?) problem of integer programming?
This is just an initial announcement. So: * I'm still working on fleshing out the language features. * I'm still working on the code gen. * I need to produce some benchmarks. Seeing as that web page is generated by a Haskell module which uses the compiler to produce the examples, it makes sense to also produce benchmark results. * I need to make the overall use for non-Chris-Done users more understandable, especially how importing modules work and the role GHC plays. * I need to flesh out the interaction between client and server (i.e. transparent serialization). * Need way more unit tests. * I want to put a web-based compiler for it online so that one can use it trivially in the browser. * It's really just Haskell, I call it "the Fay programming language" to refer to that-subset-of-Haskell, and maybe it can be marketed as a CoffeeScript alternative to non-Haskellers (and there is always the possibility of extending it beyond Haskell at some point, not sure I'd want to but it's a possibility). * I'm already using this in production, it works. But it's still alpha and there will be bugs. Anyway, give it a little try, your feedback will let me know what I need to add to the documentation and examples. There is a lot to explain, I realise many things might not work as I have described. I really wanted to keep this private until it was perfect, but decided to share, so be gentle.
What's the relationship to Jeff Epstein's original [Cloud Haskell](https://github.com/jepst/CloudHaskell/)? 
Very cool!
First thought. No EndPoint can be shared between multiple transports such as udp and tcp. Thus no node abstraction exists. In a large network multiple transports are probably deployed.
Distributed-process aims to be a successor to Jeff's remote package, one big difference being the availability of a swappable network transport layer. We can think of it as [Cloud Haskell 2.0](http://www.haskell.org/haskellwiki/HaskellImplementorsWorkshop/2012/deVries). Edsko's pretty much adhered to the API in the Cloud Haskell paper though.
Maybe "safe" and "unsafe" ought to be renamed to some more suggestive names about how the ffi mechanism works? Maybe "wrapped" and "direct" are more meaningful? Then they also become more searchable, and maybe new options could make sense too..
Seems like this was a fun project! Is there a syntax for "foreign" calls to javascript functions?
A thought: you could use the `XmlSyntax` extension in HSE and have XML literals compile to DOM node objects. Ultimately, I'd like the client-side and server-side code to be seamlessly interleaved, like in Opa. This could perhaps be achieved with a preprocessor, or at a minium there could be a QuasiQuoter for Fay similar to JMacro. I think we've discussed this on IRC. I'm not sure though if transparency is possible with Fay, though, since (by definition) Haskell and Fay are not the same languages. Perhaps you need something like GHCJS for true transparency, and then you get all the issues that come with it... I assume you plan to eventually look at type-checking with the GSoC HTE project? I wonder, in the mean time, if the use of GHC can be made more seamless, and if GHC can be told to only type-check without compiling. Anyways, Fay looks interesting and promising!
There's a `foreignFay` function, read the linked blog post. ;-)
Awesome. I'd love to see a solution that makes it easy to communicate values between a Haskell-based server and Fay-based client. For example, it would be nice to define the shared data-types in a single file that is read by Haskell and Fay, and have all the serialization/deserialization code generated automatically (for both the client and server sides).
Let's not forget to question the standard while we're at it.
An interactive interpreter, like http://play.golang.org/
I'm curious if your eventual approach will be to use GHC as more of library, by callings its compilation(typechecking) functionality instead of having to manually invoke both compilers. I could see making the process a lot cleaner that way. You may be way ahead of me or have a better way figured out, I just started looking at the compiler code. Looks like exactly what I've been wanting from a Haskell to JS compiler otherwise, and happy to see you have made a ton of progress on this idea. As someone who has been using writing a lot of Javascript in recent months this is a great improvement from writing vanilla JS or Coffeescript. 
Really excited to see this progress!
Wouldn't ConstraintKinds be cleaner for Serializable in place of the current need for UndecidableInstances, or is the package meant to build with GHC 7.0? (Someone needs to do a write-up on binary vs cereal, BTW!)
I'd be all for a two step process (first introduce the new names and depreciate the old ones, then eventually eliminate the old ones) for this. By now I know the difference, but for ages I had to look it up, and periodically there's a whole mess of confusion about what it all means. "I thought safe meant that the call was safe!" and the like. I don't know if "wrapped" and "direct" are necessarily correct, but safe and unsafe are already *way* too overloaded (as the whole SafeHaskell discussion on one of the lists revealed).
This looks awesome. When I get sometime I 'm experiment in using this to rewrite my firefox addons.
This looks **really** cool - just a couple of questions - does unbounded recursion work and how did you get it to in js and are you planning to build the fay compiler in fay so we can have a web playground thing?
That's pretty much what I did. I didn't find the FFI cookbook as useful as it could have been, it seemed rather disjointed, and I wasn't always sure what question they were answering. The FFI addendum is more readable than I expected it to be, but somethings aren't obvious (such as the fact that you cannot pass C structs by value). There is one thing missing from your list of resources however: Source code. I looked at the source for bindings to SDL and libnoise for solutions to several problems I encountered.
I really appreciate the "strict subset" idea. I've been really excited by Roy and Elm, but the subtle (or sometimes not-so-subtle) differences from Haskell really, really bug me sometimes. For Roy this is understandable, given it is basically Typed JavaScript with Haskell-like syntax, but nevertheless unfortunate. I'd appreciate an outline of which parts of Haskell are not included in Fay, and why. I suppose it's safe to assume that all language extensions are definitely out?
Chris, thank you for this. 
I was more thinking of a “send code / receive JavaScript” JSON service, but I don't see why that couldn't be done! Does anyone use inline scripts much these days?
Unbounded *tail recursion* works. E.g. the `forever` function will iterate forever in constant space. This is because forcing thunks is in [a limited kind of trampoline](https://github.com/chrisdone/fay/blob/master/js/runtime.js#L9). I'm not sure building Fay in Fay is feasible without extending Fay a lot more, for now I'm happy just to have a JSON service. I think that'll be (responsive) enough for a playground.
Indeed, I should document this more. There are [four kinds](https://github.com/chrisdone/fay/blob/master/src/Language/Fay/FFI.hs#L40) functions and methods, and pure and impure versions of each. 
Haha, I realised that later after publishing it. I replaced the word strict with “proper” as nandemo suggested.
Thanks, changed it.
&gt; A thought: you could use the XmlSyntax extension in HSE and have XML &gt; literals compile to DOM node objects. Good point! If anyone wants that it seems easy to add. &gt; Ultimately, I'd like the client-side and server-side code to be &gt; seamlessly interleaved, like in Opa. This could perhaps be achieved &gt; with a preprocessor, or at a minium there could be a QuasiQuoter for &gt; Fay similar to JMacro. I think we've discussed this on IRC. I'm not &gt; sure though if transparency is possible with Fay, though, since (by &gt; definition) Haskell and Fay are not the same languages. Indeed. Well certainly you can share code, but it has to be the subset that Fay understands, so you can have, e.g.: module Comment.Shared where data Comment = Comment { commentSubject :: String , commentContent :: String } validateComment (Comment subject content) = not (null subject) &amp;&amp; not (null content) &amp;&amp; length content &lt; 512 and then you can import Comment.Shared in both Fay and Haskell, no problem. So at a module-level granularity they can be interleaved. That said, declaration-level interleaving, seems tricky. &gt; I assume you plan to eventually look at type-checking with the GSoC &gt; HTE project? I wonder, in the mean time, if the use of GHC can be made &gt; more seamless, and if GHC can be told to only type-check without &gt; compiling. I wouldn't mind use HTE, indeed. Then I could support type-classes. For [my workflow](http://www.reddit.com/r/haskell/comments/wxi3l/fay_programming_language_a_strict_subset_of/c5hl0ty) the work with GHC is OK, but yeah I looked around, I could've sworn there was a flag like -type-check-only for GHC but must've imagined it. I can add an optional GHC API type-checking inside Fay. Actually, I think it would be quite effective because you don't really have to worry about packages at the moment. I'll make a [ticket to add this](https://github.com/chrisdone/fay/issues/4). (I did start with trying to use the GHC API instead of haskell-src-exts, but it's so undocumented and quite difficult as an API, I also couldn't seem to get hold of the type information that I wanted. But as a straight-up type-check, no problem.)
That is precisely what I'm doing! Admittedly, the serialization is [based on Show/Read instances](https://github.com/chrisdone/fay/blob/master/js/runtime.js#L131), so it's not exactly fast, but data-typeable could work, too. So, exactly, I have this: module Confy.Types.Shared where Here's the routing type: data Cmd = Ping | GetSub Int | GetMsgLog Int deriving (Read,Show) instance Foreign Cmd Here's some database entity: data Sub = Sub { subId :: Int, subAbstract :: String, … } deriving (Read,Show) instance Foreign Sub Then I have my route dispatcher (it's very small at the moment, I'm still in testing-the-waters phase!): dispatchFay cmd = case cmd of Ping -&gt; run (return "Pong!") GetSub sid -&gt; run (db (getSubmissionById sid)) GetMsgLog cid -&gt; run (db (getMessageLog cid)) I have the client-side querying function: -- | AJAX query. query :: (Read a,Foreign a) =&gt; Cmd -&gt; (a -&gt; Fay ()) -&gt; Fay () query = foreignFay "app.query" "" And finally I use all this together in, say Confy.Client.Submission: submissionDetails :: Int -&gt; Container -&gt; Fay () submissionDetails i container = do query (GetSub i) $ \sub -&gt; do case sub of Nothing -&gt; return () Just sub -&gt; newText (subAbstract sub) &gt;&gt;= addChild container … And that's it! Make sure to import Confy.Client.* into your server's Main to ensure both client and server are type-checked together! This code [renders the content section of this page](https://dl.dropbox.com/u/62227452/Screenshots/Screenshot%20from%202012-07-22%2010%3A33%3A47.png). I'll make a proper runnable example of this and put it on the site at some point. It's still rather experimental, some design decisions to be made.
Nice! In the "Comparisons to other methods" section, it would be nice to compare it with other functional languages' capacities as Javascript generators (or at least mention them). For instance Clojure can be compiled to JS too. (Basically: loose type-checking, win the macros)
[Done.](https://github.com/chrisdone/fay/issues/3) For the ones that are huge I made them collapsed by default.
The Fay compiler needs to be written in Fay, so that compile-in-browser.js can be compiled from Fay.
&gt; My only concern is that a lazy language will make it too slow in older browsers, and even an unavoidable performance hit in newer ones. Any reason not to support a strict evaluation mode? If indeed there is an unacceptable performance hit I don't see a reason not to support optional strict evaluation, technically speaking. A module-level strict evaluation mode, I presume? What's the precedent for that kind of thing? Of course, qualifying all this discussion that there are no benchmarks (suite) yet… similar to the Fay→Javascript examples generated with the documentation I'll make a Fay vs JavaScript series of benchmarks with the spec of the machine, JS engine, etc. then we can resume the speed discussion with some hard numbers. (I like the idea of the web site being generate-able from the project source serving both as documentation/introduction and a report of unit tests/translations/benchmarks that a developer can view while hacking.) There's also output-size to think about. I've been very Closure-Compiler-conscious writing this, I want to be sure that it compresses well, e.g. (here's an old paste back before it was called Fay) http://hpaste.org/raw/68191 and here is the complete compiler output including the runtime: http://hpaste.org/raw/68192 You can copy-paste that into firebug/chrome developer console and it works. Maybe I will add compression size to the docs output, too. Nicely, Closure can optimize away redundancy in some places, too, e.g. $ echo 'console.log((function(){ var x = 1; return x; })());' | closure --compilation_level=ADVANCED_OPTIMIZATIONS console.log(1); For really hard speed hits, I'm personally not against writing a bit of JS and using it from the FFI… provided it's small. ;-)
Very exiting project.. Nice it is 'just' Haskell. One to follow closely. I think a year from now the yesod/snap frameworks will have good implementations of one-or-more of these Haskell(-like)-to-JS compilers as part of their tool chain, allowing us to share sever and client side data structures.
Needs? Is there a reason why that would be an advantage?
The fay script element could have a src attribute as well. There is an [example in the Coffeescript docs](http://coffeescript.org/documentation/docs/browser.html) that does exactly this. Inline script blocks are probably not that usefull. I use them sometimes to send data to the client along with the initial page load, like translated strings, or to call some initialization code optionally depending on the server state. 
Very cool. One general question: Could a Cloud Haskell based topology replace a queue-based multiple workers setup? It's an interesting thought to have an unspecified number of slaves idling for work and one or more masters sending them work to do.
No, EndPoints cannot be shared across transports (EndPoints are transport specific). But it is possible (though not concurrently implemented) for Cloud Haskell to use multiple transports underneath the hood.
Thank you. This is easily the most important development in haskell in the last couple of years as far as Im concerned.
If performance ever becomes a problem, this could actually become a good example where Boquist's GRIN could come in handy as an optimisation backend. It requires access to the whole program, but that seems to be standard in JS, so that's not a big downside. The advantage of it is that it compiles things down to first-order constructs and that may be easier to optimise for JS compilers.
They just fork more parallel processes. If you wish, of course, you could implement some sort of protocol to talk to the slaves to see if they're busy (run some kind of service process on the slaves). Alternatively, you could run a service process on the master and have the slaves query the master for work when they're done, so that you get automatic load balancing (work stealing).
Actually, I'm not using language-javascript… [I've removed it now](https://github.com/chrisdone/fay/commit/e53addca4f0dd38d8bb7ab948e8b72511a1e2ea4) that you mentioned it. * I dislike [the AST](http://hackage.haskell.org/packages/archive/language-javascript/0.5.4/doc/html/Language-JavaScript-Parser-AST.html) because it lets me generate complete garbage. Doesn't even distinguish between statements and expressions. * [HJavaScript's AST](http://hackage.haskell.org/packages/archive/HJavaScript/0.4.7/doc/html/Language-HJavaScript-Syntax.html) is not much better, and its pretty printer is broken. Don't use it. * HJScript's (which relies on HJavaScript's AST) pretty printer differs, and is better, but I wasn't sure I wanted to use it this time, it can be quite restrictive and stop you being able to do something right when you've written half of your stuff in it, if it doesn't support something (e.g. ===) you have to patch the library, making it a rather annoying dependency rather than a helpful one. * JMacro is nice for writing JS. But manipulating it as an AST it would get in the way, I think. Kind of like when you're writing a TH macro and trying to use the [| … |] and [t| … |] syntax and finding that half of the time you're constructing the AST manually and might as well not bother. I ended up defining [an AST that suited my needs](https://github.com/chrisdone/fay/blob/master/src/Language/Fay/Types.hs#L74), it was simple and I found it was nice that it only contained things that the compiler actually used (and I could add some non-standard-JS constructs one might consider syntactic extensions but that really print to normal JS.) &gt; Both jmacro and language-javascript seem to have pretty printers. Dunno about jmacro, does it have a parser? language-javascript calls it a "pretty" printer, but it actually just prints the damn thing on one line. That's in fact why I wanted to use it, for its claimed pretty printer, which [it doesn't have](http://hackage.haskell.org/packages/archive/language-javascript/0.5.4/doc/html/src/Language-JavaScript-Pretty-Printer.html#renderToString). The Print module in Fay did do pretty printing first, but then I changed it to a flat output deciding to leave that choice to the user. &gt; Random idea: an "interactive" Fay shell that pretty-prints the resulting JS could be nice Nice idea. I might do that, it would be nice for hacking indeed. 
Oh, damn. It doesn't seem to support word-wrap. Damn. I turned off the max-width. Oh well.
Lovely! Small notes: Most, of not all, of your == operators in the generated JavaScript should be ===. Also, you should consider emitting "use strict"; declarations. 
Ok so I did $ cabal install fay $ cabal unpack fay $ cd fay-0.1.0.0/ $ fay-tests And I got tons of errors. Then I tried compiling and running one of the examples, during which time I did a `sudo apt-get install nodejs`, all of which worked flawlessly. Then I tried fay-tests again and they worked. So I'm guessing that fay-tests rely on `node` being installed? Perhaps you should clarify that in your docs.
I did some thinking about how we can reuse functions that make static guarantees for dynamic data.
Excellent post! I was struggling on the exact same problem just recently. What a coincidence! Thanks for giving me the insight.
Thanks for reading it! It was one of my major stumbling points in reading the dependent types literature, so I'm glad I was able to help.
Hm, do you have `white-space` set?
[Relevant](http://skillsmatter.com/podcast/home/silkapp-a-case-study-in-creating-rich-internet-apps-in-haskell/js-3880).
That would be awesome!
Thanks for the tip - YUI looks like it's moved on massively since I last looked at it (about 3 years ago)!
No, you can see it in the Style frame in your developer bar.
Thank you. I think fay has great potential. It seems usable even at this early stage, compared to the hurdle of setting up ghcjs/uhcjs. The simplicity might help it gain the traction it needs. I just hope it won't be abandoned.
How about doing this by creating and consuming the proof inside the EVect functions? Something like this: data EVect :: * -&gt; * where EVect :: VecIndex (Vect a) n =&gt; Vect a n -&gt; EVect a deriving instance Show a =&gt; Show (EVect a) data NatIx n where ZI :: NatIx Z SI :: NatIx n -&gt; NatIx (S n) class VecIndex v n where index :: v n -&gt; NatIx n instance VecIndex (Vect a) Z where index _ = ZI instance VecIndex (Vect a) n =&gt; VecIndex (Vect a) (S n) where index (_ :&gt; v) = SI (index v) evhead :: EVect a -&gt; Maybe a evhead (EVect v) = case index v of ZI -&gt; Nothing SI _ -&gt; Just (vhead v) This is more similar to how you'd write this in Agda, I think. It also means you don't have to change your vector functions (to an, in my opinion, more unnatural form). It's a pity about the class constraint that is needed on EVect. I think something similar to this can be done with the singleton types in GHC HEAD, but I haven't figured out how they work yet.
There's a small typo in your post: in `fromList`, you use the constructor `Evect` instead of `EVect` (with a capital V). Also, I think for this to be valid literal Haskell, you have to put the extensions above the module header (at least, for me it only works that way).
Shameless self-promotion, but I've also done some work on what I thought you meant by "flexibly-phased constraints" which may be of interest: http://www.doc.ic.ac.uk/~wlj05/files/Deconstraining.pdf.
Yeah, so there is no type-classes support, so there is no real `show` deriving. But there is a function in the runtime, [`Fay$$encodeShow`](https://github.com/chrisdone/fay/blob/master/js/runtime.js#L131) which external libraries can use to render your Haskell into a string. I [just added it to the stdlib](https://github.com/chrisdone/fay/commit/66cf2988839e8be3331cb82b0d9dd5bcccff2f2b) that comes with the runtime to make sure it's there. The things placed in hs/stdlib.hs are things that are somehow built-in in GHC and can't be defined within the Fay code without causing conflicts, so instead we pretend to use the GHC one for the type system, but provide our own implementation for the runtime. Short version: now you can use `show`. External libraries can also access `Fay$$encodeShow` via `Fay.encodeShow` if they want to serialize something. So when sending data to your server, you can use: `show` from Fay or `Fay.encode` from within JS. When sending data from your server to your client, you can use [Language.Fay.Show](https://github.com/chrisdone/fay/blob/master/src/Language/Fay/Show.hs) which renders your data type into a value that, when evaluated in Fay, gives you a Fay value. Use Fay.eval() to get that. This part is not very well thought out. It works, but it's not great, needs more documenting. Here's what I am using at work: confy.dbquery = function(cmd,func){ // console.log("Sending: %o (%o)",cmd,client.encodeShow(cmd)); $.ajax({ type: 'POST', url: '/confy.json', data: $.param({ fay: client.encodeShow(cmd) }), processData: false, success: function(payload){ var value = client.eval(payload); // console.log("Receiving: %o (%o)",payload,value); // console.log("Fully forced value: %o",client.encodeShow(value)); client.force(client.force(func)(value)); } }); }; (where client is a Fay instance.) and then -- | AJAX query. query :: (Read a,Foreign a) =&gt; Cmd -&gt; (a -&gt; Fay ()) -&gt; Fay () query = foreignFay "confy.dbquery" "" Again, I need to document and flesh all this out. If it works for you, all the better.
Couldn't you generalize that same approach to "statically" compiling code at run-time? You just create a data type with the abstract syntax tree of the code, pass it to the compiler, and if the compiler can successfully coerce the syntax tree to the desired types it `Just` returns a data structure containing the compiled functions, otherwise it returns `Nothing`.
Thanks! I look forward to reading it. EDIT: Started digging into your paper, this is really quite awesome. I was actually thinking about this problem last weekend; I'm glad you've been working to address it.
It's post like these that make me feel too stupid for Haskell. :s
I mean you probably should set `white-space` to something that allows wrapping since the default for `&lt;pre/&gt;` is not to.
Agreed, I've always felt that the names safe and unsafe are rather unintuitive.
For those looking for a changelog, see the [hackage repo](http://hackage.haskell.org/package/cmdtheline-0.2.0.0).
Ah, ok. Odd that Chrome and Firefox differ on this, quite rare. I guess it's not a big feature.
nice lib!
FWIW, upgrading to the latest Mac OS X Haskell Platform seemed to resolve my issues (I had the same problem): the 2012.2.0.0 release of the platform has a later rev of cabal and cabal-install that is unaffected by this issue. Previously, I was running on an older Haskell platform.
I found the same problem while building fay, and used the same solution as you.
You have a small typo in the argument description text: `&gt; , posDoc = "A list of files to read in. If empty, read` **form** `stdin."`
Hmm, it works for me... Try the cached PDF link.
I've been playing around with `FreeT`, and after much toil I (believe I) managed to replicate Conduit by starting with Pipe and adding one feature at a time. I'm going to clean up the code and release it as a series of blog posts detailing each step from Pipe to Conduit; hopefully those interested in these libraries can glean some tidbits of wisdom regarding how the internals work. (When I say Pipe, I mean as in Control.Pipe from the pipes package. I don't plan on making any mention of Frames.) The first step was to introduce the file that I initially called "Fun.hs", with `:&amp;:` and :|:`. I expanded it to be a preliminary discussion of functors; I found it quite amazing how much can be accomplished by merely combining these simple functors algebraically: my final result uses nothing but a composition of the four functors (identity, empty, const, and function) and two combiners (sum and product) presented. I suppose in the end that's all Haskell data types are, anyways. If at any point in this blog series, you notice that I've done something wrong, or left any loopholes in my code, then by all means let me know! I'm not entirely certain of the accuracy of my code for later posts that deals with aborting pipes and finalization, but I'll do my best to clearly explain what I've done and why. Perhaps by then I'll have done enough equational reasoning to be more certain of my methods.
thanks!
So I also managed to do this. It turns out the trick is to layer each feature as a monad transformer outside the Pipe type and then lift composition over the transformer. With this I've managed to layer error handling, finalization, and parsing, all of them building very simply on the previous category's proof. This actually results in superior finalization semantics because now you can close and reinitialize upstream resources if they expose such a functionality. The only thing I can't quite elegantly extend from frames to ordinary pipes is folding, unless you permit the pipe to fold multiple times.
&gt; It turns out the trick is to layer each feature as a monad transformer outside the Pipe type and then lift composition over the transformer. Now that is a trick. I did not manage to do this; as I add each feature, I have to tweak pretty much all the code in sometimes subtle ways. I'm really excited to see what you've done! I'll be cranking out the posts of this series about once a day, so feel free to refer to my material if it helps you at all, even if only to say "look at the dumb way Dan did it, here's a much better way!" :)
Yeah I don't think it's a huge deal. If you're ever looking for a job it might be worth fixing, because anyone who's forced to use IE at work is seeing a much shoddier site than you are.
This may be true for the final product, but one of the main challenges here is satisfying properties that enable reasoning about code. One route to that is to take the easy approach for a model implementation, and then build a more high-performance implementation by inlining, specializing, and simplifying the model. See various FRP frameworks for another example of the same technique.
I am not commenting the approach but the claim that all conduit features are replicated. Performance is a conduit feature just like error handling and finalization. Throwing it out of the picture and claiming that all conduit features have been recreated is not an accurate statement. 
What he's saying is that we use the simple implementation to prove the laws are correct, then once that is done we can hand-write the final implementation to inline all those transformers to get the performance back.
Ideally, "hand-tune" rather than "hand-write" if possible. Specialization and inlining can be compiler directives (in some cases). This could be a nice use-case for the HERMIT plugin, especially if a theoretically sound, correct by construction, modularly implemented (have I used enough buzzwords?) version of conduit could be made comparable in performance to the current one.
While seemingly just a shameless plug, this is also a request for any comments, as I'm currently incorporating it into a larger piece of work and would relish any insights/criticisms the community as a whole has. Cheers! EDIT: I forgot the [PDF] warning, so this is a resubmission. Apologies!
So my experience is that GHC does an already amazing job of optimizing code and to use profiling to prove where bottlenecks occur. For example, today I just discovered that the non-codensity implementation of free monads does not suffer from accumulating left-associative binds when compiled by GHC, something that defies conventional wisdom. I plan doing a write-up about it soon.
Very soon, please! I'm surprised by this claim. Wouldn't this also imply that the simpler cases, like `(++)`, also don't suffer from this problem (and it's easy to verify that they *do* have this problem)?
Why are you case matching on Bool?
I find it reads better when the body of each branch is a do block, but I refactored away from that anyway. Would you use if/then/else instead, or something else? I should note that omitted the other branch in that post, but there was more code for each opposite condition.
You could also have used a guard in the case: case grant' of Nothing -&gt; -- handle error here Just grant | authGrantRedirectUri grant == accessTokenRedirect tokenReq -&gt; | otherwise -&gt; -- other error
So what I observe is a constant speedup, but no difference in time complexity (both linear). The crazy part is that the core does NOT show any (significant) optimization that makes this possible. It does almost nothing to the library code, suggesting that whatever black magic making this happen is occurring below the level of core. However, every test and benchmark I've run shows that it is happening ... somehow. It may be a while before I write this up because I want to make absolute sure that it is real and that involves reverse engineering the assembly to prove that the optimization is actually happening. However, out out of all the tests I have done the most compelling evidence is that I can let it execute statements 10^10 binds deep with no observable slowdown between steps. You'd expect that if the binds were left associative and each bind had an overhead of even a single instruction that you'd start to notice a slowdown by this point, but it is still just as fast. I only noticed all of this because I'm about to spin off `FreeT` into its own library and I am releasing both the implementation found in `pipes` and a codensity-transformed version, and I wanted to see for myself the left-associative bind effect to justify including the codensity version.
This is really cool! It had never occurred to me to store code in the `Left` value. I think `EitherT` is perhaps one of the most under-appreciated monad transformers and if it catches in popularity and replaces `ErrorT` we might see a renaissance in its usage.
In this case it's more like an eerily smart compiler. GHC is well on its way to the singularity. ;)
Brilliant!
That's exactly the type that the author uses, at the beginning of the post. But it only supports compile-time verification of non-emptiness. Later on he changes it in order to be able to pass in a non-emptiness proof as a parameter in the dynamic case, so that the same function supports both compile-time and run-time verification of non-emptiness. That's the whole point of the post. Do you have a different way of supporting both compile-time and run-time verification of non-emptiness with the same function?
I'm not saying that there should be a master list... but there could certainly be better tools for finding good quality packages than Hoogle and Hayoo. Ideally, I'd like to have packages arranged by: 1. Topic 2. Reverse dependencies (and *their* reverse dependencies, etc.) 3. Longevity 4. Frequency of updates 5. API stability 6. Some kind of package rating system 7. Some kind of package maintainer score - the fact is, packages maintained by active community members are likely to be high quality than a package from a graduate student who hasn't been seen in 5 years. 8. ALL of these should be traced into reverse dependencies of the package as well; high quality packages are often those that depend on other high quality packages, stable packages are those with stable dependencies, and so on. Most of this could be automated; even categorizing packages by topic could be partially done by looking at the dependency graph: packages that share a lot of unique dependencies are likely to address similar topics... the tricky part would be tweaking and getting agreement on how to combine all of this into a conglomerate list (and lists for each topic area... etc.)
That doesn't seem to help the problem of walking conditions though. Also, if a condition requires some IO (such as getting the current time), you'll lose the association between fetching that value and using it in a guard. Nice to be reminded of other methods though!
I've made some good use of the fact that anywhere you see an `a`, that `a` can be an `IO a` just as easily. `STM (IO a)` can be fun. It also is a good way to demonstrate to people who claim that `IO` is "just an imperative language hiding in Haskell" that they are wrong. No, it really has full first-class support as values, and you can do anything you like with IO values before finally evaluating them. "Direct" execution of IO is at worst a special case, and never really the right way to think about it. Mind you, it might not be a _good idea_ to worm IO through your system in a given way (what I did with `STM (IO a)` I'd probably do with a free monad now, courtesy your writing), but it works.
PDF, 35 pages makes it sound scary. It's just a set of slides that took me maybe 10 to 20 minutes to read through. Is there somewhere I can go to read more about "partial lenses"?
[insert obligatory question about Hackage 2.0 here]
Indeed, I'd rather the install fail due to an actual compilation error, rather than "failing" because the package version numbers were over-constrained.
"Failing" is not necessarily at compile time. Following the [PVP](http://www.haskell.org/haskellwiki/Package_versioning_policy), API changes don't necessarily mean the code wouldn't compile anymore.
I think that's what he means -- let the code fail to compile, don't stop it from starting because of version numbers unless the author specifically knows that a certain version of a package will not work.
In that case a note that it is not worth the effort due to the target audience should do too.
What baffles me most, is that package management is a very common problem and there are 100s of good solutions (pacman, apt-* -- basically each linux distribution has its own!): so why could not hackage be organized in the same way? For example, with pacman you are *strongly* suggested to do a `pacman -Syu` (a full system upgrade) before installing any new package. This is because the following might happen: Suppose you have Package B installed, where: Package B depends on Package C version 1 Now you want to install a new Package D where: Pakcage D depends on Package C version 2 Unless you upgrade all the packages that depend on Package C, if you install the new Package C version 2, you'll break the old installed packages. That's why it's recommended to do a `pacman -Syu` first. But this implies that there is some authority "signing off" a set of compatible packages to be released. Hackage, on the other hand, seems to permit incompatible packages to coexist. In other words, there is no concept of "release" on Hackage. I reckon there should be.
The Haskell Platform was supposed to be our equivalent, but it has stalled due to process issues (e.g. endless mailing list discussions.)
I disagree with the "nor should there be" part. The haskell platform is for things which are the blessed, correct, tried and true, etc. solution to a problem, but it suffers similar downsides to Debian Stable. Packages which are solutions to common but not ubiquitous problems (like split), or for which there is dispute over which approach is best (the lens libraries, iteratee, conduit) should not be in the platform, but may still be important to know about. There are also packages which solve difficult problems well (like reactive-banana or diagrams), which haskellers should be aware of in case they ever find themselves trying to solve those problems.
fclabels has a good explanation inside the docs (disclaimer: I am one of the contributors)
I honestly believe 99% of the problems come from the fact that it is the global package database (worst idea ever - even if I have some idea why it was chosen). cabal-dev solves the vast majority of the problems. But in and of itself it is not perfect - because packages can still remain in the global database. In addition, it is a wrapper to cabal that has gotchyas and is far from perfect. I truly believe that while yes there are still version issues on Hackage, the vast majority of problems could go away with a more sane design for cabal. If only I had a couple months of free time :)
It would help if more packages were searched by Hoogle. 
I've been meaning to do something like this for awhile and since you ask I will try to get it done. What I've been planning is something like this: 1. Keep a list of maintained software. This list plus a brief description of what each package does would be available on a website somewhere. 2. Use this list and the hackage rss feed to automatically download new releases 3. Automatically compile anything that depends on the new version, sending errors and warnings to my email account. 4. Auto-run the testsuite, if there is one, sending log to myself as above 5. Auto-build the documentation and haddock documentation. 6. For each successful build produce patches for the cabal files and attach them to emails so I can just send them. Just keeping the packages up-to-date shouldn't be too hard. I also intend to update documentation and learn more about lightweight dependent typing and equational reasoning and using them to hunt down bugs. I will eventually create automatic benchmarks for packages that I'm maintaining but for now I'm just going to worry about making stuff compile cleanly. EDIT: Something I would like to see on hackage is better categorization. For example distinctions between toy implementations and more mature ones. Maybe some of kind of coloring to indicate how maintained something is. A place to keep software for historical purposes with indications that they are obsolete.
The reason I said what I said is that "A framework for X" is (usually) a thing that helps you make Xs, but is not an X itself. A website framework is not a website. This _is_ an RPC protocol, with a server, client, and serialization protocol. It is not a thing that helps you make RPC servers, clients, and serialization protocols. What it helps you make are APIs, which is what RPC libraries do.
rpc-framework helps you make remote procedure calls. It is not a set of remote procedure calls. 
It would be more sensible to change the name or the type of the function or module when the semantics change.
Is this is any position to be useful for real sites or is it mostly just to prove it can be done? I don't do any web design so I haven't bothered with js beyond the necessities. Is 300KB considered large for a JS lib?
There's also hayoo: holumbus.fh-wedel.de/hayoo/hayoo.html 
[rpc-framework](http://hackage.haskell.org/package/rpc-framework-0.0.0.1) is now on hackage.
This is sort of contradicted by the point about runtime errors at the end.
&gt; I reckon there should be. So, are you volunteering to make self-consistent snapshots of all of Hackage, then?
&gt; Some automated notification system that a package maintainer gets an email when a required package has a new version uploaded? http://hackage.haskell.org/package/packdeps/
partial lenses (and partial lens families) can be implemented as just van Laarhoven lenses over Pointed rather than Functor. Note that a partial lens is not a lens, it is something weaker, as there are more of them. forall f. Pointed f =&gt; (c -&gt; f d) -&gt; a -&gt; f b In practice, as pointed is usually an awkward one-off concept and doesn't give you the laws you really want to combine the results, you probably want to weaken things further still and use forall f. Applicative f =&gt; (c -&gt; f d) -&gt; a -&gt; f b which gives rise to a multilens family. The most obvious multilens family is 'traverse'.
This is an excellent series. Thanks for posting this, and please do post the rest.
Wouldn't Applicative be stronger since it is a subset of Functor?
Just be careful when discussing folding because I don't think anybody has found a way to implement it that doesn't violate the category laws yet for the strong congruence laws, although I might be mistaken. Just make sure to include a caveat so that people are warned. The pathological scenario is when you manually feed a Pipe a final result type (i.e. Left u, or something similar), followed by a normal result type (i.e. Right i). I suspect that the strong solution to folding is actually to let a pipe fold multiple times but I'm still experimenting with other ideas, too. Also, I'm going to be on vacation for the next couple of weeks but I will do my best to give feedback. I love these kinds of educational series and I think it's a great idea, so keep up the good work!
Going through some of my projects, you might check out: containers, unordered-containers, directory, hmatrix, mtl, transformers, HTTP also check out gloss, it's a gem
upvote for gloss, its quite elegant
And if you want it done for you: http://packdeps.haskellers.com/
It would seem you're using Literate Haskell to write your blog entries; the resultant posts look lovely, but it would be nifty if you could strip the leading `&gt; ` from the preformatted code blocks.
btw these are package I use a lot, I didn't write any of them :p
the interface is really simple and nice, and if you look through the source there is quite a lot of work that has gone into it
* base * safecopy - derivable, versioned serialization. Save yourself headaches. * acid-state - ACID state IO actions * random-fu - random variables for a fair number of probability distributions. * free, operational, prompt - free monads * fclabels - derivable functional lenses (getter, setter fields for values) * split - list splitting * enumerator and friends - constant space lazy IO
If you tell us what specific features you miss from Python we can let you know if there is a similar package on Hackage.
&gt; Just be careful when discussing folding because I don't think anybody has found a way to implement it that doesn't violate the category laws yet for the strong congruence laws, although I might be mistaken. Just make sure to include a caveat so that people are warned. By adding the "upstream result composition" feature, we will have already stepped away from the ability to provide a newtype with a Category instance. I'll try to give some explanation in this regard, but since the "goal" is to reproduce Conduit, the explanation will essentially be that we have abandoned the promise of Category-like behavior. &gt; The pathological scenario is when you manually feed a Pipe a final result type (i.e. Left u, or something similar), followed by a normal result type (i.e. Right i) This manual power isn't typically exposed by a pipe's "primitive operations", though it is nevertheless a concern if we want our code to be "correct by construction". &gt; I suspect that the strong solution to folding is actually to let a pipe fold multiple times Hm, sounds like connect-and-resume. I wasn't going to include that in the series, but once I get to abort handlers I'll look into it. Prior to starting the series, I wrote all of the code through the end of the series, sometimes barely understanding it as I did so. Now that I'm taking the time to explain every inch of the code, I am deeply solidifying my understanding of what it is and how it works. &gt; Also, I'm going to be on vacation for the next couple of weeks but I will do my best to give feedback. I love these kinds of educational series and I think it's a great idea, so keep up the good work! Thanks, I appreciate your feedback and support! As we add features, we'll be stepping towards Conduit and away from Pipe, one baby step at a time, so don't hold back on the criticism at any point where I "step over the line".
Just to prove that it can be done. If you have a large application written in Haskell and want to run it in a browser then GHCJS should be able to run it. 300KB minified and zipped is a lot, GHCJS does what I think should be called "forest shaking" which may help for some applications (by allowing you to lazily load just the parts of the code you need). Fay is a better bet if you need a small fast JavaScript output that relates more closely to the Haskell source.
It turns out I was really close to having it working on Sunday, I just had to fix a couple bugs in the GHCJS rts... https://github.com/ghcjs/ghcjs/commit/b02c4cddcf82e291696cfc9fbfe746e53d1b795c
&gt; It will automatically determine when an upper bound on a package prevents the usage of a newer version. I want exactly the opposite: my package broke because it **didn't** have an upper bound, and the easiest fix is to add the bound.
&gt; The two favorites right now are cabal-dev and virthualenv. virthualenv is now called [hsenv](https://github.com/Paczesiowa/virthualenv/issues/32).
I've been using the pcap package recently found that the main bottleneck was in the toBs function. Processing a pcap file was quicker when I just used attoparsec (inside conduit) on the whole stream. What unbelievable things have you seen?
Yeah, a bit of unnecessary legacy. Could as well just be Maybe.
Its a positive/negative position thing. You are required to be polymorphic over the f, so its being given to you 'adversarially'. Every lens is a valid multilens, but not every multilens is a valid lens. You lose laws going to multilens. Knowing all the lens could do was fmap and that get/put were tied by lens laws was a pretty powerful thing. When you go to Pointed lenses, you actually lose one of those laws because now, someone can punt: constML :: Pointed a =&gt; (c -&gt; f d) -&gt; a -&gt; f a constML _ = pure and so you lose the law that what you put was stored. When you got to multilens you lose even more. (But it doesn't seem like the delta between Partial lenses and Multilens is so bad). Its like the gulf between Pointed and Applicative where you should probably have been using Applicative anyways. =) See https://github.com/ekmett/lens/blob/master/src/Control/Lens.hs for a ton of multilens examples.
The communities of established languages can afford to keep APIs compatible for ages because the speed of development can be slow without making the language unattractive due to lack of libraries. I don't think Haskell is quite at the point yet where we can sacrifice innovation on the altar of backwards compatibility yet.
&gt; You would need to rebuild only the packages that depend on the package just uploaded. I think that 99% of the packages on Hackage do not have any other packages depending on them... But they all depend on the other 1% so if one of those changes you would have to rebuild a lot of packages.
I was very impressed yesterday when I had to make a minor change on a Scala project from one of my coworkers who is on vacation. A simple sbt update downloaded the correct compiler version and packages to build it without any troubles. Perhaps something like that would be good for Haskell too, where cabal is the only thing installed manually and even ghc is installed by it locally.
Hackage isn't supposed to be a self-consistent set of approved packages. That's what the Platform is for. Anything else you grab off Hackage is up to the user. While this may be a real pain, it's much easier for a small community to sustain.
&gt; Do you want to prevent a plane to fly only when you're sure it can't fly, or do you want to allow it to fly only when you're sure it can fly? I prefer the later. It depends what you're trying to do. The idea that every system must be maximally safe actually leads to fragility, not robustness, because it leads to failures in cases when failure might not be necessary.
I don't think that's true across the board. Names are important and sometimes you may have gotten the name right, but the semantics wrong. If you have established a nice consistent naming convention, then changing the name could be more costly than changing the semantics.
I was just looking into it, and came to the same conclusion. First, assume that any Pipe can be split into two components, its initial effect `e`, and its "true" value `p`, which is either `Return r`, `Yield o next`, or `Await f1 g1`, such that the pipe = e &gt;&gt; p (e &gt;&gt; p) &lt;+&lt; idP ==? e &gt;&gt; p From the definition of `&lt;+&lt;`, it should be relatively plain to see that the order of effects will be preserved, so let's ignore that, and then just choose the case where `p` is `Await f1 g1` idP = Await f2 g2 where f2 i = Yield i idP g2 u = Return u Await f1 g1 &lt;+&lt; idP Await f1 g1 &lt;+&lt; Await f2 g2 Await (\i -&gt; Await f1 g1 &lt;+&lt; f2 i) (\u -&gt; Await f1 g1 &lt;+&lt; g2 u) The regular portion of await does prove to be the same per the inductive hypothesis (\i -&gt; Await f1 g1 &lt;+&lt; f2 i) (\i -&gt; Await f1 g1 &lt;+&lt; Yield i idP) (\i -&gt; f1 i &lt;+&lt; idP) (\i -&gt; f1 i) f1 But the upstream-handler does not. (\u -&gt; Await f1 g1 &lt;+&lt; g2 u) (\u -&gt; Await f1 g1 &lt;+&lt; Return u) (\u -&gt; g1 u &lt;+&lt; Return u) We now must prove that `Return u` is the right-identity of &lt;+&lt;, which it clearly is not. However, if you add assumptions about either 1) the nature of p (e.g. it never `Await`s after receiving the upstream return) or 2) the context in which this pipe is being interpreted (e.g. it is only ever in the context of either a larger pipe composition, or the context of running a pipeline), then I believe you can prove that under those restrictions it always behaves like a Category. 
I'm putting these out about 1 a day; am I supposed to interpret the downvotes as meaning you don't want me to post each one to /r/haskell? For the record, I [was asked](http://www.reddit.com/r/haskell/comments/x4si1/pipes_to_conduits_part_1_yield_and_await/c5je6bv) to post the rest, so please also ask if you would like me not to.
Defining that function is trivial: nonEmtpy v _ = v As you see, you don't even need the proof that `n` is not zero, since you're already saying that in the constraint `n ~ S m`. So this function really doesn't add anything useful. You could wrap the resulting vector in an existential over the index that allows you to later recover the non-zeroness: data NonEmptyVect a where NonEmptyVect :: Vect a (S n) -&gt; NonZeroVect a You can then pattern match on `NonEmptyVect` later and use things like the original `vhead` on the contained vector. I'm not sure how useful this is, though.
I should have given the submission a more controversial title to encourage discussion: "Fixing Haskell with Templates".
To do a good job of warning about shadowing, you really need a version control system that understands the lexical scope of the language, and says "hey, you introduced this binding here which captured a variable over here." This approach can also produce the cowarning, "hey, you removed/renamed this binding but forgot to remove/rename this reference to it, which happened to now reference another binding instead of producing an error" By comparison, a compiler just doesn't have enough information available to it to determine whether or not name shadowing is likely problematic. And 99% of the time, warnings should be treated as errors. Otherwise, why are you producing a warning? Suppress or fix it. Prohibiting name shadowing breaks substitution and modularity. Also, I often use shadowing intentionally when I know I don't want to use a given value inside a given expression, and I'm binding another variable where that name fits. It prevents me from accidentally referring to the wrong variable, and it tells me later that I don't have to look deeper for uses of a given variable. For example, you can see this in some of my open-source work in [nubBy](http://hackage.haskell.org/packages/archive/data-ordlist/0.4.5/doc/html/src/Data-List-Ordered.html#nubBy) and [sendMany](http://hackage.haskell.org/packages/archive/split-channel/0.1.2.0/doc/html/src/Control-Concurrent-Chan-Split-Implementation.html#sendMany). (Though admittedly these are simple cases, which would be better written with lambda-case with no name shadowing at all.) Also, not worrying about name shadowing means I can think about smaller pieces of code at one time, whereas worrying about it makes me keep irrelevant bits of information in my head. I rarely find that name shadowing warnings help me, and I've even had them mess up my code because I tried to fix them. I'm extremely enthusiastic about warnings about unused variables (which more often than not turn out to be a bug), and am very much in favor of warnings regarding incomplete case statements, and would like to have a nice way of writing a type so that it would produce a warning or error if the inferred type is more general than what I wrote. But name shadowing, no thank you. Maybe someday when somebody writes a magical version control system I'll change my mind.
 liftIO (openOffline fh) &gt;&gt;= ... Looks dangerous, where do you close the handle? What about exceptions?
&gt; &gt; Ahah! We see that for any possible subvalues x0, the above value fails. &gt; Is this proven, or just tested? It may be proven by substituting that subterm for \_|\_. If the function still returns the same result (i.e. does not evaluate \_|\_), then it will also do so if we substitute that subterm for anything else. (Of course, this assumes pure functions without unsafePerformIO, yadayada...) EDIT: Correctly typing \_|\_.
I like this proposal. It is basically a more explicit version of super class defaults, which makes it simpler for the compiler and slightly more flexible. I am not too keen on the Strengthening Constraints idea. At the point where you need this it is probably a good idea to just write the instances explicitly. If you just copy constraints to all the instances, then code that worked before will still continue to work later, which is the most important goal.
There is a really easy way to prove it violates upstream identity. You defined idP to terminate when it receives a Left, so there is no way it can pass along any values following a Left like an identity should. However, if you define it to ignore all lefts, you then violate downstream identity since it will not terminate when upstream terminates.
&gt; Real-world examples to try SmartCheck on. OK here's a big challenge for you. Given [this definition of pipes](https://raw.github.com/DanBurton/Blog/master/Literate%20Haskell/Pipes%20to%20Conduits/PipeU.lhs) (or an equivalent definition using a regular algebraic data type rather than composing functors), we want to know if Pipe behaves like a category. Encode the category laws as they apply to Pipe as SmartCheck properties forall p : Pipe, p &lt;+&lt; idP === p forall p : Pipe, idP &lt;+&lt; p === p forall p1 p2 p3 : Pipe, (p1 &lt;+&lt; p2) &lt;+&lt; p3 === p1 &lt;+&lt; (p2 &lt;+&lt; p3) (Figuring out how to express whether 2 pipes are equal may be difficult) The first property should fail [under these circumstances](http://www.reddit.com/r/haskell/comments/x6w9k/pipes_to_conduits_part_2_upstream_results/c5jq1mr). Can SmartCheck automatically figure that out and tell me those exact circumstances in a clear way?
I've looked at the source of Pcap and openOffline is wrapping an internal function that's doing this: openOffline name = withCString name $ \namePtr -&gt; do ptr &lt;- withErrBuf (== nullPtr) (pcap_open_offline namePtr) newForeignPtr ptr (pcap_close ptr) So as I understand it from the docs on newForeignPtr: &gt; Turns a plain memory reference into a foreign pointer, and associates a finalizer with the reference. The finalizer will be executed after the last reference to the foreign object is dropped. There is no guarantee of promptness, however the finalizer will be executed before the program exits. The handle will be closed. Can anyone who knows a bit more about conduit and stream processing generally comment on whether this is sufficient (bearing in mind that I only want to process one file) or whether it would be better to recreate the function above using bracketP around the pointer?
Ok! Thats an interesting thought! Does SmartCheck do this though?
Yeah, that'd work, good suggestion! I chose "deriving class" because it doesn't introduce any additional keywords. However, it means something quite different than "deriving instance". I'm considering using "deriving instance" with a where as the specific indication that an instance template is being invoked (perhaps "deriver" would be a preferred term, to match the syntax) I have some thoughts on extending the deriving mechanism to support this and DefaultSignatures in a concise fashion. Essentially, empty instances are much more useful in the presence of templates and default signatures, so having deriving declarations generate these, with appropriate superclass constraints could be very handy. This'd justify the current choice of keywords.
Oops -- linked to my own pedestrian answer by mistake. The comprehensive answer I am pointing to is [Uday Reddy's answer](http://stackoverflow.com/a/9859966) at the top. There is a sting in the tail though -- he is rather scathing of us functional programmer's claims to ownership of Referential Transparency -- _Their ideas on referential transparency are to be taken with a large grain of salt_ (the tast sentence before the additional note).
I think there is probably a lot of "administration" use that doesn't ever make headlines. I wrote a static analysis tool for a C-preprocessor-based DSL that we use in work. It's only ever useful if someone commits something stupid or a merge goes wrong, but in that brief moment it can highlight a problem now rather than weeks down the line.
Previously asked [here](http://www.reddit.com/r/haskell/comments/lfwlv/what_does_your_company_use_haskell_for_ill_go/), [here](http://www.reddit.com/r/haskell/comments/vhlsj/haskell_powered_companies_hacker_news/) and [here](http://www.reddit.com/r/haskell/comments/q6qa9/companies_from_france_using_haskell/).
I really wonder how your outlook on packaging is: * Eventually a (hopefully thin) layer around basic DOM and all the API magic a browser provides will emerge; my personal play target (but I'm very short on free time) is definitely IndexedDB * Unless FFI is extended with massive injection powers (think full text with arbitrary variable substitution), these API's will always come with some additional JavaScript helpers (which need some way to be included) * However packaging is solved: I think if the result of a Closure run essentially removes the boilerplate we're in excellent shape Quite exciting :-)
What is an example of a value that has no type? I can't think of anything obvious.
I work for a VOIP company and I mostly use it for network clients and servers, and a lot of the applications I work on are heavily database-driven. As an example of one of the specific projects I did, one used pcap to sniff [SIP](http://en.wikipedia.org/wiki/Session_Initiation_Protocol) packets off the wire and store them to a database. What made it challenging was the enormous volume of data (many megabits of SIP packets to be processed real-time). I initially started off with the packet decoding code from [House](http://programatica.cs.pdx.edu/House/) to decode ethernet frames and UDP/TCP packets and Parsec to parse the SIP packets so I could extract the relevant information. Eventually I ended up writing my own packet parsing code with Cereal and switched to Attoparsec for the SIP parsing. As far as libraries and such go, I guess the main weakness I've had issues with is the database packages. None of of the ones that support MySQL even support prepared statements, which is a considerable downside if you plan to execute the same query many times. 
Hmmm, I don't think that's always a viable answer either. You group code into modules because things should naturally live together. Small semantic changes shouldn't necessarily change code grouping. It shouldn't have to change module names either, because that can break other people who might not have been effected by the original semantic change.
You can keep the old semantics and unchanged stuff under the old module path/name, and export the new semantics in new names.
As I said, now you might be breaking a lot more people than you would have if you just accepted the semantic change. If I have a group of things exported by the module Snap, I'm not going to go change that name--all other names are clearly inferior.
Agreed about your take on the jibe; I am just accepting it with the same idea in mind, namely, to try and sharpen up my thinking. I think some of the replies on the stackoverflow thread seemed to show clear signs of what you are talking about. I am not even sure that I think my answer was particularly good, though it must be surely possible to produce a more concise answer than Reddy's.
&gt;As far as libraries and such go, I guess the main weakness I've had issues with is the database packages. None of of the ones that support MySQL even support prepared statements There is a strong correlation between people using a good language and people using a good database. Expecting high quality bindings to a bad database isn't terribly realistic.
I use a system (Debian's APT) that is quite successful at using a global package database to great effect. Some package managers have problems, which honestly is why I use APT whenever possible.
Deeply embedded DSLs to be compiled down to efficient numeric code, so that I can finally do the research I'm supposed to do. Or at least that's the plan. 
If Haskell doesn't provide good support for the tools people use — and a lot of people use MySQL —then that is a weakness. I would mention that the PostgreSQL bindings for [HSQL](http://hackage.haskell.org/package/hsql) and [HDBC](http://hackage.haskell.org/package/HDBC) also do not support prepared statements. Neither does [PostgreSQL-Simple](http://hackage.haskell.org/package/postgresql-simple). Even some of the low level bindings don't. To my knowledge only [postgresql-libpq](http://hackage.haskell.org/package/postgresql-libpq) (very low level) and [Takusen](http://hackage.haskell.org/package/Takusen) (not updated in a long time, not very well known, substantially different interface than most DB packages) will do prepares for PostgreSQL. MySQL has its weaknesses too, but it's not completely bad. Sometimes you want to just shove data into a table and you don't care all that much about durability, but you do want to be able to query relationally. MyISAM tables make up their lack of durability with speed, and are useful in some cases. Your rather snarky comment seems based on a fairly incomplete understanding of what alternatives exist. It also is the sort of thing that would turn people off from using Haskell rather than encourage them to give it a try in production.
&gt;If Haskell doesn't provide good support for the tools people use — and a lot of people use MySQL —then that is a weakness. I didn't say it wasn't a weakness. I said expecting smart people to spend time writing code to allow people to use stupid software isn't very realistic. &gt;Neither does PostgreSQL-Simple Sure it does, it just doesn't let you turn it on as an automagical option. You can use prepare just fine. &gt;MySQL has its weaknesses too, but it's not completely bad Yes, it is completely bad. "It can be used in a way where the badness doesn't impact anything important" doesn't make it not bad. Sure, you can use it as a crappy sqlite if you want. That doesn't make it good. &gt;It also is the sort of thing that would turn people off from using Haskell rather than encourage them to give it a try in production. Yes, it will probably turn off PHPtards. Not everyone thinks that is a bad thing.
&gt; &gt; Neither does PostgreSQL-Simple &gt; Sure it does, it just doesn't let you turn it on as an automagical option. You can use prepare just fine. You're actually wrong. PostgreSQL-Simple uses postgresql-libpq. postgresql-libpq has a prepare function and an execPrepared function to execute the statement once it has been prepared. Neither of those functions are called from the source code of PostgreSQL-Simple. See, that's the problem with making statements with absolute confidence on a topic where you're not an expert. You end up looking rather silly as often as not. &gt; Yes, it is completely bad. "It can be used in a way where the badness doesn't impact anything important" doesn't make it not bad. If a spoon doesn't work very well as a fork, that doesn't make it a bad spoon. Anyway, it seems obvious that there's little purpose served in trying to communicate with you. I only replied to correct your factually inaccurate statement about PostgreSQL-Simple.
&gt;You're actually wrong No, I am actually not. I use it, and execute prepared queries: $ grep -r PREPARE db | wc -l 24 &gt;Neither of those functions are called from the source code of PostgreSQL-Simple. Which is what I said, it doesn't have any support to turn on "make my queries into prepared statements automagically". There is nothing stopping you from running a prepare query and an execute query, they are just normal queries. &gt;If a spoon doesn't work very well as a fork, that doesn't make it a bad spoon. So to un-analogy that, you are saying "if a database doesn't work very well as a database, that doesn't make it a bad database"? Obviously we disagree there.
I am curious as to what you are doing. In the near future I hope to do numerical simulations using Haskell, do I would be happy to know which tools you are using.
We're using it for an internal VM application.
&gt; No, I am actually not. I use it, and execute prepared queries: Okay, it seems like we're talking about separate things. Yes, both MySQL and PostgreSQL allow you to PREPARE and EXECUTE through issuing normal queries. However, both also export native interfaces to prepare/execute queries which doesn't require going through the query parser and lets you pass in parameters in a binary format. int mysql_stmt_prepare(MYSQL_STMT *stmt, const char *stmt_str, unsigned long length) int mysql_stmt_execute(MYSQL_STMT *stmt) PGresult *PQprepare(PGconn *conn, const char *stmtName, const char *query, int nParams, const Oid *paramTypes); PGresult *PQexecPrepared(PGconn *conn, const char *stmtName, int nParams, const char * const *paramValues, const int *paramLengths, const int *paramFormats, int resultFormat); Using the statement based preparation/execution is better than nothing, but it's kind of a kludge — and it's not the same thing as preparation through the client library. &gt; So to un-analogy that, you are saying "if a database doesn't work very well as a database, that doesn't make it a bad database"? Obviously we disagree there. Maybe it wasn't the greatest analogy. If one eating utensil doesn't work as well as another eating utensil for a specific purpose, that doesn't make it a bad eating utensil. Different types of utensils have different strengths and benefits. Even something like a plastic fork can have its uses in some situations. Personally, I'm not even that fond of MySQL. It's annoyed me plenty of times with its limitations and quirky behavior. However, it works perfectly adequately for many things. I would also point out that I don't have the option of reworking the entire infrastructure where I work just because I decided to use Haskell. I have to work within the constraints of the problem and resources available to me, in many cases I have to interoperate with legacy systems. It's not ideal, but the real world rarely is.
Lots of large companies use mysql for lots of things. They also employ people who use good languages and who know what they're doing--sometimes, at least. Now if those people using those good languages to do tricky things need to access certain data at those large companies (which may not even be core data, which would typically be kept in oracle or whatever, but at a large enough company, you'll find some group in some division using just about anything you can think of) then they really don't have much choice but to use MySQL bindings, no?
I agree on the database front. Our libs get the job done, but a few more people working on building out "advanced" features for the basic bindings backages (and also for HDBC and etc) would really improve things all around, especially performancewise.
I though stack traces would really be missed, but it turns out you (or rather we) don't really need them. I don't think we've had any real crashes in our Haskell code in the last 2 years where a stack trace would have helped. Once you stop using the partial functions in the prelude (head and friends) and replace them by those from, say, the 'safe' package, you're golden.
How does adding a Abort to PipeF differ from composing Pipe with MaybeT? (My brain is too full of typechecker right now to answer this, but it seems like an interesting question so I might as well ask it.)
&gt; We built a monadic parsing combinator library on top of easyVision to detect and parse multipart images from a bag of detections. Very kool! Any plans to release the source, or is this just too much of a market edge?
&gt; even if the behavior is not a category or not intuitive My goal in this blog series is to end up with a Conduit clone, so my feelings won't get hurt if you call the behavior non-intuitive, because I can just point the finger at Conduit! ;) Well in this blog post, I actually step away from Conduit and restore functionality like that of Control.Pipe, so I can point the finger in that direction. Sort of. &gt; it does a good job of teaching people how to use FreeT to build their own custom syntax trees and how to interpret them Yes, I am making a special effort to use only the simplest functors, compose them, and then throw them into a `FreeT`. Anyone following along with the series should start to get a good idea of how `deriving Functor` works on algebraic data types. &gt; I'm trying to finish documenting my package that spins off FreeT so that people can try this without using pipes as a dependency. Yay!
I see. So it's a terminology thing more than anything else.
He's actually co-authoring a Scala book right now. Given that he thinks Scala and the team writing it are a complete joke and how vocal he is about his feelings in #scala on a daily basis, I'm at a complete loss why he insists on working on things he dislikes so much. It's masochism taken to a new extreme. 
Ok, the package is up and you can find it here: http://hackage.haskell.org/package/transformers-free I will create a blog post announcing it and giving some motivating examples in the next few days (it's hard to find free time on vacation, ironically!), but feel free to use it in your series. Edit: I also updated pipes so that it now depends on `transformers-free` for `FreeT` so it won't provide a conflicting implementation, so if people following your tutorial installed `pipes`, then they can either unregister it or upgrade to `pipes-2.2.0` so they don't have a conflict.
I myself programming in an interesting style using PipeAbort today. Scroll to the bottom: http://hpaste.org/72245 Basically, I defined type (i :~&gt;* r) = forall o u m. Monad m =&gt; Pipe i o u m r Basically, this means that we have a pipe that has no effects, no output type, and doesn't care what the upstream result is, but it will `await` some number of times, and then produce a result `r`. This allows for some very elegant code, and I surprised myself with how convenient the `abort` primitive turned out to be for writing in this style!
What exactly do you mean by "web server programming"? Do you want to write a web *server*, or use a web framework to write a web *site*? In either case, if she's new to Haskell, then perhaps she should browse a few chapters of LYAH first.
Financial modelling. The tools are primarily Matlab and GHC (and gcc). I'm not really using pre-existing Haskell libraries for this. The idea is to compile down high-level descriptions of simulations to C code with SIMD intrinsics. I hope to open-source parts of this in the future, if my employer allows it. By the way, I think there are several rather similar projects out there. The [Accelerate](http://hackage.haskell.org/package/accelerate/) library is similar (though I need more domain-specific stuff, and CPU backend). Also [this small conference](http://www.nii.ac.jp/shonan/seminar019/) looks very relevant.
I understand the argument that Haskell is not the best language for new programmers (though I teach it to 12 year olds without problems... but that's a subset that's not sufficient for doing web apps!). But... node.js? Really, you want to teach new programmers in an environment where everything has to be written with explicit continuations?
The source comment -- Ceci n'est pas une pipe made me chuckle... :)
She just wanted to know how a web server worked (i.e. how it sends and receives http protocol messages and responds to links). She does want to know how to build a web site, too, but in this case she was specifically asking about servers because she likes to ask questions about every layer of the stack.
Well, the way I see it, if Haskell is not easy for beginners, then that is a problem worth solving.
Any platform is fine.
The scariness of performance and scariness of web apps were meant as completely distinct issues. And I'm willing to accept that Haskell web apps are actually not that scary, but it was somewhat scary when I tried last time.