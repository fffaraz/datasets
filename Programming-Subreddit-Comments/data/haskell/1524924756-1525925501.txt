&gt; Gelisam posted a gloss example with fairly sad news that it allocates a lot and GCs a lot. He put a performGC on each frame to try to mitigate it. I must apologize; [my comment](https://www.reddit.com/r/haskell/comments/8ekkk3/rust_instead_of_haskell_in_gamedev/dxx00r7/) did imply that GC pauses were a problem for this simple gloss example, but I was mistaken. In fact, this example displays a static scene, not an animation, so GC pauses would not be visible even if they were long and frequent. I just added a trivial animation and ran it with `+RTS -B`, and the GC pauses are very infrequent and short enough that they don't cause any visible hiccup. As I explained in a [followup comment](https://www.reddit.com/r/haskell/comments/8ekkk3/rust_instead_of_haskell_in_gamedev/dxxw2jz/), for such a trivial program which doesn't use up much of its 1/60th of a second to compute the next frame, GC pauses don't become a problem until our program has millions of live data constructors, regardless of whether we use gloss or SDL2. The reason my example calls `performGC` on every frame even though I'm not even animating anything is because it demonstrates how to use [gloss-rendering](hackage.haskell.org/package/gloss-rendering) without gloss, by writing our own event loop. I had previously experienced hiccups when writing an animation using my own event loop, and it took me a while to figure out why my animations looked smoother when I ran them via gloss than via my own event loop: it is because [gloss is calling `performGC` on every frame](https://github.com/benl23x5/gloss/blob/c63daedfe3b60085f8a9e810e1389cbc29110eea/gloss/Graphics/Gloss/Internals/Interface/Animate.hs#L67). I chose to include `performGC` in the example even though it wasn't strictly needed, to spare the reader the pain of rediscovering this non-obvious solution on their own. Now that I think about it though, I remember that the animations with which I was experiencing the hiccups were really simple, like a rotating square, and I certainly hadn't allocated millions of data live constructors. So I think something must have changed since that time. I'm certainly using a newer version of GHC, maybe its GC algorithm got faster? Or maybe I'm just using a faster computer now :)
I agree with all of this. The instance with the `Monoid a` constraint is exactly right.
Apropos-plicative
`putStrLn`
&gt; Turns out that's not possible! [It will be soon!](https://phabricator.haskell.org/D4353)
Aaaaaaaaing. That's scaringly complicated and long, and not what I expect from Haskell. Where is the two paragraph post ‚Äúuse these libraries/functions and you'll be fine‚Äù? 
I speculated about this possibility five years ago when I first wrote the package. https://github.com/tomjaguarpaw/product-profunctors/blob/master/Data/Profunctor/Product.hs#L86
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/product-profunctors/.../**Product.hs#L86** (master ‚Üí 92ae094)](https://github.com/tomjaguarpaw/product-profunctors/blob/92ae09460c9582617c6d7d8751c118e0a1574b04/Data/Profunctor/Product.hs#L86) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dy47vvm.)
I don't think you need any more math than you'd need for any other pregaming language. In particular, you don't need to know how mathematicians think about monads or commands at all to use them effectively in Haskell programming. That said, since I am a mathematician, I should say have fun learning topology and algebra, just don't expect to need them when coding in Haskell.
It seems like this version fixes the issue on Arch Linux where builds would fail because latest GCC has different defaults. Cool!
You should watch the [video](https://www.youtube.com/watch?v=T5y8sFmCFnA). Here is a quote from the slides: &gt; * Async exceptions are tricky &gt; * They aren't nearly as terrifying as lore makes them out as &gt; * Usually: use the right helper library, everything's good I think the video was excellent for learning why they are tricky. If you want to avoid having to deal with them he also lists some libraries in [this slide.](https://www.snoyman.com/reveal/async-exception-handling#/10) 
In Haskell, anything with the right type signature that satisfies the comonad laws is a Comonad. That's all you need to know for working with them. The same goes for any other abstract algebraic structure. However, some knowledge of abstract algebra will be very helpful, if you want to build an intuition for these things. It will help you understand how Monoids generalize the idea of addition \(or multiplication\) as an associative binary operation together with a zero element. Categories and monads generalize this idea further, but with a somewhat more fine\-grained type structure. The more interesting property is in all three cases associativity, which means that `a+(b+c) = (a+b)+c = a+b+c` for some binary operation nam`e`d \+. You can start "adding" on the left or on the right side. Note how you don't even need parentheses in the third expression. The operation composes in any order. That's the gist of it. Comonoids and comonads turn this abstraction around \([literally](https://en.wikipedia.org/wiki/Dual_(category_theory))\) to give you [co\-associativity](https://ncatlab.org/nlab/show/co-associativity). Here you have a split function `split :: A -&gt; (A, A)`, which "splits" a value into two parts. The idea is roughly that you can then split a value further into three parts, by either splitting the left or the right thing in the tuple. Co\-associativity guarantees you, that the result is the same in both cases. Anyway, this entire concept is probably more interesting for [substructural logic](https://stackoverflow.com/questions/23855070/what-does-a-nontrivial-comonoid-look-like?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa) than for vanilla Haskell, where splitting a value simply means making a copy of it, which is otherwise prohibited in such systems.
My idea was pretty similar: buildList = 1: fix (rule [2, 3, 5]) where rule :: [Int] -&gt; [Int] -&gt; [Int] rule [x] = map (*x) rule (x:xs) = \res -&gt; interleave (map (*x) res) (rule xs res) 
I wanted to use this, but I want to drive the whole application, not a stub of it...Do you have your full system started in your tests?
The support for Cabal 2.2 means support for [common stanzas](https://github.com/haskell/cabal/issues/2832). That's great! It removes the only reason I moved to [hpack](https://github.com/sol/hpack). Once Stack 1.7.1 becomes sufficiently widespread, I'll probably move back to `.cabal` files instead of only supplying a `package.yaml`. It's quite nice how relatively swiftly Stack upgrades to newer versions of Cabal.
Also worth nothing that, aside from being able to put `with-compiler:` in `cabal.project`, if it *is* in there with a fixed version, you can override it locally too: `cabal new-configure -w ghc-8.0.2` for example would write a `cabal.project.local` with a new `with-compiler:` that overrides the old one. This still lets you test with multiple compilers pretty easily. Add the `.local` variants to `.gitignore` and blast them away when you're done.
As I sais, simply to distinguish it from the type name, especially now that the boundary between constructor functions and types is getting blurred. It means anywhere I see the identifier `MkThing` I immediately know it's the constructor function rather than the type.
* You need to be able to learn notation and be able to understand symbols * You need to be able to perform substitution of variables within expressions in your head * You probably should be able to understand induction/recursion That‚Äôs about it from math, and tbh you can learn as you go with all those in Haskell. Be brave and use ghci‚Äôs many nice features! For comonads, I don‚Äôt use them myself, but the `co-` part has a special meaning. Look up the type class definition and contrast the type signatures with monad `return`, `(&gt;&gt;=)`, and `join`: They‚Äôre the same except the function arrows are just all reversed!
You mean with a `p a a' U p b b' -&gt; p (a U b) (a' U b')`? Ya, I think that makes sense.
But isn‚Äôt this the definition of SumProfunctor in the product profunctors library? So if I‚Äôm crazy, at least I‚Äôve got company!
&gt; Support for redundant trailing or leading commas NICE. This is something I would love to see in Haskell itself, but having it in Cabal files is still a win.
For Haskell, you may want to keep track of the [`{-# LANGUAGE ExtraCommans #-}` proposal](https://github.com/ghc-proposals/ghc-proposals/pull/87) 
Love it! Thanks for the pointer.
Good to know. I still wonder if there is some way to compute the index in the sorted sequence to the actual exponents (or triplet) in constant time.
You can use print on a string, can you not?
Thinking of moving over to hpack for the automatic filling of other-modules when exposed-modules are populated (300 module package), otherwise common-stanzas was the only other reason I was considering switching.
You can, but the `Show` instance of `[Char]` includes the double quotes.
That's numberwang!
lol how did I miss that
For that paper specifically, Alexis King did [an implementation](https://github.com/lexi-lambda/higher-rank) that‚Äôs pretty easy to follow, which might help you see how the type theory (natural deduction) notation translates to code.
One other approach is to just fork your "main" function in another thread and then have your tests hit the APIs. This way, you really drive the whole application. I've done this approach in one of my open source project ... see for example: https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/master/test/Spec.hs#L28
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [eckyputrady/haskell-scotty-realworld-example-app/.../**Spec.hs#L28** (master ‚Üí 8d82565)](https://github.com/eckyputrady/haskell-scotty-realworld-example-app/blob/8d8256583197e3dde9e72b27fab11f27530ea40f/test/Spec.hs#L28) ---- 
Just saw this which should be preferred I think: https://twitter.com/puffnfresh/status/990154797494943744
&gt; At this point, ripping up the whole data model is a lot of work. I still think it's worth it. Do you know how interested individuals could reach out to help something like this happen? Either as advocates who could lend a voice calling for action, or as contributors who could offer time to help implement something. I'm very much concerned that infrastructure critical to the community is running counter to what accepted industry practice dictates.
Not personally a fan, if only because of the `inNixShell` shenanigans it does. The ability to compose Nix expressions is the whole reason it's better than other build tools. The fact that it doesn't return the right derivation in the impure case that you're launching a Nix shell, regardless of whether this derivation is the actual target or not, is a complete deal breaker. Really this should all be solved by merging `foo` and `foo.env` in Haskell packages (not really sure why they're separate). But ignoring that, I just prefer to have different attr paths for shells and builds. Especially since most real world projects have multiple different packages anyway; it's better for default.nix to just return a whole package set with your packages in it, use `nix-build -A foo -A bar` and use the new `shellFor` to get a common shell for incrementally building all your packages in one nix-shell. It's a tad more verbose on the command line but it's extremely handy.
 Thanks for the tip! I'm off for 6 months away from all electronics, but when I come back and get back into Haskell-dev, I'll look this up again, and play with it a bit. Is this stuff documented anywhere? I spent a lot of time looking at old nix-code and reading through the manuals, but my experience is that the learning curve is really steep and the documentation inconsistent.
the text invite to discord is expired
It would be nice if you activated video archiving. This is your link: https://www.twitch.tv/ekmett/dashboard/settings There's an option called "Store Past Broadcasts".
Where I can read more about `shellFor`?
Nice! I've been looking for this sort of thing for months. I'll make sure to tune in later when it goes on, seems like an extremely interesting thing 
I would love to watch it afterwards on YouTube or somewhere else!
That's all very interesting theoretically. But for the many of us who have had the admittedly subjective experience I described, are there any concrete examples of things you can do with this library that could not possibly have been provided using (what seems to us) a friendlier applicative interface?
I studied this paper in detail for my thesis and also implemented it for a larger language https://github.com/adamschoenemann/clofrp If you have any questions about the paper feel free to pm me and I will try to be of help! 
So my first up-front question would be: &gt; During it we built a small library for commutative semigroups and commutative applicative functors, and for playing with free monads given a commutative applicative functor. Why? I don't mean that in a fasciecous way, but why make that library? Is it something you'd normally work on? What's its purpose? What will it allow us to do? Is it just a teaching project? The thing i struggle with in Haskell is not the type classes and how they work. It's applying them to practical problems. I never have the "If i made this an instance of applicative it would just fall out" moment. If you have a practical application for this library in mind, how that application is shaping the library design would be really interesting to me. Hope that makes sense. 
It seems you'll get an answer to why in the stream: &gt; I'll go through what has been built and why. There's a fair bit to do to round this out. 
[MRW](https://img.4plebs.org/boards/pol/image/1491/58/1491584271738.gif) reading Java syntax.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://img.4plebs.org/boards/pol/image/1491/58/1491584271738.gif) - Previous text "MRW" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
&gt;Why? Same question! "What's in it for me?" Does it enable the application programmer to do something easier or better?
Isn't calling this a "trash fire" a bit harsh? `Maybe` has always been the standard way to model in Haskell the semantics of "null" of weaker type systems, and that's what Bryan did in aeson. If the purpose of `FromJSON/ToJSON` is to model JSON semantics, why is it so surprising that the model is not injective? On the other hand, I certainly wouldn't say that code which relies on an incorrect injectivity assumption to uses these same classes for serialization is a "trash fire" either. This incompatibility is a subtle enough point that it doesn't seem to have come up much until. Our job now is to come up with a solution that supports both use cases as neatly as possible, with minimal breakage of existing code. I really don't think it should be so hard. If we decide that the best possible end result can't possibly include support for simple modeling of JSON semantics, then so be it. But then we'll need an appropriate deprecation cycle.
I dunno. Just brute force filtering `[2..]` for numbers whose only prime factors are 2, 3, and 5 finished in a few seconds for me at the GHCi command prompt, not even compiled or optimized: removeFactorsOf p n | n `mod` p == 0 = removeFactorsOf p (n `div` p) | otherwise = n head . drop 599 . filter ((== 1) . removeFactorsOf 2 . removeFactorsOf 3 . removeFactorsOf 5) $ [2..] Slightly faster if I skip numbers that are relatively prime to 30, replacing `[2..]` with scanl (+) 2 $ cycle [1,1,1,1,2,1,1,2,2,1,1,2,2,1,1,2,1,1,1,1,2,2]
&gt; Why? I'll answer a question with a question. Do you understand the "why" of monads? Do you have some appreciation for the fact that there was a time before monads were really understood, but yet some things are monads and it's useful to have an ecosystem of things for doing stuff with monads? Now take a natural transformation of that into "we are in the time before commutative applicatives are really understood, but yet some things are commutative applicatives and it's useful to have an ecosystem of things for doing stuff with commutative applicatives". There are a couple of caveats to my response. Firstly, I don't know whether "commutative applicative" is an especially important concept in programming, nor whether Haskell will be able to support them well. Secondly, I don't know if you were just asking "give me some examples of commutative applicatives" and my response is a bit too philosophical.
Thanks for the reply. Someone else recommended I read How to Prove it before TAPL. Would you say that comes before, after or instead of Type Theory and Formal Proof?
You mean output the schema as text, right? I haven't tried it, but at least for PostgreSQL you should be able to do that using `mockMigration`. The source code of that, together with the source code of `migrate` and `printMigration` in `Database.Persist.SQL`, should give you an idea how to do it for any other database you might be using if its backend library doesn't have `mockMigration`.
gist updated to include [example of mixing concurrent and sequential comands](https://gist.github.com/louispan/85bd50c73b3349917ef4ba02e1829cd5#file-concurrentvariantinterpreter-hs-L257)
I've created a gist linked [in this reddit post](https://www.reddit.com/r/haskell/comments/8f2ama/haxllike_concurrent_interpreter_of_commands/) that shows interleaving Haxl-like concurrent commands (for commands that return a value), and normal sequential commands. This shows another advantage of this approach. Even though the `Concur` monad requires IO, the command generator still only needs a base monad of Identity, because the IO requirement can be delegated to the interpreter.
&gt; Do you understand the "why" of monads? I understand the data class and the operations it contains. I understand how to use existing monad types. I understand that generalising families of work into classes like Semigroups, Monoids, Applicatives, Monads, etc gives a common basis for operations on things within those classes. (...and that I would label as the "Why?") ...but it's the ability to fit those classes to concepts within my own code that I'm missing. It's not that I want examples. I'd like to understand the thought process that recognises the pattern. For monads I find this simplest, as all of the standard monads feel like specialisations of the `State` monad to me. `bind` is also a function I've found myself writing a variation of more than once. It's the other data classes which interest me more. &gt; "we are in the time before commutative applicatives are really understood, but yet some things are commutative applicatives and it's useful to have an ecosystem of things for doing stuff with commutative applicatives" So by your response I take that you would see working on this as a research task. You'd be taking an area of theory as a playground and seeing what emerged. That's fine, valuable even, but not how most people use a computer language. It's science vs engineering. I guess another way of phrasing my question could be "Was there something (a problem) that prompted the interest in commutative applicatives? Something that we think they might be useful for, even if we're not sure and want to construct a research environment first?" (...and hence I'm answering your question, with which you answered my question, with a question)
&gt; Also, if you're also interested in streaming Haskell coding out on twitch, please reach out to me! That sounds fun! I feel like I'd need to practice beforehand though. I once tried streaming while me and a team were creating a game for Ludum Dare, and I was either focusing on the project and forgetting to tell the audience what I was doing, or when I was remembering about the stream, I was stressing out about making stupid mistakes, and the stress caused me to make stupid mistakes. How do you practice that kind of skill? Make the first few streams private and only give the link to a few people?
Ya I feel like 95+% of my ‚Äúcoding‚Äù time is me just sitting quietly, thinking. My screensaver at home has a 20 minute timer, and it regularly comes up when I‚Äôm coding!
If you just have a single web app running on the host, I would just use warp. If you have more than one, you‚Äôll probably want to stick Nginx in front as a reverse proxy to handle the domain name.
&gt; (...and hence I'm answering your question, with which you answered my question, with a question) And I think we're closer to an answer for all those questions :) Yes, I think it's closer to a research task. But that's only speculation. We'll have to watch and find out :)
https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/haskell-modules/make-package-set.nix#L229
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [NixOS/nixpkgs/.../**make-package-set.nix#L229** (master ‚Üí 97adb03)](https://github.com/NixOS/nixpkgs/blob/97adb03a9e542c9966712b9d88c84b4ed6d1a6b6/pkgs/development/haskell-modules/make-package-set.nix#L229) ---- 
Yes. Most commonly you wrap a Haskell package in `pkgs.haskell.lib.doJailbreak` but if you only want to remove upper bounds you can pass that specific flag with `pkgs.haskell.lib.appendConfigureFlag` 
It is activated now. I didn't activate it the first time as for about the first 2 hours of that session was me sitting on discord with a guy who was playing the harmonica to test if the audio was connected. As amusing as it was when it was looping and then being picked up again by the mic so that he was playing with himself in the round, it wasn't gripping Haskell at that point.
[https://www.microsoft.com/en\-us/research/wp\-content/uploads/2016/11/join\-points\-pldi17.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/join-points-pldi17.pdf) says in the introduction \&gt; The reader of Appel‚Äôs inspirational book \[1\] may be thinking ‚ÄúJust use continuation\-passing style \(CPS\)!‚Äù When expressed over CPS terms, many classic optimizations boil down to Œ≤\-reduction \(i.e., function application\), or arithmetic reductions, or variants thereof. And indeed it turns out that commuting conversions fall out rather naturally as well. But using CPS comes at a fairly heavy price: the intermediate language becomes more complicated, some transformations are harder or out of reach, and \(unlike direct style\) CPS commits to a particular evaluation order \(Sec. 8\). Rest of paper is good read too. E.g. "8. Why Not Use Continuation\-Passing Style?" section.
Not a distro, but look at `propellor`, a configuration and deployment tool that you configure in Haskell.
xmonad is probably as close as you will get
Fixed
To provide a counter to Section 8, Appel‚Äôs CPS implementation for ML is all the things he mentions: It‚Äôs rather large, it‚Äôs quite rigid, and it‚Äôs a pain to debug. An answer to this is Join Points, another is a [Sea of Nodes](http://compilers.cs.uni-saarland.de/papers/lkh15_cgo.pdf). There, let-floating is a function of the scheduler and subsequent dead argument elimination passes.
&gt; * Types and instances: how should I properly define my could of word (list of tuples words/weights, associative table) I'd use [`Data.Map.Strict`](http://hackage.haskell.org/package/containers-0.5.11.0/docs/Data-Map-Strict.html), so you don't end up with large unevaluated `1+1+1+1+...` thunks. &gt; * How to display a picture (which libraries do you use to generate a simple image) I usually recommend [gloss](https://hackage.haskell.org/package/gloss) because it is easy, but its support for text is abysmal, so maybe [cairo](http://hackage.haskell.org/package/cairo-0.13.5.0/docs/Graphics-Rendering-Cairo.html#v:textPath)? It seems to have very detailed [font options](http://hackage.haskell.org/package/cairo-0.13.5.0/docs/Graphics-Rendering-Cairo.html#g:6). &gt; * How to translate a word onto an image Looks like cairo wants you to [`moveTo`](http://hackage.haskell.org/package/cairo-0.13.5.0/docs/Graphics-Rendering-Cairo.html#v:moveTo) a point before rendering a path such as a piece of text in order to render the path at that point. &gt; * Maximizing space occupied by words while not having them overlap That sounds like the hardest part. Basic text-drawing systems only give you access to the width and the height of the rectangle enclosing your piece of text. gloss doesn't even give you that, which is part of what makes it a poor choice, while cairo gives you [a lot more](http://hackage.haskell.org/package/cairo-0.13.5.0/docs/Graphics-Rendering-Cairo.html#t:TextExtents). It still seems to be information about the overall piece of text though, not about the gaps between the individual letters and about which letters like `y` and `k` go below or above other letters, so maybe you should render each letter individually? Or maybe you could render a single piece of text to an image with transparent pixels, and then try to solve the more general problem of fitting an arbitrary number of images around each other? Either way sounds pretty hard. One approach which may or may not be applicable here is [this blog post series by Casey Muratori about generating plant distibutions](https://caseymuratori.com/blog_0009). Basically, to densely fill a space with a bunch of random dots, he starts from the middle and adds random points outside but close to the already covered region, and continues until there is no more room. Maybe you could start with the central word/image, and then add one word/image at a time in the free space around the already-covered region, and continue until there is no more room?
Nice to see this old paper still being read :-)
The current iteration of `var` doesn't allow the type inference of lambda expressions. for exemple: ```java var f = x -&gt; x; //Error: Lambda expression needs an explicit target-type ``` It would be nice to declare lambda expressions using var.
Wow, that sucks. Understandable if javac only does forward type inference but still. Also, apparently you arbitrarily can't use it with wildcard captures because the type errors would be cryptic.
I have a big server at home and rent a few online on which I spawn throwaway VMs/Docker-jobs using my DepTrack tool https://github.com/lucasdicioccio/deptrack-project . I'd not call that an OS because it piggy-backs on Ubuntu/Debian but a lot of things like the "debootstrapped" images are done using some Haskell.
Is live coding on Twitch a new thing or is this an established practice?
I hope stack also starts supporting backpack. There is already some work happening as far as I see and I am sure there are people like me who are itching to use backpack.
Hurrah! Thank you stack devs.
It might be due to the fact that windows doesn't yet have the ssl certificates. Check if something changes if you connect to and accept the certificates \(via internet explorer, edge,...\) for the website stack wants to connect. Sorry for my english.
[removed]
It's fairly established at this point; Twitch has had a "creative communities" section for things like music, drawing and programming for a while now, though relative to the main gaming audience it's pretty small.
Looking at this, I'm not convinced that the normal form for and is actually longer. There are more arguments, but that's because you're passing continuations around explicitly. The real difference that I see is that the control flow is different in the cps version. You don't have nested applications. Anyway, you're probably not going to be able optimize a simple function like and too much. The alternative to cps from the "compiling without continuous" paper is "administrative normal form". There's an explication in the paper "the essence of continuations passing style", but the paper is pretty dense. 
Honestly I still have to wrap my head around the idea of commutative effects.
https://mail.haskell.org/pipermail/haskell-cafe/2006-May/015642.html No idea if they ever kept up with it.
Can someone explain to me why in this line https://github.com/dariajung/haskell-bittorrent/blob/master/Bencode.hs#L31 ``` data BValue = BInt Integer | BStr B.ByteString | BList [BValue] | BDict (M.Map BValue BValue) deriving (Show, Eq, Ord) ``` BDict is a map from BValue to BValue instead of BStr to BValue?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dariajung/haskell-bittorrent/.../**Bencode.hs#L31** (master ‚Üí 368d7c4)](https://github.com/dariajung/haskell-bittorrent/blob/368d7c4477aa0394953ec6cf7baebcd940b80543/Bencode.hs#L31) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dy6hi6j.)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dariajung/haskell-bittorrent/.../**Bencode.hs#L31** (master ‚Üí 368d7c4)](https://github.com/dariajung/haskell-bittorrent/blob/368d7c4477aa0394953ec6cf7baebcd940b80543/Bencode.hs#L31) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dy6hi7a.)
I've been recording myself programming without streaming it. I don't even necessarily upload the videos to YouTube - I uploaded one because someone specifically asked for it, that's it, but I do convince myself that anything I record *could* be uploaded. I think of it as more like rubber duck development, or pair programming with a really quiet partner. The point isn't creating content that is similar in quality to prepared presentations; it serves a dual purpose of improving my own code and development process by forcing myself to explain everything, and hopefully helping others whose skills are in a similar place to mine. IMO it's actually super useful to see "stupid mistakes" and how they're resolved. What you consider to be a stupid mistake is probably one that's common among developers who are learning! There's absolutely a role for more "real world" development videos/streams where you do make and then correct mistakes, or you take a while to puzzle out a complex problem. I think they're very helpful to the viewer who's seeking to improve their practical Haskell development skills. Especially since interactive debugging with GHCI is one of Haskell's strengths - but if you write everything perfectly the first time, we never get to see it! 
Is there a bug report for the Alpine problem that led to ditching static binaries? And are there plans to reinstate the static binaries?
/u/edwardkmett this is admittedly off-topic, so I'd appreciate if you allowed me to ask what the warm yellow terminal color scheme is and if it's available somewhere?
Maps map one type to another. `BValue` is a type. `BStr` is just one possible constructor of the `BValue` type. 
You can also get to the source by clicking the `#source` links on the right on the Hackage packages for individual modules, like [Weigh](https://hackage.haskell.org/package/weigh-0.0.7/docs/Weigh.html).
In [the video](https://www.twitch.tv/videos/255827270) at around 33 minutes Kmett says that he got thinking about this because of Haxl. Haxl is a domain-specific language for automatic batching and parallelisation. It's _morally_ a law-abiding monad under certain assumptions, and this `Commutative` stuff is about codifying that idea.
Thanks, that's very helpful!
Ah, thanks. That's an interesting insight. 
Fwiw, we can basically do most of this today with Generics and DeriveVia. Any instances on `+`, `*`, etc can be given to a wrapper type using Generics. So then on your data types you just derive Generic and derive anything else using `via GenericWrapper`. It'd be interesting to have special syntax for instances on Generic types; then we could have a `deriving generically Foo` syntax.
I've made use of [this](https://github.com/jparsec/jparsec) library in the past. I'd reach for it again over regex for the usual small parsing jobs I encounter - structured URLs, simple data files, etc. I'd pass over it to ANTLR or CUP for something more complex.
same here. just leave it around for a while, until i have written a substantial amount myself, then i delete it.
Does this mean you wouldn't want to issue a bare `nix-build` for fear of building the entirety of Hackage? I avoid `developPackage` for the same reason as you, but my default.nix wound up a bit more like: let pkgs = import ./nixpkgs.nix {}; hask = pkgs.haskell.packages.ghc822.extend(haskell.lib.packageSourceOverides { ... }); in { module1 = hask.module1; module2 = hask.module2; executable = hask.executable; } I think to make `shellFor` work here, I'd want to move the definition of `haskellPackages` to a third file and have both default and shell import that.
Not sure how useful this really is in the end; Kind of depends on how often you actually want the same instances just because the internal structure is the same... and how much you think more generic-ness is worth the loss in readability. I mean, you're saying this is "free" but that's only in terms of code. There's a significant cost to doing things like that.
I'd love to hear what /u/edwardkmett thinks about the whole idea, and the different versions.
Please, _please_ stop titling proposals like announcements.
&gt; The data-as-newtype approach can be neatly reformulated as [...]: for any type A, the generic representation of A [a la GHC.Generics.Rep A] is coercible to A. I'm suspicious of this concept in practice. It requires all data types' in-memory representations to be significantly inefficient. Maybe SoP generics would do better? No matter what, there's a trade-off between a minimal set of canonical representation types and efficient representations (and code for that matter), and this approach allows only one extreme end of that spectrum. ‚Ä¶ Come to think of it, the current GHC support for this kind of thing is `anyclass` deriving, plus `-XDefaultSignatures`, plus `GHC.Generics`, isn't it? The article says &gt; The major drawbacks of generics is that its conversions incur a run-time overhead, and that it doesn‚Äôt support deriving as a first class notion: instead we have to write instances to perform the conversions. `anyclass` and default signatures alleviate writing the instances. And "avoiding" the run-time overhead by changing the original data type to use a comparably inefficient in-memory representation seems like it will most often be throwing out the baby with the bath water. In the status quo, on the other hand, with enough specialization -- which is not easy to guarantee! -- the overhead of the to and from traversals *ought* to fuse into the generic operations' traversal.
I have not read How To Prove It.
He's said on the Haskell programming stream that he believes `Mo` is a legal comonad, but he needs to take a closer look at it. Also, I've recognized that what your version is doing is taking the `w (m r)` of the `extract` function and lifting it to its [Coyoneda variant](https://hackage.haskell.org/package/kan-extensions-5.0.2/docs/Data-Functor-Coyoneda.html), which is why it scraps the need for the Functor constraint, as well as making `Mo` more efficient. It also means that our versions are definitely isomorphic to one another.
Some people doing a rewrite of nix in Haskell: https://github.com/haskell-nix/hnix
One of the best bits of advice I ever received about live coding is to try to voice your inner monologue rather than sit in silence. This means that maybe not everything can be written live on a stream unless you are _really_ good at this practice, but it avoids the "dead air" problem. I'm not very good at living up to it, but I'm finding it gets easier the more I do.
I wonder if Generics can be changed so the generic version of a datatype is representationaly equivalent to it, so to and from can become coercions. 
Excellent, thank you. I suspected something along these lines, and look forward to reading the updated `abelian` repository
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/haskell-nix/.../**README.md#minimizing-the-closure** (master ‚Üí 5aa35e3)](https://github.com/Gabriel439/haskell-nix/blob/5aa35e3d93e18011176ac38a41d2127d683f4aa6/project3/README.md#minimizing-the-closure) ---- 
Ya you did a fine job! When you first hit that unwanted join in ApplicativeDo, I was like ‚Äúthat was probably unplanned. Here‚Äôs where the rubber meets the road!‚Äù But even that went pretty well. Fortunately you quickly found (or someone in chat just knew? I was streaming from MPV so I couldn‚Äôt see chat) the right debug dump options.
We've had trouble adopting it successfully at work, but whenever we've been able to utilize Nix, it's helped a lot in these areas. - Build times are usually better because Nix is much smarter about package level build caching than other tools. It lacks incrementalism at the module level, but we've found the cost of frequently rebuilding your entire Haskell dependency tree to be far greater for CI / deployment. For developers, nix-shell gets you module level incrementalism at the cost of being less deterministic. - If you deploy to NixOS, the size of a deployment is merely the size of the change in packages since the last deployment; i.e. no having to redeploy most of your OS or dependencies. You can often get away with a couple megabytes of binaries to upload. Use `nix-collect-garbage` on the deployment to keep the resident disk usage down (usually &lt; 3G for me) - Static builds with Nix are definitely possible, though ever so slightly nontrivial (just sprinkling a few undocumented Nix expression incantations; certainly easier than without Nix). But the whole Nix store abstraction makes it pretty easy to just not care about this; simply copy closures around.
I don‚Äôt think there‚Äôs a fundamental reason other than tradition‚ÄîI think it‚Äôs just so you can translate `if x then y else z` into `x y z` with the terms in the same order.
I think any datatype with 3 or more constructors fails since you get multiple ‚Äútags‚Äù. 
Nix is the one of the recent technologies that I just haven't dipped into -- it seems like another great part of the servers-as-cattle philosophy but I just don't want to budget time to learn a whole 'nother completely new way of doing things at the OS level. When I was choosing between CoreOS/RancherOS/Nix for repeatability and stability at the OS level, CoreOS won out, and although I've switched off of it after the redhat acquisition, I can't say Nix is my #2.
The approach I use depends on a builder image as well. (I use the fpcomplete ones, but it actually doesn't matter.) Then, I create a deployment image from scratch. (You can use another base image, but it should match the builder to keep things a bit more simple.) Yes, it is necessary to copy some dynamic libraries, but it's just about a score of them (the usual suspects like libc, gmp, termutils, etc.) for most projects. And one is anyways not going to switch base images on a whim. Therefore, the resulting Dockerfile can be used as a template for other projects.
Creating docker images with Nix is easy. Not sure if [this article is up to date](http://lethalman.blogspot.com/2016/04/cheap-docker-images-with-nix_15.html?m=1), but if there's anything different it won't be any harder. It's still a well supported feature of nixpkgs as I understand it. I don't believe nix-docker is the same thing, nor the recommended method. You don't have to use NixOS on either system. It's a little easier to configure system level stuff if you use NixOS on the deployment, but not necessary. Any platform that Nix supports is fine. Just copy the closure up and start the executable however you normally would. For CI, I hesitate to suggest [Hydra](https://nixos.org/hydra/) because of how abysmal the documentation and configuration is (even for the Nix / Haskell world), but hydra is extremely effective if you know how to use it. Otherwise it's pretty easy just to make CircleCI persist `/nix` between builds and run Nix builds in there.
So the fpcomplete ones are based in `debian:jessie` (I could have sworn some were also `ubuntu` at some point but maybe not) -- just to clarify, when write "from scratch` you **don't** mean `FROM scratch` (i.e. the super minimal drop-a-binary-in base image), correct? I can't imagine/haven't seen somoene being able to use `FROM scratch` -- it would require building your binary images using musl libc (or some other cleanly statically buildable libc), AND find someway to get past the `gethostbyname` and other network-related functions that seem to be distro/platform specific, which I haven't found a way past. Would love to hear more if you have. Also, I actually literally *JUST* did switch base images on a whim (the whim was getting easy support for LTS 11.6 w/ GHC 8.2.2) -- I went from `alpine:3` to `fedora:28`, after a bit of a refactor on the codebase. The fact that **using containerization enabled me to switch to a a base image that supported what I wanted easily** is either one of the great triumphs of containerization and modularization of the build process, or a short glimpse into the darkest corners of the bazaar-induced reality we currently occupy.
Thanks -- this is exactly what I need to look into. I remember vaguely the last time that you could sort of "side load" Nix without committing to NixOS completely -- basically getting your own folder of content-addressable, layered/unioned builds... But I wasn't ready to dive in just yet. Will put NixOS on my list of things to properly research in the coming days/weeks -- it does things I want but am worried that it's going to wreck my productivity for the number of days/weeks it takes to get anywhere near knowledgeable enough to use it properly.
You can build things with nix without using NixOS though. For example, my CI machine runs Ubuntu with the nix package manager installed. The CI process is driven by Gitlab directly via `nix-build` shell commands. That way I can configure the phases (tests, style check etc) in gitlab.yml, and all dependencies are cached locally. 
This just got pointed out to me in another comment, thanks for expanding on how to do it here -- I also use Gitlab, so it's great to know somoene's using htis as well.
Commutative applicative functor means an applicative where you can reorder operations. I haven't seen the livestream/vod yet but I have seen the idea mentioned in the context of free monads and haxl.
Getting the docker image size for haskell applications can definitively be challenging, but not impossible as far as my experience goes. I have a couple examples to share. I hope the become useful to you: The first example is a small CLI tool called hadolint, originally we managed to make the docker image as small as 5MB, but then we realised that we needed some utf8 libraries in the system to be present, so the total size went up to 26MB as you can see in the history here: https://hub.docker.com/r/hadolint/hadolint/tags/ [Here's the dockerfile that you can use for reference](https://hub.docker.com/r/hadolint/hadolint/~/dockerfile/). As you can see the key to making the size small is to use a multi-stage build. The first stage will keep all the dependencies and compilation artifacts, whereas the last stage only keeps the necessary libs for the runtime system and the executable itself. I find using multi-stage builds a very good solution for the file size problem, but I also found it annoying for keeping a cache of the package dependencies. More often than not, you will have to re-compile things from scratch, which as you may know already, takes a lot of time. There are 2 solution to this problem. The first one is baked right into the stack tool. You can use `stack image container` to do a very similar process to the multi-stage build: All the building process artefacts will stay on one machine (yours) while the executable is just copied to a container. Unfortunately I felt I did not have enough control over the `stack` solution in terms of resulting image size. So I came up with a different solution: another small CLI tool that understand multi-stage builds and at the same time is able to keep the docker cache always warm. This tool I called docker-build-cacher and you can [find it in GitHub](http://github.com/seatgeek/docker-build-cacher) Here's an [example gist I put together](https://gist.github.com/lorenzo/facbd84295f14337aae8f41745dee821) to show how to use the tool. I hope this helps!
[removed]
This makes a lot of sense to me, although ya, I‚Äôm not sure what to say about GADTs. I‚Äôm tempted to say that I use regular ADTs far more than GADTs anyway, but I think that‚Äôs sort of an artefact of the limits of Haskell‚Äôs type system, because in a dependently-typed context one of the most important types you have is propositional equality, which is obviously not a simple ADT! But ya, at least on the surface, I really like this idea of defining things in terms of composing primitives. It gives you so much for free.
Hey absolutely no problem! I think all the knowledge you laid down is definitely going to benefit someone who happens upon this post! I also plan on releasing a (nother) blog post on my learnings so far (in which I go into why I use the builder pattern rather than proper multi-stage builds dockerfile wise) -- but I did want to ask one thing, **have you ever been able to build a truly static binary?** I get pretty far with alpine (musl lib c) + multi-stage builds, but GHC actually complains a bunch about relying on various network-related functions like `gethostbyname` (I think it was), and if you actually build a "static" binary in alpine vs in debian, your program will crash @ USE time of the underlying platform-provided libs. How did you get past that? Also, I'm against using the stack solution -- stack is a massive tool and I just don't know enough (or know that I want it to handle that much), and I need to be able to easily debug my problems (I learned a lot while debugging this one for example)
 &gt;&gt; The data-as-newtype approach can be neatly reformulated as [...]: for any type A, the generic representation of A [a la GHC.Generics.Rep A] is coercible to A. &gt; I'm suspicious of this concept in practice. It requires all data types' in-memory representations to be significantly inefficient. That is only true for data types with &gt;2 constructors/members (Identity, Const etc are newtypes already). With a sufficiently clever rewriting (and some compiler support for Sum/Product) we could make the representation equivalent to the status quo. E.g. we could tag the basic Product/Sum with phantoms representing strictness (as outlined in the post): For example type a *! b = Product Lazy Strict type a !*! b = Product Strict Strict type a !* b = Product Strict Lazy type a * b = Product Lazy Lazy data Triple a b c = Triple a b c -- &gt; newtype Triple a b c = Triple (Const a *! (Const b * Const c)) -- or newtype Triple a b c = Triple (Const a *! (Const b *! (Const c *! ()))) Or a strict version: data StrictTriple a b c = StrictTriple !a !b !c -- &gt; newtype StrictTriple a b c = StrictTriple (Const a !*! (Const b !*! Const c)) -- or newtype StrictTriple a b c = StrictTriple (Const a !*! (Const b !*! (Const c !*! Const ()))) That would give the compiler all the information it needs to implement `Triple` as three pointers in memory, just as it is now. It would also allow unpacking the fields of `StrictTriple`. Similarly for sums (where the suboptimal representation would incur a CPU cost rather than RAM - as in extra pointer chasing): data Either3 a b c = L a | M b | R c -- &gt; newtype Either3 a b c = Either3 (Const () +! (Const () + Const ())) -- or newtype Either3 a b c = Either3 (Const () +! (Const () +! (Const () +! Const Void))) Again, the strictness flags give implementations leeway to represent `Either3` as a single tag + pointer. Admittedly, this would likely be a non-trivial change to GHC. On the other hand it is silly that we have to pay a runtime cost to convert between `Bool` and `data FooBar = Foo | Bar` :). 
Within 30 seconds of reading this, I realized that I've been doing my documentation wrong (using `-- ^` where I should be using `-- |`). Thanks for the informative article!
Sweet! I've reached my goal of helping at least 1 person üòÑ
This is what the post is proposing. As for the &gt;3 problem, see below!
If you read the comments, there was this 1.2MB redis image: https://gist.github.com/sigma/9887c299da60955734f0fff6e2faeee0 For deployment, I wonder if static builds significantly improve on [`nix closure`](https://nixos.org/nix/manual/#ssec-copy-closure) for example. `libc` is around 10.5M on my system, so it probably does improve on it a bit, but 
The knock-on effects of this kind of post are huge -- Haskell's advanced type system seems to be a direct cause of documentation being somewhat lacking, which is one of Haskell's biggest failures IMO. By just keeping a few of these points in mind people can make more useful haddock documentation, and with some integration we could even get to a npm-like level (minus the overpopulation/over-granularity) where *most* projects you go into have some documentation in addition to the types to make things clearer. Very recently when I was working with `ginger` I was stumped as to what a type variable meant -- I believe it was `GingerContext p a b c`, and I was actually upgrading from a previous version where one of those variables wasn't there, and it took a lot of looking through the source and at examples to figure what the new one was supposed to be: ```haskell type HtmlContext = GingerContext SourcePos (Writer Html) Html ``` I say all that to say if he had been doing this, my search might have been shorter
\&gt; build times we do something like \`stack build \-\-fast\` on local machines, on your builder with many CPUs you can do \`\-\-ghc\-options=\-j\` \&gt; image sizes we reuse \`\~/.stack\`, so images for haskell builds are around 900 MB deploy builds \(where you put extracted build artifacts\) are ubuntu\-based and are 200\-600 mb in our case, without spending time on dynamic builds or other hacks this seems good enough
I think in a strict-by-default-language this would be much more reasonable since you could store everything in a flat layout. The major problem is nested sums since you really want to merge the tags. With some compiler magic to abstract tag inspection this probably could be solved for all common cases, though.
By the way, isn't there some parallel between CPS and Concatenative Calculus? 
Hey thanks for the suggestions! I actually do all 3 things (use `--fast`, use `-j` and cache `~/.stack` and even `~/.stack-work`. I also use a builder container so I can avoid a from-scratch `stack build` in containerized builder environments (CI). My builds in CI right now average around 9 mins total (~4 mins in build step, ~4 mins build + test step w/ fresh container, starting from humongous 4GB builder image) -- **all to get a dynamic binary, but more recently languages (mostly looking at rust and golang which is unfair since they're so recent, and in some ways less featureful) seem to do &lt;1min builds (for small projects like mine), and don't require such large builder images (if they require them at all). My point was a bit more fundamental -- I know projects like golang are way less featureful and specifically target fast builds as a feature (at least that's a stated goal for the golang team), but I often feel like I'm waiting too long, *especially* in an environment where from-scratch might be a desired quality. A from-scratch can take 20 mins or more for my project, on a 4 physical-core desktop w/ 16GB of RAM. Unless I'm mistaken dynamic builds are the default -- would you mind sharing any info on how you do your static builds? AFAIK a truly static build (where you can just ship the binary to any sufficiently similar machine, ala golang and rust) is not possible -- I've always had to bundle some external library deps... I'd love to be wrong. When I've started implementing static builds I've run into problems with `fPIC`, `ncurses6-nopie`, linking of various libs like `gmp`, the `crtBeginT.so` issue.... It's been a bit of a minefield for me. 
Brilliant, thank you
My heart skipped a beat when I read this tbh
We have a 16-core machine with 32GB RAM, the build step (without running tests) takes 2 mins minimum (when nothing changed, so it's kind of a minimum for loading .stack and code with .stack-work dirs), and, for example, 9 mins when you need to rebuild 91 packages and your own codebase (40K lines). So the average rebuild when you don't change deps is closer to two minutes. Simon Marlow at Facebook suggested basically the same thing, throwing more cores and memory: &gt; With an order of magnitude more code than this, our incremental build times are shorter. I recommend using large -j and +RTS -A32m at least. (lots of cores and lots of memory) https://www.reddit.com/r/haskell/comments/76zljl/hotswapping_haskell_at_facebook_jon_coens/doi8mth We wanted to experiment with spot instances for this but never had enough time and motivation yet. I fully agree with your more fundamental point that this can and should be better, and interested in static binaries as changing the deployment to be minimalistic Alpine-based.
Whether strict/non-strict is the default doesn't really matter as long as you don't have any ambiguity in your declarations. As seen above we can preserve the semantics of normal ADTs by annotating the product/sum types correctly. The strictness annotations tell you exactly when it is safe to merge tags (because differences can not be observed).
This is a shame really. Not only is it a really good book, but you actually legally bought it and now DRM is screwing you over. I don't think you will be able to get a refund, since you already downloaded it, and there is also a "read online" option. Have you tried removing the DRM with calibre? I personally keep coming back to this book sometimes to look stuff up, since it covers some interesting advanced topics in addition to the beginner stuff. I do not regret having bought the paperback version.
Great!
According to https://www.ebooks.com/information/refundpolicy.asp I think I'm eligible for a refund. It is not a big deal otherwise as long as some of the money goes to the author. It is my own fault for not doing more research about the site prior to buying. It has just been so many years since I encountered DRM, especially for tech books. I'm aware of the "read online option" but I'm easily distracted and the reading platform matters a lot for me, their software just feels really awful to use. If the refund does go through and there is no other suitable ebook vendor, I will most likely pick up a paperback version as well. Thanks!
Unfortunately this rewriting trick breaks parametricity. If we have a polymorphic function that involves a `Product`, f :: a * b -&gt; _ Suddenly the compiler has to worry about who is calling that function, because the product could be rewritten as flat structs of varying lengths. Thus `f` needs to be monomorphized instead of being compiled to a single polymorphic function. 
Hm not sure I understand what you mean. If you mean a function from a vanilla haskell tuple `(a, b) -&gt; c`, this type would be representationally equal to `Product Lazy Lazy (Const a) (Const b) () -&gt; c`, which is certainly polymorphic. If you mean a function that generalizes over any combination of strict and lazy Products, we would indeed end up with at different specializations (at most 4), but this style is not even representable in current generics, so can hardly be construed as performance regression. f :: Product sn1 sn2 f g a -&gt; c Maybe I'm missing something?
Oh, true, though Tuple3 would break the data-stylw machinery I think? I was thinking about unpacking the combinators which ghc doesn't allow to polymorphic types anyway.
Yes, I watched some of the stream. This part I gleaned despite most of it going too fast for me. /u/edwardkmett reasons about category laws at a pace which I find like lightening. Not that surprising really, he's working in that realm day in, day out. So he was making decisions whilst I was still working out what needed to be considered. I also wasn't familiar with several of the type classes he was modifying. I think some were relatively new structure, and some I'd just not discovered yet.
For the manual version that is certainly true! OTOH if this was supported by the compiler your data-declarations would behave just as they do now, except you could derive more instances and generics would be faster :).
Another way to frame my problem is that if you represent a type like `Three` as three pointers in memory, then it is no longer representationally equivalent to the nested tuple `(Const a *! (Const b * Const c))`. Imagine you write a function f :: x *! y -&gt; _ then that can be specialized to the nested tuples f :: (Const a *! (Const b * Const c)) -&gt; _ and through the `newtype` coerce f :: Three a b c -&gt; _ Isn't there a problem, because `Three` is represented as three pointers while `f` was originally expecting a pair?
The tooling around Haskell is still halfway but I'm glad so much progress seems to have been made
This would be fairly straightforward using the very nice [diagrams](https://hackage.haskell.org/package/diagrams) drawing library, together with its font support via the [SVGFont](https://hackage.haskell.org/package/SVGFont) library.
That's a perfect example of avoiding throwing away context by having different constructors, rather than using the same type for both and hence the same constructors
I recently learned that backticks are also supported for monospace now.
When a non-monomorphising compiler emits code for accessing the fields of a polymorphic data structure like `a * b`, it has to make some assumptions about the representation. The obvious assumption is that the pair is represented as a pointer to two words that contain the values of the `a` and `b`. (Let's just ignore thunks and laziness for the purposes of this discussion, since they don't really change anything.) The problem that this poses for your scheme is that if a value of type `a * (b * c)` can ever flow to a place where this assumption is made, but it has been flattened, everything will break horribly. Thus you cannot flatten without some guarantee in place to prevent that from happening. This is *exactly* the guarantee that `data` provides. Note that monomorphisation also provides this guarantee, although by a quite different mechanism. Thus monomorphising compilers like MLton can and do flatten.
I understand, but according to bencode specs the key should be bstr only... Anyway to ensure that? 
Use upx to compress the binary. The result is a *much* smaller binary.
&gt; I think that effort just died an untimely death It did. But [this article](https://www.phoronix.com/scan.php?page=news_item&amp;px=Linspire-Is-Back-2018) claims that it is now back from the dead. No idea if the Haskell stuff is still there.
Another way to state this is that `data` exists in order to provide a _programming model_ in which flattening is unconditionally possible. The benefit of programming models is that there is never any question of whether the compiler is clever enough to discover that a particular transformation is legal - you simply get the results promised by the model, every time. There's a similar story about the justification of the existence of binary sums and products themselves. Since they can be easily encoded into lambda calculus as functions, why do we need built-in data types? We need data types because they provide a programming model in which their efficient representations are _unconditionally_ justified. Encodings do not provide this. (In the case of binary constructors the transformation in question is defunctionalisation rather than flattening, but the idea is much the same.)
Ah yes but flattening wouldn't be happening with (Product Lazy Lazy), that's why we make the distinction! Operationally you'd have: a * b - two pointers a *! b - a pointer and something else a !*! b - two non-pointers In other words, for a "normal" triple with lazy fields we can have f :: (Const a *! b) -&gt; String g :: (Const a *! (Const b *! c)) -&gt; String main = do putStrLn $ f $ coerce (1,2,3) putStrLn $ g $ coerce (1,2,3) It is true you run into toruble with !*! if the left side has unpacked fields: you would need some restriction on how something like StrictTriple above is to be rewritten to make sure the size of everything is known. For normal types with lazy fields the representation above should work just fine.
Your function definition takes too few arguments. Your function is of type A -&gt; B -&gt; C but your definition is of type A -&gt; String, so the compiler is trying to match String to B -&gt; C.
(indent your code 4 spaces so reddit preserves your newlines) The implementation looks right, but if you want your function to take a receipt and return a name, why did you give it the type `Kassenbon -&gt; Kasse -&gt; String`?
Do I have to add something after = namen ?
Your pattern match is confusing. Your function should look like f arg1 arg2 = result But you have f arg1 = result
`Kassebon -&gt; Kasse -&gt; String` means that you have a 2 argument function, that you might call with e.g. `kassierervonKassebon myKassebon myKasse`. If you can get by with a single argument of type Kassebon, you want a `Kassebon -&gt; String`. You might call that function with `kassierervonKassebon myKassebon`. &gt; I thought because the name isn't defined in Kassenbon I had to tell the function to take kasse out of kassenbon and to get the name out of kasse That's right, but that's not really something that would pop up in a type signature, right? That's something to address with pattern matching or calling accessors in the function body. 
Major obstacles to the hegemony of Generics are: - Slow compilation times. (Template Haskell wins by being compiled and by being able to generate the optimized code directly.) - Unreliable optimizations. ([*inspection-testing*](https://hackage.haskell.org/package/inspection-testing) is improving the situation on the tooling side, but you still need to know when to apply all kinds of techniques like continuation-passing style and abusing typeclasses to enable partial evaluation). - They don't generalize to richer types: GADTs and types with polymorphic fields (I'm not sure whether there's anything else in Haskell). I'd like to make a point about that last one: with full dependent types, Generics can be extended to represent all Haskell types, including GADTs. Sure, dependent types are a long way off, and they have their drawbacks. But the point is that the approach of Generics is not fundamentally incompatible with GADTs, as it may seem otherwise to some. Coming from Haskell's Generics, I think one important idea to clarify things is to distinguish the (compile time) *description* of a type from its (run time) *representation*. If we first only worry about *describing* a type, so that there is sufficient information to work with in the first place, then we see that's exactly the role of a type declaration's AST. This approach automatically supports all types of course. It also naturally encourages starting with the compiler's view of the type, which contains all information that can ever be relevant about it. This contrasts with the more common approach of starting with a simple hierarchy of sums of products and adding annotations to it (such as strictness and names). The next step is to relate that AST with the type it defines. For simple sums and products, a simple type family suffices to turn an AST to an actual type isomorphic to the original one (and that also justifies having only `Rep` that plays both the part of a descriptive AST and a representation type). But once quantifiers enter the language of types, we probably need a full-fledged dependent type system to write `Rep`, `to` and `from`. (A really good impredicative type system might be a more lightweight partial alternative.) I'm missing all of the details, but hopefully that may give new ideas to some for improving the Generics ecosystem. For an example, see [Ornamental Algebras, Algebraic Ornaments (PDF)](https://personal.cis.strath.ac.uk/conor.mcbride/pub/OAAO/LitOrn.pdf), where what amounts to a `Rep` of length-indexed lists (`Vec`) appears in section 2.2, page 6 (`VecD`). Note that the first paragraph of section 2 indicates that this was already well-known; I just happened to learn about that approach from that paper. Incidentally if you are interested in Generics you might like the topic of ornaments itself. 
I like the idea of using type system features to explore the relation between the a abstract/logical and concrete/representational view of types. This sounds like a much bigger idea than unifying ADT generics and coerce though:).
This post outlines how FPCo uses Docker to sift apart build and deployment images for Haskell apps: https://www.fpcomplete.com/blog/2017/12/building-haskell-apps-with-docker
Right. But there are only two which shows that you don't need "(&gt;2) meanings"
One difference between `@...@` and `&gt; ...` style is that with `@` you can add hyperlinks to identifiers in code blocks like this: @ 'capitalize' x "Data.Functor" -- this is why \" must be escaped if you actually write strings \&lt;*\&gt; -- @ whereas `&gt; ...` code gets parsed verbatim. This makes `@...@` useful in examples to tell apart what belongs to the library. (Also the last paragraph about these blocks seems to have swapped "bird tracks" with "crazy eyes".) The extra quotes, backslashes, also sometimes explicit quantification (which doesn't get rendered by default but help resolution), etc. needed to make proper hyperlinks can make the source look goofy if you want to align different lines in the rendered code. https://hackage.haskell.org/package/lens-4.16.1/docs/Control-Lens-Lens.html#v:-37--37--126-
I think it's worth mentioning that compared to Programming in Haskell HPFFP is PDF only but has no DRM and comes with an ereader formatted PDF as well.
Note that this is a reimplementation of the nix tool, not replacing the nix language for configuration.
I wrote about a couple of approaches here: https://www.fpcomplete.com/blog/2017/12/building-haskell-apps-with-docker but I think you're already aware of them. I was definitely able to get the image size down to an acceptable size (ie. debian/ubuntu base + a few libs and binaries). But yes this does entail that the build image might be larger at times .... however since this builds on CI that's never been an issue. If you want a way to build static binaries and very small images this might also help: https://www.fpcomplete.com/blog/2016/10/static-compilation-with-stack
[removed]
Is there a persistence library that deals with optimistic locking (either using a version column or a updatedAt timestamp kind of mechanism) and/or pessimistic locking? Or is there a generic library that I'm not aware of that can help me implement this myself. I looked up the popular persistence libraries but couldn't find anything on the matter. I might have missed something obvious, though. 
Thanks for the tip. I reviewed the table of contents and sample chapters from several Haskell books including HPFFP. The structure and writing style from Programming in Haskell seemed most ideal for me. The only other tech books that I have appreciated this much before are K&amp;R and Eloquent Ruby. 
I did this course last year and even got the dead-tree certificate. It's a light introduction if you already know the basics of Haskell. If you're just starting out I recommend taking a peek and considering it.
I found that building with musl in Alpine got around the libns problems that you describe. Wasn't that your experience with Alpine? I have a teeny Linux defined here as a dockerfile https://gist.github.com/chrisdone/3b8099c3fefb29acdbf5979ee210e888 I built busybox with -static and it correctly resolves domains and such. When I tried this from Debian, I got the libns dynamic linking errors. You need the musl environment to do static linking properly. You now Alpine is based on musl now right? I haven't tried getting GHC to work inside Alpine yet but that would seem like a way to produce truly statically linked Haskell binaries to run on a basically bare kernel, such as the one defined in my gist.
On a separate note have you heard of Debian stretch slim on docker hub? It's like 10mb in size. I discovered it reading boot2docker's Dockerfile. I used it recently for building some Haskell: https://github.com/fpco/odbc/blob/master/ Line 6 installs stack and then you're ready to go.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/odbc/.../**** (master ‚Üí f939699)](https://github.com/fpco/odbc/blob/f93969939250a4e6ec1af0e0f8c7fa842016fc9b/) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dy87c7u.)
_tl;dr_: how one handle nested `State` monads and is it even okay? I'm writing a simple card game in haskell and i have a `type Deck = [Card]`. Working with it as a list isn't very easy, so i wrapped it into `State` monad to have some helper functions like `dealCards :: Int -&gt; DeckS [Card]` and `dealHand :: Hand -&gt; DeckS Hand` which give some cards and remove them from Deck. Now i'm writing a `Game` type and it looks like it would be easier to wrap it into `State` monad to have helper functions like `dealToPlayer :: PlayerId -&gt; GameS ()` I'm unsure how one handles nested `State` monads can someone point to examples maybe? Or is there a better pattern?
That makes sense, since now the "depth" matters. For example with `(a &lt;|&gt; b) &lt;|&gt; c` vs `a &lt;|&gt; (b &lt;|&gt; c)` `a` is going to be prioritized more in the former even though they are supposed to be equivalent. 
&gt; Finally, we run the tree and output the results :D. I have a feeling the library would be easier to understand if the README actually presented the output. Right now it's a lot of preparatory work leading to ??? 
Well done on the performance and usability wins! I'm really fascinated to see how easy it will be to write android apps with both Eta and Frege and see how they compare in a practical project. You guys seem to be pushing really hard and it's great to see.
Many RDBMSes support locking natively. What are you looking to do by simulating it yourself? For example, by default, postgres does optimistic locking. You can ask for pessimistic locking by using the `LOCKING` clause. My database library [`beam-postgres`](https://hackage.haskell.org/package/beam-postgres) supports the `LOCKING` clause for proper, database-native locking. As for `updatedAt`, etc, for the purposes of data provenance, etc, that ought to be handled at the database level with triggers, not at the application level.
As others have mentioned, a debugger is the best bet. GHC takes the `-g` option (like every other compiler) to produce DWARF compatible debug information. Once you encounter the seg fault in the debugger, you should be able to do a backtrace to see where the issue is.
It is pretty close to a research task, but I have a pretty real motivation. If I do Markov Chain Monte Carlo in a monad, and carefully tease apart the Applicative parts, then I can accept/reject moves in the separate bits of the Applicative separately. If say you have a small-to-mid-sized Bayesian graphical model, then you might have, say 1,000-5,000 features. The only real tool I have for working with models once they get much larger than that is to use Hamiltonian Monte Carlo. But here, the Applicative can let me accept/reject individual independent features. ApplicativeDo can let me deal with the associativity. The Commutative class lets me capture when its safe to shuffle things past each other and handle the commutativity to find even more opportunities for parallelism. If you tune your candidate selection distribution, you might wind up accepting with say a 90% likelihood in one dimension. Being able to accept with a 90% likelihood each _component_ of a move, independently, is better than accepting the entire move with 90% ^ 1000 = 1.7478713e-44% likelihood, as the odds of accepting the new state directly feeds into how much "thinning" you have to do to read out the model. This works like Gibbs' sampling. The difference is that when you know the features are independent, you don't have to worry that Gibbs' sampling may destroy the rate of your model converging whenever you have high correlation between model features.
is Frege still developed?
Good question!
It depends on the project, but the systematic approach is: * Find the issue tracker * See if any issues are marked as good starting issues for first-time contributors * If you don't find any, open a new issue asking the project lead to create some üôÇ
Or kasseVonBon = namen . kasse like the cool kids would do ;)
Either: (A) use the lower-level `transformers` library instead of the `mtl` since `transformers` let's you specify which `StateT` level you want to you use, or: (B) combine your state into a larger record and use lenses + zoom to focus in on the substate that you care about. I blogged about this trick here: http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html
Thanks for suggestions. I guess it's time to finally learn lens. Also, i've heard about HasFoo typeclass pattern which solves problem of selecting specific transformer level. Doesn't it apply here? From what i remember it's more highlevel than transformers and lighter than lens.
Is there a typesafe equivalent of `[a,b,c] &lt;- replicateM 3 action` ? I want something that would give a type error if I did the equivalent of `replicateM 2` or `replicateM 4` without having to type a &lt;- action b &lt;- action c &lt;- action 
The function takes an A, zero or more Bs, and a C? Why not `my_function : A -&gt; [B] -&gt; C -&gt; ...`? 
Thanks, though I just rewrote the whole thing in gloss the other day which fixed the error.
If your states are all of different types, then \`get\` and \`put\` \(as long as they are used at unambiguous types\) should be able to pick out the right level for you!
Answering my own question after a bit of typeclass experimentation - Is there a way of tidying up the `(Cons a (Cons b (Cons C Nil)` ? `class MRep r where mrep :: Monad m =&gt; m a -&gt; m (r a) data Cons f a = Cons a (f a) data Nil a = Nil instance MRep (Nil) where mrep _ = return Nil instance MRep f =&gt; MRep (Cons f) where mrep x = do a &lt;- x tl &lt;- mrep x return $ Cons a tl main = do (Cons a (Cons b (Cons c Nil))) &lt;- mrep getLine print a print b print c` 
What does the `| m -&gt; s` mean in `class Monad m =&gt; MonadState s m | m -&gt; s` ?
"functional dependency"
I saw this [on codewars the other week](https://www.codewars.com/kata/599aed42b9712e1afe000014). The key is return type polymorphism. Say we want a function that sums all of its arguments: &gt;&gt;&gt; mysum 1 :: Int 1 &gt;&gt;&gt; mysum 1 2 :: Int 3 &gt;&gt;&gt; mysum 1 2 3 :: Int 6 The sum of no arguments, is zero, so it seems reasonable to require &gt;&gt;&gt; mysum :: Int 0 What then is the type of `mysum`? We can rephrase the examples above as &gt;&gt;&gt; (mysum :: Int -&gt; Int) 1 1 &gt;&gt;&gt; (mysum :: Int -&gt; Int -&gt; Int) 1 2 3 &gt;&gt;&gt; (mysum :: Int -&gt; Int -&gt; Int -&gt; Int) 1 2 3 6 &gt;&gt;&gt; (mysum :: Int) 1 0 So `mysum` needs to have a polymorphic type that can unify with all of those. It can be used either as an `Int`, or as a function taking an `Int` and returning the same polymorphic type. In pseudo-code, we want type MySum = Int &amp; (Int -&gt; MySum) -- not legal haskell mysum :: MySum We can approximate this with a custom data type: data FunctionAndValue a b = FunctionAndValue { fun :: a -&gt; FunctionAndValue a b, val :: b } mysum :: FunctionAndValue Int Int mysum = go 0 where go n = FunctionAndValue { fun = \m -&gt; go (n + m), val = n } But this doesn't quite get the syntax that we want &gt;&gt;&gt; val $ mysum `fun` 1 1 &gt;&gt;&gt; val $ mysum `fun` 1 `fun` 2 3 &gt;&gt;&gt; val $ mysum `fun` 1 `fun` 2 `fun` 3 6 &gt;&gt;&gt; val $ mysum 0 What we want goes beyond parametric polymorphism (like `[a]` or `Maybe a`). We want the ability to create two completely different top-level types using the same name. This is the world of ad-hoc polymorphism and typeclasses. Attacking the problem head on lets us define the behaviour when we expect `mysum` to be an `Int`, but falls down when we try to define the function instance. class MySum t where mysum :: t instance MySum Int where mysum = 0 instance MySum t =&gt; MySum (Int -&gt; t) where mysum a = ??? Instead, we need a way to carry partial sums forward. class MySum t where mysumHelper :: Int -&gt; t instance MySum Int where mysumHelper a = a instance (a ~ Int, MySum t) =&gt; MySum (a -&gt; t) where mysumHelper a b = mysumHelper (a+b) mysum :: MySum t =&gt; t mysum = mysumHelper 0 And now we can use the syntax from the top of this comment
Thanks for sharing how you guys are using it with so much depth! I actually just stumbled upon the +RTS -A128m suggestion in a blog post. My times (~3min without changed deps) is actually pretty close to what you guys are seeing so I must be close to doing something right. I'm going to try and put together a post to summarize these approaches after I flush out the one on the switch I've recently done from alpine to fedora for 8.2.2, I'll try to combine all these approaches into one article for the good of the realm. Spot instances would actually be really good as build machines -- so far I've only ended up using them as auxillary for CI runners, but that makes absolute sense... I wonder what's actually stopping CI from being build machines as well -- for some reason I don't see CI runners as remote build machines in my mind but maybe they should be... I've written (badly, the post meanders alot) about the static binaries inside alpine before: https://vadosware.io/post/static-binaries-for-haskell-a-convoluted-approach/ Not self promoting, but if you wanted to try out alpine, maybe give those `Dockerfile`s a try -- they worked for me just fine until two days ago when I decided to move from LTS 7.15 (GHC 8.0.2) to LTS 11.6 (GHC 8.2.2) -- ran into some issues that were due to alpine itself (can't remember the actual error now) which prompted the move to a fedora base image.
What I think would be nice is the ability to have your Ubuntu LTS image and use it for building, but then produce a binary that could be run in a `FROM scratch` container. Since layer caching offers so much benefit and the Ubuntu + apt-get convenience is pretty nice I think that setup is great, but one axis that is lacking is security. The attack vector for a Ubuntu image is so much bigger than an Alpine image, and in turn so much bigger than a `FROM scratch` image. I just want haskell to produce a completely static binary that I can take to any other similarly-architectured linux.
Hey I actually was building statically with with Alpine until two days ago! [I wrote about it](https://vadosware.io/post/static-binaries-for-haskell-a-convoluted-approach/) and did post the dockerfiles that worked (that I used up until two days ago). 4GB+ builder images but 40mb deployment images, so I was pretty happy. I didn't do anything special to get the musl environment, since Alpine is built on it -- is that wrong? did I need to specify something special to get `libns` to be used statically? On a separate note, the reason I switched off alpine in the last few days was due to GHC 8.2.2 (under stack LTS resolver 11.6) wasn't building properly, and after some digging it seemed to be an alpine-specific issue that would be fixed eventually. I either need to spend a bunch of time to debug why GHC 8.2.2 wasn't building properly on Alpine (which prompted my switch, I was pretty happy with alpine otherwise), but once I do if you have any tips on how I could get past the `libns` issues I'd be very grateful! 
Link is dead :( https://github.com/fpco/odbc/blob/master/Dockerfile is what I assume you meant to link to. Wow, debian 9-slim is only 22MB! This would also have been a good choice for me. The process I've found to work is: - Start from base image - Install possibly missing haskell-external build deps (`zlib-dev`/`zlib/devel` is a common one, sometimes `gmp-dev`) - Install stack (`curl https... | sh`) - Install my actual project (stack will download ~4GB of stuff over the course of ~20 mins) - Save the container as a "build container" - In another container (starting `FROM` the build container), remove the stack build stuff, but keep the binary, which leaves me with: base image + missing haskell-external build deps + dynamic binary For Fedora this means: 80Mb + ~540Mb + ~40Mb With this shallow calculation switching to debian 9 would shave me 60Mb at that size, leaving me with a 600Mb build image... but what I really want is the holy grail of `FROM scratch` + the binary. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/odbc/.../**Dockerfile** (master ‚Üí f939699)](https://github.com/fpco/odbc/blob/f93969939250a4e6ec1af0e0f8c7fa842016fc9b/Dockerfile) ---- 
Have you looked at **Racket**? It is almost exactly what you are looking for, except that it is untyped rather than CoC based. They even call it "language-oriented programming": http://racket-lang.org/
For a lot of operations working with the CPS'd code is much messier. That said, one major benefit of doing some form of CPS is that it makes analyses simpler. Matt Might's work on various forms of control flow analysis and abstract interpretation mainly starts with a CPS transformation because then 'returning' isn't special.
Oh yeah, a close parallel. You can think of a concatenative program like *F G H* as composing functions from left to right (apply F, then G, then H to the program state) or composing continuations from right to left (add H, then G, then F to the current continuation). For example, take `dip : [B] [A] -- A [B]`, a combinator that takes a quotation atop the stack and ‚Äúdips‚Äù underneath another value to apply the quotation to the remainder of the stack. You could implement that in Haskell in direct style or continuation-passing style: import Control.Arrow -- Direct and CPS primitives: swap ((s, a), b) = ((s, b), a) swap' k ((s, b), a) = k ((s, a), b) quote (s, a) = (s, \ s' -&gt; (s', a)) quote' k (s, a) = k (s, \ s' -&gt; (s', a)) compose ((s, b), a) = (s, \ s' -&gt; a (b s')) compose' k ((s, b), a) = k (s, \ s' -&gt; a (b s')) apply (s, a) = (a s) apply' k (s, a) = k (a s) -- dip = swap quote compose apply dip = swap &gt;&gt;&gt; quote &gt;&gt;&gt; compose &gt;&gt;&gt; apply dip' = swap' &lt;&lt;&lt; quote' &lt;&lt;&lt; compose' &lt;&lt;&lt; apply' -- 5 "ignored" [ 2 * ] dip == 10 "ignored" dip ((((), 5), "ignored"), second (* 2)) == (((), 10), "ignored") dip' id ((((), 5), "ignored"), second (* 2)) == (((), 10), "ignored") The only difference is the order of composition! :) 
The third point is what I was missing. Thanks!
Take a look at how Lucid (ab)uses type-classes to achieve this.
I agree. I see no reason to use any kind of varargs when Haskell is so syntactically light anyway. Instead of `my_function a b1 b2 b3 c`, the call then becomes `my_function a [b1, b2, b3] c` Hardly worth the effort involved in trying to implement varargs. Varargs do exactly the same thing in other languages, and only really exist because instantiating a new array/list inline can often be a big hassle in those languages, whereas that's not the case in Haskell.
I'd probably make it even simpler than that. Everything is all of the same type and you know what the first and last elements of the list are? Just take in one single argument `my_function :: [A]`; you likely aren't going to want to special case the first and last item of the list by passing them in as extra arguments when they're part of the actual list still. (Just as an irrelevant side-note, the Curry programming language can do this very naturally, imo: foo :: [A] foo ([first] ++ bs ++ [last]) = undefined is perfectly legal and will give you the first element, the last element, and the list in-between.)
Damn, IntelliJ plugin? That sounds great. I tried writing few things in Haskell, but got tired of it mainly because quite bad IntelliJ plugins (randomly stopped working, almost no auto-completion, not working quick-doc or navigation to libraries or even my own code in other modules and so on - quite basic stuff from my point of view).
Oh, I see. I think the simplest approach would be to use a [smart constructor] to reject non-string keys at construction time. Another alternative to ensure it at the type level would be to use newtype wrappers around the individual elements like this: data BValue = BIntVal BInt | BStrVal BStr | BListVal BList | BDictVal BDict deriving (Eq, Ord, Show) newtype BInt = BInt Integer deriving (Eq, Ord, Show) newtype BStr = BStr B.ByteString deriving (Eq, Ord, Show) newtype BList = BList [BValue] deriving (Eq, Ord, Show) newtype BDict = BDict (M.Map BStr BValue) deriving (Eq, Ord, Show) But that incurs a big wrapping/unwrapping penalty. That penalty can be mitigated with in various ways (eg with lenses) but for an application this simple I probably just do what the author did and accept "Bencode+" which allows non-string keys, or if there was a hard requirement that prevented this then I would probably use the smart constructor approach.
This is exactly the kind of thing that `product-profunctors` was designed to implement. In fact maybe I should add `Replicator` to that package as an example and useful utility function. {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE MultiParamTypeClasses #-} import Data.Profunctor import Data.Profunctor.Product import Data.Profunctor.Product.Default import Control.Applicative newtype Replicator r f a b = Replicator (r -&gt; f b) deriving Functor instance Applicative f =&gt; Default (Replicator (f b) f) b b where def = Replicator id replicateT :: Default (Replicator r f) b b =&gt; r -&gt; f b replicateT = f where Replicator f = def' def' :: Default p a a =&gt; p a a def' = def foo :: IO (String, String, String) foo = replicateT getLine -- &gt; foo -- Hello -- world -- ! -- ("Hello","world","!") -- Boilerplate that is derivable using generics but I never got round -- to implementing it. instance Applicative f =&gt; Applicative (Replicator r f a) where pure = Replicator . pure . pure Replicator f &lt;*&gt; Replicator x = Replicator (liftA2 (&lt;*&gt;) f x) instance Functor f =&gt; Profunctor (Replicator r f) where dimap g h (Replicator f) = Replicator ((fmap . fmap) h f) instance Applicative f=&gt; ProductProfunctor (Replicator r f) where purePP = pure (****) = (&lt;*&gt;) 
R|F|A|B| -|-|-|-| F|F| | | A| |A| | B| | |B| 
That's interesting. What are you doing inference on?
I think you may have got confused.
Thank you so much :)
[removed]
hmm. then I have to read the book again. I could not find a example of it. 
Yeah, there was a [recent thread](https://www.reddit.com/r/haskell/comments/8ap6tb/frege_language_has_a_compiler_for_java_8_and/) on it in which I asked them to compare against Eta. The answers seemed a bit vague tbh. I might re-ask OP here.
Interesting name!
I'd love to play with halvm, but no support for KVM makes it impossible :(. I've been playing with MirageOS at work (for ocaml) a bit lately, and I'm convinced that's the future, so much better than containers.
I'd love if there was more content like this, this is great for beginners like me :) Thank you so much! 
oke, found something &lt;*&gt; areq (jqueryDayField def { jdsChangeYear = True -- give a year dropdown , jdsYearRange = "1900:-5" -- 1900 till five years ago }) "Birthday" Nothing 
I was thinking about doing something like this recently. I am thinking of trying to use the [courier](https://hackage.haskell.org/package/courier-0.1.1.5/docs/Network-Endpoints.html) package, and then convert between courier/FRP \(emitting incoming messages as FRP events, and also sending FRP events over the network as they are created \(is this push FRP?\)\).
I was under the impression that Nix packages that updated built on each other (so you could have 5 completely independent versions of say `redis` without using 5x the disk space) -- but looking back at the [Nix Docs](https://nixos.org/nixos/about.html) I see that's not how it works. I really need to give Nix a try one of these days -- might be able to hit a sweet spot if I figure out completely static compilation for haskell -- it seems like the best OS to have deterministic builds in.
I'd never looked into [MirageOS](https://mirage.io/) before -- it looks really cool actually. [It even works on ARM64](https://mirage.io/wiki/arm64)
Here's something you can study :) `Server.hs`: {-# language OverloadedStrings #-} module Main ( main ) where import Control.Concurrent ( forkIO, threadDelay ) import Control.Monad ( forever, join ) import Data.Function ( fix ) import Data.Monoid ( (&lt;&gt;) ) import Reactive.Banana import Reactive.Banana.Frameworks import qualified Data.ByteString as StrictByteString import qualified Network.Socket as Network hiding ( recv ) import qualified Network.Socket.ByteString as Network import qualified Data.Text.Encoding as StrictText import qualified Data.Text.IO as StrictText server :: Network.Socket -&gt; MomentIO () server serverSocket = do -- An event for when a client connects. The payload of the event is the -- Socket connected to the new client. clientConnected &lt;- do (ah, handle) &lt;- liftIO newAddHandler -- Fork a new thread that constantly tries to accept new connections. -- Whenever 'accept' succeeds, pass the 'Socket' to 'handle'. liftIO ( forkIO ( forever ( Network.accept serverSocket &gt;&gt;= handle . fst ) ) ) -- Convert the 'AddHandler' into an 'Event'. fromAddHandler ah -- When a client connects, build a new 'Event Text' - a stream of messages -- sent from this client. messageReceived &lt;- -- We use 'execute' as we need to do some 'IO' to build our 'Event'. execute ( -- fmap over clientConnected to see the client 'Socket'. clientConnected &lt;&amp;&gt; \clientSocket -&gt; do (ah, handle) &lt;- liftIO newAddHandler -- Fork a thread that repeatedly tries to 'recv' from the client -- 'Socket'. If we ever get an empty ByteString, the client has gone -- so we stop trying to recv messages. liftIO ( forkIO ( fix ( \again -&gt; do bytes &lt;- Network.recv clientSocket 1024 if StrictByteString.null bytes then return () else do handle ( StrictText.decodeUtf8 bytes ) again ) ) ) fromAddHandler ah ) -- Interleave all received messages from all clients. anyMessageReceived &lt;- accumE never ( unionWith const &lt;$&gt; messageReceived ) &gt;&gt;= switchE reactimate ( anyMessageReceived &lt;&amp;&gt; \msg -&gt; StrictText.putStrLn ( "Received " &lt;&gt; msg ) ) main :: IO () main = do let hints = Network.defaultHints { Network.addrFlags = [ Network.AI_PASSIVE ] , Network.addrSocketType = Network.Stream } addr : _ &lt;- Network.getAddrInfo ( Just hints ) Nothing ( Just "3000" ) serverSocket &lt;- Network.socket ( Network.addrFamily addr ) ( Network.addrSocketType addr ) ( Network.addrProtocol addr ) Network.bind serverSocket ( Network.addrAddress addr ) Network.listen serverSocket 10 compile ( server serverSocket ) &gt;&gt;= actuate forever ( threadDelay maxBound ) (&lt;&amp;&gt;) :: Functor f =&gt; f a -&gt; (a -&gt; b) -&gt; f b as &lt;&amp;&gt; f = f &lt;$&gt; as infixl 1 &lt;&amp;&gt; `Client.hs` {-# language OverloadedStrings #-} module Main ( main ) where import Control.Concurrent ( forkIO, threadDelay ) import Control.Monad ( forever, join ) import Data.Monoid ( (&lt;&gt;) ) import Reactive.Banana import Reactive.Banana.Frameworks import qualified Network.Socket as Network hiding ( recv ) import qualified Network.Socket.ByteString as Network import qualified Data.Text.Encoding as StrictText import qualified Data.Text.IO as StrictText client :: Network.Socket -&gt; MomentIO () client clientSocket = do -- An event for a line of text. lineEntered &lt;- do ( ah, handle ) &lt;- liftIO newAddHandler liftIO ( forkIO ( forever ( StrictText.getLine &gt;&gt;= handle ) ) ) fromAddHandler ah -- Whenever a line of text is entered, send it to the server. reactimate ( Network.sendAll clientSocket . StrictText.encodeUtf8 &lt;$&gt; lineEntered ) main :: IO () main = do let hints = Network.defaultHints { Network.addrFlags = [ Network.AI_PASSIVE ] , Network.addrSocketType = Network.Stream } addr : _ &lt;- Network.getAddrInfo ( Just hints ) ( Just "127.0.0.1" ) ( Just "3000" ) clientSocket &lt;- Network.socket ( Network.addrFamily addr ) ( Network.addrSocketType addr ) ( Network.addrProtocol addr ) Network.connect clientSocket ( Network.addrAddress addr ) compile ( client clientSocket ) &gt;&gt;= actuate forever ( threadDelay maxBound ) (&lt;&amp;&gt;) :: Functor f =&gt; f a -&gt; (a -&gt; b) -&gt; f b as &lt;&amp;&gt; f = f &lt;$&gt; as infixl 1 &lt;&amp;&gt; 
Hawaii is the only place in the U.S. where bananas are grown commercially, although at one time they were also grown in southern California and Florida. The overwhelming majority of the bananas Americans eat come from countries in Latin America and South America, including Costa Rica, Ecuador, Colombia, Honduras, Panama, and Guatemala. *** ^^^I'm&amp;#32;a&amp;#32;Bot&amp;#32;*bleep*&amp;#32;*bloop*&amp;#32;|&amp;#32;[&amp;#32;**Unsubscribe**](https://np.reddit.com/message/compose?to=BananaFactBot&amp;subject=I%20hate%20potassium&amp;message=If%20you%20would%20like%20to%20unsubscribe%20from%20banana%20facts%2C%20send%20this%20private%20message%20with%20the%20subject%20%27I%20hate%20potassium%27.%20)&amp;#32;|&amp;#32;[üçå](https://np.reddit.com/r/BananaFactBot/comments/8acmq6/banana/?st=jfof9k8d&amp;sh=acd80944)
Bad bot.
Thank you, ocharles, for voting on BananaFactBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
What. I need to dive deeper in to this package. Thanks for the motivating example!
Is this true? Don't the fundeps prevent lifting `MonadState` through a `StateT`?
See page 74 of [this](http://www.lambdadays.org/static/upload/media/1520323372815660jarekratajskibeautyandthebeasthaskellonjvm.pdf) The benchmark is 12 queens problems. Frege took 45 seconds, Eta took 26 seconds.
Ah, that seems to be exactly what I was looking for, even with comments ! Thanks a lot :)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dalaing/reflex-server-socket/.../**Server.hs** (master ‚Üí a04d059)](https://github.com/dalaing/reflex-server-socket/blob/a04d059843d9a7a0d6c8b992e78699baa056592d/example/Server.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dy9fbha.)
I am currently working my way through papers dealing with graph theory in Haskell. In *Martin Erwig: Inductive Graphs and Functional Graph Algorithms*, the following ADT is shown data Graph a b = Empty | Context a b &amp; Graph a b where `Context a b` is just some type synonym for a tuple containing a given node's predecessors, its identifier, its label, and its successors. Furthermore you have to watch out that these predecessors/successors are *already in the graph when you mention them*. I assume that this invariant has something to do with the following phrase: *"The above definition suggests that graphs are isomorphic to lists, however this is not the case because graphs are not freely generated by `Empty` and `&amp;`."* What exactly does *"freely generated"* mean? I found some stuff on wikipedia and math overflow that went over my head, and know a little about the handwavy definition of *free* constructions like free monads, but I feel at this point this is not enough anymore. Could somebody please help clear things up, preferably with an example from the realm of mortals? Thanks.
The example would turn into something like this: class Triple a b c where triple :: (a, b, c) instance Triple () () () where triple = ((), (), ()) newtype A = A () newtype B = B () newtype C = C () deriving via () () () instance Triple A B C -- Instead of deriving via () instance Triple A B C -- We could also do deriving via () B () instance Triple A B C -- to use a different instance with one parameter to via for each type parameter of the type class. Each parameter can be chosen independently of the others. I'm not sure how well it would work from an implementation perspective, functional dependencies and stuff could probably cause trouble, but from a user perspective it would really nice.
Reflex is for doing web app, isn't it ? Or can it be used for regular apps too ?
I agree here. Our stuff is not publically facing so we less concerned about it :) But it is definitely possible to do what you are saying.
What is a compact region, in the context of compiling, memory management and garbage collection?
Sounds like a view pattern, or `-XNPlusKPatterns`
It can be used for regular apps as well :) I've got a few things brewing on that front, but it'll be a month or so before I get back into them properly. 
Hopefully this answers your question: https://hackage.haskell.org/package/compact
&gt; Is it even possible to do TCP networking with reactive banana Sure, why not? In any reactive-banana program, there is a part in which you define the way in which your FRP network interacts with the IO of the outside world. This is the part of your program in which you‚Äôll use [this part](http://hackage.haskell.org/package/reactive-banana-1.2.0.0/docs/Reactive-Banana-Frameworks.html) of the library. It doesn't matter whether the IO events you react to are mouse clicks, GUI widget events, or TCP packets, nor whether the IO actions you react with are drawing stuff with OpenGL, changing GUI widget states, or sending TCP packets back. &gt; I've tried googling how to do networking with reactive banana, because it looks like the easiest FRP lib to get into, but the fact that network is used for something else in FRP makes it a bit hard to google. I doubt you'd find anything anyway. Most examples out there implement GUIs and games, you're unlikely to find an example of using FRP with anything else. &gt; I've found one single comment on some post saying you can't because you'd need asynchronous FRP, which it isn't (or wasn't ?). [Some FRP libraries](https://github.com/gelisam/frp-zoo) use the "asynchronous data flow" approach, meaning that each node in the FRP network processes events at their own speed. Those look more like the Actor model than like FRP to me, but whatever. Most Haskell implementations of FRP are synchronous, meaning that each time an IO event occurs, it propagates through every node in the FRP network, producing zero or more IO actions, before the network can consider the next IO event. This may sound like a limitation, but if you don't want your program to respond to IO events immediately, there are easy workarounds. Since you can define the IO events to be whatever you want, you can easily add fake "0.1 seconds have passed" IO events from which you can implement delayed responses. Or you could define a "run this `IO a` computation in a thread" IO action, with a corresponding "some thread has finished with this `a` result" IO event. The asynchronous data flow approach has one major disadvantage, which is that you cannot rely on the order in which the IO events occur, because that order might get scrambled as the events traverse the network. For example, suppose we're implementing a program with two buttons, in which the user may press and release a button to perform its action, or press and then move the mouse away and release if they change their mind. In a synchronous FRP system, you could have different nodes focus on the mouse down IO events which occur on the different buttons, and each of them would generate a stream of `(Button, IO ())` occurrences representing the button over which the mouse down IO event occurred and the action we should perform if the user releases the mouse over that same button. This stream would get combined with the mouse up IO events in yet another node, which would emit the `IO ()` action if and only if the mouse up IO event occurs over the same button as the previous `(Button, IO ())` occurrence. In an asynchronous FRP system, this implementation would not work. Suppose the user presses the top button, then changes their mind by moving their mouse over the bottom button and releasing; then they press the bottom button and again change their mind, moving the mouse over the top button and releasing. In that case, we don't want any `IO ()` action to execute, but if the nodes which process the top button are much slower than the nodes which process the bottom button, the final node which receives the `(Button, IO ())` occurrences and the mouse up IO actions might get the `(Button, IO ())` from the bottom button before the `(Button, IO ())` from the top button, even though the user performed a mouse down on the top button first. If the mouse up events arrive in their original order, first over the bottom button and then over the top button, we might incorrectly conclude that the user pressed and released the mouse on the bottom button, and execute the `IO ()` event. This is a downside for asynchronous data flow, for sure, but I'm wondering... in a distributed system, we can't afford to have all the machines synchronize with each other, each machine runs on their own clock and we have to deal with the same issue of messages being delivered late or out of order. So perhaps the comment you saw was about using FRP to describe the behaviour of a group of machines communicating with each other over TCP, not about using FRP to describe the behaviour of a single machine?
I don't see how viewing type systems as macros when one is implementing a new `#lang` is at odds with Racket itself (the metalanguage) being untyped. Can you elaborate?
I think the section on record syntax here will help. http://learnyouahaskell.com/making-our-own-types-and-typeclasses
There is also Typed Racket, the sister language of Racket with a static type-checker: https://docs.racket-lang.org/ts-guide/index.html https://docs.racket-lang.org/ts-reference/index.html
 kb1 :: Kasse -&gt; Datum -&gt; Uhrzeit -&gt; Wahlessen -&gt; Kassenbon kb1 = (1 "Anna") ((Montag) Februar) 2018) (0 15 13) ((Normal) 2.5) This bit shows a number of misunderstandings. If you want to create a Kassenbon, you don't need a function kb1 that takes a Kasse, Datum, Uhrzeit, etc to get a Kassenbon. Why? Because you already have one. It's called a data constructor and you got it for free when you defined the Kassenbon type: data Kassenbon = Kassenbon {kasse :: Kasse , datum :: Datum , uhrzeit :: Uhrzeit , wahlessen :: Wahlessen} deriving (Show) The Kassenbon on the left of the "=" is the type constructor. With it, you create a new types. Yours doesn't have a type variable: so your constructor only makes a single type, namely Kassenbon. The Kassenbon on the right side of the "=" is the data constructor. You didn't have to call it "Kassenbon". You could've called it "DerpDerp": data Kassenbon = DerpDerp {kasse :: Kasse , datum :: Datum , uhrzeit :: Uhrzeit , wahlessen :: Wahlessen} deriving (Show) But let's go back to what you did: data Kassenbon = Kassenbon {kasse :: Kasse , datum :: Datum , uhrzeit :: Uhrzeit , wahlessen :: Wahlessen} deriving (Show) and look at the type of the data constructor Kassenbon: Œª&gt; :t Kassenbon Kassenbon :: Kasse -&gt; Datum -&gt; Uhrzeit -&gt; Wahlessen -&gt; Kassenbon The data constructor, Kassenbon, is a function populates the type Kassenbon. In fact, it's the *only* way to get terms of type Kassenbon. So if I want to make a Kassenbon, I just use my data constructor. The same argument goes for all of your other types. Note that for the type data Wochentag = Montag | Dienstag | Mittwoch | Donnerstag| Freitag | Samstag | Sonntag deriving (Show) You have 7 different data constructors. Namely, Montag, Dienstag, etc... Each of them produces a single term that has type Wochentag. Let's now return to kb1 :: Kasse -&gt; Datum -&gt; Uhrzeit -&gt; Wahlessen -&gt; Kassenbon kb1 = (1 "Anna") ((Montag) Februar) 2018) (0 15 13) ((Normal) 2.5) So, we've already established that if we want to make a Kassenbon, we should use the data constructor. So let's start off with that: kb1 :: Kassenbon kb1 = Kassenbon Now we, since Kassenbon :: Kasse -&gt; Datum -&gt; Uhrzeit -&gt; Wahlessen -&gt; Kassenbon, we need a Kasse. Let's use the one you wanted to. Kasse has a single data constructor Kasse :: Integer -&gt; String -&gt; Kasse, so we need to use that to get a Kasse: kb1 :: Kassenbon Kassenbon (Kasse 1 "Anna") Now, let's add in all the other bits, using the apropriate constructors kb1 :: Kassenbon kb1 = Kassenbon (Kasse 1 "Anna") (Datum Montag Februar 2018) (Uhrzeit 0 15 13) (Wahlessen Normal 2.5) 
You need to use a type's data constructors to construct elements of that type. For example, in the definition data Kasse = Kasse { kassenID :: Integer , namen :: String } deriving (Show) you don't just define a type named `Kasse` (the one before the `=`), you also define a data constructor called `Kasse` (the one after the `=`). We could name them differently to emphasise that they are not the same thing: data KasseT = KasseC { kassenID :: Integer , namen :: String } deriving (Show) The data constructor `KasseC` is now a function with type type `Integer -&gt; String -&gt; KasseT`. I.e. it constructs an element of type `KasseT` if you give it an `Ingeger` and a `String`: k1 :: KasseT k1 = KasseC 1 "Anna" So to define `kb1`, we want to construct an element of type `Kassenbon`, so we use the data constructor `Kassenbon` and give it the four arguments it expects, each constructed by it's respective data constructor: kb1 :: Kassenbon kb1 = Kassenbon (Kasse 1 "Anna") (Datum Montag Februar 2018) (Uhrzeit 0 15 13) (Wahlessen Normal 2.5) 
I didn't want to complicate the proposal but proposing this would be the next step
Take a look at Dhall. It's pretty much what you describe :). https://github.com/dhall-lang
There was a short talk at BayHac about this, so you might want to look it up when the video gets released. Meanwhile, my best attempt. "A structure is freely generated by some operations if the only equalities between two instances of the structure are syntactic." The statement you're asking about is saying that graphs (the structure) are not freely generated because two graphs might be equal despite being constructed in different ways. Does that help any?
You may want three more columns to the table for additional operations that people may find helpful depending on their problem domain: 1. Project -- project a record to a subrecord that has a subset of the fields, possibly by specifying the target type, a type-level list of the field names of the target type, or the names of the fields to drop. 2. field map -- `fmap` focusing on a particular field. E.g. if some field 'name' has type [Text], maybe we can `fmap length` over it to get a new record type where the type of 'name' is now Int. 3. global map -- for some constraint C, fmap some function of type `forall a. C a =&gt; a -&gt; b` on every field. For operations where there output record type is a different than the input record type, there is also the question of whether the user needs to explicitly specify the output type, whether it can be automatically inferred or obtained mechanically with some type-level mechanism. For example, in the 'field map' operation, the input type, the identity of the field being fmapped and type of the fmap function determines the output type, so it can in principle be inferred.
You may want to look into using a `ReaderT` over your `Time` value. For example, this type of interpreter would be a good first step: type Time = Double type LiveCoding a = ReaderT (IORef Time) IO a Then you can have functions that read that `Time` value to produce a new time, and increment it: lfo :: LiveCoding Double lfo = do -- Get the time reference tRef &lt;- ask -- Read the value from it t &lt;- readIORef tRef -- Increment it liftIO $ modifyIORef tRef (+1) -- Return the 'lfo' value pure $ sin $ 2*pi*t*3 This pattern could be abstracted: withTime :: (Double -&gt; Double) -&gt; LiveCoding Double withTime f = do tRef &lt;- ask t &lt;- readIORef tRef liftIO $ modifyIORef tRef (+1) pure $ f t lfo :: LiveCoding Double lfo = withTime (sin . 2 * pi * 3) Now, I'm not sure if these semantics are correct. But if, for instance, you wanted to increment `t` constantly in the background in fixed time steps, you can use the `IORef` to do the same sort of thing: incTime :: IORef Time -&gt; IO () incTime tRef = do tRef &lt;- ask t &lt;- readIORef tRef modifyIORef tRef (+1) -- Wait 1ms between increments runTimer :: IORef Time -&gt; IO () runTimer tRef = forever (threadDelay 1000 *&gt; incTime tRef) An example program using this would look something like... main :: IO () main = do tRef &lt;- newIORef 0 -- fork the runTimer process (note the 'async' package provides nicer ways to do this): forkIO (runTimer tRef) runReaderT liveCoding tRef liveCoding :: LiveCoding () liveCoding = do lfoValue &lt;- lfo liftIO $ do print lfoValue -- Wait a second, then do it again threadDelay 10000000 Hope that helps. There's a nice article on this type of architecture called the ReaderT Design Pattern that might be of use: https://www.fpcomplete.com/blog/2017/06/readert-design-pattern
&gt; You may want three more columns to the table for additional operations that people may find helpful depending on their problem domain Agreed! As I write in my post, the number of columns and rows are work in progress, and subject to expansion. &gt; project Isn't this already covered by the two columns called subcast and supercast? &gt; field map Sounds useful! &gt; global map -- for some constraint C, fmap some function of type forall a. C a =&gt; a -&gt; b on every field. Sounds useful as well! &gt; For operations where there output record type is a different than the input record type, there is also the question of whether the user needs to explicitly specify the output type, whether it can be automatically inferred or obtained mechanically with some type-level mechanism. For example, in the 'field map' operation, the input type, the identity of the field being fmapped and type of the fmap function determines the output type, so it can in principle be inferred Yes, I think inference here is ideal. But in case the packages do not support universally good inference, we can indeed specify different kinds of inference as feature columns.
So I played with this a bit this afternoon, it's much simpler than I imagined. I've made a little IRC bot that works great with this, so thanks again. I do have a bonus question, what would be the proper way to integrate conduits with reactive-banana ? I imagine I coud pass handle to runConduit when creating the new event, maybe ?
It's also worth noting that you see this less often these days due to the popularity of `TypeFamilies` class Monad m =&gt; MonadState' m where type State m :: * get' :: m (State m) set' :: State m -&gt; m () modify' :: MonadState m =&gt; (State m -&gt; State m) -&gt; m () Although in some instances, this requires the use of `~` constraints. atM :: (MonadState m, State m ~ Int) =&gt; [a] -&gt; m a atM as = (as !!) &lt;$&gt; get
I'm aware of how useful it would be if your data set is reasonable size but if you have a moderate workflow on 500gb I wonder where you start to notice . . .
Erlang does, AFAIK.
Honest question: how many people use IntelliJ for Haskell dev?
Scala does, but it's not purely functional.
Idris.
I don't know any. My understanding is that it complicates type inference, so maybe that's why is not so popular. Anyway, I found this if it is of any use to you: [http://users.cis.fiu.edu/~smithg/papers/thesis91.pdf](http://users.cis.fiu.edu/~smithg/papers/thesis91.pdf)
&gt; field map -- fmap focusing on a particular field. E.g. if some field 'name' has type [Text], maybe we can fmap length over it to get a new record type where the type of 'name' is now Int. How does this work? Wouldn't you need a type parameter or something for your type?
Looks like it has. I'm trying to find how it's precisely defined/restricted in Idris. Any links?
you'll probably wanna look at [yesod](https://www.yesodweb.com/). it's a great project that's surprisingly modular, but can do a lot for you (a la django). the database library developed for it, `persistent`, is probably the most widely used in haskell. it's wonderful.
Thanks for the link!
I think you need to be able to refer to it by a unique path in ambiguous cases. For instance if you want two functions of the same name in the same module, you need to [define them in namespaces](http://docs.idris-lang.org/en/latest/tutorial/modules.html#explicit-namespaces).
I can fill in the labels row, because I'm the author. My email is chrisdone@gmail.com 
Okay, I think I get how this works in Idris now. Thanks!
This is also how list syntax is overloaded. It just looks for `(::)` and `Nil` constructors/functions. You could have multiple of these in scope at once, but in certain instances they might have the same types, so you'd have to disambiguate. (not sure the exact semantics, I haven't gone too wild with Idris recently)
Yeah, If I can figure it out I'll definitely post it here -- it might just be a matter of finding statically linkable alternatives to everything -- musl lib c gets us a long way, and it looks like `libns` might be the final piece of the puzzle.
I recently completed UPenn's CIS194. What should I do now to become better at haskell?
I have no idea, interesting question. But I would guess not many, since the tooling-IDE connection is so fragile and feels immature (choice is between a plugin offering some autocompletion, docs, navigation, but can break at any moment even without messing with compiler versions or build files and is difficult to setup; and other plugin which is more stable and easy to setup, but lacks even those basic features which offers the first plugin). I personally am just toying with Haskell in my spare time, but at work I am using IDEA for everything (front-end dev - mainly JavaScript and TypeScript). I have tried other IDEs for various languages and always found missing so many useful features I am accustomed to from IntelliJ IDEA. At this point I don't think I would be happy using any other IDE for anything else. :-/
How does Haskell parse expression `1-1` properly? I thought it would be interpreted as `1 (-1)`, and cause an error.
Any courses or books that you recommend for Haskell ?
The HasFoo thing can be generated directly by lens (assuming you want to do it the lens way). https://hackage.haskell.org/package/lens-4.14/docs/Control-Lens-TH.html#v:makeClassy This is particularly useful if you want to read and write to the state. It's basically a subset of approach (B).
Mostly it's just that only a few of us really *know* Nix, which is a problem because Nix isn't exactly easy to pick up on.
You probably want to create an `Handler` that you push into for each item in the Conduit stream. So I'd create an `AddHandler` and fork a thread that consumes the conduit and calls the handler for each item. That essentially converts a `Conduit a` into `Event a`. Does that help?
&gt; At this point I don't think I would be happy using any other IDE for anything else. I totally get that sentiment. Having to switch editors can be a real drain on efficiency for a while eh? If you do find that you're willing to venture outside of IntelliJ as an experiment, there are decent setups for Haskell. Actually, now that I think, you could probably get by with IntelliJ open in half your screen, with a terminal window running [ghcid](https://github.com/ndmitchell/ghcid) beside it.
This means the output record type is different than the input record type. The library will have to infer the new type based on the type of the function.
Ah yes, I was actually just trying it out and I've got it working : ```liftIO $ forkIO $ runConduitRes $ streamUser client .| CL.mapM_ (lift . handle)``` Again, much simpler than anticipated, this is great :)
&gt; Does that help any? Indeed it does. As a matter of fact, the very first example graph in the paper is constructed in two different ways, from different starting vertices. Even though `graph1 /= graph1'` via `deriving Eq`, the equality should logically hold. Thanks for your help, and for mentioning the BayHac talk also.
Well if you aren't allergic to Xen like I am, you could just use the current version. I haven't tried it at all but they are using it to release commercial applications so it must be working okay
I was thinking that `-XNegativeLiterals` was only changing the desugaring of `-x` from `- fromIntegral x` to `fromIntegral (-x)`, but it also changes the parser, and allows a more "natural" handling of negative literal. Thank you.
&gt; Racket is neither a typed language nor an untyped language. It is a metalanguage And ‚Ä¶ that metalanguage is untyped, no? lexi-lambda's work on Hackett (which is incredible) doesn't change that.
I don‚Äôt think it makes too much sense to talk about the typing discipline of the underlying language here. Racket languages are mini-compilers; if Racket is ‚Äúfundamentally untyped‚Äù because it‚Äôs core language is untyped, then Haskell is *also* fundamentally untyped because STG is. This is, of course, a silly perspective to take. What‚Äôs perhaps more interesting to discuss is how most languages in Racket are dynamically typed. I think this is going to often be the case, however, even if you build your languages atop a typed core, since implementing a type system in a typed language is not significantly easier than implementing a type system in an untyped language, unless your type system translates to the underlying system so trivially that its error diagnostics are good enough for users of the source language. (This is essentially never true in practice, though perhaps research could be done to improve such a thing, in the same way Racket has done research to improve error reporting of macros that desugar into other forms.) If, on the other hand, you mean the language you write macros in is untyped in Racket, that‚Äôs a misunderstanding. Nothing in Racket prevents you from using any arbitrary custom language as the ‚Äúmetalanguage‚Äù, including Hackett. I just haven‚Äôt been using Hackett for that currently because Racket‚Äôs library support is far better than Hackett‚Äôs for the purpose of macro construction. Eventually, maybe Hackett will be more self-hosting in that respect.
Take a look at Dhall. Pretty much what you describe in terms of the CoC backend. Only need to start writing frontends for it :).
&gt; Everything is all of the same type Everything was not of the same type in the example he gave.
There is also the ability to "traverse" usually called traverse, which allows when the record is parametrised by a functor to do things like `record f -&gt; f record Identify`. Really useful to do validation.
I think this is the first time I've seen a company brag about its surge protectors in a jobs posting...
There is also `extensible` which is quite good and somehow `Metamorphosis` which my way of solving the record related problems that I had. It's only only on [GitHub](https://github.com/maxigit/Metamorphosis) but I will put on Hackage if people show some interest. (Tl;dr it solves the record problem mainly by creating new plain Haskell record using TH by merging or modifying existing data type).
Hey, thanks for these suggestions! Would you like edit access to the Google Sheet and fill in the feature-cells for these two approaches? If so, please send me your gmail address.
Some more: - https://github.com/nfrisby/coxswain - https://hackage.haskell.org/package/record I would also add a column about whether a solution inlines well. For example vinyl has O(n) operations, but it could inline well (it doesn't currently, but could in principle) for that to not matter in various cases. In contrast, I wouldn't expect that much from any `Map` or primitive `Array`-based solutions. How can any of those do O(1) updates? Persistent data structures (which all of the mentioned solutions seem to be) need to reconstruct the updated record from scratch, which is O(log n) with tree-based approaches, and O(n) for everything else. Can GHC somehow reuse a record that it knows would otherwise become unreachable? (This doesn't solve the O(n) complexity in the worst case where the original value needs to persist anyway.) (With linear types we could have safe, purely-functional records with constant-time update!) 
I might go to sleep in a few hours. After that you can send your email address to /u/vasiliy_san and he can give you edit access as well.
&gt; https://github.com/nfrisby/coxswain Oh, thanks for this, I almost forgot to add. I hope someone who has experience with this will fill the relevant feature-cells. &gt; record This is already in there with `record-preprocessor` which is AFAIU the recommended way of using `record`; isn't it? &gt; How can any of those do O(1) updates? Oh, I think you are right! Much of those O(1) are supposed to be O(n). My bad, intend to ameliorate. --- Since I'm asking everyone else, I wouldn't want you to feel left out; would you also like edit access? ;)
To add onto the servant suggestion, [here](https://github.com/parsonsmatt/servant-persistent)'s a somewhat outdated article on using servant and persistent-postgresql together.
You mean what framework to use? Try gtk-gi or FLTKS
Has there been a proper discussion anywhere about [SPJ's old proposal](https://www.microsoft.com/en-us/research/wp-content/uploads/1999/01/recpro.pdf)? The [variant you linked](http://web.archive.org/web/20160322051608/http://research.microsoft.com/en-us/um/people/simonpj/Haskell/records.html) mentions the translation to Core as the main problem. But I think that these days, with type families, it could be translated to Core without any change to Core itself. Regardless, even if that other variant is the better one, either of these seems like a fairly good solution that I'd like to see in GHC. Main concerns I'd have would be I'd rather not overload `.`, and the type level version of records should just be DataKinds. Is there a discussion around these variants that we can rekindle? Perhaps on the ghc-proposals repo?
It would indeed be interesting to see old discussions around this if they exist. Maybe in one of the mailing list archives? &gt; Main concerns I'd have would be I'd rather not overload `.` Yeah, I'm not sure if I like that or not either, but maybe we can add it as a feature column: "Overloads existing syntax? Yes/No". Btw, I remember you liking this earlier too, have you familiarized yourself with the proposal since then? And if so, are you willing to fill in the relevant feature-cells on how, according to that proposal, this GHC feature would stack up compared to all the current approaches?
There are multiple semigroup operations available for `Either`. The existing one is "find the first `Right` value", which makes sense if you're looking through possibilities. The second is trying to promote an underlying semigroup structure through `Either`. That can also be what you want. This is a common and unfortunate gap in type classes. Because the resolution is entirely type-driven, you cannot easily cope with multiple operations that have the same structure on the same types. The normal work-around is to try to pick the most common instance on the base type, and express the others using `newtype` wrappers.
The `Semigroup` instance isn't exactly what you're after, but you should have a look at [`Validation`](https://hackage.haskell.org/package/either-5/docs/Data-Either-Validation.html)! :-)
For an example of using FRP for something other than gui and games, I recently made http://hackage.haskell.org/package/reactive-banana-automation and am using it for home automation including controlling a solar powered fridge.
I could fill out many of the columns, though many are not applicable due to lack of implementation. elvishjerricco@gmail.com
What you want to learn is category theory, check out the haskell wiki and bartoszv milewski's video series and online blog (which is basically an ebook), I would definitely go down that route over the one you suggested. Also, "the monad challenges" on github. Would add links if I wasn't on my phone.
For learning types and the theory of programming languages there is also a book I really liked called Practical Foundations of Programming Languages. It might complement Types and Programming Languages well for you, because PFPL seems to have a different approach to the same topics in the beginning, and then a lot of cool applications: state, concurrency, parallelism, and other pretty complex ideas in terms of types and logical inference. It is cool to see it done and it is surprisingly simple to do work through! Your book list looks really good to me but maybe it will be more engaging to mix up the order a bit, and read about algorithms and big data structures at the same time as preservation theorems and recursive types. There are exercises in Types and Programming Languages, but they are mostly proofs, and even the programming assignments are very much "textbook style" homework problems. I think there is no better way to learn the theory of a language than to dive into the practical use and see what problems and solutions come up. 
Probably for many companies it‚Äôs somewhat of a hassle and based in the US correlates with US working hours. For us there are regulatory/compliance reasons that having employees outside the US causes causes an ongoing headache for both us and the employee. I‚Äôm not familiar with the details, but my cofounder has talked to some similar companies who explained the problems it created. Lame situation :/
I note a lack of check if the records have compact representations. Far more than O(n) append or whatever, a lack of a compact representation will totally tank performance where it matters most.
By the way, could someone who has experience with Purescript fill out that row? Or just post the responses here as a comment? Either way is fine, I can put them in. I'm interested to see how their fabled row polymorphism stacks up against the other contenders.
Compact in what sense? Could it be that column `AA`, "supports unordered fields?" is close to what you mean?
Great! Do you think you have enough experience with `extensible` that you may be willing to fill in that row? Can I give you edit permissions?
[removed]
Some `vinyl` operations inline, some don‚Äôt. I‚Äôve tried to ensure that everything typically used in an inner loop (by me) does. The benchmarks are in the package, and I‚Äôm more than happy to add more especially if they show bad performance.
Do you think categories before or after types? It is a chicken and egg thing I think. 
I see, indeed the `Data.Vinyl.Lens` module , and it is probably the most relevant part to being a "record library", so that's mostly fine. I was referring to the recursive definitions in `Data.Vinyl.Core` and `Data.Vinyl.ARec` which are admittedly not as "record-like", so I'll take my comment back. I could also submit a patch myself but I'm currently balking at the idea of having one class for each method, especially since this is an implementation detail that leaks into the type signatures. I would rather come up a good way to refactor all of that when I have some free time, but if I find a user with a need for it, I'll be right around the corner with a PR (if you don't get to it first!). 
If you want to learn about programming language semantics through self-study, there's no other resource that comes anywhere close to the first two volumes of *Software Foundations*. Free interactive workbooks about programming language theory.
The "one class for each method problem" is huge, but I'm coming to prefer that over having non-inlined things. So many uses have concrete types, in which case you don't need to write the context. The 'ARec' module is new-ish, but provides `O(1)` access times. The `Core` module is where most of the recursive heritage lives, but I think what I want to do there is provide a `Recursive` module if you don't want the contexts or compile times, but by default have the inlinable versions. Any thoughts on that?
Keeping the recursive definitions around definitely sounds like a good idea!
I haven't ever used a Xen hypervisor (especially not directly) -- all the VPSes and dedicated hardware I've used were KVM... And I'm not quite ready to jump into OCaml just yet, looks like I'll be using small distros for a while longer :)
I see. Good luck with the hiring! :) 
So here is an admittedly fairly brutal question that I pose to anybody who offers me "monadic lenses": What, if any, are your laws? i.e. What tools can you offer users to reason about these things? That is how we got to `lens` in the first place. Other `lens`-like packages at the time, such as `fclabels` and the like all started shoehorning an arbitrary monad or a `Kleisli m` into the middle of a perfectly good notion of a lens, because it seemed useful, but not a single person could tell me at any laws that applied to the notion of a monadic lens. In fact, when I went to go dig into the topic further, I found that if you start trying to offer monadic "lenses" that work through IORefs and the like in IO, then you _can't_ pass anything at all like the lens laws. It is easy to get into situations where you have a system of IORefs that form a cycle (for a minimal example, just work with Fix IORef) and if your network of IORefs ever has a cycle in it, you can have contradictory needs to fit two different things into one reference to pass get/put and put/put simultaneously. Finally, from what I'm seeing, in all of your work you are using the type parameters of `s` and `t` completely independently, so you're going to be further challenged when it comes to laws. We went to package up a notion of "actions" into `lens` at some point, thinking they'd fit nicely in with the rest of the system. We ultimately were able to easily form some notion of a "monadic action" that acted like a getter or fold with monadic side-effects, but the above situation ultimately killed any expression of a "monadic lens." The little stuff we did have ultimately wound up getting exiled to `lens-action` and largely forgotten by users.
Yeah, I've long waited for some alternative that was the best of both worlds. Originally I had manually unrolled the lens-related loops enough that short records were quick, but people immediately outgrew that so we switched to the class. I hope that offering both will cover our bases without incurring too heavy a burden on users.
I am on the same road, though I was not so focused on my studies of late. I will share some advice, but it is up to you to decide whether to follow it or not, taking into account that I am not yet anywhere near the end of the road. My first opinion is that you do not need to learn hand-written proofs per se. Rather, toss in some automated proving manual, such as the already mentioned [*Software Foundations*](https://softwarefoundations.cis.upenn.edu/) or, perhaps, [*Certified Programming with Dependent Types*](http://adam.chlipala.net/cpdt/). People are going to point out that you are missing out on Category theory. My second opinion would be that you need to pay less attention to popular texts and rather favour academic sources. You will be pointed to [the book of Saunders Mac Lane](https://en.wikipedia.org/wiki/Categories_for_the_Working_Mathematician) but it is somewhat hard and aimed at folks that are comfortable with the usual algebra, group theory and so on. ("Working mathematicians", right.) A more approachable treatment would be *Category Theory* by Steve Awodey. A little advice to follow the last point is to read some about lattices. I found [Introduction to Lattices and Order](https://books.google.ru/books/about/Introduction_to_Lattices_and_Order.html) very accessible. Read the first chapter and then proceed to categories.
**Categories for the Working Mathematician** Categories for the Working Mathematician (CWM) is a textbook in category theory written by American mathematician Saunders Mac Lane, who cofounded the subject together with Samuel Eilenberg. It was first published in 1971, and is based on his lectures on the subject given at the University of Chicago, the Australian National University, Bowdoin College, and Tulane University. It is widely regarded as the premier introduction to the subject. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
What experience and education profiles are you seeking?
Functional patterns are quite a bit more powerful than ad-hoc view patterns, iirc. They allow you to do some fairly wild stuff due to the non-determinstic nature of curry. `last (_++[e]) = e` is a function that returns the last element of a list, for example. `isFourOfAKind (x ++ [_] ++ y) | x++y == [c,c,c,c] = True where c free` returns true if 4 out of 5 items in a list are the same item. (Technically, it does more than that because it works on arbitrary lists...) You can get even more wild with them as well.
Oh, whoops. I thought for sure it was the same type for everything in the example he gave... Thanks for pointing that out :)
&gt; Exposed brick in the office 
&gt; How can any of those do O(1) updates? Persistent data structures (which all of the mentioned solutions seem to be) need to reconstruct the updated record from scratch, which is O(log n) with tree-based approaches, and O(n) for everything else. Technically `labels` is tuple-based, which means in e.g. tight loops it can often be optimized away just like a haskell98 record.
Ah, so it's actually so much like a logic language that it's useful!
I mean, if the structure is infinite, then you sorta kinda can't, unless you decide to stop after a few levels of recursion (which makes sense for Show, but not for Eq).
ELVM is an interesting project, it is designed to be easy to compile the IR to esoteric languages like brainfuck, and Underload. When I was new to haskell I tried implementing a haskell backend for ELVM but failed because I was trying to overengineer it by avoiding IOrefs.
So, what exactly *is* wrong with the way that Purescript does it? I understand that the main reason we don't have records yet is because we'd rather not implement a broken solution, which will be difficult to fix later. But the theory for extensible, anonymous, row-polymorphic records is decades old, and Purescript has an elegant, complete solution (IIRC). What's wrong with the way Purescript does it? I believe we could actually create an even better solution in Haskell, with better syntax, due to `TypeApplications`, `OverloadedLabels`, and other modern GHC features. 
- replace `fString` with something shorter? Python uses only one letter `f'{foo}'`, such a fundamental thing should have a short name, it reads better and saves screen real estate - multiline string support without `\`, the line continuation marker, at the ending and the beginning of the line?
When we have a working LSP plugin for IDEA, there might be quite a few (HIE works more often than not). Current state of these LSP plugins is, let's say, lacking.
That's an algebraic thing. It's supposed to mean free as in free of additional constraints, beyond those imposed by the signature that defines your structure (e.g. the data type). The requirement that sources and targets of edges must be nodes in the graph ist such a constraint. And also that you don't care about the ordering of contexts.
I've run into issues deploying `upx` compressed binaries to AWS instances - it appears that rather than doing in-place decompression it reverted to decompressing via a temporary file, which is how it works on Windows by default I believe. This resulted in some ancillary binaries (such as CLI tools and service discovery processes) that were triggered via `cron` using an inordinate amount of memory + causing significant CPU spikes every time they ran.
Awesome! The image with "Haskell" in thick sparkly outline is a little "Business people transforming the world through mundane movements"-ey for me, though
&gt; replace fString with something shorter? Actually there is many quasiquoters, `f` is used to QQ to a formatter compatible with `Formatting`, but I confess it may be a design issue. `f'` use typeclass to give you what you want (Conversion to `Text`, `String`, `Lazy Text` or directly IO). ```haskell *PyF Data.Text&gt; [f'|hello |] &lt;&gt; ("world" :: String) "hello world" *PyF Data.Text&gt; [f'|hello |] &lt;&gt; ("world" :: Text) "hello world" ``` I think next release will rename `f'` to `f` and `f` to `fFormatting`. &gt; multiline string support without \ I don't understand, do you want something else than the current support? ``` *PyF Data.Text&gt; :{ *PyF Data.Text| putStrLn [f'| hello *PyF Data.Text| What is your name? *PyF Data.Text| What is your quest? *PyF Data.Text| What is the airspeed of an unladen swallow?|] *PyF Data.Text| :} hello What is your name? What is your quest? What is the airspeed of an unladen swallow? ```
Indeed. That's the kind of reason why I mention inlining.
Theory of Categories is the first to learn, types are a specific case in the category Set, and programs are made of homomorphisms and EndoFunctors in Set. This will save you 30+ years of wandering in the uneducated world of programming.
Great, i guess since it's not so urgent i'll come back to servant once i read some more. Thank you!
Nice! I'll take a look, thanks for pointing out both yesod and persitent.
This looks very interesting! I don't get though why "Existing implementations are pretty lacking, either they segfault, or they have weird bugs, and their code is difficult to correct.". How can it be that something as important as ODBC wasn't implemented properly in Haskell yet and also why can't the existing code be fixed? 
We been using `upx` extensively on multiple platforms and have never experienced any such issues. Why should AWS be different than anywhere else? If I'm running Ubuntu on an AWS instance, it's Ubuntu, and the upx-generated code will run exactly the same way that it does on any other Ubuntu. It would be shocking if a upx executable decompresses via a temp file and not in memory on any platform. That contradicts the whole purpose of upx. I have not heard of any such showstopper bugs in upx.
Strangly I dont see this one listed yet: http://nikita-volkov.github.io/record/ Or did I miss it?
Yes: &gt; This is already in there with `record-preprocessor` which is AFAIU the recommended way of using `record`
I was writing my own SQL Server clone for a client and needed to write a test suite for it that would connect to it. I went to the go-to Haskell library HDBC-ODBC. It failed with a unicode error upon connecting to actual SQL Server. I found a fork of a fork on GitHub that fixed the issue. Then I made a simple test suite that connected and disconnected to a server 10 times, it reproducibly segfaulted before reaching 10. I looked at the code and didn't know where to begin to fix it, the code is messy and not something I'm interested in working with. Under time constraints at the time, it was more straight-forward to make a quick ODBC binding of my own. I'd considered using C directly for the test suite, but in writing the C code I realized that the API is quite straight-forward, so I whipped up a Haskell FFI binding. That later evolved to this more polished binding. &gt; How can it be that something as important as ODBC wasn't implemented properly in Haskell yet
I mean, this is basically what CLOS from Common LISP does, right ?
I would suggest to just listen to bartosz's talks on category theory, one talk per week. There is no need to understand anything, at least one has heard all the weird concepts and has a good roadmap where what occurs. Since everything is related to everything it is hard to orient and listening to the lectures helps to be just exposed. 
So if `f` is `[]`, you get cartesian product? cartesian product is quite useful, too, and a pain to write manually.
This is very good advice! Though I would replace *Certified Programming with Dependent Types* with *Formally Reasoning About Programs*, by the same author.
It's neither shocking nor is it a bug - they advertise that 'in memory decompression is not available on all platforms' and is dependent upon the platform and binary format: &gt; Currently, executables compressed by UPX do not share RAM at runtime in the way that executables mapped from a file system do. As a result, if the same program is run simultaneously by more than one process, then using the compressed version will require more RAM and/or swap space. &gt; UPX recognises three executable formats for Linux: Linux/elf386, Linux/sh386, and Linux/386. Linux/386 is the most generic format; it accommodates any file that can be executed. At runtime, the UPX decompression stub re-creates in /tmp a copy of the original file, and then the copy is (re-)executed with the same arguments. ELF binary executables prefer the Linux/elf386 format by default, because UPX decompresses them directly into RAM, uses only one exec, does not use space in /tmp, and does not use /proc. Taken from https://linux.die.net/man/1/upx.
Exactly. I use it mainly for maybe or either but Cartesian product or zipping are nice too.
I did not ever open it. Can you describe it in comparison?
CLOS's generic functions must be declared ahead of time and then methods implemented for that generic function. I think what OP wants is type-directed-name-resolution which doesn't require any kind of interface declared ahead of time. 
I'll write a proper response a bit later, but meanwhile could please explain where exactly the laws fail with that `Fix IORef` example?
Type theory is a lot older than category theory. CT is super useful for someone seriously diving into the theory of functional programming but idk whether it is essential right when they are about to read an intro to types. And types aren't really "in" Set. You can model a type theory in any cartesian closed category.
Problem: CoC with fix is unsound if you erase the type-level computations. If you do not, it is sound but slow.
&gt; What do you erase type level computation? In languages like Coq, Idris, and Agda, all computations are either terminating or productive. Therefore, type-level computations need not be performed at runtime at all, which is a big performance win.
I actually don't think you've actually composed Divisible cleanly either. That you need to include fst and snd projections in your arguments to contramap in divisibleCompose shows this. Is it easy to extend it to a Predicate (String, Int, Char)? 
 divisibleCompose :: Predicate (String, Int, Char) divisibleCompose = contramap ((== 5) . length . view _1) predicate &lt;+&gt; contramap ((&lt; 6) . view _2) predicate &lt;+&gt; contramap ((`elem` ["x", "y", "z"]) . view _3) predicate Seems pretty clean to me, certainly cleaner than nested `Either`s in the `Decidable` case. One constructs `Divisible`s for product types, which will generally be records with accessors for all their fields.
Given a type like data FooBarBaz = Foo Int String | Bar | Baz FooBarBaz Using constructors as functions is what makes something like `Foo &lt;$&gt; fa &lt;*&gt; fb` work. What if we also had a way to convert constructors into first class partial functions? -Foo :: FooBarBaz -&gt; (Int, String) -Bar :: FooBarBaz -&gt; () -Baz :: FooBarBaz -&gt; FooBarBaz And could note their partiality in the type? -Foo :: FooBarBaz#Foo -&gt; (Int, String) -Bar :: FooBarBaz#Bar -&gt; () -Baz :: FooBarBaz#Baz -&gt; FooBarBaz And could combine these partial functions to make total functions? -Foo # -Bar :: FooBarBaz#{Foo,Bar} -&gt; Either (Int, String) () -Foo # -Bar # -Baz :: FooBarBaz -&gt; Either (Int, String) (Either () FooBarBaz) Then `Decidable` would be pretty intuitive contramap (-Foo # -Bar # -Baz) (fa `chosen` fb `chosen` fc)
Prisms already give us something in this direction. I wonder if anything can be made of it.
Yes, in the absence of fix. Coq does that with all Prop types.
Where `TypeFamilies` is involved, I wonder if the following bug will come into play and affect usability (compile time). I once tried to use TypeFamilies to circumvent aspects of the GHC record problem pertaining to my use case and gave up after a regretably bloody battle with a massive compilation-time wall (&gt;20 minutes compillation time, 20GB resident memory), though I didn't check whether this bug was indeed the cause of the long compilation time I experienced. https://ghc.haskell.org/trac/ghc/ticket/8095
Prisms don't "note their partiality in the type" in the sense of keeping track of the constructors they match; you could do this if your type had exactly as many variables as constructors, and each one was used as the sole argument to the constructor.
Weren't there plans to get anonymous sums (analogous to tuples)? Might that help `Decidable` compose?
That is basically how my `total` library works: https://hackage.haskell.org/package/total-1.0.5/docs/Lens-Family-Total.html
 show port &gt;&gt;= putStrLn Why did you write this? What bind (&gt;&gt;=) is for monads. This isn't the only issue but it seems like the same misunderstanding is repeated.
Would suggest updating this post with a picture of a reactive-banana controlled refrigerated banana when the fridge arrives. Bonus internet points will be granted for each additional accessory on the banana that makes it resemble the reactive banana logo.
When working with `Text` values, you want to add the `text` package to your dependencies in the `.cabal` file and import the stuff you need from the corresponding modules, for instance import qualified Data.Text as T -- text functions like split etc. import qualified Data.Text.IO as T -- text equivalents to putStrLn etc. Additionally, you have to keep in mind that string literals like `"string"` is syntactic sugar for `['s', 't', 'r', 'i', 'n', 'g'] :: [Char]`. If you want it to be of type `Text` instead, you either use the `pack` function, or you just enable {-# LANGUAGE OverloadedStrings #-} at the top of your module, which inserts calls to `pack` implicitly for you. Regarding your use of `Data.Ini`, I would start with the low-tech solution of just pattern matching, instead of trying to be clever with `(&gt;&gt;=)`. You can still refactor after wards. So I would write something like main = do ecfg &lt;- readIniFile "configs/config.ini" case ecfg of Left e -&gt; handle error Right cfg -&gt; case lookupValue "NETWORK" "port" cfg of Left e' -&gt; ... Of course keeping pattern matching like this becomes annoying. You could create some function validate :: Either String Ini -&gt; Either String Result validate ecfg = do cfg &lt;- ecfg ... (do a lot of stuff that lives in "Either String") and then just pattern match once inside `IO`, for instance. 
Hey /u/KirinDave and /u/maxigit, would you happen to know if order-independent records are also supported by `extensible`? Details here: https://github.com/fumieval/extensible/issues/21
`stack` already does most of that natively, if you're willing to have an extra terminal window open. See your dependency graph: stack dot --external --prune base,ghc-prim,integer-gmp,deepseq,array | dot -Tjpg -o deps.jpg Open an entire library's docs: stack haddock --open containers With Stack 1.7 which just came out, you can run a private version of [Hoogle](https://www.haskell.org/hoogle/) on your own machine: stack hoogle --server For editing in general, [Spacemacs](http://spacemacs.org/) has an excellent [Haskell layer](http://spacemacs.org/layers/+lang/haskell/README.html) which provides autocompletion, etc. 
What do you mean by order-i dependent records ?
As I detail in the GH issue as well, where `(x @= 1 &lt;: y @= 2 &lt;: r) == (y @= 2 &lt;: x @= 1 &lt;: r)` would evaluate to `True`. Currently this doesn't even compile.
I'm afraid this choice of FRP library might not be ideal. Bananas turn brown when you put them in the fridge.
I understand, I can't take something out of an \`Either\` and put it into an \`IO\`. Left and right side context of bind should be the same. Is that correct? main = do config &lt;- readIniFile "configs/config.ini" putStrLn $ case (config &gt;&gt;= lookupValue "NETWORK" "port") of Left s -&gt; s Right t -&gt; show t This on the other hand would work because config is an 'Either' value and 'lookupValue "Network" "port" takes an argument and returns an 'Either'. Is my understanding correct?
Nice writeup, thanks for the detailed explanation
I actually thought of that, and I'd be happy with a staged photo of a banana that was just placed in. It's the photo The Internet demands, not the actual storage of bananas in a fridge. :)
Well, whether it's a bug or not is a matter of semantics, but it renders upx useless for its primary purpose of reducing start-up times by reducing the time to load from disk. Since that man page was written in 2010, support for 64-bit binaries was added. Could it be that for 64-bit upx reverts to unpacking into `/tmp`? If so, I retract my warm recommendation. It still would be an option to reduce size, but you would have to be very careful about using it until you know for sure that unpacking to `/tmp` is OK for your use case.
Welcome to venture capital
Maybe something like [this](http://try.purescript.org/?gist=7b859782514f062a45fff165aa53d4a3)?
I migrated all my apps from hdbc-odbc to this library. I wrote a wrapper to make it a drop in replacement and in 99% of the cases it was simply changing one import line. The rest worked as is. There are 2 bugs and some changes that are still outstanding, but they are very easy to fix (see github). I have my own forked copy that I use, but it diverged too much from original library. I dropped their Query because it is not a place for it in this lib and because there are many other libraries that provide query construction that should be pluggable. And I added FromSql type class to make it a drop in replacement for my code that uses hdbc-odbc. 
Well, it's pretty easy for something not to to be implemented - all it takes that no one who wanted or needed it having both the time and the skill to implement it. There are application domains where you just don't need something like ODBC - for example, your code might be database specific, the genericity of `persistent` is sufficient for you, or you might not need a database at all. 
main is of type \`IO \(\)\`. readIniFile returns an "IO \(Either String Ini\)". Thus, when you use this in \`config \&gt;\&gt;= lookupValue "Network" "port"\` you are using the Either monad \(the type of your values are \`Either \* \*\`\). In the complete program you probably wouldn't want to return the error \(the Left value, non quoted\) in place of the successfully lookup up setting \(the Right value, quoted\). Consider you instead do something like: case config \&gt;\&gt;= lookupValue "NETWORK" "port" of Left s \-\&gt; fail \("Failure: s"\) Right v \-\&gt; Data.Text.unpack v
[I've run out of time this morning, so I'll have to post this reply in multiple parts. I apologize for teasing right up to the edge of the graph example, but I ran out of time right as I reached the meat of the issue as I spent too long on preliminaries.] First let's talk about some general issues about the idea of monadic lenses. I'll stick to the monomorphic form for simplicity. For sake of discourse let's pretend we have some kind of "monadic" lens with monadic analogues of `view` and `set`, called `viewM` and `setM` respectively. contents :: MonadicLens IO (IORef a) a viewM :: MonadicLens m s a -&gt; s -&gt; m a setM :: MonadicLens s m a -&gt; s -&gt; a -&gt; m s with the laws you'd expect in their messier monadic forms If i put back what i read there's no effect. viewM l s &gt;=&gt; setM l s = return s If i set and then view i get back what i put in setM l s a &gt;&gt;= \s' -&gt; viewM l s' = a &lt;$ setM l s a Setting twice is the same as setting once. setM l s a &gt;&gt;= \s' -&gt; setM l s' b = setM l s b First off it is worth noting that nothing that uses, say, the structure of `m = Maybe` to return `Nothing` can ever pass these laws. People usually trot out monadic lenses as an example of how to deal with cases where they want to add checks on read and/or on write. But 'viewM l s' can never return `Nothing`, because consider if `viewM l s = Nothing` for some `s`, then viewM l s &gt;&gt;= \a' -&gt; setM l s = Nothing &gt;&gt;= ... = Nothing /= Just s [Reflections on Monadic Lenses](https://arxiv.org/pdf/1601.02484.pdf) refers to the design here as "naive" monadic lenses and talks to this first point at least. Consider that if we have `PutPut` then either `setM l s` can never return `Nothing`, or it can never succeed. Let `a` and `b` be values for which `setM l s a = Nothing` and `setM l s b = Just r`. Then setM l s a &gt;&gt;= \s' -&gt; setM l s' b = Nothing &gt;&gt;= ... = Nothing /= setM l s b = Just r So there are no 'interesting' "monadic lenses" for `Maybe` that use the power of `Maybe` to check intermediate results. There are some differences with this and the Reflections paper I linked. They are coming out of the Bx community they are pretty willing to give up on `PutPut`, and it is worth noting that the non-naive notions of monadic lenses they give aren't powerful enough to handle the `IORef` lens. On the other hand, I'm not willing to give up on `PutPut` as it forms the backbone of reasoning about code written with `lens` and determining the canonical nature of the combinators we supply. I'll try to pick up from here later.
This had me puzzled for a long time before I realised that what I really want is `DecidableWithoutDivisible`. I should probably add that to the post.
Odbc is really only used in enterprises that have existing commercial databases (ms sql server, oracle). Open source dbs do not need odbc and have their own native libs in haskell (mysql, postgres. mongodb). This basically shows that the level of adoption of haskell in entrenched enterprise is almost non existent. Haskell I think is almost exclusively used in either open source or in startups that shun commercial databases due to their exorbitant licensing costs and vendor lockin. 
`setM` giving back `a` not `s`, means you can't lift non-monadic lenses to these monadic lenses. e.g. you can't mix things that walk under `IORefs` with things that do pure lensing, so a lens isn't a "monadic lens" for all monads m. They are inherently different beasts in your model. As for `PutPut`, `PutPut` is what gives you the fact that you don't affect the rest of the structure. It is the 'very well-behaved' part of very well-behaved lenses. Without it, I have far more limited ability to really reason about what you are doing elsewhere in the structure behind my back. The point I was working towards before I ran out of time to actually try to fully recall the example, was that you can't have "very well-behaved" lenses with IO side-effects that meaningfully walk through IORefs. Note: your laws do not require that `viewM` doesn't change the state. `viewM` under your laws is merely required to be idempotent and to not do any damage that isn't also done by `setM`. A `viewM` and `setM` that both tweaked an unrelated Boolean as a side-effect by setting it to `True` somewhere else in the object would pass your laws as written.
&gt; `setM` giving back `a` not `s`, means you can't lift non-monadic lenses to these monadic lenses. e.g. you can't mix things that walk under `IORefs with things that do pure lensing, so a lens isn't a "monadic lens" for all monads m. They are inherently different beasts in your model. Is this a problem? I still can do `setM (_Monadic . _Pure)`. I can't do `setM (_Pure . _Monadic), but I don't need that. There is a [combinator](https://github.com/effectfully/sketches/blob/f2a0d0a87acc473593304ad4828a428059057161/extensible-monadic-lenses/src/Main.hs#L158) that allows to dive into a structure and then monadically modify some state like this e.g.: `('d', var) &amp; _2 ./ _Var %~ succ`. It's a nice thing that monadic and pure lenses are distinct beasts, isn't it? &gt; `PutPut` is what gives you the fact that you don't affect the rest of the structure. It is the 'very well-behaved' part of very well-behaved lenses. Without it, I have far more limited ability to really reason about what you are doing elsewhere in the structure behind my back. Ah. Ok, I understand. But frankly, with monadic lenses I do want to touch different parts of state and mess with your reasoning. Some things are complex and do not have nice theoretical models, so I bring some eyesore to my tools in order to save me from writing the same annoying noisy boilerplate over and over again. &gt; `viewM` under your laws is merely required to be idempotent and to not do any damage that isn't also done by `setM` Thank you, I didn't realize that. Do you see some better laws?
How about `x &lt;$ viewM l s = return x`? This explicitly says that `viewM` doesn't do any harm and so `setM`.
&gt; Slow compilation times. (Template Haskell wins by being compiled and by being able to generate the optimized code directly.) I'm moderately surprised that you have found this, since my experience is the exact opposite. Then again, I play with GHCJS a lot, so perhaps that's why.
CPDT is a technician's book about Coq and proof automation. It implicitly assumes some prior Coq background. FRAP is an engineer's book about Coq and formal verification. It starts from scratch and covers the important stuff, though at a pace far faster than *Software Foundations*.
By the way, `modifyIORef` is lazy; if you‚Äôre going to be making many modifications without reading the time value to force the `IORef`, you probably want `modifyIORef'` which applies the modification strictly.
I think you only need applicative as the code just boil done to something like `rtraverse (Record a b) = Record &lt;$&gt; fmap Identity a &lt;*&gt; fmap Identity b`
I'm starting to realize that convention probably has a significant role here. We often declare a constructor that takes any number of unnamed arguments; we do it often without any hesitation (for n "reasonable"). In fact, the various tuple types are a bunch of essentially unnamed constructors that are uniquely identified by nothing but the number of unnamed arguments they take! So an acceptance of anonymity for products and their factors (including especially function arguments via currying) is entrenched in the language/tradition. But `Maybe`/`maybe` and `Either`/`either` are the only things in the language/tradition that allows anonymity for sums! Even the base case of `Void`/`absurd` was late to the party. And now we have `-XUnboxedSums`, but those are pretty esoteric. (You could make a good argument for type classes, but it feels different, particularly because of open-world assumption. I suppose a custom GADT with one type index also deserves mention, but such "universes" still aren't mainstream.) When I wrote my `yoko` library experiment back in the day, I immediately dismissed anonymous sums as non-viable: no one thinks of the variants of algebraic data types being ordered (aka the distinct patterns of a case expression being ordered). But to what degree is that simply because the language tradition has always forced us to name alternatives beyond two, no matter how few or obvious they are? To be clear, I think anonymous sum types are equally risky wrt code quality as are anonymous tuples, naked Booleans, etc. I desire row types for in-the-large maintainability because good names are so meaningful and modular. But our communal thoughts might change if anonymous sums had been right at hand for decades. Just musing. Sorry for hijacking. (I now see https://hackage.haskell.org/package/anonymous-sum, first uploaded in 2014.)
Pretty much as you say. I am at an enterprise\-y company, and while we write all new things that require a database with PostgreSQL \(we're sick of the licensing fees\), we still have to maintain stuff that talks to SQL Server and an IBM iSeries DB2. The latter has us looking at HDBC\-ODBC, but it seems kind of painful. We may just have to keep building out C# microservices to wrap all of that terribleness.
This new lib though is good. I migrated all my apps to it. See [my reply](https://www.reddit.com/r/haskell/comments/8gfan5/a_new_haskell_library_for_talking_to_odbc/dybybru/) here
I used to hire people in "US Timezones" (which included all Americas). But that adds an additional hassle of money across borders.
But there are other countries in the same timezones. All of Latin America, Brazil, Canada and Mexico share timezones with the US. I have seen "US timezones" before in job postings tho.
/u/dalaing has been playing with this in [invariant-extras](https://github.com/qfpl/invariant-extras), using [Generic eithers-of-tuples](https://hackage.haskell.org/package/generics-eot) It gets a lot closer, but there are still warts. The operators need to be right-associative, which means you have to think about code back to front. And the order in which you Decide is fixed, because the Eithers are nested a particular way..
Is this another monad metaphor?
I always have a good chuckle at those fake testimonials. You got me again Simon Banana Jones!
Right, there's no sane way to do it for `Eq` unless you add some kind of unique identifiers to the records so that you can decide whether or not you've already "visited" a particular node. The same thing with `Show`, or you replace the recursive bits with an opaque `"..."` placeholder value when printing.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [effectfully/sketches/.../**denotational-approximations** (master ‚Üí af8134d)](https://github.com/effectfully/sketches/tree/af8134da5af3b49b01b00875cc003bc1d58bf93d/denotational-approximations) ---- 
They should have gone with the galaxy brain meme.
That's how you get the corporate ppl to agree to us using Haskell.
You were asked an honest question, you gave a vague "throw your hands up in the air like you don't care" reply.
&gt; but it is not obviously bad not to have laws I think you've shown that laws are inaccurate, not that it's ok not to have them. The more abstract something is, the more desperately you need clear and useful laws; otherwise the level of abstraction is sure to be a frequent source of deep misunderstandings (e.g. monad-control's laws are completely useless, leading to problems). The `lens` library is absurdly abstract, so its more complex parts would be totally unusable without useful laws governing what they do. I think you could easily show that for some small subset of what the `lens` library does, the laws aren't terribly useful. But `lens` is a big library that does many things that break down without laws.
Yep. I recall you announcing it and being extremely disappointed in how limited it was (due to Haskell's limitations; not due to any flaw of the library). It didn't help that I was doing PhD work at the time designing a language with pervasive structural subtyping (think MLPolyR).
No. It's just a silly name.
The problem with libraries that can change things behind your back is amplified when there are more than two parties involved. If I use two packages that use a library, and I use that library to compose them, I'd really like to be able to reason about what I'm getting from those libraries without having to read their source code (and possibly the source code of all of their transitive dependencies) to double check that there is no spooky behaviour (or unintended clashes of hidden behaviour). If there are ivariant-preserving types or a mix of good types and good laws for that library - and those laws are obeyed in the ecosystem around it - then I can reason about everything I want to do above locally. 
 `a -&gt; a -&gt; a` would mean that `c` and `x` have to be the same type. `my_const` as you wrote it doesn't impose this constraint. `p1 -&gt; p2 -&gt; p1` means the return value has the same type as the first argument, but that the two arguments don't need to be the same type (though they can have the same type if the caller happens to pass in two arguments of the same type).
The variable names themselves are not important, they are just generated using some heuristic in GHC. However, the equality of names within a same expression is important. :t \c x -&gt; c p1 -&gt; p2 -&gt; p1 As you can see the first and last variable names are the same, while the second is different. This means that the result has always the same type of the first input, while the second type is not used in the output. Compare to `a -&gt; a -&gt; a`, as you suggested. In this case, the second parameter always need to have the same type as the first. This has two consequences: - it less general than possible - it allows other implementations, like `\c x -&gt; x`, so that the type signature is less informative.
would it be valid to write it as a -&gt; b -&gt; a then? or do p1 and p2 have some special significance?
this makes sense, thanks!
The names (like p1, a and t) are just random names that have been picked by GHCi. All you have to remember is that if you see the same name twice, it means that those two things have to be of the same type. Let's take my_const as an example. When you apply the function to two arguments (which can be of any type) you always return the first argument. This means that the return type has to be the same type as the first argument, right? Hence they are both called a. The second argument can be whatever (even of type a) and does not have to be of the same type as the first argument. Thus it is also given a random name, like b. Something you should learn to do is to write your own type definitions, since the Haskell type system is very expressive and adds both documentation and type guarantees to your code, since the compiler sometimes infers (guesses) a type different to what you intended it to be.
`t` would probably be a type that implements the [`Traversable`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Traversable.html) class (for example, Trees or lists).
The actual name used for a type parameter makes no difference as long as things that have the same type in a given declaration use the same identifier and things that can have different types use different identifiers. `p1` and `p2` were filled in arbitrarily by the compiler because you didn't write an explicit type annotation, probably just meaning "parameter 1" and "parameter 2" respectively. You could instead write myConst :: a -&gt; b -&gt; a myConst c x = c and it would be the same. However, if the type is `[a]`, then the corresponding function argument has to be a list of `a` for some `a`. 
It _is_ a logic language :) The pakcs compiler for curry compiles down to prolog and the language is the research vessel of choice for functional logical programming. Originally, free variables were the only main logic bit in the language; eventually, the choice operator was added and functional patterns were added and it turns out that those two are so much more convenient for 90% of use-cases that rarely do you explicitly request free variables anymore, heh. (The choice operator `?` encodes a non-deterministic choice, so that `coin = 1 ? 0` "returns" both 1 and 0. The fun part is that it's a function just like any other, so `foldr1 (?) [1..10]` has the intuitive semantics you would expect)
&gt; I like the shape of your article, but I think /u/HuwCampbell was right to question some of your concrete example types. I suggest revising the article to work solely with these two types: I'm not sure what you're anticipating. None of my examples break down with those types.
I take it you meant *Practical Foundations __for__ Programming Languages", by Robert Harper. Is this correct?
sweet. got it, thanks!
I would love full support for anonymous sums and products, both positional based and name based. I actually kind of wish that they were the basis for Haskell data types (plus some sort of base bitstring type for ints and floats and chars etc with all the usual CPU instructions). Although I haven't spent much time thinking about how that would interact with GADTs. In such a system: data Maybe a = Nothing | Just a would probably be syntax sugar for something like: newtype Maybe a = Maybe_ (#Nothing [] | #Just [a]) pattern Just a = Maybe_ (Just [a]) pattern Nothing = Maybe_ (Nothing []) Where `#Nothing` and `#Just` are anonymous constructors with types `a &lt;-&gt; #Nothing := a` and `a &lt;-&gt; #Just := a`. And the lists are heterogeneous anonymous product types. I agree that excessive use of positional anonymous sums and products isn't the best idea, but if for example the types of each of the choices are sufficiently descriptive and unique they can often be just fine, as any mix up will almost certainly be a type error.
Glad you like the idea! I think this could make data and `case` statements compose a lot better, a lot of Haskell composes extremely well, but heterogenous data and `case` statements do not. You could use the variance of the thing in question to adjust the types appropriately, for example: f x = #Foo x Would have type `#Foo x ‚â§ y =&gt; x -&gt; y` But: f = #Foo x -&gt; x Would have type `#Foo x ‚â• y =&gt; y -&gt; x` Likewise: f :: #Foo x + #Bar y ‚â§ z =&gt; Bool -&gt; x -&gt; y -&gt; z f b x y = bool b (#Foo x) (#Bar y) f :: #Foo x + #Bar x ‚â• y =&gt; y -&gt; x f = #Foo x -&gt; x | #Bar y -&gt; y Now there is a lot of polymorphism here so that might need to be addressed somewhat, one option might be to adjust the type system to always pick the "smallest type" whenever there is an ambiguity. But then of course you will have to be very careful about how that interacts with type families and similar that can directly introspect these defaulted types (probably type inference should just fail if you do meta introspection).
I don't know, unfortunately. I can't say whether it's perfect as-is, or if there are any warts, I have very limited experience with PS. I hope someone with PureScript experience can comment on this without/before I spend many hours/days digging in.
That's a great interview. I wonder who he found who can still provide him with dial-up modem Internet access. And where he found a modem that still works and can be connected to a computer that still works.
My provider \(Free, in France\) has rescue dial\-up access, and you just need a serial port \(perhaps USB?\) and an old modem to make it work. What I personally found surprising was the use of Haskell in a power constrained setting. I want to believe though.
&gt; I think you've shown that laws are inaccurate, not that it's ok not to have them. I'm not saying it's ok not to have laws. I'm saying it's not obviously bad which means "maybe sometimes a thing can make sense without a few inaccurate equations associated with it". &gt; The more abstract something is, the more desperately you need clear and useful laws; otherwise the level of abstraction is sure to be a frequent source of deep misunderstandings Yes, but it doesn't imply that you're always able to fulfill the need. From the operational perspective `Monad` laws lie. From the denotational perspective the laws are just some coherence statements which prevent you from defining a substantial subset of nonsense instances (a very important subset, but still a subset) and allow to reorder operations a bit which is a very limited form of reasoning. Again, I'm not saying laws are not important. The more we have laws, the better for us. But the presence of laws doesn't imply that you have a powerful reasoning framework that prevents you from writing nonsense. All I want to say is that there are abstractions which can be useful even if their laws are not clear: `Foldable` is in `base` after all (some consider this a mistake and I respect this point of view, but personally I'm fine with `Foldable` being in `base`). &gt; e.g. monad-control has problems because it's super abstract and its laws are completely useless `monad-control` has problems, because it's just a bad abstraction and that has nothing to do with how abstract `monad-control` is. &gt; `lens` library is absurdly abstract, so its more complex parts would be totally unusable without useful laws governing what they do. I think you could easily show that for some small subset of what the `lens` library does, the laws aren't terribly useful. But `lens` is a big library that does many things that break down without laws. Like I argue against that. I said "laws for pure lenses are very important" and cited Edward Kmett in the end of the post.
&gt; You were asked an honest question I attempted to answer that question in the original thread. &gt; you gave a vague &lt;...&gt; reply This is correct, but I do not have something stronger than "I'm not yet convinced that supposed laws for monadic lenses are of the same importance". &gt; "throw your hands up in the air like you don't care" This is not correct.
Thank you so much for this! 
I think that‚Äôs rather unfair to the author. They did go to the lengths of writing an eight-paragraph article with multiple quite specific examples. We can debate how effectively they made their point but the charge that the author is being dishonest or doesn‚Äôt care is a bit rich.
One of the existing problem monadic IO solves for haskell is mixing side effects and lazyness. Is your language lazy? If yes, what is the semantics of an effectful function?
That's a great interview but Joey H didn't write [the testimonials](https://wiki.haskell.org/Reactive-banana#Fake_Testimonials). 
Why do you love Haskell? I'm a full-stack developer who has recently taken to learning Haskell after being somewhat disappointed with Elixir, Haskell has quickly become my favorite language by far but I couldn't possibly explain why.
Sure, knock yourself out! This is a fine effect-tracking system. It's a lot less powerful than Haskell's, but if your goal is to have a simpler, less powerful system, which is nevertheless powerful enough to distinguish between pure and impure functions, then that's the obvious way to do it. If you restrict Haskell to IO actions, do notation, pure functions, and `unsafePerformIO`, you basically get an embedded DSL for your language. Haskell is great at making embedded DSLs, so writing programs using that embedded DSL syntax is not painful at all, but it is true that if you write a special-purpose compiler which directly implements this DSL, you can give it better, builtin syntax, and you can get rid of the scaffolding. In this case, if IO is your only kind of side-effect, you don't need a common abstraction for "this type implements a side-effect, so you can run the side-effects of one computation of that type followed by another, and you can use the result of one computation to determine which side-effects to perform next". I would go further and say that Haskell is a great language in which to experiment with DSLs, but sooner or later, we should collect the result of these experiments and make a language which has all those Functional Idioms Builtin (FIB). One of my many in-progress projects is a language named Fib which attempts to do that, and it too has an effect-tracking system. Good luck, I bet you're going to finish your compiler before I even finish designing my language :)
I find Haskell's type system an intuitive model for thinking about programs. Also, the syntax is terse and expressive.
Actually, in Haskell you don't need Monad to do IO. What you need Monad for is to compose (or sequence) IO functions : ie do notation or `;` "operator". You got rid of Monad but have to introduce in your language the notion of sequence (via your `;`). Note that in Haskell, `;` or `do` notation is just syntaxing sugar over `&gt;&gt;=`, there is nothing magic about it. Every other language that I know have to provide some black magic regarding how to chain statements. Why, for example, in Python, a statement which doesn't end with `return something` doesn't return anything, whereas in Ruby, it returns the value of the last statement ? What you describe looks like F# or OCaml.You might have a look a [clean](https://en.wikipedia.org/wiki/Clean_(programming_language) which seems to be one of the best attempt to do side effect without Monad within a Pure language.
You missed the `)` in the link to Clean‚Äôs Wikipedia article :)
Lol no the guy wrote two books with almost identical titles to trick his students ;)
Wow awesome work!
Oh, I did not get the reference!
I think you'll find that this is effectively just syntactic sugar for `IO`. Anywhere you see `!`, you can replace it with `IO` and get back Haskell. It's not really an interesting transformation, it's just restrictive. The difference is that with Haskell, we get to write some functions in terms of `Monad` instead of `IO` (or `!`), which allows them to work with more than just `IO`; e.g. `mapM` lets you iterate over a list with `IO` side effects along the way, or with e.g. `Maybe` for early exiting. Having the ability to abstract over `IO` this way is far more interesting and useful than only being able to distinguish between pure and impure.
Take a look at the paper [Data types √† la carte](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.4131). Each constructor of your language(s) defines its own functor, which makes them reusable with generic sum and product operations to compose them, and a fixpoint operator (`Fix` or `Free`) generates the syntax defined by such a composite functor. Another approach is [Trees that grow](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf) where constructors and fields can be modified or disabled depending on a type index. For certain constraints, one more technique is to use GADTs where the index may change depending on the constructor/field, encoding a kind of state machine/tree automaton that restricts the allowed tree shapes at compile-time (and you can "disable" that state machine in initial compilation phases using a trees-that-grow-like approach). It's a cute trick but it can often be unfolded into a set of regular types that are much more understandable.
&gt; Despite this, performing the same substitutions on the Map laws would still capture the semantics we want. &gt; Although it might seem silly to explicitly write out such "obvious" laws, it is the laws that give your abstraction meaning. Laws are a part of an algebraic theory along with axioms. This is syntax, not semantics. An algebraic theory doesn't have any meaning until you supply one. Laws are coherence conditions, they restrict possible models, but do not bring in any predefined interpretation. You can randomly remove any law from your set of laws and still will be a perfectly correct algebraic theory. And the `Map` interpretation still will be legal. And actually, without the `insert i b (insert j a m) = insert j b (insert i a m)` law, an interpretation of `Map k v` can be `Identity v`. So it's kinda the opposite of &gt; The implication is clear: names are helpful, but laws are invaluable. is true.
But brown bananas are the best - so sweet!
I may also be wrong. This is mostly based on the assumption that the type-level programming that comes with Generics is abusing the type checker, which may not have been optimized as an execution engine. I don't have first-hand experience with large enough applications to be affected by this issue. It seems some benchmarks are in order.
Just because a language forced functions to be pure or impure doesn't mean that same language cannot have Monads like Optional, Maybe, etc. For example, you could do something like this: ioMonad = printLn\(airlinesPartnersNetwork\).fmap\( \(\) \-\&gt; print\(‚ÄúEnter airline miles are on: ‚Äú\).fmap\( \(\) \-\&gt; nextLine\(\).fmap\( \(start\) \-\&gt; print\(‚ÄúEnter goal airline: ‚Äú\).fmap\( \(\) \-\&gt; nextLine\(\).map\( \(goal\) \-\&gt; pathForMiles = ImmutableArrayList\&lt;String\&gt;.make\(\) airlinesVisited = ImmutableArrayList\&lt;String\&gt;.new\(\) \(newPathForMiles, newAirlinesVisited, canRedeemBoolean\) = canRedeem\(start, goal, pathForMiles, airlinesVisited, airlinesPartnersNetwork\) if\(canRedeemBoolean\) printLn!\(‚ÄúPath to redeem miles: #{newPathForMiles}‚Äù\) else printLn!\(‚ÄúCannot convert miles from #{start} to #{goal}.‚Äù\) \)\)\)\)\) [ioMonad.run](https://ioMonad.run)!\(\) Note that because you know the construction of the ioMonad did not contain any side effects \(no exclamation points\), you know that no side effects were invoked until [ioMonad.run](https://ioMonad.run)!\(\) was called. Also, you could for example put a bunch of print functions in a list and map each print function to a new function that executes that print function twice and then you could execute every function in the list. I don't think this would prevent the possibility of something like that happening.
[removed]
In https://github.com/akhesaCaro/haskell-breakout/blob/master/app/Main.hs#L54-L61 Why do you have these type annotations?
These updates are always a pleasure to read.
Please ignore the second point, I was thinking something else. On a related issue, (not a feature request, or a defect, just think it's an interesting thing to think about): How should this library handle 1) leading newline 2) leading whitespaces 3) ending newline, from the code below, I can see that they are treated literally, I am asking this, because, sometimes you need to pass a multiline `[fString|xxxx|]` to a function call, it would be nice if the code indentation is still kept. putStrLn [fString|column1 column2 1 2 will produce column1 column2 1 2 not column1 column2 1 2
&gt; doesn't mean that one should disregard the practice altogether Of course it doesn't. Look what I'm saying: &gt; it is obviously good to have laws &gt; laws for pure lenses are very important &gt; Again, I'm not saying laws are not important. The more we have laws, the better for us. I always try to define laws for my classes and put them into documentation. I'm not trying to argue against laws -- that would be insane, I'm saying that 1) laws are not a panacea 2) if laws are not clear, it doesn't immediately mean an abstraction is useless/nonsense But I see how my wording is misleading. I should have said something like &gt; but it is not obviously bad not to have laws in case they weren't found for a particular abstraction As I said, `Foldable` is in `base`. It's this part of Edward's answer that made we write an article: &gt; but the above situation ultimately killed any expression of a "monadic lens." I disagree with the "no laws -- no abstraction" sentiment. I'll fix the wording. I guess I should write more code than English.
Writing nonsense and being dishonest are still distinct things.
One other note. From the type of `append`, I'm going to guess you did this in ghci: Prelude&gt; append [] ys = ys Prelude&gt; append (x:xs) ys = x : append xs ys Prelude&gt; :t append append :: [a] -&gt; t -&gt; [a] The reason you're getting that type for `append` is that by entering the definition of `append` on multiple lines, `ghci` is treating it as two separate definitions, having the second override the first. You can see this if you try to call it. Prelude&gt; append [] [1,2,3] *** Exception: &lt;interactive&gt;:2:1-35: Non-exhaustive patterns in function append Though multiline definitions are standard in `.hs` files, to do a multiline definition in `ghci`, you need to use `:{` and `:}` Prelude&gt; :{ append [] ys = ys append (x:xs) ys = x : append xs ys :} Prelude&gt; :t append append :: [a] -&gt; [a] -&gt; [a]
&gt; Finally I started looking into adding -target to GHC. I understand fully that this will be a long, convoluted, complex road, and that this is a long way from 'soon.' That being said I literally squealed with joy when I read this.
FYI, this nomenclature may confuse some Haskellers. See [GHC.Generics](http://hackage.haskell.org/package/base-4.11.1.0/docs/GHC-Generics.html) What you're talking about here is usually called 'parametric polymorphism', and despite the much more intimidating name, is a significantly more approachable concept. Mostly I mention this not to be pedantic, but to help with future attempts at google searching related questions.
Hmm. I was trying to emphasize the correspondence between those "extra" projection functions for `divisibleCompose P` and the injection function in `decidableCompose S` that's bothering us. That seemed important, but rewriting this comment over and over helped me see another important issue more plainly: `divide f` always uses both of its arguments, and `choose f` always uses only one. Also, (relatedly?) unlike `divide (\x-&gt;(x,x))`, there is no universal `f` for which `choose f` becomes the operator of a semigroup. If we require that the arguments of `divide` and `choose` be isomorphisms (which is how I think about them unless I force myself otherwise), then `\x-&gt;(x,x)` is disallowed and flattening via `divide` is as unsatisfying as flattening via `choose` (ie the projections are as unwieldy as the injections).
But I LOOVE frozen bananas, so I'll modify the code to add a freezer mode.
The IO Monad in Haskell has two purposes: enforcing an order on effects and regulating mixing of pure and impure code. Since your language is strict, only the latter concern has to be taken care of.
Explaining a tool is not the same thing as explaining carpentry.
[removed]
&gt; if laws are not clear, it doesn't immediately mean an abstraction is useless/nonsense I disagree with this in a sense. It may mean that you need to find clearer laws. But, ultimately, if you can't do so, then you don't yet in any meaningful sense have an _abstraction_. What is an abstraction, definitionally? It is something _abstracted_ from the concrete case. How is it abstracted? By being able to be reasoned about by a more general set of rules than given by the concrete cases. What is this more general set of rules? We call them laws! So if you create an "abstraction" that doesn't let you forget the implementation (perhaps up to some degree of quotienting) then you literally have not created an abstraction, you have just created a gesture towards an abstraction.
Fair 'nuff. I think you are not being dishonest. I think you are just confused, and it is frustrating because I have seen you write sense in the past, and I really think you should know better. To the extent there is any dishonesty, I think you may be being dishonest with yourself (a sentiment I understand) because you want to get away with having an "abstraction" without laws, since you really like the thing you have produced. (Also, I think the sense of "honest" in the original post was not as in "literally the opposite of lying" but more as in "straightforward.") Reading through the prior discussion, I do have a concrete, if not fully formed idea as to how to proceed. The examples edwardk gives where these things fail are all over particular monads (arbitrary IO actions and arbitrary IO refs, or making nontrivial use of Maybe). If you restricted yourself to monads with very specific semantics that could then be _lifted_ to IO, you might find yourself able to make stronger claims. Tarmo's decomposition of the state monad into the update monad is perhaps a place to start thinking about things.
Thanks, Where can I find what that 4th argument must be ? 
I noticed you left off actually defining any instances of your flavor of `Decidable`.
oke, so we discard now the birthDay .That is not what I want 
Then don't : $maybe (FileForm info con bday) &lt;- submission 
Other way around. `mapM` is a pure function that happens to work impurely if you plug in `IO`. You will lose abilities such as this.
&gt; But the presence of laws doesn't imply you have a powerful reasoning framework that prevents you from writing nonsense. I may or may not disagree depending on how strictly you define 'prevents.' The presence of laws illuminates but does not solely define the border between sense and nonsense. The absence of laws is the absence of verifiable properties in your abstraction. If I have no verifiable properties presented by your abstraction it becomes difficult to author, test, or understand an implementation of your abstraction. That doesn't sound like a good abstraction. 
Disclaimer: I played a bit with purescript records, mostly via a basic lens implementation, but never wrote a real program with them. The basic idea of row types is really nice but the type level programming api purescripts offers for them are horrible to use. Iirc the biggest paper cut was that purescript records allow the same field to be inserted multiple times even though this doesn't match what happens at runtime. This means that type level programming involves a LOT of constraints like `NotMember label row =&gt;` which totally obscures what is actually happening. 
Made 2 commits: top at thttps://github.com/Peaker/haskell-breakout/commits/master They're partial work, but show some trickery to DRY some of the code and get rid of the tedious record updates. I suggest, btw, using a record type that has proper Functor, Foldable, Traversable, Num instances, instead of tuples. Then all the vector code can be so much nicer.
Productivity: I'm really more productive in Haskell. I don't spend tons of time debugging. I do spend more time debugging performance issues, but still far less time debugging overall. Cognitive load: I feel it's so much *easier* to program, when I can change a type in some place, and let the compiler remind me everything that needs to be fixed. I'm freed from so much tedious thinking. Tooling (This may be surprising one): * with a proper haskell-mode setup, I get far faster feedback cycle (large project type-checks, tests run) than in other languages. For example, a typical Python UT suite typically takes much longer and gives me less confidence than just the type-checking phase. * QuickCheck is so much nicer in Haskell, due to purity Fun: I think the combination of the above causes programming in Haskell to be more fun (except when debugging performance issues :-( ). Also, I feel like I'm learning, and when I do spend time thinking - I usually feel I am *learning* something (rather than doing the compiler's work).
This is an interesting problem! I can provide a quasiquoter which strips leading white-spaces based on the position of the initial `|`. I can provide another one which ignores the initial empty first line, and strips based on the amount of whites paces on the second line, but this will break if you want your first (i.e: second in file) line to start with white-spaces, or if really want a line break at the beginning. If you think you have a good solution, please open a bug/feature request, I'll happily discuss it and implement it.
This might be completely obvious for you, but I thought I should mention that in cases like this (function was called with wrong number or type of arguments), you could query `FileForm` from an interactive ghci session, e.g. via `stack ghci`, and then you can get this kind of information via `:info FileForm`.
Thanks, that was not obvious for me. The datepicker works but not as I wanted. I see now a three column datepicker where want to look like a calender. 
It is possible to use StableName to find the depth of recursion needed to determine equality, though I would consider this a last resort. import System.Mem.StableName import System.IO.Unsafe data A = A Int A depthIO :: A -&gt; IO Int depthIO x = depthIO' x [] where depthIO' (A _ x) l = do name &lt;- x `seq` makeStableName x if elem name l then return (length l) else depthIO' x (name:l) I think it is fine to use unsafePerformIO for implementing eq since eq remains referentially transparent except for being able to tell the difference between infinite values and cycles by diverging or not, which is already possible in "pure" code by looking at them deeply enough on a system with finite memory instance Eq A where x == y = depthLimitedEq x y $ max (depth x) (depth y) where depth x = unsafePerformIO (depthIO x) depthLimitedEq _ _ 0 = True depthLimitedEq (A a ar) (A b br) d = if a == b then depthLimitedEq ar br (d-1) else False 
I agree, this does seem possible.
Yeah, sounds real good. And seems like the next step in the progression `GHC.Generics` to `sop-generics`. Data type declarations are currently a fixed interpretation (sum of products) of two-layers of row types (constructor names, field names/positions -- though positions are not usually considered row labels).
Yeah, that was a pleasant surprise. I'm still musing over the choice to put the context in the record field instead of putting it inside the argument to `f`. One motivation for doing so was to avoid another level of name that needed to be disambiguated by the constructor name when possible (as in `Cases{of_U1=\() -&gt; ...}` versus `Cases{of_U2=\(Args_U2 x y) -&gt; ...}`). But it also seems to match the use case of `applyCases`: for a contravariant functor, we'll only ever feed one `a` at a time to the `f a`, so we might as well open the existential layers of `a` prior to that, in case the `f a` opens *its* argument more than once.
Your reading comprehension is quite poor.
&gt; I think you may be being dishonest with yourself This may happen, but I try hard to be honest with myself. &gt; The examples edwardk gives where these things fail are all over particular monads (arbitrary IO actions and arbitrary IO refs, or making nontrivial use of Maybe). If you restricted yourself to monads with very specific semantics that could then be lifted to IO, you might find yourself able to make stronger claims. The examples Edward gives live in a different setting where a monadic lens returns a reference to some state. I have zero idea of whether those examples apply to my setting, where there are meaningful laws for my, admittedly rather ad hoc, lenses and whether clearly not law-abiding "lenses" like `(./)` worth it. So I'll just investigate.
&gt; And actually, without the `insert i b (insert j a m) = insert j a (insert i b m)` law, an interpretation of `Map k v` can be `Maybe v`. I didn't believe this at first, but given that the only law relating `lookup` to `insert` (or to anything else) is `lookup k (insert k v m) = Just v` it is, I suppose, true! We clearly want to mean that looking up *any* key in map in which it has The laws as stated in the link also allow `union` to be union empty m = m union m _ = m AFAICT. Even with the additional `insert` law, we still don't know when `lookup` should return `Nothing`. It might seem as if `lookup k m` *has* to be `Nothing` when `k` hasn't been inserted into `m` (because how are we going to come up with a `v`?), but that's just because the interpretation of `Map k v` as `k -&gt; Maybe v` is tricking us‚Äîin fact, the only time `lookup` *has* to return `Nothing` is when the map is empty; otherwise, it can select some `v` it happens to have lying around. Of course, that isn't what anyone would expect of a function called `lookup`, but the laws allow it, and it might be just right for a function called `fear`! ‚Ä¶ the only thing that actually suggests that the laws need some fixing up here is that we *named* the type a `Map`, and we *named* the function `lookup`. 
&gt; The fact that fromJSON and toJSON don't have a retract relationship is a problem that just bit someone! And we had a whole thread about ways to possibly fix it. Why is that at all a good example? If it will be fixed, it's a bad example. Will it be? &gt; Also the fact that we haven't settled on good laws for MonadPlus is quite frustrating and leads many people not to use it, or to use it less than they otherwise could. Again: why is that a good example? Because it's widely used despite not having clear laws. Should it be not used? I'm just not sure I understand your position. &gt; Random has a ton of laws, and they matter quite a bit! Granted, bad example. &gt; `Par` isn't a typeclass -- it is a concrete monad. There's nothing to expect there. Yes, my mistake. &gt; MonadHttp inherits its laws from being, in essence, ReaderT Manager where reader is well understood This is not correct. `MonadHttp` defines two functions over abstract `m` and nothing more. "Common usage pattern" mentioned in the docs is not a law. I checked the other examples I mentioned. Okay, I can dream up some laws for those classes (except for `MonadHttp`, `Backpack`ed stuff and `IsString`, but only one counterexample is needed to disprove something), but they're just not there. `Data.Vector.Generic.Vector` has asymptotic bounds, but no coherence conditions, I can write a complete nonsense in an `instance`, but I won't do that even without a comprehensive set of orthogonal equational laws. Though, it would be quite nice to have some properties that can be `QuickCheck`ed. Still, no equational laws are presented, so I count this example as well. &gt; You are arguing very poorly, because you are defending a poor position, and you should rethink this. I'm just tired of arguing about things I consider obvious, so made a few mistakes, but I still think my other counterexamples are valid. Do you disagree with the sentiment that if something doesn't have laws, but which is really hard to accidentally misuse, it's fine to have such a thing? Or do you think that if something doesn't have laws, it's automatically not hard to misuse it? If so, then our positions just differ and we can stop here.
Instead of `managerSettings`, you need to use `tlsManagerSettings` (this is in `http-client-tls`).
&gt; The basic idea of row types is great and using them normally is quite nice. But type level programming with them in purescript is awful or at least it was some time ago. In the next release, `RowLacks` will be solved automatically by the compiler, which makes that workaround unnecessary. This should make lacks constraints much simpler.
Unfortunately there is an actual difference between forall a. p a =&gt; f a and (ignoring impredicative types issues) f (forall a. p a =&gt; a) which complicates the story considerably. You wind up needing lots of newtypes to handle this correctly I think.
&gt; managerSettings managerSettings in the code is just a variable name, not a function. 
Yea "cross compilation" refers to compiling code for a *target* platform different than the platform you *build* on. I.e. building a Windows app with a toolchain on my Linux machine or building an iOS app from a Mac. A given installation of GHC currently can only target one platform. You have to rebuild the compiler and install it separately to get a version of GHC that targets a different platform. Contrast this to Clang, where any installation of Clang can target any of the platforms that Clang supports via its `-target` argument. This is particularly awesome because it means you only need one compiler binary to do all of the platforms you want. So the news here is that they're looking into adding this ability to GHC, which is complicated because GHC ships with a pretty large set of core libraries and because GHC currently "hardcodes" platform specific stuff (like the size of an int) at build time.
Sorry, I thinkoed that as `defaultManagerSettings` ü§¶‚Äç‚ôÇÔ∏è What you need to swap then is `newManager` for `newTlsManagerWith`, which takes the same `managerSettings` input.
[removed]
[Here it is](https://hackage.haskell.org/package/soap-tls-0.1.1.4/docs/Network-SOAP-Transport-HTTP-TLS.html) 
It's off-topic a bit, but what laws define "Map-like" properly? Here's my attempt in 1 hour: {- | Laws for Map: 1. (union, empty) forms Monoid 2. lookup k empty = mempty 3. lookup k (x `union` y) = lookup k x `mappend` lookup k y 4. lookup j (singleton k v) = if j == k then v else mempty -} class (Eq k, Monoid v) =&gt; Map k v m | m -&gt; k v where empty :: m union :: m -&gt; m -&gt; m singleton :: k -&gt; v -&gt; m lookup :: k -&gt; m -&gt; v instance (Ord k) =&gt; Map k (First v) (Map.Map k v) where empty = Map.empty union = Map.union singleton k (First mv) = case mv of Nothing -&gt; Map.empty Just v -&gt; Map.singleton k v lookup k m = First $ Map.lookup k m
Thanks!
It's ok :)
In addition to what /u/ElivshJerricco already said: We were able to build cross compiling GHCs \(a GHC that targets a different host than the build machine\) for quite a while. E.g. see the cross compilers I provide on http://hackage.mobilehaskell.org.
Thanks! Yes this will be a very long road, but I am hopeful it will allow us to have some foundation onto which we can build some form of Template Haskell Light that does not require any evaluation on the actual target anymore. Full Template Haskell will likely require the splices to be run on the target to guarantee the correct results.
Hi folks \- it has taken me a long time but I am starting to get to a point in Haskell where I am "building real things", and starting to look at real\-world code / libraries. It has struck me that in many libraries, there seems to be a **core datatype**. I**n aes**on, it'**s Val**ue; i**n optparse\-applicati**ve, it'**s Pars**er; i**n condu**it it i**s Condui**tM. Do all / most libraries follow this pattern? Does it have a name? What is the fundamental reason that it occurs so often \(at least in the libraries that I am getting a chance to play with?\). At the risk of sounding over\-dramatic, I feel like there is something deeper about the nature of programming that is surfacing here, but I can't quite put my finger on it.
Hi folks, I am reading the docs for [optparse\-applicative](https://www.stackage.org/haddock/lts-11.7/optparse-applicative-0.14.2.0/Options-Applicative.html#t:Parser), specifically the docs on the **Parser** type. If you go to that link, the docs don't show the **data constructors** for the **Parser** type. In general, whenever I see docs for a type that don't show the **data constructors**, am I to conclude that the **type** is an **abstract data type**, and that I will be using smart **data constructors** to create values of that type? 
Haha :))
I have heard about the **builder pattern** in Haskell. What is the builder pattern? Given that the **Scraper** data type is abstract, is the [attrs](https://www.stackage.org/haddock/lts-11.7/scalpel-0.5.1/Text-HTML-Scalpel.html#v:attrs) function in the **scalpel** library an example of the builder pattern? Given that the **Parser** data type in **optparse\-applicative** is abstract, is the [strArgument](https://www.stackage.org/haddock/lts-11.7/optparse-applicative-0.14.2.0/Options-Applicative-Builder.html#v:strArgument) function also an example of the builder pattern? If not, is there a name for the pattern that these two functions exhibit?
very cool, I have yet to require cross compiler functionality -- but i can already see that they will make their way into my homelab when the time is right. skimmed through the list so i may have missed something, but have you worked on an aarch64 RPi implementation? Last time i checked there were quite a bit of issues surrounding the platform and GHC
Since your language isn't lazy, the question isn't as relevant, but in a lazy language you'd have to sort out what it means to write code like this: let a! = () -&gt; { printLn!("Oh look. A side effect."); return 4 } let b! = () -&gt; { a!(); return "frogs"; } b!() In a lazy language, there's no particular reason for the `a!()` to be evaluated - the return value is not needed to evaluate `b!()`. Since your language is strict, the intended semantics are more apparent - `a!` would be evaluated and its side effect would happen before `b!` completed evaluation. Since you don't need to have a monadic structure to provide your sequencing of effects, this doesn't matter. In Haskell, however, you could write `b` like this: b :: Monad m =&gt; m a -&gt; m String b f = f &gt;&gt; pure "frogs" And call it with an `IO` action to print stuff, or a `Maybe` action that could be `Nothing`, or a `[]` action for a non-deterministic number of frogs returned, to name a few. To be more explicit about what you're losing by surrendering generality, take a look at the languages that eschew monads on principle and how they cope with errors, missing values, asynchronous results, or continuations. Each one winds up bringing in special case syntax - stuff like the `?.` operators, checked exceptions, the `async/await` keyword pair, or `yield` keywords which turn a regular procedure into something a bit different. You can trade the generality of monads for the specific syntax, but I'm not sure what the "pro" side of the trade-off is exactly.
Now, that I know some of the usefullness of Haskell, are there cases, where Haskell frustrates compared to its imperative and OOP counterparts ?
Examples (detailed instructions) for Servant and Scotty in the repo under examples/.
It's interesting that you make the point about writing style. I actually find Programming in Haskell quite difficult to read. Don't get me wrong it is a fantastic resource and it's highly regarded by seemingly everyone but I still find it difficult to read. 
To quote: &gt; BTW, building a 400 line library and writing reams of control code for a fridge &gt; that has not been installed yet is what we Haskell programmers call "laziness". May be it is co-laziness or "over-eagerness" on the haskell side
This is a good catch, and thanks for pointing it out. It certainly is less poetic though :(
There is so much wrong in the api design of sklearn (how can one think "predict_proba" is a good function name?). I can understand this, since most of it was probably written by PhD students without the time and expertise to come up with a proper api. Worse is tensorflow, where the graph is essentially a set of global variable all mutated at each step of gradient decent.
Well there is aarch64 support for mach-o/elf. The primary reason for not having aarch64 prebuilts is that there are no official 64Bit Raspbian Images as far as I know. You could of course always build a pure aarch64 cross compiler yourself.
Ah, of course! How that didn't quite occur to me... Done, added you as an editor.
Interesting approach. To me your simplifications don't look at all like simplifications and I by far prefer reading the original code :)
&gt; I'm sorry, but could you please elaborate a bit ? or give an example? Because I have been looking at the documentation for "sepBy" and Im not sure how to apply it specifically in this case. &gt; parseOnly ((char 'x' `sepBy` char ',') &lt;* endOfInput) "x,x,x" Right "xxx" &gt; parseOnly ((char 'x' `sepBy` ((char ',' *&gt; char ' ') &lt;|&gt; (char ' '))) &lt;* endOfInput) "x x, x" Right "xxx" Here is how you would implement it: import Data.Attoparsec.ByteString import Data.Attoparsec.ByteString.Char8 doc :: Parser [Prose] doc = prose `sepBy` ((char ',' *&gt; char ' ') &lt;|&gt; char ' ') Or once you want to include more sentence markers you could also write it as: &gt; import Data.Semigroup &gt; import Data.List.NonEmpty &gt; let endP = (sconcat $ fmap (\s -&gt; string s &lt;* skipMany1 (char ' ')) $ "" NE.:| [",", ".", "?", "!"]) in parseOnly ((char 'x' `sepBy` endP) &lt;* endOfInput) "x, x? x x x. x" Right "xxxxxx" However, if you want to parse a sentence marker at the end you have to use something else, for example, `many (prose &lt;* endP) :: Parser [Prose]`. 
Unfortunately, Using polymorphic variants carries its own contraint baggage... import Data.Diverse import Data.Functor.Contravariant import Data.Functor.Contravariant.Divisible data A = A data B = B data C = C a :: Predicate A a = undefined b :: Predicate B b = undefined c :: Predicate C c = undefined a' :: Predicate (Which '[A]) a' = contramap obvious a b' :: Predicate (Which '[B]) b' = contramap obvious b c' :: Predicate (Which '[C]) c' = contramap obvious c chooseWhich :: (b ~ Complement bc c, bc ~ Append b c, Reinterpret c bc, Decidable f) =&gt; f (Which b) -&gt; f (Which c) -&gt; f (Which bc) chooseWhich = choose reinterpret foo :: Predicate (Which '[A, B, C]) foo = chooseWhich a' (chooseWhich b' c')
Servant needs a new testing library! And I need to test a distributed job queue... Thank you for helping us test all of the states.
This looks quite neat. I guess this is more of a question about `quickcheck-state-machine` but, other than using QuickCheck, how does this compare to Hedgehog's parallel state machine testing? One thing which stands out to me is that Hedgehog (at least, the public API; I haven't looked into the implementation) only supports two threads for the parallel suffix bit, whereas your two libraries allow an arbitrary number. Do you have any thoughts about the cost (to users) of needing to use your `StateMachine` monad, rather than the `distributed-process` `Process` monad they are more familiar with? In my concurrency testing library, [dejafu](http://hackage.haskell.org/package/dejafu), I use a typeclass for concurrency, which is definitely the biggest hurdle to adoption (no matter how simple I make switching), and if I were starting it from scratch today I'd probably look at using something like backpack instead.
I believe it just means that the module doesn't export the data constructors (and yes, that means you'll be using smart constructors / combinators to build values of that type). All types that are declared with the `data` keyword are Abstract Data Types. If you click on the `#source` link next to the Parser type, you can see the data constructors. They don't look like something you'd want to use directly anyway. :)
Well, `playIO` is a type specialization - my haskell-mode IDE can show me that type on `playIO` itself, where it is used. `Library Picture` is more DRY and type-safety than a simplification. It makes it harder to make a mistake, and easier to add images to the library. Uses `Traversable` which not everybody is comfortable with. `updateWorldIO` exposes that we only apply a pure function to the first element of the tuple (the World). Harder to see with the original, larger formulation. Uses a few lenses, ditto.
&gt; I guess this is more of a question about quickcheck-state-machine but, other than using QuickCheck, how does this compare to Hedgehog's parallel state machine testing? `quickcheck-state-machine` and Hedgehog's parallel state machine testing are quite similar, and there has been some cross-pollination of ideas between the projects. &gt; One thing which stands out to me is that Hedgehog (at least, the public API; I haven't looked into the implementation) only supports two threads for the parallel suffix bit, whereas your two libraries allow an arbitrary number. I actually only `quickcheck-state-machine-distributed` supports arbitrary number of threads. It would be an easy patch to make `quickcheck-state-machine` support more threads (and probably Hedgehog as well). However I'm not sure you'd gain much by doing so. More threads means slower tests, and there's a paper that Hughes' et al cite that shows that empirically something like 95% of all race conditions can be found using two threads (this is also what Erlang Quickcheck uses). &gt; Do you have any thoughts about the cost (to users) of needing to use your StateMachine monad, rather than the distributed-process Process monad they are more familiar with? This is a good point, I'm not sure. Personally, I started writing a `distributed-process` program using `StateMachine` monad approach, because something I read in "Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial" by Fred Schneider: &gt; In practice, having to structure a system in terms of state machines and clients does not constitute a real restriction. Anything that can be structured in terms of procedures and procedure calls can also be structured using state machines and clients‚Äîa state machine implements the procedure, and requests implement the procedure calls. In fact, state machines permit more flexibility in system structure than is usually available with procedure calls. With state machines, a client making a request is not delayed until that request is processed, and the output of a request can be sent some- place other than to the client making the request. We have not yet encountered an application that could not be programmed cleanly in terms of state machines and clients. It was only after the program was written and I started thinking about how to test it, that I realised, "hang on this is a state machine, and I know how to test those"... That said, I should perhaps stress that the program you test doesn't necessarily be written using `StateMachine`, you only need a thin `StateMachine` interface/client to your program.
Thanks for writing - I hadn't appreciated you could use `Traversable` in this (nice) way.
[removed]
[removed]
[removed]
[removed]
It does in Haskell. I'm not sure how other languages handle it.
Yeah, ok. My language is a bit strong there. "That doesn't sound like a good abstraction" should probably be: "These don't sound like strong qualities in an abstraction." I would argue that `MonadHttp` and `Foldable` aren't terribly good as abstractions. They provide a common interface and allow programmers to achieve fairly specific goals, but they rely heavily on convention for being effective. In the case of Foldable, the goal a programmer may be trying to achieve by implementing an instance is so ill-defined that there is almost nothing they can do to verify a given implementation as 'valid.' In the case of `MonadHttp`, the goal is so limited and concrete that it's difficult to characterize it as a true abstraction at all - It seems more a layer of indirection. However, as it's problem domain is finite and understood, this is less of an obstacle, and as it is supposed to provide a layer of indirection over a well document and well understood series of interactions with a clearly defined protocol, I'd say it does have laws after all - Those laws are just not easily expressed algebraically. Both suffer from a lack of laws as we see them expressed elsewhere, in that it requires more effort on the part of the programmer to validate an implementation - If that is even possible! &gt; "maybe sometimes a thing can make sense without a few inaccurate equations associated with it". No, it really can't. Sometimes we don't know precisely what those 'inaccurate equations' are, or have difficulty expressing them, but if you can't form a concrete set of rules around what your abstraction is supposed to do, and how it's supposed to behave, that's an obviously bad thing. Sometimes we need 'obviously bad' solutions in the real world to get things done - But we really don't need them at the 10,000 foot level.
maybe use stack to install haskell. See https://docs.haskellstack.org/en/stable/README/#how-to-install I do not recommend the haskell platform 
Dunno how exactly TH works, but you could try to split out the heavy stuff into modules. Another option, though it will only get you so far, would be to increase GHC heap size.
I already split all the TH bits in one separate module. This is the module which seems to fail to compile, which funnily I don't need to profile. I'll try increasing the heap.
Well, if heap size helps then it's all well and good. If not, try to split up this module even further.
I just did a quick and ugly test using my `IntegerCalculator` class and `JParsec`'s `Calculator` class: Mine: 7.5k expressions/s JParsec: 475k expressions/s Pretty bad :P 
&gt; hem last summer, holy shit it was great wow, would you mind sharing what you found great about the book? thanks 
Having constraints in a data declaration is deprecated and will be removed entirely from GHC, at least, if it hasn't been already. So in any current Haskell code, all constraints should be on functions rather than datatypes. As for why it's deprecated, I'd have to get back to you on that after doing some digging.
I suppose one reason to avoid constraints in data declarations is that not all functions over your data type might require the same constraints. Functions with fewer requirements will end up unnecessarily overconstrained. &gt; we have to repeat it in the declaration for functions. The [Backpack module system](https://wiki.haskell.org/Module_signature) can provide a workaround of a sort for cases when repeating the same constraints each time becomes tedious. Backpack lets you say things like "this module depends on some abstract data type, let's call it T, that must be an instance of such-and-such typeclasses". And then the functions in your module refer to the abstract type T instead of to some type variable, and do not need to repeat the constrains that were required of T.
&gt; I would argue that `MonadHttp` and `Foldable` aren't terribly good as abstractions. I totally agree. My point is that sometimes not-terribly-good abstractions can do the job. Surely we need to strive for better tools. But I've come across `Control.Lens.Cons` today. I don't like this abstraction. And no laws are specified. &gt; However, as it's problem domain is finite and understood, this is less of an obstacle, and as it is supposed to provide a layer of indirection over a well document and well understood series of interactions with a clearly defined protocol, I'd say it does have laws after all - Those laws are just not easily expressed algebraically. &gt; &gt; "maybe sometimes a thing can make sense without a few inaccurate equations associated with it". &gt; No, it really can't. I see a contradiction here. Anyway, laws that are not expressed in some precise form are not laws -- it's just intuition. I say that sometimes intuition is enough, which appears to be a really unpopular opinion, yet `data-default` has 200k+ downloads.
I asked the same question on SO a while ago and the [answers](https://stackoverflow.com/questions/24465586/why-constraints-on-data-are-a-bad-thing?rq=1) were really interesting. 
_homer simpson voice_ "Stupid Theta" https://prime.haskell.org/wiki/NoDatatypeContexts
Generally this occurs with any NSIS installer when the temp directory doesn't have sufficient permissions for the user. For example: https://support.mozilla.org/en-US/questions/1023811
That does not resolve the question, which is about installers not working due to bad permissions on a temp folder. It isn't helpful to give a "do something else entirely" answer, especially when often that doesn't resolve the actual underlying issue.
Sure, but there is a difference between laws that cannot be expressed, and laws that cannot be expressed precisely. Default and Foldable can't have laws at all, it's anathema to their design. (Arguably you could make a recursion scheme that implemented Foldable somehow in a law abiding way, but I think that'd be a different beast with a different goal). MonadHTTP has clear, definite goals that are just kind of crappy to express with algebra. I would argue that abstraction without definable laws isn't actually abstraction, it's just indirection wearing a fancy skirt. There is a valid usecase for indirection, but it tends to harm composability. You lose the ability to reason about code atomically, without the full knowledge of all components under the lawless indirection. I will agree that sometimes that's useful in bounded domains but I will not agree that it's a good foundation for further abstraction. 
/u/Tayacan thank you! 
&gt; Sure, but there is a difference between laws that cannot be expressed, and laws that cannot be expressed precisely. Yes, but it's hard to distinguish both theoretically and practically. Is there no laws for `A`, because it's not easy to find some/express known or because there can't be meaningful laws? Depending on `A`, I guess, this can be a hard question. &gt; I would argue that abstraction without definable laws isn't actually abstraction, it's just indirection wearing a fancy skirt. I'll use the word "indirection" from now on, thank you. &gt; I will agree that sometimes that's useful in bounded domains but I will not agree that it's a good foundation for further abstraction. I think so too.
what is your perfect saturday
How do you change the directory's permission?
One in which my streaming software doesn't crash continuously, while I get everything ready for the next day's stream. ;)
oh no haha!
The pacing is absolutely perfect. The field of study it covers is fractal, at any moment you can investigate in ten different directions. It's very impressive then that they managed to create something where every chapter builds on the last, but you never find yourself out in the weeds. They efficiently carve a path through the field such that by the end of it you find yourself well-rounded, but at no point are you faced with an impossible struggle nor something that's too easy or boring. The exercises are inline with the text. They are also self-grading - the compiler tells you whether you've got it right. So on one hand you're learning in an interactive fashion, but on the other you're never waiting on a teacher or a grader or... If you've ever played a Zachtronics game, or *Manufactoria*, then *Software Foundations* feels very similar.
&gt; All types that are declared with the data keyword are Abstract Data Types. That's not true. `data` declares an "Algebraic Data Type", not an "Abstract Data Type" (Both are ADT, so that's an unfortunate collision). The fact the data-constructors are not exported, and you can only interact with the type via an abstraction indeed makes it an "Abstract Data Type".
There's no limit on the heap by default, so I'd be surprised if that helped.
I'm trying to figure out how to handle IOExceptions, in particular, for file IO. I'm wondering if there is a library that wraps the exception raising functions from base to make explicit the exceptions that need to be handled? For example, something along the lines of: &gt;openFile :: \(MonadIO m, MonadExcept DoesNotExistError m, ...\) =\&gt; FilePath \-\&gt; IOMode \-\&gt; m Handle
bad bot
Well, if I'm being pedantic and technical, even functions that return `IO` are pure. Even `IO`'s `Monad` instance is pure. It's more accurate to say that `IO` is *hard to predict* rather than impure. I'll give you an approximation of how `IO` is implemented in Haskell: newtype IO a = IO (RealWorld -&gt; (a, RealWorld)) Notice that this is just the `State` monad specialized to `RealWorld` instance Monad IO where return a = IO (\rw -&gt; (a, rw)) IO f &gt;&gt;= k = IO $ \rw -&gt; let (a, rw') = f rw IO b = k a in b rw' The only differences between this and the actual implementation in GHC are performance and laziness related. The idea here is to say that a program in Haskell is a function that statefully changes the universe, in the same what that the state monad gives you purely functional state. If you ignore the idea that "IO is somehow impure," then you'll see that the above instance is completely pure, and there's no impure magic in the `IO` type definition. The runtime gives us a `RealWorld` value, and the fact that it's different every time we run the program (at least metaphorically different) is the reason the program doesn't behave the same every time; the input `RealWorld` is different, so the output may be different too. At the core, haskell has some primitives that I would call "unpredictable", not "impure." These primitives essentially have the type `RealWorld -&gt; (Foo, RealWorld)` and are not implemented in Haskell. GHC gives those primitives implementations that metaphorically inspect the `RealWorld` and return an altered one, which in practice has the effect of doing something that seems impure. But really all this means is, since we can't inspect `RealWorld` values at all, we can't know what we're giving these functions, and we can't tell what they're going to do to the `RealWorld`. Unpredictable, not impure. If you want an example, you can look at the [primitive `MutableArray#` implementation](https://hackage.haskell.org/package/ghc-prim-0.5.2.0/docs/GHC-Prim.html#g:8). (Note that `State# s` is used where I've been saying `RealWorld`). If you look at the source, the functions are all defined like `newArray# = newArray#`, because the actual implementation comes from GHC. With these primitives, we build *completely pure* programs on top. `putStrLn` is just a combination of a few of these primitives, with a bunch of pure code for shuffling around the data. --- Finally, consider the mixing of pure and "impure" (rather, unpredictable) code in Haskell. For example, `foldr` is pretty clearly completely pure. foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b foldr _ z [] = z foldr f z (x:xs) = f x (foldr f z xs) But consider this use-case of it: foo :: [Int] -&gt; IO () foo xs = foldr f z xs where f x acc = print x &gt;&gt; acc z = return () By calling it with these values, we're specializing it to the type: foldr :: (Int -&gt; IO () -&gt; IO ()) -&gt; IO () -&gt; [Int] -&gt; IO () Clearly, `foldr` now returns `IO`, but that doesn't make it "impure." `foldr` itself is as pure a function as any. In fact, `foldr` itself hasn't even become unpredictable here. It only *returns* a function that is unpredictable. But the creation of that function is itself perfectly predictable. This is why people distinguish between `IO` *actions* and `IO` *execution*. An action can be predictably computed. The execution of that action is what is unpredictable; i.e. calling that action with an unpredictable RealWorld returns another unpredictable RealWorld. The only actual sources of impurity in Haskell are machine/OS level things, and the globally available `realWorld# :: State# RealWorld` token. `realWorld#` is how `unsafePerformIO` is implemented, and it lets you call these "unpredictable" functions with a predictable, pure value. So when those functions inevitably behave unpredictably, they are violating our intended design; all because the `realWorld#` value breaks all the rules. --- Anyway, long story short, introducing a notion of impurity at all is considered a problem. Haskell is perfectly pure *despite* `IO`. We get a lot of features from "pure" `IO`, such as talking about "impurity" on pure functions, such as `foldr :: ... -&gt; IO ()`.
i googled "change permissions directory windows 10" and this seems like a decent enough guide: https://www.laptopmag.com/articles/take-ownership-folder-windows-10-using-file-explorer
BAAAH you are right, I'm an idiot :P Thanks for pointing that out. :) 
It seems that the problem doesn't lie in GHC but in GCC. I knew I have a really BIG file which I splitted a few months ago but forgot I had a another one. I splitted it too and now I'm a waiting nfir it to compiled ... It should be finished tomorrow morning. For information, the big file is a generated one corresponding to the full schema of database (a few hundred tables) feeding Persistent Template Haskell machinery. 
You could try [/r/haskellquestions](https://reddit.com/r/haskellquestions) instead. But also probably nobody is answering you here because you're not providing any information about why you found this difficult, what you do or don't understand about Haskell, why you need this, etc. Generally this isn't a rosetta code subreddit, so we're probably not going to just translate this for you, and they won't be doing that over on haskellquestions either. But people on both subreddits are quite friendly and will try to help you if you work with them a little bit instead of expecting them to do all the heavy lifting.
What part in particular are you having troubles with? 
Note, when matching on a `Fail i [String] String` - You have the original failed input in `I`, so it's available for you to pass back into another run. So you may find it simpler to just loop your parser with an external control function than to try to use attoparsecs idea of incremental input - Especially since you're running a REPL so your input sizes are probably more than small enough to get away with the performance hit from repeated failed parsing. This might also make stuff like changing the REPL prompt to indicate a continuation a little easier to handle. If you want a cooler method of doing this anyway, for kicks, megaparsec likely has better support for your specific usecase.
Given that a few lines later, they have: &gt; import Data.Maybe (Maybe(Just, Nothing), fromMaybe) I suspect that this individual may not be using the same Prelude that I have come to know and love.
While fewer is generally better when it comes to dependencies, what is this giving up? There are a lot of ways to do things incorrectly when it comes to benchmarking and statistical analysis.
One way to do this is to create your own parser type that has three return states: - Success - Failure - Incomplete Where `Incomplete` returns a parser to parse the next line with. Here's a quick example which uses such a parser to parse balanced parentheses: {-# OPTIONS_GHC -Wno-name-shadowing #-} {-# LANGUAGE FlexibleContexts #-} module Balanced where import Control.Applicative import Control.Arrow import Data.Functor.Identity import Text.Parsec hiding ((&lt;|&gt;), optional) -- $ setup -- &gt;&gt;&gt; :set -XFlexibleContexts -- | -- -- Normal parsec can parse balanced lines: -- -- &gt;&gt;&gt; runParser balanced () "line 0" "(()())()" -- Right () -- -- But fails when attempting to parse incomplete input: -- -- &gt;&gt;&gt; runParser balanced () "line 1" "(()(" -- Left "line 1" (line 1, column 5): -- unexpected end of input -- expecting "(" or ")" balanced :: Stream s m Char =&gt; ParsecT s u m () balanced = char '(' *&gt; balanced *&gt; char ')' *&gt; balanced &lt;|&gt; pure () type ContParsec s u = ContParsecT s u Identity newtype ContParsecT s u m a = ContParsecT { getContParserT :: ParsecT s u m (Either (ContParsecT s u m a) a) } runContParser :: Stream s Identity t =&gt; ContParsec s u a -&gt; u -&gt; SourceName -&gt; s -&gt; Either ParseError (Either (ContParsec s u a) a) runContParser = runParser . getContParserT instance Show (ContParsecT s u m a) where show _ = "&lt;ContParsecT&gt;" instance Functor (ContParsecT s u m) where fmap f = ContParsecT . fmap (fmap f +++ f) . getContParserT instance Applicative (ContParsecT s u m) where pure = ContParsecT . pure . pure mf &lt;*&gt; ma = ContParsecT $ liftA2 check (getContParserT mf) (getContParserT ma) where check (Right f) (Right a) = Right (f a) check (Right f) (Left ma) = Left (f &lt;$&gt; ma) check (Left mf) _ = Left (mf &lt;*&gt; ma) instance Alternative (ContParsecT s u m) where empty = lift empty ContParsecT ma &lt;|&gt; ContParsecT ma' = ContParsecT (ma &lt;|&gt; ma') lift :: ParsecT s u m a -&gt; ContParsecT s u m a lift = ContParsecT . fmap Right resume :: ParsecT s u m a -&gt; ContParsecT s u m a resume pa = ma where ma = ContParsecT $ (Right &lt;$&gt; pa) &lt;|&gt; pure (Left ma) -- | -- Parses balanced lines fine -- -- &gt;&gt;&gt; runContParser balanced' () "line 0" "(()())()" -- Right (Right ()) -- -- Returns a parser for the next line when unclosed -- -- &gt;&gt;&gt; runContParser balanced' () "line 1" "(()(" -- Right (Left &lt;ContParsecT&gt;) -- &gt;&gt;&gt; let (Right (Left p)) = it -- &gt;&gt;&gt; runContParser p () "line 2" "))()" -- Right (Right ()) -- -- Still throws an error when parens unbalanced -- -- &gt;&gt;&gt; runContParser (balanced' *&gt; lift eof) () "line 3" ")(()())()" -- Left "line 3" (line 1, column 1): -- unexpected ')' -- expecting "(" or end of input balanced' :: Stream s m Char =&gt; ContParsecT s u m () balanced' = lift (char '(') *&gt; balanced' *&gt; resume (char ')') *&gt; balanced' &lt;|&gt; pure () 
So again we have a global/local thing here. Globally we can distinguish `Int` and `N`, but locally (i.e. in the section distinguished only by their behavior under `Eq` and `Hashable`) we can't. So in a sense we arguably want to be always representational, but we don't have to be representational with regards to the "global section" of code, but only with regards to some sort of local constraints. And "nominal" is just the top point on the lattice of constraints. This conceptual approach helps me see what's going on, but I don't see how to directly translate it to the existing machinery. One idea could be that `Coercible` becomes a compiler-solved _three argument_ constraint, where the first argument is the tuple of constraints that the coercion is with regards to. So we have `Coercible (Eq, Hashable) Int N` as the constraint instead? And `type role HashMap (representational (Hashable, Eq)) representational`. Anyway, just spitballing. 
Every function, modulo bottom and async exceptions, is pure, in that it should return the same output for the same input, every time. Functions with a return type in the IO monad produce a description of an action to be performed by the IO monad interpreter (the runtime system) - but will always return the same action for the same input. We can usually read Haskell as if IO actions happen as a function is evaluated, so any function with an IO type may have side effects. Of course, "side effects" is a poorly defined term. A function producing a big data structure has a memory allocation side effect, but it's undeclared. IO is very coarse grained, meaning anything from reading a mutable reference to launching the missiles.
Good trick! We really should get around to allowing multiple default signatures. They have a well defined order for checking at the definition site, and its just a convenience, so there isn't any theoretical reason why we can't run through several defaults in sequence.
You may want to look at `bracket`. - Standard `base` version - https://hackage.haskell.org/package/base-4.11.1.0/docs/Control-Exception.html#v:bracket - More generalized version from the `exceptions` package - https://hackage.haskell.org/package/exceptions-0.10.0/docs/Control-Monad-Catch.html#v:bracket
With the new machinery for `QuantifiedConstraints` that Simon has been working on we can make `Representational` into a `Constraint`, with no extra effort required from users. class ( forall a b. Coercible a b =&gt; Coercible (f a) (f b) ) =&gt; Representational f instance ( forall a b. Coercible a b =&gt; Coercible (f a) (f b) ) =&gt; Representational f class ( forall a b. Coercible (f a) (f b) ) =&gt; Phantom f instance ( forall a b. Coercible (f a) (f b) ) =&gt; Phantom f 
Unrelated but interesting: there's a Haskell street in St. Petersburg, Florida.
Thanks a lot.
thanks very much for your advice, lot of Haskellers here are suggesting to study category theory which some convincing me to learn, however, I am still reluctant about that way maybe to not able to see practical use as you have mentioned above.
[removed]
Just wanted to add: I see you put "How to proof it" on your list, and it seems people are divided about that. I would argue this: If you are a bit like me, that is you have an inquisitive mind but no formal education in maths (in university or anything ike that) you wil enoy this book and benefit greatly from it. Especially just the first 2 chapters. Here you will learn all about logic and all the symbols and their meanings that appear everywhere. The stuff everybody assumes you already know. If you do know -&gt; skip it it. If you don't -&gt; this book is an excellent place to start such a journey.
&gt; If you are a bit like me, that is you have an inquisitive mind but no formal education in maths exactly this is the case, I do not have background in math which is I was looking for book to start with and I am keen to study. Thank you for pointing out about to this advice since I was going to remove it from my reading list. so finally the reading would be like this as starting : at least the first two chapter of *How to prove it * plus Software foundations 
&gt; Code golfers welcome. w%n=(`drop`w).(*4).(`mod`n) main=mapM((putStrLn.)$max&lt;$&gt;show&lt;*&gt;"Fizz"%3&lt;&gt;"Buzz"%5)[1..100] [Try it online!](https://tio.run/##Fca7CsIwFADQ3a8opYV7O4QWdUsziDjp1FGEXGixoc2DNCHSn4@6HM5M2zKta1baWR@KKwViw6TV29voCgAuEHOqTQ9y9NbJhAya0w@p7SgNHjQp02tyDwAXwxD83TCsNH14JbbZJt6I8qb2vayPXJSX@N8Znx1jXdu@cv4C "Haskell ‚Äì Try It Online") 90 bytes. Uses the Applicative instance for functions as well as the Semigroup instance for functions. Requires GHC 8.4 for Semigroup in the prelude I got this while messing around trying to (unsuccessfully) shorten [this](https://codegolf.stackexchange.com/a/58800/69655) solution a few weeks ago. I believe this is the shortest publicly visible FizzBuzz. The shortest known FizzBuzz is the 81 bytes by Henkma [here](http://golf.shinh.org/p.rb?FizzBuzz#Haskell)
I followed the steps and it seems that I have the right permissions, but the problem stays... https://i.imgur.com/QpzT176.png 
Why is that so?
Nice start, showed a few notable concepts quickly. Lucky hacker rank didn't test error conditions though (e.g. what if the count of items and the list length differed?)
Did not know about the SemiGroup instance. That‚Äôs really cool!
Edward Kmett is working on a slightly more complex problem (allowing for insertion in the middle, but possible incomplete). He has a nice method he's playing with called Monoidal Parsing: https://www.youtube.com/watch?v=Txf7swrcLYs ; https://www.youtube.com/watch?v=090hIEiUoE0 
&gt; you'd have to remember or check the order for which you get That's assuming you'll ever write an instance where: - multiple defaults are plausible (which often enough you won't; see the example in the OP), *and* - the different defaults aren't equivalent anyway (e.g. defaults for `fmap @((-&gt;) r)` could come from `Applicative` or `Representable`, but they're equivalent), *and* - you care about the difference (if you're using e.g. `Binary` to transfer data, you don't care about the exact representation; only that `decode . encode = id`).
As you said, this is an async resources loader and unloader. I used it to manage and restrict effects in state updates. There'a also the text loader thingy there because i thought I'll need it to cache text loading (because you need to create a new texture each time you want to display text otherwise) but it's unused because creating a text texture each frame is actually fast enough. 
&gt; I could call parse, then if a Partial is returned, call that function with "" to see if success (valid input) or failure (needs continuation) is returned. Is there a nicer way of doing this? That sounds like the idea solution, what's wing with it?
Annoyingly that page doesn't seem to link to the actual solution :(
[removed]
Very cool! Great work!
In these kinds of coding problems, the input specifications are always trustworthy. So it's not "lucky", it's just what you *should* expect and take advantage of. The extra number specifying the input length is there to make life easier for C/C++ programmers. 
Oh wow, those hackerrank templates. Must be directly translated from C++. 
Not an expert on GADTs but it seems like the distinction is between a context on the datatype itself, and a context on a constructor. Contexts on individual constructors aren't possible on normal ADTs, but are with `-XGADTs` even without using GADT syntax: data Map k a = Ord k =&gt; MapNode k a (Map k a) | MapLeaf is a GADT identical to data Map k a where MapNode :: Ord k =&gt; k -&gt; a -&gt; Map k a -&gt; Map k a MapLeaf :: Map k a This works just fine - the `Ord k` dictionary is stored within the MapNode constructor and brought into scope when you pattern match on it, but the same is not true of normal ADTs with a context.
Thanks!
My suggestion was going to be a series of ranked choices. There is a typical usecases where there is an always sensible but generally less useful fallback after a higher quality but less likely best case. Consider if I wanted to add support for lifting coercions in `Functor`. It turns out that that is a real pain in the neck, because not every Functor can have a representational role due to the vagaries of our role mechanics, but if it does, I want to be able to `fmapCoerce` it in O(1), even when the thing is an abstract `Functor`. Ideally I'd be able to say something like class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b (&lt;$) :: a -&gt; f b -&gt; f a (&lt;$) = fmap . const liftCoercion :: Coercion a b -&gt; Maybe (Coercion (f a) (g a)) default liftCoercion :: (forall a b. Coercible a b -&gt; Coercible (f a) (f b)) =&gt; Coercion a b -&gt; Maybe (Coercion (f a) (g a)) liftCoercion Coercion = Just Coercion default liftCoercion :: Coercion a b -&gt; Maybe (Coercion (f a) (g a)) liftCoercion Coercion = Nothing Now `fmapCoerce` is always defined. If the role of the Functor argument is representational it'll do the right thing and lift. If not, it'll fall back. With that in hand, we can now define `fmapCoercion`. fmapCoercion :: Coercion a b -&gt; f a -&gt; f b fmapCoercion c = case liftCoercion c of Just Coercion -&gt; coerce Nothing -&gt; fmap coerce Then we can handle the harder cases by hand, like instance (Functor f, Functor g) =&gt; Functor (Compose f g) where liftCoercion = liftCoercion &gt;=&gt; liftCoercion ... That instance can't be properly handled in O(1) through the simpler case of trying to add `fmapCoercion` directly to `Functor`. But the vast majority of instances can pass through with user code unchanged. On the other hand, if we tried to supply just the first default signature, we'd break a _ton_ of instances of Functor. The approach given in this article is more powerful for when you need explicit control. The ability to have multiple ranked default signatures is more powerful when you don't want to break existing code or want to supply a sensible possibly-low-efficiency fallback.
&gt; data (Eq k, Hashable k) =&gt; HashMap k a The use of "stupid theta" for this makes me sad.
They sure can! I used Getters and Prisms to implement [an API for Divisible and Decidable which looks like pattern-matching](https://github.com/gelisam/contravariant-case#with-contravariant-case): allSatisfy :: Predicate a -&gt; Predicate [a] allSatisfy p = matchSum [ Case _Empty $ conquer , Case _Cons $ matchProduct [ Case _1 $ p , Case _2 $ allSatisfy p ] ] This isn't quite the API you are asking for, since it uses a list of Cases, not some binary combinator. But it comes from the same desire to avoid having to manually unpack my types into `Either`s and pairs. Anyway, this has been sitting on my hard-drive for a [long time](https://twitter.com/haskell_cat/status/789901262028468226), so thanks for this post, it gave me the kick in the butt I needed to finally open-source it :)
No, it doesn't require `Divisible`, but you're going to say that my solution is even worse: if you're missing a case, [you get a runtime error](https://github.com/gelisam/contravariant-case/blob/master/src/ContravariantCase.hs#L204), the same way you get a runtime error if you write a regular `case` expression which doesn't cover all the cases.
I am by no means an expert on this issue, but you might checkout the Yesod framework.
&gt; see the example in the OP It's not that hard to imagine a higher kinded type that has a `Fractional` instance. In fact, even just in base, `Identity` and `Complex` have both `Fractional` and `Applicative` instances and you'd probably want to use the `Applicative` default for the one and the `Fractional` default for the other.
First, Thank you very much for the feedback to all of you. I did not learn yet about Traversable that is why I did not use it in the project. For the Lens, that is the same, I wanted to practice what I learned before going further.
&gt;Wow, first thank a lot to check on my code in detail like that. I have to study what you suggest it will help me to be better in Haskell. Thanks
Would this give you a way of doing nub in an efficient way? Where the default implementation for Ord took priority over just Eq on its own?
Maybe you want to check out tasty-hspec?
I just found that interesting and wanted to share it...
Turns out the Œª-combinator has a finite-size normal form when expressed as an interaction net. That net can be used as a normal, finite program. When read back to the Œª-calculus, it produces the infinite Œª-term `Œªf . f (f (f f( ...`. I found that very interesting and wanted to share.
Turns out the Y-combinator has a finite-size normal form when expressed as an interaction net. That net can be used as a normal, finite program. When read back to the Œª-calculus, it produces the infinite Œª-term `Œªf . f (f (f f( ...`. I found that very interesting and decided to share.
Cool post! Thanks for sharing! For the record though, I find the unicode syntax particularly hard to read.
Thanks! Which part of the Unicode in particular was the most illegible? When I was writing it first, the type families were getting pretty noisy, so then I swapped in ‚àù, and pretty much decided ‚Äúin for a penny‚Äù from there.
Did you set the permissions on the global and not just user temp folder? Note that the platform is a global install, and you need to have an account that can escalate to admin permissions to install it. If you don't have permission on the computer to install globally, then indeed stack alone may be a better choice, since it manages per-user ghc installs.
Fair enough, I suppose. Without Unicode Syntax, it would have looked like this: type family (t :: k) `OfSize` (n :: Nat) = (a :: Type) | a -&gt; t n k class Finite n where induction :: t `OfSize` Z -&gt; (forall k. t `OfSize` k -&gt; t `OfSize` S k) -&gt; t `OfSize` n instance Finite Z where induction z _ = z {-# inline induction #-} instance Finite n =&gt; Finite (S n) where induction z s = s (induction z s) {-# inline induction #-} Which, to my eye, is harder to read.
At least I understand the syntax. When I‚Äôm trying to learn something new, I need something that‚Äôs familiar. And Haskell syntax would give me a nice anchor point. As it is, I have to overcome my ignorance of the mathematical symbols/syntax AND the subject matter. 
That's a fair point. I'll bear it in mind in the future.
Obligatory : iohk is hiring! If you'd like to write haskell on our wallet or language/compilers team, drop an application on our website. I've been here since February and I love it. 
Please define "normal form" in INet first.
It is specific to UTXO, which is more complicated than just keeping track of balance. The obligatory question is: why UTXO? 
An INet is in normal form if it has no redexes, it is no different from the Œª-calculus. A redex is a connection of two 0-ports. That is explained on the papers on interaction nets and combinators.
Is that true? How about in lambda calculus the following definition of fix: ``` fix f = y where y = f y ``` No redex. Is it a normal form? 
What Œª-term that corresponds to? The usual Œª-calculus has no "fix" primitive. It can be implemented with the Œª-combinator, which always has redexes, no matter how much you reduce it (which is why it has no normal form).
The definition I gave above assumes no `fix` primitive either. The trick that made it not *plain* lambda calculus was in its use of recursive variable definition, or in other words, `letrec`. The same goes to the `Y` combinator definition in INet, which also utilizes recursive knot-tying (output port feeding back into one of the input ports). So I'd be cautious in calling it "normal form". I'm glad that you are starting looking into representing recursions in INet, which is a fascinating topic with many unanswered questions. As I previously mentioned to you, readback is one of them.
Easier to parallelize and maps to functional programming concepts much more easily 
The situation isn't really any different than the current default signature situation. Except, right now you get a compile error if you don't specify the other behavior you want when you can't satisfy the default signature. See the Functor example below. I don't have a solution to that other than "just never do it" that doesn't involve something like this, because the user pain it would induce would be too high to get a marginal improvement like that through the change process if every user of Functor had to care about Coercible, and then had to give up Safe Haskell inference in order to get access to it.
[removed]
\*with memoization
Wow, this looks very promising, thank you! I will give it a proper go when I get back to the office, but thank you for all your help
Link is broken for me? Maybe it‚Äôs my phone doing something weird, but I get a 404.
Which aspect of megaparsec can help with this? I'm certainly not tied to AttoParsec: until now I've been using a modified version of Stephen Diehl's NanoParsec
Off topic, but has there been some research on efficient implementation of interaction nets? The cost of pointer indirection must be huge... 
Been a while since I read up on interaction nets. Do I remember correctly that this is equivalent to tying the knot and works by making sharing explicit?
isn't there a town in the US called haskell? somewhere near texas? I saw it on a tv show in a non US country :) 
This bit me until I properly grokked it: anything that is an Applicative or Monad supports pure. Sometimes that‚Äôs literally the only supported way to create a value. In the case of Parser above, ‚Äúpure 1‚Äù would create a Parser that consumes no input and returns 1.
Here‚Äôs some things you‚Äôll want to learn about: lens, pipes/conduit, recursion schemes. Also, just run expressions through pointfree every so often. It might produce unreadable garbage, or it might give you an insight into how to write something a better way.
I think this is splitting hairs. The title of the post already says "in Haskell" which has memoisation. Travis didn't have to introduce memoisation as an effect, just tune how he was already using it.
Both!
The former (given unlimited budget &amp; no project deadlines)
Could you please explain this in detail, relative to the linked post? That post is very concretely motivated - the `Applicative` instance as "proof by induction" makes perfect sense. With this approach - why do we want `replicate` for `pure` and `zipWith` for `&lt;*&gt;`? How does that correspond to "proof by induction"? Why is it more general (but please only address this last question after answering the first two)?
But how do you explain that other recursive terms like (Y id) do not have a inet representation free of redexes?
Yes. After diving a bit into it, this is exactly what I wanted. Also, now I finally get what lisp is all about. Thanks! 
The gauge page explains that pretty clearly. You are giving up nothing in the benchmarking and statistical analysis - it's a clone of criterion, done by Vincent Hanquez of [cryptonite](https://hackage.haskell.org/package/cryptonite) fame whom I trust knows what he's doing. You're giving up the pretty HTML and the JSON export of the high-level view of the results. You're gaining a low-level CSV dump of the measurements. So it's clear that gauge is intended to build on criterion by providing a different architecture. Pretty HTML and a high-level view of the data are actually pretty important for benchmarks. The idea in gauge is to improve the low-level API for doing the measurements, and then provide high-level views in separate libraries. That does sound like a significant improvement. But the next step of providing the high-level views in separate libraries doesn't seem to be done yet. So use gauge if you want an easier experience for the actual measurements, and you don't mind working harder to understand the results by studying the raw data, and you also don't mind giving up on the nice automatically-generated graphics for your web site.
Specifically due to the immutable model that UTXOs have
&gt; How does that correspond to "proof by induction"? The purpose of `Sing n` is to witness induction for `n`. In the case of `Nat`, OP's `Finite` carries essentially the same information. More precisely, `Finite n` is the Church encoding of `Sing n`. Since Haskell does not have first-class type-level functions, it is usually more convenient to use the first-order `Sing` instead of its Church encoding. As it can be seen in my example above, induction on `Sing` is realized with pattern matching , and induction predicates can be directly given as polymorphic Haskell types. In the case of `vReplicate`, we do induction on `n`, and the induction predicate (or "motive") would be `\n -&gt; a -&gt; Vec n a`. With `Sing`, this predicate is implicit in the overall function type. With `Finite`, we need to somehow emulate a type-level function corresponding to the induction motive. That said, `Sing` and `Finite` are equivalent modulo boilerplate. One can implement `induction` in terms of `Sing` as follows: -- import Data.Kind -- import Data.Singletons.Prelude natIndS :: forall p n. p @@ Z -&gt; (forall n. Sing n -&gt; p @@ n -&gt; p @@ S n) -&gt; forall n. Sing n -&gt; p @@ n natIndS z s SZ = z natIndS z s (SS sn) = s sn (natIndS @p z s sn) natInd :: forall p n. p @@ Z -&gt; (forall n. p @@ n -&gt; p @@ S n) -&gt; forall n. Sing n -&gt; p @@ n natInd z s = natIndS @p z (\(sn :: Sing n') pn -&gt; s @n' pn) And then we might even use some ugly point-free `singletons` type-level combinators to reimplement `replicate`: vReplicate' :: forall n a. Sing n -&gt; a -&gt; Vec n a vReplicate' n a = natInd @(FlipSym0 @@ TyCon2 Vec @@ a) Nil (Cons a) n &gt; why do we want replicate for pure and zipWith for &lt;*&gt; No strong reason; both of these functions make sense without reference to `Applicative`, so I prefer defining the `Applicative` instance in terms of them, and not the other way around. &gt; Why is it more general? It's more general in the sense that the developers have already implemented the TH derivation generically and worked out a nice API which abstracts over all singleton types. Not more general in the sense that OP's approach could possibly support a similar infrastructure, albeit probably less conveniently.
Let's put some numbers for a perspective. I've managed to implement 16m rewrites/s on Rust. For comparison, a list pattern match takes 4 rewrites. That's 4m pattern-matches per second. In JavaScript, with `-O2`, the program below takes 3.4s to complete 100m matches: const cons = (x,xs) =&gt; ({tag: 0, head: x, tail: xs}); const nil = {tag: 1}; const match = (xs, cons, nil) =&gt; { switch (xs.tag) { case 0: return cons(xs.head, xs.tail); case 1: return nil; } } var xs = cons(x =&gt; x + 1, nil); var r = 0; for (var i = 0; i &lt; Math.pow(10, 8); ++i) { r = match(xs, (f,fs) =&gt; f, 0)(r); } console.log(r.toString(2).split("").reverse().join("")); That means my computer can process about 30m pattern-matches per second on V8. The performance of GHC isn't much better than V8, so, by raw pattern-matches per second, my Rust implementation is already within an order of magnitude of GHC. As such, I expect a proper compiler from inets to assembly to be as fast as GHC, a parallel (GPU) implementation to be much faster, and that specialized hardware would do to functional programs what ASICs did to Bitcoin mining. Considering Python is one of the most used languages in the world, I'd say even my Rust implementation is efficient enough for the market's taste. Of course, that's not even accounting for all the benefits of optimality. The same program in Absal actually takes 0.1s to do the same computation and output the same result: ``` @Y #f /#x /f /x x #x /f /x x @succ #n #s #z /s //n s z @mul #a #b #s #z //a /b s z @exp #a #b /b a @for #n #x #f //n f x @40m //mul 4 //exp 10 7 @cons #x #xs #cons #nil //cons x xs @nil #cons #nil nil @match #xs #cons #nil //xs cons nil @O #xs #O #I #E /O xs @I #xs #O #I #E /I xs @E #O #I #E E @inc /Y #R #xs #O #I #E ///xs I #xs /O /R xs E @zero //32 O E @id /Y #R #xs ////xs #xs #R /O /R xs #xs #R /I /R xs #R E R @xs //cons inc nil /id ///for //exp 10 8 zero #r ////match xs #x #xs x 0 r ``` Which would mean it did 1b pattern-matches in a second. Obviously that's because it is optimizing the program is it goes; if the program was unoptimizable (hash functions?) we'd get 4m matches/s as a safe throughput.
Speaking as someone with some knowledge, but who is far from being an expert: In an ideal world, we'd have formal proofs for all programs, but I see a practical and the theoretical limitation. The practical is the cumbersome nature of the tools and complexity of their usage. Learning to use Coq (or Isabelle, or what have you) is just too much to ask from even your average Haskell programmer, who's already theoretically inclined. There'd have to be a lot at stake for me to start jotting down theorems about the code. The second problem, and the more serious one, where I haven't seen evidence that it'll ever be completely overcome, is the shifting of the burden of proof of correctness from the implementation to the specification. You might define out how your library's functions work, what preconditions they have, what the invariants are, etc.... but how do you know you've defined the right things? If the theorems become complex enough, they'll defy your intuition, defeating their original purpose... at that point, one might ask for meta-theorems, or a debugger for your types... the problem is that the guarantor of the specification is essentially your intuition that it specifies the "right thing", and that fails pretty quickly. I think formal proofs are great, but the usage of the tools and the expressiveness of the specification languages have a long way to go. Until that, QuickCheck has the best cost-benefit profile in my opinion, and let's keep in mind that even that is miles ahead of the current industry standard, which is fixed-input unit tests (j/k, the industry standard is pressing F5 in Visual Studio and deploying if it compiles).
[removed]
Looking at it simplistically, it seems to me that in the term inside `NT` on the RHS of example2, the type of `t` is forall a. T a and the type of the term to the right of `-&gt;` is forall b. Const Int b Why should `a` and `b` unify? On the surface they don't seem related. What would be the rule for that?
I suspect you're overestimating how much your lack of adoption is from lack of advertisement versus difficulty of use.
Another chance to waste gigawatts of power to accomplish nothing. Seriously cryptocurrencies are an environmental crisis in the making. They're so inefficient. 
Nice post. But most dynamic programming tutorials/examples that I have seen in Haskell, use the top down approach which is called memoization. A lot of DP problems like: Floyd Warshall, subset sum etc (I personally feel) are better expressed in a bottom up manner or what is commonly known as the tabulation approach. There are very few examples of that around and most would resort to using IORef and look very imperative.
Of course the singletons approach is better in general. However (and maybe I should have made it clear in the post), I was actually trying to figure out something slightly different from your solution. I wanted to know if I could write a general function (`induction`) that would allow me to abstract out the recursive call for a wide variety of functions (`pure` and `&lt;*&gt;` were the examples here, but I also used it for an `Arbitrary` instance). I also wanted to see if I could do it without passing newtype wrappers: In that sense, I failed, as I used `Const` in the definition of `&gt;&gt;=`. What really was the most difficult to work out was the type families: I don‚Äôt think singletons would have helped there.
Could you explain the `@@` symbols? Are they different from a normal type application? I‚Äôm really interested in your definition of `vReplicate`.
It sounds like you want something analogous to type inference for concrete types to work for free type variables. You would need a full set of typing rules for that. Is there any existing type system that does this?
We use the energy of a TV to run our network. Sorry PoS rules 
It's not splitting hairs, it's the formula for solving "dynamic programming" problems in Haskell. 1. Write the solution in terms of a recursive formula 2. Abstract over the recursive calls. You can do this in your head, or can do it explicitly in code by rewriting a function `a -&gt; b` as `(a -&gt; b) -&gt; (a -&gt; b)` with no recursive calls. 3. Redirect the recursive calls to memoization through some point in memory - a let binding, a list, an array, a memotrie, etc. If you only do the first step you get low-quality solutions like the exponential time Fibonacci solution. 
`@@` is type application for the `a ~&gt; b` kind coming from `singletons`, defined [here](https://hackage.haskell.org/package/singletons-2.4.1/docs/Data-Singletons.html#t:Apply). `~&gt;` is an implementation of more flexible type-level functions, for how it works see [this post](https://typesandkinds.wordpress.com/2013/04/01/defunctionalization-for-the-win/). Here `@@` is named `Apply`.
[This GHC Trac ticket](http://ghc.haskell.org/trac/ghc/ticket/14928) could be relevant.
Evaluation / second in absence of optimization seems like a good metric to me. In case of assembly evaluation = machine instruction. In case of lambda calculus evaluation = beta reduction. In case of Haskell evaluation = pattern matching... 
The Floyd Warshall algorithm is a perfect candidate for memoization. Remember the weight/shortest path matrix, and fold it up the list of nodes. A strict left fold results in the tabulated approach that uses less memory. A right fold results in the lazy approach that doesn't compute unneeded values. The strict fold is almost always better for Floyd Warshall because it only computes 3 times as many values as evaluating a single value from the lazy version, but uses 1/nth as much space (where n is the number of nodes).
Very interesting! Thanks!
fantastic name
Glad you like it :D
Regarding that second problem: surely "this code is proven correct for the given specification, though that specification may not be 100% correct" is better than "this code may not be 100% correct"? I don't think formal proof is the silver bullet which will eliminate all errors, but I do think it is the best solution anyone has ever come up with. 
Wow‚Äîthat's the coolest thing I've seen in a while. Thanks! One approach I was trying was to get a small typelevel language going with SKI combinators, but I couldn't figure it out. Doing straight-up lambda calculus is pretty amazing. Is there any more long-form stuff on this out there? I mean using the singletons library to write typelevel languages and so on.
/u/Phaedo thank you. That is certainly helpful. I haven't ran into a library in which the only way to create a value of the type is through **pure / return**, but this will save me a lot of time when I finally do run into that situation. 
Yeah Formal verification is a tool to convert bugs in implementation to bugs in specifications and that is really a progress.
I also get a 404 on my phone. I‚Äôll report desktop results when I get a chance.
Unfortunately I don't know of any related material, in fact I don't know any effort besides my previous snippet of using deeply embedded type-level languages. But anyway, my lambda calculus doesn't really rely on `singletons` in a critical manner. We could redefine the required parts from singleton in a dozen lines (`Apply`, `~&gt;`, `TyConN`). The basic idea is to just have a well-typed interpreter, which can be written on the term level with minor modifications. This "well-typed STLC interpreter" is a folklore benchmark in dependently typed programming, for example see [Augustsson](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.2895&amp;rep=rep1&amp;type=pdf), the [Idris docs](http://docs.idris-lang.org/en/latest/tutorial/interp.html) or [Gerg≈ë √ârdi](https://gergo.erdi.hu/blog/2013-05-01-simply_typed_lambda_calculus_in_agda,_without_shortcuts/).
I think I know what you mean about it being difficult to read. It has a very academic feel to it. Sometimes I feel like I have to re-read a paragraph to have a proper understanding. I think the biggest selling point for me it that each sub-section is so concise but still teaches a topic so clearly, which makes the learning easy to digest. This also makes the book a great reference to come back to whenever I would want a refresher on a specific topic.
The way I'm getting most success coding DP algos in Haskell is by declaring an array using the initialization function to set the values according to the DP.
Nice. I didn't quite get it at first, but the comment about transforming kleisli to cokleisli and vice-versa cleared it all up to me. I didn't realize it at first, cause of the lack of parens, but it's obvious now that you said it.
That certainly works, but as I'll show in the next post, this solution runs in constant memory, even for very very large inputs. Try running a 1MB by 1MB problem through any 2d array based solution, and you'll soon find yourself out of memory.
It seems that there are two versions of libiconv and it's hard to tell stack the right one to pick: https://github.com/conda-forge/staged-recipes/pull/4605#issuecomment-375423815
Do you mean the inputs should be flipped? You're probably right! :\)
The thunk structure built up here doesn't specify whether the solution is computed bottom up or top down. Due to the way Haskell evaluates terms though, the solution presented here will end up computing top\-down, but in the next post, I'll show how to control this and force a bottom up evaluation, which will be faster, due to more garbage being generated.
let x = a in b = (\ x -&gt; b) a
What do you mean by "a new way to do the reverse?" 
Monads from Comonads. `ComoT` does that in a different way than `Co`.
If I recall, there were similar issues with os x's screwy iconv that arose with macports in the past: https://stackoverflow.com/questions/43359289/architecture-x86-64-while-running-haskell-code-haskell-osx-iconv http://blog.omega-prime.co.uk/2011/01/28/solving-ghc-iconv-problems-on-os-x-10-6/ http://amixtureofmusings.com/2016/01/23/getting-started-with-haskell/
Incorrect, the whole purpose of `let` is to allow for self-reference. As the commentor explained, they meant a "letrec" which is not available in lambda calculus: &gt; The trick that made it not plain lambda calculus was in its use of recursive variable definition, or in other words, letrec. Which is another way of saying "what if we consider a language which isn't lambda calculus..."
In both of your examples you explicitly remember things in lists. Without remembering intermediate shared results somewhere recursion alone doesn't produce high quality solutions. The ability to define data (or more generally memory) recursively in terms of itself is what gives the Haskell term evaluation strategy the ability to solve dynamic programming problems without resorting to finding iterative algorithms. The evaluation strategy discovers an order to compute the (partial) results without the programmer needing to invent it.
Simple haskeller here, what does it mean for real monads ? What is an IO comonad, or a list comonad ?
Not too much, since in order to construct a `Mo` out of IO or list, you need to provide a way to construct an `a` given `w (m r)` (you're allowed to know what `r` is when constructing a `Mo`. You typically have it be `a`.) Since straight-up `extract` is impossible to do for IO, the only thing you can do is to ignore the IO part and just construct a value out of the comonad part... OR you can return an IO action. In these cases, you're more or less "augmenting" IO with a comonad. See the bottom of the link to the `Mo` code for an example of this. You can do more with list, since you can study what elements are within it and return a value based upon that (having a special case when the list is empty). An example could be, say, folding a list of monoids using `fold`.
I feel like a lot of the theory needs motivation and context to be approachable. Learning haskell requires multiple attempts with breaks to let things sink in because it is so different from imperative programming. I can't see a bunch of theory on top helping this. 
How this compares to http://hackage.haskell.org/package/fin-0.0.1/docs/Data-Type-Nat.html#v:induction and it's usage in `vec`, e.g. definition of `Applicative (Vec n)` is instance N.SNatI n =&gt; Applicative (Vec n) where pure x = N.induction1 VNil (x :::) (&lt;*&gt;) = zipWith ($) And `zipWith` can also be written using induction (see http://hackage.haskell.org/package/vec-0.1/docs/src/Data-Vec-Lazy-Inline.html#zipWith) 
In a project the only visible packages are the dependencies of your project. In your `*.cabal` file, add `scientific` to the `build-depends` field.
The lists are there to structure the thunks correctly, not to achieve memoization. Memoization of thunks takes place whether or not a thunk is used again. If you want to exploit this to gain caching of needed values, then you need to make sure your thunks point to shared thunks. What was made explicit via the lists is the *sharing* of thunks, not the memoization of data. This is a subtle point, but it is indeed the case. The use of lists isn't even necessary in many cases. The fibonacci example could trivially be done without any lists. In a future post, I may show that it's not even necessary for LCS. Sharing and thunk updates take place, regardless of how the thunks were linked.
People keep talking about specification bugs, but I really feel that this is one of the less severe problems with formal proofs. (The practical issues you mention, on the other hand, are currently a deal breaker, and I very much agree with your conclusion.) Reasons: - For a spec bug to make it into the finished software, it needs to go undiscovered while proving that the software matches the spec. So software and spec need to be wrong in the same way, which is unlikely. Usually, you get either an unprovable goal or a proof that's obviously too easy. - The spec tends to be considerably simpler than the software. This is no coincidence: The essence of abstraction is presenting an interface that is simpler than the implementation, and creating abstractions is the primary method for managing complexity. - Extending the previous point: End-to-end verification is arguably not the relevant scenario right now, and probably won't be for a while. Verification of key components, however, is becoming more practical, and those tend to have nice simple interfaces. - Spec bugs exist just as much for tests: If you can't articulate your spec in a formal language without making (easily discovered) errors, this is probably because your mental model of the software is wrong in some way. It is then likely that your tests will be equally wrong.
Have you reported the issue to the nix issue tracker?
&gt; There's no real difference between top-down and bottom-up DP, Thats absolutely correct. &gt; except that imperative languages force you to make a choice to express the solution to the problem. More than imperative languages I feel it is the introductory textbooks that first introduce and teach dynamic programming(CLRS or Skienna or Sedgwick) express them in a very imperative way. As a result I have personally faced a lot of issues translating my knowledge of DP to Haskell. I finally resorted to the approach stated in this: http://www.seas.upenn.edu/~cis552/current/lectures/soln/STMonad.html I will look forward to your next post :) 
As far as I can tell, neither of these packages add any more specific information to the type of something to indicate which exceptions should be handled.
Thanks. 
This resolves the issue. That is a very strange workaround.
My ideal currently is a bunch of curl bash scripts, but with some parser that can convert a given script to postman format for interactive inspection of failing tests. The list of formats that runscope accepts as input is also an interesting survey of what is out there.
/r/WatchPeopleCode 
They‚Äôre effectively doing the same thing, the invective type family just lets you use the induction function for both replicate and zipWith without newtype wrappers.
If you look at the [source](https://github.com/NixOS/nixpkgs/blob/2b499afa63f01473a19c7166c1f3750fa45a1bab/pkgs/development/haskell-modules/generic-builder.nix) for the Haskell package builder in Nixpkgs, this is what it does internally, which might be why it works. I've had enough weird issues with using `cabal` in nix-shells that I'm now superstitious about using `runhaskell Setup.hs` instead of `cabal`.
Okay, that makes more sense. And thanks again for taking on a game and releasing the source too. It's helpful to look at others' approaches.
Memoization is sharing with respect to some structure, specifically when the structure comes from the structure of some function. &gt; The lists are there to structure the thunks correctly, not to achieve memoization. A data structure is something that structures values so they can be reused. The entire purpose of a data structure is reuse, and sharing is a form of reuse. If you aren't going to use a value again later (based on how its situated in some structure) there's no point in putting it in a data structure in the first place. &gt; The fibonacci example could trivially be done without any lists. I'm intrigued how you plan on doing this recursively with no lists, tuples, or other data structures. If you plan on using something like a specialized version of the following, that's a data structure as far as I'm concerned {-# LANGUAGE RankNTypes #-} newtype Product a b = Product {flipped_uncurry :: forall c. (a -&gt; b -&gt; c) -&gt; c} -- (,) tuple x y = Product (\f -&gt; f x y) uncurry :: (a -&gt; b -&gt; c) -&gt; (Product a b -&gt; c) uncurry = flip flipped_uncurry --- &gt; 'Memoization' of thunks takes place whether or not a thunk is used again. I don't believe this is true. An implementation of Haskell is perfectly welcome to attempt to remove the thunks, for example GHC's strictness analyzer.
Interesting. I guess this is like saying that `Monoid` is more than `Semigroup` and `Default`?
I understand and agree with what you're saying, I just don't understand what you're arguing against.
Hear hear. [Microreview on the other subreddit](https://www.reddit.com/r/haskellgamedev/comments/8h0v8j/i_built_a_shmup_game_in_haskell/dyjcx2z/)
I'd not be so sure. All immutable data-structures and their algorithms are pretty much just ADTs and pattern-matching. Arithmetic and arrays are the exception, but they're the C in Haskell.
Hm. I'd need to see some data to be convinced either way, I think. Like if we could somehow measure how much of e.g. Pandoc's time is spent just on matches, that would help a lot. At the very least, *More Research Required*‚Ñ¢
Thanks
&gt; So I guess this is similar to saying that Monoid is more than Default and Semigroup? Ooh, nice analogy.
You're probably thinking of this, but it's explicitly about a lazy language and OP's language is strict. http://h2.jaguarpaw.co.uk/posts/impure-lazy-language/
Syrak is right that you need to add it as a dependency for your project. If you're using stack, though, you'll probably want to add it to your package.yaml file, rather than your .cabal file. dependencies: - base &gt;= 4.7 &amp;&amp; &lt; 5 - scientific
Nice to see more applications of state machine specification! I'd be curious to see how many, and how easily, the bugs in the [not-so-smart-contracts](https://github.com/trailofbits/not-so-smart-contracts) can be caught using this approach... 
Hmm, that ticket refers to several anonymous unlinked "Notes" which seem to be where things are explained. Are those the new set of typing rules that are needed here? Where can they be found?
It does, if they've also solved the nothing at stake problem 
The problem is that the old one you don't want is the one that comes built in to MacOS. It cannot be uninstalled.
I usually make my `Setup.hs` executable (`chmod +x`) and put this code inside them: #! /usr/bin/env nix-shell #! nix-shell ./shell.nix -i runghc import Distribution.Simple main = defaultMain That way, assuming I have a `shell.nix` in the same directory with the expected behavior (putting you in an environment were all of your project dependencies are available), then I can do stuff like `./Setup.hs configure` or `./Setup.hs build` and it just works, both from within and *from outside* the `nix-shell` (that is, you don't have to run `nix-shell` manually).
I really like this, takes the pain out of hand\-writing integration tests in things like hspec \(which is decent, but can be slow/cumbersome\). What are your plans for the DSL?
While this is generally a nice article, there are two aspects of its whole approach that I don't prefer. 1. I don't like to start by saying "here's how we would do it in C (or Java or whatever), now let's see if we can do that in Haskell." The right approach is to think in Haskell to begin with. It took a while for me to train myself to do that, but now I find starting with C-think makes the article more difficult to follow and understand. 2. I don't think it's ever helpful to use nebulous general terms like "dynamic programming" and "memoization" as if they refer to special techniques. Every non-trivial algorithm stores things in memory; how you use memory is always part of algorithm design. As for this specific problem, it seems to me that Dijkstra's A* algorithm would be a good approach. Each node is the letters selected so far, plus the remaining letters on each side after removing letters that no longer appear in both lists. The score of a node is the number of selected letters plus the length of the smaller of the two strings of remaining letters. A completed CS is a node where there are no remaining letters. Use A* to find the completed CS with the highest score.
You can use `.` instead of `&lt;$&gt;` for functions. The code is really clever by the way.
Thanks for writing that up. I would need to spend quite a lot of time studying this to fully understand it though. Whereas OP's approach is simple, clear, and very readable. I am looking forward to many things becoming simpler as DT features gradually go live in GHC.
so awesome, one of my fav movies, my wife rolls her eyes when i sing the theme song and so many people don't know what i'm referring to 
typo in law `A6: arr fst . first p = p`; the two sides have different types. Should be `A6: arr fst . first p = p . arr fst`.
Hey there! This is well beyond my level of understanding, but I think I found a little mistake in the type signature of digraph: dimap :: (a' -&gt; a) -&gt; (b -&gt; b') -&gt; p a b -&gt; p a' b' The first function parameter; shouldn't it be `(a -&gt; a')`? Otherwise, I don't see how it is possible for dimap to transform an `a` value into an `a'` value, since the function only does the opposite transformation.
That's what is called a contravariant type parameter. It transforms in the opposite direction to the function that is `contramap`ped over it!
&gt; Twist is a lawful instance of Strong _Should_ it though? An implementation of `first'` which messes with the second component of the pair doesn't fit with my mental model of what that method is supposed to do. Perhaps Strong needs some stronger laws in order to disallow this kind of implementation?
Good thinking. How about `first' id = id`?
Well, damn: still got a lot to learn. Thanks for answering and sorry for wasting your time.
Once again something that starts with "people agree partial functions shouldn't be there". The article is interesting but I strongly disagree with that intro. 
Merf, I'm looking forward to 32bit support.
That would be a great blog post.
That's no problem at all.
&gt;software and spec need to be wrong in the same way, which is unlikely. It'd be unlikely if the two varied independently, but what I imagine is that somebody writes an incorrect spec (an invariant that's too weak, just misunderstand what's required, etc.) and then the implementation is written to match that. &gt;The spec tends to be considerably simpler than the software. I guess, but as you yourself said: the specs and the tools tend to not be too user-friendly. I'm going out on a limb here and correct me if I'm wrong, but these tend to do temporal logics, Kripke structures, some custom calculus from some paper,... Even if the tools present these things in friendly ways, they are non-trivial to use and comprehend. &gt;End-to-end verification is arguably not the relevant scenario right now, and probably won't be for a while. Verification of key components, however, is becoming more practical, and those tend to have nice simple interfaces. No argument there. If, say, &gt;10M$ or people's lives were at stake, I'd invest in formally verifying core parts of the software too. &gt;If you can't articulate your spec in a formal language without making (easily discovered) errors, this is probably because your mental model of the software is wrong in some way. That's certainly an issue, but I'd argue that writing a test case is intellectually less taxing than writing a formal spec because the latter requires abstraction, whereas the former just requires an example. You can probably easily devise an example where a concrete X is supposed to happen as a result of a concrete Y (a test), but it's a lot harder to determine exactly what preconditions &amp; postconditions you have in what cases, and what invariants are supposed to hold. It's certainly *useful* to sit down and think about that, I'm just saying that it takes quite some work.
The best example to have in hand for this case are functions. Replace the profunctor for `-&gt;` and it becomes clear why the parameters are written as such. If we followed your initial suggestion, we would have a `Bifunctor` instead. You can find it in `Data.Bifunctor`, in `base`.
Thanks, found them. But they are pretty opaque internal implementation details. I hope after /u/goldfiere is done getting the first release of DT out, he'll gradually start writing more papers about GHC's new type system. 
`Strength` can apply for an arbitrary Profunctor, not just the ones with a notion of `id`.
Unfortunately, `Strong` doesn't provide `id`! Are we allowed to say "if the type also has a Category instance, make sure `first' id = id`", or can laws only talk about methods which the typeclass knows about?
May I know why? As I understand, the case against partial functions is that one actually never wants one's program to crash, so the crashing functionality they provide is not really desirable.
I am still trying to wrap my head around free monads. Nearly every article/SO question on it is packed with some category theory that I don't really understand. I've tried reading "[Why free monads matter](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html)" but I couldn't get very far without getting lost in the recursion \(there are times where I understand `Fix` and there are times where I don't...\)
Nice, totally agree with you about the hspec thing. Auto generation of these specs is a great idea! My team and DotDashPay and I will eventually build that from our protobuf defined services. I hadn't considered something like swagger but that would be a prefect fit! I will look into that :) For the DSL, my plan is to start to work on that when: - there is sufficient interest in the project - Dhall specs are implemented, but there are specifics needs it doesn't quite fill (Dhall is a fairly new idea for this that I hadn't considered when originally wanting a dsl) - the spec becomes a little more complex, to make a DSL enough of a win over yaml. For instance, the interpolation syntaxes may gain more features with time. I'm open to any and all suggestions around it though! 
I tried to couch it in language that made it a little less strong than that ("people seem to agree shouldn‚Äôt be there"). That said, I'm not sure I've ever seen someone argue in favour of `head` being: head :: [a] -&gt; a instead of: head :: [a] -&gt; Maybe a Do you have an example of where the former would be better?
There is something cabal has that stack doesn't - Backpack. Though support is being added to stack, it'll be a while.
Can someone break down the appeal of compiling Haskell to javascript? Pros? Cons?
Yes, that's not what I meant at all! I meant dimap (\x -&gt; (x, ())) fst . first' == id
Sadly Arrow isn't the pushout of Category and Strong, so you need an extra class for it. It can be called Arrow, and just contain that law. ;)
There‚Äôs already some really cool libraries that use it vs huge page size (you can reduce the second with the google closure compiler).
Usually I use `($y) &lt;$&gt; x` as it can work on any `Functor`, not just `Applicative`, but there isn't a name for the composition of those two things in base.
I assume there are equivalent js libraries, since it's js that actually executes in the js vm?
You can use almost any library in Hackage, excluding those that link to C libraries. Also, you can very easily share types between frontend and backend. For example, if you use Servant, you have to think very little about the server-client communication.
That's neat!
I imagine it's because the types work out and the counter-examples are fairly obscure.
This is reason enough. I'm just not a big fan of transpilers in general. I.e. writing one thing and executing another. 
Thanks, good catch.
This is what you're always doing with any compiled language unless you're writing assembler.
Thanks for the awesome resources! I actually tried watching Ed Kmett's livestream the other day where he talked a little about free monads, but couldn't keep up with the examples associating free monads and recursive tree\-like structures. Hopefully the links you've provided help clear that up! Thanks again.
Yes, indeed. But the more layers you add the more weird stuff and limitations you run into. There are performance hits that just isn't there when writing native (you know what I mean) Haskell. And you need to deal with the shortcomings in the js vm anyway since the transpiler works on top of it. Those things aren't trivial or obvious. But I guess more Haskell is good. Perhaps webassembly can take this even further by skipping the js part. 
I'm not sure I fully understand the difference. I wrote [a post about dynamic programming](jelv.is/blog/Lazy-Dynamic-Programming) a few years back. Is the string edit distance algorithm using an array an example of what you would call "bottom-up" dynamic programming?
Same problem though, `Strong` does not provide `(.)` nor `id`
Cons: - Lots of work: GHC is a huge compiler with a lot of surface area, getting everything to work nicely in javascript land is a heroic effort (almost entirely by luite), and leads some immaturity across the board. The hope is that WebAssembly will take out the peculiarities of javascript and making targeting browsers from GHC much easier. - Bloated applications: since haskell is a general language not designed around javascript, we can't always produce clean, compact javascript the way others (ex: purescript) can. This is improving, but it is Lots Of Work - Performance: The haskell evaluation model is very different from javascript, so a complex application written in the straightforward way may not match performance of pure javascript. This is improving (Ex: special Text implementation that uses native JS strings), but it's Lots Of Work - Template Haskell sucks: Because of the way template haskell works, it must interpret (via GHC interpreted in Javascript!) template-haskell splices at compile time, which is horribly slow compared with native GHC, to the point of making common TH niceties like deriving lenses practically unusable for large applications. This is not a fundamental limitation, but it's Lots Of Work (and really should be fixed in GHC proper, not GHCJS) Pros: - You get to write haskell - You get to share frontend and backend code (ex: the object model/wire format), "Isomorphic" serverside/clientside hybrid rendering - Easy retargeting: being one of many compiler targets makes it practical to deploy the same code on PC, Browser, and native iOS/Android with minimal changes - Compared with other haskell-like-compile-to-JS languages like Purescript, GHCJS implements the extremely powerful GHC runtime system faithfully, which gives access to all the goodies like Threading/MVars, STM, Weak References etc. In particular, reflex/reflex-dom depend deeply on the semantics of Weak References in GHC, which don't exist in any alternatives like Purescript, Haste, Fay, etc.
I only came to know about that when I was knee deep in implementing a backend project with Spock. Too bad, I guess. I‚Äôm one of those people who want to avoid writing things that run on browsers at any cost 
Very nice list! I assume that threading in the haskell code will run single threaded in the browser since js is single threaded, right?
Oh really? That is... a terrible design. How do they manage that? I would think you could just go to the location of the binaries and manually delete them.
But why would you need to use "almost any library" in clients browser? html frontends have a very specific and narrow use-case: fetch info from the server, show it on the screen, collect input and send it to the server. Practically all libraries I use on the client deal with either with UI (datepickers, dropdowns, autocomplete, dialog boxes, tooltips etc) or with server interaction (ajax, json, upload files). And there's almost nothing on hackage that covers these needs.
By far the biggest reason is that there's significant value to be had in using the same language on the frontend and backend. I might even say that to some extent the existence of Node.js is proof of this. For those of us that already have Haskell running on the backend, this is a logical place to go (just like using JS on the backend was a logical place to go for people already using it on the frontend). Here are a few highlights: * This is good because of all the reasons that most people here think Haskell is good * Better developer mobility between the backend and frontend teams (they're both just Haskell) * Legit FRP systems like Reflex that benefit from purity
Thank you! I've deployed my first web app after your tutorial. http://povovota.herokuapp.com/ 
I prefer dynamorphisms for dynamic programming problems. They are usually easier to write and yield the same solution.
Hi Richard, thanks for considering this stuff (and implementing coerce in the first place!). &gt; I don't think Coercible (Hashable Int) (Hashable N) is exactly the right constraint: that constraint says that the runtime representation of Hashable Int is the same as Hashable N Hmm, doesn't Coercible say a little bit more since the newtype constructor must be in scope in order to coerce through it? Considering instances to be the same seems pretty similar. It does seem clearer to have a special constraint for this purpose rather than overloading Coercible, though. &gt; Even supposing we could write such a thing, when would the constraint hold? Only when one instance is a GND of the other? Or could a hand-written instance be recognized as "the same" as another? Yeah, these are good questions, it is certainly not clear what the best balance of power and safety is here. The proposal I have here would be to only consider GND instances to be "the same". That seems like a safe answer, but could certainly be restricting. There are cases where GND doesn't work out, so I think that would be the main motivating case for declaring it for manual instances.
I really do like the idea of being able to express something like &gt; instance ( Coercible (Eq k0, Hashable k0) (Eq k1, Hashable k1) &gt; , Coercible a0 a1 &gt; ) =&gt; &gt; Coercible (HashMap k0) (HashMap k1) I'd be willing to write something like that down in code myself, in that it captures precisely _how_ nominal the nominal role of the first argument is, which is an idea I hadn't considered before.
I think most of the effort should be put into WASM instead; that's less of a shitshow of a language.
Cool! I'm happy it was helpful for you.
Yea it uses cooperative multitasking. So you've got logical threads and their execution is interweaved.
Getting [very close](https://hydra.webghc.org/build/517) :)
IME, the performance issue is exaggerated - I can parse large queries with https://github.com/uber/queryparser on my (admittedly modern) phone without noticeable lag. That said, I've certainly not benchmarked or even pushed things *all* that hard... but it seems plenty usable for plenty of things.
There's this package called lens you might have heard of :-) It defines: (??) :: Functor f =&gt; f (a -&gt; b) -&gt; a -&gt; f b 
Your post on dynamic programming was one of the first ones in Haskell, that I read. I would classify your approach as a "top down" memoized approach. I thought the "bottom up" and "top down" nomenclature are standardized nomenclature. Sorry for not citing. This is from CLRS Pg. 365: &gt;There are usually two equivalent ways to implement a dynamic-programming approach. We shall illustrate both of them with our rod-cutting example. The first approach is top-down with "memoization". In this approach, we write the procedure recursively in a natural manner, but modified to save the result of each subproblem (usually in an array or hash table). The procedure now first checks to see whether it has previously solved this subproblem. If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner. We say that the recursive procedure has been memoized; it ‚Äúremembers‚Äù what results it has computed previously. The second approach is the bottom-up method. This approach typically depends on some natural notion of the ‚Äúsize‚Äù of a subproblem, such that solving any par-ticular subproblem depends only on solving ‚Äúsmaller‚Äù subproblems. We sort the subproblems by size and solve them in size order, smallest first. When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions. We solve each sub- problem only once, and when we first see it, we have already solved all of its prerequisite subproblems. These two approaches yield algorithms with the same asymptotic running time, except in unusual circumstances where the top-down approach does not actually recurse to examine all possible subproblems. The bottom-up approach often has much better constant factors, since it has less overhead for procedure calls. Followed by that there is an example in the book of the rod cutting problem solved in both ways.
Cooperative multitasking. Now that's a concept I haven't heard in a long time.
I might be using the term wrong. Isn't this how all green thread implementations share a small number of OS threads among a large number of green threads? GHCJS is just doing that but with one "OS thread."
Interesting idea with having 3 arguments. One thing this helps with is that now it wouldn't need to match up the constraints from both sides. For example, would `Coercible (Eq a0, Hashable a0) (Hashable a1, Eq a1)` be inhabitable (reordering of constraints)? This would also save boilerplate because you wouldn't list the constraints twice. However, this could also make it difficult or impossible to use this with multi parameter typeclasses or flexible instances. For example: class C a b instance C Int Int deriving instance C N N I'm not sure how to cleanly express `(Coercible (C Int Int) (C N N))` with this approach. Seems like this 3 parameter approach would work well with regular GND but can't handle all uses of standalone deriving GND.
Glad you found this to be an interesting idea! That's why I'm posting it, seems like something that would be quite useful. I'd libraryify it myself, but it seems that right now it would require the user to declare in some other way that they used GND for some particular instances. Unfortunately the information does not seem to be accessible to TH. In the code I'm currently working on I wrote a definition using `unsafeCoerce` and added a TH check that the types in question are at least newtypes. I wanted it to also check the GND use, but it seems to not be possible via the TH API.
I don't know. I've used it in an embedded OS and when we had heavy loops we had to put in a system call to let the process scheduler go in and do its thing and let other processes do work. It was kind of cool. I can see the same thing work in js.
It's an infix name for `flip` &gt;&gt; (take ?? "hi!") 2 "hi"
The point is that recursive Haskell functions do not automatically leverage memoization. The programmer must intentionally use a memoization technique in order to reap its benefits. Writing a "naive" recursive function will still have poor performance.
&gt; Another chance to waste gigawatts of power to accomplish nothing. You can disagree with people who think decentralized, digital money is worthwhile, but it's certainly not nothing. People also spend energy on decorative Christmas lights, which I would argue provide less value than decentralized, digital money. It may be, however, that the cost of electricity is priced incorrectly, by not taking into account external costs. But that's a separate issue.
Hence "in `base`." =)
Yay you learned something new today!
[hoogle](https://www.haskell.org/hoogle/?hoogle=Applicative%20f%20%3D%3E%20f%20%28a%20-%3E%20b%29%20-%3E%20a%20-%3E%20f%20b) and [hayoo](https://hayoo.fh-wedel.de/?query=Applicative+f+%3D%3E+f+%28a+-%3E+b%29+-%3E+a+-%3E+f+b) are my preferred way to check for things like this.
that makes `flap` make more sense.
`Arrow` itself also has `arr`.
I'd say laziness makes the divide a bit fuzzy. fib n = fibs !! n where fibs = 1 : 1 : zipWith (+) fibs (tail fibs) `fibs` makes it look bottom-up, but since it's just a sequence of thunks, it's really top-down. We can make it bottom-up with a small change: fib n = fibs !!! n where fibs = 1 : 1 : zipWith (+) fibs (tail fibs) (!!!) (a : as) 0 = a (!!!) (a : as) n = a `seq` (as !!! (n-1))
This looks very interesting. I am currently using plain records to setup the JSON fields and converting them to a JSON string in wai/hspec before calling the endpoint. One nice thing about Records based approach is that I can change the values selectively and test many aspects of the endpoint. For example: if I want to test different age values, I can keep everything else the same but just change `age` and convert it to a JSON string before calling the endpoint again. Any thoughts on how that might work with your tool? Yours definitely looks much cleaner / readable but would love to avoid a lot of duplication if possible. Thanks! 
Ah, you bring up a great point! This is where I think yaml really shines, because this sort of thing can be encoded within yaml completely separate of curl\-runnings. [https://blog.daemonl.com/2016/02/yaml.html](https://blog.daemonl.com/2016/02/yaml.html) Has a good example of what I mean. 
 arr f = dimap id f id Just needs the Arrow laws
But for a functional language? ULC seems to map to JS very well.
Not familiar with ULC. But whether the language is functional or not isn't really a determining factor in whether JS is a good fit. OOP, procedural, and functional all map just as well to JS. It's a bad fit if you need: - Tail calls (this is not just a functional thing). GHCJS uses a trampoline, and it's really bad for performance. - Fast and space efficient memory. GHC expects to be able to use CPU registers, but in JS it only gets global vars. Extremely slow. Also lots of things have to be stored in what you would hope were C style structs, but JS frequently fails to optimize them (due to indirection in transpilation output) and you end up indexing hash maps all day. - Synchronous and parallel execution. Without `SharedArrayBuffer`, this stuff becomes much harder to do right. And the list goes on. JS is very well optimized for JS, not for other languages. The language you're transpiling essentially has to have a lot in common with JS for it to be a good target.
Is there anywhete to follow update on the progress?
Just because you can *write* JS pretty naturally in a functional style doesn't mean that it's VM is any good at making that code performant :) (I'm sure it's miles better than it used to be but you have to do some fairly crazy stuff to get js super performant and writing in a very imperative style seems to be the first step from what I've seen online)
I suppose just watching the [hydra](https://hydra.webghc.org) and [github](https://github.com/WebGHC/wasm-cross).
Thanks for spelling it out like this! It reminded me of a toy package I uploaded to Hackage once upon a time called [`runmemo`](http://hackage.haskell.org/package/runmemo), which encourages exactly this pattern. I've put together a simple example based on the blog post above and added it to the runmemo repo. https://github.com/DanBurton/runmemo/blob/master/examples/lcs.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [DanBurton/runmemo/.../**lcs.hs** (master ‚Üí cccc5a3)](https://github.com/DanBurton/runmemo/blob/cccc5a378264b3fe002d73089ad7c690f39cb86f/examples/lcs.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dylu5be.)
I think the term "transpiler" is making people's eyes twitch a little because "compiler" is the right term. "Transpiler" carries connotation of simplicity and there being a very small difference between the input and ooutput languages. Whereas GHCJS is a big project that outputs JavaScript of low level instructions that looks nothing like the original Haskell.
For beginner, the first version is much better because it doesn't involve a second types. If you are learning Haskell, you learn about list and the first thing you want to do is to take the first element. You don't know maybe yet and don't care yet about safety. Ok, maybe the Prelude should not be made for beginner, but it doesn't hurt neither. Partial functions are bad so I don't use them but I'm not bothered by the fact they barely in the Prelude. And to be honest if I got a Maybe ikk probably pattern matching on it, so I can just pattern match on the list.
I would love to try this, are there instructions for using ghcjs with a project using stack?
I'm not sure about it. I feel that both proofs and programs are both two sides of the same coin: they are formal specifications of ideas that come from fertile human minds. What's the problem with formal specifications in general in software? 1) We write them too quickly because we are impatient. 2) We make too many parts and that makes it unmanageable when specs change. This applies to both programs and proofs. This is why people outside academia and communities like Haskell's aren't excited about proof languages like Agda, Coq, Isabelle, etc. It's because their proofs are big and complex, often longer than the program itself. The tooling actually generates boilerplate code happily and regularly. Conor McBride did a talk on writing out types such that the implementation of non trivial algorithms could be generated all based on the type signature by picking the right instances. While impressive, I couldn't help feeling that this was just another programming language, but with a funny syntax and restrictions. To express more complicated real world problems it would need to be as expressive as a full programming language. And then you're back to square one. People like type systems because they're a formal spec that is almost always shorter than the program they describe. They're very manageable and less easy to get wrong. That feels like the only progress that's really been made. Bang for buck. I'm personally more excited about things like Liquid Haskell than `DependentTypes` for this reason.
My argument is also simple: the fact that Y-combinator in INet has a succinct form with no redexes is not surprising, because it makes the same implicit assumption of recursive definitions, just like `letrec`.
Nice!!! I did not know about these hidden things in Yaml. Thanks for pointing this out. Will give the tool a shot. 
[removed]
Regarding the danger of someone writing an incorrect spec: That is certainly a possibility, and I don't deny that such spec bugs can happen (nor that they can be devastating when they happen). The question is: How often do they happen and remain undiscovered? For the reasons I stated, I think that the answer is "not so often as to warrant great concern". However, I think our disagreement here may be the result of experience with different kinds of specs -- you're probably thinking of big stateful systems while I'm more familiar with nice functional batch jobs, like compilers. Specs for those are kind of reasonable already. Regarding tests as a substitute: It is certainly easier to write a unit test than a spec, for the reason you mention. But for the same reason, it is easy to write a bunch of unit tests that all miss some edge case which a spec would force you to think about. I think it's fair to count these situations as 'spec bugs' as well: the 'spec' being "whatever different people assume would be the right behaviour", which is probably inconsistent. Property-based testing addresses this issue, but also forces you to think about the spec more explicitly to identify the right properties; in this sense, it sits between testing and formal proof.
&gt; It took a while for me to train myself to do that, but now I find starting with C-think makes the article more difficult to follow and understand. This article was written for those just coming to Haskell from an imperative background. The article was written in response to a question I had at BayHac that explicitly asked me how to achieve the equivalent of tabling in dynamic programming. I felt I needed to present the imperative algorithm as introductory material to respond to this question. &gt; I don't think it's ever helpful to use nebulous general terms like "dynamic programming" and "memoization" as if they refer to special techniques. Both these terms have pretty specific meanings? Can you elaborate further? &gt; how you use memory is always part of algorithm design. No argument there, hence the pointing out that recursing on a function has different properties than recursing on a sequence of invocations of that function, even if its still recursion. 
I mean quite often no, I'm guessing there isn't a js equivalent to lens or mtl or reflex.
I wish SPAs weren't used so much. They're almost never justified by the app/product that's being made.
Yeah, that seems like a reasonable way of putting it. It‚Äôs a classic tradeoff between *proving* and *demonstrating* correctness (see also: universal vs. existential properties, types vs. tests). Functional programming in the land of APL and Lisp is biased toward the latter: ‚ÄúI know this code is correct because *I wrote it* analytically (and interactively)‚Äù as opposed to ‚Äú‚Ä¶because *it operates* algebraically‚Äù.
There is a project template for GHCjs. Be warned though that the compiler needs an awfully long time to boot. Either that, or make sure to use a precompiled one. The `stack.yaml` file should be set up that way already.
Yep. The only difference here is that the compiler is (most of the time) inserting calls to the process scheduler for you. Dunno about in GHCJS, but in GHC itself there are a few exceptions -- namely, the standard insertion point is when something gets allocated on the heap, so if you have a tight non-allocating loop you can starve other threads if you're not careful. It is pretty tough to avoid allocation in Haskell, though...
I'll fastidious bear this title of prophet. Luckily, `stack-1.7.1` and `cabal-instal-2.2` have both been released incorporating the `Cabal-2.2` library and can be used as build tools to correctly pass separate flags to separate C &amp; C++ FFI compilation targets.
&gt; The best example to have in hand for this case are functions. Replace the profunctor for -&gt; and it becomes clear why the parameters are written as such. I mean you can go further than that, when you contravariantly map over an unconstrained type variable (i.e. it's not restricted to something exhaustively enumerable like `Bool`), you are always mapping over the input of a function, or you are not mapping at all and the type parameter being changed is a phantom one.
&gt; Abstract over the recursive calls. You can do this in your head, or can do it explicitly in code by rewriting a function a -&gt; b as (a -&gt; b) -&gt; (a -&gt; b) with no recursive calls. You can also just directly recurse and handle the memoization at the definition: fib n = case n of 0 -&gt; 0 1 -&gt; 1 n -&gt; fib (n - 1) + fib (n - 2) becomes (using `Data.Function.Memoize`): fib = memoize $ \case 0 -&gt; 0 1 -&gt; 1 n -&gt; fib (n - 1) + fib (n - 2)
Can you give an easy example of a DP problem you found hard to structure in Haskell without explicit mutation?
&gt; What does it mean for replication to be explicit and implicit? OP implementation has explicit replication? Not sure I get what you are asking, but substitution (replication, or sharing) is always explicit in INet. LambdaScope made scoping explicit, which wasn't the case in other INet approaches. &gt; Is lambdascope able to do this for any term obtained from Y application (Y map, Y fold, ...)? The point is that all that is expressible as recursions in Lambda Caculus using `fix` can be expressed using recursive bindings using `letrec`. I don't see much benefit of pursuing in this direction of `normal form`, because apparently (mutural) recursive definitions do not have `normal form`. &gt; What is the trick in lambdascope which makes it possible (compared to OP version)? Does oracle play any role in it? I'm not familiar enough with OP's implementation to comment.
Functions are data, but they aren't necessarily data structures. Functions `(a -&gt; b)` are the exponential type `b^a`, or a large tuple (product) of one `b` for every value in `a`. Because exponentials are so large it's assumed that a representation as a function that does _not_ remember the results and instead recomputes results each time is more efficient. The difference between a function `Bool -&gt; a` computing an `a` for each value in `Bool` and a data structure `data Pair a = Pair a a` holding one `a` for each value in `Bool` is that the values stored in the data structure will always be reused, but the values computed by the function might be recomputed each time the function is reused. Memoization is the process of converting functions that aren't data structures into data structures that represent the function so its results can be reused. Returning to the `Bool` example, a whole memoization of functions `Bool -&gt; a` would be. data Pair a = Pair a a indexPair :: Pair a a -&gt; Bool -&gt; a indexPair (Pair x y) ix = if ix then y else x fromBool :: (Bool -&gt; a) -&gt; Pair a a fromBool f = Pair (f False) (f True) memoBool :: (Bool -&gt; a) -&gt; (Bool -&gt; a) memoBool :: indexPair . fromBool Which is equivalent to memoBool :: (Bool -&gt; a) -&gt; (Bool -&gt; a) memoBool f = let x = f False y = f True in \ix -&gt; if ix then y else x 
Fantastic work!!
üëèputüëèlensüëèinüëèbaseüëè
Aha, I'll make my version somewhere next week.
Perhaps I worded it a bit badly. The statement I was more interested in making was: Isn't this how all green thread implementations share one OS thread among many green threads? Then they simply expand that to M:N. GHCJS has M:1, which still requires the green thread to relinquish control.
`Fix Nothing` is the base case, where the recursion stops. It doesn't use the type parameter, so it doesn't contain a `Fix Test`. Your `c` value doesn't typecheck because the `Fix` data constructor expects something of type `f (Fix f)` - in this case `Test (Fix Test)`. Instead, you gave it something of type `Test (Test a)`.
Ahhh, the classic ‚ÄòMonads are like Burritos‚Äô narrative
Perhaps. I see transpiler as something that translates from one language to another. The complexity of the transpiler isn't a factor. I mean, a compiler can be simple too. It doesn't automatically become super complicated because it's a compiler. 
Is there a library kinda like `System.Process`, but with byte strings in place of ordinary strings? I want to decipher a suspicious byte sequence by running `iconv` on it. I am trying to wire handle operations from `Data.ByteString` to `System.Process.createProcess`, but it gets way verbose for a one-liner.
There is a world of difference between limiting the number of OS threads to improve performance and being absolutely restricted to just one. As I recall, deadlock gets a lot harder to deal with. And there are also likely complications in the FFI.
Don't people still play games in their browsers? 
You can go in and forcibly delete it, but that would break all software that builds on the assumption that it is present. It's not a terrible design. The idea is to guarantee that certain fundamental libraries will always be available on the platform, without the complexities of dependencies and ancillary installations. It really does simplify a lot of things. But part of the cost is that it adds a bit *more* complexity in the less common case that you are a developer who happens to need a different version of one of those libraries.
Hi, I'm trying to track down a mysterious divide by zero error. Yes, I know how stupid that sounds... I decided to keep working on a feature for my library that wasn't quite working so I cooked up a new test case, got the divide by zero error the second I ran it, and promptly put errors everywhere in my code where I was dividing by a variable suspected to be 0. None of the errors were thrown. So I decided I would ask a few silly questions before calling it a night and re-investigating tomorrow. If, inside the expression for a function, I have `let a = if y == 0 then error "line number" else x div y` then I will get an Exception telling me the line number if `a` is evaluated, correct? Second silly question: foo i x = if condition then foo (i + 1) x else (i, if y == 0 then error "line number" else x div y) `cons` foo (i + 1) x Assuming the above function was somehow useful for something and was called with y in scope and equaling zero, would the error be thrown?
Perhaps it was too strong a phrase. I guess I was coming at it from a linux mindset, where one can have complete control over what packages are installed.
Actually, you can‚Äôt, because the layout rule applies every time you try to break a too long line. In other words, writing in braces and semicolons requires you to understand the layout rules.
When I went to learn what this "contravariant functor" thingy is all about, I found a lot of abstract stuff that was great, but didn't really help me grok what exactly a contravariant functor does *in a practical sense* - why would I ever use one? If a `Functor` lets you transform the output of its computation, a `Contravariant` lets you transform the *input*, without changing the type of the output. [I wrote a very short example.](https://repl.it/repls/KnowledgeableFeminineDeveloper) We have a type `Painter`, which generalizes all functions that create a `Painting`. We also have a concrete `painter`, which is a function `Int -&gt; Painting`. (The example is simple, but pretend it's a highly complex machine learning-as-a-service algorithm on the blockchain that your company just acquired for $150 million, because blockchain + machine learning + as-a-service = catnip for the C-suite.) Now we have an *urgent business requirement* to paint strings instead of ints. Oh no! We're going to have to rewrite the highly complex algorithm to handle strings! And the lady who wrote it had 20% equity in the company we bought, so she's out sipping mojitos in Curacao and not interested in consulting work. But look! She had the foresight to make `Painter` an instance of `Contravariant`. That means that to paint strings, all we need is a function `String -&gt; Int`. Hey, we have one of those! `stringPainter = contramap length painter` That gives us back a `Painter` wrapping a function `String -&gt; Painting` - just what we need!
Not me. I like layout. I find code that uses explicit braces slightly annoying to read. I've worked with many beginners, and I've never found anyone who has had any trouble with layout after a simple explanation. I don't miss brace-matching keys. Maybe it's because in my own editing style, I hardly ever used them even before I started writing layout-based languages. Accessibility is a valid concern. It would be great to have a tool that does automatic brace insertion for that purpose. It would be useful for Python, too.
In addition to what /u/Phaedo said, it‚Äôs not *enforced*‚Äîyou can freely mix non-layout and layout code, perhaps inadvertently. And sometimes that‚Äôs handy! Like a one-line `do a; b; c` instead of `a &gt;&gt; b &gt;&gt; c`, for instance. The pitfalls aren‚Äôt as bad as, say, automatic semicolon insertion in JavaScript, but the idea behind this is a simplification/explicit mode for people who want/need braces to navigate. Also, one of the stated reasons in the Haskell Report for *allowing* explicit braces in the first place is to make it easier to generate Haskell code without having to worry about layout, which would be nicer, I think, if it were enforced. Again, niche case‚ÄîI haven‚Äôt felt the need to generate much Haskell source code except in the backends of some toy compilers, although I have used tools like [alloy](https://hackage.haskell.org/package/alloy) that generate Haskell source.
Does it, though? If you pretend the layout rule doesn't exist, then shouldn't `let x = ...` be illegal, for any value of `...`? I think you would have to write `let {x = ...}` instead, in which case I don't think the existence of the layout rule causes you any trouble - you can put newlines in there if you want.
Does that invalidate my use of the term "cooperative multitasking" to describe GHCJS's M:1 threading (cooperative among the M)?
Regardless, the fact remains that we can write pure functions in Haskell that don't know their potential for impurity when called with IO via the Monad abstraction. Let me ask you this: If I told you that code returning IO is impure, and code not returning IO is pure, then would this function be pure or impure? twice :: Monad m =&gt; m a -&gt; m (a, a) twice ma = do x &lt;- ma y &lt;- ma return (x, y) It's pure if I call it with `Maybe` for `m`, but it's impure if I call it with `IO`. The ability to express this ambiguity using an unspecialized type system is extremely valuable.
I question how helpful this would be for beginning Haskellers - they'd basically be learning a different dialect to conventional Haskell, which increases friction later when they try to contribute to projects not using the extension.
Yup, valid concern. I‚Äôm thinking it wouldn‚Äôt be primarily for beginners, that‚Äôs just one reason I got thinking about it originally. It is something you could conceivably use to introduce the language in a style more familiar to Sam Curly-Brace Programmer and then turn off, though. I‚Äôve noticed that a lot of people learning Haskell who struggle with layout (see indentation-related questions on Stack Overflow) aren‚Äôt even aware that you *can* write Haskell with explicit delimiters because almost no code uses them consistently.
You might be interested in a type-level lambda example [I did recently](https://www.reddit.com/r/haskell/comments/8h9ew3/typelevel_induction_in_haskell/dyjikgt/).
&gt; As far as I know there‚Äôs currently no editor integration that will perform brace matching on the implicit braces produced by the layout rule. Structured-Haskell-mode does this. You navigate Haskell as if it's lisp code like in paredit mode. But it's fragile because Haskell's syntax is such a mess. I agree that editing Haskell with explicit braces and parens would be far easier to create good tooling for across the board. But I think you'll pry layout rules from Haskellers' cold dead hands. My work on Hindent has shown me that plenty of Haskellers care more about controlling little details of visual look of code according to arbitrary personal rules than automation (which I find myopic and unfortunate for all of us). So either you need an editor like structured-haskell-mode, or a projectional editor which takes your source and prints it with explicit everything and then after you've made changes it prints it back to implicit layout, or else you pick another language. Otherwise you'll likely be writing code for yourself, unless you're SPJ and are given a free pass for maverick code style. Good luck on the proposal, I'd be happy to be wrong about it.
/u/AndrasKovacs just brought my attention to this thread. You may be interested in this type-level SKI article I just wrote: https://www.reddit.com/r/haskell/comments/8huj36/the_hkd_pattern_and_typelevel_ski/?ref=share&amp;ref_source=link
If you argue for a crash as soon as possible, would compile time be soon enough? On the one hand, if you use Maybe with clever enough tooling to enable you to wire monadic functions to each other (like `Control.Arrow`), you can reserve your thinking to the values they carry and have the exception part handled for you by the monad instance code. In other words, composition works the same way regardless of the kind of arrows you are wiring. It may be the case that programming with arrows and diagrams is under-represented in Haskell; perhaps, if you could draw your program instead of typing it in character by character, the pain would disappear. On the other hand, we should not throw information away. When, at some point, you get to prove that your function returns a non-empty list, you should immediately wrap it in an appropriate newtype. * If you can know the length of your sequence, you can encode it in the type of a length indexed vector. * Otherwise, you have either a finite or an endless stream, depending on whether the producer has the ability to terminate, which may be encoded in types. * If you have a finite stream, then you should perform a check every single time you want to read from it. * If the stream is endless, you are safe. * If you have a producer that is proven to never terminate on odd items, then make it give you pairs, so that it may terminate on any pair, uniformly. I have, again, an opinion that various theoretically distinct kinds of lists are under-represented in Haskell. Perhaps, if the possibility of [total programming](http://www.jucs.org/jucs_10_7/total_functional_programming/jucs_10_07_0751_0768_turner.pdf) was taken into account early on, we would have had a safer and nicer language by now. Consider that Haskell is actually two languages: a term level run time and a type level compile time. While there is a plenty of manuals on term level Haskell, the resources on type level Haskell are scarce and incomplete; in Haskell 98, the type level language could simply be hid altogether. But it is this type level language that you are supposed to employ to encode your insights about the correctness of the term level code, and the recent language extensions permit encoding many more properties than was possible at the time the `base` library was designed; no surprise that a progressive haskeller desires for the improvements in the language to reflect in the libraries. Partial functions are only handy insofar as the type safe analogue would be too elaborate and verbose, but the whole enterprise of Haskell seems to suggest that type safe code is actually more concise and elegant in the end. I am at best an intermediate level Haskell programmer, but I encourage you to challenge me with some code that you find hard to write without partial functions, and we shall see if I can refactor it to safety, and how longer it will be then. 
Easy way: do it in two passes. You‚Äôve written the first, now just do a filter. Hard way, use an accumulator of (max value found so far, count found so far). Use foldr if you know how to use that yet.
&gt;Some languages (for example Scala, I believe) have native type-level lambda abstractions. We don‚Äôt have them in Haskell. Given the result of this investigation I assume that‚Äôs purely because the inference is bad. Even if type inference were bad, having type level functions written out explicitly would be far superior to having to emulate them with type-level DSL-s and defunctionalization, as we do now, which is undeniably awful with respect to inference, compilation speed and error messages. But inference is actually quite impressive for type functions, as Agda/Coq/Lean users would attest. IMO, the Agda/Coq/Lean implementation ("contextual metavariables + pattern unification") of unification is actually all-around superior to the GHC one, and should be the preferred choice as soon as there are scoped type variables (so already in System F). 
Neat, I‚Äôll try structured-haskell-mode, maybe all it needs is more users/contributors to do what I‚Äôm looking for in navigation &amp; editing. This isn‚Äôt so much a proposal as gauging interest for one. I already have one proposal I‚Äôve been neglecting for months while I focus on other projects (and burnout recovery), haha Also, come to think of it, this could be enforced with a separate linter, not even an extension. I just like a compiler that does ALL the things! :P
it says the tortilla functor is not monoidal because you can't recover wrapped beans + wrapped rice from wrapped beans + rice, but monoidality demands the existence of a nat. trafo FA+FB -&gt; F(A+B), so from wrapped beans + wrapped rice to wrapped beans and rice, which you get but just putting everything in one wrap and throwing the other away. but it's true it's not monoidal because you can't get wrapped nothing from nothing.
This is really nice. Are we starting to converge yet on the kind of built-in machinery we will need for general type-level programming that is reasonably easy to read and understand - compared with the Haskell value-level programming we know and love - without *ad hoc* gerry-rigging? Perhaps articles like this one can be a valuable contribution toward that ultimate goal.
I know its pretty simple but I'm having some issues doing it properly, could you possibly write out the code?
I've come up with this but it still doesn't seem to be working biglist [] = [] biglist [x] = [x] biglist (x:y:xs) = [a | a &lt;- xs, a == biglist(x:y:xs)]
I'm sorry, what is the behaviour you expect from the function? Given your example (not sure if it's a typo) I'd expect 10 to be the output... If you had something like [1,4,2,4,1] instead as input you could then maybe output [4,4]
If come up with this so far but it still does not seem to be working biglist [] = [] biglist [x] = [x] biglist (x:y:xs) = [a | a &lt;- xs, a == biglist(x:y:xs)]
It's a generalization of `flip`. It generalizes one of the `(-&gt;)` to any `Functor`. I now realized we can generalize the other `(-&gt;)` to `Distributive` (from `Data.Distributive` in the [`distributive`](http://hackage.haskell.org/package/distributive) package), and use `distribute`: distribute :: (Distributive g, Functor f) =&gt; f (g a) -&gt; g (f a) 
Oh, `lens` depends on `distributive`, so why is `(??)` not generalized even further to be an alias for `distribute`?
The extra indentation requirement also annoys me occasionally, most often with `let`. But I wouldn't want to have to use curly braces and semicolons to be able to remove them. What I mean is sometimes I wish in addition to these: ‚úÖ‚ûú‚úÖ let x01 = some long expression wrapped to the next line let x02 = some long expression wrapped to the next line That we could also allow this: ‚ùå‚ûú‚úÖ let x03 = some long expression wrapped to the next line "But what about multi-binding support?" I hear you ask. the following might be ambiguous or at least not pretty, so we would still disallow: ‚ùå‚ûú‚ùå let x04 = some long expression wrapped to the next line x05 = some long expression wrapped to the next line While keeping support for both: ‚úÖ‚ûú‚úÖ let x06 = some long expression wrapped to the next line x07 = some long expression wrapped to the next line let x08 = some long expression wrapped to the next line x09 = some long expression wrapped to the next line Another, perhaps heavier (and more complete) proposal in a direction that could get rid of this annoyance for me and others is [GHC-proposals#62](https://github.com/ghc-proposals/ghc-proposals/pull/62#issuecomment-363313465). I sometimes wonder if that proposal got derailed because of some initial ambiguity, and whether re-submitting a fresh proposal based on the last two comments of mine there (Feb 6th, 2018) that I believe don't suffer from syntactic ambiguity could help. Circling back to the main topic: 1. I would think that the above two proposals would make it easier for newcomers to understand and use `let` bindings, where I believe this issue is most pronounced for them. AFAIR it was for me, so much so that it still annoys me to this day; though no longer for a lack of understanding of the layout rules. 1. As for point 2 and 3, these people can already be explicit if they wish: let { a=1 ; b=2 } And no, this doesn't relax the layout rules, but perhaps that's for the better. Imagine a visually non-impaired person finding themselves having to decipher the syntactically fully correct and machine-readable main = do{ let {a=some long(expression over here 142);b=another$long expression} ;print 1} That wouldn't be so nice now, would it? Perhaps a tool like https://github.com/lspitzner/brittany could be to the rescue here. These groups of people could write code as they wish, and the tool would auto-indent for them (e.g. on every file-save) based on the available brace-information, while retaining block-based readability for the rest of us. And this could be done without having to change GHC, already today.
Perhaps a typo in the final definition of S as a data type newtype S f g x = S ((f x) (g x))
Then, as Phaedo said, you could for example implement it as a foldr: largestIncludingReps :: (Foldable t, Ord a) =&gt; t a -&gt; [a] largestIncludingReps = foldr f [] where f x [] = [x] f x l@(h:_) = case compare x h of LT -&gt; l EQ -&gt; (x:l) GT -&gt; [x] Even if you're not familiar with it, this is how you can understand the above code. When you call this function you have a base case, when the list is empty, so the initial accumulator (i.e., the 2nd parameter to foldr) is **[]**. Otherwise, our folding function **f** will do one of two things: * Either we're on the very first element after the empty list constructor, in which case that element is by definition always the largest (cause there's nothing else). * Or we have our current element **x**, and our accumulated list **l@(h:_)** (where h is the head of the list and l is an alias for the whole list. Then we compare our current value against the head and if x &lt; h we keep l as is, if they're equal we prepend x to l, otherwise we wipe out the accumulated list and create a singleton list with our new greatest value x. Hopefully that helps.
Have you ever looked at Infer.NET? (I have no idea if its functionality is actually related to what you're doing but seems similar)
 big = last . group . sort
The accessibility concern could be addresses by an editor plugin with the following features: * Allow navigation along the syntax tree. Haskell's syntax might be complicated and nontrivial to parse, but for this usecase it would be sufficient to resolve lines, blocks, and expressions. * Allow editing of parts of the source file, maybe one block or definition at a time. * Transform code delimited by semicolons and braces into whitespaces when the file is saved. This would detach the programmer from the layout that ends up in the file, but since many projects nowadays use automatic formatting tools like `hindent` anyways, this seems not as a huge concern to me. Formatting is annoying busywork even for non-visually impaired programmers.
BTW, I don't know if this is an exercise or not, but the function you've got so far is `maximum`, which is in the standard library (maybe except for the 0 returning part?) I think what they're saying for the first part is your function returns the biggest item in the list. Now you could use a filter to find all objects equal to that biggest item. 
As an aside, we often use `x:xs` when destructuring lists. As it stands, `head` and `tail` are both functions you're shadowing with your arguments, which makes it confusing to read it and be like "huh, they're doing a max on head? How does that work?", because I thought you meant the functions. It also means that if you change the name in the arguments later, rather than getting "I don't know the variable x" you'll get some confusing error about how head is this type and it was expecting that, since it _does_ know that variable, but it's not the one you think. 
When would you use such a function??
Hey, nice explanation! Thank you very much, I can kind of see now the potential for the contravariant functor.
That's an interesting view on the matter \- To tackle my situation by expressing those properties safely through the type system. I can't think of a counter\-example at the moment. But I caution that the partiality issue can occur anywhere where pattern matching is used, with all its flavors... and because of that I suspect that there may be a part of the problem it that's ultimately undecidable. I can't imagine that all possible crash situations at runtime can already be tackled at compile time. On the other hand, if the programmer thinks that their interesting properties are guaranteed by their good type\-level programming, wouldn't a runtime\-partial function crashing the program be an even more interesting occurence in that case?
Hey, Shrinker42, just a quick heads-up: **occurence** is actually spelled **occurrence**. You can remember it by **two cs, two rs, -ence not -ance**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
No, I don't implement substitution, only evaluation into `singletons` type-level functions. In this case, it yields - operationally speaking - a big-step environment machine.
"Absal"? A google search for "absal language" came up with nothing. ("Did you mean 'abyssal language'?" "Abyssal is the language of Demons and Chaotic Evil outsiders.") 
Good bot.
The readme points the user at `app/ExperimentToRun.hs` which doesn't exist.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Abhiroop/provenance/.../**SubsetSum.hs#L13** (master ‚Üí ac898e9)](https://github.com/Abhiroop/provenance/blob/ac898e957e8d1b741b9ea817a6c30d76082e2f31/app/SubsetSum.hs#L13) ---- 
The [Hegelian taco](https://ncatlab.org/nlab/show/Hegelian+taco) is actually a thing.
https://github.com/maiavictor/abstract-algorithm
It's probably worth pointing out that `runMemo` is runMemo memoizer f = memoed where -- a memoed version of f memoed = memoizer f' -- a version of f whose recursive calls use the memoed version f' = f memoed Or equivalently runMemo memoizer f = fix (memoizer . f) And `runMemo noMemo` is `fix`. When I've made examples like this before I've immediately introduced `fix` to recover the recursive version of the algorithm.
I'll take *that* narrative over `Function Composition is like *The Human Centipede*‚Äô. 
Because as an operator, it wants to be binary: distribute :: (Distributive g, Functor f) =&gt; f (g a) -&gt; g (f a) is unary, then to make it binary you instantiate `g` to `(-&gt;) a` and get the current `(??)` type. We similarly don't generalize the type of a few other operators with degrees of freedom you can't use without explicit parens and using the operator unarily.
Can't you just wrap the entire universe in a tortilla and simplify both sides of the equation?
Good bot. I am very thankful for this service.
I dunno‚ÄîI think that array indexing is a good example of where partiality is alright just for ergonomics' sake. When iterating over complex patterns of the indices in an array it can be difficult to prove with the types that the indices are in bounds: sure, if you're just taking a slice of an array you can use a slicing combinator, and not use indices at all, but then something like iterating over the odd or even indices is more difficult to express. For lists, though, I can't really think of a case where you know your list is nonempty, but you haven't discovered that via a pattern match or the like (with the exception of maybe the `groupBy` function). 
I'd argue that, for beginners in particular, `head` can lead them into bad patterns. I've more than once seen code like this: f xs | null xs = ... | otherwise = g (head xs) (tail xs) I think it's reasonable to *want* those kind of patterns to be ungainly, so they're pushed towards pattern matching early on.
I believe there is actually a polyfill out there. But wasm is already widely supported. Every browser has had support for nearly a year now iirc.
I thought the current support was just limited to a very small MVP, where the only possible interaction with the DOM was via a javascript bridge (supporting something like just ints or floats?) (some basic info here https://hacks.mozilla.org/2017/02/where-is-webassembly-now-and-whats-next/) 
The problem statement screams "assignment".
You're right. But that's actually more than enough. We already use a JS bridge that's far more expensive in the native jsaddle backends. For example, you can cross compile Reflex apps to native iOS with normal GHC and use jsaddle-wkwebview to delegate all the JS calls to a native webview over a bridge. Turns out in practice the bridge is actually kinda cheap and using GHCJS over GHC is significantly more expensive. At least on mobile, i saw a pretty shockingly huge speed up by building apps with native Haskell and a JS bridge rather than GHCJS. I expect the corresponding wasm JS bridge to be even faster, and the generated wasm to be much much closer to native speeds than GHCJS.
The funniest thing with this code is that [13,13,13] is a reverse of the original items, but since they're all equal it's fine.
As a tip for starting out with Haskell: just avoid list comprehensions. They're mostly a waste of time. (Simply put, there's pretty much always a better way of doing it.
Cool, thanks!
ok, thanks!
I was tinkering about something similar, probably will use your API in the near future! We have the same "pain-point" at the place I work. thank you!
&gt; The Haskell eXchange is an annual conference created for and by the Skills Matter community. An opportunity for Haskellers to meet, learn and share skills, discover emerging technologies and help evolve the Haskell ecosystem. Everyone is welcome to join, whether you are an expert or a beginner, whether you are a commercial user, an academic or a hobbyist, we'd love it if you join us this year at the Haskell eXchange!
Some of these at first glance seem like glaring omissions. Are we relying on fusion and rewrites? The first that jumps out at me is `foldr1`. The default implementation wraps and unwraps `Maybe`. Another, although it is a minor point, is `null`. The default implementation is a right fold. Why not just `null = False`? Every method has a trivial obviously correct implementation in terms of the corresponding function in `Data.List`. Why not just write them all, even if some of the default implementations do work out due to GHC optimization trickery?
Thanks; that helped clear it up a bit. The confusion I had was probably because of my incorrect/incomplete knowledge of recursive types in general: Let `x` be `Fix Nothing`, where `Fix` is our usual type and `Nothing` is as defined in my example above. Starting from the top then, by passing `Nothing` to `Fix`, we have: Fix Nothing = Fix (Nothing (Fix Nothing)) -- Fix Nothing is x, so replacing... x = Fix (Nothing x) But Nothing doesn't use a type parameter and doesn't accept any arguments. From the naive definition of `fix` the function - i.e., that value which when passed to `Nothing` returns the same value - I thought this was equivalent to asking the question "what is x here?", and my answer to the question was that it can either be `Fix Nothing` itself (which I deduced to be equal to `bottom`, would that be correct?), or the compiler should error out since `Nothing` can't accept an input in the first place. I am sorry if none of this makes any sense - just trying to work it out on my own.
Thanks!
I think that case #1 could be solved with some better surfaced documentation on the subject. You definitely have a valid point here that it is confusing to beginners, but I think it's confusing because it's sort of new and other languages don't really do the 'block' indentation thing, and also, because the types of compiler error messages you get for screwing this up are extremely intimidating. Easing folks into Haskell at the possible expense of generating yet another island of non-idiomatic syntax seems potentially dicey. Points #2 and #3 I have some sympathy for, but ultimately this should be supported by an API exposed by a given version GHC, not a third party implementation with it's own idea of what Haskell syntax is. 
That's very interesting to hear. So this could be a viable approach within the next ~year. If that's the case that puts a different perspective over what typed functional language could be used for web apps over the next 5 years. (ReasonML/Ocaml was looking like the best option).
Just to provide a counterbalance to that view. I love Unicode syntax. I prefer Unicode operators to the ascii art people resort to to come up with semi-unique operators.
Please.
I have to say I'm leery of such fancy types. I like my types to be simpler than my terms, because that makes them easier to reason about. Lambda calculus at the type level means that my type language is as complex as the term language. Doesn't sound great to me.
&gt; foldl1 f ~(a :| as) = List.foldl f a as Doesn't look like churning 'Maybe's to me. The omission of `null` looks like a slight oversight. Anyways, this method probably doesn't get used because, well, it's `NonEmpty`, and one usually just pattern matches to get the first element.
Yeah, good point about the ergonomics of indexing. One case where you know a list is nonempty is when an API or the structure of the code guarantees it as a *local* property‚Äîthat is, not dependent on parameters or other nonlocal information‚Äîbut the fact isn‚Äôt reflected in the type signature, for example because that would be too complex (or dependent): -- I know this pattern match won‚Äôt fail -- because I can see the ‚Äò3‚Äô right there. [a, b, c] &lt;- replicateM 3 action -- I know ‚Äòdigits‚Äô is non-empty and contains only digits -- because that‚Äôs how it‚Äôs (dynamically) defined. digits &lt;- many1 (oneOf ['0'..'9']) pure (read digits :: Int) In ‚ÄúDependent Haskell‚Äù, these functions could return the resulting list *statically* annotated with their *dynamic* length and proofs about it, like `exists (n :: Nat). ({n &gt; 0}, Vec n xs)`. I assume there‚Äôs a way to do something like this with existentials and `KnownNat` or singletons or something currently, but that it‚Äôs more involved. 
No, not at all.
Types classes aren't just for overloading names, though. I might use null without knowing locally what type I'm using it with. 
Look into the Expression problem. It really highlights the pros and cons of the two.
`replicatM` is a good example actually. `many1` could return [`NonEmpty`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-List-NonEmpty.html).
Yeah `many1` could return `NonEmpty` but that still doesn‚Äôt guarantee the partial function `read` will succeed‚Äîin that case you need ‚Äúnonempty *and* only digits‚Äù.
Thanks for reporting back! Now I just need [`inline-c`](https://github.com/fpco/stackage/pull/3578) back in the Stackage nightly and I'll be able to start working on [`haskell-opencv`](https://github.com/LumiGuide/haskell-opencv) again. This sure has been one adventure!
These are good points! Can you create a ticket on GHC's trac or even submit a patch?
I think there's lots of potential following the OpenAPI route and generating integration tests from that. It'd greatly simplify things. Automated tests from a generated spec' on\-build would be amazing. Perhaps decorating said generated spec with templated test values could be cool too. Generally the idea of writing no code to generate tests is ideal \- driven by the compiler. I'd also love to be able to say have something like hspec\-discover where the entire test suite is detected and run as part of the build. That's always been my default go\-to even with just unit tests.
There are a few, but perhaps `process-extras` fits the bill: http://hackage.haskell.org/package/process-extras
Agree. At least we have a step-through debugger, `Debug.Trace`, and easy tests we can write for the value level. Type level programming is strictly inferior in terms of ergonomics and how bad things are when they go wrong.
I'd probably just do something quick and easy like this: maxStockProfit :: Int -&gt; [Int] -&gt; Int maxStockProfit _ [] = 0 maxStockProfit k' (x' : xs') = go x' k' xs' where go = memoize3 go' go' m k [] = 0 go' m 0 _ = 0 go' m k (x : xs) = max (x - m + go x (k - 1) xs) (go (min m x) k xs)
Thanks for the explanation! Upon further introspection everything makes sense. Recursive data types are a thing of beauty. 
Yes for the first question, no for the second, the exception will be thrown only when the second element of the pair gets evaluated. Instead of sprinkling errors everywhere, have you tried using the [`-xc` runtime option](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#rts-flag--xc), that prints traces on exceptions?
I figured I'd take pick one and give it a try, so I went with the maximizing stock profit problem: maxStockProfit :: Int -&gt; Vector Int -&gt; Int maxStockProfit ik ps = go False ik 0 where go = memoize3 go' go' False 0 _ = 0 go' False k i = case ps !? i of Nothing -&gt; 0 Just p -&gt; max (go True (k - 1) (i + 1) - p) (go False k (i + 1)) go' True k i = case ps !? i of Nothing -&gt; 0 Just p -&gt; max (go False k (i + 1) + p) (go True k (i + 1))
/u/yitz is talking about foldr1, not foldl1
yes, but of course in that context "get" means "via a recipe"
`null` optimizes *very* well for `NonEmpty`: null xs = foldr (\ _ _ -&gt; False) True xs ==&gt; null xs = cons y (List.foldr cons True ys) where cons _ _ = False y :| ys = xs ==&gt; -- GHC inlines cons null _ = False This optimization occurs in both the compiled code for `null` and in its unfolding, so there's never any extra code.
Indeed, the default definition of `foldl1` would be perfectly fine! Someone picked the wrong one to write by hand.
Oh, my bad :/
Totally fair! Honestly while I do absolutely love Haskell, I wouldn't mind better support for clean and concise mutable programming. One improvement would be unifying `ST` and `IO` via `type IO a = ST RealWorld a`. That way we can have functions like: new :: a -&gt; ST s (Ref s a) Although I think it's a little complicated because treating `IO` as `ST` + realworld stuff might not be entirely valid.
I've opened a [Trac ticket](https://ghc.haskell.org/trac/ghc/ticket/15131) to address this.
I wouldn't call the strArgument function the Builder pattern by itself because it is more similar to a constructor in OOP. However, you could very well call the chain of &lt;&gt; the builder pattern because you're effectively tweaking the fields of a large data structure one at a time.
What is the best way to get an array of all possible functions from type a to type b?
To expand on this, say you have `main` which calls `f` which calls `g` which calls `t` which divides by zero. You want to compile with: ghc main.hs -rtsopts -fforce-recomp -prof -fprof-auto This ensures you maintain your distinct call sites. Now you run with: ./main +RTS -xc And the output will be something like: *** Exception (reporting due to +RTS -xc): (THUNK_STATIC), stack trace: GHC.Real.CAF --&gt; evaluated by: Main.t, called from Main.g, called from Main.f, called from Main.main, called from Main.CAF --&gt; evaluated by: Main.main, called from Main.CAF --&gt; evaluated by: Main.main main: divide by zero
If you installed with cabal then you should just be able to use `ghci` (drop `stack`) since cabal installs to a user-global package database which ghci looks at by default.
https://bartoszmilewski.com/2017/02/09/monoids-on-steroids/ is a more accessible approach to the subject, IMO.
["Stack is provided by a team of volunteers and companies under the auspices of the Commercial Haskell group."](https://docs.haskellstack.org/en/stable/README/#why-stack) I don't think there's an official "Haskell team" as such. The various bits and pieces (GHC, core libraries, tools such as stack) are legally owned by various entities, and are licensed to the public under various open-source licenses.
That blurb at the bottom is a pretty good answer. Thanks
Thanks for the gold!
Right. Make sure to check out Blazor from Microsoft as well. Its popularity in alpha stages shows that being able to compile to was WAsm will bring a lot of new people to Haskell.
Done and done.
My mistake, I mean list of functions. 
The contrast between this answer and /u/Bliminse's answer just tickles me :)
https://www.haskell.org/hoogle/ or within your project stack hoogle -- server --local http://127.0.0.1:8080/
What are some real world uses of `Const`?
The commercial haskell group, judging by the email list, is nearly entirely inactive. Similarly, the industrial haskell group funded some specific work in the past, but nothing recent. Neither is the ‚Äúwarden‚Äù of any particular software, all of which have their own maintainer teams.
A lot of it that `NonEmpty` was written before `Foldable` picked up a lot of these methods, and then afterwards nobody went back and patched it up. A couple of them are clear oversights that date all the way back to when I amalgamated it from the 3-4 different non-empty list types the code was based on. There was no direct intention to rely unnecessarily on fusion, etc.
All the uses of it I‚Äôve used are very hard to contextualize in a Reddit comment. But sometimes an interface requires kind `* -&gt; *` but you don‚Äôt care about the type parameter in your use-case. When you need it, you really need it. And sometimes I end up writing my own definition just to get instances I need. 
For me it's usually useful for using with Applicative-polymorphic and Functor-polymorphic functions. Basically we can hitch a ride on Applicative-polymorphic function (like `sequenceA_` in the article) so we can use the functionality that they give and re-use code. There are a few of them in *Data.Traversable*, and *Control.Applicative*, and they can be all be used with `Const` to give us neat functionality. But, the *lens* library in particular is chock full of super useful Applicative- and Functor-polymorphic functions, and `Const` can be useful with any of them.
I agree to all that you have said. Some times the specs are more complicated than the implementations (cure worse than the disease syndrome). And then there is not much progress. But I will repeat here what Adam Chilipala says in his [book](http://adam.chlipala.net/cpdt/) (in my own words). We have not yet reached that stage where we (as a community) has absorbed certified programming in our every day programming activity. And the method he advocates, hand-in-hand proof+code development together with full proof automation I believe will take us far. 
in this setting? Laziness - I think you could take the time and talk about why and how fibs = 1 : 1 : zipWith (+) fibs (tail fibs) is working
I always found tie-the-knot style algorithms fun and informative. For example, implement and discuss an A* search or BFS.
The original informative example of lazy algorithms was the `repmin` problem. Take a binary tree with values at the leaves, and build a new binary tree that has the same shape but has all of the leaves replaced with the minimum value of all of the leaves in a single pass. Another fun one is the Axelsson and Claessen paper on using circular programs for higher-order syntax: http://phaazon.net/pub/axelsson2013using.pdf that is a pretty rough read for an audience that isn't familiar with name-capture but acts like a sort of 'grownup' rep-min example.
Thanks!
&gt; Which is to say, things get heated, but rarely personal You are kidding right? 
here are some links regarding *repmin* - [Haskell wiki](https://wiki.haskell.org/Circular_programming#Repmin_problem) - [Haskell cafe](https://mail.haskell.org/pipermail/haskell-cafe/2006-June/016179.html) - some slides from [Mark P Jones](http://web.cecs.pdx.edu/~sheard/course/CS457-557/Notes/Laziness-6up.pdf) - [paper by R. Bird](https://link.springer.com/article/10.1007%2FBF00264249) - sorry it's behind a paywall - no clue if you can find it for free
I liked this solution for its golfing elegance, but /u/Bliminse‚Äôs solution has more didactic value for OP‚Äôs level. There is certainty also something to be learned about the value of using the Prelude, however.
I usually like to showcase Parsec to haskell un-friendly audiences. This library is usually a good way to create a wow effect! This library is basically why I learned Haskell in the first place :)
Even so - why not make it crystal clear and just write `null _ = False`?
That's what I figured, just a minor integration issue related to `Foldable` and the move to base. `NonEmpty` is the best thing since sliced bread. Thanks for it, and thanks for pushing it to base.
Anything that has a DOI can be .. [found](http://sci-hub.hk/10.1007/BF00264249)
thanks ;) (... I tent to be a bit cautious with linking probably copyright protected stuff)
Copyrighted stuff? Never seen anything like that round these parts
never mind then
This does not seem significantly different from just using Haskell's existing records, right? The main addition appears to be first class `exists` syntax (a welcome change for more than just modules). The `record` syntax is basically just slightly nicer sugar for `DuplicateRecordFields`; I'd rather see an actual anonymous record type proposal implemented.
This dates from ~2002, it's not new at all. To confirm the date, see e.g. this reference: [1] K. Crary, R. Harper, and D. Dreyer. A type system for higher-order modules. *(To appear in POPL‚Äô02)*, Sept. 2001. 
Damn, apologies. I thought this was new! I just stumbled across this from HN, and implicitly assumed it was new, I'm sorry.
Hey, ehamberg, just a quick heads-up: **happend** is actually spelled **happened**. You can remember it by **ends with -ened**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
[QuickCheck](https://en.wikipedia.org/wiki/QuickCheck). It's a cool concept; it's very relevant to Agile, DevOps, TDD; and it's been ported to quite a few other languages so members of your audience might find it useful immediately.
**QuickCheck** QuickCheck is a combinator library originally written in Haskell, designed to assist in software testing by generating test cases for test suites. It is compatible with the GHC compiler and the Hugs interpreter. In QuickCheck the programmer writes assertions about logical properties that a function should fulfill. Then QuickCheck attempts to generate a test case that falsifies these assertions. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
bad bot
maybe this is interesting for you: https://www.youtube.com/watch?v=zZ_nI9E9g0I&amp;list=PLxj9UAX4Em-Ij4TKwKvo-SLp-Zbv-hB4B it's about rewriting an existing tool with Haskell - IMO quite a nice intro
Thank you, I will give that a try! :)
I don't understand why avg :: Int -&gt; Int -&gt; Int -&gt; Double avg a b c = (fromInteger $ sum a b c) / 3 where sum a b c = toInteger a + toInteger b + toInteger c is correct, and avg :: Int -&gt; Int -&gt; Int -&gt; Double avg a b c = ((fromInteger . sum) a b c) / 3 where sum a b c = toInteger a + toInteger b + toInteger c fails? If dot means f.g x == f (g x), so (fromInteger . sum) abc == fromInteger (sum a b c). What am I missing?
That doesn't really sound like the kind of thing category-theory would apply to, which objects and morphisms are you thinking of?
You can have a look at my [first (&gt;100 Lines) project](https://github.com/FabianGeiselhart/Tetris). It's a simple, buggy and incomplete (I never bothered to fix the bugs) terminal version of tetris.
20.9 MB, without any minification or optimization. You can check out my project here: https://dlthomas.github.io/sql-viewer/built/index.html
[quickbench](https://github.com/simonmichael/quickbench) is a ~300 line based on standard stack template.
Hi, this is a fairly simple CLI application that uploads/downloads/lists artifacts to remote/local caches (S3, Minio, FileSystem) according to certain rules and structure of the artifacts. https://github.com/blender/Rome
You can have a look at [crew](https://github.com/maxigit/crew). It's a small tool which allow to rewrite commands on the fly depending on configurations files. If you ignore instance definitions at first, it should be pretty readable.
I don't know all the gritty details, but my impression is that its a combination of: - TH is a beast, it's bloated and complex due to all the metadata it has to pass around and the fact that its untyped. Therefor not well optimized, because it's hard to work with its internals and ensuring correctness is hard enough without optimization. This can be improved but with Lots Of Work. - TH is slow in general even on GHC because its running interpreted, ie. unoptimized. This is not an easy problem to fix - should we compile the task which will compile the TH splice? This is likely to be slower than just interpreting it, but may be faster for certain complex splices. This is unlikely to be improved without scrapping TH completely and using some as of yet non-existent alternative for metaprogramming. - GHCJS has to do all of the above but interpreted in javascript, which is much slower than interpreted ghc for the kinds of things needed for TH (AST manipulation, meaning lots of allocation and indirection). There is potentially a huge gain here by using the build platform (usually native x86) to compile splices rather than the target platform (javascript), in which case TH on GHCJS would match that of GHC. Currently it seems that TH is baked pretty deeply in in a way that doesn't make this change easy, but I don't really understand the main technical challenge. Someone more familiar with this issue (@ElvishJerricco ?) should chime in to correct me.
I don't know any categorical theory of clustering algorithms. I'm not sure what that would even be. There _is_ a topological theory of clustering algorithms (among other things) called "topological data analysis" and there is a categorical side to such things, depending on how one understands the homology involved.
At work, we wrote https://github.com/channable/vaultenv/ which is a small CLI utility which makes HTTP requests, reads files, and executes processes with secrets from HashiCorp Vault. It can serve as an example of how to use (limited subsets of): - optparse-applicative - aeson and aeson-lens - string types - containers - http-conduit And probably more. It builds using stack. We use it in production and was my first serious Haskell project. If you have any questions, feel free to ask :)
How hard would it be to allow exporting type aliases abstractly in GHC? If I'm thinking correctly, what needs to be changed is some kind of scoping rule with imports and probably something minor with parsing. Compare (today): {-# LANGUAGE GeneralizedNewtypeDeriving #-} module Foo (Foo, foo1, foo2) where import The.Best.Class newtype Foo = Foo { unFoo :: Bar } deriving TheBestClass‚Ñ¢ mapFoo f = Foo . f . unFoo foo1 x = mapFoo (foo1' x) where foo1' x f = ... foo2 x y = mapFoo (foo2' x y) where foo2' x y f = ... with (adding abstract type aliases): module Foo (Foo, foo1, foo2) where type Foo = Bar foo1 x f = ... foo2 x y f = ... and if you want to export the type alias fully then do (for example) module Foo (Foo (..), ...) where
Thanks for the answer! I thought that given g is a function of 3 arguments, it will work as f(((g(x)y)z). So I cannot use dot operator with functions which can receive more than 1 argument?
Sure you can, you just need lots of them: Prelude&gt; :t ((.) . (.) . (.)) f g ((.) . (.) . (.)) f g :: Int -&gt; Int -&gt; Int -&gt; Int Prelude&gt; :t f f :: Int -&gt; Int Prelude&gt; :t g g :: Int -&gt; Int -&gt; Int -&gt; Int Better to use a lambda in most cases.
Oh, now I see. Thanks so much!
https://arxiv.org/abs/1011.5270 Classifying clustering schemes Many clustering schemes are defined by optimizing an objective function defined on the partitions of the underlying set of a finite metric space. In this paper, we construct a framework for studying what happens when we instead impose various structural conditions on the clustering schemes, under the general heading of functoriality. Functoriality refers to the idea that one should be able to compare the results of clustering algorithms as one varies the data set, for example by adding points or by applying functions to it. We show that within this framework, one can prove a theorems analogous to one of J. Kleinberg, in which for example one obtains an existence and uniqueness theorem instead of a non-existence result. We obtain a full classification of all clustering schemes satisfying a condition we refer to as excisiveness. The classification can be changed by varying the notion of maps of finite metric spaces. The conditions occur naturally when one considers clustering as the statistical version of the geometric notion of connected components. By varying the degree of functoriality that one requires from the schemes it is possible to construct richer families of clustering schemes that exhibit sensitivity to density.
Full disclosure: my answer is `O(n log n)` and /u/Bliminse's is `O(n)`.
Here is a list of a few of my own projects: https://www.reddit.com/r/haskell/comments/7ocjkx/do_we_need_more_short_quality_videos_explaining/ds8xr28/
In `C` a 'char' is a badly named 8 bit integral and not a unicode character.
tom-md answered why you can do arithmetic on C chars. I'll try to answer the first question - Char isn't an instance of Num because (a) there isn't a sensible definition of `fromInteger` that you could use, and (b) giving them a fake instance of Num isn't very helpful (using the Enum instance is sufficient) -- contrast this with giving a fake instance of Ord for Double, which is very helpful for doing math/graphics. In other words, there is a defining bijection between a subset of the integers and the codepoints but this mapping doesn't carry (+) or (\*) from the integers in any meaningful way, so it doesn't make sense to be able to do (+) or (\*) on codepoints. Enum reflects the semantics of Unicode better by just talking about the bijection and not the translation of numeric functions.
this sounds like one of those business requirements at work I try to figure out how to make types for; thanks for the rundown
This seems like a good thread to ask this: if I want to use Backpack, I cannot use Stack, is that correct? I need to be using `cabal new-build` etc? Thanks!
There's also a nice wrapper around `process` called `typed-process`. `typed-process` is my go-to process handling library. https://hackage.haskell.org/package/typed-process
There is the QFPL [Applied FP Course](https://github.com/qfpl/applied-fp-course) that was created for almost exactly this reason. It is designed to be run as a workshop but plenty of people have been tackling it on their own. You build a basic REST application with different routes, database integration, testing etc. Starts off with just a 'hello world' equivalent and then lets you into the chewy bits. :) We're keep to help with any questions you may have too, just [hit us up using one of options here](http://qfpl.io/contact/). 
I'm no Haskell expert, but I'd say you should worry more about which library gives you the functions you want vs performance characteristics of each. I have a small project that has used both System.Process and System.Posix.Process to good effect, basically it - uses System.Process.createProcess to spawn off a lot of tasks - combines their process handles together - uses System.Posix.Signals.installHandler to make cleanup more pleasant - calls System.Process.waitForProcess to block on all the processes It's probably not the prettiest (been meaning to clean it up and post here for some sort of feedback), but maybe it'll help you get some sort of idea of these libraries in action! https://github.com/cah6/analytics-launcher-script/blob/master/src/Main.hs
I'm not sure if this is exactly what you're looking for, but /u/PokerPirate, who wrote the HLearn library talked some about how he used some category-theoretic concepts to come up with better implementations: https://izbicki.me/blog/hlearn-cross-validates-400x-faster-than-weka talks about some of it.
The `eval` function turns an `Exp` into a `Val`. So for every kind of expression you can have, you need to understand what sort of value it contains. It looks like your lambda expressions are of the form `Œªvar. exp` - something that has one formal parameter and an expression to evaluate when applied to a value. The `Elambda` type must be able to hold a formal parameter (variable name) and an expression, and `eval` must be able to turn that into some kind of value. What kind of value can be understood by looking at what needs to happen in `Ecall`, which applies a function (lambda or primitive) to a value. Given a `Vlambda` and some other `Exp`, `eval` for `Ecall` needs to produce a `Val`, the same as `Vprim` does. So a `Vlambda` is probably going to look very similar to a `Vprim` - something from a `Val` to a `Val`. The piece of the puzzle remaining after that is to work out how to turn an `Elambda` into a `Vlambda`, which means you'll need something that can take a `Val` determined at application time (the actual parameter), label it with the lambda's formal parameter name, and then evaluate some expression to produce a `Val`. It's going to look an awful lot like a `let` expression where the `v` is provided later. It's going to look an awful lot like a Haskell lambda expression. Something along the lines of `eval env (Elambda x e) = Vlambda (\v -&gt; eval ((x,v):env) e)` I expect, which means "given some `v`, evaluate the lambda expression in the current environment plus that `v` bound to the formal parameter name". Hope that helps you find understanding.
Neat. My co-worker introduced me to his Trello full of cards last week. Gave me access to it today. I started thinking it would be cool if there was a command line client for it. Now this. üòÅ
I've developed a web application based on [Real World](Realworld.io) spec: https://github.com/eckyputrady/haskell-scotty-realworld-example-app Notable Dependencies: * `classy-prelude` - Better Prelude for Haskell * `scotty` - Web ~~framework~~ library * `postgresql-simple` - PostgreSQL library * `aeson` - JSON serialization &amp; deserialization * `digestive-functors` &amp; `digestive-functors-aeson` - Input parsing &amp; validation * `jose-jwt` - JWT encode &amp; decode * `slug` - Slug string builder * `hspec` - Test framework
I suppose that's reasonable.
A function composition (or.map) is not defined, unless I give (p :: a -&gt; b) to map, (or.(map p)). I expected ((or.map) :: (a-&gt;Bool) -&gt; [a] -&gt; a) function. Why is it not defined?
Export formats from runscope and postman might be interesting Har and havcr Haskell libs might be useful
There are four lights!
"Not defined" means "ill typed"? Prelude&gt; :t (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c ^ Or ^ map Prelude&gt; :t or or :: Foldable t =&gt; t Bool -&gt; Bool ^ b ^ c (from `.` type sig) Prelude&gt; :t map map :: (a -&gt; b) -&gt; ([a] -&gt; [b]) -- N.B. explicit parens ^ a ^ b Unify the `b` type variable. I hope you don't believe `[Bool] ~ ([Bool] -&gt; [Bool])`?
let "war" = "peace" in "Ministry of " ++ "war" 1984, anyone? :P
Great answer! Cheers! How do I box key-words as you did? I'm wondering what the dot means in your "Œªvar. exp". Also, I don't think we even had seen this "\v" notation. It seems like this course is going to hard. :p
How does the parser distinguish the identifier from the expression?
You're overwriting the `+` operator. `let 2+2=5 in 2+2` is equivalent with: let plus 2 2 = 5 in plus 2 2 Because `let` is just a syntax for (re)defining functions (including operators like `+`) and variables. And as you might expect, it's usually a bad practice to overwrite the predefined operators. So enabling a warning would help you: &gt; :set -Wname-shadowing &gt; let 2+2=5 in 2+2 &lt;interactive&gt;:3:6: warning: [-Wname-shadowing] This binding for ‚Äò+‚Äô shadows the existing binding imported from ‚ÄòPrelude‚Äô (and originally defined in ‚ÄòGHC.Num‚Äô) 5
Yes, it means ill-defined. You inserted [a] -&gt; [a] to b, but let‚Äôs see following example. f :: Int -&gt; Int -&gt; Int f x y = x + y g :: Int -&gt; Int g x = x (g.f) :: Int -&gt; Int -&gt; Int and it works as I expected like (g.f) 1 1 = 2. If I follow your explanation, I guess above example should not work because Int -&gt; Int /~ Int.
It doesn‚Äôt, the parent is wrong. The let is binding (+), shadowing the existing definition from Prelude. Like many definitions of binary operators, this one is infix, and is pattern matching non-exhaustively on 2 for both arguments. If you wrote `let 2 + 2 = 5 in 2 + 3` you‚Äôd get a runtime error about a non-exhaustive pattern match.
Thanks for clarifying.
Possibly more confusing is `let 1 = 2 in 1`, which returns 1 :) To see what‚Äôs going on there, think about patterns that have number literals in them, like `case x of (_, 6) -&gt; ...`, and then of irrefutable patterns like `let (x, y) = (5, 6) in x + y`. So for the weird `let 1 = 2` case above, Haskell is treating it like an irrefutable pattern that binds no variables. With irrefutable patterns, Haskell‚Äôs laziness means that it doesn‚Äôt try to perform the match until one of the variables is evaluated (try for example `let (x, y) = undefined in 5`). And because you bind no variables, you can‚Äôt actually get Haskell to notice that your nonsense 1 = 2 is indeed nonsense, so its only purpose is to confuse people on the internet :)
The dot in Œªvar.expr is equivalent to the "-&gt;" in Haskell's [anonymous functions](https://wiki.haskell.org/Anonymous_function). That is, it separates the variables/arguments of the function from the function body. Here's an example. First, we write a silly function in Hasekll: addOne x = x + 1 Now, let's write it as an anonymous function, still in Haskell: \x -&gt; x + 1 Finally, let's write it in traditional lambda notation (not Haskell): Œªx.x+1 
I see what you did there heh heh heh