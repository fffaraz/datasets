/u/danharaj got it right. If I wrote it in rust I'd have to deal with writing API wrappers for everything and a weaker template system, and have to build my own imgui from scratch, only to get only slightly better control over object lifetimes by cluttering everything with symbols, when object lifetimes in games already have a fairly ritualized flow. If I wanted to stop and write wrappers for everything for a month before starting I'd be working in Haskell. ;) I may yet go there, but I wanted to make sure that what I wanted to do could be done on 'bare metal' before I moved, and I wanted to dust off my C++ skills. Here's a quick current development video from a few days ago, just showing the status of the sky and water and physically based rendering for the controllers: https://www.youtube.com/watch?v=pBjzyi3YVVA Interesting tidbit, the sky and water are rendered with a single triangle.
I'd be interested in knowing more about how unaligned accesses hurt performance from GHC-generated code. What is creating unaligned data?
I use a lot of variadic templates to write idiomatic c++11. e.g. here is a variadic mutate-in-place map function for spherical harmonics. https://github.com/ekmett/vr/blob/f701501fc237bed7b9def2a325a852015d9b4420/spherical_harmonics.h#L36 A similar construction for a non-mutable `map` could be built as well to generalizes over `liftAn` or `zipWithN`. These remain at the RFC level in rust. https://github.com/rust-lang/rfcs/issues/376 Yes, I could give up and use macros for this functionality, but it is rather nice not to have to. Macro hell is a place that once you're stuck in it, is hard to escape. (Though much the same can be said of templates!) At least rust macros are hygienic, though. Rust simply didn't enter the equation because I already have enough to deal with talking to a bunch of external C++ libraries using a language I already understand, without adding learning a new tool-chain and having to roll my own libraries for everything to the equation.
Honestly, I feel like the category theory influence in Haskell gets oversold. Even when concepts appear in both, like monads, they're in such different contexts that seeing the connection requires some subtlety. That being said, natural transformations show up a bunch when describing Haskell abstractions. It would probably be useful for someone to collect a few examples some time.
Hopefully someone smarter than me will be able to help you understand why you need FlexibleContexts here, but if it's helpful I can at least show you how I got your code to compile and do what I think you wanted it to do. {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE FlexibleContexts #-} module Shuffle where import Data.Array.IO import Data.Array.MArray import System.Random shuffleIO :: forall a g. (MArray IOArray a IO, RandomGen g) =&gt; [a] -&gt; g -&gt; IO ([a], g) shuffleIO lst gen = do (arr :: IOArray Int a) &lt;- newListArray (0, (n-1)) lst shuffleRec arr gen 0 where n = length lst shuffleRec a g i = if (i &lt; n-1) then let (pick, newGen) = randomR (i, (n-1)) g in do pickVal &lt;- readArray a pick iVal &lt;- readArray a i writeArray a pick iVal writeArray a i pickVal shuffleRec a newGen (i+1) else do elems &lt;- getElems a return (elems, g) run :: IO () run = do let r = mkStdGen 1 print . fst =&lt;&lt; flip shuffleIO r [1,3,4,2] print . fst =&lt;&lt; flip shuffleIO r "Hello, world!" 
Oh and I assume you've read this? https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html?highlight=scopedtypevariables#the-superclasses-of-a-class-declaration This is a good one on ScopedTypeVariables https://ocharles.org.uk/blog/guest-posts/2014-12-20-scoped-type-variables.html
It is interesting though that if you keep pushing on the `forall a. f a -&gt; g a` encoding of natural transformations you learn that parametricity is actually stronger than naturality, which is both good and bad. It does mean that you can't use that naive encoding of naturality and build up categorical limits that are strong enough to build products, though.
Why?
And Neel K. has [talk summaries](http://semantic-domain.blogspot.com/2016/09/hope-2016-talk-summaries.html) for you to pick what you want to watch!
As always, the mathematicians of today are captured by a gnostic cult to his own notation, that evolved for brevity and conciseness for the purpose of making agile and precise symbolic calculations on a paper, but not for pedagogical neither expositive purposes. I regret how this esoterism has contributed to make simple concepts (like CT) difficult and has kept intelligent people away from mathematics and their applications. So It could be said that they have accomplished their goal: short sighted job security. But in the process they have undermined their own role in society.
[deleted] ^^^^^^^^^^^^^^^^0.5488 &gt; [What is this?](https://pastebin.com/64GuVi2F/00252)
He means that binary used to be faster (it was) because it did not support backtracking, but somebody added it, thereby throwing away a certain guarantee of better performance for having to do less work inherently.
FYI, I recently sent a [patch](https://github.com/bos/aeson/pull/452) which improved aeson's speed by over 20%, and i'm going to replace attoparsec with binary when my binary-parsers get released. I'm not sure the "attoparsec which binary outperforms which cereal outperforms which store outperforms" thing. it depends too much on how you construct your parser, while my own benchmark showed that binary is an excellent choice, if you think store is better, why not send a patch?
I can't see how binary "throwing away a certain guarantee of better performance for having to do less work inherently" really, from the code, it seems if you don't use `&lt;|&gt;` or its friends, you won't get any extra book-keeping.
Mathematicians often view category theory as being all notation with no content. That is a bit extreme. But it is true that much of the innovation of category theory was just coming up with any notation at all to represent these concepts in such far-reaching generality.
It's an interface lower then WAI. You can serve a WAI app with it, just wrote one: &lt;https://github.com/nfjinjing/http-pony-serve-wai&gt;
Could you perhaps elaborate on this (or provide a link for me to read)? I do know that parametricity is more general in the sense that f and g needn't be functors here, but I didn't know it was stronger too.
Hey, thanks. Worked for me too. Can you explain what the `forall` quantifier means, too? I was recently trying to learn how to use `ST` and came across something that, to me, said that `forall s` means something like, you can't give place any constraints on `s` or bind it to anything, because it has to work for *all* x's. It sort of made sense in that context but now I'm quite certain I have no idea what it it means at all.
Brainfuck is especially well suited to being interpreted by Haskell: makes a good little case study on lazy evaluation.
Yes, that's right.
Nice, I needed this at some point and IIRC I just used `V.toList`/`V.fromList` :-) Now we have two packages with the same API (modulo `List` and `Vector` types), I'm guessing that this is a use case for backpack, right?
Well, I got quite the same impression. And for the first time I appreciate reddits rather unconventional vote-based ordering of posts, as well as the hierarchical nature of commenting. But from what I've read, it seems like the 'problem' is the two different package config languages used by cabal (the `.cabal` format) and by stack (using `YAML`). I agree that it is confusing for a newcomer to be confronted with several project configuration related files and file formats, but merging it all into a single file (the hypothetical language of which is the topic of that thread) isn't even possible: It's totally worthwhile to have multiple `stack.yaml` files for e.g. different LTS snapshots. Also I posted this rather prematurely I suppose. I was deceived by the huge number of posts in a short time and hadn't even read it completely.
Unfortunately the split is not really feasible, as all of the datatypes (parser elaborates to) are in the `Cabal` library. And more granular split (`Cabal-types`, `Cabal-parser`, `Cabal-simple`, `Cabal-*`) would be a PITA from maintaining, and GHC building, povs.
I'm glad I clicked this. Didn't even realize it's the ICFP talks.
`toList . function . fromList` is actually painfully slow. I really wish there would be a way to make this fast, but I am not aware of one. I haven't looked into backpack too much, but from what I gathered you might be right.
Optimizing the desugaring as you describe seems like a low hanging fruit. Apart from that, we can improve the power of Arrow by representing those generated `arr` + *permutation functions* using new Arrow method. Those methods would then have default implementations with `arr`. It would probably be wise to investigate generalizations of Arrow as well. It should really have `Strong` profunctor as a superclass. Adam Megacz Joseph has written a PhD on the subject of generalizing Arrow: http://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2014-130.pdf Arrow seems underappreciated, considering that they are right between Applicative and Monad in the power spectrum.
`Cabal` the library does expose the old parser already, did so from the beginning I guess: http://hackage.haskell.org/package/Cabal-1.24.0.0/docs/Distribution-PackageDescription-Parse.html And it will expose the new one (and the building blocks).
[deleted] ^^^^^^^^^^^^^^^^0.6598 &gt; [What is this?](https://pastebin.com/64GuVi2F/85491)
Na... That doesn't generalize does it? I would assume that a `Data.Vector. Bundle` could somehow efficiently be represented as a list which makes conversion to and from vectors cheap-ish. But either this does not happen, or it happens and I am mistaken that it is efficient.
&gt; It does since C++11 Odd. I thought I looked for it. I'll take a look again. As for ugliness, I literally just changed superclasses in the middle of a larger refactor and did the quickest search/replace to make it work. I didn't go back and clean it up. I started with a private array type, switched to boost::array, then switched to std::array over the course of a couple of hours while focused on other things.
Well that makes sense. Sorry for being a bit slow.
`forall a b c d.` literally means "for all types a b c d". It's implicitly there in polymorphic types signatures, like `id :: a -&gt; a`, which could also be written as `id :: forall a. a -&gt; a`. In /u/screamish's code, it's being used as a necessary part of the ScopedTypeVariables language extension to bring `a` into scope within `shuffleIO`, which means that the a in `(arr :: IOArray Int a)` is the same as the a in the type signature of `shuffleIO`, rather than a new type variable. What you're thinking of sounds like some of the other uses for the forall quantifier, particularly the language extension RankNTypes. It's rather involved to get into those extensions, but a rough tl;dr would be that by putting the forall in different places, you can change how that type variable has to be treated.
&gt; But what if I implement my Arrow instance such that the laws hold semantically, but the difference is significant operationally? Am I allowed to do that? Yes. It's often useful for one module to be able to witness violations of the monad/functor/&lt;type class&gt; laws for one reason or another. That said, if the laws hold denotationally but not operationally AND there is a particular form with strictly better operational semantics, you might want to throw in enough SPECIALIZE/INLINE/RULES pragmas to have GHC "rewrite" the code into the better operational form.
&gt; Maybe someone with a horse in this race could provide a TL;DR. :) I gave up on reading it, as I couldn't even see the track for all the horses. :)
Off topic, but is there any operator for piping forward a value from the left to right? Something like this: val1 |&gt; fun1 |&gt; fun2 |&gt; fun3 I just find it easier to read a series of operations like that from the left to right.
It's the HOPE (Higher-Order Programming with Effects) workshop collocated with ICFP.
You can write a "Enter your name: _", "Hello, _" program the Arrow way, but not the Applicative way.
In order of increasing expressiveness: * With applicatives, the effects occur before the computation. * Arrows allow the effects and computation to interleave, but the computation can not determine what the subsequent effects will be. * Monads do allow the effects to depend on the computation. Monads can not be analyzed statically, because `&gt;&gt;=` requires (part of) the computation to determine the rest of the program to be analyzed. Arrows *can* be analyzed statically. Ability to analyze basically means being able to inspect all constructors of the type without having to run the computation. **Example** For example, a data fetching DSL may use Applicative and Arrow instances to construct an AST-like data structure from which all the data locations can be determined in advanced and fetched in parallel. This is evidenced by the fact that there are no (bare) functions as arguments to these class methods. However, when the data structure is extended to support a Monad instance, the analysis has to stop half-way and evaluate the `M a` for the required `a` whenever a `Bind :: M a -&gt; (a -&gt; M b) -&gt; M b` constructor is encountered. In constrast the `arr` in Arrow be ignored during such an analysis, because it is not allowed to have an effect. The `.` can be represented with `Compose :: A b c -&gt; A a b -&gt; A a c`. It can be analyzed because there are simply two more `A`s to inspect.
Also, you can write a "Enter your name: Mitchell", "Hello, Mitchell" | "Enter your name: Robert", LaunchMissiles using Monad, but not with plain Arrow
Another way: TBlock ts -&gt; sequenceA transformer ts where transformer = transform . replaceName . Map.fromList &lt;$&gt; mapM sigPair collectedFuncs Or if you defined transform yourself, you could change its argument order.
The fact that you can inspect the expression makes it sound like a great fit for DSLs but then there's the `arr` function that ruins it since it lets regular Haskell functions to appear anywhere in the AST. I guess it's still good for some DSLs, but not for things like compiling to other languages.
Just out of interest, what practical advantage would this provide? We have a few times considered doing the obvious thing and split the types &amp; parsers part of Cabal from the build system part. As I understand it, stack uses the former part but not the latter part. But what practical advantages are there? It wouldn't reduce dependencies, it'd just mean the library you use doesn't have a bunch of modules you don't use anyway (and that other lib still has to be installed so you can compile Setup.hs scripts). I think the reason we've not done it yet is simply because the advantages are not that significant and it'd involve a bunch of work to make ghc's build system and bootstrapping work again properly. But if there's significant advantages then perhaps we should do it, especially if you help with the PR ;-).
&gt; And these should be available as a library. Which at a minimum will be exported by the Cabal library. I'm not aware of any plan to split it into another separate library.
I rather suspect that several of the participants in that thread do not realise that package files (ie .cabal files) and project files (ie cabal.{config,project} for cabal, and stack.yml for stack) are different, serve different purposes and are not going to be merged any time soon. With both cabal and stack, the notion of a *project* is a particular environment/configuration for a *set* of packages. So there's a one-to-many relationship and additionally the author of a project is very often not the same as the author of all the packages in it. The distinction of roles between the package author and package builder is a useful one, particularly for libraries. So even if the package .cabal files use the same syntax as the project files (and note that for cabal they do use the same syntax), these two (packages &amp; projects) are not going to be merged. As for standardisation of formats at the project level, there is a great deal of innovation going on at that level at the moment and it would probably slow things down a great deal to try to standardise on one meaning &amp; behaviour for project files (the syntax is comparatively easy).
[What I wish I knew when learning Haskell](http://dev.stephendiehl.com/hask/) is probably still the best one-stop quick overview of all these type system features. Otherwise, check the language feature section of [GHC's users guide](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/lang.html).
I'm assuming that you mean GHC extensions. If so, search for "ghc extensions" in the search-engine-that-shall-remain-nameless. The first page of links may get you going.
I'm on vacation, but the benchmarks and code in binary vs store are both online for you to look at if you have 5 minutes. See also my "weigh" blog post, binary and cereal allocate an order of magnitude more to parse the same things. Look at the Get type from binary vs the Peek type from store.
Not that I'm aware of. If it did exist in `base`, it would probably be in [Data.Functor](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor.html), but there's nothing like that in there.
Thanks a lot!
Since Haskell is a language that's still very much rooted in academia, new type system features tend to come with papers. In my experience, reading them helps understand the motivation behind the feature in question and they often contain some good examples too. Servant also has an accompanying paper by the way which also goes into more general applications of the same fundamental idea to solve what's called the expression (extension?) problem.
Sure, and Haskell's `Functor` is much more restrictive than functors, or even endofunctors on Hask. But I was thinking more generally. I can't come up with any good examples of natural transformations right now, aside from `lift`(which is a natural transformation between Haskell functors, but can also be a natural transformation between functors in the category of monads), but general functors show up a bunch as well. For example, `first` and `left` are functors over their Arrow's category.
They also don't seem to understand that YAML (and JSON and XML) is a meta-language, and you would still need a schema and a way to extract the actual data from the parsed files.
The [24 days of ghc extensions](https://ocharles.org.uk/blog/posts/2014-12-01-24-days-of-ghc-extensions.html) was a great read. 
I think the flipped bind operator could also work here: TBlock ts -&gt; do nameMapping &lt;- return . Map.fromList =&lt;&lt; (mapM makeSigPair collectedFuncs) return $ transform (replaceName nameMapping) ts That way you only have arrows pointing left, so the visual flow doesn't get interrupted.
Yes, especially for those Categories which don't have an 'arr' (think DSLs which you build a syntax tree and 'run' outside of Haskell e.g. a graphics pipeline) There's a nice project for that 'category-syntax', though unfortunately is stalled waiting on the ideas of a free cartesian category to be figured out. 
Data Types a la Carte is a good paper that goes into the Expression Problem and I believe it inspired the work that went into Servant.
[My thoughts on haskell after reading part of that thread](https://pics.onsizzle.com/man-who-thought-hed-lost-all-hope-loses-last-additional-1489947.png)
Unfortunately, this is not an intended purpose for Arrow. Arrow (in terms of category theory) is Category + Profunctor (Strong profunctor, in particular), which necessitates `arr`. What you're looking for is probably more like Category with strength but I don't know if that's on Hackage anywhere, or if it's even actually a thing.
Well, it is interesting because the issue arises even when the category in question in just `Hask`, which is somewhat telling as also why a lot of folks' experiments with category theory stop before they get to anything interesting -- building on that parametricity-centric foundation you run into a pretty sharp cap on the abstractions you can use very early on. As an interesting aside, using `PolyKinds` and a more general `Functor` you can make both [`first`](https://github.com/ekmett/hask/blob/master/src/Hask/Category.hs#L303) and [`left`](https://github.com/ekmett/hask/blob/master/src/Hask/Category.hs#L313) be special cases of fmap (you have to runNat to lift out to the appropriate argument) but to get runNat to work, you need to solve the aforementioned problem with parametricity being too strong. To get there you want `fmap` = `left` to take a morphism in Hask to a morphism in `[Hask, Hask]`, but for the latter to exist you need to know `(Either a)` is a functor, which requires knowing something about the objects in `[Hask, Hask]` that other than just 'they are types'. With that knowledge you can preserve naturality, despite lacking full parametricity. You can switch from the bifunctor-centric view to one that includes arrows, but I mostly ignore that arrows exist.
Try this: http://www.sciencedirect.com/science/article/pii/S1571066114000346 Hermida et al. do a wonderful job of laying out the differences between parametricity and naturality. But ultimately, the main difference comes down to the fact that a natural transformation is a _set_ of morphisms that happens to be indexed by the objects in one category, with a coherence condition, whereas parametricity say you have _one_ function that can be given all of those types, but by that parametricity you can't distinguish. The coherence condition follows for free under those circumstances.
Please look into how things work before pretending you know, it doesn't need that part of Cabal. It literally only use the datatypes, a few utility functions, and the parser.
We were considering trickle-releasing the videos, but reconsidered and here they all are!
IMHO just saying Arrow is right between Applicative and Monad doesn't really do it justice. I think it's more like, if Applicative and Monad are two points on a power scale, Arrow is the continuum between them. Let me explain; (I don't have the formal background, sorry if this is nonsense.) &gt; Arrows allow the effects and computation to interleave, but the computation can not determine what the subsequent effects will be. What's not doing Arrow justice here is that you can also define what exactly the effect is. Using arrows, you can expose an interface, where an arrow will read from a file and output the contents, and another arrow will be constructed with a file name and receive some contents to be written to that file. In this setup the effect is the reading from and writing to some statically known files. But you could also expose an interface where the names of the files are also received as arrow inputs, so that the effect also includes the choice of the file to write to, but you don't know the files statically anymore. That's why I'm saying Arrow spans a continuum between Applicative and Monad, it's like a slider where the programmer can choose what's known at runtime vs. arrow composition time (i.e what information can be obtained from the arrow data structure without running it).
You actually don't even need `ArrowChoice`, you could provide a primitive arrow that has the "choice" built in. `ArrowChoice` would enable the user to compose existing primitives to obtain arrows with choice in them. But I think that's the main power of `Arrow`, it lets the implementer of the `Arrow` to choose where static information ends and where effects begin.
Note that cabal-install does support calling an external setup scripts, which can be built with any version of the Cabal library.
The key is that the kind of IOField is *not* star; thus, the kinds of the fields dictate how the types of the constructor. So not a completely horrible special case. STRefs need more metadata so the current proposal does not handle them. (Yet.)
I think /u/roberthensing is still correct in saying the computation can't determine what the subsequent effects will be. It doesn't matter that you can define arrows that do decision making effects. The composition of arrows is the part that (unlike composition of monadic effects) can't make decisions. Yes, the arrows themselves can make decisions. But a function making use of such arrows cannot. This is the limitation, I think.
Nice distinction, so going with the power spectrum slider analogy, the implementer of the `Arrow` moves the slider, the user is bound to whatever the primitives happen to provide.
Oh, you meant the compile time dependency of stack executable, and not the need for Cabal to actually build stuff. Guess I'm in permanent cabal defense mode, sorry.
Not sure if this is relevant, however zlib's Codec.Compression.Zlib.Internal seems to have runST, wouldn't that work enough?
I'll definitely accept a pr, but one problem with non backtracking parsers is that numeric parsing is very hard to get right, even the attoparsec one is [problematic](https://github.com/bos/attoparsec/issues/112).
Wow I didn't realize how much that first reference has expanded and improved. At the beginning I was a bit hesitant to recommend this reference because of certain opinionated stances. But now, this is nothing short of a gold mine.
Sure, the fact that he had to patch GHC to apply his ideas is an indication of how foreign the formulation is, but maybe that's necessary, I'm not one to say. More discussion is definitely needed though, since the current state of arrows in Haskell shows that it hasn't received much attention, there's no excuse for why `Strong` wouldn't be a base of `Arrow`. I just hope arrows get the much deserved attention, as I think they might be the gateway to a very new application area for Haskell, namely, using Haskell as a host language for an EDSL intended to be run independent of Haskell, but incorporating the full power of Haskell at composition time. Haskell would be unrivaled in this area. This way, it could subsume many ecosystems around domain specific languages such as, say, OpenSCAD for defining geometries.
&gt;“Nothing can be farther from the working musician’s mind than counting, […]” Well looks like Victor Zuckerkandl never heard of jazz music. (I feel quite insulted, tbh)
Thanks!
Indeed the modifications to GHC as proposed in the thesis seem a bit invasive. The thesis is concerned mostly with the two-level language idea, but the part about generalizing arrow can be implemented as just a modification to the desugaring of the proc syntax. It might initially even be implemented by requiring RebindableSyntax to make it usable before base receives a Arrow class refactoring. RebindableSyntax for Arrows is available, but not quite finished yet. `arr` is also still required: * GHC 8 docs: "details are in flux": https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/glasgow_exts.html#rebindable-syntax-and-the-implicit-prelude-import * RebindableSyntax ticket: https://ghc.haskell.org/trac/ghc/ticket/7828#comment:46 * Classes for less `arr`: http://blog.spacekitteh.moe/posts/new-monoidal-cats.html 
I suppose you just want to get your job done at this point, so you should probably check out how far you can get with the rewrite rules.
As ezyang says, that's not entirely accurate. Both cabal and stack can compile Setup.hs scripts against external versions of the Cabal lib. The difference is that stack always has to fork an external Setup.hs whereas cabal-install has an optimisation: in the case that cabal-install would want to use the same version of Cabal as the one it itself was built with, then we can skip compiling and forking an external Setup.hs script.
I think I can get the job done even with the current `proc` notation, falling back to using bare `Arrow` functions for composing critical parts, but I just wish desugared `proc` were a bit more efficient by itself. I'll take a look at rewrite rules nevertheless.
In support of this new online Haskell course, there is currently a 25% discount for the new edition of Programming in Haskell - just enter the code PIHMOOC on the Cambridge University Press website. 
I don't really understand this discussion. If you wan't a different way to write your cabal file, this is entirely possible w/o changing cabal at all by simple generating cabal files from your own description. See https://github.com/sol/hpack which I use and like!
Multi-threaded execution is just one example of the larger pattern, yes. It's clearly a case of choosing between either parallel operation (in the Applicative) or sequental operation (in the Monad). So, I think those terms are fine. In IO / Concurrently, the operation is execution either in the same thread or in separate threads. With Either / Validation the operation is evaluation; either stop evaluation at the first `Left` (which "forces" an evaluation order), or evaluate all and combine any `Left`s.
Well, most obviously, SpriteKit only works for Apple operating systems. It also has no 3D graphics at all. If those two limitations are okay with you, this looks like a pretty cool library.
What part of implementation, type inference or code generation?
found time to read through this finally. However, it isn't obvious to me how to put this to good use for user permissions pulled out of the DB. Won't they be in the "value/runtime" universe, whereas this library deals only with the "type/compile time" universe?
So it is fully ASCII parser and not supporting UTF-8?
Thanks &gt; You might want to use Cofree for the AST annotations instead of your AnnF. Probably you thought about that already. Is there any example how to use it for annotations? I tried it but without success. 
Yes, I'd also prefer some syntactic clue, that something different is going on. `data MutPair a b in IO = MutPair (IOField a) (IOField b)` sounds like a possibility.
s/fin/fib (∵) not visible in code listing due to black font colour. (Firefox mobile)
Since you already use recursion schemes, just take a look at this thread https://m.reddit.com/r/haskell/comments/4z28nk/tear_down_cofree/ Cofree is like Fix.
It seems that projects with many dependencies fail to build due to a new limitation in Sierra's dynamic linker that gets hit. For example, [Stack doesn't build on Sierra](https://github.com/commercialhaskell/stack/issues/2577).
Non-mobile link: https://www.reddit.com/r/haskell/comments/4z28nk/tear_down_cofree/
Yes, it's ByteString only, the char8 module is provided for convenience.
I won't be able to attend Haskell eXchange either :( So I have one more early bird ticket available for purchase. Optionally I also have a Hotel reservation at reduced cost for anyone interested (can't cancel the booking unfortunately.), it's at Z Hotel Shoreditch, a 5-10 min walk to Haskell Exchange ( http://www.booking.com/hotel/gb/the-z-shoreditch.nl.html ). Anyone interested, please write me a direct message! 
What resources have you been using?
I have had the same thoughts. My attempt on a solution was to write a quasiquoter for proc notation that used a different method of desugaring. It's [here](https://github.com/tomberek/ArrowDesugar) It's very rough, but it seemed to work. https://github.com/tomberek/ArrowDesugar/blob/master/src/Language/Haskell/Meta/Syntax/Translate/Arrow.hs Part of the project was also to write rewrite rules in a pleasant way: https://github.com/tomberek/ArrowDesugar/blob/master/src/Rule.hs There is another project that may be of interest: https://github.com/gelisam/category-syntax#readme
Will this come with an update to CCA package?
fixed!
Nice, news to me! That's interesting.
No, I have been working interactively in a stack GHCi repl and using Intero in Emacs most of the day - works interactively. I just did a stack build on a large project and the resulting executable ran fine.
Judging by the commentary, it looks like Mac users aren't upgrading to Sierra anytime soon. 
Your layout is fine, just change the module declaration in `Image.hs` to `module Draw.Core.Images where ...` and `PixMap.hs` to `module Draw.Core.PixMap where ...`. The module namespaces should include all the parent directories, such as `Draw.Core.*` in this case for the files in `Draw/Core/*`. Now you should be able to `import Draw.Core.PixMap` from both modules that depend upon it. 
Is there a reason for choosing SpriteKit over any other game engine ? I understand this is from the "haskell for Mac" website, but other than that, is SpriteKit really so good that non-portability was an acceptable price ? Maybe did its design make it easy to write functional bindings in Haskell ?
Clarification request: The second sentence seem to contradict the "No" answer in the first one..
Just curious - what do you mean by "the backend is somewhat done"? Have you moved from backend programming to the frontend? In which case things like Elm and PureScript are still relevant?
What about if an event relies on multiple things being true? (DragonBladeEvent = HiredBlackSmithEvent + DragonEvent). In addition, how do you decide what character performs an event, and how is that character structured? How does he/she know her dialogue? The thing is, right now it makes most sense to me to have a "Character" object with properties, and I don't know how that can be translated, or _if_ it should be, and if not, what's the basic FP alternative to an object?
(Off topic, but) And I strongly disagree that late Schonberg sounds good. If you want atonal, with mathematical harmonic structure, inversions upon transpositions etc., check out Alexander Scriabin. &gt; The twelve-tone technique was also preceded by "nondodecaphonic serial composition" used independently in the works of Alexander Scriabin, Igor Stravinsky, Béla Bartók, Carl Ruggles, and others. https://www.youtube.com/watch?v=xDTgj_69JKA (Agree that "counting" is relevant to humans performing/composing a harmony)
Yes. I.e. user interface stuff. I've considered PureScript but I haven't found a way to keep the native feeling. There is electron, react native ... something kept me from these. But I'm open for any ideas or hints.
Now that you're saying ... I might got something confused. Just went emotional. ; ) 
I read the title as "advice on not falling in disrepair". I have a few joints that are feeling a bit creaky myself.
Yes, there are a few reasons. (1) One big advantage of SpriteKit over other engines (that I am aware of) is that it's in Objective-C. Game engines are typically in C++ and writing Haskell bindings to C++ libraries is notoriously difficult. On the other hand, interfacing with Objective-C is quite easy, as I argued in my Haskell Symposium 2014 talk: https://speakerdeck.com/mchakravarty/foreign-inline-code-in-haskell-haskell-symposium-2014 (In fact, AFAIK, the core of SpriteKit is actually in C++, too, but we get shielded from that due to the Objective-C API.) (2) I would say the main alternative to SpriteKit (as far as having a high-level API goes) is Cocos2d. That project, or parts of it, had been in a bit of a disarray lately, especially with the Objective-C API being stale for a while. (3) SpriteKit is rather fully featured. For example, it already includes a physics engine. Cocos2d can, of course, also be combined with physics engines, but there are more moving parts in the interfacing to deal with. (4) Finally, Apple engineers have long had a tendency to use immutable classes in many places (e.g., 'NSArray' and friends). In SpriteKit many structures are immutable or only change in a few properties. That does indeed greatly simplify exposing a purely functional API. I hope that helps to understand my reasons for picking SpriteKit. Have a look at the code, it is actually not that much (if a bit messy). The fact that —to my knowledge— nothing else with similar functionality exists in the Haskell world, while clearly many people are interested in writing games, is an indication that building on SpriteKit was indeed easier than other paths.
Stack works fine as long as you're not building a project with a lot of dependencies. Stack itself is a project with a lot of dependencies, so it can't be built on macOS Sierra (not with cabal-install either IIUC), but `stack` executables built on older versions of OS X do still run on Sierra. 
Sorry for not posting the link before. Updated the post now. 
ghcjs? just sayin..
OK, I definetely want to see this happen !
Thanks a lot, very much appreciated! Will have a look at this. EDIT: This is actually a really awesome solution, but I think it still doesn't show how you could make Actors depend on each other. You could return the effects that an actor has on other actors in a tuple, but then the return values would get really really complex. Or am I missing something here?
Let's fix `haskell-src-meta`.. it breaks ghcjs, stack and more... https://github.com/bmillwood/haskell-src-meta/issues/57
Regarding the "Character" object, my understanding is that approach has fallen out of favour in conventional game development circles and that an "entity component system" is now more in vogue (disclaimer: I'm overgeneralising an entire industry). For more information on this than you could possibly want, just Google "entity component system". I mention this because a good Haskell design probably isn't based around a "Character" object either. So what would you use, if not an object? Well, in Haskell everything is a value, so you use values. That is, values of some datatype. It's reasonable to have a type called `Character` (paragraph above notwithstanding ). So your initial game state might have one `Character` value for the dragon and you might have a function (or constant) that constructs it. Something like: initialCharacters = [ dragon, grandma, barber "Ann" scythe] dragon = Character { id = "dragon" , hitPoints = 7, events = ... } barber id favouredCuttingDevice = Character ... This isn't real code but it demonstrates: * Give the things you need to refer to IDs. Don't rely on pointers. When you change the dragon, for example, you'll calculate a new value but it will have the same ID. * You can define particular values, such as the initial dragon, as constants (well, actually constant applicative forms). But if you define a function that takes parameters, such as `barber`, then you can create an entire class of barbers - and the word 'class' here is actually the correct one. A difference between what I showed and a Character class is that an object contains (or references) a bunch of methods/functions. You can do that by including functions in fields as necessary. Values that contain functions have some disadvantages though - they're harder to work with because you can't print functions, which can make debugging and serialisation awkward - so only do that when necessary. Sometimes it's better to create behaviours that do contain functions and just refer to them in the values that you're manipulating (and sometimes it's better to just work directly with functions).
You can't exactly match function `id` in the semantic approach, but if there is some law governing `SomeNonPureArrow . arr f . SomeOtherNonPureArrow` for any function `f`, then it is possible to (rely on GHC) to transform it away. So the idea here is that you should not restrict your arrow to a particular data type, but rather have it in generic form that is polymorphic in the type of the arrow. Then you can instantiate it with some particular data type for which GHC is able to optimize before reflecting back to a generic arrow. For one, this data type cannot be a recursively defined type. It may work better if there exists a normal form like in the case of CCA, but that is not a strict requirement to enjoy optimization.
Well none,this is my first try at game developing.
&gt; Is it possible to generate code for statically typed languages like Java? There are some problems with languages like Java. For example, subtyping getFoo : a@(foo : b) =&gt; b getFoo y = y.foo x = getFoo (foo = 1, bar = 2) y = getFoo (foo = 1, qux = "qux") How would you express this in Java? &gt;Can I define ADTs like data Ordering = LT | EQ | GT Yes. `type Ordering = lt | eq | gt` You can look at Maybe example https://github.com/ptol/oczor/wiki/Haskell#types &gt;What are the semantics of import and include? I can't quite figure out what they mean. import is the same as in Haskell import std import std as s include is for module reexport include numbers Haskell equivalent is module Foo (module Numbers) std root reexport everything https://github.com/ptol/oczor/blob/master/libs/std/std.oc 
How about something like that ? https://gist.github.com/bartavelle/ccec9b8521c4988b2bf286d7fd9e46fa It is a bit more abstract, it's events and game state flags instead of actual characters, but I think it models what you are describing. What is missing is a way to check if an `Event` can be selected from the list, based on the conditions, so that you can have a list of acceptable events at a given time.
I think sort of - Criterion allows you to benchmark any number of comparable implementations, and with QuickCheck you could have a property that new = old for all inputs, just means you need to combine both with some wrapper API and maybe do some work on the reporting side, no?
Cool! I've been working on [servant-auth](https://github.com/plow-technologies/servant-auth), which also supports cookies, but takes a quite different route. First, it doesn't use `AuthProtect` and instead introduces it's own combinator. This is somewhat more flexible, and ultimately I don't think using the generalized auth combinator saves that much work. Second, it tries hard to abstract away the complexities of each individual authentication system, so that cookies, API tokens, passwords all are treated the same (modulo minor configuration). (This involves doing CSRF-protection work for cookies.) Thus, you can write your API once, and have both the browser and your API users consume it, with hardly any extra work. Third, and related to the previous point, it doesn't give you explicit access to the cookie you set on each request. Instead, what happens is this: each authentication method can decide if it succeeded. If it did, it returns some datatype 'usr' which is what your handlers get. Moreover, if it did, that's just serialized into a cookie that's set in the response, and which (if the client is a browser) you'll get in the next request. Thus, refreshing cookies and signin are conceptually the same thing! 
The main problem with this is that the StateFlag is going to grow absolutely _ginourmos_ because of the hundreds of characters in the game and all the different things they can do. However, I do really like this solution the best for its clarity. But again, I think it isn't scalable.
It will not be that big a set, even with 1000 flags in it. The Mealy machine solution from /u/Drezil might be nicer, I'll ponder on it a bit (I played the game a bit, it's nice!).
Here is a distinct solution that documents the dragon / smithy / knight interaction. It is not as clear as the other, but hopefully more scalable (and terrible to debug too !) https://gist.github.com/bartavelle/a3850f547d5bf92802b19dfa86eff478
It has now been sold. 
Maybe PaaS can afford to run some degree of instrumentation at production, but conventional software is on a tight spot there. We use Criterion and QuickCheck for benchmarking and testing several components at ShimmerCat, but we would never ship something like Scientist on production code ran by our licensees, mainly because of the performance hit. It is a cool thing to use though in test suites and coverage runs, and I would imagine that a Haskell implementation can use template Haskell to make a lot easier to entangle and disentangle the Scientist construct from the code.
I'd say if you actually use SDL2 for rendering, it's fairly high level (but very restrictive). A common use seems to be to provide an OpenGL context plus all the other stuff a game needs that isn't graphics. And only the audio seems very low level. I'm not saying it's a game engine, but for something like flappy bird (sorry), a game engine seems a lot like overkill to me, mostly doing things you could do just as easily directly in SDL2 (and without even needing to use OpenGL directly). Then again, I don't really know the first thing about SpriteKit, or what is considered a minimal feature set for a game engine. Based on the name "SpriteKit" I'm guessing "sprites" are a major feature, so I'd guess SpriteKit provides a simple scene graph (with redraw, hit-testing, collision detection, maybe simple physics?) but otherwise not much more than SDL2? 
That gives me the same problem, though. `Images.hs` is in Draw/Core and if I write `import Core.PixMap` it looks in `Draw/Core/Core` and that doesn't exist, but `PixMap` is not the proper name of the module. Can I override that or something? I'm not using Cabal or anything, btw. Just GHCi in Emacs.
Very pragmatic approach to solving some of the major irritations of real world haskell development. Thanks for putting this together.
Why would you ever deal with Swift if you can write it in react-native? I haven't heard any good things about Swift. I like how these guys describe using standard iOS development tools, "it's like going back in time": https://youtu.be/r5OPRhelEIU?t=2m12s For a webapp you can choose reflex-dom, if you want to stay in Haskell world. If you want native mobile, I would go with react-native. It's also very attractive to try to just wrap a webpage on mobile, but, in the end, you often loose more time (by trying to optimize the shit out of it and trying to give it native look and feel) than you save, no matter what hybrid route you choose. And I definitely don't recommend GHCJS + hybrid on mobile, because trying to make hybrid as robust as native on mobile can be a challenge by itself, even with hand optimized JS. But at the same time you can't escape the shitteness of javascript. Technical dept is something that you'll have to deal with if you choose plain javascript. But I would try react-native with plain javascript to see how it goes and whether react-native suits you, then perhaps explore and migrate to options that a) have a type system and designed in mind to compile to javascript b) have react-native bindings and c) have reasonable performance. Haven't tried it yet, but this caught my eye: https://github.com/hoodunit/purescript-react-native-todomvc/blob/master/src/Main.purs I don't know how fast is purescript going to be on mobile, but it should be faster than GHCJS. Probably some parts could be optimized through FFI if needed. 
Thanks a lot; I will have a look into PureScript and the react-native binding. And yes: using Swift feels like going back in time somehow ... 
If your code builds with GHC 8, you can try the `dedupe` branch from github and pass the `-dedupe` flag to the compiler when linking. Code size reduction generally seems to be between 40-50% for larger apps like this. Unfortunately internet connectivity is rather poor at ICFP, but I intend to merge this into the `ghc-8.0` branch soon, since I don't know of any outstanding issues.
[This](http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/) has a fairly large sections on compiling pattern matching. Not sure if it exactly what you are looking for. (Not OP)
Do you know if there's any examples of auth in servant that use a database connection or Reader monad around the authentication functions?
You'll want to decide if you want an *open* or closed solution. Closed types are easier to work with, but are more difficult to extend. A closed solution will generally use a sum type, while an open solution will use a type class. For a closed type, you might want to say: data Character = Dragon | Grandma | Thief | Smith whereas with an open type you might say: class Character a where -- character operations go here data Dragon = Dragon Text instance Character Dragon where ... Now, you can *start* with a character based representation. But it seems like an *event* based representation makes more sense to me. The basic summary of the game seems to be: Given the past history of events and decisions, select a random event to come next, and have the player make a decision. Push the event/decision to the pile, update some state, and loop. type Game = ... -- Some monad that does state, randomness, etc. type History = [(Event, Decision)] type EventGen = History -&gt; Game Event randomEvent :: EventGen randomEvent = undefined promptForDecision :: Event -&gt; Game Decision promptForDecision = undefined mainLoop :: History -&gt; Game () mainLoop history = do event &lt;- randomEvent history decision &lt;- promptForDecision event mainLoop ( (event, decision) : history ) This doesn't do much to lift correctness into the type system: there's nothing here that says you can't return an incorrect event based on the value of the `History` type you've been given.
For a more complicated example, you might write a type family/GADT like type family Precondition event :: '[k] data DragonTribute = DragonTribute -- event details? type instance Precondition DragonTribute = '[] data SmithySword = SmithySword -- event details?? type instance Precondition SmithySword = '['DragonTribute] This says "An event is going to have a type level list of preconditions." and the various `type instance`s are specifying the preconditions for an event to occur. The next thing is a GADT which constructs the event history: data EventHistory events where Empty :: EventHistory '[] EvCons :: Subset (Precondition event) events =&gt; event -&gt; EventHistory events -&gt; EventHistory (event ': events) The `Empty` constructor constructs a value of type `EventHistory` where the `events` type parameter is `'[]`, an empty type level list. The `EvCons` constructor takes two things: like a regular list, it takes a value of type `event` and a value of type `EventHistory events` and returns a new `EventHistory` with the type of the event kept in the index of the list. The trick is that we require GHC to satisfy the constraint `Subset (Precondition event) events` on these two types in order to construct the value. So we could statically write (note that the list is in reverse chronological order): story :: EventHistory '[SmithySword, DragonTribute] story = SmithySword `EvCons` DragonTribute `EvCons` Empty and GHC is fine, since we've specified that the preconditions for `SmithySword` require a `DragonTribute`. However, we would *not* be able to write: badStory :: EventHistory '[SmithySword] badStory = EvCons SmithySword Empty Why not? Well, let's check out the constraints of `EvCons`, with some specialized types: EvCons :: Subset (Precondition SmithySword) '[] =&gt; SmithySword -&gt; EventHistory '[] -&gt; EventHistory '['SmithySword] We can *reduce* the type family `Precondition`, since we have a definition for it. The definition says that it is `'['DragonTribute]`. So the constraint is now: EvCons :: Subset '['DragonTribute] '[] =&gt; ... which GHC will fail on. Where does `Subset` live? Well, wouldn't that be a fun type family to write? :D
Generics often make the compiler give up and you end up having to break apart expressions and add a bunch of explicit types to help it along. I've seen it just outright crash a few times whereby you have to unwind your changes to see what caused the crash. It appears the Equatable protocol is fudged in for tuples and the collection types as methods that ask for the parameters to be Equatable don't work with those types. If you have "a + b + c +..." that puts the compiler into a tailspin and it'll take minutes to compile that one line of code. Appending arrays has the same kind of issues, so you end up creating a mutable array and pushing the arrays onto that. I've seen issues where I had to clean and rebuild completely because the incremental compiler didn't spot that I changed a field from say Int to String and it gives me a garbage executable that crashes.
That was indeed a really tight team at Fynder (the start up Kris mentions at the beginning). AMA :-)
If there are 300 Characters and each affects 5 things about the worlds state besides money/happines even that amounts to "only" 1500 flags. At least from a performance point of view that should be neggible especially as you would only do a few checks on the state every turn. 
Hm, when writing up an example I realize I made a mistake! When that's fixed I'll include an example with DB in the documentation.
Came here to make the exact same comment. Brilliantly delivered talk and request more info on why you rules out "Haskell all the way down"
We have GHCJS for our internal dashboard with some graphs and other stuff where we wanted to share as much code from backend as possible. Our current biggest problems is that it takes about 20 minutes to compile our project if you change a single line in one of main packages, and that's a real killer. [Issue is there](https://github.com/ghcjs/ghcjs/issues/449), but who knows when will it be resolved (hopefully soon). Otherwise, pretty happy so far, using the Reflex framework.
Great talk. Is that generic code released somewhere on Github to use with Elm? Would be a very good argument towards checking it out!
What a horrible mess. Is it Swift 3.0?
This has been Swift 2.2 granted, but 3.0 has only gone final in the last week.
Hi Luite, do you describe somewhere what that `-dedupe` flag does? Deduplication to me sounds like something orthogonal to removing _unused_ code.
What have you used for doing your graphs?
GHCJS already does split-objects style linking, which means that everything that gets linked is reachable from the `main` action somehow (even though they may never be actually used, since some branches may just never be taken). This is done on a per function level (actually the unit is a binding group, which may be slightly bigger, but that's just an optimization: these are chosen such that the result is always exactly how it would've been if it was done on a per function basis) But due to various reasons, code that is functionally the same may show up in multiple places in the output. Think of top-level string constant for example. If multiple modules bind the same string to a top-level name, that code is going to be included multiple times (assuming both values are reachable, through different paths, from `main`). Dedupe is a link-time optimization that identifies some classes of this kind of duplication (by computing and matching hashes) and removes it by choosing a single instance of each duplicated piece of code, and updating all references to it. It's now merged in the `ghc-8.0` branch. 
Packages have to be manually added to stackage. It doesn't mirror all of Hackage.
Yeah, but that's not always what you want! Let's say the user has some state in the UI that's not yet synched to the server. You'll still need to keep an old API around otherwise that data is lost.
In order to add a package to Stackage, the package maintainer has to agree to perform, well, regular maintenance. Not everyone can offer reasonably prompt maintenance. For example, that's why my packages are not on Stackage.
Thanks for the explanation!
Yeah, I saw that too, but depending on `hxt` would indeed be weird...
Check out the [maintainer's agreement](https://github.com/fpco/stackage/blob/master/MAINTAINERS.md). We prefer if package authors are also Stackage maintainers, but it's not required. The responsibility you're taking on by being a maintainer is receiving bug reports and working with the author to get them resolved. If a package isn't being actively maintained, and a bug is discovered, worst case scenario would be that it's simply dropped from Stackage.
Yes, we're using RxSwift and that's very generics and closure heavy, both of which are prime candidates for the compiler to throw its hands in the air.
Are people OK with Hackage including a bunch of things that the uploader doesn't feel very strongly about maintaining? I have no objection to what you're doing and it means there is more stuff out there for people to look at and possibly use. However I feel bad about uploading things to Hackage that I won't maintain. Sometimes people will complain that it doesn't build anymore. I have not uploaded some of the most useful software I have ever written just because I'm not going to document it and be expected to maintain it. People point at packages like that and say "oh, see, Haskell packages are poorly documented and broken." So if I have something I wouldn't maintain on Stackage, I don't upload it to Hackage at all. I don't think that's the right approach and if anything I'm sad that people complain about the state of Hackage or packages on it when it has a bunch of free stuff. Hackage should not come with quality or maintenance obligations.
Mutation testing is cool indeed. We wanted to use MuCheck to improve the mutation tactics of our tool, QuickFuzz (also presented in the Haskell Symposium this year), but we found the implementation of MuCheck was not good/robust to deal with complex packages like Juicy.Pixels or asn1-type because it works parsing files one by one..
I ran into the exact same problem, how to define Arbitrary instances of a recursive data type, just a few days ago. Looking for a solution, I stumbled upon a [StackOverflow recommendation](http://stackoverflow.com/questions/15959357/quickcheck-arbitrary-instances-of-nested-data-structures-that-generate-balanced) for the [testing-feat](https://hackage.haskell.org/package/testing-feat) library. After I overcame the usual initial problem of the lack of tutorial, I've been fairly happy with it. Can anybody compare testing-feat and generic-random? I don't think I'm switching at this point, but this is probably not the last time I'll run into the same problem. 
Sure, the library's here: https://github.com/krisajenkins/elm-export
Just SVG, looks something like this: https://i.imgur.com/fmLqjZr.png Took a day to implement that abstraction which draws time-series graph, but it's straightforward and it was clear that it'll result in some "minimal working product". In future we plan to try diagrams.
&gt; Hackage should not come with quality or maintenance obligations. When people type "cabal install foo" and the build fails, they complain about "cabal hell" and this makes Haskell look bad. So yes, uploading to hackage absolutely should imply quality and maintenance obligations because it directly impacts the public perception of the maturity of Haskell.
Wow, thats really impressive! So its generating functions as test cases? Cool! 
There's also the issue of name squatting as happening in the Rust and JavaScript package indexes. This is a hard one, because NPM and Crates.io have pulled stuff to make room for a name, but that's risky for users of packages whop will now get something totally different worst case or package-not-found best case.
Ask you anything? Erm...when's the next reunion? ;-)
(disclaimer: I only share an office with Rudy, I haven't read the implementation either) Say you are testing `sort :: [Int] -&gt; [Int]`. The mutants are functions of this form: sortMutant :: [Int] -&gt; [Int] sortMutant xs = case xs of pat1 -&gt; answer1 pat2 -&gt; answer2 pat3 -&gt; answer3 _ -&gt; sort xs ie, it generates a list of exceptions, and calls the normal `sort` function for all other cases. This is different to normal mutation testing, which actually modifies the function being tested: this is more of a "black box" approach. In normal property testing (eg, quickcheck or smallcheck), you only check the properties as given. FitSpec checks the properties as given, and *also* with mutants substituted in for the function-under-test. You can see how this works in the example in the README: the `properties` list doesn't reference the normal top-level `sort` directly, it takes `sort` as a parameter.
He just presented this a couple days ago at icfp. It looks like really good work:; I immediately sent the github url to my colleagues at work. He also gave a pretty wicked lightning talk where he built a tiny but complete property checker in like 29 lines of haskell.
&gt; You can see how this works in the example in the README This is what I was referring to in "one can imagine some scenarios based on the example given".
Not sure if it is what you're thinking of, but there is an old Google Tech Talk that gets into djinn near the end. https://www.youtube.com/watch?v=h0OkptwfX4g
What sort of level of difficulty would you gauge their expectations at? I am really interested in what they do and am thinking of applying for a summer internship 
I think there's a middle ground. I might do a good project but not test it on anything but linux (as all my projects are). Or I might think the project is useful but not want to commit to several years+ of maintenance.
We should get right on that!
I was thinking the same. `MVars` &amp; masking exceptions I guess... EDIT: Actually there's `modifyMVar`.
It's used in a function that maps nodes of an AST to `IO` actions. A node may call the actions of its children. Some nodes need local state though. That's why I can't map to e.g. `StateT IO`, because then it'd be one giant state. I thought that was a perfect fit for arrows, but it got more complicated than just using `withLocalState`. I may rethink that decision, because I'll have to deal with thread safety now.
Hi everyone, I'm the guy who gave this talk. If you have any questions or suggestions on how to improve this talk please let me know in the comments below.
Yes, thanks! I'll edit it.
I'm not sure I understand the argument in this particular case. That's the exact same error message I get when using Prelude.++ (at least in 7.10), so it wouldn't be worse.
Maybe the problem has to do with the quality of errors, not the Prelude. Also this argument barely holds water now that FTP is in place. That being said, migrating ++ to &lt;&gt; is not backward compatible. It will break code that uses ++ to distinguish between what would otherwise be any ambiguous type variable.
The compile time type system is sufficient for all cases where two different proxies need to be distinguished. It's impossible to use that type information at runtime, so there is no need for a runtime representation of the types (and in fact, there is none). The proxy does need some runtime representation though, since it could be `undefined`. If you ever need to distinguish two proxies' types at runtime, it must be done with some type level constraint, such as `Typeable` or `a ~ b`. Constraints ask the compiler to pass along dictionaries with required information, meaning the runtime representation of the types comes in the form of functions being passed around; so still no need for the proxies to include any unique representation.
`Proxy` can be `undefined`, so it has to be represented somehow. Plus, since it isn't unboxed, you can give it to polymorphic functions / data types, which expect it to be a real pointer to some heap object. So it must have a representation for that. That said, I have wondered if this is an area GHC could optimize. That is, if a function never uses one of its arguments in any way (as is common with unit types like `Proxy` and `()`), it could optimize that parameter out of the function, removing the runtime representation. But then again, how much is such an optimization even worth? Since a unit constructor is constant, GHC represents them with a single heap object, and every usage is just a pointer to that. So removing the need to pass it around really just keeps you from having a pointer on the stack. Doesn't help much. If you want to save this memory, you're going to need an unboxed type no matter what.
I'm curious why you thought it was a good fit for arrows? I've had a hard time finding instances where arrows do something better than monads or applicatives.
It gets even nicer if `FlexibleContexts` has beed enabled: &gt; [1,2,3] ++ 4 &lt;interactive&gt;:5:1: No instance for (Num [t0]) arising from a use of ‘it’ In the first argument of ‘print’, namely ‘it’ In a stmt of an interactive GHCi command: print it 
If there really is such a need for a Prelude with more special cases to hold the hands of beginners, why not have introductory texts hide the ordinary Prelude and rather import TutorialPrelude? Otherwise I agree that the problem lies in the error messages. Rust for instance has something like a tiny tutorial for a given error if you ask it to explain, e.g. `rustc --explain 1`
Awesome talk, good idea with the pseudo-live coding too. Has the "open heart surgery" video been put up yet? Could we get a link?
Sorry, I should have made that more clear. I had been using GHCi all day with no problems, then later in the day I compiled and linked an application, again with no problems.
Forgive me, but isn't it a single heap object _per type_? `Proxy :: Proxy Int` and `Proxy :: Proxy Bool` will be pointers to different heap objects, I think
I don't think so. It should be one heap object *per unit-like constructor*, regardless of type variables. Unit-like constructors can always be represented at runtime with the same heap object because the static type information is no longer present.
I thought Haskell was the first popular language to experiment with FRP, or does reactive mean something different in this case?
Hi there - oops, that shouldn't happen, thanks for mentioning! The demo is very early and definitely not production ready :) - the node tty wrapper we're using has been a bit temperamental and uses websockets, perhaps they are being blocked? Will look into it.
Story of my life when it comes to Haskell. 
I would rather have "map" renamed to "mapList" and "fmap" to "map". This way, beginners will still have their readable errors (and get the hint that there's more to map than just lists).
With TypeApplications, you can write "fmap @ []" to get the thing specialized to lists. Maybe that's better than `mapList`.
[removed]
[Reactive has become a new buzzword that has essentially nothing to do with FRP](http://www.reactivemanifesto.org/). I think it's just a fancy way to say "stream processing"
Reactive doesn't mean FRP in the JVM world. It is a fluid term that can mean many things. In this case, it means ([from here](http://docs.spring.io/spring-framework/docs/5.0.0.M1/spring-framework-reference/html/web-reactive.html)): &gt; In plain terms reactive programming is about non-blocking applications that are asynchronous and event-driven and require a small number of threads to scale. I think that's the original marketing for node.js :)
Ah ok, I agree then. Sorry I misunderstood.
I am referring to bottom as the other value. 
This then frees up &lt;&gt; to replace /=.
It does look really cool!! I haven't dug into how this library does it, but you may be interested in QuickCheck's [CoArbitrary class](https://hackage.haskell.org/package/QuickCheck-2.9.2/docs/Test-QuickCheck-Arbitrary.html#t:CoArbitrary), which demonstrates a rather clever way of generating random functions.
Okay, forget about beginners. Getting that error message for that mistake is not something I want for myself either. We're talking about things that make Haskell a little more crappy to use, and the appeal to beginners is meant to bring up situations where the last bit of added "yuck" could be what drives someone to give up. But the complexity is bad everywhere. The appeal to beginners is just rhetoric. (Of course, there are benefits here, too. Not denying that, and I'd actually be very mildly in favor of this change. Just pointing out that there is a real disadvantage here that isn't solved by putting up a "no beginners allowed" sign. There are other possible solutions, such as GHC's new facility for custom error messages attached to instances.)
I do have a suggestion. The talk goes to great lengths to make the talk accessible, which is usually great, but at this venue, is it worth it? I would expect the attendees of the Compose Conference to already know all about Either, Maybe, functional programming and so on. And conversely, as an attendee of an FP conference I would expect a talk on type-based refactoring to demonstrate an interesting, extra-hairy case, not this intro stuff. Just the open-heart surgery stuff please!
&gt; there is a different fixity for (&lt;&gt;) than (++) Oh my. &gt; and folks mix (++) on lists of Docs with (&lt;&gt;) on Docs when doing things like working with pretty printers quite a lot in existing code. Well I guess that makes sense then. Things like this make me lean slightly towards the "infix operators are bad" camp, though.
Yep, absolutely! We'd love to provide for remote at some point but as we're so early we feel there is incredible value in having everyone together at the moment. Hope for that to change over time as there are so many great Haskell developers scattered across the globe.
You might want to have a look at [classy-prelude](http://hackage.haskell.org/package/classy-prelude) which is a replacement for the standard prelude (used with the `NoImplicitPrelude` extension) it redefines `(++) = mappend` as well as `map = fmap`and other nice things like type classes for Maps and sequences to make it easier to lookup keys and index them. For instance `lookup` there has the type `IsMap map =&gt; ContainerKey map -&gt; map -&gt; Maybe (MapValue map)` so you can use it on any map-like structure (`HashMap`, `Map`, `Eq a =&gt; [(a, b)]` etc)
I would love that so much.
It was actually a screen cast demoing how Djinn works.
Glad to hear it's worked for your team. My feeling (and could be wrong) is that as an early-stage company, where requirements are often unknown/rapidly changing, the benefits of working side-by-side are worthwhile. YMMV, and tbf we do work remote a couple of days each week.
How much Haskell is in your stack?
I would normally have expected that too. Unfortunately, the FP community in Melbourne is small and in order to attract participants they tried to make it very beginner friendly. I would do a different talk for intermediate/advanced FP afficianados
I'd be really interested in seeing that if you can open source it! 
Or me. Typing @ and [] is more effort than List.
Visual Basic was the bomb. `On Error Resume Next` &lt;- IgnoreT error handling.
Something's wrong with your site. uBlock origin is blocking the whole page.
I've been doing some frontend (Web) stuff recently ... it's a lot easier if you can do it with ES2015 + Flow, or TypeScript.
I... I kind of want to try it, just to feel the pain.
I mean the resulting type of the combinator still has to not have constraints or concrete types. So no... Most functions are still NOT combinators.
I don't think you really understood, then. If data types and type classes are passed in as church encodings rather than ordinary concrete encodings, then the function magically becomes a combinator, at the expense of being a little more cumbersome to use. But this function would be completely isomorphic to the "non-combinator" function, thus putting into question whether that function really wasn't a combinator to begin with.
Surely candidates from the UK and/or continental EU would be able to be onsite frequently enough to contribute to that value? I'm only poking you with a stick because I think you'll miss out on so many good candidates!
I love your philosophy. Currently a student i sweden, but I will keep an eye out in the future.
Err, I think I totally screwed up my english in the previous post. What I meant is that less-maintained packages are generally never *as* dependency for another package. In other words, no other package is likely to depend on a less-maintained package. I sure avoid depending on packages that have no haddocks, for example. 
Compiled Heist? Interpreted Hamlet? What a great time to be alive in!
&gt;NixOS 16.03 only provides very outdated lts's It looks like it does provide [ghc801](https://github.com/NixOS/nixpkgs-channels/blob/nixos-16.03/pkgs/top-level/haskell-packages.nix#L113) though, which is what lts-7.0 needs. Instead of using `haskell.packages.lts-x_x.ghcWithPackages`, you can use `haskell.packages.ghc801.ghcWithPackages`. With a nix-shell like this, I can compile a package with `resolver: lts-7.0` by doing `stack build`: nix-shell -E 'with import &lt;nixpkgs&gt; { }; runCommand "dummy" { buildInputs = [ (haskell.packages.ghc801.ghcWithPackages ( self: [ self.zlib self.stack ] )) ]; } ""' I'm on NixOS unstable, and with this, I get stack 1.1.2 and ghc 8.0.1. I don't know if this is what you're supposed to do going forward, after the lts stuff is removed from nixpkgs, but installing ghc like this and letting stack use the system ghc makes it possible to compile stuff. If you want it in your `.nixpkgs/config.nix`, install with nix-env -iA nixpkgs.my_ghc: let ghcCompiler = "ghc801"; in {pkgs}: { packageOverrides = pkgs : with pkgs; rec { my_ghc = haskell.packages.${ghcCompiler}.ghcWithPackages ( self: with self; [ zlib terminfo unix-time digest stack ]); }; } 
Sorry to hear that. Granted, we can't compete with the salaries offered by financial firms that often hire functional programmers, however I believe are competitive for outside that domain. Please also note that salaries at early-stage startups are generally lower out of necessity and are instead offset with equity. The skillsets are just a recommendation as we have quite an ambitious end-goal, however if you're interested, please do get in touch!
You could always just standardize around ⊕ from [Data.Monoid.Unicode](https://hackage.haskell.org/package/base-unicode-symbols/docs/Data-Monoid-Unicode.html). :troll:
I think the use of `void` as an example of the bottom type is quite misleading. In C-like languages, `void` is better associated with the unit type (`()` in Haskell), so that functions that receive or return `void` such as int f1(void); void f2(int); will correctly translate to: f1 :: () -&gt; IO Int f2 :: Int -&gt; IO () as expected.
What is 4x10 remote schedule?
Very cool, thanks for the upfront response :D
Oh my, that's quite the type: λ&gt; :t (uncurry *) (uncurry *) :: Num ((a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c) =&gt; ((a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c) -&gt; (a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c param1: ((a -&gt; b -&gt; c -&gt; (a,b) -&gt; c) param2: (a-&gt; b -&gt; c) param3: (a,b) result: c I have a hard time coming up with a useful function that is the type of param1. Here's a useless one: λ&gt; :t \z-&gt; \y -&gt; \x -&gt; (+ x) . fst &lt;interactive&gt;:1:2: Warning: Defined but not used: `z' &lt;interactive&gt;:1:7: Warning: Defined but not used: `y' \z-&gt; \y -&gt; \x -&gt; (+ x) . fst :: Num c =&gt; r -&gt; r1 -&gt; c -&gt; (c, b) -&gt; c 
FWIW: the below works perfectly well: import Data.Void putStrLn' :: String -&gt; IO Void putStrLn' s = undefined &lt;$ putStrLn s putStrLn'' :: String -&gt; IO a putStrLn'' = vacuous . putStrLn' main :: IO () main = do putStrLn' "foo" putStrLn'' "bar" -- For some reason, the final value is analysed... pure () --- And you cannot do in C: void x = f2(10)
Ahh yes, that makes sense. But while they might not be depended upon, they can still contribute to cabal hell if someone is looking for something, finds a less-maintained package that looks like it might be what they want, and tries `cabal install less-maintained package`.
`filter even list`
param1 has type ((a -&gt; b -&gt; c) -&gt; (a, b) -&gt; c) and not (a -&gt; b -&gt; c -&gt; (a,b) -&gt; c) right? If so uncurry has exactly that type... 
Are you allowed to use even function and filter higher-order function, or is this an exercise to implement your own? evenList :: [Int] -&gt; [Int] evenList = filter even -- point-free style
thank you!
The arrow keys for progression are oddly-bound - AFAICT it's "right" to advance through the points on a slide, "down" to advance past the end of the slide. I think sqrt is included as a counter-example. Some of the points aren't clear from the slides (I didn't attend the presentation myself) so I'm hoping a video will be posted soon.
Void in C is ~~the bottom type~~ kinda sorta the bottom type though, because you cannot construct a value of type void. In Haskell `()` has one member, `()`. C doesn't really have anything like Haskell's unit type. The confusion lies in the fact that using void to mark functions as not returning anything is the same syntactically as how you mark functions as returning real types. At least in C++ you can get into the habit of writing `void foo()` for "procedures", and `auto bar() -&gt; int` for actual functions. EDIT: As mentioned below, void in C++ behaves more unlike the bottom type than like it. It's a incomplete type in C++ and using types to reason about it will only get you so far.
Yeah, but it's empty :/ **Edit:** Oh, it's because of uBlock Origin blocking some of the js files served by microsoft. A workaround is to completely disable javascript for the page (e.g. using uMatrix).
I'm supposed to take in 2 lists, then get all the even integers from the second list and append it to the first
Sure. The trick is finding something that enough people consider is universally as an improvement. =)
The presentation source is available in org-mode here: https://raw.githubusercontent.com/elbeno/using-types-effectively/master/presentation.org
One thing which wasn't clear (to me at least) from the slides was how phantom types helped with correctness in the example given. The org-mode notes make this clearer: https://raw.githubusercontent.com/elbeno/using-types-effectively/master/presentation.org #+BEGIN_NOTES User input is born unsanitized. It is impossible for us to execute unsanitized input. The compiler simply won't compile it. We've used types to help enforce the business logic. This is something similar to a strong typedef, or what enum class effectively does for integral types. This technique can also be used e.g. in a units library. #+END_NOTES
That's an ad blocker, right? You should be able to just whitelist the page. It doesn't have ads.
To be clear, `uncurry *` is not a complete expression on its own. But `(uncurry *)` is. This is a special bit of syntax called a "section", and the parentheses are part of it. See https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-300003.5 Just wanted to point that out, since everything said so far could leave people thinking that somehow `uncurry *` ought to parse as a simple expression on its own. It doesn't.
It's actually ((a,b,c) -&gt; (a,b) -&gt; c) -&gt; ((a,b,c) -&gt; (a,b) -&gt; c) See? Two identical types. Because `(*) :: a -&gt; a -&gt; a` (ignoring the `Num a` constraint). We fix the type of the first argument of `(*)` (to the type of `uncurry`), and naturally the second argument and the result must be of that same type. Of course we cannot ignore the `Num a` constraint. `uncurry` doesn't belong to `Num`, and any attempt to actually use `(uncurry *)` results in an error: Prelude &gt; const 42 (uncurry *) &lt;interactive&gt;:9:19: Could not deduce (Num ((a0 -&gt; b0 -&gt; c0) -&gt; (a0, b0) -&gt; c0)) arising from a use of ‘*’ from the context (Num a) bound by the inferred type of it :: Num a =&gt; a at &lt;interactive&gt;:9:1-20 The type variables ‘a0’, ‘b0’, ‘c0’ are ambiguous In the second argument of ‘const’, namely ‘(uncurry *)’ In the expression: const 42 (uncurry *) In an equation for ‘it’: it = const 42 (uncurry *) Edit: words 
Finally!
Many beginners will come from Java, C#, C++, etc. Those languages have such type application (`Foo&lt;List&gt;`) so many beginners will already know this concept. Other beginners will need to learn this, but it is relatively a simple concept to learn, isn't it? (Compared with the rest of stuff you have to learn when learning Haskell). Remember this reduces the need to learn a lot of redundant names for things.
It looks like you're using stack in a project folder. Edit [projectname].cabal to add "split" to library.build-dpeneds. library hs-source-dirs: src exposed-modules: Lib build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5 , split 
Just to clarify, this is not an issue about a module hidden by a package. this is an issue about a **package** hidden by *cabal*. So it's not a hidden module, it's a hidden package :) 
Remember that you can use any operator as a "normal" function/identifier by adding parenthesis. So you can do `x + y`, but you can also do `(+) x y` ... using `+` as a normal function like `foo x y` instead of as an operator. 
In the future I think it would be better if we didn't just give people answers to questions that are obviously part of an assignment.
A more complete explanation: Stack is a build tool that tries to be consistent. Running `stack build` today should give the same result as running it tomorrow. As part of this, stack needs to know exactly what the dependencies for a project are, and exactly what versions it needs. Stack will make sure that dependencies matching the requirements laid out in [projectname].cabal are met. You tried to import a module from a package that is not declared in [projectname].cabal. Stack realizes this, and gives you a warning that the module is 'hidden'. The cabal file must list all the dependencies so someone else can build your project from only the source, and dependencies. If you don't declare 'split' as a dependency, stack won't make sure that user has the correct version of 'split' installed, and the build would fail for them. [https://docs.haskellstack.org/en/stable/GUIDE/#adding-dependencies](https://docs.haskellstack.org/en/stable/GUIDE/#adding-dependencies)
For a type to be a true bottom type, you must be able to construct a sensible function `forall a . Bottom -&gt; a` where `a` is completely arbitrary. You can't do this in C++: template&lt;typename T&gt; T absurd(void) { // ??? } - You could leave it blank, but then this is 100% undefined behavior and therefore a violation of the C++ standard. Compilers will generally warn you if you attempt to do so. - It's tempting to do something like `return T();`, but then you are making an *assumption* that `T` is default-constructible, which need not be the case. - You could also put in a diverging function such as `abort();`. But then you are cheating, because diverging functions do return bottom! That is, any time you mark a function as `[[noreturn]]`, you have effectively changed the return type to `Bottom`. The fact that you cannot construct a `void` value is purely a syntactic quirk – it is somewhat "special" to the compiler. I would argue that `void` hardly qualifies as a real type, since you can't declare variables of type `void` nor can you put it inside a struct. There's only a few cases where you can actually use a `void`-typed value: - You can `return` a `void` value in C++: void f(void); void g(void) { return f(); } - You can discard a `void` value using the comma operator `,`: int i = (f(), 1); - You can cast anything into a `void` value (you could say this is one way to "construct" `void` values): (void)0; 
&gt;Void in C is the bottom type though, because you cannot construct a value of type void. It's a shame; it really should be the unit type. It doesn't make so much difference in C, but I've lost track of the number of times I've had perfectly reasonable generic C++ blow up because `void` is, for no apparent good reason, an incomplete type. For example, given: template &lt;typename F&gt; auto time(F&amp;&amp; func) { auto start = high_resolution_clock::now(); auto result = func(); auto stop = high_resolution_clock::now(); unsigned us = duration_cast&lt;microseconds&gt;(stop - start).count(); std::printf("Took %u us\n", us); return result; } you can write: int rv = time([] { std::this_thread::sleep_for(1s); return 42; }); but not: time([] { std::printf("%d\n", -1); }); 
Void in C should be isomorphic to Haskell's void, but they got the semantics wrong for pointers. I have spoken with C++ experts, and void is totally broken and causes a ton of problems for template metaprogramming.
The semantics of C `void` and the semantics of Haskell's `Void` are totally different. See https://www.reddit.com/r/haskell/comments/549sfi/using_types_effectively_cppcon_presentation/d80raax
Sounds like you're looking for an Operations Research job. There are definitely people hiring in that area. 
Strictly speaking C the fact that C has a type named "void" which doesn't correspond to type-theoretical uses of "void type" doesn't mean that it's "using the void type incorrectly".
Yeah, he just needs to make it a little bit less obvious. In general, on the Internet instead of asking "How to do X in Y?", you should just say "Z (Y's competitor) is better than Y because Y can't even do X". This way you get more useful detailed answers as well. :P
Better advice would be to add build-depends: ... , split &gt;= 0.2.2 &amp;&amp; &lt; 0.3 because unless you know that earlier/later versions have that hidden module as well, cabal is otherwise allowed to pick a different version of that package which didn't have that module yet/anymore.
Robotics. 
This doesn't explain why are packages 'hidden' in the first place, which is what confuses newcomers. Adding `split` to the build-depends section of the cabal file fixes the problem, but doesn't give a satisfactory answer either imo.
Thanks this is exactly what I needed.
Fair enough points. I guess you could say that void can act as a "kinda sorta" bottom type, but not always. Like all things in C++, it has its pecularities :-) As for your last point though, I would argue that since you are discarding the 0, there will be no in-memory representation. C++ defines objects as being a location in memory with a particular type, and so since there's no object, there's no construction of a a void value. But that's just splitting hairs.
Since robotics has already been mentioned: computational biology
Great library and presentation. Very cool stuff. Is it used somewhere already?
[State of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md)
I guess I meant more of the basics like how to work with strings language x == Haskell this and why. Not how to program and take OOP designs into Haskell.
Haaaaaaa
&gt; What is the biggest road-block for further development/adoption? Mid-level libraries, documentation, and discoverability. Frequently the answer I have to give to "can I do &lt;something non-trivial, but also not complicated&gt; in haskell?" is "probably, but you'll need to spend like an hour figuring out how."
The upside is that the increased attention is going to lead to more libraries being made. So I actually see this getting a lot better in the near feature.
In Scala using shapeless I believe you can define your error as a Coproduct of types and then write your functions generically in terms of the output error type by asserting that it contains the specific error with a typeclass and using that typeclass to lift the specific error into the arbitrary wider coproduct. Don't know if there's anything ready-made in Haskell that can accomplish this but I'm sure it's possible.
Everybody in the Haskell community is so eager to learn about new things that there's no time for fights. Even when a newbie starts a pointless thread about changing something fundamental about the standard libraries, everyone comes in expecting to gain new insights. It's beautiful. 
And quite lazy
Isn't that the idea of the Data types à la carte paper? Might be worth checking out.
Which paper are you referring to? Has it been implemented as a library?
I have reached the same conclusion. I have been exploring/using option 4 for some time by using an open sum type (a "variant"). The first element of the variant is the "correct" one (like Right for Either). See: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Utils/Flow.hs There are a lot of different operators to deal with the different kinds of compositions and to avoid type errors (ambiguous type inference, etc.). Then you can have generic combinators such as: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Utils/Flow.hs#L159 See an usage example here: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/System/Input.hs#L115
I have to agree on the intermediate resources area. It honestly seems easy to get to mediocre beginner stage, but crossing the seeming chasm from beginner to intermediate programmer is a hard slog. You can get there but jumping from beginner books to things like parallel and concurrent programming in haskell is... trying. I might be a bit sadistic however. Other than that the libraries may not be as mature (honestly not really found that yet), but the performance gain more than makes up for it IME. But your mileage may vary.
SPJ is absolutely brilliant, and we should be incredibly grateful for having him. He's a terrific hacker, and a wonderful human being -- the ultimate combination for a free software project.
I'm fairly new with learning the language, but started to run across it more and more in various podcasts and articles than before. I finally decided to give learning it a go after listening to an interview with the haskellbook.com authors on The Changelog podcast. This book is quite good and approaches things so much better than even books from other programming languages. A book like this seems to be what was lacking. Also there is a new startup course called LambdaSchool that was on kickstarter and indiegogo that is trying to address this lack of beginning content. I'm also enrolled in a free beginning course that the University of Glasgow is having a few professors do for free on Futurelearn.com. At least to me, it seems like there is a good effort as of late to get new folks in the language outside of academic settings. This was also an interesting article on using Haskell in a startup for a few years. http://baatz.io/posts/haskell-in-a-startup/
He's a very positive role model to be sure.
I stand corrected. But I won't touch `ghc-pkg` even with a very long, radiation-hardened pole
What do you mean by "mid-level libraries"?
Did you make this? This is great. Please keep it updated. I am new to Haskell and can use all the resources I can get.
It's not entirely clear any conventional type system would have saved the DAO. In order to create an extensible execution environment, the designers of the EVM created a calling convention that allows for user provided code to be injected into effectful function calls under many circumstances. The most common case is when the a contract sends native cryptocurrency to an account. Theoretically a type system that focused on linearizable effects could check if the invariants hold while compiling to EVM bytecode.
Yes it is except I think data types a la cart uses a higher-kinded coproduct
Yes, C is indeed consistent when compared to C.
Simon shows yet again why he deserves the respect and admiration he has.
You may also be interested in Boehm-Berarducci encoding, which is similar, but different for recursive types. newtype List a = List { uncons :: forall r. (a -&gt; r -&gt; r) -&gt; r -&gt; r } -- ^ `r` as a parameter is used to represent recursing on the type newtype Either a b = Either { either :: forall r. (a -&gt; r) -&gt; (b -&gt; r) -&gt; r } newtype Free f a = Free { unfree :: forall r. (a -&gt; r) -&gt; (f r -&gt; r) -&gt; r } Unfortunately, I too am sort of in a state of bewilderment, and have no idea what the underlying implications of these encodings are =P I do know that Boehm-Berarducci encoding can often be faster with recursive types than regular ADTs because it's just function composition, which is simpler than recursing down a data structure.
No worries, I generally agree with the statement. What I wish there were more of was examples of how to use things in most of the libraries. I think there is too much assumption about users of a library knowing how to just interpret type signatures into a working thing. Which basically circles back to my beginner to intermediate comment, it really makes it hard to feel productive in the language.
Also a distinguished academic, gifted speaker, and diplomatic leader.
There is fifth option: Checked exceptions https://www.well-typed.com/blog/2015/07/checked-exceptions/ or http://hackage.haskell.org/package/control-monad-exception
It was made by /u/tekmo
&gt; What I wish there were more of was examples of how to use things in most of the libraries. Very true. Even many good libraries suffer from this which I guess makes authors think they can get away with it. A cycle! 
Thanks for the comment. Regarding your answer on 1, I understand that it will generate a random set of lists. But quickcheck needs a Bool eventually as output right? So if I have ''mySort'' as function, how can I really test that those generated numbers by quickcheck are indeed sorted? Therefore, I assumed that I would have another sort function (e.g. sortTwo) and do something like this: test :: [Int] -&gt; Bool test xs = (mySort xs)==(sortTwo xs) (using 'quickCheck test' to run the code). But as you can see, I have here introduced another sort function. Now, is this normally the way to go?
Do you have any specific areas in mind where there are no good libraries? You might be able to inspire someone looking for a project :)
I think it's been reimplemented in variations in different libraries. https://hackage.haskell.org/package/compdata contains an implementation true to the original paper. I've toyed a bit with it in a hobby project and I think it worked well. However, had it been invented today I would have preferred it to use DataKind lists. I stumbled on a blog post that reimplemented bits of it using type level lists. Think is was here on r/haskell actually.
You may have found it already, but [Stephen Diehl's site](http://dev.stephendiehl.com/hask/) is another great resource.
I'm not sure I would say so. It's more comprehensive than LYAH but the main non-trivial thing it has is Parsers. It can feel hard to feel productive in Haskell if you're coming from another language, and I'm not sure this fully fixes the issue. (For a beginner, that looks like a great set of topics) The "write yourself a Scheme in 48 hours" is really good in that it shows you parser combinators (i.e. monadic parser combinators), which means you're using monads to do something that would have been impossible in other languages. (Unfortunately it is a bit outdated thanks to the pace of haskell development) The yesod book is also pretty good at pointing out how it uses type errors to guarantee (in many cases) that you don't run broken code live (i.e. something you couldn't do without Haskell). 
&gt;A lot of concurrent languages emerged: Rust, Go, Scala, Erlang... So the competition for attention is high! This is true! Haskell also doesn't have anything for GPU that's 100% stable. Repa works well but it's not fully fleshed out. &gt; However for me major problems are: &gt; Cryptic error messages &gt; I find the language sometime frustratingly hard. It's balanced by a good maintainability of the code written. Personally, the only time error messages were so bad I noticed/remembered was when I was using lens. They're quite informative usually - it gets you thinking functionally. I think whether you find writing Haskell hard is pretty subjective. I had some programming experience going in, but Haskell was the first language I did big/nontrivial things in. 
Thank you!
Not related to the scenarios you provide, but: in my experience, QuickCheck is often useful to test 'round trips' of data. Some examples: - Some code to serialize and deserialize/parse a data structure to bytes. If the data structure has an Arbitrary instance, it makes a lot of sense to validate that 'prop_serdes a = deserialize (serialize a) == a'. In a project, I use this to validate round-trip functions between ByteString and Vector Word8 (https://github.com/NicolasT/reedsolomon/blob/54a3bcb0b5642046d6deac3189bd74276ce39ca8/test/Vector.hs#L19) - I have some code which splits a given vector in a given number of equal-sized chunks (including padding if required), and another function to reconstruct the original (when the original length is known) by concatenating again. This has a fairly obvious 'join after split recovers original' property to check (https://github.com/NicolasT/reedsolomon/blob/54a3bcb0b5642046d6deac3189bd74276ce39ca8/test/ReedSolomon.lhs#L883) - In my Reedsolomon Erasure Coding library, there's a QuickCheck test to test whether, well, erasures can be recovered. It works by generating arbitrary data, an arbitrary (n + k) scheme, encoding the data, dropping an arbitrary number (up to k) of arbitrary resulting chunks, try to recover the data, and ensure it's equal to the original (https://github.com/NicolasT/reedsolomon/blob/54a3bcb0b5642046d6deac3189bd74276ce39ca8/test/ReedSolomon.lhs#L862)
The way it was worded it sounded like "C void and Haskell void are the same except for the semantics for pointers".
I am absolutely impressed by SPJ's take on this. See here. https://mail.haskell.org/pipermail/haskell/2016-September/024996.html However this cannot be the only thing that happens. He can't be the only one pushing for change. It is my belief that if we got a guy who is always positive and stays out of drama and always shines by example to get so disappointed in us that he has to start begging us to stop, it must mean we've failed as a community and fundamental change needs to be made. I strongly believe every member of the community should be pulling hard to achieve this; this is a turning point and we need to do something to start containing this sort of thing, especially before it starts climbing the ranks and goes all the way to the top. This is the wake up call, everyone. We need to make sure that in the future things like this don't bother people who are already spending most of their waking time to contribute to our community. We should have managed this drama long before Simon felt he had to get involved.
good point, thanks! in fact, implementation and specification of sort seem to be so close that it's hard to think of something better than equivalence tests for different implementations. but that's not too bad if you implement wildly different algorithms.
&gt; fundamental change needs to be made Like what?
Hm I don't really agree with you. I'm fairly confident that this email was a reaction to the discussion in the "contributing to GHC" email thread. I wasn't really involved in the thread, but my impression of what happened was that Christopher Allen brought up some points about what the Rust community does that he thought the GHC community should embrace. Several people responded to that email disagreeing with his points. Perhaps because he was being ganged up on by several people, he seemed to think that they were dismissive of him and of newcomers in general, and then accusations and name calling from both sides ensued. I honestly didn't feel like they were dismissive of him at all, but I suppose emails, or text in general, can typically be interpreted different ways. I can certainly see how uncomfortable it would be to have many people shooting down your ideas, especially when you think they are proven elsewhere. In general, I think that the GHC community has been stellar, at least in terms of politeness, and that this was really the first time I saw such a thing happen. Admittedly I've only been on the email list for a few months now, but I've only seen people be extremely kind so far, which was very important to me as I wanted to try contributing to the project. If anything, I would not expect SPJ to wait until things are bad to write an email but to do so at the first sign of trouble.
The Haskell community is by far, by very, very far, the most welcoming to people who are starting I've ever seen. Even the top guys that you see in videos, most of the time talking about things completely technical and far from what a novice can understand take their time to explain basic stuff to newcomers. It's almost like they relish in being part of that "eureka moment" of yet another initiate. 
&gt; Everybody in the Haskell community is so eager to learn about new things that there's no time for fights Why is SPJ reduced to begging people to stop abusing each other then?
Haskell is a great fit for it -- I've been using it for +1 year on blockchain systems. All of our tech is written in it: [kadena.io](http://kadena.io) I can't say that error messages are much of a problem for our DSL, though our system is at the extreme end of performance for this space (8k transactions per second regardless of cluster size) so our issues revolve around performance engineering more than anything. 
If your willing to give up strong typing on the `Left` values, something like the following might suffice: import Data.BiFunctor (first) renderLeftBind :: (Show e1, Show e2) =&gt; Either e1 a -&gt; (a -&gt; Either e2 b) -&gt; Either String b renderLeftBind x f = (first show x) &gt;&gt;= (first show . f) If the `Left` value is always an error condition and it always gets rendered to a `String` for reporting purposes, the above is certainly a quick route, albeit no where near as type-safe.
bring that hammer down
Indeed, and there's a lot more, stuff that I don't want to bring up in order to keep it out of my mind.
I don't know. The only quick answer I have is: changes that will prevent this sort of thing repeating itself.
I think Chris is making a bigger deal out the issue, judging by the directness of his writing, than it's really worth. As someone who's been recently looking to get my first GHC contribution in, the main source of friction I've observed *BY FAR* is just understanding GHC's architecture and codebase well enough to understand why a certain bug is in fact a bug, and more importantly what the appropriate way to fix said bug should be. Doing that will require significant time commitment and persistence on the part of the newcomer, and that's pretty typical for big, old, established open source project like GHC. I encountered a similar process while trying to get involved with the Open Morrowind project - confronting that upfront code complexity is what keeps new contributors out, the fact GHC isn't on Github isn't that big of a deal at the end of the day. If it was a medium to small scale library and/or project, then it might make a difference....but this is GHC we're talking about.
"I thinks it's officially an over simplification to talk about the community. For some time now we have communities." IMO this is a bit of a copout, there are too few people in total to make this claim. At most, you could say there's two broad communities - the "I want to make stuff" and "Doing category theory for decades" with some folks straddling both. Overall, growth isn't in decline, but I'd like to see it be on a higher velocity though.
Is Data.Bifunctor being used at all here?
Atom packages `ide-haskell + haskell-cabal-ide` (+ deps) seem good enough to me, though admittedly I don't use many IDE features. I used them for quite some time with reasonable success until I switched to Emacs for purely unrelated reasons.
"first" is from Data.Bifunctor. Should probably use explicit imports in code snippets like this
Working haskell coder here, and this is my point of view. I became a not-working haskeller for a while because of a cabal hell incident; the tooling caused a lack of faith and the haskell guy got the boot. But then stack saved my haskell career, and gave haskell a chance in the commercial world. GHC is a monopoly provider of compilation services. Services that are inside ghc are thus privileged, and the community should be sensitive to ghc development crowding out non-ghc solutions. That the opposite applies is what makes some working coders become disrespectful at times. So you can imagine the most privileged of ghc'ers then making a rare appearance here, lecturing the commercial community on decorum, just makes the power imbalances more stark. 
I agree with you. I made my first patch to GHC a few weeks ago and knowing what to actually change was far more difficult than learning any tools required to submit that change for approval. I can't imagine that anyone would go through all the work required to make a change and test it, and then decide not to submit it because they had to use Phabricator/Arc.
I'd just like to remark here that while my livelihood is fairly well tied to the language, I don't feel the need to press adoption to go any faster than it otherwise would proceed naturally. Examples of the manner in which the language is and has been effective should be marketing enough. I'm comfortable with letting the language stand or fall based on technical merit and fitness for purpose. I think Haskell really is quite good at a lot of things, and it should be quite capable of bearing this out in practice and making those who use it successful at solving a pretty wide variety of problems. At the same time, there is a significant up-front investment to be made in learning it. Haskell didn't get to be where it is by basing technical decisions on what would be most comfortable to the majority of programmers, and to some extent, that shows. That's not to say we shouldn't continue improving our tools, or that if the best decision would also be a popular one that we should avoid it, but I think putting the emphasis strongly on drawing in additional users is the wrong mindset. (Even if only because when you build the thing you want, you know it's what *someone* wanted, while if you build for an imaginary future user, there's no guarantee.) I say this while knowing full well that we need to be able to justify our use of Haskell to our clients, and that this would be an easier task to accomplish if it saw more popular use. Ultimately, if we can't defend our choices on their technical merits, what are we even really trying to do? Anyway, I say this just to contribute another perspective and maybe break up the dichotomy a bit.
I wish that it were as simple as one out-of-control email thread. But there is a much larger scope here. In a number of very different corners of the Haskell community lately, there's been a lot of conflict, often between the same kinds of personalities. I've been struggling recently to understand what's happening, but in the end, like all such issues, it is a lot of different things all coming together. For one, Haskell is now a fad of the technology industry, a major nexus of open source software, and also inherits a lot from the pure computer science academic community. Sadly, there are often very different standards of behavior between these groups. The respective admirers of Eric Raymond, Steve Jobs, and Edsger Dijkstra are in some ways doomed to be in a good bit of tension by default, which must be overcome with mutual respect to have a productive collaboration. (And I don't mean to imply those are the *only* sides to our community...) Another side to this is that the Haskell community, while it grows larger, feels more isolated than it has in the past. It is no longer the kind of place it was when I joined, and impulsively flew to the UK and spent the weekend as a new Haskeller chatting with Don, Neil, Lennart, Duncan, and Simon (both of them, if I recall correctly!), proposed and discussed a half-baked idea for a new Haskell language extension during breaks, and then followed up the day of casual talks with rowing on the River Cam. This loss was inevitable as the community grew, and many older Haskellers were just insanely lucky to be there when the community was more tight-knit. Today, the much larger Haskell community is mostly connected by knowing each other's IRC handles and Reddit usernames, and the flaws of human social graces in such settings are well-known. Finally, there's an element to the flare-up right *now*, that a lot of people just have very raw memories of recent events and hostilities. I haven't even been directly involved, but it remains difficult for me to give the benefit of the doubt to those who have been very hurtful to the community in the past. I respect Simon's request here, but I also imagine there are others who have been personally involved, and will find it even more challenging. We must accept that, as Simon mentioned too, this won't just go away, and it won't be an easy process. It is a long road to rebuilding mutual trust, and restoring the community that we *do* all want. All of this is to say, I suppose, a big thank you to Simon for bringing this up in that productive way in which he is uniquely talented. It gives me a lot of hope that I'd started to lose.
May I request you to elaborate a little more? My Haskell-fu isn't that strong yet.
Back before GADTs were introduced, you could use encodings like these to represent them. (E.g., the "tagless final" representation.) My favorite trick is introducing structural (co)recursion into a language with higher-rank polymorphism but no type or function recursion. -- recursion data Mu f = In { unIn :: forall a. (f a -&gt; a) -&gt; a } fold :: (f a -&gt; a) -&gt; Mu f -&gt; a fold f (In g) = g f in :: Functor f =&gt; f (Mu f) -&gt; Mu f in ms = In $ \f -&gt; f (fmap (fold f) ms) -- corecursion data Nu f = exists b. Nu (b -&gt; f b) b unfold :: (f b -&gt; b) -&gt; b -&gt; Nu f unfold = Nu out :: Functor f =&gt; Nu f -&gt; f (Nu f) out (Nu f b) = fmap (unfold f) (f b) -- example data ListSig a b = Nil | Cons a b type List a = Mu (ListSig a) data CoList a = Nu (ListSig a) If you restrict yourself to non-recursive functions, you can recreate most of the popular list functions on one or the other of these. `List` allows you to summarize the contents of a list (using `fold`), but you can only build finite lists. With `CoList`, you can create infinite lists (using `unfold`), but can only ever examine a finite prefix. Thus, all your programs are guaranteed to terminate.
&gt; Haskell didn't get to be where it is by basing technical decisions on what would be most comfortable to the majority of programmers, and to some extent, that shows. That's not to say we shouldn't continue improving our tools, but I think putting the emphasis strongly on drawing in additional users is the wrong mindset. Respectfully, I think haskell_caveman's narrative may be more accurate than yours. Not many people are suggesting Haskell compromise technical decisions for adoption. Note that FTP -- the biggest change pushed through recently -- arguably made `base` slightly less beginner friendly in return for being more principled and way better for everyone else. And on the tooling side, cabal-install only got sandboxes in 2013. That kind of thing definitely supports the "lack of a sense of urgency" he talks about. Somewhere between makefiles and [pick your perfect package manager] is an acceptable level of tooling, and Haskell definitely didn't hit it until then. That gives off lack of urgency vibes to me. I'm not complaining (I certainly wasn't contributing to Haskell then), but I think it's important to have an accurate view of what's been going on.
Ether looks good. Do you know how it's different from http://hackage.haskell.org/package/control-monad-exception ? All I could understand (given my limited knowledge) is that both of them use type-classes to achieve the "algebraic" properties that I'm trying to get at.
If you are an Emacs user then please ty Intero. I started using it this summer and love it.
[removed]
Oh, don't get me wrong, I'm also not trying to say that we are actually compromising technically here. I just see a lot more arguments from the perspective of what would be best for new user adoption than I'm really happy with. On the other point, having mechanisms to sandbox things is certainly great. Ironically, at the company I work for, we don't use cabal sandboxes, and we don't use stack either. We just use nix to sandbox the entire system environment. So there are other approaches. :) To be honest, we did actually somehow get along pretty well without cabal-install sandboxes for the dozen or so years I was using Haskell before they were introduced. How we got along before cabal itself was introduced in 2004 is perhaps more of a mystery -- for much of that time I wasn't really trying to do very serious practical things with it anyway, and I think the same holds of most people. Haskell only really started to get reasonable to try to use for real-world stuff around then anyhow, as ByteString showed up, and that quickly led to many other things that are fundamental to a lot of real-world use cases. The bigger improvements that made cabal-install not annoying to me were actually harder to name, because they had to do with its behaviour when it was about to do something that would cause breakage, and improvements to the solver. But while I had my fair share of cabal-hell, once you'd gone through it a couple times, a combination of being a bit more careful about installs, and learning a few tricks (e.g. installing multiple packages at the same time to let the solver act on the whole situation) reduced the pain to a fairly tolerable level. For projects of a certain size and seriousness, you'd just pin the versions of everything in use. Usually that was enough, even without taking further measures to manually isolate your package databases. I think the apparent "lack of urgency" in many cases was because there really were other more pressing things for the people trying to get stuff done to contend with. Once your build system, whatever it is, can compile your project, you usually have at least a dozen other things to worry about first before returning to make that part better. Even without sandboxes in the earlier days, Cabal really was fairly capable at being good enough. Of course there are things I would personally have liked to come sooner, and some things yet to come, but on the whole, the progression has seemed pretty natural.
You're almost certainly right about the critical mass aspect in some cases. Particularly in cases where the expertise required to carry out the implementation of a library well is somewhat high. I wouldn't really expect another implementation of most of [GAP's library](http://www.gap-system.org/Manuals/doc/ref/chap0.html) in Haskell even if I would love to use it, if only because there are only so many computational algebraists in the world, and asking them to rebuild everything they've already done in GAP is probably unreasonable. I might be somewhat ignorant of exactly what it will entail, but I feel like D3 is something we can probably replicate at our current scale. You could even start by building a binding to D3 itself in ghcjs, and then replacing bits of it piece by piece. From another direction, maybe extending [diagrams](http://projects.haskell.org/diagrams/) to capture the additional features you'd want would be a principled approach. The numerical stuff is a bit in the direction of GAP, but perhaps more approachable. Some finer aspects of that, there just aren't enough people in the world who know what the algorithms *are* to make it practical to reimplement everything in every language I think. (This is certainly the case when it comes to the big computer algebra systems.) Others are much less of a big deal. It would certainly be interesting to know the main things people want out of those libraries. ---- As for the point about programmers, while I mostly agree that individual people are not inherently good or bad at programming, I would strongly disagree that there's no such thing as good or bad programmers. There are a lot of ways in which even someone who is very prolific can contribute negatively to a project. While it absolutely is a matter of education, reading a book or two won't really teach good taste. It's something that you accumulate from years of reading and writing lots of programs and seeking out and being exposed to the best ideas people have had so far. It's really hard to do the cost-benefit analysis, and there are plenty of benefits to just having a larger community. However, almost without a doubt, there have been advantages in terms of the quality of the average Haskell library when it comes to the fact that a decent fraction of the people who know and love Haskell came to it by way of voraciously seeking out better approaches to programming. (Now Haskell is gradually beginning to stagnate a bit and losing a fair amount of such people's attention to even yet more futuristic languages, but that's natural. Give me a lazy-by-default *dependently typed* language with proper type classes, and you'll have a decent chunk of my free time too.) ---- When it comes to the Haskell Platform, personally I always viewed it as not really a big deal either way. Perhaps that's because I've typically just installed GHC via the generic binary package, and then grabbed cabal-install and everything else piecemeal from there. I've always seen HP as a shortcut way of doing that which just pulls in an assortment of the most frequently used libraries. It's not so much a "tool" as a "download option", is it? I guess the other thing it does is gives some sort of jumping off point for the Linux distributions to package up a bunch of Haskell stuff, but long ago I formed a distrust for my distro's Haskell packages (or really any language I care a lot about programming in), and haven't really looked back in that direction either.
I need to look into that. Seems interesting
SPJ is spot on as usual! I hope the involved parties will hear him so that further [resignations](https://www.reddit.com/r/haskell/comments/50389g/resignation/) from high profile contributors can be avoided. 
Which is strictly the same thing (and that's unfortunate because I find both terms confusing).
Be careful, claiming they're "**strictly** the same thing" makes for a perfect start of a heated debate. See also [this explaination](http://askubuntu.com/questions/78958/is-there-a-difference-between-free-software-and-open-source-software) which starts out with &gt; The terms free software and open source software do mean different things, though the categories of software they refer to are **almost** exactly the same. Personally I don't care about this academic distinction, but I've witnessed lots of debates over it.
You are right. I should have said that both terms cover the same licenses.
Looks like you can! {-# LANGUAGE RankNTypes #-} data Zero data Succ n newtype Vec n a = Vec { getVec :: forall u. u Zero a -&gt; (forall k. a -&gt; u k a -&gt; u (Succ k) a) -&gt; u n a } Test it *Main&gt; :t Vec (\z c -&gt; '2' `c` ('3' `c` z)) Vec (\z c -&gt; '2' `c` ('3' `c` z)) :: Vec (Succ (Succ Zero)) Char The encoding looks type-safe to me So basically GADT = existentials + equality, and it looks like you can get both by putting them behind an arrow and use a universal quantifier (a.k.a `forall`). Sloppy reason, I know.
+1 to this. Atom is working well for me. It's working fine with Stack (and cabal) and doing most basic IDE things.
Agreed.
It is clearly causing confusion, here and elsewhere, probably a lot more so than your biology example. Since C is a programming language, it is likely to be encountered by those studying programming language theory and vice versa. I personally am tempted to side with the use that came first and is formally defined, rather than say both uses are correct. I respect that you disagree.
The straightforward classic way, without invoking various newer non-standard libraries, is just to use `EitherT` instead of `Either`: createTenant :: NewTenant -&gt; EitherT AppM TenantCreationError Tenant activateTenant :: TenantId -&gt; EitherT AppM TenantCreationError Tenant createAndActivateTenant :: NewTenant -&gt; EitherT AppM TenantCreationError Tenant createAndActivateTenant newTenant = createTenant newTenant &gt;&gt;= activateTenant . (^. key) The classic straightforward way to add general error handling to your monad, as well as other general facilities you might need, would be to make your monad a transformer: type AppM = AppMT Id data AppMT m a = ... instance Monad m =&gt; Monad (AppMT m) where ... instance MonadTrans AppMT where ... Then you can add capabilities as needed to specific `AppM` actions. In this case, you would add tenant error handling by writing: createTenant :: NewTenant -&gt; AppMT (Either TenantCreationError) Tenant activateTenant :: TenantId -&gt; AppMT (Either TenantCreationError) Tenant createAndActivateTenant :: NewTenant -&gt; AppMT (Either TenantCreationError) Tenant createAndActivateTenant newTenant = createTenant newTenant &gt;&gt;= activateTenant . (^. key) 
&gt; ...literally the exact same thing as every other company insisting they are unusual Could be, but that does not mean it is not a great place to work. &gt;&gt; In short, we’re craftspeople &gt; No, you're not. They are if you believe TDD is the right way to work. They take the bark off the wood first and only then build the cabinet, not the other way around as many have done in the past.
I would say that the community is attacking the problem of package discoverability and documentation quite well: * The [Haskell Language](https://haskell-lang.org/documentation) website has started work on their documentation pages, with a bunch of great tutorials and recommendations. * /u/ocramz and @CodeSeagull are spearheading the [DataHaskell](https://www.reddit.com/r/haskell/comments/51eiyz/datahaskell_wiki/) group to make Haskell easier to use in data sciences. * /u/peargreen is working on [a site](https://github.com/aelve/guide) where people can compare packages and tools, comment on them, sort them, etc. * Opinionated package recommendations are also popping up everywhere. See Stephen Diel's [What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/), /u/bitemyapp's [Haskell Is Easy](http://haskelliseasy.readthedocs.io/en/latest/), and more. * I personally launched www.haskanything.com (*SUPER ALPHA*) to collect all the knowledge sources that discuss Haskell-related things and make them searchable by facets and tags.
You can test it against another sort function, but if you want lower assurance for lower cost you can also test properties like sort (permute xs) == sort xs. For permute you can take any function that permutes things, like reverse, or sort itself. You can also check properties like if x in xs then x in sort xs to rule out sort functions that simply lose elements. For the tuples you can also test properties like maxpair xs == maxpair (reverse xs), and maxpair (xs ++ ys) == max [maxpair xs, maxpair ys, last xs + first ys]. 
Someone has to behave as an adult :) And SPJ post is a good example how to use Non-violent Communication ( https://youtu.be/UEqmZ2E1o64?t=2m32s )
Those aren't open source. The open source definition requires that the user has the freedom to make derivative works.
[removed]
&gt; If you look at the implementation of Data.List.NonEmpty, it's a tuple of one element and a possibly empty list. So NonEmpty depends on regular lists! That's just an implementation choice; it is entirely possible to define a nonempty list type without using possibly empty lists: data List1 a = One a | Cons a (List1 a) The representation in `Data.List.NonEmpty` was likely chosen to avoid overhead when converting from lists to nonempty lists. &gt; Let's invert the question: why do we need Maybe at all? Perhaps we could get rid of it and use lists to represent optionality. Would we risk calculating "too much" if we did that? Nah, Haskell is lazy. Just pattern match on the head of the list. That's not the same thing as in OP's question: `Maybe a` and `[a]` are not isomorphic (i.e. there is no bijection between them), while `Maybe (NonEmpty a)` and `[a]` are. For this reason, your scheme introduces an additional source of errors: constructing a list of 2 or more elements when the list was intended to represent optionality. (Edit: Typo in code.)
Yes, we absolutely could have done it that way. But if so, the case of nullable lists is so common that we would have then defined nullable lists as an alias for `Maybe (NonEmpty a)`. And internally we probably would have implemented it as lists are now, rather than as `Maybe (NonEmpty a)`, as an optimization. So we end up back at the beginning.
OK that's fine. I'll take the one where they sanded at the beginning, and you take the other one. OK? :)
In your very first code snippet, shouldn't it be `AppM EitherT` instead of a `EitherT AppM`
Username... doesn't check out?
You can always "collapse" a nested monad of the same type with [`join`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Monad.html#v:join).
Salary doesn't seem too bad. 80k GBP at the top end
A [similar question](https://www.reddit.com/r/haskell/comments/51cew3/quickcheck_test_count/) was asked recently: &gt; The default number of tests used by QuickCheck is 100 - is there reason for this number? *[…]*
If you are testing for properties that should follow some kind of central limit behaviour, then increasing the number of samples only gives you more confidence in the conclusions. Otherwise, no finite amount of samples will ever be informative.
Could be posted to /r/programming.
Yes, Atom Haskell support is definitely a breakthrough in usability for regular (I mean non-VI/Emacs) users. It improves very rapidly, it seems updates are coming weekly and it becomes more and more reliable. Huge kudos for [atom-haskell contributors](https://atom.io/users/atom-haskell)!!!.
An actual answer from the paper [**QuickCheck**: *A Lightweight Tool for Random Testing of Haskell Programs*](https://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf) &gt; 100 is a rather arbitrary number so our library provides a way to specify this as a parameter. so if you want to run it with a thousand inputs use `quickCheckWith stdArgs { maxSuccess = 1000 }` instead of `quickCheck`. ---- There is also something called the “*small scope hypothesis*”, from [**SmallCheck** *and* **Lazy SmallCheck** — *automatic exhaustive testing for small values*](https://www.cs.york.ac.uk/fp/smallcheck/smallcheck.pdf) &gt; The principal motivation for this approach can be summarised in the following observations, akin to the *small scope hypothesis* *[…]*. If a program fails to meet its specification in some cases, it *almost always* fails in some *simple* case.
that's correct, very nice opinionated resource even if I don't agree with all the opinions.
Isn't the point of the PVP that if there is a major bump then by definition there is a breaking change? E.g. additions of modules, functions or even (non-orphan) instances wouldn't cause a major bump. So having this work out means very specific knowledge about your application, the package you're using, and the future: you need to know that the parts of the API that your application uses are such a core part of the library (and will not expand) that it will remain stable, even across breaking changes. It seems to me that this case is very rare, and should not be recommended as normal practice. One scenario that I can see is where you have e.g. a type class, and you depend on `text` to provide an instance for `Text` but use nothing else from the package. In this case I would leave off bounds completely: the `text` package has always had, and will always have, the `Text` type (I hope!).
It doesn't. It's just a quick check, not a proof.
Thank you for making these stack-friendly builds, it makes getting started with ghcjs much easier! Also nice to see more effort going into smaller output JS.
It's just a nice way to get mtl-style classes to compose with themselves. Like, using `MonadState` twice in a function's constraints, once for each of two different state types, just doesn't work. But it does with the Ether version. This just happens to solve the multiple-error-types problem.
I haven't used it but take a look at [*securemem*](https://hackage.haskell.org/package/securemem)
I started writing a paper about this topic in April, but I wanted to use the approach a little bit more before getting back to it. Today seemed like a good day to do it (thanks for giving me the motivation ;-)). You can find the current draft here: http://hsyl20.fr/home/files/papers/shenry_2016_flow.pdf As you will read in the paper, a Flow is simply defined as: type Flow m (l :: [*]) = m (Variant l) It is just to make code a little bit nicer, for instance compare: f :: A -&gt; B -&gt; m (Variant '[X,Y,Z]) f :: A -&gt; B -&gt; Flow m '[X,Y,Z] Then I define operators to compose these kinds of function (read section 4.1 in the paper): g :: X -&gt; Flow m '[U,V] h :: Monad m =&gt; A -&gt; B -&gt; Flow m '[U,V,Y,Z] h a b = f a b &gt;.~|&gt; g There are a lot of "fish" operators like this to handle the different ways to combine the flows/variants. This is what I wanted to show with the example in my previous message. I haven't benchmarked the approach yet. I hope that with the inlining, most operations are directly performed on variants' tag without too much indirection. It hasn't been an issue in this project yet. But it has proved to be *very* useful to handle the different subsets of "errno" (EFAULT, ENOENT, etc.) that each system call may return or not.
In general I've found that anti-patterns are the result of bad modularization / types. Obviously I can't comment on your code but it may be worth looking over your module and type hierarchies. Otherwise, you can use parametrization to easily get around this. `data Role a = Role a` Then your app module can hold values of type `Role App` and/or define a synonym.
Can't wait to try it out, thanks!
Just try submitting a bug report to GHC without using their blessed build-tool... ["reproduction instructions which use Stack are not useful"](https://ghc.haskell.org/trac/ghc/ticket/12604#comment:2)
Have you used `NonEmpty` ? Try ! Even though the idea is good, it's painfull in practice because lots of operations on `NonEmpty` can result in empty list, like `filter`, `splitAt`, etc ... You just endup using `Maybe NonEmpty` everywhere, so better use lists ...
Could you maybe give some examples? I think that would help a lot. GHC's gotten a bit better recently but I agree Elm's much better. Elm's type system is much simpler - no type classes means never hearing about "T isn't a member of C", for example. It would be good if GHC could output structured information but currently it's strings - fixing that would be a lot of work.
Thanks! It seems like it could be really tricky to use without leaking data somewhere else though.
Anybody commercializing off this with Haskell?
This really seems like an unfair characterization of the bug you're quoting. You neglect to mention that the same person who posted that reproducing with stack isn't ideal *then* went on to (a) give a pretty compelling reason about exactly why, and (b) dig in and produce a nice minimal test case on their own, which led to very quickly understanding what was happening in a way that the stack command line couldn't possibly. It's baffling to me that someone could see this as an example of poor behavior.
"Names of Pontiac Production Models in 2011" "Model Names of Cars Made By Nissan and Owned By Me" "Names of Volkswagen Diesel Models Made in 2016 In Compliance With US Emissions Regulations" Those are a few examples of empty car name lists where the emptiness of the list is actually meaningful in a conceptual sense. That last list, in particular, is likely to mean quite a bit to shareholders of Volkswagen AG. One could contrast these with lists like, say: "Model Names of Cars Which Both Exist and Don't Exist" Since that's a logical contradiction, one could argue that, using a type of Maybe [CarName], the former examples should be represented as Just [], while this last example is a Nothing, and where the difference between the two actually contains some meaning. Coming up with a practical reason to track logical impossibilities in Haskell is left as an exercise for the reader. :-) 
Answering my own question: &gt; It looks like dedupe will be ready for general use within a few weeks, with code size reduction of ~40% for bigger projects. https://github.com/ghcjs/ghcjs/issues/417 
It's probably my own failure to imagine anything better but I love Haskell's friendly error messages that seem to me to me to give several layers of context to an error. Certainly they aren't anything like the garbage your get in clojure that bubble up from the sewer of the JVM onto the floor of your lisp cathedral where they lie uncomprehending and unmediated.
The GHC project doesn't *use* github to manage PRs and such, but it *is* mirrored on github: https://github.com/ghc/ghc
The cabal-install people are working to implement their vision and goals in an open source project. Yes, this is the right thing. I appreciate stack. But will certainly try new versions of cabal-install because it has a different approach to package management with several advantages.
I've used hs-boot files in a yesod project once before. They work fine, regardless of whether you are using stack or cabal-install. I strongly recommend against using hs-boot files though. The yesod scaffold normally defines `AppSettings` in `Settings.hs`. You should be able to move the contents of `Settings.hs` into `Foundation.hs`. That would be the simplest solution.
Sure, I'll try to come up with good Haskell examples. The problem is that it's hard to stack them up against the Elm's analogues, since really cryptic ones indeed arise when e. g. type classes in particular are involved...
Thanks for the reply Simon. I'm sorry to hear that's the case, but what a funny coincidence. I hope that thread did not disappoint you further.
I think everyone agrees that haskell's numeric tower situation is far from ideal. It could be worse, but it definitely could be better. In fact, if you look at purescript's standard library, you find things like [Semiring](https://github.com/purescript/purescript-prelude/blob/ada7a297545f7ae727d74611b9f1ae0e60de6510/src/Data/Semiring.purs), which I consider an improvement over haskell's `Num`. My understanding is that the problem with improving this is that it would break a lot of code. In an ideal world, I wish it were different, but I'm not sure if that's really feasible at this point in time. I think the real win would be not just coming up with a better hierarchy (as people have already done that) but coming up with a way to roll it out and not break any existing code. As far as allowing user-supplied constraints in `Monad`, I am against doing that. Since `Monad` has `Applicative` as a superclass, you still have to worry about `&lt;*&gt;`. How would you write this? (&lt;*&gt;) :: (Ord a, Ord b, Ord (a -&gt; b)) =&gt; Set (a -&gt; b) -&gt; Set a -&gt; Set b There is no way to write it because `a -&gt; b` doesn't actually have a sensible `Ord` instance. Changing things from being fully-polymorphic to mostly-polymorphic doesn't always work out right.
It is great that the Haskell Community had the fortune to be exceptionally nice for such a long time. I don't think those pleas invoking the past will help us now. We have people in the community who are willing to drive their (often correct, IMO) points using inflammatory language, some of them are quite accomplished and able to generate a considerable echo (pro and contra). The winning move as a community which does not condone this kind of behavior is not to engage but judging from the recent past I don't think we will be able to do that and I suspect it is in (at least some) humans nature to crave and fuel drama (even if they wouldn't start it). Short of censorship I don't see a viable option to suppress this kind of behavior so either we start to moderate all major communication channels seriously (forcing verbal fights to take place on private blogs and twitter) or we get used to it (after all, this is the modus operandi of most of the internet). I might be missing options, I did not think comprehensively about this. I just think this is the discussion we should have. I do share the opinions voiced by Simon and others, but I don't think these personal POVs will persuade the people we would like to be affected.
&gt; the case of nullable lists is so common that we would have then defined nullable lists as an alias Also list wouldn't be an instance of monoid and hence ~~not an instance of foldable.~~ traversable structures containing lists wouldn't be foldable.
This is pretty exciting. I'm quite curious to see what effect this has on our payload sizes at work.
&gt;at the same time, human readable compiler output. I think they're perfectly human readable, personally! Aside from some of the stupid stuff I did in lens. &gt; are there any technical reasons Haskell can't have that, or is it just lack of polishing? I think it's a lack of polishing, but also the fact that Haskell encourages lots of abstraction. For instance, folding a list of functions (for accelerated arrays) to compose them will give complicated error if you use (.) instead of (&gt;-&gt;). And that's pretty much unavoidable off the top of my head. 
Its not just that it would break code, its that you get the question "how fine grained do you want to be". PS might hit the right balance, but time will tell. There have been a variety of efforts at what the "right balance" might be, and they tend to get stuck pretty quickly for this reason. Consider, for example, the choices made by numeric-prelude: https://hackage.haskell.org/package/numeric-prelude Also, you hit the opposite problem in PS even now. People say "I just wanted to use a number, you know, a normal number, what's this semiring thing?" And people even get confused at the principled division Haskell draws between integral and fractional stuff without auto-converting at times. This is not a concrete disagreement in any direction, just a general observation about the size and difficulty of the design space :-)
You can always import the prelude hiding (.) and import (.) unqualified from Control.Category. Its not great, but its not terrible...
Some related discussion here: https://www.reddit.com/r/haskell/comments/4ois15/would_the_smart_formal_methods_people_here_mind/
I don't know enough about Haskell or its GHC implementation to answer your question directly, however you can easily accomplish this in C. You can then use FFI bindings and call the relevant C code to do this. But then there is the problem of managing the data passed via FFI.
Yeah, that's a fair point. The balancing act is definitely tricky. Concerning your second point, the difficulty that any kind of typeclass-based numeric hierarchy adds for newcomers is very real. I remember how that used to feel. Although, it always felt much more satisfying than python/php/javascript, where implicit numeric conversions reign. It would be nice if there were monomorphization features in both haddock and GHC's error messages to ease some of this pain.
&gt; to some degree the choice of which list type to prefer is arbitrary since they can be trivially converted into each other. I wouldn't say so. List is only a monoid which is actually pretty important in that it means you can, say, fold a tree of lists. 
&gt; There is no way to write it apSet :: Ord b =&gt; Set (a -&gt; b) -&gt; Set a -&gt; Set b fs `apSet` xs = S.fromList $ S.toList fs &lt;*&gt; S.toList xs Well I just did in ghci so I'm not too sure what you mean by that. Now you cannot use it without something like `Set.mapMonotonic`, so you are limited somewhat in how you can use it. But ``(+) `S.mapMonotonic` S.fromList [1, 2, 3] `apSet` S.fromList [1, 2, 3]`` does actually work as you would expect. Now with all this said I think some sort of support for optional type classes would be a better way of doing this. As then if you give `Set` an optional `Ord` requirement (make it required only when observing the `Set`, such as with `show`, `==`, `compare` etc.) and whenever you don't have an `Ord` instance you just leave the Set in expanded and unordered form until you re-obtain an `Ord` instance, such as when trying to observe it. If we did that we would have to very very clearly state that whether or not an optional instance exists should not observably change the semantics of the program, (hence why the `Set` should only be observable when you have an `Ord` instance). One other good example is specializing `nub` when you have `Ord`.
Does the intero replaces haskell-mode, or adds to it? 
I see how my comment might have seemed. Let me make more clearly. Sewers are great works of engineering through which runs unpleasantness. The JVM us amazing but I never enjoy dealing with its contents. I was being flippant mostly for the sake of a laugh. I just wanted to make the point that the errors came from a terrible slurry (evident in the stack trace above) which does not "comprehend" the language by which which it was generated and the errors once generated are not "mediated" by the language... You see now why I have the off-hand but less boring description advice. PS for the record It wasn't me wot voted you down.
Intero replaces a subset of haskell-mode, primarily `haskell-interactive-mode`. I use intero in conjunction with `haskell-mode` and `structured-haskell-mode`. Honestly, `haskell-mode` has lost a lot of utility for me with `intero-mode` providing flychecking, completetion, repl, and devel-main support, however there are some other nifty features of `haskell-mode` (like cabal file support), so I keep it around. 
&gt;Ah, but NonEmpty is a Semigroup, and therefore Maybe NonEmpty is a Monoid! Oh yes you'e right. &gt;Not sure what you mean by "hence not an instance of foldable" though, since NonEmpty does have a Foldable instance. Yeah sorry I screwed up. I edited the other comments in this thread but I forgot to edit this one. 
As a programming language vagrant, Rust isn't bad either ;) The OCaml and F# community are also pretty good, but I don't feel that they manage to reach out as well to newcomers. Could be an issue with them not reaching some sort of critical mass.
&gt;&gt;&gt; "I just wanted to use a number, you know, a normal number, what's this semiring thing?" One easy explanation is to say to that person, "it's a fancy way of saying +"
I really don't agree with Applicative being a superclass of Monad. It's only a superclass of Monad because we literally only have one Category in Haskell as is, and it wouldn't be in the general case. Look at our Functor for instance, it is not a Functor, it is a Functor from the category of Haskell functions and nothing else. That's terrible inexpressive compared to how we could be talking about arrows in arbitrary categories.
I think my point was that the JVM-sourced portion of that error was succinct. If you write something similar in Java the entire stack trace would be but a few lines, and would have no extraneous classes or elements mentioned in it. So the sewer comment just seemed out of place given where the noise is really coming from in that particular example. And maybe we will get a better comparison once we get a Java repl w/9. I know Scala repl stacktraces are equally ugly. And I've become a bit of an apologist for Java/JVM as-of-late, so I generally expect a few comment-less down-votes. No worries.
&gt; I think the real win would be not just coming up with a better hierarchy (as people have already done that) but coming up with a way to roll it out and not break any existing code. Well, with ConstraintKinds you can already avoid breaking code which uses `Num` but does not define their own instances: {-# LANGUAGE ConstraintKinds #-} import Prelude hiding (Num(..)) class Add a where (+) :: a -&gt; a -&gt; a class Mul a where (*) :: a -&gt; a -&gt; a type Num a = (Add a, Mul a, ...) foo :: Num a =&gt; a -&gt; a foo x = x * x + x So I guess all we need to avoid breaking anything is to allow this: instance Num Foo where (+) = ... (*) = ... ... To be syntactic sugar for this: instance Add Foo where (+) = ... instance Mul Foo where (*) = ... ...
New channels for discussion? I think email lists in particular can get out of hand. 
But its not. Its got + and * and the correct distribution laws, but it doesn't have an inverse to +. But recall there's another way to decompose a ring -- rather than removing negation but keeping multiplication, we remove multiplication and keep negation, and then we have an inverse monoid. Or do we want to start with semigroups, then have inverse semigroups, then have inverse monoids on top of that? And soforth. So the lattice gets very tricky and we haven't even gotten past the properties of the naturals yet, much less to any sort of division... (this isn't really just a reply to you -- more generally just expanding on my point about the "right level of granularity") 
Its pretty fundamental -- the coincidence of monads and monoidal functors only occurs, roughly speaking, in the setting of self-enriched categories if I recall. (Its actually a slightly different criteria, but I don't remember exactly how to dig up the details).
&gt; I want to be able to effectively do things like data visualization, deep learning, GPU programming, Spark / big data etc. Lots of things in the haskell ecosystem are at the place-holder-qualit ... Python has numpy, scipy, theano, and the like. In Haskell, you don't have the depth of ecosystem that other languages do, and it's not because the language is inferior, it's because there isn't a critical mass of users to share the load. I think that python is very good for scientific computing, but python has weaknesses like Haskell does. I would use yesod over python any day. At this point, more users would be very welcome. But you don't want a situation like that with python, in which low-quality libraries abound and a very low percentage of users actually publish anything. 
I don't there *has* to be a significant incumbent advantage if everyone is respectful and friendly. Incumbents can respect your needs/feelings. The converse is that you have to tell everyone exactly how you feel, since we can't read feelings/needs/emotions over the internet. 
&gt; For example, someone could say "I think the project you worked on for the last 10 years is doing more harm than good for the goal of X because of Y and Z". That's not name-calling, but it is direct, uncomfortable, and depending on the culture, can be perceived as impolite. I think this is a really good point. I think it's especially hard because saying "I'm recruiting people to work on X project because Haskell has no good library" is often justifiable. But there needs to be a good way to be sensitive to the feelings of everyone who has worked on/is working on the same things. 
&gt; one could say that other parts of the Haskell community understand the importance of being patient and looking for the best answers, in a way that the commercial tech industry does not feel. I think that we could be sensitive to everybody's feelings while moving quickly, we would just need to have a pretty big shift in operating procedure. It's one thing to apologize and mend wounds after the fact, but it takes time. I wish there was some standard way to make sure text over the internet conveyed the right tone, and that people remembered everyone else's background. Usernames make that hard. &gt;It is precisely meant as a statement that sacrificing principle and ambition to just get something done right away is to be frowned upon. I think our community is finally getting large enough that having two concurrent projects is possible. We can have stack that "just works" and cabal that's done the "right way." 
It is recommended that ghcjs is used with Google closure compiler for production with already deduplicates, any idea whether this option has any additional benefit if you are already using google cc? In principal it is much better to do optimisations inside ghcjs, since it understands the source language. Trying to optimise JavaScript is pretty hairy. Of course that has to be weighed up against the resource behind googcc and the work already put in to it
&gt; I have to wonder, is cabal really doing the right thing here? Cabal is providing competition to Stack. What's wrong with that? I use stack exclusively but I will be revisiting cabal once it gets nix-style builds.
Somewhat hilariously, trying Elm after 5 years of Haskell, I found Elm's errors actually rather difficult to understand! Elm is such a simplistic language that it feels like a toy compared to Haskell. That's not to demean it in the slightest as it is still leaps and bounds better than most *other* front-end options and is perfectly suited to newcomers (which is not one of Haskell's strengths). Still, like many things in Haskell, the key is getting over the learning curve. I consistently find GHC's error messages quite helpful, readable, and to-the-point (especially coming from C++). That said I do see room for improvement, especially for beginners. Despite my own experiences, I've heard so many non-Haskellers rave about Elm's error messages that I must conclude they, at the *very least*, map more easily to non-Haskell minds.
&gt;You could use the same reasoning for almost any technology space yes, I think you should. &gt; My belief is that the compositionality, safety, rapidity of refactoring, and deep mathematical foundations has something to offer in all of these domains. this is exactly what I doubt. We're talking about high-performance gpu-focused number-crunching code here, a challenging environment for haskell to begin with; moreover its closely tied to proprietary libraries; if you're gonna have to just FFI the core of the algorithms for performance sake, is the haskelly glue code that much better than any other glue? It'd have to be some fascinating engineering to be a win IMHO. Or you need to make an EDSL like accelerate actually perform as good as say hand-crafted CUDA and/or calling of cuDNN; a completely orthogonal project, and apparently challenging one, as it still stands unsolved, and not for a lack of trying. But anyhow, I completely agree with your following paragraph; in machine learning more generally and even possibly deep learning, it would certainly be interesting to explore the design space with more powerfull abstractions - if someone has significant novel ideas to explore there. THAT would be a great project, as opposed to yet another theano clone I gathered you were suggesting. And thx for the pdf; you do have a point about troubles of gluing these engines into actual systems. But still you can't hope to do anything more with them than have an academic toy unless your performance can be equivalent to handcrafted convolutions for gpus if you want to cover deep learning, and that's a tall order I think. And that's great; I love academic toys. I just though you were specifically speaking about the opposite use for haskell; about mature production systems. What makes web and databases and the like a friendlier environment for haskell is that its concurrency support actually can perform rather well.
&gt; typeclasses and not types That's not even a thing!
Why is competition more worthwhile than cooperation? Wouldn't it be more beneficial for the community if they'd pull together?
I suppose that would be painful but maybe having the strong types avoids bugs.
wow, a json gist is absolutely nice!
`ghcjs` hijacks a couple of stages from the `ghc`; purescript implements everything itself.
It's a good point, but one difficult to reason about within the existing numerical tower. 
It's also not clear that an EVM with formal semantics could have saved the DAO. Legal contracts, along with pretty much all of our legal systems, build in ambiguity deliberately. "Smart" contracts, code contracts, do not: they mean literally and only what they say. This means there's no room for interpretation once the contract has been set in place. Some might say this is a good thing, but it has a few significant drawbacks. The first is that even for experts, understanding precisely what a body of code means is non-trivial; unexpected outcomes are common in software. The more complex you make the code to try and account for every corner case of the contract, the harder it is to determine the complete behaviour. The DAO was broken because its contract said things its creators didn't intend it to say: all non-trivial code does this The second is that there's no recourse. You can't appeal to a judge, because the contract is baked into the platform. You can attempt to appeal to the platform, which is where TheDAO was going before I lost interest in following it, but that sets up the platform as an ad-hoc adjudication system which will then require a complete set of rules under which it operates and rule-interpreters who can argue cases back and forth and oh, you've reinvented the legal system and "smart" contracts are meaningless again. The problem with "smart" contracts isn't the way the code is written, it's that they're not a very smart idea.
By non-Haskell mind I certainly didn't mean anything offensive by it. A lot of Python/Ruby/Elixir programmers love Elm but have never touched Haskell in their lives. So I really just meant someone relatively unfamiliar with the style of programming, abstraction, and type-safety that you find in Haskell. But based on what you've said I'd certainly not consider you part of that category. I think the example you gave is a common criticism of Haskell (and even Servant in particular). I don't think it's quite fair to take bad experiences with extremely advanced type-level programming and compare them with Elm. The fact is, you could not make Servant in Elm. Not even close. If you could, then we would really know if Elm's error messages were really better categorically. But since Elm is such a simple language, it has much less to worry about than GHC when producing those messages.
I've been told that it has a very decent impact to the size of the output after minification and/or gzipping. I _think_ the person told me that the difference between the main and dedupe branches was a 50% reduction in size after minification (via the closure compiler) and a 25% reduction in size after gzipping (since the output was less compressible). I don't know if those reductions were cumulative though. I'm very curious to here more stories about the impact of dedupe, but I'm _intensely_ curious to see what effects the Tyr branch has...
Well, I wonder if it would be fair and more specific to say that Elm has a simpler type system? That seems to be closer to what has emerged elsewhere in this thread. I don't have many useful suggestions. I guess I just tend to respond negatively to stuff in these conversations that sounds like haskell elitism but maybe not everyone would have the same interpretation.
&gt; You will have no sensation of a leash around your neck if you sit by the peg. It is only when you stray that you feel the restraining tug. Tell that to my cat. Any kind of clothing or adornment immediately confuses her keen proprioception.
Well, obviously nobody is arguing to keep the bugs in. :) I don't really remember the last time I actually ran into a problem where cabal install broke my package database, but it was at least a couple GHC releases back. Then again, in more recent times, most of my serious work has been on projects living inside nix environments, so maybe I'm missing out on some aspect of how it's been lately. I still just randomly cabal-install stuff with no sandbox while playing around -- I expect eventually that will run into problems, but it seems rare for me. But yeah, if you ever run into a situation where you want to install a handful of packages and it seems that as you install them, the things you installed earlier get broken, so that you end up in a merry-go-round of failure, the thing to do is just to tell it to install all the packages on the same commandline, and it'll try to find a solution for all of them at once. I think the last time I really ran into that was back in 2012 though...
trolling so deep they forgot which way was up.
Best things: stack and LTS. Shity things: cabal and simple.
Thank you for sharing your paper. Some unsolicited feedback: * The main section which talks about Variants progresses too quickly for my comfort. Others may not feel the same. * Wouldn't it be better to avoid the complicated operators and just replace them with self explanatory infix names?
But the language isn't failing, it's succeeding! And it's been succeeding and growing increasingly for years! As you said there are growing pains. But growing pains are the result of growing, not failing.
&gt; GHC is a monopoly provider of compilation services. I'm not sure I see what you're getting at here, but, hesitantly, I'd like to ask if you can try to expand on and explain this. Relatedly, I'm not sure what you mean by "services that are inside GHC" either. edit: nevermind, i now see there is a lively -cafe discussion on this: https://mail.haskell.org/pipermail/haskell-cafe/2016-September/125044.html
&gt; The plus side to this stuff is that it is pretty darn performant But my impression is that it's probably not anywhere close to being the bottleneck when compared with type-checking and optimizations? &gt; Anyway, GHCHQ Is that a pun? o.o
Typically the multiplicative inverse is dropped, not the additive inverse. Natural numbers are a common example of something without subtraction but not division. 
I only ever used it for project Euler problems.
[removed]
To me personally it was the difficulty of OCaml tooling on Windows and F# tooling in general.
Thanks, updated the list to reflect this.
I am just a 're-packager' ;). I think the right person to answer it is /u/luite2
Solving problems on Project Euler.
&gt; Is " I think putting the emphasis strongly on drawing in additional users is the wrong mindset." not clear enough? I find it to be counterbalanced by the first section of the para leading up to it, which reads "Haskell didn't get to be where it is by basing technical decisions on what would be most comfortable to the majority of programmers, and to some extent, that shows. That's not to say we shouldn't continue improving our tools, or that if the best decision would also be a popular one that we should avoid it." To me that context helps interpret it.
I use it for a very practical thing: Computing modular inverses for a multiple-option tests automatic grading system I use with my students. An initial description can be found here: https://www.tug.org/TUGboat/Articles/tb17-3/tb52salu.pdf
I laughed at 'hesitantly' - sharing your caution, I hesitated to repost and thought it might be better to expand on the mailing list thingy.
Would you say Java also has a powerful typesystem then?
Indeed
Why would you recommend against hs-boot files ? (I found a workaround, so I'm not planning to use it, just curious).
Why is Haskell to thank for this?
I think the problem around type-level programming with advanced language features is that has a similar effect to extending your compiler compared to how you write code in simpler languages, but that extension usually doesn't deal with secondary aspects of compilation like error messages and optimization. So, GHC + Servant, is like a new language, whose compiler lacks specialized error reporting and optimization in the Servant related aspects of the language. I don't have the background to imagine what the solution is, but maybe we need core language features that allow us to manipulate the error reporting and the optimization aspects of our libraries.
I have a pretty large application developed with GHCJS and reflex. And I'm very happy to share the result here: * all.js is the file GHCJS generated with everything in it. * all.min.js is all.js compiled with closure compiler's SIMPLE_OPTIMIZATION * all.adv.min.js is all.js compiled with closure compiler's ADVANCED_OPTIMIZATION all.js all.min.js all.adv.min.js ghcjs-0.2.0.20160414_ghc-7.10.3 11MB 8.0MB 3.6MB ghcjs-0.2.1.9007001_ghc-8.0.1 12MB 8.5MB 3.8MB ghcjs-0.2.1.9007001_ghc-8.0.1 -dedupe 6.7MB 4.5MB 2.0MB So, -dedupe really has a huge improvement on the result JS size. It's more than 40% reduced for my app. I urge you to try it for your apps and I think you won't be disappointed. :) Thanks tolysz and the GHCJS team for the great work!
In my experience `hs-boot` files work fine except for [one limitation](https://ghc.haskell.org/trac/ghc/ticket/8441).
I've used it in practice a lot and haven't found it particularly painful, but I don't tend to do further manipulations on the list. I filter the original inputs, validate to `Maybe NonEmpty` and then continue.
Thank you so much for all of your work into GHCJS. It's such an amazing project.
Good you mention `MonadPlus` since then it's also `Alternative` which has the `guard` function implicitly used in list comprehensions.
No. What Java even has to do with Haskell and Elm?
Applicative is written like that for efficiency, but if the class had the alternative: zip :: f a -&gt; f b -&gt; f (a,b) which is equivalent, you would have no problems, as you can have an Ord instance for pairs. The same happens for Storable and Unboxed vectors, where the constraint for the pair is satisfied, but not for functions. Unhappily, there is no way to do both.
Just a follow-up question, do you time the compilation time as well? And if you could compare build times for your app: http://tolysz.org/ghcjs/untested/ghc-8.0-2016-09-26-lts-7.1-9007001.tar.gz http://tolysz.org/ghcjs/untested/ghc-8.0-2016-09-26-lts-7.1-9007001-mem.tar.gz They require reinstallation :( Or maybe point me to some project which uses a lot of TH? 
With questions like this the people with incentive to answer are the one that disagree in the line of the status quo. The others see the problem as obvious, but they find it a little intimidating to support the complain. That is why it may be that the casual reader may get a little biased impression of the issue. This does not help to fix the problems in the language. In my humble opinion, after more that 10 years using GHC I still think that, despite the improvements, it has atrocious error messages and this is one of the greatest obstacles for the popularization of Haskell. Hugs, the interpreter, with almost the same complex type system had much simpler and understandable messages
I've been trying to work out all day how you get the tail of this vector in a type-safe way. I can understand cons/head/nil, but tail (and uncons) have totally escaped me. I found [this post](http://shift-reset.com/blog/2013/1/28/Tail%20as%20a%20Fold/) that explains how to use the folding function (which, helpfully, is provided by the Scott-encoded `Vec`) to get the tail, but this does not type-check correctly when the counter is introduced. Any help?
Guess we'll never know as they don't appear able to scare up enough courage to put their opinion out there. 
I'm checking ICFP YouTube channel more frequently than I care to admit and this will be near the top my list when the talks come online. :)
That sounds very cool! I've seen a similar trick used in how some folks did repeated bad predictions in a machine learnerning competitions to learn the true labels for the data in the evaluation / test data set.
We should make it magically awesome, and think of new ways to make it super duper awesome. Also it's really fun reading about how folks use the library. Bodigrim and to a much lessor extent, myself, have been putting a great deal of effort into understanding how we can clean up the current stable API. But we also hope to start thinking more audaciously about what a good number theory lib is and what we can do in that space. As time permits :)
They are a pain to maintain. Every time you change a type signature in the original file, you have to go change it in the hs-boot file as well.
&gt; The converse is that you have to tell everyone exactly how you feel, since we can't read feelings/needs/emotions over the internet. One aspect of politeness is that there are several ways of doing that. I can express frustration by encoding it in the tone of the message, usually with the intent of soliciting an emotional reaction, or I can keep the tone neutral and express my emotions in a factual manner. The latter is the polite way.
Haskell was the first (semi)popular language to explore functional web frameworks.
I'll take a guess that you were downvoted because of your impolite attitude. You have a good point in your comment, but maninalift also has a good point. Two people arguing against each other with good points means they're having a nice discussion, unless at least one of them is being impolite.
Ditto. But from what I could gather, the GHC source code isn't exactly a beauty.
I didn't pay attention to the timing. My app is large and has a lot of TH, mainly due to lens. And it takes several minutes to completely compile the whole app. I'll give it a try later this week when I have time.
"Semifoldable" for non-empty Foldable seems a bit strange to me, because there's no obvious relation with Foldable, whereas from the name I'd expect every Foldable to also be a Semifoldable. A list is both a Semigroup and a Monoid. A list is a Foldable, but cannot be a Semifoldable.
Great point. In fact GHC 8 introduces many improvements to user-defined compile errors and Servant, for one, is [actively trying](https://github.com/haskell-servant/servant/issues/576) to utilize those features for this very reason.
Can't this Semifoldable be implemented in terms of Foldable1?
To be sure: is the instance definitions the **only** thing you changed? Have you tried to change one instance at the time?
I'd try to pinpoint the issue to the single line, or symbol chanfge, e.g. n `deepseq` ords `deepseq` () to n `deepseq` rnf ords `seq` () There might be many things, like accidentally uisng the `n` for `m`, or forcing the `id :: forall a. a -&gt; a`. Shadowing is dangerous for the reason (or leaving variables unused). Try `-Wall`.
&gt; Hard to remember monoid/semiring/group/abelian group but they all have their uses. I think the names make it harder than it should be. For example, "semigroup" is a monoid without `mempty`, and "semigroupoid" is a category without `id`. Why not call them "semimonoid" and "semicategory" then?
What happens if you add `seq` () at the end of all your NFData instances in your original `rnf`-based implementation. Does that solve your problem as well?
Good point! There are definite advantages to Purescript's approach, it's an exciting language.
-64 points, seriously guys? SPJ showed us we were kind to the trolls, and this is an elaborate one. &gt; You will have no sensation of a leash around your neck if you sit by the peg. It is only when you stray that you feel the restraining tug. Thanks for the Parenti quote. Will reuse it.
&gt; Monopolies have their problems. They create power imbalances that need active management to control. A community should be particularly wary of monopolies attempting to vertically integrate up the production chain into areas where a monopoly makes less sense. I would call the whole cabal versus stack drama a text-book case of over-reach. Everyone agrees stack operates at a higher level of abstraction then cabal, on top of it is accurate. **Cabal shouldn't even be allowed to compete above it's current abstraction point.** I couldn't have said it better
There is no way to do this *and* get to play with most of Haskell's toys, since control of memory is part of your problem description, which is at odds with practically all of Haskell... ... However, you can still do it, and at a higher level than pointers. Specifically, you can create a DSL (a datatype representing instructions that is executed by a function, if you're not familiar with the acronym) that describes actions and sequencing of actions on memory that do not permit duplication or lingering of data after execution. Then you will be statically assured of your requirements.
This is awesome! I just tested it on my project and also got a 40% reduction in size: ghcjs-0.2.0.820160904_ghc-8.0.1: 6.9M ghcjs-0.2.1.9007001_ghc-8.0.1 -dedupe: 4.3M Also, thank you so much for making these builds available!
Cabal is competing with Stack rather than limiting its scope to providing only the basic foundation for Stack to build upon. Cabal should be reduced to a library only and *not* provide a tool above its abstraction point which is competing as an incumbent with Stack.
&gt; Somewhat hilariously, trying Elm after 5 years of Haskell, I found Elm's errors actually rather difficult to understand! Elm is such a simplistic language that it feels like a toy compared to Haskell. This can't be serious. Show me one difficult to understand error in Elm. By the way, "Elm minds" must be a subset of "Haskell minds", since the languages have a lot in common (they both are intransigently FP for one thing, and they both get syntax right, for another).
&gt; Cabal should be reduced to a library only and not provide a tool above its abstraction point which is competing as an incumbent with Stack. No.
Left adjoint to forgetful parking where you can't recall which zone you parked in
I would still prefer them to polish absolutely everything they can in the compiler, but let's see where this goes. I have a bad feeling it may end up like Clojure's `core.typed` though, i. e. effectively nobody ports anything, and the standard library's situation remains where it was... but we'll see.
We use a "lite" version of this in the slamdata UI. Specifically we use the MTL-like classes + Free interpreter at the edge. A common, real-world example for us that is solved with much anguish otherwise is around backend services and authentication. We use a token and auth provider for authenticating backend requests. We get to write all of our business logic around requests as if there is no authentication using our API algebra. Then at runtime, if a request fails due to authentication, it can affect the UI, bringing up an authentication routine, reauthenticate the user, and retry the request in a manner completely transparent to the caller. Previously we passed around everything we needed for this kind of stuff in a "final" manner with records, and it was awful. So I think there's definitely a lot of merit to this approach when building UIs.
What I mean may have been unclear to you. Like with other monopolies, what needs to be done is that `Cabal` and `cabal-install` have to be separated from each other into separate projects and organizations in order for `cabal-install` to lose its incumbent status towards to Stack. Then cabal-install would be just one tool among others using Cabal-the-library, just like Stack.
Yeah it's ugly. The expected type doesn't make any real sense to me: (a -&gt; a -&gt; a) -&gt; a0 -&gt; a -&gt; a -&gt; a Now I know it's a garbled specialisation of the true type of `foldl`: foldl :: Foldable t =&gt; (b -&gt; a -&gt; b) -&gt; b -&gt; t a -&gt; b But I'm not sure quite how it got to the former from the latter. There is one mystery I _can_ solve for you: there's no `a'` in the signature. It's just that the signature has quotes around it: `' ... a'`. 
Nobody mentioned UHC until this point... I think it can be fairly competitive with GHC, but seems to need more human resources. Yes, diversity matters. So why don't you start with contributing to UHC?
Thank you for confirming I'm not insane. Reading that thread the only thing on my mind was 'Where is the problem? What sort of technical issues are being solved here?' AFAIU, none. In fact, this would in fact created multiple points of contention, a la Cabal-the-program vs Stack, with no actual benefit. I would also like to say such kind of topic, that attack problems in 'holistic' (or vague) ways, has little purpose except bikesheding. When no technical issues are involved, there is no actual work to be done. 
Polite first, point second.
Why all this talk of interpreters? It's assumed that's the domain, which isn't always the case? What have I missed?
I thought it was a fairly involved process to get GHC to compile core files? Although I guess a easy workaround is to just generate a subset of regular Haskell code.
Haddock is a tricky one because: A) We want to generate docs for all the packages included with GHC, which has that fancy build system. So we need to ship Haddock at build-time, and... B) Haddock relies very closely on the syntactic AST that GHC uses very early on, so it's very closely tied to any changes in the frontend (other than pure lexical issues, or whatever) This makes things complex. You want to ship point releases of Haddock that might fix bugs to users, but you also have to keep up with GHC `master`, and that can be very non trivial sometimes (especially for anyone other than the person who made the syntax change in GHC originally). So you at least have to do two different branches, one for GHC and one for actual consumers. Then you have to reconcile all the changes in GHC vs the ones in the actual development tree after a few months. This is generally a laborous process. Then there's the actual fact we're the de-facto maintainers of Haddock. And -- here's the kicker -- we definitely did *not* choose that position on our own (and this isn't a finger-point-of-blame, either). We have actively looked for new maintainers - Ben posted about it a few months ago (and some people did step up!) GHC wants as little to do with Haddock as possible IMO, the reality is that we're the vast majority of people who contribute to it these days, because it's a complex project that's deeply tied into the build system, and one that's tied to our source code. We're a major consumer, and we also deeply impact its development. Frankly, I think every GHC developer would be happier if they didn't always have to fix Haddock too, but that's life to some extent. All the prior maintainers have done lots of wonderful work and then moved on. Keep in mind, documentation generators are hardly fun to work on, if you ask me. Some things could be fixed to ameliorate this. B) can be alleviated to some extent by replacing the API that Haddock uses with something much more domain-specific and fine-grained, and less prone to change. Something with a stability guarantee. It shouldn't necessarily need to use the *exact* GHC AST. GHC could ship a library like Clang does shipping the `libclang` C library, which provided a relatively stable interface to the moving internals. This would help decouple Haddock from the internal data structures, making it easier to change the two in tandem. The first one is harder to fix. Fixing B doesn't remove the fact we still need Haddock as part of the tree, to build docs at build-time. So it's still fixed to GHC at some level... Fixing *that* would require, like, being able to generate documentation *after* the fact, which would need some build system reworks and probably some internal reworks of Haddock. We could probably fix this in GHC a few ways I guess -- in the build system, in the compiler, etc. But it's ultimately a bit of work. And whatever we did to fix both of these, presumably, could lay the way to making Haddock much more decoupled, and allow people to much more easily write their own tools, or integrate Haskell documentation with other tools as needed, using Haddock. It's all a lot of work, but there's nothing fundamentally unsound about it. The question is, who will do the work? When you think of it all like this -- Haddock is honestly a terrible example of "monopolistic behavior" by any metric. It has never kept anyone away (it even has its own separate repo on GitHub, independent of the GHC org - we just have a fork), and nobody has exclusive control over it by any means. It's closer to an example of tragedy of the commons (everyone uses it, but there is not representative labor to drive it further), if anything.
&gt; I would call the whole cabal versus stack drama a text-book case of over-reach. Everyone agrees stack operates at a higher level of abstraction then cabal, on top of it is accurate. Cabal shouldn't even be allowed to compete above it's current abstraction point. This is fairly ironic. OP accuses GHC/Cabal of forming a monopoly, informing us - wisely - that we should be careful about monopolies. And the solution: create a monopoly for Stack "above a certain abstraction point". 
Good point.
I did the later, so I'm not sure what is wrong. I tried it in a dummy project and it works indeed.
I'm not sure where such list may be, however some similar things come to mind, in no particular order: data types a la carte, arrows, functional reactive programming, codensity, continuations, monad transformers, monad-control.
And to continue this list: zippers, pretty printing combinators, finger trees, GADTs.
In the context of free monads, "interpreter" means something different. When you write a program in terms of a free monad, that program has to be "interpreted" back to some concrete answer; usually another monad like `IO`. To do this interpretation, you write an interpreter that the free monad uses to walk through each effect and convert it to a useful result.
The pipes (including streaming) ecosystem is both interesting and practical.
Thank you. Any recommended resources on Free Monads I could read?
Agreed!
Aha my favourite Haskeller - thank you again.
Actually, I have used `NonEmpty` to good effect specifically because the more expressive type signatures prevented me from ever taking the `head` of an empty list or other such things.
Haskell is a great illustration of a third way: you don't have to become the Javas and Pythons of the world to *not fail*. Instead you can soldier on with a small but intense community decade-in decade-out without sacrificing anything for growth and popularity. It's like Linux on the desktop: I'd rather it worked better for the 1% invested in it than working fine for everyone but becoming just another flavor of Windows or OS X. Linux on the desktop, no matter what people may say, isn't a failure. It might not be *popular*, but popularity is not success! I'm too productive on Linux *today* for it to be a failure. I'd take that over exponential growth with its pain and compromises any day.
Well yeah. I don't have any particular preference for names, just that they exist in the literature. 
I think something else reddit obscures is: where is this person coming from? It's easy to forget that someone is a cabal contributor, and likewise it's easy to forget that someone is running a server in Haskell and therefore needs very high reliability. I think more conferences might be a good idea?
Oh, this is really cool! Ya learn something new about haskell every day :)
Try http://dev.stephendiehl.com/hask/
Hi, no problem and thanks for the reply. My initial post was a bit too vague on a re-read, hopefully the follow up fixed that.
Thank you very much for the feedback! I will try to improve the paper. I have added some named operator aliases for the most common operations: https://github.com/hsyl20/ViperVM/blob/master/src/lib/ViperVM/Utils/Flow.hs#L263
https://www.reddit.com/r/haskell/comments/27wqk6/navigating_and_modifying_asts_built_on_the_free/ may apply?
We use PureScript https://github.com/slamdata/slamdata with our own UI framework. Our main src tree is 40k SLOC of PureScript, supported by about 60k SLOC of PureScript core libraries and libraries we've written. Performance is definitely acceptable. We aren't building intense games or anything, but we do lots of mouse-move type inputs for dragging and all that, and we want to keep it smooth. Keep in mind this approach applies to all of our business-logic-component-state-transition type stuff, and most inputs don't incur a huge cost. This doesn't apply to performance intensive "effects" like low-level DOM diffing which occur outside of this framework.
SPY's appeal moved me to respectfully present what I think is the most miss-understood point-of-view in the community, and I happen to be in the cohort I label, perhaps badly, commercial haskell. I do this because I have read every word of every debate/argument/fight here and in github issues, mailing lists and the like, but never involved myself - a fresh perspective perhaps. &gt; Reading that thread the only thing on my mind was 'Where is the problem? What sort of technical issues are being solved here?' The technical issue is why so much ill-feeling and bad behavior has crept into the community. Everyone is out there scratching their heads wondering why temperatures are hot, and communication is fractured. The problem domain is then one involving community and organisational dynamics, which necessarily involves a discussion about economics and politics. There's no other words for trying to rationally work out what's going on with (this) dysfunctional group. In the OP, I tried to make it clear that I support GHC - backed by my career and interest - and agree with everything you say, except the nonsense and unjust part. Because of the faith I place in the GHC team, I feel justified in saying they should be cautious about conflicts of interest at the interface between the compiler and the outside world. One balance, every side (there are many camps) is guilt of poor comms, but, at times, I have seen resolutions that occur because one side of the debate is closer to the GHC team, or closer to academic opinion (again a bad label perhaps), or has their hands on issue closure. Monopoly may have been a poor choice - I was trying to be dry to avoid conflagrations - so I'll argue the same thing another way. It's been 24 hours, and SPY Respect is on its way to the #2 post (after the kmett resignation). I respond with respect on the cafe mailing list trying to get a pov across that I feel is necessary in any path to understanding and healing obvious rifts and, making its way here, it gets downvoted and howled down. Now I'm not much chop as a poster - +8's a good get for me - but I'm guessing that the voting represents an imbalance whereby people who may share my point-of-view, however ham-fistedly expressed, have long felt unwelcome here, at great loss to the community.
&gt; So can someone explain to me why the type declaration in the where block in the Wiki's version is allowed Because the Wiki's version is genuinely polymorphic. `newArray :: Int -&gt; [b] -&gt; ST s (STArray s Int b)` is correct too.
That's an informative link, but the free monad discussed there doesn't have a function type. How do you pretty print a free monad program that, at some point, requests input from the user?
Thanks! Do you have a script to build these bundles? It might even be useful to build then automated?
And how do you think I can keep up? https://github.com/tolysz/prepare-ghcjs This is just a hack as `ghcjs` is a moving target so are resolvers... but the number of changes is still reasonable. Major ones are when I actually have to patch some library or modifty `boot.yaml` to support some newer `aeson` But once more people will find it useful, maybe there will be resolver related branches inside `ghcjs-boot` with exactly the right packages and tags for each resolver's tag... and my hackery days will be over :)
You're getting downvotes here I think not because people feel want to make you feel unwelcome, but because as discussed elsewhere on this thread, I think people feel that you're factually incorrect in a number of key ways. I too think you're wrong on a number of things; elsewhere on the thread I gave some technical considerations -- later expanded on by austin. I also think there's a bit of a visceral gut response to you here, as the devs who contribute to GHC and the surrounding ecosystem do so out of altruistic motives by and large -- a desire to build great things and share them with others. As we're constantly reminded, with the growth in usage of Haskell there's increasing pressure on improving these tools in a myriad of ways, and a lot of work being done to try to draw in a broader contributor base. In such a context, suggesting that the good work all these folks do is somehow redounding to bad effect (such as, I guess stifling competition or the like) -- well, you can see why that may elicit a strong response.
Wow, thats a lot of PureScript! Pretty awesome that you've open sourced it. I am still wondering about GHCJS performance and whether or not I should choose it or PureScript. I haven't seen nearly as large a project in GHCJS, and the browser-specific library situation isn't nearly as good -- but it is Haskell.
I'm so pleased to see someone mentioning NVC in here. It really meets my need for belonging as I hold both communities (Haskell and NVC) dear to my heart. Thank you for your comment! :)
I would really want to see how to run the `closure compiler's ADVANCED_OPTIMIZATION` if you could give some pointers :)
I recall in the initial "friendly error message" they showed they changed file:line:column to something like "error in file on line line, at column column" which is a massive inflation and also breaks tooling that expects the previous format. Perhaps it's helpful for newcomers, but then it's a millstone around your neck forever. Similar things hold with all the whitespace in their messages.
You could well be right on the vote side, so let me narrow down where I might be going wrong on the facts or opinion. There have been a series of conflicts within the community all located roughly at the ghc interface. The stance the community takes towards diversity has been questioned in the past. Looking at the interface, I note that ghc is the only choice when it comes to compilation of Haskell. I note that this is probably a good thing, evidenced by results. My opinion, however, is that the logic of why GHC is best as a collaborative effort at one Haskell compiler may not apply to tooling and documentation. My opinion as a practical coder, is that Haskell is weak in tooling and documentation, at the very locus of the problem. It used to be weak in project reproducibility, not a good look for a side-effect aware language, but stack solved that (and maybe stack got lucky as the wider sector improved with nix et al). From my personal viewpoint, the stack effect has made a high magnitude difference between career opportunities, success or failure. If you allow the above, my contention is that the community has prevaricated on embracing and celebrating stack. at_all_cost $ avoid $ stack == Success My visceral gut reaction is, say what? Adoption rates are being hurt and therefore so am I. You will tolerate others implementing the above line, as they're not important to you and yours? Ok, I might be wrong on my logic, but please can we consider the possibility that many haskell coders find it hard to break through to a good development job. Through lack of talent, for sure, but not-production tooling hurts too. And then it gets argued that a future monopoly is being created, I'm pointing out an already established monopoly is running that argument, one that is natural and good for compiler development, for sure, but is a source of conflict in other areas. 
An hs file not being compiled when there is an hs-boot file is a straight up GHC bug, especially since Cabal library (which Stack and cabal-install use) calls --make which is responsible for this. If you can feed GHC a -v flag and pastebin the output somewhere that would be helpful for debugging. You /are/ using stack to build (and not some fancy autobuilder?)
&gt; Free monads permit unlimited introspection and transformation of the structure of your program. This is not true. Or more accurately, it's only true when your base functor doesn't have any lambdas in it.
Let me try to discuss this point by point: &gt; There have been a series of conflicts within the community all located roughly at the ghc interface. As I argued elsewhere, this stuff isn't about the ghc interface -- its not coupled to decisions on the part of the ghc team. (As austin has pointed out, however, when ghc makes any changes, haddock does always need to be modified to catch up, which is a great deal of extra work). &gt; My opinion as a practical coder, is that Haskell is weak in tooling and documentation, at the very locus of the problem. Many people, myself included, I think agree with you here. There are a lot of niceities of tooling that exist for languages with large scale adoption that don't exist for haskell. Likewise, there are many more books available for such languages, and for individual libraries with many times more users than the haskell language as a whole (plenty of stuff e.g. in apache-core i suspect, or boost for c++ to name a few), often there is a great deal more attendant documentation available. My complaint/disagreement here is that I don't see how this has anything to do with the position of ghc or the actions of the ghc team. Tooling, tutorials and documentation are all attendant infrastructure, and the actions of the ghc team are for the most part unrelated to this. So whether or not ghc is a "monopoly" (natural or otherwise) feels unrelated to this widely recognized concern. &gt; From my personal viewpoint, the stack effect has made a high magnitude difference between career opportunities, success or failure. I personally recognize that it had this impact for you, and I think many others recognize that as well. Stack has gotten widespread use, and endorsement. It is included in the current haskell platform. So I find it hard to see what you mean when you say it is not recognized and appreciated work. &gt; Adoption rates are being hurt and therefore so am I. Here again I don't follow the logic. You seem to now have a career in Haskell, yes? So, purely formally, what is your personal stake in higher adoption rates. Furthermore, what do you feel the obstacle to these adoption rates is? If you say tooling and documentation, then sure, I will agree. But again, this should inspire us all to work more and contribute together to better tooling and documentation. I don't see how the state of tooling and documentation we have (which, to be honest, I think is actually impressive for a language of our current size and adoption) can be laid at the feet of the ghc project in any way. &gt; I'm pointing out an already established monopoly is running that argument, one that is natural and good for compiler development, for sure, but is a source of conflict in other areas. And again, sorry to belabor the point, but what monopoly are you speaking of? The ghc team actively seeks collaborators. And ghc itself doesn't have a monopoly on anything but "being ghc". But furthermore -- neither the compiler nor the team have any direct impact on the general state of tooling and documentation that you refer to. To sum up -- the Haskell enterprise is really the product of lots of individuals across the world and over many years, all with different interests, different itches to scratch, all of which have contributed in their own ways -- be it working on the compiler, the tooling, libraries, or just providing feedback, or even answering or asking questions on SO, writing up blogposts, tutoring or teaching others whether in organized settings or informally over irc, and etc. We're all in this together, and we all contribute in our own ways and as suits our proclivities and our time constraints otherwise (day jobs, families, hobbies, the occasional need to sleep). There seems no obstacle to me to building newer and better things, or improving things, outside of the fact that it requires organizing people to do it and encouraging people to do it. And the best way to do that, in my experience and opinion, is through positivity -- through making sure we have a fun, welcoming, collegial environment that inspires others to want to pitch in and join the party. That's the best way I've seen in the world for open source to happen. It gets really frustrating when things don't work as we'd like, and when one considers all the work that needs to happen in so many arenas to move so many things forwards. One way to deal with this frustration is to try to blame things on some root cause, some immediate obstacle which, if only it was different, would just make everything better. But this is nearly never the most productive way in an open source world, I have found. Rather, I think it is better to direct this frustration into inspiration, into a decision that one wants to get involved to take up the shared challenges of building and improving things together. In the course of this, I've found that it is almost never productive to worry about how others choose to spend their time. Rather, I tend to worry about creating a welcoming environment where people feel rewarded and encouraged for spending their time in improving things at all.
The reworking of the AST sounds exciting. Is there anywhere I can track progress on that? I've played around with modifying haddock to add other backends (xml, vim help pages), and I agree that having a nicer AST would improve this situation.
Related to being specifically designed to compile to JS, the generated javascript attempts to be fairly readable. This can help with debugging and integration with existing browser tools (like setting breakpoints from Chrome or something)
I use the following command to compile the JS file: ccjs all.js --compilation_level=ADVANCED_OPTIMIZATIONS --language_in=ECMASCRIPT5 &gt; all.adv.min.js But the all.adv.min.js has a runtime JS 'undefind' bug while the all.min.js works fine. Have no idea why it happens. It used to work though. (I need to clarify that this bug started with the old version a few weeks ago. It's not a problem of ghcjs, I guess.) Now I have to use the all.min.js in production
Wow, thanks! This will take me some time to digest. (And I don't have any experience with Template Haskell, but I'll look at Data.Singletons.TH.) 
"How many trailing zeros has `100!`?" is a problem I found more interesting to solve on paper than on a computer. I was, however reminded of "If the integers from 1 to 999,999,999 are written as words, sorted alphabetically, and concatenated, what is the 51 billionth letter?" which has a 4-post series working through the solution using monoids here: https://conway.bostoncoop.net/~ccshan/wiki/blog/posts/WordNumbers1/ 
I got 28% before closure.
I'm only on my tablet, so I haven't checked this, but I think it *should* work... and concision can be nice sometimes. data Succ z = S z data Vector (n :: k) a where Nil :: Vec '() a Cons :: a -&gt; Vec n a -&gt; Vec (S n) a type Ix n = Vec (n '()) () slice :: Ix x -&gt; Ix y -&gt; Vec (x (y (z '()))) a -&gt; Vec (y '()) a slice Nil Nil _ = Nil slice Nil (Cons _ p) (Cons v t) = Cons v $ slice Nil p t slice (Cons _ p) s (Cons _ t) = slice p so t It might not be quite right though. Edit: ah, this won't work with zero lengths (in `x '()` x cannot both be a usable type variable and a no-op). Needs tinkering.
&gt; The largest Haskell codebase I've worked on is probably purescript, but only a small part of it and only as a personal project. Just clarifying -- did you mean **use** purescript, or did you mean **contribute** to the core project? 
It doesn't mention Haskell Curry.
There is a single shared heap object for `[]` (empty list / nil, not the list type) regardless of if you use nil at `:: [Int]` or `:: [Bool]`. The same holds for `Proxy` or any other trivial data constructor.
(nb: all answers given here and elsewhere in this thread pertain to haskell as compiled by and implemented within the glasgow haskell compiler [ghc]. there _are_ other compilers and interpreters, though less used, and answers would vary for them). 1) the latter -- aka "green threads". iirc they're more lightweight than erlang's even due to not having per-thread heaps, though there are tradeoffs involved in that choice. 2) the rule of thumb is well-written haskell that's tuned for performance but remains idiomatic should come within roughly 2x well-written C. Well-written Java that's been running long enough to JIT well can actually go toe-to-toe with idiomatic C, but Haskell isn't going to be orders of magnitude behind. 3) Yep, Haskell compiles to native code, and the executables embed the necessary "run time system" for GC, etc. So yes, much closer to Go or C in this regard. The downside of course is that things that compile to bytecode such as Java, while requiring more stuff installed, end up being directly portable. Meanwhile, if you want to have a haskell executable for mac, and another for windows, you have to compile it twice -- typically once on each platform (the cross-compile facilities are improving, but pretty messy still to my knowledge).
I can mainly only speak to the last two questions. In my experience, the refactoring story more than lived up to its claims. If you're doing things right, you can just make the change you want, then keep fixing type errors to guide you through all the knock-on work generated, and it's very straightforward. Ditto, testing/qa. Of course this depends on your technical culture and if you make a practice of learning and using the variety of techniques for writing types that really _do_ enforce your invariants. In my experience, while quickcheck and soforth had their purposes, a combination of careful design of data structures coupled with some straightforward integration testing was often plenty adequate. But of course there are bugs that you often won't catch without testing in some form -- for example reversing the true and false branches of a conditional. Or entering certain hardcoded times in terms of e.g. microseconds instead of seconds or vice versa.
Which, useful base functors always have lambdas. The general pattern (which I believe the OP mentioned) is such: data Op parameter result variant = Op parameter (result -&gt; variant) deriving Functor data Console a = ReadLine (Op () String a) | WriteLine (Op String () a) deriving Functor readLine :: Free Console String readLine = liftF $ ReadLine $ Op () id writeLine :: String -&gt; Free Console () writeLine s = liftF $ WriteLine $ Op s id *(Sidenote, this means common usages of `Free` like this are ultimately equivalent to nothing more than the sum of a few `Op` functors)* This pattern inherently masks steps in the computation behind functions. So anything using this pattern is difficult to get useful introspection of.
**Concurrency:** The primary approach to concurrency in Haskell uses [STM](https://en.wikipedia.org/wiki/Software_transactional_memory) and the [async library](http://hackage.haskell.org/package/async). A great introduction to STM in Haskell is Simon Peyton Jones' classic paper [Beautiful Concurrency](https://www.microsoft.com/en-us/research/publication/beautiful-concurrency/). These idioms leverage Haskell's purity and strong type system to allow you to specify a concurrent system at a higher level and with more composability than traditional lock-based concurrency. However, it is also possible to use the traditional approach, and you do still see quite a bit of code like that. EDIT: A great in-depth review of all three of those techniques in Haskell can be found in the second half of Simon Marlow's book [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929). EDIT2: And as others have pointed out, in Haskell you usually will not find simulated concurrency using explicit event loops, as in Javascript or [Python Twisted](https://twistedmatrix.com/trac/). EDIT3: Above I linked STM to the Wikipedia article. Here is a link to the Haskell [stm](http://hackage.haskell.org/package/stm) library.
Re 3., isn't GMP usually required on the machine? IIRC it's basically the only thing that's dynamically linked (though libraries may also being in dips which are dynamically linked too). Someone please correct me if I'm wrong.
This is a low-level view of concurrency under the hood, important to understand for FFI bindings to C libraries. For concurrent code written natively in Haskell you usually take a much higher level view, using libraries like [stm](http://hackage.haskell.org/package/stm) and [async](http://hackage.haskell.org/package/async).
From my observations, there is an important detail about your second question (performance). Haskell is typically in the Java range of performance, but it depends on what you are doing. When you see descriptions of Haskell matching or beating C, this is typically for programs that *cannot* be really fast, because they involve IO or pointer chasing. Ironically, Haskell has very good libraries for constructing IO processing pipelines, and the GHC IO manager is quite decent. For numerical code, Haskell is not particularly fast. You can get closer by using unboxed representations and various awkward libraries, but it easily becomes rather uncomfortable, and it will still get beat by a C compiler able to do real loop optimisations.
I don't have time this week to comment the code or even simply carve out the subset that precisely answers this question from a file doing a lot of other things but I'd argue that [this is the sort of thing you want](https://github.com/gallais/potpourri/blob/master/haskell/stlc/Bidirectional.hs#L294): a datatype with de Bruijn indices and then some typeclass magic to let the user use Haskell lambdas and names.
(Edit: sorry, I wrongly read the documentation of the slice function) &gt; slice (SSucc i) (SSucc j) (_ :&gt; xs) = slice i j xs Did you mean slice (SSucc i) j (_ :&gt; xs) = slice i j xs ? Actually `slice i j` returns a vector a length `j - i` instead of a vector of length `j`, or I missed something?
The kindle version shows at 44.90$ for me though.
Running `ldd` on a hello word app shows ``` linux-vdso.so.1 (0x00007fffc6be6000) libgmp.so.10 =&gt; /usr/lib/libgmp.so.10 (0x00007f6546310000) libm.so.6 =&gt; /usr/lib/libm.so.6 (0x00007f654600c000) librt.so.1 =&gt; /usr/lib/librt.so.1 (0x00007f6545e04000) libdl.so.2 =&gt; /usr/lib/libdl.so.2 (0x00007f6545c00000) libffi.so.6 =&gt; /usr/lib/libffi.so.6 (0x00007f65459f7000) libc.so.6 =&gt; /usr/lib/libc.so.6 (0x00007f6545659000) /lib64/ld-linux-x86-64.so.2 (0x00007f65465a3000) libpthread.so.0 =&gt; /usr/lib/libpthread.so.0 (0x00007f654543c000) ``` If you are building on the same OS/distro as you plan to run your applications this is usually not a problem since you most likely end up having these libs installed anyway (at least on Linux). However if you intend to run on a different version it makes sense to create fully static builds e.g. by using https://github.com/mitchty/alpine-linux-ghc-bootstrap
Buuut there is a edition from 1602! http://imgur.com/a/eY95y
Amazon’s pricing algorithms go a bit strange sometimes.
&gt; It's better than python, ruby, and java. Worse than C obviously but that's an odd comparison in my opinion since C/C++ aren't "safe" as Haskell is. Unfortunately the nature of abstraction is that it will always be slower. I am not so sure about this. Obviously, *low-level* descriptions will always be faster in C than Haskell, but high-level abstractions enable advanced optimizations (such as automatic parallelization) at compile- and/or run-time that wouldn't be available to a C compiler. Of course, it is always possible to perform these optimizations by hand and write the code directly in C, but in practice, you're very unlikely to achieve the same level of performance than a very smart compiler. Naturally, while GHC is very good, it is not there yet (neither is the rest of the ecosystem). But if you think about it, people used to write assembly for performance reasons. Now, this is vastly considered counter-productive.
I see quite good results from that `-dedupe` option, all.js shrinks from 7.5M to 5M and all.min.js from 2256642 to 1369614 - almost 2 times smaller. And also I see quite a bad thing, probably related to https://github.com/ghcjs/ghcjs/issues/449 - during `stack build` GHCJS during compilation of one our packages having significant amount of TH consumes about 6.7G at peak. And that makes my computer start using swap - is it some expected sids-effect of the last changes? Or maybe we should continue this discussion in that Github issue?
Oh, moving to Haskell wasn't my decision. They were already well down the line when I joined. The CTO made the call, CEO agreed to take the leap, and it's all played out very nicely. At that point, they'd been burnt enough by node to see it's not the right tool for building complex backend code, so Haskell probably came as a breath of fresh air.
Can you clarify what you mean by *large*? How many SLOCs (for lack of a better measure), how many people (coders and testers), at least approximately? I'm just curious. People label all kinds of software projects as *large*, pretty much independent of size.
Memory usage doesn't seem to be related to `-dedupe` flag so it seems to be unrelated regression, posted comment on Github
could you try: url: http://tolysz.org/ghcjs/ghc-8.0-2016-09-26-lts-7.1-9007001.tar.gz sha1: c6137a44fcecaeb2771f9bbce3450ded5ce7c736 This does not include the patch proposed in the thread please not you will need to backup/remove old install
&gt; Free monads permit unlimited introspection and transformation of the structure of your program. This isn't really true. You can't introspect past a lambda.
You cannot pretty print a free monad unless the base functor does not have any lambdas. The show instance for `Free` has: (Show (f (Free f a)), Show a) =&gt; Show (Free f a) It could alternatively be formulated as: (Show1 f, Show a) =&gt; Show (Free f a) Basically, if you cannot pretty print the base functor, there's no hope of pretty printing `Free`. 
I've worked as an intern at a Haskell shop and am currently working full time as a Haskell developer. - I talk about my job on social media and talented Haskell developers come out of the woodwork wanting to apply. Right now, we're actually only looking for folks who want to work mostly-full-time on our PHP applications, and we're finding it *extremely* difficult to find qualified developers there. We have three proficient Haskell developers and are currently training a few more. You need to be experienced to start a project, but once we've established a "framework" for the domain, it's very easy and quick to add modifications. - I've only been developing for ~2.5 years total, so I'd say juniors can appreciate Haskell fine. It's also possible that the lack of experience with other paradigms might make it easier to learn pure FP. The `filterM [const True, const False]` powerset trick, the elegance of the `fibs` definition, using `Maybe` to handle errors, etc. are all fine ways of impressing folks with Haskell's conciseness and safety. - Maintenance is great. I can make some pretty big changes to our data representation and be pretty confident that everything still works. Refactoring is a breeze. Everything is highly modular and easy to work on, and the types guide you pretty well. We have production code that was written by a ~2 month Haskeller :) - Testing and QA are certainly easier in Haskell -- the Haskell project I work on has better coverage and more exhaustive tests with far fewer lines of test code (hooray for test generation and QuickCheck!) than most of our other projects. I think we have around 20-25K lines of Haskell in the company spread across three projects. Generally, I keep finding that adding more functionality doesn't add all that many lines of code, and after refactoring, many features are a net negative for code size.
Thank you for your reply. Which company are you working for? (or have I asked you already, in a different thread?)
I keep seeing `Vec` written where the last type variable is `n :: Nat` and not the `a` it's polymorphic in, thus preventing an implementation of eg Functor. Is there a reason for that?
 type Π = Sing -- because I can You made my day !
That has to be a glitch. Also, I think the paperback price is also high - I don't think I paid nearly that much.
I lay unto þe: whomever haþ bounden a monadic value o'er return ſhalt by no meanes obtaine a distinct value. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rust] [An Onion Approach: Part 2 • \/r\/haskell](https://np.reddit.com/r/rust/comments/54wfuh/an_onion_approach_part_2_rhaskell/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
&gt; Come to think of it, this might be my number one issue with the language as it stands! As a Haskell beginner, dealing with infinite loops was my number one problem in writing my first few programs.
Try to abstract the concept of Role from its behaviors. Without knowing the details of your use case, you could write something like: data Role = Admin | RegisteredUser | Guest Then it is no problem to have a function of type `UserId -&gt; Role` in `AppSettings`. Define the function that translates a `Role` into a `Route App` somewhere else, like in an `Import` module. This is a common pattern in Yesod. Besides avoiding circular dependencies, you also want to try very hard to keep as much as possible out of `Foundation`. We have found that out-of-control growth of `Foundation` is the main bottleneck for scalability of a Yesod app in complexity.
&gt; high-level abstractions enable advanced optimizations (such as automatic parallelization) at compile Oh, this is very true. Parallelization is where haskell shines.
No, this is correct; he is trying to write (pseudocode) slice i j (x:xs) = slice (i-1) (j-1) xs What I am wondering about is how this type checks. Where is the proof that `SSucc i :&lt;= SSucc j`?
Specifically, it's because the syntaxes of Coq and Agda are such that parameters have to be before indices. (You have, e.g., `Inductive Vec (A: Type): nat -&gt; Type`, which doesn't offer any way to put `A` after the `nat`.) FWIW, Idris infers which are which and allows them in any order. So you do in fact have [`Vect n a`](https://github.com/idris-lang/Idris-dev/blob/master/libs/base/Data/Vect.idr#L13).
Usually, it means the following: Given an operation `(+) :: B -&gt; C -&gt; D` (binary here for convenience, but the same principle applies to other arities), define the pointwise lifting of `+`, `+.`: (+.) :: forall a. (a -&gt; B) -&gt; (a -&gt; C) -&gt; (a -&gt; D) f +. g = \a -&gt; f a + g a Frequently, the same symbol is used for `+` and `+.`, and the reader must infer from context which definition applies. In Haskell, the `Monoid` [instance](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.Base.html#line-268) for `(a -&gt; b)` is defined this way.
Here's some code copy-pasted from some experiments I did a while ago on this. It's not exactly what you wanted, but still maybe useful. the FOAS is `Core :: Nat -&gt; *` , the Nat representing how many free variables are in a term. `Lam :: Text -&gt; Core n -&gt; Core (n + 1) -&gt; Core n` (`Lam n dom cod` is `\(n : dom) -&gt; cod[n]`): `Bound :: Fin n -&gt; Core n` `data Fin (n :: Nat) where Fin :: (KnownNat m, CmpNat m n ~ 'LT) =&gt; Proxy# m -&gt; Fin n` newtype CoreH = CoreH { unCoreH :: forall n. KnownNat n =&gt; Proxy n -&gt; Core n } bind :: (forall n. Core n -&gt; Core (n + 1) -&gt; Core n) -&gt; CoreH -&gt; (CoreH -&gt; CoreH) -&gt; CoreH bind b (CoreH ty) f = CoreH $ \(Proxy :: Proxy n) -&gt; withNatOp (%+) (Proxy :: Proxy n) (Proxy :: Proxy 1) $ let v = CoreH $ \(Proxy :: Proxy m) -&gt; gcastWith (unsafeCoerce Refl :: CmpNat (m-(n+1)) m :~: 'LT) $ withNatOp (%-) (Proxy :: Proxy m) (Proxy :: Proxy (n + 1)) $ Bound (Fin (proxy# :: Proxy# (m-(n+1)))) in b (ty (Proxy :: Proxy n)) $ unCoreH (f v) (Proxy :: Proxy (n + 1)) lam :: Text -&gt; CoreH -&gt; (CoreH -&gt; CoreH) -&gt; CoreH lam n = bind (Lam n) pi :: Text -&gt; CoreH -&gt; (CoreH -&gt; CoreH) -&gt; CoreH pi n = bind (Pi n) runCoreH :: CoreH -&gt; Core 0 runCoreH (CoreH f) = f (Proxy :: Proxy 0) newtype CoreM m = CoreM { unCoreM :: forall n. KnownNat n =&gt; Proxy n -&gt; m (Core n)} bindM :: Applicative m =&gt; (forall n. Core n -&gt; Core (n + 1) -&gt; Core n) -&gt; CoreM m -&gt; (CoreM m -&gt; CoreM m) -&gt; CoreM m bindM b (CoreM ty) f = CoreM $ \(Proxy :: Proxy n) -&gt; withNatOp (%+) (Proxy :: Proxy n) (Proxy :: Proxy 1) $ let v = CoreM $ \(Proxy :: Proxy m) -&gt; gcastWith (unsafeCoerce Refl :: CmpNat (m-(n+1)) m :~: 'LT) $ withNatOp (%-) (Proxy :: Proxy m) (Proxy :: Proxy (n + 1)) $ pure $ Bound (Fin (proxy# :: Proxy# (m-(n+1)))) in liftA2 b (ty (Proxy :: Proxy n)) (unCoreM (f v) (Proxy :: Proxy (n + 1))) IIRC, the technique is a modified version of something from Unembedding domain-specific languages, `CoreM` is because you might want to be able to use a monad in constructing your HOAS terms. You can have `lam :: Text -&gt; CoreM m -&gt; (CoreM m -&gt; m (CoreH m)) -&gt; CoreM m` IIRC. This should work as a pass over a PHOAS AST. 
What is "sizable"? Is it a linear slowdown? quadratic? Is it still practical for high-er performance programs?
I was pretty happy reading Haskell Design Patterns.
The performance overhead with the "fastest" free monads is a constant time overhead. But the constants are quite large and can be problematic. /u/edwardkmett has more experience with profiling free monads than I do.
Cool. I am mostly interested in the semi-high performance case like high performance web backends (cases where CPU actually does dominate DB lookups). But not things like graphics or fancy GUIs.
Thank you !
Tardis computations aren't reversible, so it's definitely not the same thing as in that paper. But now that you mention it, receiving a value (from the future or from the past) constrains the value sent and the value received to be the same value. And the Tardis monad makes it very clear that thanks to laziness, the value to be transferred doesn't have to be known at the point when the sending or receiving is done. Could laziness be a good model of fractional types?
For sets you actually want to identify all permutations, not just the reverse. In a set, "first" as a function makes little sense unless you have a suitable order on the set. In which case you could turn it into a data structure that already represents this order.
&gt; The refactoring story is where Haskell is the biggest win. You can refactor fearlessly. And better yet, the compiler actually helps you with the refactoring. Not even just refactoring but general maintance. Just this weekend I was able to add functionality to a library highly abstracted by type classes without ever needing to take the time to understand the library's core. Given the types I could show that my function must be correct and get on with more important work. I threw in some tests too, but what could have been an all day job of educating myself became a 30 minute project of deriving code based on types.
Are quotient types related to the "higher inductive types" of Homotopy Type Theory? (And can the former give some intuition to what the latter are?)
Really appreciate your reply. I posted the OP in all the excitement of successfully subscribing to a mailing list, but these things then take a life of their own. Wake up to the convo bouncing here, and it's been damage control ever since. I very much regret using the word monopoly, which seems to be way more negative for the tribe than for me. I work in the government, and spend almost all of me time fixing bad stuff that happens inside (what I think of of as) a natural monopoly. And the bad stuff is almost always due to structural problems, not people. My worst mistake, however, was being inattentive to timing of an attempt to communicate with a mixture of humor and bluntness across cultures - my bad. The "ghc interface" I was referring to is more the open-source to open-source ecosytem, the people interfaces like pull requests and issues lists and standards and ways we decide what the actual code interface is. There is sometimes less fun and positivity there. And again, when I talked about ghc, I didn't mean the ghc team in any way. I don't know them, which I'm sure is my loss. I meant their iconic status as a great and rightly celebrated success. Adoption rates are important for me because haskell is important to me. At a conference in my home town there was not a single full-time haskellers out of about 50 locals (1 really, cause we regard kmett as a local). I want my friends to get jobs so it can be their shout once in a while! But I really appreciate your advice, and I'm off to atone for stirring things up by finding an open-source effort to have fun in. 
Yes, they are. Higher Inductive Types are inductive types where not only "points" (i.e. elements of a type) are inductively given, but also potentially higher structure -- i.e. "higher paths" (equalities, equalities between equalities, etc). Quotient Inductive Types (one way to describe quotients) are the special case of higher inductive types where at most one additional level of structure is provided -- i.e. we have elements and also some additional equality relationship on those elements. So we could imagine using pseudocode that the quotienting of the nats into the nats mod 2 could be given by: data NatTwo where NatTwo :: Nat -&gt; NatTwo mkPath :: (x : NatTwo) -&gt; (y : NatTwo) -&gt; (z : Nat) -&gt; Either (unNatTwo x + 2*z == unNatTwo y) (unNatTwo y + 2*z == unNatTwo x) -&gt; x == y unNatTwo :: NatTwo -&gt; Nat unNatTwo (NatTwo x) = x The first part of the data definition is a typical constructor. The second is a "path constructor" that given a proof that the underlying nats of two NatTwos differ by a multiple of 2, then produces a proof (by fiat) that the two NatTwos are equal. Note that in general we also need to "zero-truncate" these constructions. This is because it is a basic (and at first counterintuitive) fact of homotopy theory that even if we define something using low-dimensionsal structure, it may have "induced" higher dimensional structure. The famous example of this is the 2-sphere (a normal 3-d ball, like a basketball). Even though it is defined only by a point and a single 2-path (which you can think of as the "sheet of paper" that you attach at all edges to that point), nonetheless it has a higher path structure. At the 3-path level this is given by the Hopf fibration. (but it continues to have a strange higher structure as one goes up further in dimensions too!). So the "zero-truncation" is applied to just "kill" such higher structure and ensure we remain only at the level of points and one level of equality paths. Quotients of this form are discussed in section 6.10 of the HoTT book as a good motivating example of Higher Inductive Types.
Thanks.
There's a version of such syntax in the `cubical` experimental proof assistant for cubical type theory. See here for examples: https://github.com/mortberg/cubicaltt/blob/master/examples/integer.ctt https://github.com/mortberg/cubicaltt/blob/master/examples/quotient.ctt This isn't really like the syntax I used above, take note. Rather it constructs higher paths by giving their "surfaces" explicitly via construction of endpoints of intervals. Such a syntax really only makes sense in an explicitly cubical model. The syntax is weird to grasp at first but you should remember to read `&lt;i&gt;` as sort of a "lambda over the dimension variable `i`" and think of dimension variables as always ranging between 0 and 1 (and only ever given explicit fully instantiated values at those two points, with the middle 'implicit'). 
How would you disable this hypothetical warning locally, when you do want a recursive value? A redundant `~` might work (`let ~xs = 1 : xs`) but I’m not sure if there would be edge cases with surprising semantics.
Unfortunately after some hours of recompilation I still see the same excessive RAM usage, sorry :-\
I think the others have done a great job responding to these points. I just wanted to clear one thing up about point 1: &gt; What is Haskell's approach, or the most common approach taken in Haskell code, to concurrency? Do you approach it like JavaScript, emitting and handling events in a single-threaded loop and avoiding synchronous IO operations, or like Erlang/Elixir, where you spin up dozens or hundreds or thousands of runtime-not-OS processes and send messages between them? Haskell threads are extremely light weight, so it's not atypical to spawn hundreds of thousands of them. Concurrency primitives are included for erlang style message passing. One thing to keep in mind though is that the Haskell (well GHC) run-time ends up mapping all the I/O operations (which have been written in a synchronous manner) into asynchronous epoll/kqueue/etc operations under the hood. Essentially, you get the benefits of asynchronous IO (like node) but you get to write your code as though it were synchronous (unlike node). Also, unlike Node and Python, you get to fully exploit the multi-processing capabilities of your system, since the Haskell run-time system schedules the user-level threads on any CPU. 
I can only answer to the performance part. I did an experiment a few months ago in which I implemented the same Sudoku solving algorithm in multiple languages in order to compare performance. Using Python, Clojure, Ruby, Java, Scala and Haskell. Java and Haskell were comparable, solving the test puzzle in sub-second time.
I [copied](https://github.com/basvandijk/scientific/blob/master/src/Math/NumberTheory/Logarithms.hs) the Math.NumberTheory.Logarithms module into [scientific](http://hackage.haskell.org/package/scientific) (I couldn't depend on the package because arithmoi is not in the Haskell Platform while scientific is).
Truly weird!
Sounds like thievery
Wow! Monads have been around for a long, long time!
Probably to parallel normal assignment, ```binding = value```
Variants of this have been proposed quite a few times over the years. The standard answer to simple cases is to use applicative syntax; that is, where you want to write `f !x`, you can instead write `f &lt;$&gt; x`. And where you want to write `f !x y !z`, you could write `f &lt;$&gt; x &lt;*&gt; pure y &lt;*&gt; z`. Obviously, this doesn't work for many of your examples, such as in `if` statements and the like. I'm not familiar with Idris, so I don't know if it has an elegant answer, but a common issue is how to control how far to hoist the monad. So, e.g., if I write `if x then !y else z`, and `x` is false, is the action described by `y` performed? Expectations differ on whether it should be or not!
&gt; Obviously, this doesn't work for many of your examples, such as in if statements and the like. Unfortunately these are the cases where it's needed the most! Your example could be done with applicative operators (as you suggest), or even with idiom brackets. However I find using monadic expressions inline in expressions, such as conditionals and even just in bindings, very useful (even for Maybe, State, or Reader). When doing more stateful/monadic programming in Haskell I find this to be a *major* pain point, to the point where I'd rather use other languages because I feel like Haskell is not expressive enough to handle these situations. I feel it really needs to be worked on. &gt;I'm not familiar with Idris, so I don't know if it has an elegant answer, but a common issue is how to control how far to hoist the evaluation of the monad. So, e.g., if I write if x then !y else z, and x is false, is the expression for y evaluated? Expectations differ on whether it should be or not! This is a really good point! I'd argue that for most cases you'd want y to *not* be evaluated unless the condition is met.
&gt; But something about it feels strange, perhaps it's just that I'm used to haskell's way of doing things, but it's just too far from the underlying semantics. Not any moreso than other syntactic extensions. For trivial cases, the lift/applicative operators would obviously be preferred, but mainly this is useful for the more expressions involving monadic values -- especially conditionals (if, case), nested uses of monadic values, and as you note binding values where the argument order might not be convenient. It just simplifies a lot of monadic computations (IORef/STRef, State, Reader, Maybe, ...) to be more readable and easier to type/express.
You can have the 'reversed' form if you want it today. No sugar necessary. getLine &gt;&gt;= \text1 -&gt; getLine &gt;&gt;= \text2 -&gt; return $ text1 ++ text2 do notation gives you a more traditional "imperative" look and feel, though.
there can actually be a simple answer to that: an \x y z -&gt; if x then y else z is of type Bool -&gt; m a -&gt; m a -&gt; m a. So y and z and any occurances of ! within them should only be evaluated when they normally would be.
Wow, this is great. Thank you. I've been somewhat aware of type and data families but I was never able to actually make sense of them or comprehend what they're supposed to do but coming at them with this question on my mind they start to make a lot more sense. 
Typical Haskell monads are strict in the first argument to `&gt;&gt;=`, so in the second form you have to walk the application chain to reach the left-most `m &gt;&gt;= f` before you can do any useful work. For monads where the cost of `&gt;&gt;=` depends in some way on the complexity of the monadic value, like most implementations of `Free`, you end up with a situation similar to left-nesting `++` for lists. The major exceptions are lazy monads, where you don't need to find the left-most expression, and `Set`.
I must promote the [`InstanceSigs` extension](http://downloads.haskell.org/~ghc/8.0.1-rc4/docs/html/users_guide/glasgow_exts.html#ghc-flag--XInstanceSigs) which lets you write type signatures for methods of instances. Sometimes I see the type signatures written out [in comments](https://github.com/julmue/Crush/blob/d12857fccafe0f2a24853ed756935e60bdf2ec86/src/Language/Lambda/Syntax/Nameless/Exp.hs#L48) when the compiler is itching to check it for you! It makes it easier (for me) to understand method definitions, especially given type operators (like `-&gt;`): {-# Language InstanceSigs #-} instance Monoid m =&gt; Monoid (a -&gt; m) where mempty :: a -&gt; m mempty = const mempty mappend :: (a -&gt; m) -&gt; (a -&gt; m) -&gt; (a -&gt; m) mappend f g x = mappend (f x) (g x) Each method refers to itself at a different type: The definition of `mempty @(a -&gt; m)` (using the syntax of [`TypeApplications`](http://downloads.haskell.org/~ghc/8.0.1-rc4/docs/html/users_guide/glasgow_exts.html#ghc-flag--XTypeApplications)) ignores its argument and returns `mempty @m`. The definition of `mappend @(a -&gt; m)` uses `mappend @m`: mempty @(a -&gt; m) = const (mempty @m) mappend @(a -&gt; m) f g x = mappend @m (f x) (g x) (this uses [hypothetical syntax](https://ghc.haskell.org/trac/ghc/ticket/11350#comment:12), you can't specify type parameters to methods) It may help some people to see it differently ([`const x _ = x`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Function.html#v:const) inlined, [`mappend`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Monoid.html#v:-60--62-) written infix): mempty _ = mempty @m (f &lt;&gt; g) x = f x &lt;&gt; g x (visible type application [doesn't work](https://ghc.haskell.org/trac/ghc/ticket/12363) with infix operators or variable operators and it looks ugly anyway). ---- (`mappend` will [one day](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid) be removed from `Monoid` and replaced by `&lt;&gt;` from [`Semigroup`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Semigroup.html#v:-60--62-))
I think it's because `&lt;-` looks like the "element of" symbol ∈ when you're using it in set comprehensions although I don't have any historical sources to back that up. In fact, do i &lt;- [1,2] j &lt;- [1..4] return (i,j) is equivalent to [ (i,j) | i &lt;- [1,2] , j &lt;- [1..4] ]
*þe and *bounden
(Closed) type families are functions on types — more powerful than term-level function in several ways. They allow patterns that are [*non-linear*](http://stackoverflow.com/questions/35891663/what-are-nonlinear-patterns): (examples from [/u/goldfirere's thesis](http://cs.brynmawr.edu/~rae/papers/2016/thesis/eisenberg-thesis-draft.pdf)) type family Equals (x :: k) (y :: k) :: Bool where Equals a a = 'True Equals a b = 'False while this won't compile equals a a = True equals a b = False It's even possible to pattern match on partially-applied data constructors on the type level (called *unsaturated matching*), type family IsLeft :: (con :: a -&gt; Either a a) :: Bool where IsLeft 'Left = 'True IsLeft 'Right = 'False something you cannot do at the value level! isLeft Left = True isLeft Right = False **Edit:** It's also possible to split type application: type family Split (x :: b) :: Maybe (a -&gt; b, a) where Split (f x) = 'Just '(f, x) Split _ = 'Nothing while `split (f x) = …` won't parse.
Yes, the other direction is the hard part. This is why I had to write out the definition of `&lt;=` in the `Ord` instance, instead of using singletons' ability to derive `Ord`. GHC sees that we're assuming `(Succ i :&lt;= Succ j) ~ True` and simplifies to `(i :&lt;= j) ~ True`, as desired.
Nice. This is similar in spirit to my attempt, but by avoiding the singletons package, it's all more explicit. My use of `TypeInType` is incidental, not required. (It's forced by some design decisions in `singletons`, but hand-written singletons wouldn't need it.)
[Pseudocode in CS texts frequently uses the left arrow for assignment.](https://en.wikipedia.org/wiki/Pseudocode#Example)
Thanks for this excellent answer, it's highlighted some power of type families I'd never considered because they don't make sense at the value level. The breaking apart of applied type constructors and matching on unapplied ones seems really powerful.
I think you could just turn the warning off, like name shadowing. 
"Following the code" means different things to different people: function composition is traditionally written right-to-left, variable binding left-to-right ..
Can't you simply execute actions according to call by value? Why would you want to do something else?
GHCJS implements Haskell, therefor has quite a runtime (i.e.: to implement threading and laziness). Here the diffs between PureScript and Haskell: https://github.com/purescript/purescript/wiki/Differences-from-Haskell
About the difficulty to detect upstream termination in "pipes": a Pipe can't do it, but a Producer -&gt; Producer function can. Same with the "streaming" package, which altogether lacks an "intermediate stage" as a separate type.
Yes! It has a similar philosophy except that mine is extremely simple. The thing is, if you don't use NixOS, you are left with shell script or just Python tooling for configuration management. I prefer Haskell :) I had a look at Turtle and Shelly and I ended up writing my own monad for the shake of the fun and to have a bit more of control I needed like tracing and environment handling.
ordering is unambiguous with `!`: it's like eager evaluation, arguments first, then the function body
if it could be made a language option, shouldn't it be possible to do w/o any concern for demand?
but also: `&lt;-` is often used for assignment too, especially in pseudocode
VHDL uses the left fat arrow `&lt;=` for assignment.
+1. That is a thing I sorely miss from OCaml.
[Standard Chartered Haskell dev growth](http://images.angelpub.com/2011/07/7421/exponential-curve.png)
but If I just want to return a single \
I've been advocating `{-# options_ghc -fno-full-laziness #-}` for performance-important modules for a while.
Great blog post! I only had one comment, which has nothing at all to do with the main point of the post. You chose a really funny definition of `Await`. This is fine for demonstration purposes, but it's an interesting mix of `conduit` and `pipes`: like `conduit`, upstream can say "I'm done" and downstream can respond to it. But like `pipes`, it requires that upstream and downstream have the same return type. In the `conduit` world, the two choices here would be (1) s/`Either r i`/`Maybe i` or (2) add an extra type parameter for the upstream return value, e.g. `data Pipe i o u m r`. This comment has nothing to do with the actual point of the blog post, I just couldn't resist :)
&gt; this breaks the syntactic identity `x == do { x }` only if `x` is an expression that contains "free" (ie non-`do`-wrapped) `!`s, right?
I mean, to me it doesn't look much different from checking whether the expression `x` contains a free `&lt;-` in Haskell 98. It's a purely syntactic check in both cases. Unless I missed something.
All too aware :(
If closed type families are functions on types, what are open type families? Also I looked at that thesis and it led me to [dependent-db](https://github.com/goldfirere/dependent-db/) which is truly awesome! I have been looking for something like this to study.
See also: https://github.com/michaelt/streaming/issues/6
&gt; Haskell dev for low latency (&lt; 100 microsecond) I'm intrigued by that being possible using Haskell, is Mu based? Could be possible to give some more details about it? :)
what does this do, exactly?
Doubt it "We have 10 more open roles currently" "You would join an existing team of 25 Haskell developers in Singapore or London" When I was at Standard Chartered last year I think there were 8 Strats or so. The growth has been astounding.
Fortunately you only need to write proofs when you peek inside the abstraction. If your function uses only functions that have been proven to be good, like sum set or set union and intersection, then you don't need to prove anything. You only need to prove something when you open up a set and look at the list inside.
Tsuru Capital have been doing this for some time, see http://www.tsurucapital.com/en/ and search for them online (not sure how many details they have shared in the past)
Is there any example of quotient types where an imperative/OO programmer wouldn't say "just use java.util.Set and enjoy the same guarantees because it's encapsulated"?
Is this something I can pick up this weekend and apply on Monday?
/u/dons what supply/demand proportion do you observe? Namely, how many more CVs do you get per advertised position ?
I've had 125 resumes for 20 open roles this year, approximately.
Ah, good to know! Let's see, I think I understand why that works for quotient types: if `f` and `g` preserve `(≈)`, then so does `f . g` and so does `h . f` for any pure function `h`. I feel like I'd also have to say something about `f *** g` and `f ||| g`, for a suitably-generalized version of `(≈)`. Next, I'd like to translate that intuition to figure out whether that would also work for Higher Inductive Types, but that made my brain explode. I don't even understand whether such a language would have pure functions in them, or just homomorphisms...
The relevant equation from my `Ord` instances becomes (essentially) ``` type instance (Succ i) :&lt;= (Succ j) = i :&lt;= j ``` Thus when GHC ever sees `(Succ i) :&lt;= (Succ j)`, it reduces it to `i :&lt;= j`. This includes occurrences in constraints, such as the assumed `((Succ i) :&lt;= (Succ j)) ~ True`. Let me know if this doesn't clarify!
Sounds reasonable. And what's the downside of something like this?
I've been told it's impossible to get work visas for Americans without bachelor's degrees. You agree, or should I apply anyway?
&gt; If closed type families are functions on types, what are open type families? Single-method "kind classes".
So when's the Haskell team in New York starting ;) ?
Lol I took Middle English a couple years back so this was just my speciality (no hard feelings obviously)
Clash-lang is taking baby steps there!
Yes, that is exactly right. In a dependently typed language a proof is actually a value itself. So if you have x == y implies f x == f y that means that given a proof p of x == y you can obtain a proof q of f x == f y, i.e. a function that takes a proof p and produces a proof q. So associated with each function f : A -&gt; B is a function f' that transforms equality proofs of A's into equality proofs of B's. For the composition you simply have (f . g)' = f' . g', and similar for `f *** g` and `f ||| g`. For any normal defined function f the compiler is able to automatically assemble the f' function by applying those kinds of rules to the code of f. It's only when you open up an abstraction barrier that you have to provide a custom proof. There's one wrinkle that makes the analogy with Eq not work out. The x == y can't be a function that returns a boolean because that wouldn't work when x,y are functions or otherwise infinite values. Instead x == y returns a *type*, the type of equality proofs for x == y. The f' function takes a value p : x == y, i.e. a proof of x == y, and returns a value q : f x == f y, i.e. a proof of f x == f y. So the type of f' is (x == y) -&gt; (f x == f y). The thing with higher equalities/paths is simply that x == y is a type like any other type, and hence two values p,q : x == y can themselves be equal p == q. But the same reasoning applies to p == q, that is again a type. That's how you get higher equalities, but there's actually nothing particularly "higher" about them. It's just that x == y is a type, and hence we need to know when two values p,q of that type are equal, just like we need to know that for any other type.
Yes, pretty much. One valuable feature of pipes that streaming keeps is that producers have a "return value" separate from the elements they produce. This allows tricks like ensuring a function exhausts the producer it receives as an argument, by being polymorphic on its return type.
Sure, but that doesn't matter. All implementations of Set that I work with are good enough.
Another example: pipes-cryptohash (https://github.com/NicolasT/pipes-cryptohash), which allows to stream *ByteString*s through and calculate a checksum of that data 'on the side', available once the stream is exhausted, e.g. -- | Create a 'BS.ByteString' 'Producer' wrapping a 'BS.ByteString' 'Producer' -- which calculates a 'Digest' on the go. This 'Digest' will be tupled with -- the result of the original 'Producer' when the stream ends. hash :: (HashAlgorithm a, Monad m) =&gt; Producer BS.ByteString m r -- ^ Source 'Producer' -&gt; Producer BS.ByteString m (r, Digest a)
-fparital-full-laziness ? I'm not sure about this static-maximum-size idea though. String, (lazy) Test, and (lazy) ByteString can all be infinite and they appear is a *lot* of types that are, in practice, much smaller than infinite.
Why not do the CPS transform of the value like in [Control.Monad.Trans.Free.Church](http://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Trans-Free-Church.html)? newtype Pipe i o u m a = Pipe { runPipe :: forall r. (a -&gt; m r) -&gt; -- return (o -&gt; m () -&gt; m r -&gt; m r) -&gt; -- yield ((u -&gt; m r) -&gt; (i -&gt; m r) -&gt; m r) -&gt; -- await m r } This way, there's no tree structure being explicitly built in memory.
In most languages, \ is used as an [escape character](https://en.wikipedia.org/wiki/Escape_character) for strings. For example, `"\r\n\t\f"` isn't interpreted as the character `\`, followed by the character `r` followed by the character `\` followed by an `n`, etc. Instead, it's interpreted as being the string containing a carriage return, then a line feed, then a tab, then a form feed. Similarly, `"\""` is interpreted as being a string with the character `"`, instead of a syntax error. This makes it easy to have strings like `"As Feynman once said, \"I think I can safely say that nobody understands quantum mechanics\"."` The tradeoff of this (i.e. representing "characters" like carriage return, the null character, tabs textually and allowing you to use quotation marks inside of a string) is that in order to use a raw \ in text, you need to type it as \\. That way, `"\\r"` is the character `\` followed by the character `r`, instead of being a carriage return.
Want to come back laser?
`listM2 (+) x y` is just as readable as `x + y` to you? We might as well not use operators at all then.
Yes.
It would be nice to have. Meanwhile, for your simple examples, I would just write it in a pointfree style. In your first example, it can even expresses intentions better. In that case, you just want to lift a function / operator. So you can just say "Hey, lift this function that accepts two arguments" or `liftA2: import Control.Applicative (liftA2) add :: Maybe Int -&gt; Maybe Int -&gt; Maybe Int add = liftA2 (+) The `if` example is a bit less pretty pointfree, but you can do: ifExample :: IO () ifExample = do readWaldo &gt;&gt;= bool' (do print "True: It's waldo!" ) (do print "False: Nope" ) where readWaldo = isWaldo &lt;$&gt; readLn isWaldo = (== "Waldo") bool' = flip bool And with custom helpers you can do ifExampleCustom :: IO () ifExampleCustom = do if' readWaldo `then'` print "True: It's waldo!" `else'` print "False: Nope" -- or more detailed if' (isWaldo &lt;$&gt; readLn) `then'` do print "True: It's waldo!" `else'` do print "False: Nope" where readWaldo :: IO Bool = isWaldo &lt;$&gt; readLn isWaldo = (== "Waldo") where you have several helpers: (then') = id (else') = id infixl 1 `then'` infixl 1 `else'` infixl 1 % (%) = id if' :: Monad m =&gt; m Bool -&gt; m a -&gt; m a -&gt; m a if' b m1 m2 = b &gt;&gt;= (flip bool) m1 m2 -- or -- b' &lt;- b -- if b' then m1 else m2 The `%` operator can be used for any other case, it also allows you to do things like: finally % print "doing something" % print "cleaning up resources" Which is synonymous to: example = do do print "doing something" `finally` do print "finally" print "cleaning up resources" EDIT: forgot to flip the bool (by default it runs the first arg if False and second one if true).
I think it might be related to `stack build -j1` i.e. how many workers are compiling, and it you have a lot of CPUs each will eat say `1GiB` of your RAM. The default is to use the number of your cores.
That's fair, thanks for the thoughtful answer. I think the sense of urgency comes from the feeling from many that we need more rapid change, we need to experiment now. It feels like much of haskell's infrastructure is slowly drifting back in time, rather than keeping pace with modern developments.
Way better than I would do in an early form of a language other than English... you might want "whoever" (it's nominative case) and "ybounden" (past participle, the "y" is analogous to the German "ge-" prefix for past participles). Now, back to working so I know more about Haskell than Middle English, though I should set my sights higher than that. :)
No, the example was `return (!x + !y)`.
I'll try to use `-j1` later this week but that hypthesis seems to be doubtful as I have only 4 cores and 2G per core sounds scary. And also does stack support multiple workers per single package?
Sorry for that - but I realized I'd asked a dumb question and hadn't properly read the code posted!
Ah, thanks. Interesting - except those TypeLits don't give you the 'inductive style', I guess Idris and other dependent languages all have the same problem with peano representation of Nat? Is there a way to have the such types with no run time representation? This solver seems to be actively developed and used with the 'clash' DSL. https://hackage.haskell.org/package/ghc-typelits-natnormalise
Yeah but adding even a single additional operator in that expression...
This has been tried and is slower. https://github.com/Gabriel439/Haskell-Pipes-Library/issues/100#issuecomment-27728020 The use of `next` is much more important and characteristic of the use of the library, than that comment makes clear.
And a `return` which is not necessary with `liftM2`.
[genericParseJSON](http://hackage.haskell.org/package/aeson-1.0.1.0/docs/Data-Aeson.html#v:genericParseJSON) [genericToJSON](http://hackage.haskell.org/package/aeson-1.0.1.0/docs/Data-Aeson.html#v:genericToJSON) using these you can still benefit from Generics but it also allows you to have more control. You might be specifically interested in [fieldLabelModifier](http://hackage.haskell.org/package/aeson-1.0.1.0/docs/Data-Aeson-Types.html#v:fieldLabelModifier)
Oh, I see, the quasiquoter does that. Very cool. Thanks!
Yes, I just was reading this tutorial that looks a great one: https://artyom.me/aeson The section "Generics: customising field names" describes what you mentioned.
I wouldn't say that lets you start playing with higher paths, because Agda won't recognize the second as a path. There _is_ a way to "fake" noncomputable HoTT in agda, but writing the types is significantly less pleasant. You basically use private types to enforce abstraction-hiding along with postulates, like so: https://github.com/HoTT/HoTT-Agda/blob/master/core/lib/types/SetQuotient.agda What you have lets you play with quotiented data though, if you use Nat and NatTwoPath as inputs to a `Setoid` which is a type equipped with an equality relation and a proof that that relation is indeed symmetric, transitive, and reflexive, like so: https://github.com/agda/agda/blob/master/examples/Setoid.agda Setoids are widely used for capturing quotients of types in MLTT, but the additional work of discharging all those proof obligations manually (rather than passing them to the compiler via its handling of equality-as-such) rapidly leads to what I have heard termed "setoid hell". See also: https://fplab.wordpress.com/2015/05/09/hott-for-lazy-functional-programmers/ and http://comments.gmane.org/gmane.comp.lang.agda/7630
Wow, [Data.Aeson.Casing](https://hackage.haskell.org/package/aeson-casing-0.1.0.5/docs/Data-Aeson-Casing.html) looks just like what I am looking for.
&gt; I have a hard time conceptualizing the difference between the two definitions If your two definitions really do describe the same thing, then with univalence you should be able to prove them equal, if that helps :-) (which is to say, you should be able to provide a homotopy between them). 4 makes sense as best I understand it. 5 I think you're off base but I can't tell what you're getting at. In a HoTT setting you should systematically replace the word Set by `U` or `Type` if you prefer. You can define that data type just fine. It just doesn't do what you think. :-) (it may help to swap the suggestive name _=_ for it to something like `AThing` to help disrupt the misleading intuition).
For fun I've implemented it on top of the FT transformer (although I guess the 'connect' function can be made freer): https://github.com/agrafix/free-stream/blob/b5dd47ab11042efafa58023db03be4802cc91509/src/Data/Stream/Free.hs
If they'd let me work 4x10, I probably would. Especially if I got to be on your team, you big fella you. 
I've worked on 3 "large" Haskell code bases at 3 companies. They are: * [Tsuru Capital](http://www.tsurucapital.com/en/), a ~10 people options trading company using Haskell only * Google's [Ganeti](https://code.google.com/p/ganeti/) project, an open-source cluster manager used for internal services * A large client of [FP Complete](https://www.fpcomplete.com/), working on a medical device All projects are between 50k-150k LOC Haskell. I have worked on these project over the last 4 years total. &gt; How did you attract more engineering talent to the team? Did you find Haskellers or did you have to hire good people and give them time to learn? How long does it take people to get up to speed with Haskell, generally? At Tsuru, we didn't really while I was there. Due to the nature of the project, there were typically either 1 open roles or 0 open roles. I think they are attract people by having a reputation of excellent via word by mouth (that's how they got me). I _think_ every hire already had quite a bit of Haskell experience (for me it was 2 years of using Haskell in my free time alongside university), so there wasn't much learning needed. As a result, I found it to be an arrive, start coding next day experience. This was supported by the code base being quite cleanly written. At Google, things are different when you have a ~15000 programmer pool, most of which are pretty highly-skilled, but have no experience in Haskell. Ganeti was at the time around 40% Haskell and 60% Python. When people from the pool wanted to change teams and were curious about Haskell, they could consider joining Ganeti (an ~8 people team). While they could contribute to the Python part from the beginning, some would find that they like Haskell and gradually learn and get into it, getting to a full contributing level within a few months; others would find that they don't like Haskell too much and preferred to keep working on the Python side mostly. Others (me included) joined the team from outside Google, those were usually pretty experienced in Haskell, as they picked the project specifically because it was the pretty-much only place at Google to work in this language. In either case, Google takes care to ensure there's always somebody you can ask to learn and improve, and getting-up-to-speed is a well understood process across the whole company; Ganeti was no exception here. I think the take-away from it is that "if you're big-corp with lots of good programmers, you'll have no problem onboarding non-Haskell people into your team, via both the "find Haskellers" and "good people and time to learn" routes. For FP Complete, as a company that lives from Haskell consulting, I think most people joined via the "find Haskellers" route. Remote work makes FPco pretty appealing, so I guess we get lots of opportunity to pick good people all over the planet. While the earlier hires seemed to be mostly experienced Haskellers that had worked in industry, for later hires we also had people join who knew Haskell quite well but had not used it in industry before. As a result, onboarding is pretty quick. One thing that needs to be said is that remote working is very well understood at FP Complete. Anybody who'd join would find themselves in an environment that's used to doing remote work efficiently. Managers here know how to be good managers, remotely, how to resolve conflicts, remotely, etc. I am not sure if the thing would work just as well if 20 people started working remotely from scratch at the same time (it's possible, I just don't know it). So now comes my overall opinion on this point. I think you'll have no problem finding very good general purpose Haskell programmers in the community that you want to hire. What may be harder is how you can hire them. You'll have it easier if you're in a great location that people can move to. But still be prepared that you won't be able to just source Haskellers from your city. If you have the chance offer remote work, I think you should do it, you'll get access to the full engineering talent of the Haskell community, and you'll get lots of high quality applications the next day. But you should ensure that you build some competence in running a remote team. Another thing that I think is important when hiring. Finding great general-purpose Haskell developers is easy. Things get harder when you are hiring very domain-specific, for example "looking for Haskell dev with years of production experience in data science and distributed computing". By intersecting small sets with small sets you quite quickly get to very small sets! You may be in a better situation if you already have the domain knowledge, so that you don't have to hire for domain knowledge and Haskell at the same time. &gt; Is it easy for junior devs (less than 2 years of experience) to understand and appreciate Haskell ? Or is the goodness apparent only after you've spent years programming? Hmm, not sure I can give a qualified answer here. Most people I know already knew the pains of other programming languages before they learned Haskell. If you're talking about &lt; 2 years of _insdustry_ experience, that'll be no problem for sure. But for &lt; 2 years of total programming experience, I have no data to make a qualified statement. Do you think this will be very relevant for hiring? At least in my environment, most people seeking for programming jobs already have some experience with a couple of programming languages, e.g. from school, university, etc. &gt; What about the maintenance phase of the project? Did new people come in and hate the existing Haskell code-base (like it happens in so many other languages)? Or were new devs able to grok everything and start contributing much faster? My experience across the 3 companies is that with Haskell codebases, it is easier to get started than in other languages. At Tsuru, I was especially amazed that I could join a fairly specialised domain (options trading) and next day read through their code without any WTFs. For Google, I can make direct comparison with other languages. (Btw, getting data on this was one of my personal goals when working on this project.) I found Ganeti's Haskell code to be in _much_ better shape in terms of maintainability than its Python code. This is mainly because it is easier to refactor Haskell and "keep it clean" over the years (and, especially in the context of Google, over the fact that people tend to rotate teams every 18 months) than it is with Python. As a newcomer to the project, picking up bugs reported from production, locating and fixing them was easy if the bug was in the Haskell code, and was hard when it was in Python. This turned out to be a surprisingly simple rule. In terms of picking up development, and with equivalent multi-year language expertise in Haskell and Python, I could start contributing to the Haskell code base much faster, and, importantly, by myself. The python code has lots of unwritten invariants maintained in the collective hive mind of the team. It is difficult to be producive if for every change you have to ask around in your team in order to obtain an understanding of the code. You can imagine this is even worse when people are on vacation. The Haskell code did not have this problem. Haskell makes invariants explicit. &gt; What about technical debt? Did the refactoring story of Haskell live up to its claims? Yes, it did. The Ganeti team was very professional concerning technical debt, keeping track of it explicitly and planning periods of time and man-hours to work down the debt. In Python, we often had to say "we cannot remove this technical debt, as we do not have confidence that the refactoring needed for that will not break things" (for Ganeti, it was very important not to break things in production). In Haskell, we could refactor whatever we wanted.
(continuing here, the reddit limit is too 10000 chars) &gt; Did the testing/QA story live up? Considerably lesser testing required? If it compiles, it runs correctly? Yes, considerably less testing required. But "if it compiles, it runs correctly" only happens often, not always. I found it almost always the case for refactorings, but not for new features. For Tsuru, the whole thing ran very stable with comparatively few tests. For Ganeti, we wrote usual test cases for both Python and Haskell ([example for one in Haskell I implemented](http://git.ganeti.org/?p=ganeti.git;a=blob;f=test/hs/Test/Ganeti/JQScheduler.hs;h=273a2503ae17bce5590b23359566e540f1ef2916;hb=273a2503ae17bce5590b23359566e540f1ef2916#l387). For Haskell, we also made excessive use of QuickCheck ([example](http://git.ganeti.org/?p=ganeti.git;a=blob;f=test/hs/Test/Ganeti/JQScheduler.hs;h=273a2503ae17bce5590b23359566e540f1ef2916;hb=273a2503ae17bce5590b23359566e540f1ef2916#l514)). Writing good QuickCheck properties for complex systems like a cluster scheduler is harder than writing by-example test cases, but it gives you extra QA capabilities that for Ganeti were well worth it. With it, we found rare corner-case bugs (e.g. 1 in 100000 samples generated by QuickCheck) that we would never have managed to write a by-example test case for. You can't do QuickCheck style testing in most other languages in a productive way. It's important to say that you don't get these things completely for free. While these kind of success stories are frequent in the Haskell world, you need to put in a bit of effort to make them repeatable. For example, over the time my manager there became very competent at writing QuickCheck properties (you need to be careful to distribute the probabilities of tests generated uniformly over the situations you want to test, e.g. without care it can happen that most lists you generate are incredibly short), and extra effort was taken to do knowledge transfer of things like that through the team. If you work in a team of Haskell experts, this doesn't matter so much, but if you plan on hiring heterogenously (experts, unexperienced prorammers, programmers experienced with other languages), then how well you organise knowledge transfer this can change a lot _how_ much better the Haskell QA story is and _how_ much faster you can onboard people. Another related point I believe is important: While "if it compiles, it runs correctly" is often true, it doesn't mean that "if it compiles and runs correctly, you have simple and clean code". This is especially true when you only have people in the team that have little experience with Haskell. They will experience "compiles - works", but they may come up more complicated code than is necessary. An example is over-functionalisation: when beginners people notice that they can pass around functions easily, they tend to excitedly pass both `f :: a -&gt; b` and `x :: a` down a chain of 5 functions, only to compute `f x` deep inside, when they could have passed down the result of `f x` down, getting much simpler code. It is very helpful when you have some experienced Haskellers in the team that have written real-world code (if you don't, Haskell consultancies can help out here, and also bring expertise on how to build your own Haskell team), and a ubiquitous and low-overhead code-review procedure through which they can pass on how to make things "compiles - correct - simple". &gt; We are evaluating Haskell for what will end-up being a large code-base, worked upon by engineers with varying backgrounds and skill sets. Till now we've been a Rails+AngularJS shop. Haskell is a great language for building large, maintainable code bases. Especially when you're building web app backends, the ecosystem is mature and there is lots of good training material. I hope my experiences help you, and I wish you success!
Haskell is a nice language for safety and productivity (if you know how to use it properly), so being able to use it in any domain, including soft real-time domains, could mean a competitive advantage. 
Once you move on to the next step, that of using these records, you might find it easier to do with lenses. Do read and learn about them.
Hmmm. This nomenclature doesn't seem to be entirely standard (https://wiki.haskell.org/Memory_leak for instance) but Neil Mitchell has an [ACM published article](https://queue.acm.org/detail.cfm?id=2538488) that says "A space leak occurs when a computer program uses more memory than necessary. In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected.". So it must be true :-) Not sure the difference is particularly relevant in Haskell, where I suppose we _never_ have memory leaks unless there is a bug in the RTS or ghc itself. Nonetheless, fair enough. Updated the blog post to use "space leak" consistently everywhere.
Incidentally, I don't necessary have a problem with reducing the scope of full laziness. I just think that doing that, and the NOUPDATE proposal, are orthogonal. (Less so for NOFLOAT, of course.)
Chook and egg problem :'(
If you know that the leak here has nothing to do with full-laziness (which is what I was coming here to say), then why does the post persist in saying that the bug is due to full-laziness?
~~A static CAF will not be garbage collected.~~ The other forms may be, which is part of what is happening on the sharp downslope of the threadscope graphs. EDIT: See replies and links; GHC will reap CAFs bound to top-level names when they are no longer referenced by the program.
I think the standard FFI can be a source of memory leaks. In GHC, I think of ForeignPtr without a finalizer might as well.
I do wish I had the experience as well. There seems to be a dearth of jobs available for junior developers involving Haskell and functional languages.
You would apply flip to the function, like flip is defined now. The function name would still come first. 
Yes, you can use the https://ci.stackage.org CI server, which currently is running the same code. We're working on getting the main cluster back online as quickly as possible.
Unfortunately the `snapshots.json` seems to be coming from the [database](https://github.com/fpco/stackage-server/blob/67baaef082331ded6f77406be1153b93aaa4663e/Stackage/Database.hs#L761)? and I'm not aware of any mirrors for that particular content. Somewhat ironically all of the information about potential mirrors would seem to be on one of the other pages you mentioned, which along with haskell-lang.org are all returning HTTP 503 status codes. All this right after the `hackage-mirror` blog post! :)
If you're feeling adventurous you could play with [labels](https://github.com/chrisdone/labels) and more specifically`labels-json`: https://github.com/chrisdone/labels/tree/master/labels-json
Thank you! This looks really interesting. I'll have to look into it some more, but this looks beautiful: https://porter.io/github.com/sharkdp/purescript-flare
You can use `set`, `view` and `over` if you prefer, take a look at [lens-tutorial](https://hackage.haskell.org/package/lens-tutorial).
Works now! (9:53 CET)
It's my fault. We had planned on rotating this cluster but got busy &amp; forgot.
Just apply. What have you got to lose?
If you can pick it up over the weekend then you should apply. :)
Well, I got the idea from a Norman Ramsey paper (I think it's this one: https://www.cs.tufts.edu/~nr/pubs/team-abstract.html, but don't have a PS interpreter handy.). The fundamental observation is that classic literate programming favours "beauty" and linearity, whilst car manuals are full of cross-references and "here's how to fix X".
I'll keep it in mind for next time. We can't always guarantee that the CI server will be in a usable state, of course :)
And that is a ... problem? 
You can make requests, so as far as I can see the difference is only one of perspective, not behavior. 
No
I am using servant. So, if I understand this correctly: can I serve a custom client UI (with HTML, CSS, JS) using swagger-ui, which makes requests to the same server, according to a servant API spec? That would be pretty cool. 
If you look at how OpenTTD does it, I believe they basically just enumerate the possible shapes of the road (or track) piece on a tile, so you'd have two values for straight roads (horizontal, vertical), 4 values of T-shaped and L-shaped each, and one value for X-shaped. But that doesn't look close enough to what you want, I reckon you're better off with a proper graph model, where you'd model nodes (intersections, sources, and sinks) and edges (connections between nodes) independently. This would probably require maintaining separate data structures, and references between them by ID (there are ways of expressing this with infinitely recursive lazy data structures, but it can get somewhat messy). Something like: data Node = Node { nodePosition :: V2 , nodeOrientation :: Angle , nodeForwardConnections :: [EdgeID] , nodeReverseConnections :: [EdgeID] } data Edge = Edge { edgeStart :: (NodeID, ConnectEnd) , edgeEnd :: (NodeID, ConnectEnd) , edgeShape :: Bezier } data ConnectEnd = ConnectForward | ConnectReverse -- and then something like this to wrap things into one -- data structure: data Grid = Grid { gridNodes :: Map NodeID Node , gridEdges :: Map EdgeID Edge } And then you'd provide "safe" manipulation functions that modify the grid in ways that maintain integrity (e.g., edge shapes aligning with the nodes they connect to, all node and edge IDs resolving properly, etc.) From there, adding in sinks, sources, traffic lights, etc., is mainly a matter of extending or referencing your data structures. 
I'm curious, is it more or less than before? We're currently hiring haskellers and i got ~15 applications for the role right away.
Here is the kind of trajectories that can be expected when two 2x2 lanes cross (sorry for the terrible mouse-drawing). I did only represent half of the "turns" (the red curves) : http://imgur.com/a/jpVvW If I understand correctly what you suggest, I would need a node wherever two curves cross, right ?
Yes -- plenty of computationally-relevant monads aren't strong. For example, in FRP, events can be modelled using one-shot futures. However, while futures are monadic, they aren't a strong monad. The requirement that all monads have a strength ends up preventing a correct and efficient implementation. As another example, in distributed computation, the box and diamond modalities correspond to "anywhere on the network" and "somewhere on the network", and again the diamond modality is a non-strong monad. If you require strength, then you can't implement this period.
Roughly. I think you'd probably actually start from the swagger-ui codebase, and change the templates as needed. Or use a fork of `swagger-ui` like [this one](https://github.com/jensoleg/swagger-ui), which seems much more beautiful and easy to customize (and actually has the JSON editor integration I mentioned wanting in the parent comment!)
It depends; you only really need nodes where bezier curves connect, not necessarily where they intersect, as long as you detect collisions separately. So you'd need nodes on every point where a red line meets a green line, but not where green or red lines cross. Alternatively, you could extend your edge data structure to support multi-lane layouts (including different configurations on either end, to model lanes merging and splitting), and enforce matching lane configurations on connectors. OTOH, if you want to separate out traffic onto lanes based on intended direction, that is probably not a good idea.
That's infinitely better! 
Data.Vector.Generic.Vector / Data.Vector.Generic.Mutable.MVector is generic interface for immutable / mutable vectors. Unbox is simply type that has instance for both. You can look into [source code](https://hackage.haskell.org/package/vector-0.11.0.0/docs/src/Data-Vector-Unboxed-Base.html#Unbox) to see definition of some Unbox instances (Bool and Complex instances are good examples). NFData is type class that allows you to evaluate data to normal form. So if something is trying to force evaluation to normal form and your data type isn't instance of NFData then it might explain lower performance. btw. Is there a reason why you are not using V3 from Linear instead of your `data Vector = Vector !Float !Float !Float` data type?
I'm doing that program without any additional libraries, only GHC modules, thus no `Linear`. Yes I looked a bit into the definitions, although it's quite difficult to actually understand **why** they are defined the way they are. :( Do you have a good source where can I learn a bit more on this topic?
&gt;That's infinitely better! `some n &gt; 1 / 0` checks out _braces for divided by zero well actually_
In a cartesian closed category, strength is equivalent to saying that you have an `fmap` operation (ie, an operation of type `(a -&gt; b) -&gt; (F a -&gt; F b)`. Being a functor (in the category-theoretic sense) is actually a weaker notion: it says that if you have a term of type B with a free variable of type A, you can construct a term of type F B with a free variable of type F A. Having `fmap` is stronger than this, because it says that you can take a function value of type `A -&gt; B` to a function value `F A -&gt; F B` -- ie, that the functorial action is definable within your language. Category theorists tend to phrase strength as having a natural transformation from A * F B to F (A * B) because it lets them avoid baking the notion of function into the definition of strength, but I think "fmap is definable" is a better intuition for programmers. 
Does this mean you'd need to be able to operate in both the inner and a meta language to talk about things like one-shot futures and network modalities? I can't imagine how to talk about non-strong Functors otherwise since you'd need to have access to the turnstile.
`ContT` is an oddity to me. I have no idea what makes it so special, but it comes up as an exception to tons of patterns. For example, `ContT` is (one of?) the monad(s) that can't be replicated with a `Free` monad.
Neat! It looks largely similar, at least in terms of the underlying concepts. I think the big differences are that dib imposes a specific structure on builds (Targets and Stages) whereas the forward stuff in Shake looks like the pieces you could use to build something larger. Shake is a really cool project. I think if it had been around when I was starting dib I probably would have used it instead :).
It took years of emailing and begging the publisher to get them to release a digital version btw. I don't think it's necessarily the cause of this, but thought it worth mentioning.
Here's a previous thread on this same issue: https://www.reddit.com/r/haskell/comments/1qrilm/packages_and_namespaces_naming_convention/ Haskell rather deliberately separates the provenance of the module from what it provides through the current namespacing technique. Whether that is good or bad is up for debate, but it does mean we can do things like migrate modules into `base` and they'll feel like they were "always there", so it isn't all bad. To that point, I've migrated large chunks of code around between 70 or so different packages, and aside from some .cabal level changes, users were largely unaffected. This let me get the "package topology" right in a way I could never do under your proposed scheme. In python any mobility of that sort comes from duck-typing and grafting dictionaries around. As it is modules get grouped by functionality, not package. Though, some 'groupings', like `Data` are absurdly broad, so our current solution isn't perfect. Haskell's view is rather idiosyncratic though. Java for instance takes the opposite view, they embed provenance at the top of the namespace. `com.example.whatever.Foo.Bar.Baz`
At work, we build `stack haddock` into single index with everything: http://docs.futurice.com/haskell-mega-repo/all/, so I actually don't care from which package a module came. But yes I agree. I don't know any good reason why e.g. it's `Data.Aeson` and not just `Aeson`. or `Text.Parsec` and not `Parsec`, so if I make a package with a proper noun as a name, say e.g. `tarzan`, I'll put everything in `Tarzan`. OTOH, some packages like `constraints` and `ad` fall naturally into `Data.Constraint` and `Numeric.AD` respectively.
Everything in Haskell is lazy, unless you explicitly mark it as strict. "Explicit is better than implicit" is satisfied here. 
I think you might be comparing apples to oranges, as nullability and laziness have quite different uses, and as such there isn't necessarily anything to gain by treating them in the same way. Also, making Haskell strict by default to assure data is finite is throwing the baby out with the bathwater. Laziness have many other uses than infinite data structures: https://www.reddit.com/r/haskell/comments/2pnq7b/if_haskell_had_id_marry_the_language/cmyhdpa If you're worried about infinite vs finite data rather than laziness, I suggest reading about codata and corecursion.
I remember [An Approach to Fast Arrays in Haskell -- Manuel M. T. Chakravarty and Gabriele Keller](http://www.cse.unsw.edu.au/~chak/papers/CK03.html) paper (I think it is about Data.Array module, but it should apply to Data.Vector as well). Fundamental problem is immutability so you have to go around it somehow in order to provide efficient implementation with nice api. You can probably find papers about implementation of other array libraries like [repa](https://hackage.haskell.org/package/repa) or [accelerate](https://hackage.haskell.org/package/accelerate). 
Oooh, might this be a good choice for static sites made with Shake? Right now my code looks at my `.md` files, figures out the corresponding `.html` paths to generate, then calls `need` on them. Then separately I write `%&gt;` rules for how to go from `.md` to `.html`. If I understand the idea behind forward building correctly I would just be able to write one thing saying "take all the `.md` files and push them out as `.html` in this way".
I almost agree. But, it is pervasive laziness that (a) was the reason Haskell was invented and (b) allows selection to compose will with unfolding / generation -- the canonical example is `head . sort` running in O(n lg n). So, I prefer totality checking and explicit strictness.
Awesome! This is what we need to get Haskell adopted by industry. Proving it's benefits in workshops at huge tech companies.
I would think that digital would be their primary media and that they would therefore be eager to produce it.
They're not really using Haskell there ... just hosting.
Sounds like you use let's encrypt. I suggest using nginx proxy with the let's encrypt sidecar. It's all standard docker. The sidecar will reissue the cert regularly. There's even a stack template for this setup!
 x = pure 1 :: Lazy _ Now that just causes all kinds of problems. Having the strictness of something depend on whether or not you have an explicit type annotation is not sound. Also you then have to deal with unification and propagation of lazy types. You would have something akin to limited bidirectional subtyping, because you would have implicit up and downcasting from lazy to strict, lazy by putting it in a lazy box, strict by unwrapping it. Now something like `lazy (pure 1)` would work for SOME situations. But not when things are more complicated then simple lazy / not lazy. That's the thing with Haskell's pervasive laziness, there are lots of different kinds of strictness, from spine strictness to WHNF to NF to weird in between stages. Also there are situations where WHNF only requires WHNF but NF requires NF, so if later on you need NF that actually means the first function was essentially NF strict. With your strategy what would the type of a lazy list be? lazy(lazy (1) : lazy (lazy (2) : lazy [])) Would be how you would make a strict list lazy under your proposal. But the above example wouldn't have a sane type seeing as `lazy` would have type `a -&gt; Lazy a` and thus you get different levels of `Lazy` nesting as you traverse the list, which is not well typed. I would say if you are still unconvinced then truly play around with and test the limits of Haskell's laziness. Because your original suggestion would only work if it was massively restricted from what we have now. And changing Haskell to no longer be truly non-strict would make a lot of people (me included) very angry and there is a 0% chance of that ever happening. Explicit STRICTNESS, not laziness, is a viable way to make things more explicit (currently everything is assumed to be as lazy as possible, except when explicitly stated, such as `foldl'`), but explicit laziness would never work in Haskell.
I probably should have annotated to the `-&gt;` instead of the data type itself. That is my bad. Editing that now. And bang patterns still doesn't actually mean strict values, it means strict data dependencies.
&gt;With your strategy what would the type of a lazy list be? For lazy list you need to use separate date structure `data LazyList a = Nil | Cons a (Lazy (LazyList a))` &gt;your original suggestion It isn't a suggestion, just a thought. &gt;And changing Haskell to no longer be truly non-strict But Haskell already has `StrictPragma` 
Well, there is no guarantee added by that change. Your preferred form is just announcing that it will evaluate some arguments upon calling, getting into the infinite loop earlier. (And, as ephiron already said, if you want to convert Haskell declarations into your style, you just add the Lazy modifier in front of every word. Not very informative.) Yes, it would be cool if the GHC could display the evaluation semantics of the code it's creating, making it easier to trace space leaks. But on Haskell this would be something much more complex than what you are specifying here. I don't even imagine what form it would take.
You know, this is a very deep and detailed account of Haskell in production—perhaps you could turn it into a blog post to reach more people than a Reddit comment would?
&gt; Is it just because the overall goal is to formalize the informal mathematical practice of treating equivalent constructs as equal, or is there something in the language which would break if we were defining ∞-categories instead of ∞-groupoids? I think this is an open research question :) &gt; Or maybe the "zero truncate" thing /u/sclv mentioned earlier is used to distinguish between the two, in the sense that the zero-truncated PathedBool has interpretation 1 and the non-truncated PathedBool has interpretation 2? Yes I think that's right. The PathedBool is not a point but a line segment, because the path isn't equal to refl. You can add a higher path saying that your path is equal to refl, but then you have *that* path. The (-1)-truncation adds all those paths infinitely high up, thus really turning Bool into a point. &gt; Clearly, one of PropEq, substId, or zeroIsNotOne must be forbidden. In my previous comment I thought I had found a plausible reason to forbid PropEq, but maybe it's simply substId which doesn't exist? I *think* it's zeroIsNotOne. Pattern matching works differently in HoTT. It does not allow you to do anything that you cannot do with the eliminator. In normal dependent type theory there are additional axioms that let pattern matching do more things, but those axioms are indeed inconsistent in combination with HoTT.
I think you're actually asking for a distinction between data and co-data. A list as data cannot be infinite, while as co-data it can. For better or worse, in a language with general recursion like Haskell these cannot be distinguished.
Well in a sense it is, right? They're both the only things that inhabit all types. Though I suppose you could argue that `null` and `bottom` both exist in Java-like languages, meaning such languages have TWO such values. Regardless, their similarities are important for OP's topic. Though I agree that `null`, being inspectable, is much, much worse.
&gt; if you have a term of type B with a free variable of type A, you can construct a term of type F B with a free variable of type F A. I really can't think of any programming language where this isn't `(a -&gt; b) -&gt; (f a -&gt; f b)` (or equivalent) because both "If you have an X, you can construct a Y" is `X -&gt; Y` and "term of type Y with a free variable of type A" is *also* `X -&gt; Y`. In fact, I would also expect to to apply in any constructive mathematical context as well.
Yes, I think the problem can be simulated e.g. with this clumsy use of `logict` which church-encodes a ListT type: {-#LANGUAGE ScopedTypeVariables #-} import Control.Monad.IO.Class import Control.Exception import Control.Monad.Logic main :: IO () main = retry $ counter_ (getter_ 1000000) retry :: IO a -&gt; IO a retry io = catch io (\(_ :: SomeException) -&gt; retry io) getter_ :: MonadIO m =&gt; Int -&gt; LogicT m Char getter_ m = LogicT (loop m) where loop 0 cons nil = nil loop n cons nil = do c &lt;- liftIO getChar cons c (loop (n-1) cons nil) counter_ :: MonadIO m =&gt; LogicT m a -&gt; m () counter_ (LogicT phi) = do mf &lt;- phi (\_ nf -&gt; fmap (. succ) nf) (return (\n -&gt; liftIO (print n))) mf 0 
&gt; "storage zones" (not sure what it is called in English, the zones with the green vehicles here). I believe what you're indicating in the diagram are known as turning lanes. I can't provide any other advice but I hope that's helpful ;).
Would you consider summer interns?
You will see a substantial improvement if you do the following: * Insert in batches and not one-per-row * If that doesn't help, use a prepared statement * If that doesn't help, disable transactions My guess is, you won't need to do the last two things.
Actually, a term of type Y with a free variable of type X doesn't have a type in most languages, it's a compiler error! If you bind that variable, you get a type. If you bind it with lambda abstraction, then you get a type of `X -&gt; Y`.
NonEmpty a is a semigroup, making Maybe (NonEmpty a) a monoid. So it not like you lose monoid...ism? Also, I don't see why drop and take still wouldn't compose like you mentioned.
Can you elaborate on that point? I don't quite get the point you are making.
It is kind of dumb you cannot distinguish between finite and infinite lists. The two are very different. But the cost of distinguishing them is too great for what Haskell is aiming for. Idris (and other total-leaning languages) stand a much better chance of "doing this right".
Oh, it just occurred to me that another possible improvement could be being able to write x = printf @[Lit "Hello, ", Str, Lit ". You have " , Shown, Lit " new messages." ] sing and get a function of type `(Show a) =&gt; String -&gt; a -&gt; String`. I don't think it'd be possible, currently, to write the `Printf` type family for this case.
`mapM insert` is a horrible implementation. You're out if luck if you're using mySQL. The low level mySQL library has serious concurrency issues otherwise. Even if you find a solution to your mass import problem, you'll be stuck with performance problems in your app due to the mySQL library. is switching to postgres possible?
Well, that is nice to know :)
Not at this precise moment, but I'm trying to arrange that for the future.
I don't need freeform curves, as it is really a puzzle game I want to aim for. Also, it doesn't look that hard to move at constant speed along a Bezier curve. OTOH it's nice to see such an article!
This is technically very cool, impressive, and fun, but given that the non-dependent part of Haskell's type system is able to encode the awesome [formatting](https://hackage.haskell.org/package/formatting) library I think encoding printf with dependent types is a bit sad. It seems to me a bit like saying "Hey dependent types are cool, we can use them to encode exceptions/if statements/lenses/something else Haskell encodes straightforwardly". Haskell's non-dependent type system is *astonishingly* powerful and I'd love to see dependent type examples that can't be done in normal Haskell. (The units library is such a thing, I think.)
The category of linear transformations and the category of relations are very similar. In fact they're both dagger compact categories. Thus if I were implemented a strongly typed interface to tensors I would give it a similar API to Opaleye. 
What isn't actually a problem in practice? Strongly typing the neural network? My experience comes from using torch for machine learning and I find the combination of dynamic types and long running computations (training a neural network) to be horrible. Reliable software using dynamic types relies on testing .. and testing for things which have long running times is a real pain. I think it is perhaps all the code surrounding which can benefit just as much from static typing 
&gt;I find the combination of dynamic types and long running computations (training a neural network) to be horrible. Reliable software using dynamic types relies on testing .. and testing for things which have long running times is a real pain. I think it is perhaps all the code surrounding which can benefit just as much from static typing A-men, my friend. We are allies in this struggle. If you haven't already done so, join the Resistance at https://github.com/DataHaskell
It's absolutely about linear algebra!
Thanks, but .. in your own words?
Thanks, and thanks for the correction. Fixed!
+1 from me if there is voting!
I see, I didn't realize the beziers were part of the logic. I still would go with a simpler base representation than you have in your original example.
Unfortunately, there's also non-technical reasons for why systems may become unreachable, c.f. https://en.wikipedia.org/wiki/Censorship_of_GitHub, and why it's desirable to have multiple autonomous mirrors ideally located on different continents and operated by different entities to mitigate such attacks. 
Yes, they both can inhabit most types (not all: Java has primitives, Haskell has unlifted types). But, programming with them is completely different. `x == null` is `True` or `False`, `x == ⊥` is `⊥`. 
Drop them a line, maybe they will opensource their precioussss compiler
I find that I never actually need this in production software because I care about the formatting enough to build the string myself anyway. For debugging and prototyping, calling print directly on tuples works fine in Haskell.
That's fair. On the other hand Python annoys me when writing big programs.
&gt; format ("Hello " % shown % " " % shown % " " % shown) 10 True [1,2,3] In addition to [`format :: Format Text a -&gt; a`](https://hackage.haskell.org/package/formatting-6.2.2/docs/Formatting.html#v:format) there is also a [`fprint :: Format (IO ()) a -&gt; a`](https://hackage.haskell.org/package/formatting-6.2.2/docs/Formatting.html#v:fprint) formats and prints the string: fprint "..." :: IO () fprint shown :: Show a =&gt; a -&gt; IO () fprint ("I'm " %int% " years old!") :: Integral a =&gt; a -&gt; IO () fprint ("I'm " %int% " years old" %char) :: Integral a =&gt; a -&gt; Char -&gt; IO () You can use it to replace `putStrLn` if you add your own newline, or you can define: fprintLn :: Format (IO ()) a -&gt; a fprintLn fmt = fprint (fmt % "\n") and write &gt;&gt;&gt; fprintLn ("Hello " %d% " " %sh% " " %sh) 10 True [1,2,3] Hello 10 True [1,2,3]
I tried to use [generics-sop](http://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP.html) to avoid having to sprinkle the list with shows. I concocted [this function](https://gist.github.com/danidiaz/d89269cca693786fbc4394063988bc54), that works with tuples/records: str :: (Generic a, Code a ~ '[ xs ], All Show xs) =&gt; a -&gt; String str r = case (from r) of SOP (Z np) -&gt; let (K result) = hsequence (hcliftA (Proxy :: Proxy Show) (\(I i) -&gt; K (show i)) np) in result The problem is that strings passed in the tuple appear quoted, because show is also called for them: ghci&gt; str ("the list is: ",[1,2,3], ".") "\"the list is: \"[1,2,3]\".\"" One would need a different typeclass than Show for this. 
&gt; For the second point: monoids are "things that can be added" and so if you fold a tree of lists you don't need to specify an operation for the fold (it's automatically inherited). You don't have to specify an operation for Maybe (NonEmpty a) either.
An update: such a thing may be possible now, thanks to the new [`ghc-typelits-symbols`](https://github.com/konn/ghc-typelits-symbols) package! I don't know how `IsPythonIdentifier` would be defined, but as a quick exercise I defined a type family that checks if the first character of a `Symbol` is a lowercase A: {-# LANGUAGE AllowAmbiguousTypes #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE TypeApplications #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE UndecidableInstances #-} {-# OPTIONS_GHC -fplugin GHC.TypeLits.Symbols.Solver #-} module StartsWithA where import GHC.Exts (Constraint) import GHC.TypeLits (ErrorMessage(..), Symbol, TypeError) import GHC.TypeLits.Symbols.Internal (SymbolView(..), ViewSymbol) type family StartsWithA (s :: Symbol) :: Constraint where StartsWithA s = StartsWithA' (ViewSymbol s) type family StartsWithA' (sv :: SymbolView) :: Constraint where StartsWithA' 'SymNil = TypeError ('Text "Empty string") StartsWithA' ('SymCons c cs) = c ~ "a" startsWithA :: StartsWithA s =&gt; () startsWithA = () -- This typechecks... albatross :: () albatross = startsWithA @"albatross" -- but these don't! {- banana :: () banana = startsWithA @"banana" Couldn't match type ‘"b"’ with ‘"a"’ arising from a use of ‘startsWithA’ -} {- empty :: () empty = startsWithA @"" • Empty string • In the expression: startsWithA @"" In an equation for ‘empty’: empty = startsWithA @"" -} Would this work?
You might also be interested in https://github.com/mstksg/tensor-ops
`interpolate` and possibly other packages listed above depend on `haskell-src-exts` only transitively via `haskell-src-meta`. For `haskell-src-meta` there's an [open PR](https://github.com/bmillwood/haskell-src-meta/pull/58) to make it compatible with `haskell-src-exts-1.18`. To use the haskell-src-meta version from that PR in a stack project, you can add it to the `packages` section of the `stack.yaml` like this: resolver: lts-7.2 packages: - '.' - location: git: https://github.com/ddssff/haskell-src-meta commit: b7c90892b9aceef874ddf5f3f9bd88b5a9b98f9f extra-dep: true extra-deps: - haskell-src-exts-1.18.0
If we use let's encrypt, it's news to me. But it's entirely possible, I haven't been involved in the Kubernetes configuration, and don't know if it's using it internally somehow.
FYI, this reskin has already been released, and I'm working on an updated tutorial based on the new approach. I think it makes things easier to learn, but I'll definitely appreciate feedback when the tutorial is ready.
I spoke to one of the guys that worked on it during ICFP. They're not working on the compiler anymore. They hope to open source it soon. The problem is it's fixed to an old version of GHC with ext core so it can't be easily updated to a new GHC, so it has limited utility now.
I'm currently trying to understand the code by reimplementing it in Haskell (his code seems to be written in Purescript). However, I can't get the `example` function to compile: example :: forall f. (Inject Banking f) =&gt; Free f Amount example = do as &lt;- accounts b &lt;- balance (head as) return b The error I get says: • Expected a constraint, but ‘Inject Banking f’ has kind ‘*’ • In the type signature: example :: (Inject Banking f) =&gt; Free f Amount I'm not entirely sure I understand the error correctly. For future reference, `Inject` is defined in another post of his: type Inject f g = forall a. Control.Lens.Prism' (f a) (g a) This has kind `(* -&gt; *) -&gt; (* -&gt; *) -&gt; *`. The kind of `Banking` is `(* -&gt; *) -&gt; Constraint`, which means it doesn't match the expected kind of `* -&gt; *` in the first argument of `Inject`. I don't understand how this code should look like in Haskell or if the concept itself cannot be implemented at all.
Oh, and interpolate [doesn't seem to require any changes](https://github.com/sol/interpolate/issues/7) so any packages that depend on it will probably be fine too.
Right now you'd have to manually point it at one of the existing mirrors (one line in the `~/.cabal/config` file, e.g. hackage.fpcomplete.com is easy to remember :-) ). It is planned though to have a built-in list of mirror addresses for this situation, possibly via some symbolic CNAME RRs (mirror-1.hackage.haskell.org, mirror-2.hackage.haskell.org, ...). It's enough to find one single working mirror to bootstrap the TUF meta-data securely. EDIT: It's been implemented already (we used TXT RRs instead of CNAME) and works out of the box with `cabal` 1.25+
Cool, thanks for the explanation.
I generally agree with the stated criticisms of your argument against lazyness by default. On the other hand, regardless of which is the default, I thing making strictness and lazyness explicit in types is rather interesting: In "Types are calling conventions" by Bolingbroke and Simon Peyton Jones https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/tacc-hs09.pdf , they do seriously explore making the Core strict and lazyness explicit in the types, to ease certain compiler optimisations mostly - but they do mention giving the user a way of marking strictnes in the types too. &gt; We would like to expose the ability to use “strict” types to the compiler user, so Haskell programs can, for example, manipulate lists of strict integers ([!Int ]). Although it is easy to express such things in the Strict Core language, it is not obvious how to go about exposing this ability in the source language in a systematic way – work on this is ongoing. but as far as I can see, neither this nor strict core work has progressed further? And papers exploring some other different kinds of Core have continued to appear - like the recent SequentCore thing. I get confused by the subtle differences of these concepts, but isn't the current work on Unlifted types a way to specify strictness explicitly in the type, when you want to https://ghc.haskell.org/trac/ghc/wiki/UnliftedDataTypes ? Certainly looks like that, say : &gt; The evaluation rules for unlifted data types are identical to the existing rules we have for types kinded #: lets are strict, cannot be recursive, and function arguments are evaluated before calls. &gt; ... for every lifted data type a user can define, there is an obvious unlifted type one might be interested in: the one without bottom. Fortunately, we can define a data type to unlift an arbitrary lifted type: data Force :: * -&gt; Unlifted where Force :: !a -&gt; Force a &gt; ... Instead of writing f :: Force Int -&gt; Force Int, we might like to write f :: Int! -&gt; Int!. We define post-fix application of bang to be a wrapping of Force. &gt; ... Why not !Int rather than Int! as the syntax proposal? This syntax conflicts with strict data fields. data S a = S !a has a constructor of type S :: Int -&gt; S, taking a lifted type and evaluating it before placing it in the constructor; data S a = S a! has a constructor of type S :: Force Int -&gt; S, which requires the user to have forced the integer. Representationally, the data types are the same, but the outward behavior for clients differs dramatically.
What is the recommended workflow for fixing this kind of stuff? I'm looking into updating `language-c-quote`, and so far my strategy will be to set `resolver: nightly-2016-10-01` in my `$HOME/.stack/global-project/stack.yaml`, and then try to compile `language-c-quote` from Git. Complicated by the fact that `language-c-quote` does not actually have its own `stack.yaml`!
I fixed hse-cpp and haskell-packages. Working on fixing 'derive' but it's not as easy.
Oh, well, looks like `language-c-quote` works as soon as `haskell-src-meta` does.
Neil, how about C++ programmers willing to learn Haskell (and obviously willing to work on C++ projects as well)? I also wonder why REMOTE is strictly prohibited in these kind of institutions. I mean, why you can't buy services from 3rd party service providers?
To be honest the slew of operators was something that always pushed me away from conduit. When I was starting out with pipes I found I could do most things with (&gt;-&gt;) and when I grew more comfortable with the library I branched out to the other operators. This makes me want to give conduit another try.
I feel precisely the same way.
I think this is a bad idea. Deprecating 2 of the 3 operator synoyms makes sense. This will just force people (thinking of me right now, TBH) to remember a fourth variant.
I vaguely recall from some paper that it made some use of the Intel C compiler, so it might not be as easy as just dumping the source somewhere.
I'm only interested in those that already know Haskell for the moment. That said, there are other jobs in nearby teams where C++ is useful, so if you give me your CV I can pass it on. Certainly remote working can work in some circumstances, but I find there is real value to having everyone in a room, on a semi-regular basis. We're not after a service, we're after a coherently designed system with lots of interlocking components, none of whose design is fixed in stone. That requires a lot of high-bandwidth design. There are certainly non-physically-adjacent ways to get that high-bandwidth, but banks tend to prefer people in the same room.
This is one reason we have default redundancy with AWS
Here is a proposal for an implementation of the type-indexed Typeable mechanism described in [*A Reflection on Types*](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-dynamic/index.htm), presented at Wadlerfest last year. Comments wanted.
Can't wait for this feature to land
Cool. Could you possibly suggest any algorithms, worth to add, keeping in mind solving problems on Project Euler? I tried to remember my own experience with Project Euler, but it was too long ago and I was using PARI/GP that days.
Yeah, `cryptonite` is a great package. I'd be grateful if you could possibly provide a short comparison. Is `cryptonite` interface to these functions better? simpler? 
Thank you for the comment Ben, if only making were as easy as breaking :)
fwiw, we're using mysql-simple in production, and it's not a dealbreaker
Right, currently cabal-install is only pre-configured with the primary server and root keys. But the hackage-security client API lets you specify additional mirrors so there's no problem with shipping a bootstrap mirror list to use for the first time, prior to fetching all the TUF metadata (which includes the current mirror list). The same mechanism in hackage-security can be used to specify private mirrors, additional to the list supplied by the server.
Stephanie Weirich also gave a very nice and clear talk on this at Compose for those who would like a "gentler" introduction: https://www.youtube.com/watch?v=asdABzBUoGM
Yes, thanks!
For the record, it would have to be one of the other operators, as `$$` implicitly calls runConduit.
If you're already using Haskell then https://github.com/schell/steeloverseer might be a good alternative (although I can't comment having never used it). But if you're using Linux/OSX, what about just using the native tools (inotify-tools or fswatch) and some simple shell scripting to get a similar effect? If you're just running a compile command on every file change that might be the simplest way forward.
Neat, thanks! Some questions: * I skimmed the source for the documentation, but are the haddocks available somewhere? * Can someone elaborate on what "multi-pass" means, particularly for parser combinators? Do the other libraries support multi-pass parsing? * I take it that the `Alternative` instance is similarly symmetric to ReadP -- does Phase allow for preferring one branch in case of overlapping parsers? * I assume this should perform better than ReadP, but... benchmarks? (And how about tests?) 
&gt; For simple cases like `source $$ sink`, code is now quite a few keystrokes longer: `runConduit $ source .| sink`. I conjecture that the benefit of having short code for simple cases is the ease of remembering it. If the combined semantics of `runConduit` and `.|` are as easy to remember (or easier) than the existing situation, then I don't think the extra keystrokes will present a barrier.
Hey Tom, I really appreciate the effort that you've put in Opaleye. The idea behind the POC is to quickly identify the missing pieces in the Haskell web-dev ecosystem. From our company's point-of-view, it will give us a very realistic sense of how mature the eco-system is where the gaps in tooling/libraries lie. From the community's point-of-view, we can contribute back in two ways. First, some ideas for future development of various libraries. Second, if our company switches to Haskell, we will definitely set aside a budget for funding some of this development. However, I still don't have a sense of how much budget we should set aside for such feature development. We're a small company (based out of India [1]) and don't have too much money. I'm afraid that if we offer too little it might end-up insulting the library maintainer rather than motivating him/her. I tried looking at https://www.bountysource.com for cues, but could find only one Haskell-related bounty. I also couldn't figure out how many bounties got claimed over the last 3-6 months. To summarize, there are two things that we need to be really clear about: * What features to fund, and in which libraries? * What is the appropriate funding level? [1] Do factor in the purchasing-power-parity and currency exchange as well.
It certainly resonates way better with my intuition for how I want to use conduit, so I'm eager to read this new tutorial! 
I wonder what the rationale for open sourcing *now* is. There was a lot of requests for open sourcing a few years ago, and at that time it wouldn't have been obsolete.
I like the existing a =$= b $$ c =$= d format (except for subjective resistance to the all-pervasive `$`). The new plan of sticking mostly with a `=$=` equivalent and having special 'interpreter' functions seems nice, but I wonder why you aren't going with an operator like `~&gt;` or `|&gt;` that carries an idea of directedness?Also maybe there could be shorter and more pleasant names for the runner functions. So maybe finalize $ CB.sourceFile "source.txt" |&gt; sinkFile "dest.txt" or close $ CB.sourceFile "source.txt" ~&gt; sinkFile "dest.txt" or something like that
This looks like a fantastic opportunity and perfect for where I am right now. How long are you going to keep the application open for? 
Old operators have to be kept for compatibility reasons. Introducing *yet another* operator alias is probably not going to reduce confusion. Your idea would. be great if we were designing the library from. scratch! 
You should really invest some time in a responsive website. Content was entirely unreadable on mobile. 
That sounds like a great idea, thanks for documenting! To be honest I was not aware anyone was still working on yi. That's cool, the core concept sounds interesting. Here are some of the questions that immediately come to mind for me. Some of them might already be answered by the content on the github wiki. - What are some differences between yi and emacs/vim/atom/leksah? ("written is haskell" is certainly a valid one) - In what ways is yi extensible? - Does yi have dynamic loading of haskell code into the running editor? How does compiling/including any extensions I make into yi's code work? - Yi looks to be quite a large project. How is the code organized? How well does that work? 
I'll chime in with a second recommendation. Haskell was a language I'd been interested in for years, but most of my attempts at learning through the existing resources ended poorly. I could usually chip away at my ignorance each time I picked it up, but inevitably I'd hit a wall again and set it aside for a few months. I'm still working through the book, but it's gotten me to the point where I'm somewhat productive in the language (e.g. I needed to set up a dummy API for testing, and I found myself reaching for Haskell+ Servant first).
I can't link to where I read it, but IIRC it was better to let work be done for free than to pay low for it. Otherwise people felt insulted and felt that their time wasn't valued.
A week at most. Need to get started on the server-side components of the POC immediately. Frontend can wait a bit. What did you mean by "where I am right now"?
I'm doing some medium-heavy conduit work at the moment, and I want to say "yea!", and also "yay!"
Of course, late night comment. See you at the Haskell xchange next week?
Thank you for your questions, these are a great start! I'll do a proper blog post on them (and one on how to install yi, now that I think about it), but for now I'll give you the cliff-notes. &gt;What are some differences between yi and emacs/vim/atom/leksah? ("written is haskell" is certainly a valid one) I can mention three so far. * First of all (and this was one of the main reasons for my switch aside from being able to hack in Haskell) Emacs has always felt far too bloated for me. Yi is the "bare bones" of an editor, and in fact one of the devs describes it as "We see that Yi is not so much an editor than a rich library for building editors". * Second is that switching away from elisp gives you all the benefits of Haskell and in addition to Types that also means better code organisation. I've never had problems navigating Haskell source files, and that's not something I can say about huge .emacs config files. * Third is the libraries. Emacs and Vim win in this regard as there are a lot of libraries for them. I intend to do a proper comparison of what libraries are available for Emacs/Vim that are currently missing from Yi to give people an idea what gaps need filling. * In terms of Yi vs Leksah - I've only just discovered the latter! Both use Haskell, which is nice, but I haven't tried hacking on Leksah yet. I will investigate and get back on it. * Also, Yi has customisable keymaps so if you're coming from emacs or vim you can use the key bindings normally found in either of them. &gt;In what ways is yi extensible? Pretty much "the sky is the limit", but to give you a rough idea Yi consists of three DSLs (taken from http://publications.lib.chalmers.se/records/fulltext/local_72549.pdf ) * BufferM A DSL for all buffer-local operations, like insertion and deletion of text, and annotation of buffer contents. It can be understood as a monad that encapsulates the state of one buffer. * EditorM A DSL for editor-level operations, e.g., opening and closing windows and buffers. Operations involving more than one buffer are handled at this level too. * YiM A DSL for IO-level operations. There, one can operate on files, processes, etc. This is the only level where IO can be done. So this gives you an idea of both how the code is organised and what additional functions you could build on top of this. &gt;Does yi have dynamic loading of haskell code into the running editor? How does compiling/including any extensions I make into yi's code work? Yi uses Dyre for configuration (and reading the Dyre docs will give you an idea of how Yi can be configured too: https://hackage.haskell.org/package/dyre-0.8.12/docs/Config-Dyre.html ) So when you edit the config file yi.hs, next time you restart Yi, Dyre will detect that the file has changed and recompile yi (I was worried that this process might be slow, but it's reasonably unobtrusive). I think there's a way to reload the config file while running Yi, too, but I hadn't come across it yet. &gt; Yi looks to be quite a large project. How is the code organized? How well does that work? I will be able to comment on this more as I get more familiar with it myself :) If you have more questions then please post them!
Why don't you apply and let us know more about your plans in the last section of the application form. Let's see if we can work something out.
Any tutorial that you can link to?
Nulls and bottom values are similar because they are both time bombs - they cause an error in one place to blow up in another unrelated place, and good luck figuring it out from the stack trace. For a language that avoids both, see ML or Rust.
A week should be enough for you, right? Will wait for your application. Do refer to this exchange in the last question of the form.
Thank you bought for taking the time to answer my question :-) I will definitely check out that book when I get the time to start learning Haskell. Been working with Scala and "functional programming" in Javascript for a couple of years. Now the plan/dream is to change out JS with Elm and Scala with Haskell.
I have to admit that I haven't really use conduit yet, but I don't find `.|` appealing either. I thought the `=` in the different conduit operator was a visually a pipe, and I quite like it. Why getting rid of it ? Why just no keep `=$=` or, if people really want a sens of direction`=&gt;=` ? 
It is the same with `main = retry (getClockTime &gt;&gt; undefined)` but maybe there's something I'm missing.
For starters, I'd like to see an example of adding a custom keyboard shortcut that applies a function of type `String -&gt; String` to the current buffer.
Excellent suggestion :) That'll be one of the first things I'll cover. 
No, but I can see what he means. 
Point taken! 
&gt; Can someone elaborate on what "multi-pass" means, particularly for parser combinators? Do the other libraries support multi-pass parsing? I don't think Parsec or attoparsec do? Or at least they don't provide such functions out of the box (to my knowledge)
I worked on it in late 2013 as an intern, producing [this](http://www.cs.nott.ac.uk/~psxld/papers/led_gpu_fhpc.pdf) as a result. The consensus amongst the team *then* was that work on the HRC was going to cease in early 2014, and they were fielding requests to open source it back then too. As mentioned, it's tied to old GHC too (7.6.1 was the front-end). All of the resources for HRC dev got allocated elsewhere at the end of my internship. It's probably sitting in an internal repo singing Daisy. :( Edit: Intel C/C++ Compiler version 13.1.3.198 as the CPU backend as well.
ah well if there's a free option then ya, i can't agree more
This example is entirely different; you're executing the action inside the `retry` an unbounded number of times, and it is building up a huge unevaluated thunk inside the `IORef`. The examples in the blog post do not rely on the action inside `retry` to be evaluated more than once. 
Incredible. You guys are amazing for thinking of this.
parsec supports custom input streams/datatypes, so if your initial parser produces, say, a list of "Foo" tokens it's possible (with little effort) to create an instance that allows you to parse that again using parsec. See the Stream typeclass in Text.Parsec.
Up again :)
You were close it appears to be `-hidir`, [more info here](http://downloads.haskell.org/~ghc/7.10.3/docs/html/users_guide/separate-compilation.html#output-files).
This was a very clear and useful presentation. Just thought I'd throw that out there if anyone is on the fence about watching it.
What, then, does "incremental multi-pass" mean? The attoparsec documentation calls it incremental because it has the function `feed` to supply input incrementally. I don't see anything like that in phaser.
So I was trying to write something like this: sortLikeTerms [] = [] sortLikeTerms (x:xs) = [x] ++ (sortLikeTerms i) where i = filter ( == Mult[Const (-1), x]) xs doesn't quite work 
The only thing it recompiles is your config file, which is an executable that links Yi as a precompiled library.
Ah okay, that makes much more sense. Thanks!
I have a somewhat functional prototype which I reference at the end of the proposal. Lots of loose ends need tying up, however. There is a bit more information about the current state of the implementation on the [wiki](https://ghc.haskell.org/trac/ghc/wiki/Typeable/BenGamari).
Since it's pure Haskell, can it use all the typeclass machinery? All the other libraries have constraints on their constructors, which I get around using [this](https://github.com/guaraqe/constraint-classes), which is far from ideal...
2.7 GB. Yeah, that's a good point. I wonder if I can swap...
Aww, that's too bad
Out of curiosity, why do these positions usually require no prior finance knowledge?
I would really love to implement a whole Prelude utilizing that dynamic you have in that constraint-classes package, I really feel like it just gives you so much more of ghc's power. Currently I just ditch the Prelude when I feel unstrapped from it, i.e when the libraries I'm using arent made much more inconvenient by doing so
How can Yi be used for developing Haskell programs? I mean, if it's an editor that is programmed in Haskell, then the expectation could be that it might offer tools for Haskell development that other editors don't have or cannot easily have. I am using vim for development, and emacs + evil + org-mode for note taking and work tracking. I know that there are quite some Haskell tools/extensions (or how is it called) for vim and emacs, but so far I did not succeed to stick to any of them. Maybe in Yi some of this functionality is built-in and more intuitive?
Is it a problem?
Any update?
Usually it is, because it can realistically only be used in open source projects.
On my Haskell newbie/Python expert eyes this seems a great way to write tests. Way safer than the combination of monkey-patching and fake interfaces I currently use in Python. I'm looking forward to hear from the professionals here what do they think about it.
The seams in Haskell are modules and their export lists, right? This is a pretty nice write-up, though I'm not sure I find it so onerous to refactor something like `renderUserProfile` into: withUser :: (User -&gt; Posts -&gt; a) -&gt; Id User -&gt; IO a withUser f i = f &lt;$&gt; fetchUser i &lt;*&gt; fetchRecentPosts i renderUserProfile :: User -&gt; Posts -&gt; HTML renderUserProfile user posts = div [] Beyond making it easier to test `renderUserProfile`, it ensures each function only has one responsibility, makes them easier to compose (as a consequence of SRP), and makes it clear that producing HTML from a user and their posts doesn't require IO. 
Many nice answers so far. I'd add a (hopefully) brief answer: I worked on a 150+ KLOC project a while ago for more than a year. It was my first serious Haskell experience and I fell in love with Haskell. It's not a perfect language, but with a bit adjustments, it can be a really wonderful language. Compared to many other languages, there are fewer pitfalls. I used to program in Java for more than 10 years. The first task I was assigned was to write quite a complicated stream processing library in both Java and Haskell. While it was difficult for me to deal with GHC error messages at first, but eventually I was amazed by the cleanness and compactness of the Haskell code. I spend a lot more time to debug the Java code, yet we found a mysterious error in the Java code a few month later that ended up to be a misunderstanding of a code block rather than an actual bug. Those sort of misinterpretation very rarely happens in Haskell since reasoning about Haskell code is easier. I also recently used Rust for a smallish project since I heard good things about it and it's support for functional style, but it is yet far from Haskell in terms of expressiveness and even libraries. The cool thing about Haskell is that onc can refactor a piece of the source code without understanding it at all! The compiler helps a lot. Haskell has a really helpful community as well. I remember that once I saw a useful guideline about best practices for large Haskell projects, such as using stack rather than cabal, using conduit, using a custom prelude, using total functions, etc ... If someone knows what document I am talking about, please link to it below. 
None of the examples in the blog post _ever run the exception handler_. The memory leaks don't arise because the action gets run lots of times. The memory leaks arise simply because the exception handler maintains a reference to the action _in case it might need to be run_. That's why I said your example was fundamentally different. &gt; And libraries should never export named streams, but only functions that return streams (in the streaming library the exception is empty for the Alternative instance). Right, we want to try and make sure pipes/conduits/streams are never shared, and making sure that they are only ever allocated in functions is a way to do that -- unless of course full laziness is enabled. To quote the blog post, full laziness is taking away our ability to control when allocation happens. However, making sure libraries only exports function isn't sufficient, as that isn't compositional: you will need to make sure that all the call sites of these stream-returning-functions are also functions. And what if some of those arguments are static? For example, you might have a stream-returning-function that reads all characters from a given handle; what if we wanted to define one that reads from stdin? What argument would we give it? We could introduce a dummy unit argument but now full laziness thwarts our efforts to make sure that these streams are not shared. Moreover, the top-level stream/conduit itself is probably argument-less; if we then define retry $ runConduit (someSource someArg =$= someSink someOtherArg) or replicateM_ 2 $ runConduit (someSource someArg =$= someSink someOtherArg) or whatever, we are back to the space leaks described in the blog post; and adding dummy arguments here doesn't help either unless we disable full laziness again. Note by the way that these examples have a space leak even though we would be running in constant space if we didn't have the retry -- whether or not the exception handler ever runs.
You can install [`GNURoot Debian`](https://play.google.com/store/apps/details?id=com.gnuroot.debian&amp;hl=en) and get working `ghc-7.10.3` just from the [Debian repo](https://packages.debian.org/jessie-backports/ghc).
Swap is going to make everything slow unfortunately :(
I'll check this out. Thanks.
That seems like a fine solution for some circumstances. However, I don't really like to leave my computer on when I'm away, and I might as well sit at the comp if I am home...
I run my laptop Linux swapless most of the time, but do a quick `swapon` before compiling something in Haskell. I'd go for something similar on Android.
It's great.
Not only that, but some open source projects refuse to have GPL'd dependencies because then *that* project could only be used in open source projects (since it would also be required to be licensed under the GPL), as sort of a transitive thing.
It should be a considered decision, and I say that a Free Software supporter. It will force code that uses it to be GPL as well. Whether that's a feature or a bug has been a debate for many many years.
Hah, I was mostly joking, but I should probably know better than to assume tone will carry through text on the internet. More seriously, I think `reflection` is far too useful to seriously ignore, but it bothers me a little bit for something’s core functionality to depend so heavily on what I think is really an implementation detail of GHC. It’s certainly nice to have around, though, and some squinting can indeed help to soothe my discomfort. ;p I’m glad you enjoyed the post overall, though, and I appreciate someone calling me out on my less thought-out comments. :)
Yes, I knew about the 'stickiness' of the GPL, and I'm not sure how to behave here. Is a BSD license better? Or the MIT one? 
I use plain IntMap's and the elements are not constrained. Some algorithms do lose polymorphism however because of checks against Double values. Other than that, they are Functor + extra stuff. Should be "quite easy" to parallelize, too. In fact I'm very much looking forward to trying that.
Does it run in the `GetTuDaChoppa` monad?
Isn't such question more suitable to StackOverflow?
Being very strict, the term "commercial" is not adequate, since it's not forbidden to sell free software. But in practice it's more or less that.
No, more like: compareExpr :: Expr -&gt; Expr -&gt; Ord compareExpr (SymT a) (SymT b) = compare a b compareExpr (SymT a) (Mult [_, SymT b]) = compare a b compareExpr (Mult [_, SymT a]) (SymT b) = compare a b compareExpr (Mult [_, SymT a]) (Mult [_, SymT b]) = compare a b There is a shortcut using the [`comparing`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Ord.html#v:comparing) function: getVar :: Expr -&gt; String getVar (SymT a) = a getVar (Mult [_, SymT a]) = a compareExpr :: Expr -&gt; Expr -&gt; Ord compareExpr = comparing getVar Hope this helps.
Haha, maybe it's because he's the Terminator...
Ach, I was trying to keep this as a surprise for Haskell eXchange ;)
The edit operations may not necessarily commute, though. Is that required by the laws?
No, groups need not be commutative. A commutative group is called Abelian.
Would it be hard to replace `.|` with `|&gt;&gt;' It would help with users of the [Flow library](http://taylor.fausak.me/2015/04/09/write-more-understandable-haskell-with-flow/). For reference, Flow brings in the `|&gt;` operator from Elm/Elixir/Ocaml/F#, and also defines a few others. The result would look like import Flow -- etc. main = do -- copy files CB.sourceFile "source.txt" |&gt;&gt; sinkFile "dest.txt" |&gt; runConduitRes -- sum some numbers enumFromToC 1 100 |&gt;&gt; sumC |&gt; runConduitPure |&gt; print -- print a bunch of numbers enumFromToC 1 100 |&gt;&gt; mapC (* 2) |&gt;&gt; takeWhileC (&lt; 100) |&gt;&gt; mapM_C print |&gt; runConduit I feel this is more readable, given that it is consistently left-to-right, top-to-down, with operators which resemble each other. Just to add, `.&gt;` is already taken as `flip (.)`. I'm aware Haskell has a left-to-right analogue of `$` in `&amp;` but as well as not remotely indicating its purpose, the ampersand is also out of sync with the majority of ML languages out there nowadays. 
&gt; One could model the buffer of a text editor as a String (or a more efficient data structure) and let keyboard actions operate on it. That's a monoid in itself, but if you add undo, we get a group! That doesn't sound right. Aren't inverses supposed to be unique?
Could somebody give me a hint how to write the rules in Haskell way?
You're right, that's fishy. Maybe it's doable if we rethink the group structure... Although I don't have much hope there: The set of possible automorphisms is depended on the concrete buffer we're operating on. Edit: After some more thinking: I think if we chose to operate on the *powerset* of buffers and say that an element operates on all buffers in a set simultaneously, we are in the game again. That however isn't what I consider a concrete use anymore :)
The way you did is pretty much how you'd do it until recently. We did't really have proofs as values in Haskell up until 8.0 (not even sure how we would encode them now without refinement types) and AFAIK Haskells type system isn't really sound as a logic.
Sort of. Where's the functions that help you generate the XML data?
Your surprise has been terminated.
I suspect it will not build on hackage, so it would be nice to have manually-uploaded haddocks!
&gt; AFAIK Haskells type system isn't really sound as a logic. Bottom inhabits every type, so every proposition (as types) has a proof (i.e. bottom). But I think there's another reason too. I didn't know we could now pass around proofs though. How would that look like and what about GHC 8 enables it?
Training the cascade classifiers seems to be an involved process. See http://docs.opencv.org/trunk/dc/d88/tutorial_traincascade.html. Pull requests would be greatly appreciated ;-)
Great news! I have been looking for this all these years!
with great difficulty!
They let you reason, or the make your reasoning simpler? 
Structural analysis on the abstract syntax tree, and a set of rewrite rules. My bachelors thesis was making a tool for testing whether that kinda thing was carried out correctly, actually.
I'll remove if necessary?
&gt; To transform or not to transform? needs more allspark
That's what I meant by 'not sure how to encode them'. There is [propositional equality](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Type-Equality.html#v:Refl), but going from there there's not much we can do (yet) to lift entire functions to the type-level.
&gt; algebraic classes They're called "type classes". &gt; rules They're called "laws". 
The compiler never transforms your program according to the laws, because the laws are written in a comment above you class definition, the compiler does not know about them. The compiler does transform your program according to the [rewrite rules](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#rewrite-rules). Rewrite rules are supposed to be written in such a way that the right-hand side is always better than the left-hand side, so the decision is easy: the compiler always performs the rewrite. Of course, rewrite rules are often based on laws.
I agree. What was different in the past was that Gtk2 was great and very stable. So there wasn't much incentive to work on a competing toolkit's bindings. Today, though, with the many regressions in Gtk3 (compared to Gtk2 and inbetween Gtk3 releases), Qt 4 and 5 are much better positioned alternatives, which is why large applications moved from Gtk2 to Qt5 and many new applications prefer Qt. If you also consider the efforts in the Rust ecosystem to get proper Qt bindings, it's evident the pendulum has shifted from Gtk to Qt when it comes to desired native ui toolkit bindings. From a user's perspective, regardless of internals, I much prefer Gtk2 because it works, is fast and does what it's told to. I never think about Gtk2 applications, but Firefox's Gtk3 implementation is very buggy, to name one transitioned app. Gtk3 is only starting to implement a stable release policy and it seems the general position to not support any Gtk3 use outside GNOME, evident in bug reports, if it doesn't change, will be another reason why Gtk3 had a hard time gaining mindshare. It's great that Gtk3 supports Wayland, but too much has regressed, like the file dialog or windows flashing black when being displayed initially. Add the loss of stable theming and removal of the Raleigh theme engine, and you have yourself a platform only very few outside GNOME projects like using or supporting. Despite its flaws, Qt5 is a better choice than Gtk3, and Qtah supports Qt4 as well if you don't need Wayland support. I wish someone with relevant Qt experience would revive the Firefox Qt frontend, which is bitrotting in the tree.
This is great! So happy to see folks pushing native apps for Haskell.
Thanks for fltk-hs and I wish FLTK had Wayland support to be future proof.
The "laws" GHC knows about are called rewrite **rules** though.
I don't think it is fishy. It may seem like it if you think of something like "delete one character" which is an operation which has no inverse. In actuality hitting undo after typing an 'a' would result in the action "delete an 'a' " being added to the chain. Hitting it again would add the inverted action of the one before typing 'a' and so on. Basically the functionality of the undo button is more complicated than it may seem, but it can be descried with the group. It's just that elements of the group don't always correspond with something you can actually do in a text editor, for instance "delete an 'a' and add a 'b' " only makes sense if there is an 'a' at the end of the text. It also allows deleting a word before typing it. Basically, the undo operation of a text editor is more complicated that just throwing an inverse in the mix and the group encompasses more than what a text editor does. But it should be possible to base a text editor around the idea.
I believe this is the approach used for the android version of [git-annex assistant](http://git-annex.branchable.com/assistant/).
to be frank, it sounds like you want a consultant, not an intern
You could formulate them as [rewrite rules](http://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#rewrite-rules) and watch your program break in horrible ways if you don't adhere to them!
&gt; Maybe it's doable if we rethink the group structure I think there's a few mathematicians who might have something to say about that.
You could QuickCheck laws. You could write a ```MonadFS``` that operates in ```State``` and check it against some generic QuickCheck ```writeFile . readFile = id``` law and then do the same in IO against a filesystem. It's tricky with arbitrary ```m``` but I'm sure you can do it. I don't know about Haskell, but Scala libraries like scalaz publish laws for all their typeclasses as ScalaCheck properties that you can test your implementations against. Laws are just property-based tests provided by the library itself. Once again, it's trickier when working with effects but surely not impossible.
Because the intersection of people with both Haskell and finance is rare. You certainly need people with finance experience floating around, but the majority of the required finance can be explained with to Haskellers in terms they understand (it's an associative/commutative monoid), without really needing to grok all the complexities of finance. Fortunately, banks tend to already have people who understand finance, so you just need the Haskell people to have everything sorted.
And worth noting that there's a reasonably common pattern in rewrite rules, based on the sequencing of rules: 1. Rewrite `f x y` to a slower form `g x' y'` whose main advantage is that it's easier to optimize (e.g. because the rewrite means that `g x' == id` more often than `f x == id` 2. Exploit any available optimization opportunities. 3. Rewrite `g x' y'` back to `f x y`. You can see this in action if you look at the rewrite rules around list fusion - for example, the rewrite of `xs ++ ys` into a `foldr` expression and then a later rewrite of certain `foldr`expressions back into `xs ++ ys`.
Any relationship that can be expressed with operations of zero, + and - gets a free ride from our elementary schooling, thus reducing cognitive overload. Group is one awesome API. 
&gt; [Y]ou'll need sooner or later, something which need to access the database and the file system. So your MonadFS and MonadDB ends up being monad transformer (or equivalent)[...] I don't see that. If you've defined MonadFS and MonadDB for IO, you can just use IO - no need for a transformer stack if you don't want one.
It allows you to reason in the abstract, without having to expand a specific function body. This is the only way to talk about "all Functors" or really any open set of things, because even if you have an example it may be possible to produce one of those things that doesn't follow your example.
It's really hard to do in Haskell. `undefined` can slip into the most innocuous places. On top of that, it should probably be optional. I think the right mix is total `main` -- so my users don't have to deal with partiality -- but a not-necessarily-total compile process, as I'm available to compile time to deal with the lack of totality. But, my users may think they want a turing-complete rules system. Or my compiler writers may see me as clueless and unequipped as I see my users. So, totality should definitely be something we can turn off, even if it is the default.
The "all typeclasses should have laws" ideology doesn't make sense when a typeclass is a generally useful abstraction for things such as business rules. It's just a contract. I don't think it's realistic to hold yourself to such a standard.
Reddit thread for that article: https://www.reddit.com/r/haskell/comments/3ajy44/testable_io_in_haskell_at_imvu/ I'd hope that test-fixture satisfies /u/hastor 's concerns. The initial force came from a similar reasoning. Then later and prior to adding TH, there were similar complaints about boilerplate. We're still looking to grow it without venturing into features we think other projects should want.
It's worth pointing out that the second instance is not true! The `recip` function is understood to behave oddly at zero, but you probably don't want to declare a Group instance.
That is a good point. However, since the multiplicative structure is still a `Monoid`, it is still true that you shouldn't choose one over the other for a `Monoid Integer` instance; so you should still only have `Group (Sum a)`.
You mean defining `stageN` locally in `runMyConduit` instead of referring to a definition somewhere else? That depends. With full laziness enabled ghc might float the definition back up to the top and you're stuck with a CAF again. With full laziness disabled, then yes, provided `runMyConduit` has arguments, defining `stageN` locally in a where clause would avoid that particular space leak. This is effectively equivalent to giving `stageN` a dummy argument. 
Yeah, to be clear I very much want the tool available. I am not at all sure as to the best integration with the language.
https://www.youtube.com/watch?v=gXGXZiX0pCA
&gt;[**Lost in Translation Suntory Time scene [3:24]**](http://youtu.be/gXGXZiX0pCA) &gt;&gt;A movie star with a sense of emptiness, and a neglected newlywed meet up as strangers in Tokyo, Japan and form an unlikely bond. &gt; [*^Hand ^Deadman*](https://www.youtube.com/channel/UCHghFDwqa5AU8TF5LFxJX8A) ^in ^Comedy &gt;*^101,318 ^views ^since ^Jul ^2010* [^bot ^info](http://www.reddit.com/r/youtubefactsbot/wiki/index)
&gt; The seams in Haskell are modules and their export lists, right? Not quite. People have a tendency to confuse system boundaries and compilation units, though there is perhaps an argument to be made that languages assign meaning to compilation units in ways that fuel that confusion. Haskell modules can provide strong encapsulation guarantees, so sometimes treating modules as units makes sense. Other times, though, units can span multiple modules, or there can be multiple units in a single module. The parallel in OO languages is that not all classes and interfaces are good choices for seams, but seams are still usually represented as interfaces. In Haskell, not all modules and typeclasses are good choices for seams, but those are two tools the language provides for creating seams. There is a subset relationship there. &gt; This is a pretty nice write-up, though I'm not sure I find it so onerous to refactor something like renderUserProfile into… Yeah, such is the drawback of needing small, self-contained examples in blog posts. This looks pretty decent for that particular contrived function, but it becomes a lot harder when you have deeply nested sets of calls that interact with lots of different sorts of effects. That’s not something that can always be solved with “better design”, since some piece of functionality that *needs* to talk to a database, make an HTTP request, and use the system clock are going to need the results of those effects somehow. Threading them all around manually can be pretty laborious. The solution to avoid manually threading around a bunch of values is to have some mechanism for implicitly passing those parameters, and Haskell happens to have a pretty good mechanism for grouping related sets of implicit parameters together in the form of typeclasses. Plus, the added type safety is really nice.
I think this recent talk by Simon Peyton Jones is exactly what you are looking for: [Into the Core](https://www.youtube.com/watch?v=uR_VzYxvbxg).
Some resources: * [HscMain](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/HscMain) from GHC's trac should provide a high-level overview of GHC's compilation phases. * [Unraveling the mystery of the IO monad](http://blog.ezyang.com/2011/05/unraveling-the-mystery-of-the-io-monad/) by Edward Yang that shows how IO monads are desugared into IO primitives. * The "Into the Core" video linked by other comment where SPJ talks about the design and optimizations of Core.
How about following. It's the markdown code: https://gist.github.com/master-q/733dfa2cf008261a3849564c38a7933b
Ok, I got it translated. Translation : &gt; Eagle in Metasepi ☆ tentacles! &gt; Onushi not seem to read much this tentacles. &gt; Naturally tentacles in should quality be something poor. &gt; Easy'm a squid. I'm pretty sure google translate replaces any unknown Japanese word with "tentacle". That, or the original paper is actually the Necronomicon.
I wonder how it builds on Windows
Yeah... I opened an [issue](https://github.com/snoyberg/conduit/issues/259) about this on conduit a few months ago, resolved it by makin no-full-laziness about the program.... My conduit is running potentially indefinitely and this is causing quite significant space-leaks.... Going for a cup of tee to read it all.
&gt; So English is asked to cooperate in the tentacles! :)
You are right, you'd have to make sure it is never transitively used more than once. I've applied `ghc-dup` to your example. The following program does not leak with GHC 7.4.1 (while the original does): module Main (main) where import System.IO.Error import GHC.Dup data Sink = Await (Maybe Char -&gt; Box Sink) | Done Int countFrom :: Int -&gt; Sink countFrom n = let k = countFrom $! n + 1 in Await $ \mi -&gt; case mi of Nothing -&gt; dup $ Done n Just _ -&gt; dup $ k feedFrom :: Int -&gt; Sink -&gt; IO () feedFrom _ (Done n) = print n feedFrom 0 (Await f) = feedFrom 0 (case f $ Nothing of Box a -&gt; a) feedFrom n (Await f) = feedFrom (n - 1) (case f $ Just 'A' of Box a -&gt; a) retry :: IO a -&gt; IO a retry io = catchIOError io (\_ -&gt; retry io) main :: IO () main = retry $ feedFrom 5000000 (countFrom 0) (I've written [a blog post about the subject](http://haskellexists.blogspot.de/2016/01/fixing-space-leak-by-copying-thunks.html) a while ago). 
Fair enough, I can see how `reply` might confuse. Sorry about that. I will add a comment to the blog post so emphasize that these memory leaks happen even though the exception handler never actually runs. Indeed, "any condition that _might_ cause the action to be repeat" is precisely what this is intended to model. As for lists versus conduits -- you're right that this is not a conduit specific problem, and indeed "lists as streams" suffer from the same problems. As indeed do many other data structures; /u/phischu for example [pointed me](https://www.reddit.com/r/haskell/comments/55xk4z/erratum_to_sharing_memory_leaks_and_conduit_and/d8et2wq) to http://haskellexists.blogspot.de/2016/01/fixing-space-leak-by-copying-thunks.html . Moreover, in the case of conduit, sources can indeed be understood to be just a more complicated version of lists. However, the memory leaks arising from sinks and effects are significantly more subtle, because the memory leaks consist of chains of a constructor pointing to a function which, in its environments, points to the next constructor. When I started looking at this it wasn't at all clear to me how we could have a chain at all past the first `Await` or `Effect`. But perhaps I should indeed make it clearer that these problems are not conduit specific. Any lazy data structure intended as "control" will suffer from these problems, and we're studying conduit just as one example instead of lists, because of the embedded lambdas.
To be a group, you also need: -- m `mappend` inv m = mempty
This looks interesting! I'd be curious how the accuracy &amp; latency for response times in the single ms range compares to https://hackage.haskell.org/package/uhttpc where I had to keep interactive output to a minimum to avoid skewing results, taking into account GC, as well as avoid using off-the-shelf http libraries which unfortunately have quite some overhead which is unsuitable for serious benchmarking. 
My understanding is that any project on github is automatically participating. The tags are just hints for easy issues to tackle but tbh I prefer tags like `newcomer` which have a longer lifetime than october. You can find these on quite a few projects, e.g. in [cabal](https://github.com/haskell/cabal/issues?q=is%3Aopen+is%3Aissue+label%3A%22meta%3A+easy%22), [stack](https://github.com/commercialhaskell/stack/issues?q=is%3Aopen+is%3Aissue+label%3Anewcomer) or [hledger](https://github.com/simonmichael/hledger/issues?q=is%3Aissue+is%3Aopen+label%3Aeasy%3F).
I completely disagree with this comment so I think there must be some misunderstanding here. Edsko is pointing out that the full laziness "optimization" actually increases memory usage from O(1) to O(n) in fairly innocuous situations!
I don't understand what [`oneShot`](https://hackage.haskell.org/package/ghc-prim-0.4.0.0/candidate/docs/GHC-Magic.html#v:oneShot) does. The documentation says it gives a hint to the compiler that the given function will only be called once to enable certain optimizations. Which optimizations?
Sure, I mostly posted this to make project maintainers aware of Hacktoberfest, so they could add the necessary tags to their issues.
I've watched it all and I only have one question, what? I honestly envy whoever is able to understand anything about that video. I've tried reading Hott once and stopped after a few pages when it assumed about 7 lifetimes worth of knowledge that I didn't have...
`return $ sum [1..y5]` has type `IO Integer`, but `main`, and therefore the then clause of that `if`, expect `IO ()`.
If someday you have time, try to take a look at this paper [here]( https://arxiv.org/abs/1601.05035), it discusses the problem from a global and not-so-technical point of view. I found it really enlightening and simpler to understand.
Haven't heard Prof Wadler in that sort of mood before. "Read [Ross Tate's effectoids] paper, its reasonably well written, the third or fourth iteration he did on it
&gt; not-so-technical ... &gt; Suppose M and N are manifolds with spacetime metrics g and h respectively, and φ is an isometry between them. Then any point x ∈ M corresponds to a unique point φ(x) ∈ N, both of which represent the same “event” in spacetime. Since φ is an isometry, the gravitational field around x in M is identical to that around φ(x) in N. However, if ψ is a different isomorphism from M to N which does not respect the metrics, then the gravitational field around x in M may be quite different from that around ψ(x) in N. **So far, this should seem fairly obvious** I'll go watch some cartoon now. ~ Edit: &gt; (...) even in its short existence the hott/uf community has already observed that graduate students who are “brought up” thinking in hott/uf form a direct understanding and intuition for it that sometimes outstrips that of those who “came to it late”. At least this sentence gives me some sort of hope that my complete lack of mathematical knowledge can be useful in some sense. ~ Edit #2: I have a question. &gt; The point being made, therefore, is that hott/uf, the synthetic theory of ∞-groupoids, can be a foundation for mathematics in this sense. The author spends a lot of time hinting that HoTT might be a superior framework for the formalization of math. How can we be sure, though, that it is the **ultimate** framework? That an even superior system won't soon be discovered?
You gotta understand that the folks at that conference are luminaries. I went there and was probably the dumbest person in the audience. In person, this lecture wasn't so hard to follow, but if you had never heard some of the type theory jargon before, you would be lost almost immediately. 
You don't have to. You don't even have to use a computer to do HoTT. It is also not really accurate to characterize Coq and Agda as the "old". HoTT is an extension of type theory; in order to use it, you must know type theory because you would be doing type theory (plus some extra stuff). In these terms, Coq and Agda are the base type theories being extended, and the HoTT library is the extension. If you know Haskell, Agda is not that tough to learn. Norell and Chapman's ["Dependently Typed Programming in Adga"](http://www.cse.chalmers.se/~ulfn/papers/afp08/tutorial.pdf) is pretty good.
Thanks for that, by the way! I tagged my issues, let's see what will come out of it.
Putting it that way, you're leading me to think that HoTT, under the hoods, is the exactly same thing as, for example, the calculus of constructions *(~~which, if I understand correctly, is the same thing as MLTT, except for not having an infinite universe of types~~ which I'm still not sure how is different from type theory)*, as explained on [Simpler Easier](http://augustss.blogspot.com.br/2007/10/simpler-easier-in-recent-paper-simply.html). Is HoTT just a lib on top of that system?
The server-client duality you described is implemented in [Pipes.Core](http://hackage.haskell.org/package/pipes-4.2.0/docs/Pipes-Core.html).
Ahm... When mathematicians try to be concrete, they tend to give advanced physics examples, to be "friendly"... When I said less-technical it was because it has more text than equations. If I remember well, I just jumped the ununderstandable parts, it still has comments and metaphors that make it easier to get what HoTT is about. About being the ultimate, it is impossible to know, but this one seems nice as it seems more prone to mechancal verification with relation to set theory. I think later in the text it presents how a self sufficient formulation of it would look like. Mas a parada é treta.
These are great questions. * If your measuring single ms range responses, for a small number of connections, say ten, 'wrecker' is within a millisecond to something like wrk. For a large number of connections it is pretty inaccurate. * If your using the --interactive mode it adds around 300 microseconds. YMWV. * Per GC, it helps that the statistics are calculated online, so the memory doesn't grow as you run the benchmarks. * Finally per it's usability, since there is more overhead ... right now, I would say it depends what you are benchmarking. Many DB backed applications are not in single ms range under load. I've seen applications enter a livelock like situation, even under modest load, like 20 concurrent requests. If you need to script a large sequence of requests, because you need previously created resources for the API to function, it changes the landscape. There are fewer usable options. My initially hope when I was turning a one page script to the library, was to make something that would work with uhttpc, but I didn't have time. With that said I'd love to talk to you about use uhttpc more. Right now there are performances issues that are in my code, as opposed to http-client. I want to fix those first. 
I'm not sure I know enough to pick out the precise distinctions; however, homotopytypetheory.org has the following definition: &gt; Homotopy Type Theory refers to a new interpretation of Martin-Löf’s system of intensional, constructive type theory into abstract homotopy theory. Propositional equality is interpreted as homotopy and type isomorphism as homotopy equivalence. Logical constructions in type theory then correspond to homotopy-invariant constructions on spaces, while theorems and even proofs in the logical system inherit a homotopical meaning. As the natural logic of homotopy, constructive type theory is also related to higher category theory as it is used e.g. in the notion of a higher topos. (There is also a post indicating this is incorrect...) So my understanding of that is that HoTT is MLTT understood/interpreted in terms of various homotopy-related categories. Particularly, the focus is on properties of the equality or path type. But it is not a specific type theory, just a way of looking at (a certain class of) type theories.
I'm not sure where the reference to Prof Wadler comes from; the presenter is Jan Bracker: http://www.cs.nott.ac.uk/~psxjb5/
A higher inductive type from HoTT is like a burrito that you can bend around and end up with a toroidal burrito.
Wow, what a great idea! I am also self-teaching myself HoTT right now, and I've run into the same kind of problems - a lot of the time I have to take a break from the HoTT itself and go read up on some other branch of maths, be it category theory or proof theory or a simpler introduction to dependent types. It would be great to see a self contained resource with exercises. Maybe something at the level of Conceptual Mathematics by Lawvere. I would also be happy to volunteer to help out with this project. Perhaps someone could forward this to the attention of the nice folks at the n-cafe? https://golem.ph.utexas.edu/category/ Also, OP, you might be interested in the resources at the bottom of this page, as they are aimimg to explain HoTT to non-mathematicians: http://www.bristol.ac.uk/arts/research/projects/homotopy-type-theory/
The general strategy of de-sugaring is definitely followed in GHC when the surface Haskell is translated to Core, although the simplification is carried even further and I don't know that GHC would do those particular simplification steps, especially with respect to the IO Monad. One significant difference, in which GHC is maybe a bit unusual, is that GHC's intermediate languages remain typed; in fact, all of Core's terms are *explicitly* typed, as it is essentially just a higher-order polymorphic lambda calculus with a couple of extensions. Most of the high-level optimizations are done (from the diagrams I've looked at, anyway) at the Core level as type-preserving transformations. This is somewhat unusual, as many functional language compilers have discarded types by this point, but SPJ has claimed that having types here has been very helpful in developing new language features and optimizations. Right before code generation, the Core is translated to another intermediate representation called STG, which is basically low-level code for the abstract machine model that GHC implements--the Spineless Tagless G-Machine. Simon Peyton Jones has published (along with many others) a number of papers that describe how things work at this level, and they are an extension of even earlier work that he'd previously published (again with a number of other co-authors) a book on, which as far as I remember, he also makes available from his web site. It's worth reading if you're interested in getting a broader background on functional language implementation before moving on to the specifics of modern GHC.
The [newcomer](https://github.com/search?l=&amp;q=state%3Aopen+label%3Anewcomer&amp;ref=advsearch&amp;type=Issues&amp;utf8=%E2%9C%93&amp;l=Haskell) and [easy](https://github.com/search?l=&amp;q=state%3Aopen+label%3Aeasy&amp;ref=advsearch&amp;type=Issues&amp;utf8=%E2%9C%93&amp;l=Haskell) tags also have a lot of items. EDIT: although you do find things like [this](https://github.com/agda/agda/issues/1709) so YMMV on the definitions of 'newcomer' and 'easy'.
Thanks for the clarification; I listened to the first bit (I haven't got the time to concentrate on it at the moment) and I was confused as to how Prof. Wadler was involved. Makes sense now.
See Section 6.3 in [here](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cardinality-extended.pdf). It for example allows the compiler to float in let-bindings (the motivation here is to get rid of/postpone allocation of thunks) which could otherwise not be proven to only be evaluated once. Also, as hinted at in section 6.4, I *think* it will not emit instructions to update (and create) the thunk, thus disabling sharing, which is what you want here.
How can someone participate if they aren't hosting on Github? Gitlab, Gitorius, Bitbucket etc?
That is right.
I've been following along with his CT Youtube video series. I'm finding it very understandable although I feel like it would be improved by a few exercises to practice on.
Univalent type theory (Homotopy type theory is rather problematic as a description), is all about equality. Typically equality types in type theory are rather boring. Mostly only one canonical inhabitant (refl) can be constructed, given by judgmental equality (that is syntactic identity). That this need not be the case was noted a long time ago. In fact it is quite restrictive. In mathematical practice equality is a pretty malleable concept and constructions that are standard in mathematics like refining a set by an equivalence relation are sometimes painful or generally not supported in type-theory implementations like Coq or Agda. You might have heard the magic words "up to isomorphism" invoked if something is not literally equal by definition but equivalent for all considered purposes. Giving a formal counterpart to those words is what univalence and similar developments are about. You can get started by assuming univalence as an axiom which is what is currently done in Coq/Agda/Lean, afaict. But like assuming LEM or AC it is tedious to drag the assumption along and use it explicitly, all the time. But without a computational interpretation/elimination rule that's what you have to do. Finding such interpretations and evaluating their qualities is where the focus currently lays. Cubical type theory is probably the most advanced development, with a working implementation. Liberal notions of equality (and their implementations) are interesting for more applied typed programming languages because they promise generic ways to transport functions (and someday propositions) across different implementations, as far as I'm concerned. Currently you have to carefully construct and respect abstractions from the beginning in order to have modular programs. Some abstractions are more explicit (type classes), some more implicit (containers and unordered-containers using the same function names for maps and sets). Where this is not done (string types, number hierarchy or container types) it is a huge pain to fix after the fact and clever regexes and sed can only get you so far, not to mention the safety of it all. Composing slightly different interfaces for basically the same thing is no fun either. If you could leverage a proof that two types are isomorphic (giving two functions back and forth, composing to id) you would get everything you did for one for the other for free. There are a lot of other (and probably more) interesting use cases for Univalence, but this is perhaps the most practical application that I hope will come to be. Of course this is very speculative and a long way off. How this would work is by no means clear. The hype around UTT is actually quite annoying. As a programmer, I wouldn't hold my breath, if you're looking to formalize math, the story is different.
I think skipping fundeps was relatively straightforward. One of their examples was trying to cover: bind :: m a b x -&gt; (x -&gt; m b c y) -&gt; m a c y with the proposed `p,q,r` of: bind :: p x -&gt; (x -&gt; q y) -&gt; r y Notice that this identifies `p = m a b`, `q = m b c`, and `r = m a c`, for some concrete `m` but arbitrary `a,b,c`. Now ask yourself if there's any functional dependencies in there: maybe `p, q -&gt; r` ? But not enough to get where you want to be, which is `p -&gt; q, r`, where the `p` is obviously the decider. Type families, I'm on much less sure footing with. My gut instinct is "what you'd like to do to fix the above (hide middling parameters) is very similar to the [type-aligned sequences](http://www.cse.chalmers.se/~atze/papers/zseq.pdf) paper and that uses GADTs to achieve its goal; yes, there's probably a way to make type families function as GADTs at the type level"... but I'm on fairly shaky ground here. I guess my impression of all of this comes back to trying to read "Greenspun's tenth rule" backwards: what language are we implementing an ad-hoc, informally-specified, bug-ridden, slow implementation of half of? It's certainly not Common Lisp. My gut-feel is that this Xanadu language that we're approximating treats types and other compiler-oriented metadata as just any other sort of data, so actually "typechecker plugin" is right on (our hypothetical Xanadu's type checker is actually a metadata-analysis plugin for the compiler in the first place!), and probably there is some firm distinction between finite data, knot-tied cyclic data, and legitimately infinite data. (I've heard the first called "data" and the last called "codata," which seems like an abuse of terms -- because really "data" is codata too via pattern matching. Anyway I've never really heard a term for the middle; maybe "cydata" or "redata"?). With some appropriate restrictions this gives you the approach you'd ideally take: Xanadu says, "do anything you want with types as long as it halts, because I'm trying to statically analyze them." And if I could daydream a little more, if it's trying to type metadata, then it seems like it would be based on some sort of subtyping relation, since metadata would seem to be intrinsically subtyped (I-know-this-list-has-length-five is a bit of useful metadata about a list; the type of lists which I-know-have-length-five is a subtype of the type of lists in general). I wonder if you just need to bite the bullet and go all-out dependently-typed or whether there's something with simpler metatheory. Certainly a 'dependently subtyped language' sounds like it's chasing down a rabbit hole of immense meta-theoretical complexity.
But are the contents of the burrito on the inside of the toroid, or does it fill the entire space?!?!
Even if you don't know what a manifold, an isometry, a metric and a gravitational field you can take an educated guess. You should think something like this: There are two space-like sets which have each a thing called metric defined. There is a function phi between those sets, that probably somehow relates the respective metrics in a sensible way (judging from the word isometry). Another function psi does not have the isometry property but is still an isomorphism therefore preserving probably what it means to be a manifold (also isometry implies isomorphism). Whatever a gravitational field is an isometry maps it unaltered and an isomorphism doesn't have to (it most likely has to do with those metrics). You could get away with just that information, the details do not matter in what follows. This is a standard mathematical setup, you have to play along a little. Also those things are very basic and looking them up can only be beneficial. Now I understand that this might not be so easy if you have little mathematical education beyond high-school, but if you do not get a basic level of mathematical maturity (this can be done on your own, although that is harder) you have to be comfortable with the fact that much literature that is considered basic will remain inaccessible to you. That by the way is also fine, I don't mean to be harsh, I just hope you know that. Regarding the ultimate framework. That is an argument that could be made against basically anything. Personally I don't think holding out (preserving resources) in hope for a better opportunity is a winning strategy unless the payoff seems particular bleak or you know for a fact that a better opportunity will arise. In research specifically, things go slow but steady and almost always incremental. Decades of research are unlikely to leave promising routes completely unexplored. That begs the question where such huge improvements to theory, not building on the current work, should hide.
"you cannot step into the same stream twice" -- I like that :) Just a a shame that ghc's philosophical stance doesn't align with Heraclitus' when full laziness is enabled :) 
&gt; I'll go watch some cartoon now. To be fair, that section is trying motivate by giving several examples from different domains. You don't really need to understand the physics part if you're not a physicist.
Just read it, it's pretty good! It's also quite easy to read, to boot.
This is one of my favorite ways to learn - video, voice-over, follow along in a repl. Thanks! Great stuff - I'm really looking forward to the rest of the series. edit: ~~Is it~~ It is ok to post the [Patreon](https://www.patreon.com/haskell_and_nix) link here~~?~~ ! I pitched in, hope others will too!
I have a basic mathematical background - some topology, some real analysis, some linear algebra, some abstract algebra, some category theory, and a bunch of logic and type theory. I still for the life of me *cannot* figure out what HoTT is about. A non-technical program would be very cool. On the other hand, keep in mind that category theory is another alternative foundation for math that is *also* almost impossible to explain to beginners. I blame the terminology, but really I have no idea if the same concepts could be presented in a way palatable to the layman.
&gt;"if we had done all that was suggested we'd be publishing in *Nature*" Can you elaborate?
[removed]
&gt; MLTT seems much more intuitive Than ZF or than HoTT? In either case, I agree but that doesn't make it a good foundation of mathematics, necessarily. I fear without HIT you lose some expressivity, and may or may not be as capable as ZF. I'm still don't have a good example of when I'd use a HIT for a programming task, but there's several (abstract but) approachable examples of when want HIT when doing certain kinds of math. Now, if HIT = industrial strength quotient types (which may be the case), maybe I can come up with some uses.
My ultimate problem with HoTT is I need to have constantly do research into a mathematical area I'm not completely familiar with like many other people. If anything...trying to understand HoTT has made me pretty decent at autodidacticism in regards to maths since it provides a sorts frame work of what prior knowledge I need. I understand the sentiment and think it's a great idea but it would be perhaps best to start with mltt like many other peple suggested. Pinging /u/modulus this should be of interest for you. 
Licata's talk is much more technically astute than the following, and richer. But if you want to see a talk directed at a much more lay audience, and even lighter on the mechanics, you may want to check out my 2014 presentation at lambdajam: https://www.youtube.com/watch?v=OupcXmLER7I
[Add support for "cubical type theory" under flag --cubical](https://github.com/agda/agda/issues/1703)
Ye olde [data-reify](http://hackage.haskell.org/package/data-reify) offers this functionality.
The YouTube Series is based on the blog/book. It has some exercises at the end. I have to look into the HoTT youtube videos!
Thanks for the link. Is there any published code that came out of this work?
The contents are everywhere. Burritos don't bend that well.
I still don't understand how calling a sink twice can lead to memory catastrophe. Can you explain? I didn't see anywhere obvious where memory would be retained undesirably.
&gt; The following program does not leak with GHC 7.4.1 (while the original does) Where is the original? I couldn't work out where this code comes from. Did you know that the space leak goes away if you move the definition of `k` within the `Just` branch? (modulo full laziness)
Yes, that is correct. To be precise, `countFrom n` is an `Await` constructor with a function in its payload, and the environment of that function contains a reference to `countFrom (n + 1)`, the next `Await` constructor. Indeed, if you have the `countFrom $! n + 1` inside the `Await`, this does not happen, as the next `Await` constructor is not allocated until you call the function -- unless of course you enable full laziness, in which case you end up with the recursive call being floated out again. (I realize you know that :) Just making sure it's clear to others as well.)
Oh right. I missed that. So everything I said had already been discussed ...
Yeah, like I say on trac, I'm starting to be more and more sympathetic to this point of view. I think there _is_ some value in trying to make sure the user does not _need_ to think quite so operationally (it's after all not obvious why moving that `let` out, manually, would cause a space leak) -- but it should certainly be _possible_ to reason operationally. And full laziness is making that impossible. 
Not to discourage OP's ideas but my experience has been that not enough people have had a good course on how to prove things at the university. The course I attended was good but I did not do very well. It might have been me, or it might have been too much to handle at first go. It takes a while to appreciate differences between concepts (implication vs entailment, Boolean algebra vs Boolean logic). I reckon an interactive program would have done wonders.
There's also http://hackage.haskell.org/package/observable-sharing and there's the [accompanying paper](https://pdfs.semanticscholar.org/0d2c/c9ee73d9398fd273845a26b3ed79af5ebf29.pdf) which in the abstract says &gt; In this paper we propose an extension to call-by-need languages which makes graph sharing observable
I didn't write Qtah, just posted the link here, but I can say that Qt 4 and 5 have never been slow for me on the desktop, and with Qt5 you're future-proof. In fact, Qt supports more backends than Gtk3 does due to its use in embedded applications. On the desktop gtk2 is great, but it's deprecated and gtk3 is - so far - a big regression unless you need it for the GNOME desktop. So, if you want Wayland support or can imagine using ParagonTT on anything but the desktop, like embedded around the rocket launch pad, then I'd suggest Qt. Regarding features, the Qt devs have been incorporating features that used to live in KDE UI libs into Qt proper, making it available outside the KDE desktop to all users.
&gt; It's not just that GHC is using more memory. It's that GHC's standard optimizations keep the entire data structure corresponding to the stream history in memory. This means that, given the right optimizations, the streaming doesn't really even happen. If you let the program run long enough, it will eat up all of your memory. No. We - and many others - have numerous large commercial applications that depend heavily on streaming. These applications run for months in constant memory with no issues under significant load and occasional heavy load. In our case at least, the streaming is implemented with straightforward, idiomatic usage of conduits and of third-party libraries based on conduits. In short - this just isn't usually a problem in practice. So recommending use of `-fno-full-laziness` by default is way, way overkill, a huge premature optimization, and a poor recommendation. Don't do it.
May I ask what part of ESA this is and what kind of candidates, when they are, the team working on these apps would be looking for? Is it scientists only?
MLTT has a hierarchy of predicative universes. HoTT is an extension of MLTT. If you don't care about computation, you can simply postulate the univalence. I guess higher inductive types can be emulated as well.
&gt; That is, what you said isn't HoTT, just one of its features, correct? There are several HoTT variants, the most canonical or "classic" version is the one that's used in the HoTT book (people sometimes call it "book HoTT"). We get book HoTT by taking MLTT and adding: - Univalence as axiom: this is just the `isoToEq` function from my example along with a proof that it is the inverse of the `eqToIso` function which goes in the other direction. `eqToIso` is trivially definable in MLTT. - Higher inductive type definitions. These are a generalization of the usual inductive type definitions in Coq/Agda. Usual inductive `data` definitions can only have constructors that return in `A` if we're defining `data A`. Higher inductive definitions can have constructors that have return type `a = a'` for some `a : A` and `a' : A`, i. e. we can specify constructors that construct equality proofs for the type's elements. So, HoTT has two important features, one is univalence, the other is higher inductive types. &gt; Could you clarify my earlier question, if HoTT is just software built on top of the Calculus of Constructions (and one could just use your nosubst implementation) or if it is a different implementation with different grammar? The calculus of constructions is a bare-bones dependent type theory that doesn't have Agda/Coq style inductive definitions. If we augment it with HITs, univalence and universes, then we get essentially book HoTT. "Homotopy type theory" as a term is like "Martin-Löf type theory", in that it's not tied to any particular implementation and denotes a class of related type theories. We have different implementations of MLTT and likewise we can have different implementations of HoTT. There are two main groups of currently existing implementations: - Implementations based on MLTT + UA + HIT, i. e. book HoTT. There is a problem with this, namely that univalence doesn't compute. For example the `foo'` in my example does not reduce to an actual function, instead it becomes a "stuck" neutral term, and we have to manually prove that it computes to some particular expression (which is tedious). These are available as Agda and Coq libraries, which are "hacks" to some extent because they use unsafe stuff to emulate features that Agda and Coq don't have. - Implementations based on cubical type theory (there is just a [single experimental one](https://github.com/mortberg/cubicaltt) currently). The advantage is that it computes with univalence properly. The disadvantage is that it's new, relatively poorly understood and differs from good ol' MLTT significantly (more complicated too). The "MLTT + UA + HIT" implementations of HoTT (including the book) have syntax and feel very close to Agda and usual dependent type theory. Cubical type theories also look very similar; the only place where they differ is the treatment of equality proofs. So, if you don't mention equality proofs in your code then cubical type theory could have the same look as Agda as well. 
It seems I can modify the little library we are building so that the user doesn't have to worry. `oneShot` just needs to be helped along a little. The standard usage starts with `await = Await Done` and a monad instance. It seems it's generally safe for full-laziness, if I add a special RULE for `await &gt;&gt;=` that puts `oneShot` inside the constructor {-#LANGUAGE BangPatterns #-} module Main (main) where import GHC.Magic import Control.Monad data Sink' i r = Await (Maybe i -&gt; Sink' i r) | Done r type Sink = Sink' Char Int instance Functor (Sink' i) where fmap f (Done r) = Done (f r) fmap f (Await k) = Await (fmap f . k) instance Applicative (Sink' i) where pure = return; (&lt;*&gt;) = ap instance Monad (Sink' i) where return = Done Done r &gt;&gt;= f = f r Await k &gt;&gt;= f = Await (\a -&gt; k a &gt;&gt;= f) {-# RULES "await hack" forall f . await &gt;&gt;= f = Await (oneShot f) #-} await = Await Done {-#INLINE [1] await #-} cf !n = do mi &lt;- await case mi of Nothing -&gt; Done n Just _ -&gt; cf (n+1) feedFrom :: Int -&gt; Sink -&gt; IO () feedFrom _ (Done n) = print n feedFrom 0 (Await f) = feedFrom 0 (f Nothing) feedFrom n (Await f) = feedFrom (n - 1) (f (Just 'A')) main :: IO () main = feedFrom 10000000 a &gt;&gt; feedFrom 10000000 a a = cf 0 (This also works if I separate modules). I added this as a remark to the ticket.
Yes; okay. However, mathematicians do need a way to rebuild their old foundation from the new foundation. With HoTT that seems as simple as locally assuming LEM. With MLTT and excluding HIT, I'm not sure you can get back to ZF, even locally.
Check again now - only two of them aren't available.