I have the same problem as you. I'd be one reader of r/learnhaskell if existed. There's also Stackoverflow as jaspervdj says. Or we could just start posting questions here and see what it happens.
Do note that arguably the ByteString version is incorrect: it works not on strings of characters, but on strings of bytes.
Honestly, if you find mailing lists cumbersome, its probably the fault of your email client. A good email client should be able to automatically sort incoming email into foders (eg separate folders heaskell-beginners and haskell-cafe). Its should also be able to show email in threaded view. 
If you want to know when you do something that's considered poor style, check out HLint (on hackage) -Wall will catch other, less sophisticated, things. Not that those are a real substitute for an experienced mentor, but you can run them as many times as you want, whenever you want.
&gt; Not that those are a real substitute for an experienced mentor, but you can run them as many times as you want, whenever you want. I think you underestimate dons.
The problem with forming a new subreddit is that current experts have no reason to join. Just throw your questions onto this subreddit or the existing mailing lists.
Nice :) The repository in hans.cabal resolves to a private IP and should probably point to git://code.galois.com/HaNS.git
Another variant is using GMANE's NNTP gateway for reading the mailing list...
Is there anything preventing Linux distributions from using this when packaging programs implemented in Haskell? For example, on my Debian box, `du -h /usr/bin/pandoc` says `17M` and, while not a big deal, it would be fun to have it a bit smaller.
Stack overflow can be good for specific questions, but more traffic here is good too! Either way it'll be dons answering you ;)
I'm looking forward to seeing what happens when you use link-time whole program optimization and dead code elimination on a GHC executable + libraries.
If you use the [Snap Framework](http://snapframework.com), it comes with [Heist](http://snapframework.com/docs/tutorials/heist), a template system that provides some great HTML abstraction tools which could work as a really lightweight blog system right out of the box.
Excellent. The HaLVM ecosystem is really shaping up! It's time for serious work, HaLVM hosted applications, firewalls, etc!
IRC generally isn't a good place to ask for help. Firstly it is very clumsy putting more than a few words into a message. IRC is real time Twitter. Secondly you lose any record of resolution so that future people do not benefit.
Actually, AxiomShell is correct, I wish to setup a blog written in Haskell as one small push to improve my Haskell abilities.
its not just keeping it up to date. If you want 0 worries about performance you have to install a bunch of caching stuff. And you need apache/nginx in front of php. Maintaining a static site takes 0 effort. Not everyone has experience with php like yourself, or wants some :)
All the more reason to automate building the map/reduce views! We figured which views are needed could be triggered by the use of Update, Eq, etc data annotations. I am probably missing something important though :). Perhaps that would create an exponential amount of views?
hmm, looks like CouchDB package was last uploaded in Nov with a build failure. Seems unlikely it would build at the moment on ghc7. It returns JSON, but it would be better to return Aeson, and even better yet to return something not presented for the web like an alist or an insertion-order map.
Yeah, I could've just used wordpress.com, I guess :) Might not have been able to install my own plugin for literate haskell highlighting on wordpress.com though. But I think I am more willing to create and maintain my own software (which is coding and thus fun), than I am to fiddle around with updating other software (which is system administration and thus boring).
I'd suggest against trying to jam a Haskell learning community into a subreddit. Reddit is intrinsically a temporal, transient beast, and a learning community really needs some sort of archive for the vast bulk of people who actually don't just leap straight to asking a question but know enough to search for the answer somehow (and so you never hear from them). Searching reddit is hard; the reddit search that allows you to narrow to a subreddit isn't great (though functional), and most people don't know how to use Google to limit to a single subreddit. Reddit is suited to what it's being used for here, links to papers, releases, etc., other news sorts of things. You could even jam it into a short-term, focused task, like working through an [ARG](http://en.wikipedia.org/wiki/Alternate_reality_game) or something with some temporal dependency. It's not really the right community style to work as a long-term learning setup.
Kinda SmallTalk-y-ish? All over again? This seems a bit like "big-dreaming" so let's push the envelope a little bit... I think a simple key-value store of functions for erlang could be interesting, and if you used something like Riak to power it, you could make a global community of software for Erlang users that people could join and leave as they wished. This could make an interesting global "system" of non-modules, but rather metadata tagged collections of functions. (new modules). The path to a module is currently a form of metadata. It describes where it lives, but that's not the most interesting thing to say about a collection of functions is it? #include "/path/to/file.h" The path doesn't really say much about what version of that library we're using (unless it's encoded in the filename), or even what kind of library it is. Is that useful to developers? Probably. I mean look what Maven and Ivy do for Java projects that want to pull in functionality. You know you want some version of log4j, and it goes and pulls it off the web to store it locally in a file... but if that jar were in a bucket, and we could just replicate it to our local key/value storage, we wouldn't need to remember where we put it, or have 90 copies of it for 90 java projects. One could try to run one's own "code ring service" and once in a while connect it to the main ring to sync it up. It's very "cloudy", and you could replicate the ring on Amazon or RackSpace hosted stuff potentially for backups. Also consider that Microsoft SharePoint is taking off like wildfire these days. No longer is it just useful to export a CIFS share. Users want some metadata around their data. It sounds like a potentially feasible future for software, and software configuration management. I think Pharo (Smalltalk) people might be ahead of the game though, because they're doing NoSQL storage now, and their class hierarchy is just a part of one big programming system. I need to spend more time with this, but my guess is this idea is not original. (most aren't) Also, I don't see how this could work in an RDBMS (even a sharded one). Those do a little too much work for us, and don't let us have the flexibility we need to survive certain conditions needed to keep systems up and running. Let's go further. Take this idea and apply it to applications we want to run. Maybe this can help with DLL hell (partially caused by hierarchical organization of libraries needed to run a program, and symlinks which cause cycles in the hierarchy and make messes of things more than they help... it's a band aid to a problem that's not been solved well) Maybe I should have been an academic...
Have your code on hpaste or as a gist and try on #haskell again. There is no limit to the depth of help you can receive there.
Don't forget http://codereview.stackexchange.com/ Still beta but I think it would be a great place for code review. I'd say in general... * if you have a specific question, StackOverflow is good * if you have a general question, StackOverflow is still good. /r/haskell and the haskell-beginners mailing list might be better options, or #haskell for more interactive feedback * if you'd just like to start learning and don't know where to start, [LYAH](http://learnyouahaskell.com) is a great way to go
Distros should start changing over to dynamically linked executables for Haskell, in my view. Will make shipping Haskell programs less intimidating.
no shame there, most mortals aren't
 &gt; I have several times heard that one should not rely on finalizers (that is, code invoked after some object becomes garbage) to reclaim external resources (file descriptors, temporary files, etc.), on the grounds that there is no guarantee they will be promptly reclaimed and therefore one might run out. As I understand it, the situation changed a couple of years ago, and now C-based finalizers do execute reliably and promptly. On the other hand, I don't think the GC is yet well-informed enough to do collections based on anything other than running low on RAM, which isn't really the criterion we want for these external resources. I'd really like to see better GC-based solutions to resource management, since GC hides deallocation, and so removes one more reason to program imperatively (since deallocation is an action). 
 fst "Kevin McCarthy" /= "Simon"
Ideally, he wouldn't have to hack GHC to write a node.js equivalent...
your comment has been reported. please only use comments without type errors.
wouldn't monads be perfect for that explicit callback style of code?
`s/fst/(head . words)/`
Wordpress’s code makes want to gouge my eyes out. I think the only time I’ve ever looked at the code was because cronjobs weren’t working on scheduled jobs weren’t working (turned out that the URL it was scraping was wrong.) Static sites have also been a no-go for me since I do want comments on my blog, and that means I have to implement something dynamic. Actually, that was about as far as I got with my last static HTML generating framework: I had it generating RSS and nice typography, but when it got to comments, I said "no way" and installed Wordpress.
`lim [interest_in_category_theory -&gt; ∞] helpProvided(#haskell) = ∞`
It doesn't necessarily have to be all about the blog content, though. Setting up and maintaining a Haskell-powered blog could be a fun and educational experience. If you get tired of all the Haskell work, and want to buckle down on the blog side of things, then sure, go with WP.
I would think so.
not anymore- Warp/Happstack-Server/Snap-Server all greatly outperform Node.js on a single core and unlike Node.js they can scale to multi-core. And no spaghetti callback code is required in haskell either. But at the time he started there wasn't a good event manager for GHC. Fortunately Johan and Bryan were smart enough to create that :)
callbacks in evented application code are a crutch for weak languages. In Ruby 1.9 using fibers the callbacks can be abstracted away so application code doesn't require any. Haskell doesn't even need that because it has non-blocking IO by default, probably why he says it would have been the perfect language.
I think the quality of IRC help depends a lot on two things, how you ask the question and who decides to answer it. #haskell tends to have high quality people answering questions so it tends to be better than other IRC channels for getting help. #haskell has a public log so the resolution does get recorded but SO is probably more efficient depending on the type of issue. IRC is going to be better for things that need iteration or discussion.
You know I think about this a lot, and the barrier to entry for hacking GHC, and honestly I don't think it's all *that* much higher than any other nontrivial compiler. That is, it's *high*, but not unnaturally so in comparison to others. But there are a couple things to consider - Haskell has a very different compilation model than most language (Core/STG in particular,) and furthermore, some parts of the compiler *are* rather advanced and unique, such as the type checker and type inference engine. These are backed by SPJ's monstrous JFP papers, for example. They are kind of scary, for sure. But lots of the infrastructure and solved problems inside GHC are the same for any compiler. Issues and considerations like calling conventions, garbage collection, a runtime system. Tasks like code optimization (Cmm is a rather straight forward, imperative, low level IR, and even Core has many common optimizations like CSE/inlining,) register allocation, symbol tables, linking, dependency tracking (for languages with module systems.) You see this kind of stuff in every compiler. I think the sentiment GHC is beyond most mortals is kind of overblown. The amount of people who can actually do it may be smaller than say GCC, but I don't think it's because GHC is so complex, but because that the community surrounding GHC (and Haskell in general) is much much smaller. If it's true though, the situation is likely not much different from most others. Granted, there *is* some scary scary code lurking in the depths of GHC (rts/Linker.c reminds me of a HP Lovecraft story. Can anybody explain why we aren't using the OS dynamic linker?) But the same can be said of anything (GCC's reload.c also comes to mind.)
What _really_ boggles my mind is why he didn't start with Erlang. Though I have rather impolitely theorized to myself that the reason he didn't start with Erlang is that he would have basically have had nothing to do. What further boggles my mind is that he clearly realizes that other event-based libraries existed, but he still seems to have pitched his idea as something new, instead of telling people that it's "event-based code as implemented in other libraries, only in V8". And apparently he has chosen to just let the community beclown itself by continuing with the fiction that he invented event-based code, instead of it existing in the mainstream for as long as GUIs have. I have no respect for him. (He can recover it by publicly educating people about how wrong the whole community schtick has been.)
I've wondered why he didn't do it in Lua ... even faster, smaller and more configurable.
you forgot esoteric.
Finalizers should be avoided when the foreign side is also trying to manage resources "intelligently". Here's a fun example of finalizers being too helpful that bit me a while ago. It was with XAudio2, Microsoft's latest redesign of the DirectX audio API. I used a let-it-leak style, since I don't care about restarting my computer once in a while while writing exploratory code. The code was vaguely like this: interface &lt;- createInterface orSomethingLikeThat device &lt;- createDevice usingThe interface doStuff thatTakesAWhile withThe device doMoreStuff withThe device The first operation worked, but the second didn't. Sometimes. If I made the program longer, it would fail all the time. Compiling with optimizations let me use longer programs. Some debug print statements revealed that `device` would mysteriously become invalid at some point. The problem was that since `interface` was never referred to after being used to create `device`, the first garbage collection would trigger its finalizer, which ran the related COM shutdown function. DirectX uses reference counting for object management, so when `interface` was deallocated on the C side, `device`, which refers to `interface` internally, was invalid, so it was also deallocated on the C side, leaving the Haskell side with a live but invalid reference. In this kind of case, an automatic finalizer can't do anything useful. The object that needs to stay alive can be kept so by putting a statement on the end of the program that does nothing other than reference it. An overly aggressive optimizer might observe that `id` does nothing and remove it entirely, so the safest thing is to pass the value to a foreign function. One with a name like `finalize`, for example.
 Though I have rather impolitely theorized to myself that the reason he didn't start with Erlang is that he would have basically have had nothing to do. You are theorizing publicly here, not to yourself :) The first line of nodejs.org: Evented I/O for V8 JavaScript. Later on: Node is similar in design to and influenced by systems like Ruby's Event Machine or Python's Twisted. I am not saying you are entirely wrong, but maybe you could 1) tone down the hate (or increase your respect) and 2) cite your accusations (specifically about Dahl, as I am sure you can paint a community any color you like). I am a big critic of node.js, but rather than go into attack mode I think it is better to just state the simple fact that you have to write callback spaghetti code, whereas in other languages (like haskell!) that is abstracted for you.
I don't have accusations for Dahl, except inasmuch as the community needs to be corrected. I carefully phrased it that way on purpose. The hype needs to be punctured for the project's own good, because I've seen this hype pattern before; it's a fast ride up, but often a fast ride down, too. It's not a bad implementation of what it is, but feeding the meme that async event-based programming is _the_ definition of good software engineering (and I chose that phrase carefully, because I've encountered far too many people for which that is an accurate description of their mental state) is a terrible thing to do to poor, unsuspecting young software programmers in 2011.
 Couldn't match expected type `(a, b)' with actual type `[Char]'
seriously.. this from the man who has us type checking our URLs and forms. :)
Well, at least it is a good meme when applied narrowly. It could be a lot worse- your young impressionable programmer could think that the Factory Pattern is the epitome of good software :)
I think he meant: fst ("Kevin", "McCarthy") /= "Simon"
Of course I can make type errors, my name isn't Simon.
If only reddit were built with Yesod, then comments could all be run through `hint` as part of the form validation, so that only well-typed comments could be posted.
I haven't hacked *on* ghc yet, but I've *used* its internals in Tandoori and in the stepwise STG debugger that I should eventually just finish and release, and my experience has been that its code has pretty clear boundaries between its compilation stages and it's generally easy to navigate when you're looking for a specific part.
"Callback spaghetti code" is a misnomer... Spaghetti code and callback-code (CPS code) are really different things. Some people actually *prefer* callback-based code. From the interview: &gt; I liked the design of event based servers because I felt they were easier to understand: state is kept in some struct and you go around and around modifying the state. There were no infinite while loops making blocking reads or accepts from sockets (which has always struck me as a very strange pattern). I think there's something nice about having a callback for received data over having threads doing blocking reads. For example, if your socket creation function takes an incoming event handler callback -- it can guarantee there's exactly one handler for incoming data that's always available to handle the data. If you have threads doing blocking reads, you can have 0 or more readers -- which is almost always the wrong thing to do.
In environments that don't support ultra-cheap user-level threads (even 4K minimum stack is not nearly as cheap as your typical explicit state structure) -- explicit state-machine event-based code will have better operational characteristics. In that sense, it is good software engineering...
I think the answer is momentum. Lua is not accepted by many as a capable platform. The libraries are disorganized and the community is not well-organized. LuaJIT is awesome for speed, but it's all done by one guy. Contrast this with ECMAScript, which has a whole team for very smart people from Google, Mozilla, and Apple cranking out performance updates at a rapid pace. If you had to bet on your platform continuing to grow for the next 5 years, would you seriously bet on the Lua developers over that group? 
Perhaps part of the perceived barrier to entry is that one often thinks of the compiler and the RTS as one entity.
I feel like one could do a reddit backend in yesod in 2 weekends. yeddit.
I have a fork of the CouchDB package here: https://github.com/tbh/haskell-couchdb I fixed some of the build issues and made some additions, but did not try with GHC 7 yet. A switch to Aeson would be great, indeed.
This is what statements like `touch` are for. Granted, it's a bit dodgy if you have to do that, and at least touchForeignPtr shouldn't be used in GHC to express dependencies.
Its fine if you have *a* callback. The problem is when you need to make multiple asynchronous calls at the same time. Then you need design patterns to handle what should be runtime plumbing: http://stella.laurenzo.org/2011/03/bulletproof-node-js-coding/ see HN discussion: http://news.ycombinator.com/item?id=2370303 You are setting up a false dichotomy with respect to thread readers- the point is that async IO should be handled transparently by the language runtime as in Haskell or Erlang, or in languages without the runtime but with co-routines, transparently by the library writers. Unfortunately, Javascript, and by extension, Node.js has neither of these.
glad you are sharing on github, but would be great if you can get in touch with the package authors or otherwise upload the new version to hackage :).
I'm a little bit sad that [my post to stackoverflow](http://stackoverflow.com/questions/5939630/best-way-of-looping-over-a-2-d-array-using-repa) a few weeks ago never made it into the Haskell Weekly News. It's not amazing in any way, but someone following reddit but not stackoverflow might be interested.
The simple answer to "why we aren't using the OS dynamic linker" is that the OS dynamic linker doesn't do the same job - it can't link static .o files, only shared .so files. So you might then ask why we don't generate .so files instead. That's a more difficult question to answer. Probably we ought to, but there are a lot of knock-on effects.
Yes, it does.
Simon Squared have done it again!
Out of curiosity I looked this up and yes, from a bunch of old mailing list posts, there are a lot of things to consider if we just want GHC to use dlopen: * GHCi only loads .o files, so we'd need to change it to support dlopen'ing .so files (i.e. gcc -shared needs to be run on the resulting object files somehow.) * That means that every object file we compile needs to be dynamic, if you want a stage2 GHC to work - so -fPIC is necessary if you want to use GHCi * That means that any regular C code that we compile with GHC (things like sqlite, quicklz or snappy - self-contained and included inline inside packages many times) also needs to use -fPIC. * All of this means you'd probably want stage2 GHC to always use -fPIC and dynamic linking by default I'd suppose (see below RE x86_64/OSX.) This doesn't seem like a real portability problem either, considering we have to write our *own* homegrown linker for every platform combination for a stage2 GHC/ghci anyway. Naturally this is going to have to cut into cabal in some ways I think (duncan might want to chime in here.) That said, after combing over some stuff, I think this isn't as large a task as it might have been a few years ago, and in fact may be something worth investigating: * There seem to be some outstanding bugs like this one, which are 'fixed' by the homegrown linker, but still open: http://hackage.haskell.org/trac/ghc/ticket/781 which could be solved by doing dynamic linking. * Due to the above, -fPIC is probably good anyway on x86_64 (I think you've said this too.) Some platforms like 64bit OS X enforce -dynamic and -fPIC by default, so at least on one platform, you can't get away from it. * I think we now have good dynamic linking support for haskell code on all the major platforms, including windows (DLLs,) don't we? That was one thing that was a problem, but from what I've recently understood I think that's done. So using dlopen is definitely possible on all the Tier 1 GHC platforms at least.
I feel it may be necessary to clarify that my post was not about any Haskell or any other specific programming language.
I feel it may be necessary to clarify that my post was not about any Haskell or any other specific programming language.
My perspective is that this is a bug in the provider of `createDevice`; `device` should retain `interface` (or they should both retain a separate object which the shutdown finalizer is attached to).
The problem is... they *almost* are. And so are some of the critical library components, like `base`. It's somewhat rare that large changes will affect only the compiler proper, but not the RTS, or the libraries, in some way. Maybe not a lot, but they're definitely not out of consideration. But! All this again can be said of say, GCC: gcc changes will affect glibc, and its own internal runtime system in very large ways sometimes. And that stuff will also regularly spill over into binutils based code and a million other things. So again, it's not all *that* different from other systems, I think. Just unique in some ways. GHC has a much more complicated RTS than most systems, to be honest. I wish there was more documentation on it, as it is an intriguing piece of code, but alas, it is quick to change and kinda hard to grok. It feels hard to understand those components in isolation, but I don't think it's quite as hard for GHC itself (maybe there are more individual components that can be easily understood.) Luckily for us at least, we have great people like Simon to guide such a project.
I've been wanting to see the results of this for a while, thanks Simon! EDIT: is there a publicly viewable branch anywhere (or could one be pushed to say, github, for people to see?)
I really wish I could go but I will be moving at that time. :( Perhaps I can join via IRC.
Odd that it didn't make it. I was under the impression that all questions with significant upvotes got included, and that one certainly got a lot of upvotes.
it's in a darcs branch right now... I need to merge it up and push it to the git repo. Hopefully I'll get time to do this over the summer.
&gt; C has similar problems as Lua and is not as widely accessible. Wait, what? C is less accessible than Lua?
&gt;*fuction* of this subreddit I see what you did there
The info on how to use splices is great, but it seems very strange to have database queries living in the template layer. Part of the reason why it seems so strange is it means any processing of data that you have would have to be in the template layer, which is a bad idea. The model I try to stick to is to have thin layers at either end (the data, the presentation) and then try to keep the bulk of the business logic be operating in the pure world. So I like the post, but I'm scared if it leads people to design that way.
Good point, though the point of the app was more to just get a feel for how Snap works before embarking on a real project. I haven't even figured out yet how to pull data from a POST and use it in a template. 
:)
Nice talk! Most explanations I've seen about lenses stop at get/set/modify. Seeing how they can be composed and especially the connection to the State monad was quite enlightening.
`unsafePerformIO` is unsafe.
I came here to say exactly that, but you beat me to it!
Did I mention that it is unsafe?
The great thing about Heist is that the user can choose whatever level of abstraction is appropriate. I tend to agree with you though, and in my apps I find myself creating more domain-specific splices. In this example I probably would have created a &lt;dublinerCheeses/&gt; splice and then built it from more general database query abstractions implemented as an API of Haskell functions.
The author seemed to expect Data.Unique to work across multiple executions of the program. Without using timestamps, which might be used multiple times within the same time unit or the clock might be set back, or random numbers, which only give you probabilistic uniqueness, or some shared state between executions, that isn't possible.
You might be interested in checking out some of the generic splices that I've been putting together [here](https://github.com/mightybyte/snap-heist-splices).
long cat is long
have you tried this in scala: https://github.com/urso/embeddedmonads it seems to add support for using direct monadic expressions using scalas delimited continuations.
&gt; How unsafe is Unsafe? Fixed that for you. :-)
Good point. =) I usually don't like hiding the current monad behind cps-based embeddings, but it does seem remarkably suited to this.
I'm still pretty new to snap too, but probably what you are going to want to be using is bindSplices, which allows you to define splices that will be available in a given render of a template. So you will be dynamically generating different splices on every page visit - mightybyte's blog has some really good examples (http://softwaresimply.blogspot.com/). If you want to check out a large, working, example, I'm in the process of rewriting an existing webapp (http://housetab.org) with snap - an excerpt that might be helpful is at http://darcsden.com/dbp/housetab-snap/browse/src/Site.hs#L-95 - the twenty lines that follow define the way in which I render entries, which you can see in action at http://darcsden.com/dbp/housetab-snap/browse/resources/templates/entries.tpl#L-9 (sorry for the lack of line wrapping!). The sub-splices (not sure what to call them, but what is used in the code I linked to, and is the subject of mightybytes post on looping) are the thing that has made me go most "wow" about heist (there are other cool things too) - it is an abstraction that makes so much more sense than building looping mechanisms into the templates. 
Or three, if they forgot to NOINLINE
I'm not really familiar with this package, but wouldn't the version 1 generator [uuid](http://hackage.haskell.org/cgi-bin/hackage-scripts/package/uuid) be a good place to start with getting something more close to actually unique? I mean, it also uses unsafePerformIO, but it seems to implement the UUID spec pretty closely which is about as good as you can get without having a system level facility for this purpose.
Your term "sub-splices" is pretty much the same thing I meant by "...abstractions implemented as an API of Haskell functions" in my comment below. In my use of the term splice, a function "foo :: a -&gt; Splice m" is not a splice. I say this because you can't bind it directly (i.e. bindSplice "foo" foo). foo is really a function that generates/returns a splice. The question mentioned in the blog post of how to bind splices doesn't have a hard and fast answer. There has been some recent debate amongst the core Snap developers regarding the choice between binding splices all at once when the application is initialized or dynamically in the course of request processing. In my applications I tend to prefer binding them once at initialization. I like this because you're essentially handing designers a fixed "API" of dynamic tags that they can use in their templates. The other approach gives you the added flexibility of being able to dynamically bind splices depending on values calculated during the course of request processing. It's more powerful, but it means that the designers have to be more aware of the context and which splices are available when. I think each approach has merit and we plan to continue supporting both of them.
In Haskell, you can create a socket, and then multiple threads can try to read it, which is wrong. With an event-driven library with an API like: createSocket :: ReadHandler ... -&gt; IO Writer you can't. Iteratees have nice API's like that (enumerators that create the socket AND make sure its input is fed to exactly one place). The thread model has disadvantages too.
Use unsafePerformIO, and you get what you deserve.
Here's a slightly modified version that may be easier to build, for anyone who's interested: https://github.com/isomorphism/hsadventure
sweet, thanks!
Well, to start, pi is not equal to the number you just posted. Symbolic manipulation packages will keep pi as a unique symbol until some reduction can be performed -- e.g. sin(pi) -&gt; 0. It will only produce an decimal approximation if you ask for one. 
 &gt; pi =&gt; 3.141592653589793 pi, as a symbol in Prelude IS that value, [as you can see here](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/GHC-Float.html#pi). I was just trying to illustrate that it would be impossible for Haskell to do symbolic manipulation, as you suggested it might, when working in Double or whatever.
Agreed, prelude.pi is unsuitable. I was speaking of the number pi, and not the approximation appearing in prelude. Sorry for the ambiguity. 
AFAIU, `unsafePerformIO` is only safe if it always produces the same result in all `IO` contexts (and there is probably some other constraint about not affecting global state either). Calling `newUnique` is close to the antithesis of producing the same result in all `IO` contexts.
I got a t-shirt made like that. 
Shut up and take my money! Seriously though where can I get one of these?
Seconded.
I've made it publicly available [here](http://www.streetshirts.co.uk/qrcomic).
It seems to me that dynamic linking would rather make shipping Haskell programs far, far more intimidating, as it would open you up to an order of magnitude more "I installed it here and it didn't work" type complaints. Dynamic linking is a nice idea in theory; but so long as there are deep dependency constraints in Haskell libraries (i.e., not just that I depend on libfoo-0.1.6, but that I need a version that was built against libbar-2.3.0, as well), it will remain practically necessary to stick with mainly statically linked libraries.
I want a hoodie with a picture of that guy wearing that hoodie. How far can we nest this with the original hoodie being recognizable?
Didn't expect to see you on Reddit :) Hows Stacs going so far btw? Looking forward to seeing more of the hoodie in our lab next year
Ordered.
Apparently no international shipping.
Ach, balls. If you want, I could print a load of them off elsewhere and post them myself. Streetshirts is rather expensive, though maybe the next cheapest option would be a vinyl cutter and a heat press. I already have access to a heat press, so may actually be workable. Depends how much demand there is. Tell you what, anyone who wants one, send me a PM and I'll see what I can do. Other colours of logo/hoodie are available too.
Well, in imperative languages, this is worked around by having stable interfaces to your libraries. Is there a reason this could not also be applied to Haskell?
Well whaddya know, it *is* a small world after all!
As you know, I have it, too. Should ask to get a picture with it taken next week. Or we both wear it and get a picture together!
Oh, it definitely makes cross-platform distribution harder. But on a single distro, where you control the environment, it would be preferred (e.g. Arch Linux or Debian could do this safely).
The main reason is that it doesn't work for less powerful languages either; but there, the problem is small enough to be manageable because the abstraction capabilities of the language limit the amount of code reuse that's possible.
Are you sure? It seems to be that using shared libraries takes every problem we currently run into with the GHC package database on development systems, and pushes it out to become an end-user support issue as well. My view isn't quite as black and white as it might have sounded; in the long run, we should probably move toward having a certain trusted core of packages distributed in shared library form, while anything outside that core should be statically linked by default. Perhaps the Haskell platform is a good approximation of such a trusted core... or perhaps the trusted core should be smaller (e.g., base and anything it depends on) But pushing all the Haskell third-party library dependence graph into runtime resolution is a roadmap for catastrophic failure.
The cool thing is that to non geek crowds it still *functional* in that it just looks like a nice pattern. Geek respect without the *side effects*.
Yo dawg I herd he lieks recursion so I put an Xzibit shirt on Xzibit's shirt so Xzibit can exhibit Xzibit while exhibiting Xzibit.
it almost seems a little APLish...
That's not a bad thing per se. There is a need for a direct, low-level binding (with a close mapping with the original C API). (And I guess there is a need for high-level, haskellish API.) The anecdote shows that OpenGL programming doesn't just require programming skill, but also require to learn a somewhat complex library.
Could you give an example of where the abstraction abilities of Haskell pose a problem for a dynamic linking scheme?
See http://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/ for a scenario involving the GHC package database. If the relevant libraries were dynamic instead of static, this exact same thing would be an issue for the end user with dynamic libraries. Of course it's possible for the same issues to come up in less powerful languages; but in practice they don't, because people don't pervasively reuse libraries in that way; it's rare, for example, that the interface between an application and a library depends on types that are defined in a different library. A significant Haskell application may depend on something like an order of magnitude more libraries than a significant Java application; and the kind of reuse is different, too, involving a lot more type-level abstractions.
Thanks! This answers my question nicely. It seems some improvement of Cabal is definitely in order.
True! But at the same time, the part that's important here is that all the weaknesses of Cabal are *also* weaknesses of the rest of the distribution infrastructure (for anything except [Nix](http://nixos.org/), which gets this right, but isn't widely used). That is, for the same reasons it's insufficient for GHC and Cabal to identify libraries by version number, it's also insufficient for the traditional symlink structure in /usr/lib to do the same. It's also insufficient for apt-get or rpm to do it. All of these systems would need to be cured of their fatal assumption that all we need is a version number, and that the options for any piece of software are totally ordered with a "later is better" rule. As long as those pieces of software are making bad assumptions, shared library distribution of *any* code with this kind of reuse of types and abstractions (Haskell or otherwise) will be problematic.
Could one perhaps infer that the very stateful nature of OpenGL is inimical to idiomatic haskell? People often speak of a nice functional wrapper for OpenGL but I can't imagine how one would work.
It's not just that it's imperative, though it is. Haskell actually works pretty well for that part--there's a lot to be said for distinguishing between "expressions returning values" and "procedures performing actions", a distinction that some imperative languages attempt to make, which Haskell makes clear and absolute while also making "procedures" first-class entities and allowing the program to manipulate them as such. It's really quite nice from a "structured imperative programming" perspective. The *real* problem with OpenGL is that the entire design revolves around an invisible, implicit state machine with values being poked into and read from global variables, things that behave completely differently depending on that internal state, and whatnot. I've done OpenGL programming in C, and I've done imperative-style programming in Haskell, but OpenGL in Haskell gives me a serious headache. I think the first step to making OpenGL programming tractable in Haskell doesn't need to be making it functional; just making it *explicitly* stateful would be a huge step forward. Take the specification for OpenGL's internal state machine, reify it using tokens of some sort, then make the operation of the state machine clear and enforce with types that operations invalid for the current state can't be performed.
See graphics-drawingcombinators for an example of how one could work.
I'll make sure mine's washed for our trip! We'll look like some kind of cult; it'll be awesome!
any more info?
`graphics-drawingcombinators` is lovely and happens to use OpenGL, but I'm not sure how well the approach it uses would generalize to a full "functional wrapper for OpenGL". Specialized functional graphics libraries implemented with OpenGL are useful, but it'd be nice to have a less painful interface to the whole thing. Even though most of the time I'd rather use something like `graphics-drawingcombinators` myself...
Yeah, I agree that a "tower" of abstractions could be useful. Wrap the ugly OpenGL interface with a saner imperative one. Add as many layers of imperative wrappings as needed to add elegance. Then, writing the functional interface around it becomes much easier, and it is easier to make thinner layers lose less of the capabilities.
You might want to check out [GPipe](http://www.haskell.org/haskellwiki/GPipe).
The biggest obstacle is that OpenGL is really big and complicated, and at least the first set of imperative wrappings would need the attention of someone who actually understands it beyond the superficial level needed to use it in standard ways. I know that, personally, I'm not even *remotely* qualified, heh.
So couldn't you encapsulate the required global state in some kind of context, and then have Opengl functions be required to operate in that context, along with implementing a required signature, as is required with IO? Or thread the state through with some kind of arrow-like system? Sorry, total newb, just thinking here.
See, the issue is that you don't *have* the state, it's sorta ambient. In other words, there's not a tangible state value to thread through. If there was a big blob of "OpenGL state machine context" available that encapsulated everything and you could pass it around, that would be much easier, but that's not how it works (and it really couldn't work that way, because some of the state lives in the graphics card). So to create an "OpenGL monad" encapsulating the context, the only option is to pass around some opaque token that can't be duplicated, which represents access to that ambient state. This is basically how `IO` is implemented under the hood, for the same reasons. With `IO` there's not much else you can do, because its whole purpose is a catch-all dumping ground for external side effects. With a hypothetical OpenGL monad, however, we have a predetermined amount of state we care about and it would be nice to make certain state transitions explicit, via type parameters in the monad type, or data types containing certain operations, or whatever else. But since all the statefulness is implicit, the only way to do that is, essentially, reimplement the behavior of the OpenGL state machine at an abstract level. To give a more concrete example, consider the old-style Begin/End stuff; they need to be in matching pairs, and we'd like to ensure the primitives that only make sense in that context aren't called outside it. Additionally, if the mode is something like `GL_QUADS` vertices need to be specified in groups of four, which could also be made explicit. In contrast, the existing OpenGL bindings do [provide a delimited context for Begin/End](http://hackage.haskell.org/packages/archive/OpenGL/2.4.0.1/doc/html/Graphics-Rendering-OpenGL-GL-BeginEnd.html#v:renderPrimitive), a minor and simple improvement over the C version, but that's all.
Googled the title. Is this the missing link? http://groups.google.com/group/haskell-cafe/browse_thread/thread/83c60a85586707ca?pli=1
Nah, HOpenGL is not that low level. However, OpenGL itself is a state machine.
It's not nearly as bad as it seems. If you dump all of the deprecated crap that's still in HOpenGL, you have a pretty clean state machine that mostly deals with video card memory and shader state. Buffer and texture objects can be easily handled as opaque types like Ptr, and shader state is not that hard to encapsulate. What I do is encapsulate the relevant state to my shader programs in a data type and create trees that represent calls to change the state and draw. Rendering is reduced to building trees and traversing them to create an IO action. It's very nicely functional. Someone is working on the library as a GSoC project. They're going to clean up the api a lot. To be honest, they ought not to do more than make a thin wrapper because everyone has their own idea on how to implement OpenGL functionally.
Well, I was assuming the deprecated crap would remain... particularly since a lot of outdated introductory material is still floating around that uses it. I expect dumping it would be better for serious use, though. But I'm far from an expert on OpenGL; sounds like you have a much better idea about it than I do.
A good introduction to modern opengl, in Haskell to boot, is [here](http://www.arcadianvisions.com/blog/?p=224). It is absolutely thin though. You can build abstractions to make things more functional.
I got the link right this time.
Hunh, that looks much more useful than the info I found last time I did some OpenGL stuff. Thanks!
Do you have any code examples? I would love to learn how to work like that!
I intend to release some simple games once I have everything down. I feel like Haskell is lacking examples of such complete packages. Watch this subreddit in a couple of weeks.
Tbh, it'd probably be just as awesome if you just released a simple test example that draws a textured cube with some shaders. We're all just looking for the best way to get to that point. :)
There's multiple ways to get to that point that are ostensibly equally valid. I'm not sure my style works. It's working on the small tests I've done, but until I use it in a real project I am not confident enough to share it publicly.
This is true, but there's clearly interest in seeing your particular style.
Would web workers be suitable to implement threading for most browsers?
Sadly I don't think they will help much. I suspect the trick might be to create our own stack of contexts. Each context object will need to store the local variables and a kind of program counter. This will allow us to avoid depending on the JavaScript stack (in the same way yield does). If you wanted to do something like this... { var x = f(); return g( x ) } Which is currently mapped to something like this... { var x = yield f(); yield Jump( g, [x] ); } The generator could instead output something like this... if(context.pc == 1 ) { context.pc = 2; return Call( f, [] ); } if( context.pc == 2 ) { context.pc = 3; context.var.x = context.return_value; return Jump( g, [context.var.x] ) }
I think I can understand the point of Voevodsky's work on univalent foundations in the context of mathematics. However, does it have any relevance to programming languages?
It's relevant for the future of proof assistants and their corresponding languages. If I understand that correctly, Voevodsky wants to come up with a type theory that avoids Goedel's undecidability theorem by changing the notion of proof to something that is "strictly finitary" and does not allow a diagonalization argument. EDIT: I mixed up his talk on undecidability and the homotopy type theory, sorry.
&gt; If I understand that correctly, Voevodsky wants to come up with a type theory that avoids Goedel's undecidability theorem by changing the notion of proof to something that is "strictly finitary" and does not allow a diagonalization argument. [Yeah, about that](http://www.cs.nyu.edu/pipermail/fom/2011-May/thread.html). (Search the page for the thread titles "Remedial mathematics", "Are the proofs of Con(PA) circular", "Voevodsky", "Invitation to comment", and so on. It seems that Voevodsky has a confused notion of what Goedel actually proved, and what its consequences are. The correspondence with Harvey Friedman is especially interesting, as Voevodsky seems to think that any proof that cannot be carried out in PA is not a "mathematical proof", and that there's a possibility that PA may be inconsistent!)
OpenGL is how we talk to the hardware, not how we construct software. Haskell via GHC presents us with an elegant, efficient abstraction over CPUs and RAM that sometimes denies access to tantalizing possibilities (e.g. SIMD), while libraries like gloss and GPipe offer abstractions over GPUs. Those libraries also come with limitations, so it's good to know how to FFI into the GPU via OpenGL.
I was under the impression that Voevosky's lecture on undecidability was simply a side topic of interest and his work on type theory is not motivated by this. It is my understanding that his motivations for homotopy type theory is to give a purely axiomatic presentation of homotopy theory, similar to how axiomatic group theory divorced the definition of group from subgroups of symmetric groups, and it just happened to turn out that Martin-Löf type theory was almost such an axiomatization of homotopy. This in turn lead to the idea that such an axioimatization of homotopy theory could also serve as an alternative to set theory as a foundation of mathematics.
I think that is a side interest of his, and one where he is either not communicating his ideas very well or where he is outside of his comfort zone, as others to reply have noted. In order for a system of proof to even approach being useful for even the simplest mathematics, it needs to be able to express computable functions on the natural number. Robinson's system Q is more or less the minimal system of reasoning that achieves this (with a single existential quantifier in its axioms). An appropriate restatement of Goedel's Second Incompleteness Theorem also holds for this system. In systems weaker than Q where Goedel coding no longer goes through, it doesn't really make sense to ask whether the Second Incompleteness Theorem still holds. Incompleteness is an infectious little thing; being able to express the question predetermines the answer.
&gt; Voevodsky seems to think that [...] and that there's a possibility that PA may be inconsistent! It looks to me that he's merely pointing out the obvious: how can you know that PA fails to be inconsistent?
&gt; it needs to be able to express computable functions on the natural number I believe his point was that this does not need to be the case; a system of arithmetic could be useful without being able to express all computable functions. System Q looks very interesting. Apparently, he would reject it as well, though. :-) &gt; where he is either not communicating his ideas very well or where he is outside of his comfort zone I'm quite sure he knows what he's trying to do, except that he hasn't done it yet and hence no concrete results to present. Communicating ideas at this state is very difficult. :-) What I understood was that Voevodsky probably takes issue with the existential quantifier. For instance, Gödel's encoding doesn't work if you are only allowed to use *bounded quantification*, i.e. where you have to give an upper bound on a number before asking for its existence. He was talking about an operational procedure, which I understood to work as follows: * Check this proof. * If the check terminates, then all quantifiers were bounded and it's ok. * If the check doesn't terminate, then two things might happen: * 1) We didn't wait long enough and it might still terminate. We can't know, though. * 2) The proof does not actually terminate! Voevodsky only accepts proofs where the check terminates. While he excludes many theorems that might be true, this procedure doesn't contradict actual mathematical practice: you only accept proofs that you can check in finite time. (In a sense, that's precisely why the four color problem is controversial.) The open question, of course, is whether suitable notions of "proof" and "system" exist such that this idea can be made to work. 
I don't know why anyone would try to maintain the big ugly pre 3.1 core OpenGL. HOpenGL was made before 3.1 core happened and it's visible in lots of places (though there is 1 sample demonstrating shaders and so on).
what are the differences from the up coming [CloudHaskell](https://github.com/jepst/CloudHaskell)?
Yes, haskell allows you to write bad code and a library writer should expose a good API. To be clear, the ghc runtime uses threads of executions (as opposed to callbacks or co-routines), but it *is* evented, utilizing libev to manage which thread will be active. Haskell doesn't have 1 model or the other, it uses both to their greatest effect, again exposing a false dichotomy.
I'm talking about the (dis)advantages of the thread model in the context of preventing a particular user error (multiple readers on same socket). I understand and agree that GHC utilizes the event-driven model for its full effect, performance-wise. But programmatically, it still exposes the threaded model, mainly. The API can be threaded, or event-based (Or maybe have elements of both). I like the Iteratee API as it avoids the particular bug I mentioned above, which normal forkIO/blocking-reads still make possible.
CloudHaskell is for communicating between separate haskell processes (as in operating system process) that may lie on separate machines. Any parallel/concurrency tool you see will be for use within one haskell process unless it states otherwise.
My *very unsure* understanding is that with homotopy type theory one gets multiple proofs of equalities of types, where each proof represents an isomorphism of the types. Thus there would be two proofs that Bool is equal to Bool. Thus type-safe casting becomes flexible enough to apply to isomorphisms rather than simply identities. I want to stress that I am *very* unsure that what I said above is correct.
The position is open to UK citizens, and EU citizens who have been resident in the UK for the last three years.
My confusion here comes from what is being attributed to evented code. Any language with anonymous functions and error handling can trivially expose a resource solely for the duration of a callback. This is independent of whether the resource is actually being utilised by an evented runtime or using threads of execution. So it seems like you are complaining about poorly written libraries (or lack of libraries?) rather than an inherent problem with threading. If a library exposes a resource, I agree that callback style is appropriate. And a library may need to expose a lower level api for special cases. However, when possible, libraries should provide higher level APIs that just send data to and receive data from resources without directly exposing them, in which case our application code is still left with callbacks in node.js, but our haskell code looks normal.
&gt; The great thing about Heist is that the user can choose whatever level of abstraction is appropriate. I found the blog's simple sql "dsl" to be quite intriguing. It certainly touches on the flexibility and expressiveness of heist.
Aww, no point in applying from the Netherlands then. :-(
&gt; I believe his point was that this does not need to be the case; a system of arithmetic could be useful without being able to express all computable functions. It gets worse than that. Solovay has shown (with an appropriate formalism) that once addition and multiplication are total functions you fall to incompleteness. I think that includes any system that is useful for doing real mathematics. &gt; I'm quite sure he knows what he's trying to do, except that he hasn't done it yet and hence no concrete results to present. Communicating ideas at this state is very difficult. :-) I am giving him the benefit of the doubt because what he is saying makes no sense. Thankfully, it doesn't appear in his research proposal at all http://www.math.ias.edu/~vladimir/Site3/Univalent_Foundations_files/univalent_foundations_project.pdf &gt; Voevodsky only accepts proofs where the check terminates. While he excludes many theorems that might be true, this procedure doesn't contradict actual mathematical practice: you only accept proofs that you can check in finite time. (In a sense, that's precisely why the four color problem is controversial.) The open question, of course, is whether suitable notions of "proof" and "system" exist such that this idea can be made to work. This doesn't escape incompleteness. The Second Incompleteness Theorem has a fairly short proof relative to normal mathematical results, and thus would pass the operational check.
Never before have I so wished I was a UK citizen.
Thanks for the vigilance, dons.
Look forward to this continuing. I was somewhat confused by Yesod on first glance. I am not really into web stuff though! It might be fun to follow along. :D
Is this about Haskell programming?
There's a cult?! wtf guys, why wasn't I invited
When you attain comprehension, it will be as if you had an invitation all along.
First rule of the Haskell Cult is, you don't talk about the Haskell Cult.
One thing remains... Harper... You must confront Harper. Then, only then, a Haskeller will you be.
Your first initiation task is to write a blog post attempting to explain monads in a simple manner.
What about Harrop?
That's the initiation rite for the general functional programming cult. Lispers, O'Camlers, Haskellers, they've all been through that initiation.
Really? Tell me more.
We bind together, The monad, and I until only the monad remains.
I thought it was called the Haskell Cabal. Perhaps I've been doing it wrong all these years.
but you must pick a metaphor which hasn't been used yet and ideally explains at most 0.05% of the use cases of monads. I've been thinking of a blog post called: "monads are thorium fission nuclear reactors" but haven't gotten around to it yet.
As in list comprehension?
In Haskell the cocult cojoins you.
&gt; The Second Incompleteness Theorem has a fairly short proof relative to normal mathematical results, and thus would pass the operational check. Well, the notion of proof would need to be changed. It's similar to a dependent type theory not being able to implement an interpreter for the untyped lambda calculus, even though said interpreter would be very short if it existed.
You start with the level 1 spell book: LYAH...
You mean extends?
The thing is, the cult is very lazy.
 join :: Cult (Cult a) -&gt; Cult a For the proper definition of `Cult` there is a unique (up to isomorphism) solution to the above equation, which is left as an exercise to the reader.
*Couldn't resist turning this into haiku:* We bind together, The monad chimes within me. The wind can't see us.
Applications? Many are entered. Few are evaluated.
The Haskell Secret Underground does not exist, and if it does, I've never heard of it.
Another fascinating thing that turned up is Bytestring's aggressive reuse of existing chunks, so that small substrings could end up causing a larger bytestring to be retained much longer than you'd expect.
Upload a package to Hackage.
'doomed' is a bit of a loaded word. There is an argument that spreadsheets are functional programs, though they are usually pretty basic ones.
Can't we just answer "No." and leave it at that?
This is the same behavior Java uses for its Strings.
Monads are like a collection of joined strings of Christmas tree lights wrapped in flash paper. You can plug strings of lights together (bind them) but, because they're wrapped in flash paper, you can't tell what color the lights are, and they don't do anything until you plug them into the wall (run them). When you plug them in, the lights all light up in strict succession (although it happens so fast you can't see the sequence unless you combined string is very, very long). The heat from the lights ignites all the flash paper, which burns away, and now you can see the results. Tangentially, the mess left on your carpet is garbage, which will be collected by your vacuum cleaner, which, since you have automatic garbage collection, is a Roomba. 
If you compare number of humans to number of cockroaches or even ants, you can come to conclusion that humanity failed, and true masters of this planet are insects. I do not have a problem with humans occupying a "niche" in the biosphere, and i certainly do not think that functional programming doomed or failed just because it will never cross 5% plank. I feel pretty good as a functional programmer. 
There is the Cult of the Bound Variable, but their website seems to be down.
There's also [The Church of the Least Fixed Point](http://leastfixed.com/)...
If you want simulator practice for this initiation rite, there's always the Harrop Level Virtual Machine...
Maybe GC integration with ByteString code could allow identifying situations where a majority of a ByteString is inaccessible and copy just the used part?
Yes! I don't understand why nobody here mentioned ditching OpenGL. I always thought the whole purpose of CUDA and OpenCL was to be able to use your GPU on a lower level. In theory, performance should be identical, no? I guess it's GPipe is missing a lot before you can implement stuff like [Frag](http://www.haskell.org/haskellwiki/Frag) with it. Anyone? Why aren't more people trying to make a modern 3D graphics platform using GPGPU capabilities (and avoiding Direct3D/OpenGL)? Another thing I don't understand is how it mentions on the page that it is an OpenGL alternative, but in [this mail](http://www.haskell.org/pipermail/hopengl/2009-October/000929.html) it is suggested that it depends upon OpenGL. So it's just a wrapper or what?
Wow. Suddenly I understand everything!
... and now I realize that I never really understood Christmas tree lights.
great analogy. very tired of "adopted in the enterprise" == success used as a blunt metric everywhere.
Your post made me happy. You are very considerate of the poor unsuspecting young programmers. It's a noble cause; thank you. (no sarcasm!)
If you count the number of computers that ever ran a line of Lua (WoW has 11.4 million subscribers right now), it's probably gonna be a bit higher than it's gonna be with Haskell.
I've always wondered why this behaviour can't be handled by the GC. Surely it should be possible to handle this gracefully.
Upvote for know that a haiku must have a reference to nature.
I don't know why people waste so much time on those blog posts. A monad is just a monoid in the class of endofunctors. What's the problem?
&gt;First rule of the Haskell Cult is, you're too lazy to talk about the Haskell cult. FTFY 
I'll tell you about it, but you have to ask me first!
MATLAB has many libraries for scientific calculations. Too bad MATLAB is a horrible language from syntax to pricetag to documentation.
This paper at this link: http://neilmitchell.blogspot.com/2011/03/experience-report-functional.html - describes some of the libraries that were missing. As a summary, most things from applied statistics :(
So true. My guess is matlab rules because it's got such crufty old school ways of doing things and people are comfortable with that.
No good libraries exist for doing linguistics (let alone unified toolkits like Mathematica).
Yep, matlab's inertia must be huge. Keep in mind that it's also used by the type of crowd that sees programming as purely a tool to get a job done and don't really care for elegance. Some of them still write code in fortran for crying out loud. 
I think the documentation is ok.
I have a teacher who used to write fortran. He now teaches C and Java but disregards both languages. In fact, he said that C has no future and Java is a language for kids.
I thought the Haskell Cabal was a small but secretive group which hid a bitcoin farm in GHC and thus designed "cabal" to break your ghc-pkg tree anytime any package got a minor-minor version bump therefore forcing you to run GHC about 100 times more than you otherwise and farming more bitcoin for them
downvoted for being a helpful, constructive answer.
Take a look at scipy and numpy (for python) for example...
I work with evaluation of experimental data and some simple related physics simulations, mostly using Matlab. I agree that the Matlab has many failings (which I won't list), but I don't agree it's horrible. I don't want to derail this topic with too many specifics, but in my opinion some positive sides are * The syntax is simple in general * The syntax for matrix handling code is very simple and quite powerful, making it easy to slice, transpose, filter, and otherwise perform calculation on matrices. At the same time, the code can be written both short and readable. * The documentation is good. Also, as long as you're not using built-in functions, looking at the implementation of the library functions is just an "edit" away. There seems to exist quite a bit of negative opinions about Matlab, that may be partly deserved, but there are reasons why it has become so widely used. I actually hope to be disputed on the above points (hopefully by mcandre :) ). I would be very happy to find an alternative to Matlab that is enough of an improvement to me that I would want to make a (gradual) transition to.
I guess you mean in Haskell? What I would need are mostly * Good ode solvers that work well with a good matrix/array library (I think there is one in hmatrix, but Matlab has more options) * Good optimization routines accepting vector input, like equivalents to the Levenberg-Marquardt-based lsqnonlin or the simplex-based fminsearch in Matlab Maybe all of the above already exist, and then I guess what I really miss is a good way to find the right libraries in Hackage, and some tutorials how to use them efficiently and correctly.
Scipy would be a good alternative to MATLAB.
You could, but then you need special support in the garbage collector for ByteString. Which could be worth it.
I learned Fortran 77 in school as a physics major. This was as late as 2000. I'm sure it's still taught and I *know* it's still used.
That's because you've never used the Parallel Computing Toolbox.
Fortran 95 is still a great choice for a large-scale numerics project. C is still the only language that comes close in terms of performance, and Fortran 95 has many more features for working with arrays.
Prior art: Erlang had the exact same problem, and what it ended up doing is exposing the information about the binary (local bytestring equivalent) and providing a method to do a true, guaranteed copy, via the [binary module](http://www.erlang.org/doc/man/binary.html), which detaches the copy from the original binary. To use a technical term, that's icky, but I'm not sure what other solution there is. All the simple, global heuristics are wrong.
There are lots of things about scipy's semantics which are not as clean. Matlab arrays slices are semantically separate entities (with copy on write beneath the hood for efficiency). This lets Matlab have a much more functional style (side effects are limited to modifying at a variable, rather than modifying memory pointed to by a variable). Scipy slices are just pointers. I would say I'm at least a two star C programmer, but I still don't want to think about pointers unless I have to. Plus, scipy is stuck with Python syntax, including having to use commas everywhere when a space would suffice and ugly splicing. The scipy libraries aren't as complete nor is the documentation as full or easy to search for and use. I've got a PhD in Physics, most of my research was computational modeling and data analysis, and I used Matlab for it all (with inner loops in C, sometimes) and I _tried_ python, but it was just such a hit in development efficiency that it wasn't worth it.
You _apply_, they _eval_. Same thing with the Scheme cult.
It's very fast for numeric applications (faster than C) and there are many, many libraries written for it. However it's such a pain in the ass that I don't think many people are developing new stuff in it these days.
How about counting how many unreachable bytes are being kept alive by the smaller string reference, and eventually copying if some simple formula (BytesCopied * Constant &lt; BytesWasted * GCCycleCount)? Why would this heuristic not work well?
Do you meet the specifications? Can your prove it? They are non-strictly evaluated. 
Haskell is too fragmented on the basic library things: No standard vector type, hmatrix, vector, array, all use different vector types. Sometimes the only recourse for conversion is through `[a]`! This means that if you want a fast auto-correlate, fft, and other signal processing things (subset of scientific computing), you need to either roll your own, or constantly (and slowly!) translate between vector types. I wish hmatrix and other libraries all converged on a single vector type that we can use.
I would also add that the Matlab IDE is very handy and popular among its users. The accessibility of interactive plots in Matlab stands in stark contrast to the GUI options available to the Haskell programmer. If you want points of dissent against Matlab: * The conventional wisdom that if you have a "for" loop, you're doing it wrong leads to some excessively cryptic vectorized code. * Namespaces, libraries, packages, etc. aren't as helpfully implemented in Matlab (the last time I looked) as they are with Cabal. This leads to some pretty gnarly source code directories with dozens and dozens of files, as well as copy-paste code reuse. * I think Haskell's FFI is easier to use that Matlab's mex files. This depends on what you're doing, of course, but the simple things in Haskell really are simple. * Getting element types wrong is easy and not always obvious. Matlab attempts to implicitly do the right thing, and usually succeeds, but tends to silently leave you floundering if you're not careful. Matlab is head and shoulders above other options when you want to do something to vectors or matrices and see the results. What you want to do is *probably* a combination of some slicing operations (that Matlab's syntax is optimized for) and a function that comes with Matlab. If that description doesn't fit your problem, then you start running into Matlab's pain points (its syntax is decidedly *not* optimized for writing Haskell-like code!).
I'd love to see a DSL for statistical inference in haskell. Haskell seems perfect for it. From what I saw in sigfpe's and Erik Kidd's blogs, a haskell library for bayesian inference could substitute software like WinBUGS and similars with many advantages. Particularly because it would be easy to implement many ways to run the "probability monad" - you can calculate posterior probabilities exactly, use a Gibbs sampler, use a variational method... you just have to implement "interpreters" for a universal monadic computation . 
array comprehension.
This was committed to GHC HEAD, which I think was already 7.2 at that point. 
We'd assumed you'd done that part already.
Thanks.
There's a `copy` function in `Data.ByteString` for this purpose.
They will in time, I'm pretty sure of this.
Proverbs for paranoid space leaks: you hide, they ``seq``.
For future reference, there's no dishonour in just asking in the ticket, or emailing the ghc-cvs mailing list. As sunra says, it'll be included in 7.2.x. The reason is that point releases like 7.0.2 are just for bug fixes, not new features. Patches are usually first applied to the ghc head branch and for sometimes bugfixes are merged or backported to the stable branch.
You need to consider all the substrings to see how much of the parent string is being wasted. I don't think that'd be a trivial to cheap test to include into the GC. That said, I think I have seen papers where this is done, so anyone interested in this should read the GC literature on this first.
timely for me, as I am looking into writing a simple inverted index to be able to run an app on windows phone 7. If you want the heavyweight solution to full text search you can use sphinx: http://hackage.haskell.org/package/sphinx 
My endofunctors always drip chilli out the side and it runs down my hand. Am I doing it wrong?
Sorry, I wasn't being clear; my main point was that they also have every motivation to automatically solve this problem, but also ultimately had to basically punt.
In addition to dcoutts' point, here's one I encountered in real life on my Erlang system. I'm currently actually on the release immediately before they put that binary stuff in, and one of my app's common cases is that I get a megabyte message which I tear into pieces. I actually didn't hit the exact problem we're discussing now, as I did in fact correctly dispose of all the pieces and the original binary was subject to GC. The problem I hit was that GC was only running so often and I could build up several messages of a megabyte or two apiece before GC would kick in for that process and finally actually dispose of them. Multiply this by a few thousand processes doing this some significant fraction of the time, and suddenly you've got a program that can choke quite significant hardware, when the hardware ought to be fully capable of dealing with it. Point being, one of the things you have to think about is how often your GC is running. Any heuristic that ties into GC _at all_ may not run often enough to save you on a server. Of course on average that's a bit of a corner case, but it's definitely a real one. And any attempt to automatically solve _that_ problem will have corner cases of its own, for instance perhaps you end up triggering GC too frequently on some case that turns out to not actually need it, etc. Again, it may be icky and not all that "functional", but if you're really interested in performance you still need some ability to do some degree of manual tuning on the high end. Erlang tried for a very long time to avoid that, the binary stuff was put into Release _14_ (i.e., not "release 1.2", but one very far down its lifecycle), and they're much more directly focused on this stuff than Haskell is, but still in the end there's just no avoiding it. The real world is too perverse and Turing-chaotic to automatically solve this problem. (I really ought to actually define what I mean by Turing-chaotic sometime, but in this case the relevant bit is that the real world is actually out to get you; create an algorithm with any corner case you choose and something out there in the real world _will_ hit it. It isn't your imagination or paranoia.)
Sure, foldl1 min [14, 35, -7, 46, 98] impress me more than minimum [14, 35, -7, 46, 98] and fibonacci onliner is forgotten: fibs = 0 : 1 : zipWith (+) fibs (tail fibs) 
Is it possible to get a "10 Type-Expressive Haskell Programs With No Bugs After The First Compile to Impress Your Friends" page? I think that's the real benefit of Haskell :)
For high performance server code where I really want to maximize my use of the resources, I currently use C :-P
I'd say "heretic!" and loudly revoke your /r/haskell credentials, but I did just admit I use Erlang on my production code, so I guess I can let it slide this time.
So I was hoping that my caveat in the front would make it clear that I'm no Haskell expert. If any idiom was observed it was purely by chance. I appreciate the feedback, and as better suggestions come in I shall paste them into the post. 
I went with Linode and I'm very happy with them. I got the smallest one and paid for two years up front. I used Yesod with a nginx backend and found it very easy to set up and get running, though it was a bit of an adventure because I really didn't know what I was doing!
Run hlint on that code -- to find that `or $ map ...` is `any`.
What impresses my friends is writing one-liner polymorphic functions that use several functionally constrained multi-parameter type classes without specifying them in type signatures, then ':t'ing them in GHCI to reconstruct these constraints pretty close to my intentions. This impresses me as well. 
&gt; Sieve of Eratosthenes Not quite... Just call it a "simple prime number generation"?
I just read http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf over lunch; fascinating stuff. Sieve retracted.
or $ map (flip isInfixOf tweet) wordlist any (flip isInfixOf tweet) wordlist
Maybe run hlint on all your one-liners too, to find any other things to improve? e.g: Use the `any` function...
Have you considered Amazon's EC2 service? My tiny virtualised Ubuntu server is costing me approximately $0.41 per month. (I haven't set up a Haskell web server yet, but you could set up the Haskell platform from an Ubuntu repo and then use cabal to grab the rest of what you need.)
Thanks, added. 
Best advice I've gotten so far, thanks for that.
Be warned that compiling and especially linking with GHC takes huge amounts of memory (for some reason the GNU linker needs extreme amounts...), and rented virtual servers often have hard memory limits. I rent a VPS with 512M memory (for 10 euros/month), and it was almost impossible to build Gitit for example. Replacing the linker under GHC looked impossible when I tried. So I advise at minimum 512M but preferably 1024M ram if you want to play around with Haskell.
You could also do: any (`isInfixOf` tweet) wordlist
you should search web-devel and ask for advice there. You get one free micro instance on EC2. Similar with Heroku, which with their latest release is now an option. Linode is very cheap.
That gets expensive. Better if possible is to use the same OS and 32/64 bit architecture on your dev machine as your host and tranfer the binary. A binary package manager could be helpful here. Otherwise if you use something like EC2 that is easy to spin up a new instance, you would want to spin up a big ram instance, compile there, transfer the binary to the real host, then shut it down.
I like about parser combinators that the parsers correspond to a grammar. Does the change imply that the parser (char 'f' *&gt; char 'o') &lt;|&gt; (char 'f' *&gt; char 'i') now accepts the word "ffi"?
I like these replicateM 3 [False, True] filterM (const [False, True]) [1..3] 
&gt; Better if possible is to use the same OS and 32/64 bit architecture on your dev machine as your host and tranfer the binary Well that was impossible in my case (different OS+arch), though recently I realized that Virtualbox supports 64 bit guests even on 32 bit OS if the hardware is 64 bit. EC2 could work, too. That was the cheapest VPS I found anyway, though I haven't looked thoroughly.
I'm pretty sure that it means that your example now rejects the word "fi". I seriously doubt it accepts "ffi". Edit: After actually trying it, it rejects "fi" and accepts "ffi". eek!!!
The author [said](http://www.serpentine.com/blog/2011/06/03/attoparsec-0-9-a-major-release/#comment-302650) it does accept "ffi", and that this is a bug.
Yeah, I'd read the article before those comments had been posted. And I didn't read the description closely enough, I just assumed Bryan was describing what augustss had in mind, basically changing &lt;|&gt; to be committed choice by default. (which is how Parsec works, IIRC) :-/ So it seems this is more a conceptual bug, not an "accidental" one.
Well, how about this for a big old facepalm moment. Once my error was pointed out, I came up with a [pretty sane fix](https://github.com/bos/attoparsec/commit/c2ec0a4fd956a8629327dbc7bbb924de45efe78e) very quickly. Unfortunately the performance cost of this fix was unacceptable to me, about 10% slower than the original version that backtracked by default. I saved that commit for posterity, since it's important to save badges of one's stupidity. And now we return you to your previous happy semantics :-(
This got me to thinking a little bit about the cabal dependency nightmare we currently have. Maybe an improvement would be to have client packages specify a minimum version, and provider packages supply a maximum version. It still doesn't solve the problem when a newer version is mostly compatible with an older version, and the vast majority of client packages won't have any problems, but a few client packages will. So do you break everybody in an obvious way, or break a few people in a possibly non-obvious way?
Yeah. Fortunately Sebastian caught it right away.
New version: faster and less correct! It's a feature, not a bug. ;-) Note that you should make sure that the two implementations parse the same thing before comparing their performance; you might have to rewrite one of the parsers to match the new semantics. It's hard to make efficient parsers with choice, you can't really avoid backtracking in some form or another. On that note, I think that Parsec unnecessarily mixes two distinct approaches: 1. Choice does *not* distribute over `&gt;&gt;=`. For example, in `(a &lt;|&gt; b) &gt;&gt;= c)` this means that `c` has no influence over which alternative is taken. In other words, as soon as `a` or `b` have completed, there will be no more backtracking. 2. Choice commits to the first alternative that has accepted input. It's obvious that this will eliminate a lot of backtracking. One of these two approaches is enough to cut off a lot of backtracking, though parsec uses both. It's also possible to trade backtracking for breadth-first search; I've described an implementation in my [operational monad tutorial](http://apfelmus.nfshost.com/articles/operational-monad.html#breadth-first). 
Is there any way to notify the Hackage maintainers to deprecate 0.9.0, so that people upgrading don't accidentally get it?
Are you so sure the performance is worse in general? Also, 10% slower for potentially much less memory usage for some grammars seems like a win to me... Another thing is that good error messages are not really possible with backtracking choice - if the first parser fails, you try the next, even if you've gotten far enough in the first parser to know that you should not even be considering the second parser. Some libraries (like Scala's standard lib parser combinators) try to get around this with some sort of commit combinator, but I've found this doesn't work so well because there's really no good way to control the 'scope' of the commit.
I just realized - you could implement a scoped commit combinator. (scope $ (commit p1 &gt;&gt; p2) &lt;|&gt; p3) &lt;|&gt; p4 Assuming a successful parse of `p1`, a failure of `p2` would _not_ backtrack and try `p3`, but would 'throw' out to the enclosing `scope` parser and try `p4`.
I may be wrong, but I don't think you need the state token. Can't you get the semantics you want by manipulating the failure continuation instead?
&gt; Choice does not distribute over &gt;&gt;=. For example, in (a &lt;|&gt; b) &gt;&gt;= c) this means that c has no influence over which alternative is taken. In other words, as soon as a or b have completed, there will be no more backtracking. I guess this is a choice of right-factoring your grammar instead of left-factoring?
it is being parsed as `((++) xs) . tail . (reverse xs)`
Why is reverse being combined with xs? Thanks.
Because function application binds more strongly
So it should be ((++) xs).tail.reverse $ xs ! Thanks a bunch for the prompt help! 
Aside: have you considered xs=[]?
That's cool. We've been working hard for a long time. Good to get recognized.
I'm using Gleesys. They have all their servers in Sweden, but this is fine since my customers are all in Sweden too. I'm paying the equivalent of 48 USD/Month for a virtual server with 1GB memory, 30GB harddisk and 250GB of transfer. Since this system can be scaled dynamically I could run it with a minimum of memory and HD, increase those when I do major compiles/upgrades/development work and then back everything down again to minimize cost.
As many as we want until someone tries to actually look at it.
You'll need to flesh that thought out a bit before I can follow you.
The point you're making in 1. is that ((a &lt;|&gt; b) &gt;&gt;= c) could be equivalent to (a &gt;&gt;= c &lt;|&gt; b &gt;&gt;= c), independent of 2. ? Sorry, sometimes I just need to spell these things out :-)
Sure, I'll try to spell things out. The expressions `((a &lt;|&gt; b) &gt;&gt;= c)` and `(a &gt;&gt;= c &lt;|&gt; b &gt;&gt;= c)` can be equivalent independently of 2, yes. (The equivalence is called "Left Distribution", see the [MonadPlus laws](http://www.haskell.org/haskellwiki/MonadPlus)) However, what I'm saying in 1 is that these expressions are *not* equivalent in Parsec. Instead, you have the law (return a) &lt;|&gt; b = return a My point is that you can replace the former law (Left Distribution) with the latter one (Left Catch), independently of 2. In fact, as soon as you have done so, the need for 2. has been greatly reduced, because you have eliminated a lot of backtracking. EDIT: Actually, I'm not so sure about what Parsec implements anymore, this is only what I remember. 
That's a good way to put it. One could say that 1. Compels you to right-factor your grammar 2. Compels you to left-factor your grammar 
there are a few syntactic options, for example palindromize1 xs = ( (xs++) . tail . reverse ) xs palindromize2 xs = (xs++) $ tail $ reverse xs palindromize3 xs = xs ++ (tail $ reverse xs) 
 palindromize xs = xs ++ tail (reverse xs)
I've had luck with finding cheap VPS services on [Low End Box](http://www.lowendbox.com/) before. It's a site that specializes in collectning cheap VPS offers from different providers. Maybe you'll find something there?
My opinion was always that the maintainer of a package should specify which versions of his package are compatible; the user of a package would just specify the version that works for him.
Brilliant blog template, I can't decipher anything.
Windows 7 user here. I had an earlier version of this and it was great. When I upgraded to 0.3 it suddenly got really really slow. Is it just me? 
I should point out that "house burning down" is not a weird Kansan figure of speech, but a literal description of events.
No, I'm pretty sure your memory of parsec is correct. Once one branch of (&lt;|&gt;) succeeds there is nothing I can see that keeps around the other branch. I don't think it would be a small change to the structure of the library, either, but maybe there is something clever I'm missing.
I had a version on Linux/x86_64, when I upgraded, it goes really fast (like, normal speed), whereas it used to be slow. Apparently this is mostly due to which OpenGL drivers you have.
 palindromize = (id &amp;&amp;&amp; tail.reverse) &gt;&gt;&gt; uncurry (++) palindromize = (++) &lt;$&gt; id &lt;*&gt; tail.reverse
For 49 euro/month (-19% VAT?), you could get a dedicated server with 8 gigs of ram. See http://www.hetzner.de/en/hosting/produkte_rootserver/eq4/
I &lt;3 Solr. I've thought about doing a Haskell interface to it, but I haven't needed to yet. 
&gt; Soon we will start a level contest which is why we gave this release the name "Edward". Um, what? Is this an inside joke that I'm not in on or something? &gt; Level editor tutorials will be released very soon as well! I'm very excited for this. Please post to /r/haskell when this comes out!
Get it because, you know, *Edward*, was that... yeah I have no idea either.
Ah, right, the defining property is indeed the equation I wrote down: when one branch *succeeds*, the other one will disappear. Whether removing Left Catch in favor of Left Distribution is a small change depends on the implementation. :-) It would be quite simple with my [operational package](http://apfelmus.nfshost.com/articles/operational-monad.html).
Maybe because it's a Level **Ed**itor Re**ward** contest? What? Why's everyone looking at me like that?
Snap is presented as fast, Yesod is not. Yet, [Yesod is faster.](http://www.yesodweb.com/blog/2011/1/announcing-warp)
Yeah, it seems to be rather random, actually, as far as what is said about what... Snap and Happstack routing combinators are very similar, but one is described as nice, while the other is described as "a little hairy", for another example. There are definitely better sources of information.
The observation is sensible but the wording and the motivation seem suspect to me: what is modular about not having any kind of contract, specification or guarantee for composing modules? If I'm correct in my suspicion that Bracha's motivation might be to abstract the contract away behind a well-known name (possibly managed in a versioned registry) so that a global name univocally identifies a module, then I don't want to program in that language: I can't see how convention and management can ever be better than syntax and the compiler.
This seems true enough. If types are anti-modular, then how does one get away with separate compilation in the absence of module signatures and whatnot? If you need access to the module's inside to determine that type A in this module is the same type A in that module and so there's no type mismatch, how do we get away with his String example? Obviously, it's because the language *hardwires* some set of primitive types. This points to a tension between extensibility, proofs, and modularity. Is there a sort of triangle between the three properties like Zooko's triangle or CAP? Dunno, but it's an interesting perspective.
It's a simple truth, indeed a tautology, that if you want the compiler to check the consistency of interactions between modules, then the compiler must know the information that it is checking. That's all that's being said here. It really shouldn't surprise anyone. If you dispose of typing, then someone *still* needs to know that relevant information in order to write correct code; but it is the programmer, rather than the compiler. I'm skeptical that the word "modularity" should be so casually redefined to refer to compile-time dependencies; when it has a much more important and longer-standing use to refer to a discipline for separation of concerns in the meaning of the software. Nothing here is about modularity at all; both with and without typing, you need that shared information somewhere: either in a shared brain, or in a shared build artifact (or preferably both...). This is *only* about compile-time dependencies.
If you use ML style modules in a certain way, then you never need to look outside the module you're compiling in order to compile it. This is because any module that needs functionality from another module is instead a functor, which specifies all the interfaces of modules it relies on. Then you can compile every module separately (or "independently", or whatever synonym you want to pretend means something dramatically different) and don't have to check anything between modules until you go to link them together into a single program, at which point you must verify that you're in fact linking together things with the correct interface. If this is still "anti-modular," I don't see why anyone would care whether things are modular or not.
Indeed. Just as I commented on the blog, there are typesystems which enjoy the "principal typings" property which corresponds to his notion of independent compilation. Not to be confused with the weaker "principal types" which is what Hindley-Milner supports. Reading: [The Essence of Principal Typings](http://www.macs.hw.ac.uk/~jbw/papers/Wells:The-Essence-of-Principal-Typings:slightly-longer.pdf) by Joe Wells and [What Are Principal Typings and What Are They Good For?](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.3642) by Trevor Jim. 
Haven't we been down this road before? That smells an awful lot like COM.
Module A : Common Public Types Module B and Module C only export public functions that use types from module A. Internally, they can use whatever the hell they want. Java libs have done this for years. Haskell libs utilize this organizational style as well. So I don't get what the problem is.
Oh, I see, he mentions 'seperate' vs 'independent' compilation. OK, so he is talking about totally independent compilation, with no compile time dependencies what so ever. Hah, good luck. 
Types are anti-modularity in the same sense stop signs are anti-transportation. Edit: Fixed abysmal spelling.
I should also mention, writing ML in this way is actually more "modular" than almost any other so-called "module system" allows. Because, you see, in languages like Python and ... you are required in any given module to specify _by name_ which modules you depend on. By contrast, an ML functor is more or less a function from _any_ module of an appropriate signature/interface. So we can compile a functor once and link it to as many different implementations with as many different names as we wish, instead of having to change the module when we want to link to `Data.Set` instead of `Data.IntSet`. Of course, we still have a non-modular component which specifies how all the modules should be stuck together in the actual program. I guess modularity is very hard to achieve. :)
Thanks for the feedback,all! My friend Aidan is just learning Haskell, and I'll be sure to pass it along to him
I think this style is what Bracha advocates. If understood him correctly in an interview he gave on FLOSS Weekly, his Newspeak language forces you to program in (the dynamically typed equivalent of) the style you describe: each module takes as parameters all modules which it depends on. He would point out, I think, that ML doesn't *force* you to adopt this style of writing modules...
Whatever transportion is. :D
Sorry, type error.
Mircosoft is doing more than just working on parallel C++, they've hired Simon Peyton Jones, who one of the main contributers to Haskell's design. 
Frag doesn’t really do much in terms of the graphics pipeline, it’s essentially just a BSP level viewer which requires hardly more than pushing plain textured triangles. GPipe depends on the OpenGL package only for technical reasons (it was easiest to implement the proof of concept on top of the existing bindings), but you don’t actually have to use any of the OpenGL API. It provides a different abstraction that’s not necessarily OpenGL specific, just uses it as a backend in this particular implementation. A different kind of alternative, which I’m involved in, is [LambdaCube](http://code.google.com/p/lambdacube/), which is an actual 3D engine that gives you scene graph and resource management (again, no need to make any OpenGL call, it’s not even OpenGL specific), but it is still in its infancy in many ways.
That ML doesn't force you to program this way has nothing to do with types, though. So in what sense are types anti-modular? Of course, on the other hand, the reason ML doesn't force you to program in this way is because sometimes this rather extreme notion of "modular" isn't valuable. Sometimes I just want to use some particular other module, for whatever reason. And I don't care that it can't be seamlessly switched out with some other module that purports to implement the same thing. So I can just do that instead of adding a useless layer of indirection.
Are you German?
Interesting post. Thanks for sharing it. In Gilad's example, a String breaks modularity since both modules need to know about the String type. However, what good is modularity if modules can't interact with each other with some sort of protocol? And in what way should such a protocol be implemented? Surely one expects to do this using common types. It all seems like complaining that ethernet cables prevent distributed computing because all the systems need to be plugged into each other so they're not really that distributed.
&gt; It all seems like complaining that ethernet cables prevent distributed computing because all the systems need to be plugged into each other so they're not really that distributed. Word. I'd like to see the author's definition of modularity....
Yes, the word "modularity" doesn't seem sufficiently defined.
CmdArgs is fantastic and magical, but System.Console.GetOpt was what I ported to f# when I needed something light-weight.
he basically advocates compilation scheme, where you can compile everything independent of other things, but since the compiler doesn't know anything, it can't really do any compilation work (no optimizations), so the real compilation stuff is left until runtime.
&gt; palindromize = (id &amp;&amp;&amp; tail.reverse) &gt;&gt;&gt; uncurry (++) The disadvantage of this one is that your successor will hunt you down and stab you.
 fileLines &lt;- lines &lt;$&gt; readFile "data.txt"
I actually find it quite readable. Of course, such abstraction is overkill here.
if you like the style of this DSL (do blocks with descriptions) you might like [hspec](https://github.com/trystan/hspec)
&gt; ...at which point you must verify that you're in fact linking together things with the correct interface. And there's the rub. You can raise independence to such a level that you never mention the links at all, but at some point *something* is going to need to know that the **f** over here is the same as the **f** over there. Can you create a module that is so modular that it never needs to know about any other modules? In what sense would the result be modular?
The page will get stuck when scrolling to the bottom on an iPad. Just so you know.
Just because I couldn't help myself... http://cdsmith.wordpress.com/2011/06/06/mazes-in-haskell-my-version/ In case you're interested.
Snap, Yesod, Happstack guys? Any thoughts?
I would really love to hear any info about scion with emacs. Currently emacs haskell-mode does everything i need except code-completion and library code-navigation. Are those things possible with scion ? 
Not yet.
I think [silkapp](http://silkapp.com) is aiming to be. I know that [hachicode](http://hachicode.com) is hours/days from launching its beta and is also aiming to be a very large, broad consumer tool. hachicode is on yesod which we are very happy with. IIRC silk is on snap and i'm very keen to hear their thoughts. *disclaimer:* I am the founder of hachicode.
&gt; Can you create a module that is so modular that it never needs to know about any other modules? You can parameterize an ML module by every type and function it doesn't define itself. It wouldn't have to refer even to any specific built-in types. And it wouldn't need to know about any other modules, it would just have (most likely) a pretty lengthy type signature. This would mean that some larger organizational structure in the program as a whole could fill in the blanks however it wants. Or even fill them in in multiple different ways. Which I suppose is pretty modular. However, I can't really imagine a situation in which it'd be important for most of the program to be parameterized by, say, the implementation of integers. And I don't really believe that it's always valuable to parameterize yourself on even more complex types.
Also in Firefox. I guess it is browser's problem or the fact that the article is very long.
Wow. Thanks for sharing :)
**tl;dr**: here's the part where they mention Haskell: &gt; Meanwhile, a group of obscure programming languages used in academia seems to be making slow but steady progress, crunching large amounts of data in industrial applications and behind the scenes at large websites. Two examples are Erlang and Haskell, both of which are “functional programming” languages. &gt; &gt; Such languages are based on a highly mathematical programming style (based on the evaluation of functions) that is very different from traditional, “imperative” languages (based on a series of commands). This puts many programmers off. But functional languages turn out to be very well suited to parallel programming. Erlang was originally developed by Ericsson for use in telecoms equipment, and the language has since been adopted elsewhere: it powers Facebook’s chat feature, for example. Another novel language is Scala, which aims to combine the best of both functional and traditional languages. It is used to run the Twitter, LinkedIn and Foursquare websites, among others. 
Honestly, I don't think the chosen framework weighs in much when it comes to how successful your webapp will be. That being said, some frameworks might provide more rapid development for certain kinds of webapps.
&gt; I don't know any way to make javac warn about all possible places where NPE could happen. Warning: Potential null pointer at line 15. Warning: Potential null pointer at line 16. Warning: Potential null pointer at line 17. Warning: Potential null pointer at line 18. No but seriously, it actually does warn you in some places, but it's nowhere *near* all of them. 
How familiar are you with Haskell? In Haskell if you have a variable of type Char, it's *actually a Char*. If you want something that's "either a Char or null" then you have to use a type that expresses that, such as "Maybe Char". Now that I'm somewhat familiar with Haskell it sounds so obvious, but in Java almost all variables are not of their declared type, they're of the type "&lt;declared type&gt; or null" which is kind of crazy.
&gt; If not having to provide type information for parameters cuts down development time by removing places you can make mistakes, then not having to provide names for the parameters should be even more effective. There's a Haskell style that encourages doing just that. Instead of defining a function with parameters that returns the result of the calculations with those parameters, you define a function without parameters that returns a function that does the calculations when passed those parameters. They sort of lost me here. Are they talking about point-free form?
Successful applications can give you an idea whether the given technology is mature enough. Should you use Ruby on Rails? Twitter and GitHub use it, so you should be able to implement something of similar complexity with it.
Interesting. Do you plan to share your experiences with using Haskell/Yesod?
&gt;# NICE-TO-HAVES (but not necessary): &gt; * Interest in/experience with Haskell. Does that mean that they would train you in Haskell then? Hm, I've never seen a Fall/Spring internship before. What's the difference between this and a co-op?
Your link doesn't seem to be working. It's timing out.
thanks for the heads up but they're both working for me now. 
absolutely. I've already given a couple talks at HackerspaceSG (singapore). i intend to film one and put it up.
It successfully redirects, loads a while, a page with a brown background appears momentarily (not long enough to read it), then goes blank and continues to load forever (waiting for ssl.google-analytics.com -- so it might be their problem). I'm using Firefox 3.6.17 on Ubuntu Maverick; I'm behind my university's firewall and access the web through a proxy.
weird. again, thanks for the heads up. https://secure.hachicode.com wont have any redirects taht might be mucking it up.
We (Silk) currently use both Happstack and Snap for different servers on our backend. The web facing server is still Happstack. Our experiences with both have been good, though as we are currently still in private beta, we haven't had to handle very large loads yet. Note that our public website has a shallow ruby frontend which talks to the Haskell backend. The actual web app talks directly (well, through Apache) to Haskell.
Perhaps... but when you have as much money as Twitter, you can afford to implement pretty much anything in pretty much any viable language. And in practice, technology choices are often a product of someone's personal preference rather than quality of the technology. I can't recall how many times I've learned that some high-volume web application or other is written in PHP just because they never considered much else. So looking for something of that scale can provide less information than you might think. All of the major Haskell frameworks (by which I mean at least Happstack, Snap, and Yesod) are viable, in the sense above. Beyond that, be skeptical how much you learn from what a couple people's high traffic web sites use.
de-SSLed our splash page. should work. not much to see though.
yeah, but its getting to the stage where you have as much money as Twitter thats dicey.
&gt; This would mean that some larger organizational structure in the program as a whole could fill in the blanks however it wants. But, but but...! That larger structure would have to *know about the blanks* to fill-in! It **wouldn't be modular**! (At least according to my understanding of Bracha's comments.)
I was mainly joking, but I doubt that even someone very familiar with arrow idioms would be able to divine the purpose of that function as quickly as with e.g. SciK's simple version: ```xs ++ tail (reverse xs)```. Even aside from the extra indirection and implied semantic overhead of the arrow version, SciK's version involves three fewer names (no ```uncurry, id, &amp;&amp;&amp;, &gt;&gt;&gt;```), and its structure reflects the structure of the output.
Sure. There has to be some description somewhere of how the separate pieces of the program are assembled into the program. And that description is necessarily non-modular. I assume he'd stop short of calling that non-modular. One could be of the opinion that there's value in that central description being the _only_ piece that knows how everything fits together; and that anything less is in some sense non-modular. But I don't really believe that it's always valuable to program that way, and I don't think that that's how most people understand the word "modular," either.
The informal proceedings http://www.pps.jussieu.fr/%7Esaurin/tpdc2011/TPDC-2011-informal-proceedings.pdf is a nice followup...
I tend to view arrow chains as a sort of tree structure; my visualisation of the palindromize definition is (++) with two branches connected to id and (tail . reverse), which I know are applied to the single argument; so actually I can grok it quickly. `uncurry (++) &lt;&lt;&lt; (id &amp;&amp;&amp; tail.reverse)` would be preferable, though. I agree that it's probably a net loss for a function this simple, though.
Yeah, it does. It loads the page then redirects to a blank page.
I tend to view arrow chains as a sort of tree structure; my visualisation of the palindromize definition is (++) with two branches connected to id and (tail . reverse), which I know are applied to the single argument; so actually I can grok it quickly. `uncurry (++) &lt;&lt;&lt; (id &amp;&amp;&amp; tail.reverse)` would be preferable, though. I agree that it's probably a net loss for a function this simple, though.
Just in time, too. Much thanks, once again.
Congratulations!
Couldn't have happened to a nicer pair of Simons!
I noticed they are donating the award to haskell.org. How does one actually make a donation to haskell.org? 
I will offer them a beer (each!) tomorrow.
I can't help admire they passion and dedication. Well deserved.
Uhm... /r/haskell?
sorry, my bad :-P
:D Simple mistake.
It is still a problem right now. The haskell.org committee is working to join the SFC (which will make possible for haskell.org to receive donations, at least from the US). Galois has some money (mostly from previous GSoC) they would like to transfer too, and this year GSoC money will arrive too.
Can someone point to a link that explains the calculus thing Harper is using in his blogs?
Sensible advice. People are far too keen to hit things with the INLINE hammer without thinking.
GetOpt is a page of code, CmdArgs is substantially more than that. When I needed an argument processor for a functional language I use, I ported the Explicit part of CmdArgs, then wrote my own little sugar version on top of that.
I didn't mean a slight on CmdArgs - this was just a symptom of my laziness as CmdArgs was truly better for what I was doing :-)
A bit off-topic, but: "tailSafe" always struck me as strange in a not-entirely-good-way. It's a little bit like defining x/0 = 0. Sure it is not partial anymore, but I'm not sure if I'm in any better a position: my program is still wrong, but now it doesn't crash so it may take me longer to figure that out.
C has the same problem with pragmas and the inline keyword.
have you read this http://www.cs.ru.nl/~wouters/Publications/BeautyInTheBeast.pdf ? you might have problems if you use plain IO in your code. it's obvious, that your monad must be actually a monad transformer over some user monad (IO or dummy IO for testing usually), but the specific parts of your code (threads related?) would be easier to test if you parametrized your code over yetanothermonad that only provides needed methods (it would be obviously implemented by IO, but could be also mocked for tests). as a bonus your type sigs would clearly state what part of IO they do ("we start threads, but we don't eat cake")
If you don't know Haskell we would do our best to put you on non-Haskell parts. I'm not sure what a co-op is. If it's a program with your school where you get credits then go ahead and try but it's not part of the internship we're offering.
I grepped through my cabal source-package cache, and sorted by the occurence-count of the `{-# INLINE` pattern... the following ones were the top 15, with 'vector' being the clear winner... 102 pretty-show-1.1.1.tar.gz 103 statistics-0.8.0.5.tar.gz 111 aeson-0.3.2.1.tar.gz 114 attoparsec-0.8.6.1.tar.gz 114 regex-tdfa-1.1.8.tar.gz 124 aeson-0.3.2.4.tar.gz 221 bytestring-0.9.1.8.tar.gz 243 blaze-builder-0.3.0.1.tar.gz 259 gtk2hs-buildtools-0.11.2.tar.gz 278 language-c-0.3.2.1.tar.gz 368 haskell-src-exts-0.3.12.tar.gz 411 text-0.11.0.8.tar.gz 727 haskell-src-exts-1.10.1.tar.gz 887 blaze-html-0.4.1.0.tar.gz 1519 vector-0.7.0.1.tar.gz 
tailSafe = drop 1, and I see 64 occurrences of "drop 1" in the code base I'm working on, so it seems to be something people need to do reasonably often. I tend to use tailDef and tailNote for things like tail, and for the more complex functions things like readMay are useful.
haskell-src-exts includes a Happy generated parser, and processes INLINE pragmas, so has the text in more times than it has INLINE pragmas. The real count on the latest version is 5. 
so that one is excused... what about the other ones? :-)
It's an interesting idea -- I wouldn't want to expose that parameterisation over the inner monad to the user (since it's always IO underneath, it would just clutter up the API for no benefit to them), but for testing that might be interesting. I use IO for forkIO, atomically (and thus all of STM), MVars, and catch/finally blocks, but I can get away without some of that if I don't need parallel composition in my tests (often I don't). Emulating the catch/finally blocks to catch exceptions from pure code in a mocked-up reimplementation of IO would not be possible though, I think? 
btw, have you thought about providing similar "safe" functions for other container types such as `Data.Vector.Vector`?
&gt; I wouldn't want to expose that parameterisation over the inner monad to the user you don't have to. create a class MyMonad m, that has methods for "forkIO, atomically (and thus all of STM), MVars, and catch/finally blocks", write your library using that, instantiate that for IO, and export functions constrained to IO (probably leaving unconstrained versions in some private module to be able to test it). &gt; I can get away without some of that if I don't need parallel composition in my tests I don't know what your library does, maybe it's thread-safe by desing (because of STM?), but they say, that parallel bugs are the worst and now you'd be able to easily test for them - you could create instances for MyMonad that contain various IO managers, ones that would really stress your code and assumptions. nice thing about this approach is, that if you find a bug that only happens on fridays with a prime number of threads, and after long time of hard work you finally manage to fix it, you can create a test case that will exhibit this bug to save yourself from it again in the future, wheras the rest of the world, even with the policy to add a regression test for every bug, they can't do that for this type of bugs. &gt; Emulating the catch/finally blocks to catch exceptions from pure code in a mocked-up reimplementation of IO would not be possible though, I think? why not? there are functions (one is called evaluate I think, though I can't remember where it comes from) that force the calculation of pure values (to [wh]nf?) and catch even undefined exceptions (sure, they rely on unsafePerformIO, but it's ok for tests). the only hard part (but also probably solvable) would be the case if those exceptions rely on laziness and could be thrown at some other place, then forcing would change the semantics.
I don't agree with this position. While arbitrary use of `INLINE` or bang patterns is perhaps ill-advised, if you know a function should be inlined (for example, to encourage a rewrite to fire) then relying on the current state of the inliner's heuristics seems fragile. 
As one of the authors, I can tell you that the `bytestring` package has far more `INLINE` pragmas than are necessary (but it'll be quite a bit of work to find out which ones). I blame Don, he used to love the `INLINE` hammer :-) (though to be fair to Don, ghc's inline heuristics have changed).
'drop1' would be a better name then :) It's not providing a "safer" version of tail, it's providing something different than tail. 
I read every line of Core. But it was 2007.
If I'm not mistaken, this could also benefit those of us on single-processor machines just as well, since downloading one thing and compiling another can interleave.
Does [this](http://en.wikipedia.org/wiki/Sequent_calculus) help? Or did you mean something more specific?
It almost goes without saying, but--congratulations, and thanks for everything! Us regular users appreciate it too, even if we don't have shiny awards to hand out. :)
You do know that was an April fools joke right?
Was it already posted? I just saw it today and thought it was hilarious.
http://contemplatecode.blogspot.com/search/label/HWN
This is a strange link. The title suggest that this might be interesting. But there is no apparent documentation, or even a brief explanation of what this is and what it is supposed to do.
I don't disagree with INLINE pragmas, but if you know something the compiler doesn't (you have a rewrite rule, this function composes particularly well with the one it's nearly always used with, you went through the Core by hand, you profiled) then INLINE pragmas are a good idea.
Yes, but it fits a canonical pattern. What would initSafe be called? Perhaps the "Safe" suffix is not quite right, but I couldn't think of anything better - it's really tailDef_ButIWillFindAnAppropriateDefaultForYou
No, but they should probably go in a separate package, or even in the vector package - but I can imagine they would be handy.
Just be aware though - they will likely donate the entirety of the beers to haskell.org.
if psql-simple and mysql-simple are that similar, can't they be unified in such a way, that the decision whether to connect to mysql or psql can be deferred until runtime? For instance, both backends being instances of a common API typeclass?
`vector` and `text` both use fusion, which relies on careful use of `INLINE` pragmas to make rewrite rules fire. I believe CPS style parser libraries like `attoparsec` need more `INLINE` pragmas to make sure that GHC manages to compile away the CPS code to straight-line code. With http://hackage.haskell.org/trac/ghc/ticket/4978 fixed we could perhaps get away with fewer pragmas.
Really, I think this is the best Web-app written in Haskell comparing the quality of code. The only feature I wish to see in Chris Done's code are type-safe URL's. We can build some kind of Framework using patterns from amelie project by implementing scaffold generating utility: $ amelie project SomeNewProject creating directory SomeNewProject creating directory SomeNewProject/src creating directory SomeNewProject/src/SomeNewProject creating file SomeNewProject/src/Main.hs creating file SomeNewProject/SomeNewProject.cabal creating file SomeNewProject/Setup.hs $ cd SomeNewProject $ amelie view NewView creating directory SomeNewProject/src/Views creating file SomeNewProject/src/Views/NewView.hs 
There seems to be a typo here: the issue #185 starts with: "Welcome to issue 180 of the HWN, [...]".
or you could use Yesod for scaffolding and type-safe URL's :). Also Persistent for accessing the database without string manipulation. We may switch persistent's postgresql backend to use pgsql-simple. Thanks! 
Great post. Is the model separation really doing anything here? In Yesod we keep state in one place accessible by the controller, and you end up doing database queries with a "runDB" convenience function- the code ends up looking similar, but just with "runDB" instead of "model". I would really like to see the model concept expanded further in meaningful ways.
I'm sure that you could provide some pointers right now. They might not be as cool as you wish they were, but that still is a lot better than what we have now, which is *nothing*. Say, I want to get a feel of all the type goodness of Haskell, where do I start?
Out of curiousity, does pgsql-simple support binary parameter encodings and/or prepared statements? I glanced at mysql-simple, and it _appeared_ as though it dynamically generated all the sql to support parameters. I don't know if mysql supports parameters at the protocol level like mysql does.
Didn't look like it, although I only skimmed the source. It makes me sad how weak the Haskell database libraries are for high performance sophisticated applications. Right now the main choices are: * hdbc - Impossible to call a stored procedure when using MySQL as far as I could determine. This is because hdbc doesn't connect to the database with multiple result sets turned on, so stored procedures can't return a result. In addition to this, even if you hack the code to turn that one, hdbc insists on always preparing statements and MySQL (except for the bleeding edge version) doesn't support preparing a CALL. There is no way to just pass a raw unprepared statement through. * hsql - Pretty old, no prepared statement support. * mysql-simple - No prepared statement support. Tied to a single database type. * pgsql-simple - Seems the same as mysql-simple. 
This is actually probably my smallest concern, what I mainly care about is performance and being able to do whatever I need to do without some limitation of the database library throwing up a wall. I can only speak for myself, but nothing affects my perception of a library more negatively (aside from it just plain not working as advertised) than when I find I can't do something I should be able to just because of an arbitrary limitation. If I have to write some extra code to deal with converting types or whatever, I can live with it. 
Actually, I don't know about HDBC-mysql, but HDBC-postgresql doesn't actually prepare statements. The external API looks like it does, but it doesn't actually happen. I would imagine HDBC-mysql is the same. The only thing that using the prepare command in HDBC-postgresql does is that HDBC-postgresql goes through and substitutes occurences of `?` with `$1`, `$2`, etc. A "prepared" statement saves this work, but doesn't actually have anything to do with postgresql prepared statements.
It actually does. the Connection interface supplies runRaw which doesn't return results (Returns IO ∅) and run which calls doRun: doRun mysql__ query params = do stmt &lt;- newStatement mysql__ query rv &lt;- Types.execute stmt params newStatement calls [mysql_stmt_init](http://dev.mysql.com/doc/refman/5.0/en/mysql-stmt-init.html) which creates a prepared statement. I wasn't able to figure out any way to call a stored procedure and get a result (perhaps runRaw would work if no result was required or returned) even when I modified hdbc-mysql to connect with CLIENT_MULTI_STATEMENTS turned on. I haven't used the HDBC-postgresql module as much, but I'm kind of disappointed to hear it doesn't actually support preparing the statements on the server side.
Picky! Picky! Picky!
What do you mean by "tied to a single database type"? It took me maybe two days to write mysql-simple. Adding support for prepared statements would probably take about the same amount of time, because the MySQL prepared statement API is big and ugly. Patches welcome :-)
I think we'll probably find a way to share some of the code so there's less duplication. As Chris says, I think there'll remain quite a few ways in which the two will not be API-compatible, though.
What is the SFC? Also a donate button on the hope page might help. 
I meant database server type (ie, mysql, postgres). This isn't really a major concern for me, mainly convenience. It bothers me far more when I just simply can't do what I want to rather than it being slightly inconvenient. I'm motivated enough to whine about this, but probably not enough to actually do anything about it. :)
&gt; String 5N words In general, what's the memory footprint of [a]? Also, I looked up RWH and also tried some googling but, surprisingly, I couldn't find the size of basic types. What is Char's? 
The Software Freedom Conservancy, see their [homepage](http://sfconservancy.org/).
I definitely agree with you- you don't have to convince me on the benefits of separation of model code. Although I think it can be simpler to put a read query in a controller. Once that query (or a part of it) is needed elsewhere it should definitely be named and reused. Updates do need to maintain invariants and thus should stay out of the controller. I would like to come up with a good system for maintaining invariants and propagating violations to forms, and other useful systems for taking more than just the query out of the controller.
Yesod has a separate file in which the schema is declared. You can add one line to the scaffolding tool to generate a Models directory. Persistent is in a good enough state that some users are able to use it for every single query in their application. At this point there are more requests for other capabilities in Yesod, so time is spent there instead of adding more capabilities to Persistent. There is actually some limited join support to perform SQL joins or application joins, but no 'OR' support or support for selecting just some of the fields. You may also be interested in haskellDB or DSH, although that will tie you down to SQL, whereas with Persistent you can switch to MongoDB.
http://stackoverflow.com/questions/3254758/memory-footprint-of-haskell-data-types
&gt; It's not providing a "safer" version of tail, That depends on what you think of the tail function as doing. Is it projecting the second component of a cons cell, or is it shortening a list, or something else? You run into the same issue with subtraction on natural numbers when partiality is handled by defining `x-y` to be 0 if `y&gt;x` (aka the "monus" operation). Is monus providing the same kind of thing as minus, or is it something different? That all depends on what it is you think minus is up to.
A Char is defined as a Unicode code point. In general, this can be stored in 32-bits (cf., UTF-32), though we can often get by with less on average (cf., UTF-8, UTF-16). IIRC, GHC uses a 32-bit word to represent them so that it doesn't have to deal with variable length nonsense; also because the machine word length is 32-bits or greater.
Language overload!
I remember hearing that iOS doesn't allow interpreted code to run on their devices, only compiled code. Does this get around that somehow? Or is it not in the app store?
Haskell code is not interpreted...
I was referring to this: &gt; a lisp interpreter
Oh, I see. It was my understanding that Apple did not want to ship apps that were anything but compiled code; I was not aware that apps the function of which was to interpret code might also be banned.
They've loosened those restrictions sometime last year. Today they only disallow downloading code and running it.
bingo!
Thanks, that's what I was looking for!
How about [Takusen](http://hackage.haskell.org/packages/archive/Takusen/0.8.7/doc/html/Database-Enumerator.html)?
That's true, it depends very much on the perception of what tail is doing. In both cases, "tailSafe" and "monus" end up removing a precondition by mapping that case back into an element of the range (0 or []). However, since we have tail from the Prelude, we have a very concrete definition of what tail is doing, so we can follow that :). 
Johan's article does not mention that with String, you may not have the entire value constructed at once, whereas with ByteString and Text you do. So the N is different in each case. For BS and Text, N is the complete text, but for String, N is the length of the subsequence that is necessary for the current computation.
That's always been the restriction
The github project is just the open source. I do not think Apple would approve of this being distributed through their app store.
Let's not forget lazy ByteString and Text too, which are somewhat in-between.
Where did the iPhone-haskell come from? I was under the impression there wasn't really a way to compile haskell for arm processors 
It's a patched version of ghc (6.10.4 only) that can cross compile to the iphone. I got the demo app running and it uses a small set of Objective C code to call the compiled haskell code. the app builds in Xcode. Scary for production, cool to check out.
I didn't want to believe 5N words for string, with all the supposed pointer tagging optimization and stuff, so I made a fast experiment. And the result looks rather strange. I forced the evaluation of a large string, and looked at the memory residence as reported by the OS. For N=10 million, the residence was about 200 megabytes, which indeed points to 20 bytes = 5 words per character. However, for N=50 million, the residence is only 800 megabytes, which means 16 bytes = 4 words per character! So, can anybody explain what's happening here? (I tried this both on Windows 7 and Mac OS X Leopard, with similar results) For reference, the code is here: import Data.Char n = 1024*1024*50 s = [ chr (mod j 256) | j&lt;-[1..n] ] force :: String -&gt; () force (x:xs) = x `seq` force xs force [] = () main = force s `seq` do print "press enter" getLine print (length s) 
Swapping?
I'm not sure about this but I think that (1) you're right in the sense that has always been what's written in the AppStore agreement, but (2) it used to be the case that App Store screeners overzealously extended it to cover any non-(Apple JavaScript) interpreters in apps, even without having an option to download code.
As far as I can see, the only thing that the sequent calculus has in common with the notation used by Harper is that both use horizontal lines to write down rules…
Well I tried this on two wildly different machines with different operating systems, giving very similar results, so I find it a bit unlikely, though still possible.
nice! I really need insight how haskellers tackle space leaks in real world examples...
Bob is giving proof terms in natural deduction style. If you went on to restrict yourself to writing terms in let-normal form, then those would be proof terms for the sequent calculus. 
Okay. I just played with this, and I have a different theory now. I modified the program to read in a number and multiply by 1024*1024. Other than that it's pretty similar. Here are some numbers: 5: 93,212K 10: 185,556K 15: 370,240K 20: 370,236K 25: 370,236K 50: 738,572K 55: 738,580K 60: 1,476,292K 110: 1,476,292K This is all on Windows 2008 Server. Anyhow, it appears that resident size is unrelated to the exact memory cost of the list, probably because GHC is requesting increasingly large chunks of memory. So the resident size at 50 being only 4x the resident size at 10 doesn't mean that the size-50 list takes only 4x the memory of the size-10 list.
Is the full article available online somewhere?
Yes, at the JFP. The only catch is you have to subscribe :-)
Thanks! I really enjoy your blog posts.
It seems you can fall back to raw sql for the tricky queries: http://www.yesodweb.com/show/topic/352
I've looked at it (I forgot about it when I made my previous post) but I haven't actually used it. At the company I work, we've historically used MySQL. Fairly recently we've been migrating stuff over to Postgres. I may have to give it a shot at some point, although I'd have more motivation if it actually supported MySQL. 
&gt; The code described in this paper can be downloaded from the JFP web site, along with the actual plans used at Thuxton. I had a look but can't see it anywhere. =/ 
Ah, of course. However, shouldn't the memory residency be an upper bound? Which would mean something less than 5N. Also, [Simon Marlow's hack](http://ghcmutterings.wordpress.com/2009/02/12/53/) reports 1 word for [] and 3 words for (the head of) a string, which would mean 3N words (which is indeed the possible minimum with clever pointer tagging hacks, accoding to my foggy morning mind) EDIT: Furthermore, I tried +RTS -s, which reports similarly unreliable numbers. However, heap profiling with -hc (and adding an SCC to the string) is roughly linear, and clearly indicates 3N words (for smaller strings, it reports somewhat less than 3N*sizeof(word) allocation, probably because of the finite time resolution of the profiler) EDIT2: It seems that as Johan writes, there is a small table of shared Chars in play. So for disjoint unicode characters, it is 5N, but for ascii strings, it's more like 3N. Though I cannot seem to understand why there is a need for *two* extra words...
If you're stuck with MySQL, I guess that makes sense. Personally, everything that attracts me to Haskell makes MySQL a non-option.
Looks quite pricey :-( ...why does subscription have to cost that much? where does the money go to?
You have to be very careful when thinking about size of containers because of sharing of the elements in the container. So, e.g., if you have a Data.Set of elements that also have to exist even without the set then the extra cost of the set is 5N. And as other have pointed out, because of laziness the memory usage can be spread over time so that at no point it actually takes up the full amount. 
$45.00 / £30.00 &amp;#3232;\_&amp;#3232; 
http://www.scribd.com/doc/57589897/StratfordTrackLayout
work is underway to port to ghc 7 :)
Very good work. This approach to methodical design and evaluation of data structures, with serious tools for measurement and benchmarking, gives a lot of confidence in the results.
That's concurrency, not parallelism, if I understand the terminology correctly.
&gt; most bugs in GHC are quite tricky devils to fix This is a discouraging sentence...
I feel that, even though persistent data structures are extremely useful for many things, and should be the preferred way to do things in Haskell, we need more work like this to implement high performance, "industrial grade" mutable structures like this. Hashtables are probably the best example, but I'm sure there's plenty of other structures and algorithms that we're missing, and I'd love to see more of them. There are just some situations where they cannot be beaten, and we're hurting ourselves by not having good implementations available for those times when they're needed.
Is there a list somewhere of what are the algorithms and data structures that are missing and needed ? 
Not that I know of, but I'm sure a quick look through any algorithms book will turn up some. Some that come to mind are sorting algorithms, it would be nice if we had a nice library that offered merge sort, quick sort, radix sort. It would be nice to have in place versions of these if possible too, as well as some of the hybrid sorting algorithms like timsort.
We have merge sort here: http://hackage.haskell.org/packages/archive/vector-algorithms/0.4/doc/html/Data-Vector-Algorithms-Merge.html.
Why should persistent containers ever be preferred to mutable ones? Are we really doing anyone any favors by advocating the likes of Data.Map? It just strikes me as odd that the author tries so hard to explain why writing a fast mutable hashtable doesn't make him an awful person. Haskellers shouldn't have to apologize for needing performance.
&gt; Why should persistent containers ever be preferred to mutable ones? Are we really doing anyone any favors by advocating the likes of Data.Map? I suspect you're trolling but I'll answer you anyways. Persistent data structures have the nice property that if you modify them, the old version is still around. They also don't require locking to work in a concurrent context. &gt; It just strikes me as odd that the author tries so hard to explain why writing a fast mutable hashtable doesn't make him an awful person. I disagree. I don't recall apologizing for needing a fast mutable hashtable; I said that although we usually prefer persistent data structures in our programs, there are times when a mutable hash table is really what you need. No apologetics whatsoever.
In addition to what how_gauche said, persistent data structures are nice because they are well-behaved from a denotational view. They don't lead to aliasing problems which lead to tricky bugs. They also allow adhering to the convention that a function's effect is *only* to compute a return value. This means that the types are far better documentation. It also means that the compiler's type-checking is more useful. Consider: calculate :: MutableMap Foo Bar -&gt; IO () calculate = do ... ... oops, forgot to update the structure, no type error! Vs: calculate :: Map Foo Bar -&gt; Map Foo Bar calculate = ... oops, forgot to return the structure, type error and no bug!
You can't use pure functions to operate on mutable data structures, that's why.
I'm sorry if that's a question beneath asking. My point, which I hope I'm restating mildly enough as to not cause offense, is that it may be problematic to focus on certain properties of a system, to the detriment of others. Time and space complexity are the most generalized properties for which you can optimize. Independent of other factors, faster and smaller is always better. The benefits of purely functional associative containers, though, are far more specialized: you either could or couldn't alter the container in a stateful way, and you either do or don't make use of multiple versions of the same container. In most other realms, the typical pattern is to suggest the use of fast and generalized solutions, unless the specific properties of slower algorithms are required. The consensus here seems to be the reverse. That's confusing.
vector-algorithms has merge sort, quick sort and radix sort. And I've been planning on adding timsort for some time, although haven't gotten around to it. It also has heap sort, and some functions for working with an array as a heap, although looking, I could probably expose more in that arena. It's got partial sorts and selection algorithms where appropriate, too. And some searching procedures. Edit: Maybe I can get around to some of this stuff at hac-phi.
"Fast über alles" is not really what we do in Haskell-land. Haskell people tend to think about "correct" first; the fact that we can also get in the same ballpark as "fast" is something which is a testament to the decades of effort people like Simon Marlow and Simon Peyton-Jones have put into the GHC compiler. One of the core postulates of functional programming in Haskell is that stateful mutation is one of the biggest sources of errors in programming. Pure functions and data structures, as Haskell uses them, have the amazing property that calling a pure function with the same arguments will *always* result in the same output. The big problem with stateful mutation is that you get into a situation where you break the locality of the semantics of your program. What I mean by "locality" is this: in pure Haskell-land, you can usually look at the type-level interface of a function and understand its semantics in isolation from other parts of the program. With stateful mutation, any given function you call could have any number of arbitrary side-effects, and so could any of the functions it calls. To fully understand the semantics of a given piece of C code, you not only have to read the body of the function you're calling, but also the bodies of every function *it* calls, and so on, because any of those functions could twiddle some bit which you were expecting to have a certain value. It can be true that persistent data structures are sometimes slower than ephemeral ones, because they have to support history and sharing by default, which sometimes has an overhead. However, like with everything in programming, there are tradeoffs: persistent data structures are sometimes slower but like I said, they also do more than ephemeral ones and they are better for multicore programming because they don't require synchronization or locking. Fortunately, Haskell is such a nice language that it easily supports both. (Well, "easily" from the user's perspective; writing a high-performance ephemeral data structure in Haskell can require quite a bit of guru knowledge.)
[*Make it correct, make it clear, make it concise, make it fast. In that order.*](http://blogs.msdn.com/b/wesdyer/archive/2007/03/01/immutability-purity-and-referential-transparency.aspx) Note that the above quote, and the linked article, is from someone on the C# language team talking about why code in Haskell tends to just work correctly, unlike a lot of C# code. ;]
It's all over the place. At some point, a year or two ago, the conditions were modified to specify only compiled code in C, C++, or Objective C. That didn't go over too well, and it's currently at a lesser point, where you can't do some things with interpreted code, but even there exceptions are made if you're a major game studio writing a game that Apple wants to see on their platform. :)
Well, certainly not always. For example, [this](http://blog.cascadesoft.net/2010/09/10/new-app-store-rules-and-the-objective-c-versus-3rd-party-framework-question-for-iphone-apps/) &gt; In a remarkable volte-face, Apple yesterday announced that they were removing restrictions on development tools and that 3rd party frameworks could used to create iPhone and iPad apps for the app store. reports on the change that removed previous restrictions that would have made something like this ineligible for the app store. Not because it's an interpreter, but because of the tools used to build the application itself. That restriction would also have covered an application written in an interpreted language (but probably not an application that interprets user-entered code).
Its so awesome to read a great technical book ("Real World Haskell") and then have one of the authors give speedy, thoughtful and erudite answers to your questions. Even if Don doesn't get to your question first, other smart folks nearly always find a way to help. The Haskell community is fantastically friendly and helpful. The enthusiasm is contagious.
What a poor choice of reference! Just to be a jerk, I actually gave his code a try. His "functional" quicksort isn't just slower, but actually has regressed to something worse than O(N^2) time complexity. The space complexity is rather terrible, and GCs start dominating the execution time once you get to a list of 1000 numbers. In short, it is a function which would look to readers like a quicksort, and would pass unit tests guaranteeing correctness of implementation, but would actually have performance characteristics which are wildly at odds with the actual algorithm. In production code, the horrifying time and space costs would be as much of a bug as an incorrect sort result. The "bad" mutable quicksort, on the other hand, can only be wrong in ways that are actually pretty easy to check. Even without static guarantees, there isn't a chance that it is magically performing IO or allocations. If he accidentally switched two lines, then the simplest of unit tests would've caught the error. Of course, it goes without saying that some guy's throw-away code isn't representative of functional languages in general or their users in particular (and besides, it is in functionally-styled C#, so who around here is going to get too defensive?). But the *mindset* of his post is what concerns me, and that same concern is what motivated my original point. He wrote: &gt;At this point, some people will probably be thinking, "But isn't the first approach so much faster than the second approach?" &gt;Maybe, maybe not. Perhaps it doesn't even matter. &gt;I think it is important to use the following principle: Make it correct, make it clear, make it concise, make it fast. In that order. &gt;If I find that the code in question is not performant enough given the user scenarios that we need to support or if the code is overly complex for the sake of eliminating side-effects then I consider adding state. So his approach would advocate developers to not consider the crippling performance implications of their chosen coding style until users provide scenarios that force you to. In other words, write code that performs as badly as you can get away with. Of course, nevermind that if this fellow were on a team and he introduced code like this into a shared code base, he'd be giving all his coworkers an exciting opportunity to support some strongly-worded user scenarios. I also hope we've all learned a valuable lesson about protecting one's blog from such vast quantities of unintentional irony.
&gt; It is interesting to note that this problem hasn't arisen in practice for numeric literals. I think it is a combination of the difficulty of writing a Num instance, the limited syntax of a numeric literal, and the intuitive semantics of a number. Well, something similar could happen if you wrote a `Num` instance for a positive number data-type and tried to use a negative number as a literal...
Excellent write-up. Well done.
We could get rid of the String type entirely, and just have strings always be Text... Call it Haskell 3000. 
I think the title on reddit here is a bit misleading, to say the least. The essence of the issue is that the crashes are caused by the (custom, perhaps too strict?) implementation of `IsString` -- not by "String literals" in general.
The issue as far as I'm concerned is that a "convenient" instance for IsString that can error at runtime is wrong, and more people need to make the case that it is obviously wrong, and that partial functions like this, especially when they don't *have* to be partial, are a bad idea in general. For crying out loud, folks can just "resist the temptation" and cook up a dead-simple quasiquoter. If they don't, that's a library bug.
That title is exactly the point I am trying to make. It is literally (ahem) true, and not misleading. The OverloadedStrings mechanism makes the IsString class magic. It changes the meaning of string literals. The reason it was introduced was to allow string literals to represent `Text` in addition to the older `String` type. But this mechanism is too easy to abuse. People are using it as a cheap quasiquoter syntax and implementing `IsString` with partial functions. The effect is that they have redefined Haskell's string literal syntax to fail at runtime.
The solution to this is to tell people to stop it! IsString instances should be written for things that are strings; if something isn't a string data type, it shouldn't have an instance of IsString. The same thing is true of numeric literals; if you write an instance of Num for something that doesn't really have a natural map from the integers and just error for invalid input, then your integer literals can cause runtime errors as well. I have reject the reasoning that people are abusing a language feature, that the result is failing in precisely the way you'd expect it to fail, but that we ought to disable the language feature in protest over their abuse.
I certainly found it misleading. Runtime errors would be accurate. Runtime crashes mean RTS crashes, to me -- something that should require unsafeCoerce and friends. It sounds very much like you're trying to put IsString in the same category as unsafeCoerce... and that's simply *not* true.
There will and should always be a [Char] type. You might have a good case, though, for removing the String synonym, and changing more of the core libraries to use Text. Patience, though. One of the nice things about the Haskell community is that we have a nice balance between stability and change; Text is being used increasingly throughout the library community, which is exactly what should be happening at this point. It's too young to be talking about moving it to Prelude.
But it might be nice to have an alternative "unstable" Prelude that people can target as well as prepare for.
I certainly didn't mean RTS crashes, you're right. But saying "runtime errors" wouldn't get across the seriousness of this issue. It doesn't just give wrong results - it can cause pure functions to crash your program. (Someone could use `unsafeCoerce` in an `IsString` instance too, but fortunately we haven't seen it get that bad yet.)
&gt; It doesn't just give wrong results Yeah, it actually gives correct results... namely, bottom. :) I just think you're casting blame in the wrong place here. The fact that the expression can evaluate to bottom arises not because of IsString, but rather because of the use of error. The whole purpose of error is to cause pure expressions to evaluate to bottom. And I still have trouble calling this a crash. It's perfectly valid, expected behavior. If you don't catch the exception in IO, then sure, it exits and prints a message, because that's what error is supposed to do...
This criticism seems misplaced. For any given IsString implementation, you remain free to still call `(theImplementationUsedForFromString x)`, and it will equally crash at runtime. The IsString isn't causing the runtime crashes, the function is. You can implement _any_ function in a type class with "undefined", but that's not the typeclass' fault. Take away IsString, or hack on it until it stops doing what they want, and people will simply go back to re-inlining their conversion functions and it will continue to fail at runtime. I think people are seeing more magic here than there actually is. It's a typeclass. It has all the powers and foibles of a typeclass. Every criticism I'm seeing seems to actually be about typeclasses and have no specific relation to the syntax support, the only special thing about IsString. If the objection was that the IsString instance hid the fact of the function call from people I'd at least understand, but I haven't seen anyone mention that. What's being discussed is the partiality of functions, but writing partial functions is, again, not an IsString-specific problem. Haskell is not a language that requires complete functions. I see a problem, I'm not sure it's a _solvable_ problem without turning Haskell into something no longer Haskell. Which isn't necessarily a bad thing. But this seems like a poor choice of Haskell hill to die on.
Sure! If you're up for it, you could certainly see what you can come up with in terms of a "text-prelude" sort of package that is intended to be used with NoImplicitPrelude and replaces String with Text in relevant places. The code is all written, so it's just a matter of API design. I'd be interested in seeing an elegant solution to cases where you still need to expose generic list operations, but also the Text equivalents that have the same names... If the result is nice to use, chances are people may pick up on it.
&gt; I have reject the reasoning that people are abusing a language feature, that the result is failing in precisely the way you'd expect it to fail, but that we ought to disable the language feature in protest over their abuse. The language feature here is **string literals**. It's pretty basic. The default type for strings in Haskell is changing, so this new mechanism was added. The result is **not** failing in precisely the way you'd expect. String literals are not expected to fail at runtime. So this mechanism was the wrong way to extend string literals to `Text`, and that should be fixed. If enough people like this unintended cheap but unsafe quasiquoting that inadvertently arose, it might be left in GHC. But not as the mechanism for providing string literals with type `Text`, I hope. Those of us who do care about safe semantics should be able to use something as basic as string literals without having to turn on this extension.
There **is** magic here. It is not just any type class. This is a special mechanism which was specifically provided to extend string literals to apply to the new string type `Text`. So it should only be used in a way that preserves the expected behavior of a literal, and especially in a way that preserves the implied safety guarantees in the semantics of a literal. That kind of reasoning, safety through semantics, is perhaps the principle *raison d'être* of Haskell.
So, on further thought... I'm certainly opposed to changing the semantics of overloaded literals (whether they are numbers or strings), as they are currently very straight-forward, and this isn't a major issue. But there might very well be an argument for doing speculative compile-time resolution, in the following sense: 1. The compiler calls the (pure, and therefore meaningful outside of a specific environment) fromInteger or fromString function at compile-time. 2. If there's recursion going on where the application of fromInteger or fromString depends on the literal itself, then it falls back to runtime resolution and issues a warning. 3. If fromInteger or fromString fail with error/undefined, it sets the literal's value to bottom, and issues a warning. 4. If fromInteger or fromString fail to terminate in some short time limit, it kills the computation, falls back to runtime resolution, and issues a warning. 5. If unsafePerformIO and such are used in the implementation, the compiler also issues a warning and falls back to runtime resolution. This may still need to be enabled with an extension, because it could cause different results when there are "pure values" that depend on the system that's building the code (therein lies another complicated debate about purity...) But it seems like it would at least give a warning in yitz' scenario, and be semantics-preserving except in some weird corner cases. I'm not familiar enough with GHC's architecture to know whether it's easily done or not. It would involve executing code from the module that's currently being compiled, as well as detecting uses of unsafePerformIO and of some recursion patterns.
&gt; So it should only be used in a way that preserves the expected behavior of a literal, and especially in a way that preserves the implied safety guarantees in the semantics of a literal. You say this as if it is self-evident and does not need to be argued, but I disagree. I am never guaranteed that any particular combination of literals will not crash at runtime. Moving from "two literals may crash at runtime" to "one literal that is conveniently expanded into something else may crash at runtime" or even if you prefer the POV "one literal that may crash at runtime" isn't that big a change from my point of view. You have your choice of dealing with the possibility that literals may result in crashes at runtime, that literals may result in crashes at runtime, or that literals may crash at runtime. There's no other choice, regardless of what is done to OverloadedStrings. The root problem is that functions can be partial. Trying to beat down that part of the carpet will just move the problems elsewhere, and that usually makes the resulting problem more complicated, and more intractable when you hit it. Your own comment about breaking polymorphism with one of your suggestions is an example; you "fix" the problem at the cost of creating a new problem that is even harder to get around when you encounter it (I easily can write better string conversion functions, I can even do so without cooperation from the original library author, making the type system do something it doesn't want to do is _hard_), this is a net loss in software engineering terms. In Haskell, the best solution is probably to just explain this to people and say "Don't do that", because everything else will introduce substantially more complexity and _still_ fail to solve the underlying problem.
GHC bug: people can define Functor instances for things that aren't functors.
Well, I think the main problem with that is that people are used to using list functions with Strings. I think the real issue with getting rid of String is making many of Haskell's list functions more polymorphic to non-lists.
Yeah, what I meant was about the running of code, not the tools used to compile the app. That has changed, but there's always been a lot of misunderstanding about the clause about running code locally :)
So how about bad numeric literals for Num instances that have a partial fromInteger implementation? How is that any different? My response to both issues would be, "don't write broken instances".
It gets worse. In GHC, it turns out that a pure value of *any* type can cause a runtime crash: aGreatInt :: Int aGreatInt = error "die, GHC, die!" I'm not sure how to fix the compiler to prevent this, but I think the temptation is too great for abuse if we continue to allow it.
I think the reasoning is that if you write down a string literal `"Some string"` then it should always denote a non-bottom value. In other words, you have a syntactic guarantee that it's non-bottom. Strikes me as a rather minor issue, though.
At least in the numeric literal case, it'd be nice to at least have a class for `fromNatural` for non-negative literals (of course, there are no negative literals, I guess). There's no `Natural` type for that, though. I can't think of any subsets of strings that would be useful to build in, though. I certainly wouldn't expect `fromValidXMLString`. Edit: Another difference is that it's quite useful to be polymorphic in a numeric type. So it's nice to be able to do: f x = x / 3 + 1 instead of: f x = x / fromInteger 3 + fromInteger 1 However, `IsString` doesn't really buy you this sort of parameterization. There's no framework of general string combinators that is parameterized by the choice of string; it's just a matter of string literals being overloaded, probably to have their type immediately determined by what functions they're used with. So there's nothing really to lose by making the literals not genuinely overloaded (in the type class sense), but having some compile-time choice between several types.
Crashing the program is generally better than giving the wrong results: for once, it's easier to detect :)
&gt; (Well, "easily" from the user's perspective; writing a high-performance ephemeral data structure in Haskell can require quite a bit of guru knowledge.) Care to write a guide detailing this knowledge? :)
If only there were a blog post detailing a few points on writing an ephemeral data structure...like, say, a mutable hash table. That might be a good start. ;)
&gt; So his approach would advocate developers to not consider the crippling performance implications of their chosen coding style until users provide scenarios that force you to. In other words, write code that performs as badly as you can get away with. Of course, nevermind that if this fellow were on a team and he introduced code like this into a shared code base, he'd be giving all his coworkers an exciting opportunity to support some strongly-worded user scenarios. You are being ridiculous. This is an absurd interpretation of his post and it's disingenuous to pretend otherwise. Stop being deliberately obtuse.
That's probably a bug. Until it gets fixed, though, I think [this package](http://hackage.haskell.org/package/Agda) provides a workaround.
Personally, I think a `Num` instance for a natural number type sounds far more useful than the `IsString` instances in question.
Johan and I talk a lot about how that kind of thing would be really helpful for people, but neither of us has time to do it right now. The next best thing available probably would be Johan's slides from his CUFP tutorial last year: http://blog.johantibell.com/2010/09/slides-from-my-high-performance-haskell.html
This sounds like it should just be a type class "law."
What, nothing about his blog post strikes you as absurd? For example, the exhortation to put off worrying about performance, which accompanies some code that *very clearly* demonstrates the importance of checking one's performance assumptions? And here I'd hoped that we could share a friendly tut over the shame of it all. Ah me.
Did I say anything to indicate what I thought about the blog post and the amount of absurdity it contains? No, I did not. Perhaps I think it's flawless and reread the post daily. Perhaps I think the thing is mostly rubbish and just grabbed the quote because I liked the phrasing. I did not remark on the matter because it is irrelevant. You have inquired about why someone would assign priorities that differ from yours. When given a link to someone propounding their own view, you immediately seized upon a perceived flaw--in a *blog post*, no less, which are of course widely known for being free of error--that negatively impacted something *you* assign a high priority. You then criticized this portion at length, ignoring and misunderstanding the intended point of the post, and then presented your critique as bolstering your own preferences. Does nothing about that strike *you* as absurd?
How is this a bug? It's basically equivalent to aGreatInt = undefined which is essentially bottom.
Whoosh ?
Since I was the original implementor of IsString in ghc I can tell you that the motivation was exactly one of the cases you seem to want to rule out, namely using string literals for a new non-standard type. In my case it was for string literals in a DSL. Overloaded strings is just another mechanism in the language, and yes it can be abused to make partial values? So what, Haskell is full of partial values. If you want to get compile time errors, then why not suggest a general mechanism for that? For instance, a pragma that tell the compiler it must evaluate an expression to normal form at compile time. Then you can stick that in your fromString implementation and force a compile time failure. 
Good point. The motivation for the article was storing values in data structures, which typically forces the value if storing the value requires using the value's `Eq`, `Ord`, or `Hashable` instance. Note that even if a `String` isn't completely evaluated, you will need to store a thunk for the suspended computation, which will increase space usage, especially if the thunk refers to free variables which are otherwise not live in the program.
The `(:)` constructor takes three words, one for the header and two for the pointers to the value and the tail.
&gt; I didn't want to believe 5N words for string, with all the supposed pointer tagging optimization and stuff, so I made a fast experiment. And the result looks rather strange. Pointer tagging has nothing to do with storage layout. It's an optimization to see whether a value is evaluated or not and, if it's evaluated, which constructor the value corresponds to. Note that my numbers are based on the actual implementation used in GHC so they should be correct. However, it is tricky to exactly compute the memory usage of a program because of sharing, which might cause your program to use less memory, and GC, which might cause your program to use more memory. I suggest you use the supplied numbers to roughly approximate memory usage or to compare the relative space usage of two data types. 
Good points. Back-of-the-envelope calculations tend to work the best when one or a few data structures dominate the space usage of the program. A typical example is MapReduce like applications where one often allocates a huge lookup table that sits around in memory for the duration of the program. The program's space usage can then be approximated by the space usage of this lookup table. If the space usage is very spread out things get harder, but in the cases where it is spread out, the application tends to not use very much memory anyway. 
We have that general mechanism with Template Haskell. Quasiquotation can make it even more pleasant: http://stackoverflow.com/q/6122287/371753
In the light of day, ah... yup. Definite woosh.
&gt; Pointer tagging has nothing to do with storage layout. It's an optimization to see whether a value is evaluated or not and, if it's evaluated, which constructor the value corresponds to. Well my thinking was something like this: We have a [Char] value. This is a pointer, either to a thunk or to a value. We can use one bit to decide whether it is evaluated or not, and if evaluated, another bit to decide whether it is Cons or Nil. So let's suppose it is a Cons. Then where it points, we need the a character and a tail. The character is again a pointer, since it's boxed, but then comes the character itself. The tail is a similar pointer as the first one. This would give 2 pointers + 1 char per character, which is 3N words. I thought this is what GHC is doing, but apparently not. Or, I'm really not understanding something here. &gt; [...] However, it is tricky to exactly compute the memory usage of a program [...] I made some measurements, and they indicate the "rule of thumb" that ascii strings are 3N (probably because of the character pool you mention in the blog post), and unicode strings with distinct characters are 5N.
So why not 4N instead of 5N? Three words + the character is four words.
Very cool. I started something similar as a course project a while ago, but haven't had time to follow up on it. My (very incomplete) code is also [on github](https://github.com/arnar/afp-labs/tree/master/lab3), please have a look and see if there is something of use. Essentially my goal was to provide a EDSL allowing you to describe game states in terms of boards (impl. as a graph of locations), players and legal moves. From this, one could get a successor function which you could plug directly into something like A\*. I'd be thrilled if we can collaborate a bit and make something useful. Perhaps something that could even replace the horrible prolog-language used in [GGP](http://games.stanford.edu/).
Each character takes two words, one for the header and one for the underlying `Int#` (which contains the Unicode code point number).
Very nice. Along the lines of one of your comments, I'd consider having the constructor only store the canonical forms of games, and giving a function to construct a game from arbitrary sets of right and left functions. Also, in case you haven't come across it, it's possible to define left and right at the same time that you define the data structure.
Could you make a Game class, and define a function that takes a type of that class and returns a combinatorial game? Sorry, my haskell is very rusty, but this seems to make sense to me.
This is a poor man's Bookmark. Nor me please
Do you mean something like: &gt; Data CG = CG_ ([CG], [CG]) &gt; &gt; makeCG (left, right) = canonicalize $ CG_ (left, right) and then export makeCG but not CG_, and also use makeCG everywhere? I'm not sure I'm want to do that, because I want to let users deal with general games. For instance {1,0|} = {1|} = 2. The first form is not canonical since 0 is a non-optimal move for Left. But it might still be useful to allow non-canonical forms because they possibly correspond to a valid physical board configuration. Yes, I know of record syntax, but I haven't yet bothered to read the wiki or Report for the exact semantics.
What does the header do? Why is it necessary at all, I mean, we already know it's a character, doesn't we?
wrt using (+) instead of `plus`, you can import the Prelude qualified. import qualified Prelude as P import Prelude hiding ((+)) Any "normal" use of the + operator would then have to be qualified, like so: 2 P.+ 5 but you'd be free to define your own (+) that's not part of a Num instance the import Prelude hiding ((+)) is so you only need to prepend the P to the + operator, not every single Prelude function 
Every heap object has a header. It contains information used by the garbage collector, the constructor number (for use in case statements), and some other things.
This little ASCII diagram shows the layout of a one cons pointing to a Char in memory: +-----+-----+-----+ | (:) | | | ------&gt; +-----+--|--+-----+ | v +----+-------+ | C# | Char# | +----+-------+ So there are 2 pointer, 2 headers, and the character (Unicode code point) itself.
Sure, we can talk about it. But I think your approach is more on the AI side, right? In CGT, the approach is to calculate exact values. It works for "easy" games and for small subgames of hard games, but of course it won't help you much if you want to find the winning openings in chess. Also, I'm not planning to spend a lot of time on it, since it's of limited practical use. Writing this has been great for learning Haskell, but alas, even Haskell is of limited practical use if I want to find a job.
You can save reddit submission links.
My point was that with pointer tagging, the constructor number is already known (at least for types with say at most 3 constructors), so just for this there is no need for the header. Char is a single-constructor type anyway, so the constructor number is known statically. Thus I didn't think there is a header for Chars. So, can you point me to some description what the header is used for, apart from the constructor number?
Not on the android app I was using at the time. Perhaps someone knows a Betty one.
I hate apps that don't expose all the features you want.
Thanks, that's helpful.
You can read about the [layout of heap objects in the GHC commentary](http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects). The header is really a pointer to what's called an info table. The info table contains some more information in addition to the constructor tag. See the above linked document. The reason `String` uses 5N words is because it's defined in terms of a polymorphic list type. The list cons node must be able to contain any value (as it's polymorphic) and hence the value in a cons constructor is represented by a pointer. I think you're saying that this pointer could point directly to a `Char#` instead of a `Char` (i.e. a header plus a `Char#`), but this would make things difficult for the GC. When the GC traverses a list, it needs to know whether a piece of memory is a pointer (which it needs to follow) or not. If the value pointed to by the cons constructor didn't contain a header, the GC wouldn't know what to do with the contents of the word that contains the `Char#` (remember: the static type information doesn't exist at runtime). Note that you can define a linked-list based string type that takes less space, at the cost of giving up one level of laziness: data CompactString = Cons {-# UNPACK #-} !Char CompactString | Nil This type is still lazy in the tail, but not in the value; forcing the `Cons` constructor forces the `Char`. This structure uses 3N words per character (and cannot make use of the shared `Char` cache).
Have you seen the [Google ai contests](http://www.ai-contest.com/)? The beta site for the upcoming contest can be found [here](http://aichallengebeta.hypertriangle.com/), I suspect you will like it. Yes, you can write your bot in haskell ;)
Why are you storing a tuple inside the CG datatype? I would define it like: data CG = CG { leftOptions :: [CG], rightOptions :: [CG] } This way, you get the left/rightOptions functions for free and the pattern-matching and construction become easier to read.
 ofun z = someOtherFunction * someOtherFunction where someOtherFunction = sin z fun = let something = otherFunctionCall 5 in 2 + some &lt;- caret here, I want to autocomplete No autocomplete for locals, with distinction of scope. I chose the most nightmare case, just to be sure. Intellij IDEA does this. 
I presented an improved version of this the other day at the DSL2011 workshop up at McMaster, which is capable of simultaneously optimizing more than one cline. Hopefully the video I recorded will turn out.
For comparison, here's a directory with four implementations of Surreal numbers and games that I wrote awhile back. The first uses lists as the OP does, the rest use various set types. https://bitbucket.org/mtnviewmark/haskell-playground/src/58c51e1746b8/Conway/
It's also more efficient that having a pair inside the CG.
True, although that effect could also be achieved by just making it a newtype instead of a data declaration.
No, OverloadedStrings predates Text. I believe the original motivation was DSL embedding. IsString is just a typeclass, no more magical than Num.
Indeed.
&gt; But I think your approach is more on the AI side, right? Not really - merely to have a language to easily describe board games (i.e. complete information games whose state consists of pieces belonging to players placed on locations). The "output" is an encoding of the game's state machine, incl. a successor function which fits into pre-existing AI algorithms (which I don't really care about). What do you mean by "calculate exact values"?
I would love to have instances be first class objects in Haskell. It would make all these headaches with instances go away. It would take care of orphans (instances would be explicitly exported), overlapping instances (you explicitly specify which one you want), etc. The trouble being that it brings an entirely new set of headaches in....
Seen in #ghc: &lt;mux&gt; did you guys notice there is a duplicate line in the GHC 7.0.4 release notes? &lt;JaffaCake&gt; mux: it was so important, we fixed it twice
Hahaha, I hadn't seen this reply yet :-)
Awesome. That's like getting the answers for the exercises. Thanks.
I knew about record syntax but didn't care one way or the other, but had completely missed that fact that I could do away with the tuple. Thanks!
yaaaaay
Hey that's me! Bug reports, suggestions, comments, feedback, code-review all appreciated.
Is there a good reason why runghc doesn't do this by default?
Alas, Haskell solutions can't take advantage of multi-core parallelism...
Thank you so much! Somehow I thought today was still Wednesday, if it weren't for this I would have started 24 hours late....
Great work! We have this at work, and it is very useful. Next step: store the objects in the cloud and share them with your friends... ;)
Damn, I wanna join but I have no team. Any one up for a reddit-haskell team?
To participate I need to download 30G and install a new operating system? That's crazy.
According to my calculations, when my Debian download is done there will still be a few hours left of the competition.
Compiling to .o files is slower. So that's the trade-off, fast one-time run, vs fast for repeated runs (in which case why not just compile to a standalone exe?).
This is a pretty long involvment if you want to participate seriously. And now you need to prepare a vm with a different OS.. that's not that a big deal, but still, lowering the cost of entry instead of requiring preparatory sysadmin work would feel better to me. I hope the actual problem to solve will be good.
My guess is closer to 6Gb, I don't even have 30Gb free. You can download a net-install cd image and install directly from the net. Also, as far as I understand, you only need a new operating system to compile your final submission (at the end of the 72 hour), not for working on it.
Any other gems you're hoarding? If you don't want to be responsible for maintaining a bunch of small utilities maybe it would be worth finding someone from the community to shepherd them?
I'd imagine that the reason for not compiling to a standalone exe is to use haskell as a scripting language -- i.e. `#!/usr/bin/runghc` etc... A proposal for this came up on stackoverflow some time ago: http://stackoverflow.com/q/712696/371753
Under the hood, this package is of course compiling to a standalone exe, but sometimes it can be nice to not have to worry about that explicitly. In terms of tradeoff, for hello.hs on my machine I'm getting old runghc, .19s new runghc on recompile, 0.46s new runghc on cache, 0.01s Your mileage may vary depending on the application.
They used the 2nd edition of this book at my university (until they decided to scrap Haskell completely from the curriculum :().
:() is an awesome emoticon.
btw, does this book provide any additional insight, if one's already worked through LYAH and RWH?
Yes. This book introduces the "true spirit" of Haskell: deriving programs and proving them correct. I haven't read the book, but I wager that the new Rock-Paper-Scissors example is the same as the old example from Bird &amp; Wadler. I would say that it is the best introduction to the wonders of lazy evaluation. In contrast, LYAH and RWH take a more "practically inclined" approach: they explain the syntax and some concepts; RWH also introduces several libraries. But they never really introduce you to the "pen&amp;paper method" of designing Haskell programs, which is indispensable for creating *new* abstractions. 
Then you may find this emotion awesome too :[]
Ahh, I'd assumed it was using `runghc -fobject-code`. I wonder if that would work too.
Looks interesting! Do you plan to support non relational databases such as MongoDB or Redis?
Well, I happen to have *Introduction to Functional Programming using Haskell (2ed)* lying on bookshelf as well (but haven't had time to work through it yet - I started with RWH, and only later got to LYAH as it came out later), should I rather get my hands on *The Craft of Functional Programming (3ed)* and work that through instead?
Good question. Since I haven't read the new book here, I can't really say, but they are probably very similar. With your LYAH and RWH background, I would recommend to use what you have and work through sections 7, 9 and 11, 12 of Bird's *Introduction to Functional Programming using Haskell (2ed)*.
Angie Jolie emoticon
(:[]) -- the robot monkey operator. I forget who originally coined the name.
See also "Combining Syntactic and Semantic Bidirectionalization" by Janis Voigtländer, Zhenjiang Hu, Kazutaka Matsuda and Meng Wang: * http://www.iai.uni-bonn.de/~jv/icfp10.pdf * http://hackage.haskell.org/package/bidirectionalization-combined * http://www-ps.iai.uni-bonn.de/cgi-bin/b18n-combined-cgi
Nice! I'd call it a persistence DSL rather than a "connectivity library" since it is build to work over existing binding. Do you have a notion of how to handle joins (or simply many-to-many relations) by the way?
here is the generated sqlite schema, which looks, imo, quite clean: sqlite&gt; .schema CREATE TABLE "Customer$String" (id$ INTEGER PRIMARY KEY, "customerName" VARCHAR NOT NULL, "details" VARCHAR NOT NULL, CONSTRAINT "NameConstraint" UNIQUE ("customerName")); CREATE TABLE "Item" (id INTEGER PRIMARY KEY, discr INTEGER NOT NULL); CREATE TABLE "Item$ProductItem" (id$ INTEGER PRIMARY KEY, "productName" VARCHAR NOT NULL, "quantity" INTEGER NOT NULL, "customer" INTEGER NOT NULL REFERENCES "Customer$String"); CREATE TABLE "Item$ServiceItem" (id$ INTEGER PRIMARY KEY, "serviceName" VARCHAR NOT NULL, "deliveryAddress" VARCHAR NOT NULL, "servicePrice" INTEGER NOT NULL); CREATE TRIGGER "Item$ProductItem" DELETE ON "Item$ProductItem" BEGIN DELETE FROM "Item" WHERE id=old.id$;END; CREATE TRIGGER "Item$ServiceItem" DELETE ON "Item$ServiceItem" BEGIN DELETE FROM "Item" WHERE id=old.id$;END; 
http://en.wikipedia.org/wiki/Y_combinator http://en.wikipedia.org/wiki/Core_wars Anyone who wants to play had better be learning how to write loops in the SK calculus, fast. Could the bottom-line lesson of Core Wars be already lost to the dustbins of time? The way I remember it, they stopped playing when a virus won. As in, the program paradigm here should be KISS, in the extreme.
If one approaches a game as a classic search of the move tree, a good application of neural nets is to pruning the set of moves to explore. (As opposed to the more familiar "what's the value of my position" used to great success in backgammon.) One could surely be calculating the neural net using a bank of parallel machines. I don't know a better language than Haskell for easily writing parallel code.
Pretty slick! :-)
you should be interested in parsing libraries that provide combinators to declare both parsers and pretty printers from the same code.
Help with Postgresql backend will be great! I answered to the most general questions in the post edit. I was a little unclear when I mentioned IO. There have never been functions like insert :: ... -&gt; IO (). I meant change from newtype DbPersist conn a = DbPersist { unDbPersist :: ReaderT conn IO a } to newtype Monad m =&gt; DbPersist conn m a = DbPersist { unDbPersist :: ReaderT conn m a } I don't think additional layer adds much overhead since it does not do conversion. Usually it just wraps data into PersistValue. Expensive conversion can be done only for date types &lt;-&gt; String. But if profiling shows that it is important (for example I assume that if data has unboxed fields, overhead will be a little higher), specialized instances of PersistEntity created for particular backend can help. Do you mean changing schema for performance tuning e.g. adding indices or something else?
I think Template Haskell could finish the job and turn the inversion back into a compile-time case statement.
As Groundhog supports datatypes with arbitrary structure doing joins is a tricky task. In a simple case like data A = A B data B = B Int it clear how to join them. But in general situation when the values are nested in a tree-like structure figuring out when to do join automatically is difficult. Perhaps we can allow user to tell when to join in the datatype description. Many-to-many relations don't have direct support at the moment. You can define something like data Author = Author {authorname :: String, books :: [Book]} data Book = Book {bookname :: String, authors :: [Author]} and it should work despite circular references if you give uniqueness constraints on names. However, it will result in four separate tables for two lists instead of many-to-many one table. The invariant that if a book has an author, the author must have this book will not be enforced as well. edit: doing simple insert will stumble upon circular references. A workaround is to insert dummy values with no relations and replace them after values it references to are in the database. Example: let author = Author "author" [book]; book = Book "book" [author] k &lt;- insert $ author {books = []} insert book replace k author
Or, more specifically for function inversion (rather than bidirectionalization): http://www.iai.uni-bonn.de/~jv/dhug2010-slides.pdf
Would it be possible to automate this transformation so it can be used as a compiler optimization?
Could you help me understand the optimization? It seems like you allocated one cons cell, one Just, and two Nothings instead of three cons cells. Are you relying on SpecConstr to avoid allocating those Just/Nothing constructors?
Reminds me of [this SO question](http://stackoverflow.com/questions/6247111/creating-functions-over-enumerations)
Sounds like fun. Tentatively count me in.
KISS as in KISS=IS=S=`\a b c -&gt; (a c) (b c)`?
Yes (though I haven't tested if this is actually the case--naughty me). Also, the Just/Nothing constructors die ~immediately, whereas unshared constructors will live on as long as they are reachable. (So perhaps it's not a pattern for avoiding allocation, but a pattern for improving sharing.) Actually, the real reason I needed this was because I needed to map a function on large recursive structures, and know whether or not if anything changed or not. If something did change, I might need to do another iteration of a loop. Without this extra info, I'd have to do a deep-equality test: with the extra info, I know immediately.
Not obviously. There’s no way for the compiler to know that the result of a function `a -&gt; a` had no change: that’s not generally computable. What might be possible is some sort of syntax sugar which makes writing functions in this style simpler (it is pretty annoying.)
I've run into similar situations when folding instead of mapping, where an the folding function produces additional changes to the structure that have to applied to a achieve a consistent state. This led to a fun little function which I haven't seem anywhere else. I call it cascade. cascadel :: (a -&gt; b -&gt; (a, [b])) -&gt; a -&gt; [b] -&gt; a cascadel f = foldl $ \x -&gt; uncurry (cascadel f) . f x 
P.S. -- Haskell has the S and K combinators in the standard libraries; they're called `(&lt;*&gt;)` and `pure`. ;]
Count me in too. PM sent.
Couldn't you restrict it to instances of Eq?
&gt; So perhaps it's not a pattern for avoiding allocation, but a pattern for improving sharing. That's what I'd suspect. There's a possibility of reduced allocation, but the increased sharing is the real benefit.
That could become very expensive, and it falls into the same trap as just checking equality on the final result.
As for many good things, it was coined by Pseudonym (Andrew Bromage), &gt; 06.11.22:16:48:44 &lt;Pseudonym&gt; I'd be tempted to use the robot ninja operator instead of return there. &gt; 06.12.18:15:09:15 &lt;Pseudonym&gt; (:[]) is the robot ninja monkey operator. 
Except you don't have to re-traverse and you're going to have to do *some* comparison between the input element and the output element of the function anyway. And if you want to restrict the equality comparison, couldn't you make a newtype with a new tailored instance of Eq?
for a completely different technique to achieve similar result, based on information theory (something I actually experimented when I was teaching): - take the text of the two assignments a, b - zip them separately and get the sizes of the resulting files z_a, z_b - zip them together and get the size of the resulting file z_ab then, you can compute a rough measurement of redundancy r_ab with this simple formula: r_ab = z_ab / (z_a + z_b) if R &lt;&lt; 1, it means there is substantial redundancy between a and b. Of course, this measurement has no unit, and as the essays are written in the same language on the same subject, there has to be some redundancy. But you can easily circumvent this problem if you have N essays: - compute r_ij for each pair - compute the median of r_ij as a reference then, plagiarism appears as a deviation below the median.
Link for source please anyone?
The post is talking about a `cabal repl` command but my `cabal` doesn't recognise that command... which Cabal version do I need for that?
Sam is talking about his GSoC project this summer which is to implement `cabal repl`. It's not finished yet! :-)
an update on the benchmarks site would be nice.
There is an introductory talk by Simon Peyton-Jones ["Taste Of Haskell"](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-tutorial/index.htm) using window manager written in Haskell (XMonad) as an example of real world program. Also I've found the [lectures on Haskell by Eric Meijer](http://channel9.msdn.com/shows/Going+Deep/Lecture-Series-Erik-Meijer-Functional-Programming-Fundamentals-Chapter-1/) on Microsoft's Channel9 very good (though these are more exercise oriented). They are based on the [textbook by Graham Hutton](http://www.cs.nott.ac.uk/~gmh/book.html) 
(not answering your question, but as a once-beginner .. ;-)) Subscribing to [haskell-beginners](http://www.haskell.org/haskellwiki/Mailing_Lists) is a must! Even the dumbest 'sounding' questions are okay, as I have found out by personal experience. :) There are plenty of questions there, too, like "how can I make this idiomatic?", so feel free.
We generally only update the benchmarks when there's a significant change.
They look great thanks for that!!
Hey that's really great, I completely missed that when trawling through haskell.org. Thanks for that!
http://haskell.org/haskellwiki/Video_presentations
Well now I feel dumb, I totally missed that :) Thanks for that, looks like a lot of good stuff there. I appreciate the info should help a lot.
(I should clarify: once-beginner certainly doesn't imply I'm not one still ..) Haskell-cafe is also great to be subscribed to, if you're so inclined, just to immerse yourself in the kind of conversations that are taking place at a higher level; being used to the terminology and concepts (at some level) helps when it's time to grasp them! Hoogle and Hayoo are **invaluable**. Searching by type is by far the must useful thing I do with them, and I feel like it's missing whenever I'm not using Haskell (thanks, purity!).
I don't have any videos to share but I just thought I'd mention that this is a great idea and I'm glad you made the thread. With end of semester break coming up I'm looking forward to learning some more Haskell so this will help me (and others I'm sure) heaps. Upvotes for everyone.
Signed up to both :) Hoogle looks really useful, I can see it being more so once I get a bit more under my belt. Thanks for your help!
Not true: the function will probably do a case discrimination on the input element, but that's all it needs to know in order to figure out if it wants to make a change or not.
For more about SafeHaskell click here: [broken link]
Pipermail URL parsing issue. Just remove the trailing comma: http://hackage.haskell.org/trac/ghc/wiki/SafeHaskell FWIW, Reddit has the same problem: http://hackage.haskell.org/trac/ghc/wiki/SafeHaskell,
I wonder if Oleg was aware of the [Generator](http://hackage.haskell.org/packages/archive/generator/0.5.4/doc/html/Control-Monad-Generator.html) package. 
"Problem" is debatable; commas are valid punctuation in URLs.
I agree; that's why I carefully avoided using the term *bug*. It is problematic in the sense that I often see people getting tripped up by it, even if the feature is working exactly as designed. Parentheses are also valid characters in URLs, but Reddit ignores them: http://en.wikipedia.org/wiki/Paprika_(2006_film) Perhaps the same should be done for trailing commas.
I hadn't thought of that. Could you specialize it to two cases: One that runs an initial discriminator and one that tests equality after application of the function?
And Snap 5.0 is already on Hackage! ([With release notes](http://snapframework.com/blog/2011/06/18/snap-0.5.0-released).)
Woops, forgot the leading 0 (i.e. it is really 0.5.0, not 5.0).
&gt; Once you’ve seen a language like Haskell it’s very difficult to go back to programming in Java or C or whatever Wasn't there a saying along the line of "Once you go Haskell, you never come back"? ;-)
Sorry... too late now. Anyway I didn't have the resources to install Debian and prepare a webserver.
Hmm.. It looks like the article was tagged with "Ruby on Rails".. Why is that? I mean, it is brought up in the interview, but so is Django, PHP, Java, etc.. 
http://en.wikipedia.org/wiki/Paprika_\(2006_film\) You just have to escape the parentheses with \
As I couldn't infer it from the announcment: Is the Quad-Copter actually running Haskell code? :-)
Supero is a fantastic project, but... Supero was meant to be part of the YHC compiler. Unfortunately, ndm himself reports that [YHC is dead](http://yhc06.blogspot.com/2011/04/yhc-is-dead.html). It looks like Supero can also be run stand-alone. But since ndm has left university and now works at a bank, it looks like Supero itself isn't getting much support anymore either. There haven't been any updates to the darcs repo in about a year and a half.
Do you know of any such parsing library capable of defining a CSV parser/printer, for example? (CSV with all of its escaping rules, which is non-trivial).
I've only read about these things, never tried any of this stuff. but you could try this: http://www.informatik.uni-marburg.de/~rendel/unparse/
C code generated from the [Copilot EDSL](http://hackage.haskell.org/package/copilot), IIRC.
Max Bolingbroke at Cambridge is continuing super-compilation research under SPJ, http://www.cl.cam.ac.uk/~mb566/papers/chsc2-icfp11.pdf
Not to start a flame war, but versus WAI, this interface does not provide for sendfile optimizations or use builders for efficient packing of buffers. Also, I think you'll find it very difficult to design a high performance web server given the current interface for the request body. What's the reason you decided to create a new interface instead of just using WAI?
Hack predates WAI by a good year, at least.
WAI is great. So is snap-core. Both are very visionary. I'm trying to make the interface more similar to ruby's rack, or python's wsgi. Consider it an experiment, it really is. no promise on API stability :)
This is hack2, which is backwards-incompatible. And I'm well aware of Hack, I wrote quite a few packages for it. I created WAI instead specifically because of some of the issues hack2 is trying to solve.
Having recently moved to Singapore, I checked out meetup.com for a group for functional programmers but hasn't found any. I think you could gain a lot of visibility by using meetup.com. Do I need to preregister or anything? Or do I just show up on Wednesday?
Feels like just yesterday we were at 0.5.0
I have uploaded http://hackage.haskell.org/package/Transhare which abstracts my implementation of this sharing pattern. It lifts (a-&gt;Maybe a) to containers with maximum subcomponent sharing.
I think they're just a little behind on their announcements
The description of that project is all buzz words. It says nothing other than that Haskell will be used. I guess in this context, that's also just a buzz word. Nevertheless, the article *did* answer my question - no relation to Henning, apparently.
Here is the informal leaderboard, (optional for testing, not official) as frozen at contest's end: http://kokako.kb.ecei.tohoku.ac.jp/leaderBoard I wonder how many people threw a certain percentage of their games, to sandbag the competition? 
don't be so harsh.. i'm sure at least half the NYTimes writing staff would probably struggle to explain the humor behind "they're just a monoid in the category of endofunctors, whats the problem?" so this is a good first start.
What a giant strawman. No one's complaining that lazy evaluation is "magic"... or at least I've never seen such a complaint. What they're complaining about is that the simple, deterministic rules tend to lead to tricky problems in large programs. The SK calculus also has a simple semantics but I wouldn't want to reason about programs written in it.
Just half!?
Operational semantics for call-by-need is something that is often overlooked or supplanted by denotational semantics in introductory materials. Many performance questions/mysteries are easily answered with a basic understanding of the operational semantics.
lol at the pic
If it is so easy to predict the operational behavior of Haskell evaluation, can you predict how much space would this take? Using the [List package](http://hackage.haskell.org/package/List): {-# OPTIONS -Wall #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Monad(liftM, liftM2) import Control.Monad.Trans.List(ListT) import Data.List.Class(foldrL, lengthL, enumFromTo) import Prelude hiding (enumFromTo) newtype Forget a = Forget { unForget :: () -&gt; a } deriving (Functor, Monad) runForget :: Forget a -&gt; a runForget = ($ ()) . unForget sumL :: (Num a, Monad m) =&gt; ListT m a -&gt; m a sumL = foldrL (\x r -&gt; liftM (x+) r) (return 0) average :: (Monad m, Integral a) =&gt; ListT m a -&gt; m a average xs = liftM2 div (sumL xs) (lengthL xs) result :: Integer result = runForget . average $ enumFromTo 1 2000000 main :: IO () main = print result `enumFromTo` isn't from Prelude. I would expect the Forget wrapper to prevent the list's spine and elements from being memoized (given that the links between the list elements are wrapped in a `Forget`). I think a variant of this program took O(1) space when unoptimized, and O(N) space when optimized. This variant seems to always take O(N) space. Can anyone explain what's going on?
And some questions/mysteries are not answered/defined at all, and depend on compiler version/optimizations. 
I was playing with something similar here: http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad/6209279#6209279 I haven't delved into `ListT` but I'd imagine the issue is the same as what I ran into -- you can prevent sharing all you want, but you still need some additional strictness to force the folds to run in constant space. You can get the same effect with a single fold over a plain old list (minus clever optimizations).
I wonder how to add strictness to the foldlL there. The definition in the List package is very general and in "extreme" points-free style which makes it harder to figure out at a glance.
Why does a self post with no actual post have upvotes?
That doesn't contradict anything I said; you're just restating the main point of the article. Unfortunately, that point has little relevance to the emergent properties of lazy evaluation in large scale programs. All operational semantics are not equally good choices for a general purpose language. The article fails to address that point entirely.
I think it might be something that people believe, which keeps them from even really looking at Haskell. Maybe "magic" is too strong a word but I could imagine people thinking that it was just too complicated to truly understand unless you're a compiler expert or something.
something went wrong when posting. I think it must have been reddit, as the title was suggested automatically from the post that indeed does exist. reposted properly at http://www.reddit.com/r/haskell/comments/i4xhk/turning_dynamic_web_programming_on_its_head/
In fact, all questions of performance have no answers that are defined at all, because operational semantics only specify the meaning of a program, not how it actually is implemented. Any optimization that preserves semantics is unaccounted for by the semantics of the language.
We're not doing a good job teaching people how to reason about the performance of their Haskell programs. Most of the time (i.e. when you're not explicitly making use of laziness in some complicated way) you can apply local reasoning (e.g. at the function level) to reason about the performance of your program. You can, by looking at the source, say which arguments of a function will be evaluated and when, by using the same backwards propagation analysis GHC uses (i.e. start by looking at the result of the function follow the branches of the function back to the arguments).
&gt; That doesn't contradict anything I said. It does. From what I've seen, many people are complaining about not understanding the behavior of particular smaller programs, which can easily be remedied by tracing/understanding the evaluation. Recent example: &gt; [How on Earth Do You Reason about Space?](http://thread.gmane.org/gmane.comp.lang.haskell.cafe/89608/focus=89672): &gt; "But at least I don't see ByteStrings as this black box of efficiency anymore, but actually understand how they're structured, and what they're good at, and what they aren't good at." --- Concerning lazy evaluation in large programs, note that eager evaluation may force you to write an entirely different program. For instance, Peaker's example in this thread makes essential use of lazy evaluation in the form of `enumFromTo`. You can't have both: use lazy evaluation *and* insist that it should be eager evaluation. 
That's all very nice, but where's the code?
That was my thought. I found this after some searching: http://awelon.hg.sourceforge.net/hgweb/awelon/awelon/
You can't add the strictness you want to `foldrL` directly. Remember how a foldr looks: 1+(1+(1+(1+(1+(…)))))=∞ (source: http://foldr.com/) You want a right moving fold alright, but one which takes advantage of the monoidal nature of `+` directly, in particular of its associativity.
&gt; From what I've seen, many people are complaining about not understanding the behavior of particular smaller programs, which can easily be remedied by tracing/understanding the evaluation. We've seen different things then. In either case, my issue and the issues of many are with the problems of lazy evaluation and not the problems of education with respect to lazy evaluation. The article does not address the former and the problems do actually exist. It is either due to dishonesty or ignorance that they weren't mentioned in the article (the latter is forgivable); the picture it paints is misleading. &gt; You can't have both: use lazy evaluation and insist that it should be eager evaluation. I never made such a claim. Furthermore, the problems of lazy evaluation surface when writing programs that you *could* write in an eager language. You're setting up a false dichotomy here; the choice isn't "eager" versus "laziness and all its blessings and downsides". And even when it comes to the benefits of laziness, you can, of course, have codata and lazy evaluation in a language where eager evaluation is the default. 
 &gt; You're setting up a false dichotomy here; the choice isn't "eager" versus "laziness and all its blessings and downsides". Ah, I meant to include the "lazy inside eager" variety as well. In any case, I'm not convinced, I would like to see an example of the emergent behavior you describe: a program that works in both eager and lazy evaluation and that displays strange space behavior in the latter. To make things "fair", all obvious accumulator parameters should be made strict (`deepseq`). 
Foldl is a simple example and the need for something like deepseq is exactly the problem. Every use of deepseq is a full traversal over the data structure; you cannot simply litter your program with them as it's far too expensive. The programmer has to make use of such things carefully and understanding where to make things more eager can be difficult in large programs. I have other problems with lazy evaluation as it exists in Haskell as well. For example, non-termination of 'sum' on an infinite list won't be caught statically whereas such a list is impossible in an eager language; you'd have to explicitly use a stream type instead thus avoiding the problem by turning it into a statically-detectable error. For another, lazy IO is a complete disaster. Then there's the constant performance overhead. And the increased difficulty with respect to unboxing. And the delaying of errors until much later in the program... Analysis of such problems isn't even necessary however; the difficulty of reasoning about performance is fatal enough.
A lint-like program that could suggest semantically similar but possibly more efficient variations of a function would be a great teaching tool.
&gt; Every use of deepseq is a full traversal over the data structure; you cannot simply litter your program with them as it's far too expensive. There is no need to. You can annotate constructor fields with `!` or use smart constructors if you don't want to change the data structure. &gt; For example, non-termination of 'sum' on an infinite list won't be caught statically whereas such a list is impossible in an eager language; you'd have to explicitly use a stream type instead thus avoiding the problem by turning it into a statically-detectable error. You're shifting the problem to the stream type? And on the other hand, you can create a custom data type for strict lists. I don't see the problem. &gt; For another, lazy IO is a complete disaster. I see no problem with using a strict `foldl'`. If you don't like closing the file yourself, you can write a helper function withFileContents' :: NFData a =&gt; (String -&gt; a) -&gt; FilePath -&gt; IO a that forces the result, handles exceptions and closes the file. Alternatively, you can use ordinary `IO` on handles, like you would in an eager language. &gt; Then there's the constant performance overhead. I'm happy to trade the benefits of lazy evaluation for a constant factor. &gt; And the delaying of errors until much later in the program Due to purity, this is only relevant in conjunction with `IO`. Solution: validate external input eagerly. --- I don't perceive any of these issues as problematic. Furthermore, I don't see how these are issues are tricky or related to large programs. 
&gt; There is no need to. You can annotate constructor fields with ! or use smart constructors if you don't want to change the data structure. You're addressing a different point. I gave you an example of precisely what you asked for. &gt; You're shifting the problem to the stream type? No you're not. You'd have to be an idiot to write sum for an infinite stream. &gt; And on the other hand, you can create a custom data type for strict lists. Indeed. And you can replace the rest of the included data types with strict versions as well. And switch to a default of eager evaluation. And then the problems are gone! &gt; Alternatively, you can use ordinary IO on handles, like you would in an eager language. Yes, if you avoid lazy IO, lazy IO isn't a problem. &gt; Solution: validate external input eagerly. I'm sensing a trend here... All you're saying is that if you avoid laziness, those things I mentioned aren't problems. I know. That's why I mentioned them. It's a bad default. In particular, the IO primitives in the Prelude are horrible. &gt; Due to purity, this is only relevant in conjunction with IO. Completely untrue. Have you used Haskell extensively? Have you really never run into some "undefined", "no parse", division by zero, or other nonsense much later than you would have in an eager language? The problem has nothing to do with IO. Haskell is not a total language. Errors abound. I'd prefer them sooner rather than later, preferably with a simple stack trace. &gt; Furthermore, I don't see how these are issues are tricky or related to large programs. I never characterized any of these issues as being related to large programs except for unpredictable space behavior. Delayed errors certainly are related however, as is the non-local termination reasoning that Haskell often sticks you with.
Bad Haskell practices in bad results shocker. More at 9.
&gt; All you're saying is that if you avoid laziness, those things I mentioned aren't problems. I know. That's why I mentioned them. It's a bad default. I think that these issues are minor and that you have to weigh them against the benefits of lazy evaluation (modularity: short-circuiting `foldr (||)`, infinite lists, first-class control structures, memo tables &amp; dynamic programming, etc.). What I'm saying is that if you encounter a situation were lazy evaluation is problematic, it is straightforward to switch to an eager style. Of course, this doesn't work if your program heavily depends on lazy evaluation, but in that case, you simply have to accept the drawbacks because you rely on the advantages. I have written programs (prime numbers) where I don't understand anything about their space usage, but that's because I had abused lazy evaluation beyond any reasonable notion of sanity. By the way, concerning lazy IO, it was never meant as a feature to be used in "serious software". It's awesome for prototyping, but if you are serious about handling errors in input files, you should not use it. 
TIL about foldr.com That has to be one of the smallest webpages my browser has ever downloaded (see source).
&gt; I think that these issues are minor and that you have to weigh them against the benefits of lazy evaluation (modularity: short-circuiting foldr (||), infinite lists, first-class control structures, memo tables &amp; dynamic programming, etc.). Oi. I already said codata is no problem in an eager language and that lazy lists are actually a bad default; a separate stream type is better. "First class control structures" require nothing more than a nice syntax for creating thunks.. even Smalltalk had this. Dynamic programming is perfectly doable in an eager language. None of these are advantages of lazy evaluation *as a default*. I don't know why people keep unthinkingly repeating these falsehoods. &gt; What I'm saying is that if you encounter a situation were lazy evaluation is problematic, it is straightforward to switch to an eager style But it's not; that's the point. Tracking down space leaks can be difficult. 
Wait'll you see http://foldl.com ... I'll give you one guess what it does :-)
Ok. I played with this a bit more. Switching to a left fold speeds things up greatly and cuts stack usage. And either lengthL or sumL (with left fold) use constant space but the two together leak, which means there's sharing, unlike the example I showed from stackoverflow. So I looked at the source for ListT and now I just think that there's no reason, given the definition of `enumFromTo` and `cons` for `ListT` to imagine that `Forget` could *possibly* eliminate sharing as you imagine. enumFromTo :: (List l, Enum a) =&gt; a -&gt; a -&gt; l a enumFromTo from to | fromEnum from &gt; fromEnum to = mzero | otherwise = cons from (enumFromTo (succ from) to) instance Monad m =&gt; List (ListT m) where type ItemM (ListT m) = m runList = runListT joinL = ListT . (&gt;&gt;= runList) cons x = ListT . return . Cons x So yes, you haven't built up a data structure directly, but you've built up a huge chain of functions you call to get bits of your data structure, and those functions are shared. A simple heap profile reveals this, in that the bulk of the "pyramid" is used by PAPs -- i.e. by partial applications, which is to say, functions. The problem is that you're not "forgetting" anything -- just delaying some thunks. Another way to look at it is asking why `ListT` isn't iso to the branching stream comonad. They clearly aren't, but thinking through exactly how is an interesting problem.
&gt; None of these are advantages of lazy evaluation *as a default*. I don't know why people keep unthinkingly repeating these falsehoods. The point of lazy evaluation as a default is that these advantage become baked into the language; you can reuse the *ordinary* `||` and `foldr` to get short-circuit behavior. Maybe you don't value that, but I do. Parsec may be a good example. At one point, it makes crucial use of lazy evaluation, but the code looks so natural that it may take you a while to discover where exactly. (It's described in the paper.) If you were to write Parsec with eager evaluation as default, you would write an entirely different library. It's not a matter of selective choice anymore, it has become a way of thinking. &gt; &gt; What I'm saying is that if you encounter a situation were lazy evaluation is problematic, it is straightforward to switch to an eager style &gt; But it's not; that's the point. Tracking down space leaks can be difficult. Let me reformulate that, then: it's easy if you think eagerly to begin with. Sure, tracking down space leaks may be difficult if you have started with idiomatic Haskell, but then you already got a good trade for it. Again, the trade may not sound good to you, but it is for me. If anything, I would kindly ask to not spread fears about price of lazy evaluation as a default. Sure, there is a price but it's not too steep and I'm happy to pay it. 
&gt; you can reuse the ordinary || and foldr to get short-circuit behavior. What's an "ordinary" `||`? Just have it take thunks and provide a concise thunk syntax. Seems pretty ordinary to me. Again, worked fine in Smalltalk. `foldr` is a fair enough point, but it's such a narrow advantage that it hardly outweighs all the disadvantages. It's also worth considering that a `foldr` with the explicit ability to abort early makes for clearer code. In either case, I do think this is the biggest advantage of laziness as a default. It just doesn't seem to be nearly worth it to me. &gt; If you were to write Parsec with eager evaluation as default, you would write an entirely different library. Nonsense. You'd add some explicit laziness. There are many Parsec clones available for eager languages that provide very similar interfaces. &gt; If anything, I would kindly ask to not spread fears about price of lazy evaluation as a default. Sure, there is a price but it's not too steep and I'm happy to pay it. You may be happy with it, but I maintain it's the wrong default. It's not the end of the world; I still use Haskell. It is, however, one of the worst things about the language.
I agree with you, but maybe the next gen haskell will be strict! And in that case, the lazyness will have helped to make a better Strict Haskell.
I have used Haskell extensively. Finding errors later than I expected never was a problem for me. A much bigger problem is the lack of a good stack trace when you get an error. 
I'd argue that any evaluation strategy that results in errors when computations are forced rather than when they "actually" occurred — and hence, one that disallows a proper stack trace — causes errors to happen "too late" almost by definition. That said, I have had a couple of cases with assertion-littered code not signaling an error until a later stage of a compiler pipeline where an eager language would have signaled it immediately. Speaking of which... where can one find an implementation of Cayenne nowadays? I've searched and come up with nothing. All I have is my memory of the pepper gif.
&gt; What's an "ordinary" ||? Just have it take thunks and provide a concise thunk syntax. I mean the `(||)` that uses regular `Bool`, whatever regular is. &gt; &gt; If you were to write Parsec with eager evaluation as default, you would write an entirely different library. &gt; Nonsense. You'd add some explicit laziness. There are many Parsec clones available for eager languages that provide very similar interfaces. Ah, I mean the implementation, not the interface. Its use of laziness is natural in a lazy language, but far from obvious in an eager language, which means that the eager version would have very different code. &gt; but I maintain it's the wrong default. I still use Haskell. It is, however, one of the worst things about the language. I maintain that it's the right default and one of the best things about Haskell. 
I agree. I've considered writing such a tool but I haven't found the time yet. My idea would be to do strictness analysis on the AST and look for patterns (e.g. recursion with a non-strict accumulator with a "small" type, like `Int`)
But what if you have a monotonically growing corpus of works published already; your technique requires that you access the full text of each document for each comparison, while n-gram comparison can be done with a database of n-grams, and even without (relatively simple) optimization, the entire corpus needs only be searched when plagiarism is suspected.
Well, certainly Haskell is never going to be made a strict language... it would be far too big a change, and I suppose would break the *majority* of existing packages. If you desperately want a strict language, you're probably better off looking for alternative languages.
Is anyone else struck by how fluffy-lite the IT projects sound in comparison to some of the other projects? "Chris created a website [and] wants to help emancipate information around the world." "Sebastien [has] the goal of finally realizing the potential of computers". Heh.
Success! Ooops. *Avoid Success*. Okay... At least DPH is not quite done.
Hipster Haskeller points at bug #2102, around long before #2715 here :)
Can someone translate this into "no hablo type families" language, please?
A type family is just a function at the type level. Why is that useful? Well, consider this: -- these types can't be constructed and exist only at the type level data Zero data Succ a -- here's a list with the length encoded in the type data Vector n a where Nil :: Vector Zero a Cons :: a -&gt; Vector n a -&gt; Vector (Succ n) a -- PROBLEM: what is the type of this function? append Nil v = v append (Cons a as) v = Cons a (append as v) Type families give you a way to solve this problem: type family Add x y type instance Add Zero y = y type instance Add (Succ x) y) = Succ (Add x y) -- this type checks as of ~GHC6.10, I think. append :: Vec x a -&gt; Vec y a -&gt; Vec (Add x y) a Now, lets say I do 'append v Nil', where v has type 'Vec n Int' for some n. This has type 'Vec (Add n Zero) Int' Ideally I'd like to say that (Add n Zero) is n. But I can't--there's no rule that gives me that. And in fact, that isn't a sound thing, since type families are open, I can always write a nonsense entry like type instance Add Int Zero = Char and now Add Int Zero /= Int. I can prove this directly using the structure of Vec: vecPlusZero :: Vec (Add n Zero) a -&gt; Vec n a vecPlusZero Nil = Nil vecPlusZero (Cons a as) = Cons a (vecPlusZero as) However, this reconstructs the vector in memory when I call it--not cool. I *can* classify some types as numbers using a type class, though: class Nat n instance Nat Zero instance Nat n =&gt; Nat (Succ n) Now, this doesn't give me anything immediately, but if I could tell the compiler what is required to be a Nat, I could use that: -- this next line isn't supported until this change goes in class (Add n Zero ~ n) =&gt; Nat n instance Nat Zero -- valid because Add Zero Zero ~ Zero instance Nat n =&gt; Nat (Succ n) -- valid because -- 1. Add n Zero ~ n -- superclass of Nat n -- 2. Succ (Add n Zero) ~ Succ n -- apply Succ to both sides of (1) -- 3. Add (Succ n) Zero ~ Succ (Add n Zero) -- type instance Add -- 4. Add (Succ n) Zero ~ Succ n -- transitive of (2) and (3) Now I can write vecPlusZero :: Nat n =&gt; Vec (Add n Zero) a =&gt; Vec n a vecPlusZero v = v 
Wow! Can't wait to finally try the playable version of the Stunts remake!
An introduction: http://haskell.org/haskellwiki/GHC/Indexed_types
So this is going to be in GHC 7.2, right? Very nice! =)
http://contemplatecode.blogspot.com/2011/06/haskell-weekly-news-issue-187.html
Annoying that one has to declare all the identities with the type class.
It’s hardly a game in its present form, so don’t expect a high fun factor. ;) For the time being, it is primarily a test application to exercise all the packages together.
For future reference, downloading Debian is a matter of hundreds of megabytes, not gigabytes -- use the netinstall CD.
Core Wars is still played today.
Thanks, your explanation of what type families are, and what problem we are trying to solve, is clear and very helpful. However, I got lost when you started using "superclass equalities" syntax without explaining what it means. It looks like class (Add n Zero ~ n) =&gt; Nat n means "you can only declare `n` to be an instance of `Nat` if `Add n Zero` is the same type as `n` and the compiler knows how to prove that they are the same". Is this correct?
Yes. That's correct.
To put it in the language of Functional Dependencies, you can now do the equivalent of: class F a b | a -&gt; b where class F a b =&gt; C a b where
Does this have the power of C++ partial specialization and `std::enable_if`?
Exactly so. If you do speak type families (or GADTs) even just a little, we've been able to add these equality constraints to other functions for years; this change just means we can use them to restrict valid typeclass instances (just like we've always been able to use typeclass constraints).
Yes, that's correct. Just like class Functor f =&gt; Applicative f means you can only declare f to be an instance of Applicative if the compiler can prove that f is an instance of Functor. Actually, using only (~) and existential types you can get the full power of GADTs from regular data types: data Vec n a where Nil :: Vec Zero a Cons :: a -&gt; Vec n a -&gt; Vec (Succ n) a is the same as data Vec n a = (n ~ Zero) =&gt; Nil | forall p. (n ~ Succ p) =&gt; Cons a (Vec p a) You can see how this works when you do pattern matching: mapV :: (a -&gt; b) -&gt; Vec n a -&gt; Vec n b mapV f Nil = Nil -- pattern match on "Nil" brings (n ~ Zero) into scope, -- so we can output Nil :: Vec Zero b mapV f (Cons a v) = Cons (f a) (map f v) -- pattern match on "Cons" brings a new type 'p1' into scope, -- with (n ~ Succ p1), a :: a, and v :: Vec p1 a -- map f v :: Vec p1 b -- f a :: b -- Now to call Cons (f a) (map f v), we fill the existential type 'p' -- with the type 'p1' from (map f v), and then we need to show -- (n ~ Succ p1), which we have from the Cons pattern match. 
Although the exact form here wasn't possible, it was possible to write: class F a where type T a class F a =&gt; C a where since the only allowed instances for `C a b` are with `b ~ T a`, we could just use `T a` instead of `b`. However, the fundep example: class C a b | a -&gt; b, b -&gt; a where ... was previously impossible to encode, since it needs to be: class (T1 a ~ b, T2 b ~ a) =&gt; C a b where type T1 a type T2 b
does this mean we have equality constraint synonyms too now, like: class Map a where type Elem a, Key a find :: Key a → a → Maybe (Elem a) type family IMap a type instance IMap a = (Key a ~ Int, Map a) , from the "Practical aspects of evidence-based compilation in System FC" paper?
It would be good if someone could collect all of the various type-level hacks people have been using to work around the lack of superclass equalities and say how to transform them into nicer form.
I think you accidentally the URL.
Highly recommended. I wish I could join again this time, but Japan is a bit further away: Jetstar flights from Osaka are already booked out too anyway. Will join irc and try again for next year: guess some of you may be coming to Tokyo too. :)
Think this might be it http://www.yesodweb.com/blog/2011/06/yesod-docs-haskellwiki
Thank you.
GRIN is the intermediate language used by JHC and LHC.
I nominate you. My main hack has been to move entirely to one or the other, and failing that to move all those equality constraints down to all of the members of the class. e.g. in my representable-tries package, I would have rather had class (Adjustable (BaseTrie a), TraversableWithKey1 (BaseTrie a), Representable (BaseTrie a), Key (BaseTrie a) ~ a) =&gt; HasTrie a type BaseTrie a :: * -&gt; * than class (Adjustable (BaseTrie a), TraversableWithKey1 (BaseTrie a), Representable (BaseTrie a)) =&gt; HasTrie a type BaseTrie a :: * -&gt; * embedKey :: a -&gt; Key (BaseTrie a) projectKey :: Key (BaseTrie a) -&gt; a and a separate data a :-&gt;: b where Trie :: HasTrie a =&gt; BaseTrie a b -&gt; a :-&gt;: b because the latter requires me to plumb them through and use them all over the place. In practice it worked out okay, because new instances can re-use the same BaseTrie type with different embeddings/projections, which makes it really cheap when working with newtypes, but it did require a ton of plumbing!
[Related blog post](http://chrisdone.com/posts/2011-06-25-haskell-design.html) I like this guys idea, it's kinda nice and brings a more friendly feel to the site. 
This design is nice, but the examples at the bottom kind of stick out like a sore thumb. Not sure what I'd like to do with them.
I like it. +1 for me.
Design looks awesome. `print (*2) [1,2,3,4,5]` ain't going to work though :-) 
The following should be added to the CSS code: background-color: white; On my computer, under the background image, the background is black without that, because of my colorscheme. I have added a custom CSS stylesheet in my browser to change that because a lot of web pages assume a white background by default, but it looks like it didn’t work on this one (though adding it in the page does fix the problem − I have tested).
Wish I could upvote twice!
Hmm, I have to be the dissenting voice and say I like the current design better. I wouldn't mind seeing the "try haskell" widget on the front page if it's doable, but at least a few darker colors as in the current design is nice. If I look at the new one from an angle on my LCD screen, it looks like maybe there's supposed to be some light shading, but it's just text on white unless I look very closely.
I agree. The blue is a little too light for my taste and makes the site look somewhat weak. I would prefer the current richer and bolder design. 
How the heck did I manage *that*? Thanks for pointing it out.
 dexter `gave` harry $ 2000 I think this example could disregard hlint and just go with: dexter `gave` harry $2000
I really like it. The color palate is wonderful, and the spaciousness feels a lot like Haskell's succinct code--- as opposed to the current design which is more corporate, more "we're srs bsnss". The current design is good for fitting into the style of the pages for Python, Ruby, etc; but I've always been a fan of minimalism.
And they said we would never need a "printmap" function... :-)
Yeah I guess the design might be a little off there, but the idea of putting them on the front page is an excellent one and is definitely appreciated
I agree about the colors. But I think the concept as a whole is better than the current design.
Agreed. I love the current one! Though this one's quite nice too, I don't think there's any need for change. Change too often and give an impression of instability.
I prefer the color scheme and fonts from the current site, but I like that the mockup is cleaner, content wise. On my screen the mockup looks washed-out and I think the sans-serif fonts in the current design look better on screen. I like that you can try Haskell directly on the front page in the new design. Although I don't know what's intended with this new design, I'd prefer that the current design was improved using the good points of the new design, instead of throwing the current one out and putting something else in.
As the "designer" of the current layout I'm certainly biased, but I do find this proposed design quite hard to read. There's certainly room for improvement w.r.t. the content. Choosing the things to go onto the front page is the simpler part. The harder part is writing the right wiki pages to point to. The current front page mainly looks the way it is because I had to fit the content to the available wiki pages. Finally, some MediaWiki hacking is required which isn't fun, either. (It's PHP.) In short: There's certainly room for improvement of the content design, but it will require some effort.
I'm not sure what you mean by the concept. Aside from the change in colors and fonts and visual layout and all that, the major changes seem to be: * Removal of a lot of complete sentences. I'm opposed to this. * Removal of the list of hackage updates. I'm very much opposed to this. I can see replacing them with a different measure that, say, combines popularity and recent updates in some weighted measure; but having those there *both* helps people stay up to date, *and* makes the real-world side of the Haskell community more visible. * Addition of the Try Haskell widget, which could be really great, assuming it would scale to the number of people (now, and after reasonable future growth) who might visit the page and play around. * Devoting a gigantic chunk of the page to a list of one-liner examples in beginner tutorial style. I'm opposed to this, as I think it just feels out of place, and reinforces Haskell's image (still quite prevalent in many places) as a toy language that's nice for computing fibonacci numbers and primes and not much else. The gain from moving this to a second page far outweighs the difficulty of a mouse click.
I think that the money was in cents, but that's still funny. 
how can you make a homepage without an embedded youtube video and facebook like button??
Yes, and haskell.org is also sorely lacking in flash intros and animated road construction sign GIFs.
Here are some of my thoughts, based off of yours. &gt; Removal of a lot of complete sentences. I'm opposed to this. I disagree, in terms of a homepage. When skimming, most people on the internet don't read anything more than a bullet point. Elaboration can go elsewhere. &gt; Removal of the list of hackage updates. I'm very much opposed to this. Agree. But I think the hackage feed on the current design is a bit too big and noisy. &gt; Addition of the Try Haskell widget. Good stuff. &gt; Devoting a gigantic chunk of the page to a list of one-liner examples in beginner tutorial style. I'm opposed to this, as I think it just feels out of place, and reinforces Haskell's image as a toy language. Indeed. The examples are neat since they are a bit more "real world" (like JSON encoding) rather than the primes, but I think the "Try It" box would serve better as the location to try little things, instead of than an entire section for it. (I.e, small links that inject cute snippets in the Try It box or so.) ---- In terms of design, I like this sketch because it is not afraid to use whitespace. I think the current design is a little too packed and noisy. Another issue with the current design is that it hits the "layered cake" issue, meaning that when there are too many full-width vertical divisions, it starts to looked like a cake. The sketch avoids it. Finally, I think the color palette is a little more sane, but that is of course just personal taste.
But we've got a nice orange Web 2.0 Button!
Cool idea!
May be it's time to move haskell.org to a haskell web framework? People laugh when they know that haskell.org runs on Apache/PHP.
What libraries in other languages do you use or would use if they were easier to learn and handle? 
 Prelude&gt; :t (print .) . map (print .) . map :: forall a b. Show b =&gt; (a -&gt; b) -&gt; [a] -&gt; IO () Prelude&gt; :t mapM_ . (print .) mapM_ . (print .) :: forall a b. Show b =&gt; (a -&gt; b) -&gt; [a] -&gt; IO () 
&gt; When skimming, most people on the internet don't read anything more than a bullet point. If someone visits haskell.org and can't be bothered to read the clear couple sentences at the top that start with "Haskell is...", and then leaves wishing they knew what Haskell is, then I'm in favor of their finding a different favorite programming language. It's not an essay; it's three sentences. In any case, the slow death of the English language by low standards is a real shame. Perhaps we can afford to leave three complete sentences on the page somewhere, just for the warm fuzzy feeling it gives some of us? &gt; The examples are neat since they are a bit more "real world" (like JSON encoding) My impression is that they are faux-real-world. My first question, upon encountering a programming language, is generally not of the form, "I wonder how easily I can generate some JSON, if I don't even care about the structure or field names and am happy with a library choosing those on my behalf based on something arbitrary like variable names in my code." In fact, I can't remember the last time I wanted that. It feels like a cheap trick that is unrelated to the real reasons Haskell is pleasant to use. This probably contributes to my impression that the examples feel like toys and distract from the real-world uses of Haskell. &gt; Finally, I think the color palette is a little more sane On this new one? As far as I'm concerned, the color pallete looks like a blank white screen with text on it. As I mentioned elsewhere, if I look at my LCD screen from an angle where all the colors are inverted, I can see some boxes and a radial gradient... but I literally have to stand up and choose the right oblique angle to see it.
I'm in this camp too. While the proposal is certainly nice... it feels a bit thin to me. I *do* like the treatment of the logo a bit more than the current page. What both pages are lacking are graphic texture. They are both very flat text and color. Both would benefit by either a subtle background stippling, and/or photos or images. Not that I'm after stock photography of happy young programmers... nor nerdy Mandelbrots (please, no...) but something to give the page a little visual interest.
Regarding those leaks: dexter `spent` 5300 The “‘”’s seem fairly harmless, if irksome dexter `gave` harry $ 2000 Here the “$” seems catastrophic, if the amount is in cents not dollars. This syntax makes the DSL fight against the domain. We're told: &gt;We could get rid of [the leaks], but the plumbing would get a lot more convoluted. We also avoid using floating point numbers for a similar reason Well, the downvotes are going to come thick and fast, but this seems to be evidence _against_ Haskell as a good vehicle for this eDSL. The authors claim that &gt;The first reason to use haskell for an eDSL is that it has a very clean syntax which _is_ attractive, and yet in writing an example of how great this is they actually find an eDSL with a better fit to the domain too difficult to implement. I'd like to see more examples of genuinely practical programming in Haskell, but this doesn't seem to be one. 
I'm not sure I'd go with such hyperbole as "death of the English language". We can sit by and be language snobs and not attract otherwise intelligent people who want information quickly and efficiently and don't want to be distracted by needless filler words that are a historical accident (much like the syntax we love to hate in other programming languages). Or we can write our beautiful essays and papers on blogs and details pages, and do our best to hook new readers on the front page with the information they seek. The homepage isn't designed for experienced Haskellers to read, anyway; it's for newcomers who want to see what the big deal is all about, isn't it? When was the last time you went to haskell.org? I for one quite like the new design, from a layout perspective. The current design is nicer than the old one but is still rather cluttered and packed with irrelevant information, in my opinion. We want to make a good first impression on newcomers, and get them "hooked" as quickly as possible so they'll go on to read other parts of our website. A wall of text (yeah, I know the only full paragraph is at the top, but I'm talking about the overall feel of the page) with several columns and big lists of packages they've never heard of isn't going to hook anyone. I do agree that Chris Done's design could do with a bit more "texture" somehow, and a tiny bit more color wouldn't hurt, but I like where it's going more than the current design.
I'm not trying to start a flamewar but declaring that people who "can't be bothered to read a few sentences shouldn't be using Haskell" is a bit harsh and automatically reinforces the image that Haskell and Haskell users are elitist. This argument is all a bit useless anyway, the design proposed above was just done by a guy in his spare time and the only reason I posted it on here was because I felt some of the ideas he had come up with were good.
I like the current design a lot, I think both designs could take a lot from each other
Agreed. Problems that I see with the examples: * The examples section cannot be copy-pasted into a Haskell module as it stands * The examples probably won't work in the Try Haskell box, due to IO * People who don't know (or need) JSON are merely baffled * People who do know JSON still don't learn anything about Haskell except that there must be some kind of support or library for it (in Haskell?) * If I am curious about a new language then I would probably want to see the implementation of a simple function, rather than merely an example of how to call a function. That said, I do like the design principles *and* the colors :)
And also UHC.
Alas, no good libraries exist for linguistics in *any* language! :( Also, thread necromancy.
The Ruby website has a single little snippet of code for a little hello world program. It gives a nice impression of the language without being too obnoxious, nor too out-sticking.
Take a look at the official Scala website for an example of 'stock photography' being littered all over the place!
 dexter `gave` harry $20.00 
&gt; I'm not sure I'd go with such hyperbole as "death of the English language". When I wrote it, I would have agreed it was hyperbole; but now we're having a serious debate over replacing three very well-written sentences that make a coherent point with a list of bullet points that don't communicate nearly as well. It's not just any three sentences; if they were fluff, I'd suggest removing them. But take a minute to read them -- it's a really well-written introduction that leaves you with a good feel for what Haskell is about. It's the sort of thing that sounds good, and that really couldn't be talking about any other language except Haskell. That's a real gem. Replacing it with bullet points like "statically typed" and "purely functional" and such doesn't do us any favors. In any case, the bigger point I wanted to make here was that if we try too hard to make a page that's intended to appeal only to beginners... well, we end up with a page that's obviously intended to appeal only to beginners. But that's *not* the best way to *actually* appeal to a lot of real potential Haskell programmers. They aren't yet looking for a tutorial on taking their first baby steps in the language (or if they are, they can click the links at the top of the page that point to various tutorials of different levels of detail). Instead, they are looking for evidence that this is a tool to be taken seriously; that there are people doing significant things with it, that it's an active and long-lasting community... basically, that there's some substance here. My biggest concern with the proposed new design is that I could build almost the same page for a toy language I build this weekend with no actual users or projects, and that when we're presenting the language to people who are likely to have never heard of it, we need to answer those concerns. That, and I just visually like the current page better. I can see it, for one, and it feels a lot more solid.
I apologize if that came across as flaming. I didn't intend that at all. I had a more joking, humorous tone in mind; if that didn't come through, I'm sorry.
And now what are you going to do for the overwhelmingly vast majority of the world's population who don't record their expenses in any sort of Dollar? This month I will enter expenses in GBP, CHF, EUR and NOK. How does your tricksy pun work out then?
glad you asked. infixr 0 € (€) :: (a -&gt; b) -&gt; a -&gt; b (€) = ($) hans `gave` adolf €2000 works with pounds too, don't care for chf and no idea what nok is. edit: with more types you could even handle different currencies
I expect no less than 100 comments in this bikeshed discussion. At least 20 on the color selection alone. 
Guilty as charged. What tasks would you want these hypothetical libraries to facilitate?
One easy way to avoid the "$" would be to define the DSL as: dexter `gave` 2000 `to` harry Coming up with a good EDSL in Haskell without the backticks is a lot more difficult, though. 
I immediately tried to try Haskell. Imagine my disappointment when I realized that this is just a picture.
Modeling, data mining, statistical analyses, diagramming, etc.
Very cute!
&gt; Removal of the list of hackage updates. You mean the list of hackage updates that, as of today, is over 2 months old? It's a nice idea, but not part of the site that's ever worked too reliably.
I wish the article showed an equivalent parser using arrows, just for contrast.
Incidentally, while this article talks about using `Arrow`, it references a paper by Swierstra and Duponcheel that, if I'm reading correctly from a quick glance, talks about using (what would now be called) `Applicative`. In both cases, the benefits come from carefully removing the same piece of functionality offered by `Monad`: the ability to let the *structure* of future computations depend arbitrarily on the result of earlier computations. With arrows, this is done by avoiding application--constructing a model with only first-order functions, roughly. Values produced by arrows can be combined, but arrows can't themselves produce new arrows to combine with other arrows. With `Applicative`, this is done by treating each "layer" of the functor uniformly. The original functions are necessarily oblivious of the functor, and multiple structures are combined in a predetermined fashion, so at no point can the structure produced depend on the run-time values. In both cases, adding only that one bit of functionality, an application arrow or flattening of arbitrary nested structure, immediately gives you something exactly equivalent to `Monad`.
&gt; Here are some run times on a lightly loaded 2Ghz Xeon [...] This parse is clearly impractical if parsing 30 lines of text will take over 1.5 hours Without checking the details, something must be seriously wrong here. I mean, even the most stupid algorithm should finish with *30 lines of text* in a fraction of a second. Furthermore, I actually parsed ridiculously large files (several gigabytes) with monadic parsers, and I don't recall being very clever about it (apart from strictness annotations on the data types).
Also interestingly, if something forms an instance of `Applicative` *and* and instance of `Category`, then it can also form an `Arrow`.
There are some potential problems with monadic parsers, since they do expose some power which certain optimizations might not agree with, but yeah, this example sounds more buggy than demonstrative.
As long as you use `Data.Fixed.Centi` (the article used integers because the author was afraid to use floating point).
Are there any good articles on learning arrows that do not require a PhD in category theory?
And vice versa.
Arrows are equivalent to Category+Applicative, which are more useful classes. I'd say just ignore arrows and focus on Category and Applicative.
Back-tracking parsers can quickly grow to badly exponential time complexities. See this article: http://swtch.com/~rsc/regexp/regexp1.html It shows a pathological regular expression case: &gt; Let's use superscripts to denote string repetition, so that a?3a3 is shorthand for a?a?a?aaa. The two graphs plot the time required by each approach to match the regular expression a?nan against the string an. &gt; Notice that **Perl requires over sixty seconds to match a 29-character string** 
Peaker's advice is pretty sound, I'd say. And besides, all you really need is a basic introduction to Category Theory, not a PhD. The `Category` type class is named that for a reason, and is most of what really matters. Though, take note that instances of `Category` are actually type constructors for the arrows of the category... whereas `Arrow` adds additional stuff. Not really the ideal choice of names I think. Anyway, `Arrow` itself and the associated variations are mostly things bolted onto `Category` in a somewhat ad-hoc fashion, but they're pretty self-explanatory from the types once you get a feel for what the core idea is.
Yes, I should have said that.
And in my opinion, it makes the Arrow class uninteresting. Category and Applicative are simpler, nicer classes -- why bring in a relatively-messy class like Arrow if it adds no structure and no power?
I agree. The most useful stuff from the Arrow hierarchy would be better expressed with bifunctor type classes.
Which isn't too surprising, really. `arr` gives you some sort of functor from Hask to the `Category` instance, while `(***)` and co. lift tuples--which are basically what `fmap` and `(&lt;*&gt;)` do, respectively. I'd have to work through the exact details, but the rough structures are clearly the same.
&gt; I mean, even the most stupid algorithm should finish with 30 lines of text in a fraction of a second. Never underestimate the stupidity of backtracking algorithms. Not only would the 30 line file take longer than 1.5 hours, it would take about 3.7 hours: Prelude&gt; log 0.468 - log 0.015 3.440418094815437 Prelude&gt; log 16.166 - log 0.468 3.5421972543761426 Prelude&gt; log 466.365 - log 16.166 3.3620583180271257 Prelude&gt; exp (it + log 466.365) 13453.935000927891 Prelude&gt; it / 3600 3.7372041669244145 A 45-line file would take over 10 years. Prefer deterministic parsing.
Can you write down or reference the construction for this, please?
Note that an instance of `Applicative` will work like the usual instance for `((-&gt;) t)` in whatever `Category`. So `fmap` gives you function composition (implicitly lifting the function into the `Category` at the same time), while `(&lt;*&gt;)` is function application lifted to the target of two arrows. So, off the top of my head, I think something like this would work: arr f = f &lt;$&gt; id f *** g = (,) &lt;$&gt; f . arr fst &lt;*&gt; g . arr snd 
Ah, thanks.
http://hpaste.org/48387
If you have `Applicative ((~&gt;) a)` and `Category (~&gt;)`, then you can write an instance `Arrow (~&gt;)` with: arr f = f &lt;$&gt; id first f = liftA2 (\x -&gt; (,) x . snd) (f . arr fst) id Also, if you have `Arrow (~&gt;)` then you can write `Functor ((~&gt;) a)` and `Applicative ((~&gt;) a)` with: fmap f a = arr f . a pure = arr . const f &lt;*&gt; g = (arr . uncurry) ($) &lt;&lt;&lt; f &amp;&amp;&amp; g I didn't test any of this.
I'd say that you shouldn't need to know category theory to understand `Arrow`s. Most category theorists use the word "arrow" as a synonym of morphism (e.g., function). The only ones I know who don't are those who've been exposed to the programming concept of `Arrow` and are talking about it categorically.
Agreed. I'd much rather if `Arrow` was broken up into separate and more sensible typeclasses. It'd be nice to work with CCCs without assuming that Haskell's `(,)` is the category's product.
&gt; People laugh when they know that haskell.org runs on Apache/PHP. Why? Apache and PHP work fine, and I see no need to roll our own here.
Nice. Do the laws work out as well, or do arrows add additional laws beyond Applicative + Category? I know nothing about arrows, so I would be thrilled if this is the case and I don't have to bother getting around to learning them! But at the same time, I want to be sure there's really nothing new there.
I'm not sure. I hope someone has the time to figure it out :-)
Given that it is a wiki, I agree. However, the haskell.org front page doesn't necessarily need to be part of the wiki, and could be written in Haskell. As a demonstration, maybe?
Agreed.
Interestingly enough, trying to understand `Arrow`s from a category theory perspective is insanely hard, since they are weird. They are 'similar' to Freyd categories, but you actually need to build some extra structure to capture them entirely (it has been called an enriched Freyd category). If you're actually interested in this for whatever reason, check out Bob Atkey's "[What is a categorical model of arrows?](http://personal.cis.strath.ac.uk/~raa/arrows.html)".
I think it's more a case of parsing with particular monads (can be slow). The two-continuation parser monad is a lot faster than "list of successes" monad - there was a paper with comparisons by the Clean group. 
It seems that [some laws are missing](http://www.haskell.org/pipermail/libraries/2011-January/015556.html).
I'd like a standardized `CategoryPlus` type class.
About as informative as "Computing Fibonacci numbers (can be slow)". If you pick an exponential algorithm it can certainly be slow. You can do that with or without monads. 
I would suggest reading John Hughes' original paper on arrows which I think does a decent job of explaining them. No category theory needed. You can find it here: http://www.haskell.org/arrows/biblio.html
Parsing is a programming paradigm, akin to logic programming, that we pull out for specialized applications. A significant part of the popularity of scripting languages is the ease of access to regular expressions. (Controversially; there's the famous Jamie Zawinski quote, "Some people, when confronted with a problem, think 'I know, I'll use regular expressions.' Now they have two problems.") The pattern-matching in ML languages is Lisp with better parsing of data structures. If one believes the model of change put forth in Thomas Kuhn's "The Structure of Scientific Revolutions", it would appear that many of us are itching for a paradigm shift where parsing becomes the primary model of computation. Yet people tend to view writing a parser as specifying a problem, as if the implementation will provide all of the smarts to insure it runs fast, rather than coding a computer program, where the responsibility for efficiency rests with the programmer. Monadic parsers are strictly more powerful than alternatives, but one can nevertheless write fast code using monads, just as one can write a linear time algorithm on a Turing machine. A traditional, industrial strength old-school parser digests and optimizes the parsing specification as a whole, akin to whole-program optimization of a Haskell program. (However, our optimizers don't rewrite our parsing code for us.) One advantage to applicative parsers is that the restriction makes it possible to collect the parser specification for global post-processing, like a traditional parser. Of course, simply using "applicative" doesn't write this code, it's merely an option if one is willing to do a lot of work. Monadic parsers tend to run exactly as written. Monadic parsers are a form of programming language, and support good or bad code, at the programmer's discretion. 
Nitpick: You can't copy/paste to LHS because the code sections are in "&lt;pre&gt;" with two spaces before each line...
Hm, I'm pretty sure at least *some* of those laws are easily implied by the laws for `Functor`, `Applicative`, and assuming the obvious laws for `Category`. The tricky part is mostly the behavior of `(***)` when being lifted through the functor and ensuring that the sequencing for the `Applicative` effects is correct.
I used the `Random` monad to implement [bogosort](http://en.wikipedia.org/wiki/Bogosort) on lists, and it was really slow! I blame the monad.
My brain is too fuzzy to grok everything now but I'll give you an upvote anyway ;) BTW, I really liked your talk on lenses in Scala. Keep sharing!
&lt;3
Is it worth buying IDEA for? Or mostly good for those who already have a license?
The project page is short on details but there is a possibility it might run on IntelliJ CE which is free. I use IntelliJ for Java, Scala and Groovy and love it, but this plugin looks so young that I doubt it is worth using yet.
I've been looking at this for a couple days in my spare time, and it looks like at a minimum, you'll need to add an axiom relating composition in the category to the Applicative structure somehow. Something like u . v = pure (.) &lt;*&gt; u &lt;*&gt; v Without that, trying to establish most of the arrow axioms via functor and category axioms is just a non-starter, because you just can't *do* anything with the composition that pops up. Of course, that's a rather mild condition to add, but it *is* a new condition, meaning the result as stated is almost surely false. If I were to pursue that further, it would be by looking for an example of an applicative + category where that equality doesn't hold, and showing that some of the arrow axioms are false in that specific case. It is the case, incidentally, that Arrow -&gt; Category + Applicative, though establishing the main applicative axiom about `u &lt;*&gt; (v &lt;*&gt; w) = pure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w` was a real pain.
&gt; &gt; Removal of a lot of complete sentences. I'm opposed to this. &gt; I disagree, in terms of a homepage. When skimming, most people on the internet don't read anything more than a bullet point. Elaboration can go elsewhere. I have to agree with Chris here. The current sentences communicate a lot more than the newly proposed bullet points. The bullet points are still visible because most of them are links in the prose and are visually distinct enough to catch the eye as it scans across the page.
I'd happily donate CDN services but I don't think that would help your situation.
Yes, it is not usable yet (i am one of the plugin authors). It is primarily targeted for community edition.
I love it, but please have the bulleted list styled "outside" the box model, because it would look so much better and the text in the bulleted list would match up with "Read more" :)
Are you going to use Scion?
Thank you for this! IntelliJ is by far my favorite IDE.
Currently it does not, since it is slightly under-documented. When the difficulty of GHC interface reimplementation overcomes difficulty of Scion research, then maybe :)
EclipseFP plugin developers lamented recently in a blog about their desire to scrap scion interface and work with cabal directly.
This shows no special traffic load on lambda, which is on a 1Gbps line, http://lambda.galois.com/mrtg/localhost_2-day.png not sure what your problem could be.
http://www.haskell.org/haskellwiki/Calling_Haskell_from_C You can also call C code from Haskell. I believe there is a way to call it from C++ but IIRC C and C++ have different name mangling so this method may not work with C++. Worst case you may have to use the whole extern "c" construct and wrap around the GHC generated C code.
I know that Haskell's foreign function interface makes it possible to make C-to-Haskell calls. Unfortunately, I've never used the FFI this way, so I can't tell you anything else.
Step-by-step instructions here: http://stackoverflow.com/q/3859340/371753
Thanks! I intend to use haskell as the problem solver for the program (phatfinding, searches, some number crunching) simply because it is easier to do so in haskell. wether to *port* haskell code to c++ depends on performance. I swear I've searched for 'embeeding haskell' and didn't get any meaningful results :P
Best search term would be "calling haskell from c" or "calling haskell from c++ " (or vice versa). I typed the first post on my phone, so I didn't get to go into as much detail as I would have liked. Anyways, the Haskell FFI (which is what that Wiki page talks about) is good enough to link you to plain old C code, which is OK, but not exactly what you need. To expand upon the name mangling, in C if you have a function like void randomFunction() the name will be something like __randomFunction() in the library. Because the arguments are always the same (as C does not support overloading functions) the naming of a function is the same across implementations. So long as the calling convention is the same, things should work. C++ is different. In C++, functions can be overloaded, so just knowing the name of the function isn't enough. You have to know the arguments it takes, too. Enter name mangling. \__randomFunction isn't good enough. Now it becomes __[implementation defined string]randomFunction. It could use the same calling convention, but linking is different because the same function now has a different name than the C version. There's also differences because of exceptions and the like that require main() to be compiled by a C++ compiler. Because of this, you *may* need to wrap the GHC generated header files with this: &gt;extern "C" { #include &lt;header_for_something.h&gt; } Anyways, GHC in the Wiki page is actually calling GCC (in essence) but with the Haskell runtime specified. When your code is actually C++, you need to call g++ instead. Your steps would be something like: 1. Write Haskell code with -O flag (means it just produces object files, no linking is done). 2. Compile Haskell code 3. Write C++ code that calls the stub 4. Compile the C++ code with the -O flag. 5. Call ghc on your object files with -lstdc++ added (tells it to link the C++ runtime). 6. ??? 7. Profit. I think that's the steps anyways. It's been a while since I've played with Haskell, and I may have told you something wrong, but I'd be happy to help you play around with this. 
That's interesting, we're doing the exact same thing at my job (using Warp instead of Snap, but same concept). I'm using some Template Haskell tricks to embed the files into the executable itself for simple deployment. I think the relatively low footprint of Haskell versus Java for this kind of thing is a huge win; the main competitor in this space is really Eclipse help, and that thing is a massive 30MB download with non-trivial setup. __Edit__: [Relevant blog post](http://www.yesodweb.com/blog/2011/06/building-better-chm)
&gt; Are the SMP optimizations (from GHC) kept when creating a library? The SMP stuff comes from a threaded runtime. You can choose which runtime to link your module's object code against when you create your library. I put this template together a while back, based on "Buildings plugins as Haskell shared libs": https://github.com/solidsnack/hso/tree/master/well-typed 
Yep, looks pretty similar. Our problem is easier, since we get told what browser the user wants to preview in and where their files are and just serve them plus some scaffolding. All in all, even with scanning for free ports, exiting after an idle period, launching the browser, fixing the caching, etc., the Haskell server came out to about 25 lines of code + comments... much to the satisfyingly stunned shock of certain other members of the team. :)
This kind of thing happens on a regular basis at my company. "We have this 1000 line XSLT + Java code that's buggy..." "Oh, that's 12 lines of Haskell." "Oh." We're not doing really well at avoiding success, are we?
I am using yesod in production. Yeah, we live in a future. 
Very nice read, and performance improvement is a great kicker!
Three months late, but what the hell just happened? o___________o
The thing that frustrates me is the maintenance angle. I've produced some extremely elegant, stable and fast Haskell programs to solve problems that needed to be solved quickly at work. Invariably once we make the deadline someone starts talking about how the Haskell solution is great but no one understands it, so won't I please rewrite it in C++/Java/C#? No is the answer to this question (I'm not a programmer, technically, so I tell them I have other things to do) but the result has been growing hostility to quick Haskell hacks. Especially when the actual devs take three weeks to produce a C++ replacement that crashes half the time and leaks memory. I wish I could claim to be exaggerating. But then I work in finance (I'm a quant) so it's perhaps a more technologically conservative environment than many others. I've heard similar stories from Lisp and (not so much now, but 10 years ago) Python devs. Minority language blues...
Hmm... my thoughts on that same paragraph: &gt; advanced This is a statement of Haskell's approach to programming. It actually makes Haskell somewhat unique among trendy modern programming languages. Python certainly wouldn't call itself advanced -- indeed, in the comparable text its web site, it instead makes statements about how you can learn and use it quickly. &gt; purely-functional programming language. Okay, this says what it is. I do think somewhere, the page ought to say it's a programming language. &gt; An open-source product This is an attribute of the language and the community that's important to me. I've worked in environments where you *buy* the compiler, the major libraries, etc... Visual Basic, C#, C++... for all of those, even if you can get opens source pieces, a good bit of the world is using them as proprietary tools. &gt; of more than twenty years of cutting-edge research Twenty years tells people that Haskell isn't new, that it's not a fad that's going to be dead tomorrow. It isn't one person's research project. I can personally say that in advocating Haskell for variou projects, it's actually been very helpful to be able to say "no, it's not a new language; it's been around since 1991, but it's gained a lot of popularity in the last ten". &gt; it allows rapid development of robust, concise, correct software. These are real statements: * Rapid development ... concise: As opposed to C++ and such. Especially since we're about to talk about robust and correct, it's useful to point out that Haskell belongs among those languages where you write lines of code on the order of Python or Ruby, not C++ or Java. * Robust ... correct: A language focusing on robust and correct software is actually fairly rare. Why wouldn't we point out that Haskell is such a language? A lot of the commercial adoption of Haskell has been driven by using it in high-assurance settings where correctness is important. That makes Haskell a very different language from PHP or Visual Basic. &gt; With strong support for integration with other languages Okay, I don't necessarily get this one either... but other people apparently think it's true. Greg Collins did an interview recently where he specifically pointed out the FFI as an important advantage of Haskell. &gt; built-in concurrency and parallelism, debuggers, profilers, rich libraries I see the complaint about debuggers, and partly agree. I've used the GHCi debugger, but mostly there are better ways to find and fix bugs. The rest of this seems reasonable and meaningful. Hackage has it's problems, but there's a LOT more there than you might expect. Yes, we do need some way to filter out the fluff and abandoned projects from the good libraries... but that doesn't mean there isn't quite a bit of good stuff there. If I compare Haskell to Java for example, then yes I give Java the overall edge, but there are definitely libraries in Hackage to do stuff that I wouldn't have an easy time doing in Java. &gt; and an active community We certainly have an active community. Doesn't hurt to say so, especially when the point of this sentence is to reassure readers that there's a support structure and fallbacks in place when they run into trouble, and that community is a huge part of it. &gt; Haskell makes it easier to produce flexible, maintainable, high-quality software. And a conclusion.
In a dream world you could somehow persuade management the advantages (e.g. speed, terseness of code meaning quicker turnaround time for development etc) and they put a message out to the team that they need to add Haskell to their skillset and then offer training. Of course that would never happen in any enterprise environment, but one can only but dream! 
blog currently down :(
If you're right about the time they loose making a C++ replacement, you should be able to convince them to let you explain/teach a bit Haskell. About finance being more conservative... quite often in job ads where Haskell is mentioned, it is about finance...
Excellent read 
Searching arround on haskell.org found [how to make shared libraries](http://www.haskell.org/ghc/docs/6.12.1/html/users_guide/using-shared-libs.html) which may be more useful, specially considering I'm developing on VS2010. I want it portable, it does compile on linux without modifications (only some preprocesor magic). Thanks for the first leads on how to do this.
&gt; Especially when the actual devs take three weeks to produce a C++ replacement that crashes half the time and leaks memory. Sounds like you need to hire better programmers. If you're in quant finance: you can afford it!
back up :)
OK, time for a native Haskell implementation. For the usual reasons; i.e., mainly because it would be cool. Anyone?
It's not so much about the money (although you'd be surprised how unwilling most companies are to pay good money for developers) but more about the culture. Why would a competent developer want to work in a shop where he'll get no respect, be forced to work ridiculous hours, not get paid as well as he would at say, a Google or a Microsoft (particularly on an hourly basis), and on top of that be forced to hack decade old C++ code bases that implement obscure pricing models for financial instruments? To a developer this is about as sexy as maintaining COBOL. No, I'll tell you what *really* happens in industry. We get a bunch of Java programmers straight out of school who have never dealt with C++ before and mostly are young and not from the generation where manual memory management was the rule rather than the exception. They're then lulled into a false sense of security by the familiar ALGOL syntax and completely ignore how fundamentally different (and brain damaged) C++ is, resulting in code that is unstable and filled with resource leaks. This is a big part of the reason that many of these code bases are being rewritten in Java or C#, no joke. Not that I disagree that getting rid of the C++ is a good decision, mind you. But good programmers are attracted to sexy problems -- sexy by nerd standards, not finance standards. It's really, really hard to find someone who is both good at computer programming and really, really interested in how to efficiently implement a pricing model. Most of the time what ends up happening is the finance nerds (i.e. quants) implement the algorithm quick-and-dirty style in MatLab or something similar, find some academic papers where algorithms are discussed, write up a really detailed spec, and send it off to the dev team who then pretty much implements it without understanding how any of it works or what it's meant to do. Then a massive barrage of unit tests is designed by the quant group to make sure that the C++/Java implementation produces the right answers and is fast enough. Yeah, it's terrible.
I'm not sure that there's a lot you can do on the snippet you've provided. It already consists of several atomic elements, most of them are named well enough that I know what they are doing even though I am not intimately familiar with your library (except "focus", and perhaps even that is obvious in context, no criticism intended), and it's obvious what they are doing as a whole with the hint from the `plainText` name. The only thing that would leap to mind is if you have common subexpressions being repeated over and over, in which case you can pull those out with good names, but without seeing more of the code we can't tell if that's a viable option. The encoding call strikes me as suspicious, that should probably be at a higher level, so you can compose `plainText` together with some other component that probably currently has its own encoding conversion and also shouldn't. But broadly speaking, that looks a lot like the desirable end-point of Haskell code more than a "problem".
 unescapeEntities . stripHtml could probably be given a name like unFormat and IConv.convert "LATIN1" "UTF8" could get a local name like mkUTF8 then you end up with plainText :: L.ByteString -&gt; L.ByteString plainText = append '\n' . L.tail . unFormat . focus . mkUTF8 where unFormat = unescapeEntities . stripHtml mkUTF8 = IConv.convert "LATIN1" "UTF8" That said, I find my Haskell flows nicely when I use 120 char width windows. Like in Java hacking, the foo.bar.baz.bat style tends to eat width quickly. Your function as is looks like the Haskell I am used to seeing.
I haven't look at it, but would it be also a gnu gmp replacement? EDIT: I ask because it seems there is some arbitrary integer representation going on to implement the conversion of floats to strings.
Not coincidentally, I really like Utrecht-style operator alignment: plainText :: L.ByteString -&gt; L.ByteString plainText = append '\n' . L.tail . unescapeEntites . stripHtml . focus . IConv.convert "LATIN1" "UTF8" Couldn't be more clear to me.
Okay, fair enough... I did say (in a different thread) that I like the idea of including Try Haskell on the front page. Perhaps even with some one-liner examples (for example, something like on tryhaskell.org where you can click a link to insert text at the prompt -- but with just five or six carefully chosen examples placed compactly under the widget where they aren't intrusive) But aside from that, yes I do like the current page design better. (As an aside, "bikeshedding" doesn't really apply where we have a nice bike shed and there's discussion of replacing it with a new one. The whole point of the bike shed metaphor is that everyone *agrees* the project is for the better regardless of the details being argued.)
To reply only to the aside here, Wikipedia summarizes Parkinson's bike shed example as &gt; While discussing the bike shed, debate emerges over whether the best choice of roofing is aluminium, asbestos, or galvanized iron, **rather** **than** whether the shed is a good idea or not. [emphasis mine] So that's where I got that definition. (Or rather, I got it from previous reading, which is summarized there.)
I'm confused: if they have such a negative experience rewriting in C++ (et al), why are they having __growing__ hostility to quick Haskell hacks? At my job, they started with a "get permission any time you want to use a non-standard language." We've now moved to the level of "just be careful if you're sending the client executables and they'll want to be able to read the code," specifically because people are being convinced of how much more elegant Haskell solutions are. Maybe we need to start an enterprise Haskell training initiative...
Yeh, I often end up with long lines too, but still I find them readable, plus you can add newlines anywhere you want as shown by sfvisser. Maybe "plainText" can be split up into more logical units, but in this context I don't see much use for abstraction by Higher Order Functions. oddSquareSum can for example easily be written using HOF: squareSumOfAll p f = sum . takeWhile f . filter p . map (^2) oddSquareSum = squareSumOfAll odd (&lt;10000) $ [1..] 
The hostility comes from the fact that the decision to rewrite working Haskell bits typically comes from above, I think. Management is non-technical and when they see a 20 line Haskell program that was written in a few hours they naturally assume an equivalent C++ program should require a similar amount of effort -- maybe even less because C++ is the standard and therefore "better". Then, when it doesn't happen quickly, the poor schmuck tasked with replacing the code gets the blame and the pressure. I think devs typically have an "if it ain't broke, don't fix it" attitude. But then a lot of them are also uncomfortable with Haskell (and OCaml, which I've also used a bit) and aware that if something breaks later, they'll be responsible for it. So they're happy for the help when a deadline looms but once the deadline is past they often feel like it's better for them in the long run to replace something they don't really understand with something they do. I'm sort of projecting here, because to be honest with you I don't really understand the motivation for resisting Haskell in the first place. But one only needs to head on over to /r/programming to see that there a surprisingly large number of day-to-day coders who really *like* programming are downright hostile to functional programming in general and Haskell in particular. Your guess is as good as mine as to why this is.
Well, apart from the fact that it's written backwards.
That's what I do, too. Often, I put an `id` in front of it plaintext = id . append '\n' . L.tail . etc so that the indentation does not depend on the length of the name `plainText`.
Why is this called "Utrecht style"?
Interesting, but I was hoping for a simplified explanation of the algorithms being talked bout.
On a slightly unrelated note. In F#, people have "solved" this by not writing point-free style. They use the "pipe-forward" operator, which is reverse function application: plainText :: L.ByteString -&gt; L.ByteString plainText x = x |&gt; IConv.convert "LATIN1" "UTF8" |&gt; focus |&gt; stripHtml |&gt; unescapeEntites |&gt; L.tail |&gt; append '\n' (|&gt;) :: a -&gt; (a -&gt; b) -&gt; b x |&gt; f = f x For this type of code, I actually like this, where you can just read from top to bottom. There's a good comparison between [Haskell composition and F#'s pipe operator](http://stackoverflow.com/questions/1457140/haskell-composition-vs-fs-pipe-forward-operator).
I often do: plainText = append '\n' . unescapeEntities . stripHtml . otherStuff
Yeah, `focus` is actually already an "alias" for a function which extracts everything between "&lt;pre&gt;" and "&lt;/pre&gt;" in this case. It's not the best of names... The encoding is a fair point. I too feel that it's a bit out of place, and I had just tacked it on there because it worked.
Definitely works, but it reads backwards which I find a bit awkward. Maybe it's a matter of habit, in the end? Coming from an imperative background, I'm used to reading functions left to right, top to bottom.
Well, in the end it *is* this combination of those units that the program consists of. The way I see it, I can either combine them in the pure function or in the IO context of `main`. I prefer the former. :) 
This reminds me of that Dave Barry sketch where he rewrites famous literature for the computer era... "It was the best of times =), it was the worst of times =(."
&gt; Coming from an imperative background, I'm used to reading functions left to right Are you sure about that? I don't think I've ever seen an imperative language that reads from left to right. Maybe an Arabic programming language? This &gt; plainText = append '\n' . L.tail . unescapeEntites . stripHtml . focus . IConv.convert "LATIN1" "UTF8" would be written similarly to &gt; plainText xs = append( '\n', L.tail(unescapeEntites(stripHtml(focus(IConv.convert("LATIN1", "UTF8", xs)))))) in any modern imperative language to the best of my knowledge, leaving it read from right to left. If you want the top down, like an imperative language, I would use a where statement: &gt; plainText xs = f where a = IConv.convert "LATIN1" "UTF8" xs b = focus a c = stripHtml b d = unescapeEntites c e = L.tail d f = append '\n' e Though then you are back to being just as clumsy as an imperative language. I'm guessing you could make an operator with the opposite associativity of (.) in order to improve on the haskell and imperative ways of doing things.
I just put the = on the next line, plaintext = append '\n' . L.tail . unescapeEntites . etc This matches nicely with data type declarations: data Expr = Var Name | Lam Name Expr | App Expr Expr
&gt; Are you sure about that? I don't think I've ever seen an imperative language that reads from left to right. Maybe an Arabic programming language? This [...] in any modern imperative language to the best of my knowledge, leaving it read from right to left. Well, you're right in this example but I wouldn't nest that many parentheses if I was writing e.g. Python, but rather I'd do this: def plain_text(s): s = s.decode('latin1') # find i and j, "focus" on part a substring # ... s = s[i:j] s = strip_html(s) s = unescape_entities(s) return s etc. I wouldn't nest too many calls because 1. you get that huge block of closing parentheses and 2. it reads awkwardly (IMHO). So, while the syntax probably allows it, I'd avoid if I was going "too deep". The first point is obviously not a problem with Haskell's function composition and syntax though. EDIT: And sorry, I realize I wrote "imperative" when I was really having e.g. Javascript and especially JQuery in mind. You'd commonly see code like `$("element").doStuffAndReturnThis().chainingThat().moreAndMore()` which is left-to-rightish (`s.decode('latin1')[i:j].strip_html().unescape_entities()` if you could monkeypatch in Python).
You should read it left to right, top to bottom. You just need to flip the way you think about logic flow. As merehap says, read it like a sentence. Append a newline to the tail of the unescaped entities of the stripped html of the focused conversion of x. Don't think of your code as saying "do this, do that, do the next thing." Think of it as describing a transformation. Then the order is natural. 
Program in APL for a month, that'll cure you of that habit.
pdf warning [citeseerx summary page](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.78.7317)
Anybody point to an English translation?
If this is "gentle", I can't wait to read "Learning Category Theory the hard way". 
hoooly shit, someone who grew up speaking english and knows cat theory needs to rewrite this
this is a question that needs answering
I highly recommend "Categories for the Working Mathematician", by Saunders Mac Lane.
&gt; hoooly shit, someone who grew up speaking english and knows cat theory needs to rewrite this Maarten M. Fokkinga's English is better than yours.
What do you mean? It's in English.
The response to what I originally gave was "While I understand what you're going for, you might consider reducing the verbosity and reducing technical precision to engage and excite a wider audience."
it's short for "you track it". in haskell, indentation level is called track.
It is the style taught by Utrecht University. Don Stewart briefly mentions it [here](http://stackoverflow.com/questions/6398996/good-haskell-source-to-read-and-learn-from).
That's just about possible I suppose, but dmead's point stands.
What point? What's wrong with the English? 
I agree, it seems a lot of Haskell newbies want to define a `flip (.)` and use that. I'm not a fan. I like the order of `(.)` and find code written with e.g. F#'s `|&gt;` much less readable. On the other hand `(&gt;&gt;=)` doesn't bother me as much (although I do find myself using `(=&lt;&lt;)` pretty frequently) so I guess it really just is what you're used to.
It's gentle if you are a mathematician.
I know :) I was just being facetious because it went well over my head.
best 'gentle' intro i've come across is 'conceptual mathematics' by lawvere and schanuel. it starts off with very simple, often finite, examples, and has lots of pictures. 
Heh, heh... yes, it is rather brutal, but unfortunately it is the easiest-to-understand on-line tutorial I've found so far.
it's barely comprehensible.
That's not a problem with the English.
how about shut up?
/s/Give it a world/Give it a whirl
i liked this book too - very readable
What are the knowledge prerequisites for understanding Category Theory? (E.g., set theory?) My problem is that all the tutorials seems to assume certain symbols and terminology are already understood.
&gt; and knows cat theory cat theory? ;)
This title always makes me feel like I'm a total slacker.
This doesn't work in cases where `id` diverges...
Eh? What do you mean?
You need to work at a bank that uses Haskell for everything...
Just a joke. Were I able to give you a case where `id` diverges then adding `id` needlessly would be bad.
It really depends a lot on the author. In the purest case, only a general sense of equational reasoning is necessary. However, while that'll be enough to understand the theory, it's not enough to really understand why anyone should care about it. For this reason, most authors will assume a good deal of familiarity with abstract/universal algebra, linear algebra, topology, set theory, or similar--- so that they can give examples from these different areas to motivate how CT applies to mathematical theories. A good author will be able to provide examples in all those domains, and will set things up so that if you don't know enough in one field you can skip those examples and find ones in fields you do know. IMO, [Adámek](http://katmat.math.uni-bremen.de/acc/) does a good job with this; though *Joy of Cats* is still not a trivial read by any means. [Pierce](http://books.google.com/books/about/Basic_category_theory_for_computer_scien.html?id=ezdeaHfpYPwC) is far gentler and assumes only standard computer science background, though he only covers the very basics of CT.
Are there any of those?
Heh, I'm actually a fan of J. I suppose you are correct; right-to-left is usually most suitable in Haskell when not in do notation or code using `(&gt;&gt;=)` or similar.
Well, with (&gt;&gt;=) and (=&lt;&lt;) there's the order of effects, not only the order of data dependencies. When these effects are imperative (IO), it may seem backwards to read it from right to left. But of course, this happens in imperative languages: last_effect(second_effect(first_effect()), third_effect()) join $ liftM2 last_effect (second_effect =&lt;&lt; first_effect) third_effect EDIT: Forgot last join, otherwise last_effect isn't really an effect at all!
At least one.
Actually, this algorithm was used to demonstrate the ideas behind data parallel haskell. The paper on the topic can be read at http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/ in the section "Harnessing the Multicores". [direct link to paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/fsttcs2008.pdf) It's a really good read.
It sounds like your problem with learning category theory might not be the introduction.
You had me there, don't you make such a joke again! :D
Yes, that's a good point, and I suppose I often use `(=&lt;&lt;)` when dealing with monads like `Maybe`, where I more or less want to forget the monad is there at all and just pretend it's simple composition.
&gt; I’m quite happy with the end result but I’d love to get better visualization, maybe OpenGL. Simple visualization like that is really very easy with OpenGL and GLUT/GLFW. The hardest thing is probably getting it up running and showing a static triangle or something (OpenGL errors are usually "reported" as black screens...)
Yeah, I have to say that when i first saw this algorithm, i was blown away by how elegant it is, and also how much fater it makes an untrivial problem to compute. Really good explenation of it here: http://arborjs.org/docs/barnes-hut
Would torrents make any sense?
It's not incomprehensible by any means, but sentences like these could be rapidly improved by a native English speaker: &gt; The intention is to provide a fairly good skill in manipulating with those concepts formally. &gt; That project would have been a failure without the help or stimulation by many people. That sort of thing is just a niggle and is no comment on the quality of the contents (I haven't read it).
There was a torrent for the Haskell Platform last year, worked well for me and I seeded it several times over. Not sure why it was skipped this time.
Please forward to any good UK students who may be interested in applying.
Both Standard Chartered and Deutsche Bank have some degree of Haskell adoption, but the former uses it a lot more, I think.
It would be nice to have a better idea of possible work directions...
monads... duh.
I'm open to anything FP related. But it's not necessary to have ideas about topics, as the first year of a PhD is often about exploring different topics and narrowing this down to a focus. My own suggestion for a topic is to implement the worker/wrapper transformation, but this is just one idea. I'm happy to talk about topics by email or on the phone. 
Isn't that mainly because Neil Mitchell works there though?
Well, a bunch of other ones too: Lennart Augustsson, Malcolm Wallace, Don Stewart, Ravi Nanavati, Roman Leschinskiy, off the top of my head. I'm sure there are others :) It sounds like a pretty awesome place to work (although some of the ones I listed are in different countries)
Wow, that's quite a list. I thought dons worked for Galois though, I guess I'm not up on the latest Haskell gossip.
You need to get on twitter! He announced his move on there ;)
Going by what godofpumpkins is saying, I'm assuming that's Standard Chartered? How do you like it? And what part of the bank are you working in?
I really wish I saw that indentation style more often for data types.
I would be interested, but I'm in the U.S.
Is the actual parser from Cabal not available?