This isn't really surprising tbh. With current metadata building profiles for people is remarkably easy. 
I take it you disagree with [Guy Steele and Matthias Felleisen](http://www.eighty-twenty.org/2011/10/01/oo-tail-calls) here?
My reasoning for `data` over `newtype` is that it is one fewer thing to think about. The choice is not onerous, but neither are the advantages very substantial. I'd prefer to simplify. 
&gt; Unicode characters not on any keyboard [Maybe not on](https://en.wikipedia.org/wiki/Space-cadet_keyboard) [your keyboard...](http://www.columbia.edu/cu/computinghistory/dmkeyboard.jpg)
Is it just me, or does this sub see far more job postings than other programming subs?
I think that's only become true recently, probably after a few job postings went well and generated reasonable discussions. Since the sub isn't super-busy this seems like a nice development.
As an expert r user data scientist, the core language of Haskell is a much better fit for data science than r or Python. Most data science tasks are pure data transformations, the problem is that it is impossible to express that intention in r or Python. If Haskell had half the ecosystem r or Python had the average Haskell data scientist would be as productive as a team of data scientists using r. 
If it's the difference between a program that works or crashes, isn't that more than an optimization?
What's the difference between this and Utop?
The following types are not the same, denotationally: data Foo = Foo !Int newtype Bar = Bar Int This expression... case Foo undefined of Foo _ -&gt; () ... is `_|_`, but this expression... case Bar undefined of Bar _ -&gt; () ... is `()`.
&gt; Only Haskell has: &gt; &gt; - Purity does that mean monads are out for Rust?
It's a whole new parser and syntax for the language too. Take a look at http://facebook.github.io/reason/mlCompared.html
&gt; Only Haskell has: &gt; &gt; - Purity &gt; - Laziness I would add the remark "by default" (since you added to "immutability by default"). You can write pure and lazy programs in Rust (e.g. some APIs are intrinsically lazy like the Iterator Trait), and you can write impure and strict code in Haskell. It is just that the defaults are different.
Yes, that repo is the latest there is. Our focus is now on the LLVM backend. Personally, I think, it would be best to pool resources there, but we'd be happy to help with advise if you go ahead with working on the OpenCL backend.
This isn't just about optimizing resource usage. In terms of algorithms, we are talking about O(1) versus O(n) in memory usage, so that's constant memory usage versus linear growth directly proportional to the input size. And this difference is huge, because in one case you've got an algorithm that *always works* whereas in the other case you have an algorithm that always crashes, for big enough values of N. It depends on how big the stack is, but I can tell you that on top of the JVM with the default configuration you can expect a stack overflow for N &gt; 10000, sometimes less. And this has an impact on how code gets written. What is a tail recursion but a loop that can work on pure values, or a state machine where the functions are states. But if you don't have TCO, recursion becomes dangerous. Not sure what you meant by heap bounded programs and defining the expected output, but the difference really is between it works and it doesn't work.
 https://www.reddit.com/r/haskell/comments/4jpt7e/what_has_rust_borrowed/d38p75y
Chucklefish is working on their Wayward Tide game in Haskell.
Somebody started a thread about [FP shops](https://www.reddit.com/r/haskell/comments/4jo2da/fp_shops/) which led to somebody posting this job. Haskell has been gaining traction in commercial settings for a while although it's penetration is still small. A new CEO-friendly slogan might be needed soon. Perhaps "Lazily achieving success at lower cost." :)
I suppose it depends what you mean by "behave the same". You are right that they interact with `case` differently but they are both unlifted wrappers around `Int` so are isomorphic in a very strong sense.
Free isn't necessary for such decoupling. Really all you need is to use a monad, and keep its usage independent from its implementation. Then to plug in different interpreters, hide the core operations behind a type class and create different instances. Admittedly, it's not quite as easily interchangeable. But it will be much more performant, and factors out an unnecessary free monad.
It can't be done. You need to be able to generate fully general type witnesses for the STRefs, and there's no way to do that. That's why the ST monad exists in base.
Here it is: data Content = Content { text :: T.Text } deriving Generic instance ToObject Content instance ToJSON Content type UserAPI = "test" :&gt; Get '[JSON, HTML "style.tpl"] Content userAPI :: Proxy UserAPI userAPI = Proxy server :: Server UserAPI server = return $ Content "\"Hello World!\"" main :: IO () main = do loadTemplates userAPI "./templates/" run 8080 $ serve userAPI server The content of ./templates/style.tpl : {{ text }}
Kinda. On the one hand, tryna keep it simple. Nobody ever really uses "strictness" or "laziness" to mean something other than "by default". And purity for me really means "has a type of pure functions", which Rust doesn't.
The benefits can be an easy order of magnitude if a worker-wrapper transformation can't unbox everything you do in `data`. `data`-based `StateT` is almost unusable performance-wise, quite drastically increasing memory usage for no gain.
Those `&amp;quot;` markings are how quotations are encoded in HTML, so your server is doing the right thing. If you let a browser render the text, you'll see the quotes. You have to do html decoding
I've only skim read it, but I couldn't help but notice they are using "=" for mutable updates instead of "&lt;-", which I think is a shame. Such changes only make things superficially easy.
interesting! I suppose it's more powerful than building an interpreter on a ADT, but it's not clear for me what the benefits of `Free` are.
I always thought it was because there are fewer jobs working with Haskell than with some other languages, so the community likes to see evidence of growing usage in industry.
Slides?
&gt; i = ((&lt;$&gt;) &lt;$&gt; (,)) &lt;*&gt; (show &lt;$&gt; truncate) aaaaaaaaaaaaaaaaaaaaaaa 
Really pleased to see /u/snoyberg bring this up. I do agree with this as a solution to the `Control.Applicative` problem, but in my opinion `-fno-warn-unused-imports` is probably too big a sledge hammer to crack this rather small nut.
DataKinds *and* 20% time? Awesome. 
I also immediately stumbled on that one. Could maybe someone who worked on this explain why this was chosen?
An alternate fix: import Control.Applicative import Prelude I'll admit I don't use it, but it is concise.
Rust can't prove that the pure program you wrote is pure. Haskell can verify the purity of your 1,000,000 lines of code.
 do h &lt;- bracket (openFile "file.txt" ReadMode) hClose return hGetLine h would be a type error in Rust. It's an exception in Haskell. 
I'm much more productive in Haskell under notepad* than Java under InjelliJ... But my yearning for an IDE has drove me to waste so much time failing to install `ghc-mod` &amp;co. \* emacs `haskell-mode`, disabling most things
Unless you use the Compose key. https://en.m.wikipedia.org/wiki/Compose_key
For this situation, perhaps something like `-fno-warn-redundant-imports` would be useful? I like the push that warning about unused imports gives me to clean up my imports, but in the face of shifting APIs, the ability to turn off warnings for redundant imports (like `pure`) seems like a nice compromise.
&gt; Rust can't prove that the pure program you wrote is pure. I think that one has to be careful when talking about purity and low-level imperative languages, since even just entering a function already has observable side-effects (the program counter register changes). &gt; Haskell can verify the purity of your 1,000,000 lines of code. What about using `unsafePerformIO`? One can use it to write non-pure code that the compiler cannot verify (just use it to interface with a C function...). I would really just say that for "some definition of purity" one can write pure code in Rust and one can write pure code in Haskell. It is just that the defaults are IMO different. In the context of Rust a pragmatic definition of purity is needed, and implementations for imperative languages typically choose something like "the return value of a function can only depend on the value of its arguments, and cannot depend on any global state, nor values that the arguments refer to (if they are pointers), and that the function cannot have any other observable side-effect besides its return value" (which is more or less how gcc defines purity for C). 
See [Stephanie Weirich's Compose Conference 2016 talk](https://www.reddit.com/r/haskell/comments/4bkxbo/compose_conference_dynamic_typing_in_ghc/), I remember she used implementing ST as a motivating example.
&gt; The only thing Free Monads give you above this is the ability to inspect/modify the syntax tree of the computation prior to evaluating the computation. This is almost right. The real trick is that this syntax tree is normalized such that it is impossible for you to violate the monad laws by inspecting it. This can be a useful tool for designing, although I find that I usually want to eliminate Free by the time I'm done.
Thanks. I got it.
I also thought about syntax tree manipulation (i.e. 'program description') as the most significant added value of `Free`, but then I discovered that the parts that return values, e.g. this: &gt; data ScriptLang next = Exec Command Args (ExecResult -&gt; next) Are either tricky or non-trivial to deal with. Perhaps I missed something.
It should be noted that that solution is not without potential problems of its own. The PVP does not require a .B bump when you add new functions/types. So, in theory, `Control.Applicative` or `Prelude` could export a new function that now conflicts with a symbol exported from another module causing a compile time error. Now, the chances of this happening are probably low. `Control.Applicative` and `Prelude` are not exactly hotspots for adding new functions. But, if anyone is wondering why not everyone does this, that is one potential reason. 
The question is also whether `-Wunused-imports` satisfies the declared intent of `-Wall`, namely "*Turns on all warning options that __indicate potentially suspicious code__.*"
that'd be great, looking forward to!
Can Rust detect that a function satisfies your pragmatic definition of purity? Will it warn me if I then read from the PC somewhere in the body? In Haskell, pure code and impure code has different types, that's the standard definition of purity. 
&gt; -f-warn-name-shadowing This has certainly caught issues for me *while developing* that would have been much harder to discover simply because the bug that would manifest would be a very rare occurrence. So it's not all that clear-cut. (Whereas this import thing is rather a lot easier to argue for since it cannot introduce semantic changes.) EDIT: Missed out a not.
&gt; This is similar to the change to GHC a few years back which allowed `hiding (isNotExported)` to not generate a warning: it made it much easier to deal with the now-no-longer-present `Prelude.catch` function. I can't help but feel like there's some backstory missing here. When exactly was this "change", and what does it have to do with `Prelude.catch`?
Probably he is referring to this ticket: https://ghc.haskell.org/trac/ghc/ticket/7167
You can explicitly qualify the list of things you get from `Control.Applicative` as usual. e.g. import Control.Applicative (Applicative(..)) import Prelude The list of imports is considered in order as to whether the module brought into scope things you are using. You almost always use _something_ from the Prelude, so putting it last will rarely result in a warning about an unused import.
I want `(&gt;)` and `(&lt;)`. I compose more than I compare anyway.
With `coerce`?
Yeah. The isomorphism being the constructor and the accessor: newtype Name = Name { getName :: String } Name :: String -&gt; Name getName :: Name -&gt; String (if exposed)
I was replying to someone that didn't seem to understand Monad. A similar line of argumentation would say that Haskell cannot define the natural numbers because we don't have induction.
&gt; have to write AST manually instead of Haskell code with placeholder But this is not the case, AFAIR, since you can quasiquote.
I agree that there should be `Functor f =&gt; Getter a b -&gt; Getter (f a) (f b)`. I don't know if it exists though.
He isn't arguing that it's too small of a change to be called a new interface, he is saying it's too big a change to be called a new interface.
So what's the benefits of this over say haskel with stack and docker?
You might check out the Vagrant templates at github.com/GaloisInc/HaLVM-Vagrant. They _should_ (famous last words) work regardless of your host system. If they don't file a bug.
Aren't you missing a 'y' in your email?
`-f-warn-name-shadowing` has caught a lot of bugs for me. I keep it on, and use point-free style and `LambdaCase` whenever reasonable, to avoid unnecessary identifiers. 
I hate the C-style comments too, because of how they make operator escaping necessary. Sure, the lack of line comments in OCaml can be annoying, but this looks far worse.
Yep! Sorry about that! My typechecker didn't catch that one, being stringly typed.
If I didn't pick `IO` then I think the newtype would have to be `newtype Script m a = Script { unScript :: FreeT ScriptLang m a }` and then `dumbscript :: Monad m =&gt; Script m ()` would it not? I might be wrong.
We have a strong preference for on-site because of the sorts of discussions we have (very heavy in math and design), however, we'd like to keep in touch with anyone interested in remote work so feel free to email me with your information and anything else you'd like to include.
I don't turn on Wall but I do explicitly turn on the unused import warning and a few others I find useful, and do use Werror, and do that for all my Travis instances back to GHC 7.2/7.4 (depending on the project). I've been working that way for years - it suits me. I personally think this warning is useful to structure the code and see an overview at a distance. If a module imports System.IO I know it's doing a bunch of IO. I really want to know what projects within a module import each other, with no false positives. Importing something from aeson tells me this module is doing JSON, again, another useful data point. Too many imports in one module tells me the module is doing too much and should probably be split. All these insights are only valuable if I don't have unused imports clouding the view. That said, things like Data.List, Control.Applicative, Control.Monad, Data.Maybe are fairly boilerplatey. I couldn't care if a module imports them, it doesn't meaningfully alter the dependency graph, it doesn't tell me anything about the module.
I use that approach, and it works very nicely for me. CPP is just too ugly to bear, and on Windows can significantly slow down ghci reload times (especially if your virus scanner hates you).
to second /u/bheklilr are you open to remote candidates? I'd love to do haskell but relocating isn't an option for me.
We're having a bit of trouble with Reddit's spam filter so I'll be fielding questions in the mean time.
Wow, this list is so much longer than it was a few years ago.
Oh, I wasn't aware that that had actually made it in! :)
See [response](https://www.reddit.com/r/haskell/comments/4jxodc/hiring_haskell_engineers_at_leapyear_a_data/d3anc88) below.
Granted, this isn't a resource, but... Building an interpreter for a programming language or a text-based game where you input commands is a great place to start with `Free` if you're looking for projects.
I'm really growing less and less satisfied with the PVP. In a language so well founded in static guarantees, to rely on convention (and a not so well known and easy to mess up convention at that) for something so important is really starting to get to me.
Hey, did you hear the one about the redundant joke? It was redundant.
That trick there uses the fact that data Box = Box Null has the same 'shape' as data Object s = Object (SmallMutableArray# s Any) Putting the Null object in the same memory location as the `Object`'s `SmallMutableArray# s Any` member. A real `SmallMutableArray# s Any` lives in kind # but is a heap object, the singular constructor for `Null` is also a heap object, so they'll never have the same address, so i can compare one against the other with `reallyUnsafePtrEquality#` after I make it a little more unsafe. Why do this? Well, I can't make up an inhabitant of kind # directly, but this lets me have one to statically compare against, without having to deal with a `{-# NOINLINE #-}` wrapper around a real `nil` object, which then has to deal with the fact that it starts out lazy, causing each access to have to check to see if its evaluated, making null checks quite slow. Currently the constructor there is lazy, to avoid the compiler defeating me by realizing that `Null` would unpack to 0 bytes, but when I first implemented this idea, I used a strict data constructor. Basically it gives me a detectable way to talk about null pointers, when the pointers are pointers from small mutable arrays containing direct pointers to other small mutable arrays without the indirection through kind *. Why bother? Well, if I don't then I wind up having to make mutable data structures out of IORefs, which reference MutVar#'s which reference other objects. I always wind up with an extra level of indirection. In heavy memory-bandwidth limited tasks, this eats 50% of my performance right off the top.
Provability come in different forms for different aspects of our software. Differential privacy provides a mathematically rigorous definition of privacy unlike the ad-hoc methods used in practice today; cryptography relies on various definitions of cryptographic security, such as IND-CPA which can be proven for a given cryptoscheme; we also leverage advanced features of Haskell's type system to ensure correctness of implementation for critical code.
Thanks for the reply; it’s been very helpful. I had incorrectly ruled that approach out before because the docs say those flags only work for packages outside the Stackage snapshot. For the benefit of anyone who is reading this later, adding flags: json: mapdict: true in `stack.yaml` *does* work if `json-0.9.1` is also added to `extra-deps` in `stack.yaml` so the JSON module becomes a standalone dependency independent of the Stackage snapshot. It’s not sufficient to customise the flags but then rely on Stack to spot `json` in the `.cabal` file’s `build-depends` and find a suitable version in the snapshot as usual. My second question was because I wondered whether there might be some sort of unwritten rule about flag behaviour for packages included in a snapshot. If not then since testing combinations of flags exhaustively would be impractical as you say, it does make sense to treat the custom-built package as a standalone dependency rather than as part of the snapshot even though it’s the same source and version. For my third question, I wasn’t so much worried about the package itself not cross-compiling, more whether whatever I needed to do to set the custom flag during the build process might interfere with also using a custom compiler. Based on the above, this seems to be a non-issue, though.
This is insanely clever magic :)
With case! This is the difference between newtype and data with a strict field.
I've been playing around with tree lenses, and I would love to extend the combinators I have written for generic trees (Data.Tree) to work on datatypes. TH would be a great candidate for this, except that splices break mutual recursion. The ability to define lenses recursively is one of the things which makes them really well suited for tree processing. This makes me sad.
Adam, I'd like to report a problem. % brew install cyberchaff Error: No available formula with the name "cyberchaff" OK. % sudo apt-get install cyberchaff E: Unable to locate package cyberchaff Hurmmm.... % cabal install cyberchaff cabal: There is no package named 'cyberchaff'. Oh no! % git clone git@github.com:GaloisInc/cyber-chaff.git Cloning into 'Cyberchaff'... ERROR: Repository not found. Where the heck should I be looking? % ssh adams-laptop.galois.com -l everyonenotadam -- -- adams-laptop.galois.com -- unauthorized users only -- -- Run /usr/local/bin/toolchain for information on tools not -- in the normal PATH environment. -- $ ls ~adam/Projects/cyber-chaff CHANGELOG.md Makefile ... Oh gees. This is a completely inconvenient location to store cyber-chaff. Please consider releasing a .msi. 
It does strike me as odd that a cleanliness warning like `-fwarn-unused-imports` is included in `-Wall` but some safety warnings like `-fwarn-incomplete-uni-patterns` and `-fwarn-incomplete-record-updates` are not.
I love this description of laziness.
Why haven't these fixes made it into the main language, btw? They're obvious improvements. Does anyone oppose them, or is modifying the parser just too hard?
Dependent typing is indeed a superset of type-level constants which requires enforcing that level of "true" purity (we probably need two different terms at this point to describe the two concepts). Idris already has this, I suppose? There has been basic support for `const fn` (albeit feature-gated) in Rust since 1.2 and it can trivially prevent RNGs by not having the ability to call FFI functions or perform any mutations. The proposed features will potentially extend this support to complex deterministic evaluation, in the vein of referential transparency, but also allowing mutation through pointers (which can only point to some abstract location, not actual global memory). Even then, there's nothing clever about it, just marking some function with `const fn`, which disallows mutation of global state, and FFI functions not being `const fn`.
I assume, since you typed that, that you didn't read that CHANGELOG aloud. Right?
[FreeT takes 3 arguments](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Trans-Free.html#t:FreeT), but you might be able to get something shorter with Free or FreeF. I haven't tried.
Ok can you write from the top of a your head a working version of equivalent to this : instanciateField field type_ = instance Has$(capitalize field) $(type_) where $f(field) = _$(map toLower type_ type_)$(capitalize field) So that for example `instanciateField "foo" "bar"` generates Instance HasFoo Bar where foo = _barFoo 
&gt; But that's not "writ[ing] the AST manually", it's at worst a minor annoyance; It's a major annoyance because you need to know the correct function to call and it just adds noise to the template . &gt; I wonder if you could build IsString instances for varE and friends Is that note supposed to be the quasi quoter job ?
At the same time, you could say that, Facebook is maybe not trying very hard to integrate existing language/community, and just rather fork languages into their own thing.
it's easy to be dissatisfied... do you know of anything better which gets the job done without relying on conventions or versioning as well?
In a similar vein, I wish GHC had `-fwarn-unused-pragma` to warn about redundant LANGUAGE pragmas
But if that's the standard practice, why bother allowing import shadowing in the first place?
[HTML edition](https://www.haskell.org/communities/05-2016/html/report.html) for those who prefer that (like me, because Chrome is incapable of rendering a PDF without mangling the text...)
Please read the link! It wouldn't be an annotation. The existing types would be stronger. All `&amp;Fn` closures could be statically known to be pure (unless they explicitly require an `IO` capability as argument). The benefits would be the same as in Haskell. Greater ability to reason about code, more space to do things like e.g. STM, fewer bugs.
Sometimes you really do want to manipulate an exact AST model of your code. In that sense, TH is closer to Lisp macros than what you are describing. Let's face it, Haskell is not Lisp, where the code and its AST representation are one and the same. But you could build your kind of Haskell macro language on top of TH. I'd love to see that! I doubt you could get the full power of AST manipulation. But I will be thrilled if you prove me wrong.
&gt; Stack keeps my operating system clean of globally installed compilers Why would you want such a silly thing? That way each user account needs to redundantly download and install one gig worth of GHC install per GHC version installed and stressing your account quota quite a bit, and kill any page-table &amp; fs-cache sharing potential in the process. Did nobody learn from Java's mistakes? What's so bad about the proper way of installing global packages or using the system package manager?
stack will find and use local installed GHC versions and I think you can use `STACK_ROOT` to point to some shared folder as well if you don't want stack to end up in your users home --- also I always assumed that _javas_ mistake was the language and all those SingletonFactoryBuilderProxies
Me too! HLint does a reasonable approximation of that, and the internal Standard Chartered Haskell implementation does an awesome job - it's just a standard warning.
I'm interested myself if there are any packages for this available.
Time to edit my .cabal's &gt; Turns on all warning options that indicate potentially suspicious code. The warnings that are not enabled by -Wall are -fwarn-incomplete-uni-patterns, -fwarn-incomplete-record-updates, -fwarn-monomorphism-restriction, -fwarn-auto-orphans, -fwarn-implicit-prelude, -fwarn-missing-local-sigs, -fwarn-missing-exported-sigs, -fwarn-missing-import-lists and -fwarn-identities. https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/options-sanity.html
There seems to be `-fwarn-pointless-pragmas`, but I don't think it works.
Yes, I didn't use `FreeT`, I used `Free`.
It's just a simple choice of trade-offs. We choose GHCJS because we know Haskell and we like the maintainability that it brings, at the cost of some long-term risk of having to maintain GHCJS. On the opposite end of the spectrum you can write your app in JavaScript, for which you will most certainly suffer in the long-term trying to maintain your app. Yes, JavaScript is that horrible. There are some intermediate options: PureScript (young), Elm (young), js_of_ocaml (stable), but they require learning a new language. TypeScript can be considered an intermediate step, but it's more of a short-term stepping stone IMO. In the long-term, it's worth investing in a language you plan on sticking with. For Haskell, the way to do that is to start using GHCJS.
I agree. I'd have no trouble trusting an enterprise to ghcjs. It has clear community support, and while Luite's all encompassing knowledge was critical to getting it built, there is enough commercial interest that it could easily live on were he hit by a bus.
The simplest reason is that we are not working with any government agencies, in the intelligence community or otherwise. The original post references the previous work experience of our team, not our current customers. Our target customers at the moment are companies that work in healthcare and financial services, where data breaches are most damaging to organizations and individuals. Furthermore, although we are not currently contracting for the federal government, we don’t see anything wrong with developing software for the government that better protects the privacy of individuals. Christopher Hockenbrocht, CTO@LeapYear
I really need to remember to send them updates. =/
Don't get me wrong. I'd love to use it. If there's an official release and it can be installed just like ghc with stack, I'm sure it'll see massive adoption. The current state simply sends red flags.
I agree completely. Building something in JS is not so much risky as *guaranteed* to go badly, and all the alternatives to GHCJS are (in my view) at least as risky as GHCJS, with less gain to offset that risk. Also, I've contributed a few patches to the deep dark parts of GHCJS, and it really wasn't too bad.
Probably not (I don't write TH often at all), but I can try. {-# LANGUAGE TypedHoles, TemplateHaskell #-} instantiateField field type_ = [d| instance $(_ ("Has" ++ capitalize field)) $(_ type_) where $(_ field) = $(_ $ "_" ++ (map toLower type_) ++ (capitalize field))|] Then I'd see what the types of the holes are. I don't remember any of the TH module names, that's how long it's been since I've used it. I also challenge you to write the equivalent in, say, lisp (or really, anything other than bash and friends), trying to use juxtaposition for string concatenation like your example does. (I have not tried to run the code I wrote, so it could be completely wrong)
&gt; The simplest reason is that we are not working with any government agencies, in the intelligence community or otherwise. Thanks for clarifying. I think the first item in the top-level post wasn't entirely clear on that. &gt; we don’t see anything wrong with developing software for the government that better protects the privacy of individuals I see your point, but I'm not sure if I agree entirely. If those individuals include the public, then I guess I agree. If those individuals equal some secret agency, whose missing supervision by the government and/or public lead to a lot of weird happenings, then I'm not so sure.
Anyone have an up-to-date perspective on whether GHCJS is suitable for mobile web apps (i.e. tablet and phone)? Also, who do I have to pay to get a decent official GHCJS web site set up?
The blue highlight signifies novelty of the entry, not relationship between different entries.
My /usr/lib/ghc-7.10.3 is 864.1 MiB broken down into a 405.5 MiB ghc_0AG9TO... folder, a 161.3 MiB Cabal_3ux67... folder, a 106.5 MiB base_HQfYB... folder, and many other smaller ones. So there is at least some (almost) truth to this.
&gt; Then I'd see what the types of the holes are. I don't remember any of the TH module names, That's the point. Those holes which I believe you call in your previous post : "minor annoyance" is the difference between being able to write some code or not (without internet connection). Moreover, these holes are unnecessary from a user point of view and don't bring any usefull information. They are just helping TH which is why I think TH needs improvement, not us to remember all those useless function to fill the holes. I can write it in Ruby, Ruby meta-programming is based on evaluating strings (is Ruby part of the bash and friends ?). I can even do it in Haskell but not with TH. I can generate a string with the corresponding code, save it to a file and import this file (I've done it, generating files with 3000 lines) and it works really well. There are however two problems with this approach : You need makefile or equivalent, so it's not nice to export You also have a sort of 2 stages restrictions (but TH as it as well, no ?) Using any text interpolation quasiquoter (Yesod `lt`) I can write instantiateField field type_ = [lt| instance Has#{capitalize field} #{type_} where #{field} = _#{map toLower type_}#{capitalize field} |] and let haskell parse the result. Maybe the solution is to mix this with the TH quasiquoters.
Please keep the HTML edition! Personally I wouldn’t mind if the pdf version is dropped if it simplifies the pipeline to have only an HTML edition (no need to go via latex in that case).
I'll keep it.
with class C t a where f :: a instance C [Bool] Int where f :: Int f = 42 `f` has the type ghci&gt; :set -fprint-explicit-foralls ghci&gt; :t f f :: forall {k} {t :: k} {a}. C t a =&gt; a and you can instantiate `t` without a proxy ghci&gt; :type f @[Bool] f @[Bool] :: forall {a}. C [Bool] a =&gt; a ghci&gt; :type f @[Bool] @Int f @[Bool] @Int :: Int
Given your solution class C t a where f' :: Proxy t -&gt; a instance C [Bool] Int where f' :: Proxy [Bool] -&gt; Int f' Proxy = 42 f :: forall t a . C t a =&gt; a f = f' (Proxy :: Proxy t) `f` seems to have the same type as with my example, what am I missing? ghci&gt; :t f f :: forall {k} {t :: k} {a}. C t a =&gt; a ghci&gt; :t f @[Bool] f @[Bool] :: forall {a}. C [Bool] a =&gt; a ghci&gt; :t f @[Bool] @Int f @[Bool] @Int :: Int ghci&gt; f @[Bool] @Int 42
Using the global package manager does not handle different projects requiring different versions of GHC. A whole class of issues similar to that is why we use Stack instead of globally installed stuff.
&gt; Please read the link! &gt; &gt; It wouldn't be an annotation. Sorry but how is passing IO as a function argument not analogous to an annotation like e.g. `impure`? (let's make pure the default). I see how purity improves the ability to reason (and test!) code, and how even tho projects like `metacollect` can give you this information, having the compiler enforce it has its advantages (e.g. in ABI boundaries where otherwise it would be impossible to control). Fewer bugs is a consequence of better ability to reason about code, and STM requires some sort of purity but C++ Transactional Memory TS has shown that it can be bolted on in an impure language (with enough work). I can imagine how parallelism libraries could potentially make use of purity to prevent user errors, but then I also think that for a lot of parallelism libraries purity is not enough. For example CUDA/OpenCL/OpenMP/OpenACC/AMP/HSA/... disallow virtual dispatch, so these would need a stronger guarantee on closures than purity, something like, this closure is pure and it invokes no code that performs virtual dispatch and any other not supported thingy.
What is the status of closure compiler minification? Have things improved?
That's really neat! It doesn't solve the problem I mentioned (although maybe sets up the way to solve it), but it solves a problem, most definitely. I'll go check it out.
Agreed, I thought that went without saying. Without C FFI you won't have good access to the Haskell ecosystem. I am saying this will be most useful if you can combine the ecosystems without concern.
Having spent a year using TypeScript at a startup, I think it's *less* than a stepping stone: it's only marginally different from raw JS, if that. For me, it was closer to JS with types in comments than an actual typed language. All type systems might be equal but TypeScript's is decidedly less equal than others. It prides itself on being "just JavaScript" which just means it's *also* rather unsuited to functional-style programming. I would absolutely not recommend it for anyone using or interested in functional programming. The only use case I can see is using it to retroactively add types to a legacy JavaScript code base which is certainly not *useless*, but probably isn't as useful as you'd imagine either.
It's easier to pandoc HTML to pdf than the other way around anyway! Thanks for this edition /u/mmaruseachph2 !
That is an interesting suggestion. Maybe I should investigate a non-LaTeX based approach. Thanks
Given how active Luite has been regarding GHC development, this is a lot less of a risk than you'd expect. In fact the opposite has occurred. E.g. They've changed the way we spin up `template-haskell` to better allow the sort of crazy pseudo-cross-compiling stuff you need to do for ghcjs. They've added hooks to cabal/ghc for ghcjs that are proving useful to me in other projects, etc. Rather than ignore ghcjs, the ghc folks have largely embraced it.
&gt; There are some intermediate options: PureScript (young), Elm (young), js_of_ocaml (stable), but they require learning a new language. Don't forget Scala.js!
Order of type arguments is still an issue, see [ticket #12025](https://ghc.haskell.org/trac/ghc/ticket/12025#comment:2).
It has some type system features that even Haskell lacks.
Why is JavaScript unsuited to functional-style programming? It doesn't seem too bad to me. You have higher order functions, curried utility libraries, immutable data structures and can use a functional architecture with virtual DOM.
Like union types and OO-style subtyping? "Even lacks" is the wrong phrase there. Haskell *avoids* these things and ends up stronger for it. The one interesting feature are intersection types which would certainly fill an extensible-record-shaped hole in Haskell. That hole might be better-served with something like row polymorphism though.
Can you explain? I think GHC supports typechecker plugins and preprocessors, but not any backend plugins (i.e. from STG).
I've tried it on a few projects and never had it work *well*. The standard libraries and browser APIs (DOM stuff... etc) all tend to assume you're going to be mutating things. The syntax for functional idioms tend to be really heavy-weight which implicitly discourages their use. I ran into serious performance problems with linked lists vs arrays, and working with arrays functionally is tricky. Popular libraries tend to provide APIs that are not particularly functional. It feels like you're fighting both the language and the environment the whole time. Maybe life is significantly better with some dedicated libraries and a virtual DOM framework (which I haven't used but no doubt makes a big difference), but I still doubt it'd be a particularly *good* environment for it even if it does provide some support.
I am of two minds on this question: * I have been using `Reflex.Dom` for the past year, and I love it. It's made frontend development fun, which is a sentiment I never imagined possible (because I am unreasonably opinionated about frontend styling, and I have suffered greatly from the inexpressivity and unmaintainability of most JS and CSS tooling) * OTOH, through my *misplaced* sense of entitlement, I have caught myself feeling "forced" to use GHCJS in order to use `Reflex.Dom` and resultantly aggrieved by the GHCJS user experience. I thought to myself, *"Self, I'm just a "dumb user" -- I've never had to compile a compiler before, or sit around for two hours for anything, why can't I just download a binary for my platform and get to the fun stuff??* But GHCJS is magic, and I'm lucky it even exists, so therefore I try to ignore the traditional, release-y expectations I am accustomed to, because GHCJS has made frontend development not just tolerable but enjoyable. Tooling-wise, [`reflex-platform`](https://github.com/reflex-frp/reflex-platform) is a much-appreciated solution to the GHCJS install annoyances, but since I am not a `nix` person I've been much happier now that [`stack` supports GHCJS](http://docs.haskellstack.org/en/stable/ghcjs/) "almost" out of the box. `nvm` is to `node` as `stack` is to `ghc`, so I am at peace with my combination of `stack` and `nvm`. I even get `flycheck` error highlighting in my Emacs editor for GHCJS projects thanks to `stack`! In my impatience when beginning a new project, I do sometimes wish I could just specify `ghcjs-7.10.3` today or `ghcjs-8.0` tomorrow in my `stack.yaml`, but I don't think it makes a big difference in my day-to-day experience as a GHCJS user. I would encourage you and others to use GHCJS now -- the more users it has, the better! And enough others want the same level of convenience that eventually a release will be cut, either by Luite himself, or one of the companies using GHCJS. I also wonder if OP raised this question after looking at the Github activity graph on https://github.com/luite -- I don't have any anxiety seeing that graph though since GHCJS mostly "just works" for me :)
Sure. The standard library and the DOM is pretty imperative. Luckily you don't have to use those directly. But consider ECMAScript 6 which has very lightweight syntax for lambdas and other constructs that make functional programming easier. Add in Ramda which is a Haskell inspired "prelude" where all functions are pure and curried. Then use one of the libraries available with immutable data structures. Finally, bring along a virtual DOM library. Added together about 90% of your source code can be completely pure and coded in a style reminiscent of Haskell. It's far from as good as Haskell. But it really shows that JavaScript can be used in a very functional way. Probably better than most other mainstream languages.
I did not look at Luite's graph, though I did look directly at https://github.com/ghcjs/ghcjs/graphs/contributors which is even worse, with no substantial contributions since November. However, I agree the "just works" factor is already almost there :)
It worked perfectly on my simple test with Reflex a few weeks back.
I use the [katip](http://hackage.haskell.org/package/katip) structured logging library, log to elasticsearch, and use kibana to view/analyze the logs. It has built-in support for logging the source location, and there is an [open issue](https://github.com/Soostone/katip/issues/19) for adding call stack support. But you can also use katip's namespacing support to get around the call stack problem.
Ok, I thought by "annotation" you literally meant something like `impure fn` or `pure fn`. Could you clarify what your basic question or contention is? I think I'm a bit lost. Are you questioning the value of purity in general? The value of purity for a Rust-like language? The meaning I'm using for the word "purity"? The value of the particular form of purity I described? The theoretical feasibility of having purity in a Rust-like language...? Etc. :)
As I understand, GHCJS generated output is still quite large, even after minification.
Saddened to see the numerical/scientific section shrinking ; for example `arb-fft` and `HROOT` disappeared from this issue ..
We are 649Kb minified (closure compiler - simple optimisations only, advanced is broken) and zipped with zopfli. For a approx 15K lines of Haskell code front-end application.
Anytime I see a Github activity graph like that, I think of this tweet on burnout: https://twitter.com/kizmarh/status/553141889541877760 I hope that's not how /u/luite2 feels! :)
Crap. Forgot to respond to their email asking for updates. FLTKHS is actually much farther along.
The start of the inactivity coincides with the start of the port of GHCJS to GHC 8.0, which has been taking a bit longer than I hoped.
I didn't realize this, but you can embed the CallStack constraint in a type alias. For a website, you'll probably have some application-specific monad that you can attach it to. This should allow all of your application code to be automatically tracked with callsite information, and you don't have to enable it globally like with profiling code. To access the CallStack value in your error-reporting code, just write those functions with the expanded signature. Here's an example: {-# LANGUAGE ImplicitParams,RankNTypes #-} import GHC.Stack type SomeType = (?l :: CallStack) =&gt; Int z :: (?l :: CallStack) =&gt; Int z = error $ showCallStack ?l y :: SomeType y = z x :: SomeType x = y main = putStrLn $ show x This prints: $ ./X X: ?l, called at X.hs:8:27 in main:Main z, called at X.hs:11:5 in main:Main y, called at X.hs:14:5 in main:Main x, called at X.hs:16:24 in main:Main
I feel like hiding inside `katip` is a much simpler core.
EDIT: I see you have an actual proposal with lots of thoughts in a gist, I will read these tomorrow, these are my thoughts without having reading it yet (in my defense, it's long, and it's late). I am questioning the value of introducing any kind of language feature to enforce that some functions are pure "transitively", that is, that they only call other pure functions and that whether a function is pure or not is part of the function type (either because they take `IO` as a function argument or are annotated somehow like in D). I agree that there is some value in doing so. They allow reasoning better about _some_ code. Whether a function is pure or not is part of its API, you might not be able to see the code if its at an ABI boundary, so if you want to enforce the constrain you have to be able to encode this somehow as part of its type. Still, which code would pure functions allow you to reason about better in Rust? I think not that much code. Haskell has immutable data-structures, but Rust has pointers. A pure function in Rust is able to observe the value of a pointer but is not able to dereference it, because the result of a pure function _must_ depend _only_ on the value of its arguments, and in the case of a pointer, it cannot depend on the value that a pointer points to. This makes pure functions way less useful in Rust than in Haskell (a pure function cannot work on a `Vec` by definition). The other possible win I see about pure functions is that they are inherently parallel "if you don't have pointers", but in Rust you do have pointers, and you also have things like dynamic dispatch which break lots of types of parallelism. A disadvantage I see is that in Rust "printfn"-debugging is saddly a thing. Not being able to do so in the middle of a call-stack of pure functions could be painful. I used to think that being able to have some notion of purity and pass effects as function arguments, and make them part of the function type was very important for a programming language, but in an imperative one with pointers I am not anymore sure if the cost of doing so is worth the benefits. There are benefits, but they are by no means as big as in Haskell. So.. I am not questioning whether one can add purity to Rust in some way, I am questioning whether if it's worth it or not. Even if we could, doesn't mean we should.
I received no entry in the last 2 years :( But they can resurface next edition, if text is submitted.
I'm starting to wonder whether you and /u/StackLover are the same person... 
What downsides?
Have a look at e.g. the mailing list discussion and perhaps previous reddit threads on it.
I own ghcjs.com if anyone wants to build something there.
&gt; but C++ Transactional Memory TS Can you provide a reference to the TS? (Google isn't giving me much.) AFAIR C++ Transactional Memory is *nothing* like STM in terms of composability, etc., but my memory is fuzzy and I'll like to re-check my conclusions.
Look up "simd" instructions for assembly languages. It's basically a technique of dealing with multiple pieces of data at the same time at the hardware level. Theoretically, something like "add 1 to every number in this array of numbers" can be done with a single assembly instruction if it's made into a vector at the hardware level. Converting data into things that can be dealt with this way is vectorizing and the "array" you can do that with is called a "vector" at the hardware level. (I might be slightly wrong here and there but that's the idea. Different hardware behaves differently)
Do you have any numbers at hand for LOC vs payload size? Whenever I suggest GHCJS, people seem to fear the payload size, so having some numbers for decent sized apps would probably help me on that front.
Yes, that great type system feature called "being comically unsound" - https://github.com/Microsoft/TypeScript/issues/3067 I was reading the typescript spec, and I couldn't believe that parameters get passed "bivariantly" - this means that either the param is a subtype of the arg or vice versa. Insanity! TypeScript's attractiveness is substantially diminished by its lack of non-null types - https://github.com/Microsoft/TypeScript/issues/185
[We](https://obsidian.systems) build full stack web applications, so that means everything from cloud hosting and databases through javascript and CSS. For reference, we recently posted a want ad [here](https://www.reddit.com/r/haskell/comments/49jjff/haskell_opportunities_at_obsidian_systems/).
I personally like the first one better.
My tiny AWS instance has only 8 gigs of space total, and GHC taking up a gig is a not-inconsiderable chunk.
What about adding Proxy into the mix: empty :: Proxy o -&gt; (k -&gt; k -&gt; Ordering) -&gt; t o k v The downside is that a user could supply the same proxy to two different ordering functions. Beyond that, you should get the properties you want: cannot mix different data structures (t parameter), cannot mix different ordering schemes (assuming users are careful with the empty function).
CPP is actually [the perfect language for macros in a functional language like Haskell](http://conal.net/blog/posts/the-c-language-is-purely-functional).
I think the right approach to using different orderings of the same key type `k` is to wrap `k` with a different `newtype` for each such ordering. That said, there are other problems that may warrant a solution similar to the one you're suggesting, such as encoding at the type level that a given vertex is a member of a given graph to avoid accidental mixing of vertices from two different graphs. `Proxy` and `ST`-style parametricity are two possible angles of attack, but I think it's up to the programmer to provide different proxy arguments (for `Proxy`) or explicit type signatures ensuring sufficient polymorphism (for parametricity), so neither is totally for free.
I should've said in the opening post that I want to avoid the newtype solution because it feels cumbersome to have to unwrap the value with every use, but it does work. 
Sometimes I wonder if it's worth the effort having to special case the odd-one-out, namely Windows :-( Just recently we hit two bugs *only* on Windows which cause problems for Cabal/GHC: `PATH_MAX` being limited to anachronistic 260 chars, as well as another fs-related issue caused by using the `\`-separator for a library which (for good reason) expects a `/`-separated path instead... *sigh*
Another way you could do it: newtype RoundRobin a = RoundRobin (IORef [a]) roundRobin :: [a] -&gt; RoundRobin [a] roundRobin xs = RoundRobin &lt;$&gt; newIORef (cycle xs) select :: RoundRobin [a] -&gt; IO a select (RoundRobin ref) = atomicModifyIORef ref $ \case (a:as) -&gt; (as, a) Although, one may be slightly better off with storing a vector of `a`s, and an index in an `IORef`.
Yeah, i thought about cycle, but it use `++` so it creates another thunk, but your `newtype` should help, is there any hope i can find a vector version of this on hackage?
You may notice that the OP uses Windows which isn't known for having a good package-management story *nor* for having a good user-account story. I have been struggling for ages to set up a remote sshd-accessible Windows VM for GHC developers with multiple user accounts. But sadly, Windows still feels like a single-user OS :-(
The problem there isn't Windows. In fact windows hasn't had this limitations for ages now if the proper Unicode APIs are used with UNC paths. See https://msdn.microsoft.com/en-us/library/aa365247(VS.85).aspx#maxpath The problem is that GHC and Cabal and related tools do not understand UNC paths at all. A lot of code on GHC has been written for quite old and outdated versions and APIs of Windows. I think it's unfair to blame the platform for this.
I see, and it's elegant!
Thanks! Disclaimer: I haven't scrutinized the whole thing in detail, obviously. From looking at the bits about "synchronized" and "atomic_noexcept" (etc.) bits, I think I was right in my assessment. This looks like a much more restricted version of what Haskell's STM can do. E.g. there's no "retry" which is one of the most crucial STM functions. I mean I *understand* why that's so and I'm *very* certain that N4514 will be a great leap forward in multi-threading usability for C++, but selling it as STM for C++ is disingenuous, I think. 
Haskell, Stack and Intellij IDEA IDE setup tutorial how to get started https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8
 data Proxy p = Proxy Nullary args constructor. At the value level, a "Proxy p" is useless. At the type level, it's used to drive inference. Here's what it would look like for my suggestion: data OrdOrdering = OrdOrdering ordBST :: (Ord k, BST t) =&gt; t OrdOrdering k v ordBST = empty (Proxy :: Proxy OrdOrdering) Data.Ord.compare
Those are fair points.
The first version makes more sense to me. It feels really natural to wrap a lambda in parentheses while using it with an operator feels weird. (Except for $ which I just read as parentheses anyhow.) I don't think I've ever used a lambda without parentheses or $. Just defaulting to always wrapping lambdas in parentheses is perfectly reasonable.
As opposed to expecting the whole goddamn world to rewrite their code when they said "screw comparability"?
Good work! I also tried Haskell with IntelliJ at the start but then switched to Atom instead of giving it up. Maybe I will give it another try with your Gist
The first one makes more sense to me. It seems to me that the parentheses are only required because of the anonymous function, so it makes sense to put them around it.
Breaking binary compatibility and creating potential buffers overflows for basically every single piece of Windows software that uses the old paths APIs? Great idea, how arrogant of Microsoft to cause you some inconvenience just so hundreds of millions of people can keep using their software without asking the vendor for a fixed &amp; recompiled binary, assuming the vendor and all the vendors of libraries they use are still around and willing to provide it. Maybe sent your suggestion to Raymond Chen, he might write you a nice blog post explaining why they didn't proceed as you suggest.
I think problems like these often occur because people who write code forget that Windows is a separate platform that needs to be considered, and that "computing" isn't equivalent to Unix. If more non-Unix platforms were still around, like VMS, Mac OS 9, or Amiga, I'm sure these problems would be a lot more visible, but Windows is so similar to Unix that recompiled Unix code often appears to work on the surface, and bugs go unnoticed until an edge case shows up. Try to process file names with characters outside ISO-8859-1 using Windows "ports" of common open source command-line tools for buckets of fun.
True, but sadly we lack a good abstraction for that in our non-abstract `FilePath` Haskell representation. There was this [Abstract `FilePath` Proposal](https://ghc.haskell.org/trac/ghc/wiki/Proposal/AbstractFilePath) to try to improve the situation somewhat by getting rid of the error-prone `type FilePath = [Char]` representation. But the resulting public discussion showed some issues that need resolving before AFPP can go anywhere. It's also not clear to me where the sweet spot in the design space lies that would give us correct-by-construction-`FilePath`s. In other words, how much we can encode in the type-system without code becoming super-tedious to write.
Thanks for the correction. I simply removed that part as it's really beyond the scope of the Gist.
Interesting! Why is that development nok reflected on Github? Are you keeping it local?
aaaaaaaaaaa I've been thinking of something like this for months and a week or two ago I discovered that others call it domain theory and NOW you post this where I would have seen it the whole time!?
You could put an Ord constraint on keys. This fixes you to a predetermined implementation for built-in types, but there's some hope that you could dynamically choose the instance for other types using something like this https://www.schoolofhaskell.com/user/thoughtpolice/using-reflection A snippet from the link: using (Monoid (+) 0) $ mempty &lt;&gt; 10 &lt;&gt; 12 -- evaluates to 22 using (Monoid (*) 1) $ mempty &lt;&gt; 10 &lt;&gt; 12 -- evaluates to 120
This subreddit is for the programming language Haskell :)
Looks interesting. I only looked through it somewhat briefly. The last suggestion to basically squash lists periodically through going to Set and back doesn't seem too terrible, but is still much less elegant and probably a little slower than giving `Set` and `Map` optional typeclasses, as then they can actually just leave their binary tree structures as is, only resorting them when they get given `Ord` items. So I would still quite like optional type classes. One side benefit is `nub` can stop being harmful, as it can take advantage of `Ord` if it is given one.
The reason that the latter works is that a lambda "grabs" as much as it can, which is why you usually place parentheses around it. It's maybe more intuitive if you replace the `*` with `$` – `(map $ \x -&gt; x + 1) . take 5` probably seems a lot more natural, but has the same structure as the code in the OP.
Thanks for this. I can't help but notice that there is still a lot of command line work needed for common IDE tasks. Is that also the case with the Atom setup? I'm surprised because if you use an older version of GHC and Cabal then EclipseFP can do much better than this *right now*.
Why does the plugin know to use stack for the "Add Haskell package dependencies" portion, but it doesn't know how to use stack to find the compiler it needs? That is, why do I have to give it an explicit path to the GHC binaries?
ah, well then the binary size is definitely hard on them. Especially older hardware. Just interpreting the file seems to take a bit. Once it does load, performance is little different from react or elm. We have benchmarked several things against those. 
Good point. I will fix that in next release.
&gt; The transition from dabbling in Haskell and being productive enough to contribute professionally is often just a matter of lots of practice (and some determination). So my first bit of advice is to write Haskell every day, even a little bit. I couldn't agree with this more. The one thing that I would add is that it does matter what you're writing. When I started learning Haskell I spent awhile working on small things like [Project Euler](https://projecteuler.net/) problems, reading tutorials, etc. But my proficiency with Haskell didn't really take off until I worked on substantial real world project. In my case it was a website, but it wouldn't have to be. Any project that gets you familiar with a good chunk of the above list will do. Here are some axioms that I think are useful in helping you choose a project (in rough order of significance): * Intrinsic motivation &gt; forcing yourself to work on something you think will get you a job * Working with other people &gt; working by yourself * Open source &gt; closed source * Bigger &gt; smaller * Longer project duration &gt; shorter The reason I state it this way is because some people argue that open source contributions shouldn't be necessary to get a job. I agree with that. But I think it's important to realize that all else being equal, a significant contribution to hackage or cabal is to look better than claiming that you wrote a closed source chess program. You know your personal constraints best, so try to optimize that with the above in mind.
This is a feature I sometimes wanted as well, however the workaround are easy, and it might be also a bad idea anyway. I explain, the easy workaround is to not export your local data. The main drawback of that is: you need to explicitly export everything else. Now the only good reason you would like to have local data is if you wanted to use the same name (with different semantic) in different function in the same file. Is it really a good idea ? 
It was discussed a number of times, e.g. https://mail.haskell.org/pipermail/haskell-cafe/2016-January/122798.html I like local types, but I don't think we'll ever have them because nobody really cares.
The optional typclass idea reminds me of "static if" in the D language. Also of "ifCtx" : https://github.com/mikeizbicki/ifcxt
There can be benefit to knowing that the definition is only relevant in a limited scope.
I'd include &gt;&gt;= in the "except for"
I guess for me it's an organizational thing. It wouldn't really benefit me in any practical programming way, just something that I imagine is easy enough to implement and that I'd enjoy to be able to use in my programs.
(Maybe of limited use, but) Sure, certainly not a bad idea. If a data type is only used locally within a single function why not make that explicit? Same with local type and pattern synonym declarations, I'm just not sure that's a common situation for local datatypes. I'd argue that local pattern synonyms declarations are more useful. As I understand it local data declarations require the producer of the type `data MyLT = Yes | No` (`isLT :: MyLT`) as well as the consumer (`case isLT of Yes -&gt; …; No -&gt; …`) to only exist locally. With patterns, only the consumer is needed so there are more use cases: foo = \case HTTP_404 -&gt; ... HTTP_418 -&gt; ... where pattern HTTP_404 &lt;- (getResponseCode -&gt; (4, 0, 4)) pattern HTTP_418 &lt;- (getResponseCode -&gt; (4, 1, 8)) instead of foo req = case getResponseCode req of (4, 0, 4) -&gt; ... (4, 1, 8) -&gt; ... (As I understand it) With local data, the `getResponseCode` function would also have to be defined locally since it returns the local datatype foo req = case getResponseCode req of HTTP_404 -&gt; ... HTTP_418 -&gt; ... where data ResponseCode = HTTP_404 | HTTP_418 | ... getResponseCode :: ... -&gt; ResponseCode getResponseCode ... = HTTP_404 at which point our `foo` function is getting overcrowded.
So is the reason Haskell doesn't have local data types because nobody's bothered to implement it yet and we could have such a thing if someone just sat down and worked on it? Or is it because implementing such a feature would be notably complicated given GHC's current implementation details and a bad idea given some subtle trade-offs I haven't thought of yet?
Lot of fair points, I haven't thought at all about the interaction of Rust references with a notion of purity. I was basically using the same reasoning that e.g. GCC does to reason about purity when C pointers are involved (*const and *mut). guess that it can be made to work for references as long as one restricts what one can do within a pure function a bit. In the following example, `foo` is pure and does not need `IO` but its results are not reproducible (if I were to put the slice on the heap instead of the stack each program execution would deliver a different result): https://play.rust-lang.org/?gist=977216772d2f512db2084c82656214be&amp;version=nightly&amp;backtrace=0 I don't see how any form of purity would be able to work in Rust without restricting things like this.
You can use a wrapper for just the applicative and monad operations, converting back to `Set` regularly to avoid performance issues: import Data.Set import qualified Data.Map as Map data SetWrapper a where SetWrapper :: Ord x =&gt; { mapping :: x -&gt; a, base :: Set x } -&gt; SetWrapper a wrap :: Ord a =&gt; Set a -&gt; SetWrapper a wrap = SetWrapper id unwrap :: Ord a =&gt; SetWrapper a -&gt; Set a unwrap (SetWrapper f xs) = Data.Set.map f xs compact :: Ord a =&gt; SetWrapper a -&gt; SetWrapper a compact = wrap . unwrap instance Functor SetWrapper where fmap g (SetWrapper f xs) = SetWrapper (g . f) xs instance Applicative SetWrapper where pure a = SetWrapper (const a) (singleton ()) SetWrapper ff xs &lt;*&gt; SetWrapper fa ys = SetWrapper fb ps where fb (x,y) = ff x $ fa y ps = fromDistinctAscList [ (x,y) | x &lt;- toAscList xs, y &lt;- toAscList ys ] instance Monad SetWrapper where return = pure SetWrapper fa xs &gt;&gt;= g = SetWrapper (pm Map.!) (Map.keysSet pm) where pm = Map.fromDistinctAscList $ do x &lt;- toAscList xs case g (fa x) of (SetWrapper fb ys) -&gt; do (i,y) &lt;- zip ([0..] :: [Integer]) $ toAscList ys return ((x,i),fb y)
TIL
Is that working on the assumption that there is a direct mapping between backend URLs and frontend "views" ?
It would not be hard to implement. The reason why we don't have them is that there is legitimate discussion to be had about the merits of such a thing. If somebody proposes this on a mailing list, it will receive a mixed response, and the community doesn't have a way of dealing with that.
You can also use [the `set-monad` package](https://hackage.haskell.org/package/set-monad)
Good point. I didn't think of it because my personal style would be to use do-notation instead of `&gt;&gt;= \ x -&gt;`, but it's a perfectly reasonable thing to do.
Would help keep the name space from getting polluted. And potentially cause a speed increase in Idris? 
Wow, impressive bug hunt! The fix is welcome, and the story was ~~frighteningly relatable~~ great.
Wow, impressive bug hunt! The fix is welcome, and the story was ~~frighteningly relatable~~ great.
I often wonder if we're not confusing cause and effect here. The abuse of the Unix packages to use functionality that's available in process or base for instance is artificially making a things harder for Windows users. Which again has nothing to do with the platform. The reliance on Unix specific build tools like autotools and autoconf to configure packages is another one. Windows is often times an after thought in terms of these things. And the complains start things are difficult on Windows. Which is a clear consequence of decisions made. I would rephrase your comment about special casing windows differently. The real issue is special casing anything non Unix-like. If the solution for GHC is to drop Windows as a tier-1 platform then it's fine. If not, then we should work on the abstractions to correctly support other OSes. As it is, it's no wonder Windows users are the minority on hackage.haskell.org or haskell. Things have been made harder for us.
I have a few projects that have been ported to Windows, and the ports are unsatisfactory for those and other reasons. I hope that the new Linux portability layer that is being added to Windows will eliminate the need for these ports. But Microsoft is targeting that at developers, and it remains to be seen if it will be easy enough to enable and use.
I like to keep things at the top level and functions short, because then I can test them individually. Not just formal unit test style, but also REPL experimentation. Also it keeps the scopes simple. I try to keep modules around 200-400 lines to keep the toplevel from getting too cluttered. I'd prefer per-declaration export annotations to the separate export list (that also suddenly makes you have to duplicate your haddock -- ** sections), but surely that ship has sailed.
Function-local type declarations are also possible in Scala. Here's the Idris example recreated in Scala. def foo(x: Int): Int = { sealed trait MyLT case object Yes extends MyLT case object No extends MyLT val isLT: MyLT = if (x &lt; 20) Yes else No isLT match { case Yes =&gt; x*2 case No =&gt; x*4 } } Sadly there's no where, so you have to declare things in-order.
It is one of those things with a lot of edge cases. If you imported such a function with data types into ghci, should those types be visible to :i, :t? If not, how do you "load" a function datatypes so that you can inspect them and what do you do if those datatypes conflict with top level datatypes or even "loaded" types from other functions in scope. And then how do you display errors in a datatype that is within a function where clause, or even three sub where clauses below that? I would actually love to have this ability, but I acknowledge that it is not trivial, not by a million miles and someone very smart and far more dedicated than I would have to make it work. I personally believe it would have been great if someone had thought of that at the inception of haskell, but no one did.
How's that recruitment strategy going? Do you include open source?
Indeed the functions would overlap. So how would you solve this problem? Its not easy to solve at all.
I mean, I'm mainly asking because I want to see the benefits of this and how I can better get into the right mindset of how to use records. I'm most familiar with how it's done in C++, where it's handled by a lot of compiler action to rename things behind the scenes. It's nontrivial, but it's certainly not intractable. And (from what I can tell) it's implemented in GHC 8 with the OverloadedRecordFields, which means I assume there's a deliberate decision about why it's an extension rather than a language feature. The context is that I'm working on parsing a fairly large json file that has a large number of nested records, and each level has an ID field. I'm naming them uniquely, but I'm just curious as to the design principles. I can't use the GHC 8 extensions because we're currently pinned to 7.8, but I'd love to learn more!
But you certainly can name fields inside of a c-struct with the same name as fields in another c-struct, which is the equivalent case. The choice of using the field label as the access function (as opposed to something like concatenating recordname-fieldname or something that would make it longer but unique when it generates the access functions) is a design choice and I'm curious as to the reasoning behind that. [Edit: I'm totally aware that I think like a c programmer. Just trying to learn a new perspective, not trying to be difficult :) ]
Modules and records are generally not regarded as the strongest aspects of the language. The design reason, IIRC, is simply that you kinda need those for a workable language, and it wasn't clear what the ideal solution should look like. Something basic was implemented so Haskell could move forward and not too much was set in stone and later more advanced designs could be implemented. The extension you mention is part of that effort. Virtually everything added to Haskell since the late 90s is an extension, so that doesn't mean it isn't important or considered good. Most programmers just prefix their record fields. I think this practice might not end even with GHC8 as I'm not sure how the popular lens library deals with those name collisions.
I gave up on all the indentation modes ages ago. I use `M-i` for `tab-to-tab-stop` and I bind `Tab` to `indent-relative`. This would work even better if I could get `backspace` or `shift-tab` to delete one level of indentation but actually I have just now thought of that. With the indentation modes I spend way too much time futzing around with the indenter. If I tried anything automatic it would be some sort of batch formatter that just takes the whole file and reindents it. I wonder if line-by-line automatic Emacs mode indenting is such a hard problem for Haskell that it's just not worth spending any time on it. Even a perfect solution would still require lots of tab-cycling.
Yeah, that's a tough one. One of the few things that Haskell and C++ have in common is that they're both huge languages, and people pick a subset. Some extensions are clearly in the 'bad' or 'not yet' list, like ImpredicativeTypes. Then there are some like FlexibleContexts and ScopedTypeVariables that are fairly necessary, widely used and unlikely to go out favor / be superseded. There's lots of handy and benign syntactical sugar like LambdaCase, MultiWayIf, OverloadedStrings and RecordWildCards that is harmless and could be mechanically replaced if required. Then there are some like FunctionalDependencies vs TypeFamilies or GADTs vs ExistentialQuantification, where there's a lot of overlap between two different extensions, and one is newer/more general than the other, etc. This list is a good starting point: http://dev.stephendiehl.com/hask/#language-extensions
[Servant](https://haskell-servant.github.io) is an incredibly cool web server library that uses type level combinators to make type safe server end points. type MyServer = "users" :&gt; Capture "name" String :&gt; Get '[JSON] User :&lt;|&gt; "posts" :&gt; Capture "id" Int :&gt; Get '[JSON] Post myServer = users :&lt;|&gt; posts where users name = do ... -- connect to db getUser db -- type error if `getUser` doesn't return a `User` posts i = ... -- connect to db getPost db -- type error if `getPost` doesn't return a `Post`
&gt; I gave up on all the indentation modes ages ago. I'm with you, the indentation modes for haskell-mode have always sucked because they try to be clever, using a big messy hairball of special cases. &gt; This would work even better if I could get backspace or shift-tab to delete one level of indentation but actually I have just now thought of that. This is what haskell-simple-indent did support, but [it was removed](https://github.com/haskell/haskell-mode/pull/958/commits/883bfd9e115aff50686cf34a4e8a5ce575c3251f) from haskell-mode to focus on the obviously fabulous other indentation modes. In [structured-haskell-mode](https://github.com/chrisdone/structured-haskell-mode), `C-j` is the "newline-indent" command and it goes to the right place because it's unambiguous. Combined with hindent, my indentation problem is solved. &gt; If I tried anything automatic it would be some sort of batch formatter that just takes the whole file and reindents it. [hindent](https://github.com/chrisdone/hindent) does this for the current declaration. I use it constantly as I write code to format as-you-write. I just write `do x &lt;- getLine; case x of P -&gt; blah` and hit "indent" and I get it all on separate lines formatted properly. &gt; I wonder if line-by-line automatic Emacs mode indenting is such a hard problem for Haskell that it's just not worth spending any time on it. Even a perfect solution would still require lots of tab-cycling. I'd agree. A tab cycle is a poor interface anyway.
My goal was to reduce the count of indentation modes to 1. At that time hi2 (currently known as haskell-indentation) was generally claimed to be very good and I went with that opinion. The amount of time I've spent fixing haskell-simple-indent before giving up on it is visible in git history.
If I understand correctly, these techniques were explained for example in [Linear-Space Computation of the Edit-Distance between a String and a Finite Automaton](http://arxiv.org/abs/0904.4686), Cyril Allauzen and Mehryar Mohri, 2009. The second page of this paper gives an overview of previous work on the question, with articles of the late 1980s providing good algorithms.
Basically, for overloaded you need type information (i.e. the struct type) in order to get from the function name to the function implementation. for everything else, you can get the function from the name without knowing the type, and then getting the type from the function implementation/signature. It certainly not impossible, it just didn't "fit" super nicely.
The techniques might be the same. I'll have to read the paper to be fully clear on it. What I know of the algos from the 80s is that they're not obviously identical, but there's definitely a lot of similarity.
Yep, saw that. What else? 
It looks like ifCxt does pretty much what I want. Thanks for the link! Although I will say there is a fair amount of boilerplate and it seems a little finicky, but that could just be me doing things wrong.
Wow, name mangling, unboxed tuples, and casting raw pointers? I'm glad to see that loading compiled `.o` files is possible, but if the performance isn't critical, it's reassuring to know that interpreting `.hs` files is also possible, via the [hint](https://hackage.haskell.org/package/hint) package, and that using it is much simpler. I also have a [demonstration that hint can load compiled code](https://github.com/gelisam/hint-demo), but now that I think about it, I think that my demo might have been misleading. I think packages are compiled to both a `.o` and to another form (`.hi`?) which is more suitable for ghci, and that hint is probably loading that second form instead.
There is an ongoing debate about this : [example](https://www.reddit.com/r/haskell/comments/32cnt7/why_there_isnt_a_pure_typeclass/)
Is there a similar one for `Map`? Because while probability distributions are very much `Set`-like, they technically require a `Map`, preferably one that allows you to customize how key collisions are handled, but that is not necessary.
That really does not solve the problem at all. `Eq` is not the slow part of my code. The `Monad` and `Applicative` instances are by far and away the biggest issues, giving exponential space and time behavior even in situations where you could very easily get linear time and constant space behavior. And yes I realize that improper use of optional typeclasses could lead to law violating behavior. But if used correctly that will not be a problem, particularly by making sure to not expose to much of the module, just like how `Set` currently works in order to keep the trees safely ordered and balanced. Remember that `fmap id` can already be not `id` without such an extension. And if properly used my code would never violate that. As if the distribution was ordered, then sure `fmap id` would try to compact it, but because it is ordered it should have already been compacted by the FIRST thing to return it in ordered form, so `fmap id == id`.
Maybe the ports would be satisfactory if the platform differences were properly dealt with, which is the point of a port to begin with? Running the Unix code on a non-Unix platform with a compatibility layer isn't porting, it's like compiling Windows software on Linux using libwine.
I don't know if this counts as type level programming... But [liquid haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/about/) pretty much blew my mind when I came across it. Be sure to mess around with the demos. 
Woah, this is cool.
Great article, really well written. Thanks for sharing this, it's not a technique that I'd come across before. But it's a great example of a problem that sounds extremely tough, but falls into place with a few nice insights. Good job!
Some examples from the top of my head: * Database queries. These usually have a rich static part, with runtime values merely filling in the blanks. If we compute their forms at the type level, we can do static analysis on them. We could for instance demand that an SQL query only uses a certain set of table names else the program will be rejected. * [Controlling effects.](https://ocharles.org.uk/blog/posts/2013-12-04-24-days-of-hackage-extensible-effects.html) * Invertibility of functions as developed in [Algebraic](https://github.com/avieth/Algebraic). Type level programs are used to determine the nature of a composition of two functions (is it injective, surjective, etc.). * High-level user interface descriptions. Imagine planning out a user interface using a storyboard, with a set of pages, the user input they elicit, and the possible transitions to other pages. This structure can be encoded in types, and the types can be used to guide the programmer to a correct value-level implementation (I'm working on this now. GHC has some prohibitive restrictions which make it awkward but I'm sure we'll fix them soon). * Related to the above and to servant: client-side browser application routes. Lots of single-page web apps take advantage of the W3C history API to give a nice feel with respect to the forward and back buttons. While the user navigates the interface, certain actions will update the history with a new URL, and the browser's buttons are expected to navigate through these application-specific checkpoints rather reload the page. A servant-style presentation can be given for this. Keep in mind: if you've ever written a type synonym then you've engaged in type-level programming. What's commonly referred to as type-level programming is just a more complicated flavour of what we already take for granted in Haskell.
The reason is that it is added complexity to GHC's typechecker, for not very much benefit (you could always just put the data declaration at the top-level next to the code that is using it). Also, it is not that simple. Should the local data declaration be able to capture type variables that are in scope (e.g., polymorphically bound by the defining function?)
Sad to hear. Currently looking for a Haskell job.
C++ famously requires semantic analysis to be done while parsing, because some syntactic constructs are disambiguated based on the meaning of the symbol. You are right, it is not impossible to handle (people always make noise about how "C++ is hard to parse" but it's not insurmountable, certainly.) If you want to see how a more flexible design is affecting the internals of GHC, you should look at the wiki pages https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields and also look in GHC's commit history for the commits that added duplicate record fields (which will give you the most up-to-date snuff, and some more info too).
That looks neat. I am still a little confused as to what it is doing to the set when you do something like `(+) &lt;$&gt; mySet` and leaving an unordered type in that set.
Do you have an application in mind for this? It's certainly possible to make interfaces more granular and general, but that doesn't always improve things.
Is there an auto indenter for haskell? Something to the like 'go fmt'?
There are at least two: [`stylish-haskell`](https://github.com/jaspervdj/stylish-haskell) and [`hindent`](https://github.com/chrisdone/hindent).
&gt; But you certainly can name fields inside of a c-struct with the same name as fields in another c-struct Yes, but in C, those fields are not lifted to the top level like they are in haskell. In C: struct x { int y ; } The `y` can *only* be addressed by `x.y`. In Haskell on the other hand: data X = X { y :: Int } exposes a top level function `y :: X -&gt; Int`. 
Random aside, but I write GADTs w/o naming the type variables, because they have nothing to do with the type variables that appear in the data constructors. Like in your example, the `(r :: [*])` is not the same `r` as in `UNow :: a -&gt; Union (a ': r)`, because now `r ~ (a ': r)` for _different_ `r`s, which is just confusing. data Union :: [*] -&gt; * where UNow :: a -&gt; Union (a ': r) UNext :: Union r -&gt; Union (a ': r) Ahh, much better!
Hm. I should start doing that.
&gt; someone told me Haskell did full type erasure, [...] Haskell does erase all the types, which is why we have to use this OptionalOrd contraption instead of using reflexion to ask whether the type of the value we're given has an Ord instance, like we could do in a language like java. Since the types are erased, that information is not available unless we use something like OptionalOrd to keep it around. &gt; [...], which requires [types to be fully resolved at compile time] doesn't it? It is not necessary to know the *full*, concrete type of everything in order to compile a piece of code. For example, in order to compile this: square :: Num a =&gt; a -&gt; a square x = x * x We don't need to know whether `a` is an Int, or a Double, etc. This is compiled to a function which receives a dictionary containing the `Num` methods (addition, multiplication, etc.). In particular, `a` could be a type which also supports Ord, or it might be a type which supports `Num` but not `Ord`. If we want `square` to do something different based on whether or not the `a` has an `Ord` instance, we'll need more information than just the `Num` dictionary. &gt; [...] `a =&gt; OptionalOrd`? The left-hand side needs to be a constraint, but `a` is a type. An explicit lack of constraints would be written `() =&gt; OptionalOrd a`. *edit*: oops, I guess this might be even more confusing! `()` is both the syntax for the unit type and the syntax for the empty constraint. &gt; Isn't it also possible to make deriving OptionalOrd automatic? Not with the current extensions, I don't think so. You'd have to write [overlapping instances](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/type-class-extensions.html#instance-overlap), and while I'm not super familiar with the rules for determining which instance is the most specific, I think the context part is ignored, so `Ord a =&gt; ...` would not be considered more specific than `() =&gt; ...`. I don't see any theoretical reason why such instances couldn't be automatically generated by the compiler, though. Maybe in a future extension!
I think a good way of approaching type class granularity is to think about two things: * Whether the type class actually represents a coherent idea; sensible laws are the best metric for this, I think, * Whether splitting up the classes enables instances which otherwise would not be possible. PureScript's Prelude provides some good examples. The Functor-Applicative-Monad hierarchy is slightly more granular, enabling a few instances which would not be possible in Haskell's current formulation. See http://blog.functorial.com/posts/2015-12-06-Counterexamples.html A great example (imo) of a type class which *should* contain more than one method is Semiring (again from the purescript prelude), because the laws tell you about how the methods interact with each other. (+) on its own would just be a Semigroup, likewise with (*), but that doesn't really get us anywhere since we have a Semigroup class already. The interaction of (+) and (*) is arguably the important part of Semiring, and so the class contains them both, so that the laws can talk about both.
That is true; if you want to extend the API in various ways, you probably don't want to keep the particular representation I suggested.
At the very least, it isn't idiomatic, which means it will be less than useless when trying to work on an existing project.
What do you mean by mapping over the RoundRobin? actually lpsmith's `roundRobin :: [a] -&gt; RoundRobin [a]` should be `roundRobin :: [a] -&gt; IO (RoundRobin [a])` so it's mapping the IO functor, no?
Propellor is using type-level programming to make sure that the config.hs file the user writes does not try to combine together properties that require different OS's. So, combining a Property DebianLike with a Property Ubuntu results in a Property Ubuntu, while combining a Property Debian with a Property FreeBSD is a type error. Also, multiple OS-specific properties can be glued together into a property that supports more than one OS, and picks one of the input properties to use at runtime. ugraded :: Property (DebianLike + FreeBSD) ugraded = (aptUpgraded :: Property DebianLike) `pickOS` (pkgUpgraded :: Property FreeBSD) The type level programming is simple set and list operations over type-level lists of OS's. I'm working on extending this to be able to detect port conflicts at the type level when eg, adding containers are added to a server. &lt;https://joeyh.name/blog/entry/type_safe_multi-OS_Propellor/&gt;
You can only map with `a -&gt; a` functions, not `a -&gt; b`, but the concern still applies. newtype RoundRobin a = RoundRobin (IORef [a]) roundRobin :: [a] -&gt; IO (RoundRobin [a]) roundRobin xs = RoundRobin &lt;$&gt; newIORef (cycle xs) select :: RoundRobin [a] -&gt; IO a select (RoundRobin ref) = atomicModifyIORef ref $ \case (a:as) -&gt; (as, a) modifyPool :: [Resource] -&gt; ([Resource], ()) modifyPool res = (map expensiveFunction res, ()) expensiveFunction :: Resource -&gt; Resource expensiveFunction = ... resourcePool :: [Resource] resourcePool = [Resource ..., Resource ..., .....] main = do rr &lt;- roundRobin resourcePool let (RoundRobin ref) = rr atomicallyModifyIORef' ref modifyPool -- from this point, every time the result of select is forced, -- expensiveFunction is evaluated -- use select a bunch I guess you can work around this by `take`-ing a chunk from the infinite list, modifying that, then cycling the result again. You might want to hide the constructor and present a way to modify the original list. I would probably recommend keeping as part of RoundRobin the original list and a cycled copy in an IORef. When you wish to change the original list, you can modify it and then call cycle and place it in the IORef, replacing the old. Additionally, if we use non-empty lists and infinite streams, then you don't have to check for empty lists and return errors when you get one. I would expose and interface like this. Assume we have cycle :: NonEmpty a -&gt; Stream a and `NonEmtpy` from `Data.List.NonEmpty` and `Stream` from `Data.Stream.Infinite` (or something equivalent). module RoundRobin (RoundRobin, roundRobin, extractRoundRobin, modifyRoundRobin, select) data RoundRobin a = RoundRobin (NonEmpty a) (IORef (Stream a)) roundRobin :: NonEmpty a -&gt; IO (RoundRobin a) roundRobin xs = RoundRobin xs &lt;$&gt; newIORef (cycle xs) extractRoundRobin :: (NonEmpty a -&gt; b) -&gt; RoundRobin a -&gt; b extractRoundRobin f (RoundRobin xs _) = f xs modifyRoundRobin :: (NonEmpty a -&gt; NonEmpty a) -&gt; RoundRobin a -&gt; IO (RoundRobin a) modifyRoundRobin f (RoundRobin xs ref) = do let xs' = f xs atomicallyModifyIORef' ref $ \_ -&gt; (cycle xs', ()) return $ RoundRobin xs' ref select :: RoundRobin a -&gt; IO a select (RoundRobin _ ref) = atomicModifyIORef' ref $ \(a :&gt; as) -&gt; (as, a) I typed this all up in the reddit box, so there are probably some typos. Let me know what you think.
&gt; This would work even better if I could get backspace or shift-tab to delete one level of indentation but actually I have just now thought of that. [I have a patch for that](https://github.com/haskell/haskell-mode/pull/866) but it seems upstream doesn't want it.
Hint uses unsafeCoerce when you call interpret so that’s arguably not better than casting raw pointers. I don’t see a problem with unboxed tuples, but you could easily wrap the API provided by GHC into an API that uses boxed tuples. Name mangling is definitely an issue but again you can hide that behind a nice api like the `plugins` package does So overall I think it’s less bad than it might seem at a first glance.
I've considered doing this. You could use hindent, add a printer which wraps everything in braces and semi colons and parens and then when you're done editing use the johan-tibell printer to go back to white space printing, which would work with existing codebases. I'd probably do that if I were going to rewrite SHM from scratch that's how I'd do it. I think it's a very legitimate approach. But Hindent wasn't around the first time I wrote it. :)
I agree, those two are good candidates for being included in `-Wall`. 
With hindent to provide a lens of brace.unbrace he should be able to solve that problem too. I use hindent on other peoples projects too. With the johantibell mode no body really notices, or nobody complains.
I started on a paredit mode like this with explicit everything, we just need to make a hindent formatter that includes explicit everything and then format it back to the project's standard style. I think you're on the right track. As a fellow lister you know what editing can be like. I'm beginning to think significant alignment was just a bad idea from the start. Haskell's tooling would've been solved, man months of work not wasted. Everybody has been wasting their time, both users of Haskell and toolers. So if we sort out the problem if working with existing code bases above then I'll be a happy user of your mode! Go for it! 
If you don't want (de)serialisation then implementing extensible finally tagless style DSL's in Haskell is extremely easy via typeclasses (http://okmij.org/ftp/tagless-final/index.html). Extensible (de)serialisation of DSL expressions is a bit more involved (see https://github.com/ajnsit/vyom for an example) but should get better after the upcoming Typeable class overhaul.
This is the reason I always write my lambdas like this: (\ (Constr pat1 pat2) -&gt; expr) ^Always a space after the lambda 
&gt; High-level user interface descriptions. Imagine planning out a user interface using a storyboard, with a set of pages, the user input they elicit, and the possible transitions to other pages. This structure can be encoded in types, and the types can be used to guide the programmer to a correct value-level implementation (I'm working on this now. GHC has some prohibitive restrictions which make it awkward but I'm sure we'll fix them soon). Can you elaborate? 
Thanks! :) The origin of the problem, for me at least, is in thinking about how to handle malformed natural-language inputs to parsers. My idea here is to use a non-deterministic bottom-up parser (specifically, a chart parser) to get as much information as possible. If the chart parser can't completely parse, it can at least provide partial parses, which you can then put through an error parser like the one demonstrated here. Since you can use this algorithm for any state system, not just regex/NFAs, you can use it for CFGs/NPDAs, and so you can use it to recover from the errors in the nat-lang input. I wouldn't want to use this kind of technique for programming language parsing, tho. Chart parsing and NPDAs are very inefficient, and programming language inputs tend to be much much much much much larger than natural language inputs, so the search space is very large, even for typical cases. But maybe it's not too bad I don't know. :)
&gt; but should get better after the upcoming Typeable class overhaul Can you explain?
`{-# PUBLIC foo #-}` and `{-# PRIVATE bar #-}` I'd prefer a new keyword (`private bar :: _`) to a pragma (like Java), which is shorter. Backwards incompatible. The implementation of the syntax (a keyword before a declaration) could be similar to the `pattern` keyword. And might be useful for shortening other pragmas (like `inline`). Idris is the most terse: private bar : ... bar = ...
Sure. The Typeable class allows you to reify type information in the form of a TypeRep object, which means that you can serialise an arbitrary DSL term as (TypeRep + untyped AST). However when trying to deserialise this structure, there is no way to use the Typerep to generate an actual value of a matching type. All the useful operations like casting values to a specific type, and generating witnesses for type equality, require Typeclass constraints instead of taking typerep arguments. There are apparently plans underway to fix this and make typereps more useful - See https://ghc.haskell.org/trac/ghc/wiki/Typeable for more information.
There are a couple of different superclass defaulting proposals out there. One by Conor McBride, and the other by Richard Eisenberg. Conor's addresses this situation to some extent, at least handling the deep superclass hierarchy issue, but it doesn't address the inference issue I mentioned.
Thank you, I've been annoyed by this as well and didn't know about this option.
&gt; The reason is that it is added complexity to GHC's typechecker, for not very much benefit (you could always just put the data declaration at the top-level next to the code that is using it). If I can lift the declaration to top-level, then renamer can do the same, right? Then what complexity in typechecker do you mean? &gt; Should the local data declaration be able to capture type variables that are in scope Obviously it should not, unless `-XScopedTypeVariables` is enabled. And it should in the later case. Scoping doesn't add too much complexity, you can do it in renamer too. E.g. local "data D = D Int a" can be lifted to top-level as "data D a = D Int a". There are probably corner cases though. I know that [you prefer globally unique names](https://github.com/haskell/cabal/issues/2835), so you don't see the value of local types (and also [nested modules](https://ghc.haskell.org/trac/ghc/wiki/Records/NestedModules)), but it doesn't mean they have "not very much benefit". They minimize namespace pollution; make refactoring easier (e.g. you can move function around without a chance of name conflict); let us avoid prefixes in names; etc.
That's what I use, only for me it's ~/opt/ghc-x.yz. Sometimes on fresh system separate manual install of cabal-install is required (which is simple too: just tar xf, then ./bootstrap.sh). Personally I've found brew, haskell-platform, stack, halcyon and macports to be often out of date or overly complicated. And if you don't want to build from source, you can always download binary tar. Of course it's often personal preference and YMMV... 
Except that hindent offers more choice by offering different styles, which is why it'll never be gofmt. I'm not knocking hindent when I say that; I use it everyday. 
They have not announced anything on [their news page](https://www.haskell.org/ghc/) yet, but there are releases in [the GHC 8.0.1 folder](https://downloads.haskell.org/~ghc/8.0.1/). 
Thanks!
&gt; Wouldn't having it compile down to squareInt, squareDouble etc. and appropriately propagating such concrete type versions around the entire program be faster, even if it might take longer to compile? Indeed, monomorphic code like squareInt is faster than polymorphic code like square, which is why a good chunk of the optimization performed by ghc consists of specializing and inlining this kind of definitions. There are also [specialization pragmas](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/pragmas.html#specialize-pragma) in case you need to force the compiler to specialize a particular definition at a particular type. &gt; One syntax that could work would be: [...] I think the syntax which would be the easiest to implement would be to use [implicit parameters](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/other-type-extensions.html#implicit-parameters) to pass along concrete [dictionaries](http://hackage.haskell.org/package/constraints-0.8/docs/Data-Constraint.html#t:Dict), like so: {-# LANGUAGE ImplicitParams #-} import Data.Constraint orderIfOrd :: (?optionalOrd :: Maybe (Dict (Ord a))) =&gt; (a, a) -&gt; (a, a) orderIfOrd = go ?optionalOrd where go :: Maybe (Dict (Ord a)) -&gt; (a, a) -&gt; (a, a) go (Just Dict) (a, b) = if a &gt; b then (b, a) else (a, b) go Nothing ab = ab The above syntax already exists, and the `?optionalOrd` constraint is automatically propagated to the callers until you decide to discharge it: orderInt :: (Int, Int) -&gt; (Int, Int) orderInt = let ?optionalOrd = Just Dict in orderIfOrd That's probably too verbose for your use case, so you could write an extension which automatically discharges this constraint when it is required at a concrete type like Int. This way, you don't have to implement a new syntax, you don't need to worry about propagating constraints, you only need to worry about automatically adding a `let ?optionalOrd = ...` at the right spot. &gt; How would I go about making this? I never wrote a ghc extension, so I'm not sure. I get the impression that the ghc team is very careful about which patches they let in: I got a bugfix patch in, but a new feature would probably be a tougher sell. I'd try implementing it outside of ghc, perhaps as a preprocessor or as a [type checker plugin](https://ghc.haskell.org/trac/ghc/wiki/Plugins/TypeChecker). Good luck!
FWIW the GHC lexer/parser inserts zero-width braces and semis at the locations they need to be
First they release the sources to the builders who have a week to create binaries. If all goes well, the official announcement follows.
Ahh, no good. Supported versions for OS key 'windows64': GhcVersion 7.8.4, GhcVersion 7.10.1, GhcVersion 7.10.2, GhcVersion 7.10.3 Seems windows users lack a bit of love. Any way to force `stack` to forget supported versions?
You'll need a `windows64` section in the `setup-info`. I omitted it for brevity. I think this should work: resolver: nightly-2016-05-21 compiler: ghc-8.0.1 setup-info: ghc: windows64: 8.0.1: url: https://downloads.haskell.org/~ghc/8.0.1/ghc-8.0.1-x86_64-unknown-mingw32.tar.xz content-length: 154950476 sha1: 5901210c31ec903b64872765edf2a1fb697c8e18 If I got the content length or SHA1 wrong, you can just comment them out. They aren't necessary. 
Yes exactly. The choice part is both a curse and a blessing. I believe it would be useful to have some styles based on group of individuals (`fpcomplete style`, `facebook style`, `google style` ...) together with some more individual styles from well-known library authors (`ekmett style`, `bos style`, `gabriel439` ...) Maybe an active pool so users would know how popular one style is.
[removed]
As mentioned in the earlier (albeit slightly premature) post, the announcement can also be found in browser-friendly form on the [GHC blog](https://ghc.haskell.org/trac/ghc/blog/ghc-8.0.1-released). Thanks to everyone who contributed to this release. It looks like it should be a great one!
Also, note that the Haddocks of the core libraries include pretty hyper-linked source, e.g. [`base`](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Control.Concurrent.QSemN.html).
There's also a [relevant StackOverflow question](http://stackoverflow.com/q/5730270/596361).
Really? I'm surprised that GHC doesn't recognize that `UNext` isn't instantiable for `Union '[]`. I never noticed because I always put custom error messages for readers' purposes.
The user guide entry is [here](https://downloads.haskell.org/~ghc/8.0.1-rc4/docs/html/users_guide/glasgow_exts.html#duplicate-record-fields).
There are some slightly tricky constraints on when a selector usage is deemed to be "ambiguous", as described in the [users guide](https://downloads.haskell.org/%7Eghc/8.0.1/docs/html/users_guide/glasgow_exts.html#selector-functions). However, what you describe here actually appears to be an interesting (and as far as I know, not recognized until now) artifact of how GHCi typechecks. Note that the same code in a module works just fine, {-# LANGUAGE DuplicateRecordFields #-} module Hi where data A = A { test :: Int } deriving Show data B = B { test :: String } deriving Show main = print (A 42) I think it would be fair to classify this as a bug. I've open [ticket #12097](https://ghc.haskell.org/trac/ghc/ticket/12097) to track this. Feel free to chime in.
It's actually Comic Neue, a free font, and in my opinion a vast improvement over Comic Sans.
Amazing, lots of new stuff to check out ;-) btw, the download link on the official page https://www.haskell.org/ghc/download has an extra `1` at the end, doesn't work.
So why does fixPolymorphic :: forall f. (forall x. (forall y. f y) -&gt; f x) -&gt; forall a. f a fixPolymorphic f = let x :: f a x = f x in x compile but fixPolymorphic :: forall f a. (forall x. (forall y. f y) -&gt; f x) -&gt; f a fixPolymorphic f = let x :: f a x = f x in x does not? (could not match `a` with `y`)
Oh dear, never mind. I just missed the critical `test` in the expression being `print`'d. Indeed I suspect this error is due to the ambiguity rules, although /u/adamgundry would know with better certainty. The solution here should be to add a type annotation on `A 42`.
The user guide says: &gt; However, we do not infer the type of the argument to determine the datatype, or have any way of deferring the choice to the constraint solver. Thus the following is ambiguous: &gt; &gt; bad :: S -&gt; Int &gt; bad s = x s So, to get it to work, you have to write: λ: print $ test (A 42 :: A) 42 
Note that you need to enable `DuplicateRecordFields` in your `ghci` session (at the time you declare the types, even). For instance, λ&gt; data A = A { test :: Int } deriving Show λ&gt; data B = B { test :: String } deriving Show λ&gt; :set -XDuplicateRecordFields λ&gt; print $ test (A 42 :: A) fails. However, this should work as expected, λ&gt; :set -XDuplicateRecordFields λ&gt; data A = A { test :: Int } deriving Show λ&gt; data B = B { test :: String } deriving Show λ&gt; print $ test (A 42 :: A) 
That reminds me: Could we get haddocks for the `ghc` package on hackage? I know that’s not how you get the package but that’s the same for `base`.
The more detailed breakdown suggests performance improvements over the 7.10 branch. Does any one know if any of these relate to compilation time? 
See https://ghc.haskell.org/trac/ghc/query?status=closed&amp;failure=Compile-time+performance+bug&amp;milestone=8.0.1.
I applaud the Agda team for being so bold and liberal with their symbol naming. From https://agda.readthedocs.io/en/latest/language/lexical-structure.html#tokens: &gt; Most non-whitespace unicode can be used as part of an Agda name, but there are two kinds of exceptions: &gt; *special symbols*: Characters with special meaning that cannot appear at all in a name. These are `.;{}()@".` &gt; *keywords*: Reserved words that cannot appear as a name part, but can appear in a name together with other characters. `let in where`, etc. Ok, sure - you have to require spaces in a few more places - big deal. The readability you get back is tremendous, imo. Just being able to use `-` and `?` `makesAWorldOfDifference`. I mean `a-world-of-difference`. I'd also like to use `?` when binding names that have type `Maybe _`. Much cleaner than sticking on `m` on the front. Constrant do mqueryParam &lt;- get "query" case mqueryParam of Just queryParam -&gt; _ with do query-param? &lt;- get "query" case query-param? of Just query-param -&gt; _ Urgh. Now I've just made myself sad :(
Also works: *&gt; let p = A 42 *&gt; print $ ($ p) test *&gt; print $ ($ A 42) test
let a thousand brains explode
؟ ⁈ ❔
Google alerts and scripts. People definitely have google alerts or something, I don't know how they find it either.
Looking forward to a stack lts resolver for it.
Noob here, what's special about those lines of code?
&gt; How much nicer it would be to say imports? a b. How is that any clearer? It's exactly as ambiguous as your previous example.
For some reason I find the color scheme on display there extremely off-putting.
In the first one the programmer was able to specify the instance of function length, which is a member of the Foldable typeclass to be from instance Foldable [] which means foldable list. Normally we would need a type annotation to specify which kind of length it is but this is happening at the level of terms instead of types. In the second one "do notation" is being used to specify an Applicative rather than Monadic computation. Let me know if this makes sense ;)
Think looks like an amazing release, congratulations to all those who worked on it! In the migration guide it says this no longer works: {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE UndecidableInstances #-} class Super a class (Super a) =&gt; Left a class (Super a) =&gt; Right a instance (Left a) =&gt; Right a -- this is now an error What was the reason for this change? To my naive eye it looks like it should work. And do you have any idea of the impact of this change (i.e. the number of packages broken by it)?
Aware of this, and yes, it's true - but doesn't fit well in a bigger block of code, because the lambda adds extra indentation for the rest of the block.
-XTypeApplications let's you "fill in" type variables as if you were "applying a type". Until now, you could only apply **values**: the types themselves were inferred, maybe with the help of a type signature. -XApplicativeDo lets you use do-notation with typeclasses like Applicative and Functor, instead of just Monad. I think it has the potential to make syntax more uniform and reduce the need for operators like &lt;*&gt; and &lt;$&gt;.
The link to trac 11318 for -XUndecidableSuperClasses is incorrect, and should be to 10318.
I miss this from Scheme so much. I agree that these suffixes are a very efficient way to hint at the type. I wouldn’t want to go all the way to Agda, as I think the requirement for extra spaces (with commas in particular) is a nuisance, and, possibly-unpopular opinion, I think the way people use its syntax is ugly and hard to read. I’m not sure that there would be any huge problem with opening up the syntax, tbh. It’s only too late in the sense that a proposed extension is unlikely to garner an overwhelming majority of support.
Fixed. Thanks!
Thanks! I'll make sure this gets fixed.
I wonder how long it will be before the various packages get around to bumping their `base` version bounds. I'm curious to see if my own code has broken! Nice features in this release. Explicit type application certainly scratches an itch of mine.
Your first explanation is confusing me a little. I don't understand how Foldable is relevant, it does not appear in the code... The second explanation is clearer. I had heard of that extension somewhere and thought it was already available. One question though: why does f only have to be a Functor (not an Applicative)?
[removed]
I've added the ghc-8.0.1 bindists to the default setup-info metadata, so you should no longer need to add a custom setup-info section to your stack.yaml for GHC 8.0.1.
We’re thinking about using it in `haskell-ide-engine` to load plugins dynamically so it might turn out into a bigger demo, but I can’t promise anything yet.
Got it, thanks :)
I don't think you'll find enough support for this to justify the breaking change. That said, a good start would be is precisely specify how you would change the lexer / parser of GHC, then document how much of hackage would be broken by the change. Each package that is broken is resistance to the change, unless their maintainer comes forward in favor of the change. If there are more people in favor than in resistence, and the patches are ready, I'll bet the Simons let this through.
**Update: fixed!** ---- I'm having trouble getting it (x86_64-linux) via stack, it looks like an incomplete XZ archive. $ stack setup Preparing to install GHC to an isolated location. This will not interfere with any system-level installation. Downloaded ghc-8.0.1. Running /bin/tar xf /home/ashley/Settings/stack/programs/x86_64-linux/ghc-8.0.1.tar.xz in directory /tmp/stack-setup18672/ exited with ExitFailure 2 xz: (stdin): Unexpected end of input /bin/tar: Unexpected EOF in archive /bin/tar: Unexpected EOF in archive /bin/tar: Error is not recoverable: exiting now Unpacking GHC into /tmp/stack-setup18672/ ... The downloaded size of `ghc-8.0.1.tar.xz` is always 109259931.
If something was a monoid, then it was also a semigroup. When a type had a `Monoid` instance it didn't also have to provide a `Semigroup` instance. `Semigroup` is retroactively being made a superclass of `Monoid`. The mathematical concepts of semigroup and monoid remain unchanged by what hackage, GHC, and the vast majority of the Haskell ecosystem does.
So length is defined by the typeclass foldable (its actually derivative) see the code here: https://hackage.haskell.org/package/base-4.9.0.0/docs/src/Data.Foldable.html#Foldable length has type **Foldable t =&gt; t a -&gt; Int** such that t is type that has an instance of Foldable. Another way of saying this is that Foldable is a constraint on t By writing **length @ []** we can specify that we want the list instance of length. I'm not 100% on all the terminology but I hope that's helpful.
Functor, not Applicative? This is even better than I thought!
I have the exact same problem. The xz file appears to be truncated by stack?
URL is https://github.com/commercialhaskell/ghc/releases/download/ghc-8.0.1-release/ghc-8.0.1-x86_64-deb7-linux.tar.xz
One nice use is lenses type Lens s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; (s -&gt; f s) where to define `view` you need something like [Const](https://hackage.haskell.org/package/base-4.9.0.0/candidate/docs/Data-Functor-Const.html) view :: Lens s a -&gt; (s -&gt; a) view ℓ a = runC (ℓ C a) where newtype C e _a = C { runC :: e } deriving Functor and to define `set` you need something like [Identity](https://hackage.haskell.org/package/transformers-0.2.2.1/docs/Data-Functor-Identity.html) set :: Lens s a -&gt; a -&gt; (s -&gt; s) set ℓ b a = runI (ℓ (\_ -&gt; I b) a) where newtype I a = I { runI :: a } deriving Functor ---- **Edit**: Also *existentials* are a good use for this, since they don't appear in the output type. Example from the [`foldl`](https://hackage.haskell.org/package/foldl-1.2.0/docs/src/Control-Foldl.html) package: data Fold a b where Fold :: (xxx -&gt; a -&gt; xxx) -&gt; xxx -&gt; (xxx -&gt; b) -&gt; Fold a b deriving instance Functor (Fold a) Here `xxx` is an existential which doesn't appear in the data type `Fold a b` (and thus doesn't appear outside of it). instance Applicative (Fold a) where pure :: forall b. b -&gt; Fold a b pure b = Fold (\O _ -&gt; O) O (\O -&gt; b) where data One = O here the existential type `xxx` is actually `One`, in the latest version of GHC we can use `TypeApplications` to make the types explicit: pure :: forall b. b -&gt; Fold a b pure b = Fold @a @b @One (\O _ -&gt; O) O (\O -&gt; b) where data One = O according to the type argument order of `Fold` ghci&gt; :set -fprint-explicit-foralls ghci&gt; :type Fold Fold :: forall {a} {b} {xxx}. (xxx -&gt; a -&gt; xxx) -&gt; xxx -&gt; (xxx -&gt; b) -&gt; Fold a b so Fold :: (xxx -&gt; a -&gt; xxx) -&gt; xxx -&gt; (xxx -&gt; b) -&gt; Fold a b Fold @a @b @One :: (One -&gt; a -&gt; One) -&gt; One -&gt; (One -&gt; b) -&gt; Fold a b while `&lt;*&gt;` method uses a different existential `Pair` defined locally in the method. (&lt;*&gt;) :: forall b b'. Fold a (b -&gt; b') -&gt; (Fold a b -&gt; Fold a b') Fold step₁ begin₁ done₁ &lt;*&gt; Fold step₂ begin₂ done₂ = do let data Pair a b = P !a !b step (P x₁ x₂) a = P (step₁ x₁ a) (step₂ x₂ a) begin = P begin₁ begin₂ done (P x₁ x₂) = done₁ x₁ (done₂ x₂) Fold @a @b' @(Pair _ _) step begin done ---- **Edit 2**: Another place is ironically in the pattern synonym definitions themselves, which deals with both constructing `\r -&gt; numerator r :# denominator r` as well as destructing `num :# den` pattern (:%) :: Integral a =&gt; a -&gt; a -&gt; Ratio a pattern num :% den &lt;- ((\r -&gt; numerator r :# denominator r) -&gt; num :# den) where -- data Pair a where (:#) :: Integral b =&gt; b -&gt; b -&gt; Pair b data Pair a = a :# a num :% den = num % den or pattern Read :: Read a =&gt; a -&gt; String pattern Read a &lt;- (reads -&gt; [(a, "")]) where pattern Parsed :: a -&gt; [(a, [Char])] pattern Parsed x = [(x, "")] or pattern Read :: Read a =&gt; a -&gt; Remaining pattern Read a &lt;- (reads -&gt; SuccessfulParse a) where pattern SuccessfulParse :: a -&gt; Results a pattern SuccessfulParse x = Singleton (Matches x) type Remaining = String type Match a = (a, Remaining) type Results a = [Match a] where pattern Singleton :: a -&gt; [a] pattern Singleton x = [x] pattern Matches :: a -&gt; Match a pattern Matches x = (x, NoRemaining) where pattern NoRemaining :: Remaining pattern NoRemaining = ""
Thanks so much for all the hard work! Unfortunately I have a compile time performance regression to report on my [FLTKHS](https://github.com/deech/fltkhs) library. I also don't have a minimal example. There is a demo that took about 15 seconds compile and link in 7.10.3, but with no changes now takes over a minute in 8.0.1. I've reproduced this across machines and operating systems. Since there was interest expressed in using this example as a benchmark if any GHC devs are still willing to help, I'm willing to walk them through getting the library set up etc. It's not a long process. The tip of my Github branch has been updated to build with GHC 8.0.1.
I completely agree. I suspect this change could be implemented with minimal breakage in existing libraries (or you could just hide it behind a language extension). Actually, come to think of it, what situation would it break if it were enabled by default? There aren't any keywords that contain both symbols and alphanumerics, and identifiers must be made of entirely alphanumerics or entirely symbols.
The difference is that record accessors create "normal functions" -- if i had data Foo = F { bar :: Int } Then now I have a function: bar :: Foo -&gt; Int and if i had data Foo2 = F2 { bar :: Bool } that should also create a function bar :: Foo2 -&gt; Bool but...what is the type of `bar`? &gt; :t bar -- ??? is it `Foo -&gt; Int`, or `Foo2 -&gt; Bool` ? You get the same conflict if you ever decide to define two functions with the same name. If you could do: baz :: Int -&gt; Bool baz = even baz :: String -&gt; Int baz = length Then what is the type of the identifier `baz`? What would you get if you did: &gt; :t baz -- ??? You mention that you're aware of the fact that they create two accessor functions with the same name...but if think this should be allowed, then why shouldn't two functions with the same name be allowed?
And, as always, the best place to get the deets is the [Release Notes](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/8.0.1-notes.html). 
This is intriguing and I wish I had thought of this for my undergraduate dissertation many years ago. 
Also, does `TypeApplications` mean that changing the order of a `forall` is now a breaking change?
I can proudly say that I've the best build for ghc 8.0.1 on Mac :) I've even got the new style hyperlinked source enabled for the ghc Api and all the boot libs. Though aside from that one detail the official build / binary dist is always good But agreed. Any release channel that doesn't have active engagement with ghc developers and or isn't part of a large decently managed Linux distro or other major UNIX system (such as the folks who support FreeBSD) will lag and or in some cases have quite unusual ghc builds. 
http://okmij.org/ftp/ is a treasure trove for these
Interestingly, both are much better written infix: a `importsFrom` b a `imports?` b
If there are many classes, how can I tell what order the specializations should go in? This all makes sense &gt;&gt;&gt; :t sum sum :: (Num a, Foldable t) =&gt; t a -&gt; a &gt;&gt;&gt; :t sum @[] sum @[] :: Num a =&gt; [a] -&gt; a &gt;&gt;&gt; :t sum @[] @Int sum @[] @Int :: [Int] -&gt; Int but what rules out this: &gt;&gt;&gt; :t sum @Int @[] &lt;interactive&gt;:1:6: error: • Expected kind ‘* -&gt; *’, but ‘Int’ has kind ‘*’ • In the type ‘Int’ In the expression: sum @Int @[]
Both fonts and layout are messed up in Firefox mobile.
My `stack.yaml` needed to have `packages` empty: $ cat stack.yaml resolver: lts-5.17 packages: []
&gt; How does it look like when a polymorphic function calls orderIfOrd, propagating the constraint to its caller? It would work exactly like normal types do, propagating the constraint to the caller, if needed. Now if that function also called something that required `Ord`, than it would make sense to drop the optional `Ord?`. &gt; What does it look like when a function has two such constraints? weird :: (Ord? a, Show? a, Read? a) =&gt; (a, a) -&gt; (a, a) weird? (a, b) = if a &gt; b then (read $ show b, read $ show a) else (read $ show a, read $ show b) weird? (a, b) = (read $ show a, read $ show b) weird? (a, b) = if a &gt; b then (b, a) else (a, b) weird = id Something like that I guess. Thanks a ton for all the information about how to go about doing this, will check it out over the next few days. One other thought I had, is that really there should be no observable effects of these optional parameters, they should really only be for optimization. So in other words `orderIfOrd` is a terrible example, either you want to order it or you don't. In which case I almost feel like the type signatures shouldn't even mention the optional parameters used, and library creators who do violate that should be shunned thoroughly. The one reason I can think of for keeping the type constraint is to let you know what typeclass you should make sure your type has if your code is running slowly.
The problem is, since identifiers must be strictly alphanum or strictly symbols, the code `yar?har` is currently parsed as `(?) yar har`.
Best thing to do is to lodge a trac ticket. https://ghc.haskell.org/trac/ghc/newticket 
I just remembered where I first saw this - scheme!
Here's a [gist](https://gist.github.com/avieth/45a8f101d523658281c2f43390e9647d) which lays out the basic idea.
Make it an implicit parameter and you can totes have a "?". :) On a more constructive note, I'm a fan of `isSomePredicate` or `somePredicate-p`.
The problem is that you can always freely float quantifiers *up* (thereby increasing their scope), but it's not free to float them *down* (because we must ensure that the difference in scope does not include any uses of the quantifier variable, before it is safe to do). I'd still call it a "bug" in the sense that it doesn't admit a valid equality we'd expect. Though sometimes our expectations lie, and allowing the equality would actually make life worse. This behavior is still active at least up through GHC 7.10.1
It is indeed. Note that the following typechecks just fine: foo :: forall f. (forall x. (forall y. f y) -&gt; f x) -&gt; forall a. f a foo phi = let x :: f b x = phi x in x Moreover, the following also checks: bar :: forall f a. (forall x. (forall y. f y) -&gt; f x) -&gt; f a bar phi = let x :: f b x = phi x in x The reason, of course, is that we *need* the type of `x` to be polymorphic otherwise it's not allowed to be an argument to `phi`. We get this polymorphism from the type generalization of let-bindings; but to do so we need to pick a name that doesn't cause confusion (due to prenex universal quantifiers being hidden, so we can't tell whether this "`a`" is referring to the outer forall for `foo`/`bar` vs the invisible inner forall for the let-bindings of `x`. For functions like `foo`, because the quantifier over `a` "hasn't been introduced yet", we know that the "`a`" occurring in the let-binding for `x` must in fact be coming from an invisible quantifier over another variable also called "`a`". Whereas for functions like `bar`, we already have `a` introduced so when we see "`a`" we have no choice but to assume it's actually the same type we have in hand. **tl;dr:** the following also typechecks just fine: bar :: forall f a. (forall x. (forall y. f y) -&gt; f x) -&gt; f a bar phi = let x :: forall a. f a x = phi x in x
I found out when all my bookmarks to the GHC API docs broke again. :P Those URLs always seem to get shuffled around every new release.
&gt; One question though: why does f only have to be a Functor (not an Applicative)? Probably because it can be rewritten as `f = fmap (+ 1)` (not requiring an Applicative interface).
Hmm, I've actually done this before. It's been a long time ago and I forgot the specifics, but I'm certain I didn't do any manual name mangling and used the foreign keyword. If I don't forget, I'll take a look at it again on Monday.
Fantastic! Congratulations to all involved. &gt; Note, however, that Haskell processes will have an apparent virtual memory footprint of a terabyte or so. Don’t worry though, most of this amount is merely mapped but uncommitted address space which is not backed by physical memory. Just wondering if this will have any adverse impact on monitoring/logging/etc.
Commas seems like a very odd choice. Though I'd also like to see more characters available in names (especially `-`).
One person wormed their way in and is executing a coup.
I was hoping to see a link with more information for: User-defined error messages for type errors. Looks like this is it: https://git.haskell.org/ghc.git/commitdiff/568736d757d3e0883b0250e0b948aeed646c20b5 Neat!
I wish it weren't so, but opencl is a joke compared to cuda right now. The tooling around cuda is fantastic and the libraries are as well. If you need to get actual work done, it's a no-brainer.
Who chose that color scheme? The Joker?
Yes
There are already arbitrary exceptions since a'b is one identifier.
This is a nice idea. But the main issue here is design. Do you want to make '?' a non-symbol character, as a special case? Then it would become forbidden to use '?' in operators. That would make this even more of a breaking change and very counter-intuitive. In my opinion that would be not worth it. If not - then what exactly is the new lexing rule? What is your design? You should write a very clear and complete specification. Preferably it should be a diff to the Haskell 2010 report, plus a complete analysis of the consequences of this change in all of its aspects. Given such a specification, assuming it is reasonable, I have no doubt that it would be accepted, and then merged after you (or someone) writes the patch. Being a breaking change isn't that much of a big deal as long as it is protected by a pragma. If done well, I could see use of that pragma spreading through the ecosystem quickly.
Thanks. I updated the article.
The standard libraries are based on a solid literature of published research. It was built around an initial implementation that was a clear consensus of a panel of top computer scientists. I don't think there is any other language which is backed by guiding ideas the way Haskell is. Nevertheless, most people agree that change is needed. Many years have passed since then, Haskell has changed from a research language that could potentially be used in industry to a language that is in fact becoming used quite extensively in industry. But there are conflicting ideas and strong opinions about what those changes should be, and it has been difficult to reach consensus. The bottom line is that I think slide 43 isn't wrong, but it could have been worded a bit better.
&gt; Prelude&gt; :set -XTypeApplications &gt; Prelude&gt; :t length @[] &gt; length @[] :: [a] -&gt; Int I think I'll stick with `Proxy`. It is first class, after all.
The problem with the missing Show instances is that some instances can't be derived (ie. `a -&gt; b`) at all.
There isn't just one though, that's what encodings are all about. I was annoyed at Python 3 for how they handled Unicode vs bytes, but in retrospect it seems pretty reasonable. Text and bytestring should be enough, but `[Char]` is always going to be a valid type.
no no,... My point was, let us fix some particular `C` and `D` where `a` *in fact does not* occur in `C`! Now, given `forall a. C[D[a]]` we cannot *for free* convert it to `C[forall a. D[a]]`. Even though `a` does not in fact occur in `C`, the mere fact of being true isn't enough! We need to *prove* this to ourselves by actually looking through `C` and observing the truth! but that process of looking and observing isn't free, it takes effort. It's perfectly *valid* to convert `forall a. C[D[a]]` to `C[forall a. D[a]]` —because in fact `a` does not occur in `C`—, it's just that we need to do some work before we can recognize this validity. Work which GHC (evidently) considers too onerous to bother with. In the other direction there's a difference. It's *entirely free* to go from `C[forall a. D[a]]` to `forall a. C[D[a]]` (modulo alpha), since (given the binding structure) it's *impossible* for `a` to occur in `C`. It's not simply a matter of being *true* that it doesn't occur, it's that this truth is guaranteed by the very nature of what syntax trees are even possible; we don't even have to look at `C` to convince ourselves! (N.B., the stuff in this paragraph is assuming we're working with some sort of abstract representation which keeps track of binding structures (or fakes it with the Barendregt convention or something); this direction is not free if we're working with the concrete syntax trees, because there may be some other variable using the same textual name "`a`" even though it isn't identical to the variable introduced by this quantifier.)
A big word of caution: a global un-scoped `--allow-newer` is a big sledgehammer. It can result in a `.cabal`'s conditional logic being corrupted and result in a package being compiled in a subtly incorrect way or not even compile at all anymore, which would have compiled fine without `--allow-newer` otherwise. This is part of the motivation of why cabal-1.24 now allows for more fine-grained control of the effect of `--allow-newer` ([PR#3171](https://github.com/haskell/cabal/pull/3171)) 
Useful check...
OK great! A C FFI via JNI would be a nice feature, but not as useful as one might think for combining the ecosystems. For example: We do not want a `network` library compiled against the low-level C API via JNI. That would render a significant portion of the JVM ecosystem useless. We need a networking library that works the way networking is supposed to work on the JVM. Another example: Compiling the `text-icu` library against ICU4C via JNI is not the right way to build that library for JVM. You need to compile it agains ICU4J. Using JNI would be a best at temporary hack that might slightly help in certain limited cases while we are waiting for a true JVM version of `text-icu`. The reality is that good ecosystem support for a new compiler with a new target will take a lot of work. There is no quick fix. But having a JNI FFI can help get the ball rolling.
Except there's nothing to disagree on here. Having a common release sets a goal for every library developer, who can then figure out among themselves what steps to take ***before*** the casual user comes in and try. Do you think using Haskell should be about toying around with packages all day or actually thinking about real problems ? You seem to think that erasing the problem from the slate by ***not*** having a collective release will make it dissapear with magic juju. It does not. Setting a collective release allows us to say to the casual user : pick that guy, it works. You say everything works readily YET you say it's a lot of work to synchronize 2000 packages. Which one is it ? Where do you think this work goes, if not in your pocket, wether you use stack age or not ? You just do not make any sense AT ALL
Please don't swear
What type of dependency between actions will trigger an Applicative constraint?
Applicative is needed if you have several `&lt;-`s that get passed to the final function: do x &lt;- Just 15 y &lt;- Just 13 z &lt;- Just 14 pure (x + y + z) desugars into (\x y z -&gt; x + y + z) &lt;$&gt; Just 15 &lt;*&gt; Just 13 &lt;*&gt; Just 14 In fact the desugaring with only one `&lt;-` is similar, but since no `&lt;*&gt;` is needed, the `Applicative` constraint ends up relaxed into just `Functor`.
The are a number of them, from `numerical-prelude` to `subhask`, and every author has a different choice of "granularity" of the class hierarchy, ranging from verbose to extremely verbose. The best treatment I've found so far is in the Purescript prelude (https://github.com/purescript/purescript-prelude/blob/v0.1.2/docs/Prelude.md). `Num` in that case inherits the arithmetic operations from various distinct algebraic structures, e.g. `(+)` and `(*)` from `Semiring`, subtraction (additive inverse) from `Ring`, multiplicative inverse from `ModuloSemiring`. Semiring subsumes the two monoids `(*, 1)` and `(+, 0)`. 
Nitpicking: I understand slide 9 to claim that the type `φ =&gt; (α → β) → α → β`, where `φ` is an arbitrary constraint, has a unique inhabitant. This is not true; it is certainly false if `φ` may mention either `α` or `β`, for example it may be `Default β`, but even if `φ` only contains polymorphic functions it may contain a false element of type `∀α.α` or more generally a potentially inconsistent constraint (for example coming from GADT equations).
&gt; How is that any clearer? It's clearer for the same reason you used `?` in your question. Without that, you can always interpret "imports" as "generate some imports." I usually add "does" or "is" but ? is more concise.
Whoops that's a typo anyway. It was supposed to be the type of map. For which, strictly speaking, the type isn't enough anyway (you'd need the Functor laws as well). I'll just remove that example to avoid the problem. Thanks!
I think the tinting is exaggeration. But the colours are rather ugly.
I just made myself a new account and apparently I don't have Ticket privileges. It won't let me create one. My username is 'deech'.
That's a lisp convention for "predicate". 
Good question, the answer is no. Pure still requires Applicative. The 'do' syntax is now able to change the example program into: (\v -&gt; v + 1) &lt;$&gt; x. As you can see, the de-sugared program does not use 'pure', which is why Applicative is not needed (but Functor is, because &lt;$&gt; is used).
First thing I thought upon seeing this was "Oh, I should show this to Ayberk!"
avoid $ success at all costs
Yes, this compiles {-#LANGUAGE ApplicativeDo, DeriveFunctor #-} data A a = MkA a deriving Functor f x = do v &lt;- MkA 1 pure (v + 1)
Thanks! Would be nice to publish a new edition.
Yeah, I think this is a cultural problem. The argument on the other side is that, even if you choose to not use the syntax extension yourself, you may end up reading code that does use it. This is a really weak argument, in my opinion, but it gets trotted out every time there is a divisive proposal.
&gt; The type tells you if it returns a bool. I don't see types for every identifier in scope, and reading code is much easier (at least for me) when there's some level of redundancy. &gt; Trying to rely on convention tends to fail In my experience, conventions are very useful, even if they fail occasionally. Perhaps our experiences are different, or perhaps I'm mistaken and conventions aren't as useful as I think they are, but right now I *feel* like having a convention for ending functions-predicates like `doesFileExist` with `?` (so it'd become `fileExists?`) would make reading easier, and coding – more pleasant. Oh, and I would stop being annoyed by long names (for some reason they're very annoying to me in a way that typing a long comment isn't).
Cant you just present this in a normal font in black on white? Your content is great but your presentation is, well, challenging.
The way to solve that is to make it impossible to have an empty list as its type: data Union :: [*] -&gt; * -&gt; * where UNow :: a -&gt; Union r a UNext :: Union r a -&gt; Union (a ': r) b Yes, I know it adds the values "backwards", but this makes it an instance of Functor. Ideally, it would be able to add the values to the end of the list rather than the beginning, but I can't figure out how to make type families do that.
Interesting, might depend on lightning and such then. Either way I'm unsure why someone would choose such colours willingly to begin with.
The type checker checks the *de-sugared* program, not the sugared one. So `pure` and `return` are dropped to `Applicative` or `Functor` whenever possible. The reason `Functor` doesn't have `pure` is because functors come from category theory, and the definition of functors in category theory doesn't include `pure`. A functor in category theory is, put simply, a function on a category. In Haskell, that category is the set of types, and the morphisms in that category are functions between those types. So a functor is just a type constructor that takes any type and produces a new type. It does not, however, impose any morphisms on the category. That is to say, it doesn't make sure that the category has a morphism (a function, in this case) from `a` to `f a`
I gonna have to play the -ddump-ds game a bit
These make me sad. :(
&gt; Trying to rely on convention tends to fail My interest in this case is about writing code, less about reading it. In that case you know what the function you want should do, but you're not sure of its name. The convention helps you guess more accurately.
The StackLove is strong with this one
SHM already works on per declaration only, and hindent works on either. So I think yep.
They can be derived if you import `Text.Show.Functions` ghci&gt; data A = B Int | C (A -&gt; A) ghci&gt; :set -XStandaloneDeriving ghci&gt; import Text.Show.Functions ghci&gt; deriving instance Show A ghci&gt; B 42 B 42 ghci&gt; C id C &lt;function&gt; 
Don't name your functions `function` or `f` or `g`, that's convention. Just number all identifiers (`_0`, `_1`, ...), install `ghc-mod` (if it can be), click `show-type`, wait for your ghci to load your code (if you module builds), and then read it.
The problem here is really that you are using typeclasses as though they are classes in an object oriented programming language when they are not. Typeclasses are, as the name implies, classes (as in 'classification' of types). For example, 'Num' is a type class, and any type that represents a number (Integer, Float, Double, Rational, etc.) has a Num instance. Similarly, Monoid is a type class, and any data type that follows the monoid laws (i.e. that is a mathematical monoid) has a monoid instance. The problem you are having is because you are writing functions where the return type is polymorphic (`Player p =&gt; Int -&gt; p`) denotes a function that takes an Int, and returns a value that can be converted to any type that has a Player instance. That is, the *value* returned itself is polymorphic. On the other hand, if you had `Player p =&gt; p -&gt; p`, this function would not be returning a polymorphic value, but a value of type p. The equivalent of classes in an object oriented language are *types*, and you should just create a Player type (using `data` not `type`) and a GameState type. Typeclasses are similar to interfaces used with generics as in your example Java code, but not the same because interfaces are really abstract base classes, so you can look at just the MyInterface subset of any class that implements MyInterface. You convert any class that Typeclasses are really more like [Concepts][1] in C++. EDIT: Actually, I think your Java code is incorrect. The generic shouldn't be there because in the end, you are returning a MyInterface, and the compiler should complain because it doesn't know which genericFunc to instantiate. [1]: https://en.wikipedia.org/wiki/Concepts_(C%2B%2B)
1. Hungarian notation as in “let's use a prefix to indicate function's return type” is indeed pretty useless, but I don't want an indication of the return type – I want an indication of the *entity class*. I have several “entity classes” in my head (lenses, predicates, traversals, actions, getters/“extractors”, monoid-like operations), and ideally I'd like indications for them all, but it's hard to think of fitting ones (apart from, perhaps, `!` for actions). 2. At least for me, `bFileExists` isn't as good a convention as `fileExists?`, on the basis of me feeling icky about naming a function `bFileExists`. Doubtlessly I wouldn't feel icky about it if it was an already existing convention, but since it isn't, we have to choose between “make people use `bFileExists`” and “make people use `fileExists?`”, and keep in mind that some people will hate one or the other (or both) choices. “I'm feeling icky about X” is a very bad way to predict how people would react to X, but there's other evidence – for instance, people have adapted to the “`#` for magical unboxed stuff” convention without much fuss (I think). Also, other languages use `?` and `!` (Ruby, Lisp). Also, there seem to be many people who are enthusiastic about `?` – while it doesn't mean that the majority of users would feel the same, it ensures that there'll be a core of enthusiastic users who can spread this convention further. 3. Even then, I'm still pretty pessimistic – my gut feeling is “if it gets implemented, it'll become widely used only with 25% chance”. However, this absolutely doesn't mean that I think it's useless. Lots of useful things never caught on for unrelated reasons. 4. There's a chance that using `?` as a suffix could be an *especially* good idea due to some reasons concerning the fact that our brains have already been taught to recognise and interpret punctuation marks in a special way. Case in point (I don't have access to others' brains, so I'll once again talk about my brain specifically): the `M` suffix (meaning “monadic”) has been around for a long time and I'm used to it, but `zipWithM` sounds almost the same way in my head as `zipWith` does, while `fileExists` and `fileExists?` sound very differently – which means that when reading code, `fileExists?` will be marked intonationally. I don't have sources right now, but I believe that I've seen some studies showing that adding intonation (and changes/interruptions/etc in general) tends to improve comprehensibility. On the other hand, I could be mistaken about everything, I'm not a linguist, the sample size here is 1, and I haven't provided the sources, so don't take this one seriously. It was just an attempt to try to find an explanation for my feelings about `?`-as-a-suffix.
When searching for a function, I begin by hayoo-ing the name that I think the function might have, which leads to very fast lookups when I guess correctly. So, anything that lets me guess correctly more often is good.
The first place I search is in my head - my RAM, if you will - I'm looking for ways to help me guess right. (Because my memory isn't getting any better...)
I love the reasoning 9 people agree so I won't. I can imagine some Monty Python stuff based on that.
What's not to love in library authors fixing stuff before you encounter them ?
Yeah, I've pretty much given up on the idea.
What's up with [this error](http://i.imgur.com/MKrHiqd.png) from the [MVar docs?](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.MVar.html)
That's actually a really interesting solution as it mirrors the way modules work, even syntactically. Why hasn't this been a more popular option? I guess it adds some unnecessary repetition.
Would you allow this? import PersonModule (Person) printPersonName :: Person -&gt; IO () printPersonName p = print (Person.name p) i.e. allowing record accessors to be 'imported' along with their constructor? If so I think this would be very beneficial. A hypothesis of mine about why people complain about imports in Haskell so much is that in many languages, you can import a bunch of stuff in one line. For example, if you import a class in any OO language, its methods and fields also come into scope. On the other hand, in Haskell, you have to explicitly import all fields and 'methods' as well. Of course, allowing what I showed above wouldn't be a silver bullet because many (most?) types are not records, so they wouldn't benefit from this namespacing thing. The closest thing we could do would probably be to always import everything qualified, and name all types `T` so you end up with things like `Text.Lazy.T` instead of `Text`. IIRC Elm or OCaml do something like this? It's not clear this is a desirable course of action anyway.
I would not expect a Stackage LTS version for ghc-8.0 any time soon, but hopefully we can move Stackage Nightly to 8.0.1 or 8.0.2 by next month. With a major release a lot of things tend to break but let's see how long it takes to settle down. Dan Burton just wrote a good blog post about it.
Map already has a Functor instance and you can't give it an Applicative or Monad instance 
I think it would be quite cool to haven an extensive list of the macros available, for the compiler and different projects. Kind of like a curated macro list :)
MIN_VERSION is listed [here](https://www.haskell.org/cabal/users-guide/developing-packages.html#conditional-compilation). I don't know about the other flags.
The list so far: * `MIN_VERSION_packageName(a, b, c)`: true if the package `packageName` is available in at least a version `a.b.c`
I wrote a short [wiki section](https://github.com/yesodweb/yesod/wiki/ghci#using-the-debugger-with-yesod) on this for Yesod that applies if you want to read it. I think you just need to mark the model as interpreted by using an asterisk: :add *Model This will also work with `:load` and `:module`.
The description here suggests that reverse mode AD is a little bit like backpropagation training for ANNs...? Is there any fundamental relationship?
Can't wait for LTS to be there and to play with nightlies ..
This paper might be a good place to start: https://www.semanticscholar.org/paper/Enumerating-Well-Typed-Terms-Generically-Yakushev-Jeuring/6b3507ba817e5329d46895faff3e9b85afd4f6ba/pdf (I believe it and the other techniques it cites tend to require you start with a type to generate your terms _at_ -- however, you can also generate random types as a first step) [edit: this is also related work, but takes place in the untyped setting of the plt-redex tool, so may be a bit harder to transport back to the haskell world -- http://j.eecs.northwestern.edu/~robby/pubs/papers/esop2015-fcphf.pdf]
They are the same thing essentially. In the real world though, AD is generally used in numerical and scientific software, while when someone says backpropagation that can only mean neural nets.
How would you ever write an instance for these type classes?
You could generate types at random, and then naively build terms based on those. Look into "focusing", AKA proof search, and apply the Curry-Howard isomorphism to proofs to obtain terms of the corresponding types.
[removed]
Nice one, thanks. And it's `stack upgrade` actually: $ stack --help | grep update update Update the package index 
Thanks so much to whomever does this LTS stuff. I downloaded GHC today and tried to build my project and dependency breakage just exploded everywhere. I started cloning repositories and widening bounds and gave up after an hour of fighting just `time &lt;1.6`. I'm not really even sure what goes into evolving the ecosystem--do we just... open issues/pull requests on all the github repos widening bounds and hope the owner is alive/paying attention? And god help us if the library actually did depend on a breaking change instead of just being conservative with version bounds...
Backpropagation uses a special case of reverse mode automatic differentiation. You can use reverse mode AD to implement it in a very general manner.
Those games are remarkably well done.
Yes, this is by design; the current version is very conservative about how selectors get disambiguated. (You're welcome to argue that the design should be different.) See [GHC ticket #11343](https://ghc.haskell.org/trac/ghc/ticket/11343).
Let's keep it constructive please..
&gt; I'm not really even sure what goes into evolving the ecosystem--do we just... open issues/pull requests on all the github repos widening bounds and hope the owner is alive/paying attention? Yes, that's part of the proper process. If the upstream maintainer does not react in a timely matter, you're encouraged to file an issue at https://github.com/haskell-infra/hackage-trustees For more details see also [Hackage trustee policy and procedures](https://github.com/haskell-infra/hackage-trustees/blob/master/policy.md) &gt; And god help us if the library actually did depend on a breaking change instead of just being conservative with version bounds... Personally, I try to avoid packages in my projects whose authors are too liberal with the PVP, as those are the ones causing me a lot more busy work as a Hackage Trustee than those that follow the PVP properly. So please don't see "conservative version bounds" as something annoying/disturbing, they represent the contract that holds the Cabal/Hackage ecosystem together until we find a better way than version numbers to declare package interdependencies. Maybe Backpack will show us a better way to do that, maybe something else.
A lot of work has been already going into cabal-1.24, what more would we need to improve in cabal to lure you back to the cabal camp? :-)
I don't understand why you'd even want to put energy into that. Stack seems to exist as a layer on top of cabal. If I use stack, I still use cabal and specify my project with a .cabal file. Stack does things like installing / switching compilers, selecting curated package repositories, etc. That doesn't really sound like something that Cabal would want to do. For me, the three killer features of stack over plain cabal are - Installs, and more importantly, manages compiler installs. I never have to decide to globally update to a new GHC and then struggle to build projects I haven't touched in a year. I know you can probably do that with Nix or whatever, but stack just solves this problem, everywhere, with zero learning curve - Builds are 100% reproducible, no combination of libraries fails, updating to a new version is a deliberate choice, I never have to specify (guess...) version ranges - Packages are shared between 'sandboxes', no recompiling the world for every new project I don't think Cabal would do No 1, and I don't think Cabal can do No 2. Without these, I don't see why I'd even consider using plain Cabal again. I think users of stack would be helped best if Cabal would add features that benefit everybody instead of trying to win users back (that haven't really left, we're still all writing .cabal files!).
I'm excited about `platform-libraries`, but that's cabal-the-library, not cabal-the-executable, right? If so, it can be used by both build tools. https://github.com/haskell/cabal/pull/2540
The paper "Adjoint codes in functional framework" by J. Karczmarczuk shows a RAD pattern based on "lazy time reversal", newtype Ldif a = Ld (a -&gt; (a, a)) lConst c = Ld (const (c, 0.0)) lDvar x = Ld (\z -&gt; (x, z)) together with unary and binary lifting functions llift f f' (Ld pp) = Ld (\n -&gt; let (p, pb) = pp eb eb = (f' p)*n in (f p, pb)) dlift f f1' f2' (Ld pp) (Ld qq) = Ld (\n -&gt; let (p, pb) = pp ep (q, qb) = qq eq ep = (f1' p q)*n eq = (f2' p q)*n in (f p q, pb + qb)) but I have no idea about the efficiency of this approach.
&gt; even with the best automatic nix / backpack combo I bet you'll need community sign off (which will come in cabal via nix I hope). I really wish there were no human involvement in this process at all. If I make a new lib, I'm going to use the latest packages -- so my new lib naturally has high minimum versioning because why would I waste my time trying it out with various old packages? So for example, my lib would have a minimum lens binding of `4.14`. But now [some other lib which was last updated a year ago](https://github.com/dzhus/snaplet-redis/blob/master/snaplet-redis.cabal) has an upper bound on `lens-4.10` because that's what was most recent when they last touched the project. And what else could they do -- there's no reasonable way they could have chosen a looser bound, how could they know their package would work with a future version? And yet now, it's impossible for someone to depend on both of these libs in their project. This is the exact problem I have in my project -- and I basically just end up downloading stuff from github and widening the bounds manually and setting my sandbox to local dependencies. I really don't see a human solution to this problem. It's a lot of extremely uninteresting work that nobody wants to do (but thankfully some try!). We need a purely technical solution.
It was just a general request to not swear
You're right about the Java code, thanks. I edited it to return `MyInterface`. I also am also of the opinion that excessively taking this type class route is a bad idea, I said so at the end of the article. However since stack overflow couldn't figure out the typeclass returning a typeclass problem, and since I couldn't find a decent write-up of associated types anywhere else, and since I've seen them in Swift and Rust, I figured I'd write it up.
It doesn't seem to have changed. Can you put up a complete Java sample that actual compiles? I think interfaces in Java cover both Concepts and Abstract Base Classes (using C++ terminology), and here you are using them as ABCs. 
if you dont like what you read, just dont read it and everyone is happy
I understand you both (of course you want to improve on cabal(-install) and of course stack is great) But from a users perspective it's frustrating - I still hope for some long-term security: will stack still build in 2years or will it drop dead once cabal get the next but different killer-feature. It's probably exactly what you don't want to do but wouldn't it be great for both cabal and stack come together and compromise on a good user experience?
This runs into the problem with all purely functional versions of reverse mode AD, computing the derivative of `x^n` takes `O(n)` time as it can't observe the sharing of values, it merely gets to share functions, which isn't good enough.
The conflict about version bounds is an important point. Specifying version bounds is a part of Haskell I was never comfortable with. As I mostly write executables and am generally too lazy to factor out components of them as separate libraries, I stopped specifying version bounds with stack. Whatever I'm using right now works, I don't know about anything else. If I were to upload any of this to Hackage, would specifying the version my snapshot uses as Major.Minor.* be a reasonable default for Hackage?
&gt; If I were to upload any of this to Hackage, would specifying the version my snapshot uses as Major.Minor.* be a reasonable default for Hackage? Yes, that's definitely better than leaving off version bounds. To help you with that you can either use the new `cabal gen-bounds` command or, if I'm not mistaken, `stack` has a similiar feature `--pvp-bounds both`, see https://www.fpcomplete.com/blog/2015/09/stack-pvp
This is precisely the correct answer. The only other CPP macros are ones defined separately by `Cabal` itself, which are listed in the link in /u/gelisam's answer. AFAICT, they are * `MIN_VERSION_package_(A,B,C)` (note that this is defined by GHC itself starting with GHC 8.0) * `VERSION_package` (if you just want the string. Also defined by GHC itself starting with 8.0) * `MIN_TOOL_VERSION_tool` (for things like `hsc2hs`, `ghc-pkg`, etc.) * `__HADDOCK_VERSION__` (when compiling via `haddock`) Since these are `Cabal`-specific, don't expect them to work with standalone Haskell files.
This paper from Koen Claessen, Jonas Duregard, and Michał H. Pałka explains how to do it quasi-uniformly with reasonable speed: http://publications.lib.chalmers.se/records/fulltext/195847/local_195847.pdf
Nice. At first glance it looks like the paper is trying to solve a more generic problem. But it turns out that the concrete example used by the authors to illustrate their methods is exactly what OP is asking about.
What I have done in the past is naively generate explicitly typed terms, choosing the type annotations at random whenever the grammar asks me for them -- this was for a variant of Church-style System F. Then you filter out to remove the ill-typed terms. In my experience, this works well: surprisingly many of these terms turn out to be well-typed, and you can exercise your implementation this way. Note that it is can also be interesting to exercize the type-checker by generating random types and building known-to-be-well-typed from this terms (for example the identity function and eta-expansions thereof). (I did not attempt to have a uniform distribution of terms or types. I use a fuel approach, where I randomly pick a fuel which is the final size of the term, and each n-ary constructor decreases performs a n-split of the remaining fuel. In my experience this strategy leads to term that are pleasant to look at and understand when you find a breaking counter-example and you need to debug.)
Not exactly. For `Maybe` that would be class Maybe a b where maybe :: c -&gt; (b -&gt; c) -&gt; a b -&gt; c Actually, with church encoding you wouldn't need the type class. Every ADT can be represented as a function type.
You might be interested in how I handled exceptions in my [Pusher websocket bindings](https://hackage.haskell.org/package/pusher-ws). - [Exception handling functions](https://github.com/barrucadu/pusher-ws/blob/master/Network/Pusher/WebSockets/Internal.hs#L318) - [WebSocket client](https://github.com/barrucadu/pusher-ws/blob/master/Network/Pusher/WebSockets/Internal/Client.hs#L42) The code is conceptually structured like this: ignoreAll . forever $ catchNetException (networked code here) (recovery behaviour) So I divide the exceptions into two classes: those which signal a transient network failure and can be ignored (hence the `forever`), and those which terminate everything. The transient failure exceptions are `IOException` (from the "network" library, iirc), `HandshakeException`, and `ConnectionException` (both from the "websockets" library). As for how I discovered those were the exceptions I needed to catch... trial and error. Some of it was apparent from the websockets documentation, but I only picked up on the `IOException`s by actually disabling my network connection as the program ran and seeing what happened. I then dug around in the dependencies of "websockets" and found where it originated.
I think this is a tooling problem that could be solved by: * adding a functionality to cabal that automatically suggests a version for package authors for the next release (by doing an API-Diff, see the elm package manager for examle) * adding a tool that automatically suggests package bounds to package authors by checking the APIs of the dependent packages and what parts of them where used by the package The first one should not be too tricky (except for maybe Package-Flags and CPP?), the latter one is probably a bigger chunk of work to get correct and efficient. Maybe one cold use the hoogle index for that?
Very cool. I'd like to try this as a haskell light/haskellscript for command-line scripts and general programming too.
The page is served as XML or something, I believe, so Firefox gets confused. I don't get that in Chrome, for example.
Now that 8.0 is out I'm considering moving to 7.4.
The thing is, stack takes care of everything. It fetches and installs the compiler, it does ghcjs, it does docker, it does build --file-watch. All these things are way out of the realm of cabal install. Aren't they? Anything that requires my coworkers to install ghc, then install cabal and or the platform, then install cabal-install, set up paths manually, etc just to run a script I've written is automatically a no go.
&gt; Do you know what are the supported `os`es? The `{os}_HOST_OS` macro is set when GHC [is being configured](http://git.haskell.org/ghc.git/blob/9bb277269ec020f138fe70a65f5972466113ab61:/aclocal.m4#l83) during an installation, so the possible `os` values depend on what `autoconf` fills in for the `host_os` macro when [`AC_CANONICAL_HOST`](https://www.gnu.org/software/autoconf/manual/autoconf-2.69/html_node/Canonicalizing.html) is used. That being said, I don't know if there's an exhaustive list of all values `host_os` can take on. &gt; I was quite surprised to find `ghcjs` there Did you mean surprised *not* to find `ghcjs` there? Because GHC itself doesn't specify that; GHCJS [separately defines](https://github.com/ghcjs/ghcjs/issues/132) the `ghcjs_HOST_OS` macro (when building JavaScript code) and the `__GHCJS__` macro (when the compiler is GHCJS).
If you want to use CodeWorld for something closer to Haskell, try http://code.world/haskell instead. That's all the same web-based UI, but without the language customizations and modified standard library. In that case, to draw graphics, you need to import the CodeWorld module, which is slightly documented at https://code.world/doc-haskell/CodeWorld.html
You might be right in the sense that most development on Haskell tends to be pushing the boundaries of what's possible instead of fixing problems that have already been solved elsewhere. However, much of the best improvements to even simple things (like records) can't be found in everyday languages, but in even *more* niche languages like Idris and PureScript. I'd love to see those developments get adopted by Haskell before Haskell 2020.
Built-in support for row polymorphism.
Fixing partial records (e.g. separating records from sum types) (a la Ur, Elm, PureScript, Idris, etc.)
More granular tracking of effects (a la PureScript).
I didn't even know about those, thanks! ;-)
I have this vague feeling that whatever one would write in the 'setup &amp; compile' chapter of a 2016 Haskell book will be bad advice by 2020...
Get partial functions out of the prelude. I have an Elixir friend who was pretty excited about Haskell -- runtime errors during his very first project helped kill his enthusiasm. EDIT: s/had/have. I'm not that hardcore about haskell:)
How do you traverse through a tree similar to mine, do you have any examples? Simply replacing a node is proving difficult.
They compose pretty well with [Ether](https://int-index.github.io/ether/)
Get the string situation fixed so Data.Text is at least in base and the default for new projects.
In what sense they don't compose? You can always put the errors in a sum type.
`liftM2` (`liftA2` is the same thing) is only used in instance declarations, so its usage merely requires that the type for which an instance is being declared is `Applicative`. Whether or not `Applicative` is a (transitive) superclass of `*Zip` is thus irrelevant in these cases.
I think every Haskell programmer has to feel uncomfortable with the thought that such an important side effect as throwing exceptions is not encoded on the type level. I haven't really looked deeply into any of the mtl/transformers replacement libraries out there, I'd love to read some kind of survey blog post comparing them all ;-)
I don't think the big ones are fixable lacking a line-in-the-sand like Haskell 2020. There is a hint of resignation in many of the slides precisely for this reason...most of this we have to live with until we have a obvious idea of how to clean up and redefine what Haskell is. How do you stop people from proliferating stringy types? All we have are a series of complaints and recommendations. Unless you are plugged in to the community, it seems that a new contribution is just as likely to perpetuate this problem. Same with the proliferation of pragmas....how can you really fix this? I'd much rather see the community put more immediate effort into Haskell 2020. Its needed and unavoidable. Otherwise I see Haskell becoming like C++...a powerful tool mired in best-practices lore. The big challenge to both C++ and Haskell are tools like Rust and Go that are fully enabled and usable given only their single language specs.
Wondering if it could ever include partiality as an effect? (a'la PureScript) or is it just too much...
Yay!
I disagree with this one. Debug.Trace encourages sloppy coding practices. It's actually easier to debug pure Haskell by writing small pure functions. These can be tested using QuickCheck, unit tests, or even GHCi. IO code doesn't need Debug.Trace in the first place. It would be a mistake to pull out nasty stuff like partial functions and lazy IO while adding Debug.Trace to such a prominent place.
This may be tricky, I can't immediately see how to compile a row-polymorphic function to efficient machine code. I guess you can piggyback the typeclass implementation and pass field accessors with records implicitly, but that would mean an argument for a row! E.g. if you have (made up a syntax) `f :: (Field a r, Field b r) =&gt; r -&gt; IO ()` (`Field a r` means record `r` has field `a`) you pass three arguments instead of just the record. I haven't checked, but if I had to guess I'd say that PureScript is not having this problem because under the hood it's using JavaScript objects/tables/whatever. EDIT: Actually, that's an interesting point. Can we have something similar just by automatically generating typeclasses and instances for record declarations?
Looking forward to the native metaprogramming proposal. Sounds like it will finally solve the problem where TH always lacks support for new GHC features. Is the concrete proposal online somewhere? I’d love to see what exactly is planned here.
I guess you mean "eliminate lazy I/O functions from `base`" ? There's no harm from having `unsafeInterleaveIO` somewhere in the standard library, it's actually useful in some cases. I also don't understand why you think lazy ByteString is a hack.
Is not that easy. `head` and friends are handy and somehow the first functions beginner encounters. Adding a Maybe layer on top of might have also killed your friend enthusiasm.
In years past when dealing with GSoC we haven't posted the detailed timeline / full proposal. I could ask each of the students if I can post their concrete proposals up. I've created a student mailing list. Once enough of them have signed up, I'll ask.
Are some classic extensions planned to be added by default ? I mean things like pattern/type synonyms, type families, tuple section, which doesn't really arms but just improve the syntax.
Another example to changing the parse when there isn't a space between tokens: with `TemplateHaskell` `$a` is parsed as a TH quote instead of `$` applied to `a`.
`-XNegativeLiterals` is exactly this. ~$ ghci GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Prelude&gt; -10 `div` 3 -3 Prelude&gt; :set -XNegativeLiterals Prelude&gt; -10 `div` 3 -4 Prelude&gt; 
Instance declarations in where statements. There are certain things I sincerely just want to have different sorts of orderings in different situations, and I'm sure other people have similar issues. Data declarations in where statements would be awesome as well.
Everybody I've showed Haskell has thought Maybe was an awesome replacement for the nulls of other languages.
I wasn't aware of that so maybe that should be the default.
That would be brilliant, but it's seems equivalent to "non-leaking orphans instance", which seems to be impossible.
Thanks! If they don’t want to publish their proposals that’s obviously also completely understandable.
Another advantage: is the code base is not that HUGE compared to other mainstream langage.
This is a really hard problem. How far exactly would we want to go with it? Full extensible effects are a bit excessive; free monads are slow, and EE versions of classical monads are weird. Maybe we could add effect labels just to `IO`? putStrLn :: MonadIO Console m =&gt; m () printFile :: (MonadIO Console m, MonadIO File m) =&gt; FilePath -&gt; m () But even this would require standardizing plenty of extensions. `IO` would have to carry a type-level list with it, and type inference would be kinda hard. There's just a lot of baggage that comes with solving this problem.
Right, but it would at least help slow the flow of new libraries using String:(
Lazy `ByteString` is, in almost all cases, used for streaming IO. Streaming IO should use an actual streaming type such as in `streaming-bytestring` or `Pipes`.
This [Overloaded record fields](http://www.well-typed.com/blog/2013/11/overloaded-record-fields-for-ghc/) proposal contains something similar to the typeclass mechanism you suggested.
The main reason I use stack, (and I'm willing to switch back to cabal ) is to be able to install and use different version of GHC . I'm also waiting for `cabal new-build` becomes `cabal build` so all tools (editors, etc ...) can work with ix-style cabal. 
Currently, it's barely used in libraries, so I would say there is no *practice* yet. I think pattern synonyms are great and I'd be happy to see more of them in the wild. They are especially well suited for making low-level programming tolerable, for example decoding/encoding bit-packed fields, or reducing constructor noise arising from more generic data representations (f-algebras, free/cofree, etc.). One could combine them with GADT-s and [some hacking](https://gist.github.com/AndrasKovacs/340cf0300a80d3d4ec66) to get views with stronger guarantees than the underlying data has. I'm unsure how bad idea it is to hide costly operations behind patterns. It could lead to confusion since we're used to O(1) pattern matching semantics. But I also think some relatively costly patterns are fully justified, for example O(log(n)) snoc views for persistent vectors, and more so if the logarithmic cost is lazy (only paid if relevant pattern fields are forced). 
That would be a bad idea. I would much prefer record field to return `Maybe` if the field is partial.
Sounds like you might want to use NonEmptyLists
The fact that it's already implemented means it at least meets the "bang for your buck" criteria. :)
Are you talking about the amount of 3rd party source or the compiler? (I mean, regardless of how **HUGE** 3rd party source was, a "go fix" thing would still **work** as far as I can tell...)
That's actually pretty clever. If we lived in a world where writing verbose code is ok, I think instances would be values that we pass around instead of magical constraints. data Monad m = Monad { (&gt;&gt;=) :: forall a b. m a -&gt; (a -&gt; m b) -&gt; m b, , return :: forall a. a -&gt; m a } sequence :: Monad m -&gt; [m a] -&gt; m [a] sequence m [] = return m [] sequence m a:as = do (m) a' &lt;- a as' &lt;- sequence as return (a':as') This would allow us to be very explicit about instances, and wouldn't need crazy stuff like `UndecidableInstances` anymore. We could write local instances for testing. We could get rid of constraints. There'd be a reasonable number of benefits. But obviously this is too verbose and impractical. Localized instances could solve part of the problem. We'd still have the mess that comes with constraints, but we've dealt with those just fine forever, so that's ok. But we would be able to be more explicit about instances, which would be a big plus. Plus, orphan instances would be totally fine if they're local. So for those functions where you really need `X` from another package to be an instance of `Y`, but you don't want to write an orphan instance, now you can! A local instance doesn't escape the scope, and dodges the pitfalls of ordinary orphans.
I'm curious - how do you distinguish lazy from streaming?
I still think this is a bit tricky. Imagine not specifying type of your function that uses dozens of fields. It'd get a polymorphic type like `(Field f1 r, Field f2 r, ..., Field f10 r) =&gt; ...`. So you have to manually monomorphize your function for performance.
Couldn't we add a pragma for partiality that triggers a warning? But since there are reasonable ways to use head (e.g. when you know the list is not empty), also add another pragma to make ghc ignore those uses of head. Something like {-# PARTIAL #-} head (x:_) = x {-# IGNORE_PARTIAL #-} foo xs = let bar = head xs in ... It could also suggest an alternative(headMaybe) and then we could add headMaybe to base(and do the same for other functions where appropriate). 
I feel like this is a step backwards if anything, and totally something we can implement on our own!
Unary minus should be completely killed, or, at least, not be the same bloody symbol as ordinary minus. Two basic possibilities: * Use say `~x` instead (I think Erlang does that) * Just get rid of it. Use `0 - x`. I'd actually favour the second option, together with negative numeric literals hardly any code would need to change. `0 - x` might look somewhat strange but it's a Lua kind of strangeness: It's there to make things simpler, more regular, more predictable.
Pattern synonyms can be invaluable when you need to match an external C/C++ API that uses lots of defines! It can let you mimic the API exactly without making ungoogleable haskellisms. (Too many neologisms?) I take advantage of this in the `gl` package. My personal recommendation regarding pattern synonyms is to not use a pattern synonym that loses information. Using `Data.Complex`, something like pattern Real a = a :+ 0 doesn't violate user expectations, f (Real x) = Real x f y = y is a verbose, somewhat slow, identity function. On the other hand pattern Real x &lt;- x :+ _ where Real x = x :+ 0 violates that expectation and captures even the non-real cases when matched against. We have view patterns for when you need arbitrary functions. I'm also finding that I like them for things that I commonly repeat between different data types. I have a million little syntax tree types that have some form of `App` constructor. Being able to make a typeclass for matching and constructing it, and wiring it up to a pattern synonym rather drastically reduces the noise in my code. The `lens` library adopts pattern synonyms for a lot of its class-based ad-hoc prisms, when running on a new enough GHC.
&gt; I think instances would be values that we pass around instead of magical constraints. That sounds like a good idea until you consider what happens when `Data.Map`'s `lookup` uses another instance of `Ord Int` than `insert`. Haskell and Rust are the only (big) languages that get this right: In any program, there ever only can be one `Ord Int`. If you need another one, make a newtype.
&gt; Can't wait ... to play with nightlies whatever floats your boat man ;-)
fields in sum type have some advantages, first they act as documentations, then you can use them in record pun and lens uses them to generate prims. For example, do you prefer data Customer = Company String | Person String String over data Customer = Company { companyName :: String } | Person { firstName :: String, surName :: String } usind record pun you can do display Company{..} = "Company:"++ companyName display Person{..} = "Person:" ++ firstName ++ " " ++ surName Of course you can argue that I can "explode" this in 3 classes data Person = Person {firstName :: String, surName :: String } data Company { companyName :: String } data Customer = CCompany Company | CPerson Person but then you are not arguing against sum/record types, but to stop ALL sum types to have more than one fields (so simplify ADT to C struct and union). This is a big step backward IMO. 
Everyone has a set of pet features they want, but what's great about Haskell is that even Haskell98 is lightyears ahead of the rest of the industry
&gt; My personal recommendation regarding pattern synonyms is to not use a pattern synonym that loses information. That's what my intuition would do, but it's nice to see written.
Strict data types in the Prelude.
1. It is not 100% clear what you mean by "non-continuous". Do you mean that it is discrete, or that there is some sort of discontinuity in it? (I mean, I expect the former, but if you mean the latter, the topology is relevant.) 2. With 200-300 objects in space at once, unless you are doing something quite spectacularly real-time for a MUD, you basically don't have a problem; just about the only thing you can't do is be continuously doing the naive O(n\^2) collision check too often, but almost literally anything even slightly more clever than that will work. I'd start with as simple as possible and worry later if it really does go too crazy. Just boxing the "space" in chunks roughly the size of the scanner and doing the naive thing within each chunk would go a long ways towards solving your problem, assuming the objects can't all be in the same place. (i.e., if most of them are "planets", you're fine; if they're all mobile space ships this isn't so good.) Now, if there happens to be an off-the-shelf library that does everything you need and is also really fast, sure, use it, but you don't have that large a problem.
Yes, I would love that! Same with `toInteger` from `Integral`.
I disagree. I don't think Debug.Trace is nasty, especially not in the same way that partial functions are. Sometimes you need to peek into your function with real data and setting up a test case is too much work. That being said, it would be nice if the functions in Debug.Trace were marked as deprecated like [`ClassyPrelude.undefined`](https://s3.amazonaws.com/haddock.stackage.org/lts-5.18/classy-prelude-0.12.7/ClassyPrelude.html#v:undefined) so that they would emit warnings. 
3rd party.
It's in [base with the release of 8.0.1](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html), but not in the Prelude yet. I think it's possible this might change with future proposals to the libraries (e.g. making Semigroup a superclass of Monoid).
Infinite sequences of characters maybe? Can lazy Text do the same thing? 
That's a good start. Might be the way to remove all partial functions from the Prelude.
Only if you don't have to use the "forall" to be able to us it.
Being able to differentiate data and codata (finite list vs infinite ones).
It doesn't matter what shape your data structure has: to traverse a recursive structure, you need to use recursion. -- a helper function for the other examples -- replace 0 with 1 traverseInt :: Int -&gt; Int traverseInt 0 = 1 traverseInt x = x data List = Nil | Cons Int List -- replace 0 with 1 throughout, using recursion traverseList :: List -&gt; List traverseList Nil = Nil traverseList (Cons x xs) = Cons (traverseInt x) (traverseList xs) data Tree = Leaf Int | Branch Tree Tree -- replace (Leaf 0) with (Leaf 1) throughout, using recursion traverseTree :: Tree -&gt; Tree traverseTree (Leaf x) = Leaf (traverseInt x) traverseTree (Branch l r) = Branch (traverseTree l) (traverseTree r) -- two mutually-recursive data structure data EvenList = EvenNil | EvenCons Int OddList data OddList = OddCons Int EvenList -- replace 0 with 1 throughout, using mutual recursion traverseEvenList :: EvenList -&gt; EvenList traverseEvenList EvenNil = EvenNil traverseEvenList (EvenCons x xs) = EvenCons (traverseInt x) (traverseOddList xs) traverseOddList :: OddList -&gt; OddList traverseOddList (OddCons x xs) = OddCons (traverseInt x) (traverseEvenList xs) Does that help?
&gt; but then you are not arguing against sum/record types, but to stop ALL sum types to have more than one fields (so simplify ADT to C struct and union). It would be quite enough if sum types with named fields were abolished. The advantages that you cite are all bad practices IMO. It really doesn't hurt to write display (Company name) = "Company: " ++ name display (Person firstName surName) = "Person:" ++ firstName ++ " " ++ surName
Can you give an example of a problem that can be elegantly solved with your proposal and for which the existing solutions are insufficient?
&gt; peggying likewise
&gt; (I just wondered whether drop is total with respect to its first argument: It is, negative numbers are treated like zero.) And... should they?
I mean for example, changes which collide with the actual record syntax. My point is more, there is no point to reopen a "why haskell sucks thread", if Haskell 2020 is just about default some already existing extensions.
Did you actually *use* getArgs in your code anyplace?
Can you please post the name of the student who is going to be working on each of these projects?
Cool, thanks for the tips! I believe I've just solved Problem 2; here's what I've got: In order to find a contradiction in function composition, I first took the simpler category (which is correct) and played with it a bit: (f . g) . id[A] = id[A] . id[A] = id[A] f . (g . id[A]) = f . g = id[A] Ignoring the still-bad notation for subscripts, we can see that in the first category (the correct one) `(f . g) . id[A] == f . (g . id[A])`, which satisfies our associativity law. So now that we know what should happen if the category is indeed a category, let's try the same with some functions in the second category: (f . g) . id[A] = id[A] . id[A] = id[A] f . (g . id[A]) = f . g = id[A] Ok, `(f . g) . id[A] == f . (g . id[A])` still holds, but this needs to work for _all_ combinations of compatible morphisms, so let's try another combination: (f . g) . h = id[A] . h = h f . (g . h) = f . id[B] = f Woah, what a minute! Looks like `(f . g) . h /= f . (g . h)`! Now we've yielded two different morphisms simply by trying to use `.` associatively. Since we're assuming `f` and `h` are not equivalent (otherwise one would be redundant), this must mean that associativity doesn't hold here. So that makes a ton of sense, and was quite easy when you just plug the stuff into the functions :) Again, if I'm wrong, I'd love to know why, but this definitely feels like the right answer and reasoning. Thanks for the help! Now I'll take another stab at the first problem.
I am *slowly* working on inplementing RETE http://www.drdobbs.com/architecture-and-design/the-rete-matching-algorithm/184405218 At the moment I am getting comfortable with compdata and compdata-param libraries. If this sort of rule engine is interesting for you, would you like to collaborate? 
I see what you mean. I suppose my gripe with drop's partiality (not sure that's even the right word in this context) is more the fact that `drop _ [] == []` than negative arguments. Because it breaks could-be-natural law that `length (drop n xs) == length xs - n` Negatives also break that, but more in a way of "it usually doesn't make sense to provide them as arguments". I actually agree throwing an error on negatives would be worse. I'm the one who was off-topic to begin with, sorry about that.
Again, the ho-rewriting or compdata packages have rewriting mechanisms you can use to make your life easier.
That is indeed correct reasoning for why associativity doesn't hold (you found a counter-example) and as such this can't be a category (since associativity doesn't hold).
That is a very bizarre comment you got. Maybe they thought you were reading environment variables instead of command line arguments?
Very left-field, but have a look at diagrams envelope. http://projects.haskell.org/diagrams/haddock/Diagrams-Envelope.html There should be n-dimensional bounding box logic somewhere. Added bonus is you could then draw the space with a few extra lines of code.
With all due respect, this is nowhere near true
&gt; Because it breaks could-be-natural law that length (drop n xs) == length xs - n ~~That would only hold for `n &gt;= length x`, so I think you'd have to use a dependently typed language…~~ That would only hold for `n &lt;= length x`, so I think you'd have to use a dependently typed language… &gt; I'm the one who was off-topic to begin with, sorry about that. Oh I wouldn' worry about that. I always assume that everyone uses the Reddit Enhancement Suite and minimizes threads they don't care about! ;)
Why not change them to use NonEmpty?
Yeah, I dropped out of school to work professionally as a programmer and somehow found my way back here, haha. There are definitely a lot of holes in my knowledge like this. I'll take a look at some of those courses, thanks for the tip :)
I'm developing a voice control framework, to conveniently interface with different speech recognition engines and operating systems. Dragon NaturallySpeaking has an unofficial extension called `natlink`. With it, you can define a custom grammar to be loaded, register callbacks that are triggered by successful recognition, and perform corrections. It's a python dll. Currently, I generate a python shim that communicates with my Haskell server (it just POST's the recognition). For example, DNS can run in a Windows VM, and the server can execute actions on OSX. I want to replace this with a Haskell dll that DNS can directly load. Reasons: (1) Because of the difficulty of concurrency in python, I can't embed a server. Thus, providing DNS with realtime context (e.g. activate different grammars for different emacs modes) is slow and awkward. This "client driven" communication can only happen during the "speech detected" callback, and the handler (i.e. sending a request, receiving a response, marshalling it, updating the speech engine's) must block the recognition. With a bidirectional channel, I would synchronize context across the "client" and "server" in a thread, and perform the quickest possible operation during "speech detected" (i.e. reading from an `TVar`, which holds data in the exact format that DNS needs, and which is written to frequently). (2) Type safety and sharing languages between services is always helpful. I've spent more than a few days failing to build a trivial dll on windows (that works with visual studio, and only includes `windows.h`). The more that `platform-libraries` does, the more effort I can spend on porting an enormous unsupported proprietary C++ binding from the 90s. 
Isn't that why we have ceiling and floor: Prelude&gt; :t ceiling ceiling :: (Integral b, RealFrac a) =&gt; a -&gt; b Prelude&gt; ceiling 5.5 6 Prelude&gt; :t floor floor :: (Integral b, RealFrac a) =&gt; a -&gt; b Prelude&gt; floor 5.5 5
It was topologically sorting. My hacked version uses a Wengert list. I should give the new version a try. Sadly, we never did use AD in anger after my initial experiments. 
I believe you can encode `Either` separately too, since this is just Church: class Either e a b where either :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; e a b -&gt; c
Wow, that's pretty cool! Is this an open source project?
I wait for '80 - `nightlies`... Seriously why not have 3rd branch of packages with ghc-8.0 as a main compiler like `--resolver future-2016-05-24` (lets vote for name :) )... this would seed up adoption of the new compiler and keep stable people stable.
You can copy-and-paste (plus an emacs macro) C constants. e.g. virtual keys pattern VK_A :: VK pattern VK_A = 0x41 ... https://github.com/sboosali/workflow-windows/blob/master/sources/Workflow/Windows/Types.hs#L187 I'd like ports and status codes too: pattern HTTPS = 443 ... pattern HTTP_SUCCESS = 200 ... pattern EXIT_SUCCESS = 0 ... 
Honestly, at the point where you take away my ability to write normal infix arithmetic like every competent language has supported for thirty years, I'm done with Haskell. It's bad enough already. How about adding prefix and postfix unary operators instead? Then we could have unary minus and also factorial! (Pun intended.)
Ah yeah, then it would definitely have had performance problems! The Wengert list version is pretty fast but not currently made of unboxed doubles. If I get some downtime maybe I'll bang out `ReverseDouble`.
&gt;expressivity &gt; proof &gt;There is no reason to limit our specifications to what we can prove, yet that is primarily what type systems do. There is so much more we want to communicate and verify about our systems. This goes beyond structural/representational types and tagging to predicates that e.g. narrow domains or detail relationships between inputs or between inputs and output. Additionally, the properties we care most about are often those of the runtime values, not some static notion. Thus spec is not a type system. yea and stuff, I think. wat. today has been a confusing day
Can you please indent your code by four spaces? Markdown-style "```" fences don't work here.
Interesting, thanks again for the insight!
The Khan courses are a shade on the basic side, I've found, but if you work through a few chapters of [Abstract Algebra - Theory and Applications](http://abstract.ups.edu/aata/) you should be doing fine. Or, really, any work on abstract algebra will likely leave you better placed to tackle category theory than linear algebra, save where linear and abstract algebra intersect around things like cartesian product.
I always forget that `Text` isn't in `base`. It's pretty bad that `base` doesn't at least *have* a real text type.
I don't think that `length (drop n xs) == length xs - n` should hold. The empty list is simply the `drop` function's [fixpoint](https://en.wikipedia.org/wiki/Fixed_point_%28mathematics%29). I don't see anything wrong with that as long as it's acknowledged.
What are the Hackage improvements being worked on?
AFAIK, according to the original [`Applicative` paper](http://www.staff.city.ac.uk/~ross/papers/Applicative.pdf), you need `Applicative` for zipping. `Functor` won't cut it. See also: [`ZipList`](https://s3.amazonaws.com/haddock.stackage.org/lts-5.18/base-4.8.2.0/Control-Applicative.html#v:ZipList).
Isn't `Zippable` the same as `Applicative` but without a unit? Obviously the purpose is more list-oriented, but `zip_` is a monoidal operation just like `(&lt;*&gt;)`, and `zipWith_` just adds use of the functor to apply a function. The only reason it's different than `Applicative`, besides lacking unit, is that instances are meant to be more like `ZipList`'s instance of `Applicative`. The laws and methods are pretty much the same. So I guess the `Functor` super class makes sense then. `*Zip` is basically just asking for an alternative instance of `Applicative`.
Oh wow, that's awful.
[Nope](https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=15624)
I mean, we could just write fibs = fibs' 0 1 where fibs' m n = m : fibs' n (m + n) But that doesn't look as nice.
Thank you to those who donated to make this possible!
From the project proposal: Hackage 2.0 offers a lot of features but some of them aren't fleshed out fully or could use some additions improve usability and functionality. This project aims at addressing a few of them. -&gt; Issue Tracker: -&gt; for filing bugs in Hackage web interface and other hackage related issues with a hook to notify the developers -&gt; To handle package requests: There are two types of requests that can be filed -&gt; Bounds Request: Request to change package metadata -- i.e. upper/lower bounds on dependencies. This can then be done with a ""revision"" by a maintainer or a hackage trustee without a new upload. -&gt; Orphan Request: Request a package to be disowned, e.g. when the maintainer is inactive and the package has been flagged out-of-date for a long time. -&gt; Modifications to the Package Page: -&gt; Allow for provisions to add wiki/mailing list links for relevant discussions: Very often, at least with older packages, there seems to be very few usage guidelines and examples. Linking to discussions elsewhere can get a new user upto speed -&gt; Keep track of reverse dependencies (Related Fun Addition: A visualization that maps dependencies and reverse dependencies across all packages (I just love graphs!)) -&gt; A better interface for votes: It's not very usable right now. Making the UI for voting more obvious and easy to use would mitigate e.g a clickable star based system -&gt; Add a last updated stamp (very few packages have it at present. It's not easy to identify how recent/relevant the package is) -&gt; Tags and Categories: -&gt; Abandon categories and switch over to tags completely. Categories and Tags seem to more or less serve the same purpose. It makes sense to have multiple tags describe a package as opposed to having it being fit into a rigid category. -&gt; Social tagging: (This can be an opt-in feature for maintainers) A system to allow for any user to propose new tags with only the maintainers allowed to accept. This will greatly increase package discoverability. Again, hooks would need to be added to notify the maintainers of any new suggestions. -&gt; Better searching: Searching at present doesn't offer very much. Package Name | Author | Version | Description | Tags | Last Updated | Votes | Downloads Using a template like above on the web interface -&gt; Extend search to filter based on tags (multiple tags), module names, author names etc -&gt; Allow for packages to be ordered by downloads, update times, reverse dependencies etc -&gt; Allow for auto completion by partial name search More detailed goals were listed as: "-&gt; Issue tracker At present, to file any issues, users will have to contact the authors/maintainers (who may no longer be active). By having a public tracker for hackage related issues, issues will have a much higher visibility and thus, a higher chance of being resolved. As for implementation, a new acid-state instance with a web front end should be straightforward. A sample schema can be like in (https://gist.github.com/SooryaN/10e7cdfd5ced92561c919537890d092b#file-bug-yaml). -&gt; Reverse Dependencies Keeping track of reverse dependencies could help usability in a number of ways. For instance, along with efficient tagging, it'll allow for suggesting related packages, for exploring, and also for calculating a PackageRank. The implementation is still an open issue though. It could be implemented using an external store or something simple like a csv or a json file. Using acid-state to simply count and list the reverse dependencies without storing the actual branching details is also a possibility. -&gt; Votes This will involve reusing the code behind http://hackage.haskell.org/packages/votes to accommodate an out of 5 stars rating system. The average rating of a package will be appended to the search/ browse page. The user can click and vote on packages either in the search page itself or in the individual package pages. -&gt; Tags At present, the `.cabal` file of a package has a category field. The same field can be extended to hold multiple values that can be parsed into tags. This would ensure that existing packages aren't affected in any way. -&gt; Search Relevant issues to fix: - Substring search ([#208](https://github.com/haskell/hackage-server/issues/208)) - Searching and Coalescing Tags ([#27](https://github.com/haskell/hackage-server/issues/27)), ([#24](https://github.com/haskell/hackage-server/issues/24)) Additions: This will involve redesigning the present search page and devising a method to integrate substring search into the present keyword-based search in order to implement auto completion and partial name search. One approach would be to build a prefix tree with the words in the corpus as keys and their frequencies as values. " Sorry for any formatting mess.
Nice start! I'd forgotten this was there. Now just need postfix, some better syntax for fixity, a way to get rid of the spurious need for parens in postfix expressions, and some special-case code or something to overload -
If you want to use real Haskell for `id[A]` you can write `id @A` in GHC 8 GHCi, version 8.0.0.20160511: http://www.haskell.org/ghc/ :? for help ghci&gt; :t id id :: a -&gt; a ghci&gt; :set -fprint-explicit-foralls ghci&gt; :t id id :: forall {a}. a -&gt; a ghci&gt; :set -XTypeApplications ghci&gt; :t id @Int id @Int :: Int -&gt; Int ghci&gt; id @Int 10 10
I don't find them harmful enough
I want more dictionary definitions and etymologies from the Haskell community **Edit**: You might think [“helicopter”](https://en.wiktionary.org/wiki/helicopter#Etymology) comes from “heli” and “copter” but it's actually (yes: words are instances of `Num`) Ancient Greek hélix (“spiral”) + Ancient Greek pterón (“wing”) [“sophomore”](https://en.wiktionary.org/wiki/sophomore#Etymology): Ancient Greek sophós (“wise”) + Ancient Greek mōrós (“fool”) [“oxymoron”](https://en.wiktionary.org/wiki/oxymoron#Etymology): Ancient Greek oxús (“sharp, keen, pointed”) + Ancient Greek mōrós (“dull, stupid, folly”) so the origin of ‘oxymoron’ is itself an oxymoron.
It depends on what's visible to the compiler. Your polymorphic function can be specialized at its call site.
Hear, hear. You shouldn't chop your house down with an axe to look inside the walls if you could have just drilled a small hole.
I knew something would get me to move on to a new language, I guess that'd be it.
Just add `~`s everywhere! 
I agree, though if you look at the very history of Haskell, it was created to be a common substrate upon which to experiment with language features -- a lingua franca for non-strict semantics. So, given the job of the language, it isn't terribly surprising that over the course of the last 20 years or so, we've picked up more than Rust has since only really reaching 1.0 state about a year ago. ;) I'd be disappointed if we hadn't.
`Text` feels quite alien to someone used to the rest of Haskell. You can't efficiently cons or snoc or append. It doesn't share combinators with the rest of our code. It has to be imported as a qualified mess. Just saying we should make `Text` the default doesn't address any of these concerns.
You can't do simple pattern matching on `Text` characters, IIRC. 
Rich Hickey's writing about types often has a thread of "we have to leave this unspecified, so there's no sense specifying anything" that is, for some of us, cringeworthy. That said, when we get past that, there often is plenty of value in the rest of what he says. Here, it is certainly the case that there are things we care about that we can't check statically. Checking them dynamically can help improve the chance they hold, and can also serve as documentation that is unlikely to go stale. Of course, dynamically checked contracts are nothing new (see, for instance, Eiffel), but this particular point in the design space seems to be.
I gave a short presentation on this at Hac Phi last Nov, 2015. I did receive some valuable feedback then; and since I've finally gotten around to working on this again, sprucing it up for some of my own projects, I'd love to have some more feedback. So try it out for something if you'd like, contribute something or just look it over, I'd appreciate a few more eyeballs on this, especially with respect to interface design.
So you are asked to write FizzBuzz I guess and they complain on an import (instead of the actual place where you might have missed something) with a strange comment? My "warning BS interview" indicators just went crazy. Any chance you share the rest of the comments / code so we can have a better look?
Exceptions are an effect it would be theoretically nice to track... but if we're talking bang for buck, it seems the lack of consensus on a sensible way to handle errors and exceptions in types would exclude it.
That would be awesome, I really liked that about purescript when I used it. On the other hand, it makes Haskell even less accessible to newbies. Now they have a lot more typeclasses to learn about, even for concepts they already know. Ask your general programmer what a ring is and they'll look at you funny... Ask them what addition and an additive inverse are and they can probably answer you.
There's the `Witherable` type class that has some interesting laws on Hackage.
Your definition of 'proper rounding' is odd. There is a reason that the IEEE standard chose round to half even as the default rounding mode. While your customers expect round half up, I have to ask if that's what they really want? I.e. do they want a biased operation? Do they truly understand the issue? Anyway, it's too bad that you can't change it to your desired rounding mode.
Maybe we should introduce a new extension : ScopedTypedVariableWithputForall ;-)
I'm hoping that access to Java's vast platform can allow us to finally have a good solution for GUIs. And then on top of that build an nice FRP library to make GUI programming fun.
That's really awesome! I wanted to move from configurator for some time now, because of same reasons you list. But I really like the syntax and features of configurator, and all other tools seem over complicated and under-specified, like yaml. Or have hard to understand syntax, like TOML dict arrays.
But `Functor` is a superclass of `Applicative`. Note, `MonadZip` is alternative "Applicative" (no pun intended) or actually "Apply", because there are no `pure`.
ok maybe they just wanted to get fancy with Haskell with only basic knowledge then (probably the interviewer is not really one of the persons doing the job)
I really dislike the *"As we cannot have expressive enough (and simple) type-system, so better not to have one at all"* - attitude. And I don't really see how this is superior to `newtype/data` with hidden constructors, in the cases where invariants cannot be expressed constructively. Except of subjectively more (objectively = different) boilerplate.
You can use `bitmapOfForeignPtr`. Just set the format, allocate memory and fill it with data, for example by using storable vectors, storable repa arrays or Foreign.Marshal, especially the withArray functions
Apparently there is already an extension it : `NegativeLiteral`.
For the first point: [`hackage-diff`](http://hackage.haskell.org/package/hackage-diff`). It doesn't suggest the new version, but shows what changed. And yes it leverages hoogle-index. Second is tricker. There is [`packunused`](http://hackage.haskell.org/package/packunused) which relies on GHC's `-ddump-minimal-imports`, so theoretically you can see what you use from other packages, check against hoogle indexes which versions provide those and have better *initial guess* for which bounds could work. Yet, because of semantic / operational / others non visible in types changes, I wouldn't rely on those.
Have you thought about asking bos to pass maintainership of configurator to you and release this as 1.0? AFAIK bos is no longer doing much coding, and breaking API with major release would not be such a huge issue. At least not for me :)
I have no idea how I didn't notice that function in the documentation. Thanks!
A proper module system! The library ecosystem as it is currently is simply not scalable, and is already balancing on the edge of collapse for quite some time. Yes, this is a very hard, unsolved problem. But I believe a proper module system would be a step in the right direction (unlike non-solutions like version bounds and curated collections).
Yes. ghci&gt; import Data.Text.Lazy as T ghci&gt; T.take 10 (T.pack ['a'..]) "abcdefghij"
Let's stop calling it `drop` from for a moment, it makes it harder to accept thought experiments. There are multiple viewpoints a `removeFirsts` function could be made to stick to. The `drop` viewpoint (in its current implementation) is total, pragmatic, practical and easy to use, at the cost of having a complicated English definition. "Remove the first max(n, length xs) elements from a list" is mathematically beautiful, but really not something I've actually needed often. I've used it this way simply because I was too lazy to add a length check, and the rest of the program was sound enough that the case never happened. Another viewpoint, one I *have* often needed, is to "remove the first n, exactly, elements from a list". It happens to work with `drop`, *provided the list is of a correct size*. But what would I want it to do on an ill-formed list? An error. A runtime error is easy to implement; a compile-time one would be better. (Yes, I realize that sort of function is more painful to implement and to keep lazy in a sensible way.) In this context, I wouldn't want `removeFirsts i` to *have* a fixpoint at all for `i &gt; 0`. I don't have a use case in mind for wanting to remove max(n,length), but I'm confident they exist. That's what we get for trying to bind semantics to natural list operations, where the definition of natural lies in the eye of the beholder.
Last week I had a look at hot-code swap in Haskell, so you're post was very welcome. I put a little toy example together [here](https://github.com/nmattia/haskell-hot-swap) which showcases how you'd keep a connection open. I'm very annoyed that I have to link everything by hand though.
That's the kind of anwser I was afraid of ... I will try to see if I can use the exceptions library 
Well, yes, `Either` has a Church encoding just like any other ADT. But the approach OP is not to use the Church encoding itself as the data type, but rather to provide the Church encoding type signature as the method of a type class and then to implement a concrete data type as an instance of the type class. /u/tathougies and /u/theonlycosmonaut pointed out that with that approach, you can't get rid of data types completely; you still need a small set of built-in types to bootstrap the process. They mentioned tuples, i.e., product types; I pointed out that you would also need `Either`, i.e., sum types. But if you want to use Church encodings directly, you not only do not need any of those built-in concrete types, you also do not need the type classes. You only need functions.
Do you know if Backpack will be included in GHC 2020 too?
You could try the [`gloss-raster`](https://hackage.haskell.org/package/gloss-raster) package. It allows you to show animated [`repa`](https://hackage.haskell.org/package/repa) arrays or functions from `Float`(time) to `Point` to `Color`.
Are you doing anything to validate identity + student status etc? I guess at this scale the risk of being cheated is small so perhaps it's not necessary.
Have you released it? Shame this didn't get much attention, lots of official news lately and stuff. I cannot give meaningful comments as I'm not using any GHCJS on the frontend, but I find the idea really interesting!
I think I tried to do this in some of my code and surprisingly found out that there is some loss of performance. I didn't investigate it too much though.
I second this recommendation, that's what I used and I think there's an example in gloss-example.
woot https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List-NonEmpty.html#t:NonEmpty https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Semigroup.html#t:Semigroup and with default (&lt;&gt;) :: Monoid a =&gt; a -&gt; a -&gt; a (&lt;&gt;) = mappend 
Are you happy with happstack using lazy IO? And also I remember talks about problems with acid-state scalability, and proposals to switch to some kind of RDBMS. Is this still an issue for hackage now?
What about making `Data.Map.Map` carry its own `Ord` instance? Like C++ STL? Oh god wait... Like C++ STL... Edit: A few seconds after I posted the above I realized that it probably won't work that easily. Now you'll need all `Map`s to carry around another *type* parameter *just* to make sure you don't mess it up with multiple maps with the same key type and all these stuffs. Can work, but definitely needs some serious design 
No one is preventing new users from using Debug.Trace or debugging their code. They just have to import a module, so what. The fact that they import a module is an indicator that they made a mistake somewhere. Maybe what we should actually do is make the default prelude follow all the best production coding practices and have an official "newbie prelude" or "teaching prelude" or something like that that has all the bad but easy things in it. Then language introductions can start out using this "easier" prelude but say "if you actually go into production you need to switch to *this*".
isn't this what `keyDbName: ...` is used for (see https://hackage.haskell.org/package/groundhog-th-0.7.0.1/docs/Database-Groundhog-TH.html)?
I intentionally did not give you an implementation for your specific structure, because you're trying to learn, and I think it will be a much more valuable learning experience if you figure it out on your own than if you just copy-paste my solution. Still, if you really think you'd learn better by studying a solution than by coming up with one, I can give you one if you insist. Otherwise: I gave you examples for many different data structures, but like I said, the same approach works regardless of the shape of your data structure. As an exercise, how would you implement the following function? -- | replace the subsequence [0,1] with [10,11] everywhere. -- -- &gt;&gt;&gt; replace01 [0,1,2,3] -- [10,11,2,3] -- &gt;&gt;&gt; replace01 [1,0,1,0,1] -- [1,10,11,10,11] replace01 :: [Int] -&gt; [Int] replace01 = ???
[I've had some thoughts about dependency tracking](https://gist.github.com/rampion/015fa65ac654a2947045), if you're interested.
I can tell you - pretty much factually - that it is not.
2 is never going to happen, sorry. This is something so totally fundamental to Haskell *since it was invented* that changing it is basically a giant waste of time over pretty much, well, nothing.
It seems to me that there's nothing wrong with _unidirectional_ patterns that lose information. For example pattern RealPart a &lt;- a :+ _ pattern ImagPart a &lt;- _ :+ a (though this is of little utility in this case).
Here's the ho-rewriting example modified a bit. http://lpaste.net/164449
Wait, happstack switched to some kind of streaming? I was not following it, and may have forgot what all the fuss was about iteratee/enumerator/conduits/pipes few (6?) years ago. Of all HSOC projects I think hackage is the one with biggest chance of success. And I'm not arguing about adding persistence change to it, just was curious about acid-state.
I love Debug.Trace. But I also love being able to remove it as an import as part of making sure my code is "production ready" :-)
Well, in this case, by dependency tracking I mean keeping track of which `ConfigParser`s depend on which configuration keys, not in terms of cabal packages...
That would certainly be a possibliity; though I wouldn't ask until I'm nearly ready for a public release. (And I feel there's still quite a few things that should be dealt with before a public release: you might peek at the TODO I added.)
Interesting. But what if a given node can refer to itself? Eelco's Nix dissertation uses (for the intensional store model, which isn't used in Nix today, but might someday) hashes modulo self-references, where you detect self-references, zero them out, hash the result, and then plug the self-references back into the holes with the computed hash.
Do I understand correctly that monomorphic reverse-mode AD is at least tractable? I would definitely have a use for Double-specialized RAD, and would write it myself, if I understood the intricacies of its purely-functional implementation .. 
I don't particularly care, my experience is that there are enough typed terms to quickly find some of them and find bugs in my type-checkers, interpreters or compilers. It may be that, when you have a nicely designed language, small terms are often enough to exercise most of the corner cases. (To wit: many bug reports can be reduced to very simple testcases.) This is especially true for idealized implementations that do not rely in a fine-grained way on value representations, for example. Some bug happen in production implementations because the way constructor indices are encoded in the tag suddently breaks when they are 2^N constructors in the same type. But the prototypes I had to test were often not trying such clever tricks, and thus had a more uniform behavior with respect to input size.
I bet you here and now that this will be smoothened out by tooling. what will stay is having packages authors convene to get some common release (global version lts-style) both in order to iron stuff out ahead of problems and to target where the fish bank is (most users), and we'll have some tooling to widen bounds smoothly
&gt; Wait, happstack switched to some kind of streaming? I was not following it, and may have forgot what all the fuss was about iteratee/enumerator/conduits/pipes few (6?) years ago. I don't even know why the question is relevant or brought up? &gt; Of all HSOC projects I think hackage is the one with biggest chance of success. And I'm not arguing about adding persistence change to it, just was curious about acid-state. No, I agree it has a solid chance of success. I was just answering the Q: yes, there are still some problems due to usage of `acid-state` and broadly I'd like to migrate the storage layer to something newer and more modern, but it's a lot of work.
Yes, I am also strongly against removing the requirement for `forall` with STVs - the default Haskell98 rules are much, much more intuitive anyway (implicit insertion of `forall` where-ever a introduced type variable occurs), and there is very little noise introduced by explicitly introducing and qualifying the scope. (And similar to your example, the case where a top-level signature is inferred but local clauses bring type variables into scope, combined with STVs, is fairly weird IMO.) The interaction between inference of GADTs and STVs is also what necessitated this change, IIRC. People might be surprised to know GHC didn't *always* need 'forall', and I don't believe the original paper required it either.
We're going to send out some forms shortly to all the students who've applied to collect information for legal purposes. Identity is part of that.
This sounds fantastic and addresses several pain points I have had with configurator! Another thing I've been wanting recently is awareness of sensitive config fields such as passwords. There are two things I would like here. First of all, the actual values of sensitive fields should not be written to any kind of logs. I have found it very useful to write configured values to my system logs so I can easily trace the source of misconfiguration problems. But you often want to make an exception for password fields so you don't leak sensitive deployment credentials to people who shouldn't have them. Secondly, I find that it's convenient to check config files into source control so deployments can be version controlled and effectively a pure function of the state of the repo. But you probably don't want to have plaintext passwords preserved in git for eternity. One common way of solving this problem is encrypting all your config files and setting up some mechanism to decrypt them on the deployment systems. The downside to this is that it makes updating the config files much more cumbersome. A much nicer solution would be to leverage the aforementioned awareness of sensitive fields to store passwords in encrypted form. This would give us the best of both worlds. Any developer would be able to modify non-sensitive fields with no extra overhead. And people with the proper encryption keys would be able to modify individual passwords in a fairly straightforward way. The config system could even have a mode that could interpret secure fields as plaintext and automatically overwrite them with the encrypted values when requested. If you are using encryption for sensitive fields, then you could also choose to still log the values of those fields since doing so would not expose sensitive information and would regain the debugging convenience. Would you be interested in putting this sort of thing into configurator-ng?
I did! It's on Hackage under `servant-router`. Plan is to keep the version in line with the Servant version I developed with
Writing configurations to logs for auditing purposes is actually important for what I do, as I need to be able to ascertain with a high degree of accuracy what happened with any particular event after the fact. So I've implemented the "don't print database passwords to logs" thing in my code; in my case, I've implemented a secondary `displayLibpqConnectionString :: [(Name, Value)] -&gt; Text` separate from `libpqConnectionString :: [(Name, Value)] -&gt; ByteString`, with the former replaces the value field with a placeholder for any parameters that start with `pass` or `pw` (just in case someone fat-fingers the actual name of the password field). It wouldn't be too difficult to add a new case for "this is a sensitive and/or encrypted string" to the `Value` type, and some syntax to the low-level parser. Many of the issues beyond that I think could and perhaps should be punted to client code. I mean, there's nothing particularly stopping you from storing base-64 encoded encrypted data in your configuration, then decrypting it (or not) in a `ConfigParser`. Admittedly, it'd be a bit of a pain to manage the config files without a little bit of extra tooling, but it is possible. So I hear you, it's definitely something to chew on, though I doubt it'll be a very high priority for myself for the time being.
&gt; I don't even know why the question is relevant or brought up? Because of performance and resource leaks problems associated with lazy IO. I was interested if hackage had any problems with it, or are those problems exaggerated.
&gt; `[]` looks like an array I refuse to give up syntax just because Algol used it for something else.
Would removing unary minus, along with the `NegativeLiterals` extension, allow me to do xs = map (- 10) [1..20] ? It bothers me that you can currently do this with `+ 10` but not with `- 10`.
Your first link is broken. It should be &lt;https://hackage.haskell.org/package/hackage-diff&gt;. 
Yes, removing unary minus does that. Negative literals aren't even necessary, they're just a nice way to have negative well literals without writing `0 - 1234`, the `0 -` is just necessary when you've got a variable.
It's new :)
Near Old Street. :)
Hmm, if you wanted whole-file encryption, and you want it encrypted when it isn't being read by configurator, that's more complicated because of configurator's `import` functionality. There would seem to need to be some sort of loading hook to support this; it could be as simple as making calls to `System.IO.open` a parameter instead of a hard-coded function.
`pure` is `repeat`
Have you *seen* my history? Yea, I'm thinking I'm having fun.
Actually, I think I missed the point with the `filter` problem. The problem of classifying its result as either `[a]` or `Infinite a` is not that important, because you would still have to traverse an infinite input list to generate a finite output list. The real problem is: how long do you have to wait for each successive element of the result list? From a practical viewpoint, there is no difference between a large-enough gap inbetween two elements, and the infinite gap at the end (if the result is finite).
If you live in Barbican then you are simpy not allowed to consider Basinghall or Old Street not great.
I believe it was actually two bugs (one in acid-state that Duncan was looking at) and one in bytestring (but the Hackage server accidentally wasn't using the new dependency, that bug was already fixed). These compounded on each other to create an ungodly amount of RAM usage when applying the transaction long from a very long-lived acid-state checkpoint (i.e. if you didn't checkpoint regularly, and ours failed creating a very long-lived transaction log that we eventually had to apply). For reference the hackage server is using something like 200GB of storage (I think a lot of this is bloat) so the memory requirements were 'only' in the 20GB range or something. In practice I imagine we're likely running one of the bigger installs, so it's unlikely this will hurt people as badly as it did to us. /u/dcoutts can maybe get more specific. But I think it would be quite possible to boil all that down to like a fraction of the on-disk size with some retooling.
I doubt `zip` is *O(1)* for `Vector`. 
Lazy IO makes it tricky to write code with certain nice streaming properties, not impossible. Happstack uses lazyIO, but at a very low layer, and it is _very_ time tested that the code has those nice properties. So it is not a concern for a _client_ of happstack (such as the hackage app) that the network itself is written in a delicate way. (At least, insofar as that delicate code is carefully tested and works, as experience has shown that it does).
Thanks for mentioning it. I might start using it, from now on.
If you mean printing errors like: Error Here ------^ Then no, however you can create a custom renderer for given a `ParseError` value and the original input stream. renderLikeClang :: MonadParsec e s m, =&gt; s -&gt; ParseError e t -&gt; String This is possible since all of the necessary failure information is captured in the `ParseError` value. 
If I was 'just' going to serialize Haskell values to disk, and could convince myself that's all I needed - do yourself a favor and at least use LMDB. It's absurdly fast, transactional, ridiculously tiny amount of code, etc etc. On a 64-bit machine its performance/overhead is quite unbelievable (it currently doesn't work so hot on 32 bit machines but that's less of an issue these days I think). Combine this with your preferred serialization mechanism (maybe binary-serialise-cbor? ;) and boom, done. SQLite is also OK, and a better choice if you want relational querying. Many projects can probably fit into that scheme, honestly, for example all kinds of local workstation tools, basic automation, etc. So that's still also an OK place to use acid-state too, if you want. I love 'fully contained' applications like that. Hackage is in a different category of application. It's developed by a large number of people with a separate team (loosely speaking) maintaining the stuff it runs on, and it has a pretty heavily used set of live data that updates relatively frequently. It also needs to have good availability, as a service. That means that operations knowledge and tooling is really important, and it's simply much easier to find that expertise and knowledge out of hand with something like PSQL. PostgreSQL is definitely more work to integrate into a Haskell application, vs just serializing raw Haskell data types (which I admit is very powerful) down an abstract handle. But it will likely pay for itself better over time based on the structure of your application. If I was going to just start fresh tomorrow, like with Hackage (or something arbitrary really) I'd just settle for PostgreSQL for all primary data as my default, probably with some combination of local storage or object-level CDN backed storage (like any S3-equivalent storage with consistent read/write) for bulkier data. That's because PSQL is heavily used, freely available, broadly tested, and widely considered to be robust, long-term software you can depend on, which is important over the lifetime of an application, as people (like developers/ops) come and go.
Is this prohibitively inefficient?
Admittedly I applied once, didn't even get a cold reject reply. Looks like the benchmark is pretty high ;(
If I can get field-level encryption I don't see myself wanting whole-file encryption.
The default should be something like a rope instead of either a list or an array. A fingertree of character arrays would allow O(lg n) concatenation and indexing and O(1) cons, snoc, and length with only a modest speed penalty.
It works because they cheat. They internally represent a vector of tuples as a tuple of vectors. zip :: (Unbox a, Unbox b) =&gt; Vector a -&gt; Vector b -&gt; Vector (a, b) O(1) Zip 2 vectors [Hackage documentation](https://hackage.haskell.org/package/vector-0.11.0.0/docs/Data-Vector-Unboxed.html#g:23) ---- **EDIT** [Internal `zip` implmentation](http://hackage.haskell.org/package/vector-0.11.0.0/src/internal/unbox-tuple-instances) This is what they do: zip as bs = MV_2 len (unsafeSlice 0 len as) (unsafeSlice 0 len bs) where len = length as `delayed_min` length bs --- **EDIT 2** They only do the O(1) trick for unboxed vectors.
In order to separate `toInteger` from `Integral`, you'd need to demonstrate something that has a meaningful `toInteger` function, but no meaningful `fromInteger` function. If a value has a lossless `toInteger` function, then it's pretty much an `Integral`. And if it's lossy, then it'd probably be better served with a conversion function whose name advertises this.
Would `div`/`(/)` also be tagged partial? Because emitting warnings on every division is not a good way to endear your language to newbie programmers.
Thanks for the response. I think that's almost but not quite right. The `keyDbName:` entry does set the column name for the primary key column, but that's not the problem. The primary key column is `id` (and that's the default for `keyDbName`), but the column for the `userId` field is also `id`, so groundhog gets confused and tries to create two `id` columns in the migration.
I used a real bad example, I wasn't really looking for a way to solve this problem. I just have come across the desire to define local instances and datatypes before and haven't been able to do it. 
I personal uses [diagrams](https://hackage.haskell.org/package/diagrams) but it's a bit low-level : It's just missing a proper chart layer. I tried to use Chart (a while ago), but I end up choosing diagrams. I'm sure Chart as evolved since. Otherwise, I think ggplot (R) is usable with [HaskellR](http://tweag.github.io/HaskellR/) (not sure of the status though).
This sounds strange but my preferred solution is to set up plots using aeson and plotly. I usually have minimal web server scaffolding with blaze html and Julius. This is similar in spirit to Quickplot (unfortunately now abandoned) except I don't think template Haskell is necessary - it's easy enough to have some wrapper functions around aeson. It sounds like a lot but once you do it once you have quite reusable wrapper functions. EDIT: fwiw it's not even a lot of code really - the scaffolding needed is about 50 LOC. If I can get my code to the point where I'm happy with the interface maybe I'll release my attempt at this.
A few days. Likely less than a week.
Oh, my solution is very specific to my situation; I'm not proposing it as a general purpose solution. And I'm not necessarily opposed to some kind of feature to mark sensitive data as such; I can see an argument for not completely punting that issue to the client. Also, `configurator` allows you to define your own newtype wrappers (via the `Configured` typeclass), which I'm keeping around and will even be making a bit more powerful. See commit [`b33ce0`](https://github.com/lpsmith/configurator-ng/commit/b33ce0f8e6440922e42de0d97fed80d48a1d42de#diff-546d294e80e6cb9fdf8a43231244b57aL131) for starters; I also plan on enabling `Configured` instances to return informative error(s). (and possibly warning(s)?) But it would seem to me that as far as fragility is concerned, there's always the possibility that the writer of the config might mess up and forget to mark something sensitive as sensitive.
Good point. You would have to rely on other proven knowledge (e.g. the pattern of elements in the input list) to be able to determine whether it's okay to stop waiting for more elements to be filtered out. I guess the question, practically, is what is the appropriate way to handle `filter` on `Infinite` given the limitations of Haskell and the desired use cases of the `Infinite` type itself? Presumably `filter`ing an `Infinite` list in most cases would be intended to produce an also-infinite list. You could even write `filter :: (a -&gt; Bool) -&gt; Infinite a -&gt; ([a], Infinite a)` and let the caller choose which to take!
Whoops, you're completely right.
Pretty sure this is impossible, because `instance Num String` could be defined in another module.
I am the author of Groundhog. Datatypes that have an id field need to mark it as key and disable the default autoincremented id. The mapping above results in id columns. You would need to disable that key and declare your own. Here are some examples of how to declare new simple primary keys and composite keys. https://github.com/lykahb/groundhog/blob/master/examples/keys.hs mkPersist defaultCodegenConfig [groundhog| - entity: User dbName: users autoKey: null keys: - name: UserKey default: true constructors: - name: User fields: - name: userId dbName: id - name: userName dbName: username uniques: - name: UserKey type: primary fields: [userId] |] Or if you remove the id column from the datatype and omit renaming userName, you will have much smaller mapping. This is safe to do because SQLite is case-insensitive. mkPersist defaultCodegenConfig [groundhog| - entity: User dbName: users |] edit: Made the unique key primary
&gt;I personal uses diagrams but it's a bit low-level Diagrams looks interesting. Nothing's wrong with low-level if it's done in a sensible way. But yea, diagrams' charts definitely look like they can get [a little involved](http://projects.haskell.org/diagrams/gallery/Chart.html). &gt;I tried to use Chart (a while ago) What turned you off from it (and/or what features in diagrams were so much more attractive)? &gt;I think ggplot (R) is usable with HaskellR Using ggplot would be ideal, but wrapping one language around another language and its whole compiler/interpreter just to get at one of its libraries seems kind of wasteful and...*icky*.
I could see it as potentially still useful as defense in layers type of thing... perhaps if your application also blanks certain fields it expects to be sensitive, like mine, *and* you have the ability to mark extra fields as sensitive which will be blanked, then to mess up you have to mess up twice. (Also, if your `ConfigParser` is expecting certain fields to be marked sensitive, and spits out an error if they aren't, that helps discipline a bit too.)
A couple of thoughts: &gt; `Int * Int * Int` just isn't good enough (is it length/width/height or height/width/depth?) So ... don't use raw types. Type alias them or newtype them as `Length`, `Width`, `Height`, etc. Already a Haskell best practice. &gt; `(spec/keys :req [::x ::y (or ::secret (and ::user ::pwd))] :opt [::z])` So this looks a like a combination of couple of sum and union types. If we leave the types pedantically unspecified, data MyData xType yType secretType userType pwdType zType = MyData { x :: xType, y :: yType, auth :: Either secretType (userType, pwdType), z :: Maybe zType } Optionality can be captured by the type system itself, obviously. &gt; `(s/def ::a integer?)` &gt; `(s/def ::b integer?)` &gt; `(s/def ::c integer?)` Here we have an impromptu type system, I see :-)
In some cases, they don't get resolved at compile time. They get passed around as records, which may be constructed at run time.
Also, there's a typo in the section on quasiquoters: ` |[regex| .. |]` shouldn't have the leading `|`.
I see your point, and I actually agree with it. The question, though, is whether a "mathematical" language (in Haskell's sense) is the most suitable tool for doing mathematics today (because ecosystem). But that is just another incentive/opportunity for contributing. 
They could be generalized to work with any `IsString` (like other functions were generalized to work with `Trasversable`, etc)
How so? If some of it is planned for 8.0.2 even...Although I am not sure what OP meant by GHC 2020 - Haskell Prime or just GHC by that time?
I have: you're doing God's work, son
I'm definitely interested, but only starting mid 2017.
Can't wait to see it in action!
Indeed but alternative like decimal lack of support.
This reminds me a lot of the [AwesomePrelude](https://github.com/tomlokhorst/AwesomePrelude/).
But sometimes you *can* prove at compile time whether an instance exists, even without backtracking.
sounds like fun, but wish there was a better way to interface everything than just literal text code generation &gt;&lt;
I think you're thinking of the wrong kind of graphs. Don't get me wrong - I love graphviz and used it repeatedly in my writing thesis. However, for every network graph that I needed, I needed another thirty heat maps or scatter plots, and graphviz isn't much help there.
Why?
Your `mapN` is probably best named as `zipWithN`, like these http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:zipWith
Just use `(&lt;&lt;&lt;)` and `(&gt;&gt;&gt;)`.
At one point I recall a comment by a GHC developer that if you ever manage to do this, you should submit a bug report, because it means you've managed to violate the soundness of GHC's type system. (I'd be very curious how you could use this to derive something like `unsafeCoerce` though!)
https://www.reddit.com/r/programming/comments/3vv354/the_convergence_of_compilers_build_systems_and/
Oh boy, more Haskell in Germany please!
Das klingt sehr interessant! :-)
Sorry I was unclear. I meant: In some cases, it should in principle be possible for GHC to derive whether the instance exists at an early enough stage of processing that the result could be used in a type-level calculation as desired by OP.
Yes I want to learn, potentially this is the best way to do it, sometimes it helps just to get the first step otherwise I am just completely lost. Can we private message somewhere, what is your background. I am a physicist primarily, so although understanding the code is important, I want to get it working as soon as possible so I can start working on the physics. I will give my solution to that example now.
If you used gglplot2, you know how amazing it is. How does Chart compare with it ? About IHaskell, emacs org-mode (with babel) gives a similar experience (in fact better if only Haskell was working). You can mix languages, include graphics, pictures and all of that with your favorite editor (emacs of course but also vi via spacemacs). The only problem is Haskell doesn't work :-(
The soundness problem arises if you conclude that `Satisfies C ~ False` for a locally-unsolved constraint `C` in a way that conflicts with the open world assumption (i.e. if it is possible to define an instance and then derive `Satisfies C ~ True` in another module). Under those circumstances, you can derive `False ~ True` and `unsafeCoerce` is just around the corner. Otherwise, in principle GHC (or a type-checker plugin) could provide a `Satisfies` type family via built-in magic. The definition given by the OP doesn't work because associated types are really just sugar for top-level type families, so it is defining type instance Satisfies c = True and there's no way currently to have a class context on an equality constraint (though some people want this). Most of the time such a definition would either reduce to `True` or get stuck, but it could detect the fact that equality constraints are sometimes really globally unsatisfiable (e.g. `Satisfies (Int ~ Bool) ~ False` should be fine).
Das freut mich! Wir würden uns sehr über eine kurze Vorstellungsmail an jobs@cpmed.de freuen!
OS package manager is for OS users, cabal is for Haskell developers.
It depends whether you're a user or a developer. If you're a software user, ideally, you are not supposed to even know about `stack`/`cabal`. Those are purely developer tools. As a concrete example: If you just want to install `pandoc`, you expect `apt install pandoc` to install a binary of `pandoc` in under a minute, rather download a heavyweight GHC compiler, a few dozen source-packages and start compiling those for several minutes. Moreover, you want the artifact to end up in `/usr/{bin,share,lib}` and not some `${HOME}/.local/bin` (or even weirder `${HOME}/.cabal/bin` from a XDG POV). That's basically what system-wide package managers solve. If, however, you're a developer who actually *wants* to compile code, then you only need `apt install ghc cabal-install` to bootstrap your environment (which will also make sure if packaged properly that a reasonably working C-compiling-environment is made available) with dowloaded binaries (which is what the system package manager is there for), and then just have `cabal`/`stack` manage your **locally compiled** artifacts in `$HOME/`. 
OS packages are much nicer than cabal. You don't have to wait for everything to compile, and you get stable library versions that work with each other (no different-dependency-versions, no annoying sandboxing garbage).
That's true, I was laying out the boot strapping process.
&gt; Otherwise, in principle GHC (or a type-checker plugin) could provide a `Satisfies` type family via built-in magic. Hmm, could it? I think GHC can figure out whether a particular constraint is satisfiable or not, but if it exposes this information to the program as a type family, a simple diagonalization argument shows that `Satisfies` can't always evaluate to either `'True` or `'False`: class DoesNotSatisfy a instance Satisfies (DoesNotSatisfy ()) ~ 'False =&gt; DoesNotSatisfy ()
Frage aus Düsseldorf, muss man unbedingt in Berlin bzw. in Freiburg vor Ort sein?
As @adamgundry said, it can also get stuck, in this recursive case that would mean that there is no instance of `DoesNotSatisfy ()`
My work on `iris` was actually inspired by a couple of excellent Python visualization tools, namely `vispy` and `pyqtgraph`. I bet either of those can handle rapidly streaming data into a line plot. (Use `vispy`, it is newer and one of the authors is the author of `pyqtgraph`). One of the reasons I think a Haskell OpenGL library would be awesome is a lot of libraries in dynamic languages are burdened by all of the code that is executed as sanity checks because they don't have a type system. Something as simple as changing the screen size is often passed to dozens of classes, all with their own error checking code. In Haskell, the type system takes care of much of this, and there are potentially very few layers between a high level plot event and the specific OpenGL buffer that needs to be changed. However, one piece of advice I have is to try and filter the data yourself so the graphics library doesn't have to work so hard. If you have millions of points and you know that 95% of them will share a pixel with another point, then that is something you can take advantage of. The algorithm you use totally depends on your use-case though. For example, if you don't care about outliers, you can probably just take one of every N points. However, if you really care about every feature of the data, then you will need to find a more advanced algorithm that can efficiently detect when parts of your data are too close. You are probably correct to ignore JavaScript for now if you have truly huge data. A lot of plotting libraries use the DOM for their plots, but you probably need WebGL. I haven't found a good WebGL plotting library yet.
I'd very much like to see what you work out in that regard. Could we also regard it as a monad in a categorical setting enriched over some particularly special notion of a morphism?
This is exactly it. I do a lot of development in Haskell, it's my primary language. I also install some Haskell packages though nix. Why? Because I don't develop those and so don't particularly want to have to compile them, and want them available to whatever user I may log in as.
Some places use mapN because for the case where N = 1, the type and implementation are consistent with map.
There is also the security aspect. In theory (and very much in practice) distro maintainers put a lot of effort in QAing packages in their repositories. Many packages are even forked for the particular distribution. This way users only need to trust their distrubution, not the random person uploading to pypi or hackage or whatever. Then there is the updating process. `sudo aptitude upgrade` updates all my pyton/haskell/ruby/etc packages regardless and makes sure they all play well together.
How does the summer of code project differ from the package for doing this that already exists? Is there a more detailed explanation of the `DeriveStorable` proposal somewhere?
`cycle : NonEmpty a -&gt; Infinite a`?
I use Debug.Trace precisely when small pure functions aren't going to help me. In particular sometimes your test values are big and not easy to copy paste in a repl or in test code. I have one program where that is quite common. That said I don't really mind importing it, but it might help a newbie out quite a bit. 
würde mich auch interessieren. Auch ob die Bezahlung "branchenüblich" ist oder darüber liegt. Wäre Teilzeit möglich? Ich studiere nämlich zur Zeit noch, gebe aber selbst schon eine Haskell-Vorlesung an der Uni :)
&gt;I should say “(-3) instead of (-4)” not &gt;I should say “(-3)” instead of “(-4)”
 data ExprB a r = OperB a | MultB [ r ] | PlusB [ r ] deriving (Show, Eq) type instance Base (Expr a) = ExprB a instance Functor (ExprB a) where fmap _ (OperB x) = OperB x fmap f (MultB l) = MultB $ fmap g l fmap f (PlusB l) = PlusB $ fmap f l instance Foldable (Expr a) where project (Oper x) = OperB x project (Mult l) = MultiB l project (Plus l) = PlusB l reduce (OperB x) = Oper x reduce (MultB l) = case l' of (Oper 'a':Oper 'b':t) = Mult (Plus [Mult [Oper 'b', Oper 'a']), Oper '1']:t) other = Mult other where l' = fmap extract l reduce (PlusB l) = Plus $ fmap extract l -- Then the function you want is `histo reduce`. This uses `Foldable` from [`recursion-schemes`](https://hackage.haskell.org/package/recursion-schemes)
&gt; I use IHaskell in emacs with org-mode. How do you do that ? do you have any link to how to set it up ? 
The tradeoffs between OCaml and Haskell are pretty well-discussed in various forums so I didn't see a reason to waste my time writing things that have been written before: https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=ocaml%20vs%20haskell Especially given that your top-level comment makes it pretty clear that you didn't take the time to read/understand the actual post.
I'll have a look thank!
Please excuse me if I sound too critical here, but it seems to me that [`typelits-witnesses`](http://hackage.haskell.org/package/typelits-witnesses-0.2.2.0) is a clunky reimplementation of a part of [`singletons`](https://hackage.haskell.org/package/singletons). I recommend to check `singletons` out, since it's pretty much the definitive reference on dependently typed programming with GHC 7.10. Also, the `singletons` author Richard Eisenberg is the main GHC developer working on dependent types. 
A boolean typeclass would be cool for DSLs as well!
Ich auch. 
Its a fine paper but has virtually no effect on haskell as far as I know and I'm not sure why its posted here:-)
On a cursory look I would say most of `typelits-witnesses` is already covered, and list traversals could be also done better with `singletons`. If you give me a bit of time I'll write some code and explanation.
I really appreciate it :) edit: i'm starting to see how the functionality of the other module can be done with singletons too!
thank you! :)
Yes I'd be interested - pm me. It's easy enough to wrap one off scatter plot, heat map, histogram, or create other single purpose charting functions. As I mentioned in my edit it takes just a few dozen lines of scaffolding that can be reused. For a prettier, more compositional interface, the design aspect to get right is to try to get a monoidal interface to configure the plot characteristics and rendering layers. That (and intelligent defaults) is the magic of ggplot2 API design that makes it so expressive. My question is how could you cleanly wrap the plotly json construction in a monoidal interface? May not be too hard but I'm still new to Haskell so my usage of aeson has been fairly pedestrian. I'm not sure exactly how best to approach it.
I threw together some [notes and code](http://lpaste.net/164559). It's not very polished or detailed, so feel free to ask. EDIT: as a [gist](https://gist.github.com/AndrasKovacs/556ddc1125b7e65ebf816d533ffeca00) too.
Perhaps I don't understand what the first problem is asking for. By definition, a partial order is transitive, so what do the category laws have to do with it? Are they perhaps claiming that if you have a category whose objects are elements of a set P and there's a relation R on P where there's a morphism from p1 to p2 iff p1 R p2, R is necessarily a partial order, and the problem is asking you to prove that?
Also, keep in mind that at least on Debian all packages have to build from source. A typical end user is not going to care about Haskell libraries like, say, Parsec. But if an end user tool like Pandoc needs Parsec, then Debian has to package Parsec so that Pandoc builds.
I find that `haskell-mode` indentation -- I'm using `haskell-indent-cycle` -- usually does slightly worse than the opposite of what I want. Here is a representative example: f x = y where name = value&lt;cursor here&gt; Now, when I press `C-j`, I end up with f x = y where name = value &lt;cursor here&gt; Hitting TAB, I get *the same cursor position* again :P Finally, hitting TAB a second time, I get what I wanted from `C-j`: f x = y where name = value &lt;cursor here&gt; Ironically, this is the cursor position that `C-j` with `indent-line-function` bound to `indent-relative` would have given me. However, `indent-relative` has other short comings, so I don't use it either.
&gt; at least on Debian all packages have to build from source. Only on the build servers. Debian distributes both binary and source packages. But, the only installation method is from binary packages. The source packages are for reference mainly, but can be used to generate a modifed / optimized / altered binary package instead of getting that package from a repository.
Maybe [Planet Haskell](https://planet.haskell.org/) could work? [This](https://planet.haskell.org/rss20.xml) is the rss feed.
I think it's more of a claim that many people *even developers* don't necessarily need to use the latest release. Instead, they can wait for a curated set of packages that changes less often and have already been tested in combination, and target that. Though I also believe that too much sandboxing leads worse experiences because shared libraries stop being shared, and I have a lot more to patch when a vulnerability is discovered.
http://haskellnews.org/
Does this site have an RSS feed? It's a lot of stuff, most of which I would wanna filter (and I have an RSS client with excellent filtering).
/u/bos's blog is top notch: http://www.serpentine.com/blog/
Thank you for this, I'll definitely make some changes to my post to reflect this. It's such a shame to entirely ditch the `GHC.TypeLits` interface, though, I've grown somewhat attached :'( Do you think that all teaching in the future should just ignore the entire `GHC.TypeLits` interface and teach Singletons directly?
Thanks! I agree that this would be a good deficiency to address. It's something I see store addressing in the near future. In our uses of store, we've already used it to decode existing binary formats. For that application, we can assume x86 machines. To properly address decoding existing formats in general, we'll want endian agnostic utilities. From the store README: &gt; By using machine representations, we lose serialization compatibility between different architectures. Store could in theory be used to describe machine-independent serialization formats. However, this is not the usecase it's currently designed for (though utilities might be added for this in the future!) . . &gt; &gt; the binary and cereal packages use big endian encodings for numbers &gt; &gt; That whole comment is somewhat bizarre. Those packages use whatever encoding you specify - little endian, big endian, or something else you make up. I stand by this point that these are totally nutty defaults. My mind was blown when I saw these instances https://hackage.haskell.org/package/binary-0.8.2.1/docs/src/Data.Binary.Class.html#line-238 `binary` does indeed have both `putWord16le` and `putWord16be`, we can easily have those too, and hopefully will soon. However, you'd expect that the instances for the most common types would be efficient on the most common platforms.
&gt; But if I need a specific binary serialization, how do I implement &gt; it? You just use the combinators in the `Store` class (like in cereal's `Serialize` class): import Data.Store data Pair = Pair {pairX :: !Int, pairY :: !Int} deriving (Show) instance Store Pair where size = ConstSize 16 peek = do x &lt;- peek y &lt;- peek return (Pair x y) poke (Pair x y) = do poke x poke y main = do print (encode (Pair 123 456)) print (decode (encode (Pair 123 456)) :: Either PeekException Pair) Output: "{\NUL\NUL\NUL\NUL\NUL\NUL\NUL\200\SOH\NUL\NUL\NUL\NUL\NUL\NUL" Right (Pair {pairX = 123, pairY = 456}) I'll make a to-do item to flesh out the Haddocks to include a simple example. &gt; normal usage is to define the serialization format you want using &gt; the Get and Put monads with provided combinators. The common use &gt; case is when you have a pre-existing binary format that you want to &gt; be able to read and/or write in Haskell. That's what `Peek` and `Poke` are equivalent to. You can either: * Derive instances for your data structures because you're just serializing Haskell, or * You can deal with a custom format, which we've done with our clients e.g. for parsing HFT binary format stock market feeds. &gt; &gt; the binary and cereal packages use big endian encodings for numbers &gt; &gt; That whole comment is somewhat bizarre. Those packages use whatever &gt; encoding you specify - little endian, big endian, or something else &gt; you make up. Well, the cereal package defaults to big-endian (network order) for the `Serialize` instances of all numbers: http://hackage.haskell.org/package/cereal-0.5.1.0/docs/src/Data-Serialize.html#line-165 -- Words16s are written as 2 bytes in big-endian (network) order -- Words32s are written as 4 bytes in big-endian (network) order etc You have to use [a specific combinator](http://hackage.haskell.org/package/cereal-0.5.1.0/docs/Data-Serialize-Get.html#g:8) to get the little-endian behaviour. This tripped up one of our clients, it's not obvious that this is the case without reading the code.
Hah, stack takes about twice as long to decode that cache on my machine as on yours! Of course that only means that I'll get twice the absolute speedup from `store`! ;) Apart from that it would be nice to see `Store`'s instances listed [in its haddocks](https://hackage.haskell.org/package/store-0.1.0.1/docs/Data-Store.html#t:Store).
&gt; But lets say we choose a case that isn't exactly store's strongsuit, how well does it perform? In our experiments, it seems that store does a darn good job of that too! Have you got more concrete benchmarks for this? I mean, “2× faster than binary” is good, but cereal and cbor seem to be faster than binary too, so it's still unclear which of those I should choose.
True, it would be good to spend more time on such benchmarks. Contributions towards this welcome! Unlike `serial-bench`, the `store` benchmarks currently only compares `store`, `binary`, and `cereal`. `store`'s killer app is vectors of fixed-size data, so if you have that, then you should use it. For more complicated data types, `store` is also pretty darn quick. Its main weakness is that the need to traverse the full input in order to compute the result size. I'm thinking about adding an optional optimization which should make the hackage index usecase faster. The hackage index mostly consists of `ByteString`s interspersed with some metadata. Instead of copying these `ByteString`s, we could share slices with the input buffer. We don't do this by default because it could cause unexpected memory leaks. For example, if a small ByteString is encoded along with a bunch of other data, we'd keep the entire input buffer in memory just to use a small piece of it.
While this is certainly an interesting library, it's unfortunately also proving the very point I was trying to make in https://www.reddit.com/r/haskell/comments/4kmgp1/stackage_lts_and_ghc_80/d3g8zrd &gt; But I'm seeing a worrying trend of Stackage users uploading packages to Hackage leaving off valuable version bounds meta-data, and thereby violating the (non-enforced/soft) Hackage package guidelines. If this trend keeps going on, this will overload Hackage Trustees [...] It also seems strange to me that a serialization library results in such a heavy dependency footprint (c.f. http://hackage.haskell.org/package/store-0.1.0.1/dependencies ). To pick one example, `store` depends on `cryptohash` for the sole purpose of using `Crypto.SHA1`, this in turn pulls in `cryptonite`, which in turn pulls in a few other helper packages. Just recently I had to fork off a [`cryptohash-sha256` package](http://hackage.haskell.org/package/cryptohash-sha256) (and the similarly for [`cryptohash-md5`](http://hackage.haskell.org/package/cryptohash-md5) and [`cryptohash-sha1`](http://hackage.haskell.org/package/cryptohash-sha1), and in the process I [fixed a few minor bugs](http://hackage.haskell.org/package/cryptohash-sha256-0.11.100.0/changelog) in the implementation I noticed) since it had sneaked in an unwanted dependency on `cryptonite` et al. which we couldn't tolerate for `cabal` (which only needs the ability to compute SHA256 hashes)
EDIT: retrospectively, I probably should have just nipped this conversation in the bud and updated the constraints. I still stand by my perspective on these matters, stated below. I have updated the hackage metadata with lower bounds for all dependencies. For those who are interested, check out the [changes to the dependencies section of store's package.yaml](https://github.com/fpco/store/commit/d16ef7da6d8db0ab43617c81ae283078606b3199#diff-54eaec2f732ed7ea540163ee133a989fR26). There are only 4 constraints that are hard lower bounds. The rest are soft (based on the versions picked by our [7.8 stack config](https://github.com/fpco/store/blob/master/stack-7.8.yaml), and I suspect most are entirely redundant, but it is hard to verify that with our current tooling. Also, to be clear, I have a lot of appreciation and respect for y'all who tend the garden of package version constraints. I just don't think you should have to work that hard to solve that problem. And so, I've added these constraints, incase it makes your work here easier. &gt; But I'm seeing a worrying trend of Stackage users uploading packages to Hackage leaving off valuable version bounds meta-data, and thereby violating the (non-enforced/soft) Hackage package guidelines I did put some consideration into how much of the interfaces were used for each of the dependencies. If it's APIs I know are stable within the package, then I omitted constraints. Since we are doing so much stuff based on auto-deriving instances, `store` can actually adapt to many different API changes. Now that you mention it, I do spot some packages that could really use bounds, such as `template-haskell`. Thanks for the suggestion! I am curious, what exactly is the workflow you use for determining and maintaining all of the version constraints? I have never seen an efficient workflow documented that allows you to have both broad and correct version bounds. I have a proposal for how stack can automate this here - https://github.com/commercialhaskell/stack/issues/1568 - without such a solution, IMHO, it is simply unreasonable to expect people to maintain both broad and correct version bounds. &gt; since it had sneaked in an unwanted dependency on cryptonite et al. which we couldn't tolerate for cabal Yes, it does seem that there is a difference in philosophy between stack and cabal regarding external dependencies. Stack benefits greatly from being able to use many excellent libraries, at the cost of taking longer to build. EDIT: (we do indeed partially have the luxury of using `cabal` to bootstrap to thank for this)
Notably, this happened on a PowerPC machine. Pathological indeed! ;) It is interesting that it can get that bad, thanks for linking to the article. I have a note to add some architecture conditionals which prevent it from building on unsupported architectures. In this particular benchmarks, two orders of magnitude slowdown would bring us to about the speed of the binary package!
Schade; dann nicht. Ich brauch noch bis ende des jahres/april nächsten um den Master fertig zu bekommen :)
From a security point of view sandboxes are a nightmare if you want to update all libraries on a given system affected by a given security hole (or other critical bug for that matter).
Let's see. As far as I can tell the monad eqations become *inequalities* in Haskell in the sense that it may happen that one side diverges and the other not. This would mean we need a notion of "less than or equal" on morphisms (because we're comparing Haskell functions), so the category should be poset enriched, and then the laws of the monad should be relaxed using the enrichment. Here's a try. Write `f ≤ g` for some notion of order which intuitively means that "whenever `f` terminates then `g`terminates" (in reality we can just use the domain-theoretic order if we think of a domain-theoretic interpretation of (some reasonable fragment of) Haskell). The laws would then say * **Left identity:** `(return a &gt;&gt;= f) ≤ f a` * **Right identity:** `(m &gt;&gt;= return) ≤ m` * **Associativity:** `((m &gt;&gt;= f) &gt;&gt;= g) ≤ (m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g))` I hope I turned those the right way. In any case, I think I am talking about a [monoidal monad](https://en.wikipedia.org/wiki/Monoidal_monad), maybe. 
OK, submitted it as a [feature request](https://github.com/fpco/store/issues/30).
&gt;&gt; Interested in a PR for that? &gt; I'd prefer to have a discussion of the design space OK. Submitted it as a [feature request](https://github.com/fpco/store/issues/31).
&gt; It's fine. I believe it's not fine. &gt; perhaps you should open github issues rather than continuing to create top level comments here? Point well taken. Submitted an [issue](https://github.com/fpco/store/issues/32).
No, he's thinking of the requirement by Debian policy that all Debian packages must be able to be built from source stored in the Debian source packages.
That greatly depends on what you use to subscribe and read your RSS :) I have a button in Firefox, which subscribes me in Feedly. Just pasting the subreddit url in a feed reader might work. If not, the direct rss url is https://www.reddit.com/r/haskell/.rss .
What's different here is that this package has the potential to play a fundamental role in the ecosystem. But Herbert ran into some concrete problems using this package, within his context of GHC HQ where much of our basic infrastructure is built, only because of a few missing bounds. It would be a shame for such a high quality library with so much excellent hard work put into it to be used much less than it could be only because of that.
I prefer Stack over Cabal Install and I still put upper bounds on all of my dependencies
This is really nice!
So this means we can start using GHC 8 with the next nightly?
Are you kidding? Project A needs version 2, project B has decided not to move to version 2 because it isn't worth the effort to update from version 1. How do you handle this? Just not work on one of them?
Sure, that is one of many tradeoffs made. I'm a little confused at this whole discussion. It's not like I just invented sandboxes and none of this has been talked about before. Sandboxes solve a very real and clear problem. If you don't need sandboxes it's because you're the some developer or you have total control over all the libraries in use. Most of the time, that's not the case.
On modern Intel it doesn't really matter (essentially 0 difference in performance), on other architectures this might result in dramatic slowdown, yes.
Not likely, It would only make it easier to find such a collision (which is not something you care about in a non-cryptographic context). Since this is presumably a safe environment with no attackers, a collision resistant hash function is overkill. See https://en.wikipedia.org/wiki/Collision_resistance
I also support splitting off the streaming part. I don't need streaming and wouldn't want to incur a conduit dependency for a feature I don't use.
https://www.reddit.com/r/haskell/comments/149nfk/pokemon_exceptions/
It looks like `Char`s are always encoded in 4 bytes. Have you considered using UTF8 (like `binary`) or UTF16 instead?
&gt; checked exceptions, which are problematic / controversial even in the languages which natively support them. That's primarily because most of those languages don't support abstracting over the checked exceptions. E.g., I can't assert that my function will only throw exceptions that the passed in closure throws or only those plus ExceptionX and ExceptionZ. In Haskell, we are allowed to do that, at least to some extent, which actually makes checked exceptions *much* better. In fact, I prefer them to `throw` and asynchronous exceptions, though my experience may be limited.
Thinking a bit more about this, it looks like a fixed-size `Char` would be an advantage in at least some circumstances. Yet if the datastructure has to be traversed in order to compute its size anyway, the space savings of a variable-size `Char` might outweigh the speed loss. Would a `Utf8` newtype for the latter case be acceptable?
yes! Thanks. I actually came to a similar [solution](https://github.com/tippenein/etl-language-comparison/blob/haskell-implementation/haskell/src/MapReduce.hs) Now the problem is concurrently running the input files, because it's *really* slow. I've been trying to use **mapConcurrently** for this. I profiled it [here](http://lpaste.net/164562)
Does that mean there is no supprt anymore for other LTS and that ever body as to switch to GHC 8.0 ?
[Image](http://imgs.xkcd.com/comics/flies.png) [Mobile](https://m.xkcd.com/357/) **Title:** Flies **Title-text:** I don't know about houseflies, but we definitely caught a lot of fruit flies with our vinegar bowl\. Hooray science\! [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/357#Explanation) **Stats:** This comic has been referenced 168 times, representing 0.1498% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d3ko54z)
Is the profile for your current solution, or the concurrent solution?
&gt; and thatveverubidy Come again?
See my reply at https://www.reddit.com/r/haskell/comments/4l3y9f/store_a_new_and_efficient_binary_serialization/d3kpj1j
A good hashing function [Edit: like MurmurHash] isn't "more prone to accidental collisions" any more than a cryptographic one. It's just less secure against active attackers. 
&gt; Conservative upper bounds do indeed cause maintenance burden from external forces. I'd prefer reactionary upper-bounds (applied once a breakage actually occurs). Yes, but that maintenance burden is the inherent price of being a responsible maintainer. Reactionary bounds are problematic for several reasons. First, your users will have already been inconvenienced. The first user to attempt to build your package after a breaking dependency release will be greeted with difficult to understand compile errors. I would much rather have my users get solver errors that they can easily work around with `--allow-newer` than compile errors that will be much more difficult for them to track down. Second, the old versions of your package are still out there with incorrect metadata. Very few maintainers go back and fix things. They usually don't even fix the last version, let alone all the other versions that came before that. The effort to eliminate a small amount of periodic work ultimately results in a large amount of work later that almost always gets ignored. This means that the Hackage Trustees get stuck with the burden. It's harder for them to address the problem because they won't be as familiar with the package as the maintainers are. This approach simply does not scale and is harmful to the ecosystem. Third, this problem cascades down the line and can affect every transitive downstream package. I have personally seen multiple situations where users of my packages experienced build problems caused by missing upper bounds in packages I depended on. The more foundational a package is, the more severe this problem will be. Because of this experience I will definitely not be using `store` in any of my projects as long as it has such a large number of dependencies without upper bounds.
I wish you luck.
&gt; If you represent effects explicitly in types you're closed to killing polymorphism. Hmm, not sure what you mean. Haskell's polymorphism certainly *seems* alive and well.
Good luck!
That's not true. With MurmurHash you can easily end up with accidental collisions. I'm not familiar with MurmurHash specifically, but a quick google search led me here http://programmers.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed . In general all hashing functions thought to serve hash tables are not concerned with the accidental collision, but just with preserving a good distribution across their codomain.
Care to explain the joke?
I'd definitely recommend against UTF-16. It's not really a fixed-width encoding, so it lacks those advantages, and UTF-8 encoding is almost always smaller except for dense east asian text (without e.g. HTML or CommonMark decoration). Even there, you can get more savings by using real compression. [Ref](http://utf8everywhere.org/#asian) I'm not entirely convinced moving away from UTF-32 is a positive change, but please avoid UTF-16.
It is not so much of a joke, just stuff people say when certain events are happening more often/nearer. But it may well be an idiom that originated in the war (Einschlag=bomb explosion).
I am sad to hear that you attribute such malice to our opensource efforts. We've put quite a bit of time into this, and it fills a much needed gap in serialization support I am not anti version bounds or something, I think they are a good idea. If I was against having version bounds, why would I think so hard about how to properly automate determining them? https://github.com/commercialhaskell/stack/issues/1568 . I am just really against overly restrictive bounds because in my experience they caused way more pain than help. That could just be all the overall pain of trying to use cabal-install effectively, in vain, for years and years, though. `store` actually does compile against a very wide range of deps, to the extent that it would take a lot of work to figure out good lower bounds. I am curious to hear how you would suggest I procede, specifically, to add good bounds to this package.
I guess I'd have to take a look at some libraries like `ether`, but at least with just the standard libraries I can't see how you'd comfortably manage such checked exceptions with the type system. You'd need an ADT containing all the exceptions you want to throw, and for every closure you'd need to add another ctor, i.e. `data MyExceptions a b = Exception1 | Exception 2 | ExceptionsFromClosureA a | ExceptionsFromClosureB b` etc. I mean it's great to have all exceptions right in the type, but it just seems impractical and not scalable to do so.
Does it have to if the functionality behaves like a class `IsInstance :: Constraint -&gt; Bool -&gt; Constraint`, which has a magic overlapping instance in module Main?
In binary formats it's very unlikely to have strings without a size specified up front in a header.
I figured as much but I failed to see how this related to the job posting. Is it a Freiburg joke?
Reactionary upper bounds are just as bad as no upper bounds unless you blacklist old versions of the library that were missing upper bounds. The reason why is that any old release of the library without upper bounds "poisons" the Cabal solver. For example, suppose that you release version 1.0 of a library named `foo` with a dependency on `bar` which is currently at version 1.0. Then `bar` releases a version 1.1 which breaks your library. "No problem ...", you think: "... I'll just release `foo-1.1` with an upper bound of `bar &lt; 1.1`. However, the Cabal solver will now try to build `foo` and there is a good chance that Cabal will pick `foo==1.0 &amp;&amp; bar==1.1` as a valid build plan and then fail with an uninformative build error very late after compiling a lot of code. Had you used aggressive bounds, the user would have gotten a more informative dependency resolution error up front.
Thanks! I'll try doing that :)
If it were anybody other than Herbert I might agree with you, but Herbert does all the hard work of fixing build failures caused by missing upper bounds on Hackage so I believe Herbert has earned the right to lecture other people on this topic since he is the victim of the problem he's describing.
True! I was actually thinking of something a bit different by "reactionary bounds", which is to indeed amend historically uploaded bounds. On one hand, doing metadata updates is ugly, but this sort of "future knowledge" implied by upper bounds is perfect for this. I should have described what I meant, particularly as it's an idea I'm not sure has been described. It'd look like a tool that does the following: 1) Go back in version history from the current version, and add the specified new reactionary upper bound to all the metadata. 2) Stop this process whenever there's a spot where the upper bound got lowered from some value that was larger than the reactionary upper bound (unless it's un-bounded). Why do this? It's possible that an older version legitimately could use the more recent dep version. Ask the user about it. I like it better, because then the bounds are informative and not just paranoid. An upper bound in this world would have the meaning "Yup it breaks after this", not "It could maybe not work with those versions. The user wants to use old packages, right?". Haskell is actually one of the safest place to use broad bounds / no bounds at all, because we get compile errors. True, we'd like to avoid them, but at what cost to users and maintainers?
If we agree that different content can collide "in the wild", doesn't that make its usage for type equality unsound? If we rely on `hash(A) /= hash(B) if A /= B`, that's something that the contract for cryptographic hash functions cover. Performance is not an issue here -- there's one hash per type which will only be computed once. Besides, SHA1 is very fast.
No. You can't rely on that `hash(A) /= hash(B) if A /= B`, since this is impossible. The set of inputs (the domain) is much larger than the set of outputs (the range), and thus there will always be an `A` and `B` s.t. `hash(A) = hash(B)`, but `A /= B` (by the pigeonhole principle) The contract for cryptographic hash functions is just that finding these collisions is hard. See https://en.wikipedia.org/wiki/Pigeonhole_principle#Uses_and_applications
&gt; I believe it's not fine. I believe you're wrong about this. Thanks for looking closely, though
Does it work with dynamic attributes?
Yes, for a contiguous string you should probably just use `Text`. But what about `Char`s that aren't part of a string? I was thinking of a datastructure like [this ternary-tree-based `StringMap`](https://github.com/axman6/TernaryTrees/blob/master/Data/Map/StringMap/Internal.hs) which can be [serialized into a pretty compact encoding](https://github.com/axman6/TernaryTrees/blob/master/Data/Map/StringMap.hs#L166). I guess that an analogous `Store` instance might serialize and deserialize more quickly with constant-width `Char`s but in some cases you might prefer the more compact encoding with UTF8-encoded `Char`s.
Constraint resolution failures are also a really bad experience that turns off a lot of new programmers. I still don't know how to actually read cabal-install errors, but I'm out of practice.
Not possible. ghcjs requires ghc to work. purescript on the other hand requires only a small purescript compiler to work. Something small and independent like purescript compiler can be packaged with npm, Something as huge as ghc cannot. It's like trying to package dotnet or jdk with node. 
_sigh_, yes, I know about the pigeonhole principle. But part of the point of crypto hash functions is precisely that you can rely on that property in the real world (eg to verify integrity of downloads, deduplicating files, identifying commits, etc.), which is exactly what we need to do here. With the hash function that you propose you'll find plenty of collisions in the real world.
Upper bounds already shift the burden to the users, who need to deal with the pain of: * Confusing cabal solver errors * Notifying maintainers and asking them to bump these bounds, opening PRs that languish, etc. This happens far more often than encountering compile errors, and IMHO causes way more strife. * Using an escape hatch out of this unsustainable paradigm, `--allow-newer`. This foregoes the benefit of those upper bounds that actually are meaningful. In my experience, this actually worked most of the time I had upper bounds, which is anecdotal evidence for their uselessness in the common case. The goals of the policy are noble, the results in practice are a mixed bag.
&gt; You'd need an ADT containing all the exceptions you want to throw Yes, and no. The first example I can put my hands on is from my only library on hackage [cryptsy-api](http://hackage.haskell.org/package/cryptsy-api-0.2.1/docs/Cryptsy-API-Public-Types-Error.html).[1] By nesting the data structures, the top-level one doesn't have to get that big. But, depending on how you count, this covers 26 or so different failure cases. Most of those are the different ways to put together an `HttpException`, but it also covers all of `IOException` and even `SomeException`. I agree that it's not exactly what I want, but that would require type-level set operations, which I don't think any language has. There are a few that can fake it for exception types pretty well though. [1] As an aside, I think Crypsy is well and truly dead. Is there a way for me to abandon the package, so I'm no longer listed as the maintainer?
&gt; it fills a much needed gap in serialization support you may wish to reconsider this expression http://languagelog.ldc.upenn.edu/nll/?p=11326
If you'd like to help get the Haskell ecosystem upgraded to ghc 8.0, there are a couple ways you can discover which packages need help: * [this diff between nightlies](https://www.stackage.org/diff/nightly-2016-05-25/nightly-2016-05-26) (This will become stale soon so maybe prefer the next one) * Look for `# ghc 8` in stackage's [build-constraints.yaml](https://github.com/fpco/stackage/blob/master/build-constraints.yaml) Michael's blog post outlines the suggested steps to take to get a package re-enabled.
LOL, you know what I mean.
It also sucks when you've been reading documentation for the latest package, but the actual package being pulled in is an ancient version. Makes for very confusing errors for a short while.
&gt; There is also currently no reasonable way to even discover broad and correct version bounds, so I think this is a straw man. Via wikipedia: &gt; A straw man is a common form of argument and is an informal fallacy based on giving the impression of refuting an opponent's argument, while actually refuting an argument that was not advanced by that opponent This is not a straw man. This is me saying what's wrong with the tooling around cabal-install's approach, not me refuting your particular point. I care about advancing the quality and maintainability of the ecosystem, not "winning" this particular debate... If you do not require broad bounds, then you are basically saying "I don't care if my users who chose cabal-install end up fighting the solver to try to find a working build plan". And likewise, for stack users, such narrow bounds can cause it to be difficult to use a package in extra-deps without using `allow-newer: true` (which actually relaxes both lower and upper bounds). I have a detailed proposal of how we can create tooling that solves this problem: https://github.com/commercialhaskell/stack/issues/1568 . I want multiple configurations for many reasons beyond this, hopefully I or someone else will implement this soon (PRs appreciated!!). So, be prepared for a future where stack does a much better job of helping you determine version bounds. &gt; And the stackage way has exactly the same problem except it is even worse because it pins you to an exact version. Not true. I will not end up accidentally using an ancient version of a package with stackage. Sure, when building an old project, I will use old versions, but that will be on purpose. New projects will use new snapshots, and so modern package versions. Bugs encountered with a particular set of stackage packages will also likely be encountered by others that use the package set. It also gives an easy way to talk about the issue. "X bug on lts-5.5", rather than "X bug on whatever cabal-install decided to install, shall I ship you my freeze file??"
The focus of the work was to write a good serialization library. If we spent hours and hours pouring over version numbers, something feature would have been dropped. &gt; break everything I challenge you to find reasonable version selections that break store's build. You may well be able to find them, but I think you will find that this is far less broken than you think it is. We really have very light API deps on most of the deps.
I think you should try stack, the purpose is not to restrict build plans, but to give you control over them.
My point is that the use of the cryptographic hash function is overkill, and adds a dependency on cryptohash, which adds cryptonite and more packages, while [murmur-hash](https://hackage.haskell.org/package/murmur-hash) is only one package (which just depends on bytestring and base). The slower compile time is a result of additional dependencies, and possibly a more complex build plan (which is important for a hopefully widely used package). [MurmurHash](https://sites.google.com/site/murmurhash/) is faster, and has excellent collision resistance, it's just not that hard to reverse (making it non-cryptographic). 
Yeah, it is tough picking good defaults here. Another instance we got via Storable was one for "Bool" which, oddly enough, uses 4 bytes. It now uses the store generated instance, which uses just 1 byte.
No, it is about the distance between Freiburg and Nuremberg, which is about 400 km.
Luckily we can fix metadata after the fact which can fix this. Good maintainers do this themselves, and in cases when they don't, trustees can step in.
Likewise, in Haskell there are still monadic laws that are required to be followed by convention, and cannot be verified by the type system.
We run one nightly at a time, and that build is now done with ghc 8 instead of 7.10. This paves the way for the next LTS release to be based on ghc 8
As someone who spent about 6 hours this week on additional work due to the presence of upper bounds on Hackage, I don't believe I have a right to lecture people at every opportunity about the ills of upper bounds. There's a difference of opinion on this topic. In healthy discussion, this difference would be noted and we'd move on. This constant preaching from one side is unhealthy, unhelpful, and does nothing to promote three agenda of the pro upper bounds crowd. In fact, I'd place a large bet that Herbert making this argument in almost every thread is more likely to prevent people from adding upper bounds, not encourage them.
I had a discussion with Duncan once about the forced upper bound issue. I predicted to him that, if such a requirement were put in place, one of two things was certain to happen: * The "scofflaws" (I like the term) would simply add in upper bounds like "foo &lt; 3141592", making the upper bounds check useless, like already happens with the forced base upper bound check (also [commented here](https://www.reddit.com/r/haskell/comments/4hiwi0/announcing_cabal_newbuild_nixstyle_local_builds/d2qnbbo)) * Someone would create an alternate Hackage, something I've done a lot to avoid happening ([commented here](https://www.reddit.com/r/haskell/comments/4gppm8/ann_hackagehaskellorg_is_down/d2kawcn)) There _is_ support in Stack for specifying alternative and additional package indices, but by default we only use Hackage (via all-cabal-hashes and the S3 mirror to avoid Hackage instability). It's in Stackage where we're trying to avoid a fracture, which would harm cabal-install users by not giving them access to newly uploaded packages.
This is where having a blog post for reference is useful, or a Wiki page laying out the arguments for both side. The argument needn't be made in every single thread.
&gt; A user is better equipped to solve a constraint error: they can do cabal install --allow-newer or open an issue/pull request against the original package to bump the upper bound, which is pretty easy to do Yeah, the process itself isn't too difficult once you get the hang of it. The issue is how many of these upper bound bumps need to happen if you have them on everything. AFAIK, `--allow-newer` only lifts the restriction on upper bounds and not lower bounds, so this is not a cure-all for cabal woes. Lifting all constraints wouldn't give the solver much guidance, so I'm not suggesting that as a solution. However, you do sometimes need to relax lower bounds as well. &gt; Constraint resolution failures happen very early in the build process, whereas build failures happen very late in the build process (i.e. 15 minutes into the build when lens fails to build due to a bad version bound) True! Is this optimization worth the maintenance burden? Without adequate tooling, It seems to be a tradeoff between efficiency of package maintenance and correctness * broadness of version constraints. &gt; For a build error, it's not obvious what the change should be to fix the build error, even for experts True! Some build errors can be quite puzzling. I'll make the data-less, anecdotal assertion that most of the time upper bounds are not saving you from compilation errors. And most of those compilation errors are grokkable by an intermediate haskeller.
Indeed, if anyone knows of such a blog post or wiki page already, please share!
RSS feed here: http://www.haskellforall.com/feeds/posts/default
My understanding is they desugar more or less like view patterns. There shouldn't be anything technical standing in the way of more or less replacing them with their right hand side when they occur in pattern form, maybe some concerns about the type signature, but nothing earthshaking.
Have a look at latest Stack release. It adds some sort of support for extending snapshots (only read about, haven't tried in practice). I think it might be possible to take an existing snapshot, override libraries you want and then refer to that new custom snapshot in your other projects.
I just wrote this up quickly: https://gist.github.com/snoyberg/f6f10cdbea4b9e22d1b83e490ec59a10 Hopefully that explains my viewpoint well enough. In an ideal world, it would prevent [ridiculous ad hominems](https://www.reddit.com/r/haskell/comments/4l3y9f/store_a_new_and_efficient_binary_serialization/d3kbmbl) from occurring in the future, but I've been involved in this long enough to know that explaining my reasons won't prevent others from attributing malice to my actions anyways.
The difference is once you prove the laws about your monad you can know that all the code written using it is safe. You can do all your reasoning in one place in a compositional way. Here you *always* have to worry about any user ever slipping in an effect. The functor laws fail to hold because `fmap f . fmap g = fmap (f . g)` falls apart due to effect order. The difference is more that once you lift your functions to be effectively into "Kleisli IO", then non-determinism isn't something you can express enriched over that scarier arrow type -- even partiality has non-compositional aspects! You lose the ability to reason about a *lot* of things, e.g. the category isn't even Cartesian any more, so you don't even have real products. (even in the limited sense that Haskell is almost Cartesian ignoring the extra `_|_` by "fast and loose reasoning"), but rather has the rather sad Freyd category "products". e.g. `fst . (f &amp;&amp;&amp; g) = f` falls apart, because `g` can have side-effects so products have no good universal properties, they are a complete ad hoc operationally defined mess. Im often told by people who write scala/rust/swift that they want to take advantage of that extra power they get. After all they are paying to work in an object-functional environment, and should be able to mutate, etc. as they please to get performance. Embrace the language they have, etc. The downside is that you can say very very little about what happens in code that uses the powers they are given. The abstractions don't abstract -- which is a pretty dangerous sign to me. In Haskell unsafePerformIO is a big big warning sign. In scala `for` sugar for iterating over containers effectfully is really the only fast idiomatic way to do it. The monadic variants all do horrible horrible things to performance, your stack, and your heap.
Our mirroring doesn't include frontend mirroring, but that would be good to build out. We'd still need a switcher or the like regardless (and in this case we'd need to be in two independent datacenters to avoid the issue as well).
Using stackage via cabal freeze config files is no longer the recommended use, that was a stopgap solution.
I will give it a look, but around here everyone goes crazy about machine learning.
I've mirrored one in China. Only packages and the index are available. https://mirrors.tuna.tsinghua.edu.cn/hackage/
&gt; Cryptographic hash functions are also supposed to be slow, to deter brute force attacks. No, this is a common myth. General purpose cryptographic hash functions are supposed to be fast. Look for example at the outcome of the SHA-3 competition—one of the reasons that Keccak was picked as SHA-3 is that it supports very fast hardware implementations. Specialized *password hashing* functions (or as I like to call them, *password authentication codes*) like [scrypt](http://www.tarsnap.com/scrypt.html) or [Argon2](https://github.com/P-H-C/phc-winner-argon2) are the ones that are supposed to be slow. Not just slow, actually, but *memory-hard*, to defeat dedicated password-cracking hardware and attacks that exploit time-memory tradeoffs (using extra memory in order to compute solutions quicker—and particularly when the extra memory can speed up *parallel* instances of the computation!).
&gt; prevent ridiculous ad hominems from occurring in the future That'd be great! :-) ---- &gt; You could address your blatant hypocrisy in claiming that you had nothing to do with the decision that was made when you initially did everything in your power to shut down a dissenting voice. That would be some interesting mental gymnastics, but as we all know, you're up to the challenge. https://www.reddit.com/r/haskell/comments/4fm6iv/new_lecture_series_on_intermediate_haskell_from/d2bz8mr ---- &gt; I'm pretty sickened by what's happened, especially the package security screw-up and Gershom's shenanigans with dictator status on the haskell.org page. https://www.reddit.com/r/haskell/comments/4fm6iv/new_lecture_series_on_intermediate_haskell_from/d2bghqx ---- &gt; Ah, the politician returns. ... &gt; &gt; I say this with every bit of implication as possible: isn't your term on the haskell.org committee expired by now? https://www.reddit.com/r/haskell/comments/4fm6iv/new_lecture_series_on_intermediate_haskell_from/d2bxye7 ---- &gt; I'm not going to participate in this silly revisionist history you're engaging in. &gt; ... &gt; &gt; And I think many people in the community would be a little shocked to know to what extent I and other significant Haskell contributors are really outsiders to your little cartel. &gt; &gt; The fact that you continue to make these glib replies and pretend like you haven't manipulated every process available, to the detriment of the Haskell community, is distressing. But it's not at all surprising given how much you've done it to date. https://www.reddit.com/r/haskell/comments/4fm6iv/new_lecture_series_on_intermediate_haskell_from/d2brtrd ---- &gt; Apparently double-speak is in vogue right now. The revisionism from @sclv is impressive. https://twitter.com/snoyberg/status/723170621018894337 
&gt; One would almost think your criticisms are driven by some kind of ulterior agenda. And what would this agenda possibly be?
Thanks, I'm glad you approve! :D
Thanks!! I hope you find it useful. There is some discussion on [this issue](https://github.com/fpco/store/issues/30) of reducing the dependencies, which I'm certainly not opposed to, if it makes the package more palatable. The current set of dependencies is a result of getting this out the door in a timely manner so that people can give it a try and give us feedback.
A full integration in our development team requires working on-site. Getting to know our product, our development culture and our code-base takes some time. We prefer introducing you on-site because it's so much easier to look at code together and to explain a design at the whiteboard. Several students have continued working on our product after their internship. If you could work 3 months on-site you'd probably be able to learn everything you need to know. Even if you don't know our code base there's a chance that you could work on side projects (for example fixes or improvements to library packages we're using). If you're interested just send us a short CV and we'll get in touch.
I mean all I see on the linked page is things that could be said about any strongly typed language and development environment. I tried to understand it a deeper level, but it really does seem comparable. Why is my question worth voting down so much? What am I not seeing?
&gt; True! Some build errors can be quite puzzling. I'll make the data-less, anecdotal assertion that most of the time upper bounds are not saving you from compilation errors. And most of those compilation errors are grokkable by an intermediate haskeller. For what it's worth, I've spent a lot of time trying to keep HTTP building with fairly old versions of GHC, given its position low down the stack. That job became dramatically easier when the Hackage trustees fixed old version bounds on various libraries it relies on for its tests. Before that I used to have to iterate with each build failure trying to find the right older version of one of the conflicting packages to use. Now I either get a constraint solver solution quickly or an error which is much easier to proceed from.
Interesting: $ stack setup Preparing to install GHC to an isolated location. This will not interfere with any system-level installation. Downloaded ghc-8.0.1. The following executables are missing and must be installed: xz This is on OS X 10.10. Why not use something that's present by default on the platform? Not a big deal in any case, thanks for having a nightly ready so quickly! ;-)
Indeed, that would already represent a desirable incremental improvement, and we should try to do that. I was just pointing out that it wouldn't provide full redundancy without additional engineering effort.
Really great stuff!
In my case, ignorance of `formatting`;-) Thanks for pointing this out! My ideal solution would probably be something like Rust's `std::fmt` (https://doc.rust-lang.org/std/fmt/) verified by TH. Minus the compile time hit of TH, since we're already making wishes ;-)
&gt; Why not use something that's present by default on the platform? to have Reproducible Builds by default If you prefer to use your current system ghc, there is several config fields and cli options available you can use. You can look for ["system-ghc" in stackage docs](https://readthedocs.org/search/?q=system-ghc&amp;check_keywords=yes&amp;area=default&amp;project=stack&amp;version=stable&amp;type=file), to find [stuff like this](http://docs.haskellstack.org/en/stable/yaml_configuration/?highlight=system-ghc#system-ghc)
Oops, I can see how what I wrote might be misunderstood. I was referring to the last line of the output, requiring `xz`, not the whole concept of installing a local GHC. Previous GHC versions did not require different tools to be installed.
I'm not sure there is solution which would work optimally for everyone. [`Haxl` has lifted operations](http://haddock.stackage.org/lts-5.18/haxl-0.3.1.0/Haxl-Prelude.html#g:4) and I actually like them to be specific so e.g. type-errors are better. Another issue comes from lifting `&amp;&amp;` to `Applicative` and `Monad`, should it be: (&amp;&amp;) = liftA2 (&amp;&amp;) or fa &amp;&amp; fb = do a &lt;- fa if a then fb else return False `Haxl` has chosen second one, but I'm pretty sure there are usecases where you want the semantics of the first one.
I agree with the conclusions - I absolutely think curation and a continuous build of the haskell eco system is the only path forward. For what the future should hold, in a curated build like stackage, which really is a continuous integration build of the hackage eco-system, someone can make an upgrade to a package that will break other people's code. Stackage will then not use the new package for a while until it is all sorted out. This is the chicken-and-egg situation with ghc-8.0.1. But it becomes like that with other breaking API changes as well. This is not ideal. In a future, better version of the haskell eco-system, a developer releasing a version of, say `time` that breaks other packages should be empowered to change *all* other packages so they are compatible with the new `time` API. Thus eco-system wide refactorings should be supported. Currently stack detaches itself from the upper limits in hackage. To support fluent changes with eco-system wide refactorings, I think stackage should start to support automatic patches to packages generated by some refactoring tool. Thus the author of `time` or some other contributor in the eco-system will upgrade all packages at once with one refactoring tool. This will result in N commits to M packages, resulting in M stackage-modified packages that might not be on hackage, but where the authors might accept the (tiny) refactoring change at a later time. Don't just wait on the author releasing a new package, just fork the package and add the tiny fixes. Keep separate git repos for all hackage packages and deliver a pre-made fix to the author that the author can choose to release as a new version. Stack and stackage as made the eco-system incredibly much better, but there are still improvements to be made until there is a single continuously moving repository of haskell code that moves along with the least amount of friction.
Ok, fair enough, it's not a real problem, just a minor incovenience
I originally had 1.22, my config file did not change when 1.24 came in. I had to remove `~/.cabal/config` and then run `cabal update` to create a 1.24 version of it.
&gt; Currently stack detaches itself from the upper limits in hackage. To support fluent changes with eco-system wide refactorings, I think stackage should start to support automatic patches to packages generated by some refactoring tool. Actually, until September 2014, Stackage supported patches to packages to work around slow updates to packages. This was almost exclusively used to relax upper bounds, but IIRC occasionally we actually fixed incompatible code. When I was working with Duncan and Mark on the GPS Haskell plans at ICFP, they insisted that packages be unmodified from Hackage, so since then I changed the Stackage build process to always use unmodified tarballs and .cabal files (even though all work on GPS Haskell from the Hackage, cabal, and Haskell Platform sides fell through). I definitely think the ability to patch packages was a good decision in the first place, and adding it back in may be worthwhile. On the flip side, dealing with many different "flavors" of a package/version combo does complicate things a little bit.
Hooray! I've been waiting for a video of this talk for a while. The speaker also has a very [interesting blog](https://ngnghm.github.io/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/).
[removed]
Good strategy, yes. Sometimes it doesn't work. ["Someone is **wrong** on the internet"](https://xkcd.com/386/) comes to mind.
If I understand your question correctly, I think you're correct. I believe it's trying to familiarize the reader with the notion of categories where morphisms are _not_ functions. In this case transitivity of the poset and the composition rules of the category are equivalent; you don't _need_ the category laws to prove transitivity of course, but the question seems to be designed to show that they're two sides of the same coin and get the reader used to making these conclusions categorically rather than based on their previous knowledge alone.
Side note: love the typography in this presentation.
I totally agree that having multiple flavors of packages is not ideal. However, note that all of the Linux distros maintain their own custom patch sets for the software in their repos. It's a labor intensive process, but it allows them to ensure compatibility. Hopefully if we need to patch we can keep the divergence at a minimum, and the changes can get incorporated back upstream quickly - or somehow resolve the issues.
You can put `allow-newer: true` in your `stack.yaml`.
Can someone verify whether my understanding of the version bound problem is correct? - Some people want the bounds to mark what is "known not to work." - Some people want the bounds to mark what is "not known to work." - It is unacceptable to have two kinds of bounds with these two different meanings.
And indeed we both agree on this issue. Forcing authors into a version scheme is not appropriate for a central distribution point like hackage. Let the tools improve and let the debate continue, but raising barriers to entry in the central community distribution point is not a good plan. This is a balance we always have to keep in mind, the barriers to entry for authors vs the quality users can expect.
It's pretty hard to avoid unaligned memory access. To do so you have to make your format pad everything. The binary-serialise-cbor also uses unaligned memory ops on platforms that support it (and has code to support other platforms that do not, plus support for big &amp; little endian platforms).
This is extremely interesting! I've never thought about polymorphism in do binds. Thanks.
&gt; Nice library. But if I need a specific binary serialization, how do I implement it? My advice for authors of libraries such as these, is don't confuse these two important use cases: handling arbitrary binary formats vs serialising Haskell types. The binary and cereal libs made this mistake, since we didn't know any better back in the day, but we do now. The store package currently only supports the serialisation use case. If it adds support for arbitrary formats then that should imho be clearly separated in the API or it causes great confusion and requests for somewhat bonkers features.
I don't think the third point ("we can't have both") is quite right -- rather, the point is that at the moment, we _don't_ have both. My impression is that if somebody were willing to put in the work to update the cabal file syntax and improve the dependency solver to understand the two kinds of constraints that this work would be gladly accepted.
Yeah, that would definitely work. Once we identify a location and format for storing build success and failure information, having automated bots building things on upload would be wonderful.
How is this kind of poisonous attack getting upvotes in our community ?
&gt; And all of this still doesn't guarantee that some code doesn't use throw/throwIO somewhere. It's not meant to. And I don't think anyone in this thread claimed that it could. I think impure and asynchronous exceptions should be removed from Haskell, but that's not this discussion. ` throwIO` is easy enough to convert to something like this with `try`, and I wouldn't want to do away with it. I claim that checked exceptions in Haskell is a *lot* better than checked exceptions in Java. I don't claim that it's ideal. Even in the ideal scenario, I'm not sure I wouldn't want to have some "scope" in which unchecked exceptions were allowed. &gt; You are probably never going to handle all the dozens of cases anyway, so you'd probably only write code for the 2-3 you care about and then rethrow / treat as generic error for anything else. Maybe. But, as a library author I don't want to be making the decision about which exceptions my users are going to choose to catch and recover from, and which they can't distinguish. There's definitely been scenarios (when I've been the application writer) where I've had a recovery technique for a very specific failure, but I was unable to distinguish it from other cases so I either had to attempt recovery when it would only work 1 out of 10 times, or fail and document the (automatable) recovery and a manual process. &gt; You would then not get a pattern match failure if you add a new exception ctor, not really enforcing that all possible errors are addressed. Rethrow or log-then-ignore are also a way to address an error. :) Again, that's a decision I'd like to delay until the person writing the catch block decides. Most of the time, I imagine there will be some wildcard behavior -- preferring the compile to succeed even though there's another failure case. But, they can also choose to be more explicit, and have -Wall warn them when the library adds a new failure case / ctor. &gt; What if that library also could throw an HttpException, would you add another top-level Ctor with another HttpException or put it into the nested CryptsyError one? In general I'd probably make a new constructor. The goal is to preserve, both at the value and type level, as much information as possible for the person that has to write the catch block(s) or equivalent, and to make it easy to avoid losing that information if they choose to "rethrow". I write catch-blocks a LOT more than I write throw, and all this is motivated by my experiences writing those blocks.
This sounds like the ideal solution IMO. 
This is a common misunderstanding, so rest assured you're not alone. Filter will always "hang" (i.e. search for the next element) if you give it an infinite list. It doesn't care if the list is ordered, it will keep taking an element at a time to see if it matches the condition.
My misunderstanding isn't on the filter, but on the relationship between `take`, `filter` and infinite lists. For example, why does `take 5 $ filter (&lt;10) [0..]` complete but not `take 100`? 
I still think that, given `_1`, etc. are fixed, it would have been nicer to just have the convention that all optics start with an underscore.
It is because there are only 10 elements less than 10 in the list [0..], all of which are at the beginning. If you try to evaluate the 11th or greater element, it will keep filtering the rest of the infinitely long list trying to find an additional element less than 10, even though there aren't any.
i thought the "pvp debate" would be something in the direction of whether upper bounds are "good" or "bad". /u/hvr_ here merely seems to note/complain that `store` in the current form does not (sufficiently) conform to the pvp. It is _you_ bringing up the pvp debate. It is _you_ claiming that /u/hvr_ is "rehashing a recurring debate", which i read as a high-level ad-hominem. Either the `store` package is intended to conform to the pvp, in which case /u/hvr_'s comment is warranted. Or it is not intended to, in which case a clear statement to that effect would be very useful for proper community-internal discussions in general. I expect more carefully chosen statements from someone who says &gt; I'm just sharing my thoughts, and I'm actively avoiding getting into more flame wars." -snoyman [source](https://gist.github.com/snoyberg/f6f10cdbea4b9e22d1b83e490ec59a10)
When using ghc or runhaskell, it doesn't print anything because of output buffering.
The issue is that when you pattern match on `'Refl`, the type indices for `j :~: k` are and so the `k` in your pattern is forced to be the same as `j`. In other words, this would work: type family AttemptUnify (j :: a) (k :: a) (r :: (j :~: k)) :: (j :~: k) where AttemptUnify j j 'Refl = 'Refl In Agda that second `j` would be prefixed with a `.` to indicate that it is an *inaccessible pattern* - because its used as an index in the type of a later part of the pattern match its form is already forced and it is in some sense redundant to match on it. See another example at the bottom of the [Agda page on data types and pattern matching](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.InductiveDataTypesAndPatternMatching).
But why would you expect `take 5` to hang, given that there are at least 10 elements in the list it processes ?
video please, thanks. 
Thanks :) I can't take credit for any of the design, I just used one of the themes in http://www.decksetapp.com/ I love being able to make slides in ~markdown. One day, perhaps, I'll learn enough org-beamer to move over to that, but for now deckset is a pretty nice balance.
Yep, precisely. We've struggled with library discovery. That's why resources like haskelliseasy.com are very handy.
Simple answer is, we didn't know about `formatting`. How many other libraries like this are there out there? Finding and assessing their completeness/appropriateness as a beginner can be really difficult.
I don't expect it to hang, but /u/kqr said "Filter will always "hang"if you give it an infinite list". But it doesn't if you `take 5 $ filter ...`.
 Prelude&gt; take 3 (1 : 2 : 3 : error "oops") [1,2,3] Prelude&gt; take 4 (1 : 2 : 3 : error "oops") [1,2,3*** Exception: oops Prelude&gt; take 3 (take 4 (1 : 2 : 3 : error "oops")) [1,2,3]
Take a look at how `filter` is defined: filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] filter f [] = [] filter f (x:xs) = if f x then x : filter f xs else filter f xs What happens when you apply this to your list example? I suggest working through it on your own, step-by-step, before reading the rest of this answer. Okay. Think about an infinite list `xs` where *none* of the elements match `f`. What happens with `filter`? It always takes the `filter f xs` branch. It's just like writing `filter f (x : xs) = filter f xs`—can you see why that loops forever? That's exactly what happens in the `filter (&lt; 10) [1..]` case. After you've gone through the first 10 elements, the remainder of the list (`[11..]`) is infinite and none of the elements match `(&lt; 10)`. `filter` then always takes the second branch and never produces another element, looping forever. If you need another perspective, think about why `filter (\ x -&gt; False) [1..]` will never produce a single element. Again, look back to the definition of `filter`. Fundamentally, `filter` has no way of knowing that no elements in the rest of the list match `(&lt; 10)`. That might seem weird in *this* case because it's obvious that no elements of `[11..]` match `(&lt; 10)`, but Haskell doesn't have enough information to figure that out itself. More importantly, it simply *can't* figure this out in general—it runs into the limitation of the Halting Problem/[Rice's Theorem](https://en.wikipedia.org/wiki/Rice%27s_theorem).
`take 100` hangs because it's waiting on the remaining 90 elements from `filter &lt; 10`, which never come. `filter &lt; 10` doesn't know it's "done" because it doesn't know/care that the list is ordered. As far as it is concerned, there's another value `&lt; 10` somewhere in the infinite list, so it tells `take 100` "I've got more values to return", and `take 100` waits indefinitely for the filter to find the next 90 values. This will work if you use `takeWhile (&lt; 10)` instead of `filter`, because `takeWhile` knows that it's done the first time it sees a value that violates that predicate. &gt;&gt;&gt; take 100 $ takeWhile (&lt; 10) [0..] [0,1,2,3,4,5,6,7,8,9] Same reasoning applies to `!!`. For values `&lt; 10`, filter is able to find a value. For anything larger, it asks `!!` to wait while it sifts through the infinite list to find another value `&lt; 10`. 
&gt; It really doesn't hurt to write &gt; ``` &gt; display (Person firstName surName) = "Person:" ++ firstName ++ " " ++ surName &gt; ``` It does when you realize you'd actually defined `Person` with surName first! Probably unlikely in this particular case, but I've been moving away from unnamed arguments to constructors precisely because I have to keep looking up constructor argument order. 
Depends on your definition of "works". If you only care about compiling together without errors, then yes it can be done with some automation at the expense of redundant builds and time. If your definition is something else, say correctness related, then this task becomes much harder dependent on the level of assurance you want. In all the practical cases I can imagine, the packages need very comprehensive test suites. 
Yep, I think I had a check for it (as well as a check to make sure partial functions weren't imported, and a few other things) as part of "personal linter" I knocked out for use in the past.
I'm not proposing we get rid of manual version bounds, and they'd still be useful for these kinds of semantic issues. I'm talking about automating the tasks of manually checking packages against multiple dependency versions and relaxing the bounds.
&gt; In a future, better version of the haskell eco-system, a developer releasing a version of, say time that breaks other packages should be empowered to change all other packages so they are compatible with the new time API. How, exactly do you imagine this working? (No, you don't get to modify the source of the package I uploaded to Hackage. What, are you insane? That is an absurdly insecure idea.)
`_1` is obviously a pathological case. It would be unreasonable to penalise all optics because of that. Also, variable starting with `_` have a special treatment with GHC. They don't issue a warning if you don't use them, which I think is why lens expect field to start with `_`. Without that, if you only uses optics but never use the fields, you'll get lots of warnings.
The way I've found it useful to view it is the outermost expression (in this case `take`) will request elements one at a time from the inner expression (in this case `filter (&lt; 10) [1..]`). When `take` requests the first element, `filter` will check that the first element of `[1..]` is less than 10. The same thing happens again for the next 8 elements. When the number `9` has been returned and `take` asks for the next element, filter will start to scan through the list to find another element smaller than `10`. It will go on and on and never find one. If you stopped before the 10th element, it wouldn't find it very hard.
~~But published package versions are immutable, aren't they? So if you don't initially specify upper bounds, and a new version of a dependency is released that breaks your package, then you can't go back and add an upper bound after the fact. You'd have to get a Hackage Trustee to do it, for every dependency that releases a new breaking version, which creates extra work for them.~~ (edit: see below) You'd also have to monitor previous versions of your package for when new upper bounds are needed, unless you just don't care about previous versions remaining installable.
The section on `Integer` internals seems to be at least partly outdated: http://hackage.haskell.org/package/integer-gmp-1.0.0.1/docs/GHC-Integer-GMP-Internals.html#t:Integer
In Haskell, data structure elements are more like procedures in other languages than values. When you have some object `x = [0..] :: [Integer]`, x doesn't start out containing `0 : ...`; it just contains `[0...]`. It's only when something needs to know whether `x` is a nonempty list that something happens to it. Imagine the heap as being a set of named, typed cells. Each cell can either contain a constructor for its type, or a computation (or, if a function, a lambda). If it has a constructor that takes arguments, those refer to other names. When you evaluate a name, you always evaluate *just* that name and anything required to determine if it's a constructor or not. To be clear, I'm going to use a Haskell-like language where allocation is explicit ("let" creates a new allocation), and every object is named in this way In your example, these objects are on the heap: -- I'm being extra explicit here; an Integer is just a type with -- one constructor for every number. Later on I might omit -- references to these and inline them to save a bit of space v0 :: Integer = 0 v10 :: Integer = 10 i5 :: Int = 5 less10 :: Integer -&gt; Integer = \x -&gt; x &lt; v10 list1 :: [Integer] = take i5 list2 list2 :: [Integer] = filter less10 list3 list3 :: [Integer] = enumFrom v0 take :: Integer -&gt; [a] -&gt; [a] = \n xs -&gt; case n of 0 -&gt; [] _ -&gt; case xs of [] -&gt; [] (:) y ys -&gt; let n1 = n-1 let zs = take n1 ys (:) y zs filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a] = \pred xs -&gt; case xs of [] -&gt; [] (:) y ys -&gt; let predy = pred y case predy of True -&gt; let zs = filter pred ys (:) y zs False -&gt; filter pred ys enumFrom :: Integer -&gt; [Integer] enumFrom n = let n1 = n+1 let xs = enumFrom n1 in (:) n xs We want to evaluate list1. To do so, we instantiate 'take' with its arguments, replacing list1 in-place: list1 = case 5 of 0 -&gt; [] _ -&gt; case list2 of [] -&gt; [] (:) y ys -&gt; let n1 = 5-1 let zs = take n1 ys (:) y zs We now have a known value in `case` so we can evaluate it: list1 = case list2 of [] -&gt; [] (:) y ys -&gt; let n1 = 5-1 let zs = take n1 ys (:) y zs However we are now stuck, we have a case trying to look at list2, but we don't know what constructor list2 has. So we have to evaluate it: -- we remember that we are evaluating list1 on a stack list1 = ... list2 = case list3 of [] -&gt; [] (:) y ys -&gt; let predy = pred y case predy of True -&gt; let zs = filter pred ys (:) y zs False -&gt; filter pred ys Again we are stuck, we don't know what constructor list3 has, so push our current state onto the stack and keep going: list1 = ... list2 = ... list3 = let n1 = v0+1 let xs = enumFrom n1 in (:) v0 xs This allocates two new heap objects (we need to give them unique names), before giving a result list3_n1 = v0+1 list3_xs = enumFrom list3_n1 list3 = (:) v0 list3_xs We have now successfully evaluated list3, so go back up the stack to list2: list1 = ... list2 = case list3 of [] -&gt; [] (:) y ys -&gt; let predy = less10 y case predy of True -&gt; let zs = filter less10 ys (:) y zs False -&gt; filter less10 ys and evaluate its case branch. This fills in 'y' and 'ys' with the heap objects referred to: list3 = (:) v0 list3_xs list2 = let predy = less10 v0 case predy of True -&gt; let zs = filter less10 list3_xs (:) v0 zs False -&gt; filter less10 list3_xs I'll shortcut evaluating predy here, now that you get the hang of it. One thing to notice, though, is that we can garbage-collect the result of predy after evaluating its case; it's not referenced anywhere else. list2_predy = True -- available for GC list2_zs = filter less10 list3_xs list2 = (:) v0 list2_zs And then back up the stack to list1: list1_n1 = 5-1 list1_zs = take list1_n1 list2_zs list1 = (:) v0 list1_zs So our heap now looks like this: list3_n1 = 0+1 list3_xs = enumFrom list3_n1 list3 = (:) 0 list3_xs -- available for GC list2_zs = filter less10 list3_xs list2 = (:) 0 list2_zs -- available for GC list1_n1 = 5-1 list1 = (:) 0 list1_zs list1_zs = take list1_n1 list2_zs And we evaluated the first element of `list1`. Now try evaluating `list1_zs`; if you think this through to completion you'll find that the full evaluation of `list1` terminates with a reference to the infinite filter still on the heap. What happens if you try to fully evaluate `list2`, though? How does that relate to what happens if `list1` had asked `take` for more than 10 elements? 
&gt; He says, "Don't use upper bounds when you're just guessing." Do "not known to work" upper bounds fall into the category of guessing? &gt; So he recommends using both lower- and upper-bounds, when you know they are needed. How can you *know* for sure an upper bound is really needed, when you don't have empirical "known not to work" evidence yet? While the PVP doesn't give you a way to know for sure when compatibility breaks, it gives you a least upper bound up to which your package is guaranteed (under certain conditions) to remain compatible. Without this contract, you'd have no choice but to constraint your package to versions of dependencies you have empirical "known to work" evidence for!
I wouldn't bother with the last part, it's a lot of complexity for little gain to have super automatic upload/write-able failover, etc. It'll probably very rarely, if ever be used. I'd much rather have Hackage simply fail over to a read-only frontend replica via load balancing, and require manual promotion of a fixed, new leader rather than attempt to fail over writes. This is predictable and easily controllable, and not fraught with the terror of automatic promotion. This will require augmenting or replacing the data layer (it has no remote capability) as it currently stands, so it's not simple either.
I think part of a solution is to separate version bounds from packages. In other words, if I release version x.y.z.w of a package, I will specify its version bounds as presently known, but these bounds will exist outside of package x.y.z.w in some database, and they can be updated separately without updating package x.y.z.w. The next step to implement this is to create a separate database of version bounds and to allow package developers to update their database entries (for each of their versions) as they wish.
This can already be done by creating a new revision of the Cabal file on Hackage for a released version - and doesn't require a separate database etc.
The definiton is old, yeah. We're still dealing with small ints, though. I'll update the post when I get home. :-)
Is anyone doing this? Is everybody aware of this feature? If not, maybe it needs to be encouraged.
No problem I'll edit the hackage entry with a GHC restriction. But probably I can patch it to work on GHC 8 too for a bump release. :-)
As Austin said it's easy to get stats now. No need to compile with profling.
That's an important point, to be sure.
We have this at work with a single massive monolithic code base released as a single atomic operation. It's truly beautiful. Delete a bad API in minutes. Add an argument. Rarely consider compatability. 
I was pretty scared by it too, but it's worked amazingly well in practice - cabal installing things just works nowadays. I believe it only affects the cabal file that ends up in the hackage index, and not the one in the tarball you download - so the weirdness is in the gap between those two, and not that you end up with a package that doesn't itself match the VCS history. (Edit): That said, it might be cool to somehow automatically create an upstream patch/branch with the revision!
Think about how filter works. It traverses a list, dropping the elements that don't match some predicate. If you have filter (isMultipleOf5) [1..] then the filter will only keep every 5th element. So if you go from `take n filtered` to `take (n + 1) filtered`, you'll end up reading 5 more elements from the original list. On the other hand, `(&lt; 10)` is a very different sort of predicate. It's always true until it's never true. While it's never true (i.e. after you grabbed the first 10 elements), you'll perpetually drop elements in the vain search of the next element that's less than 10. That's why it hangs: it's trying to exhaustively search the integers for the next one that's less than 10. For that sort of predicate, you really want `takeWhile`
You can, but I doubt many people go back and edit bounds after the fact on anything other than the most recent version of their package.
You're very good at solving technical issues, and I'd love to read a sketch of a proposal from you (though I doubt you'd love to read all the various responses to it :-P).
It doesn't change the package though. It changes the metadata, in a verifiable way.
Love the idea, would be happy to take part in this :) Sadly, I'll be away for the first meeting - is there anything I can subscribe to in order to hear about future events?
Refs please! :)
Regardless of if there are existing plans, organizers love nothing more than more volunteers to help them. You should definitely send a message to the bay area haskell users group (http://www.meetup.com/Bay-Area-Haskell-Users-Group/) regardless to follow up.
Note: This does what is says on the tin, but not what matching on `'Refl` would do. Matching on `Refl` brings into scope proofs that j and k have the same type. Ideally at the type level, `'Refl`'d prove they have the same kind. That is beyond what you can do with the `type family`, because it has computational content, but could be done with the type classes machinery. You'd need to pass around some value level witness stuff though. You may also be able to get this to work with the shiny new injective type family stuff.
No worries, there will be more (and it's not 100% sure the first one will take place next week, there are a few things that need sorting out). If there is enough interest I'll most likely put it up on meetup.com. Glad to know you'll be joining us!
If that attack is poisonous, it's a poison that /u/snoyberg made and drank himself. 
I had missed that incarnation, and that does suck, since store looks awesome!
[Ah, so you can.](http://i.imgur.com/KRJiBeI.jpg) Thanks for the correction.
Yeah, I meant bringing the type equality into scope. Type families only can tell you something about the result of the type family, they don't feed equality information to the arguments. The computational content is learning the equality itself, it takes no storage, but still has to be "computed" eagerly.
Did you intend to reply to [this comment](https://www.reddit.com/r/haskell/comments/4l9qga/michael_snoymans_personal_take_on_pvp_version/d3m2j3h)?
&gt; But is it the best solution? What are its drawbacks? How successful is it? The point is that there is no single best solution! As you say yourself, this is about trade-offs. The referenced blog-posts below talk about trade-offs, and come to the conclusion that we need both, curated package-collections as well as solver based tooling if we want to abolish Cabal not-heaven. - http://www.well-typed.com/blog/2014/09/how-we-might-abolish-cabal-hell-part-1/ - http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/ &gt; Something I see mentioned a lot in this thread is that this is a very solvable technical problem. It only appears so. It is not *yet* a purely technical problem. First we need to make it a purely technical problem (ironically by providing more meta-data) to begin with before we can *try* to come up with a purely technical solution.
&gt;So where are those additional 16 bytes from? The implementation of minusInteger for Integer types is actually implemented as x + -y: -- TODO -- | Subtract two 'Integer's from each other. minusInteger :: Integer -&gt; Integer -&gt; Integer minusInteger x y = inline plusInteger x (inline negateInteger y) &gt;This means we're allocating one more Integer. That explains the additional 16 bytes! Is there any reason that it's implemented like this, and is there any way to optimize it? Seems like it would be a small optimisation, that would hit a lot of programs.
I just created a meetup group to make it easier to get updates: http://www.meetup.com/London-Haskell-Hacking/ . I'll try to figure out over the weekend how this thing actually works...
Makes sense. Thanks!
As you move towards automation, you move towards a single repo, and there will only be two types of packages: current and obsolete. The obsolete ones will have a timestamp at which point they became obsolete. In that world the premise that PVP is important is just not there.
&gt; I also used to think having humans maintaining giant piles of version constraints was a good idea. Then what would you propose in cases like the aeson-0.10 fiasco? If we had the best possible automated scenario where every package has bounds on every dependency and every time a new major version is released for a package an automated system looks at all its reverse dependencies and, builds them against the new version, and automatically bumps the bounds if the build succeeds, we would get tons of broken packages because things would have built fine, but not worked properly because aeson changed its parsing semantics. So there's really no way you can avoid humans having to maintain some version constraints. Note that I'm not saying we can't improve on the current situation. We certainly can. But it's not an easy problem and there will always need to be humans in the loop. &gt; I just can't seem to follow where your passion for bashing stackage, so I'm grasping for some motive beyond the technical. My motive is quite simple. I want people to be able to run `cabal install snap` (or any of my other packages) and maximize the chance of it succeeding. I also want people to be able to use snap in as many situations as possible, so I want to my version bounds to be accurate and wide. From my perspective making `snap` depend on `store` is like pointing a loaded gun at my users. (Sorry for the exaggerated mental picture, but this reflects how significant I think the issue is.) If I did depend on `store`, any time any one of store's 35 dependencies has a major version bump that breaks `store`, `cabal install snap` will instantly stop working for any of my users. This exact thing has happened to `snap` multiple times in the past and it's always caused by missing upper bounds in one of my dependencies. You argue that I should use stack/stackage, but that doesn't satisfy my second criteria of accurate and wide version bounds because `stack build` locks you down to a single version of everything. You'll be quick to argue that that statement is not true and that all you have to do is add a package to the extra-deps section in stack.yaml, but that requires an extra user action. If you're willing to admit an extra user action in that situation, you have to also admit the extra user action of cabal-install users user `--allow-newer` otherwise you're comparing apples to oranges. In the stack case, the extra user actions effectively amount to manual dependency solving, which in simple cases may not be bad, but in the general case is much more painful than the manual action of `--allow-newer`. If that last paragraph was the whole story, stack probably doesn't come out that far behind cabal in my opinion. However the kicker is that the whole stack system relies on a globally synchronized stackage beat. I don't believe that is scalable. It was pretty hard getting everybody on the same page during a significant period surrounding the aeson-0.10 fiasco and it's only going to get worse as the community grows. I think the relatively small size of our community is the main reason stackage is working as well as it is. A global beat is simply less flexible and scalable than a dependency solver. I think we should be working towards infrastructure that doesn't require the massive human synchronization required by stackage.
filter will not always hang if you give it an infinite list
Yes, without a voiceover the slides are not too informative. 
[Here](https://github.com/tfausak/flow/issues/3)'s a slightly related discussion in the Flow library's repo, about mixing `|&gt;` and `&lt;|`.
Hi all, This is the first Haskell package I've ever released, so feedback is welcome!
Good question. They're dependent on the location of your msys install and only apply to windows. So they don't make sense to add to the general cabal default config. We _could_ have the installer write a special config (as the mac one does), but that wouldn't help if you had another existing config. And having the installer modify your existing config without your say-so seems a bit rude in general (though in this case harmless). There's probably a better way to do it, but we couldn't think of one in time for the release. If you have one, and better yet, if you're willing to help implement it, I'd love to have an improved solution for the future, as it really does detract from the seamless install experience. (edit: I should add -- you wouldn't need those lines if those directories were in your appropriate paths. But on windows that is usually pretty disasterous. E.g. I use cygwin a fair amount, and if those directories are also in my paths, then cygwin falls apart terribly. Windows users will often also have other versions of the stuff in those bin dirs lying around in other ad-hoc ways. So all sorts of name confusion arises with that solution. Instead, we _only_ teach cabal about those directories, and then it passes the relevant info down to everything else it invokes in turn.)
I'm a little uncertain where the Haskell Platform fits in with the ecosystem. Application developers should probably be using Stack + Stackage, either directly or a local mirror of it. Library developers should probably be using Cabal + Hackage, since they need to be closer to the cutting edge. I can see students being interested in using the Haskell Platform for homework assignments and such, since they don't have to think about dependencies. What use cases are there for the Haskell Platform?
Nice! This is truly a more sensible Haskell distribution for Windows compared to previous incarnations. Now I can finally recommend the HP to beginners without bad conscience
&gt; We can promptly detect when dependency changes have broken it, and add in retrospective upper bounds. How promptly? How long will non-Stackage users suffer breakage until you correct the uncovered lie of a missing upper bound that now became harmful? How many package versions will you need to revise and how do you know which ones need fixing?
Try it with the `ScopedTypeVariables` extension. Using `a` doesn't refer to the `a` in the instance head without it, so GHC doesn't know you're trying to use its instance of `TShow`. Also, you might consider a couple of alternate definitions. This one uses a proxy as the argument, so that you don't have to provide an instance of `t`. Also, if you use `PolyKinds`, `t` doesn't have to be a type of kind `*`, but any kind at all instead. {-# LANGUAGE ScopedTypeVariables, PolyKinds #-} import Data.Proxy class TShow t where tshow :: Proxy t -&gt; String instance TShow Char where tshow _ = "Char" instance (TShow a, TShow b) =&gt; TShow ((-&gt;) a b) where tshow _ = "(" ++ tshow (Proxy :: Proxy a) ++ " -&gt; " ++ tshow (Proxy :: Proxy b) ++ ")" instance TShow Maybe where tshow Proxy = "(Maybe :: * -&gt; *)" If you use `TypeFamilies`, you can actually get this stuff on the type level, which is useful for static analysis stuff. {-# LANGUAGE DataKinds #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE PolyKinds #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeFamilies #-} module Lib where import Data.Proxy import GHC.TypeLits class KnownSymbol (TShow t) =&gt; TShow' t where type TShow t :: Symbol instance TShow' Maybe where type TShow Maybe = "(Maybe :: * -&gt; *)" runtimeRep :: forall t. TShow' t =&gt; Proxy t -&gt; String runtimeRep _ = symbolVal (Proxy :: Proxy (TShow t))
Hard to beat [the original paper](http://www.cs.nott.ac.uk/~pszgmh/monparsing.pdf) =) Reading that was mind-blowing when I started Haskell.
Great, glad to see the release and easier way to recommend for beginners to install Haskell on Windows. I was just wondering though, why these aren't MSIs? From a management/deployment/stability point of view MSIs offer much greater flexibility.
There's a neat solution that Roman Leshchinskiy came up with, so I don't need to solve it. Already solved :). I have considered writing it up, and may do so, but the following discussion certainly makes me so other stuff first...
I did :)
Avro is [Apache Avro](https://avro.apache.org/), an RPC framework—to save everyone else the web search.
&gt;We looked into this in the codebase, it turned out more inlining was needed. After comprehensively applying the INLINE pragma to key methods and functions, the memory was brought down to I am sometimes worried that this is not a very principled approach to optimisation – "inline more" is definitely a double-edged sword... 
Yeah it's really cool that after about 100 Lines everything fits together nicely and you can already express a lot of parsers!
Implementations from my university (Utrecht) work like this and are very easy to understand: [uu-parsinglib](http://hackage.haskell.org/package/uu-parsinglib) and [uu-tc](http://hackage.haskell.org/package/uu-tc), the latter is a little easier to use.
I believe checking the direct reverse dependencies is not enough in general. You have to check all transitive reverse dependencies because of re-exports. 
Nice! Feel free to join us then, and please let us know about your experience so far.
Yes, Apache Avro, but it is not an RPC framework but rather a serialization protocol
What does it do in particular, where is it applied?
Well, as the article already indicates, the implementation of plusInteger is not super simple due to having to account for all sorts of boundary conditions and having separate constructors for "small" and "big" integers. There is most certainly a way to optimize it, that would be to replace the current implementation of minusInteger with a "direct" implementation that doesn't have to use negateInteger. Then you can skip the extra allocation incurred by negateInteger. If you want to be able to claim contributorship to the core libraries of Haskell, this is your chance! Seems like a relatively easy way to claim that your code is running at Facebook and large banks :) (assuming they have at least one instance in subtracting Integers anywhere in their Haskell codebase of course)
oneBitAll = map transpose . permutations . transpose . g
this is true but ı already made this.ı want to generate all codes not one.
I used your definition of `g`: import Data.List g 0 = [""] g n = (map ('0':)) (g (n-1)) ++ (map ('1':)) (reverse (g (n-1))) oneBitAll = map transpose . permutations . transpose . g That is [eta reduced](https://wiki.haskell.org/Eta_conversion) form of: oneBitAll n = map transpose . permutations . transpose $ g n I am not sure this is what you need though. It just reorders the bits in all the possible combinations of bits: Prelude Data.List&gt; oneBitAll 2 [["00","01","11","10"],["00","10","11","01"]] Prelude Data.List&gt; oneBitAll 3 [["000","001","011","010","110","111","101","100"],["000","001","101","100","110","111","011","010"],["000","100","110","010","011","111","101","001"],["000","010","110","100","101","111","011","001"],["000","100","101","001","011","111","110","010"],["000","010","011","001","101","111","110","100"]]
thanks yes like this permutations is not found why?what can ı add for this? 
related: https://www.reddit.com/r/haskell/comments/3wtwx3/typelevel_show_type_family_tshow_k_symbol/
ı can generate a gray code.but for example for three bit there are 8 different gray codes.ı want to generate other 7 gray codes.how can ı generate this combinations?ı dindt find anything about hamiltonian cycle.this will solve my problem?
I don't think there's an efficient algorithm for generating all gray codes (aka all hamiltonians of a hypercube graph). I think you'll have to use some sort of intensive search to find them. Use algorithms for enumerating all Hamiltonian cycles of a general graph, and plug your particular hypercube graph into them.
https://ghc.haskell.org/trac/ghc/ticket/11470#comment:7
I'm getting 12 gray codes for 3 bits. ["000", "001", "011", "010", "110", "111", "101", "100"] ["000", "001", "011", "111", "101", "100", "110", "010"] ["000", "001", "101", "100", "110", "111", "011", "010"] ["000", "001", "101", "111", "011", "010", "110", "100"] ["000", "010", "011", "001", "101", "111", "110", "100"] ["000", "010", "011", "111", "110", "100", "101", "001"] ["000", "010", "110", "100", "101", "111", "011", "001"] ["000", "010", "110", "111", "011", "001", "101", "100"] ["000", "100", "101", "001", "011", "111", "110", "010"] ["000", "100", "101", "111", "110", "010", "011", "001"] ["000", "100", "110", "010", "011", "111", "101", "001"] ["000", "100", "110", "111", "101", "001", "011", "010"] 
ok true how?please show code
how can we generate??? 
Seriously? Damn it. Fine, I'll write one.
Cool. I need something like this. Do you know any x86 boxes or boards that make gpio reasonably accessible?
This post inspired functions `filterLE` and `filterGE` in [sorted-list](http://hackage.haskell.org/package/sorted-list-0.2.0.0/docs/Data-SortedList.html#v:filterLE). (They work for the infinite case)
Oh, it looks like this is really hard. Someone published a paper on just the 6-bit version. &gt; Abstract &gt; Finding the number &gt; of directed Hamiltonian cycles in 6-cube &gt; is problem 43 in Section 7.2.1.1 of Knuth’s &gt; The Art of Computer Programming &gt; various proposed estimates are surveyed below. &gt; We computed exact value: &gt; H6=14,754,666,508,334,433,250,560=6!\*24\*217,199\*1,085,989\*5,429,923. -- "Enumeration of Hamiltonian Cycles in the 6-cube"
I know, I guess I should have said "If I make the list NOT infinite". But your comment just made me want to try a wrapping list (integer overflow). let inf = unfoldr (\b -&gt; Just (b + 1, b + 1)) 0 take 100 . filter (&lt;10) $ inf :: [Int8] [1,2,3,4,5,6,7,8,9,-128,-127,-126,-125,-124,-123,-122,-121,-120,-119,-118,-117,-116,-115,-114,-113,-112,-111,-110,-109,-108,-107,-106,-105,-104,-103,-102,-101,-100,-99,-98,-97,-96,-95,-94,-93,-92,-91,-90,-89,-88,-87,-86,-85,-84,-83,-82,-81,-80,-79,-78,-77,-76,-75,-74,-73,-72,-71,-70,-69,-68,-67,-66,-65,-64,-63,-62,-61,-60,-59,-58,-57,-56,-55,-54,-53,-52,-51,-50,-49,-48,-47,-46,-45,-44,-43,-42,-41,-40,-39,-38] Now the infinite list will eventually return a number than will match the predicate again. 
can we modify this code ?
I was literally looking into making something like this last night - seems like I don't have to now :) Great job! Reading through the tutorial atm (which I think is a fantastic thing to have in the package!).
I'll be receiving one of those PINE64 machines soon, similar specs to the C2. Looking forward to trying it out with Haskell sometime.
btw, what about GHC for iOS? From what I understand Apple has stopped accepting 32bit-only submissions to the App Store some time ago.
Author-written bounds are not always certifications, and should not be. The most valuable semantic information authors can add when they manually code dependency bounds are their **estimations** of what dependency versions are likely to work with their package, given what they know about how they use the dependency vs. the usual release patterns of the dependency. Especially when referring to possible versions in the future.
There is no single simple way to "solve the problem". One thing is for sure - the more rich semantic information we have about dependencies, the more we'll be able to improve our build tools in the future.
this is a bit hard.can we simplify this code like above??
I really want to just ignore your drivel, but now it's showing up in my Twitter feed too. I recommend that if you're going to comment on things like this, you teach yourself the difference between an ad hominem attack and a criticism. I stand by every one of the comments I made about Gershom. He has abused his position on the committee to push his agenda for the platform, made the new Haskell user experience worse, wasted countless hours of my time and others', and uses double speak to publicly pretend like he's doing none of that. His behavior is deplorable, and I call it out hoping he will be reined in by the rest of the committee. The people upvoting your vitriol should feel bad for supporting your unacceptable and harmful behavior.
Ah OK. Yes I have seen that happening from time to time. Authors are often familiar enough with their dependencies to know when to be suspicious that a change they are making might be using a newer API feature of the dependency. So I don't think this should come up all that often to begin with. Since this case is dealing with existing versions, it would not be hard to build automation tools to flag possible build problems. Not all incompatibilities are build problems, but many are. That would make the problem even less common. Authors should be encouraged to include "Since version x.x" messages in their haddocks when they make API changes. Some package authors are already quite consistent about that.
It is both, actually.
It's pretty clear that you're frustrated, but I don't think posts like this are helping your cause. The relationship between you and Gershom needs a reboot or something.
I don't think frustrated is the right term. I have no hope that a Haskell website under Gershom's control can be useful, and the committee isn't stepping in to fix the problem, so I'm going elsewhere. After I made these points last month, I dropped the topic, and wouldn't have mentioned it again had it not been for the trolling by fpnoob. It seems pretty obvious that my involvement in matters is not wanted by Gershom (and quite a few others), and therefore I won't waste my time. It would be nice if someone else fought and won this battle though.