I'm assuming you took Exam 98-364/Course 40364A already, if so how difficult was it. I've been writing t-sql for about 3 years and thinking about taking it but wondering if I need to take the course for it or if I can just take it now.
Listen to this poster. Never trust your boss
No, I’ve been a full time SQL Developer for the last 7 years, didn’t really study as such , took one practice exam, which I fluffed, but set my expectations. I’m glad I took the practice otherwise I would have had know idea what to expect and probably failed. The extra cost for a practice is definitely worth it, in my case my company re-imbursed all cost on completion.
I took 70-461 recently. I had to write-in script style questions. A few "fill in the blank" style questions. Then a few drag-n-drop in the correct order questions. The press kit and practice exams were inexhaustible resources. Best of luck.
Presumably the employer is paying for the cert, which makes it slightly less sketchy... So long as they pay, if they reneg, they just equipped OP to get a better offer.
Most people learn by doing. If you can solve a problem in work using Access tet to do it. This will get you exposed to SQL on work time and get you started. For a lot of Database devs (me included) Access was the gateway drug to full SQL server. Access has a great little front end, so try to create something that the business needs. Once you hit the limits of Access sell them on full fat SQL server. Its a win win. You get exposed to SQL and possibly full SQL server and the business benefits too.
Thanks for the response. I tried the method you provided me and it works. Couple of hitches in my source data reared their ugly heads, but asides from that the query executes as designed. Thanks again, and feel free to solicit any other info/resources you would suggest to someone trying to get to a "Business Analyst/Developer" type level!
You want your `delete` trigger to turn around and perform an `insert` into the same table with mostly the same data? Have you considered an `instead of` trigger instead?
I’m gonna get this book. I’ve gotten this recommendation too many times to pass it up. Thanks!
I learned sql before using access, but the visual query designer is really cool imho, although the sql it produces is kind of garbage though. There are a lot of really good fundamental things you can learn really easily with Access, and I think it gets snubbed a lot because, generally, databases built with access are garbage piles full of garbage. But sql querying, joins, working with different data types, primary keys/foreign keys, constraints: its got all that good stuff to get started.
Its gotten me through a lot. Some chapters like the second one can be really dense. There is no need to rush through it. I constantly find myself sometimes going back in order to brush up on stuff. They kinda threw me in the pit like you and sometimes i'll have to reuse someones old query and I won't know what the hell is going on. Some people do learn better through video but if im practicing at work I prefer to not have my headphones on and atleast look like im doing something productive. Plus I read faster and focus better than when I watch youtube or TreeHouse or really any website I tried. 
I know exactly what you mean, like using other people’s old queries. I find it actually helps to break the queries down and try to conceptualize what’s going on. I still have a long way to go. I started the job with no SQL experience —just a few months at a help desk. Before that I had no experience in IT. I’m getting by, though. Fake it til you make it. 
Haha yeah usually Ill try to reconstruct their query myself once i finish the job. But its pretty much the same here, I started out as a GIS tech and now i'm more of a Data analyst (well training to be) that just happens to also make maps. It seems fun though, when my querys start involving math and outside of the box thinking I get a bit overwhelmed. But hey atleast you know the job pays well the better you get and will be around for a long time. Ill probably want to get into database administration at some point. That or pursue Data Science since thats more along the lines with what I do. Unfortunately Id probably have to go back to school and get my masters in Statistics or Mathematics to even compete with people in that field. 
That’s funny... similar back story, same job (data analyst), and the same possible goal (I eventually want to be a DBA). 
Please share table structure and syntax 
You ran it twice? Any triggers on the table?
Post your insert statement.
Figured it out but gracias 
When possible, avoid triggers altogether. Instead, move that logic into the stored procedure that is doing the delete (or insert or update). Triggers can become a problem later and they increase the scope of your transaction. Eventually you'll think you're just doing a simple update, but that'll fire cascading triggers and that'll take much longer than anticipated (potentially until you reach your nested limit). Keep it simple if you can. Avoid them! :)
Postgres with for example DBeaver. 
I've used MS Access as a GUI frontend for a large, crucial application over many years. While it's reliable and easy, I really wish I hadn't. It's not portable and the error handling is dreadful.
The visual tools in Access make learning almost impossible. One just ends up throwing stuff together with mouse, with no insight how it might or might not work. 
Just download an older version of SQL Server Developer Edition, like 2014. SQL Server Express is also free and will have more than enough features for a beginner to start with.
Personally I like to check if the column names exist within the table. I never actually check it with database queries but pull the structure of table from my table structure object or use ORM so Im not open to injections that way
I think I just figured it out, it's literally the for after the brackets, there's no need to specify the table a second time.
Sanitise the field names before handing them over to SQL? Just reduce them to alphanumeric characters with regex. Wrap them in backticks. Better to maintain a list of *allowed* fields to compare them to though. What if there were fields you didn't want to share?
 DECLARE @RKey VARCHAR(10) = '001'; from db..thetable where Rkey = @RKey; 
Just object_id, test_id, status_id, and create_timestamp and whatever else you want there. Basically this is an object_id to test_id linking table. Then you can have a tests table and maybe even a status table.
This solved half of the issue. Now only "Jegyzőkönyvek.LSZ" is empty.
This might be a really stupid question, but how do I make that FROM clause in this case? This is the part that really confuses me. I'm still just a beginner and there is a lot I can't wrap my head around unfortunately.
Variables, the OG reusable alias condition.
I thought of this also, but I can't come up with any ideas to utilize Access to solve something, or enhance something here at work. hopefully something will spark some ideas once I get into the learning though. 
You should definitely solve this with a variable like MaunaLoona said. But I also recommend installing [SSMSBoost](http://SSMSBoost.com) (totally free) and using the "Auto replacements" feature, it's a great way to save time typing.
Why not do something like: select SUM(units) as 'Total Units' , SUM(case when unit_type = 'W' and identifier = 'A000' THEN units ELSE 0 END) AS 'Total WA000 Units' , COUNT(*) AS 'Total Rows' , SUM(case when unit_type = 'W' and identifier = 'A000' THEN 1 ELSE 0 END) AS 'Total Unmatched Rows' , ... from db..thetable where Rkey = '001'
While not a direct answer to your question (because I don't really want to unpack this (nor have you provided the table creation SQL or relevant information from the information_schema), but please consider the following: &gt; We have 100's of tables in that db all with our customers names This is really the crux if your issue, as this is a really poor architecture and you should strongly consider changing it. A more sensible structure would have a single `error_log` table with a column containing a `customer_id`that references your `customer` table. The pain you're feeling here is a direct result of this single (poor) design choice.
Yep, I totally agree. Unfortunately it's not something I can change as I am not the admin or owner of the DB. Just trying to come up with a solution with the tools I have. The script I have essentially looks*everywhere* through every table looking for a string. In my case, the string its looking for defined at @SearchStr can only exist in the 'message' column. So that's how it's finding it. It's certainly not the best way of doing it, but it works on getting me just a smidge from the finish line. 
ping /u/sqlcheck
It's mostly multiple choice, a few adding correct snippets, and a few finish the code kind of thing. So you'll have to write some. You can google 70-461 exam dump and there is a great practice test someone uploaded to google drive. It helped me out a lot. 
At a glance - you're missing a comma between 'recovery' and 'stopat' in the 3rd statement. Correct by using 'with recovery, stopat'. Any valid date/time format will work. For the first restore statement, you have 4 options: A) back up the current transaction log, B) overwrite your current database, C) restore your database using a different name and file location, D) drop the currently existing database before restoring. If you choose option B, you will need to change the database name and include WITH MOVE for both the data and log files, e.g. 'with norecovery, move &lt;logical data name&gt; to &lt;path&gt;, move &lt;logical log name&gt; to &lt;path&gt;'. The logical names can be found in the backupfile table in msdb. If you choose option C, specify that you want to overwrite the data and log by using the replace clause, e.g. 'with norecovery, replace'
thanks great answer. its also what i found. so i did a logbackup and then the restores with no recovery and then the stopat restore and it worked. ( aka scripted it with the gui)
If you're gonna criticize something, it can be helpful to suggest an alternative
If you're expecting to see 6 columns you won't achieve that with UNION. Could you include an example output of each independent query, and the desired output when using UNION?
[Here's]( https://imgur.com/a/IdW8q) images of me executing the code provided above with a UNION join included and an example of another one of my queries.
Yeah so UNION is the wrong command for what you want to achieve. You need a JOIN in your query to bring the two tables together. Is there a mutual ID across the tables? You need to know which hall relates to which flat, otherwise all halls will be shown against all flats: SELECT * FROM ( SELECT COUNT( CASE WHEN hall_rooms.hall_place_id = 'RH123' THEN hall_rooms.hall_room_id END) [Radison Halls], COUNT( CASE WHEN hall_rooms.hall_place_id = 'MH123' THEN hall_rooms.hall_room_id END) [Maidstone Halls], COUNT( CASE WHEN hall_rooms.hall_place_id = 'RH124' THEN hall_rooms.hall_room_id END) [Round Halls] FROM halls INNER JOIN hall_rooms ON halls.hall_id = hall_rooms.hall_id WHERE halls.hall_place_id = hall_rooms.hall_place_id) a CROSS APPLY ( SELECT COUNT( CASE WHEN flat_rooms.flat_place_id = 'TC123' THEN flat_rooms.flat_room_id END) [The Cube], COUNT( CASE WHEN flat_rooms.flat_place_id = 'PH123' THEN flat_rooms.flat_room_id END) [Pack Horse], COUNT( CASE WHEN flat_rooms.flat_place_id = 'OF123' THEN flat_rooms.flat_room_id END) [Orlando Flats] FROM flats INNER JOIN flat_rooms ON flats.flat_id = flat_rooms.flat_id WHERE flats.flat_place_id = flat_rooms.flat_place_id) b; 
i believe OP is expecting 6 **rows** but getting only 3
the reason you are getting only 3 rows instead of 6 when you run the UNION with the COUNTs is because duplicates were removed run it with UNION ALL instead of UNION
That's the thing the flats are unrelated to the halls. There's 4 tables here: halls hall_rooms flats flat_rooms The halls table is linked to the hall_rooms by the hall_place_id and the flats table is linked to flat_rooms by flat_place_id I need to return the total rooms for each flat and each hall (There's 3 halls and 3 flats) and thanks for trying to help that cross didn't work for me sadly! 
Yeah sorry I meant 6 rows not columns. But I am getting 3 columns instead 
You could use a ROW_NUMBER() or DENSE_RANK() function to apply an ordering column to the entire table, where it only changes on appropriate BinNum changes. BinNum 1-A-### would 1, 1-b-### is 2, etc. Then when that ranking column becomes different (i.e. 1 -&gt; 2), you could apply your page break.
try changing "CROSS APPLY" to "JOIN"
Alternatively this might do the job. Union the room_id's so they fall under one column, then perform your pivot/count on the output of that: SELECT COUNT(CASE WHEN flat_room_id = 'RH123' THEN 1 END) [Radison Halls], COUNT(CASE WHEN flat_room_id = 'MH123' THEN 1 END) [Maidstone Halls], COUNT(CASE WHEN flat_room_id = 'RH124' THEN 1 END) [Round Halls], COUNT(CASE WHEN flat_room_id = 'TC123' THEN 1 END) [The Cube], COUNT(CASE WHEN flat_room_id = 'PH123' THEN 1 END) [The Pack Horse], COUNT(CASE WHEN flat_room_id = 'OF123' THEN 1 END) [Orlando Flats] FROM ( SELECT flat_room_id FROM flats f INNER JOIN flat_rooms fr ON fr.flat_id = f.flat_id AND fr.flat_place_id = f.flat_place_id UNION ALL SELECT hall_room_id FROM halls h INNER JOIN hall_rooms hr ON hr.hall_id = h.hall_id AND hr.hall_place_id = h.hall_place_id) a;
Well, he could do something like ... '' AS col1, '' AS col2, '' AS col3, COUNT(...) AS col4, COUNT(...) AS col5, COUNT(...) AS col6 FROM... UNION SELECT COUNT(...) AS col1, COUNT(...) AS col2, COUNT(...) AS col3, '' AS col4, '' AS col5, '' AS col6 Bam 6 columns. Except I'm pretty sure the OP is going to figure out they want something completely different once they get these results. Just a hunch ;)
 SELECT publishers.publisher_code FROM publishers LEFT OUTER JOIN books ON books.publisher_code = publishers.publisher_code WHERE books.publisher_code IS NULL 
Yes but that gives six columns regardless of the union ;)
Exactly! That's why I'm pretty sure OP doesn't know what they are looking for... classic XY problem.
Nope I get multiple errors with that one aswell it won't execute due to the [ ] brackets I've tried lots of values in there and everytime it fails to run :(
Completely agree with this, ranking functions are incredibly useful. Just in the spirit of adding options, another way if the report is going through ssrs would be to add a grouping column by substringing the BinNum column to create a grouping column, putting the results in to a list box in report builder or bids and setting a page break on the substring column 
Thanks everyone for your help I managed to figure it out only took me 5 hours haha. [Here's](https://imgur.com/a/V173l) is a screen cap of the end code &amp; result. Thanks again peeps!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/yueSQoa.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
You can definitely use Lag/Lead in SQL. I'd need to know a bit more about your function but they definitely exist. 
You could concatenate the columns... obviously this won't be perform the greatest... DELETE FROM my_table WHERE (column_a || ',' || column_b) IN ('1,2', '1,3', '2,5') ); (I assume this works... I can't get SQLFiddle to actually run a `DELETE` statement, but it works fine on `SELECT`...)
I haven't tried it before, but I assume you could do some thing like DELETE FROM my_table WHERE concat(column_a, ',', column_b) IN ('1,2', '1,3', '2,5') Although, I'm assuming that works based on my general knowledge, not that I've done it. I usually go with the second option you posted. If it's a one-time thing, I don't feel bad writing ugly queries if they work.
Thanks, I also want to do exactly this type of thing to shorten/neaten the query (there's a lot more SELECTS than I listed). This is a great start. I just need to work out how to have joins involved in only *some* of the SELECTS (as with the 'otherTable' example in my third subquery).
~WHERE ARRAY[[column_a,column_b]] &lt;@ ARRAY[[1,2],[1,3],[2,5]] This should do the trick. The double square braces are important to force it to make nested arrays and compare the arrays within the array instead of just the array elements.~ Nevermind this doesn't work. Postgresql flatterens multi dimensional arrays when using the contains operator.
Thanks, that would definitely work for what I'm doing. Looks like DELETE FROM my_table WHERE (SELECT column_a FROM (VALUES (1,1), (1,4), (1,5)) AS b (column_a, column_b) WHERE my_table.column_a=b.column_b AND my_table.column_b=b.column_b LIMIT 1) IS NOT NULL; also works, and is a decent general solution since it will work for strings etc. too.
Primary keys need to be unique. Since a patient or a doctor could have any number of prescriptions it doesn't make sense for those to be the primary key of the prescription table. 
When a table is indexed it's not necessary for the engine to scan the full table. If it knows that all the posts for user 5 are after user 4's and before user 6's then it's not going to read the whole table. There are also lots of little tricks DB engines do to optimise themselves and methods of forcing certain operations to manually optimise them in those occasions where they don't pick the best execution plan themselves.
Insert your delete parameters as rows in a temp table and delete + inner join.
&gt; then it's not going to read the whole table if all you're doing is a count, it won't read the table at all
I think this is a really interesting question. And I'd like to hear more about solutions in-between sequel. Like caching them before reading or writing? Reddit posts and comments, Facebook likes, these have millions and millions of clicks on a short time. Is that really a database write and Ajax call every single click? I think there is some queueing and caching at work first.
based on your example seems like the join between the 2 tables does not change the cardinality of the result meaning you get the same number of rows frm both tables as you get from using only "thetable", this when you use left join. in that case you can just do select SUM(units) as 'Total Units' , SUM(case when unit_type = 'W' and identifier = 'A000' THEN units ELSE 0 END) AS 'Total WA000 Units' , COUNT(*) AS 'Total Rows' , SUM(case when unit_type = 'W' and identifier = 'A000' THEN 1 ELSE 0 END) AS 'Total Unmatched Rows' , SUM(case when unit_type = 'W' AND othertable.yn = 'N' THEN units ELSE 0 END) AS 'Matched Units N' , ... from db..thetable left join db..otherTable ON thetable.identifier = otherTable.Otherkey where Rkey = '001' but again check that the 2 queries below return the same result. And not only with existing data but in the future as well. basically has otherTable.Otherkey a unique index or better is primary key? than you are good to go select count(*) from db..thetable and select count(*) from db..thetable left join db..otherTable ON thetable.identifier = otherTable.Otherkey 
replayed to myself instead of here :) but check my other comment
Repair corrupt or damaged SQL database using an excellent tool SQL recovery tool. This tool quickly recover all lost and corrupted SQL database. SQL repair tool also recovers all tables, stored procedure, functions, views, rules, triggers and associated Primary Key, Unique keys and etc. It supports all versions of MS SQL server and all updated Windows OS versions. Visit here - http://www.sql.mdfrepair.net/
Your first example works fine. Why not go with that?
You can always check out the SO database: https://data.stackexchange.com/stackoverflow/query/new
Thank everyone who offered help. Turns out I just needed to make a calculated field for 'shelf' and group by that. I don't know why it didn't work the first go around but now it does! I also tried ranking. I didn't know about this and it's a very nice tool to have. Thanks again! 
I got this error when using lead: "lead" is not a recognized table hints option. If it is intended as a parameter to a table-valued function or to the CHANGETABLE function, ensure that your database compatibility mode is set to 90.
I'm *very* wary of all these MS "no code" solutions (InfoPath, Flow, and others) to everything. I've yet to see any of them implemented in business practice successfully. Just look at their graveyard list of [applications that never caught traction](https://en.wikipedia.org/wiki/Microsoft_FrontPage) for a track record of how well they fare. Sure, some do very well. And those are worth investing in. *Infopath is also discontinued*, so if you do use it you would also need to plan to replace it soon... What kind of nice forms are you looking to make? Maybe we can give you a better suggestion.
**Microsoft FrontPage** Microsoft FrontPage (full name Microsoft Office FrontPage) is a discontinued WYSIWYG HTML editor and Web site administration tool from Microsoft for the Microsoft Windows line of operating systems. It was branded as part of the Microsoft Office suite from 1997 to 2003. Microsoft FrontPage has since been replaced by Microsoft Expression Web and SharePoint Designer, which were first released in December 2006 alongside Microsoft Office 2007, but these two products were also discontinued in favor of a web-based version of SharePoint Designer, as those three HTML editors were desktop applications. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
No coding forms are generally not a good idea, beyond the lack of future support, there are also issues with flexibility if you want certain features. [Take a look at this](https://docs.microsoft.com/en-us/ef/core/get-started/aspnetcore/existing-db). It may have a bit more code than you like, but if you have a database, most of the code is autogenerated for you.
Lookup the MS SQL Server Upgrade Advisor tool. It will help you by analyzing your database and telling you what isn't compatible with the desired destination version. 
lightswitch was probably the best for this, shame they dropped it entirely.
I want to create a form my colleagues can use to input data into my db. I want the form to look presentable and be easy to use. The form will link multiple tables in my database. I also want that table to link a filetable so they can load PDF files. What is your opinion on storing files into a db? I heard using filetables are the best method. 
I don't mind a bit of code I just don't want to run into a wall since I am not a professional developer. I will give it a try. Wish me luck. Thanks. 
Thanks for your help by the way!!!
Hello everyone, I'm trying gauge interest in an AMA with Kalen, and also figure out which subreddit we'll host it in. Would love to hear any recommendations. If you also have any burning questions you need answered, I can try to help with that too! -DB Best Technologies
 &gt;=
*outputting 'A' for grades &gt; 90, 'B' for grades &gt; 80, 'C' for grades &gt; 70, and 'F' otherwise*
F &lt;&gt; Fail
That was it! I feel stupid.
This will only end in tears.
hahaha that bad?
It always starts with the best of intentions but quickly will turn into a management nightmare. Not only controlling the input of data but actual management of the InfoPath forms becomes very fragile. Similar to SSIS packages. You make one wrong move and the entire thing is broken. Lightswitch was a step in the right direction but it has ceased to exist, unfortunately.
&gt;Also some stored procedures are slower on 2014 than on 2008 At a guess I'd say this is most likely because statistics were reset in the upgrade, meaning that suboptimal plans got created and cached when each stored proc ran. Have a look at [Brent Ozar's](https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/) article on parameter sniffing.
Yes this worked great, I have managed to rewrite the whole query so that 1) Rkey is only mentioned once at the end 2)there is only one SELECT instead of 14. Incidentally, I need to run the query in DBVisualiser where the 'Declaring Variables' method will not work (worked fine in SSMS). So this has been a better solution all round and I have learnt a bunch. Thanks so much!
Thank you MaunaLoona, for some reason this worked great in SSMS but not in DB Visualiser (I was wrong about it being for a MS SQL database, it was Sybase ASE which is why I needed to use DBVis) - i've been advised this is because DBVis uses 'Command Line Execution' (it was giving me the error 'Must Declare the Scalar Variable @RKey'). lbilali's idea below about reformatting the query below has worked.
&gt;We had to set it to 2008 compatibility to get the application to work. What do you mean by this? Was performance slower/query timeouts when the databases were set to the 2008 compatibility level? Or were things completely broken for the instance?
Queries were slower overall. Timeouts happen on the reporting server. Microsoft had us put the db in compatibility mode, but nothing was identified by the upgrade advisor. 
I’m late to the party. Just trying to figure out the issue. Wait it that you didn’t use the following on your CASE statement? *ELSE ‘F’ END)*
My guess is that you were hit by the new cardinality estimator. It's *generally* better than the old one but there are definitely cases where performance falls off a cliff and you need to rewrite the query or tune indexes. Make sure you've got all the service packs &amp; CUs installed, there were a number of adjustments made in those. The upgrade advisor looks for obvious, easily-detectable conditions that cause issues - AFAIK it can't identify individual queries that will slow down.
I am using Access as a front end to our SQL 2014 for the purposes of data entry and only that. I haven't worked with an actual Access db in many years but find that it works well as a form pointed to a table. You might want to check it out
I am just going to state the obvious, but there are datatype attributes for every field in every step of the pipeline. Sometimes SSIS will auto-detect datatypes and make assumed conversions for you somewhere the middle of the pipeline. I would double check the datatype of the field through the entire pipeline, including the datatype SSIS defined for the destination table. I have not seen this with a decimal datatype but I have seen some crapy field auto typing happen on string lengths causing truncation in an odd place in the middle of the pipeline. 
Also, SSIS' pipelines use the C# datatypes. Not all sql datatypes have the exact counterpart in C#. Indeed, this can become quite cumbersome.
You could [recompile](https://www.mssqltips.com/sqlservertip/1260/script-to-recompile-all-sql-server-database-objects/) your objects.
Thanks for your response. We did a lot of troubleshooting yesterday (the whole team quickly became curious) and double-checked the datatypes on both ends. The only explanation we could come up with is that it converted float to string, truncated, then converted to numeric, but that seemed pretty far-fetched....
There are "DB" types as well which I believe are supposed to represent the SQL types. But yeah, it's not straightforward at all.
I seem to remember somewhere in the back of my mind there's some weird conversion between float and decimal if you use a string where you end up getting a FLOOR conversion between the two, but I could also have dreamed it. :)
InfoPath has ceased to exist as well.
Are all of these updates, not inserts? is there already a value that gets nulled out when you try to save or does it just not update? is the datetime value correctly binding to the front end? I would say it's likely a Hibernate issue if everything is working from SSMS. 
Going to need more information than "doesn't work". What's the update code being run by the application? What happens when you run that statement directly against the DB? What happens when you debug? What is the application (website using java I assume) and the code there?
Are you certain that the update on the view is deterministic? If not, then it's highly likely you'll have problems. It's really difficult to determine this without more information.
Not sure about mysql but if you decide to use Microsoft Sql server (the free developers edition) I could maybe help you. 
&gt; The only problem is I'm trying to upload the data onto MySQL. Is that how it works? In a real world scenario, the database is likely there on a server and people connect to it and look at what's inside it. The data that gets there can be from an application connecting and manipulating data, users manually entering and submitting data, or individuals doing manual imports and exports from the database. (Automation can take a role in here too.) &gt; Kind of like Excel or Access where I upload a spreadsheet into a certain program where all the data is stored? Pretty much. You can commonly import data from spreadsheets or excel into many database platforms. Personally, I'd recommend a hand holding tutorial to give you a basic syntax and breakdown of SQL then dive into it. A 1-2 hour intro to give you a brief idea of how data should look, how you select it, how you filter it, etc would be valuable to give you an idea. The problem with not having a data set readily available for you to work with, is you should probably understand data philosophy and architecture. If the book provides resources and data sets for you to use, I'd highly encourage to use them instead of doing your own. Here's a few sample data sets if you aren't sure where to look: https://dev.mysql.com/doc/employee/en/ https://dev.mysql.com/doc/sakila/en/
Are the two one in the same concept? 
Thank you. I will take a look at this. I also read that SQL Server might be easier to learn from. Any experience on this? 
Generally, yes. They might have different flavors of sql for instance MSSqlServer uses T-SQL but everything standard in that book will work. Im a beginning data analyst who started as a GIS tech and i bought that and another book to get me up to speed. They use sql server where I work. Its also pretty easy to upload the scripts. For sql server you just opened it as a query and ran it, then the database would be created. It might be the same as MySQl. The hard part for me was just making sure I installed everything correctly. For that I used Wise Owl SQL server 2016 part 1 on youtube. I probably should have watched the other videos but I was eager to start. Let me know if you need any more specifics. 
Thanks! I'll save this and do a bit more reading this weekend. What's the other book you used? And in your experience is it better to learn through reading a textbook or working through samples trying to figure out problems? Like any other languages I assume its best to learn from experience. 
T-SQL fundamentals by Itzik Ben-Gan. What i did was work along with the book. Use the examples they provide. Then once that sections done or even the next day I would try to create problems in my head and try to solve them using what I learned. The t-sql book seemed like more of a walkthrough though using the database it provided. It pretty much lays the foundation for writing queries in mssqlserver but still has challenging questions at the end of each chapter. I do plan on going back and finishing the mere mortals book. Eventually I plan to get Effective SQL. I tried learning from Tree House and other websites but that just didnt work for me personally. i like creating my own pace with books. There is also the free Wide World Importers database from microsoft you can use too.
T-SQL fundamentals by Itzik Ben-Gan. What i did was work along with the book. Use the examples they provide. Then once that sections done or even the next day I would try to create problems in my head and try to solve them using what I learned. The t-sql book seemed like more of a walkthrough though using the database it provided. It pretty much lays the foundation for writing queries in mssqlserver but still has challenging questions at the end of each chapter. I do plan on going back and finishing the mere mortals book. Eventually I plan to get Effective SQL. I tried learning from Tree House and other websites but that just didnt work for me personally. i like creating my own pace with books. There is also the free Wide World Importers database from microsoft you can use too.
Do you work daily with SQL? Or is it just something you wanted to learn for personal use/education? 
I was a GIS tech who wanted access to the databases so I wouldnt have to bother everyone. Now im becoming more of a data analyst who also does GIS. So I work with it every day of the week they just havnt given me permission to access all of the databases and tend to give me simpler problems to work with. 
Out of town this weekend. What is it
Free Mini conference of sorts, lots of training sessions. http://www.sqlsaturday.com/683/eventhome.aspx 
Is this SSAS?
I tried using books to help me with SQL but I kept getting bored easily. Honestly I'd suggest starting your own project and use the book as a guidance. Create a project you're passionate about for example I'm a wrestling fan so I created a relational database and started to learn more by researching on how to design it and using SSIS to import the data etc. I understand everyone is not the same and this approach may be better for you. Good luck 
Im not doing any of that yet. I just pull stuff from databases that we have put together for clients or vice versa. All I use is the Management Studio and ArcMap or Qgis on a daily basis. What is the Analysis Services do? I havnt been shown that yet. 
I've got 7 years of SQL Server and I'm a MySQL enthusiast with ~1 year. Personally, I find them about equal, but I get better instructions and directions on How to's with SQL Server and there's an easy start setup with SQL Server. If I were to go back in time and choose again with what I know now, I'd do SQL Server still.
I copied and pasted your stored proc into my test environment and it executed successfully. Then I ran it EXEC proc_TerritoryTop5Sales_ByProduct @territoryname ='Alaska' And got 20 rows as results. Using SQL Server 2014 and AdventureWorks2014 db.
hmm...i might have to just copy and paste from reddit back into sql.... 
You're missing COUNT(*) from the SELECT in your subquery. Add it in and I bet it'll run
`||` isn't how you concatenate strings in MySQL... it's a `logical OR` in MySQL :). It'll work in Oracle, but it's best to avoid that as there's no real way to optimize the query. You're on the right track, though... just check that the two columns aren't in the subquery: SELECT vendor_name ,vendor_city ,vendor_state FROM Vendors WHERE (vendor_city, vendor_state) NOT IN ( SELECT vendor_city, vendor_state FROM Vendors GROUP BY vendor_city, vendor_state HAVING COUNT(*) &gt; 1 ) If you _really_ did want your query to work, just replace your `||` with a `,` and utilize the `CONCAT()` function: SELECT vendor_name, vendor_city, vendor_state FROM Vendors WHERE CONCAT(vendor_city, vendor_state) NOT IN ( SELECT CONCAT(vendor_city, vendor_state) FROM Vendors GROUP BY CONCAT(vendor_city, vendor_state) HAVING COUNT(*) &gt; 1) ORDER BY vendor_state , vendor_city;
Do you know how to program? The easiest ways I found were: 1.) On the job 2.) Self study (ex: building an application) On the job is self explanatory, however, if you can find something that you would find useful or fun to code you will definitely find opportunities to learn SQL. Any useful application will likely store data in some manner, and this is where you can potentially learn. Build something that is fun and helps you; or at the very least interests you. I learned through on the job, started as IT support, moved towards mostly supporting SQL, now I'm a programmer. There's a lot of different paths but I feel like realistically if you already know programming and have a cool project, it's the easiest way to learn. SQL is one of those easy to learn, hard to master skill sets (it goes way deeper than I ever even imagined starting out) but just interfacing a DB with an application is more basic than you'd think. Hopefully this helps :).
Who's Kalen Delaney? Why not provide some bio info.
&gt; "Edit: turns out SSIS doesn't like temp tables and throws a metadata error! Used a table variable instead. Still haven't figured out the original issue with the datatype not converting properly though..." You can declare and utilize temp tables in SSIS. They need to be universal/master temp tables, so ##temp - they won't work with #temp During the development and testing phase, you have to open SSMS and actually create the universal ##temp. Then SSIS testing will work. You can save and deploy and the package will continue to work just fine when executed. Every time you re open you'll get an error that the ##temp table doesn't exist so you have to recreate it. *** Floats are imprecise arithmetic values. I'm not surprised you're getting issues converting. I'd recommend going back to whatever is used to calculate the result of what you're storing in your float field, and use that math to store the decimal result. Or, if that's not possible, use the dodgy ##temp table method. I don't recommend @table variables pretty much every but especially if you're doing string interrogations and data type conversions. Good luck
Thank you for taking the time to write this explanation, I appreciate it.
CTEs ARE magical! 
Hello. I actually have a few questions for you, /u/strongbigbear 1. Are you interested in data mining? 2. Are you interested in writing queries/programming? 3. Just looking to pad your resume? 1. if you are looking into data mining, don't get lost in the details of programming and writing queries. it will be much more important to understand how data works, how to avoid looking foolish while presenting/explaining your findings. As you learn what data you need to get the report, the table structure will be a lot easier to understand and writing will be easier. Also, with this approach, your not going to be doing a lot of back end work on the server. most of it will be spent on the front end. I am familiar with Microsoft, so this would be SSRS (SQL Server REPORTING Services) 2. I have done a lot of work on this side of the world, i think its fun. Anyway, with this approach, you really need to have a programming back ground. tables, index, wild cards, etc will be easier to understand the the format of a query is just as easy to understand. But, programming doesn't really lend a hand to understand how people will want to see the data. Going down this road will lead more towards ETL(Extract, Transform, Load) construction and database and query optimization. SSIS 3. if this is just to pad the resume, Don't do it. People in this field are nerds to the extreme and they will call you out on your bullshit quickly and loudly. if you still insist on this reason, make sure it is know you are not an expert. as for setting up and actual database, here is a good example for mysql https://dev.mysql.com/doc/employee/en/ i would recommend looking at Microsoft SQL Server. there are a few more cert for it and IMO it is a much more common platform. here is a good walk through for a demo database. https://msftdbprodsamples.codeplex.com/
I’ll be there. Speaking too: https://www.mlakartechtalk.com/im-speaking-sql-saturday-charlotte/ Stop by!
 SELECT * FORM QUERIES WHERE COMMAND LIKE '%COOL%'
The customer or client is always right, but someone in your organization is in charge of controlling the message and setting expectations. If that person did not, then you may wind up in this situation. You might end up in this situation regardless. Best you can do is try, and be honest about what is feasible
thank you. 
I think you're missing probably the best and most prevalent product in the marketplace.
I went the GIS to SQL Analyst route, now I'm working as an SAP developer. I've felt that GIS is a great entry point to the world of Data programming and business intelligence.
SSAS is useful when your organization wants to create and utilize a data warehouse.
I would take a look at the triggers on those tables. It's possible there's a parameter(s) being passed and may be doing something different. 
Came to say this. Also, if there are wildly broad parameters, like beginning and ending dates or on a primary key, declare a second set of variables within the transaction, identical to the proc parameters, and set them equal. This will avoid parameter sniffing and force a new execution plan on each run. 
+1. Just FYI, no such thing as a left outer join. It's the same as left join. Also, where does not exist (subquery) will work faster. 
Use this to fill a variable table and then search said table for your error. --Find Column USE []; GO SELECT t.TABLE_CATALOG ,t.TABLE_NAME ,t.TABLE_TYPE ,c.COLUMN_NAME FROM INFORMATION_SCHEMA.TABLES t JOIN INFORMATION_SCHEMA.COLUMNS c ON t.TABLE_NAME = c.TABLE_NAME WHERE c.COLUMN_NAME LIKE '%[]%' ORDER BY t.TABLE_NAME 
If statement in the delete trigger, if meets criteria, call proc with a simple insert script. Make sure you do this after the delete or you'll probably get violation of primary key error. 
Which is? 
Delete from @table. Define @table in a variable table. Use a while loop. You can update the variable table with a column called is_deleted within your delete. While is_deleted sum &gt; 0 delete from top 1. Wrap in an EXEC @sql. 
Need to each result to the complete number and then do a percentage calculation. 
Under the Cons for Redshift - &gt; It could also get very expensive considering the fact that Amazon bills you for storage space as well as server requirements (CPU, RAM etc.). Not sure what this is supposed to mean, since it seems to imply that billing is separate between compute and storage, but [all current node types in Redshift](https://aws.amazon.com/redshift/pricing/) include both. Granted it is pretty costly, but the wording here is sort of odd (unless I'm missing something here).
&gt; FYI, no such thing as a left outer join. please, sit down... you're embarrassing yourself
https://stackoverflow.com/questions/406294/left-join-vs-left-outer-join-in-sql-server If we're talking SQL Server it's no different. It's syntax between versions...
Guys come on, there’s no reason to argue about this. I have seen left outer used simply for the purpose of reminding those who are not familiar with SQL code what it does. Left performs an outer join… It’s optional, and doesn’t hurt to include it. If someone is reading your query and doesn’t know what left, right, inner or any of that means… It might actually help to include the word outer. No reason to fight about it though. We are all here to help
the OUTER keyword is in the standard you are familiar with the SQL language standard, right? 
You can type outer all you want, but it doesn't do a damn thing. Might as well comment out a line in all your code and exclaim how butt hurt you are. That's should help just as much. 
Was surprised to not see Snowflake here.
This appears to be looking at strictly open-source solutions, however here are a few that I know of that may be good to look at: * [Netezza](https://en.wikipedia.org/wiki/Netezza) - This is some pretty cool, but expensive tech. If you can prove that this is faster and you get a better return on investment, it may be worth looking into. * [Infobright](http://www.ignitetech.com/solutions/information-technology/infobrightdb) - This is a MySQL-based solution that could be pretty good. I did a brief comparison with Oracle for a data warehouse and it was substantially faster and used much less storage. I didn't look at scaling, but it could be promising. * Greenplum and Terradata - I haven't used either and, AFAIK, they're both very expensive.
**Netezza** IBM Netezza (pronounced Ne-Tease-Ah) designs and markets high-performance data warehouse appliances and advanced analytics applications for uses including enterprise data warehousing, business intelligence, predictive analytics and business continuity planning. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
It's declarative in the sense that you declare what you want (e.g. insert this set of records into a table), but not explicitly how to do it. For instance, to populate a two-dimensional matrix in an imperative language you'd have to loop over both dimensions and populate every value. In SQL you just define the set and let it handle the details. You can fine-tune this via execution plans, but you are not defining step by step how the it gets executed like you would in an imperative language.
Ok thanks! It's still a bit unclear for me: In imperative language, the "how to" is hidden too, isn't it? I can tell (not real imperative language, but kind of): "let A = [[1, 2], [3, 4]];", without saying to the language: reserve some space in ram, then write a bit, then another, iterate, etc. And in an imperative language, I can call a "sort()" function that sort an array, without iterating over it (I'm thinking about basic, php, c, etc.). So i just "defined the [thing] and let [the compiler|the interpretor] handle the details". I'm still a bit lost.
Yes. Get a Bachelor's, then a Master's, then a PhD in data science. 
You will get several opinions on this. I have managed and hired teams of data analysts for quite a while now. For me, nothing will beat experience. If I'm hiring for a senior role, I like to see a wide range of skills and knowledge. For junior roles, I'm mainly attempting to judge aptitude. And the undervalued skill is communication. It sounds hokey, but I spend more time attempting to act as a translator between my analysts and the business that I care to. I've never paid much attention to certs. They are certainly available, but I'd suggest just saving yourself a few thousand dollars and doing Coursera ones. At the very least some of the final projects are enough that you could probably put them on a resume. The more established certs are expensive and hold little value to me as a hiring manager. Last up, if I was starting out today, I'd lean heavily into the Data Science side of analytics. If you can learn a few statistical tests and methods and apply them to real data, you can get your foot in the door just about anywhere.
What are the most relevant statistical tests one usually uses in dsta science? 
You can only call a sort function in an imperative language because it is part of the base class library. C or Basic itself does not have a native operator called sort, like it has for +, -, and other basic math operators. Yes, modern imperative languages have auto memory management and auto garbage collection. They are trying to become a bit more declarative. But they are still built with the premise that you, the dev, needs to define both what you wat, and how you want the language to handle it. You are expected to implement loops and traversal logic over your object collections.
Unless you're programming in assembler you are always going to be dealing with some layers of abstraction, but you are essentially defining an algorithm, telling the computer how to do something step by step, even if the compiler is filling in some of those steps for you. A SQL query is not an algorithm. It defines and manipulates sets (i.e. records in a table). You as the programmer don't write the step by step algorithm for implementing these set operations, you just "declare" what they are.
Google will be your best friend. But where I currently work, all things Machine Learning are popular. Resources are everywhere online these days, but here is [one](https://medium.com/towards-data-science/statistics-review-for-data-scientists-and-management-df8f94760221) that popped up and is probably a good start. Everyone has a ton of data. But figuring out how to use and produce a result that is valuable to the business will always be the challenge. Anyone can sit around all day doing data science. But no one is going to pay you (for long) to do a ton of super-cool projects that produce results that do nothing at all to help the business meet their objectives. 
GIS
As a hiring manager, do you feel masters degrees are vital to progression in one’s career as a data analyst? Everyone I work with has a masters except me and I’m wondering if I need to get on board before I miss out on advancement.
on the occasions where you need to know previous login, will you want to have it for a specific user? i can't imagine where you'd want to have a result set of the last logins of multiple users at the same time so if it's for one specific user, please don't create a column, just run this extra query SELECT login_date FROM logins WHERE user_id = 12 ORDER BY login_date DESC LIMIT 1 OFFSET 1 
Ok thanks!
Ok thanks!
Management studio will let you run a SQL server on your pc. 
I am not a hiring manager, but I am a Senior Data Analyst and have 0 official higher education. If your company is worth a damn, you can impress the higher ups with your skills and not a degree. 
The server is just software that you use to interact with the database(s). In that sense, it's not different from any other software you install on your PC. If you're new and want to set something up without too effort, I would install SQL Server Express LocalDB and use SSMS.
If you can't install anything in your computer, try http://sqlzoo.net/ You can select your database engine via the drop down in the top right. Also http://sqlfiddle.com/ If you're just looking for a Sandbox 
Download SQL Server Developer Edition. It is free. Also download and install SSMS SQL Server Management Studio
[removed]
File seems to import fine for me, just need to change the datatypes a bit because I got an error importing the file, likely just need larger values some place: https://imgur.com/a/JNXvN
I always have a bugger of a time with importing text files that often have something to do with the end of line delimiter being different in UTF/non-UTF. Try converting the text file to a different character set before importing maybe?
\^I can tell you like to live dangerously, Mr. I-log-in-as-administrator.
First off, thank you for the quick response. Secondly, I was unaware that the file was delimited. I feel silly right now.
You can run almost any relational database locally, but [SQLite](https://www.sqlite.org/) might be of interest to you.
Nope, not at all. I have two analysts working for me right now with MBAs. Their MBA didn't count for a thing when making them an offer. One of them is a Data Analyst I and is the lowest paid on the team. If you want to get an MBA and have the time and money, go for it. But don't expect it to advance you in your analyst career. The undergrad degree will gain a little more notice, and it is appreciated if it is technical (again, I'm trying to judge aptitude). But experience is still king.
What platform are you using? e.g., MySQL, MS SQL, Postres, etc?
Thanks I got this set up! 2016 version wasn't working with my windows 7 but I got the 2014 one working now and am building a database. 
There's an existing database, although I have no idea what platform I'd use
hahaha, ya its a virtual machine I don't care about
Depending on the database platform you are using you might be able to keep all of the logic and notifying within the database. I know MS SQL Server has a nice built in email agent. That paired with a stored procedure to handle the logic plus a SQL agent job to handle the automated execution/ scheduling would be a nice combination and I think could handle what you are looking for. You mentioned having some experience with python. That’s another solid option that’s more agnostic to the database backend. The python could handle the logic and the emailing aspects. I’m not too familiar with scheduling regular python jobs for execution but I think there are plenty of options depending on your orgs available infrastructure. I’m a fan of basic task scheduler on windows servers if that’s available. This sounds like a fun solution to build regardless of the path you take. Hope this helps. 
You'd presumably use whatever platform the existing database is on - do you know what it is? The details of the implementation would depend a lot on what type of database it is.
If using MSSQL Server, once you can figure out how to get the weather data and insert into a SQL data table or create an SSIS package to import it into a table you could create a scheduled SQL job to do the steps for you: 1. SSIS import (use the Import/Export Wizard and save your package that you can pull up in the first Job configuration Step) 2. Execute a Stored Procedure that runs your SQL script based on your business logic which includes sp_send_dbmail which can send the emails to whomever you want. You could even send SMS text message through e-mail for free if you know the receivers carrier.
Got it, I'll definitely check out the DB platform we already have. I just started working with this company 3 days ago and it's not part of my job description to do something like this so I haven't been given any access to the backend yet. Thanks for the help!
This really helps a lot, thank you for the specificity! Any thoughts on how I could get the weather data to be inserted into an SQL table automatically on a daily basis?
Yep they dropped support for Windows 7 in 2016 (just the sql server, you can use ssms 2016).
On the point that /u/Ton86 made... We used to get exchange rate data through an xml feed from a bank. They had an API that we would call, which would return their formatted XML. Then, once we had the file, there was an SSIS package that would parse the data and insert it into an SQL table for us, which we would then analyze. As /u/Ton86 indicated, we used an SQL Agent job, that triggered the SSIS package. Email alerts on the package were designed to contact the DBA incase of a failure, but the emails to the consumer were actually managed in a separate SSIS package. That's just cause I personally like to keep my "system admin" and my "data stream output to clients" on separate platforms. Plus, SSIS mail is more flexible. I'm not sure, but I bet that the national weather service would perhaps have access to an API or something where you can extract weather data? My fear would be that they're more likely to include past weather data, and less likely to include forecasts (which I think is what you need). Just a guess. 
Yes, depending on the distance and what day it is I could need up to a week long forecast. I know the weather channel has an API, which I was thinking of using. So it's not safe to assume that the data acquired through their API would be up to date?
I'm not sure, but you probably can find something Googling around. Maybe search for using Python to automatically get weather data. I saw a few interesting links come up. You may be able to integrate Python into the latest version of MSSQL too (since you can now run Python in it) and skip the SSIS step.
Awesome I'll check it out. Thanks again!
&gt; And the undervalued skill is communication. It sounds hokey, but I spend more time attempting to act as a translator between my analysts and the business that I care to. Been here! To follow up on your great advice... The business will not give technical specifications. Instead, you'll get a lengthy story about how "It would be great if we knew how..." Or, "We can't get "X" done because we can't figure out..." And other long and convoluted "My job sucks because" stories. There is real REAL value in being able to distill a business problem presented as "It sucks because" into actual programming and reporting. Most likely, you're not just providing reporting services. You're providing an entirely new work-flow that can happen because new and valuable information is available. It's not about reports. It's about the ability to create new and streamlined business processes by creatively using a collection of technologies to do the business's requirements more efficiently. So, with that, I would suggest OP get a PMP - PMI Certification and then get a deep understanding of how his company really runs, business-wise.
This is the path Microsoft recommends for BI: https://www.microsoft.com/en-us/learning/mcse-sql-business-intelligence.aspx
Thanks for the link. If you know, how much do these courses usually cost? Microsoft’s website isn’t too straightforward. Also, it seems like some of the courses are available online, but then it was asking me to search for courses in my area. It also provided a list of their partners who offer the course. 
 What have you put together so far? 
Try defining your variables, the concatenate the other stuff. SELECT '&amp;&amp;user_name' || ' blah blah blah ' || '&amp;&amp;location' || ' blah blah.' FROM dual
Use Rank instead of Row_number https://docs.microsoft.com/en-us/sql/t-sql/functions/rank-transact-sql Then a good demonstration of showing both names can be found here https://stackoverflow.com/questions/15154644/group-by-to-combine-concat-a-column
My advice is to never set any field to that. This is something that should be done at query time. There is no need to have a column that equals that greeting when you can just do it in the sql projection of a SELECT statement, like so: SELECT 'Hi, ' || USER_NAME || ', thanks for choosing us at ' || LOCATION || '.' FROM my_table; Having said that, if you really really want to update a column with this greeting, you can do something like this: UPDATE my_table SET greeting_column = 'Hi, ' || USER_NAME || ', thanks for choosing us at ' || LOCATION || '.' And that would update all the rows in the table. 
&gt;Note: This certification retired on March 31, 2017
I'd recommend you start with SQLite. http://sqlitebrowser.org
SSMS is just a client, it needs to connect to a server.
You can disable stopwords entirely: https://mariadb.com/kb/en/library/xtradbinnodb-server-system-variables/#innodb_ft_enable_stopword
So if I place innodb_ft_enable_stopword=0 to the above mentioned co nfig file that should disable them after rebuilding the index right? 
Yes
I highly recommend PostgreSQL is free, open source, platform agnostic, easy to use, etc 
Spot on, /u/AssHelmet !
Might be worth trying out Instant File Initialisation or run sp_who2 and check the wait type when you're expanding. Something is fishy, would be interested to know what.
&gt; Instant File Initialisation DO NOT work for log file, it is used by OS only to expand DB file (and it does it really fast)
I see, cheers.
ty for advice, will try track with sp_who2
Have you tried to replicate an a single DB that's not part of the failover topology but shares the same hardware? How about installing a completely new SQL instance and seeing if the problem follows still follows on a new DB? As mentioned already, it certainly does sound fishy, file expansion shouldn't be CPU-bound. Is all the storage you've tried been local? Or has a SAN been involved? 
It looks like a lot of the same stuff, just a lot more tests
You'll want to get a copy of the Northwinds data.... a lot of examples refer to it. Plus once you "learn the data" a bit, you have a decent sized dataset you can play with.
we did not try local DB, because everything works perfect with HT off... and 2 (old) nodes of this cluster use HT (40 cores total) withour any issue. I know how stupid it looks like, but i'm stucked at this point as well. i know that file-size change does not related CPU in any way, thats why first things we did,- move DB to a different storage, delete all HP drivers etc. Also we have second new node with HT off (36 logical cores) and it works like a charm.
You have the option to either do online or in classroom. The list they provide you are 'microsoft partners' that host their tests and courses. They each have their own incentives and their own prices, but all teach the same thing. 
What you did makes sense to me. Obviously just turning off HT permanently is an option, but it *shouldn't* need to be an option, right? So that's why I'd recommend experimenting with different configurations to try to narrow down exactly which combinations cause the problem. Other things to try would be SQL 2014/2017 to see it's only related to code in a specific version. It was a production environment I probably wouldn't bother and just leave HT off, but since it's development I'd keep digging if you have time.
we've moved from 2014 with this.. The only thing we've noticed , that a friendly admins from another dep. has pretty similiar configuration with 60+something logical cores. And it goes well. We've tested it multiple times, and this behaviour occures with 72 cores, and HT on. Server is HP proliant dl380 , with intel xeon e5
What was your self join? That should work. 
the self join exploded the number of rows in the final dataset. Also I was hoping for something other than a self-join (was thinking lag would work but I don't think I'm using the partition by clause on the right columns)
Thank you for your thoughts!!
This is really helpful. Thank you!
What do you mean by exploding the number of rows? Select the 3 columns from the logins table 1 and the max login_date from the logins table 2. You're left joining the logins table with 2 join conditions: user_id is the same, login_date in the 1st table is greater than login date in the 2nd table. 
To use lag, partition by user_id and order by login_date
First I did a dense rank partitioned by user ID and login date. Then I did a self join on t1.rank = t2.rank+1. And this created a few duplicated entries
If you partition by user id AND login date, then each partition only has one row. You don't need to rank anything though. 
You have to install a special version of SSRS that you can get fron the powerbi site. 
[removed]
i tried that, but the prev_login only looked up 1 previous row which is not what i wanted
Isn't the powerbi report server and ssrs with powerbi support two completely different things? [This is what I'm referring too](https://blogs.msdn.microsoft.com/sqlrsteamblog/2017/01/17/power-bi-reports-in-sql-server-reporting-services-january-2017-technical-preview-now-available/), have they scrapped that and just spun it into it's own server now? 
Powerbi reportserver has all the capability of ssrs and runs powerbi. And for now it's the only way to run power bi report on premise. It's only a technical preview obviously and as such doesn't have the same capability as the cloud version. My guess is they couldn't finish it in time for ssrs2017 release.
If they want snapshots, use a snapshot? https://docs.microsoft.com/en-us/sql/relational-databases/databases/create-a-database-snapshot-transact-sql If your current process works, but you don't have views... you can generate the TSQL to create all views in a database: USE [Your_Snapshot_DB] GO SELECT object_definition(object_id) FROM sys.objects WHERE type_desc = 'VIEW'
I am not an expert but can you not use the snapshotting feature of sql server? https://docs.microsoft.com/en-us/sql/relational-databases/databases/database-snapshots-sql-server Or this technique to clone a database. https://msdn.microsoft.com/en-us/library/hh272696(v=vs.103).aspx 
Just to clarify, I'm saying you should partition by id only and order by login date. That way lag is pointing to the record you are interested in (same user, previous login). But now I'm not certain I know what you are looking for at all. I thought you DID want the one most recent login by the same user for each login.
"Database snapshots are available only in SQL Server Enterprise." :( 
yeah i think i phrased my a bit inaccurately. I wanted the most previous date that was different from the current row's date
I'd create a reporting database that has an ETL process to move or aggregate data needed and set it up to be a OLAP / ROLAP / HOLAP pending whatever your needs are, and then report off it rather than dynamically creating DB's with the data. Then you can use views or synonyms to manage the naming conventions if you needed.
Well, if you want what's in your sample table, try either of my suggestions, the self join or the partition by id, order by login_date. Post your code and a sample of your results. Maybe I'm misunderstanding you entirely, but it's hard to tell without seeing exactly what you're seeing.
Sounds like a call to the CAT team might be in order.
here is a simplified version with visits as ( select user_id , login_date , dense_rank() over (partition by user_id order by login_date) as r ) select s.user_id , s.login_date , s_prev.login_date as prev_login from visits as s left join visits_til_conversion as s_prev on s.user_id = s_prev.user_id and s.r = s_prev.r+1
This is the right answer. What your department wants is a data warehouse. Your transactional database should feed and update the warehouse on a periodic basis. 
looks reasonable to me
how would you re-do this with lag?
select user_id, login_date, lag(login_date) as prev_login over (partition by user_id order by login_date) from logins order by login_date desc
I’m on mobile so I can’t see your query well enough due to the length, but something I always suggest when diagnosing sub queries is to create tables with the results of the sub queries and then join to those tables. So CREATE TABLE SUBQUERY1 AS {Subquery goes here} CREATE TABLE SUBQUERY2 AS {Subquery 2 goes here) Then instead of joining to the sub queries, you can join to tables that happen to have the same results as the SQ would get you. That approach eliminates the complexity of he subqueries while letting you confirm if they are the real problem, or if it’s the join logic (like trying to join on NULL= NULL or something.
Thanks for the reply - is it possible to create the tables in memory and then reference? Ultimately that's what I was attempting to do.
why in the world would you have this: on po_sum.BUSINESS_UNIT_ID = it_qty.BUSINESS_UNIT_ID and PO_SUM.source_order_number = IT_qty.INTR_DETA_SOURCE_NUMBER and po_sum.SOO_DETAIL_LINE_NUMBER = it_qty.INTR_DETA_SOURCE_LINE_NUMB and trim(po_sum.STYLE_ID) = trim(it_qty.STYLE_ID) and trim(po_sum.COLOR_ID) = trim(it_qty.COLOR_ID) and trim(po_sum.SIZE_ID) = trim(it_qty.SIZE_ID) and trim(po_sum.DIMENSION_ID) = trim(it_qty.DIMENSION_ID) in your join condition for rec_qty subquery?
I'm trying to link the three different subqueries. Those are the key values that link the different types of data. It's a sloppy join, and probably part of the problem, but the data is correct.
Technically you can set the Tablespace to temporary in your CREATE. CREATE TABLE SUBQUERY1 TABLESPACE TEMPORARY AS SELECT * FROM... Then after your query, you can DROP the Subquery tables. Your temporary Tablespace might be named something other than TEMPORARY.
hmm this doesnt seem to net the same results as the self join
Interesting. A ton of differences, or just in edge cases? I feel pretty sure my query should work, but I suppose I could be missing something.
this was the result +---------------------+---------------------+---------------------+---------------------+ | user_id | login_date | visit_date | prev_login | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-04-02 10:29:58 | 2017-04-02 10:10:41 | NULL | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-04-02 12:49:35 | 2017-04-02 10:10:41 | 2017-04-02 10:29:58 | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-05-03 12:03:39 | 2017-04-02 10:10:41 |2017-04-02 12:49:35 | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-05-03 12:03:39 | 2017-04-19 07:55:40 | 2017-05-03 12:03:39 | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-05-03 12:03:39 | 2017-05-03 11:54:11 | 2017-05-03 12:03:39 | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-06-02 10:30:04 | 2017-04-02 10:10:41 | 2017-05-03 12:03:39 | |---------------------|---------------------|---------------------|---------------------| | 12 | 2017-06-02 10:30:04 | 2017-04-19 07:55:40 | 2017-06-02 10:30:04 | +---------------------+---------------------+---------------------+---------------------+
I wish I could say, but I don't know. While I have some experience with APIs to get *data*, I don't have experience with weather data. I just figure that weather services are highly data-driven, and so I would expect they'd have that kind of data available. Up to the minute? Doubtful. But up to the day? I would think so. 
Thank you for taking your time to reply. I guess I'm more of #2. Ideally my interests is in data analyst and writing queries to perform the necessary tasks answering questions just seems like an incredibly rewarding job. Also #3. But not so much as padding my resume. I have no issue no issue leaving off SQL in my resume but it's difficult to get a position doing what I want in said (#2) if I have no experience either. 
In your join condition you only need conditions that refer to the result set/table/subquery that you're joining. So if in your conditions in "join (...)req_qty on..." you do not see "req_qty" on either right or left side - you can safely get rid of those. There are exotic cases when you'd want other conditions but those cases are really exotic.
I'm assuming you're using the wrong tables and you don't actually want "ListPrice" but want the "SalePrice" instead... since we're talking about SaleItems... So... Select ... from SaleItem where ... in ( select ... from Sale where ... )
 SELECT P.ProductName, P.ListPrice, FROM Product P WHERE P.ProductID IN (SELECT ProductID FROM Sale S JOIN SaleItem SI ON S.SaleID = SI.SaleID AND S.Shipping &gt; 60);
Your title states MySQL but your text says SQLite. Which is it? And which version are you using?
Thanks! But for the record, I think this does not belong in a materialized view, either. Maybe an expression in a view (so it's not actually stored), but not an MV.
That’s fair and understandable, I see your point. I mainly place concatenated columns in my materialized views for reporting or email campaigns, as I’m already placing aggregated values in it. My final select (which is in the procedure that the applications use) ends up being a really simple select from the materialized view and barely needs to be changed if there are changes to the logic this way. Changes to those procedures usually require a refresh on the application side (because the applications are horrible), which is annoying to deal with. 
Yeah this is what I ended up doing, thanks!
SQLite, DB Browser for SQLite 3.10.1
I see, that's why you used dense rank. In this case you could replace logins with (select distinct user_id, login_date from logins). Or if you need every visit_date, and visit_date is what is introducing the duplicate login_dates, you could go back to the simple self join-- select a.user_id, a.login_date, a.visit_date, max(b.login_date) as prev_login from logins a left join logins b on a.user_id = b.user_id and a.login_date &gt; b.login_date group by 1,2,3 But if your method is working . . . 
Can you give a table schema? Are there any other directions? What database system is being used?
[removed]
https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2016_03 querying inside BigQuery. that's all the directions for the query
Well it's 0630 here and I just wok up. Also this is for SQL Server. No idea what you're using. Also, I couldn't see a date field in the table you shared. That said, something like this: with CTE as ( select row_number over (partition by name, created_utc order by name, created_utc) as rn, name from [fh-bigquery:reddit_comments.2016_03]) select name --remove to time for all users ,avg(created_utc) as AvgTime --remove to time for all users from CTE where rn between 1 and 5 group by name -- only include if name is included in select.
Will check it out and let you know in am my time. Appreciate it 
I'm not able to open the BigQuery console from the link you posted. But basically what you'll want to do is utilize the [row_number and datediff functions](https://cloud.google.com/bigquery/docs/reference/legacy-sql) to get your result. It will look something like this, but you'll probably have to change some things to get it to run on BigQuery as I know the syntax is a little different. I'm also tired, so there might be a more efficient way. WITH CTE1 AS ( WITH CTE AS ( SELECT user_id, post_date, row_number() OVER (PARTITION BY user_id ORDER BY post_date ASC ) AS row_number FROM users_table ) SELECT user_id, datediff(min(post_date), max(post_date)) as days_between_1st_and_5th FROM CTE WHERE row_number IN (1, 5) GROUP BY user_id ) SELECT avg(days_between_1st_and_5th) from CTE1;
It's adding up all rows that have the same app_id.
Legend! 
&gt;sp_who2 no blocks at SQL level
it is counting all the rows with the same app_id. As a side note the code you have posted will return one row per app_id that mean you can do the same by using a group by and count(*) instead of distinct and count(*) over (...) count(*) OVER (...) is useful when you want to return all rows, no aggregations but you need some aggregated information as well. 
&gt; useful when you want to return all rows, no aggregations but you need some aggregated information as well. Adding onto this if OP wants to do more reading; OVER is a 'window function' which lets you retrieve aggregate information without aggregating the results Clipping from [Postgres documentation](https://www.postgresql.org/docs/9.1/static/tutorial-window.html) A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result.
It looks like you have to compile SQLite yourself if you want to use the R-Tree module. See here: https://sqlite.org/rtree.html &gt; The source code to the SQLite R*Tree module is included as part of the amalgamation but is disabled by default. To enable the R*Tree module, simply compile with the SQLITE_ENABLE_RTREE C-preprocessor macro defined. With many compilers, this is accomplished by adding the option "-DSQLITE_ENABLE_RTREE=1" to the compiler command-line. 
you don't want the (name) to be greater than 4, you want the count to be greater than 4 
Use HAVING like WHERE. Except HAVING uses an aggregate function. WHERE d.deptno = e.deptno HAVING SUM(d.deptno) = SUM(e.deptno)
Does this give you any data back? SELECT Rack , RackSet , b.Description FROM RackLife LEFT JOIN Resource b ON Rack = b.ResourceID GROUP BY Rack , RackSet , b.Description HAVING COUNT(*) &gt; 1
This is awesome, thanks for all of your help guys!
The error is quite straightforward. Your subquery cannot return more than 1 row. Even if the rows are identical. You probably have multiple rows with the same description as your have 2 keys in the Resource table but you are joining on only one (thus there are probably "duplicate" descriptions). I don't think you even need a table type at all here. Why not: UPDATE RL SET RL.Description = RS.Description FROM RackLife RL JOIN Resource RS ON RS.ResourceID = RL.Rack It will probably update a few more rows than necessary again due to the "duplicate" row descriptions. You could join on a subquery instead of the full Resource table by doing something like: SELECT DISTINCT Rack, Description FROM Resource But it sounds to me like you only need to do a 1 time update so might as well make it simple and go with #1. Also make a backup before running this untested code. HTH
It does not, but HAVING COUNT(*) = 1 gives me 154 rows, which are how many are in the RackLife table. :(
Why is that a :(? Isn't that what you want, 154 rows?
That's actually what I had tried first, which I why I tried the variable table route. I did just find that even if I do an UPDATE RackLife SET Description = 'test' (with no WHERE clause) I STILL get the same error. Updating post with this information, but that makes it even more odd. 
Consider this: Rack | Set | Description :-|:-|:- 1|1|First Rack 1|2|First Rack 2|1|Second Rack 3|1|Third Rack Same description but mulitple values in the subquery. I'm pretty sure this is the problem. Distinct should solve it.
Yes, I guess I misunderstood your suggestion with &gt;. I thought you were assuming there were duplicate values on one of the tables. The cardinality between each tables primary keys are 1 to 1.
Are you sure there are no triggers on RackLife? Because there are no subqueries in that statement that would throw a subquery error.
You mentioned something about a LOOP... is this process inside of a loop?
If I understand you, this: HAVING(d.empno)&gt;4; Needs to be this: HAVING COUNT(d.empno)&gt;4;
You are a savior... That was it. How dumb of me. Wrapped the above code of yours in ALTER TABLE DISABLE/ENABLE TRIGGER ALL and it worked immediately. Thank you and sorry for the trouble, that was really not like me to overlook that.
You could say you were *triggered*?
I don't think there's a set "best practice" for every circumstance here and I think there's a lot of business logic and reasoning that will go behind your decision making. What I think you should do, is use a standardized template for handling transactions and also handling errors. https://www.codeproject.com/Tips/819509/Template-to-Handle-Error-in-SQL-Server For the majority of the time, I would probably put the if condition in the code block within the commit transaction section because if it errors or if something occurs, you are raising error and can log the information. You can still do that otherwise, but then you have more code to maintain.
~T R I G G E R E D~
&gt; SELECT d.deptno,d.dname, &gt; COUNT(e.empno) AS 'Dept Count' &gt; FROM emp e &gt; INNER JOIN dept d &gt; WHERE d.deptno = e.deptno &gt; GROUP BY d.deptno,d.dname &gt; HAVING(d.empno)&gt;4; I tried that already but for some reason it does not recognise the d.empno, perhaps out of scope?
I'm not really sure what you're asking for. Are you asking for help getting the count by month? I suggest looking at DATEPART maybe with a window function to get that number. Although comparing on a date field is generally avoided if possible for performance. Or are you asking for help getting it into the text box of an undisclosed CRM software? I'm relatively a newbie, too, with only about 10 months MS SQL experience but I'd be happy to help work through the first issue with you. The second one I can't help at all without more information.
im trying to get the count per month for active contracts based on a date range( usually per year). 
Without anything to work with I'd try something like 'DATEPART(MONTH, contractDate) as MonthNumber ,COUNT(contracts) OVER (PARTITION BY DATEPART(month, contractDate)) AS ContractMonthCount' certainly anyone with more experience is free to chime in, but let's start with that and see if it works for you. 
can i use the over clause in sql 2005? 
That I can't be sure about. It *looks* like it's possible to use a COUNT() window function in SQL Server 2005. I just did a quick google search - I don't have a ton of available time right now to dig into it. But this post mentions using window functions and has an example using count with SQL Server 2005 [redgate link](https://www.red-gate.com/simple-talk/sql/learn-sql-server/working-with-window-functions-in-sql-server/)
Yeah that's pretty generic and common. Going to need more info to help you for sure. Check triggers. Check string values that are being passed to int fields. 
Keep in mind that doing something like this: SELECT 'Test' + 3 Will return that exact same error... so maybe you're doing string concatenation but aren't explicitly casting the value that _could_ be an int as a varchar?
/u/Cal1gula has ALTERed my STATE of consciousness 
So you've run that procedure / function a hundred times with appropriately typed arguments and it worked, and now once with arguments that were not appropriately types, and it stopped working? Mmh. PostgreSQL as one example (can't speak to MS SQL-S) happily accepts `select '3' * 3;` because '3' (i.e. a string literal consisting solely of digits) may be interpreted without conflict as a representation of integer `3`. In Pg, strings literals are treated as 'underspecified' value representations that may undergo implicit casting—in case the string contents do not contradict that interpretation *and* the string has not been given another type already (neither `select '3.0' * 3` nor `select 'foo' * 3` or `select '3'::text * 3` work). So maybe you always called your procedure with strings that looked like integers and now a string turned up that does not consist of digits only (or has too many digits)?
If you can confirm that you’re not trying to convert anything in your SELECT, then I am 99% sure that the issue is in your WHERE clause or a JOIN. The issue here is that one of the values being used in the WHERE or JOIN is checking against a value you’ve probably Hard coded (or have as a variable). Example: WHERE LICENSE_PLATE &gt; 1000 will return a conversion error, because it will try to implicitly CONVERT the value in the LICENSE_PLATE field in order to see if it’s greater than 1000. If there are plates with letters in them, that will fail. Check your where clauses and confirm that you’re not doing any Implicit conversions. 
Create a copy of the procedure with a different name, put print '1' print '2' ect throughout the entire procedure, run that procedure (probably in a roll back if it modifies data). The print statements will help you tell where in the procedure it is failing so that you can narrow the issue down to one statement, then you can post that one select statement or will be able to tell what the issue is yourself. 
great plan - thanks. i was struggling to get to grips with breakpoints!
I have had this happen to me so many times in my joins. It is possible that one of the columns is saved as an int and the other a stting. It is trying to use those joined columns but can't since they ate two different types. This isn't a fix, but a workaround, cast the int as a varchar.
&gt; as i have run the stored procedure hundreds of times without this error. out of nowhere, it's throwing this error If you are 100% sure the procedure did not change, then the schema or the data changed and the procedure is poking it.
You're really going to have to post the queries for us to be able to say. It does sound like a locking issue, but just running simple select queries in series shouldn't cause that. And to be clear - the fourteen statements run fine the first time, and then the same fourteen don't run well when repeated? Or is it one particular step in the 14 that's a problem?
VBA because I literally hate myself.
I'm a huge fan of Jupyter/Python &amp; SQL - depending on what you're after, Jupyter can very easily be used for reporting with/without full code included: Some examples from [here](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks): http://nbviewer.jupyter.org/github/brianckeegan/Bechdel/blob/master/Bechdel_test.ipynb https://github.com/jhroy/theses/blob/master/theses.ipynb https://anaconda.org/jbednar/nyc_taxi/notebook I've generally done a lot of prototyping in Jupyter, and then turned it into something else afterwards.
You should try power bi desktop. You can link right to the sql db and creating stunning and insightful visualizations or aggregated tabular data very easily.
SSRS 2016 - now offering more features for Power BI than SharePoint. Also no SharePoint 😎.
SSAS
Thanks for the reply: I'm way from my desk right now, but to answer your questions: 1) What changes between runs is the input dates (as it creates the two subsets to compare based on a date field in the table) 2) If I had to guess, I'd say it fails/locks on the second lrepared statement (as sometimes after force quitting/timing out, the results of prep-statement 1 are in there for the dates used when it hung 3) The query is 14 versions of Insert into c ( Fields ) Select Fields from a where A.id in (Select Id from B where date = date2) And A.field &lt;&gt; b.field Where a different column is checked. (I know the above isn't correct - just off the top of my head. Essentially it has to check to see if the ID is in both subsets, and if it is, does [one of fourteen] fields match. If it doesn't, insert to results table. I'll try and get the real query up in a bit 
PowerBI has been surprisingly useful to me lately. Free to try for 60 days and $10 a month after that to be able to create/manage reports.
So I tried a few things. I commented out the second prepared statement. And it hung on the 3rd one (which became the second). When I refresh the list of connections, under info where it shows what query is being run, it stays on the query of the (new) second one until timeout. This is despite (I already know) the net result of the (new) second query to only Insert about 10 rows to the results table. Unfortunately it truncates the query in the info field - so though I know which one it is, I can't see the entire completed query. Here is the query. All 14 queries are the same, except for which field is being compared between the two subsets. INSERT INTO my_table.changes_items ( s_id, item_fk, my_date, my_time, field_changed, old_value, new_value, change_date, change_time ) (SELECT a.s_id, a.item_fk, func_ora_to_mysql_date(a.schedule_date), func_ora_to_mysql_time(a.start_time), 'field1' as field_changed, concat(b.field1_bool, b.field1_text_value) as old_value, concat(a.field1_bool, a.field1_text_value) as new_value, '2017-10-06' as change_date, '1830' as change_time FROM siev_all a LEFT JOIN siev_all b ON concat(a.s_id,a.item_fk) = concat(b.s_id, b.item_fk) AND b.siev_date = '2017-10-05' AND b.siev_time = '1830' WHERE a.siev_date = '2017-10-06' AND a.siev_time = '1830' AND concat(a.s_id, a.item_fk) IN (SELECT Concat(c.s_id, c.item_fk) FROM siev_all c WHERE c.siev_date = '2017-10-05' AND c.siev_time = '1830') AND concat(a.s_id, a.item_fk, a.field1_bool,a.field1_text_value) NOT IN (select concat(d.s_id, d.item_fk, d.field1_bool, d.field1_text_value) FROM siev_all d WHERE d.siev_date = '2017-10-05' AND d.siev_time = '1830') ); 
there's a problem in your partition by clause, it's pulling the maxdate for the full month and not by parentid: Old version: max(datetimestamp) over (partition by datediff(mm,0,datetimestamp)) as maxdate new version: max(datetimestamp) over (partition by ParentID, datediff(mm,0,datetimestamp)) as maxdate 
It has actually all gotten weirder. I broke down the procedure into 14 procedures and ran them one by one. The first one runs fine no matter what. Over and over it's fine. Testing at random, some work, and some don't - regardless of the 'order' they are run in. So I took out the 'SELECT' rows part of each procedure, and ran it to see if there's any bad performance, or syntax errors. *The select queries run just fine* - and some of them don't even return any rows - so it should just skip over them in a matter of seconds. But it just still doesn't account for why it seems to work sometimes and other times doesn;t. Any idea what else I should look at testing? 
SSRS/SQL is 99% of what I do daily at work and we will likely transition to Power BI or Tableau in the future. Personally, I use Tableau at home as the amount of work to build out my SQL Server database, SSIS packages and then build reporting was easily replaced by directly linking to the data with Power BI/Tableau. But to answer your question directly, SQL and Javascript with VBA here and there and maybe even a little C# will likely be plenty for what you need.
+1 for no SharePoint
Yep, all you'll need to do is add a count(*) and group by the column that identifies the work centers. For example: SELECT COUNT(*), work_center_id WHERE status = 'past due' GROUP BY work_center_id Best of luck!
Thank you for this. I am still having some issues with integrating it with my current code. I can post the actual code, minus the HAVING clause (contains company name information), if you think it will help. Unless you meant to have that run as a separate query?
You could run it as a separate query, but it's probably more convenient to have it all in one place. If you can post code, I'm happy to take a look.
 I kept in the HAVING clause, just edited some of the names, so you can see everything I am working with. SELECT Job.Top_Lvl_Job, Job.Job, Job.Customer, Job.Part_Number, Job.Description, Job_Operation.Work_Center, Sum(Job_Operation.Est_Total_Hrs) AS "Scheduled Hours", Sum(Job_Operation.Est_Total_Hrs-[Act_Run_Hrs]-[act_setup_hrs]-[rwk_setup_hrs]-[rwk_run_hrs]) AS "Remaining Hours", CONVERT(varchar,Job_Operation.Sched_End, 101) AS "Sched_End" FROM Job INNER JOIN Job_Operation ON Job.Job = Job_Operation.Job WHERE (Job_Operation.Status = 'O' OR Job_Operation.Status = 'S') AND Job_Operation.Sched_End &lt; GETDATE() GROUP BY Job.Top_Lvl_Job, Job.Job, Job.Customer, Job.Part_Number, Job.Description, Job_Operation.Work_Center, Job_Operation.Sched_End HAVING Job_Operation.Work_Center='ACME' OR Job_Operation.Work_Center='ACME CNC' OR Job_Operation.Work_Center='ACME DEPT' OR Job_Operation.Work_Center='CNTR GRIND' OR Job_Operation.Work_Center='SPEED LATH' OR Job_Operation.Work_Center='CNC LATHE' OR Job_Operation.Work_Center='CNC L-2AX' OR Job_Operation.Work_Center='CNC L-2XLT' OR Job_Operation.Work_Center='CNC L-3XLT' OR Job_Operation.Work_Center='CNC TW SPD' OR Job_Operation.Work_Center='CNCP L2T' OR Job_Operation.Work_Center='DEBURR' OR Job_Operation.Work_Center='TUMBLE' OR Job_Operation.Work_Center='VIBRATORY' OR Job_Operation.Work_Center='OBS_BENCH' OR Job_Operation.Work_Center='CNC H-4AX' OR Job_Operation.Work_Center='CNC MILL' OR Job_Operation.Work_Center='CNC M-3AX' OR Job_Operation.Work_Center='CNC M-4AX' OR Job_Operation.Work_Center='CNC M-5AX' OR Job_Operation.Work_Center='ASSY TOOLS' OR Job_Operation.Work_Center='CNCP M5' OR Job_Operation.Work_Center='EDM PROG' OR Job_Operation.Work_Center='PROG MILL' OR Job_Operation.Work_Center='PROG LATHE' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='Work Center' OR Job_Operation.Work_Center='CNCL' OR Job_Operation.Work_Center='CNCM32932' ORDER BY Job_Operation.Work_Center, Job_Operation.Sched_End Asc
all those HAVING conditions should be in the WHERE clause to improve FROM clause efficiency 
I am not sure of the performance between HAVING and WHERE, but HAVING is for aggregate functions. Since none of those values use an aggregation, I see no reason why you don't need to put them in HAVING instead of WHERE. But to answer your question, you simply need to add a new column to your select list. SELECT COUNT(*) as 'RecordCount', Job.Top_Lvl_Job.... 
Retooled a bit and added the count(*) for you. SELECT Job.Top_Lvl_Job, Job.Job, Job.Customer, Job.Part_Number, Job.Description, Job_Operation.Work_Center, Sum(Job_Operation.Est_Total_Hrs) AS "Scheduled Hours", Sum(Job_Operation.Est_Total_Hrs-[Act_Run_Hrs]-[act_setup_hrs]-[rwk_setup_hrs]-[rwk_run_hrs]) AS "Remaining Hours", CONVERT(varchar,Job_Operation.Sched_End, 101) AS "Sched_End", COUNT(*) AS "Past Due Jobs" --Assuming any row which meets the filter criteria is a past due job, a count(*) gives past due jobs FROM Job INNER JOIN Job_Operation ON Job.Job = Job_Operation.Job WHERE Job_Operation.Status IN ('O','S') AND Job_Operation.Sched_End &lt; GETDATE() GROUP BY Job.Top_Lvl_Job, Job.Job, Job.Customer, Job.Part_Number, Job.Description, Job_Operation.Work_Center, Job_Operation.Sched_End HAVING Job_Operation.Work_Center IN ( 'ACME' ,'ACME CNC' ,'ACME DEPT' ,'CNTR GRIND' ,'SPEED LATH' ,'CNC LATHE' ,'CNC L-2AX' ,'CNC L-2XLT' ,'CNC L-3XLT' ,'CNC TW SPD' ,'CNCP L2T' ,'DEBURR' ,'TUMBLE' ,'VIBRATORY' ,'OBS_BENCH' ,'CNC H-4AX' ,'CNC MILL' ,'CNC M-3AX' ,'CNC M-4AX' ,'CNC M-5AX' ,'ASSY TOOLS' ,'CNCP M5' ,'EDM PROG' ,'PROG MILL' ,'PROG LATHE' ,'Work Center' ,'CNCL' ,'CNCM32932' ) ORDER BY Job_Operation.Work_Center, Job_Operation.Sched_End Couple of notes: you can use IN instead of a list of ORs, it's the same thing with less code. Also, ASC is implied in ORDER BY so you don't have to specify it.
you forgot the GROUP BY too ;o) 
Thank you for the tip about IN.
This looks much cleaner. Thanks for the tip about IN and ASC. This still doesn't quite get me what I am after. It creates a new column, Past Due Jobs, and each entry is a "1." So for my example earlier, the CNC work area shows up 12 times, but the number 1 shows up 12 times also, instead of showing up once as the number 12.
For the counting I'd recommend trying a window function. I'm only about 10 months in - I'm pretty green myself, but learning window functions has been the biggest step up towards getting exactly what I want out of SQL. Try to start with something like COUNT(stores) OVER (PARTITION BY storeBrand, location) as LocationCount As far as the formatting goes - I don't play much with that personally - it doesn't feel appropriate to worry about formatting with SQL, at least to me. Hopefully somebody else can help you with that. Sorry if I misunderstood the question or if anyone more experienced proves me wrong. Good luck! 
Your comment made me laugh! That’s awesome though! I don’t know much about VBA, but can that be integrated into SQL? Just figured I would ask; I can always keep doing research on it. 
I saw this, but I think I have the module. I have two SQLite files and the command works on one but not the other. Is there any way of checking the separate files for rtree somehow?
You should be able to automate the 30 pulls..... what are you saving the data to?
To CSV files for now. I was thinking of somehow automating the ~30 queries, however I have no direct access to the database, all communication is done through the Citrix app wizard, and apparently Citrix apps are more like image viewers than actual Windows applications that can manipulated with libraries like pywinauto32, so I'm thinking any GUI automation program is going to be brittle and should be minimized. 
`DROP DATABASE [database_name];`
As long as there is consistent placement of the buttons and such, you should (might) be able to work up something with AutoHotKey. There are macro records that create AHK scripts that can give you a really good start. The only other thing I can think of is try to pull just the columns you need. 
That's one way to solve the problem. Unfortunately I can only do SELECT queries. 
Thanks, I'll check out AutoHotKey. Paring down the columns sounds like a good idea too. 
select Right(vendor_name, Length(vendor_name) - locate(' ', vendor_name)) as 'Second Word' from vendors;
that yields the same result I already have. also, i need to use the substring command for the assignment. not sure if that changes anything. ps. thank you for helping. 
Sorry for the poor SQL formatting I'm not good at making SQL look nice... but basically use an `OR` around both of the category statements and use multiple columns in the one for category3: SELECT * FROM Table1 WHERE ( category IN ('category1', 'category2') OR (category, type) IN ( ('category3', 'type1') ) )
Sounds like you need to push back and get a DW set up in your environment so you can do your job... Some sort of macro here seems like the worst possible solution to a problem, possibly ever.
Ok so I figured it out!! I ended up using sqlite in Terminal, loading the Photos.sqlite, and used the exact command and simply added ; to execute it. Terminal is a little nerve-wracking because it doesn't really show feedback, so I just saved the file, overwrote it on my phone (I backed up the existing one) restarted Photos and BOOM it worked! I'm so glad I found the guide!
https://www.safaribooksonline.com/library/view/hacking-and-securing/9781449325213/ch04s02.html this came in handy too
Trust me, I've been trying for months now with 0 support or sympathy. I'm looking for another job because this is ridiculous. 
Thank you! that was having me stumped for longer than I care to admit. 
 substring_index(substring_index(vendor_name, ' ', -2), ' ', 1) as 'Second Word' 
 SELECT CASE WHEN INSTR(vendor_name, ' ') = 0 THEN NULL ELSE SUBSTRING_INDEX(SUBSTRING_INDEX(vendor_name, ' ', 2), ' ', -1) END as 'Second Word' FROM vendors Will return the 2nd word and return NULL if the vendor only has 1 word...? If that's what you want... otherwise I think you'll be able to adapt what you want from it.
I hear you man. I've been there. Good luck, hopefully the macro thing works out for you. Sounds like your hands are tied, and they want you to get blood from a stone here.
&gt; Insert into c &gt; ( &gt; Fields &gt; ) &gt; Select Fields from a where &gt; A.date = date1 and &gt; A.id in (Select Id from B where date = date2) &gt; And &gt; A.field &amp;lt;&gt; b.field You might get a performance increase by losing the IN. If an ID is unique in B for each date, you could do a simple inner join: Select Fields from a INNER JOIN b on a.ID = b.ID WHERE A.date = date1 and b.date = date2 Of course this would mean duplicate rows if ID exists in b more than once for each date, which may or may not be fixable by making it SELECT DISTINCT fields instead. If that's not an applicable solution then it might be worth testing with EXISTS instead of IN: Select Fields from a INNER JOIN b on a.ID = b.ID WHERE A.date = date1 and EXISTS (select 1 from B where b.id = a.id and b.date=date2) This might help but I don't think this is the root cause of your weirdness. 
&gt;If I run the query with the dates 2017-10-04 and 2017-10-05, it runs in seconds. &gt;If I run it for 2017-10-05 and 2017-10-06 - the whole thing hangs - and I'm just talking about doing a Select on the table The two things I can think of are rebuilding any indexes that those date fields are in, and if this is running within a stored procedure do it WITH RECOMPILE.
/r/samspopguy , are you doing this from within CRM or from Report Builder? Also are you using SQL reports or FetchXML? Few ways to do it, just need more details on the tools you're attempting to use. Also is your CRM environment On Prem or Online? This is assuming 'CRM' is 'Microsoft Dynamic CRM' :)
That might be worth a go. I deconstructed everything down to just the select statement - it seems like there might be bad rows somewhere in 2016-10-06. The simpler comparison queries - where not in , on the subsets, work fine (15,000 rows selected in &lt;1sec). The moment it becomes one of the queries above, selects on that date never return. Think something may go wrong when the data is first inserted. Corrupted somehow. Thinking about it - when it fails overnight, i just delete what it tried to do, and re-run the scripts (including the original fetch of the data) and the procedure, it all works fine Hmmmmmm
Yeah it’s Microsoft Crm 4.1 but I think I got it I was trying to do it in one query and I needed to Union 12 together for each month 
Do you mean you want to SELECT all the data? If so, generate a dynamic sql statement by looping through USER_TABLES and/or USER_TAB_COLUMNS. 
Yeah that's what I was doing initially. I had a python script that generated a SQL query based on the TABLES and COLUMNS tables. Though there seems to be a hard limit on the number of columns I can query at once so that's the limiting factor at the moment. 
Thank you sooo much! Still getting my head around ms-sql.
It's been a (very long) while since I had to write anything for Oracle, but wouldn't an UNION ALL of all tables work? You'd just need a common list of columns for them. So given you have tables A(a, b, c, d, e, f) and B(x, y, z), you'd go with: SELECT 'A' AS table_name , CAST(a AS TEXT) AS c1 , CAST(b AS TEXT) AS c2 , CAST(c AS TEXT) AS c3 , CAST(d AS TEXT) AS c4 , CAST(e AS TEXT) AS c5 , CAST(f AS TEXT) AS c6 FROM A UNION ALL SELECT 'B' AS table_name , CAST(x AS TEXT) AS c1 , CAST(y AS TEXT) AS c2 , CAST(z AS TEXT) AS c3 , CAST(NULL AS TEXT) AS c4 , CAST(NULL AS TEXT) AS c5 , CAST(NULL AS TEXT) AS c6 FROM B This way you know which data is coming from which table (the table_name column) and all the columns are selected for each table. Then you'd of course need to export this and use mapping between column numbers (c1, c2, c3) and real column names (e.g. for B = x, y, z), get rid of the remaining unused columns (none for A and c4, c5, c6 from B) and some such but that's 15 minutes of work in Excel. Of course you're probably not gonna write the 30 select queries by hand, especially if one table has 50 columns while the other have 3. As I said, it's been a while since I did anything with Oracle, but here's a quick and dirty PL/pgSQL script that generates the necessary queries for you: with columns as ( select max(ordinal_position) as max_count from information_schema.columns where table_schema = 'dbo' ), indexer as ( select n from columns join lateral generate_series(1, columns.max_count) series(n) on true ), cols_by_tabs as ( select tabs.table_schema, tabs.table_name, indexer.n, cols.column_name from indexer cross join information_schema.tables tabs left outer join information_schema.columns cols on cols.ordinal_position = indexer.n and cols.table_name = tabs.table_name where tabs.table_schema = 'dbo' order by tabs.table_schema, tabs.table_name, indexer.n ), tabs_with_select_list as ( select table_schema , table_name , string_agg(format('cast(%s as text) as col%s', case when column_name is null then 'null' else format('%I', column_name) end, n), ', ') as column_list from cols_by_tabs group by table_name, table_schema ) select format('select ''%s.%s'' as table, %s from %1$I.%2$I union all', table_schema, table_name, column_list) as your_select_query from tabs_with_select_list I'm sure it's not bulletproof, and Oracle afair does not support the motherfucking ANSI standard information_schema, but there are meta tables or views or whatever for sure that'd get you the necessary data. Of course you'd also need to rewrite the functions and shit, I don't even have an Oracle instance handy to do it, sorry.
This is great, thanks a lot, I'm going to try it out tomorrow. 
 WHERE TO_CHAR((yourdatefield),'fmD') = 7
1) Besides free training online (ie w3schools), I'd say to take 20761 (querying). if you company has any, you can use microsoft vouchers on this class. https://www.microsoft.com/en-us/learning/course.aspx?cid=20761 2) you might need to get more specific for us on what you need, but I'd say 1st get SSMS installed (or equivalent for your company). and make sure you have proper access/login credentials for that databases that you want to use. i honestly feel like SQL is pretty easy for people to pickup (especially if you have some understanding of data), but can of course be hard to master. basic querying skills can get you pretty far. 
Some coworkers use qlik. I believe it's around $20 a month, they may have a trial or student version to check out.
Yeah I feel Like I will be able to pick it up pretty easily as I have even studied it a bit before. Since the company was offering to pay for additional training I just thought it would be nice to utilize it. Really appreciate your advice thank you.
&gt; Yeah it’s Microsoft Crm 4.1 Oh you are way behind, do you all have plans to upgrade? Just remember you have to do step upgrades, you can't jump directly from CRM 4 to 365 without upgrading to 2011, 2013, 2015, and 2016 in between. Maybe if it's a small installation you could migrate your data,that's an option too. Anyway off topic, but CRM is kinda my thing right now :) 
I would like to but there is no plans right now
1. Microsoft SQL Server because it's Microsoft and easy to find resources. 2. Access because it's Microsoft. 3. I run daily select and group by queries to look at distributions of data in tables over time and work with other members of the business to understand outliers and uncover errors in process which result in bad data. I weekly run larger aggregate processes which took much longer to write, and which allow me to do my daily work. I infrequently deploy large scale changes to production code which either changes existing data sources, or creates new ones for the business to consume. 4. Statistics to understand data, and Python to scrape it.
Fair enough. Tag me back if you guys ever do decide to move forward. I'm no consultant or anything, just I've been around the block a few times with CRM and maybe I can give some pointers. I live in it about 6 hours a day and started on CRM 4.
Subquery??
Sorry can you elaborate? I'm a total SQL noob and got thrown into this project.
Hard to get it exactly without seeing the table, but I'm thinking something like this: Select t2.id, table.* From table Left join (select distinct table2.id from table table2) t2 On t2.id = table.id I don't really understand why you'd need a distinct column of ids, is that the primary key column on the table?
In this example TABLE_NAME is the same table, just with different aliasing . WITH CTE1 AS ( SELECT DISTINCT COLUMN1 FROM TABLE_NAME T1, ) SELECT C1.COLUMN1, T1* FROM CTE1 C1, TABLE_NAME T2
I need all columns, but distinct on id because for some reason the query is returning anywhere from 2-10 rows of the exact same row results per same id and I only care about 1 row per id. There is no primary key on this table, it's generated on the fly and held in memory...
Slight differences. 1) IMDB.com it's interesting to see the movies stars worked on early in their careers. 2) access is just a pain, syntax is different 3) at current positions, people mostly want globs of data. They do analysis in other tools, they want huge sums like all sales in 2016 include payment types and dates. daily tasks are make sure all scheduled jobs ran, fix any that didn't monthly is same thing many jobs run on the first of each month, they often get in the way of each other. (is frustrating as front end application &amp; production DB are terribly designed and it's what we're stuck with) 4) statistics are helpful. Mostly employers want competent adults (no drama). At your first job learn SSIS and SSRS or an equivalent. Those are the in demand skills and I don't think you can learn those at University.
Yah, agreed. Few jobs ago, my boss said "Hey, see if you can get this working". I think it took all of ten minutes to get a dashboard up and running on her phone. Very easy to use. 
Does your results set contain duplicates or does the table contain different data in those 2-10 rows? If the data is duplicated, a distinct in your select will give you every unique result.
result set contains duplicates.
Throw a distinct in the select clause and let us know what happens.
Edited reply. See above.
Ah, one of your columns is a clob datatype. You may want to try and convert it. You could try a to_char function on that column. Clobs can be tricky due to their size. Here is a link about the to_char function: https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions179.htm Good luck figuring it out!
Thanks. Appreciate it.
Hi, this means that there is something distinct about the combination of the values in the GROUP BY (i.e Top_lvl_job, Job, customer, part_number, description, work_center, sched_end). You can solve for this in one of two ways, I think. The most straightforward is to do a separate query for the aggregation and then JOIN to it ON work_center. The other way (overkill) is to use a WINDOW function. 
Interesting. Thanks for the documentation and articles! I’m really going to have to dig into this pretty deep. Are a lot of organizations using Jupyter/Python for these types of things these days? I just wanted to get a feel for what’s in the market because it seems to be changing so much frequently.
That’s awesome. I have subscribed to SQL Server Central and have received some emails regarding Power BI Desktop so I’ll have to give it a whirl. How do you like using it?
It's great! Once you have it set up it is very easy to create some great looking interactive reports or matter for power point slides. I use it quite frequently for data exploration at the moment. Use a query to connect and adjust your data before loading it. You will use power query formulas (https://msdn.microsoft.com/en-us/library/mt779182.aspx) in this step for more complex data transformation if needed. Then after the data is loaded insert measures to create dynamic totals, or insert calculated fields to perform calculations on the rows (look into DAX formulas for this). Those two things were the hardest things to wrap my head around at first, but once you get the hang of it you can do some cool things fairly quickly. A feature I really enjoyed is the ability to import a folder of files. If they are similarly formatted it makes it insanely easy to view them all together into on report.
If you just want to remove duplicates you can do a select *, and add this to the end of your query: QUALIFY ROW_NUMBER() OVER( PARTITION BY [the ID column name] ORDER BY [any variable(s) you would like to decide which row is selected, if you dont care which row it selects just put any column here] ) = 1 This will make it only select one row per [ID] and you can decide which row by defining the order by clause. Glhf
I've used some exclusive postgres features, like generate_series, format, string_agg, etc. If you're skilled with Excel (or any other spreadsheet software), it might be easier for you to just query Oracle's `ALL_TAB_COLUMNS` table and create the queries in Excel instead of trying to translate my query to Oracle dialect. 
Assuming you can query the metadata - for example in SQL server sys view. I can't recall the Oracle equivalent off the top of my head. Use another language and the metadata to construct the required queries. Also read this first http://www.sommarskog.se/dynamic_sql.html
This is the exact solution I was going to suggest.
You need to *insert* or *update* your tax code table? Are they unique records or do they have some sort of identifier like year to distinguish them? Can you give us a bit more on the table structure? Like the columns you need. Maybe a few rows of sample data would help too.
Hi, thank you for taking your time to answer. If I do select ID from companies where NeedsData = 1 it returns a list of all the companies that need this data: 001 002 003 etc. Now I need to iterate this list and for each list I must insert 4 new records ID (unique), Code (static), Description(static), CompanyID (this must be the ID of the company record I am iterating) and in this table is already for 1 company the data correct so for example: 1, 0, cash, 001 2, 1, card, 001 3, 2, banktransfer, 001 4, 3, prepaid, 001 Basicly these records here need to be repeating for every company in my first query, and replacing the 001 with the company ID's from my first query. So for company 2 I also need 5, 0, cash, 002 6, 1, card, 002 7, 2, banktransfer, 002 8, 3, prepaid, 002 and company 3 9, 0, cash, 003 10, 1, card, 003 11, 2, banktransfer, 003 12, 3, prepaid, 003 etc.
Where is the data coming from that you need to update? Are those id fields that relate to another table? And do you mean 4 records for each company or just 1 record with 4 different types. The table structure is not clear from this post.
I have a table: PaymentConditions There is 4 Columns: ID, Code, Name, CompanyID There is 4 Records already present. 1, 0, cash, 001 2, 1, card, 001 3, 2, banktransfer, 001 4, 3, prepaid, 001 These are the 4 available payment conditions for company 001. Now I need to repeat these 4 payment conditions for every company in the table companies. The data just needs to be copied exactly as is, as many times as I have companies in that company table. Except for the column CompanyID, this must be ID value of the other companies in the Company Table. So I take the 4 paymentconditions, and re-insert them using company ID 002. Then I repeat it again for company 003, and again for company 004. This will take me days of work, so I am looking to automate this. Re-insert 4 records, with a new company ID.
Here's what you need: INSERT INTO PaymentConditions (Code, Description, CompanyID) SELECT PC.Code, PC.Description, C.CompanyID FROM PaymentConditions PC CROSS JOIN Company C WHERE PC.CompanyID = '001' AND C.CompanyID != '001' -- or maybe C.NeedsData = 1, not sure how it's laid out At least judging by your other comments.
1. MySQL. Free and powerful for its size and price 2. Access. MASSIVE locking and transaction issues. Had a database only 20,000 rows and 20 columns, and constantly corrupted. Also fails under the strain of multiple writes/reads from different users. 3. My DB's support Applications mostly - so if something goes awry, I'm doing lots of selects and joins to find records where there are errors and massaging them. 4. Get really good at doing things without a front end (Command Line)
appreciate your suggestion, it helps! I'll need to tinker with the formatting of course but I wasn't aware of the 'partition', thanks again!
Look into SSRS and Powerbi
A web-based reporting system that calls SQL Server stored procedures eh? 
Thanks! I got it working with this! Gr8 m8 8/8
Tableau is very good, and not cheap
I think the best way would be use CHARINDEX, like this: WHERE 1 = 1 AND ( CHARINDEX('111', t.ID) &gt; 0 OR CHARINDEX('222', t.ID) &gt; 0 OR CHARINDEX('333', t.ID) &gt; 0 OR CHARINDEX('444', t.ID) &gt; 0 OR CHARINDEX('555', t.ID) &gt; 0 OR CHARINDEX('666', t.ID) &gt; 0 OR CHARINDEX('777', t.ID) &gt; 0 OR CHARINDEX('888', t.ID) &gt; 0 OR CHARINDEX('999', t.ID) &gt; 0 OR CHARINDEX('000', t.ID) &gt; 0 ) If this extends past strictly numeric characters, it can be used with additional OR statements but it could be written differently to be a bit more elegant.
I was hoping to find something that doesn't require me to define all possible combinations in their own statements, but was more generic instead, since I don't know what combinations of characters may have been used - most likely numbers, but could also be letters and special characters/symbols. Functionally, what's the difference between CHARINDEX and using wildcards?
Providing details may help you get the right help you’re looking for. 
I edited the post for something that should be more helpful for you - take a look. Looks like I'm misremembering regarding CHARINDEX; I thought it was faster than LIKE with bookended wildcards, which does not seem to be the case. Here's a quick forum response on it: https://dba.stackexchange.com/questions/46917/like-uses-index-charindex-does-not.
dammit, wrong sub
Thank you!
Looks like it doesn't accept UNION ALL in the query :/
Create a subquery with the average price, and use the outer query to compare against that
What logic is behind disallowing the customer type to have item type?
It's not exactly what the scenario is, but don't want to post real tables online :) Basically, different customers have bought packs. Those different packs only allow them to potentially receive certain items (geo-locked). 
It's sounds very complex tbh. I would look into a cheap solution like crystal reports? You could then build a small reporting app where the user can choose the report and input parameters. 
I would make a "pack" table, where you have item, item type and pack. Then customer table can have customers assigned to one or more packs. Items and types allowed are already in pack table. Hope that makes sense. 
There are some incredible online courses for developing skills towards a data scientist position. Specifically some courses will actually ear you a college degree for a fraction of the cost. There is a great article here about the differences between and data scientist and a data analyst as well as some great suggestions to courses! Check it out: http://jobsinthefuture.com/index.php/2017/10/16/data-scientist-vs-data-analytics-what-is-the-big-data-difference/ I agree, on the job is best, but you need to be able to land the job first! ;) Cheers!
There are going to be thousands and thousands of items. Like 20,000
Easier to just put 16 Boolean values on the items table? Problem is to do that, I'd have to hard code a function at insert to say what packs it's allowed on - and as I said, the packs will grow/change. 
I gotcha. Yeah that's tough. What kind of system is it? ERP, CRM maybe? 
Sort of. It's a scheduling system. 1 item could be scheduled in multiple places
I've read a bit on Powerbi and I may go for it. Thank you. 
Gotcha. Yeah, without the logic of why you disallow a customer an item, there's going to be manual work. My idea would, sorry to repeat, pack table with pack types. Item table has a column for every pack type and it's Boolean. Assign customers to pack types. When a row is inserted do a check on the pack type, rather than the item. Assuming it's less likely a customer is disallowed a pack type you could even reverse that logic and on the items table specify disallowed packs types, rather than allowed. But from them on you're only comparing customer to pack type, which is less of a headache than an individual item. 
Others wise, if you know the logic of why, and it already exists in the database, I would use that as an if statement in an "instead of insert" trigger. 
Well... Each pack gets a number of sub packs, and each subpack has a number of item_types. Might be able to break it down to that level. Smaller lookup table But that's what we're essentially saying, right? Look up table?
Yep exactly 
https://twitter.com/sqlbob/status/456445106300530688?lang=en
It’s great. You can set it up in about a day and get reports/dashboards going. And there’s a 60 day total for the Pro license.
Your query's AverageStandardPrice is finding the average standard price of all the products within a given Product_Line_ID and Standard_Price set (the Group By). This should return no results because you are grouping Product Line and Standard Price together. The question has an ambiguous definition of average standard price. Is the average standard price determined by product line, or by all the products. From the bullet point, it reads like you want to find the Products Lines which have a a product with an standard price that is less than the average standard price of all the products. To do this I would: SELECT Product_Line_Name FROM PRODUCT_LINE_T WHERE Product_Line_ID IN (SELECT DISTINCT Product_Line_ID FROM PRODUCT_T WHERE Standard_Price &lt; (SELECT AVG(Standard_Price) FROM PRODUCT_T))
no problem...
Necessarily? I think as long as you don't only use access, that needn't be the case. And the benefit to using it as part of a balanced sql breakfast is the speed with which you can put together queries, and you can make changes and see the results quickly.
- What is your favorite database and why? Microsoft SQL Server. Particularly 2012 or 2014, 2008 R2 is a hassle to support and I get annoyed with clients who still have it. The Microsoft stack integrates so well with so many products. My favorite free data set is probably the free CIA geospatial list of cities with nation and lat/long. - What is your least favorite database and why? Splunk. It's closed source, and regex is a pain to write. - What sort of tasks do you do daily, weekly, infrequently? Daily: performance tuning stored procs and queries in SSRS reports, grant user access requests, troubleshoot failed jobs. Weekly: work on SSIS packages and automation, other project related tasks like object design. Infrequently: fix broken critical path jobs in the middle of the night (ugh), work on big initiatives like a new Greenfield project. - What do you recommend I study/learn/practice to prepare for a position working with databases? Study data design and data relationships. Study syntax. Learn how to read the query analyzer and what it means. Honestly, fart around with Excel first. Get good at doing clever things in Excel that automate tasks. This gets you comfortable with the concept of automation and gets you thinking about working hard today so you can be lazy tomorrow.
Access is actually pretty good RAD tool if the dev is willing to put up with VBA. But when learning SQL from scratch it isn't so great.
Interesting, I'm working a data migration project right now for a rather large health organization but use Microsoft tools. It'll be interesting how this compares.
Me "Wow this looks really promising I'd love to try this out!" *Opens page* *Oracle only* FeelsBadMan
They do not have to be in order and it doesn't have to include all of the columns. Any column you don't include in the insert statement will be set to NULL. As long as you're okay with that you're good to go. Also the columns don't have to be in the same order as the order that the table was declared as - just so long as the columns match up with the order of the values that you provide you're good to go. Or would be good to go except that I imagine what you're doing is *illegal*!
I tried that. Is it bad if autocompleted to pornhub? 
&gt; Any column you don't include in the insert statement will be set to NULL. As long as you're okay with that. just a slight correction... they will be set to their individual column default values
Ah yes, thanks. Important distinct. Sorry to lead you astray OP.
I went to post the same thing a few minutes ago but deleted it...
I still haven't decided if it was a wise decision. No, not wise for life but wise for a good joke. 
You can use Liquibase to other databases.
I think what you're saying is "You think you do, but you don't".
As long as it's not at work...
Try to add this to the proc. IF LEFT(@parameter,1) != '''' AND RIGHT(@parameter,1) != '''' BEGIN SET @parameter = '''' + @parameter + '''' END
EDIT: single quotes without the spaces. Typed that on mobile. 
&gt; IF LEFT(@parameter,1) != '''' AND RIGHT(@parameter,1) != '''' BEGIN SET @parameter = '''' + @parameter + '''' END Did not work. These are the results. /*------------------------ exec Util.PrintSqlDef IF LEFT(@parameter,1) != '''' AND RIGHT(@parameter,1) != '''' BEGIN SET @parameter = '''' + @parameter + '''' END Util.LogStagingErrorsForEntity ------------------------*/ Msg 137, Level 15, State 2, Line 1 Must declare the scalar variable "@parameter". Msg 137, Level 15, State 2, Line 1 Must declare the scalar variable "@parameter". Msg 102, Level 15, State 1, Line 1 Incorrect syntax near 'Util'. 
You have to replace @parameter with your actual parameter. 
Also, just to clarify, you'll need to add the code to the procedure itself and not just past it into the EXEC statement when you run it. 
This is a framework proc that I am not allowed to modify. Any way to get SSMS to do this?
https://docs.microsoft.com/en-us/sql/t-sql/statements/sql-server-collation-name-transact-sql Use the COLLATE function. Probably with this collation: SQL_Latin1_General_CP1_CI_AS * latin1 makes the server treat strings using charset latin 1, basically ascii * CP1 stands for Code Page 1252 * CI case insensitive comparisons so 'ABC' would equal 'abc' * AS accent sensitive, so 'ü' does not equal 'u' Use it like this: SELECT YourColumns FROM YourTable ORDER BY AColumnYouAreHavingSortIssuesWith COLLATE SQL_Latin1_General_CP1_CI_AS ASC
The terminology you are using here doesn't really make sense. What does it mean that a "table has several thousand lines of code"? Tables don't really have lines of code. They have rows of data. Or a stored procedure could have lines of code. Is it an insert statement and you are trying to remove something from a list of VALUES? Is it a list of columns in a SELECT statement? An example of the code here would surely help. Otherwise the only thing I can suggest is find&amp;replace or a macro.
I'm not sure that I understand the question? Also - MS SQL? You mean you're trying to execute a script that will insert into a table? And your insert script is inserting 23 columns into a table that has 20 columns? If that's all true and you have permissions, the *easiest* solution would be to add those 3 columns to the table, run your insert script, then drop the 3 extra columns. This doesn't feel very clean (or safe) though. I wouldn't doubt that somebody with more experience might have a solid solution for you, but it sounds to me like your safest and best option would be to go through the script and delete the columns. You might be able to do some catered search/replace runs to clear them out, I don't know. Sorry, and good luck.
The main ERP I use switched to case sensitive on the last major update so I've dealt with this a little. The other response says to specify the collation, this will help you join between different collations. But you want to search. The easiest way to search a case sensitive column without specifying the correct case is to just force everything to upper case. I typically do something like: Where UPPER(ColumnA) = 'SEARCHSTRING' 
Yes, put single quotes before and after the text that you highlight. 
this is considered the correct solution for pretty much all instances of this issue in computer science. though most people convert to lower... converting to upper will get you some weird looks, but it'll work just as well.
another option would be to export to a local copy of SQL set to be case insensitive instead of a local copy of Excel. copies of SQL server are free and can be installed on your desktop.
hmm, it seems to not be returning anything. no errors anything like that, just empty results. i'm doing something like SELECT * from table Where column1 &gt;= 'value' and upper(column2) = 'patel';
If SEARCHSTRING is a variable being supplied by a user, wrap it in Upper also. In your code you must use all Uppercase also or wrap the string in Upper()
The ERP switched to all uppercase, that's why I use Upper. And let's be honest, sometimes it's good to be contrary to the mainstream! 
i'll have to check and see if SQL server is on our approved list. I know we can request MS Access for free. would exporting to an Access data base be a suitable alternative? Dubs seems to have solved my problem but you suggestion might be a good idea for future reference. 
this is a bit over my limited knowledge unfortunately, but i'll definitely look into learning more about COLLATE as I expand my skills. I tried putting it at various parts in my query but DBMS (Toad) wasn't picking it up as a condition. I was trying something like select * from MyTable where Column1 &gt;= 'date' and Column2 = 'searchstring' COLLATE SQL_Latin1_General_CP1_CI_AS ASC 
i'm not sure I follow, you're saying wrap all the conditions upper like so? I don't think I've seen variations in case for Column 1 since that's a date column, but definitely couldn't hurt! SELECT * FROM Table1 WHERE UPPER(Column1) &gt;= UPPER('STRING') AND UPPER(Column2) = UPPER('STRING2'); 
No, my code typically accepts parameters supplied by a user. If I'm searching a case sensitive field with a parameter it looks like this: Where UPPER(Column1) like UPPER('%'+@SearchString+'%')
ohh ok. i'm just searching a table and specifying my own parameters. then I export the data to excel for analysis. sorry if that was unclear. conceptually, does UPPER(Column1) essentially force SQL to "view" all of the entries in that column as uppercase for the purposes of the query? I noticed it was returning results in the original case. Just wanted to make sure I understand the logic. 
yeah, that could end up solving a bunch of issues for you down the line. but then you don't learn cool tricks like 'string invariants'
Yes, the where clause won't change the way a column is returned in the result set. You would wrap the column in the Select statement in Upper to change the result set. So: Select Upper(column1) as Upper_Column1 From table Where Upper(Column1) = 'PATEL' You can use like and wild card characters to improve your search. Where Upper(Column1) like 'PAT%' Would return Patel and paTriot and PATRice. There are a number of wildcard characters, Google your sql server for specifics. (In ms sql: Upper(Column1) like 'PAT____' would return Patriot and Patrice, but not Patel or Patent. 
Yes, the where clause won't change the way a column is returned in the result set. You would wrap the column in the Select statement in Upper to change the result set. So: Select Upper(column1) as Upper_Column1 From table Where Upper(Column1) = 'PATEL' You can use like and wild card characters to improve your search. Where Upper(Column1) like 'PAT%' Would return Patel and paTriot and PATRice. There are a number of wildcard characters, Google your sql server for specifics. (In ms sql: Upper(Column1) like 'PAT____' would return Patriot and Patrice, but not Patel or Patent. 
The union all should work 100% If you run that query (without bcp)you should see the first row as the header. Maybe you need and order by?
Double check your code, you have redundant string searches (definitely 'work center' possibly others) If there are no other CNC type text values, use OR Job_Operation.Work_Center LIKE 'CNC%' this will reduce scans and improve efficiency
http://lmgtfy.com/?q=mysql+get+amount+of+items+in+column
Count(column_name)
I'm not familiar with BETWEEN being used for VARCHAR. Is it possible that since there is no movie by the name of just "J" that's why you're not getting results? Because BETWEEN with numbers is inclusive to both your first and second criteria, I would really expect it to be the same for VARCHAR, but maybe you need to look at the movie titles a little differently.
May also require a GROUP BY (Column_name) after the WHERE and any AND/OR statements.
My assumption with varchar is that "J" and "Ja" are distinct values. So even though between is inclusive. "Ja" is like 1.1, so it is greater than "J". So values between 0 and 1 do not include "Ja". You can do some like: left([column title], 1) between 'A' and 'J'. 
Luckily, the courseware allows you to play around. I shifted the "J" variable to "K", to which my query returned movies starting with the letter J, but did not return any movies starting with "K". It just seems odd to me, since the variable of "2000" would return anything within the year of 2000, but not within the letter J.
That makes a bit more since. I'd imagine the inclusion of specific days would be a nightmare when working with integers. If subsequent letters matter, perhaps "J%" would work? I'll give it a shot.
I would expect BETWEEN 1900 and 2000 to leave out 2001, so I would be curious to know which way you're going with your BETWEEN.
I'd be interested to see how wildcard is inclusive. I'm not sure it will work. 
So, between is inclusive, so all values where year = 2000 would be included. I think the issue here is that the year column is an integer. So any values greater than the string J are not included. Same and 2000.1.
It is just how it is because that's how you sort strings. You either need to use a function to pull the first character of the string to compare against your BETWEEN clause or use two comparison operations.
Is there any benefit of this over LIKE ‘[A-J]%’
https://stackoverflow.com/questions/41100232/subtypes-inner-entities-in-oracle-sql-developer-data-modeler
Thanks for the input everyone, but I've come to terms with this particular scenario as it stands. The scope of my SQL knowledge is limited to a couple broad summaries of the language, and two course modules with CodeAcademy. I look forward to learning about all the additional SQL inputs recommended here, but for now, my knowledge is limited. Thanks again!
When you understand the terminology a bit better, you’ll read your question and laugh. As Cal1gula says, a table definition and your SQL would help. 
&gt; how do you choose to use a view over a join or a subquery? how do you choose an apple over an orange? views, joins, and subqueries are different, and so have different usages
&gt; The course states that... &gt; WHERE name BETWEEN 'A' AND 'J'; &gt; This statement filters the result set to only include movies with names that begin with letters "A" up to but not including "J" if that's what it says, then the course is flat out wrong it should say "up to **and including** J" however, it's unlikely that there is a movie with the name "J" so...
first, the GROUP BY clause does not take parentheses second, if you `COUNT(column_name)` but also `GROUP BY column_name`, you're going to get weird results
Just beware that using a function in your `WHERE` clause renders that portion of it non-SARGable and you may take a performance hit.
Toad isn't a DBMS, it's a GUI for interacting with your DBMS. Are you using Oracle? SQL Server? Postgres? MySQL?
&gt; copies of SQL server are free No, Developer Edition and Express Edition are free, but the former only for non-production usage. And this ignores any corporate policies about installing software on one's desktop.
If you're using Toad, I suspect you may be on Oracle. If so and you're on 10gR2 or newer, [check out this Stack Overflow article](https://stackoverflow.com/questions/5391069/case-insensitive-searching-in-oracle)
That's the point of the post: how do you know which is the right one just by reading your task? 
I already know and did that, I want to know how to do the disjoint and overlap, etc... as I mentioned above.
If you Google er database diagram you can get a good idea of how one looks. They probably want all columns per table and their attributes. Something like this: http://www.orafaq.com/wiki/images/thumb/8/8c/ER_Diagram.jpg/400px-ER_Diagram.jpg
By understanding what they are and what they do. Use a view if you have a complex query to make a virtual table for use in many different applications. A view definition will persist in the database until you drop it. Use a join to bring data from two tables together. Use a subquery if you need to alter the granularity (e.g-use aggregations like sum or max) of a table before it can be joined to another. Or, if you’re in an exists clause. These are super general and basic, but that’s on purpose.
This is probably the easiest way to explain it. http://sqlfiddle.com/#!9/00c68b/
Yes. He's not using it for production, so it's free. Also, if your corporate policy keeps you from installing free versions of Microsoft flagships, I'm not entirely sure what that policy's use is... If I'm 'ignoring' weird policies, that's one you can cherry pick, but there are literally an infinite amount of even dumber ones that I'm also ignoring so I'm okay with that.
&gt; if your corporate policy keeps you from installing free versions of Microsoft flagships, I'm not entirely sure what that policy's use is The policy's purpose is to make sure the company doesn't run afoul of software licensing (heavy fines), software compatibility is tested, corporate governance rules are followed, and security is maintained. In "a major corporation with tons of bureaucracy and redundancy", you don't just install things on your desktop. In all likelihood, you don't even have the required access on the PC to do so. And even if you do, when a network scan starts turning up rogue installations of SQL Server on peoples' desktops, it raises flags. &gt;Working for someone who actively keeps you from doing the job they've asked of you is a bigger problem If having a local installation of SQL Server was *required* to do OP's job, this argument would have some weight. But it's not. OP is considering it as a way to make life easier. Not having it is **not** keeping him from doing his job.
you forgot the GROUP BY clause, so MySQL defaults to a single group for the entire result set, hence only 1 result 
 SELECT c.IndustryType, AVG(o.Amount) AS 'Average Amount', MIN(o.Amount) AS 'Minimum Amount', MAX(o.Amount) AS 'Maximum Amount' FROM customer c INNER JOIN orders o on c.custid = o.custid group by c.IndustryType, ; Above is in MSSQL where you need to group by all non-functioned displayed columns. If you leave in all the other columns then you will need to group by them also, however this will probably increase your returned result set dramatically.
Equal to or * than makes much more sense, thank you.
Ah, that works perfectly!
How many orders were placed by customers in each state? Use the join facility to answer this question; do not use subqueries. and What is the total quantity of the products with a natural ash finish ordered by customers in each state? Only include states with a total quantity of at least 3. Use the join facility to answer this question; do not use subqueries.
you're describing an actual sql standard, but as you may know [MySQL went beyond it](https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html) this is why OP's query actually ran, instead of giving an error
Lol. Are you seriously asking Reddit to do your homework? 
Yeah, it would be one thing to ask, "here's what I was doing, is this on track?" But directly posting the question and nothing else...come on..
I did this SELECT c.Custid, c.Cname, c.City, c.IndustryType, o.OrderNo, o.OrderDate, o.SalesPersonID, o.Amount, AVG(o.Amount) AS 'Average Amount', MIN(o.Amount) AS 'Minimum Amount', MAX(o.Amount) AS 'Maximum Amount' FROM customer c INNER JOIN orders o ON c.custid = o.custid GROUP BY c.IndustryType,o.orderdate;
Ahh cool I didn't know that, I work solely in MS and am very out of the loop on other variants.
I think I know what the issue is. If I run the query outside of the job, I get an error that says "Conversion failed when converting from a character string to uniqueidentifier" Maybe it can't output the names of the columns because of the character type? 
Try to CAST all the fields as string on both sides of the union and see if that works. I don't think you need them to be typed as uniqueidentifier for a CSV export (they are going to end up as text anyway).
I did the following, but it does not calculate the AVG, MIN and MAX correctly, for some reason it just returns the amount SELECT c.Custid, c.Cname, c.City, c.IndustryType, o.OrderNo, o.OrderDate, o.SalesPersonID, o.Amount, AVG(o.Amount) AS 'Average Amount', MIN(o.Amount) AS 'Minimum Amount', MAX(o.Amount) AS 'Maximum Amount' FROM customer c INNER JOIN orders o ON c.custid = o.custid GROUP BY c.IndustryType,o.orderdate;
i will try to help you understand it, but it may take a few steps how many of your orders occur on the exact same date?
you must do yoga to stretch that deep.
You're correct, everything is locked down and installing software requires administrator access, we can't even do basic things like changing the timeout limit on our Skype messenger, nothing stopping me from setting it to always away though XD. We do have a library of various pre-approved software that can be installed, some of which is free and some isn't, you have to basically select one for approval by your immediate manager, and then IT remotes in and installs it for you. They're generally pretty open to it as long as it doesn't cost a lot for licenses or require ultra sensitive access, for example I can get access to member/client data but internal employee data is a no go. I'll have to check on Monday if SQL server is available, but I know Access is and it sounds like that should be sufficient for my purposes. 
i'm fairly certain it's Oracle, yes. not sure which version though, i'll have to check on Monday. I glanced at that article, seems like there's a LOT of ways to do this! i'll have to play around with some of that stuff next week.
Agree, bad i.t processes. Create a central etl process.
&gt; ustryType,o.orderdate; then you have only one row per each date per each industry type so min = max = avg
Quite a few: https://en.wikipedia.org/wiki/SQL_injection#Examples
&gt;On March 27, 2011, mysql.com, the official homepage for MySQL, was compromised by a hacker using SQL blind injection That's hilarious 
Here is my clarification since appearantly I didnt word this correctly... My table has 10 columns. However my query I want to execute has 13 columns. Normally deleting the 3 extra columns from the query would be easy, except for the fact that it has well over a thousand rows of Inserting to do. How can I BULK remove the extra columns from my query? (I have tried putting it into Calc and then deleting the columns, but then my query is misformed because I have to retype all those quotation and brackets that were deleted)
you'll need a **numbers table** CREATE TABLE numbers ( n INTEGER NOT NULL PRIMARY KEY ); INSERT INTO numbers VALUES (0),(1),(2),(3),(4),(5),(6),(7),(8),(9), ... ; then it's easy -- SELECT yourtable.id , yourtable.text FROM yourtable INNER JOIN numbers ON numbers.n &lt; yourtable.amount
Ok thank you that worked ! Is it possible to do it without creating a new table in one query? So its abit more elegant
yeah, it is... some kind of recursive CTE maybe... i forget... but if you weren't aware, that numbers table will come in handy in other queries where counting is required... it's really a lot cleaner to make it a permanent table
You still need to add single quotes on both sides of your text string. This should work: SELECT @SQL = 'SELECT RIGHT('''+@input_table+''',4)'
When you're assigning the string to @SQL, you're setting it to: SELECT RIGHT (2018_1_PROJECTED_INPUT_AUAP,4) Notice how the string "2018...." isn't set off by single quotes? Change your assignment statement to be: SELECT @SQL = 'SELECT RIGHT('''+@input_table+''',4)' This will add the single quotes into your @SQL string. You can easily debug what's going on if you comment out your EXEC statement and just do a SELECT @SQL. That way you can see what it's trying to interpret.
Thanks a lot for your help!
Thanks a lot for your help!
Could you trouble you for help again? Last time I promise ;) https://pastebin.com/GtMcJ6pT I'm still having trouble wrapping my head around the use of quotes when using dynamic SQL 
Could you trouble you for help again? Last time I promise ;) https://pastebin.com/GtMcJ6pT I'm still having trouble wrapping my head around the use of quotes when using dynamic SQL 
So, for the second, you're just trying to use dynamic to set the value of the @zone_peril variable (which, realistically, you don't want to do, but I assume it's just for learning dynamic here). Essentially, think of the string you're building as the actual query itself that you'd use in SSMS. So, for instance, what you're trying to end up with here is something like: set @zone_peril = select right('text_string', 4) With the assignment you're doing, instead of ending up with "set @zone_peril", you're attempting to use the value of @zone_peril, which doesn't have one yet (currently null). Thus, when you run the sql string, you're essentially not running anything, because the value of @sql is null. If you're actually trying to set the variable, you'd use this: set @sql = 'set @zone_peril = select right(''' + @input_table + ''', 4)' If you want to test that you build it correctly before running, just use a select or print statement with the @sql variable and review first, to make sure. Additionally, you should not use *exec(@sql)*, and instead should use *exec sp_executesql @sql*, as that will allow for parameterization, which will be super import to help prevent SQL injection. 
awesome dude thanks so much!
I think Sony's is the most infamous of them all. Because it kept happening to ALL their sites.
 https://xkcd.com/327/ ^^ then there is this joke kicking about with sql code for injection on the bumper of a car to exploit the anpr system ;) https://hackadaycom.files.wordpress.com/2014/04/18mpenleoksq8jpg.jpg?w=636 
 I should point out that people with the surname "null" have serious day to day issues as well.
Error: Surname cannot be NULL
"boy named Sue" for the digital age...
Interesting! I have worked with SSRS 2016 before quite a few times. It has a completely different look to it compared to the early (2008?) version. I will have to see if I can get a copy of this on my local machine. 
Haha many people don’t like sharepoint it sounds like. 
In terms of basic querying of a database, you'll be fine with your PROC SQL knowledge. That said, knowing joins is very important to SQL-only environments so make sure you're clear on all that. Shouldn't be too much of a stretch, but I don't you use joins much if you've only recently learned SQL. Database management type stuff varies by SQL installation, so unless you know what type of database they're using, there's only so much you can do. Also, it goes without saying that some keywords in SAS don't exist in the rest of SQL (INTO being the most obvious).
i have been writing SQL for close to 10 years now. primarily using MS SQL, teradta, and some ASTER. I was recently put on a team that uses SAS. My knowledge of SAS is only beginner, but from what i can tell, a proc SQL; statement is not different in essence. but what you can do with a proc sql is pretty cool. essentially what you can do with variables inside sas is neat to me. take for instance, SSIS. i can connect to teradata from ms sql but i cannot declare a variable similar to how i would do it in MS SQL. in MS SQL query window you would declare the variable, then set the variable. in teradata you cant declare or set (at least not in my environment). so in SSIS i would need to set a variable in the project, then write a string to concatenate a select statement using that project variable. in SAS i can let my variable equal something then insert that directly into the proc sql. different strokes for different folks. but otherwise, a SQL statement in a proc sql command is the same thing as a regular sql query. Here is an example of creating a simple table between the two, in my opion SAS is more wordy. i am using sas to create a table in teradata. the second is using ssms to create a table in a ms sql db. **SAS create table; %include '/sas/data/UserLogin.sas'; proc sql; connect to TERADATA (user=&amp;TD_id. password=&amp;TD_pwd. server=&amp;TD_CON. connection=global); execute ( CREATE GLOBAL TEMPORARY TABLE TABLES.test ( srvy_cmpl_id int, ctn bigint, ban bigint, cloc_id bigint, trans_dt date, srvy_chnl_nm varchar(10), rep_id varchar(100), ACCT_ID bigint, src_trans_id varchar(100) ) ON COMMIT PRESERVE ROWS ) by TERADATA; execute (COMMIT WORK) by TERADATA; quit; **MS SQL create table; USE [DB] create table test ( srvy_cmpl_id int, ctn bigint, ban bigint, cloc_id bigint, trans_dt date, srvy_chnl_nm varchar(10), rep_id varchar(100), ACCT_ID bigint, src_trans_id varchar(100) ) i can essentially take the same part of the SQL code in my proc sql and write a select query as well. Although, writing a query from a sas dataset could be different. I have seen queries where you select a dataset and then KEEP only specific columns. 
Hard to answer without knowing the type of join and what type of relation ship it is.
Hard to answer without knowing any information at all... OP, have you tried a JOIN?
INTO absolutely exists in SQL.. same with joins.. I don't know if your advice makes sense.
PROC SQL is limited though. An example would be that you cannot do window functions in PROC SQL. Also, your proc sql example uses a pass thru query to teradata so you are using teradata syntax there which doesn't speak to the syntax of PROC SQL...
PROC SQL is a subset of most common DBMS sql syntax. The answer is that other database systems typically have more functionality than proc sql, so depending on how complex the problems are you may have to know that specific DBMS. However, if you are comfortable with creating tables, joins, aggregate functions, sub-queries, and update/insert/delete statements I would say you are good to go for 90% of problems. The rest of it you can learn on the job for the most part (unless you are going to be a dba which I'm assuming isn't the case).
ahh, that is good to know, i didn't know you couldn't do windowed functions. 
This was a life saver man. I didn't do cast but I did convert on some of the offending fields and this did the trick! 
Glad I could help!
I think you're looking for the "UNION ALL" command. (Technically speaking, just "UNION" works too, but it's faster to use "UNION ALL" if the sets are guaranteed disjoint, because it saves you a sort.) SELECT t1.deviceid, t1.devicetime, t1.latitude, t1.longitude, t1.address, t1.attributes FROM positions t1 INNER JOIN ( SELECT MAX(id) as LatestGpsPoint FROM positions WHERE deviceid IN (SELECT deviceid FROM device_group WHERE groupid = 7) AND valid = 1 AND attributes LIKE '%FRI%' AND network != 'null' * AND devicetime &gt;= (SELECT deploymentdate FROM devices WHERE id = deviceid)** GROUP BY deviceid UNION ALL SELECT MAX(id) as LatestGpsPoint FROM positions WHERE deviceid IN (SELECT deviceid FROM device_group WHERE groupid = 7) AND valid = 1 AND devicetime &gt;= (SELECT deploymentdate FROM devices WHERE id = deviceid) ** GROUP BY deviceid ) t2 ON t1.id = t2.LatestGpsPoint ORDER BY t1.deviceid ASC LIMIT 0,500 * Is this really the string "null"? If it's an actual null value, you should use IS NOT NULL, not "!= 'null'". If you're using the string "null", you should convert those to actual nulls or at least the empty string ''. ** I don't think this is going to work the way you think it will. Even if you're sure this will only return one value, the query optimizer has no way of knowing that, so do a select top or max or something.
&gt; when I try to attach this select as a subquery I get an error message please show this attempt... sounds like it should be a join
Actually, looking at this again, I'm not entirely sure how smart your IDs are. You might need to ditch the union and do this as two separate subqueries, joining to one subquery on id=id and protocol = gl200, and the other subquery on id=id and protocol = dmt. It's hard to say, just from this interdirect look this data model seems really wacky.
First of all, you're not selecting anything from t2 that I can see, so your would-be join is just a filtering and duplication query. Second, I don't know what database system you're using, but `!= 'null'` will always return false if it's in standards-compliant mode. You're probably looking for `is not null`. What you probably want is a `case` statement. They usually look like this: select foo, bar, baz, ..., case when t1.protocol = 'g1200' then ... when t1.protocol = 'dmt' then ... else ... end as LatestGpsPoint from ...
Based on your error, you're probably returning multiple columns in your subquery, which isn't allowed. But I don't think you should be using a subquery, why not just put your criteria in the "where" clause? 
A simple google of “btrieve to sql Server” returns plenty of results. If you don’t have any SQL Server experience, nobody is going to be able to explain it in a simple reply here. 
You should probably do a SELECT CASE query.
So null is actually a literal string with the value of null (traccar is the main platform that sends data to the database). I think I found a semi-solution, but unsure if it's effecient? SELECT t1.deviceid, t1.devicetime, t1.latitude, t1.longitude, t1.address, t1.attributes FROM positions t1 INNER JOIN (SELECT MAX(id) as LatestGpsPoint FROM positions WHERE deviceid IN (SELECT deviceid FROM device_group WHERE groupid = 7) AND valid = 1 AND IF (protocol = 'gl200',attributes LIKE '%FRI%',attributes != '') AND IF (protocol = 'gl200',network != 'null',network = 'null') AND devicetime &gt;= (SELECT deploymentdate FROM devices WHERE id = deviceid LIMIT 1) AND devicetime &lt;= '{$todate}' GROUP BY deviceid ) t2 ON t1.id = t2.LatestGpsPoint ORDER BY t1.deviceid ASC
What are you pulling the data from?
Been pulling data from csv and trying to put that into sql server. If I had a primary key inside the table and it’s auto incrament do I need a column for that pk in the csv? 
yeah, you have no leeway with bulk insert. you can have a blank identity column in the CSV it just has to be present.
You can't use the keyword BULK INSERT, but SQL is smart enough to load data in batch mode instead of row-by-row if it can. Check out this [stackexchange](https://dba.stackexchange.com/a/99371/115194) post.
Maybe I should have used INTO:, but just so I'm clear, INTO for making macro variables does not exist in regular SQL.
Do you mean something like: select count(CASE WHEN my_value BETWEEN 0 AND 0.24 THEN 1 END) as count0_024, count(CASE WHEN my_value BETWEEN 0.25 AND 0.49 THEN 1 END) as count025_049, count(CASE WHEN my_value BETWEEN 0.50 AND 0.74 THEN 1 END) as count050_074, count(CASE WHEN my_value BETWEEN 0.75 AND 0.99 THEN 1 END) as count075_099 ... from my_table; I guess that I was hoping for a shorter query or something since that quickly becomes a bit messy.
Well, the query is from a web form and sometimes includes joins and so on but it could simplified look something like this: SELECT *, COUNT(id) as 'num_entries', SUM(my_table.price) AS 'earnings_count', (select concat(0.25*floor(my_value/0.25), '-', 0.25*floor(my_value/0.25) + 0.24) as `range`) FROM my_table WHERE my_table.currency='example' AND GROUP by `range` ORDER BY `range`; This says "Unknown column 'range' in 'group statement'", should I create a join here or something?
You need to use the full formula in the group by. This is because of the order the engine evaluates the query where and group by being before the select list (thus the engine doesn't "know" you are referring to the aliased column). I believe you can leave the alias in the order by (since that is the last operation the engine performs).
It surely can be done, but you probably want to build your column list dynamically. Something like this: https://www.mssqltips.com/sqlservertip/3002/use-sql-servers-unpivot-operator-to-dynamically-normalize-output/
Thanks, that seems to be working by a quick glance. I'm not sure if this formula is correct to begin with though since the number of occurrences is different from the SELECT CASE below. The concat 0.00-0.24 is 3390 and SELECT CASE count000_024 is 9020 but it's probably something I screwed up in a hurry and I'll figure it out. Thanks for your help.
Off the top of my head, I'd aggregate book rental table by user ID and count how many books they rented. Toss that into a temp table and aggregate by the count (number of books rented) and count the user id's. 
If I understand your problem statement correctly, for each number of books rented, you want to know how many users rented that many books. Based on that, I'd start with something like this: select NumBooks, Count(user_id) as NumUsers from ( select user_id, count(book_id) as numbooks from datatable group by user_id ) s group by NumBooks order by NumBooks The subquery tells you how many books each user rented. From that, you can then roll up how many users rented each number of books.
FYI this ended up working great. I was already use to Access db anyway. THANKS for the idea!!!!
I'm just gonna be the contrarian and say that this is neither interesting nor difficult. It's a fairly straightforward query, as shown by /u/redneckrockuhtree.
you could just be honest and say you are asking for homework help
i wish this was homework lol
Here is a unpivot code. SELECT col1, col2, col3 FROM ( SELECT col1, col2, col3 FROM yourtable ) d UNPIVOT ( value for column in (col1, col2) ) unpiv;
It highly depends on your DB engine, so you'll have to be more specific. For instance, on SQL server (up to 2016), it's select a.adminName, STUFF((select ',' + Cast(ClientID as varchar(5)) from helpTable h where h.adminid = a.adminid FOR XML PATH('')),1,1,'') from adminTable a On SQL Server 2017, there's a new function that helps with that specific type of string construct: With Agg as ( Select AdminID, STRING_AGG( Cast(ClientID as varchar(5)), ',') AS Clients from helpTable GROUP BY AdminID ) SELECT adm.Adminname , agg.clients FROM adminTable adm LEFT JOIN Agg ON agg.adminID = adm.AdminID As for other engines, it depends.
Interview question? It definitely reads like a contrived test of fairly simple SQL skills.
There are a number of different methods to concat multiple rows of strings into a single comma separated list, and sadly they're all ghetto. Until STRING_AGG is released with SQL Server 2017, the FOR XML PATH () method is what I use. See below: SELECT a.AdminID, SUBSTRING( x.cust, 1, LEN( x.cust )-1 ) AS AssignedCustomers FROM #Admin a CROSS APPLY ( SELECT CONVERT(VARCHAR, h.CustomerID) +', ' FROM #Help h WHERE h.AdminID = a.AdminID ORDER BY h.CustomerID FOR XML PATH('')) x (cust) WHERE 1=1 AND a.AdminID = 1;
yes lmao, honestly I already got it before hand, but leetcode always seems to have a better solution than the ones I come up with, and I don't want to screw this up
One issue is it appears the limitation on our SQL Server (2013 I believe) is 1,000 columns. Any workarounds for this?
I didn't even think about that... none of these columns are duplicates? The only thing I can think of off the top of my head at the moment is to split the data into smaller parts, but surely there's a better solution.
Came to say STUFF() - was already said. Good luck.
thanks man, it's SQL Server 2014 (12.0.2008)
If you put this into powerquery which is free with excel 2016, and free to download for previous versions there is an unpivot button that should do exactly what you're trying to do.
If I understand correctly, instead of joining Agent_Interval itself, maybe join a subquery (SELECT DISTINCT DateTime FROM Agent_Interval) AI Also this: http://sqlblog.com/blogs/aaron_bertrand/archive/2009/10/08/bad-habits-to-kick-using-old-style-joins.aspx and https://blogs.sentryone.com/aaronbertrand/bad-habits-nolock-everywhere/
Will it let me open a csv that is ~ 500 mb, and then save it as one that has ~40 million rows around 1.4gb? I know excel itself maxes out at just over 1M rows. I have used the unpivot in PowerBI import, the issue is I can't get the data out of powerbi with that many rows, to do more complex analysis in R.
I tried to select count(OPs_moms_plates) from buffet_table and got a stack overflow. I told her it's time to rollback; but she still wants to commit; 
None are duplicates. Each is a different account. For something that will have to be processed fairly regularly chunking it into 16+ imports would be pretty tough.
Haha. A SQL query walks into a bar and sees 2 table. He walk up to them and says ‘Can I join you’? Then a waitress walks up and says ‘Nice View’. 
It absolutely should
Can you make account it's own column in the table?
It sounds like you should be handling this as part of the insert rather than deleting duplicates after the insert. It also sounds like a perfect use case for [MERGE](https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_9016.htm#SQLRF01606).
Can you guarantee the record once changed with not change to a previously seen state? If so then this is very easy but if the possibility exists that it could revert back to a former state (except load date of course) that makes it a bit more challenging 
It is possible that *something* could revert back. Say on 10/1/17 you were in department 123 but then transferred to department 456. It got entered into the HR software on 10/2/17 but turns out you weren't eligible to transfer so on 10/3/17 your department is back to 123. Real world examples don't happen often but data entry errors do. But even then, I'd want those records because those records have changes either way. A quick way that doesn't exactly get what I want is to do a select distinct on all columns except the load date. But then I don't have the load date. 
Interesting and difficult is always relative. That said, to me, this isn't difficult.
In the real world I'd probably want to do a select (query) into new_history_table, just to get something less fragmented and with an appropriate high water mark, then drop the original table. I actually use row_number over (partition by file# order by load date desc) then subquery that for row_number=1 to get the "most recent history record per employee", but my challenge is how to incoporate that into a distinct selection per partition of "windows" of employees, while keeping the lowest load date. I feel like answer is probably simple but I can't get my head around it. 
So glad I could help! Now I have something I’m trying to work out. Maybe you can help. I have no concerns using Access directly to modify my own tables. Now, I’m trying to set up users to modify table data, obviously this needs more security and checks. I want to program a couple of buttons to show them what changes they’re going to make, and have them confirm the changes before committing them to the table. Any ideas?
Is this for an actual interview or just an online training thing?
I’m going to look at this. Thanks. 
 SELECT x.* FROM ( SELECT column1, column2, column3.... , loaddate row_num() OVER (partition by column1, column2, column3.... ORDER BY loaddate asc) rownumber FROM table ) x WHERE x.rownumber = 1 So you'd put all of the fields except loaddate into both the select list and the partition by list (loaddate goes only into the select list), and then you'll get the earliest loaded record for each combination of data items. But as noted elsewhere this will miss records that have been changed back to the original values at a subsequent time - i.e., the returned record with the highest date isn't actually guaranteed to be the current state of the data if the current state is how the record was previously. 
You guys must be developers. DBAs do not make jokes.
Yes sir, you are absolutely correct. 
They are good jokes. I thought of sharing them, but nobody would understand them if I did. It would just mean days of trying to explain what a database is and at the end everyone would still ask me to fix their computers since I "work on computers". I've been recommending that GentleSQL site for years to folks who ask for resources to learn SQL. That and Brent Ozar's site are the best I've found.
Those are good sources to pick up SQL, best source is finding a database Meet Up near you. You can make great connections that way. 
Hey we got ourselves a wise guy ova here! No, really. This works! Spent some time with this and this is the correct answer for 99% of cases and I believe it is the best it can be given the data's possible limitations and shortcomings. The other thing that I didn't really go into detail on is that this software keeps track of important effective dates for fields that make sense to keep them on. For example birth_date and SSN should never or at most rarely change under normal circumstances so those are not tracked with dates. But for things that change often such as employment status, job/position/location/supervisor name, etc, they are tracked with dates. So if someone changes departments for a day and changed back, the distinct selection of the whole record set per employee (minus the load date) is still unique and ends up as a "row 1" within the partition because of the dates on those fields. To compare, I individually did a select distinct on one person to get the same grouping and the data is the same. Except my big hangup was how to get this distinct selection/grouping going within the partitions/windows. But I completely forgot that you can have all that stuff after the "partition by" clause. In PL SQL I could have done a "for each rec in employee loop .... end", but I knew this could be solved in regular ol' SQL. And you did it! Thanks! And I came out with a more complete understanding of partitioning. 
I was bored one day and created a table named yourMom. I immediately dropped it when I was finished.
leetcode training
I laughed way too hard at this
No worries I backed up the server got yourMom table back, I dropped immediately due to the fact it had garbage data. 
Every time I query yourMom, I do a full table scan and the cost is just insane.
No you shouldn’t avoid it. It’s convenient at times, as is cascade updates. You should understand it and document in your application where it is depending on that functionality. For instance, if you have a “delete customer” screen, you need to understand that this will delete everything related to that customer. If that’s desired, put that both in the comments and in a warning to the user. 
That's funny I found the query cost quite cheap.
IT WAS HP FC ADAPTER, for some reason it could not handle 72 logical cores, (lol i know right),with 36 it was working like a charm. 
so if you echo the $sql and copy and paste into phpmyadmin it works fine? 
&gt; My query works on phpMyAdmin, but when doing the query from PHP, it fails. &gt; &gt; Is it a problem with my query? no 
&gt; I just never thought it was a bad thing. you were right
You have the knowledge of how to make Query1 a CTE so you change it into a CTE. Then put Query2 under it, and INNER JOIN the 2 UNIONs ON dbo.ContractEntityCode([?].entityref) = Cte.Entity. There's an argument against this for performance reasons but then again you're using a user defined function in the SELECT clause so I'm not worried about it... As for making this over-complicated in your head...maybe/maybe not. We'd have to know the data and the schema before we can make that call. **Example** WITH Cte1 AS ( SELECT MAX(dbo.ContractEntityCode(A.Entityref)) AS 'Entity' FROM Ac_Billbook AS A JOIN Entities AS E on A .EntityRef = E.Code WHERE A.Actual_Posting_Date &lt; DATEADD(mm, - 12, GETDATE()) AND A.Actual_Posting_Date &gt; '2014-12-31' AND Type = 'Posted' AND E.ResponsiblePartnerRef not in ('DMM', 'KM', 'AJH') GROUP BY dbo.ContractEntityCode(A.Entityref) ) SELECT dbo.ContractEntityCode(M.entityref) AS 'Entity' , MAX(E.NAME) AS 'Name' , MAX(M.Number) AS 'Last_Matter_Number' , CAST(Max(M.Created) AS DATE) AS 'Last_Matter_Creation_Date' , MAX(u.FullName) AS 'Partner' , MAX(U.EMailExternal) AS 'Partner_Email' FROM Matters AS M INNER JOIN Entities AS E ON M.EntityRef = E.Code INNER JOIN Users AS U ON E.ResponsiblePartnerRef = U.Code INNER JOIN Cte1 AS c ON dbo.ContractEntityCode(M.entityref) = Cte1.Entity WHERE M.number &gt; 3 GROUP BY EntityRef HAVING MAX(M.Created) &lt; DATEADD(mm, - 3, GETDATE()) AND MAX(M.Created) &gt; DATEADD(yy, - 1, GETDATE()) AND MAX(E.Name) NOT LIKE 'Non%Chargeable%' UNION SELECT dbo.ContractEntityCode(A.entityref) AS 'Entity' , MAX(E.NAME) AS 'Name' , MAX(A.Number) AS 'Last_Matter_Number' , CAST(Max(A.Created) AS DATE) AS 'Last_Matter_Creation_Date' , MAX(u.FullName) AS 'Partner' , MAX(U.EMailExternal) AS 'Partner_Email' FROM MattersArchive AS A INNER JOIN Entities AS E ON A.EntityRef = E.Code INNER JOIN Users AS U ON E.ResponsiblePartnerRef = U.Code INNER JOIN Cte1 AS c ON dbo.ContractEntityCode(A.entityref) = Cte1.Entity WHERE A.number &gt; 3 GROUP BY EntityRef HAVING MAX(A.Created) &lt; DATEADD(mm, - 3, GETDATE()) AND MAX(A.Created) &gt; DATEADD(yy, - 1, GETDATE()) AND MAX(E.Name) NOT LIKE 'Non%Chargeable%' ORDER BY Partner, Last_Matter_Creation_Date;
Thanks for that, that seems to work! Not sure i understand why just adding a join would filter it. I thought Id have to add the join, but then have a WHERE or HAVING clause saying where Entity is IN CTE.Entity or something.
Ah I think I get you... It's because we are using an INNER JOIN here. We're joining the CTE back to the tables, and the `ON CTE.Entity = &lt;whatever&gt;` takes the place of the WHERE or the HAVING bit. As you know from your studies ( *yeah, I read twitter* ), the INNER JOIN won't return rows that don't match so we don't need any WHERE or HAVING clause. We're just using the join back to the CTE to constrain the results to only return those where there's a match on the 'Entity' bit.
ah right ok, that makes sense. INNER JOIN wont return rows with noon matches!!! Thanks!
Run your queries in the EXPLAIN ANALYZER and see for yourself. Much depends on your indexes. Taking the time to understand EXPLAIN output is a good way to learn how your queries and indexes work. [EXPLAIN](https://mariadb.com/kb/en/library/explain/) [EXPLAIN ANALYZER](https://mariadb.com/kb/en/library/explain-analyzer/)
Are your sure that $store_id contains anything on query execution?
Could do something like this: ,Result AS ( SELECT * FROM ( select *, Ordinal=ROW_NUMBER()OVER(PARTITON BY Entity ORDER BY Last_Matter_Number DESC) FROM ( .... UNION ... )a )b WHERE b.Ordinal = 1 ) SELECT * FROM Result
Why is a CTE even necessary here? Those two queries can just be inner joined normally as subqueries, right?
Cross posted with /r/SQLServer Gotta be careful using irregular identifiers especially with dynamic sql. 
Here is a list of threads in other subreddits about the same content: * [Irregular Identifiers – Be Aware of Surprises](https://www.reddit.com/r/SQLServer/comments/78o3s4/irregular_identifiers_be_aware_of_surprises/) on /r/SQLServer with 2 karma (created at 2017-10-25 22:42:23 by /u/InternetBowzer) ---- ^^I ^^am ^^a ^^bot ^^[FAQ](https://www.reddit.com/r/DuplicatesBot/wiki/index)-[Code](https://github.com/PokestarFan/DuplicateBot)-[Bugs](https://www.reddit.com/r/DuplicatesBot/comments/6ypgmx/bugs_and_problems/)-[Suggestions](https://www.reddit.com/r/DuplicatesBot/comments/6ypg85/suggestion_for_duplicatesbot/)-[Block](https://www.reddit.com/r/DuplicatesBot/wiki/index#wiki_block_bot_from_tagging_on_your_posts) ^^Now ^^you ^^can ^^remove ^^the ^^comment ^^by ^^replying ^^delete!
Since both duplicates have different EntityID does this mean the values in the original tables are not unique/correct? I would look into row_number like /u/aplato said.
Probably going to be unpopular/asshole opinion but I'm going to state it anyway. People get paid hundreds of dollars per hour to do what you are asking someone to Skype you for free. Maybe you should take a different approach and post your problem with lots of information in a standard format (just like it says on the sidebar) and someone may come along and help you out because they are nice.
yes, I've echoed the final SQL statement and the id is there...
For each employee ID, could you take the data with the most recent load date? Assuming the most recent is the correct? 
tinyint is the datatype...
It's a tool. That's like saying you should avoid flat head screwdrivers because if you're not careful you could strip a Phillip's head screw. Sure, cascade on delete has consequences. If you use it carelessly you will have mistakes. But if you want to delete records of something all at once, it's very helpful and effective tool.
Yes.
Without knowing your data, I'd select rownum, distinct ( PT, enc) in a cte and join it back to the rest of the data you need. 
Thanks I think this did it! The power of ;CTEs. ; WITH newrow as ( SELECT DISTINCT (CAST(pt_no as varchar) + visit_type) as key_ , DENSE_RANK() OVER ( ORDER BY(CAST(pt_no as varchar) + visit_type) ) as id FROM #chgs ) SELECT id, c.* FROM newrow INNER JOIN #chgs c ON (CAST(pt_no as varchar) + visit_type) = key_ 
Is there not another table? What fields are you joining by? Those questions should help you understand it. 
What have you written so far
I'd take a look at the [unpivot capabilities of SSIS](https://www.mssqltips.com/sqlservertip/1761/how-to-use-the-unpivot-data-flow-transform-in-sql-server-integration-services-ssis/) if you are dealing in that size if data. That, some other ETL tool, or writing a custom c# script to do it. Especially if you plan to do it on a regular basis.
Irregular Identifiers you say? SELECT [😀] FROM [A 💩].[A 😀🔤Hello, World!🔤🍉]
Is there something wrong with the below? Resorting to a CTE seems overly complex. SELECT DENSE_RANK() OVER ( ORDER BY pt_no,visit_type) as i, * FROM #chgs Since you have actually used DENSE_RANK I feel like I must have missed something.
On my phone and not familiar with mySql or what you mean by using a join without using a join, but either of these would probably work in swl server. First is using a join. The second is using a subquery Select SKU, SKU_Description, i.WarehouseID From Inventory i inner join warehouse w On i.WarehouseID = w.WarehouseID Where Manager = 'Lucille Smith' Select SKU, SKU_Description, WarehouseID From Inventory Where warehouseid in ( Select warehouseid From warehouse Where manager = 'Lucille Smith') 
Maybe look in to the Mysql documentation on join: https://dev.mysql.com/doc/refman/5.7/en/join.html I spottet a few options that didn't use 'JOIN ON'.
try removing the '' around $store_id 
&gt; Use a join, but do not use a JOIN ON syntax what is this perverse evil fuckery
Already tried that, even used the dot operator to concatenate...
Ugh yeah no you're not missing anything. This works: this was the simple solution that I was somehow missing. Maybe I still was messing with partitions or something when I first tried it--I dunno I guess my brain was a little done by the end of the day. Thank you. 
Choose Postgres because it's the best overall, has the best developer community, and best documentation. IMHO Aurora is better for saving money on very large projects. IMHO MySQL and MariaDB offer no meaningul advantages over Postgres given your very small loads and hosting on AWS RDS.
if you hard code a store_id does it work? ie. not use a variable.
It usually is very simple. After restoring the DB, update the config file to use restored DB. Restart the AppPool for that site (right click on AppPool to use Stop and Start).
The app pool being on isn't going to prevent you from restoring the database.
Yea, I think its great. We've been using it in production for a couple of years, and we have zero problems with it. Aurora is used exactly as a native solution, as in you just point your connection string to the Aurora endpoint like you would normally, and it'll just work. The upside is that Amazon provides a lot of awesome tools and goodies, like its very easy to have read-replication databases across regions. We're using MySql because PostgreSQL became "Generally Available" by amazon just today, not not in all regions. I still wouldn't PostgreSQL through Aurora in production today, because it seems a bit early, like in a somewhat beta stage...
`alter database &lt;yourdatabase&gt; set offline with rollback immediate` Then restore the database over top of it. The next time the app pool needs the database, it'll re-establish the connection.
If the app pool has connections open to the database, it will prevent a restore with replace.
Is this a paid gig?
Aurora is basically MySQL, with Postgres becoming available just now. I just completed a [pricing spreadsheet](https://docs.google.com/spreadsheets/d/1lEXEHgWaLfeqijujAZbvzpdM1fORIqED1AHUM3fQO_0/edit#gid=89152068) to compare various SQL offerings on EC2, RDS and Aurora. Aurora won for pricing and options above all else in this category. 
Could use a little more specifics to answer more accurately, namely which flavor of SQL you're working with and what exact error you're getting. But I'd suggest almost always running a select on the rows you wanna update first, so you know exactly what you're about to change. Try running a select and make sure everything that you wanna change shows up in the select. Then make sure that if you're trying to change a **unique** column that you also go ahead and run a select looking for any entries with the new value. Start with that and maybe we can find the root issue and work through it together.
And aurora becomes insanely faster at the higher end scale not lower (ergo don't expect much faster if at all on a t2 instance type because there's initial overhead to afford that scalability and aurora sauce). Their team published a paper for a Chicago DB conference outlining some of the cool stuff under the hood 
Yes, you can do this: select * from products as widgets where widgets.type = 'widget' It's also really helpful to make joins more readable or dealing with column name conflicts in your select statements: select e.name as 'Employee', d.name as 'Department' from employees as e inner join departments as d on e.departmentid = d.id where d.name = 'Sales'
thank you
[S]ingle quotes are for [S]trings [D]ouble quotes are for [D]atabase identifiers As per ANSI standards.
I don't think that statement, in and of itself, should care about duplicates. Seems like there's something going on elsewhere. Could you post the full error message? Possibly that table has a composite UNIQUE constraint or something.
TIL Thanks.
Yep. Nothing to see here folks. Just start hitting F5 😀
Give the records sequential number with row_number per product, then pivot the first four.
Of course! Thank you! The coffee is a bit weak today.
You can even use it to rename the entire FROM part: SELECT * FROM ( SELECT something FROM sometable WHERE mycriteria = TRUE ) AS myTable WHERE mytable.something = 'Potato';
Nope, nada...
May as well not have a join clause and do select blah from a, b, c where some_condition = some_value The horror. I mean. Halloween is coming right?
At one point I thought that the size of the resultset was to blame (599 records) but even when I use LIMIT 5, the query fails.
Because it's not valid syntax. It's not the query failing, it's not even valid to begin with. If you tried to save this as a SPROC you can't as it's not valid SQL. Try this: DECLARE @temp TABLE ( [LONG_DESC] NVARCHAR(MAX) ) DECLARE @var XML, @p_status VARCHAR(32), @p_status_msg VARCHAR(MAX) INSERT INTO @temp ([LONG_DESC]) VALUES ('stuff'), ('thing'), ('more stuff'), ('more things'), ('yep') BEGIN TRY SELECT @var = [LONG_DESC] FROM @temp SELECT 1/0 --note the invalid division SET @p_status = 'Success'; SET @p_status_msg = CAST(@var AS VARCHAR(MAX)) END TRY BEGIN CATCH SET @p_status = 'Failure'; SET @p_status_msg = Error_message() END CATCH SELECT @p_status ,@p_status_msg 
The fun part is that the select statement might fail depending on your db collation.
 WHERE foo &lt; salesDate - INTERVAL 1 DAY
The first thing I do when coding of any kind is break the problem into chunks and get each part functional and/or use the previous chunks to build on the next parts. In your case, translate your question into SQL language and start building pieces. "Assign a new schedule" = Insert a record into Course "Accept a course code" = You need a variable in the sproc for course code, match the type to your field "and a start date" = same as above "Schedule each course module on consecutive weekdays" = modify the insert we created above to insert a record on all weekdays "the start date has to be at least a month in the future" = add logic to the sproc to validate the date input above to make sure it falls in the month range etc. etc. Pretty much how I approach all coding and query problems. Hope this helps!
You could add a before insert trigger on the table, check the date is a month in advance, if it isn't then throw an error instead of inserting.
Thanks! I kind of figured it was because it wasn't encountered during run time, or whatever... Don't have a whole lot of experience with error trapping. Next question: why would "parse" not catch this? What exactly is parse good for?
The PARSE function? It's like CONVERT but it's a bit more robust. Used for manipulating data types. Not really the same use as TRY/CATCH.
No kidding... what are we teaching the yungstas these days? Pretty soon, JOIN statements and ~~manual transmissions~~ steering wheels will be regarded as witchcraft.
You using MS SQL? dateadd(day,-1,salesDate) Be careful if you're working with DateTime instead of date - might need to cast appropriately!
That works! Thank you!
You really shouldn't be worried about the order that your data is put into the table - you should be worried about the order that it's in when you select it out. I can try to help if you provide some more info - but I think your current strategy isn't ideal if you're required to insert data in a specific order.
DATEADD(DAY,-1,salesDate)
No, I mean the parse button (Ctrl + F5). I've always understood that to be a check of your queries' syntax, and it says it's good to go.
so basically what I need is a header and a footer added to the data I am extracting and for it to be in a table so I can run a BCP command to extract it into a txt file to FTP to a customer.
Couldn't you just do this: SELECT A.A, A.B, B.C, FROM TableA AS A, TableB AS B WHERE A.Id = B.TableAId Technically, you're not using any JOIN syntax, but it's an extremely poor way of writing this query.
Because it only checks that you are using valid SQL syntax structure, not if you used that SQL syntax correctly. SELECT whateverIwanthere FROM nothing WHERE 1=0 Parsing this will succeed even though I don't have any of these objects and 1 will never = 0. All it is looking for in this statement when it is parsed is that after a SELECT, it should find a text qualifier after it and not a reserved SQL word. 
/u/Cal1gula's said: &gt; Because it's not valid syntax. You are saying parse: &gt; only checks that you are using valid SQL syntax structure The distinction is sort of lost on me.
Is the SSIS property "protectionLevel" set to "dontsavesensitive"? If so change it to one of the other options (I normally go with encryptsensitivewithuserkey, but that may not be suitable). 
Well I'm relatively new myself - somebody might have a better solution, but my immediate and hacky thought is to add an extra column (sortData?) to the temp table that you formed, and since you're already inserting a header and a footer separately into the temp table, just insert a 1 for the header, 2 for the data, then 3 for the footer. Then when you go to select the data out, just exclude it in your select but then order by sortData. select data from @temp order by sortData I'm confident there'd be a better way to do it if you needed it to be more efficient or cleaner, but if it's a one-off thing you need done to get on with your day, it should get you there. Hopefully somebody more experienced will come along and offer you a cleaner solution.
Think of it this way. Parsing is kind of like checking that you spelled all your words correctly and that you used punctuation(not that you used it properly, mind you). Pressing Execute is handing it over to your teacher to be read. Even though the words are all spelled correctly, they may not make any sense at all to the reader. So in your case, all your words were spelled correctly and you even wrote a paragraph however one of your sentences didn't make any sense.
Where is the best place to find this property? I always stumble around. 
I suppose it is *technically* valid syntax--in that the literal order of the arguments is correct. That's what the parser checks for. We're kind of using the term interchangeably (and probably shouldn't be but I'm not sure what other word I would use). There are many errors that will come from the execution of the query that the parser doesn't evaluate. Such as: SELECT LONG_DESC + LONG_DESC as not_long_desc FROM @temp UNION SELECT LONG_DESC + LONG_DESC as not_long_desc FROM @temp ORDER BY LONG_DESC Will parse just fine, but throw an error from the engine.
Since its a file I am FTPing to a customer it has to be in a certain format so their system can process it. I can't have any extra data. I am sorting the data correctly putting it into my final table. But when I select all from the final table I have nothing to sort on anymore and my footer ends up in random places
This is a good explanation. 
Would it be correct to say that all the parser does is checks for red squiggly lines in your query?
It's a bug in relation to ANSI expansion. There's some information [here](https://jonathanlewis.wordpress.com/2015/03/27/ansi-expansion/) and a link to the Oracle bug in the comments section. Avoid using * in the select columns clause.
/u/420WreckingBoi - All of this, plus error handling/throwing. So make your sproc using /u/Cal1gula 's method - but just focus on creating a sproc that accepts a student, course and a startdate, and inserts a record into a table. After that start working TRY/CATCH and or THROW (especially useful might be FORMATMESSAGE because it can take variables... and you can make your error say "You cannot enroll in Course XYZ because it doesn't exist" or "You cannot enroll in Course ValidCourse, because it must be at least one month in the future") Depending on how you detailed you wanted to get, you could also do things like insert the record, but only mark it valid if it meets the parameters. Then you would have an audit trail of who was trying to register for what, for what date - and you could add courses based on demand. I'm assuming this is a homework assignment of some sort, so I won't even go as far as pseudocode - but if you need more help, just ask!
when you put data into a database table - it's not really stored in any specific order... I think that may be the disconnect here. It's stored. If it has an identity column then by default when you pull the data out it's in the order of the identity column. So it doesn't matter how you put the data in (unless you have an auto increment identity column, even then it's not really *stored* in any order). And even if you have 7 columns in your temp table (data1, data2, data3....) if you select data1 you will only have 1 result from your select, that's why i gave you a little example with just select data from @temp instead of select * from @temp And given the little information you've given, it's hard for me to be any more specific to your needs - but build your text file with a select that only grabs the data you need, and use the temp table to allow you to order it in the order that you need it to be in.
Yeah the ordering in the final table is my issue. I understand that its not ordered in any way when I select it. What's weird is that my header is always in the correct spot. Also weird is that it's been working the way it supposed to be working for about a month. Then this week my footer has been in random places
It *is* ordered when you select it, it is *not* ordered in the table. I have 0 experience with BCP but my quick glance before I decided I might understand enough to try to help showed that you can either use a table or a select to extract data. If you need your data ordered specifically, you need to use a select - because otherwise you will never have full control over the order in which your data is extracted. My suggestion remains the same. Use your temp table to help you. Add an unnecessary column to the temp table, use a select that does not include the added column and order by your added column. I'm definitely talking out of my ass because I have 0 experience with it, but it sounds like the source of your problem is SQL and not BCP, and that you expect that your data is organized in your temp table for you, which it is not. You need to tell SQL how you'd like your data presented to you, otherwise it defaults, which may or may not be in the order that you'd like it to be in (or in the order in which you ran the insert scripts)
This is more than just a SQL question. For the SQL side of it, the column you are storing your data in needs to be a Boolean data type. With that being done, you need to store the checkbox value on the webpage in a Boolean variable in your code. When making you update call to the DB, pass it the Boolean variable in the call.
I didn't use * in my select at all. And yet I run into the problem. I'm checking the page you posted, but unfortunately his PL/SQL code to expand the select isn't working. If I can get it to run, then I can probably see what's actually being selected.
Check out the bottom of this page: https://docs.microsoft.com/en-us/sql/integration-services/set-package-properties One thing to note, if the SSIS project is using the Project Deployment model (which is the default in SQL 2014 I believe, vs. the legacy Package Deployment model), you'll want to also check the *project* properties (just right-click on the project in the solution explorer to get there) and make sure the ProtectionLevel setting there matches what you set in the package properties. 
Is your query referencing a view which uses "select *"? 
Open up the SSIS package, and go to the package properties. In Visual Studio after you open the package press F4 or go to View -&gt; Properties Window. Then under the Security category you'll find ProtectionLevel (unless if properties is Alphabetical). Usually you'll see the package properties when you open it up, if you don't see it just press F4. 
Truncate the table will empty it without using the transaction log as preserve the table structure. 
No. Your comment was very relevant, as I found out lots of the entities I was selecting were views rather than tables. But nowhere in this query am I selecting *, to get that burden of 1000 columns. My main concern is whether the limit is just being counted wrong, or the network package I'm trying to retrieve is turning out HUGE because all columns are being returned nevertheless. The page /u/ziptime referred was a great start, but unfortunately I don't have enough privileges to use that package.
Another way to think about it is a parser is just a syntax highlighter for source code, and the query still needs to be compiled in order to execute. As others have said, you can have proper syntax, but when it comes to compiling, it makes no sense and fails. 
There are a couple ways to do this, depending on what syntax you used in your query. If your code looks something like: https://pastebin.com/D47nf6U9 Then it should be easy to copy the big block of values into a notepad and save it as a csv file. Opening that csv in excel will make it easy to remove columns, then open back in notepad, and paste it back in. If you have insert commands on each line, you could maybe combine the above with a find+replace on the columns you are removing, or change the strat entirely and: Insert your values into a temp table instead (find+replace your tablename) Drop the columns from the temp table Insert the full temp table into the original destination table Drop the temp table
I’ve always wondered that myself - why on earth would I create this temporary table and then immediately delete it without viewing the contents or manipulating it?
So you’re saying to just set the checkbox value to a Boolean variable on html side and have that variable passed to dB to fill the desired Boolean columns? 
Yeah I can’t just select what columns I want when extracting with bcp. It has to extract the whole table. I figured out a work around. I put my footer in a different table. Extracted both tables into separate files and then combined them all using a bat file
Why are you worried about this? Did you company not do a poc to see if all the important features of ssrs were made available? Why did you not provide the concerns before the decision was made?
I mean you need your stored procedure to take a Boolean parameter. That's step 1. Test it by executing the stored procedure with the Boolean off and with it on and verify the results are correct. As for the web, that depends entirely on your implementation and isn't related to SQL. The checkbox should have a value of 1 if checked or 0 if unchecked. Just get it to call the stored procedure with that value as the parameter.
You’ll have to do the work a bit differently, but you should be able to do more... In any case why have a single reporting system? Each tool has its benefits and unless you are doing things in SSRS that could be supported/developed/maintained easier in Cognos then you should leave it where it is baring security concerns. My Issue with SSRS over the years sadly has been its ability to support poorly done sql, as it becomes a maintenance nightmare. Most things like Cognos/MicroStrategy/et al take longer to create things in but then allow the products to be used longer not due to the tool being ‘better’ but to get it to work you have to design the data well beforehand. IMO Tableau is a good middle ground, but has odd design limitations. I just ask the developers to use whichever tool makes the most sense for the product they are producing... heck in some cases a custom tool is the best solution.
because i'm that important - i'm just the end user. I don't get to make decisions like that. 
I didn't ask if you made decisions and you still didn't answer my question as to why you didn't provide your concerns *before* the decision was made. And are you the only person that cares about subscriptions and notifications?
I don't think you need a cursor or loop for this. Try to make set updates, it will be much more efficient. 
Thanks! I'll check this out first thing. 
Thanks! Ill look into that tomorrow morning. 
None of us have any idea what "the most recent day is" in a column called "dateclosed" but you will have to probably do some kind of calculation on the line below: select entityid , bmap.bankid , acctnum , apacctnum , bankrecid , status , [the_most_recent_date] as dateclosed , * from bmap,bankrec where bankrec.bankid=bmap.bankid order by period desc
try simplifying the query, like select * from customer .. and see if it works, then keep on adding stuff till it breaks :)
Do you only want one row returned? You should look into aggregate queries. /u/notasqlstar made a good hint to begin with.
You can also create an error handling procedure to put in catch. Something like this: CREATE PROCEDURE test.ErrorHandling AS IF ERROR_NUMBER() = 544 BEGIN PRINT 'bla' END ELSE IF ERROR_NUMBER() = 3902 BEGIN PRINT 'blablabla'; END ELSE IF ERROR_NUMBER() = 15600 BEGIN PRINT 'blablabla'; END ELSE BEGIN PRINT ''; PRINT 'See message:'; PRINT ''; END EXEC test.LogError; PRINT ' ERROR Number : ' + CAST(ERROR_NUMBER() AS VARCHAR(10)); PRINT ' ERROR Message : ' + ERROR_MESSAGE(); PRINT ' ERROR Severity : ' + CAST(ERROR_SEVERITY() AS VARCHAR(10)); PRINT ' ERROR State : ' + CAST(ERROR_STATE() AS VARCHAR(10)); PRINT ' ERROR Line : ' + CAST(ERROR_LINE() AS VARCHAR(10)); PRINT ' ERROR Procedure : ' + COALESCE(ERROR_PROCEDURE(), 'Not within Proc'); PRINT ''; GO I dont think this is the best you can have tho, you have to be smart with the rollback/commit transaction useage with this one.
Its a handy place to set your keys, indexes, column descriptions, etc.
I never felt the need to. As most Columns are named in a way I know what they are for. Like "Price", "Tax" or something like this.
sent you DM
Yes, but you're kind of missing a key part here. HTML doesn't interact with databases. You must be using a programming language in your class if the requirement is to send data to a db.
Sometimes you can click on it and then wait for 5 minutes while ssms tells you there are no diagrams. Gives you a nice little break. 
As long as you have perms, it offers an easy way to make a DB diagram. If you do the work, you can source control this, in a way, by extracting the record it creates and reinserting it after the upgrade. Wait, record? Yes, it stores the diagram as a proprietary VARBINARY format with no real meaning to anyone but SSMS. The SSMS API is undocumented, so good luck making heads or tails of it. What if you DB schema changes? Well, the diagram WILL automatically reflect the change, but it will bitch about it every time you open it. Have fun, and stay off my damn lawn.
One thing I hadn't thought of, we want to run this as a SQL Job every 30 minutes, setting a password requires manual entry and isn't going to work for this application. I'm going to look into EncryptSensitiveWithUserKey as a possible option. 
He's right you will need a backend for the website to call a stored procedure. You can use asp.net or JavaScript or any other environment you choose. Welcome to full stack development.
Another approach that may work if you don't have deep SSIS skills is just to make a linked server to the foreign db with stored credentials and either do federated queries([SERVER].[DB].[SCHEMA].[DB]) or make views that reference those tables and import from them instead.
This OP. You need some sort of application layer like PHP, JavaScript, Node, Ruby, etc, etc
2 solutions, depending on how open your platform is: If you have xp_cmdshell enabled,, you can try this: drop table #tmp create table #tmp( LineId INT identity(1,1), output varchar(max) ) insert into #tmp(output) exec xp_cmdShell 'dir L:MSSQLBackup\*.bak' select * from #tmp From there, it`s relatively trivial to build the restore SQL query required. If xp_cmdShell is disabled (which it is by default), you can use SSDT to look in your input folder, put it in a variable and build the required SQL restore backup code from there.
Thanks for the quick reply- The file location on the drive has multiple files with similar names, so it looks like I'd be running this script across all .bak files in the same folder- is that true? Do you have a link to a good reference that I can use to walk through the xp_cmdshell process myself so I'm not constantly pinging trivial questions as they come up? 
I think I need more information before I can help. You mentioned that you have a Shift table. Do you also have a DutyRoster table and an Employee table? Seems both of those would be needed. 
Disregard- your advice worked perfectly. Thanks so much
Look at this script: https://gist.github.com/JasonCarter80/3e730518587e17dde8835aa6aa58bfcf Play around with lines 29-42 to get your date format right.
Are you using .Net?
This one will PRINT the all the restore commands in order, but it won't execute them as it appears I have the EXEC part commented out. This should get you 99% of where you need to go.
The xp_cmdshell version I sent will basically take every .bak and put it in a table. From there, you can use regular SQL find the most recent record using the filename, by converting it to datetime and then sorting by converted date. As for the query at hand, now you just need to find the records in the #temp table generated by the previous script, extract the filename and just sort to take the most recent one (the beauty of ISO date format is that the can be sorted in text format). Just append this small query to the script provided above. ;With FileList as ( select * ,SUBSTRING(output, LEN(output)-charindex(' ',reverse(output))+2, 999) as Filename from #tmp where output like '%.bak%' ) select top 1 Filename from FileList order by filename desc
We use TFS (Microsoft Team Foundation Server), but then we use SQL Server and are pretty an all Microsoft office. Not sure how well it would work if that's not the case for you. 
Apologies— I’m using php for majority of interactions w dB. JavaScript is only being used for client side for, validation mostly 
Along with the other tasks listed, it's also a handy drag-and-drop way to set table relationships, if you're so inclined.
When your database has scaled to the point where this is important, you'll wish you'd started early. Ask me how I know.
you might have 2 records in your output - one with whatever PK value you're looking for and another one with a NULL in one of the columns.
OK, so your question is really about how to use PHP to turn the form input into a query that updates the database. The actual database portion of this is pretty trivial.
MSSQL (2014) - sorry about that. 
What's the plan? https://www.brentozar.com/pastetheplan/ And the indexes on the table in the join?
I thought the same, but I checked the source tables and there are not any NULLs for that Column. The data is structured as Source Table: |PKCOL1 | PKCOL2 | Other_Col1 | Other_Col2| Target Table: |PKCOL1 | NVL(PKCOL2, 1) | Other_col1 | Other_Col2 | In the source, there are 11 Distinct, Non-Null entries for PKCOL2, which are all tied to the same ID in PKCOL1. The rejected records from the ETL load showed that for the violating record in PKCOL1, every PKCOL2 came in as 1, which means PKCOL2 came in as NULLs. That tells me, at the point in time of the load, the source data had to have NULLs, there is no other way. Because querying the source now shows all records have distinct data. I hope that makes sense.
Not 100% certain but you used &lt; with no corresponding &gt;, this isn't a SARGable query. Try to set a limit on key3 so the optimizer can limit the rows it needs to search.
Indexes are on for key fields, with the 3rd of for the date-relative one. We have an index reflecting that, and I have just added a hint to use it, I will report back with results.
So if I know that the field has to be &gt; 0, will that be enough to SARG it?
Powershell to rescue! 1. Install powershell module dbatools - dbatools.io/github/PS gallery 2. Run the following: Get-DbaBackupHistory -SqlInstance server1 -Database myDatabase -Last | Restore-DbaDatabase -SqlInstance server2 -DatabaseName myNewDatabase -TrustDbBackupHistory or, if you only want to involve 1 server (will be slower though) Get-ChildItem \\Path\To\BackupFolder -Recurse | Restore-DbaDatabase -SqlInstance server2 -DatabaseName myNewDatabase For further analysis you could also use Get-ChildItem \\Path\To\BackupFolder -Recurse | Read-DbaBackupHeader -SqlInstance server1 -Simple | Sort BackupFinishDate to see the details (or just omit -Simple to get a truckload of LSNs and other stuff)
Do you have access to query the MSDB database? If so, you can look at msdb.dbo.backupmediafamily.physical_device_name; that column stores the backup path, and that table keys to the other msdb.dbo.backup... tables for various backups and backup sets.
Well to rule it out since it's oracle: is there an empty string?
yes i do :) what else do you need to know? :o 
I suspect you are not getting answers because it's really unknowable. For good reasons, you don't state the company. But, do you have details on the day-to-day job functions? TAM is a fairly generic job title, which gives no insight into how SQL will be used in this role.
An index scan is not the worst thing you could have... What are your stats like? What does the plan look like? One of the tables are prefixes vw_. Is that a view? If so why for the love of God are you querying a view?
We use flyway and it works across a lot of databases.
The other stuff here is correct so far, given the info provided. However the view also adds to the query because a view is basically a subquery, and if it's a generic view instead of specific to this situation it might not have ideal indexes on the relevant tables or it might be selecting too much data.
My company asks really basic SQL questions to basically test if you know the basics. So we will provide a document that has pictures of fake tables and ask them to write out the query to certain problems. Those basics include joins, aggregates, group by
For my entry level job I had to review printed pages with a images of Access, Excel or SQL server and identify a highlighted section. I didn't have to write any SQL, but I had to review a few queries and say what it would pull. Most had one join and two to three where clauses. There were a few left joins, %, and &lt;&gt; in the mix to identify. Finally was a logic word problem to solve. The most important thing, I think, I'd to show a strong attention to detail and a curiosity. They may try to probe your desire to investigate by giving a question that is ambiguous or open to interpretation. 
Eeeeww Access. Thanks for the info though, that seems like something that I'll likely see. 
I hope this is the case on Tuesday, I'd crush it. 
I know it's ambiguous without more detail but pretty much it's at a large SaaS company where TAMs form a team of about 5 people to manage a group of accounts. This pretty much consists of helping optimizing company software to meet customer needs. The group of TAMs usually has kind of a marketing/communication person, a finance/accounting person, an IT person and an someone who knows SQL. Other than that, I they haven't really told me, but the recruiter told me that it would be explained more thoroughly during the next interview. 
&gt; Eeeeww Access. Access pays my +$2,000 a month house payment. Excel paid for the boat, but Access is paying for the house. Now, about you getting a job... 
Point taken, I’ll go over a bit of Access. 
How you know?
A company I used to work for would focus more on basic syntax understanding, like which of the following queries would throw an error. We used Oracle SQL. There were some example table questions, where you'd be asked which query would pull the results shown. Overall though we understood that if someone has a basic understanding of the logic used in database environments, the context of the database structure, type of data, and overall work environment came with a learning curve for just about anyone. They tended to focus more on general logical aptitude than specific SQL knowledge. 
I've been doing SQL for 20 years and the interview questions I get are still pretty basic. For an entry level job I'm sure it will be stuff like what's the difference between an inner and outer join.
I always wanted to use column descriptions but in ssms I never found a convenient place to view them when I would need them.
Have you tried using a different file format? I mean a file is a file. Why not use a .text or .dat? Side note - I actually ran into an issue at work because we had commas in our data and a csv being, you guess it, comma separated caused some issues. I prefer a pipe, |, delimited file stored as .text or .dat.
Will the format always end -## or might it be -##-## etc?
only looking for '##' at the end. I am finding more things out there, but it looks like most or all will be something along the lines of a function. PATINDEX with a REPLACE. :( i was hoping to keep it simple and just plug it into a cheap select query. 
SELECT CASE WHEN Items LIKE '%[-][0-9][0-9]' THEN LEFT(items,len(items) - 3) ELSE items END FROM @table assuming in a table @table with a column of items
SELECT CASE WHEN Items LIKE '%[-][0-9][0-9]' THEN LEFT(items,len(items) - 3) ELSE items END FROM @table assuming in a table @table with a column of items Not the cleanest but it works. Not sure performance at scale
I can help you with standard q? 1) truncate vs delete. 2) inner vs left vs outer. 3) having clause and where vs having. 4) keys - primary, foreign, unique etc. 5) indexes clustered, non clustered, covering. 6) b tree for indexes it's for experienced but if you know you can talk about it for indexes how data gets retrieved. 7) rdbms , olap,oltp basic diff. 8) aggregate function. 9) how would you remove duplicates from table. 
yes i know why it doesn't work in mysql the way you think it should read up about indeterminate non-aggregate columns here -- https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html anyhow, to solve your problem, you'll want to join to a subquery the way to understand this is to first write the query that will go into the subquery -- get the supplier id and max price for that supplier when you've got that working correctly, i'll show you how to get the product name that corresponds to the max price
Excel is the culprit. It normally is ☹️. Teradata will load it no problem via SQLA import when you get the formatting right. Consider a different load tool (perhaps Fastload) if you’re loading more than 50k rows or so. Remember to create your VTs with ‘on commit preserve rows’ if you are running in non-ANSI (Teradata) mode.
that worked perfectly, Thanks so much! so i'm guessing regex can be expressed with brackets within a query. Ok makes sense now. 
I never knew the Like operator was this flexible!
In other languages I would have done the regex part differently but that seems the limit in SQL from my knowledge
right, being able to add the brackets around the regex makes this really nice. Albeit it will prob be slow and a lot of overhead, but you get a lot of options. 
The more I think about it, the more questions I have. :) 1. What are the schemas of the existing tables (i.e. what columns to they contain)? 2. Does every branch have the same shifts? And if not, is there a BranchID in the Shift table that associates each shift with a branch? 3. You mentioned that the start of an employee's work week should change by one day each week, so we know what day the first shift of each week should be on. But are there any requirements for the other two shifts? Should they be spaced any particular way? One general comment I can make without knowing anything is that cursors are almost never needed in SQL and should be avoided at all costs. When working with relational databases, you want to get in the habit of thinking in sets rather than iterations. There's a well-known acronym among database programmers -- RBAR - which stands for "Row by Agonizing Row". RBAR is not your friend! 
This is considered basic entry level information?
Ok that all seems pretty standard but definitely good information for me to brush up on. It reminded me that I kind of forget the basics of indexes, which is important. Frankly I never really dealt much with oltp etc. Is that common?
Well it would be like doing a large number of lookups. Depends on the usage. In this case inside the select maybe not too bad. As part of a where cause hmm maybe not great. I was on a computer I could run the numbers. 
Do you receive this data from somewhere else or is it some poorly constrained entry?
If you are selecting an aggregate function, you need to have all non-aggregate expressions in your GROUP BY, this includes how you ORDER BY. SELECT Suppliers.SupplierName, Products.ProductName, MAX(Products.Price) FROM Products INNER JOIN Suppliers ON Products.SupplierID = Suppliers.SupplierID GROUP BY Suppliers.SupplierName, Products.ProductName, Suppliers.SupplierID ORDER BY Suppliers.SupplierID; To be honest, I'm surprised your query returned anything at all. An exception should have been thrown.
Received from another data source. 
OK the case on the select 900k rows with 1 column random data (guids with -removed) lefting as sting length - 2 with the last 2 characters being numbers. All rows selected Query cost without the case 4.94, 5370 reads, 7781ms duration, 640 ms CPU. with the case 5.03. Same reads, duration 7310ms, 8453ms CPU. So there is a CPU hit doing the calculation. Doing 100 rows. 152ms duration with Vs 2ms without. Query costs come back as 0.0038 for both.
If the row numbers are large then I would personally process this though another application. Some basic app of your preferred language as if you are using a commercial SQL engine the costs of this will be lower than licencing more cores of SQL
Well to me it is, may be you do not need to know everything but most of the stuff I stated I had studied in my under grad course and not to say that this holds good for everyone. If you see except for binary tree or indexes you will have to know pretty much others for anything involves retrieving data from multiple tables.
Here's one possible solution. Granted, it hits the same table three times, so if it's a very large table, it might not be the most efficient way. select tblProduct.ProductID, tblReceived.[timestamp] as ReceivedTime, tblSent.[timestamp] as SentTime from ( select distinct ProductID from Product ) as tblProduct left outer join Product as tblReceived on tblProduct.ProductID = tblReceived.ProductID and tblReceived.[Event] = 'received' left outer join Product as tblSent on tblProduct.ProductID = tblSent.ProductID and tblSent.[Event] = 'sent' order by tblProduct.ProductID
Awesome this works like a gem, thanks for your help! LEFT OUTER JOIN, I will definitely remember that one.
1. List of tables and their prospective columns: Employee Table - EmployeeID - Name - Type Duty Roster Table *this is the one that needs to be updated* - EmployeeID - Branch ID - WorkingShiftID WorkingShift Table - WorkingShiftID - WorkingShiftWeekDay - WorkingShiftStartTime - WorkingShiftEndTime - dutyType DutyRosterHistory Table *this contains past shifts* - EmployeeID - BranchID - WorkingShiftID - WeekStarting other tables are sort of irrelevant to this question (about the shop items / inventory) 2. Yep, so in the WorkingShift Table there's a list of shifts everyday, different times - some of them have the same duty type, some are different (e.g. the shiftID S001 is for Monday 8am - 12pm cleaning, S002 is for Monday 9am - 12pm sales). 3. Basically everything should just be shifted to the next available day for that duty (i hope that makes sense?). e.g. if the employee had a monday cleaning shift, wednesday cleaning, thursday cleaning. and there's no tuesday cleaning shift her new roster would be: wednesday cleaning, thursday cleaning, friday cleaning. i was thinking about whether i'm supposed to iterate through all of the working shifts, and find all the ones that are for 'cleaning' for example, and then put those in a set, and do the same thing for the sales shifts - but then i don't actually know how to do that... and even if i did, i'm not sure what to do after that. HAHAHA sorry about this :( i'm such a newbie with SQL. Thank you so much for trying to help ! 
create your table with id, received, and sent columns. Insert all of the product ids into it. Then update it with the desired data from the original table-- update new_table a set received = b.timestamp from orig_table b where b.id = a.id and b.event = 'received'
Hi friend, This is what I got after a little poking around. SELECT H1.name, H1.grade, H2.name, H2.grade FROM Likes L1 JOIN Likes L2 ON L1.id2 = L2.id1 AND L1.id1 = L2.id2 JOIN Highschooler H1 ON L1.id1 = H1.id JOIN Highschooler H2 ON L1.id2 = H2.id WHERE H1.name &lt; H2.name ORDER BY 1
It sounds like a Calendar table would be nice to have. Then you can join the bookings to the calendar to get how many guests BY DAY. Once powered with this information you can summarize how many guests at each hostel on each day, a range of days, etc and also how many bookings on each day. HostelID, Count(*) as bookings, sum(GuestsNBR) as GuestsForTheDay
Ah! Thank You. Regarding the WHERE clause logic: It seems to help to eliminate the duplicates, (eg: Cassandra, Gabriel vs. Gabriel, Cassandra), by ensuring that only one of the two duplicates for each pair will return (one string will be greater than the other, and vice versa - always)? Is this a correct interpretation? Neat trick 
&gt; (I can modify the tables if necessary) Hopefully OP has DDL privileges. I've found that a calendar table and a table with every number between 0 - 1 million are great fact tables to have on every database. [Here's] a way to create a quick and dirty calendar table in MySQL, although one of the answers listed would work without creating any new tables, if preferred.
You'll need a subquery to find free spaces in a hostel on a given day and you'll need find a minimum of that for a given range. All hostels with that minimum over the given number is what you want.
This Stack overflow question is in the same vein and a good place to start. https://stackoverflow.com/a/8048494 Replace the select with one that fits your criteria and make sure you have indexes on the columns you are filtering on to keep the trigger from adding too much overhead to INSERTs as the table size grows.
So...you can do this with triggers. Please for the love of your sanity don't. Triggers are so dirty and this is a business rule and this should be enforced on the front-end. You should be able to do something like: Select count(1) from table where car_assigned = true and customerid = some_number. You get the idea. Then if the count meets your threshold don't allow them to do stuff.
Yes you are correct. Here is a related example to keep in your mind: https://en.wikipedia.org/wiki/2016–17_Premier_League#Results_table Look at last years premier league table. Notice that teams can't play each other, so ARS-ARS has a null field ("-"), and so forth with the other teams. That diagonal line where x=y is a line of reflection. The bottom triangle has all possible matchups, but the triangle above the line also has all the matchups, just that these are the complementary (return leg) matchups. Essentially, each triangle has all combinations, and the entire table has all permutations for choose 2. Setting y &lt; x would just pick the bottom triangle, giving us all combos, discarding duplicate permutations. 
Rank(score) over (partition by tree). Guessing something like that will help 
Hmm, MySQL doesn't seem to support that. Could you give me an example as to what it would look like in MySQL?
Probably not the most efficient way to do but should work: select b.id,a.mscore,a.tree from table b join (Select max(score) mscore,tree from table group by tree)a on a.mscore = b.score and a.tree = b.tree 
what is the max price of each product? that's what your query returns?
Would you be able to point me to a tutorial? My biggest problem is that I haven’t learned the code that should go inside the trigger. So while I understand that the correct syntax would be CREATE TRIGGER maxfivecars AFTER INSERT ON Cars FIR EACH ROW BEGIN I am at a loss what comes next and although I have been guessing - the code does have to be precise so guessing doesn’t help much, most often just prevents me from inserting any further data. If you could show me some sources for the correct code (I have now spent 24 hours but the internet is mostly full of examples that are not applicable to me) I would be forever grateful. Even the book I am referring to only shows me the create/drop syntax and no further than that. Where @ is used inside the code I actually still don’t understand what function it has
[You should link your cross posts.](https://www.reddit.com/r/mysql/comments/79cqzb/mysql_query_to_select_all_hostels_with_at_least_x)
This looks decent - http://www.mysqltutorial.org/create-the-first-trigger-in-mysql.aspx My point was to not use a trigger as that's the worst possible solution to this problem. I'm guessing this is for a class and you don't have a choice. I really wish they would have given you a more useful application of triggers. Your code above looks close but you actually want to use a BEFORE trigger. Think about the life of a transaction(thing that contains one or more sql statements like this). Begin transaction -&gt; Execute sql -&gt; Before trigger fires -&gt; statement changes data -&gt; after trigger fires -&gt; end transaction. If you try and do an after trigger in this case the data has already been inserted and it won't do you any good. You'll likely get an error for trying to change dirty data especially if you're using commitment control. Here's another link - https://www.w3resource.com/mysql/mysql-triggers.php#MTBI
Actually the assignment specified that we should look into check statements, but I had no luck with that either. I was creating a trigger where I would select count(*) but I don’t really know how to make it check whether one specific ID has more than 5 different items, but I am working in it at the moment. Thanks for the link I will check it out
Oh. That would be so much cleaner and is basically the same thing. Check this out: https://www.w3schools.com/sql/sql_check.asp If the data is in another table for a check constraint you'd have to define a function and refer to that in the constraint. - https://stackoverflow.com/questions/13000698/sub-queries-in-check-constraint So constraint or trigger would work. Still think the UI should handle this. :)
You can do it with a single pass through the table(s): * Find the lowest and greatest values for each pair of names * Group by the result of these functions * Use the having clause to verify there are exactly two for each pair SQLite doesn't have least/greatest functions, so you can use min/max instead: with pairs as ( select 'Bob' n1, 'Amanda' n2 union all select 'Amanda' n1, 'Bob' n2 union all select 'Bob' n1, 'Jack' n2 union all select 'Jack' n1, 'Gaben' n2 union all select 'Gaben' n1, 'Jack' n2 union all select 'Gaben' n1, 'Dee' n2 ) select min(n1, n2), max(n1, n2) from pairs group by min(n1, n2), max(n1, n2) having count(*) = 2 min(n1, n2) max(n1, n2) Amanda Bob Gaben Jack [SQL Fiddle link](http://sqlfiddle.com/#!7/9eecb7/3037/0)
Ok so can I ask you something that I’m having a problem with? Maths, basically. The way I see it, CHECK can prevent you from adding data if it’s outside of a certain range, i.e., let’s say I don’t want any salary above 500. But with ID I am happy for it to go as high as needed, as new users are added, I just don’t want to have the same ID show up on a certain row more than 5 times. I hope this makes sense. I’m kind of burnt out at the moment and coursemates seem to be too!
No that makes sense. With check you would have to add a function call to it to work. Here's how I would approach this problem: 1. Forget about checks and triggers. Ask yourself. How would I COUNT the number of cars a customer has bought? 2. Next create a trigger that will fire BEFORE the INSERT will add a new record to make sure the customer doesn't have 5 cars already.
I've tried experimenting with CREATE TRIGGER MaxFiveCars ... ... BEGIN IF ((Select CustomerID, Count(*) from Cars GROUP BY CustomerID)&gt;5) &lt;--- this is how I would normally count how many cars the customer already has, but I'm guessing there's something wrong with this syntax (I add THEN SIGNAL SQLSTATE etc). Am I on the right track? Also, thanks so much for your time. This is frustrating and you're being really helpful
1. You need to specify what kind of trigger. Triggers are BEFORE or AFTER. 2. You want a where not a group by. Group by will group by what's in your select. You should know the customer and use that. I.e. select count(*) from table a where column = value. In this case value is the customer ID from the row the trigger fired on.
But that’s the thing, I want to have one trigger so it checks all IDs and checks where the ID count exceeds 5. Was that not the correct way of thinking? Otherwise I’d need to create a trigger for each customer which is lengthy
I don't think you understand how triggers work. You define triggers on a table and when the trigger event would cause the trigger to fire it fires. There are two sets of permutations in which you can define a trigger. BEFORE/AFTER INSERT/UPDATE/DELETE For example. Some pseudocode. CREATE TRIGGER BEFORE INSERT OR UPDATE ON TABLE A. do stuff. End.
&gt; Can it be done? Yes, But unless you have experience designing databases, you're probably going to get it wrong the first time. It shouldn't stop you from trying though. Here's a [link](http://databaseanswers.org/data_models/customers_and_orders/index.htm) to a generic customers and orders database diagram. It will help you understand how the data is organized in a database. That link also contains a lesson explaining more about relational databases. you can find it here: http://databaseanswers.org/tutorial4_db_schema/index.htm 
The easiest way would be to use access. I think the correct way would be to create a database in SQL server and have your visual studio application connect to SQL server. 
Thank you. I know it’s not going to be easy or take 5 minutes to make. Even with html, css and php. I been doing it over 10 years + I am still learning it. I always like a challenge. This one will make or break me. Any more tips please send my way Thank you
I will only split it in to two applications if I am going to use multi connections. (Future features). But as it is just 2 people on one computer. I don’t think localised all in one application is best. Unless it make it easier to to program it is own SQL server. 
I added ... instead of those conditions. I understand how to write a code for trigger but not inside of it. I am trying to find a code which would start after trigger syntax (after begin) and would count the number of cars that a user already has and would raise an error if they already have five. 
OK, a couple of disclaimers: 1. I've never worked in MySQL; I only have experience with SQL Server. I think there are some minor syntax differences between the two, so it's possible this may not work in MySQL exactly as written, but it's mostly just to give you an idea of how this can be done all in one step without any looping. 2. I'm making a few assumptions in this query that may or may not be true in your data. The first is that the days of the week are stored as numerical values (e.g. Monday = 1, Tuesday, = 2, etc.) so that you can easily determine the order of the days and use functions like min(). If you don't have them stored that way, you might want to consider adding a column for that. The second is that there is only shift per day for each duty type. If that's not true in your data, the query would have to be tweaked slightly. 3. If you're a newbie, this may look overwhelming, but I couldn't come up with a simpler way to do what you were describing. :) update DutyRoster set WorkingShiftID = NextShift.WorkingShiftID from ( select DutyRoster.EmployeeID, DutyRoster.WorkingShiftID as CurrentShiftID, isnull(tblCurrentWeek.DutyType, tblNextWeek.DutyType) as DutyType, isnull(tblCurrentWeek.minWorkDay, tblNextWeek.minWorkDay) as NextShiftDay from DutyRoster left outer join ( select DutyRoster.EmployeeID, DutyRoster.WorkingShiftID, CurrentShift.DutyType, min(NewShift.WorkingShiftWeekDay) minWorkDay from DutyRoster inner join workingshifts CurrentShift on DutyRoster.WorkingShiftID = CurrentShift.WorkingShiftID inner join workingshifts NewShift on CurrentShift.DutyType = NewShift.DutyType where NewShift.WorkingShiftWeekDay &gt; CurrentShift.WorkingShiftWeekDay group by DutyRoster.EmployeeID, DutyRoster.WorkingShiftID, CurrentShift.DutyType ) tblCurrentWeek on DutyRoster.EmployeeID = tblCurrentWeek.EmployeeID and DutyRoster.WorkingShiftID = tblCurrentWeek.WorkingShiftID left outer join ( select DutyRoster.EmployeeID, DutyRoster.WorkingShiftID, CurrentShift.DutyType, min(NewShift.WorkingShiftWeekDay) minWorkDay from DutyRoster inner join workingshifts CurrentShift on DutyRoster.WorkingShiftID = CurrentShift.WorkingShiftID inner join workingshifts NewShift on CurrentShift.DutyType = NewShift.DutyType where NewShift.WorkingShiftWeekDay &lt;= CurrentShift.WorkingShiftWeekDay group by DutyRoster.EmployeeID, DutyRoster.WorkingShiftID, CurrentShift.DutyType ) tblNextWeek on DutyRoster.EmployeeID = tblNextWeek.EmployeeID and DutyRoster.WorkingShiftID = tblNextWeek.WorkingShiftID ) as tblNextShiftDay inner join workingshifts as NextShift on tblNextShiftDay.NextShiftDay = NextShift.WorkingShiftWeekDay and tblNextShiftDay.DutyType = NextShift.DutyType 
If you know how. can I have a list on which order. That I can start with. If that’s not a problem with you
Stop. Use a spreadsheet or MS Access. 
Explain why please. 
This sounds too big for you. Starting with Excel will allow you to get up and going quickly. You can then verify what data needs they have as they start using it to enter data, do look ups, create reports. This will eventually the basis for your data model, UI, and reports. Get familiar with these topics while they are using the spreadsheet: - data normalization - entity framework - crystal reports 
I understand your concerns and why you say start small. I learn from hard projects. With striping down coding and learn what’s part is what. Not from ground up. I made a program in VB from help from programs I stripped down and snippets from a few programs. as same as website making. But thank you for your help anyway. I keep you posted on my progress. 
Are you trying to design a database from the ground up? I wouldn't recommend doing that. SQL is a declarative programming language. So you tell the database what type of data you're looking for, where it's located, and then you select, update, delete, etc. SQL is able to do this because it does all of the work behind the scenes. The server has a number of algorithms that it uses to optimize query speed. So if you're not skilled in math, or don't know relational algebra, you'd likely have a hard time creating a database. This is something that you'd do in advanced programming classes. You probably wouldn't find good resources for this on videos online. I recommend just using MS Access or a SQL server like MS SQL Server, Mysql, postgres, or sqlite3. From there, you can just design UI to interact with the database.
Good luck to you then. Also, look into WPF for binding EF objects to UI. Stick with MVC pattern if you want to stay sane for now.
I am not trying to reinvent SQL. I want to use the program to fit their needs. So I am currently using Visual studio 2017. To create a program/application so my friend can use for his business. I can make a simple address book with name, address, number, and email. With ID most basic thing you can use on sql. And calling and displaying and updating the data in a form. I am not trying to be rude or stuck up. I just wanted help from here. As reddit I can get a quick reply from. Just want a list/steps to follow. Then with trial and error. I figure out how to do it. 
Sorry if I sounded rude and stuck up. Wasn’t trying to come across rude. I just needed help to get me started maybe have a list or steps to follow. With trail and error I learn best that way. As I done with other projects I done in the past. 
You can use sqlite3 with visual studio. It's fully functional and free.
If you're entry level, you're likely going to get the old database that has to be maintained but nobody wants. One if you're lucky, and all the queries begin with Qry_a, qry_a1, qry_b2 etc..
For the business needs you described, I think a MS Access database is more than enough. You can design the forms in Access and link it to a back end sql server if you want a little more challenge. Ms access with Vba can do some wicked stuff. 
I do have an alpha version of the application software. Front end. Made in excel VBA
OLTP is very common, it's basically the database that does the work. Think of any application accessing a database frequently, like Reddit or Stack Overflow, that would be the OLTP. The OLAP is the warehouse typically where data changes less frequently. Based on uses the DB will face, that is when you architect from resources and files to indexes and table / data structure. 
Get number of subscribers per year/SubscribesTo (for your nice stacked bar chart); get averages of commenters and live stream user audience per author and for the grand finale compare the above two.
This is less of a MySQL question and more of a word press / site hosting / administration problem. 
How precise does this need to be (to the hour, minute, second)? Here is some code that is precise to the minute and gives output in decimal hours. It works for shifts that wrap around midnight, but does not work for shifts that are longer than 24 hours. -- table of numbers, 1440, the number of minutes in a day with tally as ( select row_number() over (order by (select null)) - 1 as n from (values(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) cross join (values(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) cross join (values(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) c(n) ) -- enumerate the minutes and mark them as day/night shift , m as ( select cast(dateadd(mi,n,0) as time(0)) as [minute] ,case when n &gt;= 360 and n &lt; 1080 then 1 else 0 end as is_daytime from tally ) -- your data, note the datetimes are cast as time , cte as ( select id ,cast(StartDTM as time) StartDTM ,cast(EndDTM as time) EndDTM from (values (1,'2017-10-30 12:00','2017-10-30 20:00') ,(2,'2017-10-30 20:00','2017-10-31 05:30') ) a (id,StartDTM,EndDTM) ) select id ,count(nullif(is_daytime,0))/60.0 day_hours ,count(nullif(is_daytime,1))/60.0 night_hours from cte join m on (cte.StartDTM &lt; cte.EndDTM -- shift that starts and ends same day and cte.StartDTM &lt;= [minute] and cte.EndDTM &gt; [minute]) or (cte.StartDTM &gt; cte.EndDTM -- shift that wraps days and (cte.StartDTM &lt;= [minute] or cte.EndDTM &gt; [minute])) group by id
This should help. https://stackoverflow.com/questions/3162389/multiple-ranks-in-one-table/
Thank you. I ended up just creating another table that contains max values for each tree and doing lookups from that table. Probably not as straightforward, and there’s definitely a 5-7ms performance hit for writes, but now read is super fast!
There are many vendors in the market which provide 3rd party software to repair SQL database. But before purchasing any such paid software, you must try an easy manual trick. The manual trick doesn't require any paid software or any other cost, and there's no harm in trying it. You can use Microsoft SQL Server Management Studio here. You need to run a couple of database console commands (DBCC): DBCC DBREPAIR and DBCC CHECKDB. Refer to this article for more information - http://wordpress.semnaitik.com/2014/02/22/repair-sql-database/ I hope the shared information would be helpful here. Thanks and best regards.
Add some NULLS to the select SELECT field1, field2, NULL FROM ... UNION SELECT NULL, NULL, field3 FROM ... Not sure if that is what you are after but will work
Create a temp table with the dates you're interested in and left join it with the book table. 
You can join sys.extended_properties to sys.tables and sys.columns. Then, query the 'value' column in sys.extended_properties where name = 'MS_Description'. Here are the joins, shamelessly borrowed from StackOverflow... select st.name [Table], sc.name [Column], sep.value [Description] from sys.tables st inner join sys.columns sc on st.object_id = sc.object_id left join sys.extended_properties sep on st.object_id = sep.major_id and sc.column_id = sep.minor_id and sep.name = 'MS_Description' where st.name = @TableName and sc.name = @ColumnName
UPDATE: To expand a bit on my initial question, yes the left half of the query is itself a view. And there are four parts to the full key There is a parent-child relationship between Key1-Key2-Key3 and Key1-Key2-Key3-Key4. The left-side view filters out "bad" cases of the parent. Then with the resulting Key1-Key2-Key3-Key4's we have to go get history for Key1-Key2-(&lt; key3 )-Key4. This was causing my scan, even though we have an extra index on Key1-Key2-Key4-Key3. That index was apparently not being used. FIX: Added a WITH (INDEX (IX_Key1243)) to the join and decrease run times by 75%. Yes, our next change will be to re-write the full query without the view, but in the middle of production the above fix got us back on schedule. Thanks!
 SELECT player.player_id , player.voornaam , player.achternaam , gamemodes.gamemode_name , platform.platform_name , cars.car_name , rank.rank_name FROM player INNER JOIN gamemodes ON gamemodes.gamemode_id = player.gamemode_id INNER JOIN platform ON platform.platform_id = player.platform_id INNER JOIN cars ON cars.car_id = player.car_id INNER JOIN rank ON rank.rank_id = player.rank_id
This will be our step 2. The short-term fix was an index hint.
That was fast. Thanks dude!
Do it in a test environment? Only way I can see is either that or actually checking the average length of your data and doing the maths. That will only return the average size tho. Sadly not familiar with teradata...
If you declare what the beginning and ending datetime of each shift is it should be pretty simple. DECLARE @begginingAMshift datetime SET @begginingAMshift = CAST(CAST(CAST(GETDATE() AS date) + '06:00:00.000' AS nvarchar(24)) AS datetime) -- do this for the beginning and end of each shift. Then you can create just a few case statements to see whether the StartDTM and EndDTM are only within the first shift, second shift or both. Is an ISNULL to show 0 hours if the shift is only within either 1st or 2nd. 
 - Find if shift crosses a boundary. If it doesn't, identify which category it belongs to and count the number of hours. - If it does' split it into two time ranges and do the above for each. - Think of what will happen in some corner cases, such as shifts longer than 24 hours or if a shift happens during daylight savings change.
So I've tried you code, and it works, except for the platforms, it just shows zeros there. I've looked at it but it looks exactly the same as the other ones. I've tried changing some things without any luck, so I was wondering, is there anything wrong with that part?
thanks for the update... i was gonna say, it has to be your data ;o)
For $10k retainer, i'll do your job for you. My rate starts at $245/hr. 
 ORDER BY CASE WHEN myDate IS NULL THEN CAST('9999-12-31' AS DATE) ELSE myDate END DESC
Thanks!
Not sure if you've seen the edit, it's much easier: &gt; `ORDER BY IFNULL(myDate, '9999-12-31') DESC` 
It analyzes your table to learn it's skew and composition. Next time you call it there will be no need to scan it to understand it, teradata will have a better idea how to optimize for it. That's the basics 
awesome, thank u for the explanation.
Try a valid date far into the future! Like 2199-12-31 instead of invalid date 9999-99-99
Good luck with that, bud. 
Also I’ve noticed in the past that sometime you need to redder to the username running the query. I.e if I ran the collect stats I’d need Collect stats on paratwa.table whatever for it to actually work on temp tables.
don't need luck, I know how to do my job. 
You are a legend, this works perfectly out of the box. I love it. This is the kind of thing I can never wrap my head around writing. I can read that, and follow it and understand it, but trying to actually build CTE's, I always fall over. I have no idea why! Thanks again!
Need to use group by
&gt; As you can see, Martin appears twice. because you said DISTINCT for name ~and~ amount what you want is to sum -- hint hint-- the amoiunts by name
A new table which holds "room id, day, occupants" would help you in this query.
Great start and interesting project. You likely want to keep each todo_id unique and iterate them sequentially. You also may want to add another column to the Todos table for time_created. If size definitely becomes an issue, you can recycle "completed" todo_id's or find another solution. Good luck and keep up the good work.
Got it thank you!
Not only GROUP BY p.PATIENT_NAME, but also SUM(b.CHARGE_AMOUNT) if I understand you correctly.
Worth bearing in mind that Teradata does not keep these stats up to date. Recommend to run the statement again when the data demographics change (number of rows, spread of values, etc) by more than 10%-ish. As this is a VT you may not need to worry about this too much. Depends how much the volume/distribution changes during the VT's lifetime. 
Think of views like aliases to a query you want to reuse. They are only evaluated when you query them, and will get reevaluated every time. There is a concept of materialized views, but MySQL does not implement this feature. You can simulate a materialized view though, by simply creating a table that stores the necessary data and refreshing it either periodically or immediately on any sort of update (`INSERT`/`UPDATE`/`DELETE`) to `products_out` and `products_in` tables with the use of triggers. [Some more information on both approaches is available here](http://www.fromdual.com/mysql-materialized-views).
That's what I thought so too. Thank you!
Damn that's clever and insanely above my average thinking of SQL. Haha! Would you go onto this with views? 
For example you can wrap the averages queries into views and use those for the last one.
[removed]
You can definitely use SQL for this, but is SQL the best choice? You can also use Access for this and you can use SQL with Access, and you can use Access with SQL over ODBC via passthrough queries, but I don't know if it's the best choice. If it were me, being that this data is already in Excel, you could use some countif() formulas and use a pivot table based on your raw data to show what's missing on each hostname. That's what you're trying to accomplish, right? For each hostname, you want to know if x, y, and z are present? There's plenty of ways to do that in Excel, including vlookup, partial string matching, index/match, etc. Excel is very powerful for this sort of thing, provided that you have less than a million rows (2^20 exactly). In addition, Excel can also remove duplicates based on a selection of one or more columns. So if you've already got it into Excel, just de-dup your data right in the application by selecting all columns that you wish to be distinct as a group, then Data tab --&gt; Remove duplicates. 
It's about 363,000 rows. I'm happy to use Excel, but I'm far from a power user, so I only have an elementary understanding of the topics you're referring to. I'm familiar with countif(), but not creating pivot tables. Any instructables you can link me to would be most helpful.
Sounds like FOR XML PATH might be the way to go. Check out this link: https://stackoverflow.com/questions/21760969/multiple-rows-to-one-comma-separated-value 
You'll want to use the STUFF command with FOR XML PATH(''). if ( object_id('tempdb..#test') is not null ) drop table #test; create table #test ( [id] nvarchar(3) not null , [seq] int not null , [value] nvarchar(1) not null ) ; insert into #test ( [id] , [seq] , [value] ) values ( '001' , 1 , 'A' ) , ( '001' , 2 , 'X' ) , ( '001' , 3 , 'Z' ) , ( '002' , 1 , 'H' ) , ( '002' , 2 , 'D' ) , ( '002' , 5 , 'E' ) , ( '003' , 0 , 'P' ) ; select id , STUFF( ( SELECT ';' + CONVERT(varchar, t2.[value]) FROM #test t2 WITH (NOLOCK) WHERE t.id = t2.id ORDER BY t2.id FOR XML PATH('') ) , 1 , 1 , '' ) AS value_list from #test t group by id
This is great. Thanks very much.
Perfect. Thanks!
Well, if you don't care for the tools/actual queries, I'd download a trial of Tableau Desktop, load your data (text file or point to your Access DB), define a few formulas for convenience (where the dash is in the package, what's a general package name, package version), drag hostname to rows, general package name to columns, version to text and voila - you have a nice pivot table. It's really that quick.
Concat was never an option for row data unless you use a pivot(hard to do with an undetermined number of rows to pivot). For row data prior to sql server 2017 you need to use the other commenters post regarding for xml path. However, it you find yourself using sql server 2017 in the future, string_agg does what you thought concat did.
There is an r/excel subreddit that would eat this stuff up. But if you truly have to use SQL, you can definitely do it and tailor it to your needs. [See this example that I mocked up.](https://i.imgur.com/HLWr4Jq.jpg) Here I've constructed a temporary 2-column table, then from that table I'm doing a selection with if-then logic looking for cases when, per hostname, the package name is not at the version I'm looking for. The version you are looking for can be edited to be more specific than just looking for the major version which is what I'm doing. The % represents a wild card (oracle lingo). In the result, if it found the correct version, that's the output. Otherwise it is null which indicates that the hostname does not have the version specified within the case statement. Honestly though, Excel might be a lot easier for this task. I'd post this question over there because you'd have to have at least an intermediate understanding of SQL in order to massage the data in the way that you seek. 
Summary: This might work for a few situations, but there is a lot of edge cases and failure conditions that are not handled which will cause the queue to fail to run jobs or run in a failure state infinitely. A few things of note that I found on a quick read through: Major Things: * There is no check or storage of whether the previous step finished successfully. Meaning if your @DependsFromID IS NOT NULL but the job with that id failed, the second step will run anyway, which shouldn't be assumed. *Example: If you had a data migration/load followed by a cleanup job.* * Storing passwords in plain text is especially bad practice **\&gt;always&lt;**. I can't seem to find where it's used for on the look though, but an opportunity none the less. At minimum store then as MD5s and compare against them. Better yet, store a randomly generated salt with owner and combine that for an SHA1 or higher hash. Example: *** IF(@providedPassword = (SELECT HASHBYTES('SHA1', password + salt) FROM owners WHERE username = @username)) SELECT 'Password Correct' ELSE SELECT 'Password Incorrect' *** * Do not do validation in a TRIGGER, use stored procedures for DML (INSERT/UPDATE/DELETE) instead * **DO NOT USE** cursors in a TRIGGER, ever. This one was literally made me shudder. If you had to use something, at least do this: *** SELECT @ScheduleID = ScheduleID, @Weekday = [DayOfWeek], @ScheduleTyp = ScheduleTyp, @Intervall = Intervall FROM inserted *** * The @dayTime @waiting combo will prevent jobs from running if there are long running schedules. You should change the logic to DATEADD the minutes/hours onto today then do the comparison and filter the outer CURSOR; this will also drastically increase the performance. * The #TmpHistory OUTPUT works, but SCOPE_IDENTITY() instead in this case as OUTPUT will return values even if a statement fails and is rolled back. Which this also doesn't have. Minor Things: * Don't use SELECT * in production code, even with EXISTS. It's just a good habit to keep away from. * You can use SET @variable = 'something' instead of SELECT @variable = 'something' * [Personal Preference] You can SET or SELECT multiple variables in the same statement * There is no TRY...CATCH in sp_CheckToRunJobs, meaning if something fails, it might be difficult to troubleshoot * IF ISNULL(@Daytime, '') = '' is far more legible as: IF @Daytime IS NULL Edit - I should add, this is by no means exhaustive. Just what stood out on a skim read.
Would an ORM, like [Hibernate](https://www.hibernate.org) not take care of this for you?
I found a similar problem here in the UK, a lot of shady looking jobs that found me. I guess if you research the company before hand, check LinkedIn etc. that might help, and if you get to interview just ask some questions that would put you at ease with trusting them?
Can you see an execution plan on Oracle? Skimming over the query it doesn't look like any major calculations going on that would slow it down much. It could be missing indexes on tables causing you problems which the plan might highlight. I've only used SQL server so don't know what the Oracle offering is unfortunately...
Sorry I cannot send the execution plan. Here are some notes. I need a random sample from the query and a specific number of rows from that sample. In the left outer join, acct_num only exists in acct_ind if the id_one is not null
I'm on my way out the door so this is a bit of a hack but it should get you to the goal line: UPDATE T1 SET T1.inventory = T2.inventory FROM #TEST T1 JOIN #TEST T2 ON T1.retailer = T2.retailer AND CONVERT(date,getDate()-2) = CAST(T2.[Date] AS DATE) WHERE T1.inventory = 0 Would be more solid if you have a key value for retailer to join on. Also not ideal to cast/convert in a join but if you run this once per day it probably won't be *too* bad.
Google aggregates a lot of data from sites now. Just try googling ' SQL JOBS IN CITY'. As a side note, and ymmv, I've landed every job in the last decade via Linkedin and a recruiter. I've had other opportunities I've applied for but then decided not to take, but so far each job has ultimately been received by that method of a recruiter finding me on Linkedin. 
You could stem it down and eliminate some things until it runs faster, then add in those tables one by one until you see the slowness occur. Then I'd start looking at indexes. You want to view the execution plan and see if it's doing full table scans. When you find the culprit (which is probably just one table), note the carnality of the table. Is it super big? Consider adding indexes or multi-column indexes for things you need fast access to. If you can afford disk space to save time, indexes are the easiest and most obvious things to start tinkering with before anything else. Oracle performs a lot of in-memory operations but sometimes if the disk throughput isn't good, it can affect queries. Sometimes large databases are on SANs and sometimes spread among different logical volumes or even different buildings. Sometimes a test box will be assigned 10% of a CPU, for example. There's so many factors to consider. Hardware, network, database config, memory management parameters, statistics/histograms on tables, badly organized transactional tables that resemble swiss cheese, etc. 
Try something like: SELECT [CustNbr], RIGHT([ExpirationDt], 4) as [YEAR], RIGHT(CAST('0' as varchar) + CAST(MONTH(ExpirationDt) as varchar), 2) as [MONTH], [ProductNm] FROM [TABLE1] I strongly advise against the use of 'YEAR' and 'MONTH' as column names, but this should work.
Thanks a lot man, I will try this!
Dice.com has served me very well... Indeed is alright. I'm a BA...in IT obv lol
Hey, just tried it. Everything seems to be working except for the [MONTH], where for both of the as, there seems to be the same errors : ERROR 22-322: Syntax error, expecting one of the following: !, !!, &amp;, ), *, **, +, ',', -, /, &lt;, &lt;=, &lt;&gt;, =, &gt;, &gt;=, ?, AND, BETWEEN, CONTAINS, EQ, EQT, GE, GET, GT, GTT, IN, IS, LE, LET, LIKE, LT, LTT, NE, NET, NOT, NOTIN, OR, ^, ^=, |, ||, ~, ~=. and ERROR 202-322: The option or parameter is not recognized and will be ignored. Any idea why it's not working?
Post your resume on Linkedin and Dice. If you have a local SQL group of any applicable (to you) type, go to a meeting and talk to people; tell them you're job-hunting. We've all been there.
I have had good luck with dice.com. My recent jobs have come off clearancejobs,com but you need a security clearance to have account there,
Did you get this to work? i'm also stuck on this concept :( 
I used to have a book called something like the SQL cookbook that did MySQL, T-SQL, and PL-SQL
Indeed, Dice &amp; LinkedIn
I'm pretty sure they are trying to get you to use one of these.. cross apply, outer apply, apply. Probably using a cte to pull the apply info. 
Have you looked at the LAG window function? SELECT Retailer, Date, Item, Sold, Inventory, LAG(Inventory) OVER(PARTITION BY Retailer ORDER BY Date) AS PrevIntentory FROM Test You should be able to figure it out from here.
What area are you from? 
Not sure which client or language us use, but this worked in sql server management studio 2012 with mssql. Perhaps you should fiddle a bit with datatype, or check the syntax/arguments for the CAST function in your language.
Am on my phone, so won't write to whole thing for you, and it's study after all, so you don't want to be given the answer, do you ;-) Question 1 - Look at the LEFT function, and the LEN function. You will be able to do it with a combination of those. 
we haven't learnt left and len so I don't think its that complicated, for the first one I have so far Select * From Patient where pat name like ' _ _ C" I'm not sure how to do the second part of it making select second last character a and descending order 
 &gt;Select * &gt;From Patient A good start. &gt;where pat name like ' _ _ C" You could do this or consider two substrings. Give it a go and post back. I don't think that table alone will allow Q2 query to be created. &gt;I'm not sure how to do the second part of it making select second last character a and descending order Look at order by
we are required to do ' _ _ C' so that means selecting a second last letter as a is also requried 
Try looking up like %
The only thing I have control over is the query itself. 
You can use the substring() to select a certain position in a string and use reverse() to reverse it. So you could do the following. Example: select 'abcdddbab', substring('abcdddbab',3,1) , substring(reverse('abcdddbab'),2,1) as an statement: select * from patient where substring(patname,3,1) = 'c' and substring(reverse(patname),2,1) = 'a' order by 2 desc 
I hope you just forgot to paste the update statement. &gt; Update T1 should be above your querty Secondly your query is not really deterministic since what happens if a customer has no inventory for 2 days in a row? A quick dirty fix would be. UPDATE T1 SET T1.Inventory = ( SELECT top 1 T2.Inventory FROM [dbo].[TEST] T2 WHERE t1.retailer = t2.retailer and Inventory &lt;&gt; 0 order by date desc ) FROM [dbo].[TEST] T1 WHERE Inventory = '0' The subquery now simply selects the top 1 row for the retailer when the inventory was other then 0. Since this is ordered by date it will always selects the last known Inventory.
Dallas Texas 
If the user happens to be in Oracle, I would enclose the substring() function with a lower(), as well, seeing as the data is in all uppercase.
In a LIKE clause, '\_' does single character wildcarding, and '%' is a wildcard for any number of characters. So if you wanted the second from the last character to be "a," you could do something like '%a\_' You should be able to do both questions without using functions at all. Here's an example with another table: SELECT TOP 5 [country].[country] FROM [country] WHERE [country].[country] LIKE '%a_' ORDER BY [country].[country] DESC; country| ------- VIETNAM| UZBEKISTAN| URUGUAY| TURKMENISTAN| TOKELAU| 
Valid point, collation could be CS as wel. Better to be save then sorry.
I would personally say this is a job for dynamic SQL, just spitballing off the top of my head.
I've done some things with it in the past but only for fun, and generally with the advice that I shouldn't be doing it for much else. So I really don't know where to go or what to look for. 
Ask away man. I was a GP consultant for 4 years, there's probably a ~10% chance I was involved in your GP implementation. The database tables are horribly designed (artifacts from a time where everyone used CHAR fields and the table names were no longer than 8 characters long...), so get used to referencing the MS docs *dozens of times per day*. Much of the business logic is contained stored procedures (which are encrypted by MS, again a huge hurdle to get through) and a service called "eConnect", but the docs are good for researching this as well. You're about as far deep into MS territory that you can get so basically almost all of your questions and answers can be solved by a) a piece of MS software or b) a MS webpage somewhere. I kind of miss it, but then I think of disasters like the Analytical Accounting module and I think to myself ".NET isn't so bad"... Stay away from the GP web services like the plague. You have been forewarned. edit: * Bookmark this blog: https://victoriayudin.com/ Trust me. * There used to be a site called "GP table reference" but it's gone now. RIP it was my most used resource. I guess someone is making an equivalent here: http://gptables.azurecurve.co.uk/ and this site is also *ok* but not great http://dyndeveloper.com/DynModule.aspx * eConnect node reference is useful for data integration: https://msdn.microsoft.com/en-us/library/ff623839.aspx * You will possibly need to access Customer/Partner Source pages (downloads, manuals, etc.): https://mbs.microsoft.com/customersource/northamerica/GP/learning/documentation/system-requirements/dynamicsgpresource Hope this helps!
I am a big fan of "The Sun Also Rises"
I'm not a big Hemingway guy.
I'm very much enjoying [The Pheonix Project](https://www.amazon.com/Phoenix-Project-DevOps-Helping-Business/dp/0988262592) right now. It gives you a good view of DBA's (as well as other IT professionals) in the context of an entire IT department.
Great...we are investigating options for moving away from GP in the near future, so my preference is to grant access and have someone create the queries for me, charging an hourly rate. I work in inventory management and the data will mostly be related to that, so we can likely avoid the financial module! Know anyone who is willing, able, has verifiable credentials and can dedicate 3-5 hours/week, depending on necessity? Don't be shy about nominating yourself!
The art of sql, to say it's a book about how to optimize might be underselling the content, but that is definitely part of it. It's not a cookbook like a lot of the books on sql are, but instead seeks to help you make more informed choices in your SQL code and implementation. It also has a more conversational writing style so it was a welcome break from the dry technical works we have to read a lot as devs. Here is a link to the publisher's site for the book: http://shop.oreilly.com/product/9780596008949.do
LIKE '%a_' seems like a better solution than substring(reverse('abcdddbab'),2,1)? 
Got it on audiobook—worth it.
Celko - SQL For Smarties Date - Database Design &amp; Relational Theory Hay - Enterprise Model Patterns Ambler - Refactoring Databases 
I would advise you to read Franz Kafka's The Metamorphosis, The Trial and The Judgement. 
All tools have a time and place. Cursors, triggers, etc are all fantastic options as long as you know when to use them. When you need to build the where clause dynamically, this is one of those times where I would say dynamic sql is perfect for it. Just look into the caveats, restrictions, and risks. On a side note, there's a video by the team Group By with Brent Ozar with a SSRS tips and tricks. The last 10 min of the vid may have a solution that also works for you. I'll see if I can find it later, I've linked it around here before too.
This is a good read, thanks. 
I'll check it out. You're actually touching on a point I haven't started trying to solve for yet. We may have a calculation (Name: ABC) which looks at the datediff() between col1 and col2, except for Client XYZ it needs to be between col3 and col4, even though it is the same calculation name. So ideally I was hoping to do the datediff() between parameters as well as building the where clause. Right now my concept of using the params table seems to be functioning well, but I'm concerned with cases where I need to say IN () vs NOT IN(). I know I can come up with something ugly where I copy the code twice and have one piece with an IN() and the second piece with a NOT IN(), and have the top piece only executing for Clients 1-5, and have the bottom piece only executing for Clients 6-10. Just feels like a lot that is slightly beyond my skillset at this time. On the positive side of things it won't be out of my skillset for long... or I won't have a job :)
Tbh I find free online sources to be better than any book I've ever picked up, they're more targeted and the community frequently pulls it apart if something's not right; as long as you're at a level where you can separate the BS sources from the good ones then this is the way to go. You get to know the decent websites and well trusted names with time. If your using SQL Server, the best starting site is www.sqlservercentral.com - if you're a noob, w3schools. I can't help with the other flavours of SQL though. 
in German
This was such an amazing book. What were some of your largest takeaways from it?
Very new to IT and still working through the book so I haven't digested much, but it's definitely taught me to keep the right hand informed on what the left is doing. 
Gray &amp; Reuter - Transaction Processing
Worst case scenario, you can make a ton of semi redundant SQL and get things out there and then eventually improve the process. Now that I'm thinking of things... This is just a random idea and it may or may not work or need tweaking, but I feel like this is also a great opportunity for views. It could also be a thing that generates a data dump to pull from and includes Client ID's, so you can filter the report based on their ID. I just got done doing a SSRS thing for most of our reports, it hits Active Directory and queries a table when it finds you as a user and sees what permissions you have and then it filters the results based on those permissions. It actually works surprisingly well, despite how janky it is.
Consider the Lobster and A Supposedly Fun Thing I'll Never Do Again are some of David Foster Wallace's more accessible (and funny) works, so they're a great way to get into him.
Man you guys are a bunch of smart alecks ;)
I don't see the point of using views, as everything needs to be in 1 table at the end of the day, and at best I'd have to be unioning views and then maintaining the view code base in addition to each view. At the moment I think I will have (1) sproc per calculation, and each calculation will be a dynamic loop, with all of them dumping into a common end table in the same schema so that Tableau (or SSRS) can pick the data up for visualize/consumption. There are a lot of challenges with Tableau when it comes to SQL integration than there are with SSRS, but things are prettier in Tableau. 
Joe Celko's "SQL For Smarties" It's awesome. 
You've not experienced Kafka until you've read it in the original Klingon.
Looks interesting! 
The faceless bureaucracy aspect would actually be pretty relevant for my current job. 
The faceless arbitrary bureaucracy aspect of Kafka's works would actually be pretty relevant for my current job
Nope, you can do your own homework 
this isn't homework, it's an exam lol
Awesome. Thanks for the help and encouragement!
There was a lot of multiple choice. A lot of those questions seemed like they were written by HR or someone who didn’t really know what they were asking... 
Doesn't surprise me at all. Programming interviews are often ridiculous.
It's interesting to see all the different solutions. I think C1 is going for a WHERE PATNAME LIKE '__c%a_'
[SQL Server MVP Deep Dives vol. 1](https://www.amazon.com/SQL-Server-MVP-Deep-Dives/dp/1935182048/ref=sr_1_1?ie=UTF8&amp;qid=1509600516&amp;sr=8-1&amp;keywords=mvp+deep+dives&amp;dpID=51CBJGiDh0L&amp;preST=_SX218_BO1,204,203,200_QL40_&amp;dpSrc=srch) [SQL Server MVP Deep Dives vol. 2](https://www.amazon.com/SQL-Server-MVP-Deep-Dives/dp/1617290475/ref=sr_1_2?ie=UTF8&amp;qid=1509600516&amp;sr=8-2&amp;keywords=mvp+deep+dives&amp;dpID=51iXY7MrJgL&amp;preST=_SX218_BO1,204,203,200_QL40_&amp;dpSrc=srch)
Jose Portilla has a well reviewed [course](https://www.udemy.com/the-complete-sql-bootcamp/) at Udemy, while he does use PostgreSQL I think he tries to keep the content geared towards more generic SQL than PostgreSQL specifics. If you're looking for a place to practice on specific engines, then [SQLFiddle](http://sqlfiddle.com) can be very useful. A significant portion of trying to learn SQL can often be setting up the server, which is almost an entirely different area of expertise outside of 'learning SQL'.
I am a full stack developer and there are some significant performance issues we run into with our ORM. Some new/younger developers i work with have never had to manually generate and call a SQL statement from the controller. I remember the days before ORMs where 100% of the DB calls were happening manually in line in code. The right answer is somewhere in the middle. 
I've seen many of the problems outlined in the article first hand (via lazy programmers with EF mainly), but I still feel there's a place for a level of abstraction beyond writing direct SQL for all of my data access code. The sweetest spots I have found for this to date have been LINQ to SQL, and more recently Knex.js. Both are like being able to write 'near' SQL, but with the ability to keep it dynamic and NOT have it become a monstrosity to maintain. That said - people who write bad code via an ORM, are likely also going to write bad code via direct SQL. They just have to put more effort in with the SQL route. Jobs I have worked in the past thought that an ORM precluded the need for a database developer, but that is not at all the case. In fact I would argue it's the opposite - you need it that much more when you have people thinking in an 'object oriented' fashion when writing your data access code. (Why can't I execute 'ToList' in a 'ForEach' loop again?!? or... What do you mean my 'Includes' balloon the query to 978 columns?)
I find that CTEs help a lot with keeping complex queries manageable. There's also the option of breaking them down into multiple steps in code when things become complicated. 
I never understood what an ORM would help. It's just another complex unnecessary layer. 
There are plenty of reasons ORMs are useful - change tracking, lazy loading of related objects, load on first use, automatic object property - table column mappings. That said, they do lock you in to the ORMs fixed query methodology. Table per type inheritance (abstract base class) is hard, and you have to be careful about how you query data if you are concerned about performance. 
One thing I would add - and it is an adjunct to the Dapper edit in the article - is that this presumes that you are using your code to define the database, Code-first in EF parlance. Granted, this is the way that ORMs are most typically used, but, some people do write their database schema first and then write their mappings after the fact. 
I'm 100% on board with this. Every time I see a post on SO using one of these ORMs I'm thinking to myself "How is this any easier than SQL when you have to learn SQL in addition to this ORM?". Great post.
Look up the venn diagrams for SQL joins, it really helps you visualizes what is going on.
[removed]
This is the best site I've found to learn SQL -- no nonsense, just syntax. https://www.techonthenet.com/sql/joins.php
My company supports over 10 different database types. Most other companies in our space do the same. 
This is pretty nice! So if you are having to join more than 2 tables does the same principles apply like a normal join of two tables? 
Yup, joining 10 tables works the exact same as joining 2. One thing to be mindful of is when you start combining inner and (left/right) outer joins, especially when considering whether to put your predicate/filter condition in the where clause or as part of the (outer) join. Here's a good thread on SO demonstrating this: https://stackoverflow.com/questions/7332203/t-sql-additional-predicates-on-joins-vs-the-where-clause
Think of it like this: You have a set of data (columns and rows). You want to add another set (columns and rows) to create a combined set. Then JOIN is which column you choose to match between the two. Then you have a 3rd, combined SET of data. If you do another JOIN, you are creating a new data set.
(This guy knows his stuff.)
Yes, you follow the same principle comparing the third table (`table3`) to the previous dataset (`table1` + `table2`) then the fourth table (`table4`) to the previous dataset (`table1` + `table2` + `table3`). SELECT table1.field1, table1.field2, table1.field3, ... table2.field1, table2.field2, table2.field3, ... table3.field1, table3.field2, table3.field3, ... FROM table1 INNER|LEFT|JOIN table2 OM table1.shared_field = table2.shared_field INNER|LEFT|JOIN table3 ON table1.shared_field = table3.shared_field
I have [this](https://imgur.com/gallery/huvEH) printed off and laying on my desk all the time. Just a nice visual reminder.
Your left outer join is looking for (among other things) a NULL; this mandates a table scan. And, out of curiosity, how much faster is the query without the ORDER BY?
Same here. Our code is a mix of ORM and direct SQL. There are times that the ORMs we use just can't hit the performance we need.
You wouldn't really need to send them? Does the query do an index scan somewhere? This subquery: (select acct_num from acct_ind where end_dt &gt; current_date or id_one is null) Look like it is not sargable, I can't really say by experience since i have close to none. If theres a scan, what columns does it scan? Would adding an non-clustered index here solve this issue? If you check this out: https://dba.stackexchange.com/questions/132437/sargable-where-clause-for-two-date-columns quote from the link: If you're feeling lucky, and you're obeying all the ANSI SET options in your connection strings, you could add a computed column, and search on it... ALTER TABLE [#sargme] ADD [ddiff] AS DATEDIFF(DAY, DateCol1, DateCol2) PERSISTED CREATE NONCLUSTERED INDEX [ix_dates2] ON [#sargme] ([ddiff], [DateCol1], [DateCol2]) SELECT [s].[ID] , [s].[DateCol1] , [s].[DateCol2] FROM [#sargme] AS [s] WHERE [ddiff] &gt;= 48 **End quote.** This issue looks more or like it wont be possible without having some sort constant value instead of the variable date values. Also check the first answer in the link. 
http://www.sqlservercentral.com/articles/Stairway+Series/119933/ this is advanced tsql but the previous are also linked (i am also learning so got a shitload off links atm)
One of the big advantages is that mapping rows to objects is boring and tedious and people don't want to do it. Most people's first experience with not doing that is via an or.
Kind of ironic that one of the benefits of an ORM is that you write less repetitive SQL. :)
SQL builders didn't need to be part of it, but they became for better or worse. Btw, recursive CTEs are _amazing_ and even more magical:-)
holy smokes this is going to be VERY handy 
Your wording is confusing can you provide a table diagram?
PostgreSQL is probably the most standards-compliant database out there, so PostgreSQL *is* SQL
Yeah I can't parse this string into EN. But I think what OP is looking for is a calculated field.
I have a table [Table A] that comes through an XML each day. userid, fieldID, fieldName, Answer Then each week I get a file that is used to create a SP. There is one field from the SP output that goes with that table. What I would like to be able to do is if a userid is already in the table for field id 11 - then the SP would use the answer from table A. And if it's a new userid it uses what's on the file.
hmm, hearing doess that offer me some relief. i guess i can stick to bolt!
[removed]
Assumptions - TableXML is results from XML. TableA is existing table. You are joining them something like TableXML AS X Left Join TableA AS B on A.userID = XML.UserID You want results from TableA if it exists or TableXML otherwise. You could do an ISNull on every column choosing TableA's value over XML OR you could use case statments CASE WHEN A.UserID IS NOT NULL THEN A.UserID ELSE XML.UserID END AS userID, CASE WHEN A.UserID IS NOT NULL THEN A.fieldID ELSE XML.fieldID END AS FieldID, ..... The difference? If TableA has a legitimate Null value and you use ISNULL on each column the XML value will appear. If you use the CASE statement on the joined column, then you have full control over which set is used.
I'm think using a CASE. All the data will be end up in the same column after the SP runs. Would this logic work? CASE WHEN A.UserID NOT IN TABLE A THEN File Data, ELSE Table A Date WHERE fieldID = 11 