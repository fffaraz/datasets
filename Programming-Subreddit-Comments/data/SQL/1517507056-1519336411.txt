reformatting your query for readability -- SELECT ta.personfullname , ta.personnum , p.homelaborlevelnm3 , p.homelaborlevelnm4 , ta.clientusername AS 'Edit By Username' , f.functareanm , CASE WHEN ta.aeventdate is null THEN 'Punch' WHEN CAST(ta.eventdate AS datetime) = CAST(ta.aeventdate AS datetime) THEN 'Review' ELSE 'Edit' end AS 'Typecheck' , CAST(ta.enteredondtm AS smalldatetime) AS 'Date Entered' , CAST(ta.eventdate AS smalldatetime) AS 'Event Date' FROM vp_timecardaudit ta INNER JOIN functarea f ON f.functareaid = ta.functareaid INNER JOIN vp_personv42 p ON p.personnum = ta.personnum WHERE ta.personnum like '50068915' AND ta.tmshtitemtypeid like '-1' AND CAST(ta.eventdate AS date) &gt;= 'Sep 25, 2017' -- change the dates above and below to get a different date range AND CAST(ta.eventdate AS date) &lt;= 'Oct 01, 2017'
To add off of what others have already said, you can spend a lifetime learning SQL and still not know it all. There's nuances between different SQL versions, Postgres vs MSSQL vs MySQL, etc. I would recommend getting started with Udemy and make sure to do your own side projects as well. The key to learning with your own projects is to come with a project that you have no idea how you'll build it from the start. You may never actually finish it to completion, but if you stick with it, I guarantee you will learn something.
Haven’t had a chance to test it yet, but the logic looks sound. I had tried nesting the main query inside the count, but just kept tying myself in knots. Thank you very much for your help. Much appreciated. 
Yeah seems like it. I'm kinda new there so.. they have the SSN directly in the table. So querying the tables directly I can see everything. That function only works in out reporting system, so that's where it's used.
Basic queries without having to think too much about "how do I do this part....", a couple weeks. More complicated queries like you'd actually write in the business world with lots of data cleanup, formatting, and aggregation... a couple months. This one is a bit trickier because in the real world you're forced to solve problems you'd never encounter or think of on your own. Crazy queries where you really need to start worrying about optimization and execution plans and such... that's really only going to happen on-the-job. The datasets you build on your own just won't get big enough that performance problems, and their fixes, are obvious. 
Yeah, it would just be a couple of nested queries. Glad you learned something, though! Did you get sorted with your original issue?
&gt; I would store them in a table variable and use that. dude... subquery!!
it shouldn't run indefinitely with a distinct but ok. as for the first option my worry with it is that it may return more because it may not care about the group by. or at least it didn't seem to with the sample you provided. 
my god, thank you. I saw all CAPS and eyes crossed.
You need 3 servers. It should go Dev-&gt;QA-&gt;Prod. In your case, it appears that server A is your production server, which is fine. What we've done at my company is have separate databases on the same server that are cloned across the other servers; allowing for development in similar environments. The first database is an import database, we dump all raw unprocessed files there. The second is a "Processing" database that does all the staging tables and stored proc running etc. The third is a reporting front end, where we hook tableau into, so it's the "cleanest" and easiest for clients to look at. New code should be developed on your Dev server, and then run through a parallel environment on your QA server. You then take that build and deploy it to your production server. The production server is still doing file ingestion, but you're able to manipulate all the underlying framework from another server. Ideally IT would be responsible for deploying code on the production server and developers wouldn't have access to prevent rogue work. Look into source control options that can deploy changes across servers (BitBucket). Hope that helps. 
The problem with your approach, to me, is that I cannot QA or DEV anything until I am working with production grade data on a proper reporting server -- which should at no times ever go down because IT needs to fuck around. Currently our third server is our dirtiest data... but on the prod server (rightly so) we cannot create tables, etc. IT does not seem to understand the need for us to be working with clean data that is refreshed regularly and are making a big deal about what an effort it would be, and not understanding why we need it. Meanwhile I am building front end applications that need to be QA'd by business partners who expect to see the right answer (i.e. clean data.) So things are a bit of a fuck show. Server B is my "report server" at the moment and I object to calling Server A prod because it isn't a transactional system. If anything I think we should be doing the file imports on B and then pushing the data to A for QA/Development for front end technologies, and then C can be used by IT for their processes.
ODBC, I have tried using a different syntax now. substring(xml_request,charindex('presentingIllnessCode=',xml_request)+22,4) as Presenting_Illness but returned with following error OLE DB or ODBC error: Requested conversion is not supported.. The current operation was cancelled because another operation in the transaction failed. 
I think it is great that you are learning to code and want to start with SQL! As for the timeframes it really depends on where you are trying to take this. If you want to be at the "Derek Zoolander Center For Kids Who Can't Read Good And Who Wanna Learn To Do Other Stuff Good Too" level, then maybe a couple of months fumbling around. If you want to be a developer or DBA then years of work on a daily basis will be necessary. You'll need the experience of solving real world data problems and this is the only way - you won't get that from a book or online course. I like to use the analogy of learning a foreign language. Programming is like that in the sense that it is not intuitive (especially if you haven't done it before) and you won't just figure it out on your own. How good would you be at learning a foreign tongue after 2 months? 1 year? 10 years? It's all so dependent on your competency, time, work, etc. When I hire I expect someone with &gt;=5 years direct experience with SQL to be decent. At least I would hope by that time they have worked through enough problems to be confident and proficient. Hope this answers your question and gives you some food for thought. Start learning and see where you are after 3 months
That's what I would suggest. If you have a dedicated "File" server that you can pull off (be it a mapped drive to both locations) then you could import data to both the dev, qa, and prod servers. We use the "Prod" server as a location to store the input files and have a similar location on the dev server, we just download copies of the files from wherever they're hosted to there to test on. If you're using backup/restores across the servers, the datasets should stay mostly the same. Just cascade down at the end of sprints to get the latest and greatest set of data. It's why having a dedicated database of files is easy, you can just restore that across servers. 
Pretty much the same doesn't cut it when we do weekly imports that can potentially change years of history. I need production grade data to develop on and QA with. If the buisness expects a number to be X and its Y then I am not going to be able to tell if my process is off or if the data is bad.
I think I was able to get it figured out. I joined the table to its self by saying Select (e2.number - e1.number) From first_table e1, second_table e2 where e1.date = (e2.date-1)
You can do something like WHERE trunc(period) BETWEEN '1980-01-01' AND '2015-01-01' and it will pull anything for those dates. At least, this works in Oracle - and I'd imagine it would work in Postgres the same way. If you didn't want January 1st, 2015's rows then this would need to be changed to '2014-12-31' 
I don’t pretend to be an Oracle wizard (I work primarily in MSSQL), but what about making sure the tables in the background are properly indexed, and truncating the query and adding deadlock checks..? Like I said; I am not an Oracle wizard and I do not pretend to be at all. However, those are a few guidelines that I feel are good to implement regardless of syntax and structure. 
Main things to focus on when optimizing for Oracle: * Joins * Ordering * Grouping * Unnecessary "distincts" Things that usually don't matter much (unless they are affecting a join): * String manipulation * case statements * math in general For quick wins: * look at every use of "distinct" in your query. Distinct is a costly operation and often a sign of poorly structured data or lack of understanding. Get rid of them where possible with better filters or by understanding table contents better. * "order by" during a query is only needed when you're doing a top-n type operation or in a window fuction (row_number() over (order by...) ) Otherwise, ordering should be done in a downstream application. Joins: * Run an explain plan. Look for occurances of "nested loops..." These are a real killer in bigger analytic queries or when indexes are not available. Google around for getting rid of nested loops in oracle (usually a use hash hint does it, but sometimes you have to change join criteria.) * Speaking of join criteria, you'll generally want to avoid the pattern of from a join b on a.something = b.something and b.something_else = (a subquery or case statment or something gnarly.) and replace with a pattern like from a join ( select stuff from b where (a subquery or case statement or something gnarly.) ) c on a.something = c.something * Joins are inherently costly. Try to get the two sets to be joined as small as possible before joining. In a perfect world, Oracle does this for you during execution, but you can force its hand by subquerying/filtering before your joins. 
That'll do it. You might also look at the LAG function. Here's a similar example: https://stackoverflow.com/questions/7766212/how-tocalculate-percentage-increase-decrease-in-oracle-database/7767976
Do you still have the encryption key? You probably need to specify is location on the file_key_management plugin to decrypt the data. See: https://mariadb.com/kb/en/library/data-at-rest-encryption/#encryption-key-management
but that's not sargable
Thanks for your input! The main use of SQL in my job f.e would be to fix up older databases that we're logged before an update of a certain app per se. So I guess i wouldn't have to do take up crazy amount of live transactions or something like that.
Our company is in a similar situation. They are able to make simple reporting schema changes in our dev environment and then propagate those changes through our QA environment but when all is said and done, their dashboard development takes place in prod. They also use the dev and qa environments to test appliance upgrades prior to applying to prod. Granted, we are talking about a total of ~6 servers, 2 in each environment. Given the nature of healthcare data, all of the data files we load for our clients daily/weekly/monthly is done in production on a separate server from our production application. If you're doing in house report development and are having to share resources then one side has to give... That all said, it probably depends on the nature of the data. If there is PHI/PII then A+B are both prod resources. If it isn't data that needs to be protected legally (non-identifying machine data) then A sounds like prod and B sounds like QA with C being leveraged as a dev resource. That said, your dev resources need to align with your production resouces (not necessarily physical hardware, but schemas and general architecture) and it sounds like that just isn't the case right now which is a serious pain point.
Have you looked at [plpgsql](https://www.postgresql.org/docs/current/static/plpgsql.html)? From what you describe it seems the appropriate.
Well that's why you have QA test in the QA environment. You should be testing processes and using expected results, not based on what the data itself is telling you. For example; if I populate a data set with 2x2 I know the answer is going to be four, any other number is wrong. Developing with known data sets and expected results is the best way to do it. 
Perfect, thank you. I will look into that further. Which one would be more efficient? I'll be running this on a large database with millions of records.
What version of AX? And have you deployed the OLAP cubes? This is done from within the AOT in AX 
I get your frustrations, but realistically people here can only give you advice on possible alternatives or solutions. I guess I removed it from my previous post, but no one here can understand the full scope of your issues or frustrations and no one here can make your IT team bend or your C level exec make your IT team listen to you.
Get an AX consultant to help ... 
AdventureWorksDW2016.dbo.DimProduct is what I was looking for! Thank you!
&gt; [MS SQL] &gt;I'm Using Access why? for the love of god why? 
Will this work? https://www.mssqltips.com/sqlservertip/1638/using-sql-server-meta-data-to-list-tables-that-make-up-views/
We have a SQL DBA consultant that I have been chatting, just finding more out as a I go along that is new to me, so I asked here as I wasn't sure what it specifically belonged to.
We have backups, but I have no idea when they did the most recent. The specification calls for RAID 1 so thats what we use, but if they swap , drives and do not shut the computer down properly, makes it hard. 
OLAP is a way of analyzing data (Pivots and stuff). An OLAP server facilitates this. You use MDX to query an OLAP server. MDX is SQL-like
Is MDX like or similar to DAX?
*Probably* the LAG method, but like most sql performance questions, the real answer is "it depends". When in doubt, try it both ways and check the plan. Here it is against a test table with no indexes and no parallelism: --Create TST table -- 50000 IDs, each with ... -- 731 DTs (2016,2017), for a total of ... -- 36,550,000 test records create table TST as select ID, DT, round(dbms_random.value()*10,1) VAL from (select level as ID from dual connect by level &lt;=50000) cross join (select date'2016-01-01'-1+level as DT from dual connect by level &lt;=731) ; ---------------------------------------------------------------- -- Self join method ---------------------------------------------------------------- explain plan for select T1.ID, T1.DT, (T2.VAL-T1.VAL) DIFF from TST T1 join TST T2 on T1.ID = T2.ID and T1.DT = T2.DT-1 order by T1.ID, T1.DT ; select * from table(DBMS_XPLAN.DISPLAY()) ; Plan hash value: 2898795495 ------------------------------------------------------------------------------------ | Id | Operation | Name | Rows | Bytes |TempSpc| Cost (%CPU)| Time | ------------------------------------------------------------------------------------ | 0 | SELECT STATEMENT | | 36M| 1172M| | 226K (1)| 00:00:36 | | 1 | SORT ORDER BY | | 36M| 1172M| 1518M| 226K (1)| 00:00:36 | |* 2 | HASH JOIN | | 36M| 1172M| 1010M| 96102 (1)| 00:00:16 | | 3 | TABLE ACCESS FULL| TST | 36M| 592M| | 14030 (1)| 00:00:03 | | 4 | TABLE ACCESS FULL| TST | 36M| 592M| | 14030 (1)| 00:00:03 | ------------------------------------------------------------------------------------ Predicate Information (identified by operation id): --------------------------------------------------- 2 - access("T1"."ID"="T2"."ID" AND "T1"."DT"=INTERNAL_FUNCTION("T2"."DT")-1) ---------------------------------------------------------------- -- LAG method ---------------------------------------------------------------- explain plan for select ID, DT, LAG(VAL) over (partition by ID order by DT asc) - VAL DIFF from TST order by ID, DT ; select * from table(DBMS_XPLAN.DISPLAY()) ; Plan hash value: 2276309396 --------------------------------------------------------------------------- | Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time | --------------------------------------------------------------------------- | 0 | SELECT STATEMENT | | 36M| 592M| 14030 (1)| 00:00:03 | | 1 | WINDOW SORT | | 36M| 592M| 14030 (1)| 00:00:03 | | 2 | TABLE ACCESS FULL| TST | 36M| 592M| 14030 (1)| 00:00:03 | --------------------------------------------------------------------------- The bottleneck on most systems is going to be reading the disk. So in this case, LAG is probably better because the self join requires to table scans.
Thank you for the very helpful and detailed explanation! I really appreciate it!
Well... OLAP cubes are a SQL Function; deploying the OLAP cubes for AX is done from within the AX AOT (Application Object Tree) and there are some intricacies in doing that.. Service accounts, database access etc. etc. So you'll likely need someone that has done this before. 
MDX is the query language of OLAP. DAX is the query language of the Tabular Model, which is a simpler form of OLAP
Hey, thanks! Yes, I looked into that a couple of times. Then I started wondering if it's even possible to do these kind of queries and thought I ask around. EXPLAIN ANALYZE looked really good, but I can't get it to work. Same with PERFORM. It seems to come down that these return bools, so for every successful query I'd have to run the same one again to actually get the data, if that makes sense. Wasn't too sure about my approach.
Have you reinstalled the other drive again? It seems like there something using the resources of the machine that isn't allowing SQL to use them. I'd start with task manager and figure out what's going on with the base OS and hardware before trying to troubleshoot SQL.
you have a product table, right? do a LEFT OUTER JOIN from products to sales vwalah!! ;o)
One problem with modeling many:many structures when the relationship is naturally one:many is it leads to assumptions that can be invalidated later. For example, say I am writing a query against a many:many structure but want one:many results, I may assume the relationship is actually one:many, maybe based on an existing constraint. Another developer comes along and cleverly realizes he can leverage the existing many:many structure for a new purpose (like change tracking). They remove the constraint, start adding new data, and quietly break existing queries operating on a previously valid assumption.
check memory levels and windows event viewer. sql tends to cache data in memory to the computers max limit; unless you restrict it. search ms kb for that. sql limit maximum memory computer stuck in startup repair makes me think you may be having a hardware failure. How old is the pc? How many users? How often do they use it?
They took a backup last night because there are nightly backups scheduled... ...right?
Sql consultant isn't going to know AX unless they know AX. It's not transferable knowledge innately. Source: Dynamics consultant for 5 years
I think it costs $500, but also recommend this. 
Specifically the product table has to be the main table and the join is to the sales table, just in case this great advice was not clear. You’ll see every product regardless of entries in sales table. 
Yeah when I had to do it we were ms partners so we might have got a really good rate. This was a few years ago and hopefully I didn't just blow up their spot.
Because that's what my work is using. No choice in the matter. 
I couldn't figure out how to use the case expression here - I ended up doing pretty much what you recommended, but only the one nested if. 
I’m not sure in SQL, but from reading the description it looks like it operates like MS Excel. &gt; IF((condition to test),(value if condition is true),(value if condition is false))
I'm sorry for your horrible working conditions.
The middle picture says it's mysql, so yes. https://dev.mysql.com/doc/refman/5.7/en/control-flow-functions.html#function_if
The IF() function takes three parameters: A conditional statement, a value to return if true, a value to return if false. IF(condition, result if true, result if false) 
Are you saying you need to get a subject’s age at specified date? Which rdbs are you using?
They both have 3 arguments though... IF(NEW.age &lt;16, NULL, NEW.did) IF((NEW.age-New.dyear)&lt;16, (NEW.age-16), NEW.dyear)
Yes, I think that could work, too, adding a column for Age At Follow. But I'm having trouble doing that as well. I'm using Access.
Not the first time I've heard that. It's usually some NoSQL technology is blazing fast and SQL has no future. Then answer is no. Relational databases are not going anywhere because they're built on fundamentals. Everything else is compromises in some way. SQL isn't going anywhere because it's the standard for relational databases.
Every few years there is a scare. VNext of Major Database say they have eliminated the need for tuning a database so DBAs are going the way of the dodo bird. Hasn't happened. The engines are getting smarter, but not "replace coding and logic and knowledge of data interactions" smart. Relational databases are definitely moving to the cloud, hyperconverged, with huge scaling potential... but they aren't going away. On top of that, more and more people are collecting more and more data. If you are a SQL DBA or Dev that isn't learning new stuff, yeah, you're going to fall behind ... just like every other tech position. It takes pretty minimal skill to set up backups, and alerts for data integrity. But there's still tons of places that implement the green-text on the index advisor without thinking, set MAXDOP to 1 and leave CTFP at 5. Don't get me wrong, I'm super excited about moving stuff to Azure, or even CosmosDB, but theres pretty much always going to be a need for people who know relational database design and administration for those companies that can't move to the cloud for whatever reason. HIPAA/PCI/Govt compliance, sheer volume of data, secrecy and risk aversion are going to keep people away from the cloud for good reason. Bottom line, keep upping your skills, learn some new stuff, and engage in the very exciting time of computing that we are in! 
Was an internal auditor for a large airline company. Literally 2 dudes chilling out because they maintained an important legacy cobol system and it was cheaper to just maintain the system then attempt a redesign.
Can he mathematically prove something superior to relational algebra? There is just simply no better way to represent relationships in the real world at this time.
What does SQL have to do with backups? 
I think it was an example of a task traditionally performed by DBAs... in today's world, many of those tasks are becoming easier as the tools advance. Being able to do that no longer sets you apart, or makes you particularly valuable to many organizations 
My answer too. I go back to the principles of normalization as the starting point for almost everything I design, even if it's not database-centric. It helps me stay focused on separation of concerns.
CIND 110 gang???? 
https://www.brentozar.com/first-aid/
CASE WHEN LENGTH(RTRIM(TRANSLATE(test_str, '*', ' 0123456789'))) = 0 THEN 'All digits' ELSE 'No' END
are you unable to test it? '1' 'a1' 'a11' 'a11a' 'a1a' 'aa1' etc 
If I had a nickle for every time I heard that.. I'd have a good 35 cents or so by now. It's not going anywhere anytime soon.
CASE WHEN TRANSLATE(test_str, '', '0123456789')= test_str THEN 'No digits' ELSE 'At least one digit' END
That is not what OP is asking. That looks for pure digit string. OP wants to know if at least one digit exists in string.
In general, this kind of remark means that the person just doesn't understand at all SQL.
This kind of makes sense and might work. I will give it a try.
There is an equivalent: relational calculus. It's more lonely typed and easier for integrating databases.
As a biologist, when IT said they would move to NoSQL my answer was 'absolutely fuckin not' 
Short answer no.
When he makes a claim like this ask him what it would be replaced with. Hell I’d love it if there was such a thing, as that’d mean there would be an even better way of mining data. It’s not the language that is valuable after you work with data but your experience with information. I can teach anyone SQL, but I can’t teach people to understand information.
Our warehouse server has around 20tb across a dozen or so DBs, with tables over 100m rows... server is definitely beefy, but we hammer it with 2TB/s of IO for hours at a time... we couldn't likely afford to put that load in the cloud.
Maybe join the table on itself using a.id1 = b.id1 and b.id2 = (a.id2 + q). Might not work with the first row. Maybe change it to a left join.
So the data already exists and you're adding a new column? And each id1 has its own sequence of dates? update mytable t1 set prevdate = (select max(t2.thedate) from mytable t2 where t1.id1 = t2.id1 and t1.id2 &gt; t2.id2);
There is a specific feature for that called "window functions" or the "over" clause. SELECT ID1, ID2, Date , LAG(date) OVER(PARTITION BY ID1 ORDER BY ID2) PreviousDate FROM tbl What it does is: making sure you are only getting data from the same ID1 value (=PARTITON BY ID1), then ordering that (ORDER BY ID2), then returning the value from the `date` column of the previous row (LAG(date)). 
Very interesting, I had not heard of this and will have to investigate.
that did it! thanks for the solution, and more importantly the explanation :D
You might want to have a look at a site of slides about such things: https://www.slideshare.net/MarkusWinand/modern-sql It also contains the OVER clause in various ways.
Windowing function
ah, correct. my mistake. how about the upper to lower comparison? CASE WHEN UPPER(input) = LOWER(input) THEN TO_NUMBER(input) ELSE 0
Do LAG and LEAD exist in oracle? Really easy way of doing this.
Nice code.
I'd like to know how to do this in SQL server
REGEXP_LIKE might help you? Just do a [0-9]* and you're good to go
Well if you wouldn't have nested substring and instring 430 times... 
LAG() and LEAD() were added in SQL Server 2012, so if you are using 2012+ use those in a window function as explained in the other comments here. If you are on an earlier version you'll need to look at self joining using a CTE.
I'm trying to help from memory. no access to a server from home this weekend. i've done similar comp tests with unicode defined fields that don't contain unicode and written functions that do the work the long way. numbers have no upper comparision so AB = ab, but A1 &lt;&gt; a1 
Single handedly crashing an Oracle server is still on my bucket list.
Yes but your won't match on all letter strings either because you are comparing an upper case to a lower case. It actually only matches on all digit strings, again not what OP is asking for. OP wants ABC = no Abc = no Abc1 = yes A1bc = yes A1b2c = yes 123 = yes Only the last one works with your code.
Hopefully someone WITH access to a server can test this this weekend?
Typically this error indicates poor fundamentals. :)
Your post is nonsensical. What are you trying to achieve. You absolutely need to explain the data and relationships for anyone to have an idea if its right or not. Does it execute and run and give you a response? Does it does this in a reasonably short amount of time? Then it's ready for production.
One row exactly
Thank you. If you don't mind, please tell me if there's a way to write the exact same logic with more explicit syntax, so that I may understand it better. Much appreciated!
I'm sorry. I'm looking for an explanation of the syntax mostly, and I thought this would be sufficient. If it's not, NBD and I'll figure it out eventually. Thank you for taking a look at it anyways :)
&gt; RIGHT OUTER JOIN It's just taking everything from the right table and giving you any of the values that exist or null from the other table?
Since it only has one row, this is why not having a join specified is not problematic. If you wanted to write it with more explicit syntax you could write cross join dataoptions as DOPT. You want to be careful in using cross join.
The death of the PC has been predicted every few years since 1980. In 2000 I heard an passionate prediction that open source would kill Microsoft within five years.
&gt; , DataOptions AS DOPT My trouble was that part. I didn't understand what that does, syntactically.
The query I posted is a snippet out of a stored procedure, but same concept, they are parameters. This is part of a larger stored procedure but I just took the part that is performing badly.
So one thing you could do is break the joins up into a sequence. So you have pc, c, and cg. You can prepare that set into a #table and throw an index on it before doing your next join. Break things down and it should get faster. I hope.
I have tried a few different combinations of separate queries into a temp tables (and table variables depending on size). Nothing seems to be doing it. I'm thinking my next step is to change the way data is maintained. The only reason these tables are all joined is because they can be deleted on multiple levels: the group can be removed just for the contact on a given project, the contact can remove the group, and the group can be removed completely by a user). I think what I should be doing is just cascading the delete down these other tables rather than querying them every time to check if they were deleted at higher levels. Maybe that's really what I'm looking for, to have somebody validate that I just need to do this lol.
I mean a query is an expression where you want to produce a set. You can rewrite that expression a variety of ways, using a variety of tricks, but if it isn't working for you efficiently then...
Just to give you some background. This query is the result of many hours (probably close to 100 at this point) spent by me and another developer to get the results to return in a reasonable amount of time. The query itself has probably been hacked every which way possible, including different join orders, some values in temp tables with indexes, unions, TVFs, views, etc. I would gladly accept that I missed something, its just hard to believe given how much time has been spent on this. I was really hoping that I'm either missing something obvious in my query that could improve it (or maybe some trick I'm not aware of), or that there is a design flaw in my architecture like indexes or simply a different table structure to facilitate my needs. The alternative is that its simply a very large and complex dataset, and the data will have to be maintained in some other way to prevent the need to do so many joins.
&gt; AND c.IsDeleted = 0) Try getting rid of these.
Yeah, it takes a while to know exactly which pieces of the puzzle you need to pull together. Then it takes a little while to learn how to pull them. Stick with it, though, it's a great field!
Does it help if I buy a bigger computer? 
Yeah pretty much what I'm thinking. I need them right now because I'm not doing cascading deletes, but I'll have to implement that. Index-wise I have IsDeleted covered but removing it will be faster yet. Thanks for taking a look, I appreciate you taking the time to do so.
Can you provide an execution plan?
Distinct returns distinct *output* rows. So the input table(s) dont really matter.
It returns distinct results, meaning that the combination of building_name and role are all going to be unique. In this you will most likely see the same role across multiple buildings or multiple distinct roles in a single building. The distinct is ran against the select portion of the statement and doesn't apply directly to either table specifically.
god damn, that was a good explanation.
This is correct. I changed this but it has no affect on the query unfortunately. However, what you said about table partitioning seems to be pretty huge actually. It looks like I have a lot of reading to do. Some of these tables are in memory which unfortunately cannot be partitioned, but I'll do some tests to see which is faster (disk based partitioned vs in memory w/ good indexes) Thanks again!
Here is the MySQL documentation for SUBSTRING() https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_substring The second parameter supplied needs to be a position. I'm not certain from the information provided if that's what you're intending with "[FirstNameID] - [LastNameID]". I'm a little unclear on what you're trying to to retrieve though. I could provide better insight to this if you state what a given row of information contains and what you're trying to obtain out of it.
Sure. You’re essentially taking all the accounts that have a service type of Drainage and then joining the dataset to itself to see the other services (within the 4 you mentioned) that each account has. You need to self join here because the one (sa2) dataset has already been limited to the one service type. You could also use a sub query to do this. 
Hi longjaso, below is the data extracted from SQL - note that although it is an xml, it has been chuck into db as a data dump rather than actual xml file. I am trying to get the First Name and Last name out of the data. As names have different variables, I have set up a INSTR/CHARINDEX to get the field value of the firstName=" and Lastname". By having the field value, I am able to obtain any data that starts from the field value of LastNameID - FirstNameID to get the middle data. So anything that starts with firstName=" and when it hits LastName=" it stops. ie getting only the middle value. looking at the field value, firstName=" it's 521 for instance, anything beyond 521 would start capturing ie. Monkey. once it hits LastName - field value of 527, it would stop at monkeY so it wouldn't capture the remaining length of the data. not sure if that make sense below are the data chunk from db. &lt;?xml version="1.0"?&gt; &lt;tns:onlineEligibilityCheckRequest rhboIsoNum="1235" xmlns:tns="http://asd.com"&gt; &lt;claim oecTypeCde="EMF" fundBrandId="APA" peaRequestInd="false" presentingIllnessCode="341" lengthOfStay="0" sameDayInd="true" submissionAuthorisedInd="true" accountReferenceId="Coola Mooda" facilityId="12345A"&gt; &lt;medicalEvent medicalEventId="01" dateOfAdmission="2016-03-04+10:00" compensationClaimInd="false"&gt; &lt;patient dateOfBirth="1990-10-09+10:00"&gt; &lt;identity firstName="Monkey" lastName="Donkey"/&gt; &lt;memberNum="1234"/&gt;&lt;/patient&gt; &lt;/medicalEvent&gt; &lt;accident accidentInd="false"/&gt; &lt;/claim&gt; &lt;/tns:Request&gt; 
Join the table with itself over the absolute time difference in minutes &lt;= 1. Then group by the items in table a and count the items in table b where b is the table which is joined by the time difference.
You could use a case statement and sum SELECT itemlist.itemid, SUM(CASE WHEN itemids that have a time within 1 minute of this timeid THEN 1 ELSE 0 END) freq FROM TABLE GROUP BY itemlist.itemid 
Thank you :)
I will try to explain /u/drooski's query by breaking up the query, but English is not my native language so it might not be grammatically correct. 1. Firstly he chooses one instance of SERVICES which he aliases it **sa1**. 1. His WHERE clause for all tuples from **sa1** must have a SERVICE_TYPE of either 'DRAINAGE', 'OWNER', 'CONVERT' or 'INTEREST'. Whatever value of these which is found in a tuple first results in a true (since it's more or less boolean check) and the tuple is included in the temporary results table. 1. So far the temporary results table consists of all tuples where SERVICE_TYPE is one of the values inside the in() condition. 1. His second instances of SERVICES is aliased **sa2**. So far it's basically the SERVICES table without any filtering. 1. By joining **sa2** with **sa1** on sa1.ACCT_ID = sa2.ACCT_ID, the temporary results table for **sa2** now only contains rows where SERVICE_TYPE contains one or more of the following 'DRAINAGE', 'OWNER', 'CONVERT' or 'INTEREST'. 1. In the WHERE-clause **sa2** is even more filtered by only allowing tuples where SERVICE_TYPE = 'DRAINAGE'. 1. Since the self join is on sa1.ACCT_ID = sa2.ACCT_ID it means that all tuples that shares the ACCT_ID but does not contain sa1.SERVICE_TYPE = 'OWNER', 'CONVERT' or 'INTEREST' the tuples will be filtered from the temporary results table. 1. Lastly, the SELECT statements only projects results from **sa1** since it's basically filtered and there's no need to show anything from **sa2**. 
It does not like the empty string in the second argument of the translate function.
I tried CASE WHEN REGEXP_LIKE ('a110a', '[0-9]*') THEN... but it acted like it needed a full expression so I added = 1 but then it says REGEXP_LIKE in *LIBL type *N not found. I generally don't do a lot of complex queries in DB2 directly. I am trying to avoid pulling this data over into MS SQL for a one time query.
I could do this: CASE WHEN REPLACE(TRANSLATE (string, '~', ' 01234567890'), '~', '') != string THEN 'Contains numbers' ELSE 'Does not' END I just need to find a unique character or string to replace.
I use it this way: https://i.imgur.com/SiQX6UQ.jpg I hope this helps? Else you might check your oracle version 
Thank you for breaking this down much better than I did!
**bold** This is a test table SQL&gt; SELECT 'Car' item, 'Chevy' make, 'Impala' model, 24000 price from dual 2 union all 3 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 20000 price from dual 4 union all 5 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 35000 price from dual 6 union all 7 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 4000 price from dual 8 union all 9 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 500 price from dual; ITE MAKE MODEL PRICE --- ----- ------ ---------- Car Chevy Impala 24000 Car Chevy Impala 20000 Car Chevy Impala 35000 Car Chevy Impala 4000 Car Chevy Impala 500 **bold** Use CASE statement and column aliases to break into separate columns SQL&gt; SQL&gt; SQL&gt; SELECT item,make,model, 2 CASE WHEN price=35000 THEN price END PRICE35K, 3 CASE WHEN price=500 THEN price END PRICE500, 4 CASE WHEN price=20000 THEN price END PRICE20K, 5 CASE WHEN price=4000 THEN price END PRICE4K, 6 CASE WHEN price=24000 THEN price END PRICE24K 7 FROM 8 ( 9 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 24000 price from dual 10 union all 11 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 20000 price from dual 12 union all 13 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 35000 price from dual 14 union all 15 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 4000 price from dual 16 union all 17 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 500 price from dual 18 ); ITE MAKE MODEL PRICE35K PRICE500 PRICE20K PRICE4K PRICE24K --- ----- ------ ---------- ---------- ---------- ---------- ---------- Car Chevy Impala 24000 Car Chevy Impala 20000 Car Chevy Impala 35000 Car Chevy Impala 4000 Car Chevy Impala 500 **bold** Use GROUP BY and MAX to flatten rows SQL&gt; SQL&gt; select item,make,model, 2 MAX(PRICE35K) PRICE35K, 3 MAX(PRICE500) PRICE500, 4 MAX(PRICE20K) PRICE20K, 5 MAX(PRICE4K) PRICE4K, 6 MAX(PRICE24K) PRICE24L 7 FROM 8 ( 9 SELECT item,make,model, 10 CASE WHEN price=35000 THEN price END PRICE35K, 11 CASE WHEN price=500 THEN price END PRICE500, 12 CASE WHEN price=20000 THEN price END PRICE20K, 13 CASE WHEN price=4000 THEN price END PRICE4K, 14 CASE WHEN price=24000 THEN price END PRICE24K 15 FROM 16 ( 17 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 24000 price from dual 18 union all 19 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 20000 price from dual 20 union all 21 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 35000 price from dual 22 union all 23 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 4000 price from dual 24 union all 25 SELECT 'Car' item, 'Chevy' make, 'Impala' model, 500 price from dual 26 ) 27 ) GROUP BY item,make,model; ITE MAKE MODEL PRICE35K PRICE500 PRICE20K PRICE4K PRICE24L --- ----- ------ ---------- ---------- ---------- ---------- ---------- Car Chevy Impala 35000 500 20000 4000 24000 SQL&gt;
&gt; I want ones that show only have Drainage AND Owner, but can include convert or interest. SELECT acct_id , service_id , service_type FROM services WHERE service_type IN ('drainage','owner','convert','interest') GROUP BY acct_id , service_id , service_type HAVING COUNT(CASE WHEN service_type IN ('drainage','owner') THEN 'ok' ELSE NULL END ) = 2 
i think it is pertinent at this time to point out that MySQL doesn't have the CHARINDEX function (it has LOCATE instead), so OP is most probably using MS SQL
The CASE expression is by far the best way to do this. There is another method that involves using a sub-query for each new column but it would mean having a new query, new join, and additional WHERE clause for each additional column. This would be much more expensive than the CASE expression. 
Check out `UNPIVOT`
&gt; DataOptions AS DOPT &gt; WHERE ST.ReturnsSupported=1 AND DOPT.CmpId=1 This is equivalent to JOIN DataOptions AS DOPT 
Thank you very much! Yup -- that's it and now I finally know. 
Your query returns each distinct combination of building name and role. It's answering the question, "What roles are available in each building?". If Supervisor and Technician are the only roles available in Building A then you would See Building A | Supervisor and Building A Technician returned. If you just want to see distinct buildings then you would remove Employees.role from your select statement.
Good catch - I hadn't noticed that
Went from MSSQL as my primary to ORACLE as my primary a few years ago. It took me a while to begin to “think” in PL/SQL as opposed to thinking in T-SQL and then translating it. Now I go back and forth with (mostly) little issue. I’ve found ways to use MSSQL functionality for things I’d normally use in Oracle and vice versa, but it’s more or less just an adjustment you have to make in your own time. You’ll find yourself googling “*T-SQL function* In MySQL” a lot.
The more databases you know, the better. Oracle DBA for years, now doing Postgresql, Mongo as well as Oracle. Will need to dive back into MySQL (last touched it at v4 iirc) and physio looking at Influxdb. IMHO, Postgresql is really tending atm. Great db, amazing community, lots of extensions. 
One nicety is that they want to move to Amazon Aurora which is MySQL + Postgresql combined, so that would be an interesting technology set to pick up.
In my experience doing a join to a remote server always takes a while and only gets worse with size. In the past I've overcome this by copying data into a temp table, creating an index on the temp and then run your query joining on the new temp. Always try to run the query on the server with the larger table (to cut down on how much data you have to copy into the temp table). Its not ideal but gives the optimizer a chance to use the indexes. 
Started out with a little DB/2, then MSSQL for a couple years, then Oracle heavily for 6 years, and back to MSSQL. I've done a little MySQL along the way. Of them, I definitely prefer MSSQL though the modules and packages in PL/SQL can be quite handy. What's interesting is that in my experience, people in an Oracle world are a *lot* more comfortable with triggers than people in the Microsoft world are.
Pretty cool, oracle added pivot/unpivot in 11g, I've just been used to doing it the old way.
Oracle people are the most egotistical tools for sure.
I have a friend who works on an Oracle system entering data. He says it takes 2 minutes to save each account. Using more triggers probably isn't a good thing.
 I think he's talking about Mongo DB or some other such JSON type non relational database
https://www.mssqltips.com/sqlservertip/4696/exam-material-for-the-microsoft-70764-administering-a-sql-database-infrastructure/
how did MySQL get into your google search?
You might find JOIN and CASE useful for this.
OP said they’re transferring over to MySQL, so that’s why I said it. My Google searches are usually “*MSSQL function* in ORACLE”
OK, sorry :), read that on mobile and missed the context :)
I'm aware you would use those two, I'm just struggling with writing out how they would be implemented
Two minutes?! Sounds like bad application and/or database design With triggers, like anything else, you need to have a clue what you’re doing I know folks who think there is no such thing as a good trigger 
No worries mate.
where change_date between datediff(month, -1, change_date) and datediff(month, 1, change_date) 
Define a month. For example if the change date is 10/15/2017, do you want to see 30 days ahead and before and run this for all change dates you have to see how things have trended over time?
No, not really. You can use UNPIVOT, but that's a rather unfriendly syntax. None of them perform particularly well, either. This operation is essentially telling the RDBMS that you've got repeating groups. You're telling the relational database system you broke First Normal Form. You're telling a relational algebra engine that you ignored one of the basic assumptions of relational algebra. There are no simple or easy solutions to this problem because the entire system is *literally* built with the assumption that you will not ever need to do this. This is why repeating groups are bad. It's also why entity-attribute-value tables are bad. Don't get me wrong; there are sometimes good reasons for doing it, but that doesn't change the fact that you broke a big design rule doing it and you have to pay that price. Querying this data will always be particularly painful. 
Oh, I've done this before but am going to be rusty. You want each month in a bucket, correct? So you want output like this: | change_date | visits_mb | visits_ma | | :--- | :--- | :--- | | 2017-01-01 | 25 | 100 | | 2017-0-02 | 250 | 1000 | | 2017-01-03 | 5 | 10 | | 2017-01-04 | 25 | 10 | 
Yeah totally agree. Usually it's a report request where the end users want the data laid out a specific way, when I've needed to do things like this. I would get around any performance bottlenecks by building a reporting table built during quiet hours. This way you aren't storing your data in a bad form but also so the reports run in a reasonable time when the users run it.
Yeah, that's about right.
I started w MS Access (in 1995), switched to MS SQL Server, then MySQL, then PostgreSQL, then back to MS SQL Server. It went a-ok I hate MySQL though
I'm not really sure what a CTE is haha
It's because you're using an inner join and also specifying: where z.employee_attribute = 'HEALTHSTATUS' Try removing the WHERE, or try using an outer join.
use left join employee_attribute_table z on z.id = e.id
You can left join t3 and do WHERE t3.ID is null
You can left join select t3.dataid from table3 t3 where t3.ID = 123 and then do WHERE t3.ID is null 
You can do left join ( select t3.dataid from table3 t3 where t3.id=123 ) t3 where t3.id is null
have tested it, 'substring_index' is not a recognized built-in function name. Sorry I've got the wrong heading, it should be MS SQL not MySQL
Try this: select e.id,e.first_name, e.last_name, a.address_1, a.address_2, a.city, a.state, a.postal_code,z.employee_attribute,z.value from employee_view e inner join EMPLOYEE_ADDRESS_DETAIL_VIEW a on a.id = e.id LEFT OUTER JOIN employee_attribute_table z on z.id = e.id AND z.employee_attribute = 'HEALTHSTATUS' and e.id = xxxx
this is the major difference between the join types - inner means "skip rows missing on either side." you either want a right join if you only want missing rows on one side, or an outer join if you want missing rows from both sides.
imagine a result set coming out of the query with data presented like this -- ParentA ChildofA1 ParentA ChildofA2 ParentB ChildofB1 ParentB ChildofB2 ParentC if you can process this in your front-end lanuage to give it the desired look, then the query is a dead simple left outer self join if you're looking to have formatted output from the query coming back like this -- ParentA ChildofA1 ChildofA2 ParentB ChildofB1 ChildofB2 ParentC then my strong advice is, stop looking right now ;o)
&gt; exists tests for empty relations not necessarily for example, give me a list of all customers who have placed at least one order 
 &gt; people in an Oracle world are a lot more comfortable with triggers Not really. Oracle [pretty much advises against using triggers](http://www.oracle.com/technetwork/testcontent/o58asktom-101055.html), and the Oracle implementation of triggers is very ... limited compared to other implementations.
 &gt; I'm concerned about what happens to my career two or three years from now or even longer. I worked at Microsoft for 15 years (about 6 years actually on the SQL Server team), I worked at Oracle for about 2.5 years. I've done substantial projects with SQL Server, Postgres, MySQL, Oracle, and a few other systems. Based on my experience, I guess I have two answers for you. The first is that you'll be more valuable, but only if you keep all the experience in the other systems current. After Microsoft, I didn't do much with SQL Server except for foolin' around at home. It wasn't enough: big changes in the versions I missed, and things started getting Azure-flavored, too. I can catch up, but I'm certainly not totally current. The second is that you'll have to manage assumptions and confusion. Subtle issues, like locking or logging, are different in implementations. It's easy to mis-remember rules and patterns and ascribe them to the wrong implementation. A corollary to that is the assumption that is something is easy on brand B, so it must be easy on brand X. Really, it's in-fucking possible on brand X and you've just painted yourself into a corner. (Or, you assume that because Brand B doesn't do it, Brand X doesn't either. But brand X actually makes it super easy.) The third (BONUS!) is that, if you think carefully and study all the differences, you'll naturally learn more and more about actual DBMS implementation. Deeper knowledge about a broader set of subjects can't possibly be bad, right? But, I wonder: what are your specific concerns? What troubles you about this proposition?
I don't know why people were recommending anything other than left join
Are they actually named like that? Assuming they are and that your source table is also just one column, which I’ll call name, you could do something like: SELECT Name Group FROM (SELECT Name Substr(Name,7,1) as group Where Name like ‘Parent%’ UNION ALL SELECT Name Substr(Name,8,2) as group Where Name like ‘ChildOf%’ ) ORDER BY Group
&gt; But, I wonder: what are your specific concerns? What troubles you about this proposition? I see very few positions with a MySQL role and definitely a lot more postings on SQL Server. I would hope I don't lose my marketability after I transition to MySQL for a little while. I guess just being able to land another job where MySQL is not the primary RDBMS is my main concern. I don't want to fall behind with the technology for SQL Server, especially with my limited time using AG's. It's the enterprise features you don't typically see in small shops that are difficult to replicate at home that I'm afraid I'll miss out on and have held against me.
Could you elaborate on what you mean by "over the absolute time difference"? I'm afraid I'm not accustomed to that kind of abstraction and am having a hard time envisioning how this would be executed. Are you describing something along the lines of select a.itemid aID, sum( casewhen b.timestamp &gt;= dateadd(mi,-1,a.timestamp) and &lt;= a.timestamp then 1 else 0 end) - 1 as count --the goal being to capture any within 1 minute before from itemlist a, itemlist b group by a.itemid Forgive me if I'm way off base. I've tried to look a bit into what joining a table on itself looks like.
I speak ms-sql fluently, I get by in mysql, and I'd need to spend a day relearning Oracle. It's very useful to understand their "ways of thinking".
&gt; then my strong advice is, stop looking right now ;o) Are you suggesting that looping over the resultset to produce a single formatted string with line breaks and indentation is a bad idea?
 select a.itemId, count(b.itemId) from itemlist a join itemlist b on b.itemid &lt;&gt; a.itemid and abs(extract minute from(a.time - b.time)) &lt;= 1 group by a.itemid Not sure if that works but I can't try it as I don't have an oracle db on my private computer but I hope the idea is clear.
SELECT e.id,e.first_name, e.last_name, a.address_1, a.address_2, a.city, a.state, a.postal_code, z.employee_attribute, z.value FROM employee_view e INNER JOIN EMPLOYEE_ADDRESS_DETAIL_VIEW a on a.id = e.id employee_attribute_table z on z.id = e.id where z.employee_attribute = 'HEALTHSTATUS' and e.id = xxxx
 SELECT e.id,e.first_name, e.last_name, a.address_1, a.address_2, a.city, a.state, a.postal_code, ISNULL(z.employee_attribute,'NO HEALTHSTATUS') as employee_attribute z.value FROM employee_view e INNER JOIN EMPLOYEE_ADDRESS_DETAIL_VIEW a on a.id = e.id LEFT OUTER JOIN employee_attribute_table z on z.id = e.id AND z.employee_attribute = 'HEALTHSTATUS' WHERE e.id = xxxx When you use a standard INNER JOIN it will only return rows for which the JOIN could be made. It works exactly like the WHERE in that regard. A LEFT OUTER JOIN however means you return *all* the rows from the table your joining onto, match or not. For rows that it can't find a match it'll shows null for any columns from the table you're joining. This is why I've added a ISNULL which, if the column is null, will replace it with some useful text. If it's not null then what ever data is in the column will be returned instead. So that means in this case you'll get the employee with id xxxx regardless of if they have a health status attribute. I found [this article](https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) useful when I was learning. But you can Google it and find one to your taste. 
With this *exact* code, the LOJ to T2 seems unnecessary and may even produce a CARTESIAN PRODUCT join between T1 and T2 (as there are no join predicates attempting to match these tables). You could try a correlated subquery between T1 and T3 - can't comment on the performance difference though - you'd need to measure it: WHERE NOT EXISTS ( SELECT NULL FROM TABLE3 T3 WHERE T3.DATAID = T1.DATAID AND T3.ID = 123 ); 
If all of these are just stored as text in that table, then you're doing the right thing. If the data was normalized (separated into tables called car_category, lead_source, etc with an fk in booking_data), you could do a "select * from" each table, which would be much more efficient.
Nope, they're stored as text in a single table called booking data. 
 SELECT variable1, variable2 FROM dbo.Number LEFT OUTER JOIN (SELECT DISTINCT variable1 , ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS RowNumber FROM bookings_data) AS variable1 ON variable1.RowNumber = Number.ID LEFT OUTER JOIN (SELECT DISTINCT variable2 , ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS RowNumber FROM bookings_data) AS variable2 ON variable2.RowNumber = Number.ID
The actual features are available in Developer Edition, so you can use them ... but applying them *in situ* is certainly a lot harder at home for sure; there's no load, locking, scale, and so on. I'd say go for it; it's work. MySQL might be eclipsed by Maria, but I don't think it's going anywhere any time soon. The opportunity cost I'd be more concerned with is experience with NoSQL stores.
Risky click of the day?
*UNION ALL
Using exists can help you avoid very bad bugs-- . . . where name not in (select name from other_table) vs. . . . where not exists (select 1 from other_table where name = a.name) If name is allowed to be null in other_table, the first query will give you no results, because boolean returns are allowed to be null, and that's the sql standard for this case. It's a classic gotcha. The other thing to be careful with, in both cases, is that the column you ask for in the subquery is actually in that table. For example, if other_table's name column were actually named client_name or something like that, then the "name" you ask for in the subquery is the name from the original table, not other_table.
no, that would sort all the children first, followed by all the parents OP wanted them interleaved
Is there a reason not to do this? &lt;query1...&gt; EXCEPT &lt;query2...&gt;
Only because the queries I'm comparing are quite large, and I often want to switch the order of query1 and query2. As in, I'd like to compare the above query with: &lt;query2...&gt; EXCEPT &lt;query1...&gt; Which is a pain to do if I'm copying and pasting huge blocks of code (I will be manually iterating through this process as well)
I think there was a typo in that join. 
The alias comes first with ctes. WITH queryname1 AS (&lt;query1...&gt;) , queryname2 AS (&lt;query2...&gt;) SELECT * FROM queryname1 EXCEPT SELECT * FROM queryname2;
Thank you!
True. I think my edit fixed it.
&gt; Things that usually don't matter much So would you say that all the stuff that's happening in the SELECT statement I shouldn't worry too much? For example: -- Set Managers for workflow (SELECT Listagg(Manager ',') within GROUP (ORDER BY rowseqnum) AS Manager FROM (SELECT DISTINCT Manager volumeid FROM cust_cdmt_form) t WHERE t.volumeid = cdmt_form.volumeid GROUP BY volumeid) AS Managers, &gt; Joins are inherently costly. Do you see an opportunity to improve the way joins are currently being done? 
no it still puts all 'Childof' rows first, and then all 'Parent' rows
https://www.w3schools.com/sql/ You can check what youve learned on it too 
I don't think in terms of "if this is better than the query is better". These are tools to help understand what the database is doing, but the developer has to determine which costs have opportunities for improvement and which don't. It's not like "a value of 1000 is bad". It might be good for one query and bad for another. The things that jump out at me are the differences in data access, and related the differences in physical reads. Test 1 has more index scans, and less index fetches and table scans. It also has less reads overall (logical reads from cache), and way less physical reads. I think Test 1 is making better use of indexes than Test 2. Seeing both EXPLAIN PLANs would potentially confirm this. V$STATNAME|Test 1|Test 2 :-|-:|-: index fetch by key|15|50 index scans kdiixs1|65|51 table fetch by rowid|35|104 table scan blocks gotten|2|167 table scan rows gotten|88|14284 -|-|- session logical reads|209|493 logical read bytes from cache|1695744|2662400 Number of read IOs issued|2|89 physical read total bytes|57344|2113536 physical read total IO requests|4|177 physical reads|5|258 physical reads cache|0|87 physical reads direct|5|171 sorts (memory)|4|4 sorts (rows)|1386|1384 
Nah the ‘desc’ puts parent above child in each group. 
Try this: SELECT * FROM Customers WHERE (Country = 'Germany' OR Country = 'Mexico') AND (City != 'Berlin' OR City != 'Juarez')
PSQL only allows one use of the WITH command in a query. However you can write WITH ... AS in one query: WITH TempTable1 AS ( ... ), Temptable2 AS ( ... ), Temptable3 AS ( ... ) SELECT * FROM TempTable1 as t1, Temptable2 as t2, TempTable3 as t3 ; However, it is sometimes better to create views if you're using the temporary tables a lot or in different queries. ***Temporary*** Use **WITH ... AS** Example: WITH TempQuery AS ( SELECT * FROM TableA WHERE x = 1 ) SELECT COUNT(*) FROM TempQuery; After the query is run, the TempQuery table will cease to exist. ***Permanent*** Use **CREATE VIEW ... AS** Example: CREATE VIEW PermQuery AS SELECT * FROM TableA WHERE x = 1 ; Now I've got a permament table (PermQuery) I can use.
Every = needs to say what field it needs to compare to.
Go check out /r/learnSQL. SQL is a computer language used for relational database management and data manipulation. It's mostly used to write “queries” to interact with data stored in tables. Tables are the the primary objects found within a database. There are many conceptual similarities between Excel and databases. For example, tabs in a workbook are similar to tables in a database. Also, both Excel and SQL are used to perform analysis on datasets. Even so, Excel (the application) and SQL (the language) aren't really directly comparable.
You're very welcome! You may want to check the execution plan, since my response is probably not very efficient. You might also try for the City parameters: AND City NOT IN ('Berlin', 'Juarez') Alternatively, W3Schools gives us this option, as well: AND NOT City='Berlin' AND NOT City = 'Juarez' Someone with more experience in performance optimization might be able to give further direction on which is the least expensive option. Probably not the biggest concern, given the relatively simple nature of the query. Definitely more of a concern when you start adding in additional queries, subqueries, or JOINs.
i mean, i kinda agree, but this sub gets like 4 posts a day. its not like its a high traffic area.
I think it’d be hard to write an auto mod for this. Furthermore, what’s the line you want to draw betweeen beginner and more advanced SQL?
You have pseudo coded your SQL well. Just need to work on your where statements. Where color = ‘Brown’ or color = ‘rust’; Alternatively: where color in (‘Brown’,’Rust’);
Solved, Thanks a bunch!
my dude, i invite you to test it,,, not sure you understand how DESC works, it operates only on the names your ~first~ sort key is `group`, so all the parents are together, and all the children are together
you don't have SQL but you want to put it on your CV? my dude, that's going to come out in about 3 minutes in your interview
SELECT * FROM customers WHERE (country='germany' AND NOT city='berlin') OR (country='mexico' AND NOT city='juarez'); Watch the Boolean order of operations.
Walking through each statement, from the inside out. [Color coded](https://i.imgur.com/Wa2q22Z.png) Left statement: 1. CUST_CDMT_FORM has a full table scan, filtering ROWSEQNUM and VERSIONNUM 2. WSUBWORK has a full table scan 3. 1 and 2 are hash joined 4. for each row in 3 1. DTREECORE has an index seek on DATAID and DELETED, and filtered on SUBTYPE and OWNERID 2. DTREECORE has table lookups for columns not in the index 3. DTREEANCESTORS has an index seek on DATAID and ANCESTORID Right statement: 1. CUST_CDMT_FORM has a full table scan, filtering ROWSEQNUM and VERSIONNUM 2. for each row in 1 1. DTREECORE has an index seek on DATAID, DELETED, and OWNERID, and filtered on SUBTYPE 2. DTREECORE has table lookups for columns not in the index 3. DTREEANCESTORS has an index scan on ANCESTORID 4. 2 and 4 are hash joined 5. for each row in 4 1. WSUBWORK has an index seek on SUBWORK_SUBWORKID Comparing each table access in the different queries. Table|Left|Right|Remarks :-|:-|:-|:- CUST_CDMT_FORM|table scan|table scan|same WSUBWORK|table scan|multiple index seeks|left is better if it's a relatively small table DTREECORE|multiple index seeks w/ table lookup|better index seeks w/ table lookup|right could be better if there are many OWNERIDs per DATAID DTREEANCESTORS|multiple index seeks|index scan|right is better if there are few DATAIDs for ANCESTORID 5861461 The most significant difference is which of WSUBWORK and DTREEANCESTORS is hash joined and which is in nested loops. Based on the left being faster, I'd suspect that WSUBWORK is smaller than DTREEANCESTORS where ANCESTORID = 5861461. If the engine under estimates how many records will be retrieved from DTREEANCESTORS, it could underestimate the cost of the right statement. If the engine over estimates how many records are filtered from DTREECORE on OWNERID = -2000, it could over estimate the cost of the left statement.
I'm not sure I understand. My understanding is that '!=' means "not equal to". Would adding in the OR mean that no results were returned? (essentially, they cancel each other out?)
You would want it to be != to Berlin AND != to Jaurez is what he's saying, I believe.
&gt; Would adding in the OR mean that no results were returned? yes -- essentially, one of them is going to be true for example, Toronto is not equal to Berlin, so it doesn't even evaluate the second half because of the OR alternatively, Berlin ~is~ equal to Berlin, so it evaluates the second half, and whaddya know, Berlin is not equal to Jaurez no matter which city you plug in, all of them evaluate true when both sides of the OR are involved
Awesome, thanks!
I appreciate your explanation, thanks!
Group is A,B,C etc and Name is the original column he listed. so it groups all of the As together, then sorts by name desc, so parentA,childA1,childA2. 
and i should've mentioned that all results (instead of no results as you asked) are returned
I use SQL every day and would love to see more interesting and lively posts here. In it's current state this place is a graveyard of recycled introductory/beginner Q&amp;A. Part of the issue is that we need to have better resources to point newbies towards and advanced users need to post more often.
Code Academy; Data Camp; sqlzoo.net. Kaggle is having a SQL Scavenger Hunt for beginners if you're interested: https://www.kaggle.com/sql-scavenger-hunt
I would do it as: Where country in (“Germany”, “Mexico”) and city not in (“Berlin”, “Juarez”) 
oh shit i was reading your substrings as 1,7 and 1,8... sorry! okay, your method works, but i'm a bit skeptical that it would actually apply, because "parentA, childA1, childA2" aren't going to be the actual names
It looks like you wrote a lowercase I instead of an uppercase I. This has happened 6621 times on Reddit since the launch of this bot.
Ahh ok I figured there had to be some confusion. Yeah it definitely feels like a hack of a solution as opposed to just having the data organized in a better way but I guess you don’t always have that luxury. 
I'm sure this is too far down for anyone to see now but I want to clarify that if the account has a service id with service type that is anything other than ('DRAINAGE','OWNER','CONVERT','INTEREST') then I want it excluded. I also want anything excluded that doesn't include at least 'DRAINAGE'
the unique key to this table is `SERVICE_ID`. Some sample data would include Acct ID, Service ID, Service Type 001, 123, owner 001, 124, drainage 001, 125, drillings 002, 126, owner 002, 127, convert 002, 128, usage 003, 129, owner 003, 130, environment 004, 131, owner 004, 132, drainage 004, 133, usage 005, 134, owner 005, 135, drainage 005, 136, convert 005, 137, interest` I would ideally just include the `ACCT_ID` for each on that includes drainage (and the other allowed ones) but excludes anything with other service types like 'drillings'
it's `and not`, not `or not` otherwise this is fine 
so a solution to the above sample is ACCT_ID 005 as it's the only one that includes drainage and owner, still has the other allowed service types included, but excludes any with any other service type
on googling it i found schemaspy and schema scanner
There are so many learnLanguage subs that shouldn't be split from their parent sub (Language) as is. So many fractured communities despite sharing a same interest. Why not just internally compartmentalize by adding [Question] [Discussion] [Study] [Blog] or something similar to post titles instead of externally compartmentalizing and pushing people away?
Schemaspy hasn’t seen an update in almost 10 years (2010) and looks it. Schema scanner I can’t even find.
why does it matter if it's seen an update in ten years do you find that there have been big changes to sql since then?
That works
The issue is inefficiency of output in a format you like, the individual distincts are very efficient. Rule of SQL, if it is not easy to show in set theoretic notation it is not going to be easy to show in SQL
This isn’t correct. By not having a parentheses surrounding the two City clauses, you’re allowing a lot more records than just the ones in Germany and Mexico.
While i understand that SQL hasn’t changed the aesthetics and usability of the interface to users certainly has. 
Excel
Are they too expensive? What price would you pay for a tool like this?
I regularly use more than one and up to 5 different RDBMs in a single ETL event. Do not b ox yourself in...
While I can buy one of these tools, the vast majority of time I can find OSS tools that will accomplish the same exact thing. Only in very rare circumstances there isn’t an equivalent OSS tool that will do the same thing. This might be one of those cases where there isn’t an equivalent (photoshop comes to mind as a commercial tool that really has no competitor, yes i know there is gimp but it’s FAR from as capable) but figured I would ask to try and find one. 
&gt; I feel like this is not a topic that is discussed enough in the SQL world as it comes to SQL development for modeling/reporting purposes so I would like to read other criticism and opinions. In meetings things tend to be very DBA/ETL centric and the idea of having specific servers for reporting is lost. You need a data lake/data warehouse, along with a data analytics tool like SAS JMP, PowerBI, Tableau, etc. 
I love you, but in my opinion you just used a bunch of bullshit words. What we need is a server with a SQL database, and then whatever tools we want to plug-in, R, Python, blah.
&gt; If your company is a Fortune 500 company like you say, then it's hard to imagine they don't have this architecture already setup. Dude, I'm not here to wax poetic. This is literally what I'm dealing with and trying to make sure I'm not god damn crazy.
Here, if you're a DBA let me just put things in stark terms. I have been at the company for nine months in this capacity. I now have the largest table on the database (regardless of server) by more than 5M records. As my new process grows this will grow to an order of magnitude of anything else we have. In terms of statistical modeling my procedures will take an order of magnitude of any other other "dba" related functions that are going on. How do you propose to reconcile those needs without a reporting server/database? Call it a lake or whatever you want. I'm not interested in that.
I manage our company's reporting environments, and data science work. AMA
So... We have an "ETL process" which takes "excel files" from hundreds of multiple clients, some from the same source system, some not. These are "ETL'd" into a database and then there is a "materialization" process that occurs. This process actually occurs on Server A and Server B. Once finished there are "production grade tables" -- which you might describe as our BI warehouse. Now in previous roles that warehouse would exist on another server, or another database, or both, and there would be a reporting server / database which could access that database, but additionally users (me) could create sprocs, views, etc.. which would be tested to be pushed to the 'production reporting database/server.' All the ETL/downstream stuff is not relevant to my world view. I can't DEV on a server that doesn't have PROD data on it because I'm not "DEVING" code, i'm deving results of calculations.
That didn't contain any questions but... You need read access to the prod reporting data, and a sandbox with full permissions. The sandbox can be a separate database on the same server, or on a different server. You then pull any data you need into your sandbox, and have at it. Putting your sprocs and new tables on the prod reporting database, having reporting data copied to your sandbox, and a sandbox dev environment are all more trouble than they're worth in my experience.
&gt;You need read access to the prod reporting data, and a sandbox with full permissions. Yeah, this does not exist and I am looked like a Martian when I bring this up. I literally don't know how to respond. 
What's their suggestion?
&gt; But no, you're describing a server. I didn't describe anything of the sort. Please take a moment between your indignant triple posting to look at who you're talking to.
Then please revise your comments.
I don't think I'm who you think I am. Protip: posts have a name next to them.
Kind of for me to fuck off. And use DEV data which is way out of sync... I mean massively. Missing columns, historic loads, impossible to work with our business partners to QA calculations for compliance in order to automate processes. I was told several times tonight that I don't understand the "life cycle" of a "development."
No, really, I read your comment. You were describing a server. You could psychologically put that in any context as you want, and you may be correct that I am a nihilist who thinks everyone I work with is retarded... but I also could be right. So you really aren't adding shit to this conversation. I'm cool with being called an asshole if you can show me I'm wrong with data. I'm not cool with being called an asshole if you have nothing to back it up.
&gt;You were describing a server. Where was I describing a server?
&gt;When you follow this up by calling some useful advice "bullshit words" and telling someone trying to help you that "I don't have time for your bullshit", I have to wonder if the fundamental problem here is a technical setup issue, or an inability to get coworkers onside by making a case for your needs without alienating them by a grating and superior attitude. Right there, asshole. I don't have time for your cunt attitude. I need a server. Get me a fucking server. Call it a lake, or a warehouse, or whatever makes your dick hard. Get me a fucking server. Don't come at me with your weak psychology shit if you don't understand what a reporting server is. I need a reporting server. Is this alien to you?
What is sargable?
Don't ask for things you can't have, like creating tables and stored procedures in PROD reporting. Ask for things that are easy for them to give, like read access. Give them options and demonstrate consideration for their concerns on things they are more reluctant. Like your a sandbox on the same OR different server. Owner of a database OR schema. You will properly compress your data. Commit to a size limit. Limited maintenance required. Understand where they are coming from and logically clarify any misunderstandings. That your work is sensitive to data distributions, and tuning models to dev data is wasted effort when prod has different distributions. Explain your obstacles, rather than requirements. They may offer solutions to your obstacles that neither you nor I have considered.
Then you literally don't understand the nature of analytics. I don't care what you call it. It's a sever. I've been working in IT and related fields for over 20 years. I need a "computer" to "do math" with "datasets" and then "visualize them." This shouldn't be a difficult business concept. What do you think all of this data is being collected for?
lol you're literally getting mad because you think I'm [/u/creav](https://www.reddit.com/r/SQL/comments/7vt2z3/update_we_have_three_servers_which_one_is_which/dtv09v5/) and can't back down without admitting you made a mistake.
Are you getting any errors?
in short, whether indexes can be used for efficiency google it ;o)
&gt; dead set on the approach with the sub query UNION? Relational databases are designed on principals from relational algebra. I can certainly see what results you are looking to get here, just not sure whether you have this particular I got an alias error, so I tried using aliases and it worked. What did you have in mind? I'm fairly new.
I assume your alias error was the result of not aliasing the sub query? You can accomplish this with a join and taking advantage of the WHERE clause. ... INNER JOIN Laptops WHERE speed &gt;= 3.0, etc.
Tbh I used it just for testing with data in one case , but to start its a good source in my opinion 
That does not work tho. Thats not a proper sql statement 
i was referring to the sql statement embedded in your php -- the sql looks okay to me get a frontend utility like heidisql or phpmyadmin and run your sql directly in mysql, make sure it works, and then attempt it inside php
Hmm anything I could find about using multiple where's in the one line told me it was not possible 
What are you doing in the “do a thing” section?
Updating one of the other columns in the table
I have thought the same thing but i do not mind helping people learning SQL. I just don't like to do their homework for them. If they really want to learn it is okay. I'd rather have a way to mark posts with unformated querys or pictures of text.
Not sure where the cursor is coming into play but if your updates aren't dependent upon one another, you can update multiple fields with conditions in a single statement. The fact that you're using `TOP 1` without an `ORDER BY` means that the row you're updating is non-deterministic. Giving you useful guidance would be easier if you had a more concrete example and not pseudo-code. update Property set Whatever = case when PropertyType = 1 then 'Something' else 'Something Else' end, set WhateverElse = case when PropertyType = 1 then 'This other thing' else 'Yet another thing' end where Uprn ='987654321';
And use parentheses to control how the ANDs and ORs are applied.
` &lt;-- What's this character doing in there.
That's what's stopping it working :p thankyou. 
You can pull the row values into @variables.
Yeah that looks a lot better, thanks.
I feel there are two types of users. You have those who can take a hint or think critically about something and then search for information related to their problem, piece things together, experiment, weigh the results, and come to a conclusion with a resolution. Then you have people who want to be spoon fed and put in no effort or they have so little faith in themselves that they don't believe they can achieve the solution. Perhaps they just feel it's too overwhelming instead. Those are the kinds who I feel repost the same Q&amp;A things you see daily. The other users don't post as much because they are doing exactly as I said, they are looking for the answers and as a last resort they are asking for help. One other tidbit is that StackExchange is really mean. You also have to ask a very well formed question, which is what you'd expect if you are going to put in a lot of effort to help people. I've placed a well formed question there once before, oh man did I get eaten alive. Sometimes people try and shred my answers apart too. Usually people there are helpful, but I have had better luck using twitter and #SQLHelp for resources to good articles regarding problems to do the investigation myself.
&lt;I feel there are two types of ~~users~~ people I agree with you though. I would like this sub to be more than just Q&amp;A. It would be cool if we had more discussion based posts. Or sharing of interesting/unique techniques we've used to solve issues. Maybe even mini-contests to see who can write the best script to solve some mock scenario. I don't know! Even though it's a big part of my job I don't really know anyone outside of work who knows uses SQL regularly. It would be cool to shoot the shit a little more.
&gt;It would be cool if we had more discussion based posts. Or sharing of interesting/unique techniques we've used to solve issues. Maybe even mini-contests to see who can write the best script to solve some mock scenario. Agreed, it would be neat. Sometimes there is a more interesting topic that crops up. It's one of the few SQL communities where I feel I can throw a more off the wall topic and get a little feedback on. A lot of the other communities online are a bit more rigid or too serious. &gt; Even though it's a big part of my job I don't really know anyone outside of work who knows uses SQL regularly. It would be cool to shoot the shit a little more. The best I get is that there's a user group for SQL that meets once a month that I can drive 50 miles (1-1.5 hrs) to attend. Or one I can drive 2-3 hours for that occurs bi-weekly. I thought about creating a group in my city, and it's not a tiny city either. At the same time, I feel like only 1-3 people would attend, if anyone ever showed up at all and I'd have to be the presenter every time. 
Thanks for all the input. Found this format to be a bit PowerBI friendly for a task i’m working om to avoid some not-too-pretty DAX calculations. Ended up doing multiple UNION ALL within the first FROM to achieve the format needed. Thanks again.
That much SQL I know. Since I really can't disclose too much it's hard to fully explain. 
Without seeing the schema (even an anonymized version of it) and some sample data and output so we know what you're expecting, it's going to be hard to help you.
What SQL platform are you trying to schedule jobs for?
oh sorry, the server is running sql server 2012, I'm just using SSMS and excel sheet connections so far.
Do you want me to keep guessing? SELECT global.SettingName , COALESCE(user.SettingMembership,group.SettingMembership,global.SettingMembership) AS SettingMembership , COALESCE(user.SettingValue,group.SettingValue,global.SettingValue) AS SettingValue FROM Settings AS global LEFT OUTER JOIN Settings AS group ON group.SettingLevel = 'Group' AND group.SettingMembership = @MyGroup AND group.SettingName = global.SettingName LEFT OUTER JOIN Settings AS user ON user.SettingLevel = 'User' AND user.SettingMembership = @MyUser AND user.SettingName = global.SettingName WHERE global.SettingLevel = 'Global';
I don't really want to say it, but I'm going to say it, it's straight forward to create a job as long as you are not running the express version of SQL Server. Express does not allow you to create jobs through the GUI, you have to use T-SQL. Any other version of SSMS lets you create jobs, and this may have changed if you are using the 2017 SSMS tool. Think of it like this, you wrote your script and it does things. Maybe you want those things to happen every day at 3am. It's really as simple as creating a job, and scheduling that job to run at 3am. You can just copy and paste your T-SQL to run in the job, so it knows, here's the script I'm going to execute. From there it can be as complex as you want it to be, but keep it simple to start. Such as notifications, job logic on steps, complex schedules, executing other tasks, etc. That's all stuff that can be done, but it doesn't sound like you need that level yet. Does that help at all?
Ok so it's basically I have a table with settings and I created a new table that is basically the old table, but with a user id column. The problem is that I have a stored procedure that updates the new table. The problem is that when it's used there's a possibility of duplicating records which we don't want to happen. It's basically if the user id doesn't exist in the new table then it adds all of the default values and adds that user id in it's respective column. The reason I can't disclose a lot of specific information is because it pertains to my job. I'm not much of a SQL guy. I am mostly a C# guy who has to use the 2 together.
found out what my issue was :( The website is temporarily unable to service your request as it exceeded resource limit. Please try again later.
They are not at all complicated. The most basic thing a job can do, is run a query and alert you (or someone) it ran successfully or not. Its good to also read about operators, (people), alert values, and notifications. thats a start. jobs are also autocreated by SqlServer Reporting Services as ugly GUIDS. and used for database and server maintenance. you can script them out too and see how to create the entries via SQL
 IF EXISTS (SELECT * FROM Setting WHERE UserName = @UserName) BEGIN UPDATE Setting WHERE UserName = @UserName; END; ELSE BEGIN INSERT INTO Setting SELECT Settings, @UserName AS UserName FROM Setting WHERE UserName = 'DefaultSettings'; END; Maybe you can write in C# what you want to do, and I can translate it into sql.
Have you looked at the documentation for your specific version of SQL Server? https://docs.microsoft.com/en-za/sql/
On the bright side though I do feel more confident in my initial impression.
I would have to think about that when I get a chance. I'm just mad because I had something that worked, at least for now where it fixed the problem. 
Use a cursor. It'll iterate all the way through everything you want: DECLARE @a, @b, @c AS &lt;data_type&gt; DECLARE cur CURSOR FOR SELECT a, b, c FROM tableA OPEN cur FETCH NEXT INTO @a, @b, @c WHILE @@FETCH_STATUS = 0 BEGIN &lt;Do stuff with @a, @b, @c&gt; FETCH NEXT INTO @a, @b, @c END
It doesn't seem too bad. I'll look into that.
You can run scripts using Windows Task Scheduler, or MS SQL Server's own scheduling service. It really depends on which one you have easier access to
Not an oracle guy, but you could probable start here: [Tuning the Program Global Area](https://docs.oracle.com/database/121/TGDBA/tune_pga.htm#TGDBA346)
Like I said before it's hard to explain because of my job. I'm not talking about settings in SQL. I'm talking about actual data that we're using in an application. The problem was we had settings that would end up changing for everyone. Then we had to find a way to associate the user IDs with them. The new table I created associates the set of defaults with each user, but the way I'm filling it is if the procedure sees that there is no match for the user id it grabs the defaults and does what I just described to enter them into the new table. Unfortunately in a rare case that a record gets deleted then there is a possibility of silicated data being entered. 
You mean like ssmstoolpack?
ok, that's a fair point
Have you tried running ADDM &amp; AWR reports for the time period that your report queries are running? The AWR report can give you some particularly useful information. Look at the execution plans of your statements. Are they using indexes, or are they performing full table scans? If you aren't familiar with manually tuning statements (or even if you are), have you tried running tuning advisor on them?
none taken. The one was actually the same one i was referring to in my original post. i was just hoping i was just unlucky in my search but apparently not. at this point i guess my only solution is to end up buying one of the tools unfortunately.
Importing spreadsheets might be easier using SSIS packages then deploying the packages via scheduled jobs. Like everyone mentioned, the job part is simple. [Link](https://technet.microsoft.com/en-us/library/ms191439(v=sql.110).aspx) SsIs is a different beast. 
No mention of agent in this thread yet? If you have access to agent (scheduler) you can add step with tsql, or ssis packages or power shell... etc etc. But agent is a service built into ms sql as long as it isn't the express version.
The scheduler is called sql agent. Google that. You can use regular sql queries or import packages to be scheduled, plus more.
I didn't make a mistake.
&gt;They didn't tell you to fuck off. They told you you couldn't have what you were asking for. Which means you were asking for the wrong things. Having production grade data and a reporting server is not asking for the wrong things. &gt;Don't ask for things you can't have You are talking to the guy who gets the things he wants, and it is in my job description to do this. I am encouraged by my boss's boss's boss to be, 'disruptive.' His word not mine. &gt;That your work is sensitive to data distributions, and tuning models to dev data is wasted effort when prod has different distributions. Explain your obstacles, rather than requirements. They may offer solutions to your obstacles that neither you nor I have considered. This has been done and is lost on these individuals who do not seem to care, or want to care about them. So these obstacles need to be dealt with. 
Are you high? My background is in IT and servers. Where do you think databases live? Are you simply not listening to a word I'm talking about?
I literally have not mentioned a database. All I said is that your personality is the reason why you can get anyone to listen to your needs.
Apparently you're not able to read or produce evidence to back up your tantrums.
&gt;When you follow this up by calling some useful advice "bullshit words" The bullshit word you're looking for is, "server," if you read the parent comment. You seem to forget I come here for *your* expertise. My name is NOT a SQL star. I am not saying I'm NOT an IT star. I come here to ping a community for ideas, so you can hold your psychology. I don't have a problem getting what I want, because what I want tends to be reasonable and necessary. I'm not here to bitch. I am here to ask a group of people with more experience than I have with a specific technology about best practices. Your post was simply not helpful, and quite frankly you seem to not even care to understand what I'm talking about.
you know, there are language agnostic ones which work on just being told what the comment notation is the only one i know is twintext, which is also old, but, knowing that might lead you to other more modern ones 
But I didn't make the parent comment, so I literally have not described a server. Do you concede this to be the case? &gt;I don't have a problem getting what I want Isn't the whole point of these two threads that you *do* have a problem getting what you want? In the previous thread you were planning on actually using the thread as part of your argument as to why people should accommodate your needs. Like, you literally thought "Well actually here is a bunch of anonymous internet nerds agreeing with me" was the way forward in getting people to help you. You might be a great IT dude, but your soft skills could seriously use some work and you're about a decade past the heyday of the grating but technical genius IT worker. In BI especially being able to get people onside while making them like you is as important a skill as your SQL.
&gt;But I didn't make the parent comment But you commented on it and added dialogue relative to it. Are you not capable of understanding the fluid dynamics of the English language. Does it comfort you to know that prior to my career in analytics that I taught English, and consider myself a master of the language? &gt;Isn't the whole point of these two threads that you do have a problem getting what you want? No, motherfucker. If you read the post, it should be clear that I am asked for expert opinion to convince me that I am wrong (because I do not consider myself an expert here) --&gt; before I have to take the nuclear option. If that isn't clear then I have failed in my communication of the English language. &gt;In the previous thread you were planning on actually using the thread as part of your argument as to why people should accommodate your needs. I am, if I need to. I may do it in conjunction with threatening to quit if I don't get what I want. There are other jobs. And most other companies have reporting servers. So it's kind of a circular argument, no? &gt;Like, you literally thought "Well actually here is a bunch of anonymous internet nerds agreeing with me" was the way forward in getting people to help you. I don't think you understand where senior VP's of companies go to validate their ideas. They Google shit. Frankly it leads them to a lot of bullshit, but... feel me? &gt;In BI especially being able to get people onside while making them like you is as important a skill as your SQL. Dude, my salary has gone up by 40% every two years for the past six. I don't give a fuck if you like me. 
&gt;I can just go to your boss and make my case, and point to my record. Seriously dude, being a massive asshole to people in your way because you've had some experience and success will only get you so far when you're sticking around in a single organisation for any length of time. The modern IT professional needs to be collaborative, flexible, and soft-skilled. The asocial prima donna asshole who makes good code is a niche that is rightfully dying off.
Bear in mind we do not have a BI group here as described. There is no "BI warehouse" source of truth upon which analytics can be done. The company has systemic backend problems. If we had a BI group that dropped me off data on a **server** (i.e., not their server, so it doesn't have to go down multiple times a week for 4-8 hours) then I could do whatever I wanted. That's how most environments work, right? I'm not losing my mind here, am I?
&gt; But you commented on it and added dialogue relative to it. Are you not capable of understanding the fluid dynamics of the English language? You literally said to me "You are describing a server". You're really bending over backwards to avoid admitting a very common and understandable error. &gt;Does it comfort you to know that prior to my career in analytics that I taught English Yes, it does comfort me that you removed yourself from a position where your personality could cause developmental harm to learners. &gt;If that isn't clear then I have failed in my communication of the English language. The failure wasn't one of language but of understanding the fundamentals of human interaction. Lesson 1: telling someone who has replied to your call for help that they're talking bullshit is not a constructive way of getting further help. &gt;I don't think you understand where senior VP's of companies go to validate their ideas. They go to meetings and usually just listen to whoever is most persuasive. Idiots with soft skills usually get their way over people nobody likes who are right. &gt;Dude, my salary has gone up by 40% every two years for the past six. I don't give a fuck if you like me. c.f., my earlier comment about you swinging your experience/fortune 500/previous success dick around like it means anything. I know you don't care if I like you: but your belligerence has only proved that I, as someone who barely knows you through internet comments, can identify a serious gap in your skill set that is relevant to the problem you're posting about. No it's OK you don't need to thank me, this bit of career counselling is on the house.
Linked servers are delicate and need to be treated with respect. "So I can pull the data in as I see fit" is a massive red flag. Hell no, you are not getting a linked server. I believe you. They probably told you to fuck off. What you're asking for is a bit too much, but not unreasonable. They way you are ~~asking for~~ *demanding* it is way out of line.
No you're not - that's a really reasonable request, that if it were reasonably explained, and effort made to get people onside, it would happen very easily. You could even do something lame with a local or cloud DB as a proof of concept and people would be falling down at your feet to make this happen. So there's really only two conclusions here: your organisation is full of monumentally stupid people, or there's a personality clash where someone is acting as an obstacle to you. The latter is a normal part of business life and dealing with those obstacles is a normal business skill.
&gt;The failure wasn't one of language but of understanding the fundamentals of human interaction. Lesson 1: telling someone who has replied to your call for help that they're talking bullshit is not a constructive way of getting further help. I don't think you understand how change occurs. &gt;They go to meetings and usually just listen to whoever is most persuasive. Idiots with soft skills usually get their way over people nobody likes who are right. You are presuming that who I am here is who I am there. And if I have the ear of the person in those meetings who is most persuasive and they are paying me to be right... &gt;but your belligerence has only proved that I, as someone who barely knows you through internet comments, can identify a serious gap in your skill set that is relevant to the problem you're posting about. This isn't a problem. It's something I am going to take care of. Sometimes I need to push to find out if I'm right before I put my dick on a line. That's how I end up with references. 
Oh, it's going to be made very reasonably. With sugar on top.
If A) that is not a prod server, B) there is no PII/PHI in the data, and C) the remote server is properly maintained so that you cannot send OPENQUERY()'s to it.. .which are awesome... then how much more respect do you want? Why can't I pull the data as I see fit? It's FOR ME. That data exists FOR ME. What do you mean too much? Setting that up is more complex than syncing tables/databases at N intervals (where I get to define N to my choosing). Those are my requirements. Are you saying it's not ISO compliant? If not, then I can tell what my cost center is and you can bill us. Do I need to get my boss involved?
I mean, let me make this simpler for you since I don't want to solution for you. Which of the two options is simpler, and which would you like to choose? Is there a third option I am not seeing? 
Actually not familiar with that tool (or others like it). I’m essentially wanting to have a sphinx, javadoc type setup where I can document my tables, columns, triggers, etc. within the context of the code/schema/ddl and a rook generates the documentation (hence one of tools i originally asked about). Doing it outside of the context of code is brutally painful keeping it in sync and in my own opinion, anyone that does either has people helping maintain the documentation, or it’s just wrong. I agree that other documentation mediums can be used for other purposes (e.g. wikis, graphs, charts) for guides, instructions, explanations of general things about the software, but code documentation should be kept in code. Not a word document that is kept separate from the code itself.
I'd give you a blank sandbox database on Server B and read access to the reporting data copy there. You can do your development there. Anything that get operationalized gets reviewed before it moves to Server A.
Awesome. Great. Can you have it to me by Monday?
You never really had a question. The very first time anyone said anything you just started shouting about HOW. YOU. NEED. A. SERVER. You weren't here for advice, you were here for validation. I think this time you should totally show your colleagues the thread. I'm sure it will solve all of your problems.
But you seem to be fundamentally ignoring the fact I do need a server. I cannot be, nor can other developers on my team, unable to work 1 out of 5 days of the week, or 3 out of 5 days of the week if another group is testing. That is not acceptable.
But that's the point I'm trying to make. Can you *imagine* being in a situation where you need to go to *this level* to get a *basic thing*? Yeah, I want to be convinced otherwise.
**If I were**
&gt;Yes, I can absolutely imagine being in this situation. If I was in the habit of demanding everything I wanted while telling people I was a big shot disruptive highly paid genius who cared not for their opinions, I would expect people around me to become unwilling to help me and to put road blocks in front of me. Yeah, you're presupposing a lot of things here. &gt;Your posting history, not least of all your posts in this thread, have convinced me that the problem is just that people don't like you and have no interest in seeing you succeed. We're all friends. Sometimes we play in the mud. I don't go work to make people like me. Nothing you've said is of substance to the thread. I don't need to like the people I work with, it is a benefit, but I can work with anyone. My success is not tied to people liking me... or perhaps I should say it is solely tied to certain people liking me whom I can use as references.
How many connections to the db? 
You should really migrate away from both DBViz and MySQL to be honest, both are severely dated pieces of software in so many ways. It's ages that I touched MySQL, but back when I did, I made it a habit to make sure that I configured Unicode correctly in no less than four places; from memory, those were * the server encoding * the DB encoding, * the table encoding, * the connection encoding. Definitely dig around those points and make sure they are all set to the identical same encoding string, but before you do that make sure you grab the right encoding; MySQL has at least two similar UTF-8 encodings, one of them is the older one and is crap b/c it violates the UTF-8 so is in fact not-quite UTF-8 and the other, newer one seems to work for some people; I believe it's called `utf8mb4` (apparently short for 'Unicode Transformation Format 8, Multi Byte 4'). I can't give recommendations on how to replace DBViz (don't know about your needs and the MySQL ecosystem) but if I was you I'd just issue a sample query from the command line just to make sure all the critical characters all get through that far unscathed. As for a sound MySQL replacements, have you thought about PostGreSQL? It's fully Unicode-compliant out of the box and open source, and that's just the tip of the iceberg of goodness. "I am not really an expert on encoding"—this *can* be a daunting topic, but if what you want is to just make the damn thing work then checking for your tools to use UTF-8 and NOTHING ELSE (no Latin-1, no ASCII, no CP-1252 / "Windows ANSI", no UTF-16, no UCS2, no UCS4, no UTF-32, no CESU-8). Read up on https://en.wikipedia.org/wiki/UTF-8 to gain an overview if you will. https://stackoverflow.com/questions/202205/how-to-make-mysql-handle-utf-8-properly#202246 mentions `my.conf` settings `default-character-set` and `character-set-server`. Take heed that when people mention UTF-8, they sometimes mean the standard and sometimes the MySQL config value; in the latter case, that should be `utf8mb4` instead. Also, observe that some people claim that "Unicode is now the default with MySQL"; in reality, your data will likely be stored in Latin-1 with Swedish collation in case you never explicitly configured your database.
**UTF-8** UTF-8 is a variable width character encoding capable of encoding all 1,112,064 valid code points in Unicode using one to four 8-bit bytes. The encoding is defined by the Unicode standard, and was originally designed by Ken Thompson and Rob Pike. The name is derived from Unicode (or Universal Coded Character Set) Transformation Format – 8-bit. It was designed for backward compatibility with ASCII. Code points with lower numerical values, which tend to occur more frequently, are encoded using fewer bytes. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; I have exported the results as html then loaded in excel have you tried something else, like exporting things from phpmyadmin / mysql cli For example to export to mysql from command line you culd just https://stackoverflow.com/a/356605 or using phpmyadmin you have this https://i.imgur.com/HtcFkFd.png I too don't understand much about character encoding, but I think you are using the wrong tool (dbviz) 
Mobile doesn’t let me see what it is, but why not just make a procedure or a view for them to run? You could also make a procedure that populated into a table, and give them read access to the resulting table only.
I'd create a view. Then they can run the view, do further selects from that data, etc. 
Creaye it as a stored procedure. Create a new user or modify their existing. Grant execute to the procedure. Don't grant any other access or role. Eat cake.
Thanks for all of the great information, there's a lot there, but can you explain what it means for postgresql to be unicode compliant out of the box? Does it reject input that's not unicode? Does it automatically convert it? or..?
Combine two of the answers. Create a new user, create a view (Do NOT give them rights to the view), Create a SELECT based stored procedure based on the view using the dbo. Deny access to the underlying tables to the new user and only Grant access to the Stored Procedure. Additional points for making an input parameter which will take dates and change the returned result set.
You don't have time for anyone unwilling to pay attention, but spend what can only amount to hours arguing about the who said "server"? It's been almost 24 hours of this threat being revived every 30 minutes or so. You don't seem too busy to me.
SELECT * FROM customers WHERE country in ('mexico','germany') and city not in ('juarez','berlin') This will work for the most part unless there is a city in mexico named berlin (or a juarez in germany) that you'd like to see in your results. It would need to be written differently if you want to cook that scenerio into your sql.
Yeah so maybe compatible is what I should've written there, for good reasons PG behaves like a modern application should, that is, it assumes Unicode encoded as UTF-8 everywhere (data, *.sql sources, queries, resultsets). You *can* use other encodings / character sets but everything except Unicode and UTF-8 has to be made explicit, which is the only sane way nowadays IMHO. So when you send PG a query say from your app that uses something else like say Latin-1, and you do not declare that encoding / character set explicitly, then chances are everything will go well as long as your strings only uses the lower 127 character points (that includes a-z, A-Z, 0-9) but will cause an error as soon as something like `äöüÄÖÜß` pops up. What I normally do to give everything a quick stress test is to send in characters like `'äöü'`, `'一丁丂'`, `'𠀀𠀁𠀂'` and see whether or not those are stored and re-transmitted correctly; note the last three ones in that series comes from Unicode code points above `0xffff`, and often those are the ones most likely to fail (you can probably observe that when you switch between MySQL legacy UTF-8 and MySQL `utf8mb4`). What is really really annoying and I don't have a ready answer to is silent failure in encodings, like what you experienced with the quote characters. You want all of your software to complain loudly when a string is found to be improperly encoded because otherwise the (maybe lengthy) output *might* contain *some* error that you can only find through arduous proofreading.
Yeah Ive been playing with ssis a bit and have gotten the csv -&gt; database import working, sort of, I need to hit my head against it a bit more to break out some data into its own column. (ex the PLC outputs (machinnumber_errorcode_error as one column so I need to break that up. But I think the plan will be to just get what I have into its own table and then have a process scrape that table and send it into the real database periodically. I'm an excel guy so its not all new to me, just don't know how to twist sql into doing what I want :p 
I'm just trying to decipher if I gave wrong advice initially on this or if it's just the person. As far as I can tell, this seems like an office politics problem that we can solve for this person. They just need to host a meeting with whomever controls these servers/databases/whatever word you'd like to describe and agree upon an architectural resolution to this. 
I agree with you. His initial reply to you was dismissive and rude because he was just looking for people to agree with him. 
One weighs a pound, the other weighs two pounds
Is this a real question or a setup for a joke?
Double hashtag has greater duration and can be accessed from outside your connection?
One is a Global hashtag table, the other is a local hashtag table?
I haven't tried those options no, but somehow it only just occurred to e at your prompting that I do have navicat as well. My dbviz won't let me export things like that, but navicat will. I'm trying to find an encoding format that will preserve the information now.
Thanks again for the great information! Thankfully I don't need to dig into any of it too far. The other commenter reminded me that I have navicat (i never use it, so forgot).... and in navicat I can just export as an excel file with encoding set to utf8 and it all comes out fine. The weird characters still show up as question marks or something, but that's fine, I can afford to lose them. The problem I was having was dbviz was killing other characters as well. Thanks again!
It's not a real question
A trigger, an index and a collection walk into a bar
I refuse to believe that a db designer wont let you export to SQL! If you can import it into mysql then you can export it again
I'm sorry, I meant to say dbviz won't let me export the output window as a csv or any other file. There simply are no export options at all.
What you've posted would work for what you described. I'd group `table_1` on `batch_no` before joining, but the results are the same. SELECT SUM(t1.payment) AS payment FROM table_2 AS t2 INNER JOIN (SELECT batch_no , SUM(payment) AS payment FROM table_1 GROUP BY batch_no) AS t1 ON t1.batch_no = t2.batch_no WHERE t2.date &lt;= '2018-01-02';
I like this. Have your cake and enjoy some tatas
As someone who does R and SQL, this is a trivial thing to do in R, but painful in SQL. If you can use R... This doesn't treat numeric columns efficiently, but: for (i in 1:length(bookings_data)) { print(levels(as.factor(bookings_data[,i]))) } It loops through the columns and changes the values into "factors" then reports the "levels" (distinct values) of the factor. In SQL, you could probably write a store procedure that could read the metadata about the table to get the column names, and then create some dynamic SQL to generate an output where you might have the field name in one column, and a column for the distinct values, giving you as many rows as distinct values across all the columns. But this would be difficult and the dynamic part is typically frowned upon (moreso than cross-joins!). If your table structure doesn't change, I'd create an table to hold the information (as described above), and write a stored procedure that could run the `SELECT DISTINCT` for each column into that table. It would be a copy/paste for each column in your bookings table.
And @@ is not worth mentioning...
Your query should return 250, what are you expecting?
Your query looks fine. To find out for sure, you could select a single day and check the result. Also, if your date field is a datetime data type, you are only getting 2018-1-2 00:00:00.000 instead of the whole day. You would want to do &lt; 2018-1-3 instead. 
Nah m8 it's an octothorp table.
What do you mean the column is wide? like its full of empty spaces? what exactly?
It looks like this in the output, and trying to force the column to a specific width isn't helping: PHONE ------------------------------------------------------------------------------------------------------------------------------------------- 555-1234
Just a guess, but, since ORACLE must decide the return type at compile time, it sets it to a varchar with a silly length so that it fits any return value possible. You can ignore that display (it won't matter in your application anyway, it's just the Oracle cli that shows the issue) or wrap the expression in a cast to a varchar of shorter length to force a smaller display size for the result column.
I got it! Thanks for the help.
Can't seem to figure out how to write this query: Display the CCName of the client and the CCNAME of the client who referred him or her, for every client referred by a client in the music industry.
 SELECT cc1.ccname AS client ,cc2.ccname AS referring_client FROM corpclient cc1 JOIN corpclient cc2 ON cc1.ccidreferredby = cc2.ccid AND cc2.ccindustry = 'Music'; Fun Fact: You can join a table onto itself :)
Thank you so much! This has had me stuck for an hour!
what is even the question?
 SELECT z.CCName, t.CCName as 'was referred by' from CORPCLIENT t join CORPCLIENT z on t.CCID = z.CCIDReferredBy WHERE t.CCIndustry = 'Music' [Here's the SQL Fiddle.](http://sqlfiddle.com/#!18/488dd/1/0)
I can see I totally wasted my efforts on you.
and how it behaves with null in columns.
&gt; This so to avoid things like "if ($prevName != $row-&gt;name ) { ... }" in my code. and yet you would wish to code even more complex code into your SQL? come on, processing the results of a dead-simple query for cosmetic purposes is ~*supposed*~ to be done in the application language
what you posted is as simple -- and efficient -- as it gets go with that
[Come again?](http://sqlfiddle.com/#!9/abc0f5/2)
It is really working well. &gt;having to revert to bitching on Reddit like a spoiled child and calling the staff around you basically incompetent. Dude, if you're a DBA and you're reading about the environment they have set up... how does it not SCREAM incompetence? Someone else here commented that a remote server where I can pull data at will is a horrible idea. I can already do that. I already have unrestricted read access to the "production" server that isn't a production server. I am already a power user/miniDBA on the other two servers. I could LITERALLY set this job up in SSIS and have my data on an automated refresh in less time than it has taken me to take the time to talk about this issue here. &gt;If you don't think you'll be interviewing for a place in the future and your resume won't slide across the desk of someone who's known you in the past, then I have news for you... Oh, lots of people I've worked for in the past ask me when jobs come up. If you hire me to get a job done (which in this cases involves automating processes that are currently being performed by a team of ~50, which is estimated to eliminate ~50% of their work load) then I get the job done and I don't let other members of IT get in the way because of incompetence. What are you kidding me? I can already literally do what I am asking for. "Well then why don't you?" What is your job again? 
I should have mentioned: [if you negate them](http://sqlfiddle.com/#!9/abc0f5/4)
OK I'm sorry to do the same thing as the other responder, but this really doesn't make a whole lot of sense to me. This is a pure display concern, is it not? How do you know Basil is an admin if that column is null? It's possible that a window function could help down the line in your code: https://www.postgresql.org/docs/9.1/static/tutorial-window.html
TIL
Do data in the back end. Do visualization in front end.
Where are you getting "cc1" and "cc2" from? 
What's the question?
&gt; This is a pure display concern, is it not? It is not. In the end I went with array_agg()
Cool. That makes sense. You want Basil to be linked to admin somehow, and if you want all admins in one row, then the agg functions make perfect sense.
Those are abbreviated table aliases for the tables in the query. So in effect, it actually reads: FROM corpclient AS cc1 JOIN corpclient AS cc2
When you run each query individually, are the names showing up once or multiple times? Are you using UNION or UNION ALL?
You need to be much more specific. Count unique values in a database doesn't tell me anything. Unique values of what? street addresses? zip codes? city names? business address ids? a combination of all of them? I'd recommend in the future posting example data from the tables, an example of what you are getting, and an example of what you want to see. This query looks very very wrong - most of us would just re-write a new one from scratch to help you.. so without source data it makes it very difficult. the answer to get what you want would be extremely simple for most of us.. if only we had some data :p
I am trying to count the number of times an address shows up in the database once. It represents the number of times the address was visited.
"union" de-duplicates results, while "union all" does not de-duplicate. But it is important to note that "duplicate" means the combination of all of your selected records. So for example "New York, 200, 100" and "New York, 300, 100" count as two distinct record selections that are not unique. So if you are getting repeating city names then it means your counts are different based on the way you structured your query. Not trying to be critical, but based on the group by and your use of "select distinct" and your "union"s, it seems like maybe you are just throwing random "de-dupers" at the problem instead of truly understanding how the data is structured. One trick I do a lot in Access is to do a two-stage query, where the first query is a make table query that stores what your subqueries are doing, then your second query joins the make table you just created plus any system tables from your db. 
learned that the hard way: Think twice before you use NOT IN. Is it a nullable column? But TBH: I usually need it on not-null primary key columns and it's no problem there.
 SELECT a.riid , a.email_address , a.birthday , MONTHS_BETWEEN(TRUNC(SYSDATE),a.birthday )/12 AS age FROM $A$ a WHERE MONTHS_BETWEEN(TRUNC(SYSDATE),a.birthday )/12 &lt; 21 
you could do it in one step probably something like... SELECT tblBusinessAddresses.CITY, COUNT(database.[BUSINESS ADDRESS ID]) As ct FROM tblBusinessAddresses LEFT JOIN [Database] ON tblBusinessAddresses.[BUSINESS ADDRESS ID] = [Database].[BUSINESS ADDRESS ID] WHERE tblBusinessAddresses.CITY IN ('BAYVIEW', 'CITY1', 'CITY2') GROUP BY tblBusinessAddresses.CITY tblBusinessAddresses.CITY 
It's difficult to work without seeing the source table. I will say that you have a lot of what seems to be unnecessary nesting going on in your SELECT clause: SELECT DISTINCT tblBusinessAddresses.CITY, ( SELECT Count(*) FROM ( SELECT DISTINCT T1.[BUSINESS ADDRESS ID] FROM [tblBusinessAddresses] AS T1 WHERE [CITY] = "BAYVIEW" ) AS T1 ) AS [TOTAL BUSINESSES], 
Of course it does.
are there any more quizes like this?
I can't help you then without data sorry. good luck bro
It did help. I used a where statement and union and it worked somehow. Thanks man.
TIL. thank you!!
I figured. But if I let it either product.speed or laptop.speed it would ignore the one I don't choose, i.e it would just be a single table's speed. Anyway to combine?
COALESCE(product.speed, laptop.speed)
worked thx
worked thx
T'is hidden below in a comment... I have no idea why. 
And now I know why, thanks!
[removed]
Oh, by the way. Had some really aggressive meetings today. Going to get what I want. In the future I will be applying for jobs with one of the three following titles: Analytic Architect, Direct of Analytics, or VP of Analytics. The size of the company will determine the role. The people I used to work for and have known me are in lower roles... for a reason. And many of them will sing my praises from up on high and tell you how I walk on fucking water. Then some others will tell you that I am a straight up cunt. Some of them have lost their job because of me. But none of them are going to be reading my future resume and interviewing me. 
If your sure of your real life presence, then I'm sure you wouldn't mind posting your name on here, correct? Most people aren't so forward with their real life connections and the bragging of getting people fired. You seem to be OK with it though. So for everyone here, please - let's get your personal information since you're so sure of yourself. Then we can put proof to words. 
I'm stuck on the first question. I'm not sure why this is wrong. Can anyone help me? &gt; select name, sum(weight) OVER (PARTITION BY name) as running_total_weight from cats order by name http://www.windowfunctions.com/questions/1
Why would I do that and what benefit would it serve me? &gt;If you don't, then I'll just assume you're still some report-writing monkey that is a habitual liar. Yeah, you can do that. It's really no skin off my back. Good job NOT commenting on how incompetent the environment is set up and simply focusing on me. You'll go far in your life and career with that mentality.
Wow, I didn't know that. Sorry about that. 
Also, habitual liar? I came to this subreddit and created this account when I knew **NOTHING** about SQL. Literally nothing. I transitioned into Analytics after working in IT for &gt;10 years. I have certifications in COBOL, RPG, PERL, FORTRAN, C. I have set up networks and servers before. I've worked on DOD deployment projects in healthcare. Analytics (and by consequence, databases) is what I enjoy and where I want to mature my career. I'm not going to take shit off your dumb ass if you can't articulate yourself or point to a specific ISO regulation. Give me what the fuck I'm asking for or I'm going to go over your head and get what I want. I've done it before and there is a long list of casualties of people who fought me... plato o plomo? You want to be my friend or my enemy?
You've spent 3 posts complaining that the IT team won't let you make code changes to a production and SQA environment. That's exactly what they SHOULD do. * SQA should never change with comparison to prod. * Prod should never have code that's added to it by a developer, but by an infrastructure or QA team. Props to this IT department for trying to stick to doing the right thing and not letting someone who doesn't understand these concepts to come in and start fucking up their systems. At any rate, you have 3 posts with multiple comments reaching out to a subreddit group on how to handle a really insignificant issue within the work office and then expect us to believe you have the experience for a senior-level management position? Please. I'm done. 
Actually, nope, no, that isn't at all what I've said. I have no ambition to make production changes without going through change request. I am saying I want production grade data. You apparently aren't reading. &gt;SQA should never change with comparison to prod. Right... so... between SQA and Prod there is a $20,000,000.00USD variance between spend YTD. That means in less than two months the variance is &gt;5%. So if I were to start wanting to develop a predictive model... how exactly would I do that considering I cannot put sprocs/etc into Prod without a CR, but I have no idea if they should go to prod because I can't validate that they work... because the data is bad? Are you not paying attention? &gt;At any rate, you have 3 posts with multiple comments reaching out to a subreddit group on how to handle a really insignificant issue This is a **CORE** issue for my group. Apparently this subreddit is filled with DBAs. I am currently fighting with my DBA and we are going to have to get VP's involved to settle this argument. In the end I am going to get what I want. I'm glad you find this issue insignificant. I am being paid to make it significant.
what is the "window command" in q10? I just used ntile() and put "--window" on the last line :-)
I'm not sure what you're after here, so going to give a couple of answers. 1) For better results, use a flash or take the photo in a brightly lit area. You might also consider scanning the page instead of photographing. 2) Tables like this are hard to query. When you're doing a join, put the index finger of your left hand on on the upper-leftmost cell (excluding headers) of the driving table. Next, put the index finger of your right hand on the upper-leftmost cell (excluding headers) of the table to be joined. Compare the values in each of the rows your pointing at, paying especial attention to the columns in the join criteria. If those match, then call out the contents of both rows to your assistant, who will jot them down in a new table. Move your right index finger to the next row in the joined table and repeat until you get to the bottom of the joined table. Now, move your left index to the next row in the driving table, your right index to the first row in the join table, and repeat the previous steps. Ask your assistant for the new table they've jotted down, and continue on with your query as if what the assistant has written is a table in your database. 3) George Sherman is underpaid. Boris is jealous of George cause he gets a bonus, though.
When logic depends on different records, you need to bring those records (or lack there of) together. This can be done with a join or aggregate. I think an aggregate is better in this case. SELECT StuID FROM Enrollment GROUP BY StuID HAVING MAX(CASE WHEN ClassID = 'ART103A' THEN 1 ELSE 0 END) = 1 AND MAX(CASE WHEN ClassID = 'MTH103C' THEN 1 ELSE 0 END) = 1;
 SELECT STUID FROM sometable WHERE STUID IN ( SELECT DISTINCT STUID FROM sometable WHERE CLASSID = ART103A ) AND STUID NOT IN ( SELECT DISTINCT STUID FROM sometable WHERE CLASSID = MTH103C )
This sounds like a you need to know of a way to count your transactions and then query a specific sku. So as to not write it for you (it is a very simple query), investigate the COUNT() function as well as the GROUP BY clause. To get a specific SKU, look at what adding a WHERE clause to your query will do. Rule of thumb is that most queries need three parts. SELECT, FROM and WHERE You should be able to get what you need with simply modifications to the above. So show me what you got and we can go from there.
&gt; Don't ask for things you can't have, like creating tables and stored procedures in PROD reporting. Dude, I want to make sure you understand something. I am NOT asking to do this. I am merely asking to have production grade data in a separate place and server where I can do this...so I can develop and test and get it ready to go to prod. Like seriously? What is the flak here? I have been in meetings all day over this issue, and I *WILL* get what I want. But what the fuck are you all even talking about?
The problem is I can’t query this for a specific sku, that’d be easy. I need this info for all skus; which is where I’m stuck. 
Literally just a selected SKU ID? SELECT sum([SKU Units]) TotalUnits ,sum(Sales) TotalSales FROM table WHERE [sku id] like '%742%' Or do you need to show multiple SKUs? If so can you expand on how SKU ID is structured? e.g., is the part of the code you're looking to total up something like the first three digits of the field?
Well, show me what you got. How I would approach this is to see if I can sum all of my Sku Units and then group them by the SkuID. What this would do is show me a distinct list of SkuIDs and the number of SKU Units that were transacted regardless of how many times that SKU was transacted on. So this TransactionID| SKUID | SKUTransactions | Sales ---|---|----|---- 1| A | 16 | 200 2 | B | 22| 400 3 | C | 13 | 150 4 | A | 11 | 225 5 | B | 6 | 330 Would end up looking like this SKUID | SKUTransactions | Sales ---|----|---- A | 27 | 425 B | 28 | 730 C | 13 | 150 Is that correct? Does that make sense? Again, I am trying to point you in the right direction as opposed to writing it for you.
 SELECT STUID FROM Table WHERE CLASSID
You need to cross join for Cartesian product. SELECT * FROM R_1 CROSS JOIN R_2 CROSS JOIN R_3 ... ;
Call it a data lake, a sand box, whatever gets your dick hard. Just give me what I want. I don't give a shit how you sell it, or how you talk about it. I need a sever that isn't going to go down 3 nights a week because another group is testing. I need a database with production grade data. Beyond that you can figure shit out and call it whatever you want.
And again, when you use the word "stubborn" I don't think you understand that I'm the kind of guy that will walk into the corporate lawyers office and sit down to talk about ISO regulations and what we really can and cannot do. So then I'll make sure senior legal is in alignment with my strategy before escalating things, so when your "weak ass" tries to talk about how "stubborn" I am... you get a rude awakening about what is and is not ISO compliant. Give me the regulation, give me what I want, or else. This is a very simple exchange here. Based on everything I have architecturally described here I have a feeling you would just end up doing what you're told, hating me forever, and whatever else it is you do when you aren't being an incompetent sod.
Oh hey, just to REALLY put things into perspective here little guy. Your boss reports to a boss. That boss reports to a boss. That boss's boss is the same rank in the company as my boss. My boss's boss's boss has told me to be disruptive. He reports to the CEO. Wanna play some dice here? What's the easiest fucking solution for you that meets my needs? Be grateful that your counterpart in my company is not being pushed like I am pushing you for an answer and that I am trying to give her the right answer **before** the meeting we're going to have. 
&gt; Oh hey, just to REALLY put things into perspective here little guy. &gt; What's the easiest fucking solution for you that meets my needs? I gave you a blank database and access to reporting data. It took 5 minutes. Your demanding that I take 2 weeks ordering a new server, installing software, setting up security, moving reporting data, and setting up a process to refresh it. **What is the difference between the two solutions that warrants 96,000% more effort on my part?** P.S. My boss reports to the CEO.
As others have shown, there are multiple ways to do this. If performance actually mattered, you would test different queries. For simplicity in a beginning SQL class, this would do: SELECT STUID FROM &lt;table_name&gt; WHERE CLASSID = 'ART103A' AND STUID NOT IN ( SELECT STUID FROM &lt;table_name&gt; WHERE CLASSID = 'MTH103C' ) This assumes a STUID cannot be enrolled in ART103A more than once. Otherwise, you'd need a SELECT DISTINCT.
Just to make sure I'm totally on the same page as you... I have a blank database on the DEV server (C) which I have full rights over. I can pull data at will from the DEV's copy of the PROD database... except it is months out of sync. Missing columns. Except On server (B) I have full rights over the SQA copy of PROD. I can do whatever the fuck I want. This environment is mainly the same as prod but varies from &gt;20M in YTD spend in one specific data source. On sever (A) which is Prod, I have read access only. So given all of these rights, I could just pull data and do whatever I want without you. I'm not really A) willing to do this because it isn't my job, B) willing to push security issues by doing this and having to beg forgiveness. So I'm losing my mind here. You seem to be agreeing with me! Give me some prod quality data on my blank workspc database on DEV server and let me go buck wild. Solution it however you want. 
Not trying to nitpick your query, but Is it necessary to use distinct clause in your derived tables? Also in your select statement it would probably be a good spot for the distinct. 
Well played. 
I'll break down where I'm coming from. I'm not fighting you. I like you. Your work is interesting, and you share interesting sql problems from your work. You gave me gold a few times too, which is awesome. You've been getting a lot of opposition in this post. Maybe you attributed a couple replies to me I didn't make. I never said anything about millennials or datalakes or whatever. I hope that clears the air a bit. This post. Your IT is blocking you. You've explained their reasons. Maybe their reasons are legit. Maybe they're idiots. Maybe they just don't like you. I don't know. I'm neither going to defend your IT not validate your outrage. I don't have their side of the story. That's between you and them. Your company politics is irrelevant to me. Your boss' boss' boss tells you to be disruptive and threaten peoples jobs or whatever. More power to you. Sounds like you've got that angle covered. You've also asked for potential alternatives. That's where I come in. I manage this exact environment at my company. Machine learning model development, testing, and deployment. I'm on your side. I sympathize that shitty data is worthless for modeling, while it is find for normal development. We've got a setup that sounds super easy to setup over there. It works pretty well for us. Let me explain. We have a Prod reporting server with 100% data. We have a Dev reporting server with 10% data. We have a data science sandbox on the Dev server. When our data scientists need data, they pull it into their sandbox from Prod. Our DEV server has a light load, so when we're training models they have plenty of power. When they max out the server, it's not a big deal because it's DEV. Translating that to your environment, I'd give you read access to pull data from QA (B) and give you full control of a blank database in DEV (C) to stick it in. QA sounds like it has week old prod data, which is good enough for model development. You can push the DEV server all you want and I'm not going to lose sleep over it. The QA server has maintenance, but that's okay. Once you pull the data to DEV, you can train all night and not get interrupted. If you need new data and QA is in maintenance, it's not the end of the world if you wait till tomorrow. That's different than kick off a model and having it killed when it's 80% done. The benefit of this design is takes 5 minutes to setup. Your team gets read access to the existing QA reporting data, and you get a black database on DEV. I suspect you'd get much less resistance asking for that. If you don't like that advice, you don't have to take it. If I'm missing something that makes it unworkable, we can discuss and figure out what needs to change. If it's unworkable, and you don't care about figuring something else out, that's cool too. If you think the advice is alright, but want to push for more, hell yeah man. The more you can get the better. It won't be 5 minutes to setup though.
&gt;Maybe you attributed a couple replies to me I didn't make. This is the hive-mind. Don't be offended if I speak to you like someone else. I'm here for answers and sometimes you need to be an asshole to get them. I would rather look like a punk here then in the board room. &gt;This post. Your IT is blocking you. You've explained their reasons. Maybe their reasons are legit. Maybe they're idiots. Maybe they just don't like you. I don't know. I'm neither going to defend your IT not validate your outrage. I don't have their side of the story. That's between you and them. This in and of itself tells me that my position is vindicated. We are not friends. I've been cordial to you and I enjoy commenting on posts, getting your feedback, etc. But the mere fact that you would say this gives me great solace that I am not off the reservation here. Mind you, this is not a trivial decision for me to make with respects to my career. You shouldn't pick a fight you can't win, but it seems here I can win this one. &gt;We have a Prod reporting server with 100% data. We have a Dev reporting server with 10% data. This was the exact solution I proposed today and it was shot down by the DBA. Literally the EXACT solution. The rest of your post is great. Sure do it. I don't have an opinion on how you architect whatever you architect so long as it meets my minimum needs. Personally I would preferred to be spoiled a bit and get something above my minimum needs since I'm a baller, but whatever. They are not on board with any of your suggestions. They do not believe I need prod quality data, or see a reason for why I should need it. 
&gt; They do not believe I need prod quality data, or see a reason for why I should need it. I'm sure they don't. They have no training or understanding of why that it the case. Report and analytics development do not need good data. Developers can write a report that applies good logic on bogus data, and there's no problem at all. It get's validated in QA, and it's all good. They don't understand why that wouldn't be the case for modeling. With reports, logic is applied to data. With modeling, logic is created from data. If you train a model of fiction, it will predict fiction. You need to train a model on reality to predict reality. And not just a fraction of reality. If a segment of data is missing, the model cannot consider that segment, and it will produce nonsensical results when that segment is later introduced. They have no clue about any of this. They need to understand that any work you do on dev data, has to be completely redone from square one moving to qa. But if you do work on qa data (B), it is perfectly transferable to other environments. If they understand that, and still want you to work with dev data, fuck them. If they can't clearly repeat the ramifications back to you, it's not explained well enough to them. -- But the last thing I want to clarify is I still don't like your phrasing, "I need prod quality data". You need to use prod quality data. You need access to prod quality data. You don't need prod data delivered to you. It is perfectly acceptable to gather the prod quality data yourself. I think we agree on that, but I haven't heard you say it.
&gt;and still want you to work with dev data, fuck them. That's what they want. Do you see where I'm coming from now? To me this is a **critical** issue. I am not trying to be that guy. &gt;"I need prod quality data". You need to use prod quality data. You need access to prod quality data. You don't need prod data delivered to you. It is perfectly acceptable to gather the prod quality data yourself. I think we agree on that, but I haven't heard you say it. No. They will not do a linked server. They will not do linked databases. They are specifically stating that this is failing audit and ISO compliance.
You might also consider EXISTS and NOT EXISTS so that you aren't dragging around so much data. SELECT STUID FROM STUDENT WHERE EXISTS ( SELECT 1 FROM STUDENT_CLASS WHERE STUDENT_CLASS.STUID = STUDENT.STUID AND STUDENT_CLASS.CLASSID = 'ART103A' ) AND NOT EXISTS ( SELECT 1 FROM CLASS_TABLE WHERE STUDENT_CLASS.STUID = STUDENT.STUID AND STUDENT_CLASS.CLASSID = 'MTH103C' ) 
&gt; &gt; and still want you to work with dev data, fuck them. &gt; That's what they want. Do you see where I'm coming from now? To me this is a critical issue. I am not trying to be that guy. You took that out of context. I doubt they understand IF you develop in dev, you'd THEN redevelop the exact same shit over again in qa, using qa resources. They know that dev data is garbage, and it makes some things, like validation, difficult. They probably misunderstand your objection to mean that. They don't understand that dev data invalidates any model work done. That doesn't make logical sense from a normal development perspective. &gt; No. They will not do a linked server. They will not do linked databases. They are specifically stating that this is failing audit and ISO compliance. I don't memorize ISO standards, but I agree with disallowing linked servers across environments. That's bad practice for lots of reasons. I can certainly understand it failing some sort of audit. However, a linked server isn't required or preferred to move data. SSIS is safer, faster, and doesn't violate anything.
Without respect to bullshit words I was hired to use Tableau to visualize a very complex SQL process. I don't just need access to prod quality data, I need it in an environment where I can create tables, sprocs, etc., etc. They are saying this is not possible.
&gt;You took that out of context. I doubt they understand IF you develop in dev, you'd THEN redevelop the exact same shit over again in qa, using qa resources. They know that dev data is garbage, and it makes some things, like validation, difficult. They probably misunderstand your objection to mean that. Bitch, I ain't go no motherfucking time for this level of trying to help other motherfuckers do their job. I love you, but I have zero time for this type of shit. &gt;I don't memorize ISO standards Then give me what the fuck I want. I will absolutely go to the lawyers and look them up. I will absolutely say its because you made me do it. I will be that guy. 
Like what the fuck, dude. Isn't this you're fucking job? Do you want to go out for drinks and shit? Can't you design me a real sleek solution and be a bro? If the answer is no... how exactly should I deal with that? That is a rhetorical question because I have already pushed things waaaay beyond the scope of this silly thread.
&gt;You may not have the time, but I would have my sandbox already. Why? Because I do know how to do their job, and I can explain in terms they'd agree with why it's for the best. Hahaha, oh, you have no idea how fucked up things are here. Cheers, mate. &gt;If you do get a lawyer to look it up, one reddit gold says they find a standard about cross environment access. Ah bullshit, if my Windows Authentication expands across both servers, and I can use the SQL wizard to just take a table from server A to server B... then a remote server isn't against any rules. That would be fucking retarded to imply. [I made my bones when you were going out with cheerleaders.](https://www.youtube.com/watch?v=9DZNDEqcSi0) 
Video linked by /u/notasqlstar: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [That Kid's Name Was Moe Greene ( The Godfather II 1974 )](https://youtube.com/watch?v=VsbyvuO_AqM)|Shuvo Sarker|2015-06-22|0:03:52|2,019+ (95%)|844,268 &gt; There was this kid I grew up with. He was younger than me.... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/notasqlstar ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=du0scph\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
select L from R1, R2 ..., RN where C is exactly how I would get attribues L from the cartesian product of N relations with condition C in SQL. Is someone telling you this is wrong?
Wanted to make sure
100th comment. I'm calling it. This is the most commented post in r/sql history.
Without any other qualifiers, `JOIN` is an `INNER JOIN`. Without any other qualifiers, `OUTER JOIN` is a `LEFT OUTER JOIN`.
Additionally, you can just specify LEFT JOIN or RIGHT JOIN instead of including OUTER. Depends on your preference and readability requirements. 
Sometimes it's better to put pen to paper or marker to whiteboard. Both for you mentally/conceptually and for others to see that you're actively working and hopefully won't interrupt you mid-thought. Absolutely, I have days where I'm exhausted because I've done a lot of good work and made real progress. And then there's days where I'm exhausted because I've only moved things a quarter of an inch down the track despite working for 6 hours on it.
 DECLARE @TABLE TABLE ( ClassID CHAR(7) NOT NULL, StuID CHAR(5) NOT NULL, Grade CHAR(1) NULL ) INSERT INTO @TABLE VALUES ('ART103A', 'S1001', 'A'), ('ART103A', 'S1002', 'D'), ('ART103A', 'S1010', ''), ('CSC201A', 'S1002', 'F'), ('CSC201A', 'S1020', 'B'), ('HST205A', 'S1001', 'C'), ('MTH101B', 'S1020', 'A'), ('MTH103C', 'S1002', 'B'), ('MTH103C', 'S1010', '') SELECT DISTINCT StuID FROM @TABLE AS T1 WHERE EXISTS ( SELECT * FROM @TABLE AS T2 WHERE T2.StuID = T1.StuID AND ClassID = 'ART103A' ) AND NOT EXISTS ( SELECT * FROM @TABLE AS T3 WHERE T3.StuID = T1.StuID AND T3.ClassID = 'MTH103C' )
I'm Andy. It is now more locked down. There may well be other holes so let me know if you find one. 
Author here. Yeah I'm not that keen on Q10. I'm considering removing it. I like the idea of teaching people the window clause command but its not really very important. I'm open to suggestions. 
Unions can be useful for getting a combined output of different types of joins, they are a bit trickier to master but can be immensely useful 
Anytime data is selected from more than one table, a join is performed. JOIN is shorthand for INNER JOIN LEFT JOIN and RIGHT JOIN are shorthand for LEFT OUTER JOIN and RIGHT OUTER JOIN. I like the shorthand better because there is no left/right inner join anyways and it's one less word you have to type.
Damn I love these clear concise answers to a specific question, rather than some bulllshit like, "what are you trying to do"
mysql's GROUP_CONCAT 
LISTAGG was added with SQL:2016: https://modern-sql.com/feature/listagg
You've got mixed JOIN syntax. Your query: FROM BalanceToUserSettings AS BTS RIGHT OUTER JOIN State AS ST ON ST.GeoState=BTS.GeoState, DataOptions AS DOPT WHERE ST.ReturnsSupported=1 AND DOPT.CmpId=1 Is equivalent to saying: FROM BalanceToUserSettings AS BTS RIGHT OUTER JOIN State AS ST ON ST.GeoState=BTS.GeoState CROSS JOIN DataOptions AS DOPT WHERE ST.ReturnsSupported=1 AND DOPT.CmpId=1 Prior to ANSI 92 SQL (as in 1992) the way you would do an INNER JOIN would be like this: SELECT * FROM TableA, TableB, Table C WHERE TableA.id = TableB.id AND TableB.id = TableC.id Note that if you have no WHERE clause equating the two tables, you've got a CROSS JOIN: SELECT * FROM TableA, TableB WHERE TableA.Field1 = 'Value' The problem with this syntax is that you mix the join conditions in with the filters, so you'd see queries like this: SELECT * FROM TableA, TableB, TableC WHERE TableA.id = TableB.id AND TableA.Field1 = 'Value' AND TableB.Field9 = 0 AND TableB.id = TableC.id AND TableC.Value = '12A' See how it's difficult to tell what's a part of the JOIN logic and what you're using to filter the query? It's all mixed together. You could really easily delete the JOIN condition and suddenly you've got a CROSS JOIN. Worse, there was no standard syntax for OUTER JOINs. It was all based on the RDBMS vendor. For example, given this query: SELECT * FROM TableA LEFT OUTER JOIN TableB ON TableA.id = TableB.id SQL Server had this syntax: SELECT * FROM TableA, TableB WHERE TableA.id *= TableB.id Oracle and DB2 had this syntax: SELECT * FROM TableA, TableB WHERE TableA.id = TableB.id(+) Informix had this syntax: SELECT * FROM TableA, OUTER(TableB) WHERE TableA.id = TableB.id Worse, there are certain OUTER queries that just don't work well with this syntax with everything in the WHERE clause. So, beginning with ANSI 92 SQL, they standardized the modern JOIN syntax. These include: [INNER] JOIN ... ON LEFT [OUTER] JOIN ... ON RIGHT [OUTER] JOIN ... ON FULL [OUTER] JOIN ... ON [CROSS] JOIN NATURAL JOIN Critically, **ANSI *did not* deprecate the older comma join or ANSI 89 join syntax** so it's still valid syntax in essentially every RDBMS. You should *never* use this older syntax, as essentially everybody agrees that it's difficult to tell what's going on. I mean, look at this: SELECT * FROM TableA, TableB, TableC, TableD, TableE, TableF WHERE TableA.id = TableB.id AND TableA.Field1 = 'Value' AND TableB.Field9 = 0 AND TableB.id = TableC.id AND TableC.Value = '12A' AND TableE.id(+) = TableB.id AND TableC.id = TableF.id I tell you there's a bug in the query. What joins are being used here? Is there an accidental cross join? Am I using an unsupported join syntax? Was a condition inadvertently dropped? What can safely be removed from the WHERE clause if you want to modify it for another query? That is, what's the difference between JOIN logic defined by the application's structure and filter logic defined by what you're trying to query? 
how about a "select star" view with the old table name?
Merge joins, hash joins, nested loop joins, semi joins, and anti semi joins. Those aren't syntax, and you don't need to know them to write sql. They are useful to understand when wanting to optimize queries.
Start of [Date Period].
You want a synonym: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-synonym-transact-sql 
I want to be able to aggregate text fields into one comma separated list without having to use any hacky xml crap.
Max across many columns (as of 2012 this wasn't available, don't have experience beyond that one).
how about a self join SELECT DISTINCT a.STUID FROM table a LEFT JOIN table b ON a.stuid = b.stuid WHERE a.classid = 'ART103A' AND b.classid != 'MTH103C'
pull the key clauses out of the CREATE TABLE statements, and run them in ALTER TABLE statements after the tables have been created
[And String_Agg in MS SQL 2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql)
[MS SQL 2017 has you covered](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql)
Is it possible to put a link to the documentation in the hints? I couldn't find anything on that window clause after a few minutes of Googling. It was a fun quiz though
Bet you thought you would never be this excited about calculus!
shit's expensive tho 
Yeah Oracle’s TRUNC is a lifesaver. I hate having to CAST(CAST (x AS DATE) AS DATETIME) just to get a start of day with a time in MSSQL. Even more so is having to combine DATEDIFF and DATEADD to get start of week, month, year.
Thanks for this! It says NULL values aren’t returned. I’m assuming you can just use ISNULL(x,’BLANK’) or something? I don’t have a 2017 box yet so can’t try it myself.
I really like the stanford database course https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about other bookmarks I have that might be of some help: http://studybyyourself.com/seminar/sql/course/chapter-1-introduction/?lang=en https://www.khanacademy.org/computing/computer-programming/sql http://sqlbolt.com/ http://sqlschool.modeanalytics.com/ http://sql-ex.ru/ http://sqlfiddle.com/
relational algebra
Maybe I'm dumb but I can't see how to edit the wiki - https://www.reddit.com/r/SQL/wiki/index We should totally add these links to that list.
First off, this question should be stickied for exactly how a question should be asked so thanks for that. SELECT FaTable.[Employee Number], FaTable.[First Name], FaTable.[Middle Name], FaTable.[Last Name], SupervisorTable.[Full Name] FROM FaTable, SupervisorTable WHERE LEFT(FaTable.[Last Name], 1) BETWEEN SupervisorTable.[Last Name Start] AND SupervisorTable.[Last Name End] There are differences in syntax in access and SQL server in some cases, but in this case I believe it is valid in both. Normally I would have just done a join and that criteria would go in the join instead of the where clause, but I can't remember if access will let you do that or not. 
This should work in Access: SELECT A.[Employee Number] , A.[First Name] , A.[Middle Name] , A.[Last Name] , B.[Full Name] FROM FaTable A LEFT JOIN SupervisorTable B ON LEFT(A.[Last Name],1) &gt;= B.[Last Name Start] AND LEFT(A.[Last Name],1) &lt;= B.[Last Name End]; I think that query should work in SQL Server too.
you will need the COUNT(*) function in a GROUP BY query
Are you using pgadmin? There's an option somewhere deep in the menus to turn off that quoting.
I am actually. Hmm I'll try to find that, thanks. 
One of the records must have a comma in it
It sounds like there is an intermediate step where you need to normalize this data but it is hard to visualise what you need to do from your description. Are you able to post some example data and the layout of the related spreadsheet? The reason ordinary SQL isn't working for you is that the data isn't structured relationally so some amount of jiggerpoker will be required first. 
When importing CSV into excel, use Data &gt; From Text &gt; Choose your csv file Then adjust the quote character.
As CMS States, the ReportID is the primary key for everything in these files. I'm unable to easily find the header file (like anything is easy with CMS) so can't be of too much help. Basically though, you would need to grab the ReportIDs linked to the zip codes, then go from there to get back to the Facility info relating to those ReportIDs. Be careful with any joins so that you don't replicate any data.
The CMS website is a nightmare and now that I've used it I fully understand why our healthcare system is so bloated and out of control. Every time I need to get some"standard" from their site it's changed from the last time. The format for files changes regularly and unexpectedly breaking our SSIS packages and forcing logic rewrites for no reason other that someone decided to move the headers or remove columns to update a description.
I've thought about this, but I'm just not sure what kind of statement I need to write for it. I can grab the relevant ReportIDs with any particular data field easily using '=', 'IN', 'CONTAINS', etc., but I don't know how to incorporate that with another SELECT; aliasing hasn't worked the way I thought it would. I tried a JOIN last night but couldn't get the syntax to work with SQL Server 2017. Coming from a coding background, I would love to SELECT the zip code FacilityIDs, store them in a variable, then match on them: WHERE FacilityID IN (zipcodes) This isn't correct syntax, though, and I'm having trouble finding where to learn how to generate this kind of query.
SELECT * FROM congrats WHERE user = ‘riyaxo’
Try aggregating, to get all the records you want together, and a having clause as the filter. SELECT worksheet , line , MAX(CASE WHEN column = '00300' THEN value ELSE NULL END) AS zipcode , MAX(CASE WHEN column = '00400' THEN value ELSE NULL END) AS facilityname FROM data WHERE column IN ('00300','00200') GROUP BY worksheet , line HAVING MAX(CASE WHEN column = '00400' THEN value ELSE NULL END) AS facilityname = 'Hospital 12';
I'm guessing you just imported the files into tables using the import wizard? If so, if you send me the create scripts for those tables and the specific cost report files (filenames should be enough, or give me the year and file type) I'd be happy to take a more in depth look.
If youre just looking for an example, heres one (on mobile atm) Select * From TableA as tA Where ReportID in (select ReportID from TableB as tB where someField = 'filtervalue')
If I understand what you're asking...re-write this as a group by that produces the same result. Then you can just remove the aggregation to get what you want. This doesn't look like SQL Server though, NZ is an access function. 
What you probably want are windows functions. https://www.red-gate.com/simple-talk/sql/learn-sql-server/window-functions-in-sql-server/
How do you enjoy working with Aurora vs an on site or Oracle / MS SQL RDBMS?
I haven't seen that. I will check it out for sure. Bummer that you still have to use the flat files to pull new records though. That's pretty much what we use it for exclusively... Thanks for the link!
The article seems to cover what I need, but do you know if those window functions work in MS Access 2013?
I don't think they have that in Access, but I'm not sure. I couldn't find anything immediately from google.
Check out [Domain Aggregates](https://support.office.com/en-us/article/Access-Functions-by-category-B8B136C3-2716-4D39-94A2-658CE330ED83)
DATEADD(SECOND, [your epoch column], '1970-01-01')
SQL Server 12.0.5203.0 it says I'm not a DBA FTR
MS SQL! (I figured out what you were asking)
I really appreciate your help here. Unfortunately, the query you gave produced the same results (minus the quantity column). If I'm looking at my original query with dates of 2/5/2018 - 2/9/2018, there are 12 rows in the output. Three of the rows have a quantity of 2. I'm hoping for a query to output 15 rows instead of 12.
I think I found this but where do I put my column in? SELECT DATEDIFF(s, '1970-01-01 00:00:00', GETUTCDATE())
good call, i'll edit to make sure it isn't used :)
Healthcare here, it's used to quickly develop little databases and little dashboards, mostly just shared with a small number of people and used to just quickly get something done when getting an official analytics project would take too long, or the project is expected to have a short lifespan.
Just so I understand... Instead of something like Product1 2 Product1 2 Product2 1 Product3 1 You want Product1 1 Product1 1 Product1 1 Product1 1 Product2 1 Product3 1 ? 
Access isn't horrible, it's just makes it too easy for someone lacking knowledge to implement a half-baked solution that mostly works. 
Healthcare here as well. We use access.
I like to think of Access as a way to circumvent normal IT processes. I say that not necessarily in a bad way, but that it is what it is... As others have pointed out, in a pinch it is easy to stand up and get running without any official sign off. Additionally, it tends to be installed alongside other Office applications and so requires no new tickets to be submitted to get installed. When I was doing some auditing/dev consultant work I ran into this frequently in companies where internal tools were lacking and development resources extremely limited. This typically was explained by management stating that their groups requests likely wouldn't be a priority. These groups would then have some Access DB backend with some forms sitting on top of it. When the dev resources became available and the developers started looking at the "tool"... it was rarely met with positive remarks. That all said, personally, I think Access is a great tool to take Excel BA's and get them thinking in a relational manner. The syntax can burn in hell though.
In my company the people who should be using Access don't know how to, and use Excel for everything instead. Access often gets used as a quick-and-dirty solution by people who should know better, which I think contributes a lot to its bad reputation.
&amp;#x13 is the proper way to encode that for HTML/XML, no?
Yes, that is the output I am looking for. I don't know what CTE is but I do know my way around VBA. I should be able to modify the output by looping through it with VBA. Thanks for the idea!
I think that at some point one of my predecessors (like 2 ago) was actually using xml but now it's being ported into a spreadsheet so XML isn't being used anymore. The actual text (if markdown allows it) is:  To be honest, that would be the best but I wouldn't mind just stripping ALL of them out. I just don't know how many of these hidden characters exist. The users don't need these characters. I couldn't find a STUFF function without formatting for XML. Thanks for your help. 
Access is a great tool, as long as you know how to use it. I've built some great front end apps with Access which use SQL Server on the back end. My company's entire QA department runs on an Access application I built with an SSRS solution providing reports to their internal customers. Everyone loves it, and no one has any of the "typical" Access problems because it was well designed. Here's the analogy I use: Let's say you give someone a hammer and ask them to drive some nails with it. Later you come back and find that the nails are crooked, bent, and driven into the wood at crazy angles. Do you blame the hammer, or the person wielding it?
I work for a major Healthcare analytics firm, and I've spent most of my first year here migrating processes out of Access and into more resilient tools. It worked well enough, but maintenance, updating, flexibility, and audit logs all called for a better solution.
why? because there is no **pair of rows** where one is 1 and the other is 0 for the same business you probably have many businesses in the table with only one row so, yes, those do match up with themselves in the self-join... but if it's only one row, then it can't be both 1 and 0 at the same time, and hence no results
I work in a reporting team for a major health insurance company. My perception is that, at least in operations and "non-analytics to semi-analytics" groups, Access is the go to application when somebody need's to extract and manipulate relatively large amounts of data quickly. As somebody here already mentioned, I think this has to do with it being included in the Office suite and it's relative ease of use. That being said I know better solutions are available like SQL Server / Tableau / SAS. Unfortunately it seems like those are limited to "actual analytics" groups. I put in a requisition for Tableau one time because I wanted to learn it and thought it would offer better reporting solutions for my department (keep in mind I was one of two people in my department that did anything resembling reporting). My BSL came back and informed me the license was $2000 and my director shut that request down shortly after. Ironically after I left I learned that director was using a Tableau dashboard to monitor some other project and loved it. My impression so far is that unless you have a specific need AND already know how to use one of these more advanced analytics tools, management will not want to pay for it.
For a quick and dirty query I run an access query a few times a week 
Something like this might work: SELECT fa.EmployeeNumber , CASE WHEN mb.TransferTo IS NULL AND mf.Base IS NULL THEN fa.HireBase ELSE CASE WHEN (mb.EffectiveDate &gt; mf.EffectiveDate) OR (mf.EffectiveDate IS NULL) THEN mb.TransferTo ELSE mf.Base END END as CurrentBase FROM FaTable fa FULL OUTER JOIN MAXBaseTransferQuery mb ON fa.EmployeeNumber = mb.EmployeeNumber FULL OUTER JOIN MaxFlowThroughQuery mf ON fa.EmployeeNumber = mf.EmployeeNumber Might want to double check the logic, though. http://sqlfiddle.com/#!18/d1dd1/7
Totally agree. As a prototyping tool, or a mechanism for very quickly building an interface to a SQL Server database, Access is great. Once built, convert to an MDE, secure away the original MDB from untrained workers, and you've got a very robust, familiar GUI for staff to work with. If all the actual work is done through stored procedures and SSIS jobs, maintenance of the Access frontend will be minimal, and even when necessary, it will be rapid.
The STUFF / FOR XML thing is just a common trick for building a comma-delimited list. STUFF is just there to replace the leading delimiter. If you're passing it on to vb.net you probably don't need any of it.
Yes
Not on its own, but as a front end for SQL Server it can work fine.
Should an IT department move away from it, yes. Should other departments, no.
BTW, if you want to do it really efficiently and have access to do so, it's pretty easy to create a CLR custom aggregate to do it that runs a lot faster than using the XML kludge: https://www.mssqltips.com/sqlservertip/2022/concat-aggregates-sql-server-clr-function/
I'm in a manufacturing environment. Access isn't allowed anywhere near production processes, but it does get used as a poor-man's ETL tool for analysis work.
Went for a job interview today for a data intelligence analyst role. They mentioned they were doing their query designing in Access. Nearly noped out right then and there.
Looks like you need a comma between the first and second columns (between the q5 and the CASE).
This is frightening to read mate. 
When q5 is in (0, 5)
I work for a major financial company and yes, it's definitely in use all over
&gt; makes it too easy for someone lacking knowledge to implement a half-baked solution that mostly works So, like most modern web frameworks then?
My company still uses everything. I technically still have to support Windows Vista and we have documentation for Windows 98. But yes, Access is on that list.
I've recently learned to use it for etl. What's a next better step? 
We use it all the time as a middle man. Access supports linked tables via ODBC, so it gives end users a view into Oracle and Microsoft databases without having to write SQL directly. They get a read-only account of course, and not full reign on all tables. In addition they only hit a test system which is refreshed nightly. They can beat the shit out it all day long with their sorry-ass queries and it doesn't bother me one bit. But mostly they just use one of the stored queries to get what they need. Then they dump it into excel and use pivot tables to report on unruly data. But access is a one-way flow. Data comes out and might get stored in Access, but that data never returns to the original database(s). Typically it is only a stand-in product as part of the data flow chain. As far as Access as a database solution, it's piss poor. Ever have an MDB or ACCDB go south on you? If compact/repair doesn't fix it, and you can't manually export objects to a new file, you're screwed and you have to restore from your last backup. Access used to be a bit more useful prior to Excel 2007, back when Excel only supported 65536 rows of data. But with a million rows in 2007+, most of the time it's easier to just use Excel and do a bunch of vlookup() or index(match()) to "join" data across multiple named ranges or tabs. Probably the biggest beef I have with Access is that people get a result from a query they wrote and they think it's as good as gold. No review, no sanity check, no spot checking, nothing. Then they build some decision making on a piss poor query because they have no concept of 3-valued logic and how nulls can screw you, or the differences between inner, left/right outer joins, etc. They end up dropping a bunch of data because of using inner joins, for example. Then the population of data is incomplete and no one involves me until they come back to me and say "umm there's a huge problem with the database". Nope, you just suck at set theory and now it's my fault that you don't understand joins. But that's always been the challenge... companies have lots of "front end" solutions, but no matter what it is, it takes a bit of background on joining data in order for the results to be correct. So long story short, if you dumb down a program enough so that anyone can use it, expect stupid people to write incorrect queries. 
&gt; sorry ass-queries *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
SSIS
uhm did you win? I see you put is null at the bottom, versus = null. 
If you're using SSIS (which you should be if you're trying to pull data from different platforms into one location), then Attunity is your friend. There are other drivers out there. Attunity can be a hassle to install and there are marginal licensing expenses. But the capabilities are great.
Yeah it's fine and can work but the general rule is that if you're designing anything complicated with it, you should stop and find a better tool. People who are skilled enough to write good solutions in access know to use something else that would be easier to maintain in the future. Access has a few decent use cases but there isn't that much room between the use cases of Excel and the use cases of SQL server so it should be exceedingly rarely used for production. 
Bonus points if you can explain: 1) why '= null' doesn't work here 2) an example where you would use '= null' 
Yeah I believe I got what my professor is looking for. Thanks.
= null is conditioning only the literal word "null"? and you'd use it if you were looking for the letters "null" rather than simply an empty field
so what should I do ?
Thank you, but I also need to compare those open with thos closed, and I am not able to put them in a table and compare them. How should I do ?
I like Aurora a lot more than "on site" - mostly because of the out-of-the-box replications. We can replicate our database all over the world without any synchronicity issues. I haven't used Oracle.. vs MS SQL, I haven't used MS SQL in the last 5 years, so I don't know how well that works in azure, for example. I imagine microsoft offers a similar kind of service as AWS. I just noticed MySQL has some wonky behavior some times. For example, if you have a table with a varchar(10) and you try to put in anything longer, then mysql just cuts it off at 10. While most other databases would throw an exception.
depends on what you want to do, which isn't all that clear from your original post
I want to group the business open with the closed one in order to make comparison between open and closed businesses. Is it clear enough ?
A given business ID is either open or closed As an example, let’s assume Sears is ID 10. It’s current state would be 1 (open). What closed business do you want to compare it to? It won’t be ID 10 because a given business is either Open *or* Closed, not both at the same time So let’s say you want to compare Sears to Montgomery Wards. How do we know which business that is? What’s the relationship between Sears and Montgomery Wards?
Nope!!
It sounds like you'll want a self join here. SELECT b.* FROM [TableName] a JOIN [TableName] b ON a.DeptId = b.DeptPath WHERE a.DeptName LIKE '%[keyword]%'
You broke first normal form by putting multiple values into one column. No matter what you do, your query will be exceptionally difficult to write and will perform poorly. If you're on SQL Server 2016 or later, you have access to [the `STRING_SPLIT()` function](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql). '%' + DeptId + '%' like DeptPath That might work, but `uniqueidentifier` works kind of like a `binary(36)`. You've got to convert it to a varchar: '%' + CAST(DeptId AS varchar(36)) + '%' like DeptPath Mapping hierarchical data to the relational model is a known but rather complicated thing. Really, you should represent a hierarchy using a parent/child relationship and using a recursive CTE query to get the path (probably saved as a VIEW). There's an example of the basics of that [here](https://sqlsunday.com/2014/03/30/indented-parent-child-hierarchy/) or [here](https://www.codeproject.com/Articles/818694/SQL-queries-to-manage-hierarchical-or-parent-child). Alternately, you could use [SQL Server's `hierarchyid`](https://docs.microsoft.com/en-us/sql/relational-databases/hierarchical-data-sql-server) ([tutorial here](https://docs.microsoft.com/en-us/sql/relational-databases/tables/tutorial-using-the-hierarchyid-data-type)). I've never used `hierarchyid`, so I can't speak to how difficult it is to use. 
Access has poor data integrity. Every access source I've seen in the wild takes at least a full day to scrub all the fat fingered dates and other crap. If Access backends are allowed to live, they will get progressively more infected with garbage data.
Read T-SQL Fundamentals and then T-SQL Querying. Come back once you got those two read. 
I think you're over thinking it. Can't you just sum the advantage against the opponents? SELECT hero_name, SUM(advantage) AS sum_advantage FROM Hero WHERE other_hero IN ('Hero1','Hero2') AND hero_name NOT IN ('Hero1','Hero2') GROUP BY hero_name ORDER BY SUM(advantage) DESC LIMIT 2; http://sqlfiddle.com/#!7/ac8cc2/1
Yes! That is shorter than what I came up with, easier to code in, thank you. I'm fairly new in SQL so I need to play around with `GROUP BY` statement a little more, I don't really visualize it. I just realized how UNIONs are vertical and JOINs are horizontals that just blew my mind
Then you can drop the where clause and do something like Select a.name, a.is_open from business a order by a.is_open At the end instead. That will return all businesses by name and their status.
You can write SQL in Access. The editor sucks, and the dialect is... quirky. But you can do it. For non-trivial stuff, I'll often write it in Notepad++ and copy/paste it into Access. Then I can save the queries and their formatting outside of Access for future reference.
If you want to learn Tableau, you can download and use the fully functional public version for free. It does have restrictions on the data sources you can use, but that's not really a problem. The only downside is that you have to save your projects on their server and the entire world can see them. 
Data integrity is not an Access specific problem. Poor db and UI design can result in any database filling up with crappy data. 
That depends on the data source, what type of database you're loading it too, how big your budget is, and what the company will allow you to use.
One solution is to cast your datetime column to epoch (unixtime / julian). This lets you solve the problem by simply dividing by the number of seconds in the date range. Exactly how to do this depends on the database engine you use, I just mocked it up in SQLite for the test: sqlite&gt; CREATE TABLE test (ts REAL); sqlite&gt; INSERT INTO test VALUES (1), (10), (14.5), (14.99), (15), (25), (29.99), (44), (44.99), (45), (59.99), (61); So just create a single table with one column, and insert random values that we can group by 15-second intervals: sqlite&gt; SELECT ts, CAST(ts/15 AS INT) FROM test; ts|CAST :---|:--- 1.0|0 10.0|0 14.5|0 14.99|0 15.0|1 25.0|1 29.99|1 44.0|2 44.99|2 45.0|3 59.99|3 61.0|4 Notice how the CAST column matches the "group number" in 15-second intervals. You'd want to divide by 900 (60 secs * 15 minutes), and the CAST is SQLite-specific because it doesn't support `FLOOR(ts/15)` (which you probably want to use in SQL Server).. To illustrate what it might look like in the real world: SELECT FLOOR(UNIX_TIMESTAMP(the_datetime_column) / 900) AS ts, COUNT(*) AS entries, SUM(Value) AS sumvalue, AVG(Value) AS avgvalue FROM the_table GROUP BY ts; (I haven't used SQL Server much, wouldn't surprise me if there is a different approach that is preferred for this problem.. but I'm not aware of it so leave that for someone else:)
Thanks, I missed that. You’re absolutely right.
If you're interested in video based resources then the [sqlskills.com](https://www.sqlskills.com/) team have a ton of great content up on Pluralsight here: https://www.pluralsight.com/search?q=sqlskills&amp;categories=course
Select a.name, b.name from business a, business b where a.is_open = 1 and b.is_open = 0; Why this does not work? 
Remove the coffee (or another substance) spill from your 'alt' key
This happened to me a few weeks back, but I can't for the life of me remember how to fix it. I think it was a combination of keys that tiggered it. I want to say it was resolved my restarting SSMS
I'm a data insight specialist at a large customer support department. We use Access on a daily basis as an ETL tool and even as a data warehouse. We know there are better solutions and we really want to use those, but bureaucracy... I think Access has proven to be a good start point for data analytics, but has a lot of limitations when you grow bigger. Also a good application to learn SQL.
15 minutes is about 3 times too much you do not want to hire anyone as "SQL Dev" who cannot ace this
Nope, Its too easy, We have a lot of people running around without basic SQL knowledge. I would ask Who has the second lowest Salary in each team (listed/unlisted). And skip the rest.
I've gotten to be a lot more 'dickish' about a strict 15 :-p
They're given a computer when they come in to execute (because I do agree that typing and playing is a useful part of the process) but this is just to see if they're a waste of time. The in person is much harder (write an SSIS job, discuss how you would architect a problem, etc). The chief question here is 'do DBAs write select statements ever or is that not scope?'
Ok, you're doing fine then. Just a lot of people out there lying about their experience. If they can't write these queries, they are DEFINITELY not going to be writing SSIS packages. 
That is a super simple test, and anyone capable of data warehouse design should be able to do them in their sleep: if they can't do that they're not even ready to be a report writer, much less building out a data warehouse. But you ask whether you're testing the wrong things for a DBA - a DBA is NOT a data warehouse/ BI developer! A DBA is more focused on user access, backups, provisioning backups, that kind of thing. Many DBAs have only very basic SQL skills and it's not at all unusual for responsibilities to be split so that a DBA is looking after the servers and disaster recovery and a DW/BI dev is actually structuring and querying the database. Maybe the problem is you're advertising for a DBA when you really want a BI developer?
You're not alone. I even asked something very similar when interviewing consuiltants. Users table and and sales tables. How would I find out how many of my users have never placed an order? **Blank Stares** I didn't even ask them to write SQL. Just conceptually tell me how I would do this. Things like "How should I relate these tables? Do they have relational IDs between these tables? What columns do I have available to me?" are all acceptable questions to ask and shows me they have problem solving skills. You know, questions you might ask while gathering requirements? So no, these are very basic questions and any entry level analyst should know how to do this let alone a DW Developer.
&gt; do DBAs write select statements ever ever? how about every day 
Thanks a lot, that really helped! With some tuning to filter out error codes and such this appears like it'll work great! 
Bingo, its not a DBA test. its a DB or BI developer test. saying that a DBA should be able to clear off cobwebs pretty fast to get most of them in a few minutes. 
If you're looking for a DBA, these are probably okay (as others mentioned, SQL isn't really the bulk of what a DBA does, but they should have a cursory knowledge) If you're looking for a Dev, or especially a BI/ETL person who will be writing SQL all day, I would make it have more questions of varying difficulty, and have them write it out on a whiteboard and talk through it with you. Stress that the syntax doesn't have to be perfect (I still sometimes forget which parameter comes first in a CHARINDEX or PATINDEX) and if they don't know the exact code to give a pseudocode for how they would go about querying it. It's not so important that they know the specific syntax but that they are versed enough to know what's possible and how generally one would go about achieving a task. Like, give them a page-long table like this GameID | GameDate | HomeTeamID | VisitorTeamID | HomeScore | VisitorScore ---|--------|----|----|----|---- 1| 2018-01-01 | 1| 3| 0 | 3 2| 2018-01-01 | 4| 2| 1 | 2 3| 2018-01-08 | 3| 2| 3 | 1 4| 2018-01-08 | 4| 1| 4 | 3 5| 2018-01-15 | 1| 2| 3 | 3 6| 2018-01-15 | 3| 4| 4 | 2 ... | ... | .... | ... | ... | ... Write queries to: 1) Find the team with the longest winning streak 2) Find the team with the highest average win/loss margin 3) Return the season record in this format based on a @year parameter TeamID | 2018-01-08_Opponent | 2018-01-08_Margin | 2018-01-08_Opponent | 2018-01-08_Margin | 2018-01-15_Opponent | 2018-01-15_Margin | ...etc... ----|----|----|----|----|----|----|---- 1 | 3 | -3 | 4 | -1 | 2 | 0 2 | 4 | 1 | 3 | -2 | 1 | 0 3 | 1 | 3 | 2 | 2 | 4 | 2 ... | ... | ... | ... | ... | ... | ... Some might see this as a cruel, but what you're looking for is the person who is excited by this kind of challenge. If they can finish it, even better- they've demonstrated an understanding of islands-and-gaps problems, windowed functions, and dynamic SQL. That's probably someone who knows what they are doing.
Control+Z
You are joining on bunk sheet. Maybe you need a union statement.
&gt; Is my test too hard No, these are trivial. Anybody who can't answer these questions should not be writing SQL for a production application. Any developer or analyst who is required to be familiar with SQL can be expected to answer these questions. You may be getting a bunch of developers who have worked with more abstract ORMs, however, so you need to decide if your developers need to be writing SQL queries or if they need to be familiar with ORMs and querying databases. However, your first question is so easy that it's ambiguous. &gt; 1) Select all data from both tables? There's two valid answers to this: SELECT * FROM Teams t FULL OUTER JOIN Players p ON p.TeamID = t.TeamID And: SELECT * FROM Teams SELECT * FROM Players You never specified that you wanted all the data in a single result set. It's also slightly trickier than every other question because only a FULL OUTER JOIN will guarantee returning *all* data from both tables, but many databases will have referential integrity here. &gt; 2) What Team has the most wins? Trivial. SELECT TOP 1 ORDER BY or MAX() in a subquery. &gt; 3) How much does each team make? (This is a trickish question intended to make the interviewee ask a question to see how they work through poor instructions, as per the job. Since there is only 1 measure in this DB, it's pretty simple to figure out, but I wanted to see how they ask.) This is not a trick question. Nobody should think this is a trick question. There's only one answer from the data provided: SELECT t.Team, SUM(p.Salary) AS TotalSalary FROM Teams t INNER JOIN Players p ON p.TeamID = t.TeamID GROUP BY t.Team If your idea is to go, "Ah ha! You didn't ask for the ticket sales data! I asked for how much the *team makes*, not how much the players are paid!" then you're just asking a trick question because "team" has multiple possible meanings. You're not testing SQL knowledge, you're just equivocating. &gt; 4) What player doesn't have a known team? Trivial anti-join. --------------------------------------------- &gt; testing the wrong things for a DBA For a DBA as in a *Database Administrator*? Absolutely these are the wrong questions. There's nothing about indexing, nothing about the difference between OLTP and OLAP, nothing about CUBE, ROLLUP, PIVOT or other special operators for data warehousing, nothing about backup and recovery, nothing about replication or partitioning. If you want to test an *SQL Server Database Administrator*, ask something like (from easy to hard): 0. What's the difference between a CLUSTERED and a NONCLUSTERED index? 1. Write a query which returns a list of every team and the players with the three highest salaries on every team. You may include or exclude ties. [Analytic window functions like ROW_NUMBER() or DENSE_RANK().] 2. When would you use a recursive CTE or recursive query? [Hierarchical data in parent/child tables or number tables or generating a series of values.] 3. A stored procedure which had been running normally begins running much slower one morning. The stored procedure definition has not changed and it does not significantly utilize tempdb. What steps would you take to investigate the problem? [Parameter sniffing, table statistics, index maintenance, and query plan caches.] 4. When would you shrink a database or transaction log? How would you decide what size to shrink to? [Data file management. Understanding how database files work. Understanding database file sizing.] 5. You've been experiencing a large number of errors in an application, and have tracked the issue down to deadlocks. How would you approach the problem? [Understanding deadlocks and why they happen.] 6. Data in one query appears to be missing from some queries and not others. The queries are all correct, but their results are not. When the tables are queried without any filtering, the missing data is still present in the table. What might cause this? [Statistics and index maintenance, detecting database file corruption.] 7. A transaction processing database contains data from the previous 20 years, but recently performance has been a problem as the user base grows. Approximately 95% of all queries only access data from the last month, but all data must still be accessible from the application. How would you approach this problem? [Data partitioning. Possibly replication or other strategies.] Some of those would take longer than 15 minutes to answer, of course. 
 Update a Set date = getdate() From tablename a Where string in ('aaaa','dddd')
I'm an aspiring DBA trying to convince management to take this approach for my current project because I've been told that it's better than just Access. However, those people told me that using Access alone also sucked at explaining why. If I can't tell management something convincing and mostly true - it won't happen. What advantages would having a SQL server provide (Access front-end isn't going away in the near term even if I wanted to) 
In my environment, I'd say the big things are: 1. The data lives in a centralized place where you control security, backups, performance, etc. You can be very granular in SQL in terms of who has access to what, and what they can do with it. 2. You can leverage the data in other places, such as a website, or even a different Access database for a different user. Having a website talk to Access is the thing of nightmares. 3. If the Access users need assistance with a complicated query, you can write it for them, then turn it into a view or stored procedure. 4. Access files corrupt, sometimes too easily, and it's much easier to rebuild an Access file if you don't need to worry about the data in it. 
Haha i was thinking the same thing
you go through all that work writing and then this: &gt; Trivial anti-join. \*cough\* `LEFT OUTER ... WHERE TeamID IS NULL` \*cough\*
That query should get you every open business a, then every closed business b for every open business, since you aren't defining a relationship between the two tables. It'll be multiplicative (a x b). What, specifically, are you getting to make a comparison on? Open businesses in one column, closed in another? 
There's nothing interesting about a textbook anti-join, so I didn't bother. 
I'm so glad we're in the same boat. Been doing interviews for a SQL conversion specialist (basically ETL via straight t-sql scripting) and the candidates have been SO terrible... No suggestions though; just keep looking. As everyone has said, these are laughable complexity-wise and it's amazing that people apply to these jobs.
I have been a SQL Developer, a BI Developer and am currently a DBA. If a candidate did not laugh at the simplicity of this test, then I wouldn't even give them read-access into my databases. Really want you want is someone that comes in and tells you how wrong your data model is for your test and how it would be so much better another way. I strongly advise what others have already said. Revisit your job description to make sure you are targeting the right people. Also, for heaven's sake, save yourself a lot of wasted time and do some phone screening interviews first before they get to the point of taking a test.
The job description can also be attracting the wrong candidates. Can you post the job description?
Could you post the test or an link to a similar one?
I learnt quickly that if you can, write a select statement first to find/check the rows you will update. Once happy, then replace the SELECT part with the UPDATE syntax
I think a good starting place for you would be to download MS SQL Server (Express or Developer) and restore a copy of the Adventure Works database. Going through the exercise of doing the will help answer some of your questions.
Can I ask one more question? What if for ID field 1111 I have to update both lines because one of them contained the target string?
This didnt work :( I had to add cast(a.DeptId as varchar(36)) for it to run, but even then it returned no rows. However, I dont see why this would work. a.DeptId + ',' + a.DeptId would return in the same ID 2 times, and there is no such DeptPath. The DeptPath is a comma-separated string of all the DeptIds starting from root and going down to the current department. Maybe I explained poorly, I should get a picture up.
Thank you! This gives me the result I want! :) 
Thanks for your answer :) I am using an existing database, so I cant really change anything. I got it working with jc4hokies' reply, so I will just use this one. But Ill save the links to check them out anyway, interested in learning about this!
&gt; What if for ID field 1111 I have to update both lines because one of them contained the target string? this was sort of what you originally posted, so the simple answer you received is not enough UPDATE yourtable SET datecolumn = GETDATE() WHERE id IN ( SELECT id FROM yourtable WHERE string IN ('aaa','ddd') GROUP BY id HAVING COUNT(DISTINCT string) = 2 ) i'm not sure if MS SQL lets you subquery the table you're updating, so you might need to create a temp table for the subquery's results 
My test was customized for our systems (Oracle and MSSQL depending on the person’s comfort level with one of the other) but I made a more generic one for someone on here once. PM your email address and I’ll send it over.
hehe, I prefer this answer :-p One of my follow up questions for each question is 'great! Can you do it another way?'
Oh you caught me! not to mention my username has 'work' in it, for a work account ;-)
&gt; Who has the second lowest Salary in each team That would require using a function that even competent, experienced folks might not have much experience with. I think I'd been working as a SQL dev for a couple of years before I came across a use case where I needed one of the ranking functions. I knew they existed, but I wouldn't have been able to get the syntax right without looking it up.
Those questions for people who just started to learn SQL. anyone working with SQL - from developers to DBAs even through "Back End unrelated to DB but still need to fetch data from DB" should write those in two minutes while sleeping. you need to up the questions if you want to get good employees. also have a discussion with HR, they are not doing a good job in picking the people for interviews 
AdventureWorks is great for learning the basics, but a database with much greater data volume may be more suited to learning "big data" type queries. Consider the Stack Overflow database instead: https://www.brentozar.com/archive/2017/07/new-stack-overflow-public-database-available-2017-06/ It only has a handful of tables with easy to understand relationships, but it has a shitload of data in it. The smallest table in the link above has over 4M rows in it, which is akin to what you might find in the real world in a big data role. 
Select distinct msn From yourtable Where model# like 'M9%' or model# = 'Delivery'; ...or am I missing something?
Ok, but what characterizes the group colors? After all, if I have to check the color for any model besides m900 and delivery, there's gotta be some other grouping there.
Based on your post and your replies to comments - this is doing it's job. It's weeding out people you don't want to waste your time with. A DBA and a SQL Dev are not the same thing. (You mention trying to hire a SQL Dev and then mention that the test might be too hard or wrong for a DBA). Also based on what you mention the person in the position may be doing, you might want a BI Dev. DBA, BI Dev, and SQL Dev should all be able to answer this in the allotted time - especially since they don't have to execute the code. Anyone who can't complete this would be a waste of your time for what you are looking for.
As with all Computer related things, there is a risk...but there really is no more risk than if you just normally had a computer connected to the Internet. - Xampp effectively just installs an apache web server etc. onto your machine, It doesn't even require the internet once you've got it set up. The only precautions are like with anything, i.e. lock it down with a password. If you have a home router with a firewall and all of your devices are private I.P addresses (192.160.x.x) and not in a DMZ....then you're probably fine. 
Oops, the "when 'Daypart" should be 'Daypart'
Open 'er up and find out imo
Hey one more, I made the below nested loop to run based off of the output on the first query, to grab only the highest value for each machine/timequarter combination and stuff it into a new table, It runs forever, so obviously I'm not breaking out of the loops properly, what did I miss? DECLARE @MN INT; SET @MN = 1 DECLARE @QT INT; SET @QT = 1 While @MN &lt;= (SELECT MAX(MachineNumber) From TimeTemp) Begin While @QT &lt;= (SELECT MAX(QTime) from TimeTemp where MachineNumber = @MN) Insert Into SMP_PartsMade (MachineNumber, Time, TotalParts, MadeInLast, PartID) Values( @MN, @QT, (Select Pcount from TimeTemp where MachineNumber = @MN and QTime = @QT), 0, 0) SET @QT = @QT + 1 END SET @MN = @MN + 1 END
Try joining the table to itself. SELECT DISTINCT t.technology, t.tech_type, t.domain, add.date as 'Add Date', drop.date as 'Drop Date' FROM table_name t LEFT JOIN table_name add ON t1.id = add.id AND add.add_or_drop = 'add' LEFT JOIN table_name drop on t1.id = drop.id AND drop.add_or_drop = 'drop'
Run it with @help=1?
Appreciate your help. Seems like the right idea and gets me close but ends up leaving me just a step away. Here's the issue: https://imgur.com/a/6UXOi I end up with add and drops on separate lines. Anyway you know of that I could pair the add and drops on the same line? fyi there is a different ID for each add/drop so I can't use that to pair them up
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/yhXpA4L.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20du8ujb4) 
 sorry I would update tblAssetCustom.LastPatched using these results
This might work: UPDATE ac SET ac.LastPatched = Max(Convert(datetime2,qf.InstalledOn)) FROM tblQuickFixEngineering qf Join tblAssets a On a.AssetID = qf.AssetID Join tblAssetCustom ac On a.AssetID = ac.AssetID Where qf.InstalledOn Like '%/%' Group By a.AssetID untested, though.
For things that end up creating a large case statement like this, I prefer to use a table and join, when possible. It's possible to make a table (temporary or permanent), and store the string patterns in the table. In the join condition, left join on `d.daypart_text like newtable.pattern`. A second field in the table would be the value you select ('ON' in this case). In the select, you can use isnull() to assign a value to things that doubt match your pattern ('Daypart' in this case). There are a few benefits to this approach: \n1. Maintenance and readability are improved \n1. You can use `newtable.field is null`and `is not null`to test for matches. The biggest downside is that, depending on the data, you can get multiple matches. This gets more complicated, but can still be solved for. Hope this helps. I'd give better code samples, but I'm at work on my phone right now. 
 SELECT [Table 1].Item, [NewTable].Price FROM [Table 1] LEFT OUTER JOIN (SELECT Item, Price FROM [Table 2] WHERE Item IN (SELECT ITEM FROM [Table 2] GROUP BY [Item] HAVING COUNT(*) = 1)) [NewTable] ON [Table 1].Item = [NewTable].Item Should return: Orange 2 Pepper NULL Replace LEFT OUTER with INNER with you didn't want Pepper in the result.
Because if you group by t2.price, then the count will always be 1 unless the price also happens to be the same (which you did in the sqlfiddle, but OP did not)
Well it stands to reason the price of an apple or orange (of the same name) would have the same price... but maybe not. 
I was able to get this working today with a subquery. Thank you! I don't think there is a way for me to mark this as solved. 
Well, SSMS is only half the battle. You still need to install SQL Server itself. This is what runs the database engine. 
It's not that difficult and you would probably feel silly after you paid someone. It's like a 6 step wizard. Just give it a try, if you can write sql you can figure out the installer for sure.
Why would you need to ask anything on Q3 the instruction seems pretty clear...
Developer Version is free. Just grab and go. You can expand from there if needed. 
A [stored procedure](https://docs.oracle.com/cd/B28359_01/appdev.111/b28843/tdddg_procedures.htm) sounds like exactly what you need
How do I use the results in my WHERE clause? 
You could assign the values to variables using OUT in the stored procedures arguments list, similar to [this](https://docs.oracle.com/cd/E28280_01/bi.1111/e16630/app_ds_from_sp.htm) The other option is to create 2 separate functions, each returning one if the values, but a stored procedure with 2 OUT's would work
Okay so when I want to use the storeproc or function how would that look: SELECT * FROM TABLE T WHERE T.VERNUM = storeproc.v_max??? AND T.DEFVERN = ???
[This answer on stacked overflow shows working with the output](https://stackoverflow.com/q/12659545/8749212) You should really look into functionalities of PL/SQL, because as nice as it is there's only so much you can get out of ANSI SQL. Most SQL databases have their own slight take on extending SQL, but all are incredibly useful
Don't forget to make sure that you have enabled TCP for external connections. Port 1433 TCP. Reply and let us know if you need help. I struggled myself with something today and got help myself so it happens to all of us!
Maybe try putting that code into a UDF to work with it a little cleaner.
thank you.. I just don't know where to even find the import wizard? I've tried a few, but nothing is installing. I have windows 7 of that matters. 
Check out materialized views. They are available in Oracle, and should do exactly what you're after. They're sort of a middle-ground between a "permanent" table and a view/stored-procedure. They write your query results to disk, but also retain the view sql so you can run a refresh operation to update.
how would you do it without a window function? 
I think that's the direction I'm going to go in. It seems simpler but I'd still like to store those values in variables so I only need to reference the view once. But I think I can do that using: `SELECT MAX_VERN INTO MY_VAR FROM MV_MAX_VERSIONS_TABLE' Then I should be able to use the `MY_VAR` within other functions. 
 select x.PlayerID, x.TeamID from Players as x where Salary = ( Select min(t2.Salary) from Players as t2 where t2.TeamID = x.TeamID and t2.Salary &gt; (Select min(t3.Salary) from Players as t3 where t3.TeamID = x.TeamID) )
If you use CTE and Window funtion it is: ;with x1 as ( select x.PlayerID, x.TeamID , x.Salary , row_number() over(partition by x.PlayerID, x.TeamID order by x.Salary desc) as seq from Players as x ) select * from x1 where seq = 2
This needs to be higher, as it’s the only response that actually contains instructions to do so. The “get installation media” section of this article has a link to download the free versions. OP: Express is a free (limited feature/size) version that allows you to use it in a production environment. Developer is a fully fledged version that is not legal to use in a production environment. 
Looks like a two different users can work on work task and a subwork task? Also that comma before the from is going to result in a syntax error, or you’re not showing us the whole select statement (which probably contains the reason for the multiple joins)
Yes, more specifically I am trying to get all the businesses of a city and compare the ones open and the ones closed
Select a.name, b.name from business a, business b where a.city = b.city = 'Las Vegas' and a.is_open = 1 and b.is_open = 0; This is the new code I want to get all the businesses from Las Vegas
Select a.name, b.name from business a, business b where a.city = b.city = 'Las Vegas' and a.is_open = 1 and b.is_open = 0; I want to get first all the businesses from Las Vegas and then compare those open with those not
But I get 0 results
What do you mean by functions? Functionalities of SQL server or code functions?
I'm actually a beginner in Sql , So I think I misunderstood the term Functions .Thank you .
Maryland; you'll have to pass the harder part though. This was meant to 'screen the complete bullshitters', of which there are plenty.
Yea the whole select is very large so I just cut it down. 
thank you 
Unfortunately that limits our ability to answer. You might want to post this question on DBA.stackexchange.com which provides better formatting tools.
I think before continuing you might want to clarify why you'd want this specific output. I think it might be better to handle outside of SQL, which might mean you'd want to remove the STUFF() altogether. You're trying to get a specific string output from SQL data. I don't doubt that it'd be possible to have SQL output exactly what you want, but I wonder if it would be appropriate to do. I can imagine situations where it might be required, but I can't imagine a situation where it would be ideal. So I guess before continuing this thread, take a moment and make sure that it's what you want to be doing. And if it's absolutely necessary, I'm sure somebody will be happy to help. Just my 2 cents.
Yeah as I bang my head against this I'm starting to think that it might be better/easier to do the last peice in SSRS. I want the "x3" count because the results are going up on a display board in our shop, and if someone is working on 5x ABC's it gets a little cluttered. I can do a SELECT DISTINCT, but then we lose the visibility if J. Smith is working on 5 parts or just 1.
In PostgreSQL, I think you could use array_to_string(array_agg()) in your "STUFF" select list, something like: SELECT array_to_string( array_agg(PartNum || ' x' || count(partnum)), ', ') FROM ... GROUP BY partnum Other dbms have similar such support.
Take a look a Ben-Gan's books they go from Introduction to Advanced. Also, Google Sql Windowing Functions. 
This looks like a homework post. That's cool, but explain what you need to do and then I'm sure people would be pumped to help you with the syntax.
this should work WITH pnc AS ( SELECT PartNum ,COUNT(*) AS n FROM Erp.LaborDtl JOIN Erp.JobHead ON LaborDtl.Company = JobHead.Company AND LaborDtl.JobNum = JobHead.JobNum AND LaborDtl.ActiveTrans = 1 WHERE LaborDtl.Company = EmpBasic.Company AND LaborDtl.EmployeeNum = EmpBasic.EmpID AND LaborDtl.ActiveTrans = 1 ) SELECT LEFT(EmpBasic.FirstName, 1) + '. ' + EmpBasic.LastName AS "Name" ,EmpBasic.JCDept ,JCDept.[Description] ,STUFF(( SELECT ', ' + PartNum + ' x' + CAST(n AS varchar(8)) FROM pnc ORDER BY PartNum FOR XML PATH('') ),1,2,'' ) AS Parts FROM Erp.EmpBasic JOIN Erp.LaborHed ON LaborHed.Company = EmpBasic.Company AND EmpBasic.EmpID = LaborHed.EmployeeNum JOIN Erp.JCDept ON EmpBasic.Company = JCDept.Company AND EmpBasic.JCDept = JCDept.JCDept WHERE LaborHed.ActiveTrans = 1 ORDER BY EmpBasic.JCDept, EmpBasic.FirstName
My fault for not clarifying that, I am stuck on trying to join 2 tables that do not have a Primary or Foreign key in common. The GLAccounts table has AccountNo (Primary Key) and AccountDescription (column), and the other table has multiple columns and 1 key (InvoiceID) to the Accounts table. I will try to work them out as individual queries now. Would I have to add a third table to the query to create a relationship between those 2 tables? Also thank you for the quick response and helping.
No conceptual problem. Obviously won't work with one-to-many relationships as you'd end up with duplicates in the ID column of one of the tables.
&gt; Would I have to add a third table to the query to create a relationship between those 2 tables? If there's no account number on the invoice table, and no invoice number on the accounts table, then yes. There has to me *something* that can relate the two tables to one another, otherwise you have no way of knowing which invoices go with which accounts. Sometimes, the account # is right on the invoice table. Other times, you'll have a table that just lists invoices #s and account #s as an intermediary table and you have to join accounts to that table, then join over to the invoice table to complete the link from accounts to invoices.
So I joined the tables together, and did the queries separate. The last part it asks has me stumped mainly because I do not know how to combine the queries together properly, using subqueries or another method. https://imgur.com/XR5aYLc
Do you need to just create a table with two columns or do you mean you need to select that data as two columns?
`WHERE 1 &gt; 0` will get removed as a predicate by the query optimizer as it's not needed (because it always evaluates to true).
in oracle its: substr (field1,0,2) ||'/'|| substr (field1,3,1)||'/' || field2. Might have to recast the field's datatype depending on what they are.
 WITH cte AS ( SELECT Male ,COUNT(*) AS t ,COUNT(CASE WHEN Female IS NULL THEN 1 END) AS n FROM [Data] GROUP BY Male ) SELECT d.* FROM cte JOIN [Data] d ON cte.Male = d.Male WHERE t &gt; n AND n &gt; 0
Works on the Fiddle, and I think the concept is sound -- thank you! This looks promising, but I will try on the real data set at work tomorrow and report back.
What does WHERE 1 = 2 do
You want to find all Male values that have both a null/empty and a not-null/not-empty Female counterpart select * from Data WHERE Male in ( SELECT Male FROM Data WHERE nullif(Female,'') is NULL ) AND Male in ( SELECT Male FROM Data WHERE nullif(Female,'') is NOT NULL )
If you're referring to Cognos Impromptu, this is what I have to say about it. Used it for over a year as a database analyst. If you can convince them to get something better, do it. You can use raw SQL in a report file, but it gets difficult to work with if you're doing anything complex. At least for the version I was using, (6.5 I think), having raw SQL for the report disables a lot of extra features you would come to expect in reporting software, such as conditional formatting of cells, etc. I think the only significant feature supported with raw SQL was parameters, so at least that's available for what you're doing. Still, there's a lot of clunky things about Cognos, regardless if you're using SQL or not. I think it's best use is for advanced end users who want to create their own reports but can't write SQL. 
Force failure to test a subquery join? I am not sure why else..
Cognos 10 is good, 11 is better. If you can write good, effectiveness SQL or your Cognos Framework/Model is well designed then you can do it. 
It’s similar to a “SELECT TOP 0”, copying the schema associated with the query but leaves no data in the table blank. At least the way I’ve seen it used it’s easier/shorter/lazier to write than a CREATE TABLE statement before your insert statement, especially in situations where a “SELECT...INTO” is not ideal. But I’d be curious to know if there’s a reason to do that over the “TOP 0” option because as far as I can tell that’s just a stylistic difference.
I’ve developed quite a bit in SQLite, and yes, it will work as long as syntactically it is set up correctly and data types match. My senior capstone project was an android development project that was powered by SQLite. It can be frustrating at times, and definitely not as neat and clean as SQLServer, but powerful for local hosting. Keep at it, feel free to PM me if you have any further questions about SQL/SQLite. 
You can just add those to your concat statement. Depending on how uniform the numbers in field1 are you can use the following. You may need to find the LENGTH of the string for it and substring appropriately. But, for this particular example, the following would give what you are looking for: CONCAT(LEFT(db.table.field1,1),'/',RIGHT(db.table.field1,2), db.tablefield2)
If your where clause has a bunch of “or”s , then start the first line “where 1=2” with crlf for each “or”; makes remming in/out easy. Likewise for multiple “and”s you’d begin with “where 1=1”. e.g. Select &lt;columns&gt; From tableA a Join tableB b on b.id = a.id Where 1=1 and b.last = ‘smith’ and b.ssn like ‘219%’ and a.gender = ‘m’ /* you can easily rem in/out one or more line while testing w/o having to worry about the beginning of the where clause. All our devices do this, or the 1=2 for stacks of “or”s */
I don’t understand why you would ever do this. What reason could you ever have to include 1&gt;0?
Not the homework, but I have a task to do this and struggling to do it. Yea, i need to select one column ("Name") from table "Dep" and count average salary from table "Salary", which will be written in another column next to "Name". Should look like this: https://imgur.com/svBoRpq
Report design will be a key factor. Cognos may struggle if you are trying to pull a huge amount of data and perform aggregates at run time in report. Its recommended to fetch aggregates and drill down to details as required there by reducing the amount of data transferred. Cognos 11 also comes with dynamic cubes which provides in memory capabilities and can cache aggregates as per the model(semantic layer design). This significantly reduces the number of transactions required between report server and db.
I hate the interface for 11 though... 10's interface was so much more intuitive.
Could be number of things. Have the indexes been refreshed, stats couple wacked. Also the new sever may be beefy but is the disk set up optimal? The old server could have great IO setup and the new one still has defaults.
I'd start by running some cpu benchmarks on both the old and new host. If they are the same then try some file system benchmarks between the two. If all those are good, then I'd think you might have missed an index somewhere so start comparing explain plans...
Thank you :)) the google Google Sql Windowing Functions helped a lot !
You won't get anywhere from the answers given here. I advise you to hire a knowledgeable consultant and have him analyse on-site. We need too much information to provide real answers. (Or maybe you're lucky) 
Start here: https://www.brentozar.com/first-aid/ Read up on each of the first responder items. I would start with the blitzfirst and blitzindex. 
Stats and indexing have been run (several) times. 
Hello thanks for the response. There is a DBA working on this but he is out of ideas. Keeps blaming the code. Application team is running simple queries not much to tweak. Only thing that has changed is P to V, SQL 2008 to 2016 and the server itself which is now Windows 2016. 
either youre summarizing heavily or you didnt test on the new hardware before go live 
Nerd 
Yes summarizing heavily. Didn't go live yet this is in a QA environment. Setup exactly as PROD 
Histogram steps on the table or just index fragmentation. The reason why it works fast locally on shitty HDD is cause the histograms are updated, or index is created after the data is loaded so you are not shitting on the clustered index. Your description is extremely vague though.
Is your VM using all of its allotted cores? Check [this MSSQLTIPS thread](https://www.mssqltips.com/sqlservertip/4801/sql-server-does-not-use-all-assigned-cpus-on-vm/). It may not be an issue - I don't know if you've looked into the actual performance or not, but more than once in my experience I've seen this be the problem, and every time people are surprised...
wut
hahaa, totally. 
If you have a query with a bunch of filters that are conditionally applied, it is easier to add the filters if they can all begin with `AND`. Having a always true condition, like `1&gt;0` or `1=1`, makes it easier to add filters.
&gt; Only thing that has changed is P to V, SQL 2008 to 2016 and the server itself which is now Windows 2016 Those are very significant changes.
Thanks for your reply. I checked a few of the things (what I could) and seems like all is within spec (and is the same as the current production server) Cost Threshold for Parallelism is 0 min/ 32767 Max Max deg of Parallelism - 0 min/ 32767 Max Indexes are the same and have been rebuilt (stats too) Im pretty sure its CLR. It is just one job out of 170 but it is a critical job. Needs to be completed within an SLA and its now even close right now. 
We know which query it is. it is one job out of around 170. It is just 5x as slow as current production. The CPU stays within range (30-50% across all cores), paging file is fine. RAM is fine.
What is the compatibility mode of the database? Did you leave it at 2008 or upgrade it to 2016? The cardinality estimator has been changed since 2008, so it may be worth setting it back and running the queries to see if that affects it. - Wouldn't hold my breath on that, but its a box to tick off. Your response to the max degree + cost threshold don't line up with what I'd expect - they are single values, not ranges. Right click server, properties. Advanced -&gt; Parallelism Out of the box defaults are not good. 0 for Max Degree, if you are running 32 cores and leave it at 0 it may allow one query to starve out others. You may have luck dropping that to 16-ish (there are posts with formulas and lots of people with opinions on a correct value). Cost Threshold is how expensive a query is before it goes parallel. There is overhead to going parallel so dead simple queries should run single threaded. Again lots of opinions, but often people suggest anywhere from 25-50 as a starting point.
DBA is being lazy with "developers fault" its a migration issue. work the query and fix it. is it calling any embedded functions? 
Config Value on those are as follows: Max Degree of Parallelism - config/run value = 12 Cost Threshold - config/run value = 20 Cardinality has been set back to SQL2008 as we thought that might be the issue as well. Thanks for your help! 
no I don't believe it is calling anything embedded. Simple query, loads data. DBA is excellent at steady state but not troubleshooting. 
Yeah the job is pulling data from an external Source. We have verified that there are no firewall issues nor bandwidth constraints. Current production hits it just fine with desired results. Local here in the office it is acceptable as well. 
Edited my comment above with what you need. 
Obviously assuming your DDL is the same here. Check your access path before - after. Ensure you are doing statistics the same they will not carry over unless your doing a redirected restore to migrate.
Guaranteed this connection to your external source is the problem. Is this the only query that references it? 
Wow, that is a great looking solution!
Check the database compatibility level. Downgrade compatibility to under 2014 unless you have verified it is 2014+ compatible.
Please tell me that this isn't a query going over a linked db... *shudder*
it's a staging job that pulls data from my sql source to mssql table. The select query is simple( pulls 12 columns) with no where conditions and filters. The driver used for mysql connection is MySql.net 6.10.6.
Is the source the same distance on old and new? ie, same data center, same subnet? Are your network settings the same on old vs new, Full vs Half-Duplex will get you every time. What is the timing on the old import step, what about the new? Is the import part of a join or a direct import into a staging table? 
So you kind of answered this elsewhere but restating for clarity. Issue is a single 'Job' Does the job consist of multiple steps or single step? If multiple, are all steps slower or just a single one. Assuming its a single step - You mention this is grabbing data from an external file. Is this job using SSIS to gather the data or is it using T-SQL / BCP or some other means? If the file Excel? I ask because there are different drivers out there, some 32 bit others 64. Doubt the version installed would cause it, but again just another variable. So assuming its the file read/load causing the issue. If you copy the file directly to the server's drive, does it still perform slowly or is it ok? If its back to quick, sounds like a possible network issue. If the Test server is on a different subnet / vmhost it could be possibly be an issue that the network path isn't happy or there is some dns ip resolving issues. Admittedly I'm reaching a bit here. If the process is still slow when the file is local, can the step be broken down into parts to see if a single step is slow? Say its a SSIS package - disable everything after the first step or two and run from the server. Reenable parts until it crawls. My assumption is not much else is running on this server - you may be able to check long running queries to see if that pops anything https://www.brentozar.com/blitzcache/long-running-queries/
It can help to imagine how you'd do this yourself if you had a list of players and teams on paper. If someone told you a player name, you'd need to look through the list to see what team the player is on. Then you'd need to look through the list again to find other players on that same team. The fact that you'd looking at the list twice is a clue that you'll need to use your table at least twice in your query. You can do this with either a join or a subquery. I prefer the join, so i'll show you that: select p2.name from players p1 join players p2 on p1.team =p2.team and p1.name &lt;&gt; p2.name and p1.name = @pname
What about Server Max memory? You said the server has 64 GB ram, so I'd think a max of about 56GB or so would be reasonable. Less if this box is running other stuff like SSIS as that'll need memory separate from the SQL Server.
So the code DECLARE @pname char(32) SET @pname = '&amp;player_name_input' SELECT p2.name FROM players p1 JOIN players p2 ON p1.team = p2.team AND p1.name &lt;&gt; p2.name AND p1.name = @pname should compile correctly, and show me the teammembers of the player? I'd also like to remove the searched-for player from the result, so it only shows his team mates. What should I add?
 SELECT d.* FROM data d INNER JOIN ( SELECT d1.male 'male' FROM data d1 INNER JOIN data d2 ON (d1.male = d2.male) WHERE ISNULL(d1.female,'') = '' AND ISNULL(d2.female,'') != '' ) sq ON (sq.male = d.male)
I am having trouble understanding you... STRING_AGG does what you want. There is also FOR XML with PATH, but that's rather cumbersome to use.
select * from table 1 where name not in (select name from table2)
Try here: https://stackoverflow.com/questions/2686254/how-to-select-all-records-from-one-table-that-do-not-exist-in-another-table
I'm not too familiar with T-SQL so I may be misunderstanding what it is. I've always thought that MS SQL and T-SQL were slightly different languages. We use "Microsoft SQL Server Management Studio" at my place of work and T-SQL commands don't work. Is there a way to use T-SQL in this application? Thanks for the response! 
Thank you for the clarification, that explanation really clears things up for me! We are on SQL Server 2012 so that explains why String_agg() and some other functions that I've seen online aren't working. This entire time I thought that they were using different dialects
Erm, not exactly. Take a look here: https://stackoverflow.com/questions/840162/should-each-and-every-table-have-a-primary-key
Two questions: - What is your SQL Server (Database Engine) version? See https://support.microsoft.com/en-us/help/321185/how-to-determine-the-version-edition-and-update-level-of-sql-server-an how to lookup the info. - What commands do not work? For STRING_AGG (https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql) you need SQL Server 2017.
It occurs with join tables, where you'd otherwise have a many-to-many, but usually you'd either composite the two fks as your pk or go ahead and surrogate with an identity or guid.
An FK without a PK... Yeah, no.
I think you need to apply the logic yourself. CASE WHEN MAX(CASE WHEN MINUTES IS NULL THEN 1 ELSE 0 END) OVER (PARTITION BY NAME) = 1 THEN NULL ELSE SUM(MINUTES) OVER (PARTITION BY NAME) END as MIN_PER_PERSON
Works like a charm. Thank you! 
I would never have expressed it that way, but after I wrap my head around it it makes sense -- thank you! I just tried this on my data, and it looks great. Thank you!
This works! Thank you very much. Your idea of the counts makes sense, so I'll keep that in mind!
I'm glad I could help :) I can't test it right now and it isn't important anymore but maybe you can see different values when instead of to_timestamp(create_date) use to_char TO_CHAR(create_date, 'YYYY-MM-DD HH24:MI:SS')
&gt; When loading data to a table, how do you make sure the data matches and is correct when there are millions of records? - primary (and foreign, as applicable) keys - restrictive datatypes - check constraints
Yeah it's max. 56. Dedicated 8 to the O/S
RemindMe! 3 days "Try out to_char"
I will be messaging you on [**2018-02-20 00:12:39 UTC**](http://www.wolframalpha.com/input/?i=2018-02-20 00:12:39 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/7y22cb/oracle_unexpected_duplicate_results_when_using/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/7y22cb/oracle_unexpected_duplicate_results_when_using/]%0A%0ARemindMe! 3 days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Extremely elegant solution! You deserve gold, but at least have some [reddit silver](https://imgur.com/gallery/f0Iu0xE). 
 select d.year || '-' || d.month || '-' || d.day as date ,count(distinct case when e.event_name IN ('game_start') then e.user_id else null else e.user_id /count(distinct case when e.event_name NOT IN ('user_login', 'user_logoff', 'game_complete', 'user_create') then e.user_id else null else e.user_id) events e, dates d WHERE e.date_id = d.id group by d.year || '-' || d.month || '-' || d.day order by 1; 
Wife sells things to other moms on facebook
Me and a few other guys all contribute to a side business together. We setup automated reporting for couple clients and provide general IT support to a few others. Bit of an upfront commitment but once you get rolling you can kinda just let it ride month to month without taking up a ton of time. Not huge money but it's something, and the more you put into it the more you'll get out.
Yeah all of us actually have a different regular job. I joined in after they had the framework for the automated reporting for a particular bigger client. Like the connection to their db and scripts for delivery to their secure folder were already established. So I really just handle modifications to existing reports and building new ones as needed. The reports all run overnight and are summary information of the previous day, week, month, quarter, or year. Since everything is overnight when they make requests I do it later that night when I get home and its all good in time for the next day anyway. The guys that handle IT support have it worse. There have been times that they have literally had to leave work to go fix something for the client so that's more tricky. If you're more into reporting anyway and can find an office or two to automate you can do it pretty easy though.
Thanks for this. I get a syntax error starting at "else." I think the query is also missing the FROM clause. My DB is on Amazon Redshift which uses an old version of PostgreSQL. Not sure if that's causing it or something else.
SELECT SKU, COUNT(*) AS COUNT, SUM(SALES) AS TOTAL_SALES WHERE &lt;criteria&gt; GROUP BY SKU
Counts and Amounts based on Business value. Ie number of sales people/ amount of sales for each person.
I make YouTube videos on technical topics (mostly hardware related, but some software too). I've built up enough of an audience that I'm getting $70-100 a month between YouTube ads and Amazon Affiliates commissions. The nice thing is I can do it on my schedule and focus on things I find interesting.
the join on p1.name &lt;&gt; p2.name will exclude the searched for player from your result set. as you've written above, you should be good to go.
You can write a script to perform data validation on a population before inserting into a table. After running the script only import data that meets the specifications you require. If this is what you mean by having data that “matches and is correct” you can get specific with the validation rules and output. I would recommend a creating a stored procedure with case statements pertaining to each column and and instance that may be an issue. 
&gt; else null else e.user_id should be `else null end AS e.user_id` and yeah, no FROM clause
FYI, this -- FROM transactions t LEFT OUTER JOIN CodeDefinitions cd ON (T.Description = cd.Name) WHERE cd.Name IS NOT NULL is the same as an inner join
Ah, thanks :)
I will pay you to show me how to do that. 
It's a simple `INNER JOIN`. The tough part is that you then have to `PIVOT` the result of that `JOIN` to get the data the way you want it.
So it's not that simple after all?
Try something like: Select caseid,title,[1],[2],[3] From ( Select c.caseid, c.title, s.secretary_level,s.secretary From case c Inner join secretary s on c.caseid = s.caseid ) Pivot ( Max(secretary) For secretary_level in (1,2,3) ) Pv
Thanks! This looks like it works. While we're here, can I ask an even more basic one? I'm not confident in how to properly do the following: **Table1:** CASEID | TITLE :--|:-- 1 | Smith v. Jones 2 | Klein v. Hanson 3 | Cruise v. Tanner 4 | Boggins v. Diddle **Table2:** CASEID | POSITION :--|:--|:-- 1 | Jane | Secretary 1 | Mike | Paralegal 1 | Beth | Clerk 2 | Joe | Paralegal 3 | Susie | Secretary 3 | Rich | Paralegal 4 | Craig |Secretary **This is what I want:** CASEID|TITLE|SECRETARY| :--|:--|:-- 1 | Smith v. Jones | Jane 2 | Klein v. Hanso | 3 | Cruise v. Tanner | Susie 4 | Boggins v. Diddle | Craig 
Multiple side gigs: Adjunct professor at small college Softball uniform and equipment rep/salesman - everything is drop-shipped 
Its not really clear how that second table relates to your final output. Can you elaborate?
You'd want a LEFT JOIN from Case to Secretary On CaseId = CaseId and Position='Secretary'
Would it be the following: Select C.CASEID, C.TITLE, N.NAME from Table1 C LEFT JOIN Table2 N on N.CASEID = C.CASEID where POSITION = 'Secretary' I think the above would not give me the lines where there is no Secretary. I think you're saying that I'm supposed to incorporate the Position into the join, but I'm having a little trouble figuring out what that looks like. 
I only have 2000 subs, so I'm not even medium sized. I only publish about 1 video a month and a lot of that has to do with I'm going quality over quantity. I think my audience really appreciates this approach as they tend to be older (over half are 25-44 years old, which is on the high side for youtube). The big reason I'm successful though is the algo loves me. 97% of my views are from non-subs and it's entirely search and suggested driven. I get 900-1000 views a day that average 4+ minutes each. IMO this is because I do a good job with descriptive titles, fleshed out descriptions, and lots of relevant tags. A chrome plug-in that's been indispensible for me is TubeBuddy. It does a good job with helping to find tags I wouldn't have otherwise thought of. I think this also contributes to my higher than normal CPM. Link to my channel: http://www.youtube.com/c/taylordtech
Which part would you need help with? If you've built reports in the ssrs report builder and deployed those to run a schedule you've done the bulk of it already.
It's almost what you have above, but change the "where" to "and" so the secretary filter is part of the condition for the left join. Then you'll get all the rows from Table1 but only matching secretary rows from Table2.
Awesome! Thanks! 
We have individual packages for each table. Each package keeps track of the lsn for that table. A master package kicks off all packages, with some minor ordering logic, like kick off orders before customers so you don't get an order but no corresponding customer.
How do you find prospective clients and sell to them? 
This worked perfectly, I can't believe how elegant that was. Thank you so much!
Thank you so much for your help :)!
really? CREATE TABLE AS worked? you can't be running MS SQL then...
The latest version. SSDT for Visual Studio 2017 The release number: 15.5.2 The build number for this release: 14.0.16156.0
http://radacad.com/incremental-load-change-data-capture-in-ssis is what I followed for 1 table, but I want do it for multiple tables using multiple packages.
Thanks a lot for the information! 
Looking to get into applying my modelling skills to the markets. Any suggestion on where to start?
1. Find several free APIs you like. You'll want historical stock and company data. 2. Pick investments you understand. You need to be able to tell if your model is spitting out nonsense. 3. Stay away from day trading. For fun models can't keep up with market movers. It's like chasing a shadow. 4. Test with play money accounts. Let models prove themselves before you trust them with real money. 5. Diversify. Make lots of different models. Some will win; some will lose. Over time you'll collect winners.
As in day trading? 
Nah. All my puny models would be able to understand is how to chase market movers around. I have my doubts that there is enough margin there for someone late to the party. My strategies are to discover no name stocks that are undervalued, and get out of things that are poised for a correction.
How do you detirmine
The models do most of the figuring it out. They looks at things like long term trends, short term trends, market cap, revenue, profit, industry trends, and dozens of other metrics. Anything I can get my hands on for free and automated.
How does the DB connect to this external source? What nic/servers are hosting this new virtual server? I’ve seen severe performance issues due Broadcom NICs on Dells having VMQ enabled on the host of the VM. DETAILS.... the DEVIL is ALWAYS in the DETAILS How does the SQL Connect to his external source? iscsi, mapped drive? Did the external source migrate with the SQL server or is it static with no changes from the “old” setup? 
besides SELECT? no
so I would have to use individual SELECT statements for each table correct? Thank you for the quick reply.
Any APIs you can suggest?
well, you could use a front-end like HeidiSQL, lets you view the data in a table just by clicking on the table name of course, under the covers it's running a SELECT, but at least you don't have to do it
hey man thanks! i totally forgot about Heidi that works!
you have to turn the spool off to finish and write out the file spool off
I did that and also quit the instance of sqlplus, but my RTF file is empty 
The it way to know if this will make things better is to test it out on a Dev copy of the database. 
Spend an entire day playing with clustered indexes and all that fun stuff? I guess I'll have to just bite the bullet and do it I suppose unfortunately
Try these: https://www.alphavantage.co https://www.quandl.com https://intrinio.com
Thanks very much!
This seems like a poor idea in my mind. Since you'll be having to break apart the index any time you want to filter. Honestly, I'd start with: * BIGINT Identity(1,1) PRIMARY KEY * NONCLUSTERED INDEX EffectiveStartTime INCLUDE EffectiveEndTime *This may need tweeking depending on the nature of the reports*) * Set your table growth to be slightly larger than your expected daily max
Do you need this table for one specific query or are you trying to build other things around it? You could use CTE , temp tables, table variables or permanent tables...
&gt; I think that's the right tag Just pointing out that MySQL is completely different from Microsoft SQL Server. That would be MSSQL.
Is your start and end times going to have a unique constraint? Otherwise SQL server adds overhead I thought. Have you tried using an indexed view for your specific query?
I finally got it. The result was correct, but postgreSQL rounded to 0. select d.year || '-' || d.month || '-' || d.day as date ,count(distinct (case when "event_name" IN ('game_start') THEN "user_id" END)) * 100 /count(distinct (case when "event_name" NOT IN ('user_login', 'user_logoff', 'game_complete', 'user_create') THEN "user_id" END))::float AS pct FROM events e INNER JOIN dates d ON d.id = e.date_id group by d.year || '-' || d.month || '-' || d.day order by 1;
Nah it’s for a different RDB design class 
A useless int/identity primary key seems like not what I wanted but maybe you are right. I'll have a think about it because there are quite a few IDs floating around for the same record already. How bad is it if I leave the PK/clustered index as the Guid/smallint composite key?
&gt; How bad is it if I leave the PK/clustered index as the Guid/smallint composite key? On further inspection, you wouldn't even need a BIGINT, INT would suffice. Which only takes up 4 bytes and gives you up to 2.1B~ rows. You do have to recognize that GUID+SmallInt is going to need 18 bytes per row for the index; verse just 4 bytes for Int. Meaning after the first year, assuming 56M rows, your clustered index alone will be nearly 1GB(100MB/year growth); where the Int will only be 210MB~(20MB/year growth). This is a huge waste of resources and will have performance impacts.
Looks like Oracle. Can you try a simple test? hostprompt&gt; sqlplus user/password sqlplus&gt; spool myfile.txt sqlplus&gt; select * from dual; sqlplus&gt; spool off sqlplus&gt; exit hostprompt&gt; cat myfile.txt You should see: D - X Also, are you adding any options at the start? Here are examples of things you can set. Not saying you have to do these things, but there are some configuration commands in sqlplus that can affect output and formatting so I will list some in case it inspires you. clear break clear comp clear col set pagesize 0 set linesize 9999 set trimspool on set tab off set echo off set feedback off set recsep off set serveroutput on size unlimited
GUIDs are a stinker of a choice when it comes to clustered index. A clustered index is less of an index and more of a data organization and data storage and data retrieval mechanism. And not just data but also the index metadata (the index b-tree). A good clustered index should be [narrow, unique, static, and ever increasing](https://www.red-gate.com/simple-talk/sql/learn-sql-server/effective-clustered-indexes/). Narrow here means compact, that is, taking up lesser bytes. You can see right away that a guid is a horrible choice. It is big and bloated, is not unique, is not consecutive or ever increasing. The auto incrementing primary key that you consider useless actually works great at efficiently storing the index metadata and data. Remember, even your non clustered index uses the clustered index to retrieve the data, so a compact, sequential clustered index will almost always trump a big random string or dates glooped together. You might actually find an order of magnitude difference in performance but i am only guessing. In some of our past tables, we did see a dramatic difference in performance. But you should really benchmark this and it would be nice if you reported back on what you found!
Visio 
Lots of good tools exist. Back in the day, I used sql fairy. Currently data grip is a good tool. Most of my work is sqlite at the moment, I actually wrote a tool to do that sporadically that includes grouping tables, sqlite3todot
Just downloaded (free through my university) and it seems perfect! Thanks for your help.
[removed]
Do not keep auths in a plain text format either. 
I was able to figure it out; thank you for your suggestion! 
Hope this helps, Select user_id, incident_code, data1 Where incident_code = '21' AND data1 &gt;= '2 chat windows'
If data1 always begins with a number then it could be done user substr and getting a max of the first character. Here is an example I put together using Oracle: https://livesql.oracle.com/apex/livesql/file/content_GAON2NBUQD2K51QXRZET41TKH.html
Very helpful, thanks much!
According to [the postgres docs](https://www.postgresql.org/docs/current/static/functions-string.html), `left` does the following: `Return first n characters in the string. When n is negative, return all but last |n| characters` So you can just use `left('56 total', -6)::int` to convert to integers.
Thank you!
BI consultant here. Oracle tends to hold large amounts of data and I'm surprised the query will not freeze the Access application if you're incorporating data for those three data hosting providers. Access is actually a great idea functionally as long as the source datasets are small enough but it's not best practices. I would look at creating a BI solution. We typically don't recommend running regular queries against different data sources unless the query is running in a sole environment made for analytics. You be looking at creating a data warehouse or data lake design based upon the split data sources as you are referring to. Data Lakes are used for the same purpose as a data warehouse but are typically big data. Both are designed for the analytic capabilities are you are seeking but are usually done at an enterprise level and cost a lot of time and money. A shorter solution would be to create a BI solution which pulls the data using SSIS into a sole database, such as a data mart. You might only be pulling in the data you need for your analytics to keep the size of the data down. If the filtered data is small enough, Access can be linked to the SQL Server database so you can run your query. So basically, in the data mart, you would run your query there. That is how I would handle the solution.
https://github.com/Microsoft/sql-server-samples
Okay, that makes things a lot clearer. Thanks.
"seemingly unrelated tables, like there is no commonality" is a pretty general description i'm gonna guess the prof is looking for a UNION query
Adding my 2c's. Instead of trying to figure out the length of the numeric, just drop the text that always remains static. Avg(Replace(ColumnName,' Total','')) [Replace](http://www.postgresqltutorial.com/postgresql-replace/) 
For PostgreSQL, there's this DVD rental example database: http://www.postgresqltutorial.com/postgresql-sample-database/
maybe you need to derive something from columns in one table to use it for the join?
you have unbalanced parentheses the error occurs after the end of your query because the first subquery isn't properly closed it's more obvious when the query is **formatted** (hint, hint) SELECT user_id , AVG(total_intbooks) FROM ( SELECT user_id , (CAST(total_bookmarks as int)) FROM ( SELECT user_id , (LEFT(data1, -16)) AS total_bookmarks FROM events WHERE event_code=8 ) AS user_bookmarkavg GROUP BY user_id
Even though I feel sub selects aren't the best method to achieve this, it looks like you still need an outer parenthesis from your first sub select and beyond that the GROUP by should be placed at the same level as your aggregation. The missing parenthesis should at least help lead you in the right direction. Good luck!
Note that this would work only up to 9 chat windows because '2 chat windows' &gt; '10 chat windows'.
It looks like you have what you need at this point, but in cases where you want to extract a number from text, you can using a regex expression. Consider this: select regexp_replace('124 chat windows', '[^[:digit:]]') from yourTable; The answer will be 124, which is a number that you can do things with. The regex function extracts everything that is 0-9 and leaves out any letters and symbols. It's more versatile than position-based substring or depending on trying to match a partial string, so it works in all cases. Then once you've got it in a number, you can aggregate totals, etc. For example you could average a user's chat windows open over time to see who multitasks the best. 
&gt; When I try to get said data, of course it's thousands of lines long and I know that isn't what he wanted. It sounds like you're talking about a Cartesian Product and sometimes that *is* what you want. It's not common, but it does happen on occasion.
The Cartesian product, or `cross join` is exactly what you're looking for. It's not commonly used, so be sure this is actually what you need. I only use it when I need something like joining a list of dates with a list of employees or time slices, to ensure there's a row for every combination (normally as a foundation for a left join, ensuring consistent output). 
Does it need to be secure? If it can be public, both Tableau and PowerBi support a free embedded web model.
[removed]
No it doesn't need to be secure. Any advantages to using Table over PowerBi or the other way around? 
PowerBI is a little better at helping you transform your data if it's not ready to visualize it out of the box. Tableau is prettier and has a more "narrative" look and feel. Tableau Public is here https://public.tableau.com/en-us/s/download PowerBI is at PowerBI.com, just get the desktop tool and then you can publish to the web with an embed code using these steps https://docs.microsoft.com/en-us/power-bi/service-publish-to-web
What I meant with no commonality was tables that have columns like this: table 1: Name, City, ID Table 2: Product, Quantity, Order Number So I must be missing something here, because I don't see how one table relates to the other for a join.
It's mostly to relearn syntax and joining.
&gt; PowerBI Fantastic, I'll go for PowerBi. Thanks for the help
Is there some other table that links to both?
yeah, you're right -- there's nothing here to join on tell your prof it's a bullshit question
Here's a sneak peek of /r/PowerBI using the [top posts](https://np.reddit.com/r/PowerBI/top/?sort=top&amp;t=year) of the year! \#1: [MagnumBI- Due to contractual agreements with other software providers, while speaking at conferences I’m not allowed the mention our company’s use of PowerBI. I’ve found a solution for my presentations.](https://i.redd.it/3qwrymulhuxz.jpg) | [3 comments](https://np.reddit.com/r/PowerBI/comments/7cs5c1/magnumbi_due_to_contractual_agreements_with_other/) \#2: [Power BI Desktop January Feature Summary](https://powerbi.microsoft.com/en-us/blog/power-bi-desktop-january-2018-feature-summary/) | [0 comments](https://np.reddit.com/r/PowerBI/comments/7p96zc/power_bi_desktop_january_feature_summary/) \#3: [Desktop September Release Available - includes drillthrough and report insights plus theming updates](https://powerbi.microsoft.com/en-us/blog/power-bi-desktop-september-2017-feature-summary/) | [2 comments](https://np.reddit.com/r/PowerBI/comments/6z0yhu/desktop_september_release_available_includes/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
There are other tables that don't seem related, and I'm not sure how bringing them in would answer what he wants. For example, what he wants are the order numbers, product, quantity, who bought the product (name), in a specific city. Without a join, this generates a query thousands of lines long. So then there are other tables too, like: Table 3: ID, Part, Catalog_Route Table 4: Part, Description, Order Number So tables 1 has a column in common with table 3, and table 2 has a column in common with table 4, and table 3 and 4 have a column in common. But the two that I need information from don't have a table in common directly. So is there a way to join these all together? That seems like it would be a huge mess. Of course I have no idea how to do it.
My quick impression is that this is what your prof is looking for. You can join multiple tables just like you join 2 tables. Play around with it in a simpler case and it will become more intuitive.
Take a good look at those column names again. There are definitely common names to join tables on. But to get from t1 to t2 you're going to have to figure out how to go through t3 and t4. Just be sure to experiment a little to make sure that common names = common data you can join on. Because you never know. I'd help more, that that is clearly the entire point of this exercise.
Concatenate in T-SQL with '+' or the CONCAT() function
Thank you, I've been confused about this for the last hour. I should have read more carefully, because my book [says that](https://i.imgur.com/b6OG8bh.png).
Adding nonclustered columnstore indexes on big tables (MS "operational analytics") may be a useful approach: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/get-started-with-columnstore-for-real-time-operational-analytics
On mobile, so paraphrasing a bit. I'm sure things have changed, but the Northwinds sample database has been used as a great mockup of a relational dataset for a small business. There's versions for different flavors of SQL that you can easily download and play around with. In terms of practical application, think of different business questions you or other stakeholders in the business might ask, and try to answer them using SQL. Things like "what's been the growth of my customer base" or "which products have efficient/inefficient returns" are good starting points.
Yes, SELECT * FROM Table1 T1 JOIN Table2 T2 ON T1.ID = T2.ID JOIN Table3 T3 ON T3.ID = T2.ID JOIN Table4 T4 ON T3.Part = T4.Part etc. Sounds like the point of your assignment is for you to figure out how to join the tables. It sounds like a huge mess, but I just basically did it in 8 lines. YMMV.
You need to add case statement to your select... Case when (line2 gibberish) between 10 and 20 Else when (line2 gibberish) between 20 and 30... blah blah Else 0 end as countgroup
Oracle SQL Developer supports 3rd party JDBC drivers, I have mine set up to connect to both MySQL and MSSQL instances. Excel also lets you query various datasources set up via the Windows ODBC connection manager.
I just got an email from the PASS virtual group. PASS is a user group of professionals that share their knowledge, and it's free to join. This upcoming Saturday night there is a talk from a data scientist about what you need to know and how to land a data scientist job. I highly suggest you sign up for it and attend if you can. If you can't, still sign up and I think they will post the video later on. http://saturdaynightsql.pass.org
Can't you combine GREATER () with MAX()?
Your building a report on top of an excel doc? Interesting
I am getting a couple of values from an excel, to compare to the SQL and Cube data. Budget data cant be just given to the people who need it! What kind of crazy talk is that!! To say its my ideal solution would be a goddamned lie! 
Well... First things first then... How are you reading from an excel file from a stored proc? 
Maybe I am missing something here or I am not understanding your question but what not use UNION ALL and two separate SELECT statements.
I'd also check and make sure that the user running the report has permissions to administer bulk operations. Usually when I bump into issues like this, it's always permissions issues biting me in the ass. I've learned to get those out of the way first. 
Already checked that. And some of the reports are picking up the Budget figure from the same method. Its killing me slowly. 
So there would be a bunch of ways to do this, try these two perhaps; *Land the data in another table (temp even) and work from there OR *Just change the case statement to reflect something like this, sum(CASE WHEN max_tabs &gt;= 5 THEN 1 ELSE 0 END)
change 'NO' to NULL and then you can simply count the number of YESes SELECT COUNT(max_tabs) FROM ( SELECT user_id , max_tabs , CASE WHEN max_tabs &gt;= 5 THEN 'YES' ELSE NULL END FROM ( SELECT user_id , ROUND(MAX(CAST(LEFT(data2,-5)AS INT)),0) AS max_tabs FROM events WHERE event_code=26 GROUP BY user_id ) AS five_orover ) AS all_users 
MIN() only works over the rows in 1column. It sounds like you want the minimum of a set of columns, and then use that as your column, so I'd recommend you use the CASE WHEN statements.
To be clear, my comment was specific to MSSQL and to my knowledge, I don't know of a greater() function in MS. I know there is GREATEST() in Oracle, but not an equivalent for MS
Apologies for the late reply. The deadlocks certainly require investigation, even if you're using retry logic at any step. Understanding the queries involved and why they're clashing will ultimately be they key to resolving the issues; might be code refactoring, appropriate indexing. So there's lots of best practice guidelines, spBlitz by Brent Ozar might give you some pointers but I always, always suggest understanding the configuration items first; like the point made about parallelism. Index fragmentation probably isn't the issue, however, any slowness needs investigating at the query layer, check out the execution plans, related wait statistics - all will provide the pieces to the jigsaw!
When we are running reports, we use the NOLOCK table hint (READUNCOMMITTED table hint is similar) to ensure the reports don't lock the respective tables. SELECT s.StuffId FROM Stuff As s WITH ( NOLOCK ) Now, at some point this hint is supposed to be deprecated, but it hasn't occurred yet, but it will negatively affect you should that happen. It is possible that you will read data that has been changed but not committed. So ostensible, you could have false data and a subsequent run will be different. BUT, that's a risk with actively changing data, anyway. Another option might be set the transaction Isolation Level. 
select count(*) where STATUS = ''. You should also understand if the field contains an empty string or a space.
We have a custom script from the vendor that updates statistics. I run it every weekend and on Monday performance is always slow. The vendor recommended to rebuild indexes; so we did and now reports run smooth. However, inserting transactions causes deadlocks. We had the SQL Server on a Nutanix cluster, but was performing very terrible... so we migrated it to a vSphere cluster running QNAPs for storage. The vendor recommended the CPU and RAM specs. As our old server cluster we didn't have enough cluster memory to boost the server up to 64 GB and RAM and also have 7 other databases alongside of it, so we were always hitting memory maximums. I have heard of and installed the Ola Maintenance Solution but I was weary on relying on a script that could potentially corrupt 2 years of accounting data. I'll check out the white paper - I've followed best practices and a VMware article but I don't think it was this one. 
Vendors don't always know best. I've seen lots of "vendor recommendations" that are poorly thought-out and vendor code that was downright dangerous to data integrity **and** performance. At a previous job, I spent a **lot** of time fixing what vendors did/recommended because it didn't work properly. &gt;The vendor recommended to rebuild indexes; so we did and now reports run smooth Again, the index rebuild may not have caused the improvement, but rather the corresponding statistics update. Next time performance lags, update your statistics *first*. What were the vendor's CPU recommendations based upon? Is it possible that they're recommending a vast over-provisioning of CPUs "just in case"? Did the recommendation come with a corresponding change to MAXDOP and cost threshold for parallelism? I've never heard of Ola's scripts causing data corruption. I don't even think it's possible for an index or stats rebuild *to* cause data corruption. Honestly, I trust Ola's scripts for index maintenance over a lot of vendors', because Ola's got **thousands** of people running his stuff against tens of thousands of databases/configurations so if there were bugs causing problems, they'd be found out very quickly. Go get Brent Ozar's [First Responder Kit](http://firstresponderkit.org/) and run `sp_blitzfirst` and `sp_blitzcache` (for starters) so you can see what's *really* going on in your server.
How do I find if it is an empty string or space?
select count(*) from table where status = '' or status = ' ' this will check for empty strings and spaces.
SELECT SUM(CASE WHEN COLUMNNAME = '' THEN 1 ELSE 0 END) 
Just a heads up, I see relatively frequently where indexes can cause deadlocks. Generally some index tuning and adjustments will resolve the issue. I'm betting the index script fixed all the indexes so things are quick and using the indexes again. As usual though, I agree with everything you said in both posts, solid great info. Here's a few links on index deadlocks. https://www.mssqltips.com/sqlservertip/2517/using-a-clustered-index-to-solve-a-sql-server-deadlock-issue/ https://www.sqlpassion.at/archive/2014/11/24/deadlocks-caused-by-missing-indexes-in-sql-server/ https://social.msdn.microsoft.com/Forums/sqlserver/en-US/4614f93a-0961-4894-9965-6ef3880488da/deadlocks-can-they-be-caused-by-indices?forum=sqldatabaseengine /u/cmcohelp
Local and remote sql server are exact same version?
&gt; The vendor gave us a custom written script but it takes 15 minutes to execute and they said "Wow that is a bit too fast..." Perhaps I should use Ola update stats scripts? Running Ola's scripts isn't going to hurt. Most people schedule it to run during off hours so the duration doesn't matter as much. &gt;so they did not give us any recommendation related to MAXDOP and parallelism OK, I'm going to go back to the First Responder Kit and ask you to run `sp_blitzfirst` while one of these reports is running. Report back on what your highest wait stats are (I bet we'll see `CXPACKET`). Also check your MAXDOP and CTP configurations: sp_configure 'max degree of parallelism'; go sp_configure 'cost threshold for parallelism'; You'll probably see 0 and 5 for the `config_value` and `run_value`. These are the defaults and they're both meh. MAXDOP should be the number of cores in each NUMA node at most, but you might want to make it 4 to start. CTP of 5 is *way* too low for modern hardware; change it to 50 and then tune (if needed) from there. sp_configure 'max degree of parallelism', 4; go sp_configure 'cost threshold for parallelism', 50; go reconfigure Both of these changes can be made mid-day with no downtime; it'll flush your plan cache so queries may be slow as that refills but that's about it. Then re-run your reports and check those wait stats again. &gt;Also, the vendors recommendation was based on other customers? Yep, been there. "Well, it works fine for our other customers, I don't know why you're having trouble." The trouble comes in when you're scaling the system up by 10X compared to those other customers. What works for the customer with a 3GB database may not for the volume of activity that comes with a 30GB database. You might be a small customer, you might be a large customer, I don't know. But blanket recommendations of "this many CPUs" should be eyed skeptically. &gt;The VM has 2 virtual sockets and 12 cores per socket. The physical host has 2 sockets and 6 cores per socket. You're over-provisioning. What's your [CPU ready time](https://www.sqlskills.com/blogs/jonathan/cpu-ready-time-in-vmware-and-how-to-interpret-its-real-meaning/)? Over-provisioning CPUs when you have the sum of the number of CPUs in all your VMs &gt; the number of physical CPUs is common. But giving **one** VM more CPUs than your host physically has will probably cause you trouble.
deadlocks aren't typically a sign of fragmented index but a sign of poorly designed database or queries... the fact that they were fragmented makes the problem more obvious happen more likely to occur but is not the cause.
My query is a little convoluted, but I made it in a hurry. You can find the rank of each monthly call by flooring the call date time by the month (partitioning by Customer ID), and then only selecting the calls with a call rank of 1. Then rank again (ugh...) based on Call Date Time, partioned by CustomerID selecting calls with a rank of &lt;=3. SELECT * FROM ( SELECT ch.ID , ch.DoctorID , ch.CustomerID , ch.CallDate , DENSE_RANK() OVER (PARTITION BY ch.CustomerID ORDER BY ch.CallDate desc) AS rnk -- Ranking by newest to oldest call FROM #CallHistory ch CROSS APPLY ( SELECT ID , CustomerID , DENSE_RANK() OVER (PARTITION BY CustomerID, DATEADD(MONTH, DATEDIFF(MONTH, 0, CallDate), 0) ORDER BY CallDate desc) AS CallRnk --Ranking by Calls Per Month FROM #CallHistory ) ca WHERE ca.CallRnk = 1 GROUP BY ch.ID , ch.DoctorID , ch.CustomerID , ch.CallDate ) a WHERE a.rnk &lt;= 3 ORDER BY a.CustomerID, a.CallDate desc
&gt; OK, I'm going to go back to the First Responder Kit and ask you to run sp_blitzfirst while one of these reports is running. Report back on what your highest wait stats are (I bet we'll see CXPACKET). &gt; The highest wait stat is CXPACKET Priority 200 Wait Stats CXPACKET Priority 200 Wait Stats LATCH_EX Priority 200 Wait Stats SESSION_WAIT_STATS_CHILDREN Priority 200 Wait Stats SOS_CHEDULER_YIELD Priority 200 Wait Stats CXROWSET_SYNC &gt; Also check your MAXDOP and CTP configurations: sp_configure 'max degree of parallelism'; go sp_configure 'cost threshold for parallelism'; You'll probably see 0 and 5 for both the config_value and run_value of these. These are the defaults and they're both meh. MAXDOP should be the number of cores in each NUMA node but no more than 8 (0 tells SQL Server "take what you want!", but you might want to make it 4 to start. CTP of 5 is way too low for modern hardware (even simple queries will go parallel when they don't need to); change it to 50 and then tune (if needed) from there. &gt; Correct, 0 for config and run for MAXDOP and 5 and 5 for CTP. If I change the CPU configuration, would I need to revisit these settings? I can change the MAXDOP and CTP, but if I'm over-provisioning, will I need to change these back in the future? &gt; Yep, been there. "Well, it works fine for our other customers, I don't know why you're having trouble." The trouble comes in when you're scaling the system up by 10X compared to those other customers. What works for the customer with a 3GB database may not for the volume of activity that comes with a 30GB database. You might be a small customer, you might be a large customer, I don't know. But blanket recommendations of "this many CPUs" should be eyed skeptically. &gt; We are considered a very large customer for them. The thing I forgot to mention is they host everything on SQL, but they offer no help or guidance on SQL Server. They do no troubleshooting... So they threw numbers at us and we followed along so we can finally tell them "Hey, we took your advice and it still sucks!" When I heard 24 CPUs... I knew something was wrong... but because it performed so poorly before we just went ahead with the recommendations. &gt; You're over-provisioning. &gt; Here is the CPU Ready Time. https://imgur.com/a/nTFue CM-SQL02 is the SQL server CPU Real-time.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/8cl5C5x.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dujsyyc) 
&gt; &gt; sp_configure 'max degree of parallelism', 4; &gt; go &gt; sp_configure 'cost threshold for parallelism', 50; &gt; go &gt; reconfigure &gt; So we had changed this and the script ran within 3 seconds and no longer are there any CXPACKET or WAIT findings. It just says query problems plan cache erased recently. Perhaps we need to wait...? 
https://imgur.com/a/bmyjF I found an article that explains this When a high CXPACKET value is accompanied with a LATCH_XX and with PAGEIOLATCH_XX or SOS_SCHEDULER_YIELD, it is an indicator that slow/inefficient parallelism itself is the actual root cause of the performance issues. And in such a scenario if the LATCH_XX waits is ACCESS_METHODS_DATASET_PARENT or ACCESS_METHODS_SCAN_RANGE_GENERATOR class, then it is highly possible that the parallelism level is the bottleneck and the actual root cause of the query performance issue. This is a typical example when MAXDOP should be reduced. Our wait stats did show CXPACKET, LATCH_EX, and SOS_SCHEDULER_YIELD... We set MAXDOP to 4, it was 0. Is '4' a lower value than 0? Also, I was just told that we are going to have a 'professional' come on-site on behalf of the vendor. I want to iron these issues out, but the Director of Technology doesn't want me to fix the issue, have the tech fly here, and then turns out everything is fixed. I want it fixed, but also want it not fixed to see if they can figure it out themselves... When a high CXPACKET value is accompanied with a LATCH_XX and with PAGEIOLATCH_XX or SOS_SCHEDULER_YIELD, it is an indicator that slow/inefficient parallelism itself is the actual root cause of the performance issues. And in such a scenario if the LATCH_XX waits is ACCESS_METHODS_DATASET_PARENT or ACCESS_METHODS_SCAN_RANGE_GENERATOR class, then it is highly possible that the parallelism level is the bottleneck and the actual root cause of the query performance issue. This is a typical example when MAXDOP should be reduced.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/74XnD6p.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dujuskv) 
&gt; If I change the CPU configuration, would I need to revisit these settings? I can change the MAXDOP and CTP, but if I'm over-provisioning, will I need to change these back in the future? It depends on how far you cut back the CPUs. If you set MAXDOP down to 4, then you won't need to change it unless you drop to fewer than 4 cores per NUMA node/CPU. You may need/want to go higher if your (virtual) hardware allows for it and experimenting with your queries shows improvement. As for CTP, leave that at 50 for now. That number isn't really affected by the CPU count. &gt;hey host everything on SQL, but they offer no help or guidance on SQL Server. They do no troubleshooting... So they threw numbers at us and we followed along so we can finally tell them "Hey, we took your advice and it still sucks!" Sounds to me like they're just grasping at straws and throwing anything out that they can think of, assuming that it's your hardware and not their code. Spoiler alert: it's probably their code!
&gt; Are there small/mid size business that would like volunteer help? Or non-profit/local governments that wouldn't mind some extra help. There are very few organizations who would allow another user to help or contribute to their data, even if those are extremely senior and well known respectable individuals. Data is a very valuable commodity. Google Brent Ozar. You can search how to get a job brent ozar, first job brent ozar, etc. He has around 15ish blogs on how to get experience and in the door as a data person.
Null should NEVER equal anything - not even another null. If it does, then the implementation does not work properly. So Select count(*) From table Where attribute = ‘ ’; ...where the quoted string contains as many blanks as there are characters in *attribute* should work. HTH
OK, so these are your "problem child" queries (at least for starters). You want to tune these, either by fixing the queries or creating appropriate indexes (or improving the ones you have). Where CTP comes into play is this: Any query with a cost over 50, SQL Server is going to run some operations in parallel. Under 50, it'll all be serial. Parallel is not always better! The `CXPACKET` waits are what happens when one of the parallel threads completes but you're waiting on the rest of them to finish. It's normal to have *some* but if it's one of the main waits then you've got to look closer.
Ah, query tuning. Now we're into some art, not just science. Whole books have been written on the topic. Yes, really. The main questions: Can you change the queries, and can you add/change indexes? Are there even indexes? Are the tables well-designed? Take your most expensive queries and run them in SSMS with Actual Execution Plan enabled. Save the XML (right-click in the Execution Plan window) and upload to http://pastetheplan.com/ so you can share the link.
Use a Full Outer Join 
I believe you want to cross-join. This will give you a cartesian product of all the possible combinations
10/10 Brent Ozar
 select * from #table1 cross join #table2
I'm not sure but for reference, this is also called a Cartesian join. It's what you get when you do Select * from table1, table2 Not sure if that syntax works in access though. 
Doesn't that depend on the version, settings and whether ANSI_NULLS are set to ON/OFF in the query?
This is the best answer 
I am not sure if I can set certain queries to run parallel or serial. If so, I don't know how... I don't think this individual would fix issues. They have been blaming it on the environment for years and many other firms complain about performance. I asked the same question - why can't they remote in? I think they're getting a SQL expert, but I asked and they are unsure. I agree, with all of the helpful information you've given me, I think I'm in a better position to correct this than the individual who will be coming next month. Well the Director said "If he comes in and points out environment changes, we can use what your guy on Reddit said and ask them 'what about certain SQL settings?' etc. The tech will be flying at the vendors expense because we threatened to move onto another application (and we are in the process of looking). Importing a QuickBooks file with many transactions... would that help in parallelism? 
The vendor would have to fix the queries, right? I didn't code the software. And creating appropriate indexes and improving the ones we have? See, this would make sense if I was the programmer of the application, but I am not so how would I even go about creating indexes or improving them when I never built them? So perhaps the CXPACKET waiting for parallel threads to complete, but waiting for the rest of them to finish is where the deadlock is coming from? With active and recent queries, how can I determine which ones are working with the CTP 50... Like, how do I go from here? How do I figure out which queries will run better in serial or parallel? I'm no DBA... been working with SQL for many years, have it at home, but never had to go in and modify queries because I thought all of that was coded into the application... 
&gt; Take your most expensive queries and run them in SSMS with Actual Execution Plan enabled. Save the XML (right-click in the Execution Plan window) and upload to http://pastetheplan.com/ so you can share the link. &gt; This is confusing. So I have to copy a query and then run them in SSMS with Actual Execution Plan enabled (gotta figure that out), save the XML (wherever that is...) and then I can upload it. But if I re-run a query, wouldn't it screw up any accounting information that was inserted? I don't want to run something twice...
Thanks so much for your help. I'm trying to incorporate this logic into a much larger query, so I can't tell yet if this is the solution. But thank you!
&gt; I am not sure if I can set certain queries to run parallel or serial Not really. SQL Server makes that determination based on the query itself, indexes, statistics, etc. You *can* force the query plan that the engine calculates to be serial via `OPTION (MAXDOP 1)` but that would require a plan guide and that's basically a last resort (and plan guides can be bypassed with a single-character change in the query). You're better off making sure the query is written properly in the first place and that it has the indexes it needs. Another thought - since you're on 2016, you've got Query Store at your disposal. You can use that to monitor your queries and keep tabs on their performance over time (is a particular query getting slower over time, do different parameters send it off a cliff, etc.). If you have PluralSight, [here you go](https://www.pluralsight.com/courses/sqlserver-query-store-introduction). If you don't, or just want more info on it, Google "sql server query store Erin Stellato" and start readin. &gt;They have been blaming it on the environment for years and many other firms complain about performance. The vendor is blaming it on the environment *and* other customers complain about the performance? Maybe because they can't provide good guidance on how you should have your SQL Server configured? Poorly-written queries from their application? Lack of indexes? &gt;Importing a QuickBooks file with many transactions... would that help in parallelism? Probably not. And it'd depend on how the import is actually done. If it's data that can be bulk-imported, that may help, but for a big import if you need to do it in parallel (assuming the data allows for it), that has to be on the application side.
I thought you were talking about reports taking a long time. Reports should be read-only. Yes, anything that runs insert, update or delete you should not be doing this with. At least not outside a test environment.
Yes, it's on the vendor to fix the queries. If they're in stored procedures, you *could* in theory change them, but you'll probably lose support from the vendor at that point. Creating indexes doesn't require the application's programmer. You're looking at the query itself, understanding how it accesses data, and then creating indexes that will enable that to be done faster. You have the query text (from `sp_blitzcache`) of your most expensive queries. Creating indexes isn't free - anytime you update data that's in an index, you have to update the index as well. So there's more I/O load on your system. &gt;With active and recent queries, how can I determine which ones are working with the CTP 50 Are things faster or slower with the new CTP?
Isn't all of that done via the application? Create an index and somehow I can make it run faster? Hmm, I just looked and it seemed like a few more people are getting deadlock errors. One person was importing a QuickBooks file - 3 stores and took 30 minutes. The other got the message when printing reports. So based on the MAXDOP and CTP, how can I somehow test, change, and test to see if performance is better? Query store?
No, indexes are stored in the database itself. Deadlocks happen when two processes are trying to update the same record at the same time. If you're getting those routinely, then the application design itself probably needs work. You may get some relief by enabling Read Committed Snapshot Isolation, but you first have to figure out what's deadlocking See [this blog post](https://www.brentozar.com/archive/2014/06/capturing-deadlock-information/) to get started. &gt;So based on the MAXDOP and CTP, how can I somehow test, change, and test to see if performance is better? Start with baseline performance. How long does a query take to run. Now make the configuration change. Run the query again. Did it perform better? If you don't have a test instance to do this on, you're already stuck. Don't do this testing live in production.
https://imgur.com/a/RIfOx There are the deadlock messages. The bottom is the findings after running blitzcache. It says a lot of forced serialization. Also, someone submitted a crash report. https://pastebin.com/YXU7xNSX 
Yes, but doesn't a specific query developed by the vendor use the already installed and specified index? Perhaps I don't even know what an index is. I thought all of that came preinstalled when we first started the application. Unfortunately, I am not accountant so I can't import transactions like they do so I couldn't test this. I know testing the index defrag worked because I knew how to run a report... This is a live environment, but they are used to issues and so far we haven't had any until at 3 PM. Do you recommend just tuning down CTP? Maybe CTP affected it and now we are getting those deadlocks. It seems like the deadlocks occurs when small changes are being made, like changing the 'posting period' whichever that means in accounting world. One user was importing transactions. Perhaps I should run that Query Store and measure performance. 
I think you've got an application design issue at the core here. If the vendor can't even give you good guidelines for how to configure the SQL Server in the first place, you may be in a bind.
The blitzcache said something in your plan is forcing a serial query. Maybe the QB import is what costs more and the CTP needs to be adjusted to cover that 144 cost? But then smaller queries will be affected poorly.
&gt; specific query developed by the vendor use the already installed and specified index? No. The query optimizer looks at the query, looks at the indexes and statistics, then decides what query plan to use and which indexes (if any) to use. The developer should **not** be specifying indexes to use in their query.
 IF OBJECT_ID('tempdb..#PROCS') IS NOT NULL DROP TABLE #PROCS GO CREATE TABLE #PROCS ( LOG_ID INT, Procs VARCHAR(MAX) ) GO 
There's no ID column in Table 2. That's the issue. It looks intentionally designed poorly to force complicated joins for the lesson, to make them join through every table to get it tied back to the target table. I had it written out proper, but he'll miss the point if I just give the answer. The hint is that you do indeed have to chain through T3 and T4 before you can tie T1 together with T2.
If I understand what you're asking, I think you want to use something like this with a common table expression: WITH A AS ( SELECT DISTINCT(TimeCreated) AS TimeCreated FROM QRemote...Employee ) INSERT INTO dbo.tUsers_PSS(user_id,location_skey,password,first_name,last_name,active_status,create_date,access_role,employee_type,unpost_invtime_flg,is_project_manager) SELECT CONCAT(LEFT(FirstName, 1),LastName),'1','password',FirstName,LastName,'A',TimeCreated,'e','c','n','N' FROM QRemote...Employee E JOIN A ON E.TimeCreated = A.TimeCreated The point of the CTE is that you are selecting for the unique TimeCreated values on their own, and then you join that list to your select statement to filter out the records you don't want. Did this on the fly, so sorry if there's synatx errors...
I'll give it a try tomorrow and will update. Thanks for the help. 
no, you`re thinking of a cross join an outer join, whether left, right, or full, has to have a column to join on these two tables obviously do not
I was responding to ops follow up. It seems like they want a Cartesian/cross join not a full outer. I was pointing them in the direction of what to look for
Yep. Its got to be something stupid like language, or the formatting of the dates which end up being column names. but I have eliminated all the simple and stupid things I can think if. Maybe I just admit the "stupid" error is smarter than I am and try another way. 
Yes, delete the table 😁
doh, corrected it.
Thanks for the help! I'll try this in the morning when I get in the office!!
Thanks for this, but I'm afraid you lost me on the ellipsis.
Copy and paste the words from CASE to END 26 yimes replacing "a", or "b" with your 26 column names 
People still use access? 
You are going to have to perform what is commonly referred to is mapping. A reference table will be needed to map A to 1, B to 2, etc. So field 1 will contain your alphabetical characters and field 2 will have the numeric characters with each row having the relates fields in field 1 and 2. Once you have created this mapping table then you can join the original tables together by linking each to the reference table.
Hmm I don’t think that would filter anything out. Every record in Employee would still match one of the distinct times in the CTE. I think maybe using a GROUP BY TimeCreated ... HAVING COUNT(*) &lt; 2 would be the way to exclude records that share TimeCreated values.
Unpivot?
I don't think the above answer is going to get you what you want. It will still return every row in the table. You have one field you are concerned with to start, the time created field. You said you want unique values of that field, but what does that mean? You want all the rows where the count of rows associated with a distinct value for that field is 1? If you have multiple rows associated with the same TimeCreated, do you want to pick a particular row? And by what logic? I'm sure we can help you, but you're going to need to be clear about what you are trying to do. As it is, the answer above will return all rows from the table. The distinct list of all TimeCreated values from the table still contains every TimeCreated from the table, just abbreviated with no dupes. If you join that back to the table you're just going to get the whole table back again. If you want only the rows where there is a single row for TimeCreated then you could use the example above but substitute this with statement: WITH A AS (SELECT TimeCreated FROM QRemote...Employee GROUP BY TimeCreated HAVING COUNT(*) = 1)
The easy way is to take the query you already have and put the result set into a temp table. Then just do a select * from table1 where the phone number exists in your temp table.
 SELECT * FROM dbo.table1 as t1 WHERE t1.phone_number in (SELECT a.phone_number FROM dbo.table1 a GROUP BY a.phone_number HAVING COUNT(a.phone_number) &gt; 1);
I would probably do a subselect grouping by timecreated and seeing where the count is 1. Then any records that match that time are the unique ones.
If you have to do it with one query this will do it. The SubQuery is what's limiting your query b/c its only returning unique TimeCreated values. INSERT INTO dbo.tUsers_PSS(user_id,location_skey,password,first_name,last_name,active_status,create_date,access_role,employee_type,unpost_invtime_flg,is_project_manager) SELECT CONCAT(LEFT(A.FirstName, 1),A.LastName),'1','password', A.FirstName, A.LastName,'A', A.TimeCreated,'e','c','n','N' FROM QRemote.Employee A Join ( Select TimeCreated, count(*)obs From QRemote.Employee Group By TimeCreated Having count(*) = 1 ) B on A.TimeCreated = B.TimeCreated
That's exactly what I needed! Thanks for the help.
Thanks!
If you have to do it with one query this will do it. The SubQuery is what's limiting your query b/c its only returning unique TimeCreated values. Insert Into dbo.tUsers_PSS (user_id,location_skey,password,first_name,last_name,active_status,create_date,access_role,emplo yee_type,unpost_invtime_flg,is_project_manager) SELECT CONCAT(LEFT(A.FirstName, 1),A.LastName),'1','password', A.FirstName, A.LastName,'A', A.TimeCreated,'e','c','n','N' FROM QRemote.Employee A Join ( Select TimeCreated, count()obs From QRemote.Employee Group By TimeCreated Having count() = 1 ) B on A.TimeCreated = B.TimeCreated
This isn't enough info. Where is the data for na was and e-mail? Same table different table? 
Try NVARCHAR.
You explained after changing MAXDOP and CTP, it erases the query plans. I'm starting to understand this. Hours after that change, we had several issues with deadlocks. Is that suspected after changing that setting? Also, suspect licensing isn't an issue. We have a host with 2 sockets 6 cores 24 local (hyperthreading enabled) As of now, the VM is 2 sockets and 12 cores. The SQL VM is the only VM on the host. Do you still recommend downsizing to perhaps 1 socket and 12 cores, or 2 sockets and 6 cores? I did 1 socket 12 cores but decided to set it back so we don't have any issues tomorrow morning... until I can get some feedback from other sites.
So I went to print a report that took 1 hour and 20 minutes before I did the rebuild indexes. I did the execution plan and saved it as an XML. https://privatebin.net/?baf3d1a77d884bf6#2h/qmdu4JPhBIUku1S1dM6fJtAdz0tW9GhthWUA4mdc=
temp tables are very expensive compared to subqueries and CTEs
SQLite doesn't really have datatypes. They are more of a suggestion.
No doubt, but I believe these are vendor extensions. Happy to be corrected...
It's a good idea to write it as a select statement first and change it to a delete only after confirming that the intended rows are the selected properly. For this one, you can use a simple where clause... select * from some_table where column_1 &amp;lt;&amp;gt;'decline' and column_2 &amp;lt;&amp;gt; 'decline' and column_3 &amp;lt;&amp;gt; 'decline' ... and column_27 &amp;lt;&amp;gt; 'decline' If you run that, and it returns the results you expected, consider backing up the table, and then change `select *` to `delete`
I'm not sure. But this might work. select * from #table1 join #table2 on 1=1 People who know better, pls correct me. 
That website is doing something wonky to the XML. Please upload it directly to https://www.brentozar.com/pastetheplan/ (as I had asked previously) and then share that link.
I do not think that changing MAXDOP and CTP is *causing* the deadlocks. They may be causing conditions that make a deadlock more likely. The deadlocks themselves are more likely a application/query design issue, not your server configuration (IOW, I don't think you'll completely eliminate the deadlocks without the vendor fixing things in their code). In addition, the application appears to have poor or no deadlock-handling logic, such as catching the error and retrying before sending an error message to the user which is not at all friendly to a non-technical individual. I definitely recommend downsizing your VM to where your vCPU count does not exceed your host's physical CPU count. From the VMware whitepaper I linked you to previously (pate 17, section 3.3.2): &gt;When performance is the highest priority of the SQL Server design, VMware recommends that, for the initial sizing, the total number of vCPUs assigned to all the VMs be no more than the total number of physical cores (rather than the logical cores) available on the ESXi host machine. That means you should have at most 12 vCPUs allocated to your VM. At least for starters. "Rather than the logical cores" means to ignore hyperthreading in this count - you have 12 physical cores.
That website was giving me an error message which is why I couldn't upload it. 
I was going to reduce the cores to 1 socket and 12 cores but decided not to screw with anything yet. I read a long time ago that deadlocks was error code. I asked the vendor a week ago to explain deadlocks, and they said it's latency in the environment. In the car wash world, we had an application, an application server and SQL. The application server talked to SQL... and the application talked to application server. I worked on thousands of car wash POS and never saw deadlock despite manyyyyyy issues with SQL. I figured it was code... but the vendor said it's our environment. 
If you'd like to return other columns in your result set, you may want to consider using a windowing function. Even if you don't it'd be worth your time to learn about them. SELECT name, address, city, state, zip, phone_number, Cnt FROM ( SELECT name ,address ,city ,state ,zip ,a.phone_number , COUNT(a.phone_number) OVER (PARTITION BY a.phone_number) as Cnt FROM dbo.table1 a ) x WHERE Cnt &gt; 1 
Well, last night I set the configuration back to the 4 and 50, restarted SQL server VM, ran update statistics and we'll see how it goes. How can I test this CTXPACKET waits? How can I see which queries are getting CTXPACKET waits? Then once I make the change, compare it. Thanks for all of your help by the way! I think we're closing to figuring this out, all thanks to you! I'll PM you about it shortly.
Thank you. I'll take some time to learn about this method as well then. 
You forgot the code block! SELECT name ,address ,city ,state ,zip ,phone_number ,Cnt FROM ( SELECT name ,address ,city ,state ,zip ,a.phone_number ,COUNT(a.phone_number) OVER (PARTITION BY a.phone_number) as Cnt FROM dbo.table1 a ) x WHERE Cnt &gt; 1 
Oh yeah... thanks. That looks WAY better!
Hmmm I may just move to SQL Server to learn on. How easy is it to learn with SQL Server compared to other platforms?
I agree with this answer p.s. You can backup the table simply by changing it to SELECT * INTO BACKUP_TABLE_NAME FROM and then once backed up then change it to a DELETE. 
You can get SQL Server Developer Edition 2017 for free. Use the following Link. Don't forget to also download SQL Server Management Studio (SSMS) so you can actually query the database. https://www.microsoft.com/en-us/sql-server/sql-server-downloads Good luck!
Are you using Reporting Services or Report builder to create the report? You could use a matrix instead on the ALL_PROC_AS_ORDERED field i.e. LOG_ID is the row group and ALL_PROC_AS_ORDERED is the column group. It will then just append additional records as new columns. Less of a faff than using STUFF plus has the bonus of ALL_PROC_AS_ORDERED being in their own columns rather than being grouped together in the same column. You still meet the requirement of it being all in the same row for each patient. Your SQL would then just be SELECT LOG_ID, ALL_PROC_AS_ORDERED FROM OR_LOG_ALL_PROC ORDER BY LOG_ID https://www.codeproject.com/Tips/574957/Using-SSRS-Matrix-Control-to-Generate-Columns-Dyna Sometimes the best way to handle something is to handle it in the report side of things rather than the SQL and sometime vice versa. 
&gt; Insert Into dbo.tUsers_PSS (user_id,location_skey,password,first_name,last_name,active_status,create_date,access_role,employee_type,unpost_invtime_flg,is_project_manager) &gt; SELECT &gt; CONCAT(LEFT(A.FirstName, 1),A.LastName),'1','password',A.FirstName, A.LastName,'A', A.TimeCreated,'e','c','n','N' &gt; FROM &gt; QRemote.Employee A Join &gt; (Select TimeCreated,count(*)obs From QRemote.Employee Group By TimeCreated Having count(*)=1)B on A.TimeCreated = B.TimeCreated This is still entering the same entries over and over again.
&gt; WITH A AS ( SELECT DISTINCT(TimeCreated) AS TimeCreated FROM QRemote...Employee ) INSERT INTO dbo.tUsers_PSS(user_id,location_skey,password,first_name,last_name,active_status,create_date,access_role,employee_type,unpost_invtime_flg,is_project_manager) SELECT CONCAT(LEFT(FirstName, 1),LastName),'1','password',FirstName,LastName,'A',TimeCreated,'e','c','n','N' FROM QRemote...Employee E JOIN A ON E.TimeCreated = A.TimeCreated Getting this error "Ambiguous column name 'TimeCreated'."
I'm confused
Could you give me an example of the full query. I can't get any of this to work without error.
Regular expressions. Doubly good because the script will keep working if somebody throws a string other than precisely " Total" in there. AVG( TO_NUMBER( REGEXP_REPLACE(column_name, '\\D', '') ) ) Only thing to worry about with ('\\D', '') is that it'll remove all non-digits without discrimination. So if somebody writes '56 total and 1/2' it'll return 5612. But if that's every a problem you can adjust the expression to accommodate.
**I GOT IT TO WORK** USE Test GO INSERT INTO dbo.tusers_pss (user_id, location_skey, password, first_name, last_name, active_status, create_date, access_role, employee_type, unpost_invtime_flg, is_project_manager) SELECT Concat(LEFT(firstname, 1), lastname), '1', 'password', firstname, lastname, 'A', timecreated, 'e', 'c', 'n', 'N' FROM qremote...employee WHERE NOT EXISTS(SELECT create_date FROM dbo.tusers_pss WHERE dbo.tusers_pss.create_date = timecreated) 
Thank you for the reply. I'm sure this is something I will be able to use in the future. Like I said, I have to teach myself a lot. I ended up figuring out that I didn't need to make a temp table and such. I was going about it the wrong way...mostly because I didn't understand the function. I simply just put the STUFF functions in my main select statement and it worked beautifully and quickly. This matrix thing is new to me so I'll investigate it. It may even be better than what I'm doing now!
Thanks again /u/Coldchaos. I tried what you showed me this morning and I got hung up a bit, but it pushed me to try something else. I ended up just putting the STUFF function in my main select statement and it gave me exactly what I wanted. I just had to fail four times to figure out how it actually worked. But I'm sure I'll be able to use what you showed me down the road. Thanks again!!!
Hmm I've downloaded both the Developer Edition and SSMS, but this is looking far more complicated than I know how to work with right now. I essentially just want to be able to experiment with tables at the moment. Do you know of a good guide for beginners with SQL Server? Most of the ones I've found online seem to be for more advanced users already well-versed in other SQL applications.
GO has to be a line on its own (or with a positive integer or comment). You can't put other statements on the same line. https://docs.microsoft.com/en-us/sql/t-sql/language-elements/sql-server-utilities-statements-go Worth noting that GO isn't a TSQL command, it's a command used by some client tools.
Are you getting the "ambigous column name" error? You need to use the alias established in the From clause. The query above uses A and E as the table alias, so use either A.TimeCreated or E.TimeCreated wherever you referencr that column in the select of your main query: WITH A AS (SELECT TimeCreated FROM QRemote...Employee GROUP BY TimeCreated HAVING COUNT(*) = 1) SELECT &lt;your columns here, be sure to use either A.TimeCreated or E.TimeCreated, do not use just TimeCreated&gt; FROM QRemote...Employee E JOIN A ON E.TimeCreated = A.TimeCreated
I appreciate your help. I put my solution in the thread.
That's how I would set it up, but ultimately, it doesn't matter in terms of performance as they're the same. If it's only going to be SQL Server running on it (and no other licensed applications) it doesn't matter as you're paying per core so it'll be the same number of cores regardless. If the vendor is installing some software on there that is licensed per socket, obviously they would want you to have more sockets and less cores :)
Is there a script to see how many CXPACKET waits there was during a given time period? Then I can change indexes and what not and compare? A user said it's running very well after I rebuilt the indexes, but when they roll back accounting data to another period, that is when the deadlock occurs. They hit yes and it goes through. 
This is what I do
If you want to get into databases with minimal set up effort but you want to use something 'proper' then I would suggest something like the free tier on AWS or and Azure trial. They will let you get a pre configured basic database that you can start trying things out with. It's a nice place to start without worrying about installs and administration stuff which can become a headache really quickly. 
This query cannot enter in any "TimeCreated" values more than once. Maybe the other values are the same (you didn't say that was a problem) but the TimeCreated would always be unique with this query.
If you care to learn about [Correlated Subqueries](https://en.wikipedia.org/wiki/Correlated_subquery) here's a different method: SELECT * FROM dbo.table1 as t1 WHERE exists (SELECT a.phone_number FROM dbo.table1 a WHERE t1.phone_number = a.phone_number GROUP BY a.phone_number HAVING COUNT(a.phone_number) &gt; 1); [Here's a sqlfiddle example](http://sqlfiddle.com/#!18/49bfa/3)
Without using a table of years? as i need it to be dynamic. could you provide an example? Edit: pg
But assume a physical host has 2 sockets, 6 cores, and hyperthreading. Without the cost factor, would it be more efficient to have 2 sockets and 6 cores or 1 socket and 12 cores (which includes hyperthreading?
Do you have access to the underlying db/tables? Is the display value "Dad" coming from a lookup table?
Using a recursive CTE (assuming your table is named UserCertification): ;WITH YearRange AS ( SELECT MIN(year_certified) as min_year, MAX(year_expired) as max_year FROM UserCertification ), Years AS ( SELECT max_year, min_year as year FROM YearRange UNION ALL SELECT Y.max_year, Y.year + 1 FROM Years Y WHERE Y.Year + 1 &lt;= Y.max_year ) SELECT year ,(SELECT count(*) FROM UserCertification UC WHERE Y.year BETWEEN uc.year_certified and uc.year_expired) as User_Count FROM Years y ORDER BY year This works on SQL Server, i guess from your "pg" above, you're using postgres? I believe postgres supports those.
Can you rephrase or something, none of that really makes much sense
Sorry I don't know that the formatting changed. I am trying to use a specified data and compare it with db to show result of mismatches For example, Value in Table A will require Table B to contain the specific 5 digit numbers, if not, show result of what's not matching: Please refer https://ibb.co/izcwiH So if it's AA123 item, Table B must contain 42702,42703. if it doesn't, show result of the mismatch. hope that make better sense with the snapshot Thank you 
But how do those values relate? Sql Server is a **relational** database management system, so if you're trying to match magic numbers like that, any RDBMS probably isn't going to be a great solution
there's about 170 in the table to compare with.
Do you have any primary key and foreign key relationships between the two tables. Also what in your database refers to the correct values that should be in your 2nd table?
in your union all your referencing Years when it does not exist yet? Edit: i get it :p
Not sure what you mean by "efficient", but it wouldn't perform any different. VMWare calculates it by % of CPU time, so you have 2*6*2=24 cores, your VM is given 12 cores (2 sockets, 6 cores OR 12 cores) and it's therefore given 50% of the CPU time of the host (12 cores out of 24 cores = 50%)
*sigh* That doesn't tell me how they relate. You have to match **exact** values in SQL, be it a key, or a value you've created, but they have to be the same Do you even know anything at all about SQL and databases? At least be aware of the fundamentals and basic terms
If I'm to understand this, you're trying to find values in a table that do not exist in a corresponding table? If that's the case then either of these would work: DECLARE @Table_A TABLE(Id varchar(8)); INSERT INTO @Table_A (Id) VALUES ('1111'),('2222'),('3333'),('4444'),('5555'),('6666'),('7777'),('8888'); DECLARE @Table_B TABLE(Id varchar(8)); INSERT INTO @Table_B (Id) VALUES ('1111'),('3333'),('5555'),('7777'); SELECT Id FROM @Table_A WHERE Id NOT IN (SELECT Id FROM @Table_B); SELECT A.Id FROM @Table_A AS A FULL JOIN @Table_B AS B ON A.ID = B.Id WHERE B.Id IS NULL;
DECODE should take care of this for you. https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions040.htm
Sorry I should have explained better, this data are consist within the same table but different column.
Sorry I should have explained better, this data are consist within the same table but different column.
That doesn’t make sense. Like if the rows don’t line up? 
You're really going to be handcuffed long term trying to run reports off your application database. This is why the otap to olap via ETL paradigm exists. To really answer whether you've slowed down the application you need write latency metrics. It's possible that some added write latency is acceptable for faster reads, and that extra latency won't be noticed by the average user. So look at the metrics and decide what an acceptable write latency is. Once that's done you can start making changes. It's possible that compression speeds up reports and doesn't increase write latency too much. It's really hard to say. IME the only way to know is to make the change and watch your metrics. 
Not a function, but you could just use SUBSTR and INSTR in the SELECT: SELECT SUBSTR(SAMPL,INSTR(SAMPL,'_',1,1)+1,(INSTR(SAMPL,'_',1,2)-INSTR(SAMPL,'_',1,1)-1)) AS DESIRED_TEXT FROM (SELECT 'dc_DSGV6002_vm2.0' AS SAMPL FROM DUAL);
I don't see any CXPACKET waits using those scripts you mentioned the other day. Still slow stuff. I changed MAXDOP to 8... wondering if I need to adjust CTP to something lower. 
How can you tell which is the latest name? Do the items have an ID or effective date? If not, this database sounds like a nightmare to maintain or get any useful data out of :( If you have an ID and EffectiveDate you can use `row_number()` to get the latest name: ItemID|OldName|NewName|EffectiveDate --:|--:|:--|:-- 001|1|2|01/01/1970 002|3|4|01/01/1970 003|5|6|01/01/1970 004|A|B|01/01/1970 004|B|C|02/22/2018 005|C|D|01/01/1970 006|X|Y|01/01/1970 006|Y|Z|02/01/2017 ;WITH myCTE as (SELECT ROW_NUMBER() OVER (PARTITION BY ItemID ORDER BY EffectiveDate DESC) AS RowNum , ItemID , OldName , NewName , EffectiveDate FROM Items ) SELECT ItemID , OldName , NewName as CurrentName , EffectiveDate FROM myCTE WHERE RowNum = 1 
yeah - I was thinking [temporal tables](https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-table-usage-scenarios) is the most elegant solution for this
Sadly, this is all the data available. The latest name would be the NewName whc doesn't appear in OldName.