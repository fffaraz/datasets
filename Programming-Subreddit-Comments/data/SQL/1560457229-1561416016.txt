I think it just depends. I'm glad to be in the "fuck school" camp, but it came with a price, and ultimately I gained a lot of education from going to school... but I agree with you, very little of it pertained to this field. I was very fortunate to receive a "classical programming" education in FORTRAN, PERL, COBOL, RPG, etc., and I think my time in college studying other subjects greatly helped my success... but I never took a class on SQL, or databases in school. First time I worked with a database was when I was 12 and helping my mom out with some side project. I think Frank Zappa's quote that if you want to get laid, go to college, but if you want an education then go to the library is correct. College and schooling will open a whole new world of ideas, but it's on you to take the next step and actually teach yourself something useful like SQL. I just accepted a new job and no one I'm working with has a degree in CS, or IT, they have degrees in things like Economics, Mathematics, Business, etc., and just tossed SQL on at the end. I also think that not having a degree affects you more when you're younger. At 36 no one even asks me about college anymore... at 26 it was a bigger deal.
Wrap that `@tablename` in `quotename()` else you might have a malicious actor try to p0wn you. Or your code will break because there's a space in the table name that was passed in. DECLARE @SQL varchar(MAX) = ' SELECT * INTO ' + quotename(@TableName) + ' '
Just selects NULL with the specified column name.
it is used to create a column in the results, with NULL values in the rows `AS [...]` gives a name to this column -- remember, the result of a query is a table of rows and columns, so each column needs a name it is often encountered in UNION queries SELECT foo , NULL AS bar FROM footable UNION ALL SELECT NULL , bar FROM bartable
I just took the exam a second time after goi g into further detail like you said and I passed! What a weight off my shoulders. You're completely right about the general topics they touch base on.
I mean you can: select null as potato from dual; And you'll get a column titled "potato" with a null value selected as the row. It's hard to answer your question, but perhaps what they are really asking is "What is a valid use case for selecting nothing?"
There are many unions used in this query! This makes sense. Thank you for your detailed response!
Thank you for your response! Yes, after understanding this a bit more, I suppose that is what I'm questioning here. It's a very large query and it's interesting that they want to select nothing to start it off? I'm sure I will figure out what their reasoning is as I learn more. I'm sure because I'm new at this I'm just not seeing it at the moment.
Thank you!
I think it is really important to differentiate between OLTP and OLAP databases for the purpose of this conversation, and as someone who works in analytics it is really shocking to me how most DBA's seem to think they are the same. Keys really start to lose meaning in OLAP's.
Congratulations! Maybe I'll ask you some questions when I get ready to take my SQL exams!
Thank you, I'll gladly help you with anything I can!
Certs are important to contracting firms as they essentially pedal employee certs as expertise. Corporations looking for SQL expertise are much less interested in certs and more interested in what you can do for them based on what you‚Äôve previously done.
People are saying there‚Äôs no way Facebook or others are hashing images to deduplicate them, and while I respect the given reasons, we also have to remember that Facebook needs to check uploaded images against databases of images that are illegal to distribute. These databases uniquely identify the images and image hosting providers check against them. I can‚Äôt say with 100% certainty that it‚Äôs by hash but I‚Äôm hard-pressed to think of how else it could be.
I don't know what these illegal images you're referring to are but the set of illegal images would be many orders of magnitude smaller than the set of images uploaded to Facebook
Child pornography, for example. In any case, it would still mean processing all uploaded images to get the kind of unique identifiers required to check against those databases.
Good to know!
I suspect they‚Äôre not checking at upload, for the reasons others have mentioned, but i agree they still dedupe - just as a separate process after the fact.
Simply creating a column with null values in every row. There are many use cases here. A more useful (maybe) application of this concept would be ‚Äúhardcoding‚Äù a column for your table. Say you want to have a new column called ‚Äòwebsite‚Äô in your output that is populated with the word ‚Äúreddit‚Äù on each row: SELECT *, ‚Äúreddit‚Äù as website FROM table Would return your table with a new column ‚Äòwebsite‚Äô, filled with ‚Äúreddit‚Äù in every cell.
You dont have to do a full search across all hashes if you take a sample from a single point of each image uploaded and cut the search by that sample. Hell, 3 pixels 1 from each corner i would imagine cut the search size size by several orders of magnitude given the extremely large number of colors available in modern image formats.
so you have two easy options... option one is to use SSIS - grab the max ID or timestamp from your on-prem copy and store it in a variable... then pull from the source (cloud) anything with an ID/timestamp larger than the variable... append it to the on-prem table (ideally into a stage table first, but depends on your comfort)... they can run in parallel or in sequence, and you can scheduled it to run as often as you like... cloud may be slow, but you can run the process frequently enough that the deltas are still small and the overall process fast enough. option two is to create a linked server, which is basically a database that links from your on-prem to an external server (cloud in your case)... then use stored procedures to do the same thing as SSIS - grab the max ID/timestamp, query from the linked database for id/timestamp larger than the variable, append to table. either process is then scheduled within the SQL Agent... failures can send emails (cell number @ ISP for sending SMS).
&gt;and am an advanced excel user You my friend are in for a treat: in this day and age an advanced excel user with a bit of SQL developer is basically a ready-made BI developer. &gt;but just like: &gt;Select [product] [ID] If there is only one column named product in all of the tables with the name 'product' or 'id', then that would work fine - there's only one of those so you don't need to tell it which table to go to. But if you had two tables that both had a column named id, you'd get an error. That's when you *must* specify the name of the table - or else the db doesn't know which one you mean. To make code easier to write, you can add an alias when you specify the table: SELECT s.id, p.id FROM sales s INNER JOIN product p on p.id = s.productID I aliased the tables as s and p, so I didn't have to write out their full names in the SELECT part and could just refer to them as s and p instead. Out of curiosity, when you say you're an excel expert, does this include power pivot and power query? Because if so, I have some *fantastic* news for you.
Yes, you can have expressions using as many columns and arithmetic operators as you want, and they don't even have to come from just one table: &amp;#x200B; \&gt;select t1.\*, t2.\*, (t1.c1 + t1.c2) \* (t1.c1 + t2.c1 + t2.c2) from t1, t2; C1| C2| C1| C2|(T1.C1 + T1.C2) \* (T 1| 1| 2| 3| 12 Query 9 1 row ---- 0:00.0 0:00.0 0:00.0 \&gt;
Yes, but considering you're showing an example... Go ahead a try it! &amp;#x200B; Create a sample database an go nuts!
&gt; saving tables What do you mean? All CREATE/DROP/ALTER commands are persisted directly when you execute them. SQL Server uses implicit transaction start and commit so if you do INSERT/ALTER/DELETE they will be persisted too when you execute them (without starting a transaction).
One thing to be wary of with things like this is data types. If your columns are different types, they won't always be auto cast to the type you'd naturally expect, so you could get answers that seemed wrong. Things like doing 2 / 3 ending up with zero because the types are integers and the result type is also an integer that gets truncated down to zero. If you want to make sure you get what you expect to, get in the habit of explicitly casting terms into compatible types yourself.
and since there is a division in there it could lead to division by zero, which doesn't work.
Output the substring to a temporary table first then join on that, saves so much confusion!
Ah nice, I wanted to avoid that. But good as a hint
Maybe the substring has some whitespace you're not accounting for? Does the join error or just return nothing?
Return nothing üòÇ
I'd check what it's actually outputting then as it might just be a slight mismatch in the data that you're not seeing. Paste the substring output into a notepad and see if you're not seeing something.
It matches perfectly
It just doesn't work in join on equal... May be it's return not as string...
Can you share your code? Might be the join is written wrong?
I am not sure what you're doing with the line count, but to create your dates, I would suggest using a recursive CTE, something like WITH d AS ( SELECT CAST(CAST(RIGHT, [YearMonth], 2) AS VARCHAR) + '/01/' + CAST(LEFT([YearMonth], 4) AS VACHAR) AS DATE ) AS FirstMonthDay, DATEADD(DAY, - 1, DATEADD(MONTH, 1, CAST(CAST(RIGHT, [YearMonth], 2) AS VARCHAR) + '/01/' + CAST(LEFT([YearMonth], 4) AS VACHAR) AS DATE) ) ) A LastMonthDay FROM MyTable UNION ALL SELECT DATEADD(DAY, 1, FirstMonthDay), LastMonthDay FROM d WHERE FirstMonthDay &lt; LastMonthDay ) SELECT * FROM d
It's one from right. I took some stuff out select g.cn from table1 g join Table2 s on s.HostName = SUBSTRING(g.cn,DATALENGTH(g.cn) - DATALENGTH(s.HostName)+1,DATALENGTH(g.cn)) where s.HostName = 'test-int-doc1' select SUBSTRING('FW-ADM-test-int-doc1',DATALENGTH('FW-ADM-test-int-doc1') - DATALENGTH('test-int-doc1')+1,DATALENGTH('FW-ADM-test-int-doc1'))
&gt; division by zero, which doesn't work. You just aren't trying hard enough.
If it matched perfectly it would work. There's nothing in the language preventing you from doing what you want.
Yep.. That's what's making me sad.. It doesn't work hahaha
select * from table1, table2 where table1.x = table2.substring(...)
Interesting a cross join will try
Won't work either
What news would that be? FYI great post.
what's the db and the error message
It's empty like he finds nothing
most likely your substring syntax is wrong
I thought that too, but if I just change to strings, instead of columns, it works
This is an implicit join, not a cross join. Poor form from a readability standpoint and shouldn't be used. Even in this case, it's not going to make a difference.
Any reason you're using datalength instead of len?
Tried both, didn't work
From a performance standpoint, you want to avoid this type of query if at all possible. Unless you can further constrain rows in Table2 outside of the substring, you're going to force a table scan on Table2. This is because the only way for the database engine to know if a row matches is to look at and perform the substring on each and every row.
What rdbms are you using? In Oracle 11g/12c, I am forced to do joins on substrings all the time, even resorting to complex string extractions such as regexp_like(). It works fine every time. I was always under the impression that a join on a substring is an ANSI compliant thing to do. Can you select a substring as a test case and then manually search for that result in other table you're joining to? I don't see why it wouldn't work unless the substrings are different. And "different" in extreme cases might mean that visually they look the same but maybe something has a different character code. Sort of like how smart quotes can look like dumb quotes, or greek question marks look like semicolons. I'd do a character-level compare just to be sure.
Yep.. You might be right. It feels like it converts badly It is ms sql.
Without seeing the data, it's pretty tough to see where the problem is. Not sure you're going to get a lot of help as what you're asking to do is possible and the method you're describing should work. I can basically guarantee you're missing something and the output of the substring is somehow not matching the field in your other table. You can try this and see if it works, it definitely should: WITH foo AS ( SELECT val FROM (VALUES ('abcd') , ('bc') ) x (val) ) SELECT * FROM foo f INNER JOIN foo f2 ON f.val = SUBSTRING(f2.val, 2, 2) ;
Not with that attitude...
What version of SQL server are you on? We just recreated your function and got no errors at all. Maybe rewrite it again?
Also, here is your exact use case working for me on SQL Server. Can confirm len/datalength both work. WITH f AS ( SELECT 'FW-ADM-test-int-doc1' AS val ) , b AS ( SELECT 'test-int-doc1' AS val ) SELECT * FROM f INNER JOIN b ON b.val=SUBSTRING(f.val, LEN(f.val)-LEN(b.val)+1, LEN(f.val)) ;
Alright, there are two things connected to this topic, equally relevant. &amp;#x200B; Frist, you have a clustered index, which is quite important for performance. Unless specified, the primary key usually defaults to be used as the clustered index. However, those are two completly different things. The clustered index does not have to be unique (it should, but it does not have to, since SQL Server will append a numeric to make it unique if need be). The clustered index is the primary index of your table, you will have nonclustered index seeks and scans, that do lookups on your clustered index, and its also your physical sorting order of your table. The clustered index should therefor be , narrow and growing monotonously. This is why you always hear that guids are a bad primary key, many people don't know, and random guids will create a LOT of fragmentation on your table, while also being rather large at 16byte. For example when you collect logging data, timestamps can be a next to perfect clustered index, since you usually filter for time ranges, while they are bad primary keys on their own. &amp;#x200B; Now, what should your primary key be then you might ask. The primary key should be your business key, the thing that makes your row unique. If that is a combination of a guid and a numeric, so be it, compound primary key it is. Creating a "control" table, to generate artificial keys is .... lets say an intresting proposition, that your data architect would have a VERY long discussion over with me. If he is talking about not wanting to join over a compound key, just create an identity column and foreign key to that if need be. &amp;#x200B; For fun, you can ask him how he proposes to get around the insert bottleneck, going trough a function wrapped in a stored procedure, in order to write to a useless table, in order to generate an identity value, that you can easily just add as a colum to the original table.... My head hurts, i'm gonna stop thinking this trough now.
I appreciate the feedback. I see how this would work on my payments table. Each payment simply appends a new record to the table. How would this work with (in my case) our inventory table? The inventory table only adds new records when we receive new inventory to sell, but between service, payments, transfers, and depreciation, many (but not nearly all) of our old records get updated relatively frequently. The ID doesn't change, and I don't see a timestamp of any kind in any of the fields. Most of the tables I want to pull from are in a similar position. This was the reason I was having so much trouble trying to figure out a solution. The only thing I could think of was to do a full table pull and update. That certainly doesn't feel very efficient to me, considering that most records in each table will not change regularly. I guess that I will need to see if my Point-Of-Sale company can alter the tables to include a timestamp on the tables that are missing it, but I don't feel hopeful.
This is the kind of high-level answer that helps me learn. Thank you for taking your time. Regarding your last paragraph, I think he would be open to a proposal of adding a column to the original table instead if I can word it succinctly. He's an interruptor, a bad listener besides, thinks he knows best (often does, though), and has poor English skills. I'll try, though. As I told another commentor, this has been deferred a week-ish. Wish me luck!
Depends on the database. A substring function is deterministic, so we could create a function-based index (Oracle) or expression index (Postgres) and avoid scanning every row. Another possibility is a computed column and building an index on that (the SQL Server strategy).
I have tried rewriting it I even wrote it as just as "RETURN DATEDIFF(day, getdate(), @datum)" and it still doesn't work I'm losing my mind. Version of SQL server is 10.0.1600
Gotcha. As far as the 10.0...That‚Äôs SSMS. Can you run ‚ÄúSELECT @@Version‚Äù ?
Also, can you try to just PRINT CAST(@daycount AS VARCHAR)
I think you have to declare you variable as Output
I think you need declare @daycount [int] = null So it's already declared before you try to set it
Works fine for me. Are you sure you don't have a line highlighted when you're pressing the execute button?
Try changing SET @daycount = to SELECT @daycoint =
&gt;SELECT @@Version Yeah that gives me "Microsoft SQL Server 2008 (RTM) - 10.0.1600.22 (Intel X86) Jul 9 2008 14:43:34 Copyright (c) 1988-2008 Microsoft Corporation Express Edition on Windows NT 6.1 &lt;X64&gt; (Build 7600: ) (WOW64) (VM) "
&gt;PRINT CAST(@daycount AS VARCHAR) If I try to execute that it gives me the same error.
Yes, 100% no line has a red highlight under it, when I run the whole code it says it executed succesfully, when I run the return statement it gives me the error
Still giving me the Must declare the scalar variable "@daycount". error
Still gave me Must declare the scalar variable "@daycount" error..
how to do that?
Just checked in one of my 2008 servers. Runs fine as well. Possibly an SSMS bug. Try upgrading SSMS as well.
I am on 17.9 SSMS because 18 had removed the diagram viewer feature. Do you recommend any other version? Will a reinstall help?
MacOS has Azure Data Studio by Microsoft. One recommendation I have it just to RETURN the value itself without declaring the variable. RETURN DATEDIFF(Hour...etc)
&gt;when I run the return statement it gives me the error Do you mean you're highlighting the return statement and then pressing execute? You can't do that. When something is highlighted/selected it only runs that section of code, so you'll have to highlight the DECLARE statement too
Just stick OUTPUT at end of declaration
Wait a sec op. Am I reading this right? This is only failing when you try to select RETURN by itself? If so you‚Äôre invoking the function wrong. SELECT functionName(DateToCheck) Is how you would call the function.
MySQL is gonna puke all over da floor on those square brackets also, MySQL does not support input dates formatted as mm/dd/yyyy also, the MySQL DATEADD function doesn't exist
Great, thanks
Try putting the variable before your begin? Just an idea.
The connection string has to specify the mirror. See https://www.connectionstrings.com/sqlconnection/database-mirroring/
I think it may need some error catching then, I reckon if you put in a catch which returns null it may well work.
Is possible that you have a hidden non-visible character somewhere in there. I've had the same thing happen..random voodoo. If you select the individual lines, delete them, then retype them it may fix it.
so if they don't give you an ID/timestamp, then you're forced to do a full sync each time... call them, tell them they're being stupid and that it's only hurting their own interests. if they provide an ID (usually product tables include a product ID that increments with each new product), use that... you may still need occasional full syncs, for example to grab changes to text... but that can be a weekly / monthly effort. on your on-prem side... best practice is to load a stage table before going to the prod table... basic thought is that you may need to do things like validate data, or in your case wait for it all to load, before you update the final table (which is hopefully very fast).
Honestly, I wouldn't bother with that instance. 2008 goes EOL in 5 weeks, and RTM was dead a few years ago. You shouldn't be doing **anything** on 2008 RTM right now. You should have migrated off it a very long time ago. Try on a current/supported release.
you cannot have ANY of the TEXT selected (like the "must declare the scalar variable" error message).
Thank you man, but I need a solution without c# code transformation :/
Did you try declaring it before "BEGIN"?
Is your data source hard-coded? That's going to take some serious smoke-and-mirrors. E.g. DNS.
The source is programmed by me, but I have a lot of user applications and it's not very elegant to catch every program and change the connection string with failover partner. I just wanted to know if exists any good solution with SQL, but I think that I don't have another choice.
Best practice is to move the connection string to an external config file.
Shouldn't there be a semi-colon at the end of the RETURN line?
Databases can have their own collation. You can even change column collation
You should be able to set the collation on a per database level, so you can stick with your default and make the vendor DB whatever they‚Äôd like.
Hahaha yeah, I have this idea too and maybe I will practice by tomorrow.
Thanks! I do see the option when a new database is created now.
 DECLARE @daycount INT;
Additionally if you want to test the function as a whole after you create just use a select statement and call the function in a different query window. Something like ‚ÄúSELECT * FROM FunctionName‚Äù.
https://www.codeproject.com/Tips/416198/How-to-get-Connection-String-from-App-Config-in-Cs Some folks prefer json https://stackoverflow.com/questions/46441075/how-to-place-a-connection-string-to-a-json-configuration-file-using-net-core-co
Something like this. SELECT TABLE1.{fields}, AliasTABLE2.{fields}, FROM TABLE1 JOIN (SELECT SUBSTRINGKEY as KEY, FIELDS FROM TABLE2) AS AliasTABLE2 ON TABLE1.Key = TABLE2.Key This works good as an occasional query with out a need for high performance. To get a high performance query you would need to make some schema and index changes.
I thought the same thing
I believe that's back in 18.1 which is available now.
Great, will try Monday. Thanks a lot
Also, see what the restraining forces are to have more bandwidth available. Either upgrade or get your subnet a higher allocation of the pipe.
Reading the comments you did create the function but you don't know how to call it. If you run the entire code twice you should get a already excists error.
They returned the diagram viewer in the latest release, V18.1.
Agreed! Http://SqlServerUpdates.com for those who aren‚Äôt aware.
You can just select the function name itself SELECT [FunctionName](inputPram)
&gt; Yeah that gives me "Microsoft SQL Server 2008 (RTM) - 10.0.1600.22 &amp;#x200B; Problem found.
the easiest way to debug something like this is to add one piece at a time. &gt; there are future dates first step is to test this. just run select * from t_ticket_history where perf_dt &gt; current_date limit 10. What data type is perf_dt? you cast it in your select but not in your where.
I stripped it down and tried what you said (let's put aside the cast function for now). When I set the date for anything before today I get dates up until today. But if I ask it to give me dates after today, nada. Below is the stripped down query. &gt; select * from t_ticket_history &gt; where perf_dt &gt; '2019-06-14'
Alright then, if there are no performance dates in the future, then you have your answer, right? Should there be performance dates in the future? Does whatever builds t_ticket_history filter those out? Or are you saying that you always want the function return rows with the dates between startdate and enddate, regardless of whether there are any performances on those dates? In that case, you need to create a calendar table that has all dates and left join t_ticket_history from it. You can probably google "calendar table sql" and get ready-made code for that.
No, there are definitely performance dates in the future. So I‚Äôm just looking to return all records that have purchased a ticket for a future date. Thanks for the thoughts, but I guess I‚Äôll play around with it and see if there is some other way around this issue.
What I always do in a case like this is start a SQL Profiler session and capture the SQL that is being sent by SSRS. Copy it and paste it into SSMS and run it and see if it gives you want you think. If it does, and your report still doesn‚Äôt, check your SSRS tablix to be sure you don‚Äôt have a filter on it, or your SSRS dataset.
Dont sell yourself short, i didnt give a high level answer, I was giving a "let me try to make it understandable" low level answer. I was not kidding when i wrote... Most people do not know that a primary key and a clustered indey are seperate things, that disregarding sone default settings, have nothing to do with each other....
So, you have future dates in the table, but, when you run the select statement you are returning zero results? This is kind of contradictory. You mentioned the cast function; what if you try a &gt; getdate() versus using the cast?
Thanks for the assistance but see the edit.
Create a view following your needs. 2 tables can be linked together using JOIN. For sure, linked servers / db links are slower. Users should never acces dbs directly but through apps front ends and web apis.
hah; those heckin' darn DBA's. XD
alter table &gt; add a new column for a computed value for the substring &gt; join on it ??? profit
What is your reasoning for two databases?
Security i guess.
Because you can go direct from Excel power pivot/power query into Power BI and literally be a BI dev with almost no extra learnings needed.
HackerRank has been good for teaching SQL basics. Good luck!
sqlbolt.com is a good start.
Learn Manipulations, Aggregate Functions, Queries and Subqueries. Understand the relational model, normalization and syntax.
No, sadly that won't work.
Sqlzoo.net for some extra practice.
I signed up for a class online. I‚Äôm making a note of everything you suggested to make sure I learn them and understand them. Thanks!
Typically I'll only do one or two JOINs (so 2-3 tables in total) in a single query. When you get to larger queries, break them up into layered VIEWs. It's much better for easier debugging and re-usability.
Understanding those subqueries will be a big win
I;ve heard good things about UDemy but haven't tried it myself. [W3 Schools](https://www.w3schools.com/sql/default.asp) is good for a start/reference, but don't rely on it (like I did). It's simple, but because it's simplified it won't teach some of the larger concepts. LinkedIn Learning has some good starter courses too...first month is free. Learn your syntax and when you get into a job, read the documentation. There's a lot of little variations between statements and syntax; some 'dialects' of SQL don't have the keywords that others have, and it can be confusing moving between them. Download the free [SQL Developer](https://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html) or something similar and start coding freehand instead of relying on an application. It'll be harder at first, but the concepts of how SQL works will become more clear. You can download databases across the web to test things. Download a test database and some software and just play with it. Pay attention to the errors (which aren't always clear). 9 times out of 10 an error is due t your syntax or a typo. SQL isn't a difficult language to learn but it takes focus.
Thanks for the advice. I‚Äôll for sure keep this info and hopefully it makes more sense after the course I signed up for. Much appreciated
Thanks for the advice. I definitely want to practice and not just listen to a course. Hands on works best for me
Creating a VIEW is the best option in this case and the reason behind that is that it comes with a lot of benefits and the major one is Security because you can give a User access to a specific database and thus, will be easier to manage. It will also help in Data Integrity that will ultimately lead to better performance.
This Database is one of the easy-to-learn ones and will help you in sharpening your SQL skills. Go to this website: [https://www.zentut.com/sql-tutorial/sql-sample-database/](https://www.zentut.com/sql-tutorial/sql-sample-database/) and you will find the database that you can download at the bottom of the page
It's the best way to learn. It's easy to understand the concepts when you're watching a video, but it doesn't always translate in practise or when you're coming up with your own examples...in my experience anyway. It wasn't till I started fiddling around that I really started to 'get' it.
I liked your idea of learning it through a CC but my question is........ what is the purpose? If the purpose is to show that you have a certificate (just like a degree), then go for it. But if you want to learn and a certificate is not necessary, then I would advise you a number of different things: 1. Go to [lynda.com](https://lynda.com) and pick a course that you like (it is paid). 2. If you want a free solution, then my personal recommendation would be to create an account on HackerRank and do the SQL practice there. Let's say that you can't figure out the answer, go to google and search for that concept and learn from websites like W3Schools. 3. The third way is the traditional way to learn anything: buy a book. I would not advise it because I have found learning online or from an ebook much better than learning from a physical book and it saves so much time. If you still are interested, check some books here: [https://www.lifewire.com/best-sql-books-4177471](https://www.lifewire.com/best-sql-books-4177471)
1. Go to [lynda.com](https://lynda.com/) and pick a course that you like (it is paid). 2. If you want a free solution, then my personal recommendation would be to create an account on HackerRank and do the SQL practice there. Let's say that you can't figure out the answer, go to google and search for that concept and learn from websites like W3Schools. 3. The third way is the traditional way to learn anything: buy a book. I would not advise it because I have found learning online or from an ebook much better than learning from a physical book and it saves so much time. If you still are interested, check some books here: [https://www.lifewire.com/best-sql-books-4177471](https://www.lifewire.com/best-sql-books-4177471)
you might want to look into dynamic sql
I'm trying to make a Admin Panel where I have a table for each game and I can't come up with an idea how to handle this. If it was possible to bind the table name I would just bind it like that but yeah, I really have no idea how to do that. It's hard to think of an Admin Panel I guess I might just sit for hours prototyping the AdminPanel in InVision or something.
Are you inferring that FB doesn't have the algorithms and learning models to analyze and identify images at all? Because I've watched Facebook suggest an entire friends list of a previous profile to a person once they uploaded a photo from the old profile to the new profile. This was after their "requested data deletion" that took two weeks.
You have to be careful with this approach - nested views sometimes have issues pushing predicates down, and it can really affect performance. I agree that putting as much code on the database as possible so the application isn't doing work is a good approach, but views should just be viewed as one tool among many. You also have derived tables to materialize row sets that you know you'll need, and dynamic SQL procedures when performance is critical, or the potential queries need many different parameter configurations.
Which database product are you using? The concept of a "database" is something completely different in Oracle, SQL Server and Postgres
Could you elaborate a little more on what data your tables will hold?
Here's my schema so far, a marketplace is the goal. https://drive.google.com/file/d/1te_NUEc10A30X5id2n9wG_g7at6sluXj/view?usp=sharing
What RDBMS?
PostgreSQL
For ANSI compatible SQL syntax: yes For Microsoft T-SQL, it doesn't care.
&gt; * create an index for the product url Yes, though I'd store just the "product-name" part, not the whole url, and I usually call that a slug, maybe nameSlug or urlSlug. &gt; * use the product url as the primary key No &gt; * use a separate table with colums (product_id, url) and use the url as the primary key there No. Well, maybe. If your slugs are going to change and you want to be able to find a product via an old slug, you could do along these lines - but url would not be the primary key. This table would have 1...many records for each 1 in your product table, and you'd presumably use a surrogate key.
&gt;Yes, though I'd store just the "product-name" part, not the whole url, and I usually call that a slug, maybe nameSlug or urlSlug. yeah this is what I meant by product url, I don't plan on storing the full url structure in case I want to change it later, also it's just redundant data. &amp;#x200B; &gt;No. Well, maybe. If your slugs are going to change and you want to be able to find a product via an old slug, you could do along these lines - but url would not be the primary key. This table would have 1...many records for each 1 in your product table, and you'd presumably use a surrogate key. I definitely want to give vendors the ability to change the url/slug of their products which is why I'm reluctant to just add an index (is there a difference when updating an indexed column?). But I'm not concerned with supporting past handles (I'll probably want to create a permanent redirect somehow, haven't figured this part out yet, maybe I will need the old urls after all).
Something about helping learning your lessons stick in general: First, follow the guides, the tutorials, whatever you need. They're good but the lessons might not glue into your head. To make that happen, now apply those guides and lessons to a dataset that you know and care about. When I first started learning SQL and was fascinated by it all, I made a little database about a game I liked. So I'd have a table with characters, with items, crafting ingredients, spells, all that fun. For example I after I learned how to normalise data, I then made a table for `ingredients`, and another table with `items`, and another table for `item_ingredients` which showed me how many to many relationships work. Then I had some items requiring other items as an ingredient, so I refactored it again and just considered `items` and `ingredients` as one entity, placed them all in a table of items. Later I discovered recursive queries and figured out how to go from an item down to all sub ingredients required from scratch (eg: A a suit of armor requires head/body/arms/legs/boots, and each one of those was made by a sheet of metal and some assorted crystals). Your interest might be cooking, sports, the Marvel Comic Universe, but once you mix your hobbies with SQL a lot of the lessons stick. After that learning curve, you won't need to do this anymore.
Really bad...
This advice is probably the best. If you use Code Academy they‚Äôre going to give you tables you couldn‚Äôt give 2 craps about. Like Airports and Flights. The lessons will seem so much harder when you don‚Äôt care. Since the Output won‚Äôt give you that dopamine hit when you get it right. ‚ÄúWow the average time of a layover across all airports with no direct flights^yaaay...‚Äù
You could try Steven Feuerstein's book, which many consider to be the PL/SQL bible: [https://www.amazon.com/Oracle-PL-SQL-Programming-Versions/dp/1449324452](https://www.amazon.com/Oracle-PL-SQL-Programming-Versions/dp/1449324452)
TSQL Fundamentals is a great book
This article seems confused between the transaction log, the SQL Server application logs, and audits, which doesn't inspire confidence.
I have to say my spidey sense tingles a bit at the requirement for dynamic table name, but you could use dynamic sql. That just means when you construct your sql string you concatenate the table name. So if you were constructing your query in Java for example you might do something like: `String tableName = "SOME_TABLE";` `String myQuery = "SELECT * FROM " + tableName + " WHERE name = :name".`
Oracle has a devGym. Filled with videos and courses at your own pace https://devgym.oracle.com/ They also have a full 18C Db environment that lives in the cloud. You can use this to practice whatever you want Livesql.oracle.com
Oracle has a devGym. Filled with videos and courses at your own pace https://devgym.oracle.com/ They also have a full 18C Db environment that lives in the cloud. You can use this to practice whatever you want https://Livesql.oracle.com
&gt; you can use this to practice In Soviet Russia, practice can use this to **you**! ^(this post was made by a highly intelligent bot using the advanced yakov-smirnoff algorithm... okay, thats not a real algorithm. learn more on my profile.)
Thank you!
Nice......
Anything by Steve Freuerstein. He's got a few books out and courses on Udemy, etc.
&gt; sometimes have issues pushing predicates down Ah ok, I'm not really sure how to parse this? I've never done much benchmarking, but from what I read a decent engine like mssql or postgres will basically treat the same query many joins in one query as those same ones from views in the same way? But I guess there's a lot of variables here? I'm not sure where or how to quantify them though? I just figure that as long as you treat the whole chain of queries the same way, they'll end up the same way regardless of being broken into views? i.e. Don't do shit like grouping or filtering too early. Now that I've written this I guess this maybe is what you meant by predicates?
This is true, and as someone relatively new to /r/sql I‚Äôm finding myself asking ‚ÄúWhich database?‚Äù repeatedly. I should probably refine my question and ask which product as you did, or which DBMS. They‚Äôre all different, in some cases very different. In the case of this question, MySQL has its own definition of database as well - it‚Äôs synonymous with schema for some reason.
I‚Äôm currently reading these two, Practical SQL: A Beginner's Guide to Storytelling with Data https://www.amazon.com/dp/B07197G78H/ref=cm_sw_r_cp_tai_T9rbDbCTRR2RE and T-SQL Fundamentals (3rd Edition) https://www.amazon.com/dp/150930200X/ref=cm_sw_r_cp_tai_f.rbDb5AKCMTC. Of the two I‚Äôd say the first is a bit easier to take in.
if you have decent excel skills you'll be fine. the best advice i can give is find a decently large and varied data set that you work with, and load it to a database engine, and always try to use the functions you learn with data that means something to you, not just the samples you get with any class. i also highly recommend w3schools, it's the cleanest, simplest guide for sql.
Try something like SELECT * FROM table_name WHERE CASE WHEN flag_value IN ('Y', 'N') THEN D ELSE 1 END = CASE WHEN flag_value IN ('Y', 'N') THEN flag_value ELSE 1 END &amp;#x200B; If flag\_value is Y/N, this query will become equivalent to: SELECT * FROM table_name WHERE D = flag_value If flag\_value is blank, this query will become equivalent to: SELECT * FROM table_name WHERE 1 = 1 Meaning it will pull all records.
Thank you
I wrote this guide up for the 1Z0-071 exam 2 years ago: &amp;#x200B; [http://nathandav.is/index.php/overview-preparing-for-the-oracle-database-sql-1z0-071-exam-with-no-prior-sql-experience/](http://nathandav.is/index.php/overview-preparing-for-the-oracle-database-sql-1z0-071-exam-with-no-prior-sql-experience/)
Bad bot
Thank you! It is a career pivot as you stated so need the certifications
WiseOwl SQL on youtube. Its free.
I think it's an excellent book. I purchased three to use in teaching several staff SQL.
Homework?
If you do not know SQL, this SQL bootcamp will teach you everything you need to know at least in interview questions. It will be nice if you work along the course, i.e. download the pgSQL and work out the numerous queries the instructor writes and executes. Also there are plenty of SQL exercises out in the market, so I guess it will not be hard to prepare more. Please realize the logics and heuristics behind each function in sql. That's the tip, cause in real life, you will be using them again and again to write significantly long queries in your company servers to extract data. Wish you luck!!
Sort of, I‚Äôm a business student trying to teach myself sql for an internship.
Select year from movie m inner join rating r on m.mid = r.mid where rating in (4, 5)
select year from Movie inner join Rating on Movie.mID=Rating.rID where rating in (4,5) order ASC by year;
 select distinct m.year from Movie m inner join Rating r on m.mID = r.mID where r.stars in (4, 5);
I would look into puralsight or datacamp, both have great courses and issue certificates upon completion which can really help you get your first job.
Right. Predicates are things like WHERE conditions. And you're correct that in theory, it'll end up the same way regardless of being broken in to views. But at some point, the query optimizer will be unable to properly send your filter all the way down. So, lets say you have a view that normally returns 100M rows, but it has an indexed customer ID. If you query that view with a filter on customer ID, it won't make the view return the 100M rows - it will "push the predicate" down to the view. However, lets say you have three views referencing each other. At some point, SQL will say "I can't map this all out - just return everything with the view and go from there", and pull the 100M rows in to memory before using the customer ID field. So just be careful nesting too many views! If you have complexity that you want to keep away from the application, you have other options. :)
&gt; Is there a SQL function or concept that could help me create these two-digit codes with less manual intervention? create a table containing a `CHAR(1)` column, load it with 36 values, then do a self cross join, concatenating the two characters that gives you the same 1296 combos that you outlined -- 100 + 260 + 260 + 676 holler if you need this clarified further as for assigning these 1296 2-character codes, save what you assigned in a table, with the VARCHAR(6) Product Category as the PK so it can't get re-assigned
Could you please provide some additional insight into how it doesn't work?
Which platform are you working with? If Microsoft, I recommend a beginner level book by Itzik Ben-Gan. He is very clear and sets a good example for how code should be written. I think this book will put you ahead of most beginners rather quickly if you stick with it. T-SQL Fundamentals is the name of the one I‚Äôm thinking of. It‚Äôs about 450 pages but if you get through 200-250 and practice on some interesting data sets you should be doing better than a LOT of developers and analysts.
Do you have a recommendation on one of Itzik's books? Amazon lists a couple options but as an amateur, I want to make sure to get the right one to bring me up to speed the most straightforward way possible. Although seeking MCSA level would be the long-term goal, I want to make sure I try to walk before I run so as to not get frustrated or discouraged. I'm seeing the T-SQL fundamentals 3rd ed. and the MCSA exam reference set that includes Developing SQL Databases and Quereying Data with Transact-SQL.
T-SQL fundamentals is what I recommend. See if you can find the PDF copy floating around and read a few pages - if you like it, buy it, if you don‚Äôt, try another. Don‚Äôt expect any resource to be too easy, but they shouldn‚Äôt be too hard for you either. Advice with my manager hat on: What I‚Äôve noticed is a lot of beginners try to use 20 different sources, believing each one is better than what they have. Just stick with one and work through it diligently over a few months. Good luck! And PM if you have any struggles or need any guidance; I‚Äôd be happy to help you once you get started.
I remember liking the SQL course on codeacademy quote a lot.
Udemy and look for Database and SQL Querying.
You could practice connecting to api‚Äôs (maybe through SSIS if you‚Äôre learning T-SQL) to get good at connecting to external data sources, parsing, and cleansing data into databases. Then if you find some extensive data, you could practice UML design and creating tables with healthy PK‚Äôs etc. If you‚Äôre into data science, you could take the DB‚Äôs you created and connect those to python scripts or whatever other software to do analysis on the data. This stuff looks pretty good on resumes if that‚Äôs what you‚Äôre going for. Sorry for the essay.
That certificate is easy to grab because it's free though Congrats. Can anyone tell me free course where i can earn certificate too without paying? I want to learn advanced sql/plsql. Thanks.
Udemy...or....install SSMS and get some data to play with. I learned on the fly years ago with basic SQL knowledge by having to figure out work scenarios and also playing with data in my off time.
I don't know much about pivot but i believe that pivot gives you result in horizontal if result is in vertical and vice versa as per your will/requirement. Try to execute the inner query first and see.
Thanks everyone! I appreciate all the input!
Wrong on primary key and foreign key. A primary is a unique object used to define the table. A foreign key is an object in another table which reference the primary key.
So let's say you have a Person's table which has the first and last name (in columns) of several people. Some people may have the same name, so to make them unique, you create a third column called ID and in that column goes their social security number (it's unique, right? Note, don't ever use the SS# as a PK)
Each person has an address,right? Some people live together with the same address too, so to create a relationship, you add a Foreign Key column in an address table. That FK column directly related to the PK of the Person's table. So if you look up an address, you can easily see what person lives there.
Add a third table called Salary. Create a column for the salary and another for the Person's ID which is a foreign key. This related the Salary table to the Person. You can do a keep building relationships, think of organizational charts, balance trees or factoring. Each factor line (or edge) is a relationship.
I'm just glad to see other sololearn users on here! Congrats
You're joining on Initial item and PreviousValue to InitialItem, I'd guess that's why you're getting 0 rows. What database are you using? If this is SQL Server, just select the Previous value column in the CTE, remove any of your joins, and just do Update S Set S.PreviousValue = s.PrevValue.
Go to sqlzoo and sqlbolt and give them a try. They're much harder.
God of PL/SQL right there. Met him once at OpenWorld. Truly nice guy.
T-SQL Fundamentals is one of the best books on SQL/T-SQL available. You will have a very solid foundation in SQL once you complete going through the entire book.
T-SQL Fundamentals is one of the best books on SQL/T-SQL available. You will have a very solid foundation in SQL once you complete going through the entire book.
T-SQL Fundamentals is one of the best books on SQL/T-SQL available. You will have a very solid foundation in SQL once you complete going through the entire book.
Thank you! Not sure how many lunch breaks it took me, but I made it through the end.
YouTube has some decent tutorials
Perfect thanks!
Yes, build something. Addressing problems as they arise and identifying solutions yourself, on stackoverflow, or using your existing resources‚Äîthat‚Äôs the best way to learn. Remember there are multiple solutions for most any problem.
In the body parameter or are you looking for query parameter?
What's up? Can you post the question?
I tried cross-posting! Did it not work :(! Here's what I wrote. Hello! I have a report due in 4 days at work (thanks for letting me know management) via Cognos Analytics. I'm completely self-taught as nobody in my department knows it. I've taken a few basic courses on SQL and understand enough to get by for now! Anyway, I'm working from a Framework package that our BI department is letting me use. The goal of the project is to determine how many days we are away from a report timing out. My boss wants to load up the report and be able to see this. A time out is considered 30 days from the REPORT DATE It's all coming from one table fortunately and Cognos does a bit of hand-holding in all this. Still, I don't know what syntax to use. I would imagine that my boss loads up the report, somehow Cognos is able to recognize the current date he's accessing it, and then I'm able to generate a text report that shows how many days away we are from timing out. Inquiry Number Report Name Report Date Agent Name 12345 Noob Report 06-15-2019 Jason Any ideas? Thanks!
So if I understand, you want a query that returns you all the reports that are do in 30 days or under?
Sorry, I'm terrible at explaining this. Not necessarily. So if the REPORT DATE is 06-01-2019, it has 30 days before it times out. If my boss accesses the report on 06-06-2019, he would like the report to generate results that let him know hwo many days we have left before the report times out.
Check out the sidebar for resources. Specifically, the Learning SQL section
So if I'm geting this correct if your boss accesses the report the database should say 24 days left until report times out? Also from what does he accesse the report? Like a company software or he looks in the database or like he looks in the document? Sry for the questions but if you only want the first thing maby you could put one more column like days left and have a function or store procedure that updates it and if you want the sql look up the syntax on Google it has to bi something like datediff(par a, par b)
Posted this in the other subreddit but wanted to make sure you saw. Hey. If I understand correctly, you need to do two things here. 1- Create a data item (Report Timeout Date) to calculate the 30 days into the future based on the report date. _add_days([Report Date], 30) 2- Use the Report Timeout Date to create another data item (Days Until Timeout) to compare against the current date _days_between([Report Timeout Date], current_date)
Thank you both for the feedback! I appreciate it.
&gt; Is this possible for MySQL what happened when you tested it? ‚Ñ¢
no, this is wrong on two counts first, if the FK is in the Address table, a single address can reference only one person but the more insidious issue is, why are you setting up a separate table just because multiple people can share an address? **please explain** why this is any different from a scenario where multiple people can share the same first name, e.g. John i've never seen anybody implement a FirstNames table or a LastNames table, but mention two people at the same address and they fall all over themselves trying to avoid a redundancy that is of no importance why is it of no importance? because if it was important, you would take the trouble to store addresses that have no people at them at all
salary is as bad a relationship as address... see my other reply not everything is a relationship, lots of stuff is **best** left as a plain attribute oh, and since salaries are unique, if you were going to do this, you wouldn't need that ID column for uniqueness, would you
Try modelling something you know, e.g. Reddit * Users * Subs * Posts * Comments * Votes Plus some procedures that would provide the type of content you see here.
It's an abstraction to illustrate implementation of PK and FK on tables. It's absurd of course and no one would set up tables this way, but to illustrate PK and FK it's fine as a basic example of relationships. I don't think OP needs a full description of the Cross model.
an "absurd of course" abstraction may not do a newbie any good, and in fact may do harm in completely confusing what should be in a realtionship and what shouldn't not to mention that your examples were bad in the i-to-m versus m-to-m sense
I think that W3 is a great starting point. They have very straightforward and easy to understand overviews of the basic concepts you need to know. They have like a 20 question multiple choice quiz to test your knowledge. When I was starting out I would take it and make note of what I missed, then go and read their overviews of the topics, and take it again the following day. Just Google "W3 SQL"
sqlzoo.net
Download the free MS SQL Express, load in some data, and write the query.
You can practice more on HackerRank and that will help you in gaining more confidence and will also help you learn and master the fundamentals of SQL
Sounds like you've been running in Full recovery and never taking log backups. As a result, log truncation cannot occur, so it just continues to grow. Setting to simple recovery would allow log truncation on checkpoint, provided no open transactions are preventing log reuse. If you need to shrink the file, you may need to do this in multiple operations, performing a checkpoint in the database, running a shrink, checkpoint IMG the database, running a shrink, etc. You definitely shouldn't set the log file to something as tiny as 12mb. You need at least twice as much space as your biggest operation (e.g. an index rebuild) , and preferably plenty of extra room, or your just going to be growing again. Once you've got it under control, if you want to switch back to full, you need to make sure you're taking very regular log backups.
Oh yea I don't plan on shrinking to 12MB. I had meant that I currently can only shrink by 12MB less than what it currently is. Would I be able to switch the recovery mode to Full, perform a backup of the transaction log and shrink from there? It seems like once I have a backup of the transaction log I ought to be able to shrink? I am unsure, but I'll try your method as well, thank you.
Having switched to simple already, the log chain is broken from a log restoration perspective, so you may as well shrink before switching back to full. You'll need to do a full backup after switching back to full recovery in order to be able to start backing up the log again.
Here's an example: https://karaszi.com/large-transaction-log-file
,2) = referring to how many decimals to round to. AS is not required to alias a field name.
What RDBMS?
Hmm ok thank you. Yes looks like our database is stuck in replication so I will not be able to shrink as is. Need to troubleshoot that first, thank you
the "2)" part is finishing off the ROUND function, so you need it. It's telling the system to ROUND the value to 2 decimal places. You can add as "AS" in there after it, prior to *columnname*, but it's not needed--the "AS" is assumed.
Damn, now that you point it out, I see it. Thanks for your help! And to all the others in this thread!
Replication is not needed in our case. As soon as I ran: "sp_removedbreplication 'DB_PROD' go" Our available free space shot up from 12MB to 680GB : O
SQL by Kudvenkatesh on YT Ignore the accent, focus on content.
Start with simple data sets you understand, but if you start getting the hang of it and need some more data to play with: https://www.sqlservercentral.com/articles/public-data-sets
try https://pgexercises.com/
Install Postgres and start playing with it.
Oh
One *wild* option is to delete ldf file and let system rebuild it. Obviously, you should do full backup first. And you should do this only if nobody is using DB at the moment. When you are ready to do it, detach the DB, delete (or just rename) ldf file and then attach mdf file alone to the DB. (There is a sp for attaching only mdf file). When engine is unable to find ldf file it will create a new (small) one. Now, prevent same problem from happening again, by setting regular log backups.
Download the [StackOverflow database](https://www.brentozar.com/archive/2018/06/new-stack-overflow-public-database-available-2018-06/). Free up some space on your HD first - the archive is 38GB and unpacks to a 300+ GB database.
Exactly this. If you're going to be working with data in general it's best to have a sandbox either on your local box or build a cheap box for the sole purpose of practicing.
 declare @body as varchar(XXX) = 'The HELPDESK count from yesterday is: ' + @YesterdayCount + '; ... exec msdb.so.sp_send_dbmail ... @body = @body Side note: Consider creating variables which hold for your datetimes. This will allow you to include them in the @body deceleration as well.
There's a lot of confusion in this post. Ok so first off submitting an http POST request is separate from the subject of SQL. So as long as you pass the parameter through to your POST endpoint that presumably talks to the database you're good so far. &gt;add a negative value to my row You don't add values to rows. I get that you're probably asking about adding a negative integer to an INT column in one particular row, but it always helps to get our language as precise as possible. &gt;The type is set to INT with a length of 50, default none and the rest is empty. Note that the "length" of 50 for an INT in MySQL/MariaDB does not have anything to do with the maximum size you can store in your table. It's just for display purposes. I usually ignore it entirely. I wrote a blog post about this subject a few years ago as a lot of people get it wrong: [http://mwrynn.blogspot.com/2016/11/mysql-numeric-types-dont-let-you.html](http://mwrynn.blogspot.com/2016/11/mysql-numeric-types-dont-let-you.html) &gt;I've read that it shouldn't be set to "unsigned" and it's not. An UNSIGNED INT column basically means you can't store negative values in it. If you want to store a value of say -1, then don't use unsigned. It doesn't exactly have anything to do with whether or not you can add a negative value, though. &gt;insert the negative INT as a string No, don't do this. Always use the right data type, and for integer arithmetic you want integers. &gt;I'm able to add a negative INT thru the phpMyAdmin page but not thru the form I've made. This suggests that whatever you're doing on the database end is probably fine, and you should be looking at your application code.
Look up recursive CTEs
How do I call the function?
Yeah others told me that as well! I am so sorry, I thought I could call a function by firstly executing the whole code and then executing only the return line, don't know why. Thank you!
Good to hear!
Haha no worries. You had us questioning ourselves there for a bit! Glad you made it work eventually!
I believe I am a reasonably advanced level of sub query ability. Not for CTE. Is there is a significant difference between the two for this?
A CTE can be expressed as a subquery except when you do recursive CTEs since you can't expose the exit condition in the subquery. It would just be N subqueries nested.
Ahhh, I see. There is some sort of condition available in the CTE.. that would end the processing.. helps answer the question of not knowing the # of degrees a network may have.
The exit condition being the null result set
I see. Do you have any sort of logic you would be able to type that would help answer this? English is fine. Doesn‚Äôt need to be in code.
https://www.fusionbox.com/blog/detail/graph-algorithms-in-a-database-recursive-ctes-and-topological-sort-with-postgres/620/
Holy moly. That‚Äôs quite the complex outline. sql is so powerful. I often write ~5 pages of code with a number of sub queries, but this is on a whole new level. Seems like it will take quite a bit of time to map out, identify and mark each network within the database. There could be quite a number of different, distinct networks of relationships.
\&gt; 38GB ...lol. thank you, I will check this out.
Thanks for the info! So i got the book, what program/system do you suggest using?
Which DBMS? The best answer could vary greatly depending on that.
WiseOwl SQL on YouTube is a good start
It‚Äôll be created through SAS &amp; then a combo of Netezza &amp; imported CSV
Do you have any idea how to write something for this?
Well I've never used Netezza but according to [here](http://dwgeek.com/netezza-recursive-query-alternative-examples.html/) (a recent article) it does not support recursive CTEs, nor does it appear to have its own custom solution (fun fact: Oracle had "hierarchical queries" that could solve your problem since the 1980s!). The article suggest a really ugly query with a bunch of UNION ALLs. It doesn't seem to scale for an indeterminate number of levels, in other words you need to know ahead of time that you only care about say 5 levels deep. At least I think that's what I'm seeing - it's not easy to follow and I'm sleepy at the moment. :)
Ahhh I gotcha. Thanks for the info. It technically isn‚Äôt netezza, it is PROC SQL via SAS but data stored primarily on netezza servers and then created SAS dat sets
OK I barely know what SAS is but it's possible it has a solution?
Not that I know of. I will research tomorrow. I am gonna probably go back to the stakeholders and express my opinion this is a tad complex &amp; they want to tamper their expectations. Would you say that‚Äôs fair / how would you explain it?
Well I mean you could implement something procedurally but it's not an elegant solution and performance will probably be terrible. However depending on the size of your data set "terrible" might be good enough. I don't know much about PROC SQL but if it's anything like PL/SQL, T-SQL it should be doable.
Fair enough. It is similar. Data is probably 350k rows, two fields.
Thank you for this link!
Ok 350k rows this day and age isn't too bad. Assuming you just want a flat list of all related child IDs, I would do something like (in pseudocode): function get_all_related_child_IDs_for(int root_child_id) get the family_id for root_child_id get all child_ids for family_id create an array/list containing these child_ids, let's call it child_id_list loop over each child_id call get_all_related_child_IDs_for(&lt;current child_id&gt;) and append those results to child_id_list end loop return the array/list end function Warning this isn't tested and as I mentioned before I'm very tired. :) So don't take this as a literal solution - it just gives the idea and may require some tweaking. Also since it is a recursive algorithm it can hit a stack overflow if you go too many levels deep but I'm guessing in practice that won't happen. Just as I'm about to submit this comment I'm realizing this will probably run forever in many cases as the same child\_id will be retrieved and called repeatedly. You may need to pass the function a list of child\_ids already done and only execute the recursive call when the current child\_id is not found in that list. Ah, what fun!
Which DBMS?
After reading this, I'm lost too!
AdventureWorks from Microsoft?
there's a site called HackerRank where you can sort of show off challenges you have completed
Read about sql from free websites like geekforgeeks, w3school, techonnet. That's easy.
Are udemy sources better?
I don't believe so because they are like playing games rather than serious text to read and learn. The sites which i mentioned is very good to actually learn from it. Also, udemy and other similar sites have paid thing which again is nothing more than puzzle games. So to get knowledge of sql, learn from those sites which i referred. And please upvote me because my karma is in negative. üòÜ
Lol okay and thankyou!! How much time do you think it would take to complete?
First read SQL from techonnet site, It will tale you 2-3 days because contents are very clear and not big. Then from other sites like oracle-base etc. Upvote this too. Lol. BDW, I know sql, if you need to ask someone, you can ask me.
Okay I will follow you! And thankyou!!
No worries, you are welcome. Keep upvoting me and let me cross positive mark. Lol
Tho I am curious how did you get -karma?
How did you get -karma?
I also don't know. When i see my profile i have 26 negative karma (which i don't know how it happened). But i read on google where people say you gain karma by 1 upvote. But today i realised that my karma is not increasing no matter how many upvote i am getting. This is frustating.
It will after some time!!
I think im getting positive karma when upvoted. So upvote this once.
I check and that is paid after 14 days trial. Can you give us some free links where we can get some certificates for free?
 [http://sqlfiddle.com/](http://sqlfiddle.com/) \- this may help. You can create and play with schemas on a database flavor of your preference. It doesn't seem to have the latest database versions though.
SoloLearn is really good to learn the fundamentals, I went along with it writing notes in a google doc. It is free and there is a certificate but I don't think it would be too credible in terms of resumes etc.
Thanks I will check it out
Sololearn is fill-up blanks game. It will teach you only 1% of sql. Lol.
Yeah I mean it just teaches fundamental concepts and syntaxes
Great - and best of luck! Once you've got something created, make sure you network heavily on LinkedIn, particularly with application development managers in IT departments, people in technology services companies, data governance professionals, etc. Connect with them and don't be shy about letting them know you're new to the game and interested in opportunities.
Appreciate it my friend! I will be sure to do so
Not even basic. Anyway it follows mysql, not oracle sql. Please upvote my all comments as I'm having negative karma. Lol. Thanks.
Yes that is what it‚Äôs meant for as well. Remember though, you should try to be limiting your index usage as much as possible because there is such thing as too many indexes on a table and will actually decrease performance. Consider including the column in the primary key.
You can enforce uniqueness using constraints, that way you can enforce the business logic without the overhead of maintaining the index, which is preferable if you never use the index as an index. In general though I agree that enforcing business rules within the DB is a very good idea and isn't done nearly as often as it should.
Try creating a small SQL back-end solution for a 'client' or 'business', and then uploading it to GitHub and just making generally complex Views and Stored Procedures, as well as some advanced ad-hoc queries and complete with database diagrams, showing how the tables are setup with data types, calculated fields, identity columns, relationships and indexes etc, even chuck in some T-SQL showing various server and database roles and users. It's unlikely you'll ever need to build a complete solution for an established organisation or even manage all of the database solution in this detail, but it shows you at least know what you're talking about and gives the interviewer some good pointers to ask you about. It's a win-win too, because if the interviewer is a generic staff member, it'll look incredibly complex and they'll assume you know your stuff, and if they're technical it gives them some good examples they can ask you to expand on or explain and make them feel that you've done your homework. Yes of course you could have made it up and Googled it, but that too could work for your entire CV...
Thank you very much I will look into doing something like this!
&gt; It‚Äôs my understanding that the primary purpose of a unique index is to aid in rapid lookups on that column. not quite the primary purpose of an index is to improve querying the primary purpose of a `UNIQUE` constraint is to ensure uniqueness the fact that every database *implements* a `UNIQUE` constraint using a **unique index** is coincidental but the primary purpose of uniqueness is uniqueness p.s. do not forget about the `PRIMARY KEY` for a table, which also provides uniqueness
I couldn't recommend your suggestion more! I am planning to do the same, with documentation included and plans to expand it into having an interface built in Python or C#, or anything I can think of to go with it. As another tip, for different database structure models, you may want to check http://www.databaseanswers.org/data_models/ they have so many it's ridiculous! Also you could try reverse engineering as a practice, just look at a platform like say Amazon, the structure of their platform and think how their databases would be designed!
It seems not a simple assignment, it's a real data project!
Goodness me, what a resource that is!
Are you from India
SQL Server has no ‚Äúforms‚Äù functionality. You need to build your own front end in the technology of your choice. Access is one such option. SSRS is only for creating and distributing reports, it‚Äôs not for entering or manipulating data.
I heard many sql advanced users saying that the models there are way too simple and not practical for real world use cases.
Thank you for getting back to me. So would a viable option be integrate the Access tables into a DB and then just essentially inferface that with the forms and reports that they currently use?
You can use [ScaiPlatform](https://scaidata.com/scaiplatform/data-management) to automatically have forms on top of SQL Server.
"Automatically" What is the advantage of this over using MS Access as a front-end?
Yes, you can create linked tables in MS Access that point to the tables in SQL Server.
Thank you so much for your help
Did you set replication back up afterwards? Or was replication of that database not needed?
Thanks
For me, easier to troubleshoot, adjust, or replace later- doesn‚Äôt require a republish of any application files. Also keeps application code leaner. Can also be reused in multiple applications.
It depends. In 90+% cases (my own opinion), for REUSABILITY purpose, we need stored procedures! Hit UPVOTE button plz. Lol
Yes I agree, I don‚Äôt know why I put reusability last instead of first ha
Replication is not needed. Not sure why it was there in the first place
You didn't write reusability word even. Lol. And more lol because i didn't read you comment. üòÜ
Teach us math related to database first üòÜ
The thing is, few people work with only SQL. There just aren't a lot of jobs that are just "SQL". They few that do exist are looking for experts with a lot of experience. Most jobs integrate SQL with something else. Showing that you know SQL, and can use that SQL knowledge to integrate your data into a product will give you the most bang for the buck. Creating an API, is a fantastic way to show a broader skill set! Other thoughts. Create a database, then create Power BI reports on top of it. Create a database, and a full ETL process for keeping it updated. Maybe scrape a subreddit once per day or week. Something that shows you understand why we need SQL databases. It's not to have data, it's to use data.
Agreed. Whatever this is, it looks sketch. Web application of some sort.
Can you version control stored procedure? &gt;adjust, or replace later- doesn‚Äôt require a republish of any application files. Can also be reused in multiple applications. I think this is a double edged sword. If you adjust or replace later, you have to make sure every application is updated accordingly.
For version control, I use bitbucket. Yes, that is true. I don‚Äôt have a ton of stored procs that span applications so I don‚Äôt run into many problems there. Reusability would be the big benefit - I have some processes to replace values quickly if something is invalid for the team to use (like a location that is still in the system but doesn‚Äôt exist anymore). Something that lets me input 2 values and then allow it to run a series of checks and execute an insert/update depending on the results of those checks.
Some people who write stored procedures don‚Äôt work on application code. Analysts are writing them for reporting. Their stuff doesn‚Äôt belong in the application codebase. Their sprocs probably also don‚Äôt belong in the application database tho but rather a data warehouse.
Right. Too many indexes can negatively affect write / update speeds right? As a matter of style, I prefer always using an identity column as a primary key, as in my experience it does make integrating with data access layers much, much easier, and makes it actually possible to foreign key to the table more easily if that becomes necessary. Also, a lot of the type of the indexes I tried to describe end up being filtered indexes, which cannot be done with a primary key. Thank you for the comment though, assuming those are the biggest concerns, sounds like there‚Äôs no issue with this practice.
Thank you for your input... good point about the unique index being more or less an implementation detail of a unique constraint - I had not considered that. As I just replied to another comment, I try to remember primary keys, however it is hard to utilize them for this when: * It is often I need to utilize filtered indexes, which cannot be done in a primary key, and * My personal convention through experience is to always use an identity column as a primary key. Agree or disagree, but this is a practice I‚Äôve adopted after having to backtrack on using compound keys one too many times due to the added complexity when integrating with applications, and just for general querying.
The battle on this subject has been raging for decades. I'm in the NYC tech scene and application code has pretty much "won" here as far as I can tell - I've seen the occasional developer write a database procedure/function only to get ripped apart by his peers for writing "obsolete" and "not portable" code, and "business logic has no place in the database". It's kind of funny because I've seen some really bad database APIs written in Java and Python that are for one thing using database-specific SQL syntax (which renders it also "not portable") and are horribly performing too because anti-patterns such as row-by-row (aka slow-by-slow) processing that queries the database one row at a time in a loop to perform some algorithm that could be better achieved by one larger query. Or they don't use bind variables which stored procedures make easy (automatic in most cases) which is a security and performance killer. Anyway I'm rambling a bit, here's an article by Lukas Eder (of JOOQ fame) that may help: [https://www.vertabelo.com/blog/notes-from-the-lab/business-logic-in-the-database-yes-or-no-it-depends](https://www.vertabelo.com/blog/notes-from-the-lab/business-logic-in-the-database-yes-or-no-it-depends)
Many sprocs are processes that update db-like objects.
They may be simple, but they're from various industries and well.... they're there! And they're good at least for a pointer in the right direction if you don't exactly know where to start, though mind maps and a decent knowledge of database normalisation are also a good place to start :D
There are indeed jobs out there where you only need SQL and maybe some basic system admin combo, but they're few and far between. Good for getting your foot in the door though! But yeah I defo agree with you that few work with just SQL and in my opinion at least they're something you start off with but eventually have to grow out of if you want to progress. It's just a matter of which way you want to go. Why I've started Python and will be doing C# as well. Somehow I like giving myself headaches. Any pointers for working with Power BI? Is the desktop version they've got good enough for practicing?
I know reddit loves to recommend portfolios but I‚Äôve had 3 Data Analyst jobs and never needed one. It will probably depend on the type of role you‚Äôre applying to but I always just list SQL on my resume. Just explain in your resume what you‚Äôve used SQL for in the past and what level you‚Äôre at (eg. ‚Äúcomfortable with advanced queries including analytic functions, pivoting, stored procedures,‚Äù etc). Most interviews they will ask you technical questions to verify what you say on your resume. Some places will have a technical interview separate from the personal interview, some places will have take-home technical evaluations. I‚Äôm sure a portfolio helps. When I‚Äôve worked with people doing hiring they will certainly check your portfolio to verify what you claim on your resume. But I don‚Äôt think they‚Äôre an absolute necessity for every type of job.
For the SQL basics, either buy SQL For Mere Mortals or look at courses on Udemy as they often go on sale for ¬£10.99 (suspect not far off in other currencies). Or you can sign up for Google opinion rewards, answer surveys as they come through and save up for one course at a time and pay for a course with the earned credit!
Look into joins (joining tables).
You may want to look at ququeing theory.
The biggest reason to not use SP‚Äôs is that some of the largest ORM frameworks work purely because they are able to dynamically build their SQL application-side. There are always tradeoffs of course, but in a lot of cases this means you can cover 80-90% of the queries your basic application needs without writing a line of SQL. For a small team, this can save you a lot of time, and allow you to write reusable repository code way easier than if you were to develop your own framework based on SP‚Äôs. When you use these types of systems, you also get the benefit of compile time checking; your SQL schema and your code will _always_ be aligned, because the schema is usually derived from the code itself. Then you also get benefits like easy migrations and version control. But in my opinion, as long as you have a solid process and convention for SP‚Äôs, they can be just as, or even more, effective. Plus, being able to write complex SQL that no ORM could generate becomes invaluable in some situations. I don‚Äôt think I‚Äôd ever try to hand spin my own SQL queries in code though. It‚Äôs messy, and hard to read and maintain. To me, the options come down to: * Use an established ORM * Use all SP‚Äôs * Use an established ORM for the 80%, and use SP‚Äôs for the rare complex operation.
&gt; create a table containing a &gt; &gt;CHAR(1) &gt; &gt;column, load it with 36 values, then do a self cross join, concatenating the two characters This is a very elegant solution. I never would have come up with it. &gt; as for assigning these 1296 2-character codes, save what you assigned in a table, with the VARCHAR(6) Product Category as the PK so it can't get re-assigned I need to do some reading about this. I only have a high level view of what a primary key does, so I need to understand a bit more intuitively and figure out the mechanics of the solution you described. &amp;#x200B; Thanks for taking the time to respond!
One good reason is that keeping code close to your data is super fast. But otherwise code that deals with data should be made as a service, or micro service. Because it can be versioned, deployed, and the database behind it can change, and the clients wouldn't care.
&gt; utilize filtered indexes, which cannot be done in a primary key had not heard of filtered indexes before now, had to google i was not suprised to learn they are not an SQL feature, but a Microsoft feature
Code that lives in the database can be managed and deployed just like any other part of the schema, with a tool such as Liquibase.
Yeah I should have mentioned MSSQL. I forgot that‚Äôs still something apparently exclusive to SQL Server; it would be hard for me to go back to _not_ using them as it allows you to set up uniqueness for very common situations, such as enforcing the combination of columns A and B being unique, but _only_ when neither value is NULL. And the type of example I listed in my main post, where you have situations where ‚Äúif this column‚Äôs bit value is set to 1, in that case enforce this uniqueness rule across these other columns.‚Äù
Nicely made report, very complex :) i usually make simpler stuff that satisfy business requirements. **What Information About Customers Is Important For A Business?** that is a question you should ask the business, we can tell you what 'might' be interesting but it is better to ask the people who work with customers and run the business, what insights they find useful and need.
Actually, I am not satisfied with this report because it is just a vomit of data. I want to know what are the important key metrics out of all those fields, and how to track them meaningfully over time. I'm assuming some sort of graph, but I am not exactly sure what to do.
&gt;SSRS is only for creating and distributing reports, it‚Äôs not for entering or manipulating data (that hasn't stopped people from trying, but I cannot endorse any attempt to do so). &amp;#x200B; As someone who has had to maintain a legacy application with a bunch of business logic in the reports, I can assure you that this person is absolutely right on this.
There are no requirements set and there is no owner here. To put it bluntly you need to do more research (go talk to people in the business) to find out what you can do for them. you can't invent the wheel and then go 'okay, what can i do with this', it's the other way around :), problems require solutions, you have a solution with no problem. you can use your solution as an example to say "hey this is what we can do, is it helpful to you if we apply this for a problem that you have? " And then in your case you need someone who can tell you: "Yes, I need a report that shows us how many return customers we had over the past year per month" "I want a Pareto diagram of return customers" "Can you give us a list of our top 25 highest grossing customers per state over the past 18 months ?" These are requirement that you could satisfy with your report. try digging into research methods like 5W2H and Ishikawa.
The desktop version should be fine. Might be worth using the web version just to add it to your portfolio. Easy to put into production.
FYI, sqlbi.com is probably the best resource for Power BI.
and here i am working on a new system that will live almost exclusively in the database. granted it's all about data mapping between several systems... but still. it'd be a crap-ton slower to process via some external code... the database is the fastest place, and we have the folks to write good code. but i'm sure some folks in other teams will bitch about it being a database-centric system.
Sweet, thank you!
&gt; Often their database-touching code is horribly performing too because of anti-patterns such as row-by-row (aka slow-by-slow) processing that queries the database one row at a time in a loop to perform some algorithm that could be better achieved by one larger query. can you give an example of a larger query performing quicker? Like for instance if I have to compare two datetime fields AS a new field, where would I see a performance increase?
I'm not sure what you mean by your comparison of two datetimes, but basically the idea is to let the database do the work using a set-based approach, rather than running a loop or loops in your application, and in each iteration running tiny queries that do a single row insert, or get a single row, one at a time. Maybe the loop iterates thousands of times or more, and that's when it becomes a big problem. Here's a good example: http://thedailywtf.com/articles/Very_Slow_Service
Sounds like /u/mwdb might have been referring to the N+1 problem that's pretty easy to encounter with a naively-used ORM layer. I feel like it's a little harder to generate one manually as you're staring it in the face rather than having the guts abstracted away, but I've caught my boss putting a couple of queries in loops that never should have been. &amp;#x200B; Trivial example: foreach (child in query('SELECT id FROM accounts WHERE parent=&lt;parentaccountid&gt;')) { child_users_array += query('SELECT * FROM users WHERE accountid=&lt;child&gt;') }
Yes, that's exactly it, thank you. I had heard of the term "N+1" but the name eluding me when I wrote my comment. :) So I turned to "slow by slow" which is what Tom Kyte called it back when I was first learning this stuff (early 2000s). Though I actually have seen this done manually, not just generated by ORMs, many many times. :-/
What are you having trouble with and what SQL platform are the queries supposed to run on (PL-SQL, T-SQL, mySQL)?
I have not much experience in SQL, the queries are supposed to run on mySQL. I just need some guidance on how to proceed with the assignment
&gt; How do you version control stored procedure? There are tools out there that will automate deployment of stored procedures to the database, from source control.
Using ORMs works well, assuming that your developers are willing to learn how to properly leverage the ORMs to keep them from writing bad code. If your developers can't be bothered, I'd *much* rather have a stored procedure so I can fix the query there without arguing about devs needing to push updated code because their queries suck.
It depends a lot on your environment. If you've got good devs who are willing to invest the time and learn an ORM or to embed good queries in the code, then those tools work well. If you've got lazy devs who don't want to actually *learn* it or cannot grasp why what they're doing is bad, or your change control process is such that getting new code to production is painful, put it in the database. That way, when production grinds to a halt, it's easy to fix the bad queries in the stored procedures.
Why to use stored procedures: * Code reuse (you might use the same sproc in an application and a report) * Compiled execution plans (the benefit here is not really as great as it used to be) * Permissions abstraction and hardening (you don't need to grant table/column select permissions to a user or to an application, just to a procedure) * Protection against SQL injection * It's easier to perform code reviews on straight up SQL syntax vs. SQL that is dynamically built up into a string in application code, making it easier for DBAs to review * Stored procedures can return output parameters in addition to result sets &amp;#x200B; Why to use application code: * Dynamically writing SQL and submitting to the database is quite often much more performant. Often, stored procedures have too much logic that results in non-sargable queries, or huge IF/ELSE blocks with little difference between them * Easier integration with source control
In a past life, I supported an application that used its (custom-built) reporting engine to import data files into the database in addition to doing a lot of data manipulation. Because they didn't ever consider building a proper standalone data import/export process. "Hey, we got this thing over here, let's torture it until it does this other thing too" was good enough for them. What a train wreck that software is.
I agree that ORM + shitty developers can lead to some true hell, and that fixing SP‚Äôs is generally easier _as long as you are only modifying the query itself_ and not the inputs or outputs. Once you‚Äôre in that domain, where you have a SP you need to modify and you in some way need to change its inputs or outputs, and you have shitty code sitting on top of it that will break in mysterious ways if you make any changes, that can be a much, much worse situation to be in. Really the problem is bad devs in general, not so much the process. As bad as ORM code _can_ be, at _least_ you usually get static typing and have a good idea of what you‚Äôre breaking when you make changes to queries. You have not seen true pain until you need to modify a SP used in _n_ disparate pieces of independent code, looping over the returned data with 3-4 nested loops, mixing parsing and application logic, referencing returned SQL data via magic string or _column index_ (shudder).
Absolutely. Easiest way is to install an express version of SQL server (free) and then using the upsizing wizard. This allows you to keep the Forms component of access intact, while using SQL server as the data repository. SQL server Express https://www.microsoft.com/en-us/sql-server/sql-server-editions-express upsizing wizard: https://support.office.com/en-gb/article/move-access-data-to-a-sql-server-database-by-using-the-upsizing-wizard-5d74c0df-c8cd-4867-8d07-e6e759d72924 You can also simply create the tables yourself on SQL Server, then add them as LINK tables in access yourself.
thank you
You're not using parenthesis properly. Your query, simplified, basically consists of select 'true' where 1=1 or 1=2 or 1=3 or 1=4 or 1=5 and (2=3) Due to the lack of parenthesis, the single AND statement is bound only to the last OR. OR 1=5 AND (2=3) So if ANYTHING at all is positive within all of your "OR" statements, save for the very last "or v\_GS\_SMS\_MRUPATHS.DrivePath0 like '\\\\192.%'" THen it will return. The only thing your and statement is bound to is the check on v\_GS\_SMS\_MRUPATHS.DrivePath0 like '\\\\192.%' &amp;#x200B; This can be represented and proven here. select 'true' where 1=2 or 1=2 or 1=3 or 1=4 or 1=1 and (2=3) --This shows that the and is bound to the 1=1 check. Doesn't return, because both aren't true select 'true' where 1=2 or 1=2 or 1=3 or 1=4 or 1=1 and (2=2) --This shows that the and is bound to the 1=1 check. returns, because both are true. &amp;#x200B; So if you're wanting ONLY things to show up if ONE of the or statements evaluates to true AND the second AND evaluates to true, then you'd want something like where ( v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario1%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario2%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario3%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario1%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario2%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario3%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\10.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\10.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\172.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\172.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\192.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\192.%' ) and ( v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.166%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.147%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.0%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.11%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.12%' or v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.166%' or v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.147%' or v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.0%' or v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.11%' or v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.12%' ) This evaluates the entire first block as a singular true/false based on any one of them being true. But, due to the parenthesis, it evaluates the singular outcome of it with the second condition, and since it's an AND, it requires that at least one of them come back as true as well. &amp;#x200B; On another, unrelated note....I see you're querying what looks to be SCCM data. Welcome to the ninth circle of Hell.
Keep doing the Lord's work.
Wrap your BETWEEN's in parenthesis WHERE (a.ORDER_DATE BETWEEN :startDate AND :endDate) AND (a.ORDER_NUMBER BETWEEN ...)
parenthese have no effect here you really only need them if you're mixing ANDs and ORs and not always then
just use `IN (subquery)` rather than `= (subquery)`
&gt; Will this work? My program runs out of time while executing the program. easiest way to find out is to crete a small test table and try it there
Because you are working on an app that needs to scale and the benefits are reduced network traffic and it helps guard against SQL injection attacks. Using sprocs also improves performance since the query processor doesn't have to create a new plan.
It's for legibility purposes. Thanks for the downvote tho!
If you're talking about being able to search by the order number, range of dates, or both, something like this could work for you: WHERE a.ORDER_DATE BETWEEN ISNULL(:startDate,a.ORDER_DATE) AND ISNULL(:endDate,a.ORDER_DATE) AND a.ORDER_NUMBER = ISNULL(:orderNumber,a.ORDER_NUMBER) If you supply dates and null order number it will return everything in the date range. If you supply the order number and null dates it will return everything with that order number. If you supply both order number and dates it will return everything with that order number that is also between the specified dates. If you supply all nulls it will return everything.
You can't wildcard with by using the '%wildcard%' as the variable. The wildcard portion needs to remain in the query. e.g. Instead of declaring '%Easy C %' you should only put the name 'Easy C'. Then change your LIKE statement to the following: like '%' + @companyName + '%'
This won't solve their direct problem, but definitely should be updated.
&gt; On another, unrelated note....I see you're querying what looks to be SCCM data. Welcome to the ninth circle of Hell. Ha! It's usually not that bad for me to work with, actually one of the easier ones from my limited exposure. But I have a pretty good understanding of the underlying data which I suppose makes it easier. So, everything you said made sense, but even changing that I get the same results as if the whole "and" statement is just commented out. The desire is to return any value that matches any of the criteria from the first two blocks, and then use the third block to filter that out undesired results. Tried OPTION (FORCE ORDER)... No luck, though forcing execution plan rarely helps.
You don't have to pull the wildcards out of the parameter (at least on the versions of mssql server that I've tested on). The problem is that he's declaring the variable as only VARCHAR which is defaulting to 1 character length, which means that when you set it equal to a string only the first character is kept so it's searching for LIKE '%' which returns everything.
Thanks, this solved the issue.
Actually, it did.
Hmm, so, you're only getting results back for the company you requested and not the entire table?
This did not work, u/r3pr0b8 's solution did. Thanks for the varchar length advice.
Interesting. I used varchar in my sandbox which is a 50 and using the wildcarded variable returned the entire table but, breaking out the LIKE statement only returned the variable's matches. I'll keep this in mind.
Yes, strangely if you don't specify a length when declaring a variable or data definition then it defaults to length 1. If you don't specify a length when doing a CAST or CONVERT then it defaults to 30.
This makes sense as far as not specifying a length. I suppose it boiled down to dumb luck on my end that I've always broken out the wildcard versus having it in the variable. You learn something new everyday. :)
Thanks, will check.
Thank you this was very helpful and insightful this is my first dev job so i really dont want to mess it up :)
Are Udemy courses credible in the sense that you could list on a resume? Or just for for self learning?
It's not really solving the issue so much as masking it (though it is a good idea to use IN when comparing to a subquery unless the subquery is only returning 1 row via TOP). The real problems is that since you didn't specify a length for the VARCHAR declaration it's defaulted to length of 1, which means that only the first character in the string that you assign it is kept. You can test it by executing: DECLARE @companyName VARCHAR= '%Easy C%' SELECT @companyName This means that the subquery is comparing companyName to '%' which will return everything. To fix it you will need to assign a variable length, preferably the size of the column in the table as /u/noesql mentions.
Case when date1 between startDate and endDate then date1 else date2 as ‚Äònew field name‚Äô I can write something that‚Äôs as fast but over a large set, as long as you index and sort optimally, this would be preferable and your application code would be able to execute the stored procedure. Note that the procedure is readable and could easily be updated if database systems switch but the code portability remains intact
I'm not big on stored procedures. I used them for minor stuff like downcasing email addresses on insert. I'll put it this way, business logic should be entirely in repository code, or entirely in stored procedures. Splitting it leads to bug hunts.
Are all the joins repeats of numbers 0-9? If so you could maybe grab substrings of 1 character and then compare them. If they go onto patterns like 101010 I think you may get hard to figure out how long of a string you need.
select oid,value, id, c1, c1 as c2 into #temp1 from #temp
Instead that renamed all of the numerical values in the column 'C1'.
Update #temp Set c1 = c2 Am I missing something
So do all the OR's in the third block need to be AND's?
&gt; Update #temp Set c1 = c2 Yes I think so. The original table that C1 and C2 came from were values in a different column, so I made the select statement (from my original post), to extract those values and create a column out of them. Example below (idk why it wont format as a table... sorry) ID Value 21 0 c1 7764 asd TRUE c2 1823 rocket FALSE blue FALSE green 1
[Otherwise known as RBAR: row by agonising row](https://36chambers.wordpress.com/2014/02/26/t-sql-anti-patterns-row-by-agonizing-row-rbar-queries/).
&gt;As bad as ORM code can be, at least you usually get static typing and have a good idea of what you‚Äôre breaking when you make changes to queries. You have not seen true pain until you need to modify a SP used in n disparate pieces of independent code, looping over the returned data with 3-4 nested loops, mixing parsing and application logic, referencing returned SQL data via magic string or column index (shudder). Yikes!! Yeah the bottom line seems to be know what you're doing. :D
It‚Äôs my pleasure! Steps to not messing up: 1) BACK UP YOUR ACCESS DB!!! Hell, take it completely offline into a local disk. 2) install and setup a local SQL server instance to practice using the wizard. 3) create a fake access Db that mimics some functionality you currently have 4) make the switchover with your two local DBs before you even try to test it out in The dev/QA environment. 5) use LucidChart or Visio to map out all the changes! This will build a checklist for you. Good luck: you got this.
Hadn't heard that one. Thanks!
It looks like the user you replied to is an employee or other type of shill.
Hahaha this is my thought exactly!
If you care about performance, you trust your database developers to write efficient code over your front-end rube
&gt;Easier integration with source control This is way less of an issue than it used to be. There are solutions from companies like RedGate and others that basically deploy from source control.
They're for self learning. As far as I know, if you want a paper to say 'yep, I know SQL', you either go through an accredited course (which typically are expensive) , or you sit an exam to become certified, so I would look into getting a certification.
[pretty simple](https://imgur.com/a/1jphiDL)
Yes good point! However now completely valid entries are getting excluded... Strange stuff.
If you are using TLS, then it will be encrypted between the browser and the web server. You'd still be able to see it in developer tools though, and that's okay
It'll create a new plan under certain circumstances. If the plan cache is flushed all those plans are lost and there's no guarantee the optimiser will pick the same plan again.
Thanks
you were so close!! SELECt col1 , col2 , COUNT(*) AS howmany FROm tableA GROUP By col1 , col2
Do you have any examples?
I have that feeling as well but didn‚Äôt want to come right out and say it.
[Low hanging fruit from Redgate](https://www.red-gate.com/hub/product-learning/sql-change-automation/moving-from-application-automation-to-true-devops-by-including-the-database) I'm most familiar with SQL Server, as that's the world I live in. Microsoft has tools built into TFS and Azure DevOps. Redgate offers even fancier tools. A quick search [shows some tools from Octopus Deploy](https://octopus.com/database-migrations)
In a perfect world, business logic wouldn't ever exist in a database. In the real world, there are many situations where an abstraction layer in the database (of stored procedures, views, and/or functions) is beneficial and makes sense: \- When application developers are bad/lazy with their SQL queries. Bad queries slow the application down, tax the database, and can have large repercussions to any other application using the database. Leave the database querying to the guys who know it most. \- When you don't want the devs to have access to the full, raw database. Think 3rd party vendors, interns, etc. Giving them an abstraction layer to work with is a great way of limiting their access. \- When it's just easier. Sometimes things can be accomplished much more easily with a simple SQL query. Why have a dev deal with sorting algorithms when a 1-liner "ORDER BY" clause will do? Why have a dev write complex logic to link a half-dozen different datasets when some simple JOINs can accomplish the same task?
Oracle, not sure of which version.
You could try inserting the results from your query into a temp table and then select and join to the order number from the second query.
In T-SQL, I'd use an inner join on itself: `begin try drop table #orderdetails end try begin catch end catch;` `create table #OrderDetails (id int identity(1,1), order_number int, part_name varchar(10));` `insert into #OrderDetails values (1, '123'), (2, '456'), (1, 'newprod'), (1, 'another'), (2, '789');` `select od2.order_number, od2.part_name` `from #OrderDetails od1` `inner join #OrderDetails od2 on od1.order_number = od2.order_number` `where od1.part_name = '123';`
Something to keep in mind: whenever you are asked to list "x" and a null if there is nothing matching, you should immediately be thinking of left joins. Here's something to get you started. Not my prettiest, but it hopefully gets the idea across (T-SQL): &amp;#x200B; -- set up and insert dummy data begin try drop table #Product end try begin catch end catch; begin try drop table #PC end try begin catch end catch; begin try drop table #Laptop end try begin catch end catch; begin try drop table #Printer end try begin catch end catch; create table #Product (maker int, model int, type char(1)); create table #PC (code int, model int, price decimal(9,2)); create table #Laptop (code int, model int, price decimal(9,2)); create table #Printer (code int, model int, price decimal(9,2)); insert into #Product values (1, 1, 'p'), (1, 2, 'p'), (1, 3, 'l'), (2, 4, 'n') insert into #PC values (1, 1, 500.00), (2, 2, 750.00) insert into #Laptop values (1, 3, 250.00); insert into #Printer values (1, 4, 100); select p.maker, max(coalesce(pc.price, l.price, pr.price)) from #Product p left join #PC pc on p.model = pc.model and p.type = 'p' left join #Laptop l on p.model = l.model and p.type = 'l' left join #Printer pr on p.model = pr.model and p.type = 'n' group by p.maker
Generally speaking, they should. There are different ways of relating things, however. High level, for a transactional database you usually want to have a normalised structure, but for a data warehouse you will often see de-normalised data. This is because in the transactional side you want to minimise the locks by keeping the different components separate (so updating an event name doesn't lock the table with venues, say), but on the reporting side there should be few if any writes so it is optimised for reading and might merge some of the data into a single row. HTH...
Stored procedures are pre compiled hands will run faster than just regular queries.
I think this may work, but, please, use selects first and/or a begin tran + rollback tran. delete from countries where not exists ( select countryid from clients where clientid is null ) and not exists ( select countryid from offices where officeid is null )
No dice, unfortunately. When using selects it returns all the countries rather than just the ones not used in Clients and Offices.
I think I have my not exists wrong with exists. Can you replace the not exists with exists? If that still is no go, sorry for the waste of time. :/
I‚Äôve never seen an inner join on itself. Is that uniquely a t-sql thing?
Still nothing. Not a waste of time, I appreciate the attempt.
Maybe I come from a weird place? We use stored procedures for everything. Keeps our application code nice and light.
hello, &amp;#x200B; thank you for your response! I have tried your solution, but it still didnt go through :(
It should be standard SQL, but I haven't used anything other than T-SQL for quite a while, and I don't have anything readily available to try it on... I'd put it at around 99.98% sure that it is standard.
Are you trying to modify and delete records from the Countries table? Or create a query that excludes these values? To query without altering the base table, you will want something like this: Select Countries.CountryID, Offices.OfficeName, Clients.ClientName From Countries Inner Join Offices on Offices.CountryID=Countries.CountryID Inner Join Clients on Clients.CountryID=Countries.CountryID &amp;#x200B; As an example: &amp;#x200B; Countries |CountryID|CountryName| |:-|:-| |1|United States| |2|Canada| |3|Mexico| |4|Germany| &amp;#x200B; Offices |CountryID|OfficeName| |:-|:-| |2|Ontario| |5|Paris| |4|Berlin| |7|Tokyo| &amp;#x200B; Select Countries.CountryID, Countries.CountryName, Offices.OfficeName from Countries Left Join Offices on Offices.CountryID=Countries.CountryID A left join will yield all of the rows from the country table, and only the matching values from the office table. It will fill NULL values for the offices that don't have a match in the country table. This is where you were manually excluding the NULLs &amp;#x200B; |Countries.CountryID|Countries.CountryName|Offices.OfficeName| |:-|:-|:-| |1|United States|NULL| |2|Canada|Ontario| |3|Mexico|NULL| |4|Germany|Berlin| Select Countries.CountryID, Countries.CountryName, Offices.OfficeName from Countries Inner Join Offices on Offices.CountryID=Countries.CountryID The inner join pulls only the data that exists in both tables. &amp;#x200B; |Countries.CountryID|Countries.CountryName|Offices.OfficeName| |:-|:-|:-| |2|Canada|Ontario| |4|Germany|Berlin| &amp;#x200B; If you inner join the client table, you can limit this query even further. &amp;#x200B; I'm still learning myself, but I like the visualization and practice tools on [w3schools](https://www.w3schools.com/sql/sql_join.asp)
I am trying to alter and delete the records from the Countries tables. For example, I know that CountryID(2) is not used in either the Offices table OR the Clients table. Therefore, I need to delete it.
The problem statement isn't really clear... the first statement says if they have any product, display the maximum price, but the output implies that if there are any nulls (i.e. they don't have a product in that category) then display null. Maybe that's part of it? I think it would make more sense to have maker, type, max price, but that's not what is asked for (though if it was, you would need to left join on the types as well to get all of them). I'm not really sure what they are looking for, sorry!
Hmmm. I feel like the exists would work; my apologies.
I edited my original comment, same worthless crap?
Yeah I use stored procedures for almost everything now and I went to school for application design and almost feel bad for how little I use it. When all the data is already living in the database it makes sense to just make a stored procedure.
The query you used is, as you stated, correct, and probably the way I would go with it. As for an explanation, a left join basically says to take all the values from the left table and try to match them to values in the right table, returning nulls if there is no match. This is contrasted with an inner join where any that don't match are not returned. When you look for null values from the right table you are essentially asking for any records from the left table that don't have a match in the right table. If you have to do it using only cross joins, inner joins, unions and intersections... something along the lines of: select ID from countries where ID not in (select ID from offices) intersect select id from countries where ID not in (select ID from clients)?
lmao it's not worthless. It didn't work for this problem but your code actually helped me optimize my code in a different database. Thank you!
Haha, well, that works for me. XD
Left joins make a bit more sense now. As for the code, it works when using selects, however, when doing a delete statement it throws a syntax error at intersect. I know that I can't use cross product style in delete statements, but I thought I could use intersect?
If you are hitting physical limits (CPU, I/O, etc.), using the same box won't help - and would probably hurt, since there could be additional overhead. If it is a locking issue, you might be able to (somehow) force the reports to read uncommitted (dirty reads), but a duplicate DB could also work... though keeping them in synch could be fun. Don't forget the opportunity here, though... If you want to become a DBA, you could probably push for training and a new server, since it is vital to the company, but if you would rather do something else then try the above for a quick fix.
What about: delete from country where ID in (select id from countries where id not in (...) intersect select ...) (you get the idea...)
Shard off the data you need to another db server, have a dts package replicate it back if you can't do HA stuff. Have the knobweasels use the second server for their reporting.
I don't know why you wouldn't. Way fewer things that can go wrong handling data in SQL rather than outside of it. Also it makes sense to return just the data you need, not extra data that you will not use.
Works like a charm! Awesome, still think I prefer the left join method but don't want to get a 0 for using techniques in chapters we haven't read yet. One last question, I asked the instructor for some help and got this tip from him: right outer join countries c on c.countryid = o.countryid where o.countryid is null) x1, (select c.countryid from clients cl right outer join countries c on c.countryid = cl.countryid We definitely haven't learned anything like this yet, and it's obviously just a snippet of the code. But what exactly does 'x1' signify here. I haven't seen it in SQL before.
If rebooting the box fixes it for a while you are probably running out of ram on the box. How much ram does it have and can you easily add some? This won't fix all of your problems but it might be a fairly easy way to get a decent bit of additional performance.
It's just an alias, like c and o.
Ah okay, well thank you for your explanation on left joins and the advice on using intersect. I greatly appreciate it.
Just do an inner join on the query that you pasted above (so that query you pasted becomes a subquery). No need to write it to a temporary table.
No problem - and BTW, the right outer joins are basically the mirror images of left outer joins (they take everything from the right table and anything that matches in the left), so your instructor is kind of using the same approach... though I can't think of any time I've ever come across a right join in the wild.
SQL Server licenses are a lot more expensive than hardware upgrades in most SMB situations. Throwing a ram kit and a NVME card or array of SSD's is a cheap way to get the most out of any server. Storage is key for SQL, yes you can get the whole DB in memory but eventually it has to hit your storage. You might as well address the bottleneck first. If you are on spinning rust still literally anything would improve your setup dramatically. Frequency optimized CPU's (fewer but faster cores) tend to be winners in transactional databases if you can swing that, you'd be shocked at what you can buy on ebay for under a &lt;$100 for the most common server systems of the last 10 years. If this is a VM, committing all your memory will help a ton, as would taking a look at your network offload settings and optimizing them. Latency rules with databases, lower latency means more transactions.
You should practice with datasets that can help you prepare for interviews or daily work. I personally used strata scratch. I thought it was worth the time and expense. I found this platform cheap but quality course to fit my needs. I'm sure it will save your time digging on the internet to get it all yourself.
correct answer is WHERE PART_NAME LIKE '%123%'
Shit happens. I went to school for economics. Started using SQL for analytics. Now I write functions and Stored Procs for applications.
Shit happens. I went to school for economics. Started using SQL for analytics. Now I write functions and Stored Procs for applications.
Let me know if you find one
What version of SQL Server? What are the specs of the box? How big are the databases? In house app or third party? If it‚Äôs third party, any support? It definitely sounds like I/O problems due to hardware, but what about missing indexes? Do you have any maintenance plans? Are they running regularly? If this is a critical LOB app, and this is old hardware, it probably justifies a new server(s). Also, look at: https://docs.microsoft.com/en-us/sql/database-engine/sql-server-business-continuity-dr?view=sql-server-2017 You probably can‚Äôt justify everything in this list, but it‚Äôs worth a look. Especially for read only copy options. I believe at least one of those options gets you one extra copy with a Standard license, but I could be remembering incorrectly.
What exact waits are you seeing for the critical processes? Thousands of seconds isn‚Äôt a realistic wait for any IO so it sounds like some serialisation is happening, perhaps reporting users are trying to query some data that happens to include a row that someone has had locked since lunch time because they forgot to commit their work. If this is the case then my first option would be to find out how users are able to keep their transactions open for so long - perhaps the application in use needs to use an optimistic locking strategy? Perhaps users need to be educated? Perhaps you need to change your workflow so that long running transactions are reported and the owners are contacted so they are completed. Reports typically would benefit from snapshot transaction isolation so they don‚Äôt require locks on the data being read, if this is easy to setup and test then I suggest testing it. SQL Server Always On would be my suggestion if you want to duplicate out, have a google around. Of course, this all assumes your bottleneck is row locks - the first step is to confirm this. Are you using sp_whoIsActive? As an accidental SQL Server DBA myself (Oracle background), I‚Äôve found that invaluable to getting started figuring out what‚Äôs going on.
If there's none of this out there, PM me and I'm happy to write a course. This is my favorite part of SQL honestly and to me gets so much more of my pride and joy than any dbadmin or boring other kinda stuff. I also took a class on this in college (systems analysis and design) and I saved all my coursework and still have the textbook so I'd have some materials to go off of. Happy to help either way or happy to run a little class for /r/SQL. Cheers
Often, but it depends on the business. Imagine Amazon's database. Customers have orders; Products have orders; Merchants have orders; and those relationships are all potentially many-to-many without a clear "top" of the hierarchy unless/until you specify the question.
This is bad advice. First check your Page Life Expectancy and all disk related counters to see if there is any memory pressure.
Help please ? I tried SELECT user.Login as login_demandeur ,file_request.* FROM `file_request` JOIN user ON (file_request.Detendeur=user.User_ID) WHERE file_request.Demandeur="68e8e51b8de47c3d8b7daedcb646a2ca" But i only get the 1rst line and not the 3rd of the file_request table
I've bee working on enterprise software where part of the business logic is written in SQL, and the other in .NET for almost two years. My biggest issues with splitting the functionality flow have been releasing new versions and bug fixing. When it comes to developing new features and releasing new versions, it's happened on multiple occasions that I simply cannot go through with a change request because part of the solution has to go through the application code, and this means having to discuss it with the guys working on that side etc. Bug hunting and fixing having to split between two entirely different technologies, often times jumping way too much. Another, slightly lesser issue has been that I've felt like SQL has sometimes been too rudimentary for me to use even though I've been forced to use it to implement some functionality so I've been placed in a situation where I have to more or less write functions and SPs that would've been easier to concoct in .NET (at least maybe for me), or they might've even existed somewhere in the first place. All in all, I think that most pains regarding this question have been logistics related, so I'd prefer that functionality is placed strictly on one side. These are just my two cents.
There are many ways you could approach this. If you suspect the problem is the reports being run by finance, do you have control over the queries or database schema. If so you could try to tune queries/database. More often than not I find problem performance issues can be resolved with changes to the queries or making schema changes. I don't think throwing hardware is necessarily going to solve the issue unless you understand where the bottle neck is. Yes hardware is cheap, relativel, to other options, but if that isn't the problem it may be wasted money. As for them needing to run against live data is there a reason for that? I'm not disputing that they may have a legitimate need to but I've seen cases of trans confusing wants vs needs and when it comes down to costing out a solution a need becomes a want pretty quickly. Normally when I troubleshoot SQL performance issues I run a few scripts to get some historical wait and performance data as well as another script to get some information about what is happening now. The queries i use are pretty much the same as [Glenn Berry's SQL Diagnostic Queries ](https://www.sqlskills.com/blogs/glenn/category/dmv-queries/) and [Adam Machanic whoisactive](http://whoisactive.com). [Brent Ozar](https://www.brentozar.com/responder/) also has some good scripts to try to tackle these types of problems. Let us know what you find and we can help trying to move the problem along to fixing it.
Report Builder can connect to SQL Server and it‚Äôs free.
Thanks for the reply , ive had a look at that but im not sure it does what my boss is looking for. At this point I might just say that what he doing is probably the best way to do it and to just integrate the access db to sql via upsizing wizard.
100% except it's not a "standard" (as in part of the language), it's just something that ever database can do
 SELECT maker , MAX(max_price) AS max_max_price FROM ( SELECT p.maker , MAX(pc.price) AS max_price FROM product AS p LEFT OUTER JOIN pc ON pc.model = p.model GROUP BY p.maker UNION ALL SELECT p.maker , MAX(l.price) AS max_price FROM product AS p LEFT OUTER JOIN laptop AS l ON l.model = p.model GROUP BY p.maker UNION ALL SELECT p.maker , MAX(pr.price) AS max_price FROM product AS p LEFT OUTER JOIN printer ON pr.model = p.model GROUP BY p.maker ) AS d GROUP BY maker
Thank you very much for your message and your feedback, it means a lot since we are foreigners. I am sorry it offended you in any way. Yes, we are a small start-up and we build ScaiPlatform as a SQL UI on top of multiple SQL databases, including SQL Server, PostgreSQL, MySQL, etc. The advantage of it is the fact that it combines the ability to insert/delete/edit/merge data through the UI, to load files into the database, to create reports, dashboards, and join data without any coding. It also acts like a centralized reporting system which can run on all cloud providers and on premises.
Report Builder only gets you halfway - just the reporting side of the Access stuff you have now. Please don't just "use the wizard" to move your data to SQL Server. You have to keep it on a short leash so as to not get screwy data types and all of your constraints &amp; relationships may not come over. Not to mention, mistakes made in the Access schema (like storing dates as strings) will just be persisted. This is your chance to make things better. Instead, recreate the schema in SQL Server properly, then import the data from Access.
&gt; The advantage of it is the fact that it combines the ability to insert/delete/edit/merge data through the UI irrespective of the database type, to load files into the database, to create reports, dashboards, and join data without any coding Every time I have seen these sorts of "without any coding" promises, the end result has been either limited (no complex joins or logic in the queries) or a mess that the DBA has to clean up (performance &amp; schema). Or both.
It's SQL Server 2014. The box is a VM running on private cloud infrastructure with a VMFS file system. It's a third party app and the database is about 50GB. The box has 28GB of RAM and 8 processors. 2 sockets with 4 cores each. I'm committing 85% of that RAM to SQL.
Hi all Thanks so much for the replies. Here is the server specs: It's SQL Server 2014. The box is a VM running on private cloud infrastructure with a VMFS file system. It's a third party app and the database is about 50GB. The box has 28GB of RAM and 8 processors. 2 sockets with 4 cores each. I'm committing 85% of that RAM to SQL. &amp;#x200B; I am running solarwinds DPA and on Friday the server locked up on me and the wait types were as follows at around 4PM when the server had to be rebooted: LCK\_M\_U - 2 hours LCK\_M\_IX - 90 minutes OLEDB - 90 minutes ASYNC Network IO - 13 minutes &amp;#x200B; Also, the person's computer that does the reporting shows up as the highest impact on wait time. He runs Microsoft Access to run reports. I think the reports that he runs in access are leaving transactions open or something because when I see him on the graph with high wait time caused, I will follow up with him and he says he's not doing anything. &amp;#x200B; The app is called FasTrack and it's a medical supplies line of business app that does ordering, billing, etc. Is it possible that the app is just coded poorly? I know that some users will open up a patient and start to key data and keep that patient open while working on another order because calls are constantly coming in and they have to multi task. is it possible that this could cause wait times? I am running Brent Ozar's scripts and I just ran SP\_BLITZ to run a healthcheck and here are the results(I'm filtering just on the production DB): [SQL SP\_BLITZ FTDB1.xlsx](https://wholetechnology-my.sharepoint.com/:x:/g/personal/anthony_wholetechnology_net/EcMr4w8Bd41Nm_5iFZd4840B5lBS9GtmX1JKOtTzlyOHug?e=h13tdC)
Oops, nope. Was in a hurry last night, saw results appear and assumed they would be correct. My bad.
Oh, didn't know that. Interesting. Thanks.
Everyone, thank you for your input and time, I learned some new things about SQL from your comments. The issue was due to me not specifying a length for the VARCHAR variable @companyName.
&gt; The app is called FasTrack and it's a medical supplies line of business app that does ordering, billing, etc. Is it possible that the app is just coded poorly? Most LOB apps are. Can you run `sp_blitzfirst` while the problem is happening? Based upon the results of `sp_blitz`, I suggest the following: * Get all user databases (preferably _all_ databases) moved off the C drive * Set the autogrowth for your database to 1GB (1024 MB) * Set the autogrowth for your log to 128 MB (unless it's larger right now) Is the database set to `FULL` recovery model and if so, how often are you taking transaction log backups? How often are you performing index maintenance? &gt; the person's computer that does the reporting shows up as the highest impact on wait time. He runs Microsoft Access to run reports. I think the reports that he runs in access are leaving transactions open or something because when I see him on the graph with high wait time caused, I will follow up with him and he says he's not doing anything. Ask him to not run reports for 3 hours. Then let him run reports. If the problem happens during those 3 "idle" hours, then it's not him. If everything is great during those 3 hours and then goes to crap once he starts running those reports, you have a smoking gun.
This does not work.
You should get a list of where they are in the Application and what Queries are run when they perform these actions/reports. May need some optimization on the Reports If it's general slowness everywhere then it's probably the Server (CPU most Likely at 90-99%. You can check in Task Manager). I think it's a User issue bogging down the Server. &amp;#x200B; &gt; finance is running reports As I was saying. This is more than likely the problem. Multiple people running reports that are not indexed properly or have large return data sets will Suck the life out of the Processor on a Server. Large reports should run overnight. NOT put the business at a standstill. Especially when multiple are involved running the same report. &amp;#x200B; &gt; I have to reboot the SQL server and the application server to resolve the problem. Well yeah. Of course this works. You're killing all processes and starting over. But, then Marsha and Debby run Financials from January 2005 to Today and you're back at square one. &amp;#x200B; &gt; Do I need another SQL server or can I just **duplicate the DB** on the same box. Both DB's need to be updated then through the Application (when posting changes to financials) or you'll have two different data sets when running Reports; causing a bigger problem. You may need a more powerful Server. But, I'd get info on Which Reports and what they're doing before you can proceed.
Learn Power BI, you wont regret it.
I am going to take a guess that you are placing your Access database in a shared drive. Are you using Access as a reporting tool? Or will people update records to the Access database? Are you familiar and experienced with VBA? Because without knowing all the details it is hard to suggest a proper solution.
You are right, we wish we could say this, but we are ashamed that people will give us so much hate. It is very tough as a start-up, since none of you would not even trust it to give it a try. &amp;#x200B; You have a valid point and that is why we let users create joins through the UI and only the DBAs or users with write rights in place can store these queries. At the same time, all the SQL code that is generated with the joins, pivots, aggregates or custom fields can be viewed at any given moment and can be changed or previewed, until you have reached the desired query. We built Scai such that it is lightweight, because we did not want to bring any performance overhead.
&gt; we wish we could say this, but we are ashamed that people will give us so much hate. It is very tough as a start-up, since nobody wants to give it a try. People will have a more positive opinion if you come right out and say that you're representing your product than if you try to hide it. Honesty really is the best policy.
Thanks so much for the reply. I will run sp\_blitzfirst when I start to get wait time alerts. &amp;#x200B; Currently the database is set to autogrow by 1MB and max size is unlimited. The log file is set autogrowth by 10% and maxsize is unlimited. Initial size for DB is 47897 and initial size for Log is 2915. &amp;#x200B; The database is set to full recovery mode and I'm taking log backups every 15 minutes. &amp;#x200B; I'm not sure what you mean by index maintenance. I'm doing cleanup after the backups and I do a checkDB on the database each night.
Here are the current SP\_BLITZFIRST results when nothing is going on: [SP\_BLITZFIRST 6-18-2019 930AM.xlsx](https://wholetechnology-my.sharepoint.com/:x:/g/personal/anthony_wholetechnology_net/EUWUWN4E5q1LiUWTOXpEfeABBp01DGtEbNh4OCpntDtDQA?e=L3Pu8P)
It‚Äôll work, maybe slowly, but make sure you aren‚Äôt running afoul of your corporate IT policies. My firm is pretty antsy about ‚Äúrogue IT‚Äù like this.
Thank you for the suggestion. Maybe you are right and people will be more open to it this way.
Depends on the Laptop Hardware, technically it's possible, and with a decent laptop 8GB shouldn't be a problem. Also put the server on a VM, makes life easier.
Yeah, this would be through IT, I just wanted to make sure that this made sense before I floated the idea by them.
I think you might need a subquery. You can basically put the above query you wrote, into the where clause of a parent query. For example, if I wanted to know the average order value in states where my company sales were over $100k, I would use a subquery. This page has a tutorial. [https://www.essentialsql.com/get-ready-to-learn-sql-server-21-using-subqueries-in-the-where-clause/](https://www.essentialsql.com/get-ready-to-learn-sql-server-21-using-subqueries-in-the-where-clause/)
&gt; Currently the database is set to autogrow by 1MB and max size is unlimited. The log file is set autogrowth by 10% and maxsize is unlimited. Initial size for DB is 47897 and initial size for Log is 2915. Yeah, these are both bad settings. Change them to what I gave you above. It's an online operation, no one will even notice when you do it. &gt;The database is set to full recovery mode and I'm taking log backups every 15 minutes. How is this happening? Maintenance Plan, Ola Hallengren's maintenance solution, other? &gt;I'm not sure what you mean by index maintenance. I mean exactly that - index maintenance. Rebuilds, recreate statistics, reindexes, etc. Ola Hallengren's Maintenance Solution will install an Agent job that will do this fairly intelligently for you with its default configuration. If your data is growing at a decent clip and you aren't doing basic index maintenance, you're going to have performance issues. OTOH, given that your database has heaps (according to `sp_blitz`), there may not be many indexes _to_ maintain.
Excel is completely butchering the formatting of this (I'm assuming you did a copy/paste?). Save the results as CSV from SSMS and then convert it to Excel format.
Hi, feel free to check out the chapter _Recursion_ (http://db.inf.uni-tuebingen.de/staticfiles/teaching/ss17/advanced-sql/slides/advanced-sql-05.pdf) of my _Advanced SQL_ course (https://db.inf.uni-tuebingen.de/teaching/AdvancedSQLSS2017.html). Cheers and happy recursing, ‚ÄîTorsten
Thanks!
Figured it out. Since this isn't StackOverflow :), here's what ended up doing it : where (v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario1%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario2%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario3%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario1%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario2%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario3%') or (v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\10.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\10.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\172.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\172.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\192.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\192.%') and (v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.166%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.147%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.0%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.11%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.12%') and (v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.166%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.147%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.0%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.11%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.12%') I guess each subsequent data type needed its own evaluation block to filter out the virtual tables in the right order? Not quite sure, but this does seem to return the correct results. &amp;#x200B; Thanks for your help and pointers in the right direction!
Figured it out. Since this isn't StackOverflow :), here's what ended up doing it : where (v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario1%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario2%' or v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario3%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario1%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario2%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario3%') or (v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\10.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\10.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\172.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\172.%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '\\192.%' or v_GS_SMS_MRUPATHS.DrivePath0 like '\\192.%') and (v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.166%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.147%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.0%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.11%' and v_GS_SMS_MAPPEDDRIVES.DrivePath0 not like '\\10.161.12%') and (v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.166%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.147%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.0%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.11%' and v_GS_SMS_MRUPATHS.DrivePath0 not like '\\10.161.12%') I guess each subsequent data type needed its own evaluation block to filter out the virtual tables in the right order? Not quite sure, but this does seem to return the correct results. &amp;#x200B; Thanks for your help and pointers in the right direction!
Please create and share this course as I and many others would be interested in learning from this. Thank you!
detendeur and demandeur users are different, so you need to join to the user data set (table) twice. in order for SQL to properly address columns with the same name, you'll need to give these 2 tables their own table aliases. It is generally a good idea to always give table aliases, especially if joins are involved select u1.login as login_demandeur from user u1
SSRS is always good ;)
Literally everything is better than Access. Here's some free ones. https://blog.capterra.com/top-8-free-and-open-source-business-intelligence-software/
Ok for prototyping/development, but not for production. If you do this, set things up so you can port everything over to "real" infrastructure once you get some buy-in.
+1 on the class! :)
&gt; I'm currently working with about 8gb of data, but the main reason I want an actual DBMS is to bring more data in What are you using right now for this?
you have a simple scenario with one of two options... Option 1 : create a separate Reporting or Analytics database ... create views (to support the reports) that query data from the main LOB database, but include "WITH (NOLOCK)" on every table... create procedures if necessary to support the reports... switch the reports to use the new database. Basically, the option tells SQL "let other processes make changes to the data without worrying about my reports"... the risk is that perhaps a report will include a line item that's slightly off from the final answer, only if they're reporting on data that's actively being manipulated. Option 2 : use log shipping / replication to create a second copy of the database... switch reports to query the database copy. (technically there's a third option - you *may* be able to tune the reports... but depends on a *lot*) first option is cheaper, but with a risk... second option has more cost, lower risk... let the business decide how much they want to spend.
depends on the hardware and the data... if you're on SSD with more than 8gb RAM, you'll do ok for up to 10m rows of data... non-SSD or &lt;8gb RAM or more data -- then it depends.
Started to see some wait time alerts and here is the recommendations from SP\_BlitzFirst: [SP\_BLITZFIRST 6-18-2019 123PM.docx](https://wholetechnology-my.sharepoint.com/:w:/g/personal/anthony_wholetechnology_net/EehMWG0p3ZlFpseqXvM_tbcBnFuRiGLfJHJcVchdB--fjg?e=Xbhia4)
in option 2 do i need to put this db on another sql server? or can i just put it on the same one?
use rownumber() to figure first/subsequent rows and use case expression to set your description.
It‚Äôs good habit and practice to use the: ‚ÄòUSE &lt;DatabaseName&gt; GO‚Äô command on all queries. Not only then do you know for certain what database your query is being executed against but so does any else picking up your query. However, the drop down at the top of SSNS does work just fine. I would recommend explicitly referring to databases in the Database.Schema.Object format unless your accessing multiple databases.
So, it is completely reasonable that by just leaving it as master and running the code it could take longer to run than if I used the "Use" statement?
I find [Project Euler](https://projecteuler.net) to be a pretty good source of programming problems. Many of the early ones map really nicely to recursive CTEs.
That worked beautifully, Thank you!
Do the tables your looking at excist in the MASTER dB? It‚Äôs going to be difficult to give you a for definite answer without seeing the query.
First of all, thanks for taking your time! The master database has nothing I want. I am working from a server that has a lot of databases on them. When developing a report or process, I typically do not change the database or utilize "use". I always am explicit with my objects. &amp;#x200B; Some reports I need to pull data from a couple of the databases but for this one, it is all in the same database. I know some colleagues that do put "use" at the beginning but I just haven't ever done that since I am going to be writing out the objects anyway. I always thought it was just so you didn't have to actually write out the full object and could just use the table. It seems though that using that database is actually sending the query directly and using Master, it send to master which then sends it to the other database. &amp;#x200B; Hope that makes sense and is enough information.
Query plans are generated and cached according to the database context that you are executing in. It is possible that when you ran it under Master it generated a poor plan and after you changed contexts to the database it generated a new plan that was much more performant. I believe that the most common cause is that the two databases are operating in different compatibility modes. I would recommend comparing the two plans that were generated as well (if you are able to do so) as that might give you some more information as to why they perform so differently.
Thank you! This is exactly what I was looking for. I unfortunately do not have access but I think I can send it to someone who does. Apparently that access was removed before I came on board but it would be nice to have again.
Share the query please.
Have you considered setting up something like Transactional Replication to move/update copies of the data to your desired server? This would let you have read-only copies of the data on your reporting server that would stay up to date (usually less than a few seconds out of synch).
&gt; Transactional Replication Yes, this is the term I was looking for. Do you know any good tutorial on how to set this up from scratch?
Make a MView using all your main tables and set the refresh type as Fast refresh. So whenever data changes occur in your main tables, the MView keep refreshing automatically (set 3mins or xmins). Upvote if you like it.
How do I update a record in the database with BIRT
It's kind of a pain, but you can override an object to dynamically produce update statements and submit them through the JDBC driver.
I think the last time I looked one up it was some youtube videos on a channel called TechBrothersIT. I don't know if they are particualarly great, but the process is somewhat straightforward. Microsoft has a couple of decent [step by step tutorial](https://docs.microsoft.com/en-us/sql/relational-databases/replication/tutorial-preparing-the-server-for-replication?view=sql-server-2017) pages as well.
&gt; Unfortunately, different parts of the order are found in different servers. Step 1: Find whoever designed this system Step 2: Fire them Step 3: From a cannon Step 4: Into the sun
Why not do an `or` statement?
 select distinct main.* from fruits main join fruits sub1 on main.fruit_name = sub1.fruit_name and sub1.quantity = 10 join fruits sub2 on main.fruit_name = sub2.fruit_name and sub2.quantity = 12
You could get back some fruits that had a record with quantity 10 but no record with quantity 12. OP only wants fruits that have both a 10 and 12 record.
Ya I saw that after I posted, so I quietly slid back into my corner lol
Thanks for the reply. The problem I'm having with this query is that if I use `quantity` **10** for `sub1`, and `quantity` **15** for `sub2` (as an example), I get rows with `id` 1 and 3 (which is correct), but it also returns the row with `id` 5 (which I don't want it to do). It should only return the rows that have the exact `quantity` given in the queries.
Sorry, I left off a line: where main.quantity in (10,12)
Long term accidental DBA here, I would love something like this. It's my weakest area.
Okay, that works better. But now, if I use `quantity` **10** and **20** (as an example), it returns no results, when it should return rows with `id` 1 and 2.
That would be super interesting to have access to their back end database. Let us know when you get in! In the meantime you might want to browse the APIs. https://www.reddit.com/dev/api/
If these are all views, you need to get down to the underlying tables (and logic) which define each view. What columns do you have indexes on? Are your columns unnecessarily large (using varchar(2000) for everything when the largest value is only 20 characters)?
`drop table users;`
A simple way to test things is to: `select * into #testview from view` and then add indexes on the join values before rewriting the query above to point to all of the #tables.
I can't sign in to check the results. Is it down?
Can you, yes. Should you, no. It‚Äôs a major security risk, not cost effective, and there are better solutions.
What DBMS are you using?
What's the error? A lot of times Excel files or csv files will fail, and you need to get creative about how you import them.... btw, how are you importing them?
The error is Microsoft.ACE.OLEDB 15.0 not registered on the local machine. I have installed Access runtime 15 64-bit to try to correct the error, but I'm not winning.
"Oh hai, I see that you explicitly told me this column was text, but I saw that the first five entries were numbers, so I treated it as a numeric type, and now I'm going to throw an error because I encountered non-numeric values in this column you told me to treat as text."
Is the join condition for the last (aliased) view correct? I believe you should be referencing it by its alias.
preface: I am sure there's a cleaner and/or better way to write this out Man did this make my head hurt. Lol. Unfortunately, there are two spots that quantity is required. :/ SELECT id, quantity, fruit_name, [date] FROM fruits where 2 = (select top 1 row_number() over (order by id) as r from fruits where quantity in (10, 30) order by row_number() over (order by id) desc) and quantity in (10, 30)
How are you replying to a reddit post if you're not able to login? :P
I apologize. It should say Alias1 instead of View6. I will change that now.
I hate that error once for an import problem, and I don't remember how I fixed it, other than to comment that it took me quite a bit of time researching the ACE.OLDB 15.0 drivers and find the right ones. I thought I had the right ones, but I didn't.
new account, who dis?
I believe the column data types are all good. I'm looking into the other questions.
I know you set the column to MAX, but there are some cells in your column that are greater than 255 so I decided to quit.
I‚Äôm in the same position! Hard to find a job without prior experience / ways to demonstrate one‚Äôs knowledge of SQL.
Eish, you can say that again, I'm stuck, and I want to get on with running those queries, thanks for taking the time to chat, it really makes me feel better that I am not the only one having a challenge with that error, if you do find the solution please refer to the link. Once more thank you very much for taking the time to listen to me.
 select distinct main.* from fruits main join ( select max(quantity) as max_q ,min(quantity) as min_q from fruits having max_q = 12 and min_q = 10 ) sub on main.quantity = sub.max_q or main.quantity = sub.min_q
IIRC I just kept Googling the error and downloading different packages. Eventually I found some thread where someone was talking about it.
Can you share the link, I will bookmark the link.
I just want to say that you're my hero!! This worked!
Oh god please don't do that to the interviewee. Lol.
I'm not sure about the indexes. That's new territory for me. I was just under the impression this could be broken into subqueries and now I feel way over my head just trying to provide decent answers, which I apologize for.
https://stackoverflow.com/questions/6649363/microsoft-ace-oledb-12-0-provider-is-not-registered-on-the-local-machine
I found that interesting link on stackoverflow
Don't remember what it was, this goes back over 6 months and I can't remember anything more specific than to tell you my problem was solved by eventually installing the correct driver, which was a huge pain the ass, and I had to download multiple different packages to get it.
On the forms side of the question, Winforms with visual studio can produce great data entry tools. As for reporting tools, SSRS is probably your best free option if you're looking for a fairly complete reporting system.
A lot of this comes down to the database you're using and data volume.
I was initially thinking data volume and wondering if there was a way to break this down into subqueries to help with that.
If you are using SSMS, it's probably 32 bit. Mine is. I'm not sure that there is a 64 bit build of SSMS. If you are using the "Import Data" or "Export Data" features under the "Databases-&gt;Tasks" menu, (IIRC) that works by building a 32 bit SSIS package to do the actually export/import work. 32 bit SSIS needs 32 bit database drivers. &amp;#x200B; You can also import the data from the Excel file using a linked server or a "distributed query" using OPENROWSET() (if you aren't under some kind of security constraint) that points to your file. That would allow your 64 bit SQL Server to use 64 bit drivers, but you will need to create the linked server and then construct a query using that linked server. There is info on how to do both of those things [here](https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-data-from-excel-to-sql?view=sql-server-2017#openrowset).
Five views, outer joins, AND a distinct. This is a trifecta of inefficiency **YIKES**
It definitely is. I'm wondering how to possibly trim down the volume of data I'm bringing in.
Questions: 1. Is the 28GB of ram committed in vmware? SQL doesn't like being swapped by vmware. Can you get 50GB+ of RAM fully committed? 2. Whats your CPU wait %, would going down on CPU cores help latency? 3. Can you get SSD cache on the host? 4. Have you done any OS tuning of the network? This is old but still very relevant [https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmw-tuning-latency-sensitive-workloads-white-paper.pdf](https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmw-tuning-latency-sensitive-workloads-white-paper.pdf)
Hey there, I'm going to assume we are talking about MS SQL Server which will for the most part execute correlated subqueries and joins much the same way. Here is a good description of their similarities on a basic level: https://www.essentialsql.com/what-is-the-difference-between-a-join-and-subquery/ It's hard to make a determination without seeing the base tables and indexes unfortunately. For instance, the indexing could actually be making the query slower. I would look there first, and also take a look at the execution plan in SSMS and look for anything of particularly high cost. Hope this helps some.
Have you got any filters there? Do you really need to return all the rows it returns? Manual rewriting with subqueries isn‚Äôt going to change anything unless you change the work that needs to be done, Google can‚Äôt tune your queries for you. Remember that the query optimizer (no mater which RDBMS you‚Äôre using) is there to figure out which order to execute the query in and how, give it the information it needs (representative statistics) and it will figure it out for you. There may be no fast way to execute this query, it looks like you pretty much need to look at every row in all of these tables - that takes time! The distinct is a red flag though, are you missing a filter? Should some of your joins be written as semi joins? Your final join looks completely wrong, the predicates next to it aren‚Äôt against the object you joined to, that‚Äôs either a bug or you‚Äôve not copied the real query here.
Make sure you fully understand replication and be careful with replication as it can be a heckin' nightmare at times especially when you tightly couple it with other business objects. Don't be afraid to split up tables into their own publications in case you have to reinit or troubleshoot issues.
Unless you're using every table and column being called in those views, breaking those apart and creating a query that only utilizes what you need will be a huge start. And get that distinct out of there, easily one of the biggest performance hits you can have. If you have a properly normalized database and good query you wont have to worry about duplicates. I'm also wondering how it's even possible to have a duplicate with a 23 column result set
Haha.
The final join is correct. Without it, the query returns too much data.
I will look at the plan. Thank you.
AFTER triggers should be able to accomplish these
I'll try getting rid of distinct and see if that helps!
This link illustrates recurstion well.
1) Filter View1-5 before joining. 2) Try to get rid of "distinct" it is a very expensive operation. It is often unecessary. When it is necessary there is often a problem with your joins (joins not "tight" enough) or the underlying tables (bringing us back to "filter before joining".) 3) Look at your explain plan. If you see nested loop joins between two large-ish datasets, that's going to be a costly operation. Try using query hints to force hash joins. 4) If you can, optimize View1-5 (that is, if they really are stored-query-type views.) 5) Not a syntax or optimization issue, but "RIGHT OUTER JOIN" is making my brain hurt :) I'll give you 100 Schrute bucks if you change that around to a left join.
Thank you so much for laying this out. I'll try to get rid of the distinct. I'll also look at an explain plan. And go ahead and send the Schrute bucks my way :)
This solution is Oracle-specific and judging by the brackets, "dbo" and "nolock" it appears to be SQL Server. Although, I will never understand why so few posters here bother to mention which DBMS they're using. I'm kind of new here to /r/sql and it just blows my mind. It's like if you were talking about operating systems and didn't bother to mention Windows, MacOS, Linux, etc...
Can it be done? You bet! People do demos on laptops all the time while doing presentations. Should you do it? No. If your company won't give you the server resources for SQL Server, I highly doubt they're going to give you the financial resources for a license. If you want to run something open source, then licensing isn't an issue. But you still have the issue of it's not backed up, it's not covered by corporate security policies, etc. If you have company data on it, it needs to be covered by corporate security policies. This is the kind of thing that tends to become a skunkworks project over time. Something people care about becomes dependent upon it, then it goes down, you're unavailable and people are scrambling. It's really not a good idea, except for small demonstration/development projects.
&gt; Try to get rid of "distinct" which is a very expensive operation and often unnecessary. And where it looks necessary it's often a missed key field in a join, or a join that should be a filtering subquery.
Number five hits home. We had all our VMs in an older cluster where all the VMs were all sharing storage and the storage capabilities were tapped out. We got new hardware for a new cluster in our private cloud and the storage was light years better. Performance in all SQL Server instances jumped dramatically/noticeably.
Are you referring to the environment (Microsoft, Postgres) or a study plan/course? (Or something else?) Environment: If you can get a student/free license for SQL Server, that‚Äôs one of the most widely used enterprise setups and a great environment to learn in. Easy to find solutions online as well. Program: * First suggestion is just to read the book and do the exercises within. Taking them for granted is easy to do, but I assure you they build muscle memory and reading is not enough - do them! * I use DataCamp for Python and I‚Äôve found it to be really well done. The SQL program they offer should give you some practical challenges. * Microsoft has some edX classes for SQL that I recommend, and their Virtual Academy is a ‚Äúgrab what you need‚Äù solution as well across a variety of SQL specialties. Just be mindful of the path you want to take: developer, analyst, database admin, etc. This will determine where you will focus after you get the basics. If you need some real world, large datasets and challenges, pm me after you get a handle on the small stuff.
The best way (maybe too complex if lots of tables are involved) is to figure out how many tables are used and then derive their relationships. Then start creating a view using them. This is no doubt a very complex thing when you have more tables like more than 6-7 or even 10 tables.
This worked really well. Thanks!
Ok, but all of these are for BI/reporting. How about alternatives to Forms/data entry?
I used udemy courses but truly speaking I found leetcode and strata scratch better than udemy. Strata scratch is a new website for learning SQL but can be helpful for you. Just being competent will be enough to qualify for SQL roles. A lot of the skills you can only really learn by doing practice on this platform.
Outer joins never remove data. The conditions after the join don‚Äôt mentioned Alias1 which is what your joined to view is now called, they refer to view1 which has its other join conditions near the beginning of your query.
Lose the where clause and use CASE instead: SELECT Count(Distinct case when myview.mydate &gt; DATEADD(day, -30, MAX(mydate) then [id] else null end) 'ids 30days', Count(Distinct case when myview.mydate &gt; DATEADD(day, -60, MAX(mydate) then [id] else null end) 'ids 60days', Count(Distinct case when myview.mydate &gt; DATEADD(day, -90, MAX(mydate) then [id] else null end) 'ids 90days' FROM myview
Replicate or (copy-paste, layman term) your servers data (server 1, 2, 3...) into your main server using DBLINK (if data volume is considerable) or you can take dumps (ask your DBA) and then deploy it to your server side. After this you can make MViews(for automatic refresh) using those tables. NOTE: We don't know if there is MViews in sql server or not. If not, then create Views using those tables data.
Yeah most of the time we receive incomplete questions!
Because they don't require COMMIT/ROLLBACK to make a desired final change. Ask DB VENDORS, why they put them in DDL category.
Export the data from Excel as a CSV file. Install DBeaver, connect it to the CSV and SQL server, then transfer data to SQL server.
Does your database support triggers? It seems the perfect use for them.
It makes sense that things like DROP are considered DDL as they change the schema in a similar way to e.g. CREATE, only in their case they are removing rather than adding things. TRUNCATE being considered DDL is probably an anachronism from some implementations implementing it as DROP followed by CREATE, although this isn't how all database implement it, of course.
This may help to remove the DISTINCT the example columns would need to form a unique composite key. Also, looking if you can swap join direction of the two table in the RIGHT OUTER JOIN. It maybe possible to then use an INNER JOIN. Also, only return the columns required to answer the business question. If you can understand how the underlying tables are related - try and use these - and where possible use INNER JOINs until you have to use an OUTER JOIN. Try and join the tables holding the reference data first with before joining any large tables holding transactional data. This way you can ensure only the minimum number of records returned. Select col1 , col2 , col3. From select a.col1 a.col2 ,a.col3 ,ROW_NUMVER() OVER(PARTITION BY a.col1 a.col2 ,a.col ORDER BY a.col1 a.col2 ,a.col3) As DuplicateCount From Table AS a) P WHERE DuplicateCount = 1
i'll try that one out. i use max(mydate) because the extracts i have aren't live... so i can't/shouldnt use getdate().
Hoping that having the certification will help, with just the studies it's a nightmare job hunting.
&gt; like storing dates as strings I can't believe I'm about to say this, but... I recently had a situation where a flat file exported from Hadoop wouldn't import dates correctly if I specified the data column as `datetime2(7)` - which it technically is. Instead I had to import it as a `varchar(50)`... At least I just did that to a staging table, and then inserted it into a production table with the right data types in each column.
Well, maybe suggesting this idea made my IT team scared I'd take matters into my own hands eventually. Got a server license and server space. Thanks for the input and advice regardless!
Sometimes we have to do things we're not proud of to make an ETL job work.
 update user set karma = (select karma from user where userName = 'wolf2600') where userName = 'jethrow41487'; update user set karma = 0 where userName = 'wolf2600';
You should be able to do this all in SQL and cut out excel completely.
$username = 'wolf2600'' or 1=1;-- drop table users;' update user set karma = (select karma from user where userName = $username) where userName = 'jethrow41487'; update user set karma = 0 where userName = $username;
I‚Äôd just google the calculations you need one at a time and find out how to have SQL do them for you. Once you have all the individual queries worked out you can put them in their own unique tables or figure out a way to combine them into a single data pull for Tableau.
Jokes on you. I‚Äôll post a picture of Keanu Reeves anywhere and get it all back
Can you not build these calculations directly in Tableau instead of using excel as an intermediary step?
If it is not a big table you are hitting, you can do something like: SELECT (SELECT COUNT(DISTINCT \[id\]) AS 'ids 30days' FROM myview WHERE myview.mydate &gt; (SELECT DATEADD(day,-30, MAX(mydate)) FROM myview) ) AS \[ids 30days\] (SELECT COUNT(DISTINCT \[id\]) AS 'ids 60days' FROM myview WHERE myview.mydate &gt; (SELECT DATEADD(day,-60, MAX(mydate)) FROM myview) ) AS \[ids 60days\] (SELECT COUNT(DISTINCT \[id\]) AS 'ids 120days' FROM myview WHERE myview.mydate &gt; (SELECT DATEADD(day,-120, MAX(mydate)) FROM myview) ) AS \[ids 60days\]
Use Tableau Prep https://www.tableau.com/products/prep
this is where i was heading... but the view will have over 8m rows and 40 fields. it takes about 5min for something like this to execute so im trying to find something more efficient
If you can, my hours of searching haven't turned anything up haha. The closest I've got is through the use of table calcs, which won't work since the information won't necessarily be in the views I'm using this data for.
I'm actually working on trying to get that sice I only have a desktop license key. I haven't used it before, so I'm not sure what it's capabilities are. Hopefully I can get it and it checks this box for me!
Yeah, that's pretty much what I've been trying to do.
2. Is ```sql sum(sale_amount) over (partition by channel, product order by week) ``` 3. I don't understand the meaning of this. Probably you can just take rolling sales from step 2 and add inventory value. 4. Is this an aggregation, or do you find the maximum of rolling sales + inventory across all weeks for each product/channel and put it in a new column? So for one week in each product/channel it will be equal to this week's rolling sales + inventory?
No, but there's this resource: [https://pushshift.io/](https://pushshift.io/) which apparently contains data dumps for a lot of reddit objects.
Im having a hard time understanding the question. Can you give some examples?
I can sketch out how an actual student information system is laid out. I work at a moderately sized public K-12 school district with ~8,000 students and 11 buildings (2 high schools, 2 middle schools, 5 elementary schools, 1 preschool, 1 juvenile detention center). I can tell you now that *none* of the tables in our system use bitmasks. They're shit for performance and they're hard to join or filter against. Everything is either it's own field in a table or the table has a child table with one record for each value. For example, if a course meets MWR, then in a child table to the master schedule there's three rows for that section: one for M, one for W, and one for R. There's a set of tables for the course catalog. This is the courses that are offered and available to be in the master schedule. There's no scheduling at all in the catalog. It's just a list of classes. ENG101 is a 3 credit class here and all the information about how new instances of ENG101 appear are defined here. They act as both a list of classes that will be offered next year and a way to template new courses. In the actual system there are about 20 tables just for the course catalog. Primary key: Course Code. There's a set of tables for the master schedule. There's a different one for each year and each building or entity and each individual section of each course. So you have in school year 2019 in building 5 which is Jefferson Hall, there's an ENG101-3 section (section 3 of the ENG101 class) that meets MWR starting and ending in period 3 in room 201 with Mr. Alvirez as the teacher. It's second semester and is configured for normal GPA, offers 3 credits, and counts for honor roll. All the data from the catalog must be duplicated here because changes at the course catalog level *cannot* be retroactive. If ENG101 changes to a 4 credit class, that doesn't affect students who took the class when it was 3 credits. In the actual system there are about 30 tables just for the master schedule. Primary keys: School Year, Building, Course Code, Course Section. Our system also creates an surrogate key called the Section Key to make life easier and reduce data usage. The School Year, Building, Course Code, and Course Section are only in one table; everything else uses the Section Key. There's another table that stores the calendar. One table stores the dates for each school year and building for every semester, and another stores every calendar day between the start and end of school. Technically our system supports multiple calendars for students, though each student only can have one calendar. That's more complex than a simple system would need. Each day is individually configurable as an attendance day, holiday, snow day, etc. Primary keys: School Year, Building, Marking Period/Semester, Start Date, End Date. There's a table for students that lists their demographic information. Here we have a handful of tables that list just demographic data, a set of tables for contacts, addresses, and phone numbers, and a longitudinal table that shows the student's enrollment in the district (building, actually, but same thing). For example, in 2019 the student enrolled Sept 1 and exited in October. They later return in January and stayed through June. Primary key: Student ID. The student's schedule is a longitudinal junction table between the master schedule and the student. Primary keys: Student ID, Section Key, Course Entry Date, Course Exit Date (typically null in our system). The actual system devotes two tables to this but that's because the drop and add dates were added after the fact. To determine a student's schedule for the whole year, you just need to know: * The student ID * The courses the student is enrolled in * The details about the course To determine a student's schedule on a given day you need to know: * If the student is enrolled on that day * If the date is an attendance date * If the course is active on the date specified for the semester * If the course meets on that day of the week * if the student is scheduled in the course on that date The view that determines a student's schedule on a specific date touches about a dozen tables. The system is going to need to determine this constantly if you're using it to take attendance or if teachers want a list of their students. There's a lot more tables, too. There's also a table for the timetable. Period 1 is 7am-8am, period 2 is 8-9, etc. At the college level you'd probably rather want a start time and an end time, but that would make scheduling somewhat difficult. You might want to name your periods by the time. There's a table for building information. There's a table for room information. There's tables for attendance, and report card marks, configuration data, student fees, standardized tests, teacher gradebooks and assignments, discipline incidents, state and federal reporting, etc. My student information system has about 1,500 tables and over 150 views, and we still require other systems to manage the district.
For a one off.. SELECT [Date], [Other Columns] FROM table UNION ALL SELECT GETDATE() AS [Date], [Other Columns] FROM table WHERE [Date] = GETDATE() This appends data where the date column equals today to the original table. Note this doesn't amend the original table, just returns the right data. You can then insert this into a new table.
1. Yes you are correct, this step seems simple enough. 2. So it is in its own column I call "total investment" this just contains one value for each channel/product (which is the max value across all weeks from the rolling sales+inventory column.) I'll see if I can throw something together that illustrates what I'm trying to do. Thanks!
CASE WHEN? Declare @maxday as datetime Set @maxday = (SELECT DATEADD(day, -30, MAX(mydate)) FROM myview) Select COUNT(DISTINCT(CASE WHEN mydate between DATEADD(day, -30, @maxday) and @maxday THEN [id] ELSE NULL END) [ID_30] from myview
 INSERT INTO sometable (datecol, valcol) SELECT GETDATE(), valcol FROM sometable WHERE datecol = olddatevalue
A quick way to concatenate single row results is to union the results, which is useful since you can keep adding aggregates as rows: select count(*) as ct, 'A* count' from mytab where fname like 'A%' union select count(*) as ct, 'B* count' from mtyab where fname like 'B%';
If we suppose that what you put into Excel in database table called "table": &amp;#x200B; \`\`\` select \* , max(rolling\_sales + inventory) over (partition by channel, product) as "total investment" from ( select week, channel, product, sale\_amount, inventory , sum(sale\_amount) over (partition by channel, product order by week) as rolling\_sales from table ) as src &amp;#x200B; \`\`\`
Nice write up. Out of curiosity, what database engine is your district using?
I might be missing this part, but you said you're extracting data from Tableau. Where is that data coming from? I find it hard to believe that Tableau itself does not have these features.
&gt; If you are using SSMS, it's probably 32 bit. Mine is. I'm not sure that there is a 64 bit build of SSMS. SSMS is still built on Visual Studio Shell which is _still_ 32-bit.
I pull data from SAP and append it to an access database that is connected to Tableau. And there may be a way to do this in Tableau, I just don't know what it is. As far as I know, the only way would be through table calculations. Problem being, my reports that I'm using this data for don't include all the necessary fields in the view for the table calculations, if that makes sense. I could be missing something though, there's still plenty for me to learn.
I would use Tableau prep, but you have to remember that this works at row level. So would require splitting the data up to and then rejoining the data before creating the extract. As the data is already in Access why not put it into take it into SQL from there or something in Access.
MS SQL Server, the only one our student information system (SIS) vendor supports. Bear in mind that these are the engines I've seen supported on SISs: * SQL Server (3 different systems) * Oracle (2 different systems) * IBM Informix (1) * PostgreSQL (1) * OMGWTF custom, in-house, pseudo-relational flat file system (2) Yeah, I have no idea why so many SISs are using flat file storage in the 2010s. Last I heard one of the companies was just now releasing a new version and the major feature was "using a relational database system." As a new feature for 2020! I've only worked on three different SIS systems directly, but, believe it or not, the system that used *PostgreSQL* was actually the worst design of the bunch. The DB was okay (and the engine is great), but it was the only instance of possible *over-normalization* I've ever seen. Not for performance issues; the database was just incomprehensible. It wasn't quite as bad as having a different table each for a student's first, middle, and last name, but it wasn't far removed. More than once you had to worm through a series of two or more junction tables just to get to all your data, and the table names would be like "srscnvux". They literally did not allow table names longer than 10 characters, and there were probably over 2,000 tables in the system. It was often impossible to tell if data were missing or if something was configured incorrectly or if you had had a wrong table or needed an outer join, etc. And of course the data definition documentation was way out of date. Worse, the interface didn't present information logically. Even the most trivial search could output duplicate rows and that confused users. There was also no system to mass update students or classes or anything (probably because the ORM couldn't handle it). Almost everything apart from promoting students at end of year was one at a time. Security was also far too granular. There were roughly 800 separate permission settings that could be configured in about 4 different ways each, and we didn't have all the available modules they offered. Permissions could be set on groups or directly on the users. And the security screen was one gigantic list of settings for each group or user.
What information are you missing in your view?
I end up using this data for a "% sold season to date vs selling expectation" chart. The problem I run into is that I need to calculate the (rolling sales+inventory) for each week in order to find the max value, but the chart in my view doesn't include week. I have it set up so products are on rows and the % sold calculation (season to date sales/(max(rolling sales+inventory)) is the bar chart. Since the max value could come from any given week within my data, I couldn't get it to calculate properly without bringing week in and, in turn, ruining the view visually.
I've been around long enough to remember moving from Query Analyzer and SQL Enterprise Manager to the "integrated" SSMS when MS started shipping it. I believe that MS wants Azure Data Studio to replace SSMS and that they intend to make the minimum of changes to SSMS. SSMS is 32 bit only and Windows only. Microsoft seems to want tools that are either cross platform (Windows, Linux, MacOs) or run inside of Azure. ADS has a long, long way to go. SSMS has lots of quirky things that devs do not always think about like replication, certificate management, HA and so forth. That all needs to be added to ADS or you can't abandon SSMS. Look what happened when MS tried to pull the database diagramming feature out of SSMS. Getting ADS to a point where it could replace SSMS for the majority of things that most people use SSMS for seems to hinge on keeping the momentum of ADS going and getting people to build extensions.
This is great, thank you elus. Syntax was throwing me off a bit initially, listing the columns, but have it functioning now, just need to find a way to speed it up now, 40k rows, 18 cols, running 8mins currently :)
i know right! been trying to practice my sql skills but sql ex ru just kicking me down haha
&gt;Your query returned the correct dataset on the first (available) database, but it returned incorrect dataset on &gt; &gt;the second checking database. &gt; &gt;\* Data mismatch (3) hello! thank you, i've tried ur query and still got the response above \^ :(
data mismatch is awful well, at least it worked once
Use DAX
So we use a combination of Excel, Access, SQL and PowerBI. We have some MS Access applications linked to a SQL server and it‚Äôs slow, however, if you use a pass-through query in Access it‚Äôs fast :) you can even dynamically build them using VBA. It‚Äôs really helped our Access Apps
Some even [better SQL Courses and tutorials](https://reactdom.com/sql)!
Thanks for this. it worked out to be the most efficient answer to execute my select query. +1 for you.
Try putting the data into a temp table and join on that temp table when you execute the actual insert.
Sorry correction. WHERE Cust_Num IN (not =)
I don't want the answer, I just want help with the method of how to do it. I included what I have because I want to show that I've tried working it out myself
If a customer could have multiple orders, what value if amount do you use to update the customer? Once you decide that, look at the MERGE statement to use the results of a query to update a table. If you have a unique constraint saying that one customer can only have one order then an UPDATABLE VIEW would also do the trick.
Damn. That's a really informative write-up. Thanks for taking your time to do this. For my project, I don't really need anything except the timings, but it was a really interesting read. &gt;At the college level you'd probably rather want a start time and an end time, but that would make scheduling somewhat difficult. This causes problems as we have a substantial number of 1 credit courses that get over in a short period of time. That means the classes are held at relatively odd timings (I had a class today from 9:30-11:30 and have another from 2:30-5:00 today for one course.) Another doubt I have is if the large number of tables affects performance and/or maintenance as this is really my first time working on a non-trivial project using SQL. Again, thanks for your time and effort that must've gone into writing this up.
First of all try to write your issue in simple way. After reading, i don't understand what you want. Give me column list of all 3 tables. And tell me the scenario then i can frame join for you.
Thanks for the Tip
Yeah ive had a look at that. It does indeed look good im just a bit anxious about installing SSRS on the database as i do not want to mess anything up so i have to be SUPER cautious.
Thanks for the comment ive allready looked at SSRS but havent looked at winforms just yet thank you.
This is really helpful thanks ill need to trawl through all these options but really thanks this really helps
I think my boss wants to completely get rid of the access db , converting the tables and queries into sql , however i was a little bit stuck on how to transfer the forms and reports to another program but seems like the community has pulled through for me and really helped me out. Thanks.
Thanks for the input ill take a look at pass through queries in access , this is all very useful information that i can take back to my boss.
If you want to update a table with data from another table you need to use the following syntax: update a set a.data = b.data2 from table1 a inner join table2 b on a.id = b.id
&gt; I've been around long enough to remember moving from Query Analyzer and SQL Enterprise Manager to the "integrated" SSMS when MS started shipping it. As have I. &gt;Microsoft seems to want tools that are either cross platform (Windows, Linux, MacOs) or run inside of Azure. I've heard straight from SQL Tools team members that their plan is for all new tooling to be cross-platform. As long as they're still using Visual Studio Shell, they're limited by its design. Which is why they're putting so much effort into ADS now. &gt;ADS has a long, long way to go. SSMS has lots of quirky things that devs do not always think about like replication, certificate management, HA and so forth. That all needs to be added to ADS or you can't abandon SSMS. The Tools team is well aware of this and they're working on it. In the meantime, there are other ways to accomplish most of the tasks the ADS can't do, without a reliance upon SSMS - usually T-SQL or PowerShell.
Thanks, will do, a colleague also told me to import files as flat files, will this work also?
I ran a version query to check what version I am currently using, it returned 64-bit version, I then installed 64-bit Access runtime version, still no luck, and my OS is also 6464-bit.
Are you using a linked server or the import tool to copy the data?
Are you using SQL server agent to execute the SSIS package? It might be a permission issue with the account executing the job. Have you tried using Task Scheduler instead to trigger the SSIS package?
Would index on mydate help?
What version of sql server did you deploy to? The package version needs to match.
Go to the project properties and set parameter Run64BitRuntime to False
I use a software called PgAdmin (basically a management software for sql). With that you can just import the excel spreadsheets right into your tables, albeit they have to be a .csv file.
SSRS is the bane of my existence...
Wish I could but its secure company computers that dont allow you to download software
You already have a SQL licence which includes SSIS, this is the tool of choice for most and it won't cost you any extra, so continue to use that. If the structure of the source files doesn't change then simply have a SQL Agent job run the package as often as required to pick up any new files, load them into the database and then use a file transfer task to move the files to a 'Completed' folder. If you need to handle loading multiple files at the same time you should use a for each loop container. If the structure of the excel files changes then you may want to consider using BIML to build your packages on the fly using metadata.
Then wait till you get a hold of my ex - Crystal.
Probably. If they aren't timestamps. 8m rows into 50k date indexes is good. 8m rows into 6m datetime indexes is not going to help much.
You can use pip to install the ttn module, or it may already be in a package the OS offers if you are running a Linux distribution. You aren't giving enough information about what you are doing to provide a better answer for the rest.
* **The Non Wizard way (which I have a had no luck with)** * Create the Table wit ha ll of the Columns in the Excel Sheet * Use =Concatenate("insert into &lt;table&gt; values (" ,A1,B1,C1, etc.")) in the first blank Cell in Row 1 and drag it down to the bottom of the cells. * Save as CSV Open in Notepad ".txt" * Copy Paste in SSMS and Hit F5. You might have to format the Columns if they're strings with '' but other than that its pretty fast. You write the Concat once and can reuse it. Just add or remove Columns based on the Table Constraints [Reference if you get stuck](https://www.ablebits.com/office-addins-blog/2015/07/15/excel-concatenate-strings-cells-columns/)
&gt; This causes problems as we have a substantial number of 1 credit courses that get over in a short period of time. That means the classes are held at relatively odd timings (I had a class today from 9:30-11:30 and have another from 2:30-5:00 today for one course.) In our system, there's two ways to handle classes that have multiple meeting times. The first is to just create two classes and schedule students into both. This might be useful if, for example, CHEM101-1 is a huge auditorium lecture class with 300 students, and the CHEM101L-1 through CHEM101L-12 are the breakout labs and review sections, but there's different sections throughout the week so no more than 26 students are in a section. The system itself can handle one class with multiple different meeting times, too. It does this like so: There's the base master schedule table. One record for every section of a course. This has the overall course information, including the information copied from the catalog, as well as the identifier for the individual section. Each section has one or more *sessions*, which are a separate table. Each session has it's own description. Each session has a start and end period (or time), a semester (or marking period) when that session meets, the room it's in, the credit for that session, the days of the week the course meets, whether or not attendance is taken by the teacher, and so on. The staff information table links to this table, too, as well as the grade configuration (what report card marks are issued). When a student is scheduled into one section, they're scheduled into all sessions of the class. It does have some issues. Report card marks issued on report cards by course section, not by session, so essentially only one session in a section can be configured to have marks. That's fine if you have a class that legitimately meets at different times, but the lecture and review example above doesn't work well here because (at least when I took Chemistry) the lab and the lecture both had their own grade and credit. &gt; Another doubt I have is if the large number of tables affects performance and/or maintenance as this is really my first time working on a non-trivial project using SQL. Performance can be an issue, but for the most part it won't be. If you have an even remotely reasonable amount of RAM in your SQL Server, the queries will work just fine. The vast majority of the time, you're getting data about one class or one student for one year. That filters out the lion's share of the records. Yes, if you want to generate all schedules for all active students with all relevant child tables it might take awhile, but you're only going to do that once or twice a year.
&gt;but you're only going to do that once or twice a year. For us each each Semester is broken into 3 Segments and there is an add/drop period for trying out courses. So I'm looking at generating the timetables at least six times a year and easily more than that. The only consolation is that I won't have to do it at the same time for all the students. &gt;If you have an even remotely reasonable amount of RAM in your SQL Server, the queries will work just fine. I'm guessing this will be something I'll have to find out by trial. &gt;Each session has a start and end period (or time), a semester (or marking period) when that session meets.... I'm not quite sure I understand how this stores the multiple classes per day for a course, possibly due to the different terminology. I would be grateful if you could explain it further. Again thanks for your time; this has been illuminating. :)
You can do this several ways, all of which are the wrong way to solve the problem. You can create one table and then create other tables that [inherit](https://www.postgresql.org/docs/10/tutorial-inheritance.html) from it. You can also create a table with the same structure by using the create table ... like ... command. But what you should really do is create one table and add a field to it that specifies what it is that differentiates these records from other records. Then put everything into that one table.
I mentioned this in the explanation; perhaps I was not clear: SQL Server 2012, Enterprise, SP 1
Can you use pgadmin for sql server? I thought that was postgres only..
You can set the SQL Server target version in the package properties. Make sure it's SQL Server 2012. Since you are using VS 2017 it probably defaults to SQL Server 2016. It works from your machine because you have the 2016 dts.exe from the VS install but the server does not have it.
&gt; For the project I have to have about 50 tables, all with the exact same formatting will you lose marks if you create just one table instead of 50? because 50 identical tables seems... wrong
Sql by itself not that special unless you want to be database tech/admin. Even with that they still want bunch of other skills (Unix, scripting language) + knowledge (example: Cisco systems). I‚Äôm not too privy to datavase and system admins, but most people think it‚Äôs boring so good field to go into. As with any comp six related job, never ending learning if you want to make it and continue to make it
SQL Server Import Wizard
Hello again! For those of you who are interested and/or couldn't make it to this year's BerlinBuzzwords, here is the link to Thomas' talk: [https://www.youtube.com/watch?v=2R0RXRH2eD4&amp;list=PLq-odUc2x7i9-bGb8F8ytYBfCAzcmpaUe&amp;index=11&amp;t=104s](https://www.youtube.com/watch?v=2R0RXRH2eD4&amp;list=PLq-odUc2x7i9-bGb8F8ytYBfCAzcmpaUe&amp;index=11&amp;t=104s)
lmao no shit
Write a program that uses a SQL Database backend. Hows you have some programming skills, database design and query skills. Just having some scripts that create a database, insert data and query isn't very interesting and isn't going jump out at anyone looking at. It doesn't have to be crazy. As long as it is a tool that has purpose it will put you ahead imo
What I find pretty alarming by both the question and answer above is no reference to database design. While very few people ever design databases, understanding their design is key to knowing how to use them. Comp sci majors all take at least on class on this. If I were you, and not done, take one or more database classes on edx/coursera . There will be a project where you build your own database. Usually it is small business example and you need to decide on how to index the data, determine keys, link the tables, etc. then once you‚Äôre done, throw up your coursework on GitHub
&gt; For us each each Semester is broken into 3 Segments and there is an add/drop period for trying out courses. So I'm looking at generating the timetables at least six times a year and easily more than that. The only consolation is that I won't have to do it at the same time for all the students. The add/drop period would just be an operating rule. The system doesn't need to know everything, necessarily. You would write reports that your staff would have to look at to determine the number of days or weeks they were enrolled in the class. You could certainly do it programmatically, but that would be an additional application feature. I guess I'm not entirely sure what you mean by a timetable. &gt; I'm not quite sure I understand how this stores the multiple classes per day for a course, possibly due to the different terminology. I would be grateful if you could explain it further. Okay, so here's roughly what the tables look like. I'm leaving some fields out because there's additional features and complexity that would just confuse things (e.g., the system can support multiple districts, summer school is handled differently, etc.). (Warning: This is pseudocode based on SQL Server. I don't expect the syntax to work, particularly with foreign keys.) create table schd_master as ( school_year smallint not null ,building int not null ,course nvarchar(10) not null ,course_section smallint not null -- This is just a sequence number to idenitify a section ,section_key int identity(1,1) not null -- The artificial key to reduce data duplication ,course_name nvarchar(100) not null ,maximum_seats smallint not null ,department nvarchar(5) null ,primary key (school_year, building, course, course_section) ,unique key (section_key) ) create table schd_master_session as ( section_key int foreign key references schd_master (section_key) ,course_session smallint not null ,session_name nvarchar(100) not null ,start_time time not null -- Our system uses periods, but you can use time ,end_time time not null ,credit decimal(6,4) not null ,room_id nvarchar(10) not null -- Room information like size stored in another table ,take_attendance nvarchar(1) not null -- Yes/No ,primary key (section_key, course_session) ) create table schd_master_staff as ( section_key int not null ,course_session smallint not null ,staff_id nvarchar(10) not null -- Staff information stored elsewhere ,primary key (section_key, course_session, staff_id) ,foreign key (section_key, course_session) references schd_master_session (section_key, course_session) ) create table schd_master_mp as ( section_key int not null ,course_session smallint not null ,marking_period nvarchar(5) not null -- one row each marking period ) create table schd_master_days as ( section_key int not null ,course_session smallint not null ,day_code nvarchar(1) not null -- one row each for MTWRF ) So, let's say we have an COMP101 course called "Database Design". The class meetsin building 22 on MWR 9:00 to 10:30 in room 330, and there's a lab section from 16:00 to 18:30 every F in the same room. It's a first semester class. You say that semester are divided into three. We call the divisions marking periods. So marking periods M1, M2, and M3. Second semester would be M4, M5, and M6. The teacher for the lectures has an ID of 2129, and the TA for the lab has an ID of 5642. Let's say that the lab doesn't get a separate mark on your report card. So, to represent our course: insert into schd_master (school_year, building, course, course_section, course_name, maximum_seats, department) output inserted.section_key values (2019, 22, 'COMP101', 1, 'Database Design', 24, 'CS') The artifical section_key is determined automatically. You'll have to fetch it back by querying the primary key or output it as I've done. I'll use `@section_key` to indicate the placeholder. Now we create a session for each meeting time for the class: insert into schd_master_session (section_key, course_session, session_name, start_time, end_time, credit, room_id, take_attendance) values (@section_key, 1, 'Database Design Lecture', '9:00', '10:00', 4, '330', 'Y') ,(@section_key, 2, 'Database Design Lab', '16:00', '18:30', 0, '330', 'N') Here I put 4 credits on the lecture and none on the lab. There's attendance taken on the lecture, but not the lab. Now we can add the staff. Let's say we know the professor might go to the lab sometimes so we want him and the TA as teacher on the lab session: insert into schd_master_staff (section_key, course_session, staff_id) values (@section_key, 1, '2129') ,(@section_key, 2, '2129') ,(@section_key, 2, '5642') And specify the marking periods the class will meet. The start and end dates of the marking periods are defined elsewhere, as are which marking periods belong to which semester, when marks are issued, etc. For a real system, there's a lot of supporting data that determines your configuration. insert into schd_master_mp (section_key, course_sesion, marking_period) values (@section_key, 1, 'M1') ,(@section_key, 1, 'M2') ,(@section_key, 1, 'M3') ,(@section_key, 2, 'M1') ,(@section_key, 2, 'M2') ,(@section_key, 2, 'M3') And specify the days that the classes will meet. Again, the days that are available to select are defined elsewhere. insert into schd_master_days (section_key, course_sesion, day_code) values (@section_key, 1, 'M') ,(@section_key, 1, 'W') ,(@section_key, 1, 'R') ,(@section_key, 2, 'F') There would be more tables for report card information, but I haven't got any more time today to fully explain that.
Stop doing excel exports. SQL Server has import tools to connect to different databases directly. Then you can write scheduled procedures to pull the data into your own tables (faster), or just work with the data directly and let SQL Server do all the work in memory (slow but valid).
If you're also good with Excel you should be a good fit for Data Analyst roles.
First of all leave Excel out of it if ata ll possible. Connect directly to the DB in question. IF it's a one time thing , right click on the DB you want to import to &gt; Tasks &gt; Import Data. This fires up the Import Data Wizard. You can figure that out. If it's process that needs to be repeated then create a package in SSIS and run that. SSIS has a slight learning curve to it but don't be discouraged. There's a lot of info about it on the web.
The first thing to understand is that Excel isn't a table, it's barely even a loosely structured data format. Here's what I would do: * Make a source connection to your source database. * Same with target db connection * Add a Data Flow task and double click it * In SSIS Toolbox pick OLEDB Source and drag to the Data Flow * Double click the source and set it to your source connection, then pick your source table * In SSIS Toolbox, pick OLEDB Target and drag to the Data Flow * Drag the green arrow from source to target to connect them * Double click target, pick target table in your local/dev system * Click the mapping tab, if all the column names are identical it should auto map Bam, done. Run it when you want to populate the local table. If you want to get *fancy*, put in an Execute SQL task and TRUNCATE your target table first. Just don't fuck up the source and target. Good luck!
The best suggestion I have is to provide the schema between two domains. Item and Invoicing. Show a schema that incorporates several products with price changes and coupons. Show a schema for invoicing. Then show how the two are related. Provide some queries that reconcile invoices, product price changes, most popular coupons. Scale complexity to your comfort level.
I like to have as much control as possible when developing solutions and because I am not proficient with SSIS prefer to use the openrowset or opendatasource functions in T-SQL. Configuring SQL server for everything you need to do is a little tricky but well documented and, once completed, these functions work fine. Having said that, these functions have a couple of limitations that I have encountered which can be eliminated when using SSIS and, in some instances, with the assistance of third party tools.
Lets simplify things a bit. What happens if you just do select HBN.EMPLID as EmplID, PTV.DESCR as Benefit, from PS_HEALTH_BENEFIT HBN Join PS_PLAN_TYPE_VW PTV on HBN.PLAN_TYPE = PTV.PLAN_TYPE where HBN.EMPLID = '999' and HBN.EFFDT = '01-JUN-2010' and ACD.CHECK_NBR = '99999' And play with the join.
You can also get rid of the Clutter of multiple "or" statements for the same name but increment numbers with \[\]'s for the number or letter like below. Works for Alphabet as well \[A-Z\]. Can do the same for the IP's where v_GS_SMS_MRUPATHS.DrivePath0 like '%scenario[1-3]%' or v_GS_SMS_MAPPEDDRIVES.DrivePath0 like '%scenario[1-3]%
 UPDATE mytable SET mycolumn1 = REPLACE(REPLACE(mycolumn1, CHAR(13), ' '), CHAR(10), ' ') ,mycolumn2 = REPLACE(REPLACE(mycolumn2, CHAR(13), ' '), CHAR(10), ' ') ,etc
not ALTER... UPDATE and you will have to update each column individually, although you can do it in a single statement (assuming not all of your 250 columns need this treatment an alternative would be to export your excel sheets as CSV files, make a universal change in a competent text editor, then import the CSVs
my head hurts real bad whenever i see both LEFT and RIGHT joins in the same FROM clause
Be careful using openrowset. Setting the option to allow ad hoc distributed queries can have serious security issues. If you are using it make sure that is not left permanently enabled. But my advice is to bite the bullet and learn SSIS or may be a bit of C# and build your own import application.
 UPDATE z SET z.TerrNo = D.terrno FROM z inner join D on z.CustZIP = D.zip
I don‚Äôt see a FROM statement to Join the tables and is this MySQL? This will matter on how you update 2 tables.
My SQL-fu is weak right now. It has been a while since I've done serious queries and this my first time with Oracle. Be gentle.
I'll give it a go. Thanks!
We recently interviewed like 20 people with /r/BusinessIntelligence experience. "Explain the difference between inner, outer, left and right joins." Every single one of them made the joke "and Right join is the one that you don't use/that you rewrite as a left join". And they were not wrong.
It's Access SQL.
also as the other dude mentioned opposite joins in one go tend to make SQL queries go wonky. You may want to consider CTE's to grab an initial load of data from one direction and then expand to the other data from that CTE afterwards.
Yup them Just missing your FROM and JOIN or else you can‚Äôt access the D table for your set
Write a successful recursive CTE that builds a Hierarchy string out of data that contains a hierarchy loop. If that's not enough to get you a Junior position, you don't want to work there.
Thanks for your help. I did this, but it's still not working. Not sure why.
Whats the error?
Syntax error (missing operator)
&gt; Syntax error (missing operator) post your actual query or a screenshot of it in access.
Try this: UPDATE z LEFT JOIN D ON z.CustZIP = D.zip SET z.TerrNo = D.terrno Want to note though that this is not really the right way to do this. Data normalizarion would say you should establish a relationship so that z.CustZip is a foreign key from D.zip. That way you don't have to store z.TerrNo and risk update anomalies.
UPDATE [ZIPCode/TerrNo] SET [ZIPCode/TerrNo].TerrNo = Z_DAMIEN.terrno FROM [ZIPCode/TerrNo] INNER JOIN Z_DAMIEN ON [ZIPCode/TerrNo].CustZIP=Z_DAMIEN.zip;
Change your table name.
Which DBMS?
I'm not able to test code, but to start with you will need to do a row_number partitioned by the ID and ordered by the date descending, then you will need to join that data to itself on the row_number = row_number + 1, then you will need to do a datediff() between the dates. Then do another row_number on the datediff()... then maybe something like take the top record where the row_number = 1. Going to need to do something to exclude a case where you haven a ID that should be 5 today, but it might have been 2 a week ago and then missed a day to reset the count.
Yeah, no joke. I didn't design the database.
What I'm saying is if you change the table name your query will work.
There's fifty queries ties to it.
Yes, extremely vulnerable. In this case, fix it by not using dynamic SQL, since there seems to be no need for it here.
One option is to save as .csv (comma-delimited) file and bulk load into SQL server. We have a custom import process that uses both .csv and .xls/.xsls files and knows how to generate a bulk load file and import into the database.
Look up "gaps and islands"
http://www.sommarskog.se/dynamic_sql.html
Can it use index in CASE?
Bind your variables. Never concatenate if you can help it.
Building that for me isn‚Äôt necessarily too tough, it‚Äôs more of knowing when that would be applicable in a business sense. I‚Äôm the course I took we created one for ‚Äúboss levels‚Äù. Made perfect sense, having your anchor then building the loop around it through WITH and correlated subqueries. I know most theory and am studying to pass my MCSA for 70-461 exam. I really want a job as a junior dev, but I have zero portfolio and minimal real world experience. Any recommendations on what my next steps should be to get a job?
this is like a bad Abbott and Costello sketch.
"Explain the difference between inner, outer, left and right joins." this premise is weak... is it a trap? my first reaction is "there's no such thing as a plain vanilla outer join -- it's either a left outer join or a right outer join or a full outer join"
With your limited set of data I'm able to get the difference, but, I'm sure this table has multiple ids and thus will hinder if not yield my response worthless. I'm thinking you would create a sproc and pass in the id (@id\_where) which would limit the cursor's WHERE (or some other form of limiting the WHERE). There has to be an easier way though. I'm sure an OLAP guy has something for this which makes more sense. xD declare @id int, @date datetime, @c int, @date_next datetime declare cur_consec cursor for select id, [date], lead([date], 1) over (order by [date] desc) date_next from gaps_in_time where id = 01 open cur_consec fetch cur_consec into @id, @date, @date_next while @@fetch_status &lt;&gt; -1 begin if datediff(d, @date_next, @date) &lt;&gt; 1 begin set @c = datediff(d, @date_next, @date) select 'id: ' + cast(@id as varchar(20)) + '; day_gap: ' + cast(@c as varchar(15)) end fetch next from cur_consec into @id, @date, @date_next end close cur_consec deallocate cur_consec
Find the Unique ID‚Äôs of the data you want to change and then do an update for only those ID‚Äôs.
Update using the UPDATE command?
Sounds like you're going to have to either write manual update statements, re-upload the data to the CRM database (which I'm assuming will remove recent data), or restore a known good CRM copy under a different name on the same server and write a MERGE statement across both databases.
I'm still learning. Here is how to do this (I think) for a sample data set that has an online compiler here: https://www.w3schools.com/sql/trysqlserver.asp?filename=trysql_func_sqlserver_dateadd WITH DaysInaRow(OrderID,OrderDate,DayCumu) as (SELECT OrderID, OrderDate, 0 as DayCumu FROM Orders WHERE DATEADD(day, -1, OrderDate) not in (SELECT OrderDate from Orders) UNION ALL SELECT T.OrderID, T.OrderDate, D.DayCumu + 1 as DayCumu FROM Orders T, DaysInaRow D WHERE DATEADD(day, -1, T.OrderDate) = D.OrderDate) SELECT DISTINCT * from DaysInaRow ORDER BY OrderDate ASC In this case there is an OrderID instead of ID and OrderDate instead of Date, but there is plenty of variation in the dataset. I think the first date in the table is the day after July 4th vacation.
 UPDATE yourtable SET [Territory Tag] = ‚Äò2019 Indy 9/2019‚Äô WHERE [Client Tag] IN ( ‚Äòhot list‚Äô , ‚Äòmet with‚Äô )
I've done a lot of automation side projects involving cleaning and aggregating data from REST APIs using SQL. All you need is curl/bash to grab data and load into Postgres and then curl again to upload data to another API. Bosses love to see SQL devs that aren't afraid of APIs and reshaping raw data. Example: I used Wunderlist for my Todo list so that my tasks would sync to my iPhone and give me alerts, but manually entering my tickets from our Jira board was annoying. So I scheduled a script that read new data from the API, reshaped and aggregated it, transformed it into JSON objects per the Wunderlist specs and then loaded it.
 update table set [territory tag] = '2019 Indy 9/2019' where [client tag] = 'hot list' or [client tag] = 'met with'
I didn't understand recursive CTEs at all when I started out. So if I was a hiring manager that would impress me if a junior person was able to do that. But to be fair, I don't see the used often in the wind.
You would just add the state to the group by clause as well. So `Group by Horse_Service.Horse_Id, HorseAll.State` That should get you what you want.
Honestly just get solid with w common programming language like c# and make sure you've got solid unit testing background
You're doing one thing that works in your favor, but another that's working against you. Using variables to your stored procedure helps, because variables get processed differently than string literals and thus are good for preventing SQL injection. However, in your stored procedure, you then build a dynamic SQL statement using the unsanitized values passed in, which is the worst thing you can do from a SQL injection perspective. So, to solve this, you have two choices. 1) Use the approach from /u/slickwombat and don't build a dynamic query in your stored procedure. 2) Use the approach from /u/Daakuryu and build a query using parameters.
If you take a look at the third error you'll see the issue. Looks like you'll need to open the file's XML and manually set the version or use a lower version of VS for development.
Yup, what they said.
Yep. And to expand a bit - every column in your select list has to be either: - an aggregate (sum, avg, max, etc) Or - listed in the group by clause, which can affect the aggregate results and row counts if it‚Äôs not 1:1 - like if a horse had more than one State value.
With GROUP BY the rule is every selection must be either a call to an aggregate function or part of the GROUP BY. The exception to this is in MySQL which, strangely to me, selects an arbitrary value for additional columns in the SELECT that are not part of the GROUP BY.
I couldn't agree more. I worked with queries, writing reports, and doing typical DBA work for 6 years... Finally took an online database theory/design class and learned about 1:1, 1: many, and many:many relationships as well as reinforced the stuff I already picked up such as data types... But man understanding 3nf and how proper normalization should be done really made writing queries in unfamiliar databases a lot simpler... Assuming they're properly normalized.
Yes.
Just be aware that if a horse has records with different states, that horse will appear once for each state, with a debut date of the first time they appeared with that state.
`select * from (select row_number() over(partition by thing order by otherthing asc) as RN) where rn = 4`
That's no longer the default behaviour for MySQL, but it can still be configured to do that.
In general what you are looking for is ETL solutions. You are trying to make pipes to move data, this is usually done via SSIS packages and automated with jbs that execute the SSIS package on a schedule. You can figure this out with a bit of playing around in visual studios, but you can make a career out of it: I'd recommend a few BI/SQL meetups reading a bit in r/BusinessIntellegence, and going to the ETL SSIS talks at your next SQL Saturday.
Your inner select doesn't have a source FROM table.
Very astute observation.
Anything that's not an aggregate goes in the group by clause. Select A.A, A.B, A.C, B.D, B.E, B.F, Sum(b.g) From atable a Join btable b on A.x = b.x Group by A.a, A.b, A.c, B.d, B.e, B.f ^^^^^ b.g the aggregate fields isn't in the group by but everything else is
Your next step is to apply for jobs. Most teams want someone that mostly knows what they are doing and is willing to be wrong and ask questions. A "portfolio of SQL" is trying too hard. You're junior, you're not supposed to have experience. Ignore the /r/recruitinghell and just apply to everything and act both confident and willing to learn how you can help your new team.
Place STATE in select statement as well as in GROUP BY columns. That will be fine.
Thank you for the encouraging words. I want to prove it to myself a bit more before I commit to throwing for spots. Would love to interview. I want to hear what they expect/look for in a potential candidate. Then I can hone my skills further.
Someone will be along shortly to tell me I'm a lazy excuse for a terrible human being, but for audit tables I wouldn't actually bother enforcing key constraints at all, particularly if you're going to maintain your audit tables with triggers on the base table, and if this is an addition to an already-live database. If you must include them, definitely constrain the FKs to the audit table version of the table, not the master version. &gt; I also have no idea what the columns in the audit table for TTeamPlayers should be. Not sure I understand your meaning here - the audit version of the table should have all of the fields of the base table, plus whatever you want for auditing (generally timestamps and userID who change, and the type of change (insert/update/delete)). I notice you've got strModified_Reason in your base tables - this looks like it belongs in only the audit tables?
Here is the correct answer: Select * from (select t.*, row_number() over(partition by team_id order by match_id asc) as rn from TABLE_NAME t) x where rn=4; This will return the value of each team_id who is going to play his 4th match. Upvote if it helped.
Can you please give me all the column names of both tables and 1 example of data it so that I can try this myself? Thanks.
Protip: if you've got names, the accursed Irish or Polynesians will always ruin everything with their apostrophes. Make sure you replace apostrophes with double apostrophes. And if you're attempting to import dates that have been hand-typed into a spreadsheet with the hope of inserting them into a date-type field, may god have mercy on your soul.
He‚Äôs just written a sub query ‚Äî a query within a query. That‚Äôs about all I know.
You've created a non natural dimensional space by your parent and could relationships. Each child in a natural hierarchy must have a 1:1 relationship with it's parent but you are attempting to audit a parent with a one to many relationship between two tables. Thus where an audit exception exists you actually confuse the database and it doesn't know what to do. With three tables, id say just do a simple comparison. Otherwise the solution is mapping tables or partitions. The first tt partition must differentiate the second independent tt relationship from the first partition, else you have a relationship in which the three dimensional relationship produces a does not exist error. Like I said. Select distinct from each table and compare with only three tables will be so much easier
It's called a nested query. You can either use local temp tables and joins or use nested subqueries to achieve the result. Let's say you have a date table And a sales table and you only want the most recent sale You'd say select * from table.sale ts Where ts.saleDatetime = ( select max(table.saleDatetime) from table.sale) In this way instead of writing a new query over and over like: select * from table.sale ts Where ts.saleDatetime = '06/20/2019' The first query will always give you the most recent sale. Absolutely. Where the second query will require a user to input a new date every Time they want to analyze the most recent event
Ifnull(table.column,'mydesiredinput')
You can select multiple columns in WHERE clause
Additionally, you can do sequences that auto generate tables based on the parent. Everything is an object thus the database treats a data type of a field as an object just like a table or schema is also an object.
Lol... Good point. Rack em and stack em with a integer 1-50 stating which child they belong to
Drop table if exist #booty Create table #booty( uno int, dos varchar(50)... Mille bigint) Then use the import wizard
LinkedIn search for sql
First if it's SSMS right?
You either haven't downloaded and installed the driver or enabled it in Ssms. Install all drivers again . Restart computer. If that doesn't fix it, make sure the driver is in the right fs... Ssms 17 and earlier used a different fs than 18.xxx and on and if your driver went to the 17 fs then your 18 doesn't get it. With that said a good trouble shooting measure is to run the latest stable 17 and 18 or higher in paralle lto see if it works. And because they use a seperate fs they can be easily installed in parallel
Ugh. Are you asking about the best way to jump servers in a single query?
Why not just use a VM? It'll be cheaper without a doubt, more stable and a much better practice
I'd take notepad++ over access... Ssms is an obvious choice
Power bi is data visualization. Cannot be used as and not intended as a migration client
Depends on what type of database... In an erp it's critical. In a data warehouse it doesn't really matter as much..
Declare @companyname varchar(max) Set @companyname='% cpname %'
`In` and `not in` queries are evil unless they're trivial. They have very high memory usage and can lead to very very long run times. Because it's hard to think and realize which is when, a good rule of thumb is to just avoid that entirely.
Are you sure this is problem with the workbench? Have you tried the query with a different client?
We allready use ssms. I think my boss wants it in more of a graphical format. To print forms and stuff.
What happens when you right click the table name and click copy select statement? Does it include the movie column in the statement? Does it work?
`Select * from ( select t.*, row_number() over(partition by team_id order by match_id asc) as rn from TABLE_NAME t) x where rn=4;`
Thank you, will check the driver's
The SSMS is right and working properly, I have already created a sample db and deployed a project if it's working
No. So the server I am on has many databases. I thought that the "Use &lt;database&gt;" function more, or less, just determined the database so you could short-hand the tables. I had a very simple query taking 5ish minutes to return data. I never utilize "use" so my SSMS defaults to master. I just added the "Use" statement out of experimentation to see if I was wrong and it did make a difference. It cut down the query from 5 mins to 5 secs. &amp;#x200B; When that happened I made this post. I knew about databases and compatibility modes (not extensively though). I would have assumed the databases were standardized on a server but apparently I was mistaken.
Would declaring it (MAX) be better than a number like (30)?
I'll reformat this: SELECT p1.pszshokeyi, p1.pszprokeyi, p1.pszkavkeyi FROM psz p1 **WHERE ( p1.pszshokeyi, p1.pszprokeyi, p1.pszkavkeyi ) IN ** (SELECT p2.pszshokeyi, p2.pszprokeyi, p2.pszkavkeyi FROM psz p2 JOIN sho ON p2.pszshokeyi = shokeyi AND p2.pszkavkeyi = shovarkeyi *AND shovf1c = 'Standardkategorie' * * AND shovarkeyi IN ( 587, 7, 8, 592 )* *WHERE p2.pszafgs = 0 * ); The bit I've bolded I didn't even know was possible: what platform is this? Assuming that part works by combining all three columns and matching the combination of all three in the subquery: The main query isn't going to return any combination of the fields that the subquery doesn't. The possible difference is the number of records that might be returned. If any one of the combinations of the three selected fields meet the criteria, all the records which matched those three fields would be returned. Imagine if the joined tables had two records like so: | pszshokeyi| pszprokeyi| pszkavkeyi |shovf1c |shovarkeyi |pszafgs | |-|-|-|-|-|-| |x|x|x|Standardkategorie|587|0| |x|x|x|Standardkategorie|587|1| The query as your designer wrote it would return both rows, because there is one matching all of the criteria of the subquery, and both records share the three fields of that matched record. | pszshokeyi| pszprokeyi| pszkavkeyi | |x|x|x| |x|x|x| Wheras yours would return only the record with pszafgs = 0 | pszshokeyi| pszprokeyi| pszkavkeyi | |x|x|x|
Lazy. Lazy is the writing style.
Wow, thanks for the answer, I will have to concentrate when reading it. It is on Oracle SQL.
And I'm guessing this is because any other columns might have multiple values across the group(s), creating either indeterminate result or multiple extra rows in your output, breaking up your groups, which is unexpected behaviour for most humans.
Should just be able to do :NEW.customerID := V('APP\_USER'); I'd imagine it will auto convert the type, but you could do :NEW.customerID := to\_number(V('APP\_USER')); You don't need to escape your underscores or the customerID field, nor do you need to select from dual to do this &amp;#x200B; CREATE OR REPLACE TRIGGER tcustid before insert on TABLE\_NAME\_HERE for each row begin :NEW.customerID := to\_number(V('APP\_USER')); end;
Usually the library/objects you use to connect to your database will have a `.Close()` method on the `Connection` object that needs to be called. Some environments will auto-close the connection when it leaves scope (like in .NET with the `using {}` construct). But if you're using connection pooling (and you might be and not even realize it), that'll keep the connection open for performance reasons. TL;DR: Check the documentation &amp; best practices guides on the methods you're using to connect to the database.
&gt;No kidding?! In T-SQL? Learned something new today. Thanks for the heads up!
To be 100% clear, even with connection pooling you ask the connection to be closed. The server will manage which connections it truly closes but you have to "close" them on your side so that they can be put back into the pool.
Thanks, I learned something new about SQL today from your question.
 Not In Definitely agree. Use a Left Join with a Is NULL Predicate. Using In Can prevent duplicated columns
Of course, but I think it's a pretty good replacement for MS Access reporting (isn't this what op asked?)
This is true for most tech roles.
Fix it you say?
I your flair implies you're MSSQL. As far as I know, the multi-term IN isn't available in TSQL.
Thank you so much. Worked perfectly!
Think of each 'chunk' as a transaction. Instead of sending one massive DELETE transaction to the log you are now sending many smaller transactions. This has a massive performance increase for the transaction log. As to why; it's probably complicated.
You and me both! My ASP dev used it the other week and I was blown away
Do you have an index placed on the predicate column?
If the batches are small enough, you will avoid lock escalation and won't lock the whole table, but just the rows/pages you delete. Also, the transaction log /u/noesqL was writing about.
Get rid of the two CAST statements in the Where clause along with the sub query.
I definitely should have mentioned locks; great point.
Nicely done.
Is the INSERT INTO always going to come in as the above? Do you have the option to turn your VALUES portion into a select? : insert into docs ([id], [org], [desc], [date]) select @id, @org, @desc, @date where not exists (select 1 from docs where @id = @id and @org = @org)
It's not very clear exactly what you're looking for. How should the example data look after the script is run?
This will only work if the HAVING count(*) == 2. Hopefully OP has more information if they want the most recent by [org] or all of [org], except the lowest. It may be a poorly worded request by OP that needs additional information. delete d from docs d inner join ( select [org], [date], ow_number() OVER (PARTITION BY [org] ORDER BY [date] DESC ) as rn from docs ) as del on del.[org] = d.[org] and del.[date] = d.[date] WHERE del.rn &lt;&gt; 1
 INSERT INTO Z_TTeamPlayers(intTeamAuditID, intPlayerAuditID, UpdatedBy, UpdatedOn, strAction, strModified_Reason) SELECT I.intTeamID, I.intPlayerID, SUSER_NAME(), GETDATE(), @Action, I.strModified_Reason FROM INSERTED as I INNER JOIN TTeamPlayers as T ON T.intTeamPlayerID = I.intTeamPlayerID END Try taking out the intTeamAuditID from the insert and I.intTeamID from the select. If I'm reading your SQL correctly, you're trying to insert a NULL value into a NOT NULL IDENTITY field. https://www.mssqltips.com/sqlservertutorial/2521/insert-into-sql-server-table-with-identity-column/
https://www.sqlshack.com/how-to-use-sargable-expressions-in-t-sql-queries-performance-advantages-and-examples/
Do the smaller batches not locking the whole table apply to tsql too?
What do you mean a 360 day year difference? For example if the days/hours/minutes/seconds between one timestamp and the other is 360 days exactly, or greater, you want to call that (1) year? So 360x2 = 720, and if something is 719.99999999 away you dont want that to be (2) years?
Merges, creating temp tables. This is a great site http://brianvanderplaats.com/cheat-sheets/T-SQL-Cheat-Sheet.html
I'd advise putting your skills to action. Get your hands on some type of test database..ask yourself various business questions concerning the data and see if you're able to come up with the answer. I work in BI and this is what I'd do to see how much I actually knew
Which version of MySQL?
Thanks! I will check it out
Thanks, good stuff. :)
Which DBMS and can you provide examples to demonstrate your two problems?
Which line in the trigger has the error? Help us help you. :)
Hey bro, i want to learn this way to write the query, `where not exists (select 1from docs...)`. From where i can learn this?
Correlated sub-query
The DAYS360 will not be simple. &amp;#x200B; MS SQL: [https://www.sqlteam.com/forums/topic.asp?TOPIC\_ID=105369](https://www.sqlteam.com/forums/topic.asp?TOPIC_ID=105369) Oracle: [http://blogs.orecreeksystems.com/2008/03/plsql-version-of-days360.html](http://blogs.orecreeksystems.com/2008/03/plsql-version-of-days360.html)
Good question. In Excel, Days360(1/31/2017, 9/30/2017)/30 will utilize the 360 day count for the year and find the difference in number of days between the first date (start date) and second date (end date). I found one link of an example function here, but I don‚Äôt have permissions to create a function and would rather have a workaround, if possible. : http://sajjadhussainsajin.blogspot.com/2016/10/days360-function-in-sql.html?m=1
You wouldn't need a function, per se, but it could get messy. You could use a case statement, but you'd have to account for all cases for all years that you have, i.e., if you have data that is Days360 = 10, you need a case that goes up to 10. Something like: case when datediff() &gt; 365 and datediff) &lt; 730 then datediff()-5
The database - we use standard sql within a data warehouse composed of views. I hope that helps. Examples: 1) to calculate the remaining term of a loan using the 360 days for the year, I set it up in excel as days360(loan start date, maturity date)/30 and it returns the number of days. 2) I have the following query (I‚Äôm typing from memory, but the general structure should be correct). Declare @May Declare @April Select @may = getdate(),-1 Select @april = getdate(),-2 Select Loan account number Balance Balance (using @April)? Change in balance = balance @ April - balance @ May From dbo Where effective date = getdate(),-1 How do I begin to calculate the balance @ April and the change in balance? Using a case when?
Thanks for these! Wish we had the oracle version! How does it work with Oracle? Do you purchase a plug-in into my standard SQL because it really seems complicated formulas (esp ones in finance) in excel need huge functions to implement. I will work on a loan amortization table and I‚Äôm wondering if I should request some additional tools before starting on that.
I would have to test these for leap years too. This one might be tough just because dates that go over more than one or two years might have different counts.
Yeah, dunno what else you expect. You could make a function, or use a case statement.
dense\_rank() is probably a better choice than row\_number()
i am not sure, probably indexes, creating etl jobs etc.
table partitions, triggers, cursors
I will definitely be testing this out first thing Monday. If I didn‚Äôt say this earlier, I am really grateful! I was at wit‚Äôs end. Super appreciate your help!
Lol making fun of me?you're a complete fucking nerd lol. you probably never touched a women.
SELECT * FROM docs WHERE org = ? AND id NOT IN ( SELECT id FROM docs WHERE org = ? ORDER BY date ASC, id ASC LIMIT 1 ) Index(org,date) can help in this query, as long as there are many rows for same org. If there are not many rows within same org, index on org only will be enough.
Nice!! I had to do something similar this week on SQL Server 2014 and settled on using 2 functions as well: Levenshtien and a function to get the length of the longest common substring. My results were OK but I sounds like they would be better with soundex! (Pun intended)
you neglected to mention which database platform you're running this on
Apologies. This is for Microsoft SQL Server 2016
Wait until you have 20 tables that exist strictly to compute likely typos based on weighted numbers for keys nearby to every other key in the keyboard. Fuzzy matching is a deep dark hole, friend.
that's a shame SUBSTRING_INDEX nested a few times would've been a trivial solution
On over 20 thousand misspelled words it gave me comparable performance to commercial cloud spellcheck tool. It came to a point that I actually decided to remove spellcheck api from my code completely and leverage just this. Don't have to pay for it and response time is just fine.
Excellent. If the load on the CPU can take it, use those resources. That's what they're there for. Nice work.
I‚Äôm guessing those dates are stored as DATETIME. All the ‚ÄúCAST as DATE‚Äù is doing is taking the TIME out and making it YYYYMMDD So the BETWEEN and &gt;= doesn‚Äôt have to be typed as a LIKE or include the time. Is that the actual query? The DATE is missing 1 character. Yours is 7 characters YYYYMMDD is 8.
Look into recursive CTEs as a possible solution.
If using sqlite, there's [an extension](https://www.sqlite.org/spellfix1.html) for this sort of spell checking.
As u/y186709 mentioned it's a correlated subquery (which is fancy for a part of the subquery references the outer query). The where not exists portion of the query will result in a boolean result versus scanning (or seeking) through the table (which a where in would require).
It over my head. Lol. Any good guide with examples?
 [http://www.sqlservertutorial.net/sql-server-basics/sql-server-exists/](http://www.sqlservertutorial.net/sql-server-basics/sql-server-exists/)
Yikes, UPDATE Table\_1, Table\_2, Table\_3 is telling Access to CROSS JOIN Table 1 with Table2, and then takes that result and CROSS JOINs with Table3. A CROSS JOIN matches every combination in the first table. So if you have a relatively low 1000 rows in each, you just told access to combine them into 1000 x 1000 x 1000 = 1,000,000,000 rows. So please don't do that. :) You would only include multiple tables in a single UPDATE if it makes sense to join them. From what you've said it sounds like it does not make sense to join these tables, or even if it does, for the purpose of this update you don't really need to. You can just run an UPDATE once per table. As for why \[Student ID Number\] is wiped out, offhand your call to Mid() looks ok for what you're trying to accomplish, but just see if it works once you run the UPDATEs correctly.
Does that mean I have to do an individual update query for each table? I have done that and it has worked, I was just trying to find a way to update all of my tables at once from one query
Following up, know you've found the solution but you should learn how [window functions](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql?view=sql-server-2017) can be used (which is what the solution you found contains) you could also have used a [HAVING](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-having-transact-sql?view=sql-server-2017) + Union.
&gt;Does that mean I have to do an individual update query for each table? Pretty much, but if you're having to update 9 tables with the same update your database might not be set up optimally. Typically you would want just a single table of students where the primary key is just an auto generated number or something. That way when you need to make a change in the student id syntax, you'd only need to update 1 table.
Could you explain using the primary key a little more? I‚Äôm new to access and only knew basic SQL prior to starting this project.
Yes but there's nothing wrong with that.
You could use something designed for this, like Sphinx
I was thinking the lag function could have made this easier as well.
Here's a simple solution, using the LAG() window function: CREATE TABLE #TimeRecords ( TimeRecordId int not null, StartTime datetime2 not null, EmployeeId int not null, [Status] varchar(32) not null ) INSERT INTO #TimeRecords (TimeRecordId, StartTime, EmployeeId, [Status]) VALUES (1, '2019-06-18 12:00:00.000', 4444, 'Punch In'), (1, '2019-06-18 12:01:00.000', 4444, 'Punch In'), (1, '2019-06-18 14:00:00.000', 4444, 'Punch In'), (1, '2019-06-18 14:15:00.000', 4444, 'Punch In'), (1, '2019-06-18 16:30:00.000', 4444, 'Punch Out'), (1, '2019-06-18 17:00:00.000', 4444, 'Punch In'), (1, '2019-06-18 19:30:00.000', 4444, 'Punch In'), (1, '2019-06-18 19:45:00.000', 4444, 'Punch In'), (1, '2019-06-18 21:02:00.000', 4444, 'Punch Out') SELECT TimeRecordId, StartTime, EmployeeId, [Status] FROM ( SELECT TimeRecordId, StartTime, EmployeeId, [Status], LAG([Status]) OVER (ORDER BY StartTime ASC) PreviousStatus FROM #TimeRecords ) tr WHERE tr.PreviousStatus IS NULL OR [Status] != PreviousStatus
This problem is called gaps and islands. Try to lookup some articles from Itzik Ben-Gan if you want to learn effective solutions and more in general.
I tried to do this for a project I'm working on, and empty cells in the Excel file were causing the package to fail. I'm going to try an approach from a stored procedure next.
I tried to update one of the values referred by the foreign keys; the changes cascaded so it seems to be working just about fine.
Wonderful, thank you!
Awesome! Thanks a lot for the solution.
Thank you!
Are you trying print the string "8 minutes 40 seconds"? In that case, you need to contatenate with ||. Just say (round(number, 0) || ' minutes ' || round((number-round(number, 0))*60,0) || ' seconds'). Just replace number with your variable. I'm typing on my phone so you might have to tweak it, but that's the basic concept.
Duration_sec % 60 = leftover_seconds Then you don't need to convert back from decimal.
I would rather keep it an INT. So for example, 8.40 seconds. Sorry I'm still a SQL noob, what do you mean when you say replace the number with your variable, do you mean my SELECT query? Note that I just wrote down 8.40 as an example, It's a faily large data set. Also, where would I insert the query which you wrote above? Thanks for the help /u/cheese_inspector !
It sounds like he‚Äôs getting you exactly what you asked for. Can you post an example input and output value? That might clear it up
&gt;Duration\_sec % 60 = leftover\_seconds Thanks for the help! Where would I insert that in my SQL query?
The only problem is I don't know where to add that statement in my SQL query :/.
Convert the result to a DATETIME or equivalent type for your RDBMS (you didn't mention what it is). Usually you can do that by using whatever your function for adding some unit of time to a zero time. So in SQL Server this would be something like SELECT CONVERT(TIME, DATEADD(ms, AVG(duration_sec) * 1000, 0) FROM trips So you first calculate your `AVG` in seconds, multiply that by 1000 to get miliseconds, add them to time "0" and then convert the resulting `DATETIME` to just the `TIME` part. [Working example](https://dbfiddle.uk/?rdbms=sqlserver_2017&amp;fiddle=87996543ede4cb2e72b80582b2cd07c5) In MySQL it's similar, though you can work on `TIME` directly: SELECT ADDTIME(0, AVG(duration_sec)) FROM trips; [Working example](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=6f8a53769eeb1fde535b350b80abcc25) Just please don't ever, EVER, represent 8 minutes 40 seconds as `8.4` because it makes no sense at all.
Thanks for the response, you're completely right I shouldn't label my time as 8.4. I added your query: ADDTIME(0, AVG(duration\_sec) into mine, it worked great for I received a null value for one of the cells. Is this normal? The data I'm working with is completely clean and the duration\_sec is an INT.
An integer cannot have a decimal
OK, I'm glad you're making the effort to learn, but it's going to be a long road from here. To start, after the select statement, you are listing the columns you want to create. In your case you are using the average function to create one column. If you instead use the function I posted, substituting the word "number" for the function you had, you should generate the response as requested. Alternatively, you can just break it out into two separate columns of minutes and seconds. Basically the round function will take two arguments: the number you mean to round and the number of floating points you want. If you supply it with a 0 as the second argument, you are truncating the floating point and answering the question of how many minutes. If you subtract that number from the original number, you will just have the remainder. If you multiply that by 60, then you have the number of seconds. My answer will generate the string containing the answer.
I highly recommend looking up w3schools for a quick reference on how to use all the various functions of sql.
Well that could probably only happen if duration was NULL on all records belonging to a given group as defined by your GROUP BY. Or maybe if the resulting average was negative it'd also result in a NULL? I haven't worked with MySQL in a while, I don't remember the behavior. What our want to do though is check which group are you getting NULL on, then look at those raw ungrouped records to see what might be going wrong.
Perhaps you're looking for the RANK and/or ROW_NUMBER functions, explained [here](https://www.c-sharpcorner.com/article/rank-denserank-and-rownumber-functions-in-sql-server/). My suggestion, roughly, is: RANK() over (order by Date_Field desc) as _Rank, DENSE_RANK () over (order by Date_Field desc) as DenseRank , ROW_NUMBER() over (order by Date_Field desc) as RowNumber Which will order your rows by descending date_field.
Could help if you provide some data and some example output you hope to get.
Do you need to handle the Paths dynamically? i.e. number of levels in the path can vary and/or names within the path can vary? If they are fixed the solution will be simpler.
Your question is jumbled. Try to write a meaningful sentence.
Depending on your database engine, if you store the numbers as a float instead of decimal /numeric, you get that happening.
Ceiling(avg(duration)/60) = minutes Avg(duration)%60 = leftoverseconds You can then do whatever you want. Add them, cast as varchars into larger text result, etc I saw you wanted 8min40sec as 8.40 and you really shouldn't do that. .4 minutes is not 40sec so you're just asking to confuse someone.
Sounds like it is due to binary rounding. You can look up some good numberphile videos on it. Basically, some numbers produce rounding errors due to how they're expressed in binary. Think of as an example 1/3*3. We know that is 1. A computer might think it is 0.999999999 You look like you might have one of those here, maybe. Try using the round function or casting it as int.
!remindme 2 days
I will be messaging you on [**2019-06-25 20:17:07 UTC**](http://www.wolframalpha.com/input/?i=2019-06-25 20:17:07 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/c494qv/my_work_will_reimbursement_my_sql_training_id/ervo8ge/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/c494qv/my_work_will_reimbursement_my_sql_training_id/ervo8ge/]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I would look at something like Coursera, Pluralsight, or Udemy. They will provide you with more than enough learning and knowledge to get what you're looking for. Good luck!
DOUBLE takes 2 arguments https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html
thank you so much!!
Have one CTE get the months + categories, a second CTE do the ROW\_NUMBER() partitioned by month and ordered by the count descending. Then select from your CTE grouping by a case statement that's the category if the ROW\_NUMBER column is &lt;=10, and 'Other' otherwise.
The code academy ones are good.
You can also look into a UNION as well. I don't have the same data set which you do, so unfortunately, I had to use the AdventureWorks database. At least, I *think* this is correct. select cast(t.OrderDate as varchar(15)) as 'category', t.orderCount from ( select top 10 convert(date, h.OrderDate, 103) as orderDate, count(*) as orderCount from production.product p inner join Sales.SalesOrderDetail s on s.ProductID = p.ProductID inner join Sales.SalesOrderHeader h on h.SalesOrderID=s.SalesOrderID group by convert(date, h.OrderDate, 103) order by 2 desc ) as t union select 'Other', t1.orderCount from ( select count(*) as orderCount from production.product p inner join Sales.SalesOrderDetail s on s.ProductID = p.ProductID inner join Sales.SalesOrderHeader h on h.SalesOrderID=s.SalesOrderID ) as t1
Code academy is pretty chill for SQL.
I like pluralsight
No one can answer this from such little info. How does this date get set? Is there any auditing at the database or application level that might provide a clue? What is this stored procedure that you're talking about?
Usually you solve problems like this by recreating all the activities that generate the data. Trace and debug to see what's really happening. Sometimes you have to perform the entire businness process/workflow step by step, sometimes one procedure run is enough.
Go CBTNuggets if work is footing the bill. Includes virtual labs and practice exams for certifications. Start with the database fundamentals course then move onto 70-761 for querying deep dive.
On SQL Server you have lock escalation, row lock -&gt; page lock -&gt; table lock. If you got a massive transaction, chances are excellent you will end up with a full table scan and a table lock
Very confusing one! Type your question correctly and provide table structure and table data in text file. Also, provide your desired output in a text file that how you like to get the result. I'm waiting... Hurry up!
I basically did this: with ranked_cte as ( select month_purchase, category_name, rank() over (partition by month_purchase order by purchases desc) as rank from (select date_trunc('month', created_at) as month_purchase, category_name, count(*) as purchases from orders group by 1,2) x -- subquery to get the actual count, to then rank in the parent query ) select month_purchase, case when rank&lt;=10 then category_name else 'Other' end as category_name_grouped, sum(purchases) as purchases from ranked_cte group by 1,2
Is this is what you are looking for. IF OBJECT_ID('tempdb..#Transactions') IS NOT NULL DROP TABLE #Transactions GO CREATE TABLE #Transactions ( Date DATE NOT NULL, SrNo INT NOT NULL ) INSERT INTO #Transactions VALUES ('2019-04-02', 1), ('2019-04-02', 2), ('2019-04-03', 3), ('2019-04-03', 4), ('2019-04-03', 5), ('2019-04-04', 6), ('2019-04-04', 7) GO WITH Data AS ( SELECT Date, SrNo, ROW_NUMBER() OVER (PARTITION BY Date ORDER BY Date) as Seq FROM #Transactions ) UPDATE T SET T.SrNo = Data.Seq FROM #Transactions T INNER JOIN Data ON Data.Date = T.Date AND Data.SrNo = T.SrNo GO SELECT * FROM #Transactions GO
I assume you want incremental sequence? UPDATE t SET t.SrNo = ROW_NUMBER() OVER(ORDER BY [Date], [TransactionId] ASC) FROM Transactions t WHERE CompanyId = 5 AND TransactionTypeId = 3 AND BookId = 12 And if you want to do this, say for each of the Transaction Types or something else, you can include a partition by: UPDATE t SET t.SrNo = ROW_NUMBER() OVER(PARTITION BY [TransactionTypeId ORDER BY [Date], [TransactionId] ASC) FROM Transactions t WHERE CompanyId = 5 AND BookId = 12
As others have said, you want to use [ROW_NUMBER])(https://docs.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql?view=sql-server-2017) ROW_NUMBER() OVER (PARTITION BY Date ORDER BY Date) as Seq
Makes sense on the CAST - I know at least the create\_date field is a datetime field. For the dates, that's how teradata be. It's not YYYYMMDD, it's 1YYMMDD. YYMMDD is for 1900 - 1999, and 1YYMMDD is for years 2000 - 2099
Ah, not familiar with the syntax. Sorry for the misguided answer.
OK, so the fundamentals and 70-761 course is MS's official coursework/
Pulling out the cast statements seems pretty straight forward (one of the variables is already in a date format, so it's redundant, and the other I replaced with &gt;= TIMESTAMP '2019-05-06 00:00:00') I removed the sub query and replaced it with session\_id not in base.session\_id Still running into the "no more spool space" error. Should I just go to IT and ask for more memory to be allocated to my profile?
It's on the CBTNuggets site as training paths. I followed them to obtain my Microsoft certifications. MTA, MCSA, MCSE... Very easy to watch videos and labs where you do the things in the videos to reinforce the material. They also have a mobile application with offline viewing so you can watch videos anywhere.
Oh, I legit thought you were using some slang or something that I wasn't familiar with. Thanks - I will check out CBTNuggets.
Yeah it can be slang. I wonder why I got in trouble at work talking about how dank the cbt nuggz were and about how they brought me up to a higher level.
Yeah, what the heck is with that name?? Sounds like a strain of weed.
It's the good stuff. That kinda stuff that gets you places.
`select` `Project,` `sum(case` `when Account in('40000', '40100') then Amount else 0` `end) [Account 4x],` `sum(case` `when Account in('50000', '50200') then Amount else 0` `end) [Account 5x],` `sum(case` `when Account in('60000', '60200') then Amount else 0` `end) [Account 6x]` `from Billings` `group by` `Project`
look into case expression
Worked perfectly! Thank you!!
Now what if i wanted the next column to be from another table (Expenses or Labor cost table for example) but still have the summed amount put into the appropriate Project Row?
little bobby tables
Check out this StackOverflow article: https://stackoverflow.com/questions/16667251/query-to-get-only-numbers-from-a-string Essentially, you create a function to return numbers only and update the phone number values to the value returned by the function
&gt;I need them to all be in the exact same format, as example, like this: 5551032546 A couple questions: 1. For what purpose (IOW, can the problem be pushed downstream)? 2. How did they get this way in the first place? 3. Has any work been done to prevent phone numbers in non-compliant formats from being entered in the future (IOW, are you going to be chasing this forever)? There is no "best" way to do this. As long as you have users (or data feeds) permitted to enter these numbers without any constraints, you're going to be doing a crapload of text transformation - find and replace, etc. - and SQL is not a great language for doing this. You can start by doing a `replace()` with `AreaPrefix` against the phone number - `replace(PhoneOne, AreaPrefix, '')` and then strip out the spaces with `replace(phoneone,' ','')` and dashes `replace(phoneone,'-','')` and keep going with other non-numeric characters (you can nest `replace()` as well, but it starts getting hard to read). If your RDBMS has good regular expression support, that may help. But it may be even better to do it _outside_ the RDBMS entirely with another tool, basically doing an ETL and then swapping in the scrubbed data _en masse_. But if nothing's being done to prevent this "bad" data from getting into the table in the first place, you're just chasing your tail. You need a constraint or trigger on the table to block any future update/insert from writing bad data to the table, and any data inserts (application, ETL process) need to clean it up before inserting.
I'm on mobile so I'm not able to write a well formatted query, but you can join the table on itself with its ID column and do a count. Something like: SELECT hp.hotel_name, hp.lng, hp.lat, count(hp.posneg), count(hn.posneg) FROM hotel_reviews hp INNER JOIN hotel_reviews hn on hp.ID = hn.ID and hn.posneg = 0 WHERE 1=1 AND hp.posneg = 1 GROUP BY hp.hotel_name, hp.lng, hp.lat
Sorry on phone so bad formatting, but it should be something like Select *, (select Mac eta from table where id = something) as eta from table where active.
You may be able to use UNION which will merge two separate select statements. You just need to ensure that the same number of columns are being selected
Store JSON in PostgreSQL.
Yes, you may want to get advice from your DBA.
What rdbms? You didn't specify. In Oracle this could be accomplished by select REGEXP_REPLACE(PhoneOne,'[^[:digit:]]') from yourTableName; Which would preserve digits only, in order, and discard anything that isn't 0-9. You could expand on that by using case when length(code above) = some number, then do (code above) in the format you want, possibly by using substr to further chop out what you don't want from the numerical string. If you're using MS I think there is a regexp equivlent using a STUFF(PATINDEX()) nest.
This is really bad design. You should use one table with an indexed time field and a daily materialized view for the last version of the table you seek for.
 SELECT Property1, 4 AS Property2, --a new column with '4' as values into #temptable FROM Table
If i want to continue adding new columns of data to my original dataset but using queries from other tables, should I use join? [example](https://i.imgur.com/8N6v0XH.png) Should I join my original query into a temporary table, then join queries from other tables into my temporary table?
Yes, you can write a proc to take table name and store it in a variable, then write a dynamic sql query using the variable. Then exec it.
Lots of IO and little CPU usually means out of date statistics, poor indexing choices by the vendor or no indexing at all. Possibly something is going on with the storage subsystem, it could have a failed disk in the raid volume, it could be overwhelmed if it is shared with other servers. I think those are the most likely/common things. SQL will use as much memory as you let it, up to the size of the database. Caching data in RAM from the disk is one of the ways most, if not all, RDMS engines work. Some questions, in no particular order, to answer might include: Is the server 32 bit or 64 bit? Are you running maintenance jobs regularly? If there are no maintenance jobs running, consider running SP_updatestats manually and/or installing Ola Hallengren's maintenance solution. Is there server sized according to the recommendations of the vendor? Are the PDF files stored in the database or on a file share? If it is a share, how is that server performing? What version of SQL Server do you have? What edition of SQL Server do you have? (Express, Standard, Enterprise?) How large are the database files? How much RAM is SQL Server configured to use?
update TRANSACTIONS set SrNo = row\_num() over(order by Date); something like that
Legend. Dynamic SQL was indeed the answer: DECLARE @TableName NVARCHAR(20); DECLARE @SQLQuery NVARCHAR(200); SET @TableName = (SELECT TOP 1 table\_name FROM information\_schema.tables WHERE table\_type = 'base table' AND table\_name like 'Table1\_%' ORDER BY table\_name DESC); SET @SQLQuery = 'SELECT \* FROM ' + @TableName; Exec (@SQLQuery) &amp;#x200B; (I tried CREATE PROCEDURE from this guide first, but it wasn't having any of it: [https://www.mssqltips.com/sqlservertip/1160/execute-dynamic-sql-commands-in-sql-server/](https://www.mssqltips.com/sqlservertip/1160/execute-dynamic-sql-commands-in-sql-server/) )
Thanks a lot for help. It‚Äôs 64bit with sql standard 2012 . The pdfs are stored on different server . There is no maintenance job happening as far as I know. Server is hosted on dell san . I looked into activity monitor noticed the latch has high cumulative wait time . When scrolled down saw queries are taking long to run .
Not a book, but w3schools sql tutorial is a good hands-on resource: https://www.w3schools.com/sql/
Thank you. That's kind of the statement I already have. The sub-select 'MAX eta' gets overruled by the main select's 'where active=true' clause. So if the record that has the actual ETA of the whole trip is not the active trip, it would bring back nothing for an ETA.
This. Use any of the many relational database engines that support JSON storage or even plain text. Microsoft SQL Server doesn't have JSON types, per se, but it has JSON functions. MariaDB has had JSON support since 2016, I believe. And don't go "schemaless" until you need to, and have thoroughly analyzed both the current requirements and projections for the future and decided it's definitely the way to go for your project.
I updated my post, I guess the best description of what I'm looking for is 1. Search docs table for duplicate \`org\` entries and group them in sets 2. Locate within each set, the entry with the lowest value in \`date\` 3. For all other set entries with a value higher than the lowest value in \`date\`, set entries \`org\` to null An example run with the rules from above would be: 1. Find sets with duplicate \`org\` would produce 2 sets {1,2,3} &amp; {5,6,7} 2. Locate the lowest value in \`date\` for each set would produce {1} &amp; {5} 3. set 'org' to null for all entries with a value higher than the lowest, this would set {2,3} &amp; { 6, 7 } \`org\` to null
Thanks, same here, over my head need to learn this style my self.
As long as Project is a column in your other table(s), you can join on that - FROM Billings B INNER JOIN LaborTable L on L.Project = B.Project - and use the same SUM..CASE structure with columns from the joined table (LaborTable). Thanks for the gold! Let me know if you need more clarification!
This is not 100% true : All NoSQL and NewSQL products have a *distributed architecture* ‚Äî data is sharded across machines to improve performance. You have to configure Mongo to do sharding.
I'll have to go through your examples and see if I can understand how it works. I guess the best description of what I'm looking for is 1. Search docs table for duplicate \`org\` entries and group them in sets 2. Locate within each set, the entry with the lowest value in \`date\` 3. For all other set entries with a value higher than the lowest value in \`date\`, set entries \`org\` to null An example run with the rules from above would be: 1. Find sets with duplicate \`org\` would produce 2 sets {1,2,3} &amp; {5,6,7} 2. Locate the lowest value in \`date\` for each set would produce {1} &amp; {5} 3. set 'org' to null for all entries with a value higher than the lowest, this would set {2,3} &amp; { 6, 7 } \`org\` to null
Thanks! I tried doing it this way &amp;#x200B; select into #TempTableA select into #TempTableB &amp;#x200B; Select # from TempTableA join #TempTableB &amp;#x200B; but this is giving me some very messy code.. second potential issue here is that this creation of a #TempTable is working fine in our local database where i am testing my queries, but I just remembered that once I start executing this query in our live realtime database, it won't work because it's a read-only access... So creating a temporary table to load data into will be a deal breaker.. &amp;#x200B; Does it make more sense to create a SQL view in this case ... I'm testing a join right now but i'm always afraid of redundancies when I perform a join..
we are using the default ubuntu release, mysql Ver 14.14 Distrib 5.7.25, for Linux (x86\_64) using EditLine wrapper
An example run with the rules from above would be: 1. Find sets with duplicate \`org\` would produce 2 sets {1,2,3} &amp; {5,6,7} 2. Locate the lowest value in \`date\` for each set would produce {1} &amp; {5} 3. set 'org' to null for all entries with a value higher than the lowest, this would set {2,3} &amp; { 6, 7 } \`org\` to null
you're thinking of UNION SELECT 'Table1' AS source_table , SUM(foo) AS sum_foo FROM Table1 UNION ALL SELECT 'Table2' , SUM(foo) FROM Table3 UNION ALL SELECT 'Table3' , SUM(foo) FROM Table3 UNION ALL ....
We are using Hands-On Database by Steve Conger in my database development course. The book uses a scenario based approach to stepping you through requirement analysis, constructing the conceptual, logical, and physical models, and finally building and testing the database in SQL Server. It a college text so it's fairly expensive but you can pick up the e-book version from VitalSource at a discount (but still kind of expensive).
That worked! Thank you so much
They shouldn't partition by date, only order by it right?
Those redundancies \*should\* be minimized by the aggregation, but I can't be 100% sure without seeing the data. Something like: select b.Project, sum(case when b.Account in('40000', '40100') then b.Amount else 0 end) \[Account 4x\], sum(case when b.Account in('50000', '50200') then b.Amount else 0 end) \[Account 5x\], sum(case when b.Account in('60000', '60200') then b.Amount else 0 end) \[Account 6x\] sum(case when e.Account = '70000' then e.Amount else 0 end) \[Account 6x\] from Billings b inner join Expenses e on e.Account = b.account group by b.Project &amp;#x200B; This code is likely oversimplified, as the INNER JOINs depend on all projects having rows in both (or all, as you add in more) tables. You'll more likely use LEFT OUTER JOINS and change x.Amount in each case statement to COALESCE(x.Amount,0), so that the nulls are replaced with zeroes. That way, when Project A has expenses, but no billings yet, the billings will still sum to zero instead of being eliminated for lack of an INNER JOIN match with the expenses table.
 select hotel_name, lat, lng , sum(posneg) as postive , sum(case posneg when 0 then 1 else 0 end) as negative from reviews group by hotel_name, lat, lng;
You could create views that manage the pivoting and aggregation, and join those... That might be easier to read, but if your account numbers etc. change, you'll have more objects to edit. If this is the only place that this perspective on your data will be consumed, I'd use Common Table Expressions (CTEs).
Wow, thanks. I can't believe it was a 4 word solution.
Exactly. I had been looking for something radically different, as I'm trying to store BOM's for electronics. I never bothered looking at any of the other flavors of sql.
Do you mean something like update have Set colA = cast(REPLACE(colB, 'a', '') as int) where colA is null; ?
Thanks! I'm going to ponder this over and the best way to do it. I've always had a hard time knowing what's the best join to use... So you bring up a good point... Let's say i'm joining two tables, and there are projects that exist on the second table, that aren't on the first table, but I want every project on both tables that have data to appear in the output query... Do I perform an outer join or inner join? I basically never want any project to slip through the cracks when I perform a join. I did a join an hour ago, and only 100 projects are coming through, even though when I join both tables, I should theoretically have about 175 projects...
You do a SUM(CASE WHEN DateCol BETWEEN X AND Y THEN ShipQty ELSE NULL END) for the first field and another one with different case for the other
Thanks,but I get an error for using date types with the sum operator. "Operand data type date is invalid for sum operator"
Haven't tried it yet, but graph dbs are pretty awesome. Will check it out. Thanks for the link!
I would probably start with a CTE that represents all of your projects... maybe something like: WITH ProjMaster as ( SELECT Project, ID FROM Projects WHERE Status = 'Active' -- or EndDate &gt; CURRENT\_TIMESTAMP, ) SELECT p.Project, sum(case when b.Account in('40000', '40100') then coalesce(b.Amount,0) else 0 end) \[Account 4x\], sum(case when b.Account in('50000', '50200') then coalesce(b.Amount,0) else 0 end) \[Account 5x\], sum(case when b.Account in('60000', '60200') then coalesce(b.Amount,0) else 0 end) \[Account 6x\], sum(case when e.Account = '70000' then coalesce(e.Amount,0) else 0 end) \[Account 6x\] FROM ProjMaster p LEFT OUTER JOIN Billings b ON b.Project = p.Project LEFT OUTER JOIN Expenses e ON e.Project = p.Project etc.
You shouldn‚Äôt try to ‚Äúthink in gremlin‚Äù. Instead, you should learn some graph theory and build a regular SQL DB that integrates nodes and edges. Graph databases aren‚Äôt anything new, the concept has been around for awhile. The only new part is the fancy syntax
Sounds like you're trying to sum the dates. Notice the function above is summing the ShipQty column
Maybe this will help? https://stackoverflow.com/questions/13124944/converting-nvarchar255-to-date
As you join more tables into your predicate, you'll use LEFT OUTER JOINs so that, for instance, if Project B has expense rows but not billing rows, the rows in the expense table will 'JOIN' up with NULL 'rows' in the billing table, instead of being eliminated from the results altogether for not matching anything in the billing table. &amp;#x200B; \*When I was young and silly, I liked to keep this straight by saying this join let me keep everything that would've been LEFT OUTTA the query. Apologies for American dialect if that's confusing... just suffice it to say that I was a goofy kid.
FWIW, this sounds like a bad idea. Possibly an [XY problem](http://xyproblem.info/)?
Fair point! What I meant was when I go to tackle a problem that fits a relational model, I know how to think in terms of joins and filters and what I'll need to aggregate or project. I don't have a good mental model for traversing nodes and edges or how that gives a particular shape to the data. This tutorial feels like a nice way to develop that intuition.
Your right...I had a typo in a different SUM function.
Apparently I'm not even smart enough to get the formatting right. Bear with me folks.
Post nvarchar example, so we can determine correct way to convert it
You seem very knowledgeable with DBA topics. Were there any resources that helped you with your DBA skills? Thanks!
So it in SQL. Do as much as you can in SQL and as little as possible in SSRS.
My bad, I should have included the date format. Data coming in format: '24-06-2019'
something Like this =iif(len( Fields!BILL\_ZIP.Value )=5, Fields!BILL\_ZIP.Value ,left(Fields!ISBN.Value,5)&amp;"-"&amp;right(Fields!ISBN.Value,4))
This worked. Thank you!
&gt;24-06-2019 select CONVERT(datetime, %yourdate%, 105)
You are a gentleman and a scholar.
Correct. The query above would assign 1 as the serial number for the first row of each date
Check if any of your CPmtKey are null on the production db.
I just checked and there is one record with a NULL. I am going to have to figure out what the correct key is to put in there but I'm certain that you are right. I will follow up in the morning. Thank you. That was amazing.
It's SQL Server telling you that it's long past time to upgrade to a supported release :)
If the version of SQL you‚Äôre using has Regex extract or substitute functions that would be a simple way to do it
&gt;I work on an ERP system so I have a massive DB at my disposal to do exercises with. You're going to do that on a development server though, right? PluralSight vets their authors _very_ thoroughly. To be accepted by them as an author is not as simple as throwing together a quick demo and a blog post. Don Jones talked about the process [last month when he was on the PowerScripting Podcast](https://powershell.org/2019/05/episode-324-powerscripting-podcast-don-jones/) but I don't have a timestamp I can give you. The whole podcast is worth listening to though.
I would consider creating a reference table with a series of timestamps, stamps, with minute intervals and joining that on stamps BETWEEN sessionstart and session end. You'd have to truncate those to catch cases like row 1. Then you can group by the reference table to get the session counts.
One approach: Create a table that has all the times at one minute intervals. In my example, I'm building mine dynamically in a CTE (TTM). Strip the time intervals finer than minutes off of the times in the data (I'm converting to varchar(20) and back to datetime for this). Join the tables together and count the intersections. ;with rownum as ( select top 2000 RowNum = ROW_NUMBER() over(order by object_id) from sys.all_objects ), TTM as ( select TimeToMinute = dateadd(minute, RowNum, '2019-06-24') from rownum ), theData as ( select * from (Values('A97C403F-2747-4F61-A64F-8CDD29098405',convert(datetime, '2019-06-24 14:44:47.180'),convert(datetime, '2019-06-24 14:44:59.273')), ('98ACCC96-87C8-4818-9DCD-77DE159A04C6',convert(datetime, '2019-06-24 13:47:46.857'),convert(datetime, '2019-06-24 13:54:45.277')), ('38C28694-936C-40AC-9B1C-489791A0F5F0',convert(datetime, '2019-06-24 13:16:05.910'),convert(datetime, '2019-06-24 13:22:17.050')), ('C3BCCA56-7601-458F-878E-6BBFD47B3AFF',convert(datetime, '2019-06-24 13:15:13.613'),convert(datetime, '2019-06-24 13:55:39.370')), ('B9B4D463-DE8B-46D1-A6D0-E9069A97EA6F',convert(datetime, '2019-06-24 10:05:08.923'),convert(datetime, '2019-06-24 10:11:20.860')), ('A39170FD-58CD-4D2E-A15A-48021771ED5D',convert(datetime, '2019-06-24 08:43:08.787'),convert(datetime, '2019-06-24 08:46:40.553')), ('98ACCC96-87C8-4818-9DCD-77DE159A04C6',convert(datetime, '2019-06-24 08:14:21.983'),convert(datetime, '2019-06-24 08:15:19.013')), ('A39170FD-58CD-4D2E-A15A-48021771ED5D',convert(datetime, '2019-06-21 16:37:08.117'),convert(datetime, '2019-06-21 16:37:31.163')), ('AF8E6B14-D592-4A91-B076-95A028955AE2',convert(datetime, '2019-06-21 15:22:46.813'),convert(datetime, '2019-06-21 15:25:52.407')) ) as TheData ([USER],[SESSIONSTART],[SESSIONEND]) ) select TimeToMinute, SESSIONCOUNT = count(*) from TTM inner join theData on TTM.TimeToMinute between convert(datetime, convert(varchar(20), SESSIONSTART)) and convert(datetime, convert(varchar(20), SESSIONEND)) group by TimeToMinute order by TimeToMinute desc
Can you give some insight into why upgrading is good from a DBA perspective? From a developer/analytics standpoint, it's nice to get new features. We just went to Azure and I immediately found STRINGAGG() useful, but beyond that it doesn't seem to matter much, and I suspect that staying with 2008 would be just fine for most applications. I'm assuming there is a lot more going on behind the scenes though in terms of security, supported patches for new vulnerabilities, etc.
Thank you. I've been doing SQL Server pretty much exclusively since 1998. The best single thing I can tell you would be to join SQL PASS, which is a free, world wide user group dedicated to SQL Server. They are many local chapter, all over the world, dedicated to helping people become better at SQL Server. Joining also gets you access to videos given by a variety of speakers, at all levels of ability. They keep a few years of people videos online at their site and they are free to watch. Some of the local groups post talks in YouTube.
OK, 5.7 means window functions are out.
OP clarified that he has version 5.7, so window functions are not available.
There was a typo in my post, but now that I'm on my PC I can do a proper reply. http://sqlfiddle.com/#!17/96002/14 There's the two queries, one yours slightly modified, one with using `max`. The trick is you need to rename your column you want from the subquery or some engines will override.
&gt; Can you give some insight into why upgrading is good from a DBA perspective? * If I call Microsoft, or any of the major consulting firms, saying "hey, something on my unsupported release of SQL Server isn't working right" they won't help until I've upgraded * Eventually there won't be a host OS that's still supported which also can run my old &amp; busted SQL Server instance * In regulated environments, running software that's explicitly no longer supported by the vendor will hurt you in your audits * If you don't self-host, your hosting provider will likely stop supporting your environment. They may even issue you an ultimatum - upgrade or find a new place to get hosted * If you're running on physical hardware, you're probably _well_ past the expiration date on those boxes because your OS &amp; SQL installs are probably 5-10 years old at this point * Security and bug fixes * New features * Potential elimination of code thanks to leveraging the above new features (just today, I got to show a developer how to use `string_split()` instead of falling back to uglier methods of processing a comma-separated list) * Generally improved performance &gt;From a developer/analytics standpoint, it's nice to get new features. DBAs like to get new features too. Especially ones that make us look better to our bosses and customers. The value of "new features" really shouldn't be discounted. The below are technically "new features" but developers won't see them nearly as much as the DBAs will. 2012: * A lot of new windowing functions that greatly improve analytics (and with better performance than the kludges we had to do) * Columnstore indexes, but with a number of limitations and gotchas; improvements to them have come with each new release (this feature can make a _huge_ impact on performance and space usage) * Availability Groups for HA/DR 2014: * New cardinality estimator, giving a noticeable improvement in query performance in the right scenarios * In-Memory OLTP (aka Hekaton). Again, some gotchas and limitations. But if you have a use case that matches up with it, it was a huge boost. 2016: * Basic Availability Groups * Query Store - lets us monitor query performance on a per-database level and look for outliers, force a particular plan for all instances of the query (if appropriate) 2016 SP1: * _All_ of the programming features that used to be reserved for Enterprise Edition came to the unwashed masses. 2017 * Linux * [Interleaved Execution, Adaptive Joins and Memory Grant Feedback](https://docs.microsoft.com/en-us/sql/relational-databases/performance/intelligent-query-processing?view=sql-server-2017)
Just know, for future reference, that UNION will include duplicate data, whereas UNION ALL will not.
From a SQL Server perspective, they have courses by Glenn Berry, Erin Stellato, Kimberly Tripp, Paul Randall, Pinal Dave, and many others. The names I listed are just some *very* well known and respected speakers on SQL Server; I'm confident that if I spent the time, I'd find even more in the list. I'm partway through some of Kimberly's content.
Highly recommend anything by Paul Randall or Kimberly Tripp for SQL server, both helped me a lot learn SQL
Highly recommend anything by Paul Randall or Kimberly Tripp for SQL server, both helped me a lot learn SQL
okay, i think i'm making progress but i'm running into the issue where i seem to always run into with joins where my results get multiplied by random amounts, i'm assuming that it's getting multiplied by the number of instances that the project number is appearing in the second and third table i'm joining into my CTE table.... Can you look at my code and let me know if i can make any improvements or refinements, I hope it's somewhat readable.. I really appreciate your help so far! here is my code: With ActiveProjects AS ( Select distinct wbs1 from LedgerAR where left(period,4) = '2018' and amount &lt;&gt; 0 and left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') AND len(wbs1) = 6 and left(account,1) = '4' Union Select distinct wbs1 from LedgerMisc where left(Period,4) = '2018' and amount &lt;&gt; 0 AND left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') AND (Account between '50000' and '59900' or Account between '60300' and '69999' or Account in ('50201', '60201', '50000', '60000', '50200', '60200')) Union Select distinct wbs1 from LD where year(transdate) = '2018' and regamt &lt;&gt; 0 and left(wbs1,1) not in ('z', 's', 'u', 'i', 'c', 'p', 'a', 'e', 'f', 'g', 'h', 'j', 'l', 'm', 'n', 'r', 't', 'v', 't') ) select distinct ActiveProjects.WBS1, sum(case when LedgerAR.Account in ('40500', '40100', '43000') then (LedgerAR.Amount) else 0 end) [Fixed Fee Billings], sum(case when LedgerAR.Account in ('43100') then (LedgerAR.Amount) else 0 end) [Hourly Billings], sum(case when LedgerAR.Account in ('40300') then (LedgerAR.Amount) else 0 end) [Reimbursable Consultant Fees], sum(case when LedgerAR.Account in ('43200') then (LedgerAR.Amount) else 0 end) [Reimbursable Consultant Expenses], sum(case when LedgerAR.Account in ('40302', '42000') then (LedgerAR.Amount) else 0 end) [Ennead &amp; Consultant Markup], sum(case when LedgerAR.Account in ('40500', '40100', '43000', '43100', '40300', '43200', '40302', '42000') then (LedgerAR.Amount) else 0 end) [Gross Fees Total], sum(case when LedgerMisc.Account in ('50000') then LedgerMisc.Amount else 0 end) [Reimbursable Consultant Fees], sum(case when LedgerMisc.Account in ('60000') then LedgerMisc.Amount else 0 end) [Non Reimbursable Conultant Fees], sum(case when LedgerMisc.Account in ('50200') then LedgerMisc.Amount else 0 end) [Consultant Expenses 5x], sum(case when LedgerMisc.Account in ('60200') then LedgerMisc.Amount else 0 end) [Consultant Expenses 6x], sum(case when LedgerMisc.Account between '50001' and '59900' or LedgerMisc.Account between '60300' and '69999' or LedgerMisc.Account in ('50201', '60201') then LedgerMisc.Amount else 0 end) [DirectCosts], sum(case when LedgerMisc.Account between '50000' and '59900' or LedgerMisc.Account between '60300' and '69999' or LedgerMisc.Account in ('50201', '60201', '50000', '60000', '50200', '60200') then LedgerMisc.Amount else 0 end) [TotalExpenses] from ActiveProjects left outer join LedgerAR on ActiveProjects.wbs1 = LedgerAR.wbs1 left outer join LedgerMisc on ActiveProjects.wbs1 = LedgerMisc.wbs1 where left(LedgerAR.Period,4) = '2018' and left(LedgerMisc.Period,4) = '2018' group by ActiveProjects.WBS1
either you're not on MySQL, or else you didn't test what you posted, because MySQL doesn't support TOP