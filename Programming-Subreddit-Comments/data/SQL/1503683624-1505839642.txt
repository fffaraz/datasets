Right, so what can you do with that data? Anything else besides store or push it to a spreadsheet?
What can you do with the stored data? Other than exporting the data to a spreadsheet or joining tables via queries. 
You can also retrieve it. And analyze it. Use your imagination. Many massive companies exist because of data. Data is one of the most (if not *the most*) valuable commodities in the tech industry.
I use a crappy little used thinkpad still. I just hook it up to the dock, output to monitors and use a mouse / keyboard. I don't need a lot of processing power to learn things on my own and make a dev environment. (Not yet...)
This is definitely the right way to do it. It's worth mentioning, however, that it's bad practice to store date values as text in the first place... there are **lots** of places this can trip you up. Hopefully this is just a learning system, and not used by a real application.
Right? Google is basically a database, and [database management system](https://cloud.google.com/bigtable/). That's all they are at the core of their company. Look at all of the things they are doing with their data. Google ads (adsense, adwords) is a database of ads and user data that relates to those ads. Google music is a database of music. Google itself is a database of website addresses (and metadata). Google books is a database of books. Google groups is a database of user groups. Google video is a database of videos. Google + is a database of users with related user data. Google Drive is a database of user data. ... you get the picture. Pretty much any Google application uses these databases or manipulates them in one way or another.
I'm always on the go because I do a lot of contract work and freelancing so for me being stationary at my desk attached to a monitor isn't really an option unfortunately - so that's why I'm so attached to my MBP. 
Thank you this looks great! What is dual? is that just a dummy name for this example or is that a real function? Thanks again!
Oh wow. Did you learn this from personal experience or is there a good resource I could use? Thanks for the help!
I don't know if I'd call this 'simpler' but I think the query would be easier to read with a CTE. https://sqlite.org/lang_with.html
Just an update, I didn't realize I could do to_date(field, 'YYYYMM').. that solved everything! to_char(add_months(to_date(min(cr.month_number), 'YYYYMM'),2), 'YYYYMM')
Ah ok, that makes sense then.
&gt; I have to do some registry changes. Haven't tackled that one yet, if you have a powershell script for that one, then you will officially be THE man. https://blogs.technet.microsoft.com/heyscriptingguy/2015/04/02/update-or-add-registry-key-value-with-powershell/ &gt;Is SQL Server Express Edition full compatible? Ehh.....sort of. It's quite possible that features you're using in production don't exist in Express Edition, especially when you're looking at older versions. As of SQL Server 2016 (or was it 2014?), Developer Edition is free and aside from licensing restrictions, identical to Enterprise Edition. Any code/queries that work against 2005/2008 should be compatible with newer versions unless it's doing weird version-specific stuff. However, once you get to 2014, you may see performance differences on queries due to the new cardinality estimator (which you can switch off if needed)
&gt; Right, so what can you do with that data? Anything else besides store or push it to a spreadsheet? it's like asking what can you do with money? well, you can spend it on shit same with data what can you do with data? well, you can use it to generate information
I agree with the other commentors that your description is confusing, but I'll give it a try. It looks like you just need to join them. You need a column that they both have in common. The ideal situation would be if "Person" had a HouseID that could match up with the HouseID that is in "House" then you could do this: Select person.UID ,person.firstname ,person.lastname ,house.houseID ,house.houseName ,House.HouseLocation from person join house on house.houseid = person.houseid where house.name like 'Stark' This would give you a table that included all rows within the columns selected, where the data in the column "house.name" is "Stark," regardless of if the rest of the data was the same. I hope this helps. Edited to say: I work in MSSQL and Postgresql. So if you are using a different language than the syntax might be different. Good luck!
sadly, mysql does not support CTEs yet 
I'm stuck here as well. I'm looking into either development or BI, but from what I've seen on indeed there doesn't seem to be a difference between SQL development and BI since they both utilize SQL to interact with databases so what's the difference between the two? On my target is data analyst/data engineer with a mechanical engineering degree. 
As someone looking to get started in the industry, I have a mechanical engineering degree but am looking into SQL certs. What would be the best route to go in terms of demand and getting a data analyst/business analyst/data engineer/data scientist role? BI or SQL 2016? Thanks in advance for any input 
Many enterprise environments are on Oracle or SQL Server (from what I've seen). MySQL is often used for website databases. Column store databases are for analytics. 
All you can do is basically what you have described, because that is all SQL is designed to do. Its sole purpose is to be a common language used to interact with stored data. If you want to do anything else with the data you have to move to other tools. The thing is... interacting with the data is extremely important. It's not very interesting, but keeping it clean and ready for analysis is key.
MySql lacks magic :(
At least in the SQL Server world, the [2016 Microsoft SQL MCSA] (https://www.microsoft.com/en-us/learning/mcsa-sql-2016-certification.aspx) paths are a decent rough outline. From my experience: SQL Development/DBA would be creating tables, views, stored procedures, functions, (god forbid) triggers, etc. Generally these would be the backend to a transactional application, and design would be relational (usually third normal form). You need to make sure your system quickly and correctly handles data for each individual account/transaction, though you may also need to help generate some operational reporting. If you screw up you could prevent customers from accessing the system, or even corrupt or lose their data. As you advance, you'd likely get more into performance, robustness, and security so you'd need to understand indexing, locking, query plans, encryption, backup, etc. (which is where get more into DBA territory). BI Development is going to deal with other people's data, after the fact and in aggregate, to enable useful reporting, analytics, and automated recommendations. You'd probably work with data of varying quality, types, sources, and formats. You need good query skills to get the right data out of systems, but will also likely employ ETL tools to orchestrate moving, cleaning, transforming, conforming, and re-shaping data (into a dimensional/star schema model). You may also work with OLAP or in-memory analytics tools that build a semantic model optimised for storing, aggregating, and calculating large amounts of data. In some cases, you may be creating reports yourself. As you get more advanced you may get more into data modeling, data quality and master data, big data processing, data science. Starting out, you can begin in a junior SQL developer role, get really comfortable with SQL, databases, and data in general, and then move more into full development, administration, or BI (or get sucked into one because there's a void, or end up doing some of everything). Beginning as a BI developer is less likely, because it requires a broader understanding and wider range of tools, but it is possible to start as a data analyst and work your way "up" the data stream, moving from reports and analysis to semantic models to building and loading data marts. 
If you really wanted to get one of the MCSA SQL 2016 certs without showing the experience with it, I'd say Database Development over the BI one. As someone in the BI space who would like to hire a junior dev, I would prefer they have the SQL development cert. I'll teach them the ETL/data warehousing/modeling as needed. Meanwhile, someone hiring in a development/DBA area wouldn't put much value on a BI cert.
I don't know how you would do it in Access or Excel but in SQL you could do a cross join between your dates and ID's to get every single combination between the two, and then join that to your dataset and use an isnull() to populate a zero wherever there was no record found?
I was hoping that this could be done in a query within Access. Could you possibly provide an example of how that cross join would look between my dates and ID to get every single combination? I do believe your suggested solution could work for me.
Exactly. In most of the real world, database development or BI means, "we know you're supposed to be doing other stuff, but there's a project done by consultants and it's half-baked. Can you fix it and give us nice reports, ASAP?"
http://access-excel.tips/cartesian-product-cross-join/
Thanks for linking me this, I think this is what I can use to get every combination between my unique ids and date table. Once I have every combination I have to figure out a way to remove duplicates so I can only enter in the unique id/date combo that are not included in my original list. Any advice on how to check for duplicates using two fields/columns? 
Why would you have duplicates? Certainly you won't have them on the null records. Remove duplicates before you join. I don't know Access. I refuse to work with it. In SQL I would just select distinct if they were true duplicates, but then I'd want to know why you have true duplicates in a datasource.
[removed]
so the SQL 2016 SQL Data Development cert would be a good fit? 
Access literally provides the Nz() Function for this. Nz([FlagField], 0) Nz stands for NulltoZero.
* When do people start looking for back to school items? * When should we put the xmas display shit out? * how far are people driving just to get to my store? * what percent of my customers come between 9 and 10 PM? Would it be profitable to stay open to 11? Go 24 hours at this location? * If we lower an item by $2 and get it below an arbitrary price point, do we sell more? Is the increase in sales worth the loss of profit?
 &gt; &gt;&gt; select COUNT(bg.id), &gt; &gt;&gt; dt.date &gt; &gt;&gt; from dates as dt &gt; &gt;&gt; LEFT JOIN Bug as bg on &gt; &gt;&gt; bg.open_date &lt;= dt.date &gt; AND bg.close_date&gt;=dt.date &gt;&gt; WHERE dt.date &gt;= '2017-01-01' AND &gt; &gt;&gt; dt.date &lt;='2017-01-31' &gt; &gt;&gt; GROUP BY dt.date 
What is the setup file you downloaded? Here are the language requirements. https://docs.microsoft.com/en-us/sql/sql-server/install/local-language-versions-in-sql-server
when you download SQL express setup it shows 3 options : Basic, Personaliado and Decargar Medios. They mean Basic Instalation, Personalized Installation and Download Files. The set up file i get it with the third option, I tried the first two options to install the SQL express but it gives an error (i asume is the first because it doesnt say what error it is)
 SELECT dt.date , COUNT(bg.id) FROM dates AS dt LEFT OUTER JOIN Bug AS bg ON bg.open_date = dt.date WHERE dt.date &gt;= '2017-01-01' AND dt.date &lt; '2017-02-01' GROUP BY dt.date
That's still only going to count jobs opened on a particular date. You need to test for whether a date is between any job's open and close date
yes... quite so
As unorthodox as the join seems, this might do what you want. SELECT dt.date, COUNT(bg.ig) FROM dates dt LEFT JOIN Bug bg ON dt.date BETWEEN bg.open_date AND bg.close_date GROUP BY dt.date
Dual is a special, one row table. You can use it to "select a function" like I've done above. It exists in all Oracle Databases. You can read more about it [here](https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1562813956388)
Your candidate keys for the line table are: * (order_id, donut_id) * (order_id, donut_name) -- assuming names are unique To be in 2NF, all the columns that aren't part of a key must be determined by the whole key. But donut_description and price are determined by just donut_id. One of the two columns in your candidate key. So you need to remove these from the line item table. And stick them in a donut table. So how do you get down to three tables? Well, it depends on what your functional dependencies are ;) But I'm guessing you can stick all the customer attributes in order table. And ditch customer table. If order_id is the only key for this table, it's now in 2NF (no columns depend on part of the key). But not 3NF. The customer attributes depend on the non-key column customer_id. But (cust_id, order_date) could feasibly be a candidate key for the order table (a customer can only place one order at a time). Which would make this not in 2NF. So to answer this you need to ask your teacher what all the functional dependencies are. The question is unanswerable until you know these ;)
My latest attempt (still waiting for it to be graded) goes as such: ORDER: **order_id**, date, customer_id, first_name, last_name, street, apt, city, state, zip, home_phone, mobile_phone, other_phone, special_instructions LINE: ***order_id***, ***donut_id***, quantity, donut_price DONUT: **donut_id**, donut_name, donut_description, donut_price The "donut_price" attribute, while not necessary, is copied (but not a foreign key) in the LINE relation simply for accounting historical accuracy. My problem stemmed from the fact that I was a late add-on to the class (I'm doing an accelerated learning path), so I didn't receive the material that was sent to all the students at the beginning of the term that shows a case study for normalization that almost identically mimics the requirements of final assessment. After getting a meeting with the course mentor (who sent me that document), I understood quite well why my attempts were being rejected. :) I'm almost positive I have the correct answer now and thank you, and everyone else in this thread for your/their assistance.
That was it, thanks! I can't believe I forgot to test for the close date as well.
I would add an incremental key and break those 2000 columns into sets of 500 or less an see how that works, then join it all together and dump it into a table. It sounds like you have some cells which are in a format that SQL doesn't like, either they're too long for the datatype you're trying to import them as, or they're incompatible for other reasons. Someone else may be able to give you a solution to find these cases but that's what it sounds like. edit: You might also try selecting all 2000 columns and all 20,000 rows, converting them to text in Excel, then cutting them out and pasting them into a new workbook, saving, and trying to import that.
I know you want specific help with this error so you can understand it better, but as for the goal you are trying to accomplish, it might help to look at a working example: https://stackoverflow.com/questions/12364086/how-can-i-achieve-initcap-functionality-in-mysql
What is the character after "WHILE values_to_be_transformed !=" ?
Yes there could be due to Joins, as any of the table in join might have duplicate values due to which it finding more that a row matching in Other table so it returning duplicate value
I tried both things you say and still got error, not the same error tho
_I tried both things you_ _Say and still got error, not_ _The same error tho_ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^edugomez28 ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
What error do you get if you take just the first 50 columns, copy them into a new file and try to import? 
Does the hash need a space after it to comment the rest of the line? The error code made me think there's something wrong with your commented line. I looked up how to comment in MySQL and I read double dashes doesn't work unless you add a space between the final dash and what is commented. There were some examples of using the hash, but they added a space too. 
- Copying to [dbo].[Sheet2$] (Error) Messages Error 0xc0202009: Data Flow Task 1: SSIS Error Code DTS_E_OLEDBERROR. An OLE DB error has occurred. Error code: 0x80004005. An OLE DB record is available. Source: "Microsoft SQL Server Native Client 11.0" Hresult: 0x80004005 Description: "Error no especificado". (SQL Server Import and Export Wizard) Error 0xc020901c: Data Flow Task 1: There was an error with Destination - Sheet2$.Inputs[Destination Input].Columns[participants-1-stats-firstInhibitorAssist] on Destination - Sheet2$.Inputs[Destination Input]. The column status returned was: "The value violated the integrity constraints for the column.". (SQL Server Import and Export Wizard) Error 0xc0209029: Data Flow Task 1: SSIS Error Code DTS_E_INDUCEDTRANSFORMFAILUREONERROR. The "Destination - Sheet2$.Inputs[Destination Input]" failed because error code 0xC020907D occurred, and the error row disposition on "Destination - Sheet2$.Inputs[Destination Input]" specifies failure on error. An error occurred on the specified object of the specified component. There may be error messages posted before this with more information about the failure. (SQL Server Import and Export Wizard) Error 0xc0047022: Data Flow Task 1: SSIS Error Code DTS_E_PROCESSINPUTFAILED. The ProcessInput method on component "Destination - Sheet2$" (779) failed with error code 0xC0209029 while processing input "Destination Input" (792). The identified component returned an error from the ProcessInput method. The error is specific to the component, but the error is fatal and will cause the Data Flow task to stop running. There may be error messages posted before this with more information about the failure. (SQL Server Import and Export Wizard) Error 0xc02020c4: Data Flow Task 1: The attempt to add a row to the Data Flow task buffer failed with error code 0xC0047020. (SQL Server Import and Export Wizard) Error 0xc0047038: Data Flow Task 1: SSIS Error Code DTS_E_PRIMEOUTPUTFAILED. The PrimeOutput method on Source - Sheet2$ returned error code 0xC02020C4. The component returned a failure code when the pipeline engine called PrimeOutput(). The meaning of the failure code is defined by the component, but the error is fatal and the pipeline stopped executing. There may be error messages posted before this with more information about the failure. (SQL Server Import and Export Wizard)
select tbl1.year_quarter, tbl1.county, tbl1.start, tbl1.end, T1.cnt from ( select query1.country, count(*) as cnt from query1, tbl1 where query1.county = tbl1.county and initiate_date between tbl1.start and tbl1.end and query1.enter_date&gt; tbl1.end ) as T1 where county=t1.county
In this version, I'm using back ticks. I tried with single quotes as well and I got the same error.
I got the same error with the line uncommented. I commented that line out because I originally get the same error but it said it was on line 5 instead of 4. 
It sounds like you're going to need to go through the columns and make sure they are importing in a compatible datatype. I'm unaware of there being any easy way to do this. You have 2000 columns so you need to go through them and see how they're mapping in the wizard. I mean a simple way to understand this is to go 1 column at a time and try to import it. If it works, you know that column isn't bad. If you try to import 100 at a time, you might only have 1 column that is incompatible. A simple work around might be to create a blank table that has all 2000 columns in a generic datatype... then go into the edit window and start pasting rows in groups of 500 or 1000. Beyond that you've moved into a space where I am not appreciably learned as far as tips and tricks go. I would start going through each column and seeing how they're mapped and making sure its compatible. Shitty job.
You should stick with single quotes for delimiting strings. It's a bit hard to tell which line is line 4. Do you think it is this line? If so, try testing the length of values_to_be_transformed instead of comparing it to an empty string. WHILE LENGTH(values_to_be_transformed) &gt; 0 DO
&gt; DECLARE value_add varchar(250); This is the line where the message says the error is.
use this syntax substr(string,instr(string, "abc",1),7)
&gt; You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use **near ''** at line 4 This is the part that is confusing to me; it looks like there's a problem with something that isn't showing up in your code. I don't have MySQL on this laptop, so I can't test this directly, but my suggestion would be to try just compiling the function with the body commented out and see if that helps. That is: CREATE FUNCTION initcap(values_to_be_transformed VARCHAR(250)) RETURNS VARCHAR(250) DETERMINISTIC BEGIN DECLARE value_add varchar(250); /* #DECLARE i INT DEFAULT 1; WHILE values_to_be_transformed != DO SET value_add = CONCAT(UCASE(LEFT(values_to_be_transformed, 1)), RIGHT(LOWER(SUBSTRING_INDEX(values_to_be_transformed,' ',1)))); INSERT INTO piece(piece_value) VALUES ( value); IF LOCATE(',',value_add) &gt; 0 THEN SET values_to_be_transformed = SUBSTRING(values_to_be_transformed,LOCATE(',',values_to_be_transformed)+1); ELSE SET values_to_be_transformed = ''; END IF; END WHILE; */ return select CONCAT_WS(' ', piece_value) from piece; END 
Yep, even with most of the body commented out I still get the error. From what I've read in [the documentation](https://dev.mysql.com/doc/refman/5.7/en/create-procedure.html) I've written everything correctly.
Ignore everything I said above. I believe that you're running into a problem because whatever you are using to edit your function (MySQL Workbench?) is seeing the semicolon and trying to interpret it as a local command rather than as part of the function definition. See https://stackoverflow.com/questions/6740932/mysql-create-function-syntax for a solution.
it is better if you could provide sample data and expected result
Perhaps I don't really understand your question. Generally, there are two types of commands that affect data in tables: 1. Insert 2. Update Insert adds data to a table and update changes existing data within a table. You can also use a MERGE command; but if you are just starting -use those two. A stored procedure can run multiple combinations of the two (or none at all) so if you wanted to do something like 1. Check to see if a runner exists or not 2. If the runner doesn't exist in the database add that person to the runners table 3. Add the race results to the runner. Then a stored procedure may do these multiple steps. You can also use a temporary table (preferred) or declared variables to have the different steps interact with one another. But, like I said -I don't really understand what you are doing exactly. So take what I'm saying with a grain of salt.
A perspective that I have taken before is. 1. Think of a use case, it appears there is one listed but I can probably think of about 8 here. A. Creating a new course (insert) B. Editing an existing course (update) C. Deleting a course (delete or update for soft delete like isactive flag) D. Creating a new race E. Editing a race F. Deleting a race G. Adding an entry to represent a competitor competing in a race H. Updating the entry I. Deleting the competitor race info Each of these "actions" or use cases probably wants its own sproc. They don't have to be massive or complicated. They may very well do stuff to multiple tables if they want to. Some (especially the Create and Update) will probably have as many parameters as the web form has fields. The delete sproc may just take an ID from a select list or checkbox from the front end. Future enhancements - multiple competitor race times import from spreadsheet. Multiple record delete, update possibly using fn_split for a multi valued list param. 
This does not look complex. What part of this seems to be causing concern for complexity?
I understand the query and I agree that it's not very complex. The trouble I'm having is translating it into Ecto DSL syntax specifically. I couldn't really figure out a way to have those inner joins work with Ecto. So I was hoping someone could help me re-write this with Ecto syntax in mind. Could this query be written differently, perhaps with subquery?
Based on your code, I'm going to assume you are programming against Microsoft SQL Server, but its always good to make that clear in your comments. :) A few things: First - you are creating an AFTER trigger but trying to enforce integrity. It is best to use a INSTEAD OF trigger as this trigger will fire and check your integrity before the row hits the table. See the link here: https://docs.microsoft.com/en-us/sql/relational-databases/triggers/dml-triggers Second - The way you have written your EXISTS it would be checking the ENITRE Orders and CustReviews tables EVERY TIME an INSERT was made. So assume you are Amazon and were searching through millions of customers and billions of reviews - yea not great. Look into the use of the reserved tables inserted and deleted. They are special tables and hold the data that's currently being deleted or inserted into a specific table. See the link here: https://docs.microsoft.com/en-us/sql/relational-databases/triggers/use-the-inserted-and-deleted-tables Another good resource is here: Look at Section C. - Using a DML AFTER trigger to enforce a business rule between the PurchaseOrderHeader and Vendor tables https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql This uses an AFTER trigger. Typically I like to use INSTEAD OF because what happens if a Customer DOESN'T exist and tries to write a review? The way your AFTER trigger is currently written, it would return ANY row because at least one customer is going to have one review meaning it will exist. If you have had INSTEAD OF plus the use of the inserted table you would stop any possibility of this occurring. Again, you could get away with an AFTER trigger with the use of the inserted table, but I just tend to like to put all my Business Logic and Consistency checking in the Trigger if I am going that route. Also, it ensures I'm never hitting the actual constraint on the table if defined. This can have a bigger impact on performance.
As a follow-up here is what the code might look like - don't have my SSMS open at the moment. CREATE TRIGGER TI_CustReviews_CheckOrder ON Sales.CustReviews INSTEAD OF INSERT /************************************************************************* Instead of firing on only INSERTS, you may want to also amend or add another TRIGGER for UPDATES as well, but that's another level of programming. *************************************************************************/ AS /************************************************************************* We want to determine what rules are necessary in order to make the INSERT not define the ones that exclude it. Bad data can creep in if we are only excluding particular options. In this case, I have defined the EXISTS to look for three elements: Customer ID, Product ID - NEW If the Order Shipped - CHANGED This ensures that these three business rules are always enforced for INSERTS. *************************************************************************/ IF EXISTS (SELECT 1 FROM Sales.Orders AS so INNER JOIN INSERTED AS cr ON so.custid = cr.custid AND so.productid = cr.productid WHERE so.Ordershipped = 'yes') BEGIN /************************************************************************* You should specify columns as a part of your INSERT, these will be need to be updated should you deced to change the table *************************************************************************/ INSERT INTO Sales.CustReviews (custid, productid, comment) SELECT custid, productid, comment FROM INSERTED END ELSE BEGIN RAISERROR ('Need to place order before posting review', 16, 1); ROLLBACK TRANSACTION; RETURN END GO 
Create a view, so you have only the SELECT, FROM and WHERE parts and don't need to translate the JOINs.
Hey man I really appreciated that you went in depth, really helped thanks.
This was exactly what I was looking for, really appreciate the help
The challenge is that I'm passing in the result data as a table parameter, there might be as many as 1000 results in the table with each row referring to a single runner. Some of the runners will have been seen before so their results should be linked to them, some of them will be running for the first time so a new row will need to be created in the Runners table to cover them. Table structure wise I currently have the following tables and data type, I've purposefully left out the foreign key constraints to save some space. -- Finishers stores the information about the individual results in a race. -- EventId links to the event that they were taking part in -- RunnerId links to the runner's information in the Runners table. CREATE TABLE [dbo].[Finishers]( [Id] [int] NOT NULL, [EventId] [int] NOT NULL, [RunnerId] [int] NOT NULL, [Position] [smallint] NOT NULL, [FinishTime] [smallint] NOT NULL -- Finish time is measured in seconds to save space. ) CREATE TABLE [dbo].[Runners]( [Id] [int] NOT NULL, [Name] [nvarchar](50) NOT NULL ) CREATE TABLE [dbo].[Events]( [Id] [int] IDENTITY(1,1) NOT NULL, [EventNumber] [int] NOT NULL, [LocationId] [smallint] NOT NULL, [TotalRunners] [int] NOT NULL, [EventDate] [date] NULL ) My current thinking is that I'll need to use some form of join statement to find all the runners that don't already exist and then pass the result to a second stored procedure to create records for all the missing runners. Following that I'll need some form of join statement to get the data I need to store the data into the Finishers table.
&gt; Just a bit of simplification would be really helpful. SELECT sq.version AS version , xt1.signature AS harness , xt1.status AS harness_status , xt1.job_uid AS job_uid , xt1.test_ring AS ring , sq.Id AS test , sq.Status AS test_status , timestampdiff(MINUTE, sq.StartTimestamp, sq.EndTimestamp) AS exec_time FROM table t1 INNER JOIN table1 xt1 ON xt1.pjob_id = t1.job_uid AND xt1.status &lt;&gt; 'Invalid' AND xt1.job_type = 'Harness' AND xt1.test_ring IS NOT NULL LEFT OUTER JOIN ( SELECT t3.Status , t3.StartTimestamp , t3.EndTimestamp , t5.version , t5.Id FROM table3 t3 INNER JOIN table4 t4 ON t4.GId = t3.GId INNER JOIN table5 t5 ON t5.Id = t4.Id ) AS sq ON sq.JobId = xt1.job_uid WHERE 1 = 1 AND t1.status &lt;&gt; 'Invalid' LIMIT 1000 i dropped t2 from the query altogether, because you don't use any columns from it, and it isn't required for the joins (since it's an outer join)
worked on Toad and PLSQL many years. I think toad is much more cluttered, complicated and slow whereas PLSQL developer does the EXACT same things and even more but with much less mess, windows, sub tabs, text and buttons - toad got so many it is confusing.
Good bot
Thank you Cal1gula for voting on haikubot-1911. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Is this a test for school? Why do you need to use a TVP for this? Seems that you could simply do a sequence of inserts.
Why are you using triggers to enforce referential integrity? Wouldn't a foreign key be better? 
It's my personal learning project, nothing to do with school work or any other test. I was looking at the TVP option as it would drastically reduce the number of calls to the database. If a race has over 1000 runners then that's over 1000 calls to the database which isn't really optimal.
&gt; I'll need to use some form of join statement to find all the runners that don't already exist You'd be better off with a subquery and a temp table but I wrote a simple script: &gt; I've purposefully left out the foreign key constraints to save some space. My advice is to not leave anything off. The devil is in the details. This returns the runners_id: Create Procedure [dbo].[Upsert_Runner] ( @name_to_check [nvarchar](50) ) AS BEGIN SET NOCOUNT ON declare @ID int; --Assumes only positive values for ID SET @ID = (SELECT top 1 ID from [dbo].[Runners] where [Name] = @name_to_check if ISNULL(@ID, '-1') &lt; 0 BEGIN SELECT @ID RETURN END --IF NOT FOUND THEN INSERT --Only works if you are auto updating ID field in runners table INSERT INTO [dbo].[Runners](Name) VALUES (@name_to_check) Select SCOPE_IDENTITY() as ID return END Edit: forgot to put compare value in if statement
That is a pretty good idea. Thank you!
Thanks! This is helpful as well. Atleast with this I can try to structure the Ecto DSL with a bit more ease.
Are you just looking to generate more intervals? DECLARE @StartDate smalldatetime = CONVERT(date, '2017-01-01') , @EndDate smalldatetime = CONVERT(date, GETDATE()) , @Days int; SELECT @Days = DATEDIFF(DAY, @StartDate, @EndDate) + 1; WITH cte10 AS (SELECT NULL AS n FROM (VALUES (0) , (0) , (0) , (0) , (0) , (0) , (0) , (0) , (0) , (0)) AS a (n) ) , tally AS (SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM cte10 AS a CROSS JOIN cte10 AS b CROSS JOIN cte10 AS c) , cteDays AS (SELECT TOP (@Days) DATEADD(DAY, t.n, @StartDate) AS [Day] FROM tally AS t) , intervals AS (SELECT DATEADD(MINUTE, 15 * t.n, d.[Day]) AS interval_start , DATEADD(MINUTE, 15 * (t.n + 1), d.[Day]) AS interval_end FROM (SELECT TOP (96) tally.n FROM tally) AS t CROSS JOIN cteDays AS d WHERE t.n &lt; 96) SELECT * FROM intervals;
Yes and no. Ideally what I want to create is an interval "filter" that I can apply to any date range I select in the "thereal" sub query; I was trying to convert the @today value to only be like 06:00-19:00 with no specific date limitations. Thanks for the response! 
06:00-19:00 for a single day or 06:00-19:00 each day between Jan 1 and Jan 12 or Jan 1 06:00 - Jan 12 19:00 
For each day; so 0600-1900 Jan 1, 0600-1900 Jan 2 etc. Frankly it can be continuous, as the events being tracked should 99.9% of the time happen during those hours anyways, but for simplicity's sake a start and end date each day would be ideal. 
 DECLARE @StartDate smalldatetime = CONVERT(date, '2017-01-01') , @EndDate smalldatetime = CONVERT(date, GETDATE()) , @StartTime time(0) = '06:00' , @EndTime time(0) = '19:00' , @Days int , @Intervals int; SELECT @Days = DATEDIFF(DAY, @StartDate, @EndDate) + 1 , @Intervals = DATEDIFF(MINUTE, @StartTime, @EndTime) / 15; WITH cte10 AS (SELECT NULL AS n FROM (VALUES (0), (0), (0), (0), (0), (0), (0), (0), (0), (0)) AS a (n) ) , cte1000 AS (SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM cte10 AS a CROSS JOIN cte10 AS b CROSS JOIN cte10 AS c) , cteDays AS (SELECT TOP (@Days) DATEADD(DAY, t.n, @StartDate) AS [Day] FROM cte1000 AS t) , cteMinutes AS (SELECT TOP (@Intervals) DATEDIFF(MINUTE, CONVERT(time(0), ''), @StartTime) + t.n * 15 AS StartMinute , DATEDIFF(MINUTE, CONVERT(time(0), ''), @StartTime) + (t.n + 1) * 15 AS EndMinute FROM cte1000 AS t) , cteInterval AS (SELECT DATEADD(MINUTE, m.StartMinute, d.[Day]) AS interval_start , DATEADD(MINUTE, m.EndMinute, d.[Day]) AS interval_end FROM cteDays AS d CROSS JOIN cteMinutes AS m) SELECT * FROM cteInterval;
SQLNCLI is a bit like a package that contains both OLE DB and ODBC. This was deprecated in 2012 and mainstream support just ended in June 2017. After that ODBC was shipped on its own and OLE DB was deprecated. SQLOLEDB is the old version of the OLE DB driver, before it was shipped with ODBC in SQLNCLI. 
Thank you! So help me understand exactly what you did, it looks like you just added a start date (01/01/2017) and times, then an end date of today? 
The trick is to separately create a dataset for intervals (without a day component) and a dataset for days (without an interval component). Because each of them are consecutive things, they are easy to create from a list of consecutive numbers. Then they can be joined to create complete intervals with both date and time components. cte10 and cte1000 create consecutive numbers that will build our date and interval. cteDays builds a data set of dates between @StartDate and @EndDate. I use TOP (@Days) instead of a WHERE filter for efficiency. The cte1000 number builder will create the minimum number of records, instead of the full 1000 then reducing them. cteMinutes builds a dataset of desired intervals per day, as integer minutes from midnight. That makes it easier to use with DATEADD later. Same story with TOP (@Intervals). cteInterval cross joins cteDays and cteMinutes creating the complete list of intervals with date and time components.
Thanks for your help! I have a question if you don't mind; since we're separating out days and intervals, is it possible to set it so we're only using the interval times (for example 0600-1900 regardless of date)? That way the date can be determined by one of the other select statements based on the data I'm looking at and doesn't need to be manipulated as time goes on. 
I learned something. Thank you for this.
There is a list of training resources in the sidebar. I bought https://www.amazon.com/SQL-Minutes-Sams-Teach-Yourself/dp/0672336073 and its pretty handy. I get most of my basics now from w3schools or stack overflow.
Looks like this is for MS SQL.... Could be wrong but my guess is that "tablename" is actually a user defined table valued function so you would have to check and look at its code. It would be setup to take two parameter variables, a date and an integer so that is what the "GETDATE(),1" is doing initially, but you would need to check the function for what it uses those variables for. 
Judith Bowman's books are good.
&gt; is actually a user defined table valued function That's what it looks like.
I'm guessing based on the limited information you've provided, but m2.naam is the chef's name, while m1.chef is some sort of identifier. If you can post table definitions or sample data, you'll get better answers.
http://imgur.com/a/KRJK1 the assignment says : give a summary of all employees who are cheff together with the inferiors/subordinates in 1 colom with the text : ''is chef of'' in the second colom there should be displayed the age difference between the cheff and in inferior in days, sort the table by names of the cheff
i also dont understand why they use m2.mnr because in the table i dont see the mnr comming back?
It says this: * When the tran_date field is between 360 days before today's date, and 180 days before today's date, then multiple the sign and amount columns. Else return 0. Call this column bucket2. Since UNION requires the number of columns to match, bucket2 is required on the second statement. So they are simply setting bucket2 = to a string "Clt Bal 180 days prior'. Since this is an aging report it seems to be 180 days aging. I *assume* there are only 2 rows returned? when you run the query? The first is the text "Clt Bal 180 days prior" and the 2nd is the 180+ bucket sum? Since you haven't included that, I am guessing. However, your expected results aren't making sense to me. "I'd just like to apply a simple aging to these balance". The code already is doing that. What exactly do you want to add? Another bucket for a different time period?
It sums all transactions (`sign * amount`) older than 180 days, but not older than 360 days from today. Doesn't seem like it would return a balance of any kind, just the sum of transactions during that time... you would need to add these transactions to the starting balance 360 days ago to figure out the balance 180 days ago.
Ok, that makes sense, I think. So is the code saying - this is what the balance was 180 days prior? As in, it's looking at a snapshot from 180 days ago to what the balance was at that point in time? Or is the code saying this is the balance of items aged 180+?
select m2.naam || ' is chef van ' || m1.naam "baas en medewerker", m2.gbdatum - m1.gbdatum leeftijdverschil from medewerkers m1, medewerkers m2 where m1.chef = m2.mnr order by m2.naam || ' is chef van ' || m1.naam as you are concatenating two columns together use the same syntax in order by 
Between 180 and 360 days prior. Multiply sign * amount. Sum the total. Try not to think in accounting terms like balanced and aged, it makes it more confusing. We're counting integers (days prior) and multiplying decimals and summing decimals.
Much cooler, even: `RANGE` allows for *"logical windowing"* where the range includes a fixed time or date range. Unfortunately, not all databases support this: https://blog.jooq.org/2016/10/31/a-little-known-sql-feature-use-logical-windowing-to-aggregate-sliding-ranges/
&gt; Try not to think in accounting terms I think that's the problem I keep running into. I'm a financial analyst, trying to teach myself SQL for reporting purposes...and sometimes it's hard to turn that off. :/ 
Yeah that's "get the table name based on the value returned by the 'tablename' function when given two parameters". I had no idea you could do that, even in Sql Server.
I can't even tell what flavor of SQL this is but if it's SQL Server: SELECT EOMONTH(GETDATE(), -24) Returns `2015-08-31` Or in your case SELECT EOMONTH(AReceived, -24)
This don't have much to do with SQL, you are wanting help with a REST API. You will have better luck on programming subreddits.
Murach's MySQL
Dive in. It's the only way. Http://docs.microsoft.com/sql Use this to install a free instance. Then use BrentOzar's guide to import the Stack Overflow DB. (If you like podcasts/webcasts, subscribe to "Office Hours" by Brent Ozar and "Dear SQL DBA" by Kendra Little. https://www.brentozar.com/learn-query-sql-server-stackoverflow-database/
I subscribe to Pluralsight. It costs thirtyfive a month but also has other classes beyond SQL Server like studying for the PMP and Adobe After Effects. https://www.pluralsight.com/
This [website](http://sqlzoo.net/) is really helpful for upping your SQL skills because it gives you instant feed back and provides problems that aren't overly easy to google. This [book](https://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145) is excellent and written by a God that is known to mortals as Itzik Ben-Gan. This [online SQL code formatter](http://www.dpriver.com/pp/sqlformat.htm) is my personal favorite When you hit the wall [this](https://teespring.com/shop/Help-Me-Stack-Overflow-2?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=google_shopping&amp;aid=ts-boosted-pla#pid=522&amp;cid=101877&amp;sid=front) is also helpful Hope that helps and remember in Codd we trust!
I think I found a better way to word this... can I shift Datepart(week,...) to be Friday through Thursday?
/u/scarydba has been blogging about SQL Server fundamentals for a while. check out r/sqlserver and his blog https://www.scarydba.com/category/database-fundamentals/ 
You might want to take a look at SQL Server Central's Stairways. They have a ton of great material, all free. Scroll to the bottom for the most basic stuff: http://www.sqlservercentral.com/stairway/ 
This is the best tl;dr I could make, [original](https://techcrunch.com/2017/08/29/salesforce-is-using-ai-to-democratize-sql-so-anyone-can-query-databases-in-natural-language/) reduced by 68%. (I'm a bot) ***** &gt; Their recent paper, Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning, builds on sequence to sequence models typically employed in machine translation. &gt; A reinforcement learning twist allowed the team to obtain promising results translating natural language database queries into SQL. In practice this means that you could simply ask who the winningest team in college football is and an appropriate database could be automatically queried to tell you that it is in fact the University of Michigan. &gt; &amp;quot;We don&amp;#039;t actually have just one way of writing a query the correct way,&amp;quot; Victor Zhong, one of the Salesforce researchers who worked on the project, explained to me in an interview. ***** [**Extended Summary**](http://np.reddit.com/r/autotldr/comments/6wwvbm/salesforce_is_using_ai_to_democratize_sql_so/) | [FAQ](http://np.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/ "Version 1.65, ~200933 tl;drs so far.") | [Feedback](http://np.reddit.com/message/compose?to=%23autotldr "PM's and comments are monitored, constructive feedback is welcome.") | *Top* *keywords*: **query**^#1 **question**^#2 **database**^#3 **data**^#4 **SQL**^#5
Well, you can do something like this. You just select all your stuff into temp table and use sp_help to get column info. select * into #temp from MyTable exec tempdb..sp_help '#temp' drop table #temp
Thanks for the reply! I'll give this a go as it seems promising, thanks for all the other replies I'll be sure to try them out ðŸ™‚ 
www.sqlpublic.com
Are you looking an advanced professional SQL database recovery tool using an excellent tool SQL database recovery tool. This tool easy to recover corrupt damaged SQL server database. It supports all versions of MS SQL server and all updated Windows OS versions. Visit here - http://www.sql.mdfrepair.net
A HA HAHAHAHAHAHAHAHAHAHAHA [deep breath] HAHAHAHAHAHAHahahahaha.............
&gt; An INNER joint is saying "my set has a working condition". this is the funniest article on here in a long time
&gt; Unlike the inner joint, there are several types of outer joints. i'm dead
You might be looking for /r/SQLServer/ SQL is a language for working with relational data
Not available in my county unfortunately :(
In more recent versions of SQL Server - [this](https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-describe-first-result-set-transact-sql)
Thanks, that worked really well. Much easer that INFORMATION_SCHEMA.COLUMN on 8 or 10 different tables.
best shit ever....
How many Google translates was this run through?
Also: &gt; This joint may seem useless at first glance but it is very fast
I'm assuming that this is SQL Server. Have a look at [SET DATEFIRST](https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql).
 ,FIRST_VALUE(country IGNORE NULLS) OVER(PARTITION BY customer_key ORDER BY last_updated_date desc ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS COUNTRY ,FIRST_VALUE(CASE WHEN country IS NULL THEN NULL ELSE country_code END IGNORE NULLS) OVER(PARTITION BY customer_key ORDER BY last_updated_date desc ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS COUNTRY_CODE 
I want some of the INNER JOINT that OP is smoking...
Looks like this has work, run it through a test and it's come back with my expected results. Will deploy it and see how it goes. thanks!! I almost had it, but had the wrong structure... 
Could you get an execution plan?
My experience with SSMS 2017 is that intellisense is much improved. Also, CTRL + SHIFT + R is your best friend. Those are all the suggestions I have.
Tried that... Still very buggy. 
 &gt; free intellisense replacement? I use notepad++ for large projects. It's not a true replacement for intellisense- it does however make suggestions for what to place. I like it because the search/replace functions are much better.
Good luck on your search. I don't get the hate for Intellisense. It works as expected for me. Maybe I'm doing something differently.
I wish it worked for me - Everyone in my office that uses SSMS has trouble with it.
What does "trouble" and "it's not working" mean? I feel like I'm doing tech support with people who don't know how to use computers. Last week we spent an hour trying to "fix" a monitor for someone in my office. She was *SURE* the thing was working and simply taking vacation somehow blew up her monitor. Yeah she just turned it off and didn't turn it back on. Try CTRL + SHIFT + R
I mean it doesn't work. I get no auto-suggestions 
Proper indices and making your where clause sargable may help. Doing the where like twice with a group by is probably where a lot of your pain is. Doing any case when logic and then wrapping aggregates around that probably hurts too. I'd have to see the execution plan on the latter to be sure though.
Not a good tool for this, but it does work. This may assist you a bit, if not, hopefully someone finds it helpful. SELECT t.NAME AS table_name ,SCHEMA_NAME(t.schema_id) AS schema_name ,c.NAME AS column_name ,st.NAME 'Data type' ,c.max_length 'Max Length' ,c.precision ,c.scale ,c.is_nullable ,ISNULL(i.is_primary_key, 0) 'Primary Key' FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID INNER JOIN sys.types st ON c.user_type_id = st.user_type_id LEFT JOIN sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id LEFT JOIN sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id ORDER BY schema_name ,table_name;
I'm working with a third party to develop our APIs, and this is what they expect from the stored proc.
Does the "name" from the WGConfig table line up with a "name" column in the recordArchive table? If so, this join might be close to what you want. If not, how are the tables related to each other? SELECT c.[name] ,SUM(a.delayTime)/60.0 AS Total FROM dbo.WGConfig c JOIN dbo.recordArchive a ON c.[name] = a.[name] WHERE c.genmodel = 'C:\Zone.mdb' AND a.recordid = 'B123' AND a.[date] BETWEEN '2015-04-15' AND '2015-04-22' GROUP BY c.[name]
Sounds like your are working with a pretty lazy third party. That said, another alternative is to write your own CLR assebly using [json.net](https://www.newtonsoft.com/json/help/html/SerializeDataSet.htm). The advantage of this approach is that the procedure can serialize any table to json. Since you said SQL Server 2008 R2, you will need to target the dotnet 2.0 framework.
Something simple and small you could do: subset your tables before you join then. Select blank,blank,blank From Schema.table V Join (select * from schema.table where ...) P On V.field = P.field Join (select * from schema.table where .....) MMC On V.field = MMC.field
It's not free, but Redgate SQL Prompt blows everything else I've found out of the water. It's incredibly fast, code formatting is almost perfect, and snippets have changed my life (i.e. "yesterday" instead of "DATEADD( DAY, -1, CONVERT( DATE, GETDATE() ) )"). It even hints foreign keys comparisons when you're writing joins. If I'm ever in a DBA gig that doesn't pay for it, but would let me use it, I'll buy a personal license.
Yeah...RedGate is the best. Just not cheap.
The quick and dirty way is to dump the results of the query into another table that is periodically truncated and repopulated. A long term fix involved looking at the execution plan for where table scans are taking place for large tables and prevent them from happening with indexes. If that doesn't help, you may want to look at moving to an in memory database.
Thanks for the quick replies! I am using WinSQL and I'm a beginner so I have no idea how to extract the execution plans
Yes it does line up with the name column in recordArchive table
That's an idea. I've used json.net to deserialize a JSON response in an SSIS job.
Here is a third way, consise but probably not efficient: SELECT [STOP_REGISTRATION] ,[STOP_GRADES] , REPLACE( SUBSTRING( ISNULL(NULLIF([STOP_REGISTRATION],'Y'),'_Registration'),2,20)+ SUBSTRING( ISNULL(NULLIF([STOP_GRADES],'Y'),'_Grades'),2,20) ,'nG','n, G') AS [THINGS_STOPPED3] FROM @stopTable 
&gt; Tip 6 - Use EXPLAIN &gt; Most databases return the execution plan for any SELECT statement that is created by the optimizer. This plan is very useful in fine tuning SQL queries. The following table lists SQL syntax for different databases. Database|Sql :-|:- Oracle:|EXPLAIN PLAN FOR &gt;Your query&lt; DB2:|EXPLAIN PLAN SET queryno = xxx for &gt;Your query&lt; MS SQL Server:|Set SHOWPLAN_ALL ON &gt;Your query&lt; Informix:|SET EXPLAIN Sybase ASE:|Set SHOWPLAN_ALL ON &gt;Your query&lt; &gt; You can also use third party tools, such as WinSQL Professional from Synametrics Technologies to run EXPLAIN commands against databases. ^[source](https://synametrics.com/SynametricsWebApp/WPTop10Tips.jsp)
So does the query above run? Does it do what you want?
A few more options: CASE WHEN [STOP_REGISTRATION] = 'Y' AND [STOP_GRADES] = 'Y' THEN 'Registration, Grades' WHEN [STOP_REGISTRATION] = 'Y' AND [STOP_GRADES] = 'N' THEN 'Registration' WHEN [STOP_REGISTRATION] = 'N' AND [STOP_GRADES] = 'Y' THEN 'Grades' WHEN [STOP_REGISTRATION] = 'N' AND [STOP_GRADES] = 'N' THEN '' ELSE 'Error' END AS [THINGS_STOPPED] Or ISNULL(STUFF( CASE WHEN [STOP_REGISTRATION] = 'Y' THEN ', Registration' ELSE '' END + CASE WHEN [STOP_GRADES] = 'Y' THEN ', Grades' ELSE '' END, 1, 2, ''), '') AS [THINGS_STOPPED]
Beware of **OR**s in your WHERE clause!! When in doubt, use parentheses to make your intention clear! I found this section: WHERE V.DIM_DT_ID BETWEEN 20161001 AND 20161231 AND V.VISIT_CAT_CD = 0 AND MMC.CONTENT_MMC_VEND_METADATA LIKE ('%Display%') OR MMC.CONTENT_MMC_VEND_METADATA LIKE ('%PSocial%') AND MMC.CONTENT_MMC_CMPN_CD IN ... But I think you meant to do this (note the added parentheses): WHERE V.DIM_DT_ID BETWEEN 20161001 AND 20161231 AND V.VISIT_CAT_CD = 0 AND ( MMC.CONTENT_MMC_VEND_METADATA LIKE ('%Display%') OR MMC.CONTENT_MMC_VEND_METADATA LIKE ('%PSocial%') ) AND MMC.CONTENT_MMC_CMPN_CD IN ... Get the parenthesis correct, or else there can be a huge difference not only in performance but also in the results.
Which RDBMS are you using?
Is this for MSSQL? I've never done this exact method before, but you could try putting all 200 SQL queries into a stored procedure USER_SP_TEST and then run sp_depends: EXEC sp_depends @objname = N'USER_SP_TEST' It should analyze the stored procedure and give you a list of all tables/ columns that it depends on. One thing to keep in mind is that if you are selecting on a view it will tell you the column in the view and not details on the view's dependencies.
This is a great way to handle it, unfortunately it's for an Oracle DB where I only have read access. I was hoping for more of a script or program to analyze the .sql files, rather than interact with the DB itself. 
It's an Oracle DB, however I am hoping to do this outside of the database and only use the scripts which were exported from their database. 
Not sure your going to be able to do it without interacting with the DB. Only way if all of the columns are referenced like [FULL_TABLE_NAME].[COLUMN_NAME] then you could probably just do some text parsing to get all the tables/ columns that are referenced, but if there are any "*"'s or referencing just the column name without the table then it's probably not likely to get anything meaningful from just the sql files.
Yes, I've made the modifications (had to use a different table) but yes, it works now! Thanks for the help!
codeacademy was great for the basics. They also have some good stuff to take you into intermediate use, like subqueries. w3schools is great for general reference material as well.
I'm guessing that the naming convention isn't strictly adhered to, so you'll need to make a third table with mappings (for convenience more than anything) between each naming convention. I feel for you, looks like a right shit show.
Yep, sql prompt is the best. I use ssms tools pack for auto query saving and snippets, but sql prompts auto complete join syntax and other shortcuts save so much time writing sql.
If the slightly different naming convention is consistent, could you use a regular expression string?
the general scheme would be to use a full outer join between the subquery from the dealership sales (returning dealership, dealer model name and total quantity sold) and the manufacturer table via the dealership name and some way to link dealer model name to manufacturer's model name (e.g. via a mapping table or a string function). If you do not care for getting the mapping 100% correctly and expect that the naming conventions line up pretty consistently, you can also get a record counter to each dealership/respective model name and match the sets on this sequential number instead.
If you're willing to try a different IDE, check out [DataGrip](https://www.jetbrains.com/datagrip/). I prefer their 'intellisense' over SSMS built-in and RedGate (which I used for years) - also has code formatting, join hints, and decent dark themes (see [Material UI EAP](https://plugins.jetbrains.com/plugin/9377-material-theme-ui-eap)), and a ton more... if you're paying for it out of pocket (individual license) it's $89 USD, which is pretty reasonable.
Solution currently in place: SELECT all entries from 'step_materials' table. Compare the results to materials in current 'step' object. Create a list of IDs that either need removed from or inserted into 'step_materials'. The inserts are done individually, though I could mogrify them if performance is an issue. The delete is done using a "WHERE material_id IN [array]". It turns out I didn't need to do any UPDATEs because a step either has or doesn't have a material. If there ends up being a column that has data that can be updated, I can handle it the same as the inserts.
I thought along similar lines. You could get all tables and views referenced by grepping lines containing FROM or JOIN. A bit of a last resort, imo, since quality level drops quickly. 
You could start this mapping table with a query: cross join the two lists of names and calculate string distance between all pairs of names. The lowest scoring match(es) for each name are likely the right ones. You can tick them off with a visual check. In Oracle you'd do that with the built-in function utl_match.edit_distance, other libraries hopefully have similar functions.
You could create a simple bash or python script to count all words (\w+) between FROM and WHERE in those files. Then just look at what words (tablenames) are most common.
Formatted code: SELECT CONTENT_MMC_CMPN_CD AS CMPN_CD , CONTENT_MMC_VEND_METADATA AS VENDOR , CONTENT_MMC_CAT_METADATA AS CATEGORY , CONTENT_MMC_PLACEMT_METADATA AS PLACEMENT , CONTENT_MMC_ITM_METADATA AS ITEM , COUNT(DISTINCT CASE WHEN FIRST_PAGE_FLG_BNRY = 1 THEN SESN_ID ELSE NULL END) AS VISITS , COUNT(DISTINCT CASE WHEN PAGE_ENGAGEMT_TYPE &gt; 0 THEN SESN_ID ELSE NULL END) AS ENGAGED_VISITS , DIM_DT_ID FROM (SELECT P.SESN_ID , MMC.CONTENT_MMC_CMPN_CD , MMC.CONTENT_MMC_VEND_METADATA , MMC.CONTENT_MMC_CAT_METADATA , MMC.CONTENT_MMC_PLACEMT_METADATA , MMC.CONTENT_MMC_ITM_METADATA , MMC.CONTENT_MMC_OFFERG_CD , P.FIRST_PAGE_FLG_BNRY , P.PAGE_ENGAGEMT_TYPE , V.DIM_DT_ID FROM V2WBTF2.V_FCT_PAGE_VW P INNER JOIN V2WBTF2.V_FCT_VISIT V ON P.SESN_ID = V.SESN_ID INNER JOIN V2WBTF2.V_FCT_MMC_CLCK MMC ON MMC.SESN_ID = V.SESN_ID WHERE V.DIM_DT_ID between 20161001 and 20161231 AND V.VISIT_CAT_CD = 0 AND MMC.CONTENT_MMC_VEND_METADATA LIKE ('%Display%') OR MMC.CONTENT_MMC_VEND_METADATA LIKE ('%PSocial%') and MMC.CONTENT_MMC_CMPN_CD in ( '000001DE', '000000QR', '000000XG', '000000TJ', '000000XA', '000000XB', '000015HM', '000000TK', '000000XC') ) X GROUP BY CONTENT_MMC_CMPN_CD , CONTENT_MMC_VEND_METADATA , CONTENT_MMC_CAT_METADATA , CONTENT_MMC_PLACEMT_METADATA , CONTENT_MMC_ITM_METADATA , DIM_DT_ID Can't say much without indexes or the data. But like %x% is always going to be painful for any RDBMS and I think you got the OR wrong (use parentheses).
you can [free download SQL Repair tool](http://www.sqlrecoverytool.com/) which is capable to recover SQL database objects like Tables, Views, functions, SPs, keys etc of SQL Server 2016, 2014 &amp; all below versions MDF/NDF files.
Have you tried without a where/Group by? You'll have to remove the count from your select as well but it should confirm whether your where clause is causing it. Also, I always used to test my queries in a simple Matrix view before adding anything fancy. Are you displaying the results in a Tablix or Matrix?
You have your current tables in Access? In theory, you could create some forum software that connects to the existing database, and read/write data to it. Its probably a better idea that you move your tables to another actual database before starting though. Access isn't really built with that in mind. What programming language were you planning on using?
Got this from a recruiter: &gt;Intermediate SQL (Data Aggregation, CASE, CAST, INSERTS, etc.) Depends on your personal comfort with features. Someone can be an expert in Excel, for instance, up until the second they learn about VBA, then they are a toddler in the woods again, even if by the business needs they are SME.
Please let me know if you need me to further my explaination. I am exhausted and confused. This may be easy to do but I suck at SQL. Again, thank you if you took the time to read.
I think this is my favorite breakdown of skills. https://softwareengineering.stackexchange.com/a/181657/256008 Based on your courses completed, I'd consider that very beginner and just put that you know basic SQL Syntax. Kind of like programming, there is a very large depth pool of knowledge in SQL. The amount of time you put in is what you'll get out of it. 
I like this list except that they put cursors in with beginner. Cursors are like, the best way to write bad SQL code, especially at a beginner level. Cursors allow someone to skip learning about set-based theory and turn SQL into iterative language. Should be in the intermediate/advanced. I put them in the same bucket as triggers. Useful in very specific cases when you know the exact impact, otherwise avoid like the plague.
Nevermind, I believe I can just use the top 100 to get a general idea of what the tables are composed of. Then just create my own examples using the Mere Mortals text. I guess I was just confused on how the tables were named. the SELECT * without a WHERE seems to pull everything up. If you do know of a walk through or a better book for me just let me know. Or if you seem to know the reason for me not being able to download .mdf that would also be helpful. Thanks for those that viewed this!
Having used a few recruiters lately, they are not the source I would use for any kind of knowledge at all. They are the least knowledgeable people involved in the hiring process--seriously. I would not want to work at a company where a CASE statement, or INSERT was considered intermediate level... I do agree with your assessment that each skill set is different and someone could be an expert in one focus and lacking in another.
If you would prefer walk throughs and examples, I think adventure works is a better starter for you. World Wide Importers is the future of sample databases for MS, but there is less material because it's much newer. Otherwise I mostly tell people to use WWI and not AW, but in your case it makes sense in the reverse. If you are doing basic things to get data out of the DB, I don't think you need the depth and breadth of every feature that has come along since then. Just the basic useful tool set to start and then when you feel comfortable, work your way up the chain. https://msdn.microsoft.com/en-us/library/jj590844(v=sql.120).aspx 
I'm on the fence personally, I totally agree with all of your comments though and it probably should be an intermediate skill. I think it's good for someone who lacks the skill to be able to extract what they need, even if it's RBAR. Best practices would be set based unless absolutely necessary, and different engines do cursors differently. They are pretty easy in Oracle, but SQL Server makes them so much more difficult.
For the last couple of years, I've been using [dbForge Studio Express for SQL Server](https://www.devart.com/dbforge/sql/studio/) . The express version is free (you just have to register on their website) and has incredibly reliable intellisense along with a ton of other features that SSMS is lacking. I too have given up on SSMS - even the latest version. The intellisense is complete junk for anything but very simple joins/queries. And you can't even sort the results in the grid? Or filter? C'mon - I shouldn't have to install addins for basic functionality. In my previous job I used TOAD. It was fantastic, but too expensive for a license. They do have a free version, but there were some limitations that keep me from using it currently (don't remember what they were).
change your group by order
Thanks! Would you mind being a little more specific? I'm not seeing the issue right away. Is it because I'm grouping by employee first, and not datetime? 
basic: much of your time is spent on the internet (researching the issue - not youtubing or whatever) intermediate: you know enough to be able to develop and/or troubleshoot most queries on your own, with only minimal internet support. advanced: you know how to use features that most don't, and you rarely the internet for more than a minute or so (quick syntax checks happen - don't care - but it's not about learning concepts or finding out which features will solve the problem)
Yep. Personally, I believe that a truly seriously advanced SQL writer should have to look up how to cursors in the rare case they're needed. If you know what the different cursors do and what the features are, either you have a photographic memory or it's more likely you're an [expert beginner](https://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/) at databases. 
check your case statement
**Basic:** Proficient with all clauses, and can write normal queries (several joins). **Intermediate:** Proficient with a variety of functions, and can write advanced queries (many joins and subqueries). Can design tables and indexes. **Advanced:** Understands how databases physically work (not just logically) and can write optimized queries. Can design and integrate multiple databases.
This article series is awesome. I've had a few teammates who were expert beginners. Even with those titles mentioned "fellow", etc. One of my bosses indexing strategy was literally to just do whatever SQL execution plan suggested. Every time. He didn't ever read the plan or try to come up with a strategy. Just verbatim "did you add the index the plan suggested". I had to eventually spend a bunch of time researching on my own to figure it out. Now my SQL is way better than his ever was. At the time though, I thought that's how you did SQL tuning because that's what he taught me. Another example, a different "fellow" who put every query in a view. Every single one. NOLOCK everywhere. His justification was that it's super quick to query the view and if he needed to change the query he could simply update the view and it took about the same time as rewriting the query. So we ended up on a project together. He had written a few SSRS reports which tracked profit and a few dozen other accounting data pieces from a credit card processing system. Since the data came from a bunch of different tables, there were probably a dozen or more nested views in this SSRS report. Not a huge deal I guess. Except one thing. The dude had hard coded all of the fiscal years into one of those views like 6 nests deep. So the customer would call him up every month when they ran the report because the data would come back wrong or null. And he would update the view with the new dates. So yeah, this "technical fellow" with the highest position in the technical department couldn't figure out how to give the user the ability to choose their fiscal period through a parameter using a dateadd function, so he just had them call him to update the view when they ran the report. I quickly realized I needed to get a new job ASAP. 
Are you intending to alias `employee` as `SkillTargetID` in the first cte, or are you missing a comma?
Each of the things you mention is a sort of bastardized strategy that does make sense under certain conditions (e.g., wrapping everything in a view *is* useful in things like dw systems policy) but it sounds like they overgeneralized. 
Oh yeah. Sometimes those practices can be good. Using the index SQL suggests is often good. Views have their place. It was just like the culture of expert beginners as described in the article. The "fellow" with the hard on for views taught everyone else to do that. Junior guy fresh out of college? I look at some of his work and... Everything is in a view. Just because that's what he was taught by the senior dude. edit: Oh yeah I almost forgot. Just like the article. Everything at that company was based around seniority. They paraded everyone's seniority around quarterly. I mean they literally had a department/company meeting and celebrated it with a party and prizes. Someone has 5 years at "the firm" (that's literally what they called it) and you had only 4? They were clearly superior and their title reflected it. People with a certain amount of tenure were untouchables.
on a completely different track than the question is asking: why are you putting stuff you have no professional experience in on your resume? if you haven't gotten paid for it or at least donated time doing it to an open source project, you have NO professional skills in SQL, either basic or otherwise. you can mention in the interview if they ask you that you've taken some courses in your free time, but putting it on your resume seems extremely dishonest to me.
No that's just a formatting typo, I commented out a line there to show where I thought the problem was and it got moved down when I added spaces to format to post here. 
Not OP and I agree with your points, I think it is reasonable though that if you spent a substantial amount of learning time on a particular technology, you could list it on your resume. If you have not done it professionally, you should be ready to present real life examples / portfolios and be able to answer questions they hurl at you with that technology. 
I pulled your 3 source tables together with a join on SkillTargetID, instead of cross joining ctes. SELECT Agent = (P.LastName + ', ' + P.FirstName) , ReasonCode = CAST(AED.ReasonCode AS varchar) , NotReadyCodeDescription = CASE AED.ReasonCode WHEN '17' THEN 'Break/LunchTime' WHEN '19' THEN 'Documentation' WHEN '0' THEN 'Default' WHEN '999' THEN 'Supervisor Log Off' WHEN '25' THEN 'Project' WHEN '16' THEN 'Additional Wrap Up' WHEN '5' THEN 'End of Shift' WHEN '3' THEN 'Lunch (TryCHOP)' WHEN '18' THEN 'Coaching' WHEN '2' THEN 'Break (TryCHOP)' WHEN '22' THEN 'Outbound Call' WHEN '26' THEN 'Technical Issue' WHEN '6' THEN 'Training' WHEN '7' THEN 'Meeting' WHEN '50002' THEN 'System-Unexpected Logout' WHEN '50003' THEN 'System-Technical Issue' WHEN '50004' THEN 'System-Agent Timed Out' WHEN '50020' THEN 'System-Agent Skill Group Changed' WHEN '50030' THEN 'System-Agent Skill GRoup Changed' WHEN '20001' THEN 'System-Forced Not Ready' WHEN '20002' THEN 'System-Forced Agent Log Out' WHEN '20003' THEN 'System-Not Ready for Logout' WHEN '50010' THEN 'System-Calls Unable to Route to Agent' WHEN '32767' THEN 'System-RONA' ELSE CAST(AED.ReasonCode AS varchar) END , HourofDay = DATEPART(HOUR, AED.Datetime) , Interval = CASE WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 0 AND 14 THEN '00' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 15 AND 29 THEN '15' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 30 AND 44 THEN '30' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 45 AND 59 THEN '45' END , Date = CONVERT(varchar, AED.LoginDateTime, 110) , TimeUsed = CAST(NULLIF(SUM(AED.Duration), 0) AS decimal(10, 2)) / 86400 FROM pin_sideA.dbo.t_Agent_Event_Detail AS AED INNER JOIN pin_sideA.dbo.t_Agent AS AG ON AED.SkillTargetID = AG.SkillTargetID INNER JOIN pin_sideA.dbo.t_Person AS P ON P.PersonID = AG.PersonID WHERE AG.SkillTargetID IN ('5793', '6004') AND AED.LoginDateTime BETWEEN DATEADD(d, -10, GETDATE()) AND DATEADD(d, -1, GETDATE()) AND AED.ReasonCode NOT IN ('21', '0') AND AED.ReasonCode IS NOT NULL GROUP BY (P.LastName + ', ' + P.FirstName) , CONVERT(varchar, AED.LoginDateTime, 110) , CAST(AED.ReasonCode AS varchar) , CASE WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 0 AND 14 THEN '00' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 15 AND 29 THEN '15' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 30 AND 44 THEN '30' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 45 AND 59 THEN '45' END , DATEPART(HOUR, AED.Datetime) HAVING CAST(NULLIF(SUM(A.Duration), 0) AS decimal(10, 2)) / 86400 != 0 ORDER BY CONVERT(varchar, AED.LoginDateTime, 110) DESC , DATEPART(HOUR, A.Datetime) , CASE WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 0 AND 14 THEN '00' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 15 AND 29 THEN '15' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 30 AND 44 THEN '30' WHEN DATEPART(MINUTE, AED.Datetime) BETWEEN 45 AND 59 THEN '45' END;
It depends how complex the query is... Access talks to SQL server via MSJet, which has it's own parser and has a different set of reserved keywords than the SQL Server. For instance SUBSTRING doesn't exist in MSjet, it's MID instead (albeit with same argument list.) IN SSMS: SELECT * FROM dbo.syslogins where Substring(name, 1, 2)='SA'; In MSJet: SELECT dbo_syslogins.* FROM dbo_syslogins where MID(name, 1, 2)='SA'; As you see, the syntax is quite different. While on SSMS, you have to address a table as schema + "." + tableName, in MSjet, it's instead schema +"_" + tablename. 
Thanks, I think that answers my question. It's a fairly long and complex query. It sounds like there's no way around translating the SQL script if we want to run it in Access. I just wanted to confirm that there isn't any straight through work around before digging in to the work of converting it. Thanks again. 
Thank you! I'm still learning a lot of this and had reverse engineered the original from some canned reports I had access to. This is a big help. 
Try a pass-through query in access it bypasses jet and interacts directly with SQL server if I remember correctly.
Yes. Do a pass through query. Those are done in the SQL dialect that the target databases uses. I do MS SQL, Teradata, and Oracle pretty regularly. As a bonus, the hard work is being done by the database, not Access (i.e. your PC), so you don't need to worry about the usual Access performance woes. If you're linking to the tables on the SQL DB, *then* you have to use Access SQL and suffer the performance hit. [Click Me](https://www.google.com/search?num=100&amp;newwindow=1&amp;safe=active&amp;source=hp&amp;q=ms+access+pass+through+query&amp;oq=ms+access+pas&amp;gs_l=psy-ab.3.0.0l4.686.2644.0.3895.13.12.0.0.0.0.300.1803.0j6j2j1.9.0....0...1.1.64.psy-ab..4.9.1803...0i131k1.-oYyqjqohkA) How you deal with the data that gets returned depends on your goal. Often times, store it in a temp table and do my reporting out of that. For that, I'll just write a pretty straight up append query that inserts everything from the pass through into the temp table. Then all you have to do is call the append query. (welll... first you clear the temp table, then you run the the append) 
Depending on the query, you could wrap it in a stored process or a view...
Thanks! I was able to figure out for the most part. My issue was solved by adding SET NoCount ON to the beginning of the script. I have no idea why or how that worked but it did. 
I would put OP at intermediate. I would disagree with that article a bit. I'd consider basic to be someone comfortable with data retrieval - select statements with where clauses and inner/outer joins, such as someone in a data analyst role. Intermediate would be a true database developer and include skills to modify data, create tables, and construct multi-statement SQL batches. Advanced would include those more inclined to performance tuning (query hints, diagnosing common problems like parameter sniffing, etc.)
That helps to explain why my professor would be willing to use a common lookup table. That, and a common lookup is much simpler to implement and teach to newbs. My professor is an Oracle Ace and former senior DBA at Oracle. 
really good... and well written, too
If its an inline query, you can put the sort on the outer query in there where clause
Are you saying an ORDER BY on the outside?
Yes SELECT * FROM ( SELECT COL1 AS Column1 COL2 AS Column2 ..., ROW_NUMBER() OVER (PARTITION BY &lt;SOME_ID&gt; ORDER BY &lt;A_TIMESTAMP_COL&gt; DESC) ORDRED_ROWNUM FROM SOURCE_TABLE ) sub ORDER BY sub.Column1 sub.Column2
Wouldn't this re-sort the inner query by COL1 COL2? The inner timestamp column is a timestamp of the last time said record was updated. I need this order propagated to the outer query. Say I am populating a column in target, and target column=NVL(COL1, COL2) If I do the outer query simply as a SELECT *, it preserves the order, but if I explicitly specify, COL1, COL2 it loses last update order.
Maybe I misunderstood the question. What are you trying to sort by? 
Yeah my wording is not the best. The sort is already handled inside the FROM () query. What I want to do, is grab specific columns from that inner query, but keep the same order. So say: &lt;SOME_ID&gt;=234 and the inner query: SELECT COL1 AS Column1 COL2 AS Column2 COL3 AS Column3 ..., ROW_NUMBER() OVER (PARTITION BY &lt;SOME_ID&gt; ORDER BY &lt;A_TIMESTAMP_COL&gt; DESC) ORDRED_ROWNUM FROM SOURCE_TABLE Returns three rows, with the first row being the latest entry for &lt;SOME_ID&gt;=234. I want to grab from those 3 rows, COLUMN2 &amp; COLUMN3, but in that same last updated order.
ORDER BY ORDERED_ROWNUM ?
Not familiar with mono prompt line. What's the query?
Oh my God....why am I so stupid.
tell 'em you know about SQL anti-patterns
&gt; I would like for each CustomerID and VendorID to have only one row. I tried GROUP BY customerID and VendorID but it still returns the 2 rows. not buying this please show the query you tried... just the SELECT clause and the GROUP BY clause (we don't need to see the entire FROM clause again) SELECT vl.nCompanyIdCustomer as 'CustomerID' , vl.ncompanyidvendor as 'VendorID' , CASE WHEN c.szname = 'Convenience Fee Percent' THEN REPLACE(c.szDescription, '&amp;&amp;', vdc.szConditionValue) ELSE 'No Fee' END AS [Convenience Fee] , CASE WHEN c.szName = 'Email Payment' THEN REPLACE(c.szDescription, '&amp;&amp;', vdc.szConditionValue) ELSE 'No Email' END AS [Accepts via Email] FROM ... WHERE ... GROUP BY ??? 
Start thinking in terms of data sets! When you do SELECT * FROM (another data set) you get all of the columns in the new set too :)
It's a query to check sales from all regions and compare this year to last year. The portion with the prompt currently looks like this: WHERE IHFYR IN( @PROMPT ('Prompt Name','N',,MONO,FREE,NOT_PERSISTENT,USER:1) ) I can get it to pop up with the prompt screen and let me enter the fiscal year, but when I switch MONO to MULTI, it lets me select multiple fiscal years, but then gives me an error about 'Right Parenthesis'...
I think that is the part that i'm confused on. Which of the fields would I GROUP BY and which would I not. Also, I would need to have some aggregate function for the field that's not in the GROUP BY. SELECT vl.nCompanyIdCustomer as 'CustomerID' ,vl.ncompanyidvendor as 'VendorID' ,case when c.szname = 'Convenience Fee Percent' then replace(c.szDescription, '&amp;&amp;', vdc.szConditionValue) else 'No Fee' end as [Convenience Fee] ,case when c.szName = 'Email Payment' then replace(c.szDescription, '&amp;&amp;', vdc.szConditionValue) else 'No Email' end as [Accepts via Email] GROUP BY vl.nCompanyIdCustomer, vl.nCompanyIdVendor, c.szdescription, vdc.szconditionvalue This is what I would imagine the GROUP BY should be, but I can't because of c.szname not being in aggregate function. I've tried the 3 different variables quickly where I did MAX one of the values and then added the other 2 to the GROUP BY. Even if I just GROUP BY vl.ncompanyidcustomer and vl.ncompanyidvendor it says the other 3 are not in an aggregate function. 
Most initial phone interviews are set ups for take home tests. Set up a database and be prepared to work on it. Also be careful as sometimes this work will interfere with your regular schedule so be prepared to put some time aside.
They will likely be quizzing you about things needed for the job. The more you find out about the job description the better. Just with the small details provided I wouldn't be surprised for them to expect you to be pretty good with writing stored procedures. Understanding locking, concurrency, isolation levels, etc. 
Yeah unless someone else knows what mono/multi prompt is and can help you, you're going to have to be more specific. I have no idea what mono prompt is. Does it pass a variable to a stored procedure? You said you are trying rebuild your queries using SQL so what is the old query vs the new query? Missing right parenthesis probably means your query is getting truncated or you are missing apostrophes around your string values or something like that. Which would make sense if the program is designed to accept a single vs multiple line of SQL.
The tough part, (and thank you for even responding and attempting to help, I really appreciate it,) is that I have no idea what you're talking about. My company hasn't done any training or classes on how to do this, I basically got picked because I know how to use computers. So I'm trying to teach myself as I go. Basically by having that prompt statement it allows whoever is running the query to input a fiscal year that they want sales numbers from. By having it set as Multi instead of Mono, it allows the user to request more than one year. Again, I'm sorry I can't provide more information since I don't really know what to look for, but I appreciate you trying!
Surely you can provide more information than none. What is this software even called? Is it something that was developed internally? Does it have a website? Support? I guess a picture would be better than nothing. A WHERE clause is only part of a SQL query. So are you building *only* WHERE clauses? Where is the rest of the query stored? lol
Yes, that's how SQL works. If you use one aggregate function, then all must be aggregated. How could you possibly return a sum of data and also all of the individual data pieces at the same time? You can use a subquery to return the grouped customer/vendors and join that to your other data if you need to return values that cannot be aggregated. But you don't have any aggregation to begin with. Which leads me to believe your joins are simply broken and you don't quite understand the relations in your database... Or you just need DISTINCT, but probably not.
the reason you got 2 rows for customer and vendor is because you were grouping on 4 columns, and they were different your issue can be solved by adding MAX() function to the two CASE expressions it might be easier to do this on an outer query, by pushing your original query down one level to a subquery -- SELECT CustomerID , VendorID , MAX([Convenience Fee]) AS [Convenience Fee] , MAX([Accepts via Email]) AS [Accepts via Email] FROM ( /* put your original query here */ ) AS q GROUP BY CustomerID , VendorID 
 SELECT Teachers.* FROM Schools INNER JOIN Teachers ON Teachers.School_ID = Schools.School_ID WHERE Schools.District_ID = 500 
So you're trying to determine the school ID joining on DISTRICT_ID between the 2 tables, is that correct?
WHERE SchoolTable.DistrictID = 500
I'm assuming this is what you're after. Some screenshots of the table's data would help. select t.* ,s.school_id from teachers t join schools s on t.district_id = s.district where t.district_id = 500
Wow. Reddit is amazing. Problem solved folks! Thanks. 
A buddy of mine just took an interview for a data scientist position and the guy asked him straight up to describe to him his most impressive query lol. He is pretty noob so he diverted the question. 
basic SQL is really easy. A couple weeks is enough to be competent at it. Find out what database they're using if you can. MS SQL and Oracle both have free versions you can download to learn on. Most of the others (MySQL, PostgreSQL) are free anyway. If you can't find out, I'd just grab MS SQL and their Northwinds db and start working through some online tutorials. Concentrate on learning how joins and basic data aggregation stuff works.
I remember him saying something about squid databases? Edit: also thanks for the tips.
wow, I agree. I am used to these blog posts being utter garbage. This was very refreshing to read!
As a person who has given many SQL whiteboard tests for a large technology company, here are the things we're trying to make sure the candidate understands before we consider hiring them: * Join (inner vs left is mostly what we care about) * Aggregation / grouping (sum, average, max, min) * what a "having" statement does, and when to use it * order by * coalesce (what it does and when you'd use it) * ordered analytical / window functions (row_number, rank, dense rank() etc) * union / union all and what the difference is between them * subqueries ^^ this all sounds pretty easy, but it's funny and sad how many people come in with a masters degree in Information Systems, and have "years!" of SQL experience, who rank themselves as an 8 out of 10 at their SQL ability, and then they totally crash and burn on our simple whiteboard test. I've only had 1 interview where the person made it through all of the questions in the 1 hour we had, and that guy had been heavily coached because his friend worked there already. 
What's your recommendation for a better way to do it, that will allow for a database agnostic approach to see if someone understands SQL concepts?
Most companies would not be ok with an employee turning over SQL they wrote to someone outside the company. If a candidate was willing to do this, then they'd probably also be willing to hand over SQL they wrote at your company. This can be a security risk. There also isn't a way to actually verify that they are the one that wrote the SQL. I think we'll stick with whiteboard tests.
Two weeks is plenty of time. Just take some initiative and work through some exercises and youll be fine. 
Corporate BI type here. I've given whiteboard tests, looked at projects candidates have done in SQL, maintain personal projects, work with tech and languages other than SQL, perform serious analysis in and downstream of SQL, and I sincerely can't follow the point you're making. Why is solving hypothetical SQL problems not indicative of practical SQL solutions?
Not even remotely screwed. Get a learn SQL in 21 days book or something and do those few hours a day. You'll pick up the basics in no time. 
If you are interviewing for an SQL job and cannot take a reasonable shot at the basic questions outlined above, I wouldn't hire you either. White board interviews work really well. Especially when you ask multiple people the same question and find out some have no trouble and others have no idea.
Projects they've done or the ones their "friend" did? Cheating is a real thing. Whiteboard interviews disclose it pretty quick.
Really doable, do you know what form of SQL? If you do not mind me asking what is the pay for an internship?
Those are super basic questions that could be answered with no code at all. 
I work for a well known tech company. u/nootanklebiter suggestions are spot on. Besides knowing how to use SQL, we are also interested in how well you understand what goes on under the hood. How do you debug performance issues and how can you improve the execution speed? How well do you know indexes? Do you know about database normalization? How do you enforce data quality? Anyone can write SQL. Not everyone can write good SQL. 
Whenever I interview a person who has SQL on their resume I start with basic selects, joins, group bys, and transactions. Next I ask questions around the normal forms. They donâ€™t have to have all of the forms memorized, but need to understand the trade offs of normalization and denormalization. The conversation will then naturally head into table design. What are foreign keys, One to one, one to many, and many to many relationships? Finally, I like to touch on optimization. What are the first steps when trying to optimize? What are common solutions?
That makes more sense. Indeed, they're better suited for testing core proficiency than creative problem solving. The reason to administer them on site is to ensure a controlled environment and that each applicant has access to the same resources. I like questions that give a defective query and ask to fix it. Like "eliminate the duplicates" or "make it faster". I like to think they involve more thinking and less reading schemas/writing words.
I dont know the form of SQL. I didn't really know enough about it to really ask questions. Low 20s but I know a current employee. My minor is in CS so they thought it would be a good fit but I highly doubt a few CS classes have fully prepared me for whats ahead. I'd just like to come in with a basic working knowledge. 
Phone the company and ask to speak to whoever is leading your internship. Say you want to do some pre-work and want to know what type of sql they use. They should be more than happy to tell you - it would make your first day with them easier from their end too if you have done this background work
This is good advice.
I really liked sqlbolt to get the basics. I wish I knew more about joins when I first started.
I would be useful to find out which specific DBMS they use but in the meantime I would recommend getting [SQL in 10 Minutes, Sams Teach Yourself](https://www.amazon.co.uk/dp/0672336073/ref=cm_sw_r_cp_api_fDLQzb8P47SZT). The book teaches ASNI (American National Standards Institute) SQL - all the general main concepts like SELECT, UPDATE that all DBMS share. It has 22 chapters that each take 10 mins to read (but you should spend more time after each chapter practicing examples). It was incredibly helpful for me when I started learning as i knew 0 about SQL. There's a reason that books it #1 Best Seller. Once you find out what database they use you can learn DBMS-specific functions in addition.
I like to download raw data, and organize it into tables myself. Pick some data that you're curious about, and you'll have no shortage of queries to write. https://aws.amazon.com/datasets https://cloud.google.com/bigquery/public-data https://www.kaggle.com/datasets
Easiest way is to actually work through examples. I used http://sqlzoo.net/ when learning. Alternatively, download a local instance of PostgreSQL and play around with toy examples. https://www.postgresql.org/download/
you got it, just be confident. lied about my SQL knowledge for my first job out of college - got the position and learned SQL in about a month. Having access to a real database and some pre-written queries is all you need to learn. They're not going to make you write any queries from scratch since you're an intern and there's probably not enough time for you to learn their tables. You taking initiative to learn new skills is great though.
There are many factors to consider. On a properly indexed and setup db with good storage, correct memory configurations and processing power, it is slow. My answer has to be vague as your question leaves so many assumptions. Number of other databases? Number of other transactions? Physical virtual or cloud based? Other queries having issues? Version of SQL?
Query optimization is a complex subject. * You want to reduce resource use (disk, memory, cpu) * By understanding what the database is physically doing (not what the SQL is logically doing) * To identify opportunities where the database is doing excessive/wasteful work * And providing the database options to do the work more efficiently (through a multitude of techniques) Execution plans are the best way to examine query performance. I feel query optimization is best learned from specific examples. Particularly in this subject, rules of thumb or best practices have as many exceptions as applications.
Great book! I began with this resource and w3 schools. Moved to cbt nuggets. I've been a SQL development for 5 years now. 
The slow query is actually a core WordPress query, so I am thinking the issue is some issue with our database. Maybe poor indexing or something?
[PostgreSQL Exercises] (https://pgexercises.com/) is a great interactive online SQL tutorial which moves quickly through the basics onto some advanced topics. It all works through the website so you don't need to install anything locally. If you're new to SQL, you'll need to read the linked documentation too, but this will guide you through it in a logical order.
15GB of data and several million rows seems pretty high for a table used in 'core' WordPress - maybe you're missing a [maintenance routine](https://www.elegantthemes.com/blog/tips-tricks/cleaning-up-your-wordpress-database-to-optimize-your-websites-performance) of some sort?
It could be any number of things. Without seeing an execution plan, and seeing what the database is doing, it's just guesswork what the database could do differently.
Is an execution plan something mysql has built in that I can use? Or do you recommend another program to see that?
I rarely use MySql but this might help. https://dev.mysql.com/doc/workbench/en/wb-performance-explain.html
Show us the query. And give us the table structure.
I learned SQL on Codecademy. Its free and I really liked the set up of their tutorials. 
 &gt; this all sounds pretty easy, but it's funny and sad how many people come in with a masters degree in Information Systems, and have "years!" of SQL experience, who rank themselves as an 8 out of 10 at their SQL ability, and then they totally crash and burn on our simple whiteboard test. Sounds like petty bullshit to me. 1. Extroverts would do well in this situation and introverts would not. 2. Memory is contextual and often times you would encounter a problem that wouldn't be solvable by a whiteboard. Personally, I use a whiteboard for thinking about an approach to a larger problem like a hypothesis or a theory. I would never write actual code on a white board and I wouldn't judge a nervous job applicant on her ability to perform on command, like a dog. 
and the explain plan.
I'll take a stab at answering your questions even though we need more information. &gt; Is 2 minutes normal for this size of database table? A query against a 15 GB table can take anywhere from a few miliseconds to several minutes to execute. It depends on too many factors to say that 2 minutes is "normal". &gt; Will a more specific query with WHERE speed it up or slow it down? Doing this will either make your query faster, slower, or the same. Again, not enough information to give a definitive answer. &gt; Any tools to diagnose issues like this? Let's start with the SQL and the table definitions. The explain plan is your friend. &gt; Does size matter? Will table x affect table y? That all depends on what you are doing and how the tables are related. 
hackerrank's SQL tutorial 
They're all posts. It has about 2 million posts.
Binary search is like this. I'll try explaining it fast because I have to go to sleep. Consider the list of numbers 1 3 7 11 18 26 56 156 265 You have to search for the number 156 and by knowing that the list is sorted you look at the middle of the list. You find the number 18. Knowing that your list is sorted you don't have to search the left half of the list. Now you have elements from positions 5 and 9. The middle element from the list is (5+9)/2=7 The 7th number is 56 which is lower so the left part doesn't need to he searched because it isn't there. Hope it helped Of not tomorrow night ask me again 
Not a SQL expert, but here's the general CS idea: A binary search assumes sorted data, and can locate a result in log2(n). It will check the middle element and see if it needs to go higher or lower. It will then check the middle element of half of the set where the target is. This continues until the correct values are found. Ex: find 74 in the set of integers 1-100. Check 51, is higher. Check 75, is lower. Check 63, higher. 69, higher. 72 higher. 74 hit. So compared to a table scan, which will average 0.5N, two binary searches can be quicker for large sets- 2log2(n). 
&gt;does it literally mean, you make a copy of the table and sort the author column alphabetically Yep, that's what an index is. It's a copy of the data sorted/filtered in a specific way and/or including specific columns along with the sorted/filtered data. However, the index isn't created when the query is executed, it's persistent, so it's already there... I'm guessing this is where the "why does this method save time" question comes from. The data is already in alphabetical order so it's much faster to use a binary search to figure out _where_ the specific data is than it is to sort through all rows of the table.
Itzik Ben-Gan's T-SQL Fundamentals is specifically oriented towards (Microsoft) SQL Server. But most of the proprietary T-SQL stuff is noted as such, and the standard non-SQL description of things is given as well. I'm about 300 out of 375 pages in and can tell you that it's an absolutely fantastic book. I would highly recommend it for learning SQL.
I'll try to explain it with arrays: Let's say we have [1,2,3,4,5,6,7,8,9](is has to be sorted, this is key), we are searching for 9. Now we take a look at the element in the middle, which is 5, so now compare 5 to 9. 5&lt;9 and we know that the array is sorted, so we dismiss the first and the middle part. What is left ist [6,7,8,9]. There is no "middle element", as we don't have an odd number of elements, so we compare 9 to either 7 or 8, because they are both "in the middle" (they are the second and third element out of 4, which one doesn't really matter), let's say we compare 9 to the second element, which is 7. Again, 7&lt;9, so we dismiss 7 and everything before and are left with [8,9]. Again we compare 9 to the first of the two "middle elements" (which are the only two left) and get 8&lt;9, so we dismiss 8 and are left with 9, which compared to 9 is what we were looking for. Now, this is absolut worst case runtime, we had to compare 5 times, which is the maximum we can have with binary search in 9 elements. But you can easily see that we still had to compare way less than if we simply had gone through our array [1,2,3,4,5,6,7,8,9] one element after another, because we then would have compared 9 with *each* element in the list, so 9 times, before finding it. This is why binary search is the more efficient way (and therefore saves time) of searching for a specific element and it works exactly the same for databases. 
oh cool thank so much I understand it ! How do you "make a copy of a table"(is it like creating an alias table) and whats the syntax for binary search on sql ?
Glad I could help you, sadly I don't really know anything about SQL. I'm sure there is someone else in this thread who can answer these questions.
This is what the sql engine would do. The binary search is not something you would do yourself. Although you will need to create the indexes on your tables if you want the engine to use them. Index for stuff like primary keys or unique field might be created automaticaly depending on the engine you are using.
This sounds perfect as it's TSQL at work.
A couple clarifications. Databases use B-trees. They use the same concept as binary trees, but nodes can have several branches, not just two. Also trees take the search part and make it into a physical structure. So you're not so much searching as following a path. The rest is a copy paste from a previous post. ---- First, let's understand what a page is. A page (or block) is a fixed size chunk of data on the disk that the database is optimized to read into memory and process at one time. Basically bite sized chunks of data. Indexes are stored on disk, separate from the table. Indexes contain two parts. Leaf nodes and the tree. Leaf nodes (each node is a page) store the indexed column(s) in sorted order with a reference to the table record (usually a PK). Tree nodes are created to link to the location of every lower node, and are created in levels until a single top level node is able to contain all references to the next lower level. Unlike vanilla indexes, clustered indexes are the table (are not stored separately). The clustered index sorts the table on the indexed column (usually the PK) and creates a tree structure like a normal index. Below is an example diagram where each page holds a limited amount of data. In reality they hold A LOT more, but the example would be too big if I did that. Legend: page = record1data1:record1data2 record2data1:record2data2 CREATE TABLE t (ID,Letter,DOW) CREATE UNIQUE CLUSTERED INDEX cix ON t (ID) --Table Leaf Nodes t01 = 1:Q:Mon 2:W:Tue t02 = 3:E:Wed 4:R:Thu t03 = 5:T:Fri 6:Y:Sat t04 = 7:U:Sun 8:I:Mon t05 = 9:O:Tue 10:P:Wed t06 = 11:A:Thu 12:S:Fri t07 = 13:D:Sat 14:F:Sun t08 = 15:G:Mon 16:H:Tue t09 = 17:J:Wed 18:K:Thu t10 = 19:L:Fri 20:Z:Sat t11 = 21:X:Sun 22:C:Mon t12 = 23:V:Tue 24:B:Wed t13 = 25:N:Thu 26:M:Fri --Clustered Index Level 2 cix14 = 1:t01 3:t02 5:t03 7:t04 cix15 = 9:t05 11:t06 13:t07 15:t08 cix16 = 17:t09 19:t10 21:t11 23:t12 cix17 = 25:t13 --Clustered Index Level 1 cix18 = 1:cix14 9:cix15 17:cix16 25:cix17 CREATE INDEX ix ON t (Letter) --Index Leaf Nodes ix01 = A:11 B:24 C:22 D:13 ix02 = E:3 F:14 G:15 H:16 ix03 = I:8 J:17 K:18 L:19 ix04 = M:26 N:25 O:9 P:10 ix05 = Q:1 R:4 S:12 T:5 ix06 = U:7 V:23 W:2 X:21 ix07 = Y:6 Z:20 --Index Level 2 ix08 = A:ix01 E:ix02 I:ix03 M:ix04 ix09 = Q:ix05 U:ix09 Y:ix07 --Index Level 1 ix10 = A:ix08 Q:ix09 Let's consider the query `SELECT * FROM t WHERE Letter = 'K';`. This query could go through the following steps to complete. 1. Read ix10. A &lt;= K &lt; Q so... 2. Read ix08. I &lt;= K &lt; M so... 3. Read ix03. K = ID 18 so... 4. Read cix18. 17 &lt;= 18 &lt; 25 so... 5. Read cix16. 17 &lt;= 18 &lt; 19 so... 6. Read t09. Return "18:K:Thu". Reading a total of 6 pages read. Alternatively a table scan would read t1 through t13, or 13 reads.
Well done! Window functions change the world... Just a suggestion: put the execution plan too, and the execution cost for further analysis.
Rank works the same way in SQL Server. Not sure about dense rank. But row_number is a similar function that is very useful as it produces a unique value for each row.
It's doable. SQL is a [declarative programming language](https://en.wikipedia.org/wiki/Declarative_programming). So you just need to express what to do, not how to do it. Not like Java or C++ programming requiring a couple of years for really grasping it. Go for practice oriented tutorial. The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. 
**Declarative programming** In computer science, declarative programming is a programming paradigmâ€”a style of building the structure and elements of computer programsâ€”that expresses the logic of a computation without describing its control flow. Many languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describe how to accomplish it as a sequence of the programming language primitives (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps. Declarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
This model was developed by IBM in 1960â€™s, When they developed a project called IMS (Information Management System). In this model data is always arranged in the form of a tree structure in different levels.
The typical indexing structure that is used for such purposes is called a "B-tree." Whereas a binary-search always divides the data in half, a B-tree's approach is much more like what you might have seen in L. Frank Baum's now-famous office, where he had two filing cabinets: "A-N" and "O-Z." (Yes, that's where the name of the Wonderful Wizard's homeland came from.) Once you selected from several cabinets, you now select from several drawers, then locate your starting search-position within the selected drawer and so on. Each time, you reduce your search space by much more than one-half.
when using outer join you need to add where qryHistory3Month.tblSongs.SongID is null You can use NOT EXISTS as well Edit: Gold???? My first gold :) thanks, I appreciate it 
Does [this](https://www.reddit.com/r/MSAccess/comments/6y1kos/doesnt_recognize_the_except_statement_is_there_a/dmk1bsr/) have any bearing on your response? I ask because, I may have mentioned, I'm not too good with this stuff! :) I'm not sure OUTER JOIN works in MS Access. How is NOT EXISTS implemented?
LEFT join is actually LEFT OUTER JOIN, so by that logic left join doesn't work either. As for where not exists, it would be: Select t1.Key From T1 WHERE NOT EXISTS (select t2.key from T2 where T1.Key = t2.Key) https://docs.microsoft.com/en-us/sql/t-sql/language-elements/exists-transact-sql https://docs.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql Also, pretty sure EXCEPT and INTERSECT need to have matching column counts, like UNION/ UNION ALL The basic rules for combining the result sets of two queries that use EXCEPT or INTERSECT are the following: The number and the order of the columns must be the same in all queries. The data types must be compatible. 
Are you using MS Access as a front end to SQL Server, or is your whole database MS Access? If the latter, MS Access does **not** use T-SQL and has other "quirks" that make it different from a normal SQL database. The question shouldn't be tagged [MS SQL] in this case because it's definitely not going to work the same. If the former, wrap up your whole query in a stored procedure in the SQL Server database and call it from MS Access. That way you have the smallest possible chance of Access b0rking your query.
It's a stand-alone MS Access database. Shoot, Sorry about the tag. I thought I got it right! :(
So I miss-tagged the post, and this is MS Access SQL. I'm reading your response now but in case that has any bearing I thought I would mention it.
No problem, if NOT EXISTS isn't accepted, you can use WHERE x NOT IN y Select t1.key From t1 Where t1.key NOT IN (Select t2.key From t2)
Yeah, sorry. Microsoft doesn't help matters with how they position Access here, and their non-standard implementation combined with the history of doing little to encourage good database practices in Access leaves a lot of DBAs with a bad taste in our mouths.
Jeez this makes me feel old. Worked on IMS in the 90s. 
OKOKOK, I'm not used to writing this syntax but I got Access to accept, at least, what I wrote. But, when I run the query I'm stuck with the error: "The specified field 'SongID' could refer to more than one table listed in the From clause of your SQL statement." PRoblem I have is I have two "FROM" clauses, so which one is it referring to? Or both? :) Do I need to rename the field to something else other than, say, SongID? &gt;SELECT tblSongs.SongID &gt;FROM tblSongs &gt;WHERE NOT EXISTS &gt;(SELECT qryHistory3Month.SongID &gt;FROM qryHistory3Month &gt;WHERE tblSongs.SongID = qryHistory3Month.SongID); 
So then I'm really in the wrong forum then, I guess. :( Either way, thank you for your help!! I love databases, fascinating stuff.
Access might not be able to decipher the "ambiguous" column itself. Try giving your two tables aliases... SELECT s.SongID FROM tblSongs AS s WHERE NOT EXISTS (SELECT qhm.qryHistory3Month.SongID FROM qryHistory3Month AS qhm WHERE s.SongID = qhm.SongID);
Still got the same error... :( &gt;SELECT s.tblSongs.SongID AS Expr1 &gt;FROM tblSongs AS s &gt;WHERE (((Exists (SELECT qhm.qryHistory3Month.SongID &gt;FROM qryHistory3Month AS qhm &gt;WHERE s.SongID = qhm.SongID))=False)); 
Try WHERE s.SongID NOT IN (select qhm.SongID from qryHistory2Month AS qhm)
As suggested by u/AXISMGT you should be able to use at least NOT IN in Access (I hope) SELECT SongID FROM tblSongs WHERE SongID NOT IN (SELECT SongID FROM qryHistory3Month); 
I got it. I used the graphical builder (Access can't complain about that now, can it???) and changed the join to LEFT as you had suggested. Thank you for sticking with me through this! I think this will do it going forward. It's hard to tell at the moment with a db of 466 songs and 47 performed (with a couple duplicates) over the past couple months, but the numbers add up! As long as the SongID doesn't appear in the tblLinkHistory (segregated out qryHistory3Month) then it'll show in this query. &gt;SELECT tblSongs.SongID, tblSongs.Title, qryHistory3Month.tblLinkHistory.SongID &gt;FROM tblSongs LEFT JOIN qryHistory3Month ON tblSongs.SongID = qryHistory3Month.tblSongs.SongID &gt;WHERE (((qryHistory3Month.tblLinkHistory.SongID) Is Null)); 
That was a bit harsh I think. I mean, it's true, but it's a bit harsh. What I usually suggest if conversion isn't possible is a linked server from SQL server to that access database. Dba.StackExchange.com would be a good source too. 
the post says you can't use full outer join but what you have to use LEFT JOIN which seems to be supported so the query you posted with a where condition should work. Did you try it?
Yep, that's exactly what /u/ibilali suggested at the top :) Glad it worked for you. Consider Opening an account on DBA.stackExchange.com and asking there if you don't feel comfortable here :). Just tag your questions with Access.
I just did! And the graphical query builder helped. It appears to be working now! It's like magic, lol.
Harsh? Nah. Professionals know how to it right. I try to do what I can from the books I've read, making sure I'm following the normal forms as best I can, using proper naming conventions, and documenting as much as I can so in the future I can figure out what in the heck I did. I made this db over 10 years ago and set it aside as I wasn't leading a band anymore. Just started back up again and what it does isn't exactly what I need, so I'm adjusting it accordingly. :)
Shoot, that went over my head as I didn't understand when he said outer join...
Well before getting out I think data modelling, data warehousing is a nice skillset to aquire, data science for forecasting and data mining can be a way to use your statistical knowledge. Of course there is no BI without ETL, knowing SQL you could try to work more as data engineer and move to work with big data in the next future. If you like working on visualization you could explore different kind of data visualization libraries with python and d3.js
I'm a Sr Oracle and SQL server Dba. It's a bit harsh and it's a good way of making sure no one will want to work with you due to lack of flexibility. To clarify, I wasn't saying you were being harsh. The comment you were responding to was. You're giving it your best with the resources you have. What I do recommend is that you do switch to SQL server (available free via docs.microsoft.com/sql) if you want to expand on your DB further and use the enhanced functionality available.
At the end you gave the right answer and I didn't understand it. Can you forgive an earnest but fairly ignorant newbie?? :)
Haha no worries, I get being wrapped up in trying to fix your issue. Have a beer to celebrate fixing it, and keep up the good work. Thanks for the gold!
Man... That's like, a dream. I'd love to set it up where I could use some of the functionality on my phone and such. Can I install the free version on a 2012 MS Server Essentials? Cause nerd that I am I got one of those ;)
I have both developer and express (both free) installed in windows 10. You can install them pretty much everywhere, just be aware of the memory they like to grab. Make the services MANUAL start and you'll be able to turn it off and on as needed. Brent Explains it well: https://www.brentozar.com/archive/2014/05/much-memory-sql-server-need/ With visual studio and Xamarin (developer.xamarin.com) you'll be able to make an app with the DB as the back end.
Maaaaaaaaaaannnn... I would love that... I went into engineering when I think I should have gone into CS. But when you take your hobby and make it your job sometimes the joy goes out of it, I'm told.
Ehhh, it's what you make of it. I graduated with a computer engineering degree, started my career as an Application developer, and fell into DB development and eventually Development DBA. CoE will get you anywhere CS will, but you'll also have some EE which will help for robotics/AI if you want. Keep on keeping on my dude.
LAST QUESTION (in this thread anyway, lol): Would I install the Developer or Express version?
They're both free. The main difference is that with express you can technically take it to Production. Developer is technically for development only. If you're making an app for yourself, it technically doesn't really matter too much, but it's anecdotal. https://www.red-gate.com/simple-talk/sql/sql-development/edition-sql-server-best-development-work/ https://www.microsoft.com/en-us/sql-server/sql-server-2016-editions https://docs.microsoft.com/en-us/sql/sql-server/editions-and-components-of-sql-server-2016
EE? I'm a plumber (chemical). hehehehehe Thanks again for your help and patience!
Electrical Engineering. CoE started out as a Branch of EE. You're welcome. I'm not sure if you were saying you're a plumber to express that you're not qualified, but that's certainly not the case. If anything it shows you have the discipline to take on challenges. Get to it! Some motivation: https://youtu.be/Dv7gLpW91DM
There's still a lot of scope in BI. I've found that the main issue in BI is adoption, so what I've decided to focus on is culture change. In my experience, BI or analytics is about automating the presentation (or datasets) of small data to save time in day-to-day decision making. It can add a lot of value, but is often implemented poorly.
I'm not totally disagreeing with you here, but saying it's necessary is going a bit too far. I get the "access is terrible" line is an instinctive thing for us, but if I simply said "I'm not touching that" or something of the like, it would most likely put me in a negative light with my customers and superiors. If you're a consulting company that specializes in sql server, then that's fine, I get it. But this question was just "how do I get Exists/where in to work," and an answer like that wasn't really warranted. Poor documentation/development isn't really a RDBMS-specific thing, bad planning/architecture/development is an agnostic thing.
Thanks, it looks like either data modeling or ETL is an easier path to follow. I've looked into those as opportuntities. 
On a hard drive with spinning platters, it will help because the read head doesn't need to jump around as much. On an SSD, it probably doesn't matter.
 Cause the index can be fragmented as well? Also even if it has an index. You may need to read data from the table in order to get the result
There is a Microsoft-produced course that goes into this on edx. It's alright, and it's also free, so you might as well check it out. I'm just starting to learn it myself. IMO, the syntax seems unnecessarily awkward, but it's not a difficult thing to learn.
I will start there.. thanks! 
Indexes are not the only entry point to a table. First of all, a clustered index can get fragmented (clustered index = physical representation of the table), and so can a heap. Depending on the queries and on the indexes, sometime the query optimiser will determine it's actually more efficient to hit the raw data rather than using an index, in which can, you'll pay a heavier price. Even if all select queries each has their custom non-clustered and you never want to go out of those bounds, if you ever insert data, you have to find a physical location to insert it. Either inside a clustered at a specific address showing vacancy, at the end on a clustered for identity, or at a some random location where it doesn't mess up the B-tree for heaps. So yes, fragmentation usually matter. The ONLY realistic use case I see where it doesn't matter would be a reporting table being exclusively loaded via truncate/single insert, since a single insert can't create fragmentation.
Window functions make things SO EASY. They're wonderful. 
[Fragmentation (or poor performance due to fragmentation) is a symptom of a larger problem, not *the* problem](https://www.brentozar.com/archive/2012/08/sql-server-index-fragmentation/). Don't "fix" the fragmentation, tune your system and database so that it doesn't become an issue in the first place.
If your server has enough RAM to cache the data in memory, it'll rarely matter regardless of your storage.
Try Lynda.com it is a paid subscription but you might be able to get access through your library or school.
I couldn't agree more. You see this a lot at regional banks. 
So its been awhile since I've dealt with relational algebra but hopefully this can help. It does appear that your answer source for the second part has an error in that the two parts are joined with a natural join instead of a cross join. Additionally, the second projection's query is projecting A and D attributes instead of D only as stated in the problem. The statement *Ï€A(Râ‹ˆÏƒB=8(S)) x Ï€D(Râ‹ˆÏƒA=6(S))* can be simplified into two statements as you have done. This is done splitting the two projections and analyzing them individually prior to handling the cross join (x). **Ï€A(Râ‹ˆÏƒB=8(S))** If we read this in plain text, it reads as project the attribute A from the natural join of R with S where S is selected only if attribute B has a value of 8. This breaks into the statement: SELECT A FROM R NATURAL JOIN S WHERE B = 8; Going from the data set, this will produce a result of: *Natural Join of R &amp; S* A|B|C|D :-:|:-:|:-:|:-: 6|8|7|6 6|6|7|7 *Selection of B = 8* A|B|C|D :-:|:-:|:-:|:-: 6|8|7|6 *Projection of A* A| --| 6| **Ï€D(Râ‹ˆÏƒA=6(S))** If we read this in plain text, it reads as project the attribute D from the natural join of R with S where S is selected only if attribute A has a value of 6. This breaks into the statement: SELECT A FROM R NATURAL JOIN S WHERE A = 6; Going from the data set, this will produce a result of: *Natural Join of R &amp; S* A|B|C|D :-:|:-:|:-:|:-: 6|8|7|6 6|6|7|7 *Selection of A = 6* A|B|C|D :-:|:-:|:-:|:-: 6|8|7|6 6|6|7|7 *Projection of D* D| --| 6| 7| **Cross Product** Now we need to take the cross product of the two result sets. This simply takes all rows in our first table and matches it with all tables in our second table. This gives us the result of: A|D :-:|:-: 6|6 6|7 **Final Query** Our final query ends up being something along the lines of: SELECT T1.A, T2.D FROM (SELECT A FROM R NATURAL JOIN S WHERE B = 8) AS T1 CROSS JOIN (SELECT D FROM R NATURAL JOIN S WHERE A = 6) AS T2; **Queries added together** I'm not aware of that notation or query being supported. I would have to say that it is not the same but this may be due to ignorance than it being incorrect. 
Thank you so much, that's a perfect explanation. One more (small) question, sorry: if you have two tables, R and S with the same values as above, but with a duplicate row inserted into R, and natural join R and S, why does it produce duplicate rows when joining (here's what I mean: https://imgur.com/a/GBfCI ) and what would be the best method (if any) to remove that dupe. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/COl9u30.jpg ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dmkx50i) 
Well, there is nothing to remove the duplicates. Even if you just looked at R by itself, the result gives duplicates. When you join the data, it joins the two data sets based on that criteria. With a natural join it just joins based on the common attributes. Side there are duplicates in R, R natural joined with S produces duplicates. If R and S both have a duplicate, you could end up with the same row being shown four times. In SQL, you will want to look into DISTINCT and GROUP BY. I don't recall a notation in relational algebra that removes duplicates. I don't know what your course covers but if you are studying databases, you'll likely cover normalization eventually. Tables would typically be normalized where you wouldn't store duplicates like that since they don't really provide any real use that I can think of. It wastes storage and can be problematic with using the data.
This really depends on the goal of what you are trying to study. By saying querying, I read doing SLECET statements. Joining tables is really a part of that. The comment about injections is really more of a software development issue than purely an SQL issue. Dealing with a database in regards to supporting an application versus a data warehouse is also two different worlds. I think a good method for learning is to first worry about how you can access data (SELECT) , then how do I store data (DDL), then how do I populate and change data (ETL and rest of DML). As you learn try and figure out how to optimize things based on what you're learning and how it changes what you were doing previously.
stackoverflow said it best -- "Please edit the question to limit it to a specific problem with enough detail to identify an adequate answer."
Something like... SELECT tblSongs.SongID, tblSongs.Title FROM tblSongs WHERE tblSongs.SongID, NOT IN ( SELECT tblLinkHistory.SongID FROM tblHistory INNER JOIN tblLinkHistory ON tblHistory.EventID = tblLinkHistory.EventID WHERE tblHistory.Date&gt;=DateAdd("q",-1,Now()) GROUP BY tblLinkHistory.SongID ); Not tested. 
Another overlooked things in BI are: -Import of foreign dataset: By definition, BI implies data availability, and unless you're in a startup that has started this week, it means that you'll have to deal with a heterogenous system where the data isn't always SQL Server. I've got to poke anything from the activedirectory API, as well as more common JSON, XML, oracle, mysql DB. And I used to have a few DB2 flat feeds as well. -BI is also about huge datasets, so optimisation is critical. Not only the queries and indexes, but the physical layout of the tables as well as ways to improve the ETL throughput. For instance, look up "big XML" in google, and you'll find forum post about someone complaining about 50GB XML (and when I rwote my ETL, the only thing I found was complaints about a 700MB blob...)...Except on my end, it was a total of 250GB XML representing about 60 million records in highly unbalanced filesizes. Since SSIS is limited to a single thread per file, I used gnuwin32's split to split all of them into smaller chunks (thus minimizing impact of highly unbalanced filesizes), and allowed me to used all cores in the server. Also, at the end of the process, I have 2 main tables used by powerusers...(we open a lot of the data to users, as we follow the self-serve BI mentality). A table contains 13 billion records, while the other contains 87 billion. I had to teach users about partition alignment to help with their query performance.
Is it in Full recovery mode? Are you taking regular log backups? Are you compressing the backups ("WITH COMPRESSION ")?
The primary server's log can't release space until it is fully replicated to the secondary. This is indicated by `last_redone_lsn` of the secondary matching `last_hardened_lsn` of the primary. WITH cteLogFiles AS (SELECT d.database_id , d.name , d.log_reuse_wait_desc , SUM(mf.size) AS size FROM sys.databases AS d INNER JOIN sys.master_files AS mf ON mf.database_id = d.database_id WHERE mf.type = 1 GROUP BY d.database_id , d.name , d.log_reuse_wait_desc , mf.type) SELECT ar.replica_server_name AS ServerName , d.name AS DBName , d.log_reuse_wait_desc AS LogReuseWaitType , REPLACE(CONVERT(varchar, CONVERT(money, d.size / 128), 1), '.00', '') AS LogSize , drs.last_hardened_lsn , drs.last_redone_lsn , CONVERT(bigint, (MAX(drs.last_hardened_lsn) OVER (PARTITION BY d.name) - MIN(drs.last_hardened_lsn) OVER (PARTITION BY d.name)) / 1000000000000000.) AS Diff , CONVERT(bigint, (MAX(drs.last_hardened_lsn) OVER (PARTITION BY d.name) - MIN(drs.last_redone_lsn) OVER (PARTITION BY d.name)) / 1000000000000000.) AS Diff2 FROM cteLogFiles AS d INNER JOIN sys.availability_replicas AS ar ON drs.replica_id = ar.replica_id LEFT OUTER JOIN sys.dm_hadr_database_replica_states AS drs ON d.database_id = drs.database_id;
I agree that the wording is a little iffy, but they want to know if "...the specified value is present in a list...". To me, that sounds like they are purposefully looking for the IN clause. If you were marked wrong for putting "D" as your answer, I'd go to the professor (or whoever) for an explanation. I think that would be more helpful than what this sub could offer. It's entirely possible the grading key is wrong. Sometimes they alter questions slightly (such as "present" vs "not present") and the answer key is accidentally not updated to match.
Ask your professor? Looks like a stupid question to me.
For what it's worth I was not offended by your post! I'm not a professional so I'm not working with professional level tools; I understood where you were coming from. I have learned that I could migrate this to SQL server, so maybe someday, but that's a tall task for a beginner like me. :)
It looks like you combined the two queries that I made. I'll have to give that a try as I'd rather have everything in one place, I suppose. This database won't be a huge thing; it's probably got a lot more info in it than I'll ever need as it stands right now! Final working solution [here](https://www.reddit.com/r/SQL/comments/6y1ic2/ms_sql_doesnt_recognize_the_except_statement_is/dmk5o2o/).
That's how I feel about pretty much every SQL "test". I would put $5 on seeing old comma style joins somewhere on this test.
SQL injection isn't really a SQL thing.... it's more of a "build your project so SQL injection can't be used against it" thing. There's nothing super special about the SQL used in injection attacks, the tricky part is getting the attack query to the db and running it. 
Horrible question. Fight this in class.
Having the data in memory will make it faster. And if it is fast enough, the difference will be negligible or unnoticeable. However, the issue is still there. Logical reads from memory still take time. Reading from memory is orders of magnitude faster than reading from disk (even SSD). But it still adds up. Fewer blocks in memory are processed faster than many blocks in memory, although the fragmentation would need to be huge to amount to a significant difference. But it happens sometimes.
I agree, most likely go with D. While both IN and NOT IN will get you the result you're looking for, I'm guessing the professor is looking for IN.
Here "NOT IN" is best suitable and fast , so using not in query will execute much faster
They all are correct aren't they? In the end, I believe the correct answer is that it depends on how you get the values. * Is it a hard coded list? * Or is it a subquery, if so how is it indexed? A) SELECT value FROM Foo WHERE value NOT IN (subquery) B) SELECT value FROM Foo WHERE EXISTS ( SELECT NULL FROM subquery WHERE value = sq_value ) C) Similar to B) D) Similar to A)
Awesome, thanks!
I lie to get ahead, sue me, its a dog eat dog world out there. Also its a graduate level job, they dont expect you to have experience. 
False. "NOT IN" must look at every value before returning a result, but "IN" can use **short circuit logic** to return true on the first match. So IN can actually be faster than NOT IN.
Do all the instances SQL have that database? All the instances must have a copy of the database in the availability group before you can reuse the log. SELECT arcs.replica_server_name, d.name, d.log_reuse_wait_desc, drs.log_send_queue_size,drs.redo_queue_size FROM master.sys.databases d INNER JOIN master.sys.dm_hadr_database_replica_states drs ON d.database_id=drs.database_id INNER JOIN master.sys.dm_hadr_availability_replica_cluster_states arcs ON drs.replica_id=arcs.replica_id ORDER BY name ASC That query will give you the reuse wait description and the send/redo queue sizes. The reuse wait description will help you identify your problem if it is a problem with alwaysOn
I often use Windowed Functions in the ORDER BY clause. It's super useful when I'm looking for examples in my data of frequently occurring / high number of events. For example... SELECT * FROM dbo.Episodes ORDER BY COUNT(*) OVER (PARTITION BY PatientID) desc Which will return all the rows in table Episodes, ordered by the patient with the most Episodes recorded. 
ah awesome ELI5 explanation. &gt;However, creating the sorted table would take a while (~230 million operations, depending on our engine). In the article above it says 230 million operations to sort a table with 1 million rows, what formula have they used to come to that figure ? are they talking about sorting a column ? 
So for example if I create a table of with unique id, columns book name,year published and author name. if I arranged the author_name column alphabetically does that count as creating an index ? do I get to tell SQL which method to use binary vs full search or does it to that automatically 
ah right, so the binary search is done automatically, when you do where="JK Rowling" as long as the column is in alphabetical order, if the column isnt in order then SQL will do a full search which could take longer 
DBA skills. Learn how the dbs work use OEM, spotlight for ms sql, foglight. The ability to kill querries, make them more Efficient, find and resolve bottleneck are skills that put you above develpers. 
Yeah, SQL does the binary search of the index as part of the query, assuming it is available, it is on that specific column, and it is not too heavily fragmented.
Thanks for your responses. With this said I also wanted to do a compare and contrast against a No SQL scheme to see what are the benefits of one over the other with same type of topic.
cool ! thanks. Why not always filter the data for large data sets to make it easy for sql to do the binary search ? from what ive read there seems to be a debate between full search and indexing 
Don't delete the old data? You say "updated every week", but I don't think you are using that word correctly...
awesome Thanks ! Final point of confusion from the article &gt;However, creating the sorted table would take a while (~230 million operations, depending on our engine). If we were executing that query many times (more than 23 times) or if we already had that table created, then that second plan would be better. * How do they get the figure of 230 million operations to create a sorted table? * Why is it better to use the second plan only if we were executing query more than 23 times, I understand that a binary search for 10 million columns at most needs 23 lookups, but I dont understand the link to executing 23 or more queries Thanks 
Yeah I want to create my own table which includes both the old and the new data. Because i use it in a tableau viz and i want to know which new employee replaced whom
You can simply add a DepartureDate field to the table Or you can learn about datawarehousing and implement a versioned copy of the table using one of Kimball's type of dimensions as some kind of history table: https://en.m.wikipedia.org/wiki/Slowly_changing_dimension Also, third option, is to go the "log route (sorry, forgot the name. It's basically a methodology that keeps only a log of actions and is thus append only without ever modifying existing rows; for instance, hiring and then having a departure would look like this: Id;logdate;employeeId;hiredate;departuredate 1;2015-01-01;007;2015-01-01;null 2;2017-09-01;007;2015-01-01;2017-09-01
**Slowly changing dimension** Dimensions in data management and data warehousing contain relatively static data about such entities as geographical locations, customers, or products. Data captured by Slowly Changing Dimensions (SCDs) change slowly but unpredictably, rather than according to a regular schedule. Some scenarios can cause Referential integrity problems. For example, a database may contain a fact table that stores sales records. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Data just doesn't disappear from a database. That's not how these things work. It's persisted unless modified. There is no expiration date on data unless you give it one. Either the query you are running filters only active employee records, or something is actively deleting the data. Hopefully it's not #2.
I get the table from my HR dept. So i have no control over the table. They delete the employees who left and my visualization goes for a toes. So i want to take a snapshot and keep it another table
They probably delete the employees that left for HIPAA reasons or privacy or otherwise. So I'm going to assume that you have approval to keep these records... What does the table structure look like? Do you have rights in your server to create tables, or only to read (select) data? 
|USERID | EmpNumber | Last | First | DEPT | TITLE | Location | EFFECTIVE_START_DATE | Phone | I have only the read access.
I don't particularly like this article. Many details are incorrect. What it's trying to say that turning an unsorted table (heap) into a sorted table (clustered index) is an expensive operation. There are many [sorting algorithms](https://en.wikipedia.org/wiki/Sorting_algorithm) and the best ones take O(n log n) operations to complete. They probably used this to estimate the number of operations to sort 10,000,000 rows: `10,000,000 * ln(10,000,000) / ln(2) = 232,534,967` What I don't like is you don't compare the cost of creating an index (or a sorted table) with the cost of a query. It makes no sense. You compare the benefit of the index (running the query) with the cost of maintaining the index (inserts and updates). Assuming storage isn't a constraint, the cost of initially creating the index is irrelevant. You do it once during a maintenance window, and you're done. And no one runs a query 23 times. You run it once a month, or 10k times a day. You do compare different alternatives to run the same query. A B-tree seek on 10 million rows is probably index depth 3, so that's 4 reads not 23 lookups for a binary search (again databases don't use binary searches). My hypothetical sorted table has 400,000 pages. So scanning the entire 10 million records (400k reads) would be as fast as 100,000 seeks (100k * 4 reads). If you lookup 10k rows, and index will help. If you lookup 200k rows, an index won't help.^^1 *^(1: In the unrealistic example that you are looking up 200k unique values. An index could help if you lookup 20 unique values and still access 200k rows.)*
Yes, they're talking about creating a copy of the table sorted on the column that we're designating as the index. The formula that they're using is n log2(n), with n equaling 10million in their example ([wolfram calculation](http://m.wolframalpha.com/input/?i=10%5E7+*+log2%2810%5E7%29) ). Most sorting algorithms will complete a sort in nlog2(n) on average. [Wikipedia article on sorting algorithms ](https://en.m.wikipedia.org/wiki/Sorting_algorithm) 
Non-Mobile link: https://en.wikipedia.org/wiki/Sorting_algorithm *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^108669
Good bot
Thank you JimmyTheFace for voting on HelperBot\_. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/is5zXZq.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dmmh0pb) 
Well the second update would update the first update? 
You missed the trailing space.
The bane of exam quiz banks...
Yes, I m already using Kernel for SQL database recovery tool, really this tool work very well. with the help of this tool quickly recovers corrupt or damaged SQL database. Thanks rodick willision 
You're skipping from "Salesforce is too expensive" directly to "so I'll roll my own"? There are myriad CRM solutions between those ends of the spectrum. Your time has value - don't spend it when you could put a smaller number of dollars into a solution. Every hour you're spending on your custom solution is multiple hours you aren't making money on your real job.
I guess they forgot `DISTINCT` when selecting the possible answers.
Well, a big part of "my real job" is to make our data available throughout the entire organization :) And I'm already building my own. Just a non-scalable Google Sheet solution with a lot of messy calculations... 
You can make the data available without rolling your own. Seriously. This is a solved problem, and solved at a lower price point than Salesforce. Spend your time on things that make your organization unique or make you money. Not re-implementing things that others have already done for you.
Not helping...
I'm trying. I'm just not giving you the answer you previously decided you want to hear.
I asked for what languages to use. You answer with general business processes... I've done that part. It's no longer a question of if but how...
if its an inner join, no. Left/Right joins can make a difference.
Let's say for fun it's 4 LEFTs, followed by an INNER, followed by 4 LEFTs?
I've implemented two different solutions in the same system for two different purposes. One is a more traditional log that I call "Audit" that has the old and new value together in a different table, useful to see who made changes and when and to be able to with some ease revert changes. The other is a more robust snapshot that saves how data looked a a specific time so that users can go back later and see the same version of many tables at once. This is analogous to slowly changing dimensions but kept in the same system, not a DW. Assuming sql server: For the first the way you set it up is a copy of the original table with two sets of fields (old and new) with some extra fields for date of change and userid. Then a trigger for update/insert/delete that populates from a full join of the inserted and the deleted virtual tables using the full primary key as the join predicate. The second way is done in two steps, we take out the entire set of data to snapshot and then merge it with the existing snapshots of all rows. We could make it more efficient by keeping a payload hash / hash of all non key values for the merge comparison. When there is an exact match we just up the enddate of the existing snapshot of that record. If it doesn't exist we add it and if it has changed we upsert, ending the old record and insert a new one with the same key but the new payload.
Database engines will typically order the joins efficiently, regardless of what order you write them. There are exceptions, though.
Using ROWS for your OLAP window specification really refers to rows while RANGE refers to the value / content of the column. This means if you have defined the OLAP window as follows [ROWS / RANGE] BETWEEN 1 PRECEDING AND 1 FOLLOWING It refers to 3 rows if ROWS has been specified (and if they exist) and to 0 - n rows with a value of x-1 to x+1 if x is the current rows value.
The process I'm looking to optimize has a series of joins with an INNER square in the middle, and uses all of these functions to perform currency calculations. It's really a nightmare but most of the tables are tiny except one towards the end of the chain that's 30M rows. My initial reaction was to join everything besides that, put it into a #table and index it, then join to the larger set and then as a final step do all the currency conversions.
Never left before inner
That was more or less my assumption. Saw a few threads on SO but nothing that drove the point home.
an "INNER square"? Functions? Are they TVF's?
&gt; an "INNER square"? I don't understand what this means. &gt;Functions? Are they TVF's? They're really simple functions that could be solved using a join instead. Basically they take a currency code, look it up on a table, and then retrieve a value to multiply the spend so that it converts it into USD. 
"Sorry, the page you were looking for in this blog does not exist."
I hope you understand what it means, he's quoting you. He's asking what's an "INNER square"?
Oh, sorry, poorly phrased. The inner is 'square' in the middle of the sequence of joins... right in the middle... smack in the middle, e.g., from a left join b left join c left join d inner join e left join f left join g left join h left join i
It depends on the database. The db will try to come up with the most efficient join order, but they won't always get it right. And the more tables you have, the harder it is. I know older Oracle DB's could benefit from having the joins with the fewest resulting rows at the end of the list, but it's been awhile since I went down that rabbit hole. I've never, ever read MS SQL folks talk about it. You can google "SQL Driving Table" for more info.
Are you *sure*? You'll have to check out Fabian's Business Intelligence blog to find out!
Why not choose an opensource CRM and then customise it? You'll gain experience with an architecture, you'll contribute to something used by other people and after that you can still build your own CRM if it doesn't fit you. If I had to do it, I'll probably use C# and MVC5 because you can easily create CRUD or nodejs with postgresql or probably even Python and django. I don't personally like PHP, but with a framework like symphony it's not that different from the other languages if you have a good database. 
Make sure you actually need to join from all of those tables. Don't make the engine do [unnecessary, mandatory work](https://blog.jooq.org/2017/03/08/many-sql-performance-problems-stem-from-unnecessary-mandatory-work/).
&gt; SQL Driving Table Thank you, this is what I was looking for.
What does the structure of the two tables and execution plan look like? Are you able to modify the table (to add indexes etc.)?
I honestly have no clue what the tables and execution plan look like. I doubt the vendor will want us to modify the table. I did notice that there is a space in the "CsContactRelatedCompany_I D" in the 2nd line which doesn't look right to me. Going to fire off an email asking about that now.
Ok, sure, sounds like a table-valued function than, if it's being joined to. Question: there are more than one of these? Why?
I second c# and ado.net for MVC and all that good stuff. Use a Ms SQL back end. You will thank yourself when you need consulting or support for it or reporting maybe not so much for licensing but you've got to choose your battles
That's just a column alias. You can alias a column with whatever you would like. That's effectively just relabeling the "ID" column. Should not affect performance whatsoever. I am guessing what is happening is that there is no usable index on vBoCsContact.Email for that WHERE clause so SQL is doing a full table scan (or another poorly performing index) on one or both tables. How many records does it return? How many are in both tables? Turn on the execution plan (CTRL + M is the hotkey or click the button with the three squares on the toolbar) and show us. https://www.brentozar.com/pastetheplan/ 
I have a feeling if they think SF is too expensive they won't like MS SQL pricing... then again they are going to dedicate a developer to reinvent the (CRM) wheel so who knows Â¯\\\_(ãƒ„)_/Â¯
&gt;This exam is "credit towards MCP, MCSA certifications" Sometimes that literally means you get a free attempt at the certificate exam. Maybe contact someone at the training company you're using for the class and see if they can provide you more information. I've taken some classes with Global Knowledge where you literally finish the class, have a few hour "exam prep" discussion, then walk a few rooms over and sit for the exam.
Because it was written by an asshole. You should see the WHERE statement I'm going to rework. It has multiple OR's. Can select everyhing (~30M) rows without any WHERE conditions in about 30 minutes without optimizing the joins. With the WHERE conditions it jumps up to two hours and forty minutes to return 24M rows. And that is without the function being used at all. So with the function the execution times would blow up even more but I don't have time to wait and see how long that would take to run as its currently written. I just wanted to benchmark the initial pull and then start working it out.
Thanks. I've asked them for this information and see what we can get back.
The `REPLACE()` function should be part of your `SELECT` clause: SELECT ProductName, REPLACE(ProductName,'''','') AS [Apostrophe Removed] FROM Products WHERE ProductName LIKE '%''%' 
I think you are trying to join a text column without any indexes, which is causing a table scan to take plane. I think this due to the lack of a 'key' or 'id' at the end of the column name 'company'. If this is the case, the simple solution is to create an index on the column in question. Alternativly you can create a columnstore index if you are not dealing with that much writes to the database.
Thanks a bunch. I had it that way at one point but was getting an error. Turns out I constantly forget a comma between SELECT clauses. Is there an order in which you always have to use things?
Any time you're returning data from the query, it needs to be in the `SELECT` clause... whether this is a column name, calculated column, function, stored procedure, etc... it should all be in there: SELECT 1 as 'One', 2 as 'Two', REPLACE('a t','t','test') as 'Test Column', 3+4 as 'Addition' WHERE 5=5 For the most part each comma separated part of the `SELECT` clause turns into their own column.
I would think B and C because it requires a sub query to write exists??
Depending on scale, they could start with Web Edition, or maybe even Express, that'd keep costs down. Move up to Standard, or even Azure SQL Database or Amazon RDS if needed.
I would say check the execution plan and see where the time is beeing spend. The order of the tables has something to say but sql server will try to optimise it so what u write does not matter. If you want to try different orders than you can add option force order in the end of the query and experiment with different table orders. see how they change Again use estimation plan to see how it perfoms and not have to wait 30 min+ for every change you do. A nice trick is to have 2 versiones of a query in the same sql file and check the execution plan on both. SSMS will show how much percent of time is going to be spend on each query. Keep in mind that estimation plan is just an estimation and the execution plan might be different but in ur case is the best option you have to be efficent.
If it was me I would use one of JS frameworks for frontend(ReactJS, Vue.js, Angular, ...). For backend Java with Spring Boot and for DB Postgres.
Hey OP! sounds like a fun time
Start thinking of each part of the select clause as a column and you won't forget the commas. They aren't different select clauses. And you can create columns that aren't in the table you are selecting from as well (ex: `SELECT 1 as Column1` will give you a column called Column1 and every value will be 1).
Have you tried: select eomonth(dateadd(y, 2, '2017-09-07'))
`DATEADD(DAY,-1,DATEADD(MONTH,DATEDIFF(MONTH,'2000-01-01',@date)+24,'2000-01-01'))`
Didn't work :( This is in access - I was trying to use the SQL view to make things easier. 
can you post the code you are using? It should literally be about 5 lines of code. I'm assuming you have included the MySQL dll included as a reference.
Q1. no that's not correct, that pulls all occurrences, disregarding the criterion "appears more than once over a week period" Q2. likely a self-join, based on the scant information provided
&gt; This is in access you should have said so on the original post, to save people the time and effort of writing a solution that you can't use please see sideboard about posting your platform
What code would I be using at this point? I am only trying to get VS to connect to the database. At this point it is purely declarative. I am using the server explorer in VS to make a connection. My goal is to get it to appear in the database connections.
Thanks for this list, I needed a free ide and google wasnt much help. I went with the oracle sql developer
ok I understand. That's really a VS issue, not a SQL issue. This is the best I can find but I'm guessing it's the directions you are already following: https://dev.mysql.com/doc/visual-studio/en/visual-studio-making-a-connection.html
Oh, you should have said so, since dialects vary quite a bit from one implementation to another. Either way, here it is: DateSerial(Year(DateAdd("yyyy",2,Date())),Month(DateAdd("yyyy",2,Date()))+1,0) 
thank you for the response, and that is all the info the prof gave me!
Do you need to maintain history of these tables (including items that have been removed from AD), or is it sufficient to just have the tables represent the current state of AD with no history? If the latter, trunc &amp; fill will be much faster &amp; easier. If the former, I'd suggest importing into a staging table, then perform an upsert (merge) into the real table - update records that have changed, insert new records.
A good practice is to first load the data into a separate table, called a staging table. Then you can either switch the data out (partition switching, drop / rename, schema transfer) or insert/update/delete. Doing it this way, you can wrap the switch in a transaction so that no one ever sees an incomplete table.
Yeah, that is the second step in what I posted above. :( Unfortunately I have tried this. Thanks for trying though.
Silly question.. do you really NEED it in server explorer? I mean, I understand the convenience but it doesn't actually DO much for you, you know? You just need a MySQL Connection object and a connection string and you should be able to do everything you need.
My experience with VS 2017 has been sub-optimal. They don't even have the full SSDT available yet. I'm guessing the technology simply isn't there for you, but maybe I'm wrong. I had to revert back to my 2013 license to make a SQL project...
-- Possible bad Indexes (writes &gt; reads) DECLARE @dbid int SELECT @dbid = db_id() SELECT 'Table Name' = object_name(s.object_id), 'Index Name' =i.name, i.index_id, 'Total Writes' = user_updates, 'Total Reads' = user_seeks + user_scans + user_lookups, 'Difference' = user_updates - (user_seeks + user_scans + user_lookups) FROM sys.dm_db_index_usage_stats AS s INNER JOIN sys.indexes AS i ON s.object_id = i.object_id AND i.index_id = s.index_id WHERE objectproperty(s.object_id,'IsUserTable') = 1 AND s.database_id = @dbid AND user_updates &gt; (user_seeks + user_scans + user_lookups) ORDER BY 'Difference' DESC, 'Total Writes' DESC, 'Total Reads' ASC; --- Index Read/Write stats for a single table DECLARE @dbid int SELECT @dbid = db_id() SELECT objectname = object_name(s.object_id), indexname = i.name, i.index_id, reads = user_seeks + user_scans + user_lookups, writes = user_updates FROM sys.dm_db_index_usage_stats AS s, sys.indexes AS i WHERE objectproperty(s.object_id,'IsUserTable') = 1 AND s.object_id = i.object_id AND i.index_id = s.index_id AND s.database_id = @dbid AND object_name(s.object_id) IN( 'tablename') ORDER BY object_name(s.object_id), writes DESC, reads DESC; -- Show existing indexes for this table EXEC sp_HelpIndex 'tablename' -- Missing Indexes SELECT user_seeks * avg_total_user_cost * (avg_user_impact * 0.01) AS index_advantage, migs.last_user_seek, mid.statement as 'Database.Schema.Table', mid.equality_columns, mid.inequality_columns, mid.included_columns, migs.unique_compiles, migs.user_seeks, migs.avg_total_user_cost, migs.avg_user_impact FROM sys.dm_db_missing_index_group_stats AS migs WITH (NOLOCK) INNER JOIN sys.dm_db_missing_index_groups AS mig WITH (NOLOCK) ON migs.group_handle = mig.index_group_handle INNER JOIN sys.dm_db_missing_index_details AS mid WITH (NOLOCK) ON mig.index_handle = mid.index_handle ORDER BY index_advantage DESC; -- Missing indexes for a single table SELECT user_seeks * avg_total_user_cost * (avg_user_impact * 0.01) AS index_advantage, migs.last_user_seek, mid.statement as 'Database.Schema.Table', mid.equality_columns, mid.inequality_columns, mid.included_columns, migs.unique_compiles, migs.user_seeks, migs.avg_total_user_cost, migs.avg_user_impact FROM sys.dm_db_missing_index_group_stats AS migs WITH (NOLOCK) INNER JOIN sys.dm_db_missing_index_groups AS mig WITH (NOLOCK) ON migs.group_handle = mig.index_group_handle INNER JOIN sys.dm_db_missing_index_details AS mid WITH (NOLOCK) ON mig.index_handle = mid.index_handle WHERE statement = '[databasename].[dbo].[tablename]' -- Specify one table ORDER BY index_advantage DESC; -- Examine current indexes EXEC sp_HelpIndex 'dbo.tablename' Once you get your SQL Plan, I really like using SQL Sentry Plan Explorer. (Free tool)
Danke.
I'm fully beginner (MySQL) but I think a starting point would be to self join and compare strings so Select count( table.string) as t From table Some join table(self join) On table2.string =table1.string And primary key1!=primarykey2( to ensure we don't count the same entry as a match) And Date2 between date1-7 days and date1 +7 days Order by t The syntax won't be right but I think it'll be along these lines, might just need to play around with it a bit.
See if this gets you what you need. SELECT t.NAME AS table_name ,SCHEMA_NAME(t.schema_id) AS schema_name ,c.NAME AS column_name ,st.NAME 'Data type' ,c.max_length 'Max Length' ,c.precision ,c.scale ,c.is_nullable ,ISNULL(i.is_primary_key, 0) 'Primary Key' FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID INNER JOIN sys.types st ON c.user_type_id = st.user_type_id LEFT JOIN sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id LEFT JOIN sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id ORDER BY schema_name ,table_name;
Man, I am open to anything at this point. Here is the full setup. I have a remote MySQL server that has data I want. I want to push that data to a different remote SQL database. I assume you are referring to [something like this page here?](https://dev.mysql.com/doc/connector-odbc/en/connector-odbc-configuration-connection-without-dsn.html) I will be totally honest with you and state that I don't have much experience with this. Do I do that in the "Add connection" screen where it has the connection string field?
That's enough to get me started, thank you!
OK, so I tried to make a connection string. I used the Add Connection method, with the Microsoft ODBC Data Source. Here is my connection string: "Driver={mySQL ODBC 5.3 Driver};server=***.***.**.***;uid=AzureDiamond;pwd=hunter2;database=whatever"
Did you try writing a simple little program like this? &gt;MySql.Data.MySqlClient.MySqlConnection conn; &gt;string myConnectionString; &gt; &gt;myConnectionString = "server=127.0.0.1;uid=root;" + &gt; "pwd=12345;database=test"; &gt; &gt;try &gt;{ &gt; conn = new MySql.Data.MySqlClient.MySqlConnection(); &gt; conn.ConnectionString = myConnectionString; &gt; conn.Open(); &gt;} &gt;catch (MySql.Data.MySqlClient.MySqlException ex) &gt;{ &gt; MessageBox.Show(ex.Message); &gt;}
Thank you for your continued help. Where would I put this code? Where would I execute it? Sorry I am really not that strong of a developer. :( I understand the code, surprisingly, but I don't know how to execute it.
ok, can we reboot this for a second? You are just looking to move data from Server A to Server B? There must be a tool out there already to do this. 
That is c# code, you would put it in a script task or in some method. You are familiar with c# right?
I understand that you have an idea of what you **think** you want to do, but I think you need to reconsider the cost/benefit analysis of a totally bespoke CRM. The basic functions are really commoditized at this point so there is no point is creating your own from scratch. Unless you want to waste some of your employers money, and a lot of your own time and brainpower- don't do this. As others have said there are several open-source CRM solutions that can be extended using things like C#. However, if you're really, really set on building this yourself and you need something that will be web-based to ensure the same kind of availability and multi-platform access: I'd suggest building it with something like the Google Compute Platform or Microsoft Azure. Both of them have extensive capabilities for building data capture, processing, and analysis pipelines as well as general computation capabilities so you can build whatever visualization or application on top of it that you like. Since they're both enterprise-class cloud platforms, you can also rely on them to provide similar response and uptimes to SFDC and Google Apps.
I am not familiar with C#, but now that I know that is C# I can GET familiar with it! I have learned code as a consequence of my position - I have never had any formal training.
Yeah. Just move data. I like using Visual Studio because I have a bridge that then connects THAT database to Salesforce, which is really the end goal. That said, as long as I get the data into my SQL server, I can handle the rest! edit: This would need to be repeated monthly if not more often.
Wow this makes sense, thank you! In laymens what does self join do ?
How many rows are we talking here? The easiest way for you to do this is to write a giant insert statement inside Excel. If you will only do this once, this is the way to go.
It's not huge, but it is about 7 tables that have a few thousand rows. And the bad thing is this would need to be repeated monthly, if not more often.
As you said that size of data is relatively small, you can use delete/update instead of truncating whole table each time and inserting records. You can create cubes/view to do the job 
Running a SQL query once a month really isn't that bad...
Each query request a read lock on [table] from the sub-query and an update lock on [table] from the update. The database allows both reads to occur, returning the same FileId, then the updates take turns.
...but if I could get it to sync more often then it would be. I have to connect once a month, but my hope is to get it every night. I would like the data to be more up to date.
The code in c# to get you started is below. I would segregate each dataload to its own task (for speed) and keep the models for each of the tables in a seperate folder called models (for ease of reading). namespace ImportSQL { class Program { static void Main(string[] args) { var connectionString_mysql = "Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;"; var connectionString_SQL = "Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword;" //should probably seperate these into 7 tasks for each table to prevent locks and improve speed list&lt;Table1&gt; someDataTable = new list&lt;Table1&gt;(); //..repeat for each table //specify scope of connection using(var conn = new MySql.Data.MySqlClient.MySqlConnection(connectionString_mysql)) { try { conn.Open(); MySqlCommand cmd = new MySqlCommand("Select * From MyTable", conn); MySqlDataReader reader = cmd.ExecuteReader(); while (reader.read()) { var t1 = new Table1(); t1.Column1 = reader[0]; t1.Column2 = reader[1]; //..repeat someDataTable.Add(t1) } } catch (exception ex) { Debug.WriteLine(ex.ToString()); } finally { conn.Close(); } } //Repeat for sql server but either do a bulk insert or loop through each element of the list and perform an insert statement //Be sure to execute truncate statements in execute scalar or execute nonscalar functions. } } class Table1 { public string Column1 {get; set;} //give the the actual names not Column1 public double Column2 {get; set;} //... repeat for 7 tables } }
Our .Net shop resorted to using dbForge Studio for MySQL dev work. https://www.devart.com/dbforge/mysql/studio/
Have you looked into using linked servers? You can also right click on a database in SSMS and go to tasks &gt; import data. The SSIS package can be saved and be reused if you have integration services and a SQL Agent account setup and running.
Try to do some reseach, if you know what a clustered index is, then you will know why it is taking so long to create. The bigger question is why you are storing everything as heaps instead of having a clustered index on some identity column. For your fn3 case, your index will likely not be used as the optamizer needs to drill pretty far into the index for find where the fn3s are so it would perform worse than if you had used an index on that column only. Typically when you run your query, and show the execution plan, it will tell you if you are missing any indexes that can improve performance. Alternativly you can ignore this and let tempdb cache the data for you and things would probably be ok but might run slow once in a while when the cache is cleared or not up to date.
&gt;Try to do some reseach, if you know what a clustered index is, then you will know why it is taking so long to create. I don't think you understand. If I am working with staging tables it might take me 30 seconds to create an index + 2 minutes to run the next process, but a clustered index will take 2 minutes + 1 minute to run the next process. I'm being general here but in terms of improving performance as I rewrite things one is not necessarily better than the other. &gt;The bigger question is why you are storing everything as heaps instead of having a clustered index on some identity column. I am but a mere analytic developer. Architectural changes are not in my scope, and something the business isn't interested in. My job (at the moment) is to rewrite a few processes which are causing my database and my server to be locked down once a week for a full day while they process. The current logic is unnecessarily complex and in breaking it down into sub-processes I'm able to greatly improve the overall time. &gt;For your fn3 case, your index will likely not be used as the optamizer needs to drill pretty far into the index for find where the fn3s are so it would perform worse than if you had used an index on that column only. Typically when you run your query, and show the execution plan, it will tell you if you are missing any indexes that can improve performance. Alternativly you can ignore this and let tempdb cache the data for you and things would probably be ok but might run slow once in a while when the cache is cleared or not up to date. This is what I was looking for. Normally I just run incremental steps and make modifications to see what performs best before I move on to the next part. 
1. There are two ways to solve this problem, either with a self join or subquery. Since the other guy did the self-join way, here is the subquery way: Select Sum(Cappa) CountNumber From ( Select Count(1) as Cappa From Table1 Where TextColumn like '%some string%' Group By DatePart(Week,DateColumn) Having Count(1) &gt; 1 ) 2. I don't know what you mean by physical location, is this a lay/log or a street address or easting/northing? Depends on the case.
My advice would be to try different indexes on your tables and determine performance impact on your use case. Stay away from clustered indexes until you understand their purpose. Then set them accordingly. Note that indexes really do nothing to the data itself, so you can play about relaitevly freely. Also, not only examine select performance, but also examine transaction performance! 
That is exactly what I'm doing but each run takes about 30 minutes so I figured whilst I was waiting I'd check here to see if there were any tips. I typically use clustered indexes on ID's only, or sometimes dates, it just depends on how one set is joining to the next set based on the conditions. Table size, things like that. &gt;Also, not only examine select performance, but also examine transaction performance! Whatchoo talkin'bout Willis?
So off the bat - clustered index is the table itself, not a 'copy' like a non-clustered index. You only get the one. Also creating a clustered index on an existing table is slow because the entire thing has to basically recreate itself using the new structure / order. You mentioned you are working with staging tables - so to be honest you might not even want clustered indexes on those as they typically get truncated and filled, and writing quickly may be more advantageous than the order. You can create non-clustered indexes on heaps (tables without a clustered index). You may get a boost by creating indexes on the columns you are joining up between the tables. Heres the thing though - the order of the columns in the index matters! An index of (State, City) performs different than (City,State). (State,City) works well if you want all cities where state = 'TN'. (City,State) works well if you want all states that have city = 'springfield' but if you only have an index of (State,City) and you want city = 'springfield', it won't help and SQL Server will have to go back to the base table to look for the data. There more to them than that though and everyone has an opinion. Reading execution plans or looking at statistics can also help determine the amount of improvement you've made. Runtime in SSMS can be a bit misleading.
&gt;So off the bat - clustered index is the table itself, not a 'copy' like a non-clustered index. You only get the one. Also creating a clustered index on an existing table is slow because the entire thing has to basically recreate itself using the new structure / order. I have been told that in terms of performance the best practice here is to truncate/insert into and order by the field which is the clustered index. Your thoughts? &gt;You mentioned you are working with staging tables - so to be honest you might not even want clustered indexes on those as they typically get truncated and filled, and writing quickly may be more advantageous than the order. I'm not using them. I threw them in here for discussion, but also because I am looking at how the parent tables might be best indexed using a cluster (or key). &gt;Heres the thing though - the order of the columns in the index matters! An index of (State, City) performs different than (City,State). (State,City) works well if you want all cities where state = 'TN'. (City,State) works well if you want all states that have city = 'springfield' I have been creating my indexes such a A, B, C, if the join conditions are A, B, C. If T1 has a join to T2 on ABC, I create index ABC on both, if T1 has a join to T3 on B, I create index B on both. Whether they are clustered depends on size, etc, but typically I see no need for them in intermediate tables. 
To understand how indexes improve joins, we also need to understand join algorithms. Suppose we have two tables Main and Lookup. Main has 100k records and is the basis of our query. Lookup has 1k records and contains attributes we want to append to our Main records. * NESTED LOOP - For every record from Main, we do an index seek to Lookup on the join criteria. If we return 100k Main records we access Lookup 100k times. If we return 100 Main records we only read 100 records from Lookup. Good for small record sets; bad for large results. * MERGE JOIN - We sort Main and Lookup on the join criteria and match them up side by side in sorted order. The join itself is very fast an efficient, but sorting both sides is very expensive. * HASH JOIN - We pull the entire Lookup table into memory, and link the correct record as we read our Main data. This is the compromise join. The join is slower than merge join, but you don't have to sort first. The join is faster than nested loops, but you have the overhead of pulling all of Lookup into memory (and spilling to disk if it doesn't fit in memory). ---- Here are the scenarios indexes can be useful to support. * Filtering Data - We have a large table, but only wan't a few records from it. Create a covering index on the filter criteria, and you can avoid reading a ton of table you don't need. * Joining two large tables - Create clustered indexes for each table on the join criteria. One of the tables' clustered index should be unique. This supports merge joins by pre-sorting the tables. * Join a large table to a small table - Create a unique clustered index on the small table for the join criteria. This supports nested loops if the large table is filtered (from a filtering index), or at least the database engine will understand it can't be a many-many join. A matching index on the large table is not particularly useful. --- Also, multi key indexes are only useful up to the first inequality condition. For example consider `WHERE Status = 1 AND State = 'NY' AND Date &gt;= GETDATE()-1`. An index on `(Status, Date, State)` will seek on Status and Date, but will scan all States. An index on `(Status, State, Date)`will seek on all three filter criteria.
Hey thanks for the answer, it makes sense with the where clause. and yes you are correct physical location is an address. 
Great info, thanks. Just so I'm 100% clear. If I have a main table of with 100K rows and one of the columns is an ID to another table with 1K for mappings. I join to the other table to retrieve a description for what that title is. I am expecting all 100K rows to come back with a description. What type of join is this if both tables are indexed on that ID? Or does it depend on the type of index?
This is a good scenario for a hash join (small table easily fits in memory), and no index is necessary for join performance. A unique index on ID of the 1k table would help the optimize understand that the result set would not exceed 100k rows. This could help allocate resources appropriately for subsequent operations in the query.
Let's say its 30million to 1million instead of the arbitrary example I gave?
Did you set the default value for the state and county parameters? https://streamable.com/ijvah
A hash join (without perfect indexes) won't be terrible, but now it starts to depend on environmental factors how not terrible it might be. If the 1 mil table is returning ~10 bytes per record, no biggie. ~500 bytes per record is like 0.5 GB held in memory. Now we have to think how beefy the server is. It may be no problem to allocate 5 GB for a query, or it might be limited to 200 MB. In the later case the hash join would spill over to disk (not good). A merge join would of course be faster, but has serious index requirements to support it. You'd wan't covering (typically clustered) indexes on ID of both tables (one of them unique). It should be blazing fast, but adding covering indexes willy nilly can lead to tables taking 10x-20x their base footprint and inserts/updates taking 10x-20x as long.
I guess where I'm struggling, and forgive me if this sounds really stupid... but how does one change: inner join table b on b.id = a.id Into a hash, or a merge join? Does that question make sense?
You can force it using query hints, but query optimizer will automatically pick the best available option. The skill is understanding when a better option is unavailable, and creating indexes so that query optimize can pick it instead. The query hints are: OPTION (HASH JOIN) OPTION (MERGE JOIN) OPTION (NESTED LOOPS) This will apply to all joins in a query, so are only useful if you have one join or want all joins to be the same. Hinting individual joins is dangerous, and requires manually setting the join order and the right-left sides of each join.
If you want to know the downside to doing a truncate &amp; insert, I can think of a few. - Truncate cannot be rolled back if something goes wrong. You do not want to be left in this state. Compared with insert/update/delete which can be committed at the end as a single transaction. - For that brief moment when the table is empty after the truncate, the data is wrong. Contrast that with the insert/update/delete which can be done in a single transaction which is committed at the end (and the unchanged table can still be queried while the operation is in progress) That said, truncate is faster than deletes and updates, but with so little data it might not be worth it just to eek out a little speed. One advantage of replacing all the rows at once is that it is less error-prone and easy to code. I would be more confident in a full replacement being accurate. My recommendation? For the best of both worlds replace all the rows every time, but do it with a delete/insert (delete all the old rows first, then insert all the new rows) and do it in a single transaction that you commit at the end. (no truncate)
Are you talking about temp tables specifically? I'm confused as to why you are worried about how long the index takes to create. If it improves the speed of a query that is (frequently) run against a table then why would it matter how long it takes to create the index? If you have the time, I would highly recommend watching this series. https://www.brentozar.com/archive/2016/10/think-like-engine-class-now-free-open-source/
&gt;Are you talking about temp tables specifically? Not necessarily. They are staging tables, might be dbo. or #. 
I've thought about this more and was hoping you could give me a bit more detail? 99% of the joins that I use fall into two (four) categories: 1. A large table left joining to another table, sometimes very small by comparison, sometimes only slightly smaller. 2. A large table inner joining to another table, sometimes very small by comparison, sometimes only slightly smaller. Can you comment on how each of these scenarios relate to the three descriptions you gave? Oh, and what does this mean: &gt;requires manually setting the join order and the right-left sides of each join.
But if they are persisted tables why do you recreate the index?
They would be truncated each time the process runs. And often times, even if they are permanent tables, I have found you can increase efficiency by inserting data into a table then recreating the index, as opposed to having the index present while you insert. I typically test each possible combination and find the right fit for the job.
Well yes, usually creating an index after truncate and insert is more efficient. Just seems odd that most (all?) of your tables are staging or temporary. 
I work in a weird space man. I used to be a programmer. Almost everything I use SQL for has to do with staging or temporary tables in order to make calculations. For me the entire concept of being a DBA is more of an annoyance, not in a bad way, it just isn't how I access the language and my concerns are not often your concerns. So in that sense it can be difficult to find information for specific situations.
What flavor/version of SQL? If it's SQL Server 2017 you can use the STRING_AGG function.
You'll need to give us the DB....there are ways to do this in most databases but they differ :)
Does your table look like this: | Code | GroupName | | :--- | :--- | | 100 | Apples | | 103 | Apples | | 304 | Apples | | 204 | Reds | | 222 | Reds | | 199 | Greens | And you want to return a dataset that looks like this: | Name | String | | :--- | :--- | | Apples | 11;103;304 | | Reds | 204;222 | | Greens | 199 | Or are you looking for it to return a single row that says, `Apples =11;103;304 Reds = 204;222 Greens = 199`
 Your join is incorrect. You shouldn't repeat the same Scientist_Num field in each of those joins. Also, the Scientist_Num should be in the WHERE clause.
Look into the STUFF function. It works well in concatenation. (TSQL)
It's difficult to guess without knowing your table structure. Use this as a guide: SELECT Scientist.SCIENTIST_NUM, Scientist.OTHER_NAMES, Scientist.SURNAME, Sample.Site_ID, Site.Description, Measurement.Recorded_On, Measurement.NAME, Measurement.VALUE, Measurement.OUTLIER_INDICATOR, Measurement_Type.Units FROM Scientist INNER JOIN Sample ON Scientist.Scientist_Num = Sample.&lt;Some Field&gt; INNER JOIN Site ON Sample.Site_ID = Site.&lt;Site ID Field&gt; INNER JOIN Measurement ON Sample.&lt;Some Field&gt; = Measurement.&lt;Some Field&gt; INNER JOIN Measurement_Type ON Measurement.&lt;Some Field&gt; = Measurement_Type.&lt;Some Field&gt; WHERE Scientist.Scientist_Num = '31415';
Thank you so much!
I hope we get an A!
If you want to understand how indexes work from bottom up, you might be interested in reading my free online book at http://use-the-index-luke.com/ In particular, the chapter on joins might help: http://use-the-index-luke.com/sql/join (From your questions you might be only aware of nested loops join). However, it's a book desinged to be read cover to cover. Your milage may vary if you start reading in the middle. At least skip through the TOC before jumping into the middle: http://use-the-index-luke.com/sql/table-of-contents
&gt; EXEC sp_HelpIndex 'tablename' Select the tablename in SSMS and press ALT+F1
Look into the BETWEEN operator for your date filter. 
&gt; sp_help '[table name]'. Save that in a hotkey,like alt+f1. In a default SSMS config, Alt-F1 already is `sp_help`, with the selected string (object name) passed into it.
I try to stay away from using BETWEEN, especially with dates http://sqlblog.com/blogs/aaron_bertrand/archive/2011/10/19/what-do-between-and-the-devil-have-in-common.aspx 
You might be looking for /r/SQLServer/ SQL is a language for working with relational data
As a general rule, whenever I deal with a bill of materials, I use a Recursive CTE. CTE was introduced in 2005 so it should work for you here. This is untested since I don't really know your table structure but you should hopefully be able to tweak it to finish the query. WITH cte AS (SELECT ParentPart, Component, QtyPer, UnitCost FROM YourTable WHERE ParentPart IS NULL UNION ALL SELECT t.ParentPart, t.Component, t.QtyPer, t.UnitCost FROM YourTable t INNER JOIN cte ON cte.Component = t.ParentPart ) SELECT * FROM cte The first SELECT should return all the base records (which I tried to identify as those with ParentPart IS NULL) and union all the records. The recursion happens by joining the ParentPart to each of it's Component in the CTE. The query should return all the components at each level for each ParentPart. If you want to return specific records you can add a WHERE clause to the final SELECT and filter. http://rextester.com/SKLM52413 Note that I completely made up the LAMP values in the test so the query would work, but hopefully this gets you what you need!
where does the 11 in apples come from
I believe, assuming the database meets the requirements (size is probably going to be a huge limiting factor here), that it should work. Have you tried it?
A few things here 1. it could be the year is not stored as an integer, in which case it would be (if stored as DATE or DATETIME) WHEN ISS_YR =&lt; '12/31/1995' THEN '1995-' ELSE ISS_YR 2. Is the ISS_YR_GRP at the end an alias for that column? If so, iy should be in single quotes.
What is the actual error message? What is the database engine? 
To me it's weird to say that "1995-" is everything *before* 1995. To me, "1995-" means 1995 and onward (1995 and after). Sorry, I'm not really commenting on the SQL, but on how it is displayed. Why not "&gt;1996" or "1971-1995" or "Inception-1995" or even "-1995" or something like that?
Thanks for the reply. Yes, the year is stored as an integer. 
ORA-00932: inconsistent datatypes: expected CHAR got NUMBER Thanks for the help!
Agreed, I'll be changing it to make it clearer. Thanks!
The problem is that you case statement, for '95 and under, is returning a string, while for the other years, it's returning an integer. As the data is validated row by row, I assume the resultset got typed to char as the first entry gave a '1995-', and it bombed on the first year above '95 as the case statement returned an INT for the same field try this instead: case when iss_yr &lt; 1996 then '1995-' else TO_CHAR(iss_yr) end 
I think its because of the size as well, with sql server 2016 it can be 10GB or 4GB in prior editions. If its not because of the size, if you upgrade to 2016, it should work due to cross compatibility of editions.
\*bookmarked\* from the article -- &gt; The real answer? Use an open-ended date range. Always. Seriously, try to break it. This will always work, regardless of the underlying data types (I'll explicitly state that I assume that the data types are, in fact, in the date/time family, and not integers or strings): SELECT OrderID, OrderDate FROM dbo.Data WHERE OrderDate &gt;= '20110701' AND OrderDate &lt; '20110801'; 
&gt;but that that appears to be looked down upon and unwieldy. You know I don't know the answer to your question and feel like I'm not qualified to have an opinion over what is best, but I would like to comment on this point you made here. A lot of times in my line of work I am doing things that others look down upon, or which isn't a best practice. But it works and it results in a big improvement over the way things have been done in the past. Take breaking normal form as a quick example. Everyone will tell you why you shouldn't do it, how it's looked down upon, etc. But now say you're trying to create a data source for a front end facing reporting technology like Tableau or SSRS. If you keep everything in a pristine "sophisticated" way... a process might become unwieldy, and your front end tools will take forever to produce the output that end users are desiring. What's the solution? Fuck what other people think. Always stay open to learning and listening, but if I can take something that takes 10 minutes to do and make it nearly instant for the front end user... and all you've got to say is, "you're breaking normal form" or "that takes up extra storage space," then I'm going to laugh and ask you how much storage costs, then tell you to go buy more and bill my cost center (or add it as an expense, put it in your budget, etc.) You go to your boss, and have him to go his boss, and have his boss talk to another C-level executive that will talk to my boss and they can sort out whether or not the request is worth the expense or not. Not trying to sound like a dick, but too much emphasis is placed on what the correct way to do something is than on what works, and what works better, given the specific nature of your environment. Databases are *for* something. Not all databases are for the same thing. Same with servers. In my case I work on a database/server that is *for* analytics, which includes reporting technologies. So I am much more interested in what works *for* those purposes, and less interested in what works for our DBA. It sounds like you need to ask questions like this and figure out what you're doing this for. Once you do that your idea of having an exchange table of ID's might be really sleek and sexy. Don't let someone else who is smarter and better with databases, but who has no idea what you're doing tell you it isn't a good idea. Listen to them, ask them questions, research their concerns, and then make the decision yourself.
Though I lack the ability to attach the appropriate motivational music to your post, consider it being done in spirit. 
[Proc Freq?](https://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_freq_sect006.htm) or sort it by the variable and retain a counter that switches when a current obs doesn't equal a lag1 variable if you don't want to use procs for some reason.
I can't test it right now, but if you want to do it in sql then this should do it: proc sql; select col, count(col) as col_count from my_table group by col; quit; Edit: include from clause.
Are there any NULL values in your data? 
Can you mark the records in A as used as you use them? 
You don't say what DBMS you are on but this seems like a good scenario for a materialized view (oracle) or an indexed view (sql server) or their equivalent on your platform. 
I think this is a question for /r/sas, but I think the answer is proc freq data=xxx; Table x; Run; Something to that effect 
The name of what you are looking for is anti-join and it is commonly used under the hood to serve queries like your first one. A potential problem with your subquery is that it is correlated. I would first try writing the subquery in a NOT IN clause and evaluate the query plan and its performance. This might alleviate performance issues because it cuts down the ammount of times that the index and/or table b are accessed, doing everything upfront instead. If the index that backs the foreign key fits into memory, it is all good. Another possibility would be to store the PK of A in a third table and delete it from there when a row of B started to point to it. Wouldn't use triggers though. Looks more like something you would queue and and run asynchronously. This will not work if you have more search criteria applied to table B edit: archiving old data from table B would help if possible 
There is some filtering going on table B. You would have to know ahead of time every possible filter to flag every possible scenario. It would get unwieldy pretty quick
I follow this everyday at my job because, simply put, our resources (employees, technology, time) are too limited to do things the "best" way all the time... Usually what's more important is that we make minor improvements over time, with the resources we can afford, to keep the business profitable, the data correct, and the offices running. It's refreshing to know I'm not the only one who feels this way.
There is a partition function that is not in the regular version... I will try and post the exact error tomorrow. I may have a workaround, but I don't don't have access to the main DB so we'll see if it can get implemented. 
Thanks. I'll look at the size. I don't think it's that big... Maybe 4-7 GB
Thank you so much for pointing me in the right direction. Its been a crazy week so sorry for the delayed reply. In my investigation i found my secondary db in a 'not synchronizing / suspect'-state and resuming the data movement fixed it.
You have to restore the database: https://www.youtube.com/watch?v=1Dwni-aLeII However, you first need to install an instance of SQL Server at home in order to be able to do anything. SQL Server Express Edition should be enough.
What the full message from the error log when you try to restore?
Yes, and implementation dependant.
&gt;We made a database and table at school, we were taught how to make a back up of it by generating script (schema and data) and we saved it on a USB That's not how one makes a backup of a database and IMHO your instructor is doing you a disservice calling it such. You're scripting out a copy of the schema, and a (likely very large) *copy* of *most* of the data, but I wouldn't consider it a complete or even totally reliable backup of the database (due to binary data types, mostly). *Assuming* that you have installed SQL Server as well as Management Studio, you need to connect to your instance for each query window, then execute those scripts. In the correct order, because there may be inter-object dependencies (unless you scripted it all out to a single file). Or, you can do a *proper* [backup and restore](https://msdn.microsoft.com/en-us/library/ms187048\(v=sql.120\).aspx) of the database and get everything back in place *exactly* as it was on the original server.
Did you run a checkdb against the 2000 database to make sure it isn't corrupt first?
Looks like you're running Express Versions? It's just telling you that you have a limit. Probably error code 1827. Fix should be here: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/fd9c97b3-0f7a-4d1d-be34-15ad05425907/database-size-would-exceed-your-licensed-limit-of-4096-mb-per-database-error?forum=sqlsetupandupgrade Also, you should try RESTORE VERIFYONLY to confirm the Bak file is good before attempting to use it. https://docs.microsoft.com/en-us/sql/t-sql/statements/restore-statements-verifyonly-transact-sql
This is the difference of DML (Data Modification Language - Data) and DDL (Data Definition Language - Schema). Consider: DELETE FROM LotsOfData; ---This is a DDL (data) statement TRUNCATE TABLE LotsOfData; --This is a DML (schema) statement First what does `DELETE` do? We'll it goes to where the data is stored on disk, and starts emptying it page by page, block by block. Indexes are also emptied. It also checks any foreign key references so that it doesn't delete something that would break referential integrity. Statistics are probably tagged to recalculate, and will scan the empty table when the transaction completes. To put it simply, lots of work. Now how is `TRUNCATE` different? It creates a new, blank table; new, black indexes. If there are any foreign keys defined, it scolds you for being fast and loose with you're integrity and rolls back. It creates new statistics too. What happened to the old data? Nothing! It wasn't even touched. What used to be millions of rows is still there; it's just forgotten about. It's now free space and is eligible to be overwritten with whatever the database needs to do later.
how can i see where my table schema and data is storage ?
The way I've always done it is I have three tables: * `roles`: stores each unique role. * `permissions`: stores each unique permission. * `role_permissions`: links roles to their permissions. (Columns `role_id`, `permission_id`). Then in your users and groups tables, you would just assign a role_id. You could create a view which would show you unique roles for each user if you wanted to cut down on code duplication. I found this method easier because I didn't want to have to modify a table just to add a new permission. Now, you can add/remove permission as needed without having to make a production change to your database. You can also expand this by giving users and/or groups individual permissions without having to create new roles or modify your tables.
I built a database for a video game I play to analyze the data. Always better to use things your interested in to learn skills. Good post!
Read the manual. No information about your (R)DBMS given, nobody will be able to help you.
In Cincy you would at best get told to "F-off whitey" at worst you would get shot
Ever been to Cincy??? , as recently as 2001 we had race riot and it's been on the verge of another one ever since. There is a lot of built up racial tension and distrust, especially between the PD and black communities. There some parts of town the cops won't even go into and let the hood dole out their own justice.
Both would very useful. But the second one, a single row, would be the most valuable. Thanks. 
I suspect a typo 
Product Version: 11.0.6020.0 Product Name: SQL Server 2012 Product Level: SP3
I think you should delete this post and make a new one with more detail than you provided here. As someone else mentioned I think the STUFF() function is what you're looking for but I have no experience with it and cannot help you quickly. To get your post to look like a table use the following code: | Code | GroupName | | :--- | :--- | | 100 | Apples | | 103 | Apples | | 304 | Apples | | 204 | Reds | | 222 | Reds | | 199 | Greens | And you want to return a dataset that looks like this: | Name | String | | :--- | :--- | | Apples | 11;103;304 | | Reds | 204;222 | | Greens | 199 |
Yeah, when I searched videos on youtube how to back and up restore, they show something different from what my professor showed us. I'll be sure to ask him about this. 
Yes, it is considered illegal. Edit: in America at least.
But would I detailed above even be plausible?
It's plausible, but not likely. When developing a public facing site or any site for that matter, it's good practice to take precautions against sql injections, but I wouldn't be surprised if they didn't. 
At a glance the site appears to be working for me. I'd be surprised if you actually managed to damage their site; but stranger things have happened. I'd say it's about as likely they check for injection attacks then redirect all subsequent requests from attack IPs to a fake response to throw them off. I'm also not sure "drop table all" is a thing unless they have a table named "all".
That's what it looked like to me. It began working on the machine I tried from again after a pretty similar amount of time after trying it. Will not be doing that again. About the time I take down a massive company site is the same time they haul my ass off to jail. Noooo thank you. 
If you're just looking to play around, I'd shy away from drop commands or directly editing data in any way, maybe go for "SELECT * FROM INFORMATION_SCHEMA.TABLES". Then you can take more of a white-hat approach and even notify the site owner if you encounter a vulnerability. But that's *unlikely* to work on most major websites since the people that build them *usually* know better. Injection attacks tend to work more on older or neglected web sites...
You can use the `COUNT` and `SUM` [aggregate functions](https://dev.mysql.com/doc/refman/5.7/en/group-by-functions.html) to count rows and get a running total for a column. Using the `DISTINCT` modifier and a `GROUP BY` clause in your `SELECT` query will let you condense the rows down to the information you're after. How exactly to put them all together depends on your schema. Suppose you have a table called `big_hits`, like this: +----+---------------------+-----------------+----------+ | id | timestamp | ip | bytes | +----+---------------------+-----------------+----------+ | 1 | 2017-09-09 23:15:03 | 192.168.8.5 | 10420209 | | 2 | 2017-09-09 23:16:37 | 10.5.8.2 | 10420209 | | 3 | 2017-09-09 23:20:29 | 10.154.20.218 | 3019388 | | 4 | 2017-09-09 23:21:00 | 172.16.166.201 | 1190257 | | 5 | 2017-09-09 23:21:00 | 172.16.166.201 | 3019388 | | 6 | 2017-09-09 23:24:08 | 192.168.25.250 | 3019388 | | 7 | 2017-09-09 23:24:08 | 192.168.25.250 | 3019388 | | 8 | 2017-09-09 23:26:17 | 192.168.229.225 | 1190257 | | 9 | 2017-09-09 23:26:18 | 192.168.229.225 | 10420209 | | 10 | 2017-09-09 23:34:38 | 10.158.176.146 | 1190257 | | 11 | 2017-09-09 23:34:38 | 10.158.176.146 | 10420209 | | 12 | 2017-09-09 23:35:28 | 10.228.133.157 | 10420209 | | 13 | 2017-09-09 23:39:00 | 203.0.113.163 | 10420209 | | 14 | 2017-09-09 23:41:23 | 192.168.25.250 | 3019388 | | 15 | 2017-09-09 23:41:23 | 192.168.25.250 | 3019388 | | 16 | 2017-09-09 23:43:45 | 240.12.76.149 | 1190257 | | 17 | 2017-09-09 23:43:45 | 240.12.76.149 | 10420209 | | 18 | 2017-09-09 23:45:59 | 192.168.19.236 | 1190257 | | 19 | 2017-09-09 23:46:07 | 10.67.89.179 | 1190257 | +----+---------------------+-----------------+----------+ Here's a query that will get a list of unique hosts, along with the number of hits each one has made and the total bytes they've downloaded: SELECT DISTINCT(ip), COUNT(1) AS hits, SUM(bytes) AS total_bytes FROM big_hits GROUP BY ip ORDER BY total_bytes DESC; This will give you results grouped by IP and sorted by the biggest "hogs," descending: +-----------------+------+-------------+ | ip | hits | total_bytes | +-----------------+------+-------------+ | 192.168.25.250 | 4 | 12077552 | | 240.12.76.149 | 2 | 11610466 | | 192.168.229.225 | 2 | 11610466 | | 10.158.176.146 | 2 | 11610466 | | 192.168.8.5 | 1 | 10420209 | | 10.5.8.2 | 1 | 10420209 | | 10.228.133.157 | 1 | 10420209 | | 203.0.113.163 | 1 | 10420209 | | 172.16.166.201 | 2 | 4209645 | | 10.154.20.218 | 1 | 3019388 | | 192.168.19.236 | 1 | 1190257 | | 10.67.89.179 | 1 | 1190257 | +-----------------+------+-------------+ 
Okay, that's a pretty cool idea. But yes, most people should have their bases covered. 
&gt; avoid lakes with an unknown depth (-999) this is in fucking **graduate school**???
&gt; it's good practice to take precautions against sql injections Yet it's still on the top of the [OWASP Top Ten Security Risks](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project#tab=OWASP_Top_10_for_2017_Release_Candidate_1)
Here's an example from MS Dynamics CRM that is very similar: https://msdn.microsoft.com/en-us/library/gg328230.aspx Keep in mind that this CRM uses a lot of "type" and "intersect" tables which aren't always necessary in other applications. But this flexibility allows a significant amount of customization through the front end because you don't need to make db modifications. 
This is where I stalled out: &gt; Microsoft Access to build databases for GIS data But the data described isn't *really* GIS data, it's more like Wikipedia-level stats 
That's why I said it's plausible
Yep for graduate school. It's for a database management class. And this is the first time I have ever used MS Access or really any in depth SQL. I am having a rough time with figuring out how to lay out the code to return the results where the capitals are grouped, and how to get the averages for the lakes.
Do you mind telling me which game? I'm looking for a bit of inspiration. 
Tiny Tower
Thanks
non correlated version of the subquery. You should probably use paralelism hints if the query is not covered by an index SELECT KEY, c1, c2 FROM table_a WHERE blabla = bla AND KEY NOT IN (SELECT FKEY FROM table_b WHERE location = 'Kanto') 
You cannot backup and restore a enterprise database ( with partioning) to a standard edition instance. The database wont come online. Also you have to make sure your are not trying to restore to a lower version on your machine (2016 db restoring to 2014 instance) Sql express i believe limits you to 10gb database sizes.. Just download a developer(all functionality enabled) free version to bypass for you project
select a.in_countr, b.capital, avg(a.depth_ft) as avg_dep from lake as a, city as b where a.id=b.id and a.depth_ft &lt;&gt; -999 group by a.in_countr, b.capital
Right on my friend! Thank you for the help!
Thanks. I believe we have a Deve version somewhere. I'll give that a shot. Preciate it! 
Cool beans!
Why would you think that was a good thing to do? Especially if you're unsure of the consequences? Sounds to me like you're more worried about getting caught since at first sniff it seemed like you did exactly what you intended to do. I don't get the 'playing it off as ignorance' angle. You knew what you were trying to do, otherwise you would have said SELECT TOP 10 * FROM customers that wasn't so blatantly malicious. Maybe you're young, maybe you're naive, I don't know. But if you weren't trying to be malicious, then just walk away from this knowing that it was a very stupid thing to try. But yeah. As someone else pointed out, the probably don't have a table called all, the app account probably doesn't have schema mod permissions, and they probably dump their web form input into a prepared statement.
Try this: SELECT Groupname, SUBSTRING((SELECT ', ' + code FROM table WHERE groupname = t.groupname FOR XMLPATH('')), 3, 10000) AS codes FROM (SELECT DISTINCT groupname FROM table) t Sorry for formatting, I'm on mobile. 
Thanks, I'll give that a whirl. 
There sure is, I've actually come to this one not too long ago, take a look, might help you out - https://www.starwindsoftware.com/blog/ubuntu-manage-your-sql-server-from-linux
forgot about the MINUS operator that some databases have. https://www.techonthenet.com/oracle/minus.php
IIF doesn't exist in SQL Server 2008. https://docs.microsoft.com/en-us/sql/t-sql/functions/logical-functions-iif-transact-sql &gt;THIS TOPIC APPLIES TO: SQL Server (starting with 2012) Use CASE instead. CASE WHEN [CalWk_1] &gt; 35 THEN [CalWk_1] -35 ELSE [CalWk_1] +17 END as FiscalWk Although, when I work with Fiscal Periods I prefer to make a table with those dates and join my queries to it. That way if I do multiple reports I don't have to keep copying my CASE statements everywhere. edit: Why would you delete the post? 
That is entirely too clever.
Eh, I would just make the count a subquery by itself as one of your columns, and then add `WHERE Status &lt;&gt; 'Cancel'` to the main query.
Could you do... SELECT COUNT(*) AS t_Count, DATENAME(MONTH, dbo.transactions.t_Date) AS t_Month, dbo.transactions.t_Type FROM dbo.transactions GROUP BY DATENAME(MONTH, dbo.transactions.t_Date), dbo.transactions.t_Type UNION ALL SELECT COUNT(*) AS t_Count, DATENAME(MONTH, dbo.transactions.t_Date) AS t_Month, 'All' FROM dbo.transactions WHERE Status &lt;&gt; 'Cancel' GROUP BY DATENAME(MONTH, dbo.transactions.t_Date) --, dbo.transactions.t_Type 
Restore failed for server 'myserver' Microsoft.sqlserver.smo Additional information system.data.sqlclient.sqlerror: CREATE DATABASE or ALTER DATABASE failed because the resulting cumulative database size would exceed your licensed limit of 4096 per database. 
Yeah, it's still good 
SELECT COUNT(*) AS t_Count, DATENAME(MONTH, dbo.transactions.t_Date) AS t_Month, dbo.transactions.t_Type FROM dbo.transactions where dbo.transactions.t_Type &lt;&gt;'Cancel' GROUP BY DATENAME(MONTH, dbo.transactions.t_Date), dbo.transactions.t_Type WITH ROLLUP dbo.transactions.t_Type &lt;&gt;'Cancel' will exclude those records
This got me in the right direction. Thank you very much. I suspect you probably just missed the WHERE clause of the first SELECT. SELECT...WHERE Status &lt;&gt; 'Cancel' GROUP BY...WITH ROLLUP UNION ALL SELECT...WHERE Status = 'Cancel' GROUP BY...
This with a UNION query selecting a COUNT of Cancels Grouped by Month did it for me. Thank you for the idea.
Makes sense, this helped get in me the right direction. Thank you. Queried all excluding Cancel with then `UNION ALL SELECT COUNT(*)...FROM...WHERE Status = 'Cancel'`
CHARINDEX and SUBSTRING should do it. Something like this: https://stackoverflow.com/questions/11651074/isolating-a-sub-string-in-a-string-before-a-symbol-in-sql-server-2008
I didn't miss it, I got rid of the ROLLUP all together. The second query replicates the ROLLUP (with the ability to filter out the "Cancel"). But, I do see where I left the dbo.transactions.t_Type in the second query in error. Either way, I'm glad you got what you need. 
Assuming Table1 looks like this: Column1|Column2 :--|:-- ActualData|thisdoesnt matter NULL|PullThis RightOfSpace NULL|PullThisToo RightOfSpace SomeOtherData|Dont Pull This Then you can do this: SELECT CASE WHEN [Column1] IS NULL THEN LEFT([Column2], CHARINDEX(' ', [Column2]) - 1) ELSE [Column1] END FROM Table1 To output this: Results| :-- ActualData| PullThis| PullThisToo| SomeOtherdata| SQLFiddle: http://sqlfiddle.com/#!6/386d3/1
Do you mean TRUNCATE vs DELETE? Or are you doing some sort of data mart replication where you have to pick between Drop/create/insert and Truncate/Insert?
Thank you, I used something very similar to this!
I really appreciate this! I had to modify it ever so slightly to: SELECT CASE WHEN [Column 1] IS NULL THEN LEFT([Column 2], CHARINDEX(' ', [Column2] + ' ') - 1) ELSE [Column1] END Basically, it kept tripping up on the fields that *didn't* have a space, so I just added one at the end. It works like a charm. Thank you so much!
&gt;we are told that truncate is horribly inefficient and may cause slowness in our daily SQL agent job instead of using the drop table command Ha... that makes no sense. I wonder if someone got confused and assumed `truncate table` means `delete * from x`. Truncate _literally_ just says "Hey, this table is now empty"... just like `drop table` says "Hey, this table doesn't exist." Both are minimally logged and run immediately... I've never had an issue running `truncate table`. `delete * from x` on the other hand is really inefficient and you should probably be using `truncate table` or `drop table` if you're deleting the whole table. Edit: ah if we're talking about inserting data _after_ one of these.. yeah, `insert into` vs `select into` will have different behaviors depending on the situation... but there isn't a single "correct" way in that regard
He means `TRUNCATE` and `INSERT INTO` as opposed to `DROP TABLE` and `SELECT INTO` OP, for what its worth I use both. I can't really give you good examples of when I choose one or the other, but I typically use TRUNCATE/INSERT INTO for a finished product, and the real performance difference comes from indexing. If you have an index on a table then inserting into it can take a long time... but sometimes you can just drop the index and then recreate it (which is basically the same thing as dropping/indexing) but with a truncate I get to more specifically control the column structure. For example in a drop/into you might get a column that is varchar(100) but if you decide to run a historic load instead of a the last 12 months you might know that it really needs to be varchar(200) or else you'll get an error. Granted in the drop/into model you'll get a compatible column type but you might want it to be nvarchar(200) because you will be joining it to another table that is nvarchar. I don't know off hand if joining varchar to nvarchar is less efficient than nvarchar to nvarchar but I like to keep things as consistent as possible. So for example if I have a large process I might drop/into tables along the way, but then at the very end to put a nice pretty ribbon on things I tend to truncate/insert into so that I can minimize any errors I might get when plugging those tables into a front end analytics/reporting technology. Last thing I want to have to do is going into Tableau or SSRS because something went from bigint to float for some stupid reason and now my final product isn't working. 
You could create a user-defined function (or a procedure) that takes in a 'as-of' date or if your formulas have a well-defined structure (linear polynomial over same variables, for example) you can define a reference table for terms with validity date ranges and more or less directly incorporate that into your queries. I'd still vote for an udf even in the latter case.
here's a discussion on stack overflow: https://stackoverflow.com/questions/135653/difference-between-drop-table-and-truncate-table Short answer: truncate isn't 'horribly inefficient' compared to DROP TABLE. Look in the comments on how truncate table *can* be rolled back inside a transaction. What actually happens when you truncate table is that it deallocates the leaf (data) pages, but keeps the trunk pages in the table. The issue people have with Truncate table is that it does compromise the restore - of the table being truncated. If it's just an intermediate product taken from base data, no big deal. Truly enlightened programming though is that you don't need a temporary table at all - just do a view, read from the view to send whatever dataset whereever it needs to go, and don't go through the expense of reading from base, writing to temp table, then reading from temp table - do it once instead of many times. 
As a best practice, I would recommend truncate to clear the data and using bulk insert operations for reloading data. Additionally, you should disable all non-clustered indexes when inserting your data. So it would look something like this TRUNCATE TABLE x ALTER INDEX my_index ON my_table DISABLE BULK INSERT ... ALTER INDEX my_index ON my_table REBUILD * Bulk insert will perform better * DELETE is horribly slow because it's a logged operation * DROP TABLE may get you into trouble if you are using things like auto-deployments or if the schema happens to change and you forget to update your code that recreates the table. For performance it's really no different than truncate. Truncate and disabling / rebuilding the index is just safer for keeping the existing schema intact If you're doing things like bulk inserting a lot of data, depending on your process, some or all of these may apply to you * Modifying the Network Packet Size on the connection may improve performance * Using SSIS or the Import/Export wizard may allow you to take advantage of changing the Rows Per Batch and Maximum Insert Commit Size options which may improve performance * Partitioning your table that you're inserting into may improve performance because you can load a staging table then switch the partition in to the target table quickly with a simple schema lock. Alternatively, if you're truncating because you have to delete lots of data you may save overall loading times by switching old data out and truncating only that old data. This is a more advanced topic and it sounds like you're probably not going to use this type of suggestion but I just wanted to throw it out there for you. * keeping your data types as narrow as possible may help too because if you're using NVARCHAR() types where only VARCHAR() is needed then you will be wasting extra bytes and space that take time to allocate upon loading the tables and rebuilding the indexes * if you have lots of NULL values, you should look into using sparse columns to also save space Some of these features are only available on Enterprise edition so you will have to verify before attempting to use it.
Have you tried disabling the index when you insert? That way your index won't slow down your inserts but you also won't lose your column definition. 
What RDBMS are you using?
How?
ALTER INDEX ALL ON [TABLE] DISABLE or you can do them individually ALTER INDEX [indexname] ON [TABLE] DISABLE then rebuild after you insert ALTER INDEX ALL ON [TABLE] REBUILD
Can you give me your thoughts on this? I have been told that if you are inserting data and planning to use a cluster or a PK, that you can improve performance by ordering the data in the way that you expect it to be indexed. I've never really tested this personally in many situations. Just curious what your thoughts are.
It does make sense. There are performance implications by ordering the query to begin with though. So I am not sure how significant the gain would be, if any. Would also be a way to avoid page splits, if that is a concern. Then again, if your clustered index is an integer ID, it may not give you any increase. Changing the FILLFACTOR of the table itself could help as well, but probably not so much on a bulk insert. Would be an interesting to test out and see.
That might be where the issue was before with the truncate. They'd usually have to import roughly 300k-400k lines with 10-20 columns on a linked server that doesn't exactly perform well. Wonder if the insert into was what was causing the slowdown and not the truncate itself.
He shouldn't just blindly use truncate though, there are scenarios when delete is more appropriate (transactions are logged / can be rolled back)
If I understand correctly I would have a table that holds the functions, their valid start and end dates where if the end date is NULL then the that function is good any time past the start date, you could also just set the NULL dates to the MAX date and not worry about it: function_name | start_date | end_date ----------------------------------------------------- FormulaA V2 | 2017-09-01 | NULL FormulaA V1 | 2017-08-01 | 2017-08-31 Then you could join from your main table to this table (using an IN clause) to determine the function by the date, you just have to make sure you don't have overlapping dates.
I'm assuming we want to delete every row of the table, obviously delete is the way to go if you don't want to remove everything... Truncate and drop are logged and can both be rolled back if wrapped in a transaction. If I wanted to delete an entire table and be able to revert it without a transaction... I'd take a full backup, do a `select * into table_bak from table` then truncate... I wouldn't use delete to remove every row of a reasonably sized table... it's just going to fill up the transaction log and take a long time to run. I do the select into so I can just drop the table, rename the backup, and get everything online faster. I do the full backup just in case I do something stupid and truncate the wrong table, delete the _bak table too early, etc... But yeah, the usage of each depends on the nature of the table and how it's getting reloaded, backed up, etc...
My bad, I thought you were just saying go for truncate regardless. 
Table based maintenance seems like a great idea! I think your assumptions here are correct. The only question I have is in this case would the table be used to build dynamic SQL which is then executed? Because those formulas need to be given inputs and calculated so that they can be used downstream.
That's the method that was given as an example. Basically there would be a "shell" user function that takes in the DataDate and runs the right function based on the date. So instead of having one long script with conditions they broke up the functions into different versions. I understand the reasoning behind that but I wonder how much of a nightmare it can grow to in terms of maintenance.
The task is unnatural so I doubt the question of "maintenance" is really relevant. In any realistic case if your inputs change and your logic also changes you'd want to capture the results at the moment of when they were relevant with links to whatever defined your relevance context - date, environment, code version, etc. There are better systems of keeping track of code/logic versions than relational dbs. There's CLR as well and with Oracle (or another DB that has UDT/methods) you would have had yet another approach, but in the end it doesn't change my overall sentiment.
I agree, in the interview I mentioned that I would explore offloading the data processing from the database altogether. So just use the database for storage and have a process that uses a scripting language for crunching the data. Outside of RDMS, what other tools or approaches would you think are feasible for something like this?
Thanks for all of the info you put here. I haven't seen bulk insert before so I'm going to do some more research on it. It looks like at first glance that it might be a good way to get flat files into the database. I'm curious whether or not it would work when querying linked servers. Thanks again!
The index idea is intriguing. It's a table that's being pulled in from a linked server that is actually a merged data source so there's a decent chance that the index that gets applied is not the most efficient one that it could be. This is definitely something that I'll look into.
Agreed on the enlightened programming bit. I'm coming in and will be the third or fourth person to work on this SP this year. Since it has changed hands a few times (without any documentation being created around why people did things the way they did), it isn't as efficient as it could be. Moving it to a straight pull instead of the temp table may be easier to do in the long run. Thanks for your input!
I have seen a few misrepresentations of the TRUNCATE command lately. Thank you for accurately describing it. I can further chime in to let you all know the reason why people say that there are possible performance impacts to using it. The actual command is amazingly fast because it is just a change to the table's metadata. The drawback is about the statistics of the table. A TRUNCATE command does not trigger any of the statistics auto-update. If you just truncated 5 million rows from your table, the SQL optimizer will continue to pick query plans that assume there are still a crap ton of rows in that table. Thankfully you can completely get around this drawback by simply updating statistics on the table after truncating it. 
Both TRUNCATE and DROP are logged operations just like DELETE. The reason DELETE is slower isn't due to a difference in logging.
if u are on sql server 2016 u can use STRING_SPLIT to split the string. unfortunatly microsoft "ruined" this function by not returning an index but u can mimic it with a row_number() or use top 1
If you have an ugly series of transforms, its' often easier to just change them into views. (updates are another matter, but you can figure out how to make them views by making sub-views based on the selects in the updates). Deletes you just put into the where clause in the view. I blame BOL for encouraging this sort of programming by expert beginners, but the upside is that it's pretty easy to do a 'dumb' refactor if you just focus on redesigning a view to do the typical single step for each transform w/o trying to understand the logic chain, and letting the optimizer worrry about the hideous tortured logic that got you from a platonic single SQL statement to a 8 temp table transform. Good luck! 
I'm trying to imagine a production table where you have concerns about perf with other sprocs hitting the table, but at the same time you're allowing a process to truncate it. It wouldn't be the dumbest thing I've ever seen (or writen, ahem) in my career, but it would still be a WTF type code review were I conducting it. reference: http://i.imgur.com/J1svNp7.jpg 
https://hs2n.wordpress.com/2012/04/03/oracle-create-database-link-to-mysql-database/
Looks like you should first sort out better what you want and what you don't want. I regularly suggest to people to codify specific select queries for specific purposes as (materialized) named views so later you can just `select * from view_xy`; it seems that idea would be practical here, too, not least because you can easily set up a number of experimental views with differing sorting criteria and compare the results in a piecemeal fashion. Teaches a lot I think. Your observation that `order by published_at desc, totalSum desc` 'ignores' `totalSum` is not correct. When you say `order by a, b` what you get is an ordering of everything according to the value in `a`; then, for all the cases where two adjacent records have the same value for `a`, then `b` will be taken into consideration (and if both `a` and `b` are the same in two records, their ordering is random). What you observe is more likely that since your first ordering field is a quasi-continuous quantity (time) and / or the number of records is not quite high and / or you haven't looked hard enough, a case where two records have the same `published_date` but two different point sums has not been met with so far. You say that when someone adds a record without price and no image, it gets zero points, so it should show up late in the list, but because it has a recent date, it still shows up near the top, and you don't want that. Next, you propose to *add* a point for *recent* dates, but when ordering by points descending, that would of course help to push up those newer records, so I don't understand what you're after. What you could do is derive a 'granular age' of the `published_at` column, say, age in either integer number of days, weeks, or months. Within such a larger group, there's then a chance for two records to be both 'roughly 0 months' (=less than 30 days) old, say, and *within that group*, records with points for an image or a price could float to the top, as it were. If it's that what you want. 
Didn't OP stipulate MSSQL? Would love to be able to do this without having to install Oracle client on SQL Servers. 
Oh, right. I read MySQL there, sorry. :)
 This works , and you can substitute the variable with a column. DECLARE @STR VARCHAR(50) = 'ABCDEFG HIJKLM'; SELECT LEFT(@STR,CASE CHARINDEX(' ',@STR) WHEN 0 THEN LEN(@STR) ELSE CHARINDEX(' ',@STR) END) AS LEFTOFSPACE; 
Store the function names and then call them using dynamic sql.
http://www.dba-oracle.com/t_heterogeneous_database_connections_sql_server.htm I don't have much experience with this method specifically, but it reads very much like doing a Linked Server on the MS SQL side with the Oracle client installed, which my environment is full of. Take care to handle your security well. If this is anything like the SQL version, it is really easy to open up some really big security holes in your environment.
Well if I'm understanding you correctly, for sql server you would do IF DATABASE_PRINCIPAL_ID('RoleName') IS NULL --insert script to create GO Sorry for poor formatting. On mobile. 
Do you have the project in VS? You can do this with SSDT and Schema Compare I believe.
Looks close, is this what you were after? DECLARE @B Int; DECLARE @A Int; UPDATE A SET column = GETDATE() FROM A INNER JOIN B ON A.ID = B.ID WHERE @B IN () AND @A =
You'll need a stored procedure, with those two parameters as the parameters for the procedure. It may be best to do a little input sanitizing first before going straight away to updating your data. Alternatively you could use a staging table and MERGE to make sure you don't get any bad input or data from your users. https://docs.microsoft.com/en-us/sql/t-sql/statements/create-procedure-transact-sql If you need a UI component, I would recommend SSRS. It's easy to overlay on top of a procedure and you can easily pull a list of values, or create blank text boxes, that allow users to pass parameters and execute your stored procedure.
&gt;If you need a UI component, I would recommend SSRS. I don't like reports running stored procs that can change the state of the data. IMHO, reports should be read-only, not a RAD environment for applications that update data.
What is your learning path? Online courses/classroom?
Technically, yes, but I wanted to keep my explanation simple. I don't know how to explain it without going into too much detail otherwise.
No problem, you may also want to read this https://technet.microsoft.com/en-us/library/dd425070(v=sql.100).aspx Things like changing the DB recovery mode and using the table lock hint may also give you some added performance. And yes, using linked servers should be fine using INSERT INTO ... SELECT according to that link above. Just check the limitations on the chart in the article. You may want to opt for one of the other methods because it'll give you more optimization options. Also, this line in the article is wrong as of SQL 2016... &gt; You can truncate the entire table only, not a single partition (but see â€œDeleting All Rows from a Partition or Tableâ€).
Oracle's Gateway product is very good for this. Of course it costs. 
_Oracle's Gateway_ _Product is very good for_ _This. Of course it costs._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^So_average ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
You can do like below IF NOT EXISTS (SELECT name FROM master.sys.server_principals WHERE name = 'LoginName') BEGIN CREATE LOGIN [LoginName] WITH PASSWORD = N'password' END
As far as speed is concerned the difference should be small. And anyway if you don't need the table structure at all, certainly use DROP.
I've never been clear on how modifying the packet size may improve perf. Under what conditions would say increasing the packet size improve the performance? 
Typically in any scenarios where you're transferring large amounts of data you would want to use a larger network packet size. This option would be used if the server is a warehousing / ETL server. If it's a mixed load or not used for bulk operations it probably shouldn't be modified. It should also be thoroughly tested with both default and modified settings to see if it helps the workload.
I agree with you. It sounds like this scenario is going to have users modifying data regardless, though.
This is completely dependent of how you're wanting to handle this, but I'll answer this question first: &gt; Would that slow down the db too much? You're not going to slow down an Oracle DB with the limited data you'll be inserting into this table just for your bank transactions. Extrapolate that out to millions of users - and then yes, but Oracle is a beast and unless you're mocking millions of rows of data to churn through - you're talking about milliseconds of slowness, not seconds. &gt; Do I essentially write out a big fat IF/ELSE statement for all the "rules" for assignment and have each row/transaction that gets inserted go through that whole code? That's one way to do it. If this were a production application that you were looking to create, then I wouldn't let the database touch this job - and instead have the application or application server handle this categorization, and only once the categorization was done write the data to the database. &gt; Is this the best/optimal way? You should refrain from creating triggers(which is essentially what you're describing) too loosely when designing a database. You should further refrain on creating triggers BEFORE INSERT OR UPDATE, especially if this will be a heavily trafficked table you'll be working on. This isn't to say you should never use triggers, they have their uses, but I tend to rule on the side of only using triggers as a last ditch effort for performance reasons on the initial write of the row to the table. You could have a lookup table with common keywords and the category they map to, and then have a scheduled job in Oracle that runs periodically to categorize the data after the initial write to the table. Defaulting the category to something like 'Pending' until the scheduled job picks up those rows and runs the categorization on them. Or, you could go the trigger route and do an AFTER INSERT OR UPDATE. EDIT: As a note, every database environment is different - with different architecture, resources, and experience levels working on it. In larger, production, database environments you could performance tune until your eyes bleed and still have room for improvement. Don't fret too much over "is this best practice for millions of rows in the database" - you'll still end up reiterating over the code multiple times before usage ever gets that high. Spend time developing for what you need today, and then expanding your feature set - instead of wasting time developing for tomorrow or a year from now and possibly never needing it. 
Thank you for the reply. I did figure that in this really small database the "slowdown" would be negligible, however I still want to write this thing correctly rather than duct tape together something that works for just my purposes. (this is a learning experience/pet project for me). As for the trigger I was definitely thinking of categorizing each row AFTER the insert into the table. As for the Lookup table, that could probably work. Something like a table that has columns: lookup_id, trans_category, desc_rule? Then I can compare descriptions in the master table to rules in the lookup table and update the master table accordingly. And I can probably manage the list of rules more efficiently by just adding a row to the look_up table rather than editing SQL code in an IF/ELSE statement. Does that sound about right?
I don't understand why you're doing this in a loop. Wouldn't this be sufficient? You're using only one database that I can see. update set Misc_Reference = Description where D.ID = 'ID' and Line_No in (select Line_no from Document where id = 'ID1') Even if you have to work with multiple databases, as long as they're on the same instance and the same credentials get you access to both, you can perform a set-based `UPDATE` operation using a `JOIN`.
Misc_Reference is in Database 2 and Description is in Database 1, so I couldn't think of a way to do that without using a loop. I need different credentials to access both.
There's a Microsoft product called SSIS that is great at cross-database data manipulation. That would be my go-to, rather than learning python. Now, if you want to learn python and this is a good excuse to start, then by all means continue on and good luck!
Ok great, I'll give that a try as well thank you. 
Yup, though again I would say that if you plan on keeping this on the database side, to use a scheduled job instead of a trigger. A trigger is going to write after each row is processed (for an AFTER trigger), a scheduled job will batch the rows needed to be processed and write them accordingly. If I were doing this on the database side in Oracle, I would: 1) set the default value of your trans_category column to something like 'PENDING' or 'P' 2) Create an Oracle package with your matching logic included. 3) Create an index on trans_category that indexes based on trans_category being your default value (performance bonus!) 4) Schedule job to pick up any rows whose trans_category = 'PENDING'/'P' and run them through your Oracle package for matching, updating the trans_category. 
Excellent! That's exactly what I'll do. Thank you for your help.
I'm actually going through /r/WGU on the BS DMDA (Database Management/Data Analytics) 
Genre should use a link table unless every book will always have two genres Will every book only have a single author? This seems odd Address should be considered carefully - what happens if two authors or publishers share an address (e.g. Ladybird and Ladybird kids, or authors from the same family) Is the information in ISBN that specific? E.g. Does a hardback version have a different ISBN? Does a book get a new ISBN if republished, do you need to track that information? I'm not sure on this one, so it's a question rather than an error, but if that information can change then you may need to track it as releases under each ISBN, or something
To expand on that, it's usually frowned upon to use data elements that have intrinsic values as primary keys for a table. Modelers would ask you to create a new key (usually an identity field works) for the primary key on the table in case the ISBN had changed meaning. A more practical reason for this, clustered indexes.. clustered indexes are set up to sort the table based on the order of the clustered index field (ISBN or the identity key). If you insert a new record and the key is the ISBN then the table has to re-sort the data so the keys are correctly ordered, but if you use an identity, then it's all good as the new record is already sorted to be the next one.
&gt;To expand on that, it's usually frowned upon to use data elements that have intrinsic values as primary keys for a table. I still have scars from the last time I got in a natural key vs. surrogate key fight on this sub. It got very nasty very quickly.
I'd look at it a little differently. First of all you cannot use aggregates or sums in a single update (I'm guessing MSSQL). So... create a temp table using your sum function and update joining on your source table to the temp table. You could then structure your error handling to only begin if the rows of the temp table are &gt;0 else return the response
see https://www.reddit.com/r/mysql/comments/6zsz4y/beginner_needing_help_with_pivoting/dmxypnh/
What do you mean about using aggregates or sums in an update?
If I were coming from an Analyst role to a DBA role, I'd have to figure out a few things. The main thing from me as a commentor perspective is that I don't know what DBA means to you and your company. I was a "DBA" at my last job. I did backups / restores, indexing, reporting in SSRS, SSIS packages, development on production DB's that hosted web apps creating the architecture, procedures, logging, error handling, historical tracking, I did alerting on prod for our systems, and I did a bunch of cluster configurations and tons of other things. It really was more of a, you have databases, go do database tasks type of job. DBA will differ greatly from place to place. Some DBA's will never touch indexes or backups because they are a developer DBA and the system DBA does that. Others may focus on ETL only, etc. So I'd definitely first figure out what you will be responsible for and what your duties would be. I'd find out what you will spend 60% of your time on in that position in the next year, try to break those out into categories, and then learn those.
Thank you very much for taking the time to respond. It's a very quick process, so i don't have ages to learn (we found out three days ago - put in our applications yesterday, and the interview is in three days). I will have nothing to do with web apps (NHS role that will have nothing to do with apps). I will definitely have to write reports in SSRS, and if our SSIS procedures fail, I will need to be able to identify why and correct the problem (having spoken to our DBA, this will largely be due to all our files not being delivered on time, or one of our tables having had a column added). I only know a tiny bit about indexing, but i think most of that will already be done, however i may have to "index occsasionally" (apologies if that's the wrong terminology - i only really know it's something that will speed up queries). Our data gets sent daily by an external company, so i think backups will not be my job, and restoring would be very similar to our daily load procedure. I guess I should put my efforts into SSRS (I have made one report before, so i kinda know what i need to look up &amp; practice), indexing &amp; SSIS. I've just had a quick google of indexing and think i should be able to fairly easily get my head round that, so any basic tips of where i should start with SSIS packages would be good if you have any more time. If not once again thank you for the time you have already taken to reply.
SSIS and SSRS is best taught via projects. I'd recommend to do a few easy SSIS projects for a few varied tasks. Perhaps a few that load data, some that transform, some that extract. Maybe do a few where you email results or SFTP files, move around things, archive / delete files. SSIS is a pretty broad topic though and it has a relatively steep entry learning curve. I don't really have any good beginner links but you could probably google for some.
Much obliged.
I should have mentioned. MS SQL.
This kind of popped into my head: http://www.sqlservercentral.com/stairway/72494/ Side thoughts, I highly recommend to learn some basic information about what wait types are and how you use them, along with indices, and how SQL Server stores data and works. (Talking how data gets allocated by pages and such.) I would also recommend to learn about backups, really just some of the big differences between full, bulk, and simple. You may end up taking about a day to do all of those topics and I'd probably review it again later to see what stuck. 
One thing you might have to do with SSRS/SSIS in a DBA role is security. I'd take a quick peek at the differences in the Roles on SSRS, and figure out how your organization partitions access to data. We use specified AD groups (RSOperations, RSFinance, ... etc.) to centralize access based on an AD login. This has proved very helpful in administration as people leave. Same thing with basic SQL security. Use AD groups not individual logins, use Windows Auth if possible, disable the sa account etc... Learn the differences between roles, and know how to audit permissions - so you can see what logins have the access to do something stupid, like drop a production database... 
I recommend to give us some metadata about both tables. This query should do that, although you will need to limit the query to both tables. SELECT t.NAME AS table_name ,SCHEMA_NAME(t.schema_id) AS schema_name ,c.NAME AS column_name ,st.NAME 'Data type' ,c.max_length 'Max Length' ,c.precision ,c.scale ,c.is_nullable ,ISNULL(i.is_primary_key, 0) 'Primary Key' FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID INNER JOIN sys.types st ON c.user_type_id = st.user_type_id LEFT JOIN sys.index_columns ic ON ic.object_id = c.object_id AND ic.column_id = c.column_id LEFT JOIN sys.indexes i ON ic.object_id = i.object_id AND ic.index_id = i.index_id ORDER BY schema_name ,table_name;
Based on the query you provided, MAX(dt_last_fm) is being aliased as "fm", which is joined against Orig.dt_last_fm as "Filter.fm". You will likely need to investigate the table metadata like /u/Rehd mentioned, and potentially do a CAST or CONVERT before the join.
\u\Cal1gula is completely right. Use a recursive CTE. I actually work with Syspro for work, so I already have a recursive CTE written. I'm off for the day already, but if you're interested, send me a PM and I'll sanitize my code and send it over. It may be what you're looking for, maybe not. er.. Holy crap, this is a few days old. Statement still stands to send over code - but I'm hoping you got this figured out by now. If not, let me know!
It's joining a table back to it's self. Dt_last_fm is datetime. Length 8, precision 23, numeric scale 3. Should work comparing to it's self? Tried replacing every instance of it with a cast as datetime. Same error. If I replace it with a static value it works.
How does Datetime have a length, precision and scale? I feel like my whole knowledge of SQL is wrong or something. What is the actual MAX value for the query? edit: I threw some test data in a table and joined on a datetime field using MAX and it's working fine... edit2: Also used the same query as a CTE and then joined it to the same table with no problems edit3: Tried with the same query in a subquery joined to the same table on the datetime field... still working... Are you *sure* there isn't some kind of different data type or conversion going on for this field as part of your "main" query?
 It works with a static datetime and pukes with the same error if statically set to null. When I noticed that I tried a coalesce to replace it with a static date if null. That doesn't work either, though I don't understand why. In my mind it should lol. It also still pukes at the same place if I comment out all of the on statement that references that column, so problem isn't in there.
Unless I read it wrong he is trying to update the column value as existing value +5. His existing syntax wont work here. as with all other problems, consult stackoverflow :) https://stackoverflow.com/questions/2009927/aggregate-function-in-an-sql-update-query
Never mind. I solved it. SSMS was pointing me at the wrong line. That whole section works fine. :( 
I would recommend going with what /u/Rehd said then, and looking at your meta for all your JOINs. You can pull this from INFORMATION_SCHEMA.TABLES.
I have two sayings I like to say, especially at work. "Sometimes you lose, sometimes you lose some more." And my second favorite. "Nobody ever listens to Rehd."
Adding 5 to an existing value isn't an aggregation. His syntax is off... But he could do A = A*. 05 (to add 5 percent)
&gt; I guess I should put my efforts into SSRS I guess this goes back to what /u/Rehd said about the DBA role varying at every company, but I work with SSRS on a daily basis and never imagined it was something a DBA would do. Initial configuration of SSRS? Sure. But actual query writing / report design? That seems more like a job that would fall under the Analyst or Developer title.
Thanks for the link. I've had a busy night creating and altering tables, then inserting &amp; truncating different sets of data into my tables. I couldn't get SSIS to work on my laptop, so i'll look into making a package tomorrow at work, i'll also give our DBA a call and ask him about backups. Thank you once again for the tips.
The obvious error I see is the extra comma after film.title. But also you should use the JOIN syntax instead of comma-style joins. They are easier to read and newer syntax was introduced in 1992. select count(rental.rental_id), film.title, &lt;---- extra comma causing syntax error from rental, film, inventory &lt;----- don't use old comma style joins they are 25 years outdated, use JOIN where rental.inventory_id = inventory.inventory_id and inventory.film_id = film.film_id group by film.title
The error is coming from the extra comma after the film.title field in your select line.
thank you! The online course I'm in hasn't introduced JOINs yet but really good to know. I'll take what they're teaching at face value if they're 2 decades behind. the comma fixed my problem. :) Thank you so much for helping!
I dunno, I just looked at the details for that column in SSMS and thats what it showed :) The query DID work though. It was SSMS being funky with which line it told me wasn't working. Only reason I can think of is it made one of those little collapsible break points at line 16 that it likes to randomly make based on some logic I can't figure out. The actual error was much further down (long complicated query with lots of string formatting) where the person I had been working on the query for had made a change I didn't notice and was comparing two columns with different data types. Good ol collaboration. It took me rolling back to a previous version and tossing it in there to notice it worked over there, then compare the two to figure out the difference.
&gt; I automated inserting new transactions into the db Out of curiosity, how did you do this? 
Take a lookat this awesome Oracle feature: PIVOT and UNPIVOT. First, you'll need to modify your query to list total hours grouped by project and month. Then, you can UNPIVOT based on your month column, enumerating all values you have... This approach will transpose to a matrix just as you described... Good luck!
Which online course (just curious)? 
How many records are you dealing with in each of these tables, and how many records would likely be archived/deleted per year? My first thought is to consider disabling logging (only during execution of this job) and potentially to split the work into batches so that you are inserting and deleting fewer records at one time. I'm sure that some experts will have more thorough suggestions if you can provide a few more environmental/situational details.
It's at most 60000 rows added to the audit log per day, so at an estimate maybe 13 million rows a year would need to get moved? I could cut that into 6 month blocks I guess, but can't do too much more than that for business/bureaucratic reasons. 
&gt; As such, I want to run the following query every year or so (via a SQL Job) &gt; Because of the sheer volume of data, this query takes quite a while. Is the runtime of the query really a big problem in this case? If you only run it once a year, maybe this could be done overnight or on a weekend. This may be a case of premature optimization. I'd recommend to measure first and optimize later.
Moving 13M records in one shot is going to hammer the transaction log and probably lock the source table due to lock escalation. If you're auditing activity in real-time and can't write to the audit table because the `delete` has the table locked, your users aren't going to be able to work.
The "best" (for certain criteria) way to move large amounts of data in SQL Server is to use [partition switching](https://technet.microsoft.com/en-us/library/ms191160\(v=sql.105\).aspx). But that doesn't work across databases, so you'd have to swap your partition out to another table in the same database, then move the data over to the archive database. The root of your issue is that you're trying to move it all in one transaction. So, [break the operation into smaller chunks](https://sqlperformance.com/2013/03/io-subsystem/chunk-deletes) - say, 50000 records at a time. Move 50K records, commit. Move another 50K, commit. Repeat until you have no more records to move.
&gt; I tried using Attendance.Login IS NULL as well and it only shows me the absences. Sorry, I'm probably misunderstanding you, but isn't this what you're looking for? 
If you: FROM Employee LEFT OUTER JOIN Attendance Does it work?
Welcome aboard!! Yep, the tools you need are the ones you mentioned. For charts/graphs/dashboards, SSRS will be needed. You can also look into Microsoft PowerBI. You'll also want to look into SSIS to import/move your data around (even from flat files/excel/csv). Http://Docs.microsoft.com/sql should help you out quite a bit. Have fun!
I want it to show everyone who is currently a student, whether they are present or absent. The result I got when I tried that was current students, but only if they were absent.
I get the same results. 
Along with this - i would recommend clustered index 
Try out www.sqlbolt.com
You have to use the left join like stated here but any reference to the right table in the where clause has to be put part of the join. For example: Select * from table a Left join table b on a.col1=b.col1 Where a.col2='x' And b.col2='y' Is the same as using inner join. So what you have to do instead is: Select * from table a Left join table b on a.col1=b.col1 and b.col2='y' Where a.col2='x' Hopefully this example helps. If not, I can show you another.
Can you give us a few data pieces? Like an example of one that is and one that is not working? Obfuscate the data if you need to, of course.
A clustered index on what? I don't disagree that almost all tables should have a CI, but selecting the *right* CI is critical. I've got lots of tables in my databases which have poor choices for CIs. I mean, they work OK, but if another field was the CI, I could improve performance on a number of queries (like changing a frequently-run query from a full table scan reading about 2000 pages to an index seek that reads 4 pages). If you're partitioning, you have to take that into account when selecting your CI (and vice versa). Not knowing the details of how these tables are laid out, I'd suggest a CI that uses, and probably even starts with `flddate`so that you don't get crazy page splits when inserting. Unless `fldAuditId` is an `identity` column, in which case that may also be applicable.
What about a rolling window where you archive the 60.000 records of -365 days ago each day?
My only worry is if one job fails, I'll have a block of 60k rows that don't get archived. Not a bad idea though
AuditId is an identity column on the source database, but not the destination. adding date to an index might be a good idea though. Will certainly look at breaking up the operation into smaller chunks though.
Well then you could use this code: if it fails one day, it'll get done the next. WHERE datecolumn &lt;= DATEADD(dd, GETDATE(), -365) 
I think I got it. I added in a second column in the join and am seeing all of the students whether they logged in or not. Thank you.
So firstly I made a .bat file to run all these steps through the task manager: I used selenium to navigate my banks website and download all of my transactions for each account. Then I have some plsql that changes the text of a .ctl file according to the newly downloaded files' names. Then I use the new .ctl files by calling sql loader. This loads all the data into staging tables. Then I merge the standing table into the master tables for each account. This makes sure only the new bank transactions are inserted. And then finally the .bat truncates the staging tables. And now there is the bit at the end that categorizes them too. I'm not sure it's the most elegant solution, and I think I can use UTL_FILE to avoid the sql loader, and maybe then just schedule things through Enterprise Manager rather than stick everything in a .bat, but that's what I have so far. 
&gt; I work as an junior analyst for a small consulting firm. &gt; I'm currently using MS SQL Server and SQL Server Management Studio. Does your consulting firm have a licensing agreement with Microsoft/3rd Party for SQL Server? I would make sure, that before spinning anything up for commercial use, that pricing is looked at and understood. SQL Server Express (free) may be a good option for this database depending on how large you think it may grow. Paying per core on a 500,000 row database for SQL Server seems overkill and it may be best to use something like Postgres or MySQL instead. &gt; Are there any good materials for learning SQL that are targeted for users who are already proficient at Excel? [CodeSchool](https://www.codeschool.com/courses/try-sql) has a beginner-friendly SQL/Databases introduction, albeit brief. 
The subselect in the delete looks like something that could take a long time. Look at this: delete from [DB1].[dbo].[tblAudit] where fldAuditID in ( SELECT fldAuditID FROM [DB2_AUDIT].[dbo].[tblAudit] ) Think about what that will do. It will either do a query for **every row** in the audit table, resulting in millions of lookup queries. Ouch! Or, even if it runs the subselect just one time and does some kind of anti-join to find the records to be deleted, that is one expensive data set operation, considering the millions of rows in the audit table and the even bigger archive table with audit record history from all time. No thanks. Too slow! Instead of doing that, get the max ID and put it in a variable beforehand. Doing so will do two things: (1) It will make your delete run much much faster because it does not have to do a lookup against another table, and (2) it ensures a consistent data set inserted and deleted even if users continue to add records to the table while this operation is running. select max(fldAuditID) into var_max_fldAuditID from [DB1].[dbo].[tblAudit] insert into DB2_AUDIT.dbo.tblAudit select [fldAuditID] ,[fldUserID] ,[fldForename] ,[fldSurname] ,[fldData] ,[fldDate] ,[fldAction] ,[fldContext] ,[fldUserHostAddress] FROM [DB1].[dbo].[tblAudit] where tblAudit &lt;= var_max_fldAuditID delete from [DB1].[dbo].[tblAudit] where fldAuditID &lt;= var_max_fldAuditID I recommend you do this job (the insert and the delete) in one transaction. I completely disagree with the poster who claims that the "root of your issue is that you're trying to move it all in one transaction". That is not the case here, and it rarely is the root cause of performance problems. I do however agree with the other poster here who suggested archiving more frequently (perhaps daily on a rolling 365 day window). That would give you a constant one year audit history that is always there and never too big. You can use my method above to accomplish that. Just modify the first query that selects the max ID: select max(fldAuditID) into var_max_fldAuditID from [DB1].[dbo].[tblAudit] where fldDate &lt;= DATEADD(dd, GETDATE(), -365) This way lets you use the fldAuditID that is already indexed and also gives you control over the rolling window of audit history that you keep. Choose to keep 365 days or 90 days, or whatever is desired.
As an aside, you want to get a good grasp of how to do this. It is part of new financial regulations coming down the line. CECL regs require a lot of stuff like this.
500k rows is small for a SQL database so you could probably get away with using sql express or localdb. If you don't network your database, you probably can get away with using the developer version of sql server as well since it is not in production use then. Other stuff: * SSAS - Avoid, you are better of using R or Python integration into SQL Server * SSRS - Avoid, excel is better; however, if you want to do GIS integration or dashboards, you could look into this. 
This is awesome. You haven't had any trouble with the bank website trying to prevent scraping? I guess you don't have to hit the site very often. I'm definitely going to try this, thanks.
Is this ongoing or a one off data import? If it's ongoing and you have the infrastructure maybe you could send the client a link to your DB and ask their DBA to write a stored proc that wrote data to your tables on a periodic basis. 
What's your `WHERE` clause look like for these operations? That's what you need to focus on with creating the index. Don't just create what turns out to be an inappropriate index just for the sake of creating an index.
Not really. I'm going in only a couple times a week and downloading a file (of my transactions) they provide, so nothing fancy.
Having worked as a consultant in the accounting software field for 5 years. I can tell you one thing. It's a *pain in the ass* to make Excel work with SQL. Problems you will face: * Accountants love accounting type columns. They translate to nothing in SQL and all the characters will mess up your calculations. * Numbers/General will constantly be a problem. Missing leading zeroes? Check your column types. * ODBC drivers can be your best friend and worst enemy. The Excel driver can be a huge pile of doo doo though. It will analyze the first 20 rows of your spreadsheet and make a determination of column data from there. What if they are all numbers and suddenly there is text a hundred rows down? Welp, it marked your column as numeric and now it can't read the text. It's like the people designing the drivers don't actually want it to have any usability. You will get used to changing data types, which leads me to... * Prepare to spend time learning the SQL Import Wizard and how to handle anomalies in the data by updating your Excel sheets before importing to SQL. This actually is a great skill to have though. Having said all of that. The problems you will face are mostly relating to Excel and data types. SQL can handle your calculated fields, vlookup (this is what SQL is great at), SQL can PIVOT, and SSRS can make charts if you learn that tool as well. Many of the Excel functions translate exactly to SQL language too so you already have a leg up there. Good luck!
This is were it's at in my version, but it`s pretty much standard accross all of them: Tools -&gt; options Scroll down to SQL Server Object Explorer - &gt; scripting On the right pane, in the "Object Scripting options", check for Object Existence; change false to true
You might be looking for /r/SQLServer/ /r/SQL is about SQL which is a language for working with relational data
I omitted the where clause since I figured that didn't really matter in the overall picture; All the data needed to be moved eventually. Here's the SQL as it stands before I made this post declare @temp table ( fldAuditID int ,fldUserID int ,fldForename nvarchar(50) ,fldSurname nvarchar (50) ,fldData xml ,fldDate datetime ,fldAction varchar (50) ,fldContext varchar(50) ,fldUserHostAddress varchar(200) ); insert into @temp select [fldAuditID] ,[fldUserID] ,[fldForename] ,[fldSurname] ,[fldData] ,[fldDate] ,[fldAction] ,[fldContext] ,[fldUserHostAddress] from [DB1].[dbo].tblAudit where fldDate &lt; DATEADD(YYYY, -1, getdate()) insert into DB2_AUDIT.dbo.tblAudit select [fldAuditID] ,[fldUserID] ,[fldForename] ,[fldSurname] ,[fldData] ,[fldDate] ,[fldAction] ,[fldContext] ,[fldUserHostAddress] FROM @temp delete from [DB1].[dbo].[tblAudit] where fldAuditID in ( SELECT fldAuditID FROM @temp )
Is Work_Date actually formatted as a DATE field? I've seen databases where date fields are actually formatted for NUMBER, and instead of a NULL they pass '0' or spaces... really bad practice, can cause lots of problems, but it happens. To try and account for spaces, you could do: where trim(Work_Date) is null
If we have to deal with people spamming their blog posts here all day long we can deal with an SSMS configuration question.
To be fair, I don't mind the MS SQL Server questions all that much. But I do think some people are legitimately confused about this sub. By pointing out the other sub, I'm hoping they might find additional help. Not trying to say they are unwelcome here! 
I wouldn't say they are confused at all. I do agree that cross posting to the SQL Server would help get additional responses though.
&gt; ODBC drivers can be your best friend and worst enemy. The Excel driver can be a huge pile of doo doo though. It will analyze the first 20 rows of your spreadsheet and make a determination of column data from there. What if they are all numbers and suddenly there is text a hundred rows down? Welp, it marked your column as numeric and now it can't read the text. It's like the people designing the drivers don't actually want it to have any usability. You will get used to changing data types, which leads me to... That's not a problem with the ODBC driver, or the import wizard, etc. - you should scrub your data before trying to import it into your database, otherwise you risk polluting your database and lessens the data integrity. 
Yes to batching so you don't kill the transaction log, yes to clustered index so you can walk through the table while batching. I assume that *fldAuditID* is monotonically increasing INT value. If that is the case suggest the cluster on fldAuditID. What I don't see here is making the DELETE and INSERT an single transaction so you don't end up with duplicates or missing records. With an assumption of a cluster on *fldAuditID* and that the desire is to clear out the entire table during the job (could be adapted to only do a portion but that's going to be a bit more complicated) my suggestion would be something like DECLARE @batch INT = foo; WHILE EXISTS (SELECT * FROM [DB1].[dbo].[tblAudit]) BEGIN WITH t1 AS (SELECT TOP (@batch) * FROM [DB1].[dbo].[tblAudit] ORDER By fldAuditID) DELETE FROM t1 OUTPUT DELETED.* INTO [DB2_AUDIT].[dbo].[tblAudit] END
&gt; ODBC drivers can be your best friend and worst enemy. The Excel driver can be a huge pile of doo doo though. It will analyze the first 20 rows of your spreadsheet and make a determination of column data from there. What if they are all numbers and suddenly there is text a hundred rows down? Welp, it marked your column as numeric and now it can't read the text. It's like the people designing the drivers don't actually want it to have any usability. You will get used to changing data types, which leads me to... Yup, not just data type but data **length** too. I still remember wasting half a day trying to figure out WTF was wrong the first time I ran into that issue, then having to edit the registry to increase the number of rows it scans to determine data type/length. That was the last time I used that method. If OP wants to learn some coding too (not a bad idea), the EPPlus .NET package [makes it pretty easy to move data from Excel to SQL](https://www.mikesdotnetting.com/article/297/the-best-way-to-import-data-from-excel-to-sql-server-via-asp-net).
This is 100% a problem with the ODBC driver. What if you have optional values in a field, or a comments field, or a mixed alpha/numeric field? Let's say you have a comment or note column that doesn't get used often. We'll say they put an alternate phone number for a contact person in the notes. Note 1: 2128675309 Then a few more lines later we have: Note 100: Doesn't want called, email Jenny in accounting jenny@tommytutone.com The ODBC driver scans a few rows and finds only some numbers in the first note, so it determines the data type is numeric but it's a comment column with mixed characters and now the import is fucked because 100 lines later someone typed a string comment and the driver picked the wrong data type. What exactly would scrubbing do in this situation?
This is actually super helpful for me as I'm literally working on a project right along these lines in VB.NET. Thanks a ton!
The ODBC driver doesn't take into account the types that are set on columns in Excel? Or are you basing this on an import of an excel file allowing Excel to identify your columns for you.
Don't use a table variable for this. They have their uses, but they're quite limited. Use a temp table instead. But even then, you're moving too much stuff around. BUT! I just realized you can do this in a single query. delete from [DB1].dbo.tblAudit output deleted.[fldAuditID] ,deleted.[fldUserID] ,deleted.[fldForename] ,deleted.[fldSurname] ,deleted.[fldData] ,deleted.[fldDate] ,deleted.[fldAction] ,deleted.[fldContext] ,deleted.[fldUserHostAddress] into [DB2_Audit].[dbo].[tblAudit] where fldDate &lt; DATEADD(YYYY, -1, getdate()) Definitely get an index that uses that `fldDate` table hooked up, it'll make this faster. If you can make it a clustered index, that may be even better. The `OUTPUT` clause is kind of magical. In this usage, it lets you grab what was deleted and direct it elsewhere - in this case, you're taking a copy of what you just deleted and sending it into the `DB2_Audit` table. Wrap this up in the 50K-at-a-time batching loop/logic from the link I gave you above and you're good to go. (oh, and while we're at it, please don't use [Hungarian Notation](https://en.wikipedia.org/wiki/Hungarian_notation) for your tables &amp; fields)
All else equal if you are deleting from 1 set with a smaller subset - if they are both clustered on the same column sql will most likely use merge join and merge joins outperform most others
Guess I should have scrolled/reloaded before I posted [mine](https://www.reddit.com/r/SQL/comments/7014f8/optimising_an_archiving_query/dmzxqyv/) :)
Oops. Sorry.
http://www.informationqualitysolutions.com/FreeStuff/rettigNormalizationPoster.pdf
Deletes are extremely expensive, so I guess this is where the performance problem is coming from. The easiest way to deal with this is this: First, partition by date, both your source and your destination tables. (I assume that you absolutely want to deal with 2 databases, but thanks to partition swap you can make it happen in a snap-second inside a single DB.) Also, create a perfect copy of each, with a "_dummy" suffix. same fields, indexes, etc. -create a control table for the archiving process. Doesn't need to be complex, but you need the date and a processed field. the archiving process can now be written; -insert new dates in the control table, basically anything up to Getdate()-365 Loop through all dates not marked as processed, the loop consists of: Since truncate partition doesn't exist in TSQL: -truncate dest_dummy -partition swap dest with dest_dummy -truncate dest_dummy -copy src to dst -begin transaction -truncate src_dummy -partition swap src with src_dummy -truncate src_dummy -mark date processed in control table -end transaction Loop I usually truncate _dummy before and after swap in case of some process aborted in an earlier execution. I also minimized the size of the transaction, in order to hasten rollbacks (rest is repeatable anyways.)
Are you using PowerQuery and/or PowerPivot? If not, look into it before making the jump to SQL Server. Or maybe consider Access, as an easier, more "light-weight" alternative. 
&gt;Advance Excel user SQL gonna be real easy for you to learn once you get your bearings. &gt;The dataset (time series) is getting progressively larger (~500,000 rows with 50ish columns) and I create a lot of pivot tables from it. You can do this right in SQL. &gt;I was wondering if I could import this large excel file to a SQL database and work on from there. Yep, just install some flavor of SQL (I use SSMS) and create a new database then import your file. &gt;Creating Calculated Fields from existing fields (e.g. time differences) Easy to code right into SQL, and much easier to modify/maintain overtime in a code base then in multiple Excel workbooks. &gt;Create mapping tables and "Vlookup" to create new fields A VLookup is a JOIN in SQL. &gt;Create Charts/ tables for consumption e.g. bar charts, Line charts, Statistic Process Controls and be able to put these onto powerpoint Yep, you will write SQL queries that will prepare the data in the exact format you want it in order to visualize in something like Excel. In fact you can take a blank Excel workbook and point it to your SQL table and then once you re-import data, and rerun your query... you just hit refresh and all your graphs will auto-update. You can even link these to PowerPoint and then immediately be finished. You could also look into using SSRS to automate this a step further. &gt;I'm currently using MS SQL Server and SQL Server Management Studio. For analytical type of work, are these tools I need to do the tasks I have listed above? Yep. 
Yeah I would do partition swapping all the time for things like this. I assumed that OP wanted a quick answer to his issue, which is why I simply tuned the delete, which should be sufficient. But in the long run it could be better to roll off entire partitions (by month or year probably depending on their needs). Swap it out of the current log table, swapping it into the archive. 
I'm not disagreeing with you but I had nothing but ease using Excel and SQL together once you get past your formatting issues. I think it just comes down to how compatible the datasource is. If it is highly compatible then you'll have little problems, if it isn't then you'r fucked. Reminds me of that guy a few weeks ago who had like 2000 columns and couldn't import his file because of some weird formats... and he would have had to manually go through and map. 
Haha glad I could help. EPPlus is pretty awesome, we use it for all our Excel-related projects. Only problem we ever had was excessive RAM usage, but I think they fixed that last year. Another one worth looking into is SpreadsheetLight.
Thanks. As it turned out, the data i was looking to report on wasn't even going to be available accurately in the DB. I was gonna have to export to excel and add fields manually. But this will be useful going forward..
In your situation I'd consider learning Micorsoft Access and doing your work there. 
Great idea. I'll look into that. 
Doing this in one transaction is not the issue. The problem is that delete statement with the subselect in it.
Powerpivot (Excel 2016) is the way to go. It's fast as hell and you don't need any transformations because Excel. We have data models with millions of rows and it runs like a breeze. [EDIT] The data models are connected to several Oracle databases, from where the data is retrieved by SQL.
Thanks for the useful information there. The genre and author parts seems very obvious once you said it. The ISBN I was going under the assumption that each ISBN would be unique but given what had been said below about the clustered indexes-I decided it would be best for an ISBN ID. The part I don't understand is that you mention addresses should be considered carefully. I have an Author and Publisher ID and so don't see what problem there would be with your example of Ladybird and Ladybird Kids (they would simply have 2 different ID's with the same address which, aside from some duplicate data, I don't see the problem with-but may be missing your point). The same with the Authors side of things. [This is my revised plan] (https://imgur.com/a/pIwKl)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/67Sj9y6.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dn0k1jv) 
Again, Thanks for the useful information. 
Very useful. Thanks!
Ladybird are a big company, part of Penguin Let's say they have a few departments that would come under "Publisher" (I've made a couple of these up) - Ladybird - Ladybird Kids - Ladybird Educational - Penguin - Penguin Classics One day, Ladybird/Penguin decide they don't like their office. They decide to move offices. Now you have to update 5 different rows. To do so, you're going to have to do some kind of search/replace on Publisher, (eg update every row that matches the address). That's easy to mess up. Or you can just update a single row. Even more of a nuisance: what if Penguin decide one day to sell Ladybird, but Penguin will stay at their head office? Now you have to do some kind of *manual* search and replace, or at least a convoluted one. Suddenly you have about 5x as many places it can break as even the above example. Or you can just add a new row: you'll still have to update the Ladybird entries manually, but at least you're just changing an ID now, not the details And last but certainly not least: what if Ladybird adds a new publisher, Ladybird Horror. Oh, and Ladybird Religion. And Penguin Kids, why not. But because they're entered at different times by different people, the addresses aren't all the same. One uses a different Zip code (they accidentally used the delivery entrance or something). Another is a typo, and a third just uses a different formatting. Now you have 4 different addresses for the same address. Repeat the above two examples in this scenario... Addresses are one where it's possible to get carried away, but I believe that splitting out addresses like this makes sense. And that's before we get into things like "What if an Author and Publisher share an address" and all the ways that could get messy A table should contain the information about one single "thing", and where that "thing" references another "thing" you should use a foreign key. In this case, an Author and an Address are not the same "thing" and belong in different tables
Ahh I see. Again, makes perfect sense now you say it. Thanks again
No worries - it's not always easy to see the possible awkward examples until you've been the one having to fix someone else's messy data
I was shocked to hear him say that, especially now that we have power query/m. sql server and excel play together like best friends. 
I hate PowerQuery. He's right though when it comes to big files, etc. It's a nightmare.
As I mentioned, I tried this, but it alters the behavior of the MODIFY for a stored procedure. This change is just as annoying. 
By the way, I as not offended at all by the correction. I want to post in the right place.
https://www.amazon.com/Data-Analysis-Using-SQL-Excel/dp/111902143X/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1505443462&amp;sr=1-2
This is a business intelligence/ OLAP application. You can look into something like Essbase (expensive) or MS SQL Server Analysis Services. You need a multidimensional database.
OP doesn't even need that. MS Access will be happy to gobble a million records. Even Power Query can do this.
No worries! You will find help here, too. Just trying to point out another helpful community.
I'm also an advanced Excel users (Macros, VBA, some Power Query, etc.) I'm actually just finishing up a SQL book as well (will finish this month if I stop being lazy.) You can handle a large amount of rows in Excel 2010 or later. You just need to use PowerPivot. PowerPivot is actually the analysis services for SQL Server (SSAS tabular.) If you're on 2010 you need to install it as an addon. In 2013 or later it comes installed in the program. I've read that PP has a 2 billion row limit from some sources, and other sources say it has no limit. In addition to that, it can handle all that data and load it up in the familiar PivotTable. I'm not trying to dissuade you from learning SQL. SQL is great, and there's many things you can do pretty simply in SQL that take complicated VBA or PowerPivot / PowerQuery to emulate. But SQL is better for things like data storage, data validation, and general querying (you're trying to get data from a table or table that matches certain criteria) while Excel is better for analysis. You can certainly do a lot of what you want in SQL, but you can also do it in Excel.
Been a DBA for 20 years and an Architect for 5. All I can say is THIS! So much THIS! Great advice. Be very careful with CTEs - they operate in parallel, where temp tables are serial, so if your CTEs are numerous and deep your performance will tank. Just redesigned a huge sproc that drove an ETL feed that had 11 CTEs (1 select statement) changed it to 11 temp tables with some indexes - run time went from avg 4500 secs to 3 minutes. And by the way, nice sarg solution!
Sounds like you're an ETL Developer with some emphasis on analysis 
Alright buddy I found it for you. *Tools&gt;Options&gt;SQL Server Object Explorer&gt;Scripting&gt;Object scripting options&gt;Check for Object Existence* and *Scripting&gt;General scripting options&gt;Script USE &lt;database&gt;* Also other options in there that may be relevant for you. 
Hi - I just wanted to say thank you again for your help, The interview was about an hour ago, and i'm afraid to say i didn't get the job. I did upskill enough however to get exactly the same score as my "competitior" for the roll on the test (he took it on experience). I was also told that had it been a permanent post I would have been more suitable, but as it was for six months they needed someone who could get going straight away, rather than someone who had a better "mind for logic", who would end up being more capable in the long run. None the less it's been a positive experience, and you'll now probably find me around here a little more, at first to ask stuff, and then hopefully to try and help people out when i get good enough. It's great that there are strangers out there willing to give their time to help the me's of this world out, so at the risk of becoming repetitive i'm going to say thank you one last time.
Good work going for it! I highly recommend to not stifle or stop your learning and don't feel discouraged. I worked at a call center on the floor for about a year, and kept trying to move up. I did eventually, to a small reporting position. It was a miserable job, both of them, but after a year and 70+ interviews, I landed a gig with SQL / administration / networking / telecom, etc. Oh boy was it trial by fire. Lost data, corrupted backups, application errors, DTS packages failing, and I had little to no experience at that point. Back to back 80 hour work weeks and tons of learning, 6 months later I had a good handle on things. Fast forward a few years later I became a sys admin and had interviewed for a DBA position. I felt like I had the job in the bag, but then a guy from outside the company was hired for the position. (Rightfully so, he was far more ready to hit the ground running and he was incredibly knowledgeable.) So I persisted and kept on learning. Not too much later I snag a DBA gig, and another, and now a data engineer job. Stay persistent, continue learning, and follow your technology passion. You'll get there. My best honest advice though, learn three new things every day. They can be little or big, just learn three new things. Weekends too. Just browse stackoverflow or reddit when ya poop, or read sql central / pinal dave / brent ozar, etc. At 7 years and learning 3 things a day at a minimum, that would be 7,665 things I have learned about SQL now, independent of working towards certs or on the job training. Also see if you have a SQL meetup group in your area and look for SQL Saturdays. I'm attending two SQL Saturdays this year and try to hit 6-9 meetups a year. Really great free training and you meet a lot of connections. 
If you can see and ping the server, does that mean you can do a the same thing on the other end too? If so, why not make a script that pushes the data on the WindowsNT box to the 2012 server?
SSMS uses SMO to connect to servers, but SQL Server 7 isn't supported by SMO. https://social.msdn.microsoft.com/Forums/sqlserver/en-us/5a028dc7-4711-4614-a858-6fac80c84f9e/use-ssms-with-sql-server-70?forum=sqlexpress
Take a look on here to know [how to Shrink SQL Server Transaction Log](http://www.sqlserverlogexplorer.com/how-to-clear-transaction-with-dbcc-shrinkfile/)
When speaking about SQL, as in writing SQL, I tend to differentiate it by stating T-SQL(transact sql) as opposed to SQL. My reason is that if I say I am well versed in SQL, it could be confused with DBA responsibilities. I know plenty of excellent DBAs(database administrators) who are great at what they do, but writing T-SQL is not their forte. They know it, but running a SQL server is more their skillset. In my career, it is the opposite. I am a much stronger SQL Developer and architect than I am running a SQL server to its peak efficiency. Again, I can manage one just fine but I would rate my T-SQL skills higher than my DBA skills. Which isn't a problem because I am a Senior BI Analyst.
Cheers bud. I'll just set up a old school DTS package on sql7 while I migrate to the new production app.
Can you do an SSIS package (Import/Export Wizard) from SSMS?
I can export it from the winNT box and make a DTS package which was ssis back in the day I'll just shcedule it for every hour after that.
You're doing it hourly? Make it an SSIS package, don't burden yourself with even more technical debt by putting the albatross of DTS around your neck.
Oh so ssis can deal with sql7? I'll knock up a package on Monday morning then. Cheers
MariaDB is basically MySQL right? Does this article help? https://blog.sqlauthority.com/2014/04/09/mysql-finding-first-day-and-last-day-of-a-month/
here's how to get the first day of the current month... SELECT CURRENT_DATE - INTERVAL DAY(CURRENT_DATE)-1 DAY then subtract 1 day to get the last day of the previous month also, subtract 1 month to get the first day of the previous month putting it all together... SELECT CURRENT_DATE - INTERVAL DAY(CURRENT_DATE)-1 DAY - INTERVAL 1 MONTH AS first_day_of_last_month , CURRENT_DATE - INTERVAL DAY(CURRENT_DATE) DAY AS last_day_of_last_month
I saw that article too but went a different way. What I have now is... SELECT LAST_DAY(now() - interval 1 month ) AS LDPM FROM deals.test_table; SELECT concat(date_format(LAST_DAY(now() - interval 1 month),'%Y-%m-'),'01') AS FDPM FROM deals.test_table; However, using DataGrip I'd like to have both of these execute in the same query so I worked this up SELECT LAST_DAY(now() - interval 1 month ) AS LDPM, concat(date_format(LAST_DAY(now() - interval 1 month),'%Y-%m-'),'01') AS FDPM FROM deals.test_table; Is there any better way to do this? The reason I'm still asking is because I have a larger query that I wanted to put this into without mussing things up too much.
&gt; Is there any better way to do this? yes... never convert dates to strings or vice versa use date arithmetic only see my reply
How would you tackle this if you were able to freely implement this? I've been thinking that breaking the number crunching out of SQL DB and into something like a rules engine could be a cleaner way to do it.
Three points (well, a point and 2 hints): * Not everyone has a first name. Maybe there's an employee ID you can count? * `Group By` is your friend when using aggregate functions * There's an `avg()` (average) function in SQL
cool, I will spend some time reserarching/trying these things and ill report back. 
I don't know for certain but it's worth trying.
 select departmentid, count(firstname) as tot_emp, (salary / tot_emps) as avg_sal, (yearsinservice/tot_emps) as avg_years from employees where departmentid is not null; yielded: DEPARTMENTID TOT_EMPS AVG_SALARY AVG_YEARS ------------ ---------- ---------- ---------- 1 3 71000 9.66666667 2 4 52500 6.75 3 3 63000 8 which is almost exactly what i need, i just have to get rid of the department 2 info, since the avg_years is not &gt;= 8. the only problem is im not sure how to do that since avg_years is an invalid identifier. one more hint please? thanks!
Why aren't you using `avg(salary) as avg_sal, avg(yearsinservice) as avg_years`? You still need a `group by`, and then to filter with aggregates you need to use `having`
oops, that was a copy/paste fail. im using select departmentID, count(employeeID) as tot_emps, avg(salary) as avg_salary, avg(yearsinservice) as avg_years from employees where departmentid is not null group by departmentid;
i think ive got it now! thanks 
I'll try it and let you know either way bud
You'd need to use dynamic SQL to do this. Generate your query in a string variable and execute it. Be aware, dynamic SQL can be very useful but also a huge pain to debug and a security hole. https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-executesql-transact-sql
You're absolutely correct. What you're referring to is called an associative entity. It's the bridge between two, or more, major entities that share a M:M relationship. So you colleague is somewhat correct in that a relationship database cannot support M:M relationships directly, but they can achieve the M:M relationship with an associative entity all while maintaining 3NF or higher.
Let me say first that there might be better ways to do what you are trying to do than using temp tables but without knowing details can't really say for sure or give alternatives. As for doing what you are trying to do that is not possible but what you can do is: Create the table if it does not exists, so you have both tables where one of them is without records. you are using LEFT JOIN so an empty table will not have any impact on the result For the columns you need on the select than you do something like `coalesce(##projects.neededcolumn, ##itemprojects.neededcolumn) as neededcolumn`
I have simillar thought on that. Data and stored computed values (if time is of essence) stored with effective date time stamps. Models stored with effective date time stamps in a model warehouse. If models are simple enough (ie just a set of coefficients or the like) these could be in same db. But most models used in finance sector are usually not quite that simple, so you might want to use a dedicated model warehouse to store those.
Seems to me that an analytic function like LAG could give you the time between events. Then just exclude those time differences that are outside the desired number of seconds.
&gt; How do I join to the temp table only if it exists, and join to a different table if it doesn't? Well if you've got materialized views with query rewrite, that's kind of how it works. If it's fresh, the MV is used, if not then the base table is used. You may or may not be using Oracle, but other databases have similar concepts. 
&gt; most RDBMSs like SQL SQL is a language, not a database. But it's true that no M:M relationships exist directly between two tables in 3NF. But it can be done indirectly by introducing a third table, or junction table.
If you need to disprove a general statement, then (very academically) a set of points {(x[i],y[i])} is a many-to-many relation and it is in 3NF 
I'm using SQL Server. Apparently indexed views are the equivalent of materialized views, but I can't figure out what the equivalent of query rewrite is.
As others have already said, your colleauge is wrong. Here is a simple counterexample: A relation schema with a many-to-many relationship that is not in 3NF: Orders( order_id, customer_id, product_id, customer_name, product_price ) Customers can order 0:n products and a product can be ordered by 0:m customers. The primary key of the schema is "order_id". The schema is not in 3NF because there are functional dependencies between non-key attributes and thus transitive depencies from the key to non-key attributes. The functional dependencies between non-key attributes are: customer_id -&gt; customer_name product_id -&gt; product_price To fulfill the 3NF the schema has to be divided as follows: Customer( customer_id, customer_name ) Product( product_id, product_price ) Orders( order_id, customer_id, product_id ) The relation schema "Orders" still contains the many-to-many relationship between customers and products but is in 3NF because there are no functional dependencies between non-key attributes.
Here is the full text of the error: =================================== Failed to retrieve data for this request. (Microsoft.SqlServer.Management.Sdk.Sfc) ------------------------------ For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&amp;LinkId=20476 ------------------------------ Program Location: at Microsoft.SqlServer.Management.Sdk.Sfc.Enumerator.Process(Object connectionInfo, Request request) at Microsoft.SqlServer.Management.Smo.ExecutionManager.GetEnumeratorData(Request req) at Microsoft.SqlServer.Management.Smo.SqlSmoObject.InitQueryUrns(Urn levelFilter, String[] queryFields, OrderBy[] orderByFields, String[] infrastructureFields, ScriptingPreferences sp, Urn initializeCollectionsFilter, DatabaseEngineEdition edition) at Microsoft.SqlServer.Management.Smo.DefaultDatabasePrefetch.PrefetchUsingIN(String idFilter, String initializeCollectionsFilter, String type, IEnumerable`1 prefetchingList) at Microsoft.SqlServer.Management.Smo.DefaultDatabasePrefetch.PrefetchBatch(String urnType, HashSet`1 urnBatch, Int32 currentBatchCount, Int32 totalBatchCount) at Microsoft.SqlServer.Management.Smo.DatabasePrefetchBase.&lt;PrefetchObjects&gt;d__1.MoveNext() at Microsoft.SqlServer.Management.Smo.SmoDependencyDiscoverer.SfcChildrenDiscovery(HashSet`1 discoveredUrns) at Microsoft.SqlServer.Management.Smo.SmoDependencyDiscoverer.Discover(IEnumerable`1 urns) at Microsoft.SqlServer.Management.Smo.ScriptMaker.Discover(IEnumerable`1 urns) at Microsoft.SqlServer.Management.Smo.ScriptMaker.DiscoverOrderScript(IEnumerable`1 urns) at Microsoft.SqlServer.Management.Smo.ScriptMaker.ScriptWorker(List`1 urns, ISmoScriptWriter writer) at Microsoft.SqlServer.Management.Smo.ScriptMaker.Script(Urn[] urns, ISmoScriptWriter writer) at Microsoft.SqlServer.Management.Smo.ScriptMaker.Script(Urn[] urns) at Microsoft.SqlServer.Management.UI.VSIntegration.ObjectExplorer.ScriptGenerator.ScriptCreate(SqlTextWriter sqlwriter, Server server, Urn[] urns, ScriptingOptions options) at Microsoft.SqlServer.Management.UI.VSIntegration.ObjectExplorer.ScriptNodeActionContext.Script(SqlTextWriter writer) at Microsoft.SqlServer.Management.UI.VSIntegration.ObjectExplorer.SqlScriptMenu.OnScriptItemClick(Object sender, EventArgs e) =================================== An exception occurred while executing a Transact-SQL statement or batch. (Microsoft.SqlServer.ConnectionInfo) ------------------------------ Program Location: at Microsoft.SqlServer.Management.Common.ServerConnection.GetExecuteReader(SqlCommand command) at Microsoft.SqlServer.Management.Common.ServerConnection.ExecuteReader(String sqlCommand, SqlCommand&amp; command) at Microsoft.SqlServer.Management.Smo.ExecuteSql.GetDataReader(String query, SqlCommand&amp; command) at Microsoft.SqlServer.Management.Smo.DataProvider.SetConnectionAndQuery(ExecuteSql execSql, String query) at Microsoft.SqlServer.Management.Smo.ExecuteSql.GetDataProvider(StringCollection query, Object con, StatementBuilder sb, RetriveMode rm) at Microsoft.SqlServer.Management.Smo.SqlObjectBase.FillData(ResultType resultType, StringCollection sql, Object connectionInfo, StatementBuilder sb) at Microsoft.SqlServer.Management.Smo.SqlObjectBase.FillDataWithUseFailure(SqlEnumResult sqlresult, ResultType resultType) at Microsoft.SqlServer.Management.Smo.SqlObjectBase.BuildResult(EnumResult result) at Microsoft.SqlServer.Management.Smo.SqlObjectBase.GetData(EnumResult erParent) at Microsoft.SqlServer.Management.Sdk.Sfc.Environment.GetData() at Microsoft.SqlServer.Management.Sdk.Sfc.Environment.GetData(Request req, Object ci) at Microsoft.SqlServer.Management.Sdk.Sfc.Enumerator.GetData(Object connectionInfo, Request request) at Microsoft.SqlServer.Management.Sdk.Sfc.Enumerator.Process(Object connectionInfo, Request request) =================================== The column prefix 'cstr' does not match with a table name or alias name used in the query. (.Net SqlClient Data Provider) ------------------------------ For help, click: http://go.microsoft.com/fwlink?ProdName=Microsoft%20SQL%20Server&amp;ProdVer=08.00.0760&amp;EvtSrc=MSSQLServer&amp;EvtID=107&amp;LinkId=20476 ------------------------------ Server Name: TRIGGERFISH Error Number: 107 Severity: 16 State: 2 Line Number: 1 
This is normally something you would do in the front end/presentation layer. Not in SQL. Are you sure the application you are using has no way to filter out the columns that have no value? That is definitely the way to go. Otherwise, you will most likely need to use Dynamic SQL. Unless someone has a crazy trick that I am not aware of. 
Why not just click "New Query" and type "CREATE VIEW ..."? I mean, that doesn't solve your error, but you don't need to use the scripting menu to do anything in SQL at all. As for your error itself, was this query created in an older version of SQL? Might be that the version it was created it was more lax with the syntax rules. edit: Oh yep, that's definitely it. Saw your error in the other post: ProdVer=08.00.0760 That's SQL Server 2000. I would guess that since you are running a database that's 17 years older than the IDE that there are some compatibility issues. Maybe your best bet if you *really* need to use Script As is to try to go into the SSMS Options and have it suppress some of the error messages? I don't have my SSMS 2017 in front of me (I'm at home) but I *know* there are some options in there for suppressing errors. Or maybe install an older version of SSMS. And get to upgrading that SQL Server soon. It's long unsupported and you will continue to run into problems like this.
Presumably your current process creates and populates the global temp table at the same time but only the population of that table is slow. I'd split the creation and population of that table into separate steps, so that the table is always created but it's only populated if you need it to be. That way you'll be trying to join to an empty table rather than a nonexistent one.
I agree. Unfortunately I either live with slivers or figure out a query. Unpivot may be the trick...
I can think of a way, I believe. But it's definitely a hack... * Use dynamic SQL * WHILE loop for each column in the table * IF...ELSE in the loop, something like: `IF (SELECT SUM(@columnname) FROM TABLE) &gt; 0 @columname = @columname ELSE @columname = ''` * Then build your SQL string by concatenating the columns in the WHILE loop. Columns with sum of 0 will get added as empty string instead of the column name. * EXEC your statement Like I said, it's a hack but it should work. 
you've proven that two different ways of writing an inner join, which are semantically equivalent, also run the same okay, now do some research on how you write outer joins across platforms if not using explicit syntax dog's breakfast, innit so that's why you should use explicit join syntax, and it only follows that you might as well use it for inner joins too
&gt; M:M is two 1:M relationships with a linking table vwalah
Despite all the upvotes, M:N relations do exist and could be easily represented in relational DBs. In fact, the much referenced 'linking table' is that very same representation. Also in fact, any relation (a,b) that has entire (a,b) as its key it is going to be an "M:N" relation in the 3NF. "a" and "b" can themselves be tuples too - &lt;a1, a2, ...&gt; and &lt;b1,b2,...&gt;. The original relation would still be M:N (between what now are multi-dimensional points/coordinates a and b) and would still be in 3NF _IF_ the each set of coordinates ({a1,a2,...}, {&lt;b1,b2,...&gt;}) has ALL of their relevant fields in the key. This relation will NOT be in 3NF if one or both coordinates (or sets of attributes) in "a" and/or "b" will have a key smaller than the whole set - this clearly creates a transitive dependency and breaks 3NF. What (I guess) confuses folks is that it's a common practice to add a synthetic key to any "complex" data/entity (i.e from (a1,a2, ...) you would add (**a_id**, a1, a2, ...) to all your relevant entities/attribute sets). This causes the breaking of the 3NF (the situation described above) for the 'straight' M:N relation. So in these common circumstances the usual way to get M:N relation without breaking the 3NF is to represent them via tables that have their keys consisting of the combination of other synthetic keys, which basically moves the situation into the first example I went through. **tl;dr:** 'linking tables' are M:N relations and they are in 3NF. 
Try /r/SQLServer
Unpivot your results so every column total comes as it's own row with a relevant label. Exclude rows if the value of the metric is 0. Use remaining rows to split slices in your pie/whatever chart.
followup question to this: is there any RDBMS that represent m:m directly?
 Could you use "having sum a total &gt; 0" as well as for b and c? 
&gt; vwalah Eh?
SQL is a declarative language, so a programmer declares to the DBMS what result he wants to receive. But he also declares it to himself and other people who may work on a project. Each query part: SELECT, JOIN, WHERE, etc. serves it's own purpose: - SELECT declares what fields returns the query - JOIN declares what tables are used by query - WHERE filters result set And that's why implicit join is bad - it mixes JOIN and WHERE parts of query into one, making query much less intuitive to understand. 
Yes, I mentioned that. â€œSeparating the join-predicate and the filter-predicate by using the explicit notation makes the query easier to read and debug.â€
Not sure what you mean. 
If you want to filter by an aggregate function use "having" instead of where. Select a From b Having Sum(a) &gt; 0
As I mentioned, I tried this, but it alters the behavior of the MODIFY for a stored procedure. This change is just as annoying. Changing Script USE to FALSE just removes the USE, it does not change the behavior of the object name.
thank you... as a canadian, i really appreciate it 
Hint: from A left outer to B
But why? If B doesn't exist, then I should get an empty result set.
Read it wrong, but I still do not ever do the old way of from A, B with join in the WHERE clause, that is Old waY
&gt; What am I doing wrong? mixing comma join syntax with explicit join syntax explicit has precedence, similar to the way ANDs have precedence over ORs, and multiplication/division have precedence over addition/subtraction solution: go all explicit SELECT * FROM a INNER JOIN b ON b.field3 = a.field3 LEFT OUTER JOIN c ON c.field1 = a.field1 AND c.field2 = b.field2 
Thanks, I'll give it a try! I didn't know I couldn't mix the styles like that. I really appreciate your insight.
*I want to have a record even if C is not there for B* Remember that your join is on C--&gt;B and C--&gt;A. If both ON conditions are not met, your join won't return records.if you want either to be met, use OR instead of AND,
Sorry, I misspoke. I want to have a record even if C is not there for B and A.
Ahh gotcha. Then you're good to go! /u/r3pr0B8 is absolutely right btw: comma joins are not the way to go. ANSI joins will make your code much more readable, and will cause much less confusion.
Agreed. 
Agreed. While it doesn't show much with such a low number of tables being part of the query, but anything above 5 tables makes an immense difference on the readability scale
First, I echo the use of explicit joins instead of comma notation. I typically would turn the A, B inner join into a derived table, then outer join to the derived table. To me that makes my intentions much more clear. Example looks a little funny with and A,B,C tables and 1,2,3 fields, but with better defined names it makes a lot more sense to me. It also allows me to better manage my base set independently of the rest of the query which is very handy for large queries. SELECT * FROM ( SELECT A.field1, B.field2, A.field3 FROM A INNER JOIN B ON B.field3 = A.field3 ) baseSet LEFT OUTER JOIN C on c.field1 = baseSet.field1 AND c.field2 = baseSet.field2 ;
Go into a field because you're passionate about it, not because you think you're going to make a bunch of money from it. That said, there's nothing about what you said that says you can't learn SQL. Check the course catalog at your school for database classes - those as well as self-study will help you. 
I can't quite say i'm incredibly passionate about coding but I do get really excited while talking about it and discussing different avenues it might bring me. I would have to actually get into it to know if it's for me or not but I think that's the same for everyone. 
I think it is perfectly reasonable to pursue a career because of money and job opportunities. But the reason why people talk about "pursuing your passion" is because it is assumed you will become good at it. So my two cents. Become good at this field. And learn the fundamentals. Understand data, how to analyze it, how to query it, how to catch data errors, how to load it and store it. Perhaps start with Excel. Load some public datasets in Excel, learn how to manipulate it. Then move to database management systems, RDBMSes such as Postgres, entity relationships, normalization and normal forms. Join data tables, pivot them, do unions and intersects. How to load large amounts of data into these systems. How to do QC of data. Then "no sql" databases. And perhaps at this stage, get into the whole "big data" world and complex stuff like map/reduce, hadoop, distributed computing etc. This is more programming territory and there is a lot more to learn, from how RAM works, programming data structures especially graphs, trees, stack, heap. How Java works. How Python works. You can certainly choose this career for money, jobs, etc. But you absolutely need to become fairly good at it. Even average is not going to cut it. For lack of a better word, you need to become a total geek - maybe not about everything but at least about one aspect of this field. 
Nice way to do it. Thanks!
&gt;I do get really excited while talking about it and discussing different avenues it might bring me. This still kind of reads like you're just interested for the money, honestly. The reason people (including myself) are advising you to think about this a little bit more is because programming is an incredibly complex and demanding field, and if you're not really into it you're going to get chewed up and spit out by the job and the employers. 
Anyway, my advice is here as well. Learn a DBMS, and set up a database for yourself. Find a dataset, load it in, and start messing around with simple queries, joins, and accessing your data. From there, read about normalization and database design. How to design a database and why databases are structured the way they are is just as important as being able to access the data inside them. Try incorporating that data into projects. Build a simple CRUD (Create, Read, Update, Delete) application with your language of choice and practice accessing data from your database using a language/SQL rather than a DBMS/SQL. Once you're done, build a more complex one. Rinse and repeat until you're comfortable. 
It's well worth thinking about the direction you're looking to move your career in. There's a whole bunch of quite diverse roles that use SQL as a hugely important part, but very few that require SQL and nothing else. Data science, analysis, report writing, data architect, web development, app development, business intelligence, DB administrator and more all require SQL, but are all very different roles for very different people. Just learning SQL won't get you into a &gt;100k job, or even a side job to do while studying - you really need to develop a better idea of what you're looking to do, and why you're looking to do it.
I code to make money. 20 years now
I would elaborate on this. Follow your passion no matter what career it's in but learn to code. Any monkey can code. This is why companies hire South Asians for pennies on the dollar to write their code. What these people can't provide is insight. You like marketing? Fine. What are the pain points of the marketing industry and use coding skills to close that gap. Same with accounting. Same with Personal Training. etc. etc. etc. Everything is an opportunity if you have the right tools and are able to look at it outside the box.
The vast majority of SQL most people learn is pretty simple. Most people don't do more complicated things than selecting, updating, deleting, doing joins, etc. Pretty basic stuff. FWIW, outside of a few conceptually tricky SQL concepts (window functions, correlated subqueries, etc.) I found SQL to be much easier and straightforward than, say, VBA in Excel. It's a useful language that's surprisingly versatile. And I've come to appreciate it more than I did initially when I started learning it. I think taking a SQL class or reading a SQL book would definitely be a good investment of your time.
&gt; Any monkey can code. This is why companies hire South Asians for pennies on the dollar I agree with the rest of your points, regarding domain knowledge and passion. I'm not sure how you meant this point, but you should be aware that the way you wrote this can come off as horrifically racist. (Edit: punctuation typo)
oh I'm fully aware people might take that the wrong way but I stick by the wording. It's in the same vein as 1000 monkeys given typewriters could write the best novel in the world. Anyone can learn how to code. That's the overarching comment. And if you, a person in North America or Europe, think you can compete with someone in India or Vietnam (as examples) you're sorely mistaken. So then the question becomes, what sets you apart. What sets you apart is being able to understand that valuable context and where things fit. That's your value. Coding is merely an (optional) tool to help you realize that value more. edit - and for the record. [Code Monkey](https://en.wikipedia.org/wiki/Code_monkey) is a fairly common term. Or at least common enough it's on wikipedia.
Spot on all the way through. I would even change your first sentence to 'The vast majority of SQL most people *use* is pretty simple.' I don't do as much coding as I once did - but it's still 15-20 hours of my week. When I write queries, it's 95% basic stuff. Sometimes I hit something that requires something beyond a simple SELECT with a few joins. But mostly, it's basic stuff and the hardest bit is remembering which particular source I'm calling as everybody has a different way of implementing DATEADD (or DATE_ADD...sigh).
I found the Complete Reference to be useful for learning the technical underpinnings of the language https://www.amazon.com/SQL-Complete-Reference-James-Groff/dp/0071592555 If you just want to make a database in in mysql, then you could be looking at any one of a hundred equally good.
I bought this and it has just about everything youll need. https://www.amazon.com/SQL-Minutes-Sams-Teach-Yourself/dp/0672336073/ref=pd_lpo_sbs_14_img_0?_encoding=UTF8&amp;psc=1&amp;refRID=P8XWVVGP8XMS0MA10R0G 
I know getting into SQL won't get me into a 100k job at the snap of two fingers but I'm pretty interested in learning about it as it might be a useful skill one day. Maybe one day I could even expand on it if I really wanted to. I know I want to be a cop and I've made some choices to make me go into that field because I love what they do and it's always been a dream as well as a passion. Thank you for your input I greatly appreciate it. I've recently found out my community college has some coding classes that will expand on web dev, analysis and whatnot. 
Just curious, how did you go about making a relational database in Excel?
I 100% agree with this, but I've heard a lot of people become miserable over time, but what isn't better than being financially stable? Thank you for the advice! 
My thoughts exactly! I'm getting a book called SQL for dummies and my mom is downloading an express version of SQL to get me started. Thank you for the suggestions. Any books you would recommend? 
Certainly I wouldn't want to get chewed up and I don't doubt how complex it is. How has your experience in coding been so far? What was your biggest struggle? 
No doubt that I would be in it for the money if my skills in SQL or what have you would be good enough to pursue a job. 
How does this remove column a from the select list if it's = 0? I don't think you understand the requirements.
Ok edit: Just like to say that when I posted this the OP was half a sentence. Leaving it up though.
Can you be more specific on what you need help with? It sounds like you know what you need to do, not sure how we can help you any more than Google can. Here's some documentation on creating stored procedures and setting up SQL jobs: Stored Procedures: https://docs.microsoft.com/en-us/sql/relational-databases/stored-procedures/create-a-stored-procedure SQL Jobs: https://docs.microsoft.com/en-us/sql/ssms/agent/create-a-job You might also want to look into DB Mail if you want the job to send a specific email under certain circumstances: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-send-dbmail-transact-sql
I was hoping if someone can point me in the right direction since this does not seem to be working for me. For testing purposes, I've added If/else clause and excec sp_send_dbmail but I don't get an email where there is value more than 0, but I get notification via email if there is no result returns. Edit: I don't want to hardcode email to in the SP. I want to do select recipients from the daily job. Now my challenge is that can I even do it? Can I run SP in job, and let the job determine if there is result then send out an email if not then do nothing. 
It sounds like your intended point was that underdeveloped countries offer programming services at rates that are hard to compete with, without additional added value. It is ironic that in this thread, you are missing what you are proposing a western developer offers - valuable context and where things fit.
&gt; Can I run SP in job, and let the job determine if there is result then send out an email if not then do nothing. Not that I'm aware of, you'd have to code this, what I usually do is an if statement where count &gt; 0 then sp_send_dbmail else return end Can you post what you have so far so we can see why it's not working?
 Create PROCEDURE PendingTime AS BEGIN DECLARE @recordCount int SELECT @recordCount = ISNULL(COUNT(*), 0), employee_name, entry_empl_uno, COUNT(*), MIN(post_date), MAX(post_date), MIN(tran_date), MAX(tran_date) FROM TableA With (nolock) INNER JOIN TableB (nolock) ON entry_empl_uno = empl_uno WHERE entry_status = 'n' -- and post_date&gt;=dateadd(month,-2,getdate()) IF (@recordCount &gt; 0) BEGIN EXEC msdb.dbo.sp_send_dbmail @profile_name = 'DB Mail', @recipients = 'ABCD@XYZ.com', @query = 'SELECT @recordCount = ISNULL(COUNT(*), 0), employee_name, entry_empl_uno, COUNT(*), MIN(post_date), MAX(post_date), MIN(tran_date), MAX(tran_date) FROM TableA With (nolock) INNER JOIN TableB (nolock) ON entry_empl_uno = empl_uno WHERE entry_status = 'n' GROUP BY employee_name, ENTRY_EMPL_UNO', @subject = 'Test', @Body = 'Test', @attach_query_result_as_file = 1; END END; This does not send me email even if results are more then 0.
I believe you're not actually setting the @recordCount to the value there, try this: DECLARE @recordCount INT SET @recordCount = ( SELECT ISNULL(COUNT(*), 0), FROM TableA WITH (NOLOCK) INNER JOIN TableB (nolock) ON entry_empl_uno = empl_uno WHERE entry_status = 'n' -- and post_date&gt;=dateadd(month,-2,getdate()) )
the devil is in the details. If you have a fully baked set of requirements to do ABC you are using a developer in an underdeveloped country. What I'm saying is that if you're in a non-IT industry it doesn't matter if an external coder is from Bangladesh or Seattle if they're not in the industry you're in they do not know the context around what you're doing. Hence, if you're in marketing, learn to code. If you're in accounting, learn to code. If you're in real estate, learn to code. If you're in construction, learn to code. On and on and on... The opportunity is to build efficiency in industries which traditionally programmers do not know about or could care less about, but to build those that efficiency you need to know the industry. I'm hoping that makes sense as it seems I'm not coming across clear enough.
&gt;could care less *[You DO care?](https://www.youtube.com/watch?v=wDmIRY3Zveo)* ^^You ^^probably ^^meant ^^to ^^say ^^"Couldn't ^^care ^^less"
I really couldn't care. However this is one situation where I really could care less if I applied myself.
Ultimately, all this really boils down to the "pursuit of happiness", doesn't it? Or the pursuit of a fuller life. This is probably one of those super elusive answers that just about all of us are hunting for. But I can perhaps share my personal thoughts. I personally do **not** believe that (for most of us), there is any "overarching passion" or "mission" that will fill our lives with fulfilment and happiness if only if... we identify it and we submit our everything to it. It is interesting that most of us, especially when we are young, are routinely asked the question "what do you want to become when you grow up". Like, what does that question even mean? What do you mean by "become"? And how is it that at the age of 16 or 18 or 25 or 35, you magically know what you need to "become"? Instead, here is my unfashionable viewpoint. I feel that life is all about the little things. It is all about the small joys, the small fulfilments, the small thrills, the small disappointments and frustrations. We feel good about ourselves when we are competent and capable in a particular subject area, and when we are able to execute tasks that utilize our skills and competence. Then we find a sense of satisfaction at "getting it right" and "doing a good job". As such, it matters very little what we are doing and how it fits in with the over-arching grand vision. Which is why I personally claim that it is perfectly okay to pursue this field because of money, job opportunities etc. But the key thing is to become really good at it. If you become really good at this, you **will** have a satisfying and fulfilling and rewarding career. Not just for the money reason but for many other reasons - this field is all about expertise and in most software companies, their key assets are their people. So if you work for a good company, you will be treated right. Your skills and contributions will be acknowledged and appreciated (or if companies do not do this, you will quit and join another company). 
Again, I couldn't agree more with you and I've never understood why people question me about chasing a job for money. I certainly agree that life is about the small moments, frustrations, and disappointments. From those moments we can learn our mistakes and take action to make sure we don't repeat them again and have a better experience. It's very nice to see someone put something in wording of what I think life itself is about. Hope this isn't weird, but I'm going to screenshot this to remind myself what life is about. Thank you for the advice and suggestions. I hope life is treating you well. 
Thanks for the kind words! And all the very best!
 @query = 'SELECT employee_name, entry_empl_uno, COUNT(*), MIN(post_date), MAX(post_date), MIN(tran_date), MAX(tran_date) FROM TableA With (nolock) INNER JOIN TableB (nolock) ON entry_empl_uno = empl_uno WHERE entry_status = 'n' GROUP BY employee_name, ENTRY_EMPL_UNO', Having issue with following, the SP is failing because of quotes and it does not take [] either. &gt;WHERE entry_status = 'n'
You need to escape the quote within a quote by using 2 's @query = 'SELECT employee_name, entry_empl_uno, COUNT(*), MIN(post_date), MAX(post_date), MIN(tran_date), MAX(tran_date) FROM TableA With (nolock) INNER JOIN TableB (nolock) ON entry_empl_uno = empl_uno WHERE entry_status = ''n'' GROUP BY employee_name, ENTRY_EMPL_UNO',
I created another SP with just a query, and put @Query = 'Exec NEW_SP' now its giving incorrect syntax errors. I will try with double quotes. 
Thanks, /u/Cal1gula! &gt; Why not just click "New Query" and type "CREATE VIEW ..."? I mean, that doesn't solve your error, but you don't need to use the scripting menu to do anything in SQL at all. Excellent, I think that's probably the winning answer. I'm very new to the SQL game, and I guess I hadn't realized (or didn't remember) that you could create a view like that. My apologies for being kind of dumb. I think that will probably solve the problem of creating the view and preserving the code formatting. Although, without the ability to use the "script view as... ALTER" functionality, I don't know if anyone would be able to look at the formatted code anyway! &gt;As for your error itself, was this query created in an older version of SQL? Might be that the version it was created it was more lax with the syntax rules. edit: Oh yep, that's definitely it. Saw your error in the other post: ProdVer=08.00.0760 That's SQL Server 2000. I would guess that since you are running a database that's 17 years older than the IDE that there are some compatibility issues. Yeah, I hear you. It's certainly not an ideal situation. But, I'm OK with it just starting as a complete blank in terms of the query, so it's not like I'm trying to edit an old query. But yes, I do think this is where the problem lies in general. &gt;Maybe your best bet if you really need to use Script As is to try to go into the SSMS Options and have it suppress some of the error messages? I don't have my SSMS 2017 in front of me (I'm at home) but I know there are some options in there for suppressing errors. Or maybe install an older version of SSMS. Awesome! I did not know that that was possible. I'll see if I can do something like that. Thank you for the suggestion. &gt; And get to upgrading that SQL Server soon. It's long unsupported and you will continue to run into problems like this. Yes, absolutely. I would if I could, but for number of reasons I don't know how possible that is (one of the main ones being that our parent company is trying to upgrade everyone to a new ERP system). Thanks again!
Thanks, finally I bought *The Definitive Guide to MySQL 5 Third Edition* because that's what I found in the library, and I preferred having the book now rather than waiting for the delivery.
Thanks, finally I bought *The Definitive Guide to MySQL 5 Third Edition* because that's what I found in the library, and I preferred having the book now rather than waiting for the delivery.
Dude, try DESCRIBE over that Stu_Activity table. You need to specify a value on your INSERT statement for every NOT NULL column. Probably the "status" column is one of them. Remember that when you run a INSERT statement, you add a new row to your table, and every NOT NULL column must have a value. The problem here is not the sequence at all. You could try the following expecting the same error: SQL&gt; INSERT INTO Stu_Activity (StuNo) VALUES(123);
1 sheet = 1 table You use functions like VLOOKUP, INDEX &amp; MATCH, or AVERAGEIFS (where the number intended to be averaged is actually one) to search for values in other sheets. The way I implemented it used up a lot of memory so I had to paste values very often.
quick question back atcha -- what would happen to the correctness of your WHERE conditions if you ~didn't~ apply the CONVERT function and used just the raw column `t.APPR_DT`?
One step ahead of you homie, testing that now and adding an index to the table. I was curious more about the performance difference between &gt;= AND &lt; vs BETWEEN based on various things I've read here.
In that case it would only work for accuracy if I added a +1 to the end date, no? APPR_DT is datetime, not date.
there is no difference in performance, but a huge, important difference in semantics BETWEEN syntax is automatically interpreted as two `&lt;=` comparisons, whereas, as you seem to already know, a far more robust approach is using strictly less than but not equal for the upper bound so where you want all rows for February, you ~could~ try this -- WHERE foo BETWEEN '2017-02-01' AND '2017-02-28 23:59' but that's highly problematic because you have to add the correct precision to the time portion for whatever platform you're on, **plus** figure out the last day of the month far more robust is this -- WHERE foo &gt;= '2017-02-01' AND foo &lt; '2017-03-01' which ~always~ works for **both** dates and datetimes 
You can actually work with multiple tables in Excel. Excel 2010 and later has support for PowerPivot, which is an OLAP database. Since it's a database, it has certain things that an RDBMS has, like support for joins. And PowerPivot also has equivalent functions for many SQL table operators like CROSSJOIN, UNION, etc. This may be suitable for your purposes. But it would make a poor database, and I don't think it would be an RDMBS. I don't know if PowerPivot supports data validation for primary keys, for example. That's a super basic thing that most relational databases have. And if PP doesn't support it, you can only get real validation in Excel through complicated VBA. But even if you could do that, it would be hard to maintain. And the amount of people capable of maintaining it would be much smaller than the amount that would be able to maintain a SQL database. So it's probably a good idea to invest some time learning SQL.
You can use old way to specify inner join using where clause SELECT * FROM ( SELECT A.field1, B.field2, A.field3 FROM A,B where B.field3 = A.field3 ) t1 LEFT OUTER JOIN C on c.field1 = t1.field1 AND c.field2 = t1.field2 ;
2
How? 1 row for user_id 142 and 1 for 143 right?
technically, it goes like this -- run the subquery (which produces two rows), append an imaginary column to each row containing the non-null value "1", and then count the 1's so, yeah, 2 rows
But how do you know if 142 and 143 are unique user ID's?
Because you specified them in the query? You're not selecting some unknown number of rows. You made a row with 1 column user_id, and 1 row of data = 142. Then you made another row with 1 column user_id and 1 row of data = 143. Then you combined them through a union all. Now you have 2 rows. Then you take the count of the 2 rows. Your answer is 2. There's no ambiguity because you specified every part of the data set in your query. SELECT 142 AS USER_ID USER_ID| :--| 142| SELECT 143 AS USER_ID USER_ID| :--| 143| (SELECT 142 AS USER_ID UNION ALL SELECT 143 AS USER_ID) X USER_ID| :--| 142| 143| SELECT COUNT(1) as ct FROM X ct| :--| 2| 
geez louise, who downvotes this? i'm trying to help the guy
&gt; But how do you know if 142 and 143 are unique user ID's? you don't... they are **numeric constants**
I have a feeling there is some other query at play here and OP is testing by replacing the values with literals and now the counts aren't matching and he's confused as to why.
What a great article. Your actions are described very clearly and concise. I do sense you left out some rough spots you must have encountered on the way. With regard to the performance results, I'm left wondering what happens when compression is off. You will lose the columnar benefit, but win on decompressing. Thanks! 
Your right in that I left out a lot of dead ends I went down getting this to work. Decompressed the dataset is 6x larger and the I/O really does take it's time. I'm not sure if the time could be made up releasing the CPU from decompression tasks. I'm considering putting together a flame graph of Spark's processes to see how much time is spent decompressing versus other tasks. If I can find any serious optimizations that'd make for a good follow-up post.
Creating a subset, right? I like that. I ended up expliciting all my joins as suggested by many. But thanks for the insight!
Good to know. Anyways I have Excel 2007 so I lack some functions.
I did have the "status" column set to be NOT NULL. It made sense why it was throwing the error after you pointed that out. I did wind up figuring it out. Thank you very much!
do it
Thanks for the feedback! What do you think of the structure for the subject? I wanted to help people avoid some of the pitfalls I went through.
in my opinion, which i shall freely admit is biased, the most important chapter is how to write good SQL (your chapter 7) all the other chapters should be as brief as possible
Excellent thanks for your opinion! What do you think should be touched upon when writing good SQL? I guess a better question is what is your definition of it?
Bingo. This guy SQLs.
I learned by reading Itzik Ben-Gan's [T-SQL Fundamentals](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X/ref=sr_1_1?ie=UTF8&amp;qid=1505782294&amp;sr=8-1&amp;keywords=tsql+fundamentals) From what I read, he's considered the world's foremost authority on the T-SQL language (Microsoft's implementation of SQL) and also does a very good job teaching ANSI (standard) SQL. I'm actually finishing up this book now. I have around 30 pages or so left.
SQL Azure is the cloud-based version of SQL Server. SQL Server Management Studio is the user interface for managing the server. You will need to get SQL Server and then connect to it using SQL Server Management Studio.
Interesting idea. I'll have to give that a shot next time I run into this sort of issue and compare the performance!
I'd read it. I'm still a noob.
Hey thank you for your input! Since you're still new to it, what types of roadblocks are you hitting? Anything you're struggling with in particular?
I would start with the software that needs to be downloaded. Add pictures too
Hey thank you! No problem. I have this covered already. It's specific to Microsoft's SQL Server. It shows you where to get the download package, installation steps and every other step in order to log in. Some people just don't know, so I wanted to be straightforward with it and cut out confusion.
If the book is tailored to one specific product, make this super obvious. Also in the title. Personally, I think every topic that is specific to one product should be labeled again as being specific. Too often, beginners think they know SQL just because the used one dialect, often heavily using specifics, without knowing they are specifics. To be clear: there is nothing against writing a tailored book. Just be fair to the readers and tell them. One example for SQL Server that comes in my mind is the way to quote identifiers: [] instead of the standard "".
Keep us in the loop.
I second that emotion 
Take a look at [table partitioning](https://www.mssqltips.com/sqlservertip/1796/creating-a-table-with-horizontal-partitioning-in-sql-server/), you could partition by color and then use partition locks.
I'm the kind of person who likes knowing why I'm doing something and like knowing the bigger picture. I can learn SQL but how does it relate to real life application or data base theory. Would you happen to have any great recommendations?
If you need to move all that data from one table to another then there isn't much you can do to optimize it. See if you can reduce the data that you write, such as inserting only the Id and Value, and then joining back to the waTimesheet table if you need other columns.
You should probably rethink your use of global temp table. Whatever it is that you're trying to solve by using it, it's almost certainly the wrong approach.
This sounds like a great idea! If possible, I think you should include a section that carefully explains the order in which queries are actually executed/read. Not just in the context of joins, but for simple aliases as well.
Read about document, object, key-value DBMS. They usually start with why SQL and relational DBMS are bad.
[removed]
Great points here. I can definitely convey that it's for SQL Sever specifically. Though I know that some of the syntax can be used in other database types, like MySQL and Oracle, but fairly different. I have seen some confusion here when quoting identifiers before. Pointing this out can help streamline learning and avoiding some pitfalls. Great idea!
Thanks! I plan on releasing this for free for about the first week. So I'll post that here with a link so you can grab a copy!
I will for sure!
This query doesn't create any records records at all. I believe you will need to replace that CONNECT BY subquery with a self join or a recursive CTE but my Postgres nor Oracle is the best so I will defer to someone with more expertise. Something like this: https://stackoverflow.com/questions/22626394/does-postgresql-have-a-pseudo-column-like-level-in-oracle
Got it. I don't have any recommendations at the moment. I'll do a bit of research and perhaps incorporate some of that into the book. I'll get back to you though with a resource or two!
Jeez, in order to know disadvantanges you should now the topic very well. How could one ask for it in a introductiory lecture. 
Hey thank you! This area is discussed a little in how the database engine executes statements, but isn't broken down into detail regarding the FROM, WHERE, SELECT etc. Great point! 
&gt;The programmers who use SQL doesnâ€™t have a full control over the database because of the hidden business rules. It's probably referring to rules for data that are coded into/enforced in application code, as opposed to using things like constraints, foreign keys, etc. right in the database. I usually see it in systems where the person who created the database doesn't have a good grasp on database fundamentals and codes everything into their application, using the DBMS as a "dumb" data store. The database *should* be designed in such a way as to enforce referential integrity, instead of depending upon the application to do it. One of the primary reasons being that by enforcing it at the database itself, you can code *other* applications (or queries directly to the database) without having to worry about re-implementing various things. It also makes the database better-documented and easier to discover relationships; with proper foreign key relationships established, one can point a database diagramming utility at the database and it'll be able to automatically generate an ERD with a good degree of accuracy.
Yeah we had only one lecture of 45mins on the subject. Seems like a weird way to teach..
Hate to be a KillJoii here but... There are a lot of words that you are using incorrectly which makes me believe that you are going to have a real rough time fixing whatever the problem is. Databases aren't stored in a DLL. That's a library of code which your application is calling. ODBC isn't a server, it's a protocol and associated drivers. They are stored in the registry and the ODBC administrator utility. Looks like he's using Visual Studio to write and compile the application. The application itself is in the executable file (Contract Management System) that's a few thousand characters long. And without the decompiled application and project/solution to start with, you won't be able to do anything. It could be in that .vs folder. If your application is using a MySQL ODBC driver then I am guessing your data is stored elsewhere and that's why you need the ODBC driver in the first place. For the application to send data/commands to the database. This is like the stereotypical "it can't be that difficult to learn how to query databases, write a software program, compile build and distribute it! I can do it in an afternoon!" Nah bro, it took me like 14 years to get that kind of knowledge. Just wait until the brother is back from vacation. And if you don't wait, make a backup of everything before you touch it. Preferably 3.
SQL is one of the oldest programming languages around. There aren't too many disadvantages--hence why it has stayed for so long. Disadvantages compared to what? A noSQL solution? Not using a database at all? Seems like a vague question with no concrete answer, which is probably why you are struggling to find information. A few years ago there was a "craze" where lots of people thought MongoDB and other noSQL software would take over the world. Then people realized that usually data integrity is a good thing. Kind of like when people were throwing the "Cloud is the future!" comments around until everyone realized the cloud is just someone else's server.
Thank you for the suggestion, but that seems to be a bit more static of an option then I am going to need. If I am reading this correctly, table partitioning is limited to 1,000 partitions and can not be created dynamically. I would need over 2,500 partitions and the ability to add more on the fly.
I can definitively tell you that in the process I am currently testing that truncating and inserting into is quite a bit slower than dropping and selecting into. By an order of 2.
They're completely different concepts. Triggers fire when an action happens on the database/server/table to do "something." Stored procedures are pre-compiled bits of code that can be re-used from a variety of places - including triggers. https://stackoverflow.com/a/18694905/1324345
SQL Server 2017 supports up to 15,000 partitions by default. In versions earlier than SQL Server 2012, the number of partitions was limited to 1,000. You should be able to add colors to the partitioning function via a trigger and some dynamic SQL.
We are in 2008 R2 unfortunately.
Those are called "aliases", you don't need to use them all of the time, but they are exceedingly helpful. When you say you want AuthorID from two tables, which table do you want AuthorID from? You can use the actual table name, like Books.AuthorID or Authors.AuthorID. But it's also convenient to use an alias in the form of an identifying or meaningful abbreviation. It can be anything, but it can helpful to use something you can remember that makes sense. Sometimes you may want to join a table onto itself, so it can help keep your mind straight using good identifying aliases. An example is an employee table. You could join on itself so you could find someone's boss or create a hierarchy view. 
Thank you! I just want to make sure I'm understanding this explanation. Given the example: &gt;select a.AuthorId, a.AuthorName, b.BookId, b.BookTitle &gt;from Authors a &gt;join Books b on a.AuthorId = b.AuthorId The line "from Authors a" is me assigning the alias of 'a' to the 'Authors' table. The interpreter then knows that the two column names I listed in line 1 for a.AuthorId and a.AuthorName are calling back to the Authors table. The same thing happens at "join Books b on". Is it fair to say there is an implied "AS" between statements like that? So that it would really be like saying "join Books AS b on" or "from Authors AS a"?
Correct. I would say best practice is to use AS between to indicate you are aliasing, but it's not necessary. I always try to maintain ANSI standards when performance or style are not at risk so it can carry to any RDBM. By doing the alias and then column name, as you said, it is pulling that column from the aliased table. 
Check out Teamcity. I honestly can't add more to that though, I haven't gotten into it but I know it can be done. 
good explanation of 1 how joins work, without using those fucking Venn diagrams 2 how grouping builds new tables (i.e. you're not seeing any actual table rows after grouping) 3 correlated subqueries and derived tables
Thank you so much, this is finally making sense! And those aliases can be anything, as long as I have an AS or implied AS statement afterward that defines what table the alias relates to? For example, I could do &gt;select pirate.AuthorId, pirate.AuthorName, ninja.BookId, ninja.BookId &gt;from Authors AS pirate &gt;join Books AS ninja on pirate.AuthorId = ninja.AuthorId That's essentially the same thing?
This seems like a good point to start, as the example is close enough to what you're trying to do: https://technet.microsoft.com/en-us/library/ms186243(v=sql.105).aspx 
Recursive logic has two parts, CTEs of this variety are no exception; it can be thought of as a BFS if that helps: * The base case: This is the start of the recursion. This will be something that can be determined to either be at the top of bottom of your recursion tree. *Thinking about it from the top is generally easier in my experience.* This would be employees who are very high in the company and don't directly report to someone else (example: supervisorId IS NULL) * The recursion step: This is were the logic self references. You join to itself to expand on the base case. Example, SupervisorID = EmployeeID As an SQL example: **** ;WITH cte AS ( -- Base case SELECT e.employeeId, e.supervisorId, e.username AS reportStructure FROM employees AS e WHERE e.supervisorId IS NULL -- Recursion Step, union results together UNION ALL SELECT se.employeeId, se.supervisorId, c.username + '-' + username FROM cte AS c INNER JOIN employees AS se ON c.employeeId = se.supervisorId ) SELECT employeeId, supervisorId, reportStructure FROM cte **** 
Lay it all on me, don't hold back. I tried to make it as clear as possible that I may be using these words wrong, the general idea is that I want to learn how to do this stuff, and you've provided some very clear info, thanks. Your 14 years compared to my dabble in a private Maplestory server using Mysql (years ago) plus a semester of C++ programming using Visual Studios, is like comparing a Marine to someone who plays COD for a living and says they can be a Marine in no time with that experience xD Sorry if it came across like that. Your reply has helped with understanding a couple of new things. I never really looked into what a dll was, but just because I saw it called MySQL, figured it had to do with the database. The ODBC had a database option and it was calling to a separate MySQL file, is it safe to say that that file is where all the information is stored when people input things into the executable? And no need to worry, I copied the contents of the folder into my own computer. However since the program isn't just in that folder, I don't have everything on my own PC, which is fine since I just wanted to understand the basics of it. So what would you say I should look into creating something like this? It won't happen in a day, and that's fine, I'm genuinely curious in this stuff. Any video tutorial recommendations, any book recommendations, anything. I have like a month till he gets back, and would like to have an even more clear understanding of these things in the meantime. Thanks in advance :D
I was debating on diagrams or real examples. I figured a simple example using data to convey how joins work would suffice. But I'm glad that you don't like Venn diagrams haha. Makes me think that it doesn't help most people. Do you think points 2 and 3 are more on the advanced side for beginners?
"by George I think She's got it!"
Yup, that's the same thing. There are gotchas and common sense. If you want to do something stupid like alias butt-stuff!$f_n, you'd have to add brackets and crap. I recommend to keep it simple, descriptive, and short. You may have to type that alias a lot and if you have a long alias, then you may as well just type the table name in front of the column you reference. I typically just alias based on camel case. Example: Table ImportFromAmerica I'd alias as IFA for ease of typing and I know it by the acronym now. 
 SELECT * FROM teachers WHERE EXISTS ( SELECT 'ok' FROM science_assignments WHERE teacher_ID = teachers.ID ) OR ( SELECT 'ok' FROM reading_assignments WHERE teacher_ID = teachers.ID )
Check out Jenkins and Octopus. Script it with Powershell.
&gt; Do you think points 2 and 3 are more on the advanced side for beginners? no i don't i've seen too many cumbersome shitty applications built around a lack of understanding of those points
Definitely. I'll working on breaking this down into some simplistic examples. By the way, when you're talking about grouping tables, you're referring to grouping the result set when joining tables, correct?