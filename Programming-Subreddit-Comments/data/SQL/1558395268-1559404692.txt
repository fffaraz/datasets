Maybe they want more of a database / SQL developer role. I started out as a data warehouse developer because of my programming experience. That's definitely an area where the procedural languages are key.
Personally....i like to get in my car, drive down the road to the local bodega, smoke a bowl or three. Drive back, grab a coffee from dunkin, smoke a cig while the high hits and think about the coding. My creative juices are flowing by that point and i come up with an even easier way to code to the end result. Then im like wtf, why did i waste 4 hours when i could i have done this in 1hr and feel useless coder...i get over it quickly though becuase im an accountant who can code in sql, pythhon and c
The skills are very transferable between dialects. Once you are good with one dialect or procedural language its easy to get good with others. But learn PostgreSQL and its PL/pgSQL language. - PG is definitely the fastest growing database - It's the most ANSI compliment - Because you already have some familiarity with PL/SQL and PL/pgSQL is very similar because it was modelled after PL/SQL (but it's much better IMO). - Lastly, because it is just the best damn database around.
So I've tried the where condition. My problem is it seems to ignore the max(updated_at) and returns all the fields who's status is 'waiting for vendor'.
Not proud of this, but once or twice I've taken a power nap in the restroom because I just could not keep my eyes open.
Yes, I‚Äôve found certain habits to help with that slump though; 1. Stay hydrated all day - office buildings pump AC, which dries out the air and dehydrates you. If you don‚Äôt have a water bottle, get one and try to finish it by lunch each day. 2. Don‚Äôt eat a big lunch. I usually eat a good breakfast but for lunch, I either pack in or hit up the salad bar. I find that heavy lunches drain my productivity for the rest of the day. 3. Stay hydrated. Thought this should be said since it‚Äôs the single thing that‚Äôs helped my productivity most. 4. If you‚Äôre going to drink excessive amounts of coffee in the morning, at least be consistent. If you need to drink a bunch of coffee just to wake up, that‚Äôs fine, but you‚Äôll crash hard about 3 hours after you stopped. 5. Break are good if your company lets you. Other than that, it‚Äôs called an afternoon slump for a reason. I think it‚Äôs somewhat part of working 8 hour days, but the above steps have works for me.
Get up, walk around, eat a snack I always thought that‚Äôd be a good time for a nap. Unfortunately, most employers say that‚Äôs inappropriate
maybe don't?... i have noticed that i can get about 3-4hr of concentrated coding done per day.. after that even if i think that i am doing ok work i am actually not, i make mistakes and don't notice them... i usually switch to tasks that require less concentration or focus after that...
Standing desk
I'm not saying you're wrong, but every analyst position I've had/worked with has developed sprocs. More often than not the analyst is what drives the development of new sprocs.
I primarily write queries and I have adopted Azure Data Studio lately. I like the speed of the intellisense.
Great stuff!
Same brother!
Nap time. Recharges the brain.
Generally speaking in IT, take 10min walking breaks, where you can only be distracted by your thoughts. I often come back with what to do next.
Funnily enough, shit starts heating up right when it's getting near quitting time, and before you know it the kids are already in bed when you get home :-(
Postures also has a lot of nifty commands other languages do not. PGadmin 4 is also now browser based (awesome) so it's even lighter and easier to use than languages with an absurdly heavy duty GUI (stares at ms sql)
Do you want to write about the best open source C# / SQL program released in 2019? DataTier.Net creates all stored procedure data tiers written in C# and is better (to me) and easier to use than Entity Framework. &amp;#x200B; Maybe if a magazine writes about it, someone will actually try the program I have worked on for 15 years; it older than Linq to SQL, Entity Framework and NHibernate for .Net. &amp;#x200B; [https://github.com/DataJuggler/SharedRepo](https://github.com/DataJuggler/SharedRepo) &amp;#x200B; Here is a video: [https://www.youtube.com/watch?v=hnkroLjgKfk&amp;t=917s](https://www.youtube.com/watch?v=hnkroLjgKfk&amp;t=917s) &amp;#x200B; DataTier.Net creates the C# data objects, the readers and writers, the Gateway\* and the stored procedures to perform the full CRUD methods. &amp;#x200B; \* The Gateway is code generated, and in my opinion superior to a Data Context in Entity Framework because instead of this: &amp;#x200B; Entity Framework Person person = dbContext.Person.FirstOrDefault(x =&gt; x.Id == 123); &amp;#x200B; Gateway: Person person = gateway.FindPerson(123); &amp;#x200B; Underneath the hood Entity Framework will create the inline SQL to generate: Select \* From Person where Id = 123; &amp;#x200B; Where DataTier.Net will execute the Person\_Find stored procedure. &amp;#x200B; In building distributed systems. I prefer the Gateway because you can load an object with one instance of a Gateway and save with another instance. Entity Framework requires you to work with the same context which is not always ideal. &amp;#x200B; The downside is DataTier.Net does not keep track of changes like EF. I prefer this due to the EF executes too much SQL in a production environment (to me). &amp;#x200B; The difference is EF has been tested by tens of thousands internally at Microsoft and probably millions world wide, and DataTier.Net is just one bored programmer with too much time on his hands, so far. I keep waiting for some smart people that appreciate stored procedures to come around the corner, but most programmers drink the Microsoft Kool Aid and like having a job and capitalism and all that stuff. &amp;#x200B; It also comes with a sample project called DB Compare, which compares two instances of a SQL Server database such as a development database vs one located on a Test Server. [https://www.youtube.com/watch?v=13HipAOyAqU&amp;t=5s](https://www.youtube.com/watch?v=13HipAOyAqU&amp;t=5s) &amp;#x200B; New in version 2.0 is a remote compare feature, where you can export your current database schema to XML, and then perform a compare against a SQL database located on a virtual machine (also demonstrated in the video above). &amp;#x200B; Giving away lots of free programs is harder than it should be. I can't even get anyone to clone it, so I can't get someone to tell me they hate it like I am used to. &amp;#x200B; I will give your magazine 95% of all sales. I am not that good at math, but zero times 95% has to be cheaper than Paying Google Ads $20 to watch a 1,000 people skip a video. &amp;#x200B; Every C# programmer will like at least one program in the repo, or double your money back. &amp;#x200B; The Data Juggler Shared Repo code is lonely.
PGadmin is definitely a solid tool. And Yeah...I've never been a huge fan of SSMS, but I'm happy now that Microsoft has Azure Data Studio (for which MS recently added support for Postgres and Jupyter Notebooks). Pretty much a one stop shop üéâüëç It's wayyy better than the boat anchor that is SQL Server Management Studio..
If you can do 4 hours of real work a day you're doing fine. The rest of the day can be stackoverflow, thinking about how to solve problems, reading tutorials, etc.
Run SQL Server in a VM using Parallels.
DataGrip (or PyCharm with the DataGrip plugin). It's the best SQL IDE around and supports just about every SQL database you'd ever need to use (Postgres, MSSQL, Oracle, SQLite, Redshift, and several others). Azure Data Studio is on my radar though because it supports PostgreSQL and Jupyter Notebooks now (with support for Python, R, Spark, and SQL kernels) .
Data analyst positions? In as much as job titles have any meaning in our field, I'd consider a data analyst generally a near end-user of the data. A BI/reporting/data warehouse analyst I'd expect to develop procs, but a data analyst I'd expect to be able to clearly specify what they need but not necessarily be able to code one. Having said that, I have made sprocs in a role that was titled as a senior data analyst, but that the role was specifically misnamed so that IT couldn't claim that they should have the role in their team.
This is a video reply to a question asked in this forum, so please do not consider this spam; it is free and open source and it directly answers his question of how to query lists from Excel. \-- This is the description copied from the YouTube video I posted: Delimiter is a very simple Windows Forms (desktop) application that I show how to integrate with SQL Server to run queries against lists from Excel. Delimiter is part of the Data Juggler Shared Repo, which is one very large repo (all free so forgive me) available here: [https://github.com/DataJuggler/Shared...](https://www.youtube.com/redirect?event=video_description&amp;v=eTA9X5zc2IA&amp;redir_token=umjnn6rLvAvafpoxNUK6_3gSqV98MTU1ODQ5NTM1NkAxNTU4NDA4OTU2&amp;q=https%3A%2F%2Fgithub.com%2FDataJuggler%2FSharedRepo) Delimiter is located in the Win folder: DataJuggler\\Win\\Delimiter\\Delimiter.sln To integrate with SQL Server: 1. Build the solution with Visual Studio (2019 is shown in the video), 2. In SQL Server Management Studio, click Tools - External Tools. 3. Enter the name Delimiter 4. Browse or paste in the path to the Delimiter.exe in the Command section. Hit Apply. Now anytime you need to launch Delimiter, it is available under the Tools menu.
adderall xr
I posted a video reply on the main thread of this sub Reddit to answer your question of how I do it, because it won't let me post a video here. The video is also on YouTube here if for some reason the video gets down voted: &amp;#x200B; [https://youtu.be/eTA9X5zc2IA](https://youtu.be/eTA9X5zc2IA)
I work 24 hours per week for this very reason. It is funny, I can be tired coding my (boring but it pays good) work code and barely stay awake. When I get off work and it is my code that I want to write, I can stay up all day and all night. It comes down to motivation and am I living my dream or someone else's dream.
A little bit more detail might help.
I listen to "work study" music on youtube when this happens. I usually start with this: https://www.youtube.com/watch?v=yydZbVoCbn0
I wish I was that lucky. Then again, on days I work at home I use my two 15 min breaks and lunch usually to take short power naps if I need.
Haha, I know this feel. I actually coordinate projects most of my time at work with bits of SQL coding spread around depending on my workload. A few weeks ago on a Saturday I had to sign off on a UAT, and I never work weekends. After work on Friday, I got a call saying files are ready and if we could sign off on all of them that night no one would have to work Saturday. So I log back in from home and put in an extra hour and sign off. I spent the rest of my night trying to complete a game in PS4 Dreams with a group on Discord. I had to get to bed early that night because I had plans early Saturday but realized we could publish the game that night. Guys in the Discord kept saying they wanted to just add one more thing, fix that up a bit, etc. After they were all done I checked out the game to run it through some tests and then published it once everything looked good. Realized right then and there I was doing the exact same fucking thing I do for work every day.
Are all the [number] fields different? If so, then you‚Äôre going to get a row back for every unique value in the [number] column since you‚Äôre grouping by it first.
I'm the opposite. I'm nearly worthless first thing in the morning. I try and schedule all my busy work on the mornings so I can be productive in the afternoon. Like this morning I spent like two hours doing my quarterly password update and sync (it really shouldn't take that long, but that's what I have to work with). Any deep thinking on my part has to be after lunch.
I believe truncate can be rolled back
I honestly always got my work in the morning done. Then afternoon meetings or research a problem.
So this database is of a ticketing system the [number] is the ticket number and a row gets generated each time the state changes from open to waiting for input or to close. For each ticket number there are several entries representing the various ticket states. I want to grab each ticket number or [number] if the most recent updated_at has 'waiting for vendor'
TRUNCATE TABLE cannot be run inside a transaction in MS SQL. Source: [https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-2017)
Normalization is not about minimizing redundancy. It's about data organization to fit certain needs. Application back end favors something close to the 3rd normal form. Data warehouses favor a star schema, Snowflake, flat files and 4th normal forms.
I use 4-2-2 as a way to break up my day. 4 hours normal work 2 hours work related project/research/learning 2 hours other things I used to feel burned out a lot before, but now i feel better without losing on productivity. I used to feel burned up a lot before, but n
Post your problem and your attempt so far.
Yes, but it is only in Azure SQL Data Warehouse and Parallel Data Warehouse as the source says. Additionally I ran simple query [sqlfiddle](http://sqlfiddle.com/#!18/ecf2e/1) and it rolls back truncate statement perfectly
There are more than 3 types of indexes. Also the wording makes it seem like a unique index is mutually exclusive with clustered and non-clustered indexes.
What a terrible article. It looks like someone took definitions out of dictionary. Skims or ignores important distinctions and few times is plain wrong. Might be good for an SQL novice pretending to know his stuff to land the first job.
Books online say that Truncate is more efficient in the transaction log storage. If it's in a transaction log, it can be rolled back.
There‚Äôs currently a big convergence effort to simplify how many different roles are required. Companies want people who can solve an entire vertical problem instead of requiring multiple handoffs
Thanks for highlighting the issue. Updated the text. Will soon update the infographics.
Yes. Then i'm browsing reddit like now
Validate your 70-462 Exam learning and preparation with our most updated 70-462 dumps. Dumpsadviser has experienced IT experts who gather and approve a huge range of Microsoft 70-462 Questions Answers for MCSA-SQL Server 2012-2014 Certification seekers. Practicing our 100% updated [70-462 Dumps Questions](https://www.dumpsadviser.com/70-462-dumps.html) is a guaranteed way towards your success in Microsoft 70-462 Exam. Get the huge discount.
We have a very similar set up at my work too. We use SQL Server on Macs with Docker so that won't be a problem. However, you can't use SSMS without Windows. Instead, look into Azure Data Studio. It is the equivalent of VS Code for SQL Server. It doesn't have all the bells and whistles of SSMS, but if you just need to write queries then you should be good to go. A year and a half on this configuration and I haven't really ran into anything that I _needed_ SSMS to do. Your mileage may vary, of course.
Don't eat lunch and hit the gym or go for a walk
A pet peeve of mine as well. Glad I'm not alone.
Thanks! I'll definitely look into this!
Depends on the environment. In Oracle, it can't be rolled back. In Postgres, it can be rolled back in a transaction.
[You can Try This. Don't know how good it is. I hate Tablet/Phone querying. But, more power to you](https://itunes.apple.com/us/app/mysql-querydb-client/id576948046?mt=8)
Always the same...
You are not. I always wonder how their "explanations" of (inner) join would differ at all from their explanations of set intersection (`INTERSECT`).
Yep u/sekretzivilist is right. Then just Add your '$' CONCAT to your Select. Since you said they care about that and you should be good. SELECT CONCAT(firstName, ' ', surname) AS "Employee Name", CONCAT('$',salary) AS 'Salary' FROM EMPLOYEE AS E WHERE E.salary &lt;= (SELECT CAST(AVG(salary) AS DECIMAL(9,2))FROM EMPLOYEE)
Anything for sql server that you know off. Even just the ability to run stores procedures on the go would be pretty cool.
[This guys says 'Lets SQL' does this.] (https://www.experts-exchange.com/questions/28245424/app-for-iphone-or-ipad-accessing-sql-server-database.html)
&gt; anyone out there good at SQL that could help with a hw prob? yes, me
‚ÄúNot in my country or region.‚Äù United States. Appreciate the attempt though!
shit, man "A Primary key constraint has automatic unique constraint defined on it. But not, in the case of Unique Key."
1. have a [calendar!](https://www.mssqltips.com/sqlservertip/4054/creating-a-date-dimension-or-calendar-table-in-sql-server/) table 2. you will only need month-level data from the calendar table (however you want to do it) 3. join 4. calculate overlap 5. profit
https://en.m.wikipedia.org/wiki/Database_normalization &gt; Database normalization is the process of structuring a relational database[clarification needed] in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model.
No it can‚Äôt, not in the same sense as a delete. Truncate can be rolled back if you do it inside of a transaction. However, delete operations can be rolled back even without a transaction. Delete operations are written to the log file, truncate is not
"What is SQL?" "What is a table?" This thing reads like a high school teacher's lecture on day 1 of a SQL 101 class.
Sounds about normal. I don't think there's any productive advice. That's about 5-6 hours of peak level coding for you or 25-30 hours a weak. That's pretty good. Take more breaks, but that's probably not going to give you 8 hours of coding flow. I think you're doing alright.
Thanks. I agree
maybe I'm overthinking it, but you might be able to use this answer to list all weekends, and add those to your "holidays" https://stackoverflow.com/questions/12489522/how-to-get-all-the-weekend-dates-of-the-current-year-in-sql
Agreed. Would enough of us be interested in a new thread, where we go through all 50 questions to provide newbs, like me, more realistic/real world answers, not out of a dictionary?
Utter tosh.
create table #example\_table ( RecordID int ,StartDate date ,NumDays int ) insert into #example\_table values (1,'1/15/2019',5) ,(2,'2/17/2019',67) ,(3,'3/4/2019',45) &amp;#x200B; select RECORDID, YEAR(StartDate)YEAR, MONTH(STARTDATE)MONTH, DAY(STARTDATE) NumDaysMonth from #example\_table &amp;#x200B; drop table #example\_table
&gt; if the most recent updated_at has 'waiting for vendor' AHA!!! this is demonstrably a different requirement than what you originally posted (**note**: you did not post any original requirement)
What RDBMS are you using? I would use a tally table to enumerate the dates and then count 15 back excluding the appropriate ones. I can give you an example if you let us know what SQL engine you are using.
TY much! Calendar table is the answer. We even have one of these in our environment, it just didn't even cross my mind to utilize it in this use case. Chalking this one up to first day back from a week long vacation and the brain is still rusty :)
here ya go -- SELECT t.number , t.state , t.updated_at FROM ( SELECT number , MAX(updated_at) AS latest FROM Samanage.dbo.Incident GROUP BY number ) AS m INNER JOIN Samanage.dbo.Incident AS t ON t.number = m.number AND t.updated_at = m.latest AND t.state = 'Awaiting Vendor Support' ORDER BY number
To anyone that down voted this or is thinking about it. &amp;#x200B; This is a video response to a question asked this week. I couldn't post a video in the reply, so I made a 4 minute video of a free open source project. &amp;#x200B; Some internet users treat open source promotion as if someone is selling flip phones or aluminum siding. You spend 15 years writing code and you give it away for free, doesn't provide anyway for to reach all the people that might like it. Down voters keep the people that might like it from ever seeing it.
Here's a working example using a tally table... WITH tally AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) ) SELECT RecordID ,DATEPART(yy,DATEADD(dd,n+1,StartDate)) AS [Year] ,DATEPART(mm,DATEADD(dd,n+1,StartDate)) AS [Month] ,COUNT(*) AS NumDaysMonth FROM #example_table et JOIN tally t ON et.NumDays &gt; t.n GROUP BY RecordID ,DATEPART(yy,DATEADD(dd,n+1,StartDate)) ,DATEPART(mm,DATEADD(dd,n+1,StartDate))
saying we using sql server how can we do it?
Yet the 4th normal form introduces redundancy access tables.
The goal is to REDUCE redundancy, not eliminate it There are times that redundancy improves performance at the cost of maintainability. This is a careful balance and must be properly evaluated depending on use case.
if only you had a table consisting of the author_id and the max published_date for each author then you could join it to the books table on both author_id and max date this additional table can actually be derived by a subquery in the FROM clause -- where such a subquery is often called a **derived table** is that enough of a hint?
I usually go put the seat back in my car
I never heard of tally tables before but I did some reading after seeing OP's comment and found [this article](https://www.sqlservercentral.com/articles/the-numbers-or-tally-table-what-it-is-and-how-it-replaces-a-loop-1), there's a date example at the bottom.
Yes. I literally started with ‚ÄúSQL for Dummies‚Äù... and worked my way up to an analytics position with a prominent financial services company. I have no degrees or certifications. You just have to set your mind to it, and find the right opportunity. You absolutely can, though.
Format your code better pls. It's hard to read.
You could create a function that takes in a date and puts out a date with code like this (though i would use physical tables for tally and holiday, not CTEs)... DECLARE @StartDate datetime2 = '2019-06-01'; WITH tally AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0)) b(n) ) , holidays AS ( SELECT [date] FROM (VALUES (CAST('2019-01-01' AS date)) ,('2019-01-21') ,('2019-02-18') ,('2019-05-27') ,('2019-07-04') ,('2019-09-02') ,('2019-10-14') ,('2019-11-05') ,('2019-11-11') ,('2019-11-28') ,('2019-12-25') ) a ([date]) ) , rn AS ( SELECT CAST(DATEADD(dd,-t.n,@StartDate) AS date) AS [date] ,ROW_NUMBER() OVER (ORDER BY n) AS row_no FROM tally t LEFT JOIN holidays h ON CAST(DATEADD(dd,-t.n,@StartDate) AS date) = h.[date] WHERE DATEPART(dw,DATEADD(dd,-t.n,@StartDate)) NOT IN (1,7) AND h.[date] IS NULL ) SELECT [date] FROM rn WHERE row_no = 15 This is example code that will run on SQL Server. Feel free to ask questions.
I agree with #2, for me it's "eat a small snack around 12:30, and then my actual lunch around 3:30." Puts me out of sync with other folks' lunch schedules, but works well for my blood sugar/circadian rhythms. If I eat a big lunch between 12-1 I definitely hit a wall around 3.
for reddit code formatting you can put 4 spaces before each line of code (with blank line before and after) &lt;Enter&gt; &lt;space&gt;&lt;space&gt;&lt;space&gt;&lt;space&gt;# comment &lt;space&gt;&lt;space&gt;&lt;space&gt;&lt;space&gt;code &lt;Enter&gt; # comment code if you're in ise or notepad++ you can ctrl+a, tab, ctrl+c, ctrl+z then ctrl+v into reddit also, these \` can be used in pairs for inline code like this: \`inline code\` here is some `inline code`
Think you mean sub query in the where clause...
You can also use windowed function row_number. select author_id, author, book_id, title, publish_date from( select authors.id author_id, authors.author, books.id book_id, books.title, books.publish_date, row_number() over(partition by authors.id order by books.publish_date asc) rn from authors join books on books.author_id = authors.id )res where res.rn = 1 This query should do the trick.
no i mean subquery in the FROM clause
Well....let me read about Tally tables because apparently me SQL knowledge is nowhere near yours... i wanna become a DB Developer but as am seeing what other people can do the only thing that comes in mind is to study harder.
49) should be LEFT not RIGHT
Min and group by, or my personal fave... RowNumber
The third normal form is defined by relational algebra. There is no "reduce". There are quite clear rules about them. The times that redundancy improves performance are irrelevant to the matter of what normal forms are.
Generally speaking you're looking for window functions (e.g. the ```OVER()``` clause) for selecting the top N objects per group. That said my preferred method is to use ```APPLY``` as I find it easier to read, like so: SELECT authors.id, authors.author, books.id, books.title, books.published_date FROM authors CROSS APPLY (SELECT TOP 1 * FROM books WHERE books.author_id = authors.id ORDER by books.published_date) AS books
Appreciate the quick response. I tried this and it got me closer. For some reason it returns every row of tickets who's latest status is awaiting vendor support and makes all those rows awaiting vendor support. It seems the group by number isn't working.
Going through exercises like this counts as studying. I'll break it down a bit... A tally table is just a table of numbers. Many times a tally table can be used to avoid writing a loop. You can look through my comment history for many more examples. The `tally` CTE in the code above generates one on the fly, but they are more efficient if you materialize them. Here's a script to create and populate a physical tally table with 1000 rows... CREATE TABLE dbo.tally (n int NOT NULL CONSTRAINT PK_tally PRIMARY KEY CLUSTERED) INSERT dbo.tally SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS n FROM (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) a(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) b(n) CROSS JOIN (VALUES(0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) c(n) The `holiday` CTE above is just a table with a list of dates to exclude. Again in a real world scenario this should be a physical table, not a CTE. The `rn` CTE uses the `DATEADD()` function to subtract days from `@StartDate` variable (cast as a date to round off the time part if it existed). The number of days subtracted is determined by the row value of `n` from the tally table (which is zero-indexed). To exclude weekends the WHERE clause contains `DATEPART(dw, ... ) NOT IN (1,7)` and to exclude holidays we LEFT JOIN to `holiday` and filter out the rows where the join succeeded with `h.[date] IS NULL`. Finally a row number is generated for each day remaining. All that is left to do at that point to select the row from `rn` with a row number of 15.
 SELECT InvoiceID, OrderID FROM invoice_table WHERE InvoiceID IN (SELECT InvoiceId FROM invoice_table GROUP BY InvoiceID HAVING COUNT(*) = 1) AND OrderID IN (SELECT OrderID FROM invoice_table GROUP BY OrderID HAVING COUNT(*) = 1) &amp;#x200B; There is probably a more elegant solution out there, but this should work.
2 o'clock coffee with my colleagues helps with this. We get coffee (or tea, or water, or nothing), talk, walk around sometimes.
That appears to have done the trick, thank you!
i would like you to look a little more closely at what you said you wanted and what i gave you also, this is the first time you mentioned a GROUP BY
I don't believe the PIVOT keyword supports a subquery for the column_list (the IN (Col1, Col2,Col3) bits). I think the only option is dynamic SQL, which rules out views and table value functions.
No, they mean joining on a sub query SELECT * FROM someTable a INNER JOIN (SELECT * FROM someTable) b ON a.id = b.id; Ideally I'd use a CTE if available in the specific "flavour" of SQL you're using, but joining against a query works too
Thank you everyone, a subquery in a second inner join using a min funciton did the trick.
Ran this query against a dataset of users and userevents. The books table (userevents 78,549 rows 34.258MB) and authors table (person 1000 rows .305 MB) https://i.imgur.com/Gt2Fq40.png Cross apply didn't correlate the subquery (Thank you SQL optimizer for being smarter than the query) but rather scanned the table, sorted it did a nested loop join against that dataset. In 12 seconds it returned the dataset. The second query hash matched the keys, then sorted the dataset and returned the same amount of rows in &lt; 1 second. I've said it a few times but correlating your subqueries in some engines (particularly MSSQL) give poor results and users should be wary.
No different. Really depends on what the business currently has in place. Ive worked in places where the tables/views are in place so your selecting from a handful of aource tables and transforming it to your needs and in some companies you have tk build it your self. That requires building the etl process to get data into tables and then querying the data as needed
It's in these moments when I really love Postgresql ``` SELECT DISTINCT ON (authors.id) authors.id, authors.author, books.id, books.title, books.published_date FROM authors INNER JOIN books ON authors.id = books.author_id ORDER by authors.id, books.published_date ``` something as simple as this query, comes at such a big price.
what if the author publishes 2 books on the same date?
But it's a technically wrong solution. Never join on the value of an aggregate function. You don't have any rule in place that says that it's a unique value for instance. The correct and fast solution is to do a subquery with row_number() function and then filter it out.
The table has several rows per ticket number or [number]. A row gets created Everytime a ticket status is updated. Some tickets have 5 or more updates. I want tickets who's latest status is "awaiting vendor support" when I run your query I get every row of every ticket who's latest status is awaiting vendor support. It also seems to update those various status with awaiting vendor support. So a ticket that has been 'opened' , ' in progress', 'awaiting vendor support', then 'closed' returns 4 rows with the same ticket number and replaces it's original statuses with 'awaiting vendor support'. A ticket whose status is just 'opened' , 'in progress', then 'closed' doesn't return anything. Hope this makes sense. Your query gets me close.
&gt; I want tickets who's latest status is "awaiting vendor support" when I run your query I get every row of every ticket who's latest status is awaiting vendor support. are you sure? this part of the join -- AND t.updated_at = m.latest will ensure only the latest status is returned and this part of the join -- AND t.state = 'Awaiting Vendor Support' ensures that only the latest status is returned if this is the state i am seriously thinking you are running something other than the exact query i gave you
both would be returned
&gt; You don't have any rule in place that says that it's a unique value for instance. well, duh... last time i checked, MIN() returns **only one value**
But it can match 2 values when you join it, duh...
And he asked specifically for: &gt;book Not books
In my personal experience, when doing string manipulation on large data sets, dumping it all in a temp table and materializing your work step-by-step tends to actually be faster than doing it all in one shot. So I would create a temp table with columns for an index and value for each of the columns you want to split out in to. Then, set "StreetIndex" as CHARINDEX ('&gt;', COLUMN). Then, set "CityIndex" as CHARINDEX ('&gt;', COLUMN, STREETINDEX + 1). Likewise for StateIndex and ZipIndex. Then, set Street = SUBSTRING ( COLUMN, 1, StreetIndex - 1), State = SUBSTRING (COLUMN, StreetIndex + 1, StateIndex - StreetIndex - 1), and so on. Might be some off by one errors in my sample code because string manipulation is always finnicky if you're not actively validating it. But hopefully you see the general approach.
Well with sage100 i either look at my notes acquired over several years or hunt around until I find the columns I need then do all my joins. As for knowing what I need to do, as a starting point, it depends on the generic request. "I need all sales from sales rep 2 that had a product cost of x and shipped to a customer type of z.",
Is there a cost? I saw it said free trial but I don't really want to pay anything
There needs to be a little more information. Do you just need to tell if any plan was active at the time? select acctnum,product,startdt,enddt from accounts where acctnum = &lt;acct&gt; and &lt;indate&gt; between startdt and enddt; If you are looking for a listing, something like: select acctnum,product, case when &lt;indate&gt; between startdt and enddt then 'Yes' else 'No' end as flag from accounts where acctnum = &lt;acct&gt; and &lt;indate&gt; between startdt and enddt group by acctnum,product; Not going to say my formatting is correct, but idea should be correct.
A cost to what? Of the things I just talked about, the only thing that has a price is SQL Server but the developer edition is free for a single person to use so that wouldn't affect you either.
So in your particular situation how do you manage to know what column to join tables at? From my understanding, and taking your example, lets say that the customer column is in a different db table from the sales rep column. Do you purely from yours years of note taking know what the PK/FK is or do you ask the dba what the connection between them are? That seems pretty tedious
I have two main databases. The first database is a commercial ERP product, which involves a front-end user-facing application and on the back end it is an Oracle database. The other database is our own data warehouse which is basically a central hub, where some tablespaces are normalized and organized, and some of them are more like the dumpster behind your local Chinese buffet. Today I was asked "Can you get me the balance (the sum of unit cost x quantity on hand using an average costing model) of our inventory at location x, by day, for the last two years?" This means I need to use SQL developer to directly query the sum of transaction quantities for 730 days, each day cumulatively adding or subtracting from the prior day. While the result is simple, just 730 rows paired with some positive dollar value, actually getting to that answer is very complex. That's why those sorts of questions are delegated to the DBA or analysts. A query like this within the ERP means joining about 15 tables to get the result, 3 hours of design, about 45 minutes to run, another hour to validate, and if it's wrong, run it again! As far as "How do I know what tables to use?", may ERPs have thousands of tables, but there's probably 100 base tables where most of the answers are. You simply get to know those tables. Seems like a lot but you just work with it enough until you know it. And chances are, if you're using a commercial product like SAP, EBS, Salesforce, etc, then you're probably not the first to ask the exact same question or a variation of it, and often you can find queries on user group forums specific to your product. I usually start there to figure out what base tables I need, then branch out from there. If I get lucky, might even be a copy/paste job. My other database, the data warehouse, we get a lot of questions that we can't answer with certainty because the data is mostly unstructured. It's a good estimation tool but you have to really understand what data is in those tables and assuming nothing because once you start assuming things, it's gonna be wrong!
My typical approach is to have a source of truth to compare things against. Generally, developers are going to be storing data in a table, but then they would also built tools or other interfaces that a customer service rep might use, or things that a customer might see. You might start with pulling up an order number that you're looking at, either on the website, or in a CS tool, and then looking at the fields in that table. You might fight something like a "customer\_id" field, with some number. Now you go to the customer table, and query for that number. Now you know how those 2 tables relate to each other. Rinse and repeat for everything you're trying to hunt down. A lot of it is just common sense, business knowledge, comparing things against some source of truth, or as a last resort, nagging the developers and asking them.
The attached picture happens, where queries are often not optimized and behave more like the purple slow line, rather than the fast line. What will set you apart is knowing how to turn the slow queries into fast ones, or build databases that make most queries inherently fast. In terms of what you query, it all depends on what you're trying to summarize, but will be similar to what you're learning about now. I got that picture from [use-the-index-luke.com](https://use-the-index-luke.com), which is a great resource.
If you're on SQL 2016+ https://docs.microsoft.com/en-us/sql/t-sql/functions/string-split-transact-sql?view=sql-server-2017
Can confirm that I'm using 2012. It's a work pc otherwise I would bump up to 130 compatibility and call it a day using STRING_SPLIT.
You could query from that table, and then join back to itself, and figure this out. Something like this: &amp;#x200B; select account\_number, product, start\_date, end\_date, case when pt2.product is null then 'Y' else 'N' end as standalone\_flag &amp;#x200B; from product\_table pt1 left join product\_table pt2 on pt1.account\_number = pt2.account\_number and pt1.product &lt;&gt; pt2.product and pt2.start\_date between pt1.start\_date and pt1.end\_date &amp;#x200B; where pt1.account\_number = ??? and pt1.product = ??? &amp;#x200B; Hopefully that is SQL-92 compliant. I'm too lazy to go lookup a full list of what works in SQL-92, but the above is pretty basic, so hopefully. That would essentially give you a Y/N flag, indicating if there were any products that overlapped with the one you're searching for in your where clause.
When it comes to validating the query pulls the correct data, does that refer to separating the query into chunks and running them to ensure each piece works? And then once each small chunk of the query works combining it all into 1?
This is a lot more than a "SQL" question. You need to: * Put the database somewhere * Create a website that interfaces with the database _securely._ * Create an Alexa skill to accept and interpret the input (words) * Create an interface between the skill and the database. Again, securely. And has to handle the possibility of missing or duplicate input (do you update existing records? create a new one?) I'm sure I've missed a few other pieces. The actual SQL portion is pretty simple; it's all the plumbing to _get there_ that's difficult.
Business users don't run queries directly. Usually you have some sort of business dashboard or pre-written reports which the users can trigger to execute (or execute on a schedule).
Most large corporations have schemas for their data warehouses that show parent to child table relationships, as well as data types for each column.
He would probably run a much simpler query to pull the data back from the sum and export it to excel or something to be certain the query is calculating everything correctly.
Being a business user and understanding how to write and optimize queries can make you a very valuable asset. A lot of times IT is backed up and almost always under staffed, so being able to make ‚ÄúThe Magic‚Äù happen is a very valuable skill.
Yup I second your input. The other element that needs to be taken into consideration is input validation. There is no doubt that Alexa has some impressive speech to text recognition - but if certain columns in the table are restrictive to data lengths and types, there will need to be an additional layer of logic that processes the values generated by Alexa to ensure they meet the various column constraints.
The company I'm with has thousands of tables across departments. Each department has tables they use regularly and when needed, you kind of have to know who to reach out to if you want to join onto other tables that you've never seen before. As well, we have the ability to create tables, so if I need to create one for a project or a process improvement, I can just go ahead and do that. As well, we have an email group for SQL users so if you have a query question or table question you can ping the email account and someone will help you find what you need. I'm on the business side(supply chain) and learned SQL just through my current job. Makes me a lot more valuable and honestly it's very nice to be able to pull your own data rather than having to bug someone else to do so.
Delete logs
I think the biggest difference is that In an actual business environment, often your data will be messy as hell. my time is spent exploring data, then picking tests cases to see how they look and figuring out how to interpret the data, then looking for edge cases and trying to figure out how to adjust my query to include/exclude them. If you‚Äôre lucky your Data Warehouse will be documented and there‚Äôll be diagrams to show how to link tables together. If you‚Äôre not lucky then the EDW will be a poorly documented clusterfuck and you have to figure out how to interpret data using business context and by hounding down other people to explain weird results in the data.
https://docs.microsoft.com/en-us/sql/relational-databases/databases/display-data-and-log-space-information-for-a-database?view=sql-server-2017 Then, https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-shrinkfile-transact-sql?view=sql-server-2017
When I first started the data came through an etl from a proprietary db format to mssql, there were no fk or pk that I had documentation on. I had to build all of it. Now it's native SQL but at this point I know where everything is so I don't look at the keys too often. The sales rep number is referenced in invoice history header, so that's one join between customer and invoice history header. History header and detail will join on invoice number and sequence number. I generally use the same 10 tables all the time. Last month I started a new project and had to dig deeper in to the bowels of the db to find the data then join it to my normal working tables, but again the hard part was even finding it. I could see the data in the application, but where was it in the db? As for the join, if my first set of data has 18000 rows, and I add another table with 3 rows and end up with 360000000 rows, chances are good I fucked something up.
fyi, you can just indent your whole code block with 4 spaces, instead of doing back ticks on every single line It‚Äôs a bit easier to read for blocks of code
Hi, so how does the solution look?
Exactly this, PIVOT doesn't support subqueries, I think there is something in oracle for returning XML but nothing apart from that. You can work around it using dynamic SQL as in the first example. I think it probably has something to do with a lot of systems break when SQL returns a different number of columns than expected, I know RS fails it columns are missing (and ignores additional columns). Personally I just always pull data in long format and pivot in the front end (and often I have to unpivot wide data, UNPIVOT is awful for this, CROSS APPLY and VALUES definitely the way to go).
The only way I could see this being an issue is if an author published 2 books on the same day that is also their very first published date. I would say it's not a bad thing to return 2 books in that (very strange) case, because how do you decide which one to pick?
I had a really "fun" project (had its ups and downs) where we were revamping how our reps get scored on work. We were relying on an honor system of sorts, but mgmt knew our tables kept records whenever a rep made a change in the database so the idea was to run queries every night to look for changes made by our reps and score them based on how many changes were made. I still don't like the method but it gave me a chance to learn a lot so I took the task. We already knew a lot of the tables that were being used because we had to query for stuff all the time, but there was still an "unknown" factor because we literally had ~14 identical servers, each with.... I don't even know, 400+ tables at least. I basically just queried as many tables as I could to see (1) if anything were in the tables and (2) what sort of data they held and (3) if they could be used to track rep activity. I work for a large health insurance company, so our tables weren't built by us. They were built by the government and what notes we were left with about how everything was connected was not entirely sufficient, so it forced us to use this trial and error method. So in summary... you don't always know where to look. But in business, your boss will either know you will need time to look or will know where to find that information. And that's what makes me such a valuable person at work now. I probably know more about how all the tables connect to each other than anyone else in the department.
and if the author publishes 2 books on the same date, **which one would YOU return?**
We use SYSPRO as our ERP here and it's mostly to do with experience. Whats really helpful is understanding how something gets from A to Z and how it goes through all the different modules within the ERP. Once you understand that the tables are named pretty well for you to delve into and figure out how they connect. SYSPRO comes with around 1k tables as standard, I rarely have to connect more than 5 tables, although the most complex one (our KPI view) connects around 50.
you can either spend the time creating the next year or so of tickets, then have a cron create missing events every {timePeriod} (generally ahead of time), or you can save the rule and closer to each event create each event individually, making sure whatever you use to display inducts rules to ensure they display. I'm a fan of data existing, as then you can use constraints to check. I'm also a fan of storing actively queryable data in it's format, so i'd have every day (least specific), every hour (24x more specific, but necessary for some), or every 15 minute period (most specific I've had to get, is 96x data volume of daily) have a row in the database. I'd probably avoid primary key that wasn't the date or time period, and constrain that two events cannot overlap, using transactions and rolling back in the case of constraint violation. I'm not sure anything with a periodic is pure SQL (happy to be shown otherwise) Further to this if you use the time period + other identifiers as compound primary key / address, then you can vaccum the database by removing events and storing as a bitmap between ranges for historical records. What I mean by this is - start_date - end_date - bitmap `(ceil(count(records_between)/8))` You can then either treat the contiguous bytes of the bitmap like a buffer or bit-string, which is space efficient without loss of data.
That‚Äôs not true at all.
That should be specified, but if it isn't, then a random one will do. This is more about the very bad practice of joining on the result of aggregate functions. In most cases the result won't follow 100% the requirements: * Join on a result that is float, will likely fail entirely because of precision * More than 1 row can match the value of an aggregate function So basically most questions are about getting just 1 row, but you're likely to get more than one or none, and waste precious hours debugging this bad code
I think the message is pretty self-explanatory. Either you don't have enough space on the drive hosting your database's files, or you've set a maximum file size for your database file(s) and aren't allowing it to grow to what's required. So, either get more space on the drive, move the database to a drive with more space, fix your database growth settings, or delete existing data from the database.
I would argue that the question being asked is "what is the first book each author published" and so if an author published 2 books on the same day then a random one would not do. I see what you're getting at, but I also think it's important to be nuanced and not always assume "well they said they only wanted one row". Of course it matters what the result is for, if it's a program that only knows how to deal with one row results that's different than if you're doing data analysis.
A *book*, unless otherwise specified. Let's say you're doing it for a data export to another system. The devs there specified "book" so they'll assume they'll get one book per author. You however do this max thing. It works, everybody is happy. But then Joe Smith decided to publish 2 books on a Friday: "Vaccines cause autism" and "MMS - best cure for malaria", and your shop suddenly dies on a Saturday morning because you didn't follow specifications and your export broke production. However nobody will give a damn if you say that Joe first published "Vaccines cause autism" instead of the other.
Typically validating results comes in the form of picking individual test cases and "solving it on paper" so you can visually unbutton all of the aggregation to see if it is reasonable. Of course we don't use paper, we use Excel. So in the example I mentioned earlier, the inventory balance example, it is possible to get a quantity on hand for any given date by adding up all of the ins and outs for that particular inventory item. So starting from the beginning of the item's existence, I add up all the transactions up to the first date in my criteria, and up to the last date in my criteria, and I see if it gives me the result I want. I choose a few items and a few random dates. Once I know that the mechanics/method is working, I can build on that and include all items at a specific warehouse because I've done the test scenario and I know I can count on it. I usually select all relevant columns and don't put any "where" criteria when validating test cases, because I want the whole pool of data so I can do it manually. If I restrict data up front, I'm creating assumptions that I might not be accounting for. And of course when I saw "the whole pool of data" that's not going to work in Excel if you have more than 65k rows for older Excels, and 1m rows for newer Excels. But you also don't need Excel, it's just convenient. You can do the same validation within your RDBMS via temp tables and whatnot. It just takes longer. And to add another answer to your original question, today I was presented a query from one of the analysts that won't run (or will finish sometime way down the road) and I can't even run an explain plan on it. They handed it to me and said "fix it". As a DBA, my job is to first check the joins and see if there's non-indexed fields involved, see if the type of join is the best way, etc. I might check schema statistics and I might run a new scan on those tables. I also might find out what question they are trying to answer and do the entire query a different way. I might defactor in-line views to CTEs, for example. Or I might put subquery results in a table, index the table, then join it to the original query.
SSMS still has it's uses for maintenance &amp; DBAs but yeah Azure Data Studio is amazaing from a data analysts' perspective.
Ok so after further review if a tickets latest update is 'awaiting vendor support' it will pull multiple rows for that ticket number that is 'awaiting vendor support' see screenshots. Example A https://imgur.com/gallery/fMJvdof Here is a query for just ticket#37058 Example b https://imgur.com/gallery/ZLZpWEk
You let users run queries on your production DB?
you lazy bastard [answers on stackoverflow](https://stackoverflow.com/questions/15708027/mysql-select-hour-in-datetime-and-group) SELECT ID, DATE(checkin_datetime) as 'day', HOUR(checkin_datetime) as 'HR', MINUTE(checkin_datetime) FROM YourTable didn't even start to look at https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html - hour https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html#function_hour - minute https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html#function_minute
If you just need it so the user can visually read it then I‚Äôm pretty sure you do string manipulation to get it in a format that does that. If you need 2 separate values for hours and minutes then you do something like Hours: TRUNCATE(TIMESTAMPDIFF(Minute,StartTime,EndTime)/60 ,0) This will take the difference in times in minutes and then find out how man hours evenly divide into those Minutes: MOD(TIMESTAMPDIFF(Minute,StartTime,EndTime),60) I could be wrong on that mod sytax. This will take the difference in times in minutes and then return the remainder of minutes that don‚Äôt evenly divide into hours.
you just skimmed the requirement, admit it because you overlook the fact that the Minutes result column should be the **difference** in minutes between checkin and checkout
&gt; Let's say you're doing it for a data export to another system. The devs there specified "book" so they'll assume they'll get one book per author. you would be a pretty poor developer if you did not take 2 books on the same day into consideration you would be **an even worse database practitioner** if you didn't haul your ass over to the developers and ask them what they wanted if 2 books came out on the same day &gt; because you didn't follow specifications clearly, you've never worked anywhere except in a silo where people don't *discuss* requirements
&gt; Edit* Ahh just realized all these updated at are the same **clearly, you have bad data** looking at the rows for 37058, how the hell can they all have the same `updated_at`????
OMG, you're so cocky but also so stupid. The date and time functions I linked have the answers you seek. You can pass the result of one function into another, there's a whole programming paradigm based around it.
Oh, irony. Do you realize that the phrase "you're so cocky but also so stupid. The date and time functions I linked have the answers you seek" is pretty much the definition of "cockiness"? What's even more ironic is that u/r3pr0b8 "skimmed the requirement" as well.
show me where i went wrong, then how would you interpret how the Minutes column should be calculated? it is clearly **not**, as buddy above suggested, produced by `MINUTE(checkin_datetime) AS 'Minutes'`
So this works on every test case I throw at it, feel free to hit me with improvements or alternatives. &amp;#x200B; SELECT a.id, a.author, b.id, b.title, b.published\_date FROM authors a INNER JOIN books b ON b.author\_id = a.id INNER JOIN ( SELECT author\_id, min(published\_date) AS published\_date FROM books GROUP BY author\_id) g ON g.author\_id = b.author\_id AND g.published\_date = b.published\_date;
/u/lk167 /u/deny_conformity The third party app I am connecting with does indeed break when the number of columns changes but I had built the underlying queries in a way that they won't pull the changes until I add a row to a certain table and then it's just a matter of refreshing the data structure with the push of a button, I was just trying to avoid having to change the query and the app when the number of columns changes. The third party app also allows google spreadsheet data connections and I've figured out how to access stored procedures through the spreadsheets so I can workaround by just dumping the data set there but I was also trying to avoid that extra step. Thanks for the notes.
I am kind of confused. Are you saying the Distinct ON (field) comes at a huge performance cost?
10gb per database, so you can have 5 databases @ 10gb each for 50GB. &amp;#x200B; Keep in mind that you will be limited to \~1.5GB of RAM per instance (this is for all 5 of those hypothetical database combined) and 1 physical CPU or 4 cores. &amp;#x200B; If this will not be used for production you can use the SQL Server Developer edition which has no limitations... other than not being used for production work.
You don't store the recurring events, you store the specification of the recurrence of the events 1. Store the recurrence specification in a table - preferably using iRules and exDates 1. Use a library to calculate when the events will occur, for a certain time-frame, like a month or a year 1. Generate these events in a materialized view 1. Recalculate them as needed PostgreSQL has a few extensions that can calculate dates based on iRules and exDates, as well as supporting materialized views
&gt; show me where i went wrong, then you missed this: "each interval is split into hourly intervals" (think cross join or a join with calendar/time table or somesuch). &gt; how would you interpret how the Minutes column should be calculated? The main thing is that "minutes" are the part of the interval falling within (intersecting) with a particular hour. For example, for full hours you might want to hardcode 60 instead of doing a calculation. &gt; it is clearly not, as buddy above suggested, produced by MINUTE(checkin_datetime) AS 'Minutes' this is absolutely true
Can you clarify what you mean by production work? I use SQL Express at work just to keep track of one big table that is too big for Excel. I've had to start using a 2nd database because I surpassed the 10gb size limit. Would I be eligible to use SQL Server Developer?
I could be very wrong here but I imagine production would be defined as financially benefiting?
Thank you! This is the answer I've been looking for.... &amp;#x200B; But... &amp;#x200B; Here we go down the rabbit hole! I have a 8GB database that was performing slow on a Lightsail 2GB, 1core instance. This morning I migrated it to a Lightsail Linux instance with 4GB, 2 cores which cut query times in half, but still uber slow. Would the RAM resource restrictions you mentioned above have anything to do with the slowness? I'm curious if I should migrate to MySQL or Postgres?
Select a.name From database.table a Put an alias after the table and before the field. Not sure in the db though
More than likely, yes. If the DB is 8GB and you can use 8GB of RAM, you'll notice little performance hits except for really poorly designed queries... since everything is already in RAM, it won't need to hit the disk and it will perform much better. I would normally say a good first step would be to ensure you have proper indexes setup for your query, but indexes count against the database size, so you might hit the 10GB threshold. If possible, maybe try installing the developer edition and see if query times improve, as it can then utilize more RAM and CPU.
Could be that you're not in the proper default database. When starting a new query, there is a particular db you're querying in, and will not need to prefix that one. If you are navigating around another db on the left side, the new query you open will be "localized" to that selected db.
Genius. Thanks for taking the time for lil' ol' me!
And if you are often in different databases, but default to master, just use the USE command to switch to the proper database, e.g. `USE MYDATABASE;`
Does that switch *only* the current query window? (I doubt "localized" was the right word for what I was describing, but it seemed fitting)
Yes, only the current query. You can though use the USE statement multiple times in the same query window. i.e. USE DB1 do thing USE DB2 do thing.
"each interval is split into hourly intervals" is vague and receives no further explanation in the body of the original post
Thanks! New to this subreddit, and already finding nice nuggets, never really used CTE (had to lookup acronym) with or join with statement like above but looks extremely useful. Would in past use a temp table which sometimes in nice but guessing worse performance if no need to reuse? Cte info link for reference https://www.red-gate.com/simple-talk/sql/t-sql-programming/sql-server-cte-basics/ **requires sql 2005 or above (no wonder didn‚Äôt use!)
in fact, if you examine OP's "This is output I am looking for" it is clear there is no cross join or hourly time table required because right after 13:xx and 14:xx there should appear 15:xx in the desired results, but the only times in the output are those of the input
alter session set current schema = 'blah';
do the math on several checkin-checkout differences and you'll see `Day` and `HR` are taken from checkin, with `minutes` being the diff in minutes *for the interval* to checkout
If you're using more than one database located on the same server, using table aliases is going to help here. &amp;#x200B; SELECT t1.column1, t1.column2, t2.column1 FROM db\_name1.table\_name t1 join db\_name2.table\_name t2 on t1.key = t2.key &amp;#x200B; If you're not using more than one DB, another comment below had the right answer, just put USE DB\_NAME at the top of your query or stored procedure and you shouldn't have to include the DB Name in the FROM section either. &amp;#x200B; SMSS should also have a drop down box on the SQL Editor toolbar for you to set the default DB for the query window you have open.
Replying to your own comments - are you using the phone app? It's horrible. Anywho, "is vague" is more or less, your opinion. Splitting a time interval into hourly blocks/intervals sounds perfectly defined to me and, also, very much common and customary task (at least in my experience linked to scheduling). Reasonable and customary assumptions aren't a bad thing. Also, yes, the OP seems to have missed 4/1 15 hour data point, but you can certainly see that IDs 25 and 29 have been duplicated in the result set, can't you?
17 has debugger, 18 is the version you likely have as it is the most recent version. [just roll back to 17.9.1 until microsoft pulls their head out of their asses and shoves debugger back in.](https://docs.microsoft.com/en-us/sql/ssms/release-notes-ssms?view=sql-server-2017#download-ssms-1791)
Here's some fun to grok in your spare time: &amp;#x200B; `declare` `@s varchar(100) = '12345 Apple St &gt; Anywhere Town &gt; State &gt; Zip';` `declare` `@street varchar(100) = left(@s, patindex('%&gt;%', @s));` `declare` `@rem1 varchar(100) = replace(@s, @street, '');` `declare` `@town varchar(100) = left(@rem1, patindex('%&gt;%', @rem1));` `declare` `@rem2 varchar(100) = replace(@rem1, @town, '');` `declare` `@state varchar(100) = left(@rem2, patindex('%&gt;%', @rem2));` `declare` `@zip varchar(100) = replace(@rem2, @state, '');` `select` `@s,` `ltrim(rtrim(replace(@street, '&gt;', ''))),` `ltrim(rtrim(replace(@town, '&gt;', ''))),` `ltrim(rtrim(replace(@state, '&gt;', ''))),` `ltrim(rtrim(replace(@zip, '&gt;', ''))),` `--Now, without the derived variables...` `ltrim(rtrim(replace(left(@s, patindex('%&gt;%', @s)), '&gt;', ''))),` `ltrim(rtrim(replace(left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), '&gt;', ''))),` `ltrim(rtrim(replace(left(replace(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), ''), patindex('%&gt;%', replace(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), ''))), '&gt;', ''))),` `ltrim(rtrim(replace(replace(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), ''), left(replace(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), ''), patindex('%&gt;%', replace(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), left(replace(@s, left(@s, patindex('%&gt;%', @s)), ''), patindex('%&gt;%', replace(@s, left(@s, patindex('%&gt;%', @s)), ''))), ''))), '')))`
In SSMS there is a dropdown in the toolbar that lets you select the default database of your query tab, or you can set it in the connection properties.
Developer edition can be used for development or testing. If you're using it for work that you can't categorize as development of software or a database, or testing of software or a database, then it's production. I would consider switching to PostgreSQL.
&gt; In SSMS there is a dropdown in the toolbar that lets you select the default database of your query tab Don't do this if you have a few thousand DBs on your instance. Just...trust me. Don't. Unless you've saved all your work already.
yes i can, now that you've pointed it out, thank you so any duration between checkin and checkout should generate an additional row for **every hour boundary it crosses** i have to give credit to OP for showing such detailed data and yes, i did not look at it closely enough
as for a solution, a table of hour boundaries might help FROM table INNER JOIN times on times.boundary BETWEEN table.checkin AND table.checkout
and as for replying to myself, i just want my main thoughts posted, and then add additional ones later, if necessary i know i can just edit a reply to add more sentences, but i don't like getting a reply to something while i'm adding to it
Thanks for the link. It looks like that's what I will have to do.
use min() and max() with dense_rank
A few thousand DBs??!! What madness leads to that?
Your scientists were so preoccupied with whether or not they could, they didn‚Äôt stop to think if they should
I'm new to SQL, so would love more than a "that's wrong!" if it's wrong. I came up with this: SELECT customer_id -- List of customers FROM trips WHERE start_from = "A" AND -- Started at point A end_at = "P" AND -- Ended at point P DATEDIFF(CURDATE(), start_at_time) = 1 AND -- Started yesterday DATEDIFF(CURDATE(), end_at_time) = 1; -- Ended yesterday
google \`WITH ROLLUP\`
You don't need that to be a sum, it can just be plain addition, can't it?
So if I‚Äôm using it for development /testing for a prototype DB that my organization might be interesting in buying way later down the road, I should be okay? Thanks I‚Äôll look into PostgreSQL although I‚Äôve become comfortable on MS SQL.
&gt; So if I‚Äôm using it for development /testing for a prototype DB that my organization might be interesting in buying way later down the road, I should be okay? Yes that's fine. You can also use it for a testing environment for existing applications. So if you have a production system and want to set up a test environment for configuration changes or similar, that works, too.
Select all columns, and a row number column, partitioned by customer ID, ordered by start time ascending. Select another row number, partitioned by customer id, ordered by start time descending. In the first row number column, the earliest start time per customer will be row #1, the second row number column will have the last start time per customer as row #1. Wrap that query up in a cte. Then Select * from CTE a where (location = A and ascending row number = 1) and exists (select * from cte b where a.customer_id = b.customer_id and (location = P and descending row number = 1) Basically saying give me everyone whos earliest location is A who also has another record with a last location of B. I'll try to update the formatting when I'm home, doing this on the phone in the train.
No. I'm saying that it's something that every SQL dialect should have.
Comparing that to having to write a ROW_NUMBER() OVER and then putting that into a subquery. I totally agree.
 &gt;you would be a pretty poor developer if you did not take 2 books on the same day into consideration Developers are humans. They make assumptions, just like you make the assumption that duplicate values are fine, that it's the other dev's responsibility to deal with it. But it's yours, because of the specs. &gt;you would be **an even worse database practitioner** if you didn't haul your ass over to the developers and ask them what they wanted if 2 books came out on the same day That's what specs are for. They are quite specific about how many books to return. What I'm saying that it's better to follow the specs rather than not. Asking, identifying problems, etc is obviously better. But you didn't do that, did you? You offered a solution that didn't match the specs, which I highlighted as non optimal. &gt;clearly, you've never worked anywhere except in a silo where people don't *discuss* requirements Well you surely didn't. You just ignored them :)
So from a pure efficiency standpoint, why wouldn't you do something like this: ROW_NUMBER() OVER(PARTITION BY Things ORDER BY Things ASC) AS RN1 , ROW_NUMBER() OVER(PARTITION BY Things ORDER BY Things DESC) AS RN2 Then take anything where RN1 OR RN2 = 1. You avoid a completely separate process by doing this. From a high level you might be able to see how this simple example is more efficient than what you did.
 &gt;then a random one would not do. Choosing a random one would make it 50% the right answer, and even if it's not right, it's close enough that nobody would care. Choosing 2 books would be wrong in 100% of the cases because only 1 was expected, with potentially exceptional consequences that developers would have to figure out at 11PM on a Saturday. &gt; not always assume "well they said they only wanted one row". Obviously it's best to ask. But every such problem has hundreds of questions, most of them being silly. It's pretty obvious that one book is expected. And from experience, joining on aggregates is almost never the right or best way anyway.
When you're self-joining work items, instead join on td.WORK\_ITEM\_ID = td2.ORG\_WORK\_ITEM\_ID (so flip your join around). Then do WHERE td2.Work\_Item\_ID IS NULL. That will have the effect of removing all rows that are specified as some other rows ORG\_WORK\_ITEM\_ID, which sounds like what you want.
Yup
I can‚Äôt give you the whole command right now, but you need to use the ROW_NUMBER function as part of a temp table or sub query. You can PARTITION BY the distinct columns you need. Then say WHERE &lt;row number column&gt; = 1
Last question for you: do you know what the process is from switching from one instance to another? Will I have to Re-upload all my files and re-create all my reports/views? (Not a huge deal)
Also note that any query using NOLOCK can give results that were never true at any point in time if the database can be updated while you're selecting. [https://www.brentozar.com/archive/2018/10/using-nolock-heres-how-youll-get-the-wrong-query-results/](https://www.brentozar.com/archive/2018/10/using-nolock-heres-how-youll-get-the-wrong-query-results/)
&gt;It's pretty obvious that one book is expected. But it's obvious not at all. The exact opposite is what's expected. They want to know the first book published, not one book. In my experience this would be one of the times that joining on aggregates is the only right answer.
What helps me is saving a project or task that I know will be interesting/challenging for the afternoon. Many times I hit that 2pm wall because I‚Äôve already conquered the most difficult tasks by then, but if I have something remaining that I‚Äôm actually interested in then it‚Äôs easy to burn the last 3 hours of work. The last 15 minutes of work...well that‚Äôs just a matter of time actually slowing down. It‚Äôs real.
After some digging and talking with the guy who set up the database I found what the issue is. In short the data is imported every week and has a imported column. Each import brings the entire db and keeps 5 weeks worth of changes. So a ticket created more than 5 weeks ago will have multiple identical rows. Talk about a roller coaster. By filtering by the last import only I get the data I need. Appreciate all the help.
No experience with this myself, but have you tried Azure Data Studio?
For SQL Server, run a full backup from the old instance and then restore it to the new instance. That's the best practice method.
&gt; Talk about a roller coaster. ;o)
 SELECT DISTINCT Customer_ID FROM TRIPS WHERE start_from = 'A' AND end_from - 'P' AND start_at_time BETWEEN (yesterday) AND end_at_time BETWEEN (yesterday) obviously those (yesterdays) and the actual date time stamps we need. I just don't want to type them out.
Just do a SELECT DISTINCT.
I suspect that you are confused with respect to DB name, table name and schema name. "FROM db_name.table_name" is wrong. SQL thinks that db_name is a schema name. "FROM db_name.schema_name.table_name" is probably what you are after. Most dbs still use the dbo schema for everything. "FROM db_name..table_name" might work, depending on what schema the table is in and what your default schema is. I would avoid that, though. Formatting, phone, on, apologies.
Not in our production environment, but most of the places I've worked (as a business user), I've been able to query backups of our production tables when needed. Nevertheless, the pre-defined reports/dashboards just isn't a reality for a lot of business users. Probably 90% of my day is taken up in SSMS querying a data warehouse or other non-production databases. I've never worked in an environment where all of the information consumed by business users was in the form of pre-defined reports or dashboards.
Hell if its free, why not. Save the money and try it out. We use it for internal office and DR. Works fine atm no complaints. We use both VM and azure.
When I had my free year they didn't have Azure studio, but I had a free year to keep a virtual machine up. &amp;#x200B; The same VM would have cost me most of the $150 per month I had free. This was around 2013 - 2014 I think. Now there is a lot more stuff on Azure, although the prices are too high for individuals after your trial runs out (to me).
Truth. I feel like it‚Äôs really catching on with data analyst/scientist folk. I‚Äôm not a DBA or analyst though. I‚Äôm a actually a data engineer/consultant .. so I typically use PyCharm Professional (Datagrip extension) for SQL and Python development because these are the most flexible. For me, flexibility is primary. I have to use whatever DB the clients are using. So really I don‚Äôt use SSMS or others as much because they‚Äôre not cross-platform or multi-lingual (neither multiple programming languages nor SQL dialects), but Azure Data Studio isn‚Äôt far behind now. I‚Äôm excited to have more options now.
SQL Server, when it creates a query plan, will sometimes say "gee, if I had an index on these fields, I think this query could run faster." Sometimes, it's _really_ stupid about it. You might already have an index that's close to what it wants, just with the fields in a different order. It might suggest the wrong ordering of the fields in the index it's suggesting. You might even have that exact index, but for whatever reason it's not using it. It might not be able to use the index you create, even if it told you to create it! There are lots of things [it can't do](https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms345485(v=sql.105\)) Consider the missing index hints a _suggestion_, a place to start. **Definitely** don't just take them at face value. https://www.brentozar.com/archive/2017/08/missing-index-recommendations-arent-perfect/
I use SQL Server in Docker on Mac. But also, I attended Microsoft‚Äôs private preview webinar today for their new SQL DB Edge for IoT devices. It‚Äôs essentially a slimmed down SQL Server that runs on ARM devices with 750mb or more of RAM and it syncs with Azure DB. Think baby ships and mother ships. It runs T-SQL, Python, R, ML stuff. So I imagine it‚Äôs PERFECT for Docker images. I‚Äôm excited to try the preview version as soon as it‚Äôs ready.
In Oracle, you could just use FIRST_VALUE
The people who started at A are &amp;#x200B; select customer\_id from trips s where start\_from = A and -- date predicate and not exists (select \* from trips t where t.end\_at = s.start\_at and t.end\_at\_time &lt;= s.start\_at\_time and -- date predicate) &amp;#x200B; The people ending at P is a similar query, and you can intersect them. You could also do a recursive CTE and build the transitive closure of the hops.
I can reformat it when not on my phone if you need Select // Columns // Max(startDate) From ta Join td On ta.fk = td.pk Group by Ta.WorkItem ,customer Having max(td.startdate) = ts.startdate No left join needed
Alternatively employ a slowly changing dimension type 2 style currency indicator.
\&gt; How would you find the list of all the customers who started yesterday from point A and ended yesterday at point P? Just start by literally translating this requirement to SQL ‚Äî step by step. 1. *list of all the customers* You want each customer at most once, but now you have each of them possiblty many times. How to make sure you get each of them only once? --&gt; `GROUP BY customer_id` `SELECT customer_id` `FROM ...` `GROUP BY customer_id` 2. *who started from point A and ended at point P?* (I'll take care of the 'yesterday' requirement next) Obisouly we dont' want all customers with a trip, just those where all trips has some special properties. After `GROUP BY` the way to filter is `HAVING`. Let's rephrase the requirement: *who started from point A* is the same as: where the first start\_at\_time of all trips is the same as the first start\_at\_time of the trips that start\_from 'A'. If there is no trip starting from 'A', it doesn't qualify, of course. Getting the *first start\_at\_time of all trips* is easy: `MIN(start_at_time)`. Getting the *first start\_at\_time of the trips that start\_from 'A'* is not much more complex: `MIN(start_at_time) FILTER(WHERE start_from = 'A')` Unfortunately, this syntax with the `FILTER` clause is not widley supported by SQL databases yet (althouh it is in the SQL standard since 2003. However, the same result can be obtained by using `CASE` inside the `MIN` function: `MIN(CASE WHEN start_from = 'A' THEN start_at_time END` Trips with different start\_from are mapped to `NULL` (due to the implied `ELSE NULL` of `CASE`) (see See [here](https://modern-sql.com/feature/filter)). Both expressions (the one with `FILTER` and the one with `CASE`) will return `NULL` in case there is not a single trip starting from 'A'. A comaprison will then yeild `UNKNOWN`, which is discarded by the `HAVING` clause. See [https://modern-sql.com/concept/three-valued-logic](https://modern-sql.com/concept/three-valued-logic) The same approach can be used for the other criteria, giving the following: `SELECT customer_id` `FROM ...` `GROUP BY customer_id` `HAVING MIN(start_at_time) = MIN(CASE WHEN start_from = 'A' THEN start_at_time END)` `AND MAX(end_at_time) = MAX(CASE WHEN end_at = 'P' THEN end_at_time END)` 3. Finally, the \`yesterday\` requiement: Only consider the trips started and ended yesterday‚Äîa simple `WHERE` clause. `SELECT customer_id` `FROM ...` `WHERE start_at_time &gt;= CURRENT_DATE - INTERVAL '1' DAY AND start_at_time &lt; CURRENT_DATE` `AND end_at_time &gt;= CURRENT_DATE - INTERVAL '1' DAY AND end_at_time &lt; CURRENT_DATE` `GROUP BY customer_id` `HAVING MIN(start_at_time) = MIN(CASE WHEN start_from = 'A' THEN start_at_time END)` `AND MAX(end_at_time) = MAX(CASE WHEN end_at = 'P' THEN end_at_time END)` There are other ways to express this conditions ‚Äî e.g. `CAST(start_at_time AS DATE) = DATE'2019-05-20'` ‚Äî but they have some drawbacks (indexability). I generally recommend following the "inclusive lower bound, exclusive upper bound" patters for date/time filters (note that the upper bound has no `- INTERVAL '1' DAY`). This allows flexible indexing and avoids the need to find the "last moment" of the upper bound. See here for more details: [https://modern-sql.com/feature/extract#anit-patter-where-clause](https://modern-sql.com/feature/extract#anit-patter-where-clause) Some details of the requirements might need further discussion: can trips cross date boundaries and if yes, how to cope with them? I'd say this the simplest solution but not necessarily the most efficient at runtime. Giving the small number of trips your example has (3 per customer) it is probably also the most efficient one. However, if there would be thousands of trips per customer, getting the first `start_from` and last `end_at` for each customer using an indexed top-n query might be faster. See [https://use-the-index-luke.com/sql/partial-results/top-n-queries](https://use-the-index-luke.com/sql/partial-results/top-n-queries) Index wise the query as shown in (3) would like an index that starts either with start\_at\_time or end\_at\_time. Adding customer\_id at the second place could avoid a sorting for the `GROUP BY`, but I guess that only pays off when the index can be used for an index only scan (so it also has end\_at\_time, start\_from, end\_at somewhere in the index). &amp;#x200B; Bonus question: how would you change the query that it is not limited to \`yesterday\` but lists all days at which a custmer did such a trip?
https://www.theodinproject.com/courses/databases See this from odin project, after each lesson you get pracitcal sql examples. Bthw one of the webistes you can leverage your sql skills is called 'Sql zoo' search for it in google :)
Hackerrank
I create reports and dashboards in a midsized business. As already stated by others, the hardest part of the work is to find the right tables and columns. Another part is to find and filter flawed data. Example databases are usually flawless. And the last issue to master, is to combine all datasources (oracle, mssql, mysql, ldap, excel, rest-apis,...) in one report.
The output table which I have in the post does not have all the details that are required. The output with the input should look like this &amp;#x200B; Becomes table 2, where the log in times needs to be split into multiple columns for example, for the ID 4 above the user logs in 14:55 and logs out at 15:06 so that needs to be attributed across those 2 hours 5 minutes in 14 hrs and 6 minutes in 15 hours. &amp;#x200B; &amp;#x200B; Hope this clarifies :)
The output table which I have in the post does not have all the details that are required. The output with the input should look like this &amp;#x200B; Becomes table 2, where the log in times needs to be split into multiple columns for example, for the ID 4 above the user logs in 14:55 and logs out at 15:06 so that needs to be attributed across those 2 hours 5 minutes in 14 hrs and 6 minutes in 15 hours. &amp;#x200B; &amp;#x200B; &amp;#x200B; Hope this clarifies :)
On mobile, these should get you in the right direction but youmight have to modify a bit... Listing: Select \* from listing l Join employee e on e.id = l.employeeid Join dealership d on d.Id = e.dealerId Join vehicles v on v.type = l.type Where d.suburb = v.suburb Should select all vehicles sold in the same suburb where its currently registered Sales: (not great at aggregate functions by memory but I'll try) Select employee, sum(price) from employee Group by employee, sum(price) Where year = 2016 The aggregate syntax is almost certainly wrong, I don't think you can group by a a sum(value) but you can google that one, this should just point you in the right direction. Good luck
pgexercises.com
Simply use First\_value
The output table which I have in the post does not have all the details that are required. The output with the input table which is this [https://imgur.com/a/cOsj810](https://imgur.com/a/cOsj810) Becomes table 2, where the log in times needs to be split into multiple columns for example, for the ID 4 above the user logs in 14:55 and logs out at 15:06 so that needs to be attributed across those 2 hours 5 minutes in 14 hrs and 6 minutes in 15 hours. [https://imgur.com/a/qhq3uKK](https://imgur.com/a/qhq3uKK) Hope this clarifies :)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/omlkGrl.png** **https://i.imgur.com/eSyMy14.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eoi8t0t)
Yep! It really was free but they prompted me to convert my subscription at like 30 days so just ignore that. I did and now all of my Azure services cost about $20 a month. That's still pretty dang reasonable though as a paid service. I'm using SQL Server, Keyvault, Container Registry, and Logic Apps for my personal four-run project.
 1. Select sum(price),employee from listing where datepart(yyyy,dateclosed) = '2016' group by employee order by price desc Pretty sure. You said Leases but I dont see them.
I will give this a try and let you know how it works. Thank you!
Coming from a Postgres team, readjusting to the SQL-92 life took some time. No more window functions, no CTE's, etc. It was painful. I'll give this a go and let you know how it works. Thank you for taking the time to respond!
This has a great example that replicates your issue... https://www.techonthenet.com/oracle/errors/ora02256.php
 SELECT store.pid , store.suburb , listing.dateopened , listing.dateclosed FROM store INNER JOIN listing ON listing.pid = store.pid AND listing.dateopened + INTERVAL 3 MONTH &gt; listing.dateclosed you want inner join because i presume you don;t have any listings for stores that don't exist you don't need the IS NULL check if `dateclosed` is involved in a calculation (if it's NULL, the condition fails)
SQL fiddle
Thank you, flipping the join removed the duplicate line. It did give an incorrect start time, but I was able to fix that by joining the table to itself again. If you have suggestions on a better practice for a 3rd join let me know!
If you could, reformat. I see where you're going with it, but having a little trouble with the group by clause. Thanks!
Thanks for the tip - the inner join is definitely better, should have considered that. I'm not sure why but adding in the INTERVAL 3 MONTHs as a comparative doesn't seem to give any results (as in just did not work blank screen rather than no actual results from running the query). Could it be something to do with the non traditional way my dates are formatted? &amp;#x200B; The codes set up exactly as below so not sure why it isn't working. I've taken the last line out and it gives the correct results (just obviously not filtered by the condition of being within 3 months. And there's definitely data that meets the criteria. Alternately is there another means of achieving the same outcome? &amp;#x200B; (SELECT [STORE.pid](https://PROPERTY.pid), STORE.suburb, LISTING.dateopened, LISTING.dateclose FROM STORE INNER JOIN LISTING ON [LISTING.pid](https://LISTING.pid) = [STORE.pid](https://PROPERTY.pid) AND LISTING.dateopened + 3 MONTH INTERVAL &gt; LISTING.dateclose )
Which SQL database are you using? For MSSQL I'd use a DATEDIFF -&gt; "AND DATEDIFF(MONTH, Dateopened, dateclosed) &lt; 3"
&gt;AND DATEDIFF(MONTH, Dateopened, dateclosed) &lt; 3 It's MYSQL through PHP MY ADMN &amp;#x200B; I'll give that a go thanks!
[https://sqlzoo.net/](https://sqlzoo.net/) =\]
Ah, MySQL does it in days automatically, so it's slightly different... "AND DATEDIFF(Opendate, Closedate) &lt; 90" TIMESTAMPDIFF might be the analog to MSSQL's DATEDIFF, but I don't use MySQL much.
&gt; LISTING.dateopened + 3 MONTH INTERVAL there's your mistake right there take a closer look at what i posted
sololearn is a free android app that has tutorials for lots of things including SQL
&gt; Any help would be magical okay, fix your join, unless you have listings for properties that don't exist also, change the condition on the year of the listing so that it's **sargable** SELECT property.street_name , property.suburb , listing.price FROM property INNER JOIN listing ON listing.pid = property.pid AND listing.type = 'rent' AND listing.datelisted &gt;= '2014-01-01' AND listing.datelisted &lt; '2015-01-01' when you understand these changes, i'll show you how to include 2015
Stack overflow
SELECT left(ta.Location\_ID,3) as Site\_Codes\_Grouped ,ta.customer\_ID // &lt;- whatever your customer identifier is from the TA table ,CONVERT(varchar(8),td.Work\_Item\_Start\_Time,112) Visit\_Date ,td.Work\_Item\_ID ,COALESCE(td.orig\_work\_item\_id, td.work\_item\_id) as WorkItem2 ,td.Orig\_Work\_Item\_ID FROM \[Work\_Assignment\] ta JOIN \[Work\_Items\] td ON td.Work\_Item\_ID = ta.Work\_Item\_ID group by ta.customer\_id ,ta.WorkAssignment // &lt;- assuming you want to the current work item for the work assignment having max(td.work\_item\_start\_time) = td.work\_item\_start\_time
No, because in case of more than 1 row, the nested select returns a set of values and addition doesn‚Äôt work on them.
I‚Äôd try something like SELECT ‚Äò‚Äô, ‚ÄòTOTOAL‚Äô, (select sum(b) from a) + (select sum(b) from b)
thanks
you have WHERE DATEDIFF(l.dateclose, l.datelisted) &lt;28 OR DATEDIFF(s.s_dateclose, s.s_datelisted) &lt;28 AND l.type = 'sell' AND s.s_type = 'sell' or, showing just the structure... WHERE cond1 OR cond2 AND cond3 AND cond4 this is interpreted as WHERE cond1 OR ( cond2 AND cond3 AND cond4 ) that's because ANDs take precedence over ORs use parentheses to specify exactly what you want p.s. unless you have sharedlistings or listings for properties that don't exist, you want inner joins, not left outer joins
are you and /u/gubber85 in the same class? or are you the same person?
57 sql questions book was also very helpful
is there a pdf version of this?
&gt;But it's obvious not at all. The fact that it's singular, kinda explains it very well. Kinda the idea that there can only be ONE of every "first" things. It's just that the DB isn't able to recognize each one is the first, and the user of the data wouldn't care if it's close enough. The "close enough" is a crucial point in most computer-related topics. It's extremely expensive to go for the exact, and close enough is good enough, and saves a ton of time and headaches. Nobody will care that you returned the book that was published 2h after the first one, instead of the first one, in that case out of a million. A whole system will break however, if you return 2 when people assumed there will only be one. &gt;In my experience this would be one of the times that joining on aggregates is the only right answer. Why? Can you describe a question that makes sense, that would have you return that? "Return the first books published"? That doesn't make sense. First in what area? How many books? "Return books published on the first day"? What would be the use case for that? "Return all candidates for a first book given the arbitrary low precision of our data"? Asked no one ever. --- Actual use case for this would be to show it as a property of the author, to pre-compute it in order to show faster queries on a website. It's likely a single field for that author, and if you return 2 books it will probably duplicate the authors. The person specifying this would have never guessed that some authors might publish 2 books in a day. Just like the majority of people make assumptions that data is clean and predictable. It's not. But the frontend users don't want to see that. They just want to see a book.
Thank you very much ! That makes perfect sense with the join and how the structure the datelisted queries. &amp;#x200B; If you could give me some help on how to include 2016 you would be my hero
depending on what is the possible duration is between your start and end times you might need these steps in order: 1. join with a calendar table (google it) 2. join with 24 hour table (0...23) 3. join with a table with 4 rows: 1,2,3,4. 4. the above 3 joined would represent day -&gt; hour -&gt; quarter of hour. 5. calculate overlaps.
I suppose out difference is out assumed use case, I come from an BI background and so I'm used to queries being used for analysis and not programs. It's not like the author published one book 2 hours later, they published both of them at the same time. So their "first" is actually 2. This is a super strange situation, but it's the kind of thing that is important to show in analytics.
Only the ‚Äúsales‚Äù table. The ‚Äúproduct‚Äù table could have many vendors for a unique product; creating a M-M relationship. However, one vendor is sufficient for the sales table based upon the ‚Äúinvoice number.‚Äù
[GalaXQL](http://sol.gfxile.net/galaxql.html)is an interactive SQL tutorial GalaXQL is based on the [SQLite](http://www.sqlite.org/) database engine. 1.0 and 2.0 were written with [wxWidgets](http://wxwidgets.org/) for multi-platform capabilities, which is kind of ironic because wxWidgets seems to be the reason people can't build new versions for osx and linux. Follow the instructions by your virtual teacher. The teacher can understand several possible mistakes that you may make, and gives hints on what may have gone wrong. You can skip to any chapter in the tutorial whenever you want. Use the query editor to interact with the database, and to complete the assignments given by the virtual teacher. In case of syntax errors, SQLite (the database engine used in GalaXQL) returns helpful, human-readable error messages. If you need more information, you can look things up in the integrated reference. Oh, and the OpenGL-rendered galaxy map is not there just for show; you can alter the galaxy using SQL. Table of Contents: &amp;#x200B; 1. Welcome.. 2. SELECT .. 3. SELECT .. FROM .. 4. SELECT .. FROM .. WHERE .. ORDER BY .. DESC 5. MAX(), MIN(), COUNT(), AVG(), SUM(), NULL 6. INSERT INTO .. VALUES .. 7. INSERT INTO .. SELECT .. 8. Transactions, DELETE FROM .. WHERE .. 9. UPDATE .. SET .. WHERE .. 10. SELECT FROM table1, table2.., DISTINCT 11. SELECT .. FROM (SELECT .. FROM ..) 12. SELECT FROM .. JOIN .. 13. CREATE VIEW, DROP VIEW 14. CREATE TABLE, DROP TABLE 15. Constraints 16. ALTER TABLE 17. SELECT .. GROUP BY .. HAVING .. 18. UNION, UNION ALL, INTERSECT, EXCEPT 19. Triggers 20. Indexes
I also come from a BI background, and if someone says singular, it better be singular, or you get unexpected behaviors, like an author skewing the average because he has 2 records influencing it, in the best case scenario. And in ETL, that means duplicated records down the line, failing unique constraints, duplicated revenue records, wrong results and an overall disaster. And in analytics I've never seen the value in highlighting that this particular author out of 20k has actually 2 books published on his first day. It's completely useless knowledge, yet something that can cripple a system. It's "dirty data".
I'll admit that this particular iteration of "outlier to the norm" doesn't seem all that useful to be aware of, but it's the kind of thing that you should consider designing your ETL system to handle. But I don't think that most non-technical people know what they're asking for. I have never had someone mean singular even if they say it. They might phrase it that way because they hadn't thought about the possibility of edge cases and assumed singular, but if they had thought about it they'd want to know about it.
 SELECT property.street_name , property.suburb , MAX(CASE WHEN YEAR(listing.datelisted) = 2014 THEN listing.price ELSE NULL END ) AS price2014 , MAX(CASE WHEN YEAR(listing.datelisted) = 2016 THEN listing.price ELSE NULL END ) AS price2016 FROM property INNER JOIN listing ON listing.pid = property.pid AND listing.type = 'rent' AND ( listing.datelisted &gt;= '2014-01-01' AND listing.datelisted &lt; '2015-01-01' OR listing.datelisted &gt;= '2016-01-01' AND listing.datelisted &lt; '2017-01-01' ) GROUP BY property.street_name , property.suburb note you can use a function on the column in the SELECT clause, but if you did that in the WHERE clause, it wouldn't be **sargable**
of course it is when only date literals are compared to a DATETIME or DATETIME2 column value, the date literal is assumed to have a time component of 00:00, i.e. midnight
Well I couldn't find much about using IN with datetime/dt2 on Google, hence asking. I knew that implicit conversion adds 00:00:00 when going from date to datetime. When evaluating does it act like you would expect with IN, or does it act like between?
You can test this on a local db or maybe I'm not understanding your question. &amp;#x200B; create table TestDateTime2 ( DateTimeTest DateTime2 ) insert TestDateTime2 ( DateTimeTest ) select '2019-05-23' union select '2019-05-22' union select '2019-05-24' union select '2019-05-25' select CASE WHEN DateTimeTest IN ('2019-05-23', '2019-05-22') THEN 'Success' WHEN DateTimeTest = '2019-05-24' THEN 'Also Success' ELSE 'Eh, I''ll take it' END FROM TestDateTime2 That should run
&gt;should consider designing your ETL system to handle. That's when the very clear distinction of a higher cost of "absolutely correct" as opposed to "good enough" Let me give you an example: No analysis whatsoever will ever be influenced by a 1% discrepancy in a distinct count for example. The precision is high enough. And by going to a slight error, one can save 99% of computational resources. It's much faster to use HyperLogLog, and it's *"good enough"*. Just like this case: Making sure your algorithm, all over the long ETL pipeline knows about any potential discrepancies in data, is **extremely** expensive in developer resources, and computational resources. Simplifying the pipeline, by assuming a very innocuous thing, that nobody really cares of, is something that really everyone should do. The sheer reality is that no algorithm is ever perfect, and no program is ever bug-free, because nobody can foresee bugs, since they're part of the "unknown unknown". Building a logic based on assumptions that people are flawless, is naive.
I agree with all of that.
&gt; does it act like you would expect with IN yes, it's a list ;o)
Leetcode.com w3resource.com/sql-exercises/
I only know mysql, is sqlite and mysql the same thing or similar?
thank you
thank you
Oh god why
?
Why? In 2019 it wouldn't take too long to whip up a little web interface that allows users to update the values in a single database table. On the flip side, could this just exist as a Google sheet, maybe? It's just hard to imagine what strange set of constraints would lead to this being the best solution.
p.s. it's also a list when it's a subquery that returns a single column WHERE foo IN (SELECT bar FROM fap WHERE qux = 3 )
Step 1: Open up notepad. Step 2: Type `&lt;html&gt;` at the very top. Step 3: ??? Step 4: Profit
PHP is not being used as much anymore and isn't really suggested for new web programmers. Learn Node.js or some other newer language.
"How it works" video for using Sheets as a front end for a SQL Server database: https://youtu.be/QmOxqhEuBUM
You need to use an aggregate to find the MAX stop_nom_date, I'm assuming per ID. Use that in a subquery and then do a join to your other table. Your subquery will be something like this select id, max(stop_nom_date) from exampletable1 where sitename = 'SITENAME' group by id You can then join it to the other by id &amp; stop_nom_date to filter to just the rows you want. [This Stack Overflow example should help](https://stackoverflow.com/questions/3491329/group-by-with-maxdate)
Dude. Thanks so much. That makes total sense. I‚Äôm a little embarrassed I couldn‚Äôt come up with that on my own.
Aggregates and subqueries can be fun to figure out, sometimes.
why you choose row number 15 and what is that -t.n?? you can subtract a table??
Not if you want to get crucified,stackoverflow is not for beginners
How many concurrent transactions are you expecting? How much data do you expect to accumulate, and how quickly do you expect it to grow? What are your plans for backups, replication, etc? Do you expect any large resource intensive reporting to run, and how frequently? There are tons of questions that you need to ask yourself before making a decision on what solution will work best for you. One of the apps I made used a system that I built out of old server components I pulled out of the recycle bin at my company, you could probably build one for $200 if not cheaper. It didn't have any issues supporting about 30 people, submitting several thousand transactions a day. As the application grew, I preemptively migrated the server to a more robust system. Your mileage may vary, but that's just what my experience was. Would need more details on the use case to really provide a more specific answer.
Wow...that‚Äôs all I got
In terms of collecting and entering new data, we are probably looking at 100 different "categories" of data, and could be and entering of new concurrent data around 5 people at once. Backups depended on hosting, and were not running big reports or anything. It is really small scale and would run less information than you probably did with yours and 30 people. In terms of long term costs do you think hosting our own server is cheaper?
The 15 came from the original question... &gt; I'm trying to write a code to display 15 days before a particular date. `t.n` is the column `n` from the tally table. So you are subtracting the value of that column (in days) from `@StartDate`. Try running this sqlfiddle that shows all the columns and it might become more clear how it works. I also simplified the code and fixed a counting error. http://sqlfiddle.com/#!18/ed932/6 Now try running it with the last two lines un-commented and see how it changes.
Theoretically the long term costs of hosting something that small of a scale is ostensibly free. If you have a single machine that is dedicated to running the server it would be able to handle 5 people concurrently performing simple insert update and select statements without breaking a sweat. You could then have a second machine to serve as a dev and staging environment, as well as to store regular backups from production in the event it goes down. That said, there is a lot of non-tangible costs you could potentially inherit. You're responsible for updates, maintenance, backups, etc. Where most hosting services will provide those things in packages. You also have to be relatively confident and comfortable with your knowledge of databases. A poorly written statement could screw an entire table, or worse. You have to make sure that users only have access to data they should have access to, make sure your app and server are secure and not prone to injection, and a multitude of other considerations. If you're willing to learn basically how to be a systems administrator, understand the potential risks, set up backups, and have an adequate accessibility / security approach, then I think self hosting for something that small is fine. It is exponentially easier and more practical to do if you take the time to set it up from the start.
Thanks for the response! Its been really helpful. One last question is on your servers were you running through like Windows SQL 2014 or Linux?
The first system was actually 2012, then it was migrated to 2016. SQL Express is free - but I believe your limited to 10gb of data and a 4 core CPU. 10gb is actually massive for a small operation if tables are set up properly with good keys. The core limitation is also almost a good thing for Express. You can go on craigslist and buy a quad core system and slap 32gb of ddr3 memory into it with a few TB of storage for a couple hundred bucks these days. Theoretically you could run a virtual machines and have more than one server on a machine - but I personally feel more comfortable with my prod server being dedicated if it's on a system like that. The logistics of running a small server is basically free. But you have to take into consideration the value of the data being stored.
This has to be trolling or psychopathy. Hard to tell.
why are you joining on recordgroupGUID but not filtering by it?
SQL, it employs, is the same as in mysql well, SQLITE actually lacks some minor commands, but once you comfortable with the SQL concept, it won't be a problem to read about them in other sources
After checking his post, I'm going to assume we're in the same class :)
Can you then provide a little details on your quick whip up? SQL users in general want to use and massage data so being able to quickly have a spreadsheet that can hook into a DB and manage data is critical. Most SQL DBAs are not developers that can whip up something quick in python or Java.
Awesome
Why not?
OP, it's sounds like you're not sold on this as the solution to whatever your problem is. Perhaps if you explain what you're trying to accomplish with this setup we can be of more assistance. I can't think of a scenario where this would be the ideal solution.
Can you use postgresql?
AWS is the best place to host in the short term, until you figure out your scale. Then host with dedicated servers elsewhere to lower costs
After posting that I realized they were probably hoping for something shorter: &amp;#x200B; select x.customer_id from (select customer_id, min(start_at_time) as stime, max(end_at_time) as etime where start_at_time::date = 'date' group by 1) x join trips a on a.customer_id = x.customer_id and a.start_at_time = x.stime join trips p on p.customer_id = x.customer_id and p.end_at_time = x.etime where a.start_at = A and p.end_at = P
I don't think you need the best accreditation for getting a job. I practiced using strata scratch, you can check out this platform. They have datasets pre-loaded with questions and answers you can practice with. They source their questions from technical interviews from companies so I found it helpful to use for interview practice.
Awesome Thanks!
For your frontend you could look into [webix](webix.com). That framework has made projects like this easier for me in the past.
If you're considering cloud, yes AWS is an option, but also check out GCP. Azure is there if you like Microsoft, but of the three it's my least favorite to work with (personal preference, however, there's nothing wrong with it).
PowerApps.
got you, thank you
Yeah I thought the why was self explanatory, I expected better from the people in this sub lol
Certainly wasn't suggesting a DBA should be tasked with creating a web app, but it's something an intern on the dev team should be able to handle in a few days - a simple webapp that writes values to a database is usually one of the first projects for any CS major or aspiring web dev. Sorry if I came across as shitting on this thing - it's more of a knee-jerk reaction that many people here share. Anyone who's been working with databases long enough has encountered several Excel jockeys (in finance, accounting, etc) who want to pipe data between spreadsheets and a relational database, and so a thing gets built, then it eventually gets bent and adapted to do other things, and it evolves organically into a catastrophy. The right way to manipulate data in a database is through code or an IDE (for someone who knows what they're doing), or via a purpose-built application (for people who don't) - it'll save headaches in the long run. That said, this thing is actually neat and I'm glad you've shared it - I'd just never implement a process that used something like this in an actual business is all.
Why not just use SAS?
Yes there is! Just google it pdf and it‚Äôs right there
This article seems outdated (streams?) and missing the most important difference: concurrency and consistency. FYI the FREE Oracle XE comes with every EE feature and pretty much every additional cost option for free. There‚Äôs a space limit but if you fit, you‚Äôre onto a winner for free
I think Postgres vs Oracle would be more interesting. After being on Oracle for 15+ years, we moved to Postgres over the past year and couldn‚Äôt be happier.
[Accounting ERP software](https://www.deskera.com.my/erp/accounting-software/) is Managing all accounts for your business can turn out be a nightmare, The Good [ERP Software](https://www.deskera.com.my/erp/) empowers the finance department to record, organize, maintain and analyze financial data captured from all enterprise operational systems
We've got a big push to migrate everything off of Oracle to PostgreSQL. Small issues here and there but once migrated everyone is happy. Better machines due to license cost savings, using standbys for reporting, and with native partitioning in 11 it's a win win.
Depending on the database you're hitting, you could use STRING\_SPLIT on MSSQL 2016 and above. If not, google CHARINDEX to be able to find the second occurrence of the string and pull the value out of it that way.
Thank you for your time and thank you for making it a little more clear to me. I will have it in mind from now on.
technolush: delivering curry smelling spam since way too long. We guarantee you the freshest of stolen content from other shitty blogs, bad copy-pastas to break your code and more useless outdated information about deprecated methods presented here as ‚Äúthe new thing‚Äù. Experience the terrible consequences of blind offshoring first hand on the IT sector and discover how indians became the poop flies of the internet, how their self-entitled parasitic tactics make them look like the vomit or a poop eating insect. Spammed from india with love (of ourselves).
Not OP, but I helped build SeekWell. This is mainly for people who don't know (or want to pay for) something like SAS. This also provides some interesting options for collaboration on a larger team. For example, say you have 10 sales reps updating a couple different spreadsheets very regularly and want to keep them in sync with a database. Doing that with SAS could be time consuming to set up and maintain. With SeekWell it only takes a couple minutes to set up and zero to maintain.
It helps to think about how many rows the inner select in the second query returns. It selects only the tracks with duration &lt;=90. Then, you take records (no no more) and do an inner join. Does that help?
No, the subselect in your join is pulling in ONLY tracks where the duration is equal to or less than 90 secs **and** **then** limiting those results by only displaying the ones that match the criteria t.album\_id = [a.id](https://a.id) It is doing it backwards from the first query where you are finding ID matches first, then limiting by size.
Here is helpful articles to know how fix SQL Server database errors https://www.datarepairtools.com/blog/fix-sql-server-database-consistency-errors/
JOIN is the same as INNER JOIN, so it will only return rows where there is a matching key on both tables. Using JOIN or INNER JOIN can restrict the rows returned.
Thank you for taking the time to reply. I think this makes sense when we join by a unique value, but since in this case the album id is shared by all the tracks in that album, how does it pick exactly the records from the 'track' table that match the length criteria? Instead of, for example, the first record that matches that album id?
Without knowing exactly what you want the Site column to be, it will be quite difficult for us to to tell you what you need. Just replacing the name Program with Site without an understanding of the underlying tables and aliases, is most not going to work unless you have an incredible stroke of dumb luck. &amp;#x200B; I'm going to go out on a limb and guess that you just need a better understanding of what Aliases are and how they work (Google it). &amp;#x200B; Let's take a look at this line... ,IsNull(I.Program,IsNull(S.Program, IsNull(P.Program, IsNull(I2.Program, IsNull(S2.Program, P2.Program))))) As Program All the references in your Program line (I., S., P., I2, S2 and P2) are shorthand table references that were declared in your FROM statement. &amp;#x200B; So anything after the word "AS"... &amp;#x200B; Incident AS I ServiceReq AS S Frs_Project AS P So... unless the column Site is the all of the tables that you are aliasing to exist, your code will error and will not complete. &amp;#x200B; I think you will be well on your way if you do these things. &amp;#x200B; Step 1: Determine in which table the Site column actually exists. Step 2: Determine what Alias you are using to reference that table. Step 3: Utilize that Alias and column in your code. &amp;#x200B; For example... if Site is a column in the Incidents table. Simply add the line ,I.Site to your code. &amp;#x200B; Start as simple as possible and move to more complex things later. Your 1st goal is to get the Site column to actually return some values to your Report.
The thing is the key is not unique since the album ID is shared by all tracks on the same album. If you have 2 records on the left table and 5 on the right, and they all have the same value in the field you are doing the JOIN on... How does the system know which ones of the 5 correspond to the 2 on the left table?
This situation is useful to visualize as a Venn diagram. Seen the first picture on this page: [https://www.dofactory.com/sql/join](https://www.dofactory.com/sql/join) &amp;#x200B; You have rows in the left table and rows in the right table. You're doing an inner join which means you only want records that exist in both. Read that sentence an extra time. When does it hold true? It only holds true for tracks &lt;= 90 seconds. So you can't get more rows than that. You can't magically get tracks that doesn't match the criteria. Makes somewhat sense?
In case any reads this I got it to work properly by dong a join &amp;#x200B; SELECT p.sku FROM LKVEntityMarketBasket e INNER JOIN Product p ON e.MatchedEntityID = p.ProductID or e.EntityID = p.ProductID WHERE EntityID = 10110 and RN = 1
It is clear now :) I also realized that the album id is unique on the album table, which is what the second one is checking. It took me longer to realize that I would like to admit. Thank you for the detailed explanations, it really helped me understand. Onward to the next video of the course!
Doing this date math in your `WHERE` clause will cause the database engine to not use an index for searching on those date fields if one exists.
Is your only access to SSRS, and not the tables/database itself?
I'm guessing the id column on album is unique? And on tracks the album\_id is not unique as you have multiple tracks for each album? So let's say you have: 1 row in album with id = 1 12 rows in tracks with album\_id = 1 Of the 12 tracks, 5 of them have a duration &lt;= 90 The join between album and tracks would produce 5 rows (rows from album would be duplicated 5 times).
&gt; It is clear now :) I also realized that the album id is unique on the album table Yes, this is an important distinction. You can try making duplicates in the album table and see how that affects the result. &amp;#x200B; You're doing the right thing. Questioning when you don't understand. SQL is timeless knowledge in the sense that these concepts were put into code in the -80s and are as valid today and will be for a long time. So it's worth it to make the effort of understanding it.
This is extremely basic/superficial at best.
I nearly have it figured out however I could use some help with the last piece. I've been using SUBSTRING and CHARINDEX. I'm able to get the first, second and fourth values but I'm struggling with the syntax for the third value, State. Can I get some direction or help with this code? I'm stuck.
Thanks for your point
Will have a look. thank you!
Genius. Thank you
I can access the tables and db.
 [https://www.codecademy.com/learn/learn-sql](https://www.codecademy.com/learn/learn-sql)
Adding the following: IsNull(I.Owner,IsNull(S.Owner,IsNull(P.Owner,T.Owner))) As Site Failed?
 ,IsNull(I.Site,IsNull(S.Site,IsNull(P.Site,T.Site))) As Site Failed. I tried that because the correct as shown version of the Owner line pulls the (ITSM) ticket owner, so to me, I wanted to pull the ticket Site... Which did not work... because I do not know what I am doing. :)
&gt; would I put the playerid in each character table because it's a one to many where one player has multiple? And would that mean the playerid in the character table would be a foreign key to the player table? Yes, sounds good. If characters were shared among players you may need to set up a many to many relationship, but I think for most intents and purposes your design is fine.
&gt; my DB is for all the information of my d&amp;d campaign. * You want to make sure that all identifiers have a Foreign key back to its related table. Like Powers to Class to Race. * Thing like powers should be its own table. I would never put it with the Classes table since, they may have same powers for more than 1 class. Makes it easier to Query from a Join. Also gets rid of redundancy. * Don't add extra columns/relational keys that aren't needed like classes **to** players. Since characters to players will already have a unique identifier/relation. &amp;#x200B; Just a few things to look at in my opinion. I think this is a great learning Subject. You'll notice you branch out a lot more and make more sub-tables as info comes to your head.
If that works, then verify that site is in the following tables and then change owner to site. Incident ServiceReq Frs_Project Task The list above is using the same priority as the Owner.
That was it! It took me the longest time to realize, thank you for the thorough explanation :)
No problem. Happy to help!
Ask your self the relationship questions. What is a player? Someone with a name, number, and email address. What is a character? Someone with a name, race, gender, attributes, background, experience. How are they related? Does a player have a character or does a player have 0 to many characters? What is an item? Does a character have a item or one to many items... Then you build relationships between the tables. To keep things simple since your learning a basic identity key should work out well enough.
OK. With that line added, I receive the following error: "An error has occurred during report processing. Query execution failed for dataset 'DataSet1'." If I remove just THAT line, the report processes without issue.
Update: I was able to get the Owner line to work in the Site field. Progress! OK. This is good. Now the next step.
Site is in Incident and Service Request. It is NOT in FRS-Project or Task. How should I handle that?
Right now, swapping all the Owners generates an error for the Dataset.
What you want to do is join twice. FROM person INNER JOIN ehis ON person.person_key = ehis.person_key INNER JOIN ehis2 ON person.person_key = ehis2.employer_key;
IsNull(I.Site, S.Site) AS Site
You're the man! That did the trick. Thanks a bunch!
Normalization is a hot topic, so I'm sure there will be differing opinions. From my view, based on you're examples, I would have lookup tables for things like Race, Class, and alignment instead of repeating those values in the Character table. Age and Level probably not (since they're likely int's anyway). When I'm designing I ask myself, what is the likelihood of the values being repeated? If there going to be repeated then it's a lookup table. Take race for instance. Are you going to store 'American Indian' over and over again in the Character table? Or store it once in a "CharacterRace" table and fk to it? I realize this may seem like overkill for small data-sets, but if you're trying to learn, this is imho the best practice. &amp;#x200B; You could make an argument that FirstName and LastName should also be normalized and that's where it get's into some gray area and really depends on the size of you're data and what you're trying to do with it.
Have you tried doing a sub query in the FROM? You can two https://www.google.com/amp/s/www.geeksforgeeks.org/sql-sub-queries-clause/amp/ You can create two sub queries and then select what you want from each of them
Non Google Amp link 1: [here](https://www.geeksforgeeks.org/sql-sub-queries-clause/) --- ^^I ^^am ^^a ^^bot. ^^Please ^^send ^^me ^^a ^^message ^^if ^^I ^^am ^^acting ^^up. ^^Click ^^[here](https://medium.com/@danbuben/why-amp-is-bad-for-your-site-and-for-the-web-e4d060a4ff31) ^^to ^^read ^^more ^^about ^^why ^^this ^^bot ^^exists.
Ah, that did it, thanks! I knew it would be something simple...
Probably the comma after the last NOT NULL.
Oh boy... I am that blind... Shoukd I be concerned that it says "line 1"
That privileges versus default privileges irks me on postgres. And their main tool is web based. But it would be interesting to see that comparison. Seeing as a lot of mysql functionality was ported to Oracle.
it says \*\*near\*\* line1 near ")" which is actually right after the "," in question
intersection or junction or many-to-many tables are not EAV, no matter how many columns in the intersection table EAV is when you store multiple *entity* values in a single table for example, CREATE TABLE eav ( entity VARCHAR(17) NOT NULL , attribute VARCHAR(17) NOT NULL , PRIMARY KEY ( entity , attribute ) , value VARCHAR(49) NOT NULL ); INSERT INTO eav VALUES ( 'area' , '1' , 'Area One' ) ,( 'area' , '2' , 'Area Two' ) ,( 'area' , '3' , 'Area Three' ) ,( 'taxrate' , 'A' , '35%' ) ,( 'taxrate' , 'B' , '25%' ) ,( 'taxrate' , 'C' , '15%' ) ,( 'stooge' , 'Curly' , 'Jerome Horwitz' ) ,( 'stooge' , 'Larry' , 'Louis Feinberg' ) ,( 'stooge' , 'Moe' , 'Moses Horwitz' ) ;
also the single quotes -- you can't define a table with strings in place of column names
It wasn't . The content and OC material provided by the author was very helpful and information until you provides your Facebook education on it. Mozel tov, bro
It probably is pointing to the query, which start at line 1. If this was a stored proc, with several statements, it would give you the line where the query starts.
Okay that makes slightly more sense but slightly confused. What is the definition of an ‚Äúentity value‚Äù then? So in my last example that would be considered an EAV because it stores the different ‚Äútypes‚Äù of a training data? I‚Äôm still slightly confused on what would be considered an ‚Äúentity‚Äù. Entity to me is a table an attribute is a column and a value is what‚Äôs in a cell. So what defines it as a different entity? I mean take this for example. Animals ‚Äî‚Äî‚Äî‚Äî Name, Type, Legs Cat, Mammal, 4 Dog, Mammal, 4 Cricket, Insect, 6 I mean really this could be split up into a different table of mammals, or insects etc. I guess I‚Äôm still not seeing what defines it as a EAV.
&gt; I‚Äôm still slightly confused on what would be considered an ‚Äúentity‚Äù. areas, tax rates, and stooges are different entities and should have separate tables if you augmented your animals table to include furniture, you might see Entity Name Type Legs Animal Cat Mammal 4 Animal Dog Mammal 4 Animal Cricket Insect 6 Furniture Armoire Cabinet 4 Furniture Ottoman Footrest 4 i am obviously stretching the analogy here, because i needed something that also has legs
Just top to bottom
"Help posts If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own."
I understand that, I didnt word it correctly im looking for someone to call and help them explain how it works im not asking for the answers.
A free private tutor is a pretty lofty ask. And how what works? You didn't even mention which platform you're working on, let alone what you're having problems with.
Yeah I know :/ im not expecting for anyone to help just throwing it out there. Im working on Oracle SQL, what i'm working on is writing queries to show information in the tables. Could I possible pm you what i'm stuck on?
You‚Äôre going to have a much better response if you post it here. What do your tables look like, what data are you trying to get out, and what have you tried so far. You need to be able to do these kinds of things if you‚Äôre going to work in IT.
Im just trying to pass my course, im not going to work in IT. I dont like it. But thank you, i will post it if i cant figure this out.
Use SSIS to coordinate and control the steps, but the actual interaction with TFS and SSRS is probably best done with PowerShell inside Execute Process tasks.
Hi guys, Not sure why I'm getting this error. My postgres database I start with dropping tables then create tables [with data](https://i.imgur.com/nNEMFf0.png) . I don't see what is causing this error. Help is appreciated.
https://i.imgur.com/qGhiEIe.gif
I'm no SQL expert, but I do see "Drop Table" there in the line 1 error, so that may be the issue, as the table is no longer there.
"Column Ambiguously Defined" errors occur when a named column in the SELECT list is present in more than one of the tables/views in the FROM clause. In your case, STAFFID exists on several of the tables you've listed. You need to revise your query to fully qualify the column (staff.staffid) or at very least add a table alias to differentiate the column (a.staffid).
There is nothing wrong or inefficient about a giant stored procedure that uses a lot of #tables, joins, etc. The question only becomes how long does it take to run. If it takes awhile to run, then you can just dump the data out into a real table, schedule it to automatically run at some kind of set interval, and then point at the table.
I put the 'Drop Table' so I can run the script more than once. Other sample databases I've seen start by dropping tables and don't return the errors.
You need to post your query and what you're trying to do in a format where people can help you. Just post your questions, a sample of the data, and what your query looks like.
&gt; entity is a table and attribute is a column You‚Äôre exactly right (at least close enough for our purposes). An EAV model takes those columns and turns them into rows (pivot table). Each row will have EntID, AttributeKey, AttributeValue It‚Äôs a massive set of key/value pairs where each pair is tied to an entity. So a table with 10 columns (1 I‚Äôd, 9 attributes) could be converted to a pivot table with 3 columns and 9 rows
I had this issue on 2016 instances using same method. Drop all end points completely and recreate via TSQL and explicitly grant permissions to the accounts to connect to the end points via tsql. That fixed it for me
There are plenty of ways to optimize queries on large tables. Just creating smaller tables instead is a poor work around for actual optimization.
I am not sure if it's the reddit formatting, but it looks a bit off to me when it comes to the syntax/quotes, perhaps try this: ``` CREATE TABLE `user`( `user_id` Integer PRIMARY KEY AUTOINCREMENT, `firstname` varchar(32) NOT NULL, `lastname` varchar(32) NOT NULL, `email` varchar(96) NOT NULL, `mobile_number` varchar(32) NOT NULL, `password` VARCHAR(40) NOT NULL, `status` smallint(1) NOT NULL, `approved` smallint(1) NOT NULL, `registration_date` date NOT NULL ) ``` Some things to note here, the SQL syntax requires you to provide just the column name with the `` (or they can be skipped in most DBs), however you were quoting the whole column definition. If you look carefully you were also using the regular single quote '' in place of the back tick `` which means that it is a string/varchar value, which won't work for column &amp; table names. And I believe the trailing comma might have caused you some trouble.
Figuring out how to implement that stored procedure as a view could be a big win for your use case. Then you could filter down the scope of the data you are querying at runtime, and the DB engine would be able to optimize more effectively. Could take some pretty serious brain strain to make it happen but it would probably be worth it.
that's why i'm asking for help
Why do you want to trim them? Is it something to do with wanting the columns to be a fixed length at that join step for performance or something? If so, you could always start them as maxes, LEFT(n) them, then alter the columns to be nvarchar(n). I had to do that for a data scrubbing routine (although in my case, it wasn't about truncation - I just found the longest n in each column and altered them to nvarchar(n) because of performance trouble with nvarchar(maxes).)
If you try to remove a table that doesn‚Äôt exist you will get an error. Do the tables already exist when trying to drop them? DROP TABLE IF EXISTS
I mean you‚Äôre giving us almost no details dude, how do you expect us to help? What query are you running that gave you the error? After you dropped everything, did you create the tables that you‚Äôre trying to access? From your screenshots it just looks like you dropped all your tables and then tried to access them but they obviously don‚Äôt exist anymore so you got an error. You understand that `DROP Table` deletes the table right? If you dropped it then you can‚Äôt access it anymore because you deleted it.
you should do conditional drops on those DROP TABLE IF EXISTS Customer; DROP TABLE IF EXISTS Address; etc... start there then see if you get additional errors.
How are you ensuring the rows are in order?
I get the same error, I'm using postgresql
Is it possible that the table `Order` is actually `order` or `ORDER`?
The table 'Order' remains as 'Order' throughout the script.
Try it with the quotes. I wonder if Order is tripping it up due to being a reserved name (for example, ORDER BY).
ah i just noticed... it might be becasue order is a keyword... which means you need to escape it with double quotes try: DROP TABLE IF EXISTS "Order";
What are your aspirations in life to have taken an SQL/database course?
It's just included in the course im taking. But im not wanting to do anything with IT anymore. I want to change to bio chem or economics once I have finished this course.
The getThisBread SP has description columns so if I set an arbitrary length in my new SP, one day down the road someone could make the description longer which would crash my application. I need the trim to happen during the INSERT not after. It's to force silent truncation instead of erroring out when a description too large is inserted.
If you want help, post the query/problem and what you've tried/what errors you're seeing, and then maybe we'll get back to you. Seriously. This ain't /r/ChoosingBeggars.
then yeah, I think my first inclination would be the approach I mentioned, starting with maxes, then trimming everything with a left(n), then altering the column to the length you desire. depending on the average size of the temp table it might help to check for the longest value in the column and only bother with the left(n) if you need to. A bonus to this approach is that you can first store off all the too-long strings in an error table, and then have some sort of daily agent job that sends an email saying "Hey I just decided to silently truncate these strings instead of failing loudly, but here they are if you're wondering" for every day with an entry in that table.
It is common to have tables that are built specifically to output to reports. It is also common to run reports of off complex queries. It just depends on your use case. A purpose built table will be a lot faster to query than a complex query, but generally is more work and requires a more complex ETL. Is reducing that 15 second load time to 4 seconds worth the effort?
I get this error https://i.imgur.comSlgaxS5.png
The row name is in order alphabetically A-Z
you're going to find out quickly that this was done in SQL because it's the most performant way to do that type of data manipulation in any language or platform known to man (assuming it's configured correctly). trying to do the same thing in Excel can take literally thousands of times longer. what you're looking to do instead is to either **index** the ginormous (in terms of # records, not # columns) tables better for your specific queries, or **partition** the table rows across to split out into new tables (like, splitting [tblLotsOfRows] into [tblLotsOfRows201901], [tblLotsOfRows201902], [tblLotsOfRows201903], etc.) in such a way that nearly all the data not relevant for incremental report runs to not need to be joined in. when i read the subject, i thought you were talking about partitioning, but honestly you'll want to start with indexing since it seems like you probably haven't exhausted all the speed you can get there. another idea if it's an older server might be to make sure your SQL DB is on SSDs instead of platter HDDs. that can make a large difference as well (10 fold or even more, if you were on cheap 5400 RPMs).
What you're proposing isn't inherently a bad idea, but I'd recommend you first try to profile the queries that the stored procedure is running and work out where that time is being spent. It might reveal that the bottleneck can be circumvented with better queries. If you would end up using roughly the same queries to build the "smaller table", you'd just be adding more complexity and I/O to your original problem. Using (nested) views to dissect your queries and make them more manageable should be a safe bet, though.
Yes, but you‚Äôll probably want to use a program language to do the ETL into your database and then use sql to query it
that data would be so helpful for one of my projects at work.
you should look for a free crm and upload the data there. It will parse everything and give you reports and whatnot. I bet you could get a demo subscription to salesforce pretty easily.
What version of postgres are you using?
https://blogs.msdn.microsoft.com/sqlrsteamblog/2017/09/25/msbuild-support-for-reporting-services-projects-now-available/
if people are allowed to make arbitrary changes to the SQL schemas without QA'ing your app and re-releasing a new version at the time of the SQL changes, then you're screwed no matter what. they'll always be able to delete columns, change ints to decimals, alter the underlying SPROC so it changes output column names, etc. varchar length is the least of your worries. your best defense is to take a step back and not compile the SQL snippet you have above into your app in the first place. if that SQL you wrote is also a SPROC, you can make changes to it on the fly to match any SQL changes 'the hostiles' have made and un-crash your app without needing to re-release anything.
You could do this using dynamic SQL... what is your purpose for doing this?
With Over 1M records, I need to figure out a better solution to store it locally which is why I was looking at SQL.
So, something like Python?
What are you doing? PM me.
I've been given a question, that asks: List the model ID, aircraft ID, total number of seats, destination country and actual flight duration (in hours) of every flight that has departed from NEX airport (excluding flights that have not arrived at their destinations yet) The tables I have are as follows: MODEL(modelID, economySeats, buisnessSeats, firstClassSeats, cargoCapacity, fuelCapacity, length, wingspan, serviceHours) LOCATION(airportCode, country, address, phone) TICKET(ticketNum, luggageLimit, seatNum, classCode, medicalCondition, mealChoice, customerID, flightID) ROUTE(routeID, description, arriveAirportCode, departAirportCode) IRREGULAR_EVENT(eventNumber, flightID, eventDateTIme, eventDescription) SERVICE(serviceDate, aircraftID, description, cost) AIRCRAFT(aircraftID, mailCargoCapacity, numMedPacks, numDefibritlators, haulType, modelID) NEACC_MEMBER(memberID, flightGoldPoints) STAFF(staffID, name, address, email, phone, passportNum, pilotYesNo, prevHrsPilotExp, attendantYesNo, memberID) CUSTOMER(customerID, name, address, country, email, phone, birthdate, passportNum, memberID) FLIGHT(flightID, estDepartureDateTime, actDepartDateTime, actArriveDateTime, avgSpeed, avgHeight, estDuration, estFuel, haulType, captainStaffID, firstOfficerStaffID, routeID, aircraftID) ADDITIONAL_PILOT(staffID, flightID, activityCode, activityDesc) HOSTING(staffID, flightID) PILOT_QUALIFICATION(qualification, staffID I just assumed in order to get the total seats, I would have to COUNT the economy, business and first class seats.
Python can work, so can pretty much any object oriented programming language. So whichever you‚Äôre most familiar with, I‚Äôd go with that.
&gt; model ID, aircraft ID, total number of seats, destination country and actual flight duration (in hours) of every flight that has departed from NEX airport Slow your roll, let's take this in chunks. 1. List all flights departing NEX airport, including the model ID, the aircraft ID, the total number of seats (on the flight), the destination country, and the flight duration. So starting from model we have something like select * from model m This is going to give us seats, and model ID. Now: select * from location l That's going to give us the destination country, and the airport code. This will then need to be joined to route, which will give you the routeid and departAirportCode. You need to keep working through all these tables until you can link them together to get all the data you want, BUT, you don't need to count anything, you need to just add. Your data *sounds* like it is structured in such a way where: | modelID | economySeats | buisnessSeats | firstClassSeats | | :--- | :--- | :--- | :--- | | XYZ | 200 | 100 | 45 | So you can just: SELECT modelID , economySeats + buisnessSeats + firstClassSeats AS 'totalSeats' FROM model m
You don't. It'll have a number of seats as an integer. You actually want to add the values in two columns together, not count. So for example, if I want Column A + column b for flight 1 I'd just use select (column a + column b) as 'Total' from table where flight = 1
this is another good idea that doesn't require much knowledge of SQL internals. you'd essentially trade 'real-time' access for pre-scheduled result compilation, and storage space for speed.
A couple of comments: First, in the `FROM` clause, you don't have a join condition specified so it's doing a [cross join](https://www.essentialsql.com/cross-join-introduction/) which means that every row in `TCustomerJobs` is being combined with every row in `TCustomers`, which probably isn't what you want. &amp;#x200B; Second, it looks like you're just interested in selecting values from the `TCustomers` table and joining to `TCustomerJobs` isn't necessary in this situation. You are already accounting for rows that don't exist in the `TCustomerJobs` table (once you give it the proper alias) in the `WHERE` clause so you can remove it from your `FROM` clause. &amp;#x200B; That should help you get going in the right direction.
Fwiw - the cloud, especially azure is cheap as hell nowadays and they have all sorts of tools designed to make things massively easier for the non-engineer to stand stuff up like this. You could also just shop a database guy out. Your hours of learning a programming language and then learning sql retrieval is going to take a very very long time compared to my or somebody like me's short term. Then you could go nuts with sql learning how to retrieve everything and manipulate it in a form that was already correct
How do you identify the "top ten" customers? By number of orders? Total amount of orders? Something else?
Thanks! This didn't end up working for me, but I found the code that works: SELECT DISTINCT TC.strName as 'Customer Name' FROM TCustomers as TC WHERE intCustomerID NOT IN (SELECT DISTINCT intCustomerID FROM TCustomerJobs)
Every decent Database system has some form of bulk export/import available. This is what you should be using to load the data to the database. I google ‚ÄúMySQL import csv‚Äù and found a few hits, including a wizard built into MySQL workbench https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html . Once you‚Äôve loaded the data into your tables you should absolutely use SQL to read the data and run your analytics.
You should be able to have a table with tens of millions of rows and retrieve them without issue; so the source size isn't your bottleneck. It sounds like the stored procedure itself is the performance problem. It wouldn't be unique in being created long ago with minor updates over the years to the point where it gets bloated and inefficient. You have a few options. * Redo the procedure, don't use temp tables unless you have to and make proper use of indexes on sorted/joined fields. * Create a second procedure that pulls less data, but more specifics of what you need. Depending on the number of rows and your network setup the actual transmission may be part of the issue, Excel isn't designed for giant volumes of data. * If you don't need realtime, set server agent to cache the results of the query into a different table every hour or so. Then it should load nearly immediately if the execution is the bottleneck. Chances are you'll want to look at multiple options here. If you have time for it, I'd take a hard look at the original procedure and see if I could optimize it.
DISTINCT is clearly not required here, as i assume the customers table includes each customer only once
here'a an alternative... you should check the performance on the live table compared to the NOT IN solution SELECT TC.strName as 'Customer Name' FROM TCustomers as TC LEFT OUTER JOIN TCustomerJobs as TCS ON TCS.intCustomerID = TC.intCustomerID WHERE TCS.intCustomerID IS NULL
Agreed, every decent database will allow loading of CVS files. I've done this with SQL Server with a few clicks. What I would focus on is learning some database design principles and table building in SQL. Learning a bunch of programming just to load your data seems superfluous. If your data is consistently formatted you'll probably need very little code.
Nope, saving off scripts to a shared folder is a great idea, but you‚Äôll need to comment your scripts to ensure that others who use it understand what it does
Is there a better idea tho? More able to scale ideally?
GitHub can serve as a query repository for you and for an added bonus will provide you version control to make sure your query ‚Äútemplates‚Äù aren‚Äôt inappropriately edited without approval.
https://www.google.com/amp/s/caitlinhudon.com/2018/11/28/git-sql-together/amp/
I'm not sure where your skill level is at, but you could build these code snippets into parameterized stored procedures, e.g. pass a customer account number into a procedure call and get your result set via the stored procs. You would get highly reusable code without having to tinker with finding the static values in a complex SQL script.
I was thinking about using stored procedures directly on the DB. My problem is more people (with no knowledge of SQL whatsoever) would need to update the queries rather frequently, so maybe using a combination of versioning and stored procedures after people know exactly what data they want would be the right route. Thanks!
Hi, I finally managed to read the file! Here is the code, in case future readers might find useful: USE DBGamedevStackExchange; DECLARE @DocHandle int DECLARE @XmlDocument XML --nvarchar(max) SELECT @XmlDocument = BulkColumn FROM OPENROWSET(BULK 'D:\Dataset\StackExchange\gamedev\Posts.xml', SINGLE_BLOB) x; EXEC sp_xml_preparedocument @DocHandle OUTPUT, @XmlDocument INSERT INTO DBGamedevStackExchange.dbo.Posts ( Id , PostTypeId , AcceptedAnwserId , ParentId , CreationDate , DeletionDate , Score , ViewCount , Body , OwnerUserId , OwnerDisplayName , LastEditorUserId , LastEditDate , LastActivityDate , Title , Tags , AnswerCount , CommentCount , FavoriteCount , ClosedDate , CommunityOwnedDate ) SELECT * FROM OPENXML(@DocHandle, '/posts/row', 2) WITH ( Id int '@Id' , PostTypeId int '@PostTypeId' , AcceptedAnwserId int '@AcceptedAnswerId' , ParentId int '@ParentId' , CreationDate datetime '@CreationDate' , DeletionDate datetime '@DeletionDate' , Score int '@Score' , ViewCount int '@ViewCount' , Body nvarchar(max) '@Body' , OwnerUserId int '@OwnerUserId' , OwnerDisplayName nvarchar(max) '@OwnerDisplayName' , LastEditorUserId int '@LastEditorUserId' , LastEditDate datetime '@LastEditDate' , LastActivityDate datetime '@LastActivityDate' , Title nvarchar(max) '@Title' , Tags nvarchar(max) '@Tags' , AnswerCount int '@AnswerCount' , CommentCount int '@CommentCount' , FavoriteCount int '@FavoriteCount' , ClosedDate datetime '@ClosedDate' , CommunityOwnedDate datetime '@CommunityOwnedDate' ) EXEC sp_xml_removedocument @DocHandle
Yep. Done this on a few contracts. We use a specific soultion in tfs so everyone can view and update when appropriate
First guess: with start_and_end as ( select a.customer_id, first_value(start_at) over (partition by customer_id order by start_at_time asc) start_of_day_loc, first_value(end_at) over (partition by customer_id order by end_at_time desc) end_of_day_loc where start_at_time between ... and end_at_time between .... ) select * from start_and_end where start_of_day_loc = 'A' and end_of_day_loc = 'P'
Easiest dirty way is to do 10 queries and just do date &gt; first day of the week AND date &lt; last day of the week and then just shift the window of days ten times (one for each query) probably take ya 10-30mins
Where salesdate between dateadd(w,-10,getdate()) and getdate()
Create a date dimension table, Google dimdate for some scripts that'll do it, then just join to that on the date and pull the week of year to group on.
group by solution is good (i love date tables too) but you forgot the where clause
where clause looks good (although i always shudder at using a "zero date") but you forgot the group by does this where clause calculate whole weeks, or is the earliest date the same day of week as today?
just... no it takes less time to do it properly, and you won't have to repeat the damned drudgery a month down the road
The earliest date is the first day of the current week and it's not a zero date its adding zero to the week in the date add. What's the aggregate you want? Can't write a group without an aggregate. I would do that calc for the earliest date on all the sales dates as part of the query return and you'd have a column that only had the earliest date of all ten weeks in it.
&gt; What's the aggregate you want? not me, OP -- `SUM(sales)`
&gt; and it's not a zero date its adding zero to the week in the datediff and how does the datediff work? it calculates the number of weeks between the zero date and today
I don't think at was that clear all sales activity grouped by week. That could be counts, totals, revenue, profit... Etc etc
Depending on what you're doing, one of the following can work in many scenarios - Providing an API to pull data from, rather than having everyone running their own queries - Adding standardized views which you can optimize and maintain, and are available to all developers as the "core" of new queries Obviously sometimes people just need to be able to hit the database for something niche, but by having standardized views which are properly maintained and optimized, and kept up to date on all clients, you at least have a solid foundation for most queries to work on.
In this case it's actually deterministic, it will always return the same input. It is generally non deterministic which is what I assume you mean by "zero" but in the case of adding dates it becomes deterministic.. i.e. any input always returns Monday. Dynamic query writing shouldn't make you shudder, it allows you to repeat without any rewriting
Sql containers, git, and devops
That's a lot of records but not unusual for a CRM to handle. I think the Salesforce limit, for example, is users, not number of records.
In this instance, yes, but since it's homework I assumed I might lose points if it wasn't generalized/applicable to something else. We also learned about DISTINCT this week, so it can't hurt anyway ¬Ø\\\_(„ÉÑ)_/¬Ø
&gt; Dynamic query writing shouldn't make you shudder you're not listening... i said a "zero date" make me shudder perhaps you do not even understand how the solution you posted actually works
I‚Äôve got a team of five with pretty similar problems historically. Need some source control and easy viewing options for it. Git + your preferred mgmt tool will be an awesome place to start. Github is fine, Gitlab or VSTS/ Azure Devops will give you a bit more mileage with some small cost.
&gt; We also learned about DISTINCT this week, so it can't hurt anyway ¬Ø\_(„ÉÑ)_/¬Ø sure it can DISTINCT works by an *extra* sort step, which sorts the query results **by all columns**, and then compares every row to the previous row, **on all columns**, to see if there are any duplicate rows for a large results set, this could kill your performance please do NOT just throw DISTINCT around because you think it can't hurt
I definitely do. Not looking for an argument. The getdate() and today() in this case are being used deterministically
Have procedures in place to save everything in db views and use information scheme views? If you create a dedicated database for this and make it part of regular backup schemes you may have just enough to work with.
The Ultimate MySQL Bootcamp on Udemy, taught by Colt Steele, is an excellent way to learn SQL. It's one of the best online courses I've taken so far.
&lt;voice type=arnold&gt;it's not an argument&lt;/voice&gt; here's the syntax, from the manual -- DATEDIFF ( datepart , startdate , enddate ) This function returns the count (as a signed integer value) of the specified datepart boundaries crossed between the specified startdate and enddate. here's what you wrote -- datediff( week, 0, getdate() ) please, explain the startdate of "0", and then how the datediff works
That's part of what I wrote. Also the zero value of a date isn't zero at all
Why can‚Äôt you just use Report Designer and Database projects in Visual Studio via SSDT and then push the solutions up to whatever you want to use as your source control?
Because your index includes all columns that make up the table it's just as big as the entire table. It takes just as much reads to scan the table as it does to read the index.
But it doesn't have to read the entire index. Since the index is sorted by `a`, it can quickly determine the boundaries of index rows for each distinct `a`, and then count the rows by simple arithmetic.
Also, can more than one trip end at P in the course of the day? E.g. P is at the top of a hill, so the data shows one trip from A‚ÜíP, then perhaps they walked/biked/skateboarded back to A, then took another trip from A‚ÜíP later in the same day.
Your answer can be [found in this article](https://www.citusdata.com/blog/2016/10/12/count-performance/) Specifically, this statement: &gt; However both both forms of count(1) and count(*) are fundamentally slow. PostgreSQL uses multiversion concurrency control (MVCC) to ensure consistency between simultaneous transactions. This means each transaction may see different rows ‚Äì and different numbers of rows ‚Äì in a table. There is no single universal row count that the database could cache, so it must scan through all rows counting how many are visible. Performance for an exact count grows linearly with table size.
If there isn't enough memory, the Postgres will probably switch to a nested loop to begin with. If it chooses a hash join and that doesn't fit into memory, it will store intermediate results on disk. If you run explain (analyze) you can see that.
popsql is (apparently) an editor. you have used to have (apparently as well) a local DB server running on 3306. Re-start the server, then re-try connecting.
Reflect everything you do in Jira tickets. Tag them appropriately. Structure your coffee files, document them with comments and markdown files with the same name if necessary. Commit your changes to git, have every commit message start with the Jira ticket ID. Configure git integration for Jira. When you need to do something that might have been done before, search in Jira by tags and text description. If you find the ticket, you will find the commits associated with it. You can then create a linked ticket and make your changes. With these principles we manage a huge code base quite successfully.
In MSSQL world I've used MDS (though it doesn't get much love any more, it's pretty feature rich) and PowerApps for this.
Comparing query plans between the environments would be a good place to start IMHO.
If there's an ERD or other documentation, I'll refer to that. If not, I'll generate an ERD based on the key constraints which exist in the database. If things are really bad, I'll ask whoever developed the database and/or examine its codebase, use queries to test any assumptions I'm making. At least on MSSQL, object dependency trees are maintained on the database, which is very helpful when trying to map data flows.
Hm, this is something I was thinking about so excited to hear about what some people's solutions are. Part of this is why I used Looker. It allows us to create a model file to define all of the joins, while also allowing full control of the dimensions and measures. I always wondered what an alternative would be with something like Power BI or just pure SQL.
Does postgres have any sort of "gather statics" functionality? In Oracle we use gather_table_stats to create the needed metadata. This is automated but for newly created objects developers have to gather starts manually.
I'm still learning so I could be completely off, but I thought batch statements run serially. Is it possible that because the select statement is used in a previous batch before the create index statement that you would have a table scan on first run?
Might need From databasename.dbo.Invoice
For the second part, add a statement to your where clause - And invoicedate &gt;= date(getdate())-90 Or Date_part(‚Äòmonth‚Äô,invoicedate) &gt;= date_part(‚Äòmonth‚Äô,getdate())-2
The date part could become an issue if you run into data with over a year's worth of data. Ex: you're only looking at data from March-May, not necessarily the year. In addition to the month, I would also make sure that the date part of getdate = date part of the year of the invoice date.
Not sure of that flavor of SQL, but since you aliased the date part in the select, would you need to specify it in the group by? So you would just say GROUP BY m. Just a guess, as I only really work in SQL Server
Yeah, fair
It does. pg_tables contains a tuplecount field
IME the most scalable solution is to make all reports, etl's and ad-hoc queries shareable and searchable through a tool that centralizes all those use cases. Git and so on are nice but cumbersome for ad-hoc work.
Do you need to put 'month' in quotes?
Also Germany in double quotes I believe should be single quotes
table name is MENU
Select country, min(price) [least] From MENU Order by country
Yeah, the answer is what /u/herdnerfer said, but in more pressing matters, who thinks Hawaii is a country?!
You haven't REALLY given enough information to properly answer your question, we don't know what columns are in MENU; However from context I can hazard a guess. SELECT COUNTRY, MIN(PRICE) FROM MENU WHERE FOOD_NAME LIKE 'Pizza' GROUP BY COUNTRY; Where PRICE is the column that lists the price and FOOD_NAME is column that lists the name of the food.
Thanks for the pointer. I will have a look a this today.
"Check the table doesn't extend off the right side of your page" **LOL** so, what have you tried so far√â
Send me a pm
Shoot me a PM as well. Would be happy to answer any questions for your class and for your own knowledge about career working with SQL in general.
It won't bog down your machine. The services don't do anything if they're not being asked to do anything. It's easy to write a batch file or a PowerShell script to start and stop the services if you don't believe they're dormant when idle. Or, if you insist, build a VM. Whatever you do, it seems like you should get around this excuse and get to learnin'.
Install docker and run it in a container. When you don't need it, turn off docker.
yes daddy. installing now.
Great! I hope you'll post if you have any specific questions. There's also r/SQLServer.
I actually do use VMs for this. I have four Windows VMs - DC, MS SQL, Web Server, and a client. I run Linux so I need this to test Kerberos and other facets of MS SQL on active directory.
This is sound advice for most servers or things that could potentially let someone access your machine
I use a vm becuase it let's me take my environment on the go so when I'm out with thr laptop I can just copy the vm over.
What is the key to join on between the two tables? What specific columns are you trying to aggregate? Or are you just trying to count number of rows?
It doesn't seem these tables have any columns in common. Is there an additional table that you can join on that has tables in coming with both the payments and the people list?
Even ignoring your original performance concerns, there's other benefits to using a VM for this kind of thing: * Whole systems can be snapshotted and cloned - gives you a lot more room to try new things with less concern over breaking something * Doesn't mess with your main desktop config * Not affected if you reinstall your desktop or switch host system * Can test networking related issues to SQL, seeing it's a separate host * If your production servers would likely be served from 'Windows Server', you're doing your learning/testing a closer match to production * Outside of SQL skills: Learning more about VMs and Windows Server in general So generally a VM is going to be better in every area, except performance/resource usage.
Thanks for targeting and showing your hatred towards India. You keep on spitting same message without reading the content considering yourself at God level. Collecting resources and presenting them in meaningful way seems copy pasting to you. I appreciate your pshycic behaviour towards India as whole country just because you didn't get what you wanted at some low price. Just browse about progress made by India and update your knowledge before targeting it for your personal benefits and attraction.
Use a VM. It‚Äôs simple and easy.
[removed]
I was going to grump about the futility of cheat sheets and the dangers of getting a job you're not actually qualified for but then I realised that wasn't a danger. &gt;All modern database management systems like SQL, MS SQL Server, Oracle and Microsoft Access are based on RDBMS. .... &gt;SQL clause is defined to limit the result set by providing the condition to the query. .... &gt;The fields in a view are fields from one or more real tables in the database. .... &gt;MySQL is a relational database management system, used to manage SQL databases, like SQL Server, Oracle, IBM DB2 etc. ... &gt;Constraints are used to specify the limit on the data type of the table.
useful link, thanks
VT-x / VT-d and secondary hardware to hand-off to VM more or less mitigates on modern HW. It's no good for having multiple VM's AFAIK, but if you just want one, it's pretty nice.
 UPDATE &lt;table&gt; SET GameOver = true WHERE TotalScore = (SELECT TotalScore FROM &lt;table&gt; ORDER BY TotalScore ASC LIMIT 1); That should do it. Explanation: We search over the table for every player whose total score is the same as the lowest found total score.
You could just use a subquery e.g. UPDATE Table SET GameOver = 1 WHERE TotalScore = (SELECT TOP 1 TotalScore FROM Table ORDER BY TotalScore)
You can use Row number, or Rank Rank deals with ties, and will assign the same number foe equal scores
&gt;thank you, its worked =)
thank you too =) its working too tested both
Why the hell would anyone down vote my comment? That is literally the perfect solution.
Sometimes reddit auto downvotes for some reason, so it might just be that.
Right now I am using the RedGate tool suite and liking it so far. Check out a trial and see if its right for you.
We've got Idera but it's not super impressive. Basic functions like silencing alerts kind of buggy. The installation and registration is also a pain. Don't think I'd recommend it.
We just started using the SentryOne product. There‚Äôs a ton of data in there, but I‚Äôm not quite sure how to use it. I‚Äôve used spotlight in the past and love it.
Can you explain this further please? Or link me? I I only run my learnining enrionment in VM and the performance sucks sometimes.
We are using SCOM in combination with are own audit system that is based on sql broker.For example when something happend to sql server like creating database it is writen into event log on server like error and than scom alerting us by email. And SCOM has his own alerts that audit us about state of the disk space, conditions on server and etc.
VT-d &amp; VT-x are processor options that are uefi / bios enabled. What virtualization software are you using? I personally like Vagrant, but it's more *nix focused so won't translate well to windows (although VT-x / VT-d will, you just need virtualization software that takes advantage + those extensions turned on if you have them). Google for your machine and vt-d / vt-x.
I‚Äôm using spotlight cloud. The mobile app is great and I love the fact that data is collected and stored in the cloud. Even. If my server goes down. I can replay everything up to the point of failure. The dev team had two meetings with us to show off the product and get us up to speed with our own data. It‚Äôs been fantastic and is only $500/year.
oh, this probably wants to be changed a bit, since we probably only want to see players _still in_ the game? i.e. UPDATE &lt;table&gt; SET GameOver = true WHERE TotalScore = (SELECT TotalScore FROM &lt;table&gt; WHRE GameOver = false ORDER BY TotalScore ASC LIMIT 1);
I inherited SQL SentryOne from a previous DBA. So far, not a massive fan of it, too 'busy' in my opinion.
The ui needs refinement for sure. Spotlight wins that over just about anything.
ApexSQL
Thanks for your answer, as I mentioned, the SP only takes about 10 to 15 seconds to run, but, since it's running through Excel (which kinda works as our "visualization" tool), it's way too much time (Even sometimes making the vba command expire)
Idera &amp; Zabbix for the server level WMI stuff. Query Store and Extended Events for query monitoring.
I'm using voice vmware. I'll Google it and check my bios. Thanks for thr help.
I had the same view of SentryOne. I do like Solarwinds SAM for basic info and alerting capabilities in a readable modern format. It doesn't drill too deep unless you break the bank for Solarwinds DPA, though.
Redgate SQL monitor ?
We use SentryOne too. I don't use it personally as I'm not really a DBA, but our team seems to like it.
What platform are you using? If it‚Äôs SQL Server, why not just create a DB level job to updates your average star rating every day/hour/minute? Although personally i‚Äôd stay away from every minute jobs unless it‚Äôs a very small DB. Also, again if SQL Server, i would stay away from Triggers they are hard to track if there is a issue and completely Global (they fire no matter why type of table activity there is).
I'm using InnoDB and unfortunately the hosting I'm using doesn't allow me to create a job. I'd stay away from those as well. What do you mean about issues with triggers?
Why don't you just create a view over the table with the calculated field in place?
You‚Äôre going to want a scheduled task of some sort for this. I‚Äôm not familiar enough with innodb to help out too much. Cron Job for Linux, System Task for windows. Stay away from the trigger, it seems like a good idea, and will be fine in testing but will not scale well. At first the DB will not have any reviews, so it‚Äôll be quick. But each review will need to be scanned on every insert. Eventually, you‚Äôll have 1000s of reviews getting scanned on every update, and records won‚Äôt get returned until the scan is completed. Real World experience.... I had a ‚Äòdashboard‚Äô that would calculate some statistics on every page load. It was fine at first, but as the DB grew, users were waiting 30-45 seconds for the real-time totals to come up. The users never looked at this dashboard, and it could have been refreshed and cached hourly or placed on a separate page if they really wanted it. They didn‚Äôt have the option of opting out, and suffered for it. We took the dashboard off and page loads were less than 5 seconds. This is a great case to think about how the data is used. You want reviews to be painless, so don‚Äôt burden the user with your maintenance tasks. Depending on your volume, once an hour is a start, once a day is probably enough.
Is this average going to be for reporting? If so, I'd just do the calculation on the front end. No need to store it. Otherwise your options might be a trigger (most synchronis) or a job set to a schedule. Another option is a calculated field and just keep the data in the users table.
A scheduled job would be more ideal but if you really can't do that, maybe you could attempt an incrementally calculated average with triggers. The idea is to make your mean calculation scalable by not needing to recalculate the entire mean every time. Given the running mean thus far and the number of terms you can calculate the new mean in constant time - https://math.stackexchange.com/questions/106700/incremental-averageing FWIW Oracle's materialized views support something like this out of the box with its "fast refresh" feature, but you're not using Oracle so I won't explain in detail.
If you are interested hit me up. I work at redgate and would be happy to set you up with a trial üëç
+1
Seems like either Brute Force using REPLACE and some hardcoded characters. OR A lookup table of two columns. First column e-accent-aigou, second column e. First column u-umlaut, second column u. Then loop through the table and dynamically build your SQL statement again using REPLACE.
Will keep an eye on this thread!
Codeacademy has a really good rudimentary course on manipulation, aggregate functions and queries. I‚Äôd start there then move onto picking up sql books from the library then download a DBMS to practice on your desktop
Agree. Plus its graphs are unintuitive, imo. I should be able to change the date range displayed on any graph by right clicking on it and selecting from a menu. IIRC, it's possible, but very convoluted.
Also Udemy.com can help you
set up a database on your own computer, and run queries against it tourist in NYC: "excuse me officer, could you tell me how to get to Carnegie Hall?" officer: "Sure... practise, practise, practise"
Sqlite.org has pretty decent tutorials. Just set up a local db and start following their tutorials.
https://sqlbolt.com/
Are you looking to learn basic SQL commands or are you looking to learn SQL in the context of a specific platform (Microsoft SQL Server, Oracle, MySQL, MariaDB, etc.)? What platform do you plan to use for learning?
https://sqlzoo.net/
I‚Äôm installing a copy of Server 2016 and SQL Express 2017 and the AdventureWorks database. After that I hope to get some study materials related to the MCSA cert, haven‚Äôt done much research but after having a year of SQL experience I don‚Äôt think it should be too difficult.
If you're trying to count drivers rather than rides, a couple of things: count(distinct rides.driver_id) total_drivers without distinct you'd be getting the number of rides. exists (select 1 from drivers where rides.driverId = drivers.driverId and completed &gt;= 1) You might as well just replace this with a WHERE is_completed = "Y" But you've also got a column for not completed, which doesn't appear to be in the question. Using WHERE is going to be more effective, but of course you wouldn't be getting the incomplete rides for drivers who had a successful ride. If you do continue to use the EXISTS option, you need to change it: at present it's only filtering out those drivers who have never had a completed ride, rather than just those who haven't completed a ride in that week.
Very very boring but very very good if you're serious about learning https://www.youtube.com/channel/UC5ZAemhQUQuNqW3c9Jkw8ug
https://www.youtube.com/playlist?list=PLFt4E5CtqucwbD_JY0ljS0VWu7Pfwoeu1
Thanks so much for the help. Initially I had thought just doing where is_completed = "Y" wouldn't work but thinking it through again it does. So I don't need the case when and exists sections
Kudvenkat on YouTube. 150ish parts, each 20 mins long or so. Totally free.
My organization recently migrated to Salesforce so Salesforce is the immediate platform I'm using but I also use Tableau which has a lot of SQL-like properties.
I‚Äôd recommend picking one tutorial to start, doing it completely in one week then choose another the next week repeat until your a SqL wizard.
If your org is using Salesforce, it might be worth your time to spend most of it on Salesforce's free platform, Trailhead. It's a great resource.
Our database was offline for a while but I spent the weekend finally building an all new infrastructure. It's not as nice as our old one due to money constraints, but at least it's available again.
I know the OP asked for beginner SQL resources but what resources are useful for someone who already has the basics down?
I'm a big fan of sqlservercentral.com. They have super helpful forums and this section called Stairways that will teach you a variety of subjects.
Are you me? Right down to the teaching thing.... At any rate, following this thread.
Pretty good starter guide here: [import or link data in Access](https://support.office.com/en-ie/article/import-or-link-to-data-in-an-sql-server-database-a5a3b4eb-57b9-45a0-b732-77bc6089b84e)
Thanks! I'll give it a look
Perhaps just write a basic import process where they could supply/import CSVs with the data to change.
As a beginner, I found Strata scratch the best way to learn SQL. The best part of this platform is they provide datasets with the questions and answers we can practice with. And all the questions are taken from top tech companies and universities.
Well shit I‚Äôm gonna check this out tomorrow when I get to work, thank you!
umm sorry but i have log table that saves player id, total score itself for tours can i say if second player had more score than first then gameover for first player ?
A basic PowerApp would work well for this.
I don't understand, can you re-phrase?
i have two person that played 2 tours Name | TotalScore | TourID Test1 | 25 | 1 Test1 | 35 | 2 Test2 | 10 | 1 Test2 | 20 | 2 and this third tour they have same lowest score 20 can i tell if test1 totalscore more than test2 make test2's gameover true and let test1 continue game
in MySql you could do: SELECT Name, sum(TotalScore) FROM scores GROUP BY Name; and then some ordering if you wanted to see the top/bottom player.
I'm giving a SQL Training to an intern at work today, and this website is the perfect tool for practice with him. Thank you!
It wouldn't be overly complex to do. The simple answer would be to build a case statement and create a separate case with REPLACE for each "expected" entry and what to replace it with. &amp;#x200B; SELECT CASE WHEN First\_Name like '%√´%' THEN REPLACE(First\_Name, '√´', 'e') ELSE First\_Name END AS New\_Description FROM "Your\_Table" &amp;#x200B; From there, just add a entry in the case as you come across "unintended" values. It could be managed as a view overlaying the actual table, the alternative would be writing a stored procedure that does the replace against the actual data in the table. I personally would prefer a view as it leaves the original data intact.
yes its worked thank you =) practicing with sql queries and you all helped me a lot
Might need a little clarification. What would be defined as a successful or failed project? Just meeting the goal? Is how much over or under the goal is a factor? I think the simple answer would be if you're looking for the number of successful projects by type would be just to do a count of each type of project, then divide the number by the total number of all projects to give you a percentage.
Oh and I guess going further, you could split the projects into success or failure, then rank the project types in each category.
By the amount spent.
They do - I was able to join them.
I think you're on the right path. A unique player id that joins to the playerid in the character table would be what I would go with. You could even build the index on the player table based on playerid.
So it really comes down to your definition of the entity? For example what if you wanted to store things that had legs and named that table ‚Äúthings with legs‚Äù.
You could borrow the book *SQL Cookbook* from the library, or buy it for $20 or so. I've made a whole career out of what i learned from that book.
&gt; what if you wanted to store things that had legs and named that table ‚Äúthings with legs‚Äù. that seems... unrealistic
 [http://www.sql-ex.ru/](http://www.sql-ex.ru/)
I think your description of the last table is inaccurate? What columns does it contain?
I had the same issue and ended up building a quick MVC app to handle the request. Now they go to a website. It was easier than I thought it would be. https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/database-first-development/creating-the-web-application
Jose portilla course is good
Trust me when I say this but you wont find a free site to learn better that this: ( [https://www.sololearn.com/Course/SQL/](https://www.sololearn.com/Course/SQL/) ) Good luck
Select tbl1.[name], (isnull(tbl1.[count],0)+(isnull(tbl2.[count],0)+(isnull(tbl3.[count],0) From tbl1, tbl2, tbl3 Where tbl1.[name] = tbl2.[name] And tbl2.[name] = tbl3.[name] And tbl1.[name] = tbl3.[name] Group by tbl1.[name] You didn‚Äôt include table names or column names so you‚Äôll have to play with it but this is one way to do it Sorry for any typos, replying by phone
First of all, it seems awkward to have three different tables for this. Use one table and then a column to indicate if the match was Center, Ass1, Ass2. &amp;#x200B; But if you have to have three tables. Do a UNION ALL on all three tables and then group by the name and then sum. &amp;#x200B; I don't want to type it out since I would have to assume a bunch of things about your tables. If you would have posted CREATE TABLE and INSERT statements so we can try it out easily you are likely to get better advice.
Omg yes that makes sense! I agree that the 3 tables is awkward but the database could only let me export it in that format. Thanks!
Thanks!
In this case it might be important to know the difference between UNION ALL and regular UNION. You want UNION ALL. Look it up if you don't know the difference.
If you do create tables you copy the data. If you really need to do it that way use views. You can just use a spreadsheed to generate the DDL-SQL. But also if you have access to a database and can create tables you definitely should have some better way to get that data.
Interesting
This is under the assumption that all referees are in each table...which may not be true. As another person said, you'll want to union all.
Can you try removing the first character, remove last character, and the convert results to an int and go from there? Assuming that all values have letters wrapped around them
Fair enough but for me, something similar actually caused this issue and I can‚Äôt really tell but people are saying it‚Äôs an EAV. Name, type, values Where name is the name of some file on the file system, type is considered training, test or validation set. I just don‚Äôt see how they can be considered a EAV. Now I had another case where I did have attributes I wanted to keep that were all different datatypes and varrying number. So the ‚Äúattributes‚Äù were items like epochs, batch, learning rage, etc. Each of those have their own datatypes. So i couldn‚Äôt say them in a single column/table and could see how they would be an EAV at that point. The other one I just don‚Äôt see how it could be an EAV.
That‚Äôs what I thought of, however I‚Äôve been unable to find a satisfying explanation of how to go about this. I can not remove the first and last character from the database entirely, and I‚Äôm not sure how to make an instance of the string with only the middle 3 characters that I could throw to a cast function. I assume the end result will have to look something like this: UPDATE table SET status = ‚ÄòDeactivated‚Äô WHERE (CAST([int part of code] AS int)%2 = 0) AND *other conditions*; I just am not sure how to get the [int part of code] bit down.
A quick and dirty solution. If the all follow the same format as the example substr(ID,4,1) in ('2','4','6','8','0').
Rarely mentioned but I found mode's intro to SQL quite useful: &amp;#x200B; [https://mode.com/sql-tutorial/introduction-to-sql/](https://mode.com/sql-tutorial/introduction-to-sql/)
If it's always the same number of leading and trailing alpha characters you could trim the lead \[A-Z\] and trailing \[A-Z\] and cast it as an integer and delete the ones that can be divided by two.
This is probably the simplest way to do what I want since I don‚Äôt have to do any long conversion process, Thank you!
Homework problem?
Something like this DECLARE @minID int, @maxID int SELECT @minId = MIN(ID), @maxId = MAX(ID) FROM Table WITH numsequence(Number) as ( Select @minId as Number union all Select Number + 1 from numsequence where Number &lt; @maxID ), leads AS ( SELECT ID, LEAD(ID) OVER (ORDER BY ID) AS LEADING_ID FROM Table ) SELECT number FROM numsequence WHERE number &gt; ID AND number &lt; LEADING_ID
Regular expressions to the rescue. The syntax will vary depending on what system you're on, but most implementations have a function like regexp_replace() that lets you solve this and many other string manipulation problems. In PostgreSQL you would do: ...where regexp_replace(hole_id,'[a-zA-Z]*','','g')::int%2=0 This may be different on another platform though, so for an exact answer, you'd have to let us know what you're running on.
This solution will break if you have holes like 'AB004A'
Have you ran the script without the TOP in it? Maybe the first 20000 rows are actually 01/01/2019
Can I PM you? I have some questions about working with SQL career wise.
Sure
does BETWEEN in your Where clause make a difference? As &amp;#x200B; where LIN.[BUS_DAT] between '2019-01-01 00:00:00' and '2020-01-01 00:00:00' Also, can you add another Clause for the Dates? As &amp;#x200B; and PS_TKT_HIST_LIN.Date = PS_TKT_HIST.Date --idk the name of the date column ".date" is a filler
&gt; The other one I just don‚Äôt see how it could be an EAV. yeah, i agree
Luckily the holes only go up to Z and do not go into double letters. I saw your solution in your other comment and while it is likely a better option in general I am pretty new to SQL so for right now this solution has a simpler syntax for me to follow and coincidentally I can actually edit it to fit my other problems (not all the holes are even numbers and not all even numbered holes need to be deactivated). there‚Äôs just blocks of the table that fit into patterns so I can change them without having to do it manually. For example this syntax also gave me the solution for deactivating holes A129A-A139G since I could apply it to any part of the identifying string.
That suggests in your data, there are multiple matches in one of your joins. An easy way to figure out what is going on is to first run: SELECT COUNT(*) FROM TKT_HIST_LIN then SELECT COUNT(*) FROM TKT_HIST_LIN INNER JOIN IM_ITEM ON TKT_HIST_LIN.ITEM_ID = IM_ITEM.ITEM_ID then SELECT COUNT(*) FROM TKT_HIST_LIN INNER JOIN IM_ITEM ON TKT_HIST_LIN.ITEM_ID = IM_ITEM.ITEM_ID FULL OUTER JOIN TKT_HIST ON (TKT_HIST_LIN.BUS_DAT = TKT_HIST.BUS_DAT AND TKT_HIST_LIN.STR_ID = TKT_HIST.STR_ID) Then you should be able to tell which join is causing the extra records. You'll probably need to reevaluate your join logic at that point. At an uneducated stab in the dark guess I'd say it's probably your TKT_HIST join. Are you sure you want to include every record in TKT_HIST even if it doesn't match anything in TKT_HIST_LIN? You might intend this to be a left join rather than a full outer join. e: by way of explanation http://sqlfiddle.com/#!18/3632e/2 - I've added a record to TKT_HIST that's completely unmatched to the other records, it still shows in the output.
5 and 6, but not 3? Anyways, look up "gaps and islands".
What do the execution plans look like before/after? What is the definition of the view? What is the underlying schema? What does the query look like? Has the hardware / server configuration changed? There can be lots of reasons for different performance, difficult to make a diagnosis without more information.
Both in your example and in my example, I somehow missed that I'm getting duplicate results on the SALES column. 14.22 repeats itself twice, when there is only one entry.
XD SQL claus
neat way to automate queries! unfortunately, in a company environment IT depatment will want you to use company approved software/tools and in my case these are sql developer and ssrs.
Cool man. If it works it works :) edit: for something like manipulating holes A129A-A139G you can actually do: ... where hole_id between 'A129A' and 'A139G' and in most systems it will work exactly like you'd want it to.
/u/distraughthoughts helped me!! thank you guys!
Man I wish, I haven‚Äôt been as specific as I should have been but A129A-A139G was meant as only A-G on all numbers between. And unfortunately A129H-Z comes before A130A
Its sounds like great idea and would definitely help me at my job but unfortunately my company would never approve something like this.
Not to mention all security rules, especially when working for FI
You are getting duplicates because of the key you are using in your JOIN. Your fiddle gave a great example of this, not just your $14.22 but your $12.21 (100 for both) was duplicated as well. The execution plan will show you why this happens. This might not be what you want to hear, but because you have duplicates that can occur due to not having a unique key, you will need to weed out the duplicates using `DISTINCT` . Depending on which version of SQL you are using, you could also use a window function to remove these duplicates instead of using `DISTINCT`. My suggestion would be to remove the dual key join and stick with just the store ID. SELECT DISTINCT TKT_HIST_LIN.BUS_DAT , TKT_HIST.STR_ID , TKT_HIST.SALES , IM_ITEM.ITEM_DESC FROM TKT_HIST_LIN JOIN TKT_HIST ON (TKT_HIST_LIN.STR_ID = TKT_HIST.STR_ID) JOIN IM_ITEM ON TKT_HIST_LIN.ITEM_ID = IM_ITEM.ITEM_ID Also, please don't use `TEXT` . Use `VARCHAR(MAX)` or some form of it if you can . It will save you a lot of time/hassle in the long run. Just my 2 cents.
Why are you splitting the data? The `CREATE EXTERNAL TABLE` should be able to handle the 1 million rows easily. It will just be a large CSV file. Other option would be to use the CLI, but that is much slower. Also, depending on which application you are using to connect, you can utilize their built in systems for exporting to CSV. SSMS for example allows you to export results to a file. If you can't use either of those, my suggestion would be to move outside of SQL and use Python / JS(node/?) / C(++,#) / Java to extract the data. Using any of these, you could either iterate by using a simple loop, or take a stream and pass it to a CSV.
Is there a way to automate them in G-Suite?
I was trying to figure out how re-racking your servers could have broken the data in your data warehouse for like a minute before I realised you were talking about data about a physical warehouse.....
Haha sorry, I definitely worded the post poorly. I was just trying to find a general solution to the problem and I‚Äôm not really supposed to be on reddit while at work so I wrote it kind of quickly.
Depends on the site what "cant reach the page means" i dont understsnd your question
If it‚Äôs MS SQL Server there is the statement WAITFOR for the poorest man‚Äôs solution.
Google tabibitosan
You may be able to take advantage of some sort of region normalization function used in sorting. Oracle lets you strip diacritics using NLSSORT()
\*My final query here : I'm selecting top lowest scored players and displaying in my DataTable then giving them another chance : Thanks all who helped =) &gt;`select [Name],[TotalScore] from Participants where CompetitionID='"+UserInput.ActiveID.activeCompetition+"' " +` &gt; &gt;`"and TotalScore=(select top 1 TotalScore from Participants order by TotalScore) order by TotalScore ASC` Name | TotalScore Test 1 | 23 } Test 2 | 55 }-------&gt; Test 1, Test3 for Duel Tour Test 3 | 23 }
Thanks for the comment above. However I still had the same issue so kept looking at this and managed to find the cause of my issue. I needed to install the SQL Server Native Client 11.0 and used a UDL file on each system's desktop to check the connection was allowed between them. After doing this and then Dropping the end points again and recreating the mirror it finally worked. &amp;#x200B; Only other note is that I needed to set up the mirror from the Principle to the Mirror server first before I could then add a Witness server to it (as I was also attempting to add a Witness).
Looking through the WayScript site. There is [a whole page on Google Sheets.](https://wayscript.com/documentation/module/google_sheets) It looks like the site is still in closed beta, but I'm pretty impressed with how this looks.
Might be handy for personal projects but just a friendly reminder - don't use third party tools when working with your company's data.
There is a larger list of html symbols [here](https://www.w3schools.com/html/html_symbols.asp), but that's as useful as I can be.
https://stackoverflow.com/a/48399700/3303251
Once your query has GROUP BY, you can't have any column that isn't either a statement in your group by or within an aggregate function. Remember, a GROUP BY changes your query to be "One row per X", where X is the columns you're grouping by. So your query is one row per country. What's the "customer first name" of Canada? The question doesn't make sense, which is why the query doesn't work. Instead of grouping by country alone, group by customerid, customer.firstname, customer.lastname, AND country, and store it all in a temp table (or use a CTE, but you said "I need to use a temp on this one", so it sounds like that's what your lessons have focused on.) Then, have a second temp table / subquery START with that result set, group it by country, and return country and the MAX of the invoice sum column. Once you have that, your actual query should inner join the first temp table to the second on country and sum([invoice.total](https://invoice.total)), so that it limits the rowset to just those customers who have a highest SUM([invoice.total](https://invoice.total)) for their country.
The times SSMS tends to freeze on me all have to do with the VPN and server connections, not SSMS itself.
Wow, thats great! Thanks.
IMO, your first task should be fixing your application so that no more HTML entities are stored in the database. You've got to validate your inputs. I would expect that you could run something like .Net's `System.Web.HttpUtility.HtmlDecode()` against your data before it's submitted. I suppose you might have a company whose name is actually an HTML entity, but I can't imagine that would be *that* common. I wouldn't aim for a comprehensive solution. Something comprehensive (comprehensive as in *actually comprehensive*) to fix your existing data is going to be very difficult because there's [an extremely large number of codes](https://dev.w3.org/html5/html-author/charref). There's at least two ways of representing any entity, too, and often three or more. For example, an ampersand is `&amp;amp;`, `&amp;AMP;`, `&amp;#x00026;`, or `&amp;#38;`. Furthermore, there are entity constructs for multicharacter entities. For example, `a&amp;#768;` represents `aÃÉ`, not `a~`. But `&amp;atilde;` also represents `√£`. Or rather, it represents the same glyph, but `aÃÉ` (the former) is two characters and `√£` (the latter) is only one. It gets really complicated really fast. Some RDBMSs can parse HTML entities with xml functionality, but they're not always perfect. For example, this works fine in SQL Server: select cast('Q &amp;amp; A' as xml).value('.[1]','nvarchar(max)' ); But this throws an error because it's not valid xml: select cast('Q&amp;A' as xml).value('.[1]','nvarchar(max)' ); Frankly, the easiest way to fix your existing data might be to just do a series of searches and to fix them as needed. These are the most common named HTML entities by far: SELECT * FROM CustomerList WHERE CompanyName LIKE '%&amp;nbsp;%' OR CompanyName LIKE '%&amp;amp;%' OR CompanyName LIKE '%&amp;lt;%' And this should find any decimal or hex coded entities: SELECT * FROM CustomerList WHERE CompanyName LIKE '%&amp;#[0-9];%' OR CompanyName LIKE '%&amp;#[0-9][0-9];%' OR CompanyName LIKE '%&amp;#[0-9][0-9][0-9];%' OR CompanyName LIKE '%&amp;#[0-9][0-9][0-9][0-9];%' OR CompanyName LIKE '%&amp;#[0-9][0-9][0-9][0-9][0-9];%' OR CompanyName LIKE '%&amp;#[0-9][0-9][0-9][0-9][0-9][0-9];%' SELECT * FROM CustomerList WHERE CompanyName LIKE '%&amp;#x[0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f][0-9a-f];%' And this should find anything left, with some false positives: SELECT * FROM CustomerList WHERE CompanyName LIKE '%&amp;%;%'
On the contrary; SSMS seems to be one of the most solid applications I've used. Usually if SSMS is being disagreeable, it's because the server instance is clogged with traffic..
Something like that could work, I'd recommend to check out the [REPLACE](https://dev.mysql.com/doc/refman/5.5/en/replace.html) command though.
I'm going through Colt Steele's MySQL course and it's been super useful.
Which version of SSMS?
Not every time, but often. Even at home when it's a local install and there's not even 1G data total in the 5 or 6 hobby db's on the server. But M$ claims it doesn't happen so, I must be wrong.
This is a job that is usually done by a DW. Everything to do with scheduled queries, load jobs, metrics etc. But it's cool that you can schedule a lot of things like apps to run instead of using chron
To answer your other question the new Azure Data Studio, while not a replacement for SSMS, is already damn good. I use them both; ADS for heavy report/script writing and SSMS for administration tasks. Worth checking out either way and if that's also having problem connecting to the same source then you'll have ruled out a specific SSMS issue.
I input something to bypass login and the page loads then goes into a page that says cant reach page because proxy or connection. 3 bullet points pop up,I can screenshot! Curious why other bypass keys I use get rejected quickly but when I use 2 specific ones the site just loads for a long time then says cant reach page error.
Wow, that is significantly easier than what I was trying to do. Thanks for the input it looks to be working well!
but `REPLACE` works by doing a delete followed by an insert this isn't always desired if there are other column values that need to be preserved
there actually is a way to do this in MySQL that is rather elegant first, you need a UNIQUE constraint on your two columns then you use the `ON DUPLICATE KEY UPDATE` option of the `INSERT` statement INSERT INTO info_table ( GameNumber , PeriodNumber , GameDateTime ) VALUES ( '561665650' , '0' , '2019-05-29' ) ON DUPLICATE KEY UPDATE GameDateTime = VALUES(GameDateTime) that's it, that's all there is to it
It can be slow for me to boot, and freezes if I'm restoring a DB from right clicking the DB itself in Object Explorer (to work around this, I right click on the Databases folder under server, and populate the destination/source manually). Aside from that, its pretty damn solid. Must be something up with the environment.
Thanks, I modified your last script to render just the string in question. SELECT distinct substring([CompanyName],charindex('&amp;', [CompanyName], 1), charindex(';',[CompanyName], charindex('&amp;', [CompanyName], 1)) - charindex('&amp;',[CompanyName], 1)+ 1) FROM [dbo].[CustomerList] where CompanyName like '%&amp;%;%' with a few false positives.
I did not know that, but that makes sense. Thank you!
Are you aware of any performance gains from this method over doing an \`IF EXISTS\`? Regardless of that, this is definitively the more elegant solution.
@silvelix\_reddit if connecting the db to WayScript is an IT concern, you can setup a server endpoint that returns the necessary data (that way no other queries can be performed). Then, you can set WayScript to call a get request on the endpoint for the data on a timed schedule
Yes, you can pull data from the G-Suite as well, see the [relevant docs](https://wayscript.com/documentation/module/google_sheets)
Word. It's crazy how elegant SQL language is. Thank you for this.
Can do this same stuff on corporate server or local machine with Pandas Python library
I am a fan of DBViusalizer
haven't tested it because IF EXISTS is way more clumsier -- for one thing, you can only use it in a stored proc (hence OP's error "Unrecognized statement type"), and then you have to deal with the problem of passing in the values as parameters
Sorry to be the dummy in the room, but can you tell me what Forward Engineering means? I have an open source project called DataTier.Net that creates C# class objects and builds a C# stored procedure driven data tier and it works for SQL Server and Azure SQL, but I am not sure by your description if this is what you are talking about? Basically it is very similar to Entity Framework, except it uses all stored procedures instead of inline SQL. &amp;#x200B; Maybe if I knew what forward engineering means I would know if I just stepped into the deep end of things I do not understand. &amp;#x200B; Sorry if I did.
Just a heads up, SQL changes depending on what database you use. There are ANSI standards for how to do things, but not every database follows every standard. Once you learn enough in one database, it's great that you know you can do X, Y, and Z. Once you pick up a new flavor, you know about those results and how it was done before, now you have to translate that same need in a similar but different fashion. (This goes beyond syntax and also into how the RDBMS actually handles the SQL. Different engines can have different effects and outcomes. MyISAM to SQL Server are a great example because they are both incredibly different but use a lot of similar syntax.)
Well that sounds intimidating....thanks for the heads up
Why is this best practice? I am not a DBA but do work in the analytics space. Is the third party tools a security risk?
Yea. You're essentially handing over your data to a non-vetted website and placing it in the possession of whoknows.
What do you have so far?
select top 5 Customers.CompanyName, Orders.Freight as sumofsubtotal from Customers join Orders on Customers.CustomerID=Orders.CustomerID order by freight desc
it works, but still, I believe there must be another way to solve it. Its my second day with SQL and I got an assignment to solve 15 related questions
Any resources are highly appreciated
Database technology goes as deep as you could possibly want and then some. Great to make a primary career out of or even just gleam a few tricks and tips to help with a relatable position.
There likely will be. It looks like the aggregation of the totals is already done for you. You would need to do a SUM(totals) and a GROUP BY. You'll likely encounter them later in your excercises
SSMS uses a ton of memory. So if you have a lot going on it can start paging on you and just freeze for a minute.
A good example of this: Oracles solution would be a MERGE statement to fix OPs problem.
can you please check this out and let me know if I can shortcut some of those inputs? Link: [https://lh3.googleusercontent.com/-NxeOV87TCw8/XO63i7iOrKI/AAAAAAAAACI/Y9t2Gb-CzmEMshR7FaANCIJ\_-69otaNSgCK8BGAs/s0/Screenshot\_5.png](https://lh3.googleusercontent.com/-NxeOV87TCw8/XO63i7iOrKI/AAAAAAAAACI/Y9t2Gb-CzmEMshR7FaANCIJ_-69otaNSgCK8BGAs/s0/Screenshot_5.png)
SQL Server also has a MERGE function, but historically in SQL Server it is recommended to NOT use this function. (This could be completely opposite in Oracle, I have not done research for that RDBMS.) Things may have changed since the SQL Server 2017 edition, but I try to research functions and reviews of them before implementing into a production environment.
Yeah, my plan was to go down that route. It doesn't turn me away at all, I just wish it didn't sound as easy as the coursera class made it out to be, if that makes sense.
Depending on where you work, this might even be illegal, with a fine that can reach millions of bucks. EU-wide law known as GDPR states that giving personal data to any third party without the prior agreement of the people that the data is about is expressly illegal. Which means (and I asked a lawyer about this to be sure) that even translating an e-mail using services like deepl.com , if that e-mail contains any personal data (a name), is illegal. I mean nobody will probably notice it if you do it, but the risk is big enough to deter from doing it.
Your platform is Access. I'm not that familiar with access but I think this is right. `SELECT title, yearOfPublish, DATEDIFF("yyyy", yearOfPublish, DATE()) AS age` `FROM films` `WHERE DATEDIFF("yyyy", yearOfPublish, DATE())&gt; 25` `ORDER BY yearOfPublish ASC` &amp;#x200B; You're close. You can't use the alias of "age" in your where. You just need to put your age formula into the WHERE. Also I changed the sort order to ASC for ascending, oldest to newest. If your sort order used your age calculation you would use DESC since it would calculate the number of years old the film is and you would want the oldest (largest integer) at the top.
You're using Microsoft Access it sounds like which is a database system but is very different from MS SQL which is also known as Microsoft SQL Server or SQL Server, but you will find most people frown on DB Access due to it's features and scalability. Regardless of this, a lot of concepts are similar and the same. Here's what is probably going on: &amp;#x200B; Typical order of operations is: 1. FROM, including JOINs 2. WHERE 3. GROUP BY 4. HAVING 5. WINDOW functions 6. SELECT 7. DISTINCT 8. UNION 9. ORDER BY 10. LIMIT and OFFSET &amp;#x200B; SQL is based on English and is a declarative language. You tell it what you want it to do and it does it however it deems necessary. Example: Bring me the red book from the living room = SELECT BOOK FROM LIVINGROOM WHERE COLOR = RED. &amp;#x200B; How does SQL see this? 1. Where do I need to look? (FROM / JOINS) 2. What is my filter once I reach the place to look? Do I bring everything in or nothing? 3. Should I group these results once I have them? 4. Should I filter by my grouped results? 5. Calculate window functions. 6. Return columns you were asking for. Your select clause is creating a column "Age". "Age" doesn't actually exist until step 6 because you are calculating the age in the select. You can write something around what you have written and then filter. Example: SELECT * FROM ( SELECT title, yearOfPublish, DATEDIFF("yyyy", yearOfPublish, DATE()) AS age FROM films ) WHERE age &gt; 25 ORDER BY yearOfPublish DESC This is typically bad practice. You should apply filters as early as possible to restrict the data coming back. SELECT title, yearOfPublish, DATEDIFF("yyyy", yearOfPublish, DATE()) AS age FROM films WHERE DATEDIFF("yyyy", yearOfPublish, DATE()) &gt; 25 ORDER BY yearOfPublish DESC The above example calculates your clause twice, once in Select and once in Where. This is also "bad practice" because you are performing a function on a column which you are filtering. You should perform the function on the value you are comparing instead, example: WHERE yearOfPublish &gt; DATEDIFF("yyyy",DATE(),25) Notes: I am not fluent in DB Access. I am also not 100% sure on these facts: 1. Your select clause is creating a column "Age". "Age" doesn't actually exist until step 6 because you are calculating the age in the select. 1. I am not 100% sure it happens here, it could happen in steps 6 - 9. 2. Syntax above may still be wrong, concepts should be true however. [I have talked about similar concepts before but directly relating to SQL Server if you are interested.](https://jonshaulis.com/index.php/2018/12/25/eli5-how-are-join-statements-semantically-structured/)
SSMS doesn't seem to handle memory very well. If you routinely return large result sets in SSMS, you will eventually freeze and have to start over. Other than that I've always found SSMS to be very stable.
SQL Server would look like this: SELECT title, yearOfPublish, DATEDIFF(yyyy, yearOfPublish, GETDATE()) AS AGE FROM films WHERE DATEDIFF(yyyy, yearOfPublish, GETDATE()) &gt;= 25 ORDER BY yearOfPublish DESC
Thanks for clearing it up. The filtered table now shows the title, year of publish and the age! :) There's still a bit of a problem though; it tells me the age is '114' for every film, eventhough they all have a different year of publish. The table also shows some films from 2001, which is not older than 25 years. Do you maybe have any further solutions? :)
Thanks for clearing it up. The filtered table now shows the title, year of publish and the age! :) There's still a bit of a problem though; it tells me the age is '114' for every film, eventhough they all have a different year of publish. The table also shows some films from 2001, which is not older than 25 years. Do you maybe have any further solutions? :)
Try breaking it down in baby steps. Try just selecting the year published and title but only where the age is 25 years difference. Look at the ones coming back that were from 2001 and reverse-engineer the problem. Notice how I'm not suggesting to incorporate finding the age with your SELECT yet. You are returning two columns with a filter. This means there is a problem with the data or the filter statement and it needs some fine tuning. The next piece once that's fixed, is to incorporate your SELECT where you are calculating its age. I'm not familiar enough with Access to help here, but the syntax above would seem correct to me.
SSMS is one of the best pieces of software I've ever used. It's for sure the best thing I've ever used from Microsoft.
I bet your yearOfPublish is not a date field. I bet it is just the year, either 2019 or 19 for this year (I hope 2019). If that is the case there is no need to use DATEDIFF. Just replace that whole function with 2019-yearOfPublish. Like this `SELECT title, yearOfPublish, 2019-yearOfPublish AS age` `FROM films` `WHERE 2019-yearOfPublish &gt; 25` `ORDER BY yearOfPublish ASC` &amp;#x200B; or if you want it to be dynamic use DATEPART `SELECT title, yearOfPublish, DATEPART("yyyy",Date())-yearOfPublish AS age` `FROM films` `WHERE DATEPART("yyyy",Date())-yearOfPublish &gt; 25` `ORDER BY yearOfPublish ASC`
Thank you so much! :D
I use ssms extensively and it never freezes.
Both of your ideas have issues when it comes to searching on the data present to see what you have. To do this right, you'll want a few tables. 1. Restaurant - contains just the information about the restaurant 2. Cuisine - contains the name of the cuisine 3. MenuItem - contains the information about the item and has a pointer to the Restaurant table 4. MenuItemCuisine - links a specific menu item to one or more Cuisines. If you intend for a menu item to have only one type of cuisine, this table is not needed and you can have the MenuItem table have a pointer to the Cuisine table.
I have one Dev server on an old domain that one vm is on that I use to connect to it. Something is really funky on this domain which I refuse to address because I am waiting for the actual IT department to deprecate it. Connecting via integrated authentication and Kerberos makes ssms lock up doing anything. Only other thing I've seen is when I'm doing a large 300gb migration on a database and I start thumbing through table storage information. It will crash ssms 50% of the time.
Echoing the others here: I find SMSS stable as a rock. I think I had more freezes in the first month of using Oracle SQL developer than I did in the previous several years of SSMS.
It's likely either a connection issue or a memory issue. Does it behave differently if you're connected to a local database? Are you working with very large datasets with limited ram? Are you using any third party plugins?
I feel you. Every once in a while SSMS goes completely insane and starts gobbling up all available RAM, even faster than Chrome. You can see in Task Manager memory utilization for SSMS process increasing by 100 MB every 3 seconds. Restarting SSMS resolves this for a few hours, and then it starts doing that again. Don't know what it is, but it consistently happens at least once a week on 3 different development machines, for 2 different people.
To be clear: the issue isn't "third party tools". Third party *tools* are super helpful. The issue is *uploading your data* to a service that is hosted outside your company's infrastructure. You should assume that service will retain a copy of your data, and mishandle it so that it ends up publicly available on the internet. Those services should be vetted and approved by your company's security folks.
I‚Äôm not quite clear on what you are asking. You may want to research SQL cross apply and pivot functions. I think one of these will give you the data the way you‚Äôd like it returned.
&gt; So I'm going to be a little vague too vague for me especially the "kind of overlapping things" part
All that means is that because the other containers contain the same elements of the first container it's an overlap.
Never thought this would be a thing. Thanks for the link!
Plugins can cause freezing, if you have any, try u installing one by one to see what fixes it. So can automatic saving on a roaming profile, you can disable automatic saving if it's causing issues. Finally, if your script is outputting a lot of result sets in sequence from a single execution; with this one, if you don't actually need the result sets (like your looping through something, sometimes it can't be helped), right click, query options, and in there somewhere you can set it to disregard the query results after execution.
So... Do you want all things in two or more containers? Or all things for container 1? Or something else.
Very nice , thanks for the overview makes sense.
Yeah I can re-word it. It may also be possible to get the data into a different form. Something like this. Container | C1 | C2 | C3 | C4 | C5 ---|---|----|----|----|---- 1 | 1 | 2 | 3 | 4 | 5 2 | 1 | 2 | null | null | null 3 | null | null | 3 | 4 | null Where I would want to return 1, 3, 5 still.
Ah, I think what you want is a CTE with a Union ALL inside. Then use DISTINCT to weed out the duplicates. Do you need to know the container they come from? For example, you could come into a race condition where an item from container 3 will show up before container 1. Or are you trying to split container 1 into the other containers? [Is this a good example of what you want?](http://sqlfiddle.com/#!9/3f7ba9d/7) I don't know what columns you are looking for or how you are manipulating the data, but here you go. You can also lower your dataset before the CTE, by putting the WHERE on the original SELECT statements.
You probably want to do a cte or temp tables off container a. Do an inner join between the containers on the thing. Combine everything with a union.
I don't need to know which container they came from. I really just need to act on the results of a select that is in the form Container | Thing ---------|----- 1 | A 1 | B 1 | C 1 | D 1 | E 2 | A 2 | B 3 | C 3 | D So I don't know ahead of time how it will break down. The only thing I know is how many letters (and which letters) are in container 1.
Yeah I use a temp table of container 1 to get to the result set I have.
 WITH containter_1 as ( SELECT DISTINCT Container, Thing FROM dbo.container1 ) , container_2 as ( SELECT DISTINCT c2.Container, c2.Thing FROM container_1 cte join dbo.container2 c2 ON cte.Thing = c2.Thing ) , container_3 as ( SELECT DISTINCT c3.Container, c3.Thing FROM container_1 cte join dbo.container3 c3 ON cte.Thing = c3.Thing ) , container_union as ( SELECT * FROM container_1 UNION SELECT * FROM container_2 UNION SELECT * FROM container_3 ) SELECT * FROM container_union ORDER BY Container, Thing That gives you what you're explaining, if its not what you're looking for then please clarify. If you don't need to know which container they came from... then why do you need to even bother with anything except Container 1?
My fiddle example still works. If container 2 contains data that is not in container 1 it will still show with the distinct on the CTE. I guess I should have added it in my example....
Brent Ozar is a good starting point and he has a couple free classes to get you started and give you the feel for his style before you purchase more classes.
I put your example into a fiddle. http://www.sqlfiddle.com/#!18/adab6/3/0. But it doesn't really return the expected result that I need. I really need 1, 2, 3, 5 from that example.
No the query you just gave me is what I already have. From the results of that query I need to break it down further to return just 1, 2, 3, 5
What is the criteria that gives you 1,3 5?
If we go back to the example with the letters. So it would be A, C, E. I'll show an example where I would want A, B, C, E to hopefully make it more clear. Here is the data we are working with . I added the fourth container to hopefully make the example more clear. Container | Thing ---------|----- 1 | A 1 | B 1 | C 1 | D 1 | E 2 | A 2 | B 3 | C 3 | D 4 | A In the context of container 1 and letter A: We have A valid in container 1, 2, 4. In the context of container 1 and letter B: We have B valid in container 1, 2. In the context of container 1 and letter C: We have C valid in container 1, 3. In the context of container 1 and letter D: We have D valid in container 1, 3. In the context of container 1 and letter E: We have E valid in container 1. So now because letters C and D are the same (valid in the same containers) I only need to return one of them. All the other sets are unique. So I want to return A, B, C, E.
I would suggest to not have 4. Stick to the simple complexity. [Like this.](http://sqlfiddle.com/#!9/c33593/3)
I think your example might be a bit round about for your overall idea/end goal. Using the constraints you gave, I would almost use the evil CURSOR word to fix this issue. [Here is how I made the below table](http://sqlfiddle.com/#!9/6b67bf/23): ||C1|C2|C3|C4| |:-|:-|:-|:-|:-| |A|1|1|0|1| |B|1|1|0|0| |C|1|0|1|0| |D|1|0|1|0| |E|1|0|0|0| You could then use a cursor to validate which ones are duplicates. You cannot use DISTINCT since the "Thing" is distinct..... This is a terrible approach. I would almost suggest to go back and re-assess how you are getting the original dataset.
On our team we did change a setting: in ssms options, under "SQL Server Always On", try to uncheck "automatic refresh". That helped some people, while we also discovered RedGate SQLPrompt to be the main reason for program freezes.
Could you provide your source where you learned this? It does sound interesting (also did some quick Google search and there was one article explaining some basics but you have probably another source)
Thank you so much! This makes much more sense.
Yeah I might have to go with a non sql solution to get the sets first. Feels like a sql solution is just too hard. Thanks for taking a look though.
From the [privacy policy](https://tableconvert.com/privacypolicy.html) of tableConvert.comÔºå &gt; We take precautions to protect your information. If you submit data for conversions via Pasting the data or reading the data from a file, that data stays on your computer and is processed by the browser. If you submit an URL that points to data, that data is read by our servers but is not retained. The last CSV file processed is saved on your computer in the browser's storage area. If you are processing private information on a public computer, you may wish to process dummy data if you do not wish that data to be saved on the computer you are using.
[removed]
Please see https://tableconvert.com/privacypolicy.html#security
yeah!!!
Of course, I prefer to be able to convert data anytime, anywhere, as opposed to writing python code.
Thank you! I'll spend some time with the free classes today!
Fair enough
Sure. I‚Äôll add the articles I‚Äôve found to the original post body üëç.
Sweet thanks a lot Tbh I actually read a totally different article instead of machine learning although still the same category It was about neuronal networks using pure sql and since I just happened to watch some yt videos about neuronal networks it caught my attention Nonetheless super interesting since my background is only that of a b·ªã developer and I'm still in grad school so thanks for the interesting read/concepts
You could start by explaining why you want this result set... you explain what's in the containers and what the result should be, but no explanation why that should be your result set. ***Why*** do you want your result to be A B | C D | E? Why is E alone? I'm guessing A B and C D are together because containers 2 and 3 exclusively have A B and C D, but what is up with E? &amp;#x200B; Sure, we can get a select to return A B | C D | E from A B C D E, A B, C D - but we have no idea if it meets any other criteria because you haven't explained it. It's functionally impossible for us to adequately answer your question, because you're not asking for an answer to a question, you're asking for us to give you a specific result set - which doesn't help anyone.
What RDBMS are you using?
Vertica - its not a standard RDBMS
There are very good interactive SQL courses on [www.codecademy.com](https://www.codecademy.com) many of them are free. [www.w3schools.com](https://www.w3schools.com) provides a solid introduction to the basics of SQL, all the commands with simple exercices. [www.sqlservertutorial.net](http://www.sqlservertutorial.net/) focuses exactly on MS SQL, although it doesn't matter what platform you start learning with, the basic concepts of the language are the same.
But.... E isn't overlapping? So why does E show up in your result set? A B and C D overlap with container 1. Still incredibly vague that container 2 overlaps with container 1 (in most relational databases, there is no *order,* so you'd have to explicitly store that container 1 is *first*) so AB should show up because they're "kind of overlapping", but E isn't overlapping anything, why should it show up?
Saving this for later, thanks for sharing! Seems like a really fun way to spend a Saturday (seriously) learning SQL.
I think the logic boils down to "any record that doesn't have a record within 14 days ahead to disqualify it". SELECT ResultDate FROM (SELECT ResultDate , DATEDIFF(day,ResultDate,LEAD(ResultDate) OVER (ORDER BY ResultDate)) AS LeadDays FROM Result) r WHERE r.LeadDays &gt;= 14 OR r.LeadDays IS NULL;
Look into window functions. If vertica has them, you should be able to use something like a `LAG(result date) OVER (partition by person order by result date)` type syntax to get the previous date onto the same row with the current date. Then do some more filtering to exclude the ones you don‚Äôt want. If vertica doesn‚Äôt have window functions, depending on the size of your data maybe you could use some other processing program like pandas to do the same thing
Ah. Well Neural networks are still the same thing for this discussion. I‚Äôm not actually differentiating between machine learning and deep learning per se. I just used ML as the umbrella term. So yeah, deep learning topics are still a part of the discussion.
what would happen to the sequence of 1/1, 1/7, 1/14, 1/21, 1/28? Is the only "positive" outcome 1/28? If so, just use exists clause to check the results table for relevant results.
It would still be Considered 1..but the problem is.. If positive on 1/1 and 1/7, negative 1/14 negative on 1/21 but positive on 1/28 it's two So if exists would only show one for even this scenario
Could you do something like: SELECT PatientID, ResultDate FROM Results r1 WHERE IsPositive = 1 AND NOT EXISTS (SELECT 1 FROM Results r2 WHERE r2.PatientID = r1.PatientID AND r2.ResultDate &gt; r1.ResultDate AND r2.ResultDate &lt;= DATEADD(DAY, 14, r1.ResultDate) AND r2 IsPositive = 1)
I am working for an apparel company and was tasked to create a report that let's us view customer retention rate, broken down by month. I built my data using SQL, and now I need to find a good way to present it. Here is the data, I put it all into a google sheet, it is not too long. https://docs.google.com/spreadsheets/d/1Ez4cGryObKwOv2mjthhBUTsv5mPxWVZmcg7KAkO5-ow/edit?usp=sharing I will try my best to explain it. The data works on two date ranges, a base range, and a range range. The base for this data are the dates between 5/30/2017, and 5/30/2019, or two years. This is the base that will form all of our customer information. The range for this data is 1/1/2019 to 5/30/2019, so that means we will only investigate what happened to the customers who purchased at-least once in the last 6 months. So the bigger you select your base to be, the more information you will have when doing calculations for customers in your range. More on this later. So the first field is TotalCustomersOverBase, pretty simple enough, it is the total amount of unique customers (using email address) over the 2 year base period. Next field, TotalCustomersOverRange, is the same but for the 6 month range period. Now, next field is NewCustomersOverRange, which calculates over that 6 month range period, how many customers are new and never purchased before. From now on it starts to get a little complicated. The next field is RecurringCustomersOverBaseButNewInRange(LostButFound). These are customers who are new over the 6 month range period, but when you go back and look at the entire base of customer information, they are actually recurring customers. I consider these customers LostButFound. So your range period allows you to specify when you consider a customer lost. These are customers who placed one or more orders during the base period, then got lost (ie stopped being a recurring customer), and then popped up again and placed an order during the range period. If we were only looking at out range period of 6 months, we would never know that some of these customers aren't really new but just pretty old. NewCustomersOverRange minus RecurringCustomersOverBaseButNewInRange gives you the next field, NewCustomersOverBase. This is the true number of new customers, customers who have never placed an order before (within our base). I consider there to be three types of questions, New, "New", and Recurring. These are New, the previous field was "New", and the next will be Recurring. The next field is pretty simple, RecurringCustomerOverRange, is just customers who placed more than one order over the (6 month) range period. NewCustomersOverBase + RecurringCustomersOverBaseButNewInRange = NewCustomersOverRange NewCustomersOverRange + RecurringCustomerOverRange = TotalCustomersOverRange Then I take this information, and further break it down by month. The way I was thinking is some sort of bar graph, where I have five adjacent bars representing the last 6 fields (broken down by month), for each month. And then somewhere off to the side, I'll have the total fields, which are represented by the first 6 columns. Also, if anyone has any better names for these terms, that would be greatly appreciated. Currently, I am planning on adding calculations for average amount of time between purchases and price and quantity changes over subsequent orders (do customers spend more or less as time goes on). If anyone has any recommendations for some cool calculations I can do to this data, I'd love to hear it.
To me as a person who reads these kind of reports, there's nothing more frustrating than figuring out what our "100%" ("whole pie") reference point is and what categories are supposed to total up to that. Customers aren't balances and they dont just increase and decrease organically, so taking "BasePeriodCustomersNumber" as 100% doesn't make sense to me. Your base + reporting period (btw, 1/1 to 5/30 is 5 months, not 6), imo, should be the "whole" pie with "base" and "reporting" periods. Then you naturally have "new in reporting period", "returning in reporting period" and "nonrecent customers". You can split the first and last categories in "recurring/non-recurring". Per month, you can find (looking back in history, not forward) the number of recurring customers, average/median times from prior purchase, etc.
Most of these are like building a full scale car out of Legos. It looks like a car, it's really neat-o and if you drive it very carefully it moves like a car. However, it took a ton of effort to build and there's no way you're taking it on the highway. The ones that I looked at beyond the linear regression (which is cool, btw) all involve a procedural language beyond SQL (like Python or MySQL procedures.) That's sort of like "cheating" on your lego car build by using a gasoline engine :) Interesting stuff though, thanks for sharing!
Very welcome. And yeah, its mostly an interesting challenge for me to do in pure SQL. I already know that I can do it with procedural SQL extension (PL/pgSQL, T-SQL, and others), and especially with Python running in-database with PostgreSQL or MSSQL, but the idea of making something complex within such limited constraints is intellectually stimulating. I mean.. ANSI SQL is Turing Complete now, so it's theoretically possible to do any computation with it, but probably not very feasible or pragmatic with anything more than linear/logistic regression and a few others.
You might want to look into RFM segmentation - [https://en.wikipedia.org/wiki/RFM\_(customer\_value)](https://en.wikipedia.org/wiki/RFM_(customer_value)). The idea is to classify customers into segements based on their behavour - recency, frequency and spending. &amp;#x200B; I created a somewhat similar analysis as yours for a client (e-commerce) that turned out to provide great insight value. We split the customer base into five groups &amp;#x200B; 1. Frequents: Purchased during last week 2. Somewhat frequent: Purhcased during last month (but not last week..) 3. Seldom: Purchased during last year (but not last month...( 4. Lost: Purchased more than one year ago 5. Recovered: Purchased last week, but before that more than one year ago &amp;#x200B; This classification runs every week and stores the results. The results are plotted in a stacked area chart and the customer behaviour trends becomes easy to see (are we losing frequent customers, did we recover a lot of lost customers etc + trends) &amp;#x200B; The segmentations are specific to just this company and can be very different depending on the nature of the business,
You clould also try with a recursive query with self join selecting the newest and then iterating downwards
I asked myself that before I got mine... then I switched it round and asked myself why not have it? It can only make it better. Then I went and sat another 3. It wont land you a senior developer role.. but it might give you that edge over someone else. Go for it dude!
This sounds like "survival analysis" -- figuring out the how long customers will continue to use your Services/buy your goods. If your customer identification info is solid, I would try to plot the number of recurrences or time along the horizontal axis from initial purchases to max number or time interval. The vertical axis would be a percentage of customers. This should look like a trailing off from 100% to zero.
What do you mean you sat for another 3? I'm looking into it and most likely going to try it out, but I'm completely new to SQL and I'm just learning PostgreSQL atm. I've seen some practice tests and can barely do any of the problems haha.
I meant 3 other MTAs... Server Administration, Networking and Security. The Database should give you enough knowledge to get started with SQL and its concepts. Everybody is different.. but I bought the book, read it twice. Made a note of what I wasnt 100% sure of and made a point to go and look into them. Hasn't failed me yet.. but everyone is different. Go for it!
What book exactly? I'm interested and thanks for the information and positivity.
Put the data in PowerBI and play around with it.
We actually came up with a solution today. We reordered the columns to map things to containers. Then threw those into an XML string then used RANK on that result set to get the rank 1. I'll write up the full solution as an answer to this question when I get time sometime this weekend.
&gt; don‚Äôt store html entities...validate your inputs This is useless advice. There‚Äôs nothing wrong with storing html if that‚Äôs what the user submits. If a user puts html in a comment, it should be stored as-is. That way you can display it on multiple different clients later by processing the source value.
What takes precedence, 25 transactions in one batch, or $250 in one batch?
You could try to import the data into Excel and mess around with it. Excel has some decent built in analysis tools that work pretty simply once the data is formatted.
PM'd you.
There are some surprising things in the data relative to the points assigned to each commodity, how to use your $, how to hunt effectively, etc. I hadn't played Oregon Trail since I was a kid and kept hitting replay until I had a perfect game. Scores over 8000 are difficult because of the math, but that's what makes this a fun SQL experiment. If you chose to 'game' the system, you could, with patience, probably get above 9000. Getting 8000 alone is a pain in the neck. But it's all simple math, analytics, etc. I started a second game based on my learnings from the first, and [here](https://drive.google.com/file/d/1EbmvsVi1O9PPGISogh50Hy_fUdZa8d0l/view?usp=sharing) is the data from it. Had someone die randomly, for no reason, so rage quit.
How valuable is the goal of having the fewest number of batches?
Yeah that's the challenging part. Interesting problem!
Do the transactions in a batch have to be consecutive? Or could a batch contain #1, #7, #3000, etc.?
if you dont care about getting to "optimal batches", simply do a running total of your sums, divide by 250, round up to next integer, and here's your "batch number".
It‚Äôs really difficult to go off of job titles alone because every company makes up their own titles. Does the job posting mention SQL as a requirement? When I hear CRM I picture Salesforce and assume that you‚Äôd be working with it or another CRM software. Unless the posting me tions SQL I wouldn‚Äôt expect it to be required. But if you‚Äôre interested in the role then just apply for it. You can ask questions about the analytics requirements during the interview to decide if it‚Äôs the kind of role you‚Äôre looking for.
Yes but there's also the constraint of no more than 25 transactions in a batch. (If I'm understanding the problem correctly.)
You need to learn how to interview. I get annoyed when people look down on my contract work, and I take it out on them in the interview.
Are you in a position to move to a big city like NYC? SQL opportunities here are aplenty.
non-consecutive transactions, as long as it is 25 or fewer, preferably exactly 25
If you have a requirement to index your data for searching and your indexing function doesn't understand HTML entities, you do not have much choice.
I‚Äôd layout your contracting work as being self-employed showing long term and key gigs as projects. That and make sure you‚Äôre not projecting the negative energy in your written and verbal communication.
the goal is to have complete batches (count of 25 transactions per batch), while having the least number of remaining transactions that are not in a batch. singular transactions cost more to process than the batched transactions.
From my understanding, the real issue is that the html is being escaped into html encoded strings. So users search for ‚Äú&lt;‚Äù but the data uses ‚Äú&amp;lt;‚Äù That‚Äôs what makes the data impossible to search
What do you mean take it out on them? I‚Äôm in a similar spot and I want to put a crack in the matrix
I believe the issue is HTML encoded strings, not the html entities themselves.
25 years of experience? Maybe you're encountering some age based discrimination? The fact that you're getting interviews means you look good on paper. Your resume sells. Something is not clicking on the interviews or maybe with your references (if you get that far).
It means the web server couldn‚Äôt load the page, probably because you gave invalid input. Your question doesn‚Äôt really make sense because no one here knows the web server your trying to hack and even if we did, we don‚Äôt know how it‚Äôs configured to respond to invalid input. Maybe you simply found the website‚Äôs landing page for a 403 error.
Thanks. Any tips or tricks? I honestly thought I was interviewing well. I always get good feedback.
That sound's like a really tough place to be in. Life is really unfair but sometimes that's just how it goes until eventually something breaks the cycle. Have you considered changing areas? Major cities might have intense competition but some smaller cities surely have high demand for SQL developers that aren't getting filled. If you're getting interviews that's good at least. Your name isn't on a blacklist. If the interview is what's stopping you, the issue most likely is with your people or presentation skills rather than your actual experience. Learning to be super personable goes a VERY long way in the developer world, and I think many of us don't take that seriously enough. The interviewer already knows you've done mostly contract work when they bring you in so keep that in mind. When they ask about your work history, they're trying to understand if you're going to bring positive attitude into the team or not. As someone who interviews and has interviewed many times for data science roles I can guarantee most non-technical questions are to learn about your attitude. Obviously I don't know you or your situation very much but based on your post, but I'd start looking at your location, and very importantly what attitude you bring to the job. Hope things turn around for you!
That's probably a good idea. I usually arrange my experience in chronological order. But I might get a different impression if I give the temp work it's own section. Thanks.
Whats the minimum transaction amount? Are they all whole dollar amounts?
Unfortunately no. I've got a son to raise here in the Tampa Bay area. I'm probably here for the next 20 years or more. On a different note, I've looked at remote work, but I keep striking out there as well.
Here's the curve ball. I was getting interviews. I think I've appliend and interviewed with all the major job openings in my area for the last six months. It's been kinda few and far between lately. I haven't had an interview in maybe a week. The last was a phone interview, and I can't even get a call back. Ugh. I've thought about the age thing. But, it doesn't bother me too much.
individual transaction amounts: 0.01 to 24.99, they are not whole dollar amounts.
Have you tried working with IT recruiters? They'll do what they can to place you into a job, even if it's not a perfect fit. They'll also be able to give you feedback as to why you might not be landing jobs and what's going on with hiring in your area
Have you looked at the jobs (some remote too) posted on stack overflow? I've seen some pretty good ones.
Thank you for the kind words. Unfortunately I can't relocate. But I do try and drive longer distances for some jobs. In regards to my personality. I've often joked about how I have the personality of a jar of rocks. (I have my moments) In fact, I've oft wondered if I'm on the spectrum. I do identify with some Asperger type of mannerisms. I try to do the usual stuff: maintain eye contact, smile, laugh, and show a friendly demeanor. I tend to apologize a lot. I usually try to make the other person feel comfortable in my presence. But I will admit that this can become a juggling act. One thing that I keep running into are situations where the person interviewing me isn't a "database" guy. Maybe they're a web developer or a developer of some sort, and don't really expose themselves to the deeper underpinnings of SQL software stack. They might ask a generic/high level question. When I given them a detailed answer...they don't understand. I don't mean to sound like an arrogant a-hole. But how does one interview with unqualified people who are supposed to qualify me?
Fun fact: I've actually wrote a web scraper that looks for job openings. I will see about adding support for stack overflow this weekend. Thanks for the tip.
Just overall feedback: chin up. It's not you, just something you're going through at this moment and it will pass. It also may be where you live, or current job market, combo of some things. Glad you reached out and asked for some feedback. Wish you luck soon bud.
Here's part of my issue. Save for a few times, I get 100% of my work with recruiters. They were always helpful, giving work, until this year. This is the longest time that I've gone without work. Now it seems that when recruiters call, they are offering up non-existent opportunities just to harvest my resume. It's adding no value to my job search, and only making me distrust them at this point. I honestly blame them for part of my disheartening experience. Otherwise, I am generally plugged into the local job market.
Chicago as well, we have half a dozen openings at my company alone.
&gt;Wish you luck soon bud. Thank you for the kind words. I honestly hope I get something soon.
Are any of these positions remote friendly? I'm based in the Tampa Bay Area. I'd be willing to share my resume, if it helps.
I've only worked in IT in Nevada and Southern California, and seems there are more jobs than qualified/good candidates, so maybe there is something in the air down in Fl, but I get having to stay put for your kid. You're doing the right thing. Have you gone to your local SQL group or SQL Saturday? We have a meetup local group and yearly SQL Saturday nearby, maybe you could network - if you're not doing that already.
I can check, but I don't believe so. I do have a friend that works in a permanent placement staffing agency in Tampa - he doesn't work in the DBA area, but I'll see if he has any coworkers who might.
&gt;I haven't been to the local user groups. But professional networking couldn't hurt. I will start research sql pass/et al. Thanks for the tip.
You haven't just ticked over to the big 50 have you, I heard that age is entering the dead zone in terms of finding work.
I just finished this. Great practice for a novice like me. Thank you
I mean I get angry, and let my frustration out on the interview for asking a dumb question, and try to frame the narrative that they are dumb for not thinking of something as a strength.
An interview is their job to impress you, not your job to impress them. Google modern interviewing techniques.
I may need a hand with a client in Tampa. PM me your contact info
&gt;You haven't just ticked over to the big 50 have you, I heard that age is entering the dead zone in terms of finding work. I just turned 38 last week. Hopefully I wear it well, haha.
I was in a similar-and-different scenario up until a few days ago. In my situation, I was the problem. My resume sucked, and my attitude sucked. I finally asked for some help. Got some help. Applied help. After many e-mails, phone-calls, trips-into-the-city, etc., I received an offer. If I had not had the help I had, I would probably be homeless right now. I guess my suggestion is to ask someone for help.
Information sent!
Can you explain this: \&gt; and worked my way up to systems/application development, and systems engineering. For the last 15 years or so, I've been directing my career towards SQL Server DBA/Developer work. This feels like a backwards move to me. do you mean something different by Systems engineering than I do?
I've considered that maybe I don't always present well. Unfortunately it's not always easy to get honest feedback. However, I will certain take your advice to heart. Thanks.
What's the transaction value distribution like? What's the mean, median, variance like? If the distribution means the average is less than $10 then you should be able to just order by transaction value and then make the batch number equal to the row number mod number of batches (20k/25~=800). That way each batch had a good distribution across the values. Look at the last full batch ~800 if it's under $250 then so will all the lower batches. If it's more than $10 then you can still use this method, you'll just have to get trickier by upping the number of batches till the last full batch sits under the $250 mark
Do NOT list it as temp work. List it under working for yourself. You had to sell yourself to get those jobs. You need to do that again. Sell yourself.
&gt; I've been directing my career towards SQL Server DBA/Developer work. Your career is directed at technology, of course you're going to have a hard time finding a job. Technology is a tool used to solve problems, in your case business problems. Why should I hire someone with 25 years of experience in a particular tool? I get another worker bee. I don't need another worker bee, and especially for that price. I need someone to come into the business with knowledge of those tools who understands how to make real change. Who understands business. Who understands how to speak to stakeholders. Who understands what drives revenue and how to use the skillset they have to make those changes. When I interview, I spend about 5-10 minutes going down a list of my most fluent technologies - but ultimately the conversation leads to what I can do for the business to solve their current most pressing issues, look for issues they may not even know about and focus on the business side of things. The technology is the way in which I solve these problems but no one cares about that, they want to know if you recognize the problems, how your thought process is in solving the problems, etc.
I guess the longer answer is that I took programming courses in high school and college. On some level I've been programming since the mid 90s. In parallel, my career started as help desk person. The natural progression was from help desk to system administrator and light networking type roles on the operations side. Eventually, while being a programmer I've gravitated from operations to the project side. My day to day becomes more project and less operation? But because I have the operations experience, and especially I usually the only "linux" guy in the room, my day to day will always have some type of operations. These days I seek out sql server work, it's my favorite. Soooo devops?
&gt;Sell yourself. I do have an LLC that I contract under. I guess I can just file it under that. Thanks for the advice.
&gt;When I interview, I spend about 5-10 minutes going down a list of my most fluent technologies - but ultimately the majority of the conversation leads to what I can do for the business to solve their current most pressing issues, look for issues they may not even know about and focus on the business side of things. The technology is the way in which I solve these problems but no one cares about that, they want to know if you recognize the problems, how your thought process is in solving the problems, etc. You know, I think this is great advice. I do find the best interviews are when I try and get a list of problems that the organization is having, and frame my response in a way that can help add value to their bottom line. Unfortunately, I don't always get the opportunity to direct the conversation in that direction. Sometimes I'm on a 30 minute time crunch, and/or it's some HR person who hasn't the slight idea what the issues are.
Select @maxBatchNo = count (*)/25 from transactions -- @maxBatchNo should be an integer cause count is Select @maxTransNo= @maxBatchNo*25 Select rownumber() % @maxBatchNo as batch no, * from transactions order by transactionAmount where rownumber() ‚â§ @maxTransNo I've not tested any of this just on my phone so...
i am not in USA, but i think this apply all over the world. Do not apply help desk position if you have 25 xp. &amp;#x200B; If you are boss, will you hire a person with 25 xp for a help desk position. What will you think ? you will think this people will leave soon, this people want to be here just because he does not have job for long time. He is finding a better one. He will not commit to my company. &amp;#x200B; You have 25yr xp. Sell this 25yr xp. What is your advantage over a 25 fresh university grad. Why you are better to other so that they need to hire you. You need to have an answer. At least an answer you think is ok.
Usually the recruiter or HR personnel will at least have a general idea of why the company is seeking to fill the position in the first place. The hiring manager needs to write up a business need and reason as to why the position is important to the company to get the position budgeted. You can usually either glean this detail from the HR/Recruiter or from the job description itself. For example, it's not hard to determine what a company is doing if it's a medium-sized company and they have multiple OLAP solutions listed. They're most likely just now standing up their BI infrastructure and are looking for someone who has knowledge in multiple OLAP solutions to determine what route they should best take for this. Information like this can be gleaned from just simply knowing the size of the company, if it's had crazy growth over the past few years, and what technology they've listed.
Forwarded potential solution.
That‚Äôs awesome lol.
&gt; One thing that I keep running into are situations where the person interviewing me isn't a "database" guy. Maybe they're a web developer or a developer of some sort, and don't really expose themselves to the deeper underpinnings of SQL software stack. They might ask a generic/high level question. When I given them a detailed answer...they don't understand. &gt; &gt; I don't mean to sound like an arrogant a-hole. But how does one interview with unqualified people who are supposed to qualify me? You frame it in a way that would give them benefit if they were to hire you. Most software developers I've worked with hate having to dig into the database. As a database developer, one of your core responsibilities is providing an easy and accessible means to data. If I'm you, and I'm being interviewed by a software developer, one of the first things I'm going to tell them is they won't need to manage pulling data from the database anymore. One of the first things I'm going to do is come in and begin writing a Data Abstraction Layer/Data API for them where they can simply call methods on a related object and not have to worry about the underlying whys and hows. This makes their job simpler and allows them to get back to doing what they do best which is writing the software.
Here is my advice: The next 3 interviews are not for you to get a job, they are for you to get feedback on your performance as an interviewer. Meaby you come off as aggresive because your are nervous, meaby you seem unstructured or some other reason - you cant know really know without interviewer feedback because an interview is not a normal thing, it is not who you are. Also, you may need to prepare to move if you havent found something in some given time frame.
I'm in the same exact boat as you are, quite parallel. I am going to ask for some DBA resume help.
It sounds like it might be inherently difficult like a knapsack problem, but you can get close... Can you explain the goal more precisely? Are both the $250 and 25 hard limits and your goal is just to separate them into the smallest number of batches as possible meeting those hard limits? Or is the goal to build as many batches as possible of _exactly_ 25 transactions adding up to less than $250? (Do they charge more for a batch if 20 than for a batch of 25?)
What you say might be right, but it's awful and annoying.
Why is it awful and annoying?
Try consulting
Using Postgres, you can do this : select t.id as missing_id from generate_series( (select min(id) from the_table), (select max(id) from the_table)) as t(id) left join the_table d on d.id = t.id where d.id is null;
I would tweak your good example to something like: &gt;Developed SSRS Reports for the Accounting and Finance department to develop month end reporting that led to a 23% decrease in hours spent to complete month-end closing. The action verb still needs to be something technical and a hiring manager will still want to know what tools he's using.
Truth enough, good looking out :)
select acteurs.achternaam, acteurs.voornaam, regisseurs.achternaam, regisseurs.voornaam, count(\*) from \[tableName\] group by acteurs.achternaam, acteurs.voornaam, regisseurs.achternaam, regisseurs.voornaam having count(\*) &gt; 1;
You have some dev experience consider learning mobile development and put some apps out there. Nothing major to start a Sudoku game or weather widget whatever. Make a site for yourm company. Now you are self employed not unemployed and have something to talk about in interviews. Theres other options get heavily involved in a open source project, start one, make a site for learning MSSQL and studying for the certs .... lots of options to be more involved than the average guy. It helps a ton to stand out. Most people just show up to the interview with nothing to talk about except a degree and past jobs. You got the time might even pick up some side money and roll into something successful.
Sell this in the interview. Frame it like this, when you get to the ‚Äúanything else you want to tell us part of the interview‚Äù: You: ‚ÄúI wrote a web scraper (explain this to non techies) and your job was one that came back (tell them where you found it posted). These are just one of the many ways I can think outside the box. What type of projects are you working on that needs some fresh ideas? What are the current projects that are on the table that I can help with? ‚Äú Then maybe try this: You: ‚ÄúIs there something small that I can help with today? Anything I can take home and get back to you in a few days? That way you know you‚Äôre getting what you want from me as developer and I can get an idea of how I can contribute to your organizations needs?‚Äù Now I am sure some people on here will balk and say ‚Äúdon‚Äôt offer to work for free, etc‚Äù and I do agree, but I promise it will make you stand out from the other candidates. You‚Äôre a nerd like the rest of us, but if you‚Äôre a nerd that presents as someone that wants to contribute and really be part of a team, and you bring a ton of experience I think that will leave a great impression.
I worked as a consultant and had a bunch of "part time" projects that were hard to describe on a resume. The traditional "job and roles" shit just did not describe the work. I redid the whole thing. I put all of my projects under a single "Professional Consulting" category. Then I just listed my PROJECTS and not duties. What sounds better "designed processes to integrate data between .NET front end to SQL back end" or... "Developed SQL integration processes for presidential candidates"? Give it a shot
Using the query builder UI, drag the combination you want onto the bottom (in this case, actor and director), and then click the "Totals" button on the ribbon. Set actor and director to "group by," add another field and set it to count. It's been years since I've used Access, so hopefully that's close enough to get you going. You can Google "access aggregate query" for dozens of tutorials. Here's the first result: https://www.teachucomp.com/aggregate-function-queries-in-access-tutorial/
We just interviewed a guy with a bunch of temp/self work on his resume. He was completely out of touch, his resume had a picture of himself on it, he sent us an edit copy with all his corrections still there and his resume was like 5 pages long. Not saying you are like this but if a lot of your work self employed you might not have as much interviewer experience which helps so much when you see bad interviews.
**Potential solution provided at the link below:** https://www.mediafire.com/file/es4j9n92h5g77rc/PMT\_TRANS\_TO\_VNDR\_BATCH\_DDL.sql/file Essentially, the easiest thing I could come up with is working on a transaction-by-transaction basis.
I haven't seen many SQL developer or DBA jobs lately. Perhaps rebrand yourself as a Data Engineer. It's roughly the same thing but with a more sellable name.
Again, this is really good stuff! Can definitely see improvement from your first game to the second. :) Keep bringing this type of content around here!
Use my go-to line when asked about extensive contracting work: *"Contracting was fantastic, and gave me a way to learn new and different techniques, understand various different company policies and increase my knowledge and experience in different fields.* *Now I'm looking for a permanent position for me to settle down into for my family and a long term career I can sink my years of experience into".*
So, working full time since you were 15? Maybe you look older than you are? Maybe "25 years experience" is intimidating to some
With SQL Server I would recommend a Trigger, but it has been 15 years since I have used Access extensively. &amp;#x200B; SQL Server Express is free if you wanted to migrate your data to SQL, then a trigger could perform an update query anytime a values is added or updated. &amp;#x200B; I would say Google Triggers, but first you would want to install SQL Server Express and import the database. &amp;#x200B; SQL Server is better on your resume than Access (as far as salary goes).
I like this. It frames your experience as building you up like a Swiss Army knife. Like: yeah I'm a database guy, I know databases. But what really gets me going is solving problems. No one ever has just a database problem. They have an information problem. You can help with the technical side, sure, but your real value is in designing/implementing/managing a system that works. Honestly the best part of my career is that I work with every aspect of my field. I'm classically trained in algorithm development and I'm spending the next month literally building an instrument frame out of fibreglass. I put things like that as a positive on my CV: you have a need? Guess what? I can do that. With 25 years experience I bet you're in the same boat or better. Chin up mate. There's a lot of positive energy in the replies here and I hope that helps you through this rough patch. That's all this is.
from my perspective, it depends on the org. some orgs that are caught up in the "big data" hype, or the new orgs that "i know we're going to make it big" and need to build everything "web scale"... agism is probably more likely, though it'd be reported as "wrong skillsets" since RDBMS's "aren't web scale". but in orgs that aren't in those bubbles, I would suggest that RDBMS and ETL/BI/Reporting skills are still hot, and age can actually be a benefit (sure occasionally higher costs, but experience so plusses and minuses). i know a few places around here, all in the same situation... if you can SQL (some places TSQL, others PL, etc), do ETL, do reporting... you're in a decent spot. I spoke to someone in another area (similar industry / traditional non-bubble businesses), same thing. so age isn't always a bad thing... we're generally looking for a mid to senior level anyway, so assuming the person is looking to stay more on the technical side (rather than management track), folks with experience on sub-ghz systems is usually a benefit since they're more familiar with focusing on system performance. No prob with a newbie, but what schools teach about "theory" doesn't always align with reality, and they better know the diff.
feel free to PM your resume or linkedin - i can give it a quick once-over and any feedback... if the resume is good, I would question the interview... or *maybe* the market, but the two areas that i've probed suggest that the market is indeed good.
Try moving? I was once stuck and felt "locked out" of positions. Then I changed cities and everything changed. Sometimes there is a higher demand for something somewhere else.
Honestly it's probably interview skills. Some people are just very good at them. I don't think you need help with your Resume or any experience. You clearly have that. It might be time to look at your 1 on 1 personality and people skills. Maybe you just "feel like you deserve" the job and that triggers some complex in your interview. Maybe if you flip your mindset to "they need me" to "I need them" it'll make you interview differently.
This is the ticket. Self employed long term, and if they ask what you did during that time, explain some of your projects.
Dig into Azure and devops focused around sql. Those keywords will open a lot of opportunities
How did you arrive at $10?
$250 / 25
Good catch! I was 16, haha. You may have a point with the lack of degree. But, it's honestly never been an issue before. I to try and take the occasional online class to help though.
You can spin a free instance of [ScaiPlatform](https://scaidata.com?utm_source=red&amp;utm_campaign=ps_r_sql_promo_q119), load the data and create a report through the UI, by grouping by actor and by director and count distinct per actor and filter the count to be larger than 1.
I mean, I don't start yelling, or acting a fool, I'm a professional, but if someone starts asking me questions that are stupid, then I definitely turn the question back around on them. E.g., "Blah, blah, blah, contractor experience." "Do you think it's a negative that I've worked for X premium brands and see how their data is structured, analyzed, etc.?" The interview is their opportunity to impress me, not the other way around, and if they are going to ask dumb questions then they are wasting the time I took out of my schedule to meet with them. This indicates to me the person that is interviewing me isn't qualified to interview me, and/or that the position might not be a good fit. Case in point, I applied for a job yesterday that really stressed how "busy" the role was, and how many requests come through... then they stressed they really needed someone who had high level SQL skills. OK, no problem.... but then after I said my skills were high level, all the followup questions were about Access and Excel. Uhhhh, what? That annoyed me, and it showed in my answers, because I wanted it to show in my answers. I don't know for a fact that a large part of the company is on Access, but I high suspect it, and I hate it. I'm not interested in that job unless their first order of business is to move away from Access. I want an interviewer who can pick up on that and says, "Oh, you don't like Access? Great, we've been trying to get away from it. Have you ever done that before at a previous role?"
Information sent!
I have a little bit of xp with Azure. I've honestly found it very refreshing. However, I will dig a little deeper. Thanks.
Some days, I feel so desperate I feel like I need anybody, haha. The more I think about it, it might be my presentation. That tends to align with the feedback here. I'm going to research interviewing techniques today. Thanks for the feedback.
I would if I could. However, I have a family here in the Tampa Bay area. I'm in this area for the next 20 or so. I try to compensate by looking for remote jobs. But I seem to be striking out on those too. :(
I've only recently started to spin my contracting into a more positive light, and that seems to help. But, that doesn't seem to be enough to wash off that negative stigma. :( If you don't mind, I'd like to use your line. Maybe it would help. Thanks for your feedback.
This might be a great idea. It's certainly worth the consideration. I've also thought about re-branding myself as a full stack developer. I have the systems, programming, and data to make that claim. Or I'd like to think I do. I'm going to rebrand my resume tonight. Thanks for the feedback.
I think that might be part of my issue. I do have lots of experience. I don't list it all, just the high-lights. It's around the 5 page mark. However, the recruiters, and interviewers in the past have never said anything negative about it. As weird as this might sound - I don't think I'm getting enough negative feedback. If it would help, I'd be happy to forward my resume/linkedin information to you.
This still doesn't fully work. For example, director Woody Allen directed 3 films with all of them having a different actor. Why does Woody still show on the list eventhough the combination actor-director doesn't show up multiple times?
instead of using subquery in the select list, join to it (by tableID) join (SELECT tableID, MAX(ItemID)+1 as m FROM Table1 GROUP BY TableID) mx on mx.tableID = table1.tableID
Cheers bud, will try this when I'm next on the server.
I think that Toad Data Modeler can do that: [https://www.quest.com/products/toad-data-modeler/](https://www.quest.com/products/toad-data-modeler/)
&gt; I redid the whole thing. I put all of my projects under a single "Professional Consulting" category. Then I just listed my PROJECTS and not duties. I'm actually going to rebrand my resume and linkedin. With some other's feedback, I might have too much information. Rather, the same information could be of higher quality in a different spot. Rephrasing helps too, I'm sure.
I agree. Remove any mention of help desk from your resume at this point. It‚Äôs not helping.
i'll give a more thorough reply over the next day or so... TL;DR: experience seems solid, but resume seems overly technical and not enough business value (there are places for both)... IMHO, getting a follow-up would be highly dependent on the person looking at it.
knew someone that had a ton of success getting involved in the PASS community... spoke at a handful of them, wasn't long before job offers started to pop up.
Create a one page resume. It‚Äôs hard to do. Do it. No one has the time to read through five pages. 5 pages just pisses people off. The cardiologist that saves your life from a heart attack doesn‚Äôt have a five page resume. So you certainly don‚Äôt need one for IT work. Also, tailor the resume only to the job you are applying for. Use THEIR job description as your guide for exactly what you list in your one page resume. Please reread that. Use THEIR exact job description as a big starting point to create your resume. If your work experience is relevant and accurate, there won‚Äôt even be an ounce of bullshit on there.
I've heard the advice before. I'm 30, but I've been hearing from people since my early twenties that age-based discrimination is very real, and it's worse in the tech professions. I was advised to leave off my graduation year past around 35 or so, and avoid statements like 20+ years experience unless it's for a management or leadership position.
I had this approach. I completely disagree and have had nothing but failure with it. Strongly suggest you suck up and make them want you, then you decide after if you liked them or not. In other words, an interview is your chance to get the job. Afterwards you decide if you want it or not, so make sure when it is time for 'any questions for us?' you ask the two or three most important things to you, because the rest of those 55 minutes should be about them loving you and nothing else.
&gt;This indicates to me the person that is interviewing me isn't qualified to interview me, and/or that the position might not be a good fit. There seems to be a lot of this lately, for me anyway. And I honestly don't know how to say this without coming off like a jerk. Usually I apply for positions that were once held by a previous DBA. However, said DBA isn't there anymore and there isn't anyone available, someone with truly deep SQL Server technical chops, to interview me. It's usually the next best person, like a web developer, or some programmer. Some type who is exposed just enough to data backends. They usually end up just copying and pasting SQL interview questions on paper. They'd ask some simple question like "whats the difference between a group by and the order by" or my recent favorite "whats the difference between a primary key and a clustered index". I'd give a detailed, technical answer. They clearly didn't understand what I just told them. Then, I'd visibly watch them scan their paper for matching words. They would then proceed to read back to me their answer, repackaged for the layman. In their mind, I got it wrong. It's that sense of arrogance that really gets me. Dunning Kruger in the interview. Don't get me wrong. I keep it professional. This person is clearly doing the best they can. But this gets annoying after a while. Has this happened to you? How do you deal with this bullshit?
This is what I've found successful during job interviews. You're there to convince them that they **need** to hire you, not just that you would do well in that position. I think there generally are a strong pool of candidates of which you are one, but that pool can often be sizeable. They may be aware you can do the job well, but so can several others. It's about making it clear you can contribute to some sort of gap, or bring something new, I think. Granted, I majored in CS but I'm not a tech professional. I'm just here because my supervisor recently told me I need to learn SQL. But I'm sure the same principle applies for skilled jobs like this.
I think the closest thing on my resume is some sysad type stuff. But that's not even really emphasized. The last help desk position I had was over 20 years ago, and isn't mentioned on my resume. I was just saying earlier that I'm getting desperate and applying for positions with msp that I am clearly over qualified for. That's all.
You might be right. Have you found a sweet spot for the number of years xp? Should I trim it down to 15 or even 10? The extra years doesn't really seem to add that much more $$$ to the paycheck. In fact, I've seen some orgs in my area offer 95k for five years of xp.
without knowing how \` BIS2\_FN\_HostQuarterly \` works (a stored procedure?), i think you could try this -- FROM dbo.BIS2_FN_HostQuarterly(CURRENT_TIMESTAMP - INTERVAL 1 MONTH, CURRENT_TIMESTAMP)
You have to flip the script. Call them out on it, get them to admit this isn't their area, and then try to explain it to them using an analogy that is designed for someone without a technical understanding. Your goal is for them to leave that interview with an actual understanding of the words on the page.
I generally ask so many questions about what the job is actually like that when it comes time for them to say, "Any other questions" it's generally met with a few laughs, and I just shake my head and say, "No, I'm going, I see we're already 10 minutes over our time. It was nice to meet you." Then I hold my hand out signaling that *I* am done with the interview. I try to never let them end the interview.
I think that I get where you're coming from. I should express how I can add value to their organization. I generally try and do that. However, I think part of my weakness is that I'm a socially awkward person, not very charismatic in the moment. I guess I have precanned statements that I can rephrase. In fact, I find that idioms, might help a lot. But I also find that it just goes so far. Any tips for a dude that can get out of the deep-in-the-weeds technical talk, and just speak more like sales/value proposition/business style?
Explaining your work history is an interview trap. The quicker you get off the subject the better.
Fair enough. I have learned I lack the ability to straddle the fine line of confidence vs arrogance so I'm changing my strategy to pleasant (which I know I can pull off.) I'll report back in a couple months if I have any success!
It took me years, YEARS, of failed interviews to get "good" at it, but looking back there weren't a lot of jobs I really wanted, and I'll often apply places just to practice interviewing. Frankly, I don't know if I want a job until I talk to you about it in depth, and I'm always willing to take time out of my day to do that. But it's my favor to you, not your favor to me. My resume impressed you, therefore you called me in for an interview where you are getting paid to talk to me. That interview is your turn to impress me and make me want the job. There are a lot of articles about "modern interviewing techniques" and its all about psychological power plays. Like, I never, never, never, never want them to be the ones who initiate the handshake and end the interview. Ever. That's for me to do when the time is right, because I'm a professional. If they end the interview its a strong signal to me that they are done, and not interesting. What my goal is in every interview is to go longer than the scheduled amount of time because we're having a good interview where I am discovering a lot of facts about the job, and then like a total pro I glance at my watch and go, "Oh, wow, we're 15 minutes over..." which is a nice way of saying, "Shit, gotta get back to work since you aren't paying me to sit here." Do you have any other questions for us? Little laugh, little question designed to get another little laugh as you're putting your hand out and getting ready to stand up. Then as you're walking out of the room you might ask some conversational questions like, "Do you guys think you're going to make the deadline of your website relaunch?" Something stupid, keep the conversation going, keep it organic... bantering as you walk out, then "OK, take care, thank's for inviting me in.
I would strongly suggest having Reddit/friends/ anyone who does hiring that you know look over your resume and give you some feedback. 5 pages is definitely a red flag; I know you have a ton of experience and want to showcase that, but hiring managers tend to have the attitude of "one page or I'm throwing it out". The fact that youre getting interviews is great! If you feel that that is where your weakness is, look for opportunities to do mock interviews at local community colleges, or even just with a friend who will give you honest feedback. Refresh yourself on "how to interview well" via YouTube and articles. Make sure you have every edge. And make sure your interview outfit/haircut are well fitting and clean looking. Ask your friends if there's anything about your interview look that you could spruce up. Good luck OP!
Like, if you see someone fumbling through answers to see if you're right... why do they need someone like you? What's the guys actual job and why do they need someone more skilled? How much experience does the interviewer actually have, and what challenges are they facing? Then after you get that information, then you give them an analogy that actually answers their question, while simultaneously educating them, while simultaneously explaining to them how it will help with the challenges they have. You just don't sit there and recite a text book answer. Are you applying for an entry level job for a kid just out of college, or jobs for a seasoned pro? There are no rules in an interview, and if all you're doing is giving a bunch of technical answers that aren't understood by the interviewer then exactly what is your purpose, and what do you expect the outcome to be when they go and interview someone else who doesn't play by your rules?
A while back I tried rebranding myself as a back end developer and ended up hating doing web development. Now I'm exactly where I want to be as a Data Engineer/Data Scientist.
You can query pretty much any time interval with the DATEADD() function in MS SQL. It takes 3parameters: datepart - which part of the date (year, month‚Ä¶ second), number - the amount of time, date - the actual date you want to add the amount to. Let‚Äôs say you want to add 4days to today: DATEADD(‚Äòdd‚Äô, 4, CURRENT_TIMESTAMP). With this you can write interval queries like: SELECT * FROM TABLE WHERE DATE BETWEEN CURRENT_TIMESTAMP AND DATEADD(‚Äòd‚Äô, 4, CURRENT_TIMESTAMP) Edit: since the return value is date as well, you can pass it as parameter to other functions.
Thanks for the input. I've started cleaning up my projects, and publishing them to github. I've also got a site for my company. I've been about mobile development for awhile, looking into cordova.
It's a lot of bullshit on top of being technically competent
The reason behind this is your subquery returns a set of values. Since you can‚Äôt insert a set of values in the place of one value it gives the error. Note that if for some reason the subquery only returns one value, it won‚Äôt cry.
I hear this rule all the time and I'm not sure I agree with it, only because I have anecdotal evidence that it works to have a slightly longer resume assuming it's well done. I understand I probably have some thrown right into the trash but I bet it's the reverse as well for resume's too short as well or not well formed enough. &amp;#x200B; * Page 1 * Cover letter, \~500 words. * Contains who you are sending this to / from. Current contact information and title. * Page 2 * The most recent jobs I've worked. Each one has 5-7 bullet points of highlights. I just make sure the two most recent jobs fill this page. * Page 3 * The rest of my jobs. If the job was in my last 5 worked, it will get \~3 bullet points. Otherwise it just becomes a line of my title, company, and dates worked. I just make sure three jobs and their descriptions fit on this page. * Page 4 * This may be a continuation from page 3, there will only be lines of titles / jobs worked here. * Page 5 * Highlights / accomplishments. * Keywords and experience illustrated in years for software / technology. * Education / certificates achieved. * Social media - Articles that were published / received acclaim, public interactions with customers, questions / answers highlighted by famous well known SME's, Linkedin, etc. &amp;#x200B; So for anyone else reading, it's worth taking my advice with a grain of salt. The best recommendation I can make is to track the resume's you send out and experiment a little with it. Ideally when you already have a job and don't need one. You can find what works for you and what helps you find interviews with companies that have cultures you can / enjoy assimilating to. If you aren't getting interviews, it's probably based on your resume / method of applying / the jobs you are applying for. (Like applying for Senior when you are a junior.) If you aren't getting job offers but you are getting interviews, either you didn't get lucky, you didn't interview well, or someone else just beat you, but this means your resume is good.
Are you in a database job already or looking for one? If you are junior or trying to get your foot in the door, I recommend the MTA but the MCSA holds far more weight. This doesn't mean the MTA is useless, it may server as an educational stepping stone. This truly depends on how much knowledge you already have. I found the MTA too junior for me and I went for my MCSA, I also already had seven years of experience in database jobs when I took it. If you have been in database 3+ years and have worked semi-regularly with SQL Server, skip the MTA unless you want the confidence boost. If you are newer to databases or SQL Server, I'd consider the MTA.
Sorry about that its a view.
Yeah I just spoke with my boss and he said the same thing, after a page we just don't even pay attention anymore. When hiring we have you know maybe 5-10 canidates each with a cover letter and resume it's a lot of information to cover. More targeted information is better. I have a Master Resume it is like 5 pages and then I tailor it down for each job I apply for. I am also a DBM and with 4 years experience in SQL 10 in IT. If you want me to take a look I would be happy to just filter out your personal information.
you sure? i've never heard of a view allowing you to pass in parameters...
It shows up in the views folder. Not sure if that classifies it as a view or not. Sorry SQL is so not in my wheelhouse of skills. I think i got it working though with your suggestions and u/t_peter_008 suggestions. Doing some data valadation now. Ended up adding this dbo.BIS2_FN_HostQuarterly(DATEADD(m,-1,CURRENT_TIMESTAMP),CURRENT_TIMESTAMP) Query completed with no errors and numbers are matching up to the web report. Going to run a few more time throughout the day and make sure. Does that look right?
I once read some post from guy who told that have been interviewing for many years. First of all he throws away all the resumes longer than 2 pages (not this case cause OP actually gets invited to interviews), than he just selects aprx. half of the left resumes and throws away half of that cause why would he want to hire unlucky person. And couple more criterias. This guys post affected me quite strong and I changed all my resume-creating habits.
&gt; It shows up in the views folder. Not sure if that classifies it as a view or not Based on the name and how it's being used, it reads like a user-defined function.
That syntax won't work in SQL Server. `dateadd(month,-1,getdate()), getdate())` will though
If it executes successfully and the numbers do what you expect, then the best we can say is "well, it looks like it's right from here"
Ill take that as a win.
Im running in on MS SQL 2008r2. Query runs fine.
did you try the version i gave you, with the INTERVAL syntax should produce the same result, and uses standard SQL rather than the proprietary microsoft DATEADD function
i did but it tossed an error. Incorrect syntax near '1'
Are you planning an upgrade for that environment? 2008R2 goes EOL in about 6 weeks. You need to move to a newer release soon.
Yes actually in the process of doing a side by side migration right now.
I might be misinterpreting your intent, but I think you want ItemID to be an IDENTITY column so that it automatically increments. Once you've changed that, just leave it out of the INSERT and SELECT lists altogether.
The row acteur.x (=actor) is from a different table than regisseur.x (=director). I managed to create a table with the two, but now I have to filter out some actors. I only want to have actors that play in multiple films of a particular director.
Thanks for the reply, I didn't create the database, (I'm no SQL expert; but I'm learning. I'm a Sysadmin) it's from a CRM company. I know the rows auto increments from the front end, is this the same thing as Identity? Either way, I'll try all the sugestions in a duplicate, backup up db and let you know if it solves my issue, thanks :).
Then say 5+ years experience. They can't just keep offering you more money, then when you have 60 years experience (you may still have all your faculties and still be working, at age 75!) it doesn't mean they pay you $4,000,000,000 We had something at my work where your number of days of vacation is tied to years of service, we had people who would take a solid 3 months of vacation. Which is tough to manage, to find people to cover for all that vacation. So I can see that a smart company finds a way to not pay for someone with infinite experience.
`select` `director_first_name + ' ' + director_last_name AS director_full_name` `, actor_first_name + ' ' + actor_last_name AS actor_full_name` `, COUNT(nr) AS times_appeared` `FROM my_table` `GROUP BY 1, 2` `HAVING COUNT(nr) &gt; 1`
What actor/Woody combination is returned, and what value is returned for the count column? I'm not sure I understand why u/Wafzig's suggestion would fail.
I manage the database behind an industry-specific ERP, and the developers chose to have the application manage the row IDs (PKs). They do so by maintaining the next available PO number, Invoice number, Order number, transaction number, etc. in a table called SEQUENCE, and locking tables down while transactions are processed and committed. It's a performance, constraint, and bug nightmare. SQL Server does the job of auto-incrementing so well that I'd need a damn good use case to design anything otherwise. But if, as in my case, they're the cards you're dealt, then so be it. I know your pain...
One page resumes are out of fashion, and it's all about the 2 pager now. 1st page gives some kind of a summary paragraph, a bulletted list of the various technologies / disciplines that you have experience with, and it concludes with a description of your *current* job. If you have problems filling that out, add education, but honestly I don't even list education on either pages of my resume at this point. 2nd page is a work history going back a number of years, where you worked, what you accomplished there, and what skills/technologies were used. The 1st page is basically all you need, but the 2nd page will give you a few years of work history and looks sort of like an "appendix" in a book. It really breaks things up nicely and drives good conversation.
Thanks for the reply! But where do the 1 and 2 in GROUP BY come from?
It is great advice. Workers talk about technologies and buzz words, but good workers talk about accomplishments. Like at this point in my career, like it or not, if you ask me if I know what a GROUP BY does or how an INNER or LEFT JOIN differ then I'm going to take offense. I'm not going to take it out on you in a bad way, but like, WTF are you interviewing me for a senior role if you think I don't know a question like that? In my response to your question I might not even use the term "inner" or "left join" and just talk about something I accomplished where understanding those things were necessary.
The actor 'Woody Allen' combined with the director 'Woody Allen' is being returned. The count value is 2 for all of them.
 SELECT ... FROM ( select ... FROM ... order by 1 DESC, 2 ) DETAIL , -- THIS IS WHAT YOU SHOULD PUT BETWEEN THE SUBQUERIES ( SELECT ... FROM ... ) EWS WHERE DETAIL.ITEM_ID = EWS.ITEM_ID (+) --my second subquery ends here GROUP BY ...
It may not do you any good, but as someone who has also worked in tech 20+ years, I was homeless for about 5 years, but thanks to friends, family and / or motel money I never hit complete bottom. &amp;#x200B; I did have $31.50 cents in change and I had to buy an internet card that cost $33.25 with tax, and I knew if I used the coin star machine I didn't have enough money. As I was counting out the change and taking all this poor cashier's time, the man behind me started complaining and I explained I had a job interview in 3 hours and if I do not have internet I do not have a phone either and it was a web cam interview. &amp;#x200B; The man that complained behind me line paid my last $2 I was short, and I got a job out of it. And that is just one of the many stories that saved me. The biggest thing you do in your favor is constantly learn new things. &amp;#x200B; Never give up. I can tell you where the homeless in my neighborhood sleep, and it is really sad the way society (capitalism) leaves those behind that are broke. There is no pass go and collect $200. Life is not a game and you are just out if you get to the place of no place to live, no phone and for anyone in this group you have to have the internet to find a job. &amp;#x200B; Colonel Sanders (Harland David Sanders) founded Kentucky Fried Chicken when he was 63, and he didn't make a profit until a few years later.
they are the first 2 columns in the select statement above. I BELIEVE, this is just short hand. the group by is required because the count statement is an aggregate function, so, it needs to know how to roll everything up.
Taking a guess that you are using the forms inside Access to make this nice and pretty. if you are, you should be able to have a calculated field on the Form/Report. (Its been years, cut me some slack.) The calculated field wouldn't be terrible. It sounds like the license amount is finite so that could be hard coded and updated when you buy more. then it would be a (Finite License Count) - (count of the machines with the software) = Remaining license count.
Thanks for clearing that up! :)
Select every column, group by every column, where count of every column &gt;1
Already tried this, but it unfortunately didn't work... When I try this method, it will filter out 2 of the 3 Woody Allen movies, but it needs to remove all of them (because every movie has a different actor). I'm actually clueless on this one and on the edge of just giving up...
So then you haven't phrased your question properly. What are you trying to do?
What RDBMS?
Yes actually in the process of doing a side by side migration right now.
MySQL
How are you defining those aggregate columns? The first is probably something like: with t1 as ( select date, user_id, count(video_id) as ct from table group by date, user_id ) select t.date ,sum(ct) from table as t left join t1 on t1.date = t.date group by t.date; The other column is probably something like: with t1 as ( select date, user_id, count(video_id) as ct from table group by date, user_id ) select t.date ,sum(ct) from table as t left join t1 on t1.date = dateadd('d', 1, t.date) group by t.date; How you get both of them in the same table is going to depend on RDBMS, how large the tables are, and the requisite performance from the query. What I wrote isn't really optimized but it's a starting point depending on how you are defining those columns.
All of these suggestions are good ones and you should try them. However, remember you are not employed. You have nothing to lose. If you think the gaps in your employment history are a problem, hit them in the interview process before they do. Then pitch yourself as being willing to work under wage to allow the company to understand that you aren't the risk they see in your resume.. Of course you need to put a timeframe on it and say when you hit that ykj should get a proper wage. Taking lower pay in the short term and betting on yourself is the best risk you can take.
 Group by 1, 2 Does work, but I dobt use them very often because it may not be clear in the future. Its typically better to explicitly name the values like Group by director_first_name + ' ' + director_last_name, actor_first_name + ' ' + actor_last_name
RDBMS, SQL Again, I‚Äôm no expert in this matter, when you say ‚Äúct‚Äù is that COUNT and would you simply put these queries above all in one big query? Also, how would you tackle the 3rd column?
Real time tableau dashboards.
Or use excel to create dashboards.
Not sure if it's universal across SQL dialects but in Postgres &amp; MySQL it's shorthand for "first and second column of the SELECT statement"
Optimization and indexing. These will go a long way towards tightening up your SQL and speeding up process while not taking up resources on the server.
ü§§ live data ü§§
Learn dynamic sql, stored procedures and functions, it's not just about can you retrieve it, most business they've in can you repeat it consistently at scale
Download SSMS
Indexes, indexes, indexes.
In my experience; knowing your joins are incredibly useful
Or powerBI
&gt;Explaining your work history is an interview trap. The quicker you get off the subject the better. How so? Would you mind elaborating on this?
I like this, thank you.
Qlik Sense is the way to go.
Knowing the difference between the joins was the most complicated thing in my interview. And it took me about 2 hours on HackerRank and w3 to learn all I needed about them. In fact, all I did was a weekend of HackerRank. Companies know you'll come in pretty raw, they expect it. Just be confident, know joins and then learn it all on the job like we all do.
It all depends on what you want to do. Are you interested in reporting alone or do you want to do performance tuning and more ‚Äútechnical‚Äù things? How hard you wanna go will dictate what type of experience you need
Both the bane of my existence and my favorite thing.
Joins are important, but kind of a basic knowledge of SQL. Getting into performance optimisation, sub queries/CTEs, window/aggregate functions. Once you hit that point, you're golden. OP sounds like he needs to put it on his CV.
I think because I am fairly new to SQL, I want to continue with data reporting and analysis. I believe that as I learn more about SQL I can take on more advanced stuff. I guess what I am trying to ask is if joins, filters, and subqueries are good enough to land a job that focuses on using SQL for data reporting mainly.
Well.. it's been said that the traditional sysadmin-type DBA job is dying out slowly (and sadly), but there is a ton of work for people with that skillset in different positions. Why not study up on Azure and present your skills as a SQL Server professional with DBA-esque knowledge. Learn Python if you can find the time, pretty prominent in data and the cloudscape and a first class citizen with Microsoft these days. Also, are you on LinkedIn? Learning topics and putting keywords in your "skills" section on there immediately draws attention from recruiters.
If you know these areas you'll be light years a head of most other people. And by knowing these areas, I mean know that they exist and can Google stumble your way to the answer. 1) SELECT, FROM, WHERE, GROUP BY, HAVING 2) Left Join vs Inner Join vs Full Outer Join vs Subquery 3) Able to SUM and find Percents 4) Conditional and logical operators ( &gt;, &lt;, =, &gt;=, &lt;=, BETWEEN, AND, OR) 5) Temp table vs CTE vs Derived table 6) Know how to use row_number, know of rank and dense_rank 7) NOT IN and NOT EXISTS
Its doable with VBA with an Event Procedure too. I'm not sure how your forms are set up, but here's some sample code you can repurpose: Private Sub Check5_AfterUpdate() 'making assumption checkbox is tied to a specific piece of software check = Check5.Value 'using generic value of textbox for current license count license = Text7.Value If check = -1 Then 'checked license = license - 1 ElseIf check = 0 Then 'unchecked license = license + 1 End If 'Update count of unused licenses Text7.Value = license End Sub
Learn about optimization and indexing. These will tighten up your SQL, make your queries faster, and utilize less resources on the server. Other things you can learn if you haven‚Äôt are temp tables, CTEs, pivoting data, cross apply joins, and SQL agent jobs. There‚Äôs always something to learn.
I guess you mean "for Microsoft SQL Server", right? "SQL" is a query language used by all relational databases.
It generates invalid standard SQL.
I'm still in college, I'm trying to learn it and acquire some certificates before I graduate. I'm fairly new to SQL at the moment.
Do you still need a solution?
Go on...
understanding them, as well as how and when to use them will make your databases soooo much better, it will make bug hunting *much* easier when you know your queries will run well against your tables. Another good thing to study would be normalization, or in other words, *preventing* data anomalies. A good rule of thumb is normalize til it hurts, then denormalize til it works. Dont worry so much about ER diagrams, I never use them, I don't even know if any of my colleagues do. They are a good exercise to learn though. If you are looking to make an app and take sql into a wide range of possibilities (beyond web), take a look into sqlite. It can be embedded into all kinds of shit. There are probably hundreds of them on your phone, router, computer, even a few in your microwave. For a web app, look into node.js, it has a bunch of libraries that are handy for interfacing with Postgres, sql server, mysql, oracle, sqlite, db2, on and on. node can be embedded onto things as well so its a handy frame work if you know some javascript.
I think it's because the actors and directors are from different tables. Woody Allen does occur more than once as an actor and as a director, but only occurs once when Woody Allen is both the actor AND director.
&gt; normalize til it hurts, then denormalize til it works I love this!
Ok - and I think you‚Äôll be good. I would learn about views, stored procedures and functions (table valued....scalar values are still fairly problematic). I do some reporting at my job now and we use stored procedures fairly often.
Good! Here, let me make it a lot easier for you. * https://dbatools.io/ * https://docs.dbatools.io/#Start-DbaMigration
I think the MTA is a good stepping stone and I'd work for the MCSA and MCSE afterwards.
Yes, this. Any knob from India can write a query that will return the correct results, but writing a working query that won't take hours to execute is less common.
From your description, it sounds like your SQL class stopped before you started learning advanced querying techniques. Database design (normalization, indexes, stored procedures) are great things to learn, but you might want to firm up your querying knowledge more completely before stepping into that pond. I suggest you make sure you learn the following if you haven't already: * Set operators (e.g. UNION, INTERSECT, EXCEPT) * Common Table Expressions * Subqueries * Case expressions * Aggregation (grouping) * Windows functions * Data modification with INSERT, UPDATE, and DELETE BI tools like Tableau, Power BI, and Alteryx are great, but they can be fussy about extracting data. SQL is remarkably efficient. If you learn SQL well, you can get your data in a format that works well for plugging into those tools and worry less about each tool's particular quirks of data wrangling. As general advice about what to learn next, I suggest you consider your goals: Do you expect to be doing more data retrieval or more development?
You nailed it with ‚Äúknob from India‚Äù! Brent Ozar has his own web site with a few free classes that are a great starting point. Stackoverflow is my go to for researching SQL questions. Have fun
If you're an actual developer I agree, but the default functionality is overall not significantly better than pbi or Tableau. With mashups and extensions that changes. Qlik easily has the best core, but their default frontend is meh.
I work mostly with SQL Server, so some of this comes from that perspective. Download [SQL Server Developer Edition](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) and use the [Stack Overflow Database](https://www.brentozar.com/archive/2018/06/new-stack-overflow-public-database-available-2018-06/) to continue learning after you leave your current job. As for "how much do I need to know", that depends on what you're trying to claim you know and what job you're trying to apply for. We are currently doing interviews for developers and if someone says they know SQL, the first questions around that are to try to figure out A) whether or not they've actually used it, and B) how much they actually know. Having a good understanding of the various types of joins is important. Understanding what a cartesian product is can be beneficial. Do you know aggregates? How about subqueries and CTEs? Do you have a local SQL User Group? If so, start going to meetings. See if there's a SQL Saturday happening anywhere accessible to you in the near future - it's a great way to get a day of free training on SQL Server.
This is the real answer. OP's class stopped at joins, recommending things like query optimization and dynamic SQL is a running-before-you-can-walk situation. Obviously those things are good to know, but not before things mentioned in this post.