Thanks Laziness. `~Your username doesn't seem apt, if you don't mind me saying.` :&amp;#94;) This looks very promising – almost exactly what I'm looking for in fact – although I think I'd need figure out limiting the bookings examined to just those spanning the criteria dates. Unfortunately, however, before I can even get to that, I'm getting syntax errors. I've stripped out the DATEADD stuff (which, as you rightly say, I don't need anyway) and corrected the date column names (added underscores), but it's still erring. I've tried to isolate the syntax error by temporarily running just the sub query without the union, and the problem seems to be with the second ON clause (the subquery). If I remove that clause – or change it to just: `AND B1.booking_ID = B2.booking_ID` – it stops the syntax error there, but it then causes different errors when I try putting it like that back into the whole query. Not that it would work as planned like that, I'm just trying to grasp what the problem is and how it might be solved. Any ideas? Oh and I don't know if this makes any difference, but the version of Access I'm running is 2007. 
Thanks hypo11. &gt;Is there any reason this needs to all be done in 1 query? None whatsoever. &gt;I don't know if access allows stored procedures, but is there any way to store a series of queries and use temporary tables? [See here](http://msaccessmemento.hubpages.com/hub/Stored_Procedure_in_MS_Access) – in particular, check out the section on limitations. It seems I can create some limited stored procedures but it doesn't support temporary tables. That said, I have no objection to using permanent tables and emptying them out before usage, if that would work.
Thanks taejim. I was sure I'd posted a reply to this post earlier, but it seems to have been eaten by the Reddit gremlins. :&amp;#94;( &gt;This is known as a gaps and islands query. This little snippet of information would have been very useful in my search for a solution to this problem. Searching on "sql dates availability" etc., was proving somewhat useless. Access does have a lot of shortcomings it seems, but I think I should be able to work out how to do most of the above – albeit very slowly as I don't know T-SQL at all and I only know rudimentary SQL anyway. 
Wouldn't this work as well without a UNION? SELECT dbo.cats.columns, dbo.dogs.columns FROM dbo.cats, dbo.dogs WHERE dbo.cats.cat = "black" OR dbo.dogs.dog = "brown"
If you are running MS SQL server, you can download and install the Express Edition on your workstation (for free, though limited) and mess around on your own system without causing harm to the Production server. MS SQL has a built-in query designer from which you can learn correct syntax (building a query is easy, debugging it takes most of your time). I've learned from W3Schools SQL course and a lot from pre-existing queries already used on our systems. You should also look at your tables and learn what the primary keys are ect., but thats more 'meta-knowledge'.
I wrote this in TSQL which is very similar to Access' SQL dialect but not quite. I apologize for that. If I remember right, Access might not allow complex ON clauses. Try removing the part right after the error and make it a WHERE condition. In other words, change: LEFT JOIN Booking B2 ON B1.slot_ID = B2.slot_ID AND B2.booking_ID = ( SELECT TOP 1 booking_ID FROM Booking WHERE slot_ID = B1.slot_ID AND startDate &gt; B1.startDate ORDER BY startDate ) to: LEFT JOIN Booking B2 ON B1.slot_ID = B2.slot_ID WHERE B2.booking_ID = ( SELECT TOP 1 booking_ID FROM Booking WHERE slot_ID = B1.slot_ID AND startDate &gt; B1.startDate ORDER BY startDate ) Oh and I'm pretty lazy. But typing doesn't take much effort. ;) Edit - spelling
There's absolutely no need to apologise for anything! I'm just extremely grateful you've taken the time to try and help me. Right, that's getting me a lot closer, I think. The Null expression in the union was causing a "`Data type mismatch`" error, so I changed it to "`CDATE(0)`" and the final WHERE clause to "`WHERE startDate = 0 OR endDate = 0 OR startDate &lt; endDate`" and that's got rid of all the errors, but now I'm getting a dataset returned that's not quite right: slot_ID|startDate|endDate :---|---:|---: 1|00:00:00|00:00:00 1|2011/10/25|2011/10/31 2|00:00:00|00:00:00 3|00:00:00|00:00:00 3|2011/10/19|2011/10/23 5|00:00:00|00:00:00 As you can see: apart from the two "islands," none of the other dates abutting bookings are being returned. Also, there should be two records for slot 2, three for slot 3, and none for slot 5. This is the query as it's working now: SELECT slot_ID, startDate, endDate FROM ( SELECT B1.slot_ID, B1.end_Date AS startDate, B2.start_Date AS endDate FROM Booking B1 LEFT JOIN Booking B2 ON B1.slot_ID = B2.slot_ID WHERE B2.booking_ID = ( SELECT TOP 1 booking_ID FROM Booking WHERE slot_ID = B1.slot_ID AND start_Date &gt; B1.start_Date ORDER BY start_Date ) UNION SELECT slot_ID, CDATE(0) AS start_Date, MIN(start_Date) AS end_Date FROM Booking GROUP BY slot_ID ) WHERE startDate = 0 OR endDate = 0 OR startDate &lt; endDate EDIT: Fixed the table.
What kind of database is this? Can you post the table structure for both tables? When you say 'make sure the data is the same', do you mean constraints to prevent data from ever being different or simply find those differences?
How do you plan on joining the 2 tables together? Is there a shared key besides start and end datetime?
You don't really need to write a function. Converting the dates to a number or text in your sql (YYYYMMDD) should work.
There is, lets call the it the appointment_id column, one per start and end date/time. In both tables it has the same appointment_id.
Yes, data in both tables should be identical, every start and end date/time has the same 'appointment_id' but both need to be checked since the start and the end date could differ.
I think I know why. Moving that chunk from the ON clause to a WHERE clause didn't quite translate properly. It essentially turns the LEFT JOIN into an INNER JOIN. So now it's ignoring all of the cases where B2.booking_ID might be null. We might need another UNION. When I get home from work I'll look around for my old copy of Access 2003 (backwards compatible with 2007) and see what I can come up with. 
 select t1.appointment_id, -- check if start date/time matches case when cast(t1.date as char) + ' ' + cast(t1.start_time as char) = cast(t2.start_datetime as char) then true else false end as start_matches, -- check if end date/time matches case when cast(t1.date as char) + ' ' + cast(t1.end_time as char) = cast(t2.end_datetime as char) then true else false end as end_matches from t1 inner join t2 on t1.appointment_id = t2.appointment_id You might need to play with that cast function, depending on what SQL flavor you are using. Same for the concatenation method. But I think the basic idea is there.
http://stackoverflow.com/questions/700619/how-to-combine-date-from-one-field-with-time-from-another-field-ms-sql-server you said you tried this but lets give it ago anyway :p begin with trying out just the cte itself to see if it actually works the way you want it to (joinable data). using tbl1 as select date+starttime as dt1, date+endtime as dt2 from table1 select * FROM tbl1 inner join table2 on tbl1.dt1=table2.startdate and tbl1.dt2= table2.enddate
I'll try to toy with that. I hoped I didn't have to build a string from them, because I know how to do that. *SELECT Combined = MyDate + MyTime FROM MyTable *doesn't seem to work, could there be way to null the days and/or time? So they will add right? Unfortunately for me I really wish I had a shared key, those 2 tables are from different SQL databases (which are not linked). The tables only have a start, end date/time, customer_name, employee_name and a unique_id that differs in both databases... It looks more like this IRL: table|Start time|End time|(date)|appointment_id|customer|employee |:------------:|:------------:|:------------:|:------------:|:------------:|:------------:|:------------:| table 1|08:00:00|09:00:00|31-10-2011|892413|CustomerA|Employee50 table 2|31-10-11 8:00|31-10-11 9:00|null|5476|CustomerA|Employee50 But that's not the point of my exercise. I can only run the query on my own workstation's DB.
&gt;I think I know why. Moving that chunk from the ON clause to a WHERE clause didn't quite translate properly. It essentially turns the LEFT JOIN into an INNER JOIN. So now it's ignoring all of the cases where B2.booking_ID might be null. Yeah, that's what I thought too… honest! :&amp;#94;D &gt;We might need another UNION. When I get home from work I'll look around for my old copy of Access 2003 (backwards compatible with 2007) and see what I can come up with. That's uncommonly kind of you, thanks! I wish I understood this stuff better, but it's hard finding time to learn something I only deal with very rarely. This is for a report that I'm trying to create in between all of my other onerous duties. :&amp;#94;) That said, once it's done I'm sure I'll find some more uses for it.
It *might* work if you cast MyDate as datetime (assuming it is just date now) before adding MyTime. The key issue sounds like a mess :) Good luck.
 select t1.appointment_id, case when ((DATEPART(YEAR, t1.date) - 2010) * 31556923) + (DATEPART(DAYOFYEAR, t1.date) * 86400) + DATEPART(second, t1.starttime) + DATEPART(minute, t1.starttime) * 60 + datepart(hour, t1.starttime) * 3600 = ((DATEPART(YEAR, t2.starttime) - 2010) * 31556923) + DATEPART(DAYOFYEAR, t2.starttime) * 86400 + DATEPART(HOUR, t2.starttime) * 3600 + DATEPART(MINUTE, t2.starttime) * 60 + DATEPART(SECOND, t2.starttime) then 'true' else 'false' end as start_matches, case when ((DATEPART(YEAR, t1.date) - 2010) * 31556923) + (DATEPART(DAYOFYEAR, t1.date) * 86400) + DATEPART(second, t1.endtime) + DATEPART(minute, t1.endtime) * 60 + datepart(hour, t1.endtime) * 3600 = ((DATEPART(YEAR, t2.endtime) - 2010) * 31556923) + DATEPART(DAYOFYEAR, t2.endtime) * 86400 + DATEPART(HOUR, t2.endtime) * 3600 + DATEPART(MINUTE, t2.endtime) * 60 + DATEPART(SECOND, t2.endtime) then 'true' else 'false' end as end_matches from table_1 t1 inner join table_2 t2 on t1.appointment_id = t2.appointment_id
The only reason to have photo_belongs is to allow photos to exist in multiple albums. If deleting album A removes photo P and photo P also exists in album B then your data is now fucked. If a photo can only exist in one album then you want to store the album id in the photo table linked via foreign key. Then the delete is cascaded to Photo on album delete. I'll leave it to you to decide what to do for photos not in albums - there are a few options. 
It would be nice to know the basic schema to know how these tables are connected. Also Coderascal makes a good point, if photo_belongs only serves to connect the two And the photo may be in a single album, then you should just link photo_id to album_id.
Okay, I assume that salary grade is in the demo.salary_grade table, so expanding on your initial query, you could get what you need by using a simple where clause limitation. (comfortablydrei was close): SELECT e.last_name ,s.grade_id from demo.employee e join demo.salary_grade s on e.salary between s.lower_bound and s.upper_bound where s.grade_id in ( SELECT s.grade_id FROM demo.employee e, demo.salary_grade s WHERE e.salary between s.lower_bound and s.upper_bound AND e.last_name = 'FISHER' group by grade_id) ORDER by grade_id desc; EDIT: Or if you prefer to keep your join formatting consistent, then use this: SELECT e.last_name ,s.grade_id from demo.employee e, demo.salary_grade s WHERE e.salary between s.lower_bound and s.upper_bound AND s.grade_id in ( SELECT s.grade_id FROM demo.employee e, demo.salary_grade s WHERE e.salary between s.lower_bound and s.upper_bound AND e.last_name = 'FISHER' group by grade_id) ORDER by grade_id desc; EDIT2: Formatting
Depending on your flavor of SQL, you could first create a temporary table using a row_number() function, then do a self join from that table in order to do the process. Example: CREATE VOLATILE TABLE serpaderp, no log as (select s.* ,row_number() OVER(PARTITION BY s.keyword_id,s.search_engine_id ORDER BY time_retrieved DESC) seq_nbr from 'serps' s where search_engine_id IN (1,2))WITH DATA PRIMARY INDEX(keyword_id,search_engine_id) ON COMMIT PRESERVE ROWS ; select s.* ,s2.rank AS prevrank ,s2.time_retrieved AS prev_time ,s2.rank-s.rank AS moved from serpaderp s JOIN serpaderp s2 on s.keyword_id = s2.keyword_id and s.search_engine_id = s2.search_engine_id and s.seq_nbr = s2.seq_nbr - 1 where s.seq_nbr = 1 order by s.time_retrieved,s2.time_retreived DESC ; Again, depending on your flavor of SQL this may not work for you...
Okay, go to this link: http://www.2shared.com/document/Ldq6UMyR/bookings.html I googled for a file upload service and this was the first hit. So I uploaded it to here. Sorry if it's spammy. Anyway, I uploaded booking.mdb. You should grab it and open it. It's got your table, two working queries, and a parameterized query where you can enter a date and get non-booked slots for that date. It took awhile to get around Access' weirdness but I eventually figured it out. The query is pretty much the same, but shuffled around a bit. Let me know if you have any questions! 
might be good to cast a static there to identify the table source so if you plan to update the tables you know which is which. if you make the above query a view - can you update it as a view? or would union break that functionality? wierdest thing i've seen lately using union to update twice instead of sending two update statements
That is fantastic! Thank you so much for going to all that trouble. When I have a bit more time later, I will study it to see if I can work out what it's doing, and how. In the meantime though, it's provided me with exactly what I needed. The only small problem was it ran mighty slow (several minutes) on the real data, because it evaluated every booking in the table rather than just those within the criteria dates. I spent some time trying to write those dates directly into the query, but I couldn't get it to work. So instead of that I've created a temporary holding table, populated only with details from bookings spanning the criteria dates, and run the query against that. And it works great! Taking just a few seconds to run, which is perfectly acceptable for a report building process. Thanks again for all of your invaluable help on this, it is very much appreciated. 
Make sure your date columns are indexed. That should speed things up.
This is a very interesting query, thanks!
The original values weren't good for me (see below), so i had to convert them, this is the solution: Select (CONVERT(VARCHAR(10), table1.[Allocation Date], 105) + ' ' + CONVERT(VARCHAR(8), table1.[Starting Time], 108)) as Starttime, (CONVERT(VARCHAR(10), table1.[Allocation Date], 105) + ' ' + CONVERT(VARCHAR(8), table1.[Finishing Time], 108)) as endtime FROM table1 A simple statement like I said in my previous post doesn't work. The following query, gives me a 'wrong' answer: SELECT [Allocation Date] + [Starting Time] as result 2011-10-12 00:00:00.000 + 1754-01-01 08:00:00.000 = 1865-10-12 08:00:00.000
Yep, that's speeded it up nicely, thanks.
You're welcome. BTW the way the query works is pretty simple. For each row in the table it takes the booking end_Date as the non-booked start_Date, and the booking start_Date of the next row in order as the non-booked end_Date. Joining a row to the immediate row ahead of it chronologically is achieved via the subquery that begins with "select top 1 ..." which I'm sure you can figure out how it works if you examine it. The query after the union is for adding the rows that have a null start date, which the first query doesn't cover. I enjoy encountering and trying to new solve problems with SQL so this was fun for me. I've never had to solve something exactly like this. This isn't really quite the "islands and gaps" problem that it appears to be, since the islands and gaps already exist in the table. And approaching it that way would be overkill. Rather this is essentially a reordering problem, and a surprisingly simple one by comparison. 
Well it may be pretty simple to you, but it's quite a bit more advanced than anything I'm used to. Prior to this, I've never really gone beyond relatively simple JOIN and GROUP operations. I'm glad you enjoyed it. I generally like solving problems too, but you sometimes have to admit when you're out of your depth. I'm not exactly fond of SQL either, to be honest. I find it tedious having to type out all the terms and field names, ad nauseam. And nothing ever seems to work quite as I expect it to, first time. I guess if I spent more time working with it, and understood it better, I'd find it more rewarding trying to solve such problems as I occasionally encounter without seeking outside help. 
Thank you very much for your help. 
I struggled a lot when I first started. And it's taken me about 10 years to get comfortable enough with the different database engines and SQL dialects. But I still struggle with some things. Interpreting query profiling output is currently a thorn in my side. I think if you keep at it, you'll get to a point where things start working as you expect them to. And when that happens then it becomes kinda fun. Anyway, glad I could help and if you ever have other questions don't hesitate to ask.
Isn't it mainly a question of how to order the results? How about something like this? SELECT AnimalName, FoodName, Purchase_Date FROM animal JOIN animal_food USING (a_id) ORDER BY AnimalName, CASE WHEN Purchase_Date &gt; current_date THEN 0 ELSE 1 END, Purchase_Date 
Yes but the problem with this is the results would be ordered like this, wouldn't they: AnimalName FoodName Purchase_Date ---------------------------------------- Bird Bird Seed 2011-12-12 Cat Tuna 2011-11-11 Cat CatBix 2010-06-04 Dog DogBix 2011-11-27 Dog Bones 2012-06-08 Dog Pedigree Chum 2011-02-08 Rabbit Rabbit Mix 2011-09-02 Rabbit Carrots 2011-05-04 (I've had to change a date as I'd sorted them in alphabetical order by accident) The animal's name would be sorted before the earliest date.
Unfortunately – or fortunately, depending on your point of view – I don't have to deal with it very often, so it's unlikely I'll ever become proficient enough to consider it "fun." :&amp;#94;)
So you want the dates sorted in ascending order for the future, and descending order for the past?
Yes, but once they've been grouped by the animal after being sorted by animal with date closest to now.. I've got this SQL snippet hanging around: ORDER BY purchase_date &lt; NOW( ) , ABS( DATEDIFF( purchase_date, NOW( ) )) 
 AnimalName FoodName Purchase_Date ---------------------------------------- Cat Tuna 2011-11-11 &lt;- date closest to now in the future. The food belongs to cat, so Cat CatBix 2010-06-04 now list the rest of the cat foods in ascending future, followed by descending past Dog DogBix 2011-11-27 &lt;- date SECOND closest to now in the future. The food belongs to Dog Bones 2012-06-08 dog, so list the rest of the dog foods in ascending future, Dog Pedigree Chum 2011-02-08 followed by descending past and repeat Bird Bird Seed 2011-12-12 Rabbit Rabbit Mix 2011-09-02 Rabbit Carrots 2011-05-04
Thank you for your response, this makes sense. Photos are in fact only able to be in one album. This makes the design simpler. Sorry if my explanation of the schema was not complete.
So going with distalzou's contribution combined with yours, it looks like we'd have: ORDER BY AnimalName, CASE WHEN Purchase_Date &gt; current_date THEN 0 ELSE 1 END, ABS( DATEDIFF( purchase_date, NOW( ) )) asc I would think that would probably get you. I think you got the tough part licked with the datediff.
Wait... you mean you don't want the animal groups to appear in alphabetical order?
My favorite part about this is believing that three days ago you turned in a wrong answer for homework.
Too bad. With MigraineGO and my professor's help, I was able to get the correct answer. 
I wonder whether you told that professor that the code you brought him or her wasn't written by you. 
Yeah, cause I totally took this answer and submitted it. I didn't ask for the answer, I asked for help and I got it. 
What you want is an [RDBMS](http://en.wikipedia.org/wiki/Relational_database_management_system). Try something like postgres or mysql.
http://www.google.com/search?q=sql+learn The top links are all great resources and will ramp you up quickly.
[Here you go](http://www.w3schools.com/sql/default.asp) Enjoy :)
Subscribe to: http://www.sqlservercentral.com/ Then subscribe to: http://www.microsoft.com/web/websitespark/ (Includes Visual Studio, Expression, SQL Server, Windows Web Server and more...) This will get you all the software you need to get started. You just need SQL, and it will come with all the tools you need. Also install the NorthWind database, which it normally has, as MOST websites and books use this as the main reference for examples. Then, learn the 4 basics of SELECT, INSERT, UPDATE, DELETE. (T-SQL is very much the same whether its MySQL, Oracle Access or MSSQL, might be slight syntax differences here and there) And focus on how to use JOINS in SELECT statements (Putting two or more sets of data together.) 
The Schemaverse will force you to learn SQL :D http://schemaverse.com
[This](http://www.amazon.com/Beginning-SQL-Programmer-Paul-Wilton/dp/0764577328) this the book I'm using and I'm quite happy with it. It's not free, but it's only like $4 second hand.
Thank you everyone for all the links, I very much appreciate it.
MySQL.com is a very good resource. It is not a tutorial, but the docs are very clear and easy to understand. I would download either Oracle express and Oracle SQL Developer or MSSQL express and MSSQL Management Studio and start building, populating, and running queries on your own database. The MS suite is easier to get started with if you are absolutely new. You have to use Oracle or MS which sucks, but the GUI tools are really nice to have while learning. I have not found comparable tools for open source databases.
I don't suggest you to learn _MySQL_ first. Learn _SQL_ before. The reason is simple : MySQL is not close at all to follow ANSI SQL. If you focus solely on MySQL, you will lack __a lot__ of knowledge and you will most likely not know how to do very simple queries when/if you make a switch to more advanced DBMS (for instance, CTE and windowing functions, even CHECK constraints that beginners surprisingly are not aware of). You can either read DBMS-agnostic books, or use PostgreSQL which is very, very close to follow every ANSI SQL standards. Then, you can go back to MySQL.
You can download trial versions of SQL server and run it locally on your computer. Then get a good book with lots of exercises - some of the MS ones even come with trial CD's. The only way to learn is by doing.
Sorry to hear about your job. I hope you find a good one soon. You can download free developer versions of Oracle, Microsoft, DB2, and of course open source databases like PostgreSQL and MySQL. The manuals are all online for downloading. SQL itself isn't that hard to learn, it is very much a "Do What I Mean" kind of language. The tricky bits are schema design, normalization, performance, operations and security. Abstrct's schemaverse.com can be a great way to motivate yourself to learn SQL in a game setting. I had to stop playing because it was sucking up all my time.
People posting that you should learn standard SQL before any vendor specific implementations are absolutely correct. After you get through the basics, here are a few books that are awesome for intermediate SQL developers: http://shop.oreilly.com/product/9780596518851.do - will show you the special extra features of the major DB implementations. http://pragprog.com/book/bksqla/sql-antipatterns - There are not enough books like this out there, this will show you many common database design pitfalls and how to avoid them. 
Might also look into [recursive CTE's](http://msdn.microsoft.com/en-us/library/ms175972.aspx).
I agree with this. Learn ANSI SQL than any brand specific ( MySQL, TSQL, PL/SQL or whatever ). Once you get hold of ANSI SQL .. you can pick up almost any RDBMS Product.
Take a look at [Database normalization](http://en.wikipedia.org/wiki/Database_normalization).
Definately a better structure, you really only need one Database. In that database a table for games, one for players, onefor standard stats that would contain a forgein key to the game table and a nullable one to the player table. If the player key is null then its a general stat, if not its a player specific stat. For those stats that are one off, you'd need another table layout that takes a few different variable, maybe like a name &amp; value column. Once you standardize it all it will be very easy to pull stats for every game, every player, and more. Be sure to add some timestamp fields and you can calculate stats on an incremental basis to show how the players got better or worse, etc. 
Will this work if I am tracking different stats for each game? And in particular the thing that I would want to track gamestate wise (board position related stuff) would be different than things I would track for players of that game (points, ships destroyed, countries conquered) so I would need different fields to track them regardless.
Are you looking for the combined byte-size of all values for each row in that column? I know in Oracle you could run: Select sum(vsize(reader_list)) from table; Since you're not using oracle that's less than useless, sorry. 
From a fellow sysadmin, check [this](http://ola.hallengren.com/) script out. The guy who wrote this is a MCITP Database Administrator, MCITP Database Developer, SQL genius and awesome for sharing his hard work for free with the rest of us. Lots of companies use his script, and so do we.
What database are you using?
We'll need a bit more information. What RDBMS are you using? Are there indexes that could contain reader_list that you'll need to take into account? Is your calculation (of the percentage of the database size that readers_list) a valid comparison - that is, given that NetName is likely to be minuscule compared to reader_list, could you simply take the entire table size, and accept that you might be 20 bytes/row off in your calculation? What are you trying to achieve with this calculation? Is it programming-related, or DBA-related?
Agree - the maintenance plan functionality is OK, but in my opinion, running stored procs through SQL agent jobs is your best bet.
Brad McGehee recently published a decent introductory free book covering the basics of sql server maintenance plans. You might find it well worth your time. [Brad's Sure Guide to SQL Server Maintenance Plans](http://www.red-gate.com/products/dba/sql-backup/entrypage/backup-maintenance-plans)
You might want to consider one of the more fully featured versions of SQL Server. You'll find many more tutorials and applicable documentation sets for the more widely used editions. Both trial versions of Standard, and the freely available Express edition are available under the "SQL Server 2008 R2 Downloads" tab [here](http://www.microsoft.com/sqlserver/en/us/get-sql-server/try-it.aspx). Most tutorials covering SQL Server reference either the old Northwind (no longer included with SQL Server 2005 and up, but available [here](http://www.microsoft.com/downloads/en/details.aspx?FamilyID=06616212-0356-46a0-8da2-eebc53a68034&amp;displaylang=en) or the newer [AdventureWorks](http://blogs.msdn.com/b/bonniefe/archive/2008/02/28/the-adventureworks-2008-family-of-sample-databases.aspx) sample databases.
I am using sqlite3. Well, i guess i can live with that assumption. But, is there any command that i can call through sqlite3 C APIs programatically.
i m using sqlite3 through C APIs.
Actually just wanted to know if there is any query in sqlite3 so that i can fetch the size of a particular column..
Another option is if SQLite gives you a LEN or LENGTH operator you can use against a Text column. I don't have any experience here, so I'll have to bow out.
Exactly this - Express edition is a restricted version of SQL Server, but the language is the same. Compact Edition barely resembles SQL Server, and has a very cut down version of the SQL language. 
Appreciated, thanks for the insight (both of you)
U up and running yet? PM me if u have questions.
SELECT sum(length(net_name)) from rtl_reader_tbl; resulting in "22092072". this is what I am using as of now.. IS there any better solution for this ?
SELECT sum(length(net_name)) from rtl_reader_tbl; resulting in "22092072". this is what I am using as of now.. IS there any better solution for this ?
The query looks about right, and 22 MB (assuming 1 byte per character) sounds reasonable for a SQLite database. Does it match your estimates of what the table size is? 
Nice graphical representation :)
Only residents of the USA, UK, Canada, Australia, or Germany can win the prize. guess ill build my own spaceship =(
&gt; guess ill build my own spaceship =( With hookers, and blackjack.
actually, forget the lunar module, and the blackjack.
Create a temp table that contains the animals and an ordering column that's populated on your datediff formula (i.e. cat = 1, dog = 2, ect.). Then join to that table in your select instead of the animals table and order it by the ordering column and your purchase date order formula.
I have 3 maintenance plans. 1.Full every 24 hrs. 2.Differential every 4 hrs 3.Transaction every 15 min. This allows me to recover within max of 15 minutes if something were to go wrong. The Full MP reindexes and updates statistics every night. I maintain databases for school districts. I am going to have a serious look at the script DrTrunks posted the link to. 
OK, I really like that script and what it does. I was easily able to setup full\diff\log backups and all I had to do was schedule them. Time the index, then integrity to run once a week before the full and you're good to go. I also clean up the files after 10 days, after they get copied offsite. 
Hmm, I know in phpmyadmin it lists the column/table/row sizes right next to the chart. SQLite...no clue. Yay for being super helpful!
Does your database have a calendar table? You should be able to utilize that to ignore weekends
 select datediff(hh, startdate, enddate) - 48*datediff(ww, startdate, enddate) This is for SQL Server TransactSQL. There may be some strange behavior if either startdate or enddate is on a weekend. Documentation from MSDN below: &gt;When interval is Week ("ww"), however, DateDiff returns the number of Sundays that occur starting one day after date1 through date2. The date1 parameter is not counted if it is a Sunday, but date2 is counted if it is a Sunday. [link](http://msdn.microsoft.com/en-us/library/xhtyw595(v=vs.85\).aspx)
http://msdn.microsoft.com/en-us/library/ms189794(v=sql.90).aspx First comment in the MSDN article, function to return difference of BUSINESS DAYS.
Does it seem to anyone else like the vast majority of posts in this subreddit seem like homework problems?
SQL is a pretty mature tech, not much is changing except for the occasional OO db which pops up and then dies. There are only ~800 subscribers as well.
Demonstrate knowledge of different transaction isolation levels. Demonstrate joins with different index support, and execution plan to illustrate the search algorithms in play. Nested Loop, Merge, and Hash. Demonstrate lock hints like NOLOCK. Demonstrate a deadlock with help of transactional mechanics. Is this an admin or developer position? Developer may demonstrate how flow control loops and math in T-sql is poor, while CLR stored procs make math fast. Admin would demonstrate server/database DMVs like sys.dm_exec_session and sys.dm_exec_request and sys.dm_os_waiting_tasks. They'd also demonstrate row snapshot. 
Look up the TSQL challenges (sorry no link, on the phone, but I think it's Beyond Relational) and solve some of those. They're challenging! Edit: [http://databasechallenges.com/SQLServer/TSQL](http://databasechallenges.com/SQLServer/TSQL) [http://beyondrelational.com/tc/](http://beyondrelational.com/tc/) The good thing about these challenges is that you have to solve a reasonably complex problem using a single statement, and it's scored against correctness and performance.
[Where's RelevantXKCD when you need him?](http://xkcd.com/318/)
Prick, I was just asking a question. This is not homework. Just never had to do this before
Somebody is sensitive about not knowing how to do their job.
Somebody already posted the BeyondRelational challenges, which are excellent. What I've been doing for fun lately is solving the Project Euler problems using SQL. Most problems aren't really well-suited to the language; though I have yet to run into a problem that I can't solve with SQL, however convoluted and hacky the solution may be. I'm not job hunting at the moment, but when I do start looking around again, I intend to clean up my solutions, document them, and throw them out on my website alongside my resume.
So did you get the job?
Yes. http://www.reddit.com/r/SQL/comments/j9f19/interview_questions_for_an_entrylevel_sql/c2blp1e
Recommend an index on (keyword_id, search_engine_id, time_retrieved) if you use something like this: select s.*, s_prev.* from serpes s inner join serpes s_prev on s.keyword_id = s_prev.keyword_id and s.search_engine_id = s_prev.search_engine_id and s.time_retrieved &gt; s_prev.time_retrieved left join serpes s_max on s.keyword_id = s_max.keyword_id and s.search_engine_id = s_max.keyword_id and s_max.time_retrieved &gt; s.time_retrieved left join serpes s_prev_max on s.keyword_id = s_prev_max and s.search_engine_id = s_prev_max and s.time_retrieved &gt; s_prev_max.time_retrieved and s_prev_max.time_retrieved &gt; s_prev.time_retrieved where s_max.keyword_id is null and s_prev_max.keyword_id is null
Prompted by a co-worker asking for something, when they hadn't previously responded to an equally valid request of mine.
We also default to failover clusters for our high availability operations. So, +1 for this.
That's how we operate too, on 2005. It's been reliable as hell. In the event of a failover, the clustering service takes care of everything automatically, and instantly, our (constantly loaded, high-traffic) apps don't even notice.
Failover cluster only works on the same subnet though... this is for offsite DR protection
What is the goal of this offsite DR? Are you wanting to failover to it and run your apparently from there, or just have a warm standby to protect the data? A cluster will give you hardware redundancy, but not data redundancy,unless you pair it up with SAN replication. Asynchronous mirroring might be an option (it's basically realtime log shipping with a limit of around 10 databases). Synchronous mirroring would be OK if latency over the WAN is small, as each transaction would need to be committed remotely. Log shipping is low tech, but reliable and scales to large numbers of databases. I would lean towards log shipping, but it will depend on the answer to the first question. 
Agreed. Log shipping is great as a warm standby and it is very low maintenance. It's how we deal with most of our DR stuff. Mirroring is great for contained databases that don't interact with each other. Right now mirroring isn't set up to gracefully handle groups of databases that need to be consistent with each other, so it's a stand alone database, mirroring will provide you a relatively hot standby given small transactions and low latency depending upon your transaction load.
My guess is that this may have been solved, but if not - and if you're using SQL Server 2005+ - then check out RANK OVER (PARTITION BY ... ORDER BY ...) here: http://msdn.microsoft.com/en-us/library/ms189461.aspx I've used that or similar (the OVER clause is magic) no more than a handful of times. Neat stuff when you need it. I think there's something similar for Oracle, no idea about MySQL.
It was clear, and I think coderascal is correct. If possible, eliminate photo_belongs and put the albumId inside the photos table, and then create your FK from there. Actually, you mentioned that this was a possibility in your post, so...why not do it?
Check your sql file, you might need to drop the database first.
Does that erase the database or just the contents? sorry I'm new to all of this.
It will remove the entire database. Near the top of your sql file, you should see a create database statement. Depending on what you're trying to accomplish, you could comment that out and just run the inserts.
Ok I tried that but now I get an error saying: "#1062 - Duplicate entry '1' for key 1 " What do I have to change to make it insert or overwrite what's in the database already?
Since your database already has some of the same data in it, you need to clear it out before you import. Either add a 'DROP DATABASE xyz' command before the 'CREATE DATABASE' or add 'DROP TABLE' statements before each 'CREATE TABLE'. Make sure your sql file has all your table data in it though. Next time you export your database, check the box labeled "add drop table statements" or whatever it's called, that way the import will go more smoothly.
awesome thanks, I think it's all working smoothly now.
If you really want to write efficient code, you should know your database. Start the query in the correct table. In the 'where' clause, start with the biggest filter. ect If you run into performance issues you can use the 'Display Estimated Execution Plan' (CTRL+L) to see how your query is executed and play with that.
I don't believe that I can use the execution plan at work (server permissions) however I'll definitely look into it. Ordering by 'query cost' intuitively makes sense and definitely something I'll see out more information on. Any more suggestions would be greatly appreciated.
I'm in a similar situation. I found the book: SQL Queries for Mere Mortals quite helpful. I don't know how well it relates to your particular SQL Flavor though.
Anything in particular that stood out to you as you went over it? Any epiphany moments you can pass on?
I think I'm going to check this book out. Thanks for recommending this.
This book helped me a lot: Inside Microsoft SQL Server 2008: T-SQL Querying (except I have the 2005 version) What I would suggest is only SELECT the columns you actually need in the result, try to use Stored Procedures where possible and relevant and use the Execution Plan to find what Indexes the query is using or not using. 
[http://db-class.org](http://db-class.org) is open this quarter, and covers the fundamental relational database concepts. [SQL and Relational Theory](http://www.amazon.com/SQL-Relational-Theory-Write-Accurate/dp/0596523068) is a solid next step. (and available using the free 10 day [Safari Books Online](http://safaribooksonline.com/) trial account) For specific syntax, refer to Microsoft's own product documentation. The [SQL Server Books Online](http://msdn.microsoft.com/en-us/library/ms130214.aspx) should have been installed on your workstation along with the client tools if you're using SSMS. The T-SQL language reference begins [here](http://msdn.microsoft.com/en-us/library/bb510741.aspx). You might also want to install one of the SQL Server [editions](http://www.microsoft.com/sqlserver/en/us/editions.aspx) on a workstation as a playground so you don't need to worry about breaking your production servers. [Developer Edition](http://www.microsoft.com/sqlserver/en/us/editions/developer.aspx) is under $50 a seat. [Express](http://www.microsoft.com/sqlserver/en/us/editions/express.aspx) is free. There's also a time-limited full evaluation copy available [here](http://www.microsoft.com/sqlserver/en/us/get-sql-server/try-it.aspx). 
I'm also at that "knows enough to be dangerous" stage. A couple of tips I tend to keep: * Avoid cursors if possible. The MSSQL engine tends to be bad at iterative functions. Always aim for set based solutions. Though sometimes you just have to bite the bullet and use a cursor. * Avoid string manipulations. The MSSQL engine is pretty bad at these too. * Know which columns in your tables are indexed and filter on those as much as possible. Index scans are cheap, table scans are slow. * Try to only select the data you need. 
Although his style can be unnecessarily combative at times, Joe Celko has written a lot on how to approach/solve problems in a SQL (i.e. set orientated) manner. His book "SQL Programming Style" lays out guidance on things like naming conventions, database design, etc while "Thinking in Sets" and "SQL for Smarties" give various query techniques of use for non-beginner programmers. He's not quite as correct/insightful as he sometimes thinks, but at least he's consistent in his message/philosophy. Celko is also an irregular contributor to [Simple Talk](http://www.simple-talk.com/sql/) which more generally is a good place for SQL/development topics, especially for those working with MS SQL Server. Their content leans slightly toward DBA type stuff at times but the signal to noise ratio is high enough for me to keep up with, unlike most other forums / newsletters. [Explain Extended](http://explainextended.com/) doesn't appear to have been updated lately, but has some good articles in its archive. In terms of general forums/websites - [SQL Server Central](http://www.sqlservercentral.com/) and [SQL Team](http://www.sqlteam.com/) both have active forums and plenty of content. [SQL Blogs](http://sqlblog.com/blogs/) has various authors checking out (Adam Machianic, Aaron Bertrand and Kalen Delaney all spring to mind). You'll tend to find that beyond a certain point, content (and authors) become quite specific/focused on particular areas of SQL and therefore the content can seem a bit abstract or even irrelevant. It's only when you come to grapple with a similar problem that you become eternally grateful that others have already gone to the effort to benchmark the endless ways available to [split a string in T-SQL](http://sqlblog.com/search/SearchResults.aspx?q=string+splitting) or whatever. My point is that most of the reading I do is when I'm faced with a specific problem - rather than simply reading an article each day. Your mileage may vary of course... 
 &gt;* Avoid string manipulations. The MSSQL engine is pretty bad at these too. Especially watch out for conversions between varchar and nvarchar types. If you're wondering why a LIKE is particularly slow, this may be your culprit.
Oh, and implicit conversions on a join (or anywhere for that matter). I had a query to fix a few months back that was joining a nvarchar(100) onto a bigint as part of a table scan (700,000 rows). All in memory, about 3800 page reads and 200ms per execution. 9000 executions per day, about 312 GB of memory read daily, and 30 minutes of CPU daily. A simple change to pre-filter the rows before the join to avoid a scan, and it dropped to 6 reads, and under 16 ms per execution (and SQL Server doesn't get much more accurate than that). A better fix would have been to change the data types from nvarchar to bigint, but that would have required a lot more testing than just the single query I was fixing.
Solid post. One upvote simply isn't enough.
You'll need SHOWPLAN permissions on each database, which is a reasonable request to be granted for any developer (just say that it will have negligible effect on server load, as you can't generate that many plans that often, but the consequences of NOT having the permission could have a significant effect. If it's not a production server, I can't think of any reason to deny access). It is possible to write good, efficient queries without being able to see execution plans - after all, the plan tells you how SQL Server is planning on running the query, potentially with limited information. You, combined with sp_help and sp_helpindex and SET STATISTICS IO ON are all that's necessary, but it is easier to see what's happening with SHOWPLAN. You will get more experience faster by figuring out the best access methods (whether or not they exist yet) for a given query, without looking at the plan.
You want two tables: table album (albumId int pk) table photos (photoId int pk, albumId int fk references album(albumId) on delete cascade)
I would recommend a ROLAP structured DB. 
Are you able to change the schema of the database - if so, you could simply make cust_balance_due be a calculated field (cust_balance_due AS (totalextendedamount - custPaid) in the table creation script). Otherwise you could use a trigger: create TRIGGER TR_update_cust_balance_due ON TBL_opportunity AFTER UPDATE AS UPDATE TBL_opportunity SET CUST_Balance_Due = TotalExtendedAmount - CustPaid GO
Then I think you'll need to set up the trigger as I show above.
Instead of a trigger the due amount could be a computed column that you persist.
Building on what hypo11 said: Instead of a trigger or modifying the table, can you create a view? You can create the view such that you can join in all of your customer data and include the calculated field he mentioned as part of the view. No direct schema modification needed (which helps to prevent breaking applications which rely on this database already).
 create temporary table t2 select * from t1; -- sql usually balks at doing updates with selects in the same table alter table t1 add column parent_transaction int default null; update t1 set parent_transaction=(select id from t2 where t1.customerID=t2.customerID and t1.date &gt; t2.date and t1.date &lt; date_add(t2, interval 1 month)); update t1 set repeat_transaction=1 where parent_transaction is not null; create temporary table t3 select * from t1; update t1 set original_transaction = if ((select count(*) from t3 where t1.id=t3.parent_transaction) = 1, 1, 0); probably :)
What's the question? How to add new columns or how to maintain them? Also, your TransactionID is a bad idea - shouldn't store two types of data in the same column.
What SQL flavour / variant are you using? The equivalent of rogueman999's first statement in MS SQL would be : SELECT * INTO #t2 FROM t1; 
If you can change the schema, you might also consider making the Balance_Due column a computed column instead of an actual column. This way, the field is always calculated based on the current values in teh other two columns. http://msdn.microsoft.com/en-us/library/ms191250.aspx http://www.mssqltips.com/sqlservertip/1682/using-computed-columns-in-sql-server-with-persisted-values/
Enterprise tools, and someone to sue if shit goes south.
&gt;someone to sue if shit goes south. care to elaborate on that?
Whether it's "worth" the money will depend on the individual case, but the common arguments/reasons I've encountered tend to be: 1. (Perception of) Core engine stability / reliability. 2. (Perception of) General performance with standard transactional workloads. Yes, MySQL runs some huge systems/sites but most of those are either very specific workloads or are utilising any number of tricks to deal with high traffic/throughputs. Such "tricks" tend to be inappropriate for businesses either for data consistency reasons but more importantly, most businesses don't have the skills (or time) to tune their systems to such extremes. My feeling (zero empirical data here btw) is that, "out of the box" MS SQL Server performs better in more complex workloads. 3. MS SQL is often seen as a better "fit" if a company generally runs Windows servers/desktops. In a lot of environments some DBA work might be assigned to IT support who might be primarily (or only) experienced with MS systems. 4. MS SQL server comes with an ETL solution (Integration Services), a reporting system (Reporting Services) and a BI/analytical system (Analysis Services). This is extremely attractive from an IT management perspective who don't necessarily want to have to worry / deal with finding independent, possibly incompatible packages to do these things. I personally have problems with all three of these programs but they have their merits and many users swear by them. 5. Inertia. A lot of business / "enterprise" systems have quite long life-cycles and you may well find that a businesses core system was written originally 5-10 years ago (or more in some cases). Such systems will have updated their backends over time but in most cases elected to stay with the same vendors RDBMS. In these cases the relevant comparison isn't (or wasn't) SQL Server 2008/2010 vs MySQL 5.5 but SQL Server 2000 vs MySQL 3.2. At that point MySQL didn't have triggers, stored procedures or even *views* and frankly wasn't the realistic competitor you see today. 6. Some of the toolkit you get with SQL Server is quite nice. I've not installed MySQL in a while but I'm not sure if it comes with anything as straight-forward to use as the Profiler or Database Tuning Advisor. I think overall if you're running a business and you've got substantial amounts of data to work with then it's also quite likely that the cost of licences is going to be comparatively small versus the perceived risk of the alternatives. I'm currently working for a smallish firm which operates in a competitive sector and subsequently doesn't have vast sums to spend on IT. Even there though, the cost of a SQL Server licence is very small compared to the value of the organisations data, the accumulated staff time invested or the potential costs of any downtime. If I was personally starting a commercial product today then I'd almost certainly go with Postgres as my platform. But I've worked with SQL Server for ten years now and although it's desperately lacking in a few features I'd love to see I'd generally say it's pretty good and it's probably Microsoft's best (major) product (imo).
My guess is it's not just the SQL engine itself, it's the integration with other technologies that all fall under the SQL umbrella. Things like: * Analysis Services (OLAP) * Reporting Services * Integration Services * Master Data * Crescent (Auto-gen PPT...kinda) * PowerPivot In my experience, it's actually less about that than it is the company has made the decision to go with a MSFT-based stack for their Intranet. SharePoint, Office, Exchange, Communicator/Lync, etc...buying those licenses usually gets you in the door such that you can negotiate SQL and other dev pricing fairly easily. That said, I love me some OLAP! (edit: formatting)
Probably not a good idea to do it all in MS Access. Maximum database size is ~2gb. Concurrency is poor. http://office.microsoft.com/en-us/access-help/access-2010-specifications-HA010341462.aspx#_Toc296343503 If you need MS Access to be the front end, consider migrating the MS Access application to a centralized database server. This can be done through microsoft toolsets for MS Access to SQL Server conversion. Or, you could just grab all the MS Access databases and jam them into a SQL Express 2008 R2 instance. That has a maximum db size of 10gb. http://www.microsoft.com/sqlserver/en/us/get-sql-server/try-it.aspx Importing them is possible: http://geekswithblogs.net/tewissler/archive/2006/08/15/88077.aspx But painful. But you're working with Access for large datasets, so you should be used to that ;)
You can write .NET code against the Access Engine, so if you start with Access (2010, or at least 2007, seems like a good improvement) then you can always pull the data out of Access programatically and put it somewhere more....robust. [Check out some MSDN for great good](http://msdn.microsoft.com/en-us/library/ff965871.aspx)
I've seen some fairly crazy things done in Access so what you're describing could be achieved, but I wouldn't necessarily recommend it. Access used to have a 2GB file limit, but I think that's been removed for a while now. When you start dealing with larger amounts of data though you will start to find performance suffers considerably and a lot of your life will be spent exporting individual bits of data and the whole thing may well get difficult to maintain. When you say you have 100+ stores sending you their databases - do you mean they'll each be maintaining their own Access databases and emailing the individual files? Or just an extract/spreadsheet? Either way, it sounds like a potential nightmare in terms of keeping things in sync, ensuring no-one changes the design of what they're sending you, etc. In terms of products to check out, if you're already dealing with Microsoft files/systems then you might want to check out SQL Server Express which is restricted to 4GB per database but will give you a feel for the platform and it's quite easy to shift data from Access to it. You can work around the 4GB limit by splitting your work over more than one DB which is practical in *some* cases. Otherwise you might want to check out MySQL or PostgresSQL. Or, if the queries are quite simple and you don't want to install any services you might even want to look at SQLite which despite being less than 1MB can support databases of substantial size. More generally I'd say you need to consider: 1. How much data is being sent to you? 2. Is the data all identical? Will some idiot add 10 extra columns and/or enter data at random places because they want to help? 3. How is the data being sent? Is this a one-off? If it's not a one-off then avoid email if you can - it's pretty terrible as a method of regularly transferring files, especially of any size. 4. If it's not a one-off then you can't rely on 100+ human beings individually remembering to do something. But I'm assuming it is a one off. Having said this, 50% of all one-off tasks get requested again. 5. If you're going to be ending up with 100+ files which need processing then regardless which platform you end up using, you'll want to do the work in a manner that's scriptable/repeatable. How you do that will depend on your existing skills and the tools you're using. But it could be a bunch of SQL scripts, or a VBA macro or program written in some other language. Either way, you won't want to have to repeat the same manual process more than once or twice - it just becomes far too easy to make mistakes as you lose the will to live. 
Also, simplest upgrade from an Access DB is to MSSQL database.
MS access is pretty much only useful for power users to build relatively simple and small databases. Once you start to put it through it's paces, expect corruption issues. Keep good backups. If you have any developer chops at all, I would avoid it. If cost is an issue, look into MySQL or PostgreSQL.
It's less about the number of rows you're returning than it is the relative complexity of your queries and the amount of data you're dealing with at any one time. A table might have 500k rows but if you're selecting individual records via an indexed column then the actual amount of memory needed might be quite small. How many tables are involved in your queries? How wide are the columns involved? Is the server doing much else? All these factors make a difference. Do you find yourself getting entries in the event log about SQL Server being forced to page results to disk because of memory pressures? Generally speaking though more memory = better and by current standards 2GB is probably quite low for a MS SQL server. It's not abnormal though, the last 2005 instance I managed had 2GB and held several databases with many millions record tables in them as well as a web server and a SSRS instance on it. Performance could have been better (especially at peak times), but it's wasn't terrible. I tend to find Disk IO is a bigger bottleneck than anything else anyway, or at least slightly harder to work around. If there is any physical capacity remaining in the server then given the low cost of hardware it'd be getting some more memory. You can buy an additional 2GB of (desktop) RAM for about the same price as a large pizza so if that options is available you might as well try it. Where I work (after much debate) recently upgraded a server from 16 to 32GB RAM. In the end, the cost of the upgrade was dwarfed by the wasted staff time spent arguing about it. Throwing hardware at problem isn't always the way to go, but when it's so cheap it's worth a try.
Wow thanks for the well written reply. I'm not concerned with the server but the agents who will be accessing it. I'm concerned with the amount of casts, case whens, insert intos and temporary tables involved that an agent with 2g may have not the cleaniest story to the SQL pull. There are instances where the query results won't past into a clipboard BC of memory issues on the agent side.
I'll have 100+ stores with 5 tables I need data from. The databases for each store will need to have a table added to uniquely identify them as they were setup to work within the store, not intended for outside distribution. Ideally, I'd like to setup a website where the owners send in their database, it crunches the numbers for us, gives us the result. Then, at any given time, have our CEO be able to login to said website, and say, OK, I want the numbers for store #100. Click, bam, done. Any advice on implementing this? I haven't been tasked with this yet, but I know I will get handed this project since I am the only technical person in the company of less than 30 people. Btw, I only make 35K a year. I should probably ask for a raise.
I think you're probably right to be concerned. The problem with ad-hoc queries is that it's very easy to run hugely expensive queries without realising. Permissions can stop accidental data loss but they don't stop people accidentally returning giant result sets. I confess to often using "SELECT *" when writing adhoc queries when I only need two or three columns. And even if you avoid those sorts of bad habits, it's far too easy to write code like: SELECT A1.Account_Code, A1.Account_Name, O1.Order_Date, O1.Order_Value FROM Accounts A1 INNER JOIN Orders O1 ON A1.Account_Code = A1.Account_Code -- D'oh Start it running, go to get a coffee and come back to roughly five hundred thousand times more rows than you were expecting, or more likely a System.OutOfMemoryException message thanks to that last line... 
*1. How much data is being sent to you?* **Starting average size is ~30MB. Data for a store with 8 months of activity has swelled to ~90MB.** *2. Is the data all identical? Will some idiot add 10 extra columns and/or enter data at random places because they want to help?* **The data is all identical, yes. They are sales figures and department summaries and customer counts/average ticket. This is all the data we care about for each store. It will not be possible for them to add extra columns or enter data at random places because they are operating under the confines of the POS software. Their use is restricted. Certainly they could in theory open up the database and start poking around, but most are not savvy enough to do so. For our tech savvy owners, I'm going to look into security options for the database so they can't cook their numbers.** *3. How is the data being sent? Is this a one-off? If it's not a one-off then avoid email if you can - it's pretty terrible as a method of regularly transferring files, especially of any size.* **I will be setting up a system (dropbox possibly) so that at the end of each night, or week - whatever they decide, the database is automatically send to our servers. Then we are left with the task of OK, we have 100 individual databases, we now need to run a script that scrapes all of them and either puts it into a place where we can run a query and view the sales info and print a report if necessary. One thing I know I'll have to do is add a unique identifier table to number our stores, so I know who is who.** *4. and 5.* **I would probably look into SQL scripts. This is my first project involving databases but I catch on quickly. I'm literally the ONLY technical mind in the entire company. Thanks for the solid feedback.** 
It sounds like you're talking about the amount of memory on the client's machine, and not the amount of memory on the server. In this case, the client makes a request, and all processing (casts, case, temporary tables, etc) occurs on the server itself. The only requirement for memory on the client is to be able to hold the results coming back and display it (via Management Studio or other application). The number of rows has little effect - it depends on the average size of each row. You could return 500,000 small rows (at 4 bytes per row, this is close to 2 GB), or a smaller amount of wider rows. Virtual memory will kick in to help the application deal with too much data if necessary. As it's ad hoc, I'd be less worried about what the user does to their own system (by way of a bad query) and more concerned about the state of the server - I don't want them messing with other people's queries. The SQL Server 2008 Enterprise Resource Governor feature is pretty good for limiting the amount of CPU and memory that ad hoc users can use, but they can still take locks (in which case a reporting database is better).
This relies on a [tally table](http://www.sqlservercentral.com/articles/T-SQL/62867/) called dbo.Tally with one column called N. It's a bit convoluted because I misread your question originally and thought you wanted something else. This works for SQL Server 2008. -- STEP 1 : Create Sample Table and Populate With One Row. CREATE TABLE #SampleData ( id integer, string_value varchar(4000) ); INSERT INTO #SampleData VALUES (1,'30Y,95Square,I2Y,3SRegistered,19Y,95WhateverBlahBlahBlah,F12,ABRandomCrapGoesHere'); -- STEP 2 : CTE to Split By Commas with SplitByCommas as ( SELECT S1.id, SUBSTRING(S1.string_value, N1.n, CHARINDEX(',', S1.string_value + ',', N1.n) - N1.n) AS element, ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) as pos FROM #SampleData S1 CROSS JOIN dbo.Tally N1 WHERE N1.n &lt;= LEN(S1.string_value) + 1 AND SUBSTRING(',' + S1.string_value, N1.n, len(',')) = ',' ) SELECT LEFT(C1.element,2) AS name, SUBSTRING(C1.element,3,999) AS value FROM SplitByCommas C1; Returns: name value ---- ----- 30 Y 95 Square I2 Y 3S Registered 19 Y 95 WhateverBlahBlahBlah F1 2 AB RandomCrapGoesHere I concede that's somewhat hard to follow though. The sub query involving the tally table may make more sense if you read [this](http://social.msdn.microsoft.com/Forums/en-US/transactsql/thread/e9a031da-9578-4c30-8b6e-a2de29c9534a) which is a function that this is based on. 
I hear you :) I'm in the habit of using sysindexes and sp_helpindex constantly when writing queries (I still use 2000 a lot, and sysindexes is simple) to get an approximate number of rows and table size in pages. Knowing the expected number of rows allows you to determine if you need to make a SELECT TOP 100 query to test join conditions first on a small sample. "I selected 100 rows, joined onto the other table and got 10,000 back? That doesn't seem right. Oh, you're an idiot." I say "You're an idiot" quite a lot when writing T-SQL... There's nothing wrong with SELECT *, as long as you also use NOLOCK if on a production system, and your SELECT * doesn't run longer than a minute or so. As soon as it turns into production code, I get a lot more careful, and pare down the columns and remove the NOLOCK hints. 
Depends how big the rows are. You'd want to look at the performance counters regarding Page reads/second. There are recommended levels for these counters. I think 20 pages/sec is the upper limit. You'd also want to look at Page Life Expectancy and DBCC Memory Status.
There's a few main reasons. Probably a weak reason, but popularity of the commercial systems means there's more people to replace your people that leave - it's a safer option. This isn't a very good argument when compared with the popularity of MySQL, but it's a consideration for the less popular free packages. Another is performance. If you look at the [TPC benchmarks](http://tpc.org/), you can see that all the highest performers are either commercial products (TPC-C and TPC-E), or a mix of commercial or highly specialised products (TPC-H). MySQL, as a generalised product in the realm of DB2, SQL Server and Oracle doesn't seem to get a mention in the top results, as opposed to SQL Server 2008, which seems to be dominating TPC-C and TPC-E at the moment. Otherwise, just refer to what DharmaPolice wrote. Solid comment.
Make a CLR stored procedure and do it using regular expressions. T-SQL is terrible at string manipulation. 
thanks for the link
This is good information. I'm trying to get my guys a bit more justification in getting a slight upgrade in RAM. I had a hunch that it might not be as bad as it sounded. I just didn't find any documentation of the sort. 
Indeed, nolocks are awesome. Thank you for your reply, good information. 
The results grid doesn't through the agents that error, but the copy to file does. I figure things are a bit wonky on their pcs. Thanks for your detail response. You highlighted what Microsoft's documentation was saying with grace and diligence. I'm sure you are awesome at your profession. Thanks again.
As cheap as ram is, I'd throw 4GB at it at least.
I had a look at the TPC site, and I think it's far from impartial. Take a look at who its members are: http://tpc.org/information/who/whoweare.asp
Good point! The full members are a rather diverse group, but anyone can submit a benchmark for auditing. It is a lot of work, however, and to get competitive results involves a fair amount of hardware and time dedicated to optimising the system (and documenting 100+ pages of system specs for the auditors). However, the non-database vendors (Dell, HP, Fujitsu) have published results for multiple database engines, and I'm sure they'd have considered MySQL (Oracle would have at least) if they thought it capable (or likely to sell more systems running MySQL). As the full members do influence the types of benchmarks set, it is understandable that they would play to their strengths, but there's nothing stopping MySQL or Postgres from implementing these features with comparable performance, except for development costs and TPC testing (time and hardware). Now that Oracle has MySQL, maybe we'll see some (although Oracle doesn't seem to have entered a TPC result by themselves for a while). There was one 300 GB TPC-H benchmark submitted for MySQL with a Kickfire applicance (submitted by Kickfire), which is the cheapest build ($48K in 2008), but you can more than double the performance with SQL Server for an extra $30K. [This link describes a TPC-H on MySQL from 2008, unauditable, but still interesting reading.](http://www.mysqlperformanceblog.com/2008/04/10/tpc-h-run-on-mysql-51-and-60/) I'd love to know exactly why we don't see more variance in the submitted results - competition is good!
Cheers
1. vss backups - in a vm or not - easy to backup so it's not crash consistent 2. cheap - see maps/bizspark for ways to use it in your business for cheap. 3. atomic - last time i used mysql it wasn't very parallel friendly 4. SSIS/SSMS - jesus christ without touching a sql server ever before i was writing monstrous views and ETL'ing data from all kinds of sources to crap out a set of tables to get work done. if you haven't used the management console to create joins, views, diagrams - mysql had phpmyadmin which is years apart in complexity. sql server truly is as easy to use as microsoft access (even more with denali) - mysql is not all easy to go from ZERO DBA to wannabe DBA in a week. mysql isn't free anymore - it is encumbered by oracle. any good framework (ahem php) will have equal support for sql server and what i call it's free-cousing postgres - Mysql centric frameworks are childish unless they are designed for mysql only. - postgres is essentially sql server but open source (more so than mysql) and is where everyone that doesn't want to touch sql server ends up going to after they realize how limited mysql is(was). Keep in mind i left around mysql 4 before 5 came out. There was terrible problems with innodb and locking and load distribution. Microsoft paid off the major php framework makers to go make their drivers sql server compatible (esp denali) i'd say cake and whatever uses doctrine2(symfony?) and maybe 1 more got it right - i had to pick 1 bug out of cakephp to get it to work perfectly - 5 out of yii, burn CI to the ground it was so bad. Doctrine seems right on. I have ported many mysql apps to sql server because when i started my last job it was a 100% move from unix to 100% windows - so i grabbed ADODB PHP (awesome lib 7-8 years ago) and sql server 2000 and went to town . how exactly do you get the execution cost/plan of a compiled query? (every query is compiled, hence parameter binding) in mysql? do you just rely on slow-queries log to find issues after they happen? How would you join 6 tables that do not follow normalization rules? jedi mind tricks? the UI is golden man. it is superior to any other database 10 years ago - and now. If i'm wrong please redirect me i'd love to use Mysql with the same ETL(DTS) and management studio (SSIS/SSMS) - i don't really use OLAP/CUBES but if they have a ui to handle that as well in mysql - teach me. oh yeah - how exactly do you tell mysql about numa? last time i heard you had to use affinity to force SUMA? That might have worked in the old pre-nehalem days but now everyone has dual-socket numa setups? sql server (remember sqlos) can be aware that SOCKET 1 cpu with XXX RAM is on the left with a nic card and raid card so it should not bother socket 2 which has another nic card and raid card and its own memory. NUMA thrashing was the last thing i remember with mysql. you had to limit your mysql to 1 socket and scale out rather than scale up. i lol when i am told a real SQL server has both offset and limit (denali 2012 has this now) - but that's about when i know i'm dealing with a script kiddie on their framework/app/etc. 
What DB are you using? I can do this fairly eloquently, but it would use Oracle-specific regular expression functions.
Yeah, i forgot to mention - this is oracle 11g. 
Update: Been playing with Database Roles and found that adding some of the users who created the Views to the Owned Schemas does all people with that Database Role view the Views but that seems like a horrible work around, has to be something at the Database level though there is no way to add Owned Schemas to a Database Role...will check back if I find more. Chime in DBAs!
I would agree with the suggestions for log shipping but would lean towards the log shipping solution in Red Gate SQL Backup if you're going over a WAN connection. It recovers from temporary network/machine outages with much awesomeness when 2k8R2 sometimes surrenders. Mirroring is getting a boost in SQL Server 2012 so that might be a time to reconsider it. We have probably 300-400 databases so it isn't a real option for us now. Adding some heresy to what taejim said, I have been a DBA since clustering came out and have never seen it help with a single outage anywhere, ever. If you have a problem you now have paid for 2-4 nodes that don't work because the shared storage/data is jacked up. 
that sql server auditor code from the sql server bible is pretty k-rad
Damn. It probably takes longer for the engine to compile the query than it does to run it.
Here's the [correct link](http://thedailywtf.com/Articles/The-Query-of-Despair.aspx); yours is to the comments. Seems a combination of over normalization and auto generated code.
This brings back memories of doing contracting. The downside of contracting is that you get the shittiest jobs that no one else wants to take on, often with legacy systems like this. The upside is that you often get the toughest, thorniest problems that no one else wants to take on. Were I getting paid by the hour, I'd love to take on something like this for a week or two and come back with an 8 line query that does the same functional thing, only 10,000 x faster. 
If it's old and legacy, it's probably not autogenerated by a tool. If it is autogenerated at all, it's probably done by some guys procedural code. One thing I will say for it is it doesn't appear to be using temp tables. In MSSQL especially, when a programmer who wasn't particularly ept with set-based theory starts on a complex query, he often uses the engineering approach of breaking the complex problem down into steps, doing a join from two tables into a temp table, and repeating as often as necessary to get to a solution. So you have a mess of complex queries and temp tables trying to get to this solution. I've done more than a few jobs cleaning up legacy code like this. 
Cool. Now let's try that again without a rage comic.
&gt;So you have a mess of complex queries and temp tables trying to get to this solution. To be fair, a query this large probably SHOULD be broken up! It seems unlikely the optimizer could be finding an optimal execution plan, especially if statistics are off even a little. Temp tables are much over-used, but there are instances when they may be appropriate. 
just use an AS2 transport program like nsoftware freeas2.com - free use version at each 100 site - then 1 unlimited copy at main site (AS2 is drummond certified non-repudiated transport system over http using FIPS140-2 encryptiong and signing) - schedule the file drop on the free version machine it will get to home base or keep retrying - batch file will dump the data into the dir and boom off it goes. since its encrypted using industry standards for PCI compliance you will be good to go - the main base will get the script with the unlimited version (2500$?) and run a script to move the file say to the dir /today/sendername and you process it into your core database. then rename today each day to date and learn some php :) this way you will have reliable 100:1 transport with PCI compliant encryption to CYA - and the cost is very little - plus AS2 is the standard on how people do EDI. I've used the freeAS2 transport to do $20M in business along with edi (PHP 850/855/856/810/870/832/846) - Total cost $0 - look on ingram face when i tell them cost was $0 - PRICELESS :) anyone else out there want to work on PHP EDI and FREE? EDI is like CXML it's an invite only club and most free php interpreters suck balls. You will find that the world runs off EDI and your business eventually will indirectly or directly use EDI so AS2 is the transport method of the messages
Well, yes and no. I'm well past my youthfull folly of saying you should *never* do a particular thing, but in the case of breaking up a complex query into simpler queries IMO you should not use temp tables - you should use derived tables instead and break up the longer query that way. Then at least the optimizer has a chance, and you don't risk blowing up TempdB as the data set grows and expending a hell of a lot of writes. 
I personally don't use temp tables very much. Instead I use these little things called "tables". If the relationships between entities becomes so incredibly complex, it's probably best to store and maintain that relationship data in ETL instead of calculating it every time you query it. 
You might want to give the POS vendor a call. You can't be the only person with this problem. Also, take a look at Tableau Public for an idea of what a database should be able to do. I'd also recommend using Access in small chunks that is read by a second "master" dB where you write your reports and make the out put available. I was recently tracking data for about 230 "stores" with 100,000s of rows and 40 column and never had an issue. The secret is "rolling up" the data in the base dBs and linking that output to the master. Eventually, you'll want to look at Cognos or something higher-end. 
It goes without saying that if you have more than a beginner's skill in databases, you can envision and build a single query to do any particular job. What we're talking about here, fhowever, is unskilled database coders (altho they may be skilled in some other language) approaching the problem by reducing it to a series of steps. Preserving duplicate data to do calculations with isn't good practice. 
Yup, especially in OLTP. And even more so when you're working with a clean design or one you can modify in any way you chose. In other words... in heaven. Besides, I live in a BI world now...the rules are different. Try building a cube with a 200 million row many-to-many relationship on anything other than a single joining table and you'll quickly lose any requirements you had for ideological purity. You'll probably also do a lot of crying, but that's another story.
sure, I get denormalizations and messy compromises. If necessary I suppose you could have intermediate tables. Even in BI land, however, you should be focusing on integrating as much as you can into those intermediate tables, and for god's sake, don't use #temp tables! Many were the times where I'd see basically a full dump of large tables into #temp tables, and then there'd be these mysterious crashes of TempDb, bringing down the entire server, that just happened to occur during the reporting jobs...... 
[SQL Zoo](http://www.sqlzoo.net) is perfect for you.
So this [deleted] is reliable as hell, eh? Please can you tell me what he said?
He said they're running an active/passive cluster on 2008. Basically you get two servers (ideally, identical hardware), SQL Server runs on one of them at a time with the other one sitting around as a hot standby in case shit happens.
Appreciate you re-posting that, thanks... don't think it would work for me though... the "other" server I have to set up is going to be a whole nother country.
Aha. Look at transactional replication, then.
Thank you!
Your post is a little confusing, but I _think_ I see what you are getting at. You want the entire row out of INVSERIALSTATUSHISTORYVIEW for the row which has the max(statusdate) for each serial number? You could do this in a few different ways: SELECT * FROM INVSERIALSTATUSHISTORYVIEW isshv1 WHERE statusdate = (SELECT max(statusdate) FROM INVSERIALSTATUSHISTORYVIEW isshv2 WHERE isshv2.serialnum = isshv1.serialnum) OR, using a CTE (if you're in a relatively new version of MS Sql Server): ;with maxdates as (SELECT serialnum, max(statusdate) as "statusdate" FROM INVSERIALSTATUSHISTORYVIEW GROUP BY serialnum) SELECT * FROM INVSERIALSTATUSHISTORYVIEW isshv INNER JOIN maxdates md ON isshv.serialnum= maxdates.serialnum AND isshv.statusdate = md.statusdate If you're in a language that doesn't support CTE's, you could just nest that table in the query like this: SELECT * FROM INVSERIALSTATUSHISTORYVIEW isshv INNER JOIN (SELECT serialnum, max(statusdate) as "statusdate" FROM INVSERIALSTATUSHISTORYVIEW GROUP BY serialnum) as md ON isshv.serialnum= md.serialnum AND isshv.statusdate = md.statusdate Or finally by checking for the lack of existance of a later date: SELECT * FROM INVSERIALSTATUSHISTORYVIEW isshv1 WHERE NOT EXISTS (SELECT 1 FROM INVSERIALSTATUSHISTORYVIEW isshv2 WHERE isshv1.serialnum = isshv2.serialnum AND isshv2.statusdate &gt; isshv1.statusdate) Let me know if I've misunderstood what you're trying to do or if you need me to explain how any of these solutions work. EDIT: I just noticed your username is "oraclenoob", so you can scratch the CTE solution since that is a SQL server syntax - but the other ones should all work in oracle. EDIT2: Fixed a couple of small mistakes.
Thank you! Might I ask you to explain the last solution? I think your first 3 solutions are brilliant, but I can't even wrap my head around the 4th. I believe I understand 'Exists' and then 'Not Exists' but, I'm not following the loop.
I use window functions regularly, along with lag () and lead(), in Oracle to calculate the time between events. They took me a few uses to get comfortable with, but they are indispensable now.
&gt;The LAG and LEAD analytic functions were introduced in 8.1.6 to give access to multiple rows within a table, without the need for a self-join. Making me jealous. Is there an equivalent to this in MS SQL that I have not come across yet? This would be nice to have.
That's a more interesting question than you might expect. SQL Server has window functions http://msdn.microsoft.com/en-us/library/ms189461.aspx HOWEVER, it has notoriously lacked the so-called analytical functions LAG() and LEAD() .... until now, or, more precisely, as of SQL Server 2012 http://blog.sqlauthority.com/2011/11/15/sql-server-introduction-to-lead-and-lag-analytic-functions-introduced-in-sql-server-2012/ It *is* possble to compute inter-event gaps (for example) without LAG() and LEAD(), but it's more cumbersome, as I suspect kill-t already fully understands.
Sure. The last one is pretty straightforward, actually. Imagine we start with 2 copies of the entire view. Then we go row by row through the first copy: if there is a row in the second copy which has the same serial number as the row we are up to but which has a later status date then we cross out the row in the first copy - since we know it isn't the maximum date record for that serialnum. When we have gone through all of the rows in the first copy the only ones which aren't crossed out are the ones with the latest status date for each serial number - so that is our final result set. Let me know if you need any more help or clarification. 
Thank you! Makes perfect sense (after re-reading x 3) So, We have been looking for the most recent status date. Truth be told, I'm really looking to find the "last status date" before a certain date (Jan 1 11). So what I did was edit the view, so I don't see any rows with status dates greater than the 1st. So the results show something like this Asset1, dec 24, in Asset2, dec 19, out Safe bet that on the first of this year, asset1 was IN and asset2 was out. So, with your help, this works. But did I approach this problem inefficiently? Thank you! Edit: Formatting
Whether this was the right approach or not depends on a lot of other factors, such as will anyone else have a use for this view who won't want it limited by the 1st (or will you want the same query next year and then want it to be 1/1/2012). The more ideal approach would probably be to leave the view as inclusive of all dates and add where clauses to whatever query you used to retrieve the max dates to only consider dates prior to 1/1/11. In the NOT EXISTS example you would need the where clause in both the query and the sub query. I would type it out but I am on my phone. If you need to see the code write back and I will respond tomorrow. Also, you could make it into a stored procedure that takes the date as a variable and then you could run the exact same query this year for dates before 1/1/11 and then again next year for dates before 1/1/12. 
I would just create a new database role called accessMyViews, add the db users into that db role then assign that role onto the view and grant select or whatever you need. You are pretty much doing this already from what you said.
Do you know if they removed the ability to Edit completely when you are not the creator even with a solution like the one you've posted is in place to allow other users to at the very least view these Views? Thanks for the reply!
You want to look into a UNION statement. Here's some pseudocode to get you started. select &lt;Fields&gt; from &lt;statement1&gt; UNION select &lt;Fields&gt; (Must be the same fields) from &lt;statement2&gt;; UNION gives you the distinct list. UNION ALL gives you everything, with duplicates. PS: GROUP BY is generally preferable to DISTINCT, performance wise. Good luck!
Aha, very cool, exactly what I was looking for. Thanks a ton!
Glad to help. Good luck. :)
As someone just getting into databases, I think it's appropriate to ask this before I get too deep into the wrong one: What are the employability options for each? What kind of work does one do in one versus the other? All other things being equal, what kind of pay can one expect? How much time investment should one expect, assuming a basic understanding of each, plus some novice experience in object oriented programming? What's the best/fastest path to a paying gig for each?
Instead of building @C in a loop so that @C = @C + CAST(@B), change it so @C = @C + ' *'.
Me too :-)
thank me in 5 years when you're a senior DBA and rich.
Thanks :D
Untested, but in Oracle 9i or higher, something like this... select substr(NAMEVALUE, 1, 2) as NAME, substr(NAMEVALUE, 3) as VALUE from ( select regexp_substr('30Y,95Square,I2Y,3SRegistered','[^,]+', 1, level) NAMEVALUE from dual connect by regexp_substr('30Y,95Square,I2Y,3SRegistered', '[^,]+', 1, level) is not null) 
Untested but something like this... select * from ( select i.*, rank() over (partition by serialnum order by statusdate desc) as statusdate_rank from INVSERIALSTATUSHISTORYVIEW i) where statusdate_rank = 1 
As a DBA administrating 40+ MSSQL servers as well as couple of MySQL servers, at an organization that otherwise uses a lot of open source software (PHP, mongodb, freebsd, etc.) I have to say MSSQL has some fantastic features that I would not quickly give up: 1. Its implementation of replication is very solid and we use it extensively, especially as a way to horizontally scale read-only data, but also to make the core data of the company local to many servers. 2. Change tracking is an awesome new feature that can be used for incremental data transfer to any destination (requires a lot of custom code, but I'm using it to populate a Netezza data warehouse). 3. SSIS can be a bit cumbersome to test/develop but otherwise is incredibly useful. 4. Linked servers, while incurring nontrivial overhead, are also extremely useful for environments with a lot of servers. 5. CLR functions are an incredible feature, we're using them for regex, FTP, requests to web services, etc. and at this point could hardly live without them. 6. Powershell is awesome for management of many SQL servers and its integration with MSSQL through SMO, RMO etc. is great. 7. The stored procedure/function language has a lot of awesome stuff in it. 8. Execution plans in MSSQL are pretty easy to read and use for tuning, and you can get them through SSMS/DMVs/Profiler. 9. SQL Profiler and extended events are amazing administrative tools for figuring out what's going on in the server or troubleshooting bugs/performance issues in real time. 10. Features like missing index views, index usage stats, index fragmentation stats are incredibly useful for DBAs. In general the dynamic management views/objects provide a huge amount of useful information. 11. SQL Agent is probably the best job scheduler I have ever used, and in fact we've been using it even for not-strictly-sql tasks (such as calling a PHP script via a web request SSIS task). 12. Service broker as a built in message queue is also fantastic if you can take the time to implement it. 13. The MERGE statement in SQL 2008 is amazing. 14. The backup compression (available in standard edition in SQL 2008) makes backups significantly faster and smaller. 15. Web edition is very cheap and actually quite useful for servers with simple feature requirements (especially read-only slaves). 16. The implementation of DDL and DML triggers is quite nice for certain situations. The list goes on but these are just the things I could quickly come up with. 
That's okay, Although I prefer a CTE variety without the order by. With Serialnum as ( select *, max(statusdate) over (partition by serialnum) as statusdate_max from INVSERIALSTATUSHISTORYVIEW) select * from Serialnum where statusdate = statusdate_max 
I'm a big fan of Hypo11's code: SELECT * FROM INVSERIALSTATUSHISTORYVIEW isshv1 WHERE statusdate = (SELECT max(statusdate) FROM INVSERIALSTATUSHISTORYVIEW isshv2 WHERE isshv2.serialnum = isshv1.serialnum) 
Yours is still doing a sort order by as part of the explain plan to find the Max! I think mine is more readable.
Hmm. I was thinking something similar, but was thinking of just dumping all the data onto an XML page and letting that handle it. Ideally, the query executes, prints all the dates and times onto an HTML table (but for bands), and you can scan the venue or just browse by date or time.
Looks like I have my work cut out for me. I suppose the queries will look something like SELECT * FROM Event_ID WHERE DayOfWeek = 'Monday' UNION SELECT * FROM EventID WHERE StartTime = '8am' Maybe I'm way off.
Thanks for all the help, I really appreciate it. I'm going to get down to coding and may float another question by if a problem arises.
AFAIK you're going to need some sort of ETL process to make that happen. If you don't have money to buy software/pay for a solution, you could probably do it with Perl or Python from the MySQL side (assuming this is a *NIX box)
Minor nitpick, its MySQL, not mySQL.
If you have an ODBC driver for MySQL (which is what the linked server would be using, but it sounds like there might be an issue there), you can use the SQL Server data import/export wizard (right click on your source SQL Server database, go to "Tasks", then to "Export Data", and use the ".NET Framework Data Provider for Odbc" (unless there's a more appropriate MySQL specific driver installed - I'm not a MySQL expert). You can then use an existing MySQL DSN as a destination. The resulting SQL Server Integration Services (SSIS) package that the wizard creates can then be stored and executed whenever necessary. You can also make changes to it to make the process more robust (adding logging, other steps, etc). As a general rule, I'd stay away from linked servers unless you're doing something incredibly simple, or you've got two SQL Server machines. 
Ask for INSERT credentials? If you can't insert into MySQL due to credentials then no process is going to help you.
If he has SQL Server 2008 R2 then he can use the ETL functionality from SSIS for free.
 Sounds like you need 2 tables a customer table that you already have. and a 2nd table to record datetimes on which will have an id, datetime column so you can match it back to customers. Then turn the day / datetime string back to a real datetime and insert it. 
 If you go down the route that is described above your next problem may be unsolvable. 
No prob, I was typing fast out of frustration.
Right now the biggest problem is basically how it will actually retrieve data. What I've gathered is that I will need at least enough tables to store venue/site data (phone, address, etc), days of the week, and time of the day. The problem I've run across so far is that when building the query, I get an error that's essentially "the number of columns don't match". That's because I'm trying to use a UNION in the query, obviously. Trying to combine the results of one table with the results of a table where the columns don't match (except for maybe the id column). I'm thinking it might also be wise to try a mysqli multiple query since I'm running into that issue. I just can't think of a workaround to this aside from running two queries in one script. I guess I could just rebuild the entire database and match the # of tables, but that seems redundant at best. As you can tell, I'm relatively new to all of this. And I do appreciate the help, whatever the opinion may be. In doing more research 
http://sql-ex.ru/ has progressively harder exercises. I've been using it for a while and found it helpful.
I don't have the full implementation for you, but I suppose you could first sort by datestamp and then set up a cursor to march down the sorted list checking the difference between consecutive records.
Smells like homework! Here's what I would do. Assuming your postid is sequential, you would make a join between the table and its self with postid+1. This would allow you to compare datestamps. The goal is to make a table with the following columns: postid, datestamp, postid+1, datestamp^1, datestamp-datestamp^1 If your postids are not guaranteed sequential you can accomplish the same thing by ordering by datestamp and pushing into a temp table with sequential ids. Good luck!
Not homework, I promise :) Thanks for pointing me in the right direction. 
Well, kind of an update - I thought about it and thought about it. The way I was thinking about the database was all wrong. I ended up going with a table from each day of the week, with the needed times placed as columns throughout each table as well as one Primary key in each day to hold it all together. So now when I'm writing the queries, it's much more specific and relational throughout all the tables. I don't want to say "problem solved", but it's getting much closer. :)
I'm no expert, but I'd just check for and block certain characters before processing any SQL. E.g. Brackets, semicolons.
Depending upon the language / systems you are developing in, there may already be packages/modules available to assist with filters and protecting your platform. One of the key ways to safe guard is to never pass through unsanitized data. Always expect everything that comes in to be suspect. If possible use parameterised queries, where you use placeholders in the code and pass the data in as parameters to the query. Using stored procedures is even better. Assume the data you're returning is also possibly suspect and sanitize it before passing onto the presentation layer. There are a lot of resources out there on the web. Doing a simple google search for SQL Injection prevention gives some really good results. 
 Depends on the development system. Normally 2 simple rules 1) When using sql make sure all argument always go though the AddParamater functions with the db supporting lib. 2) When using dynamic sql. See rule number 1 and don't be lazy about it. 
 And all unicode variants of it. You also need to block all escape sequences of characters. Use the functions provided by the database. Otherwise your code is most likely exposed to sql injection!
This is your best bet - there are too many variables to keep track of if you want to roll your own.
** EDIT: some ppl doesn't realize this is just an example showing what SQL injection really are and how all functions providing protection from them works in fact. YOU SHOULD NOT USE FUNCTIONS FROM THIS POST, NEITHER WRITE YOUR OWN. Every database provides them and they are much better and safer then my is. ** There is only one golden rule regarding security: **never trust user input**. All other rules can't even be called rules when compared to this one. Never ever trust user input, even if you think something isn't user input. For example take some string constant in your program. You made it constant in your programming language, so you think there is no need to check for it when using it to build SQL queries? Think again. It might not be clear at first. For example, hacker can use reverse engineering, find your string constant and change it even in compiled executable file. Hacker just got whole access to your DB. Better example, let's take a look at this code: (python web app) table = server.request.get ("page"); # reads information from URL sql_query ("SELECT * FROM " + table + " LIMIT 10"); This sucks. Why? Well, because user can change table variable to whatever he wants: 1st case: index.php?page=products: You have table product and this works fine. 2nd case: index.php?page=not_existing You don't have table not_existing and hacker can see error telling him information about your site. Not good. 3rd case index.php?page=products; DROP TABLE products -- Wow, what just happened? You lost whole table! Let's see how this works. The third case becomes this: table = "products; DROP TABLE products --"; sql_query ("SELECT * FROM products; DROP TABLE PRODUCTS -- LIMIT 10"); So SQL server now have 2 queries: SELECT * FROM products Which is OK. Then: DROP TABLE PRODUCTS Which is obviously not OK. Then everything else is ignored because of --. Since it's vaild syntax, SQL will execute both of them and you can say bye bye to your data. Now... let's see how we can protect. As I said, never trust user data. So you could check if table is indeed what you're hoping to get: table = server.request.get ("page"); goodTableArray = [ "products", "users", "more_existing_tables" ]; if table in goodTableArray: sql_query ("SELECT * FROM " + table + " LIMIT 10") else: print "GO AWAY YOU HACKER. Or just some error occured :P". Now we are creating an array with all acceptable values for table variable. We check if variable value is one of acceptable values and only then executing query. Otherwise we print out adequate message. This is called **input validation**. Now your application is secure. Another example: row_amount = server.request.get ("limit"); try: row_amount = int (row_amount); # Try to convert to integer. except: # If it fails (aka not integer) print "Integer value expected. Bailing out." sys.exit (); # End the application if (row_amount &lt;= 0): print "Can't display negative amount of rows!"; sys.exit (); sql_query ("SELECT * FROM `products` LIMIT + row_amount); Application secured (a bit)! Also input validation protects from huge amount of security holes related or unrelated to SQL. But that's not all you have to do to make your applications save. Because, sooner or later, you'll forget to put input validation somewhere. Or you'll think that some input can't be changed, while some cunning hacker managed a way to change it anyways. Or hacker will find a way to bypass your input validation. Everything is possible, but you want to make your application completely SQL-injection-proof no matter what. To archive this, use method called **string escaping**. Let's go back to third case of first example. So we have this code: table = server.request.get ("page"); // reads information from URL sql_query ("SELECT * FROM " + table + " LIMIT 10"); And this is user's input: index.php?page=products; DROP TABLE products -- Now... To make this thing secure we will remove or escape evil characters. 1) What are evil characters: &gt; Evil characters are characters which are not useful to us (application), but can change behavior of program if used. In SQL that characters are: &gt; ; for query seperation &gt; " ' and ` for starting/ending string literals &gt; -- for making SQL server ignore piece of query. &gt; \ can be used to escape something you don't want to be escaped. 2) What is escaping: &gt; Escaping is telling programming language that something inside a string is not a keyword, but just a regular value. For example, you want to store *He sad "Hey babe"* string in C++. How do you do it: char string[] = "He said \"Hey babe\""; &gt; You can't use write: "He said "Hey babe"". That won't work, because " is keyword which tells compiler where string starts and ends. &gt; The same is with SQL. The problem is that evil characters are keywords in SQL. They tell SQL something while in fact you want them to be just data. The answer is to escape all evil characters and they will be treated as data by SQL server. You can escape them by using \ in front of them. Ok so basically to secure your self, you need to escape every evil character in string. You can write function for this: def sql_escape (str): evil_characters = [ ';', '\'', '"', '`', '\\', '--' ]; for i in range (0, len (str)): # for (int i = 0; i &lt; strlen (str); i++) in python if str[i] in evil_characters: str = str[0:i] + '\\' + str[1:]; # inserts \ before i-th character to str return str; table = server.request.get ("page"); # reads information from URL sql_query ("SELECT * FROM " + sql_escape (table) + " LIMIT 10"); Now we have code that is absolutely immune to SQL injection . (Thus this version will print error if requested table doesn't exists, because there is no input validation). **Every variable used to build SQL query MUST be escaped first. ** No matter if you used input validation or not. Why? Well because it doesn't harm at all, and only then you're 100% sure your whole app is secure from SQL injection. Note that every decent API provides function for escaping strings, so there is no need to write your own. Every decent framework does escaping automatically. (eg web frameworks, ORMs, abstraction layers ...) **;tldr** Use escaping and input validation. EDIT: typos
What everyone else said, but one of the most important things is to plan security from the start of a project, not as an "add-on" at in the end stages.
 Your evil characters lists is missing *a lot* of things! Please use the functions provided by the database engine.....
 OH here is an example of why the above is poor http://bugs.mysql.com/bug.php?id=22243
No offense but anyone doing the above should be taken out the back and shot.
This is just explanation on SQL injections. This are just example functions so ppl can see how they work in fact. Of course API functions should be used instead.
omfg this is just example. This is so people know what is done by escaping functions from DB API.
I know for bytecode attack. My article is explanation on SQL injections. I've told in article they should use function from DB API.
MySQL is now owned by oracle, expect the worst. MySQL top days are simply over. I stay the hell away from oracle, they will find a way to suck money from you. Real question is, MS SQL Server or Postgres SQL. It depends, I default to MS SQL unless I have a reason for postgresql. 
Thanks for the input! I didn't expect a lesson, but I appreciate you taking the time to give this advice. +1.
What is this for? It looks like homework, so I am not going to write the SQL for you. However, I can give you some pointers. First off, I'd be surprised if the query as you have it written there returns any data at all. Max(cnt) is an aggregate function, so to use it such as you are (SELECT nm, max(cnt)) it would require a GROUP BY clause. Does this have to be done in one query? Or are you able to use some temporary storage? If you can, I'd recommend an approach where first you calculate WHAT the maximum count for any one disease is, and then look for diseases which have that value for their count (the HAVING clause is going to be required here). Start there and see what you can come up with. Write back if you need more help.
yes its homework. sorry with that i was just trying stuff... i started with select d.name ,count(i.iid) from disease d, study e, institute i where d.did =e.did and e.iid=i.iid and i.cidade like 'London' group by name but i cant seem to understand how can i get the max from the group by with a single query 
So to do this all in 1 query is going to take some subqueries. You had the right idea in your original query, but let's start with the bottom and work our way up. Step 1: Let's get a list of every disease as well as the count of studies for that disease in London. Step 2: From the list we built in step 1, let's determine what the maximum count of studies is (it doesn't matter how many diseases have that number of studies, we just want to know WHAT the number is). Step 3: Let's get a list of all of the diseases which have a count of studies equaling the number we found in step 2. See if you can put that together...
for sql server: select a.name, a.cnt from ( select d.name name, COUNT(d.name) cnt from disease d join study s on d.did = s.did group by d.name ) a group by a.name, a.cnt having a.cnt = (select max(b.cnt) from ( select d.name name, COUNT(d.name) cnt from disease d join study s on d.did = s.did group by d.name )b) 
 I am pretty sure you can do this without the sub select ...
Not sure what version of SQL you're running. Also, at the least you're missing parentheses. Did you copy/paste, or try to retype this? I'll try to clean it up: {{{ SELECT Replace(Replace(Mid([Discover String],InStr([Discover String],"&gt;category&gt;"),Len([Discover String])),"&gt;category&gt;","[1] http://www.website.com/category/",1,1),"&gt;","/") AS [DataW URL], [Page Number Index].[MAX Pages] AS [Num Pages] FROM [Page Number Index] GROUP BY Replace(Replace(Mid([Discover String],InStr([Discover String],"&gt;category&gt;"),Len([Discover String] }}}
Thanks for the response! I completely appreciate it. I copied and pasted this from the SQL view of my Access query. I'm not sure how to check which version of SQL I'm using. I'll give this a try tomorrow morning and get back to you. Thanks again!
Wanted to follow up on this since I finally had time to read it all (it is lengthy). The site I'm working on, as it currently stands, seems to be pretty secure. I have all user inputs pushed into a few dropdown HTML forms, which executes an AJAX script when called upon. The URL doesn't change at any point when a query is being executed, so I feel like it's fairly safe from that standpoint. I have all the variables escaped within the main search script, which is working pretty well. I went through and used some of your suggestions to try and expose some data and it held up. Now onto the next question, which I will be posting shortly... :) Thanks again!
np, I'm glad I helped. :)
I didn't actually change anything but the spacing. Also, knowing you're using MS Access is enough for the version question. What are you trying to do with the Replace? Can you give an example of the data in [Discover String], and what you want it to look like on the report?
No worries, I started from scratch and got it to work. Just in case you're curious, here's what I went with: DataW URL: Replace(Replace([Discover String],Mid([Discover String],1,(InStr(1,[Discover String],"&gt;category&gt;")+8)),"http://www.website.com/category/"),"&gt;","/") Discover string was basically the first half of a URL with some tags changed and all of the "/" switched with "&gt;" .
Upvote for the correct answer! For what it's worth, we run 2008R2 with a Linked Server ODBC connection to an AS400. Realtime performance sucks, but SSIS ETLs move gigs of data every night. If the "two SQL Servers" comment was with regards to stability, I've never, ever, ever seen Linked Server or anything else crash our 2008R2 box.
&gt; Every database provides them PHP + Linux + MS SQL Server is a ghetto.
Yeah, the "two SQL Servers" comment was regarding stability. I've done a lot of loading of data across a linked server to an Oracle server, and using four-part names caused massive queries to be run against Oracle - SQL Server would start pulling back the entire table, and perform filtering on the SQL Server side instead of the Oracle side. The much better option was to run OPENQUERY against the linked server, letting Oracle run that query, and then process the rows (any joins, etc) in a secondary step. FWIW, I've seen similar issues between SQL Server 2008 and SQL Server 2000, so I'm wary of any four-part name query that won't be run solely on the remote server.
&gt;SQL Server would start pulling back the entire table, and perform filtering on the SQL Server side instead of the Oracle side. Oh yeah, that's definitely how it works. SQL Server can't translate your queries into whatever other dialect of SQL is running on the other end of the ODBC connection. So it pulls the whole table and processes the query locally. But when running between SQL Servers, we see quite a bit better performance. &gt;The much better option was to run OPENQUERY against the linked server, letting Oracle run that query, and then process the rows (any joins, etc) in a secondary step. Hey, now that's cool! I did not know you could do that. I will be reading about this and trying it out tomorrow!
If you find yourself building sql queries by concatenating strings, you're doing it wrong. Use parameterized queries. (example in c# and untested.) Don't do this: cmd.CommandText = "SELECT * from t1 where id = " + some_value ; cmd.ExecuteDataReader(); Do this: cmd.CommandText = "SELECT * from table1 where name = @p1"; cmd.Parameters.Add("@p1", some_value); cmd.ExecuteDataReader(); 
Yeah... I should puted that specific case in the guide. The solution is to encode the strings to hex bytestring, and then append 0x in query before the hexed string. This can be done by using unpack php function. Or you could use PDO which does it automatically in prepared statements. Another solution is writing your own function for escaping just like mine is, just expended a bit. That is what most ORMs and frameworks does. However I'm not trusting frameworks and web servers on input escaping as I trust official DB API, because there are a lot of catches like unicode bytecode attacks where you need to be really really careful while writing your own functions.
&gt;The solution is to encode the strings to hex bytestring, and then append 0x in query before the hexed string. This can be done by using unpack php function. Oh yeah, I forgot about that option. But it's definitely foolproof! I use bin2hex instead of unpack, as it only takes one arg instead of two. I did not know that's what PDO did for MSSQL. We can't use PDO because it always binds every parameter as an INT :-/
Thanks for being honest about this being homework, but without source data it's not exactly an easy answer. In general: * Make sure you're getting their last game; then do your calculation. As I'm reading it now, you're checking all games played. * Also, your return statement seems to be in the wrong place and using the wrong value 
I'm not positive, becuase I use SQL Prompt (a great intellisense alternative to the built in one) - but try Tools-&gt;Options-&gt;Text Editor-&gt;Transact SQL-&gt;IntelliSense and uncheck "Enable IntelliSense"? EDIT: Make sure you close out all of your open query windows before or after you change the settings - i think it only takes effect in new windows.
You glorious bastard!!!!! Thank you very much!
Can I ask what it is that bothers you about the intellisense?
there's one thing i hate about the intellisense in sql server: null does not show up in the list, so if you type null and hit tab (habit for me as i'm more of a developer than DBA, so i'm used to visual studio) then you will get @@CONNECTIONS
Install Visual Studio 2010 SP1 (No really, it broke my type ahead!)
I get those sort of problems frequently. I'm in the habit of reading what Intellisense is proposing, and getting quite good at hitting escape to cancel Intellisense's guess. I find that having it on, and having to hit escape or undo occasionally is a small cost compared to the benefit of having Intellisense for table and column names. It just takes a while to get used to, and is nowhere near as good as C# or VB's Intellisense (yet).
I completely agree, it's just annoying. It's my main gripe with intellisense in sql server. The other one is when you add a table, you don't get it in intellisense until you open a new query window
I am interested. Sent you a PM. 
I'm keen, I'm a long-time Redditor and DBA. EDIT: autocorrect said I was a long-time redactor!
Please welcome your new moderators: ## [hypo11](http://www.reddit.com/user/hypo11) and ##[mgdmw](http://www.reddit.com/user/mgdmw) Both are active and well-karmaed posters and commenters with what I feel is a good sense of community. Do right by each other, reader, subscriber, and moderator, and /r/SQL will thrive and be a source of joy to all!
Horray!
Thanks; I'm humbled, and here to serve :)
Hi there! I think you will get a better response from /r/Database; technically, the question you have asked is not SQL per-se.
Hi there, Thanks for your submission to /r/SQL. In this case, I really think you will get a better response from /r/Database or /r/Sysadmin. Technically the question you have posed is not SQL-related (i.e. the structured query language, as opposed to a specific database management server like SQL Server).
Forgetting about your count for a minute, why don't you use some maxs and mins to see which column in your dataset is yielding the extra rows? Alternatively, take out the group-by, return the raw data for one example, and this will help you narrow down and troubleshoot. I don't see anything upfront wrong with the code, but I am not experienced with group_concat and wonder if this could be partitioning extra rows which are throwing off your counts.
I'll give MAX a try in a minute, but I just wanted to respond about the GROUP BY. Without GROUP BY, I get a result like: (object_id) 22 (category_id as group) 15, 17, 14, 18, 43, 3 (linked_times) 30 It adds them all up and concats all of the groups for whatever reason.
For the record, it is GROUP_CONCAT that is messing things up. SELECT `objects`.`id` , COUNT(`object_relations`.`object_id`) FROM `objects` LEFT JOIN `object_categories` ON `object_categories`.`object_id` = `objects`.`id` LEFT JOIN `object_relations` ON `object_relations`.`object_id` = `objects`.`id` WHERE `object_categories`.`category_id` = '17' GROUP BY `objects`.`id` ORDER BY `objects`.`id` is fine, and returns the correct number of results. However, adding the GROUP_CONCAT seems to make everything get multiplied. I suppose I could implement something server-side that will divide the link backs by the amount of categories, but I'd rather write the query correctly =)
You can refresh the Intellisense cache using the "Edit", "Intellisense", "Refresh Local Cache" option in SSMS. That should then reload the table data, so you'll be able to use your new table and its columns. Alternatively, there's the Control-Shift-R hotkey mapped to the same function.
Wouldn't the group concat essentially be squaring the number every time? Could you just do the squareroot of the count(*) inline?
I thought so too. However, there is still an issue. For objects that are only linked to once, but have one category associated with them, `object_relations`.`object_id` references correctly shows up as 1. Similarly, but incorrectly, when an object is associated with two categories and only has one `object_relations`.`object_id` reference, the number shows up as 2. When adding another reference to the object with 2 categories, the number of references then shows up as 4. This behavior is the same when adding another reference to an object with 1 associated category (it will show up as 2). I am trying to find out what the problem is with my query/logic. I realize that this could be fixed by dividing and subtracting in the code, but I don't think it is a proper solution.
The trick here is to seperate the group concat part of the query away from the count. An easy fix for this is to get the group concat results in a temp/derived table, join it with your main query and select the results of the derived tables group concat. Not sure if this answer was clear. If needed I can get on my sql box and post code for you.
Sorry, I am kind of lost. I need to create a table to use temporarily, for each search, to store that concat information in? I wonder if this will be a performance hit or cause issues when there are many searches going on at once. I'll try to play around with that.
How about mis-autocompleting column names because I hit return when all I really wanted was a new line?
There's an icon on the toolbar by default that's enabled. If you hover over it, the pop-up text will say something along the lines of "IntelliSense Enabled." Click to disable.
I started out by reading a book titled (creatively enough) _Learning SQL_, an O'Reilly book that had a lot of useful information, presented in a format which was easy enough for me to understand. However, as I progressed in my learning, the biggest thing I've learned is that using SQL is not so much about knowing SQL but knowing the data that you're working with. Dig into your databases, look at the tables and the relationships between them, and you will have a much easier time. Also, feel free to join us over at /r/SQLServer.
I just got done taking my first Database Management class, and we used Wiley Pathways' Introduction to Database Management. It is a great source for JOIN statements, although it might be a little basic for you. I do however think it would be a great source to cover the basics for JOIN statements, transaction queries, and database administration. Also check out the [MSDN](http://msdn.microsoft.com/en-us/ms348103) site, more specifically here is the [T-SQL Page](http://msdn.microsoft.com/en-us/library/ms189826%28v=sql.90%29.aspx)
There are plenty of good books to get started (which others will surely mention), but what I wanted to share was just some general advice since you say you have some programming knowledge: Programming in T-SQL requires an entirely different approach to solving problms compared to object oriented programming. The reason is that SQL is optimized for "Set based" operations like updating a column on an entire table at once, instead of looping through each record one at a time. Just because the language supports what you are trying to do (like Looping via Cursors) doesn't mean that its a good idea. Most tasks can be easily accomplished using very simple set-based operations. **Design ideas you understand from OOP don't work well in SQL because it is optimized to perform different tasks.** I also recommend that you study/review how SQL actual writes/reads data in the database - to fully understand how your queries can lock certain parts of the database, at the page, row level, etc. Understand how it is using indexes, and in what order it grabs things. For this you should study Execution Plans. More info and a free e-book about execution plans [here](http://www.simple-talk.com/sql/performance/execution-plan-basics/). All this stuff plays a huge role in the effiency of a large production database. 
One has a "y" the other has a "G". That non-technical enough?
PHP is a scripting language. It is very common for web sites which want to run code on the server in order to serve you (the visitor) with dynamic web pages. It's popularity revolves around the fact that it runs on *nix operating systems, integrates with Apache Web Server very easily, and is free to use (Open Source style license). Also, it has built in functions for accessing a MySQL database (I think it has them for PGSQL as well, but I don't know this as fact). MySQL and PostgreSQL are both Relational Database Management Systems (RDBMS). Basically, they are the programs which hold, organize, and provide you access to your data. In short, they both do *roughly* the same thing: hold your data; however, how they go about it differs on a technical level. Each RDBMS has its pros and cons, when trying to figure out which one to go with you'll have to spend some time getting into the technical details. So, to try and wrap this up. A server which runs PHP and MySQL and a server which runs PHP and PostgreSQL will both do *roughly* the same thing. Which one you want to use will require digging into the technical details as each RDBMS has it's advantages and disadvantages. [This](http://www.wikivs.com/wiki/MySQL_vs_PostgreSQL) can get you started on that ball of wax. TL;DR: No, not really.
To add on the wise Sylver_Dragon, I was experimenting at work the other few months, I installed php / apache on ones of my boxes and ran it off a trial version of MSSQL. Forums, php and Wikis worked fine - but it was a weird world to be in to read any help or faqs, because it all assumes you're using the standard PHP/MYSQL combo. It does roughly the same thing.
PHP = Programming Language. MySQL and PGSQL = Database systems. You can use PHP with MySQL or with PGSQL.
PHP MySQL is the PHP driver to the MySQL database. PHP PgSQL is the PHP driver to the PostgreSQL database. It's a little like asking how the gas handle at Exxon is different than the one at BP. They're competing brands, and they do essentially the same thing, but some of the details inside are different. Also, Postgres doesn't soak Florida in oil.
The Microsoft Books Online are actually a really decent source of information. One of my favorite bookmarks: http://msdn.microsoft.com/en-us/library/bb510741.aspx
[W3School](www.w3schools.com/sql/default.asp) has some basic explanation on it. What certificate is that? Don't you mean SQL administration? Also SQL differs between systems. A query you write for MySQL could not work on SQL Server.
For the certificate I meant is it worth spending the money to take the test for one like this: http://www.microsoft.com/learning/en/us/certification/cert-sql-server.aspx And I will be sure to point him over to W3School! 
&gt;A query you write for MySQL can not work on SQL Server Just a minor nit-pick, "can" should be "might" in that sentence. Pretty sure that's what you meant, but some could read it to mean that no valid MySQL query can also be a valid TSQL query.
It sounds like your friend has found a SQL Server-based job, and wants to brush up on Microsoft's T-SQL variant of the SQL language. The SQL Server 2008 MCTS developer test isn't a bad start, but opinions are quite split as to whether it's worth it, given that tests a decade ago were quite easy to braindump for and this viewpoint has persisted despite Microsoft's changes to the way exams work. There's also a feeling that certs are generally just a money spinning industry, and hold no real value. I feel that it shows you have some knowledge about the topics being presented on the test. You might not know exactly how to write a trigger, but you'll remember the concept of why you might want to use a trigger. If nothing else, the certification exams are a good reason to focus your self-directed learning, instead of being used to compare two job candidates. 
If you want to learn SQL, I'd suggest you to use PostgreSQL. It's free, open source and very close to match ANSI SQL. I can't give you any resource though, beside [the doc](http://www.postgresql.org/docs/)
There is no need to be ashamed. We have all had a 'friend' who wanted to learn SQL before. I have a 'friend' who did some Oracle certification and thought that it was good value for money. This 'friend' of mine learned a lot about standard SQL and found that it provided a good basis for using MySQL, MSSQL, Postgres and obviously Oracle too. There are differences between different databases but if your 'friend' is as enthusiastic as my 'friend' then he will pick things up soon enough.
Yes i changed my sentence while i was typing(bad habit), seems very strict now indeed.
He'll want to do this one then: http://www.microsoft.com/learning/en/us/exam.aspx?ID=70-433&amp;locale=en-us
[SQL Zoo](http://sqlzoo.net) is all you need... you get realtime feedback as to whether you've *got* something or not, which is a fundamental aspect of effective learning.
Stanford just went through a formal class online, for free. And they left all the material online for posterity. You can register [for the class here](http://db-class.org), or just access the materials from their public [open classroom page](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToDatabases). The registered class includes quizes, homework, and exams, and tracks your progress through it all. Per other people's comments w3schools and sqlzoo are also good, for starters. The Stanford course is the full package. In my case I need to learn db stuff from multiple different angles to get it.
Just off the top of my head: Select * from [am_jobs] where [job_status] = 'ABORTED' and ROUND([job_id],0) = [job_id] order by [job_started] asc
you could try: select job_id from am_jobs where job_status = 'ABORTED' and isInt(job_id) order by job_started That's if you're using sql server. if you're using mysql, you can try converting it to a string, then checking to see if it contains a "." 
Are you looking to truncate? Ie 32.01 becomes 32?
no. if both exist in the table the query should only yield 32
thank you. i ran it and it looks good. 
Hello, I have already completed this project and successfully sold it to one company and now have two more interested in a very similar setup. Thanks for the heads up though I will be more careful to read the rules from now on to get a better idea of what posts are better suited for what subreddits, you are the first mod from any subreddit to personally offer me up useful information :) I appreciate it keep up the good work. Feel free to delete this post if you wish.
I'm not too familiar with it but from Googling I see that SQL Azure uses TSQL as its dialect, which I am familiar with. If that's the case then *'committee''s'* should work. I haven't a clue why it's not without seeing the whole query. As for the second way, try *'committee' + char(39) + 's'*
Yeah, it is supposed to be T-SQL. I should've mentioned that. I'm really not sure why 'committee''s' doesn't work. 'committee' &amp; char(39) &amp; 's' complains that the types "varchar" and "char" are incompatible with the "&amp;" operator, and the "+" operator produces the same [unrelated] errors.
That's really strange. I've done both of them in SQL Server manager console and it works fine. Maybe if you posted some of your source? Driver issue maybe? Are you using an ODBC layer? 
Another important reason medium-sized businesses choose MS SQL Server: Application support. Lots of business applications require a database, and many of them only support a handful. Often it's only one or two of: SQL Server, Oracle or DB2.
&gt;Microsoft paid off the major php framework makers to go make their drivers sql server compatible Yeah, but only if you run PHP on a Microsoft server. What a waste. PHP + Linux + FreeTDS is a ghetto.
&gt;I'd love to know exactly why we don't see more variance in the submitted results - competition is good! Last time I checked, the EULA for SQL Server doesn't allow me to publish or talk about benchmarks.
Right you are: &gt; BENCHMARK TESTING. You must obtain Microsoft's prior written approval to disclose to a third party the results of any benchmark test of the software. I'd wager that the main reason for that is to avoid other database vendors from claiming that SQL Server performs badly ("Check out our benchmark that shows how poorly SQL Server runs this workload, and then see how awesome Oracle is!"), and not to deter the "Hey, I got 3 million transactions per second so upvote me on Reddit!" blog posts. If you have a competitive TPC benchmark running on SQL Server (and you've put the time in to document everything), then Microsoft are unlikely to be a barrier. But why don't we see TPC benchmarks submitted for Postgres or MySQL?
Never mind, I figured it out: SELECT GROUP_NO, PARTICIPANT, DEPENDENT_NO, PAYMENT1, PAYMENT2, (PAYMENT1 + PAYMENT2) as PAYMENT_TOTAL into #Temp FROM Table
Its a pretty standard one-to-many relationship. You need a table of hypervisors, and a table of VMs. The hypervisors table should have an integer identity column as it's primary key, and the VMs table needs a hypervisorId column that's a foreign key to the hypervisors table. This way you can query, say, Select foo from VMs where hypervisorId = 5 That would return all VMs in that "family," going with that analogy. 
Your query makes perfect sense, thanks. Is there any reason why you would split this into two tables? It seems like the query you listed would work without doing that: Select foo from VMs where hypervisorId = nameofhypervisor 
The part where I'm struggling is how to process the resulting lists programmatically. All data comes back as one table in its raw form, and I need to restructure it in a way that makes sense. As for how it should be structured, I understand that now. But how to get it there...I'm stumped.
&gt;I'd wager that the main reason for that is to avoid other database vendors from claiming that SQL Server performs badly I'm with you on that one, but I still think the agreement stinks to high hell and back. It gives me serious consideration about using my favorite Microsoft product. &gt;But why don't we see TPC benchmarks submitted for Postgres or MySQL? That's a good question. I imagine a lot of it has to do with money, but I would think the EnterpriseDB folks would have a lot to gain from publishing high scoring TPC benchmarks. I do often hear of Postgres performance problems, and while I assume a large percentage of those are due to mis-use, there is a real chance that a properly deployed Postgres DB just isn't quite as fast as SQL Server or Oracle. The difference might only be a few percentage points, but enough to keep them off the top of the charts. Personally, I'll take the free (and also FREE) MVCC database over SQL Server any day. But when my employer is footing the software cost, I am very happy to use SQL Server.
One reason is that you can stick family attributes in the family table and member attributes in the family members table, that way you don't pull redundant rows for every family member each time you query for family attributes (and vice versa).
OK- I'll back up. I realize this may be outside of the scope of the subreddit...but the boundaries are blurry. I am running WMI queries via powershell against servers in a large environment. The data is collected and pushed to XML or a CSV. I am asking each virtual machine "who is your host?" and it the reply is added to that row of data. I am now beginning a process to structure and add said data to a SQL DB in a way that makes sense. I'm having trouble with this phase. I need to reorganize this flat data into a structure that makes sense...probably using a mix of Powershell and SQL.
Thanks for your help! I managed to get it to work: My script started by dropping all of the tables in the database, re-creating them, creating a bunch of constraints, and finally inserting some default values (where the problem with the apostrophes was occurring). I found that if I drop the tables in an order that didn't violate any of the constraints, using the double-apostrophe worked. I had thought that if I'd dropped all of the tables before saying "GO," it would drop them all at the "same time." Guess Verilog has had too much of an influence on me :/. Thanks! Edit: By the way, I was using SQL Server Management Studio and the Visual Studio "LINQ to SQL" auto-generated classes.
Everything went better than expected
Indeed! So - followup question. I'm potentially finding myself with a circular constraint reference between two tables and the normal way I'd drop the constraint: ALTER TABLE EventVenues DROP CONSTRAINT EventVenues_EventVenueIdRef GO doesn't work. It's saying that the name of the constraint isn't valid. Do you know if the syntax for this is any different on Azure? I created the constraint with ALTER TABLE EventVenues ADD CONSTRAINT EventVenues_EventVenueIdRef FOREIGN KEY ( EventVenueID ) REFERENCES VenueList ( ID ) GO
Normalizing data makes it less error prone for doing updates. If you need to change data that's specific to a single or group of hypervisors then you would only need to modify a few rows in the hypervisor table instead of all the rows in the VM table. This may not be an issue for smaller tables but can become a problem if your tables grow very large. 
I know very little about Microsoft Azure, or whether or not they're using some altered dialect of TSQL. The only thing I can think of is maybe try specifying schema in the name. dbo.EventVenues_EventVenueIdRef 
One of my mentors once told me there are 10 types of DBA, and I honestly agree with that sentiment highly. Which subtypes are you looking at mastering? Programming tsql/plsql/etc, database architecture, database tuning, database maintenance and physical design, application accessibility in a database, advanced query design? What draws you to sql? for the record, BI probably most interesting is gonna be architecture and query design. For architecture, I'd read the Kimball series. For query design, depends on the language. I can say that for MSSQL, http://www.amazon.com/Inside-Microsoft-SQL-Server-2005/dp/0735623139 is a great book. The inside MSSQL book, however, is not my first reccommendation for neophytes. If you've got a good six-seven months of sql experience on the job it's great; for first learning sql querying there are likely better books (dunno which, sorry). 
To add another reason for linking by id's: every typist makes mistakes. Your database will think that 'MySever' and 'MySver' are distinct, while you might know that the difference comes from too much caffeine. Linking via id's tends to help mitigate this, especially when a front end is built. The developer can create a drop down box based on the hypervisor table, rather than relying on someone typing it correctly every time.
One option is to assign each host an id number and a parent id number (or, in your family analogy a personId, a motherId and a fatherId.) You then join the table back to itself on parentid = hostid. It can be a bit of a mess and will require some recurrsion in you application to build a pretty tree; but, there it is.
http://sqlzoo.net/ got me up to speed pretty quick.
Thank you for your direction, and yes I will benefit from query design. I acquired lynda.com's sql essential training which requires some setup to be able to work on existing databases(included). I have already gone thru 2 chapters, but I will check into your links.
"The goal of /r/SQL is ..." Query language, in structured form. The more tables, the merrier.
Here's a reference that explains the purpose of the (+) operator: http://oreilly.com/catalog/orsqlpluspr2/chapter/ch01.html
Are you using TBL_TRANSACTIONS.EQUIPSVCSEQ anywhere else in the WHERE clause, possibly triggering the &lt;&gt; oddity I spoke about in my other post? You said you're using this in Crystal - is Crystal using that field somewhere? I use XIR2, and I don't think it modifies the query when it is entered as a command before it is executed on the Oracle side, but other version might. Crystal can be an odd beast.
LOL, no problem. I'm at work right now actually, and needed a break from copying and pasting crap into Excel. Today is my Monday due to the holidays, and I usually have about 3 hours of executive reports to update every week. I hope I'm able to help you get it solved. I use PL/SQL + Crystal XIR2 + BOE every day, so I know how frustrating it can be. I usually just use it to dump a tab-separate file on an FTP server somewhere that I can later access over an Intranet to update an Excel file that is the actual report people use. It's a real mess, and I'm trying to get moved over and replace the whole scheduling process with Python scripts and cron jobs...
Yeah, the EquipSvcSeq is being joined already: (TBL_TRANSACTIONS.EQUIPSVCSEQ = TBL_EQUIPANDSERVICES.EQUIPSVCSEQ (+)) I'm also trying to learn this on the job, so I know how you feel.
One step ahead of you, sir. Lunch was delicious. Chicken fried rice from Trader Joes.
Styling results is not something that SQL Studio is designed to do. You will need to use Excel, HTML, whatever file format you want, just as long as it supports styling.
Perhaps not all people are expected to have a customerId? compare the two resultsets you get with this code (sql server) declare @foo table (customerId int null, state varchar(10) not null,fullname varchar(50)) insert into @foo values(1,'Texas','John'),(null,'Texas','Charlie'),(2,'Texas','Tommy'),(3,'New Mexico','Kate'),(4,'New Mexico','Steve') select state, count(customerid) from @foo customers group by state;select state, count(state) from @foo customers group by state;
Can't you use NVL?
I was looking at using NVL2, actually, as it takes NVL2(expression, If Not Null, If Null), but I'm still trying to get it to work. Right now (still non-functioning) NVL2("TBL_TRANSACTIONS.EQUIPSVCSEQ", "GREATEST(TBL_EQUIPANDSERVICES.DEINSTALLWORKORDERSEQ, TBL_EQUIPANDSERVICES.LASTWORKORDERSEQ) = TBL_WORKORDERS.WORKORDERSEQ (+)", "TBL_TRANSACTIONS.WORKORDERSEQ = TBL_WORKORDERS.WORKORDERSEQ (+)") This gives me an "ORA-00972: Identifier is too long" error message. When I try it without the double quotes, I get "ORA-00909: Invalid number of arguements."
Well, yes this is going to be for exercise purposes. 
type "T-SQL how to use insert" into google. seriously - that is my secret. T-sql then whatever command - enourmous returns. i assume everyone else does the same! 
Parameterised queries!!!!
Untested..... select a.name, a.cnt from ( select a.*, rank() (order by a.cnt) as rnk from( select d.name name,count(i.iid) cnt from disease d, study s, institute i where d.did =s.did and s.iid=i.iid and i.cidade like 'London' group by name) a) a where a.rnk = 1 
Between the top of my head and google, ASP.Net caches objects for a specific amount of time (I think this was in web.config). More info on that [here](http://msdn.microsoft.com/en-us/library/ms178597.aspx). You can also explicitly set a dependency on an object, for example, a [SQLCacheDependency](http://msdn.microsoft.com/en-us/library/ms178604.aspx). I've not used the later; however, there should be enough info there to attach one to your page and (hopefully) get roundtrips to the database when (and only when) the underlying tables change.
When are we recommending books on 7 year old products?
Right now. There's likely a version updated for 2008, but most of the principles don't change even up through Azure.
Dude. *Duuuuude* Find beginning of day: &gt;SELECT CAST(datetime2 as date) Finding the end of the day: Why on earth would you want to do that? Find the beginning of the next day, and use &lt; instead of &lt;=. The way this guy does it could miss values like 11:59:59.01. Find beginning of month? Using DATEPART instead of DATEDIFF makes more sense to me.. I bet that guy's last two examples contained &lt; or &gt; characters which broke the HTML that he shouldn't be writing. You should always be using datetime2 instead of datetime fields.
I'm pretty lost. At some points, you alias domain_tree and at other points (well, one point) you don't. I see where you are trying to calculate a depth in your query, but I'm not seeing any reason why it would compute more than two levels deep. Maybe you should write a recursive CTE (does MySQL support them?) to blow out your hierarchy and then order on the domain list that comes out. 
&gt; there are some architectural changes that would make this a lot simpler to perform Would you mind expanding a bit on this? The OPs schema seems like the obvious way to implement this, I'd be very interested in learning about more optimal ways of achieving the same thing.
You really have two options: Put the files in the database - or put the files on disk somewhere and put the file names/locations in the database. Putting the files in the database has pros and cons. Some advantages might be that you get the security features of SQL Server, as well as the backup capabilities. The disadvantages are that SQL is optimized for serving data, while file servers are optimized for storing files. Also, the database backups will take longer and longer because SQL will backup the files everytime - while a backup of the disk can be configured to only backup anything that is new/updated. If you decide to go the route with storing the files in the database, I would recommend using SQL Server 2008+ for file storage, SQL Server Integration Services to upload the files (configure it to check a file location every 5 minutes and upload any files it finds), and some sort of web based front end tool. Good luck! 
As a heads up: Make sure you split up the drawing name and its revision into two separate fields. Some places (like the one where I work) made the mistake of putting "DrawingName-RevisionLetter" crammed together into the same field. This can make your life more difficult down the road, so just remember to keep all important information in completely separate fields.
So they use an SQL database for drawing management? 
We use a heck of a lot more than a SQL database, but ultimately our CAD system pushes all of its internal information into SQL. Having it in SQL opens the doors wide open for all sorts of reporting, as well as integrate the information into other systems. For example: our drawing system is completely separate from our "Item Master" that contains most of the important information about all of the parts that we have. One of the things our Item Master needs to know about is drawing information. Because our drawing information exists in SQL, we have an automatic process that pushes our latest drawing revisions into our Item Master. This way, our designers drawings end up in our Item Master system, but the designers don't ever have to leave their set of tools to get the information copied over.
SQL sounds like a good place to store your data, I would also point you at SQL 2008 specifically. The reason being the [FILESTREAM](http://technet.microsoft.com/en-us/library/bb933993.aspx) feature which was added in 2008. It can be used to store the actual files in a much better fashion than in earlier versions of SQL and then have them included properly in backups and prevent orphans (e.g. a file gets moved/deleted but the database is never updated to reflect this or vice-versa). That said, from some of your other posts, unless you have a development team handy to give you the front end you are after, you may want to look into Sharepoint. It will handle most of the messy details for you (and there is [/r/sharepoint](/r/sharepoint) to help) and give you a front end. It uses SQL as the backend database; but, allows (and encourges) you to avoid delving into SQL directly. SQL is just a database engine. It will hold your data, but that's it. Unlike Access, it does not come with tools for making applications to interact with that data.
Well, it would be select sum(quantity) from transactions where item_number = 'xxx' to start, and I know dates, but I guess I don't get how a join will do this. I want to do this in a single SQL statement; I don't want to have to query all the part numbers, then walk through them one by one with separate SQL statements. Does that make sense? I can get the counts properly for the items if I only want a specific date range, but not if I want all date ranges. I can see all the quantities removed in 90 days for example, but not 90 days AND 14 days AND within the calendar year. Thanks. edited for clarity
Found this thread: http://social.msdn.microsoft.com/Forums/en-US/sqlservicebroker/thread/e8346a87-4225-4468-a143-66682f58973c/ Check permissions on the endpoint - there are some trace items referenced to help troubleshoot.
Hey Hypo, I was going to use this little project to kick start my ass into learning more about SQL, but I think you have a very good point and you are right - I just need some simple version control. Thanks for the help and I'll go check out those links you gave me. I really appreciate the input. 
No problem. I certainly don't mean to dissuade you from learning SQL, but as I said, I just think there are better tools suited for this particular task.
FYI: Another solution is to set up any one of the free (or commercial, if your company wants to pay for it) wiki applications. After all, a wiki basically has its own built in version control. You can make your wiki pages for each project just be a link to an attachment, and the attachment is the pdf. You should even be able to version the attachments themselves. Just look at how [wikipedia works with an image file](http://en.wikipedia.org/wiki/File:Hurricane_Isabel_from_ISS.jpg#filehistory) and you can see how this could be used for your purposes. Wikipedia runs on open source software called [MediaWiki](http://en.wikipedia.org/wiki/MediaWiki) but there are plenty of others out there.
One approach... select item_number, max(countall), max(count14), max(count90), max(countyear) from ( select item_number, count(*) as countall, null as count14, null as count90, null as countyear from mytable group by item_number union all select item_number, null, count(*), null, null from mytable where GETDATE() - trans_date &gt;= 14 group by item_number union all... ) group by item_number Or you can use analytic functions, if supported by your platform... ...count(*) over (partition by ... )
This approach works in Oracle, so some minor changes for SQL Server and you should be good to go. select ITEM_NUMBER, count(case GROUPIT when '14' then 1 else null end) as COUNT14, count(case GROUPIT when '90' then 1 else null end) as COUNT90, count(case GROUPIT when '365' then 1 else null end) as COUNT365, count(case GROUPIT when 'MORE' then 1 else null end) as COUNTMORE, count(*) as COUNTALL from ( select ITEM_NUMBER, case when DAYS_AGO &lt;= 14 then '14' when DAYS_AGO &lt;= 90 then '90' when DAYS_AGO &lt;= 365 then '365' else 'MORE' end GROUPIT from ( select ITEM_NUMBER, trunc(sysdate - trans_date) DAYS_AGO from MYTABLE ) group by ITEM_NUMBER 
&gt;SET ROWCOUNT 10 What is this magic and how long has it been in SQL Server???
Learn SQL The Hard Way by Zed Shaw. http://sql.learncodethehardway.org/book/
I'm not familiar with the bulk insert command, but after a quick glance through MSDN I'd suggest using OPENROWSET instead, as outlined on this page: http://msdn.microsoft.com/en-us/library/ms178129.aspx. It looks like this will let you use normal insert into table1 (column1) select column1 from table2 syntax, which will allow you to specify source on a per file basis. You can set the MD5 column to be a computed column that uses the HASHBYTES function to get the hash of the email. 
[w3schools](http://w3schools.com/sql/default.asp) is usually a good place to start. Also, you can always pick a project, poke at it for a while and come back here. 
The first 4 chapters of the SAS Advanced Certification Manual cover it pretty well and compare/contrast the results produced by different queries and joins to their data step counterparts. That will be very useful for you if you are familiar with base SAS manipulations. I learned SQL syntax in SAS and the transition to SQL server is pretty painless. Some of the comparison operators are different but nothing major.
Best book for quick learning of pretty much everything you'll need to know is Teach yourself SQL server in 10 Minutes. There's more resources later but that covers pretty much the gambit.
I didn't notice this until just now.
I'm not familiar with the 24 hour series but I can certainly see how that would be the case. Since I can't compare I'll defer to you. Personally for SQL I loved Itzik Ben-Gan and his book would be great but I'm not sure how much theory OP wants so I rolled with what got me started.
I am in SQL Server 2008. Thanks guys. 
That looks like what I am aiming for. However, I need to find the word within the text. Right now, I just have: Select * from tx_shared_labs_evaluation Where remarks like ‘%HIV%’ COLLATE SQL_Latin1_General_Cp1_CS_AS
I'd agree that it sounds like a permissions issue based on my experiences with Database Mirroring. What's the output of sys.dm_db_mirroring_connections? If it's able to establish a connection, you should see two rows in here that are both connected. You can also check the SQL Server ERRORLOGS for login attempt failures, as that would be another indication that it's a permissions issue. If you're looking for a quick workaround for testing purposes only : GRANT CONNECT ON ENDPOINT::name TO PUBLIC on both servers and see if that allows it to succeed. Please don't do this in a production system though as it opens a giant security hole.
The way I've always done this is below where column = upper(column) I guess what I am not understanding is - do you need to find a specific word in uppercase only? Or are you more interested in finding a list of any words that are all uppercase, regardless of what string literal the word is made out of. People have already responded to the former, and this will do the latter.
I have good experience by browsing blogs and looking at interesting posts. Usually these 'case studies' in random quirks end up doing a pretty good job of filling in the gaps and rounding out my knowledge also, pinal dave is the man! http://blog.sqlauthority.com/
Thank you, I'll pick it up!
[T-SQL Fundamentals](http://www.amazon.com/Microsoft%C2%AE-Server%C2%AE-T-SQL-Fundamentals-PRO-Developer/dp/0735626014/ref=sr_1_1?ie=UTF8&amp;qid=1328239168&amp;sr=8-1) by Itzik Ben-Gan.
Ben-Gan is a tsql ace!
Itzik Ben-Gan. Period. http://www.amazon.co.uk/Microsoft-Server-T-SQL-Fundamentals-PRO-Developer/dp/0735626014/ref=sr_1_1?ie=UTF8&amp;qid=1328266686&amp;sr=8-1 and http://www.amazon.co.uk/Inside-Microsoft-SQL-Server-2008/dp/0735626030/ref=sr_1_2?ie=UTF8&amp;qid=1328266686&amp;sr=8-2
Karwin and Celko. http://www.amazon.com/SQL-Antipatterns-Programming-Pragmatic-Programmers/dp/1934356557/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1328272864&amp;sr=1-2 http://www.amazon.com/Joe-Celkos-SQL-Smarties-Programming/dp/1558605762
You will need to use what is called a "group by" to aggregate data. Your group by will list the columns whose duplicates will be merged. You need to determine two things to use a group by effectively 1) Which rows constitute a merge candidate (in your case, is this group_name, local_name, or both? 2) How you want to represent columns which are being merged. In your case, I don't undersstand how you leap from showing total members at 11 and 3 from the data you provided. In your case it looks like you would want to select min(local_code), local name,division name (not sure how you want to calc the rest) from table_name group by local name, division name
If the subscriber count from your rows are going to be what you want to horizontally apply to the result of the query, you are going to have to CROSS APPLY the results
That's what I need, yes .. the subscriber count, set in to 2 different columns, in a single row, with the like values of each row, also there
I don't see 11.00 and 3.00 in the original table otherwise I'd recommend pivoting on it.
Why use NUMBER? To have a faux-primary key on the result? Since you are grouping on NUMBER, won't it kinda break the grouping and provide a row for every row in the source table?
oh, haha... I'm sick and kinda out of it right now... you had NUMBER capitalized and I was thinking of the ROWNUM keyword ;)
http://www.jasonfollas.com/blog/archive/2008/03/14/sql-server-2008-spatial-data-part-1.aspx Assuming you can format the data properly you should be able to use a combination of STConvexHull and STWithin of the geometry spatial type. Not knowing your data, I'm a bit murky on the specifics, but MSSQL does have functions for this. I'm uncertain if the AVL GPS strings follow the OGC format -- you may need to do some ETL depending on the translation. As for speeders, well, that seems easier doesn't it? Distance/time ?
Depends on your database engine. It'll probably be optimised before running and won't make a difference. Easy enough to check though, right?
I am not sure on the telnet thing. I had to contact our network team to verify that, and they take a really long time to get back to me. Named Pipes are enabled on in-rca-sql and ob-rca-sql\installs. I tried using the IP addresses, and got the same result. Thanks for the suggestions, anything else would be appreciated.
select * from sys.dm_db_mirroring_connections says that the state_desc = 'LOGGED IN' and the login_state_desc = 'ONLINE' .. not sure what other columns are relevant here. I tried the 'GRANT CONNECT ON ENDPOINT...etc' thing, and got the same error as before. Any other thoughts? Thank you for the suggestions so far!
Does the account you are using to connect to each node have access inside of the database, and also running as a windows service account? 
At the domain level you can set accounts to be "service" accounts. You would still define your sql service to run under them but they are designed to not rely on your user account, since it changes, and it is bad practice. With all of that said, your user account with SA rights should be able to get things going. If you are not reaching the server, try plugging in the IP in place of the server name and running it up that way. (You will need to turn on IP address connections in the configuration) 
Make sure you aren't specifying the KEEPNULLS statement in your bulk insert and set a default value on the column to '' and then perform your bulk insert. Bulk insert has the advantage of being minimally logged - only page allocations are logged instead of individual rows commits (this means it's faster, yo). If you don't want the default value in the future, just remove it after the fact. Is this actually using BULK INSERT or are you using bcp?
or RTRIM()
Len(rtrim(string)) will give you the same results as just len(string). If you want the result to take trailing spaces into account you need a different solution. 
The main reason for this is due to how CHAR and VARCHAR data types interact. CHAR columns are fixed length, and exposed this fixed length via trailing spaces. They were also a lot faster than varchars back in the days of slow CPUs. As LEN() ignores trailing spaces, you can convert columns between these two types without rewriting code. 
Or just: SELECT LEN(MyString + '_') - 1
I'm going to argue that this isn't a technical problem - it's a process problem. The simple answer is that users should not have this sort of control over tables. They should insert, delete and update, and that's it. Randomly adding and deleting columns is a nightmare, and this sort of thing should be reviewed and executed by a DBA - especially when there's a 100+ GB database involved. By reviewing their changes prior to you executing them, you can adjust your triggers to take into account the change in table structure (or recreate the trigger, when they drop the table). 
Is it always going to be two bytes per character? Not all unicode translations use two byte characters (or even fixed length characters)
Exactly the kind of ambiguity that would keep me from using DataLength().
varchar is the way to go, but the size depends on your DB engine. MSSQL has a limit of 8000 unless varchar(MAX) is used. I believe MYSql has 64K limit for varchar
I'm not sure how to tell what database engine I am using... 
&gt;The simple answer is that users should not have this sort of control over tables. I completely agree, but that is how the application was made. Every time a maintenance is done (nightly) it drops all tables and recreates them, every time a field is added, 4 tables are dropped and recreated. I'm currently trying to figure out how to have a database level trigger recreate table level triggers so that when DROP_TABLE, ALTER_TABLE and CREATE_TABLE happens the table triggers are re-created as well.
I know what LAMP is... and I know I'm connecting to an SQL server... (not to be rude). I don't know how I made it seem like I didn't know what I'm doing... I just wanted a quick answer on what datatype to use for storing text. I may want to preserve some formatting such as spaces between paragraphs and stuff, but maybe I have to do that a different way. I wasn't sure when to use the "blob" datatype, but I can just look that up.
Looks to me like your IDs are getting mangled somewhere, and are thereby breaking your foreign keys. Run this on one of your local servers that is working and on the Rackspace server and paste the responses. select id, title, pageid from page where id=39 Also, why in the **HELL** is there a page.id and a page.pageid!? I hereby hold the writer of this application directly responsible for all the crap that I have to deal with to get anything done through IT. He (or she) is the reason I don't have root access of my own development server.
He left the company a few years ago before I started working here, and after dealing with 2 websites written by him, I actually curse his name and make death threats against him at least once a day. I have a fantasy of finding out his phone number or email and just harassing him, telling him what a bad person he is over and over again. That page.id, page.pageid (which is supposed to work like page.parentid for subpages, though you seem to have figured that out) is not even an ice cube compared to what floats under the water with this site. The master page is *35 printed pages*. His code does things that shouldn't be possible. I described him yesterday as The Superman of Terrible Coding, because our earthman physics/coding standards simply don't apply. But I'll try that, thanks.
Just wanted to say I found out what was wrong. There was no primary key set ಠ_ಠ
Well now that you've got me thinking outside of the box, here's another way to do it: &gt;SELECT LEN(REVERSE('hi ')) -- returns 3 Edit: Don't listen to me. Reversing may help to account for trailing space, but then ruins the calculation for any whitespace at the beginning of the value.
Decent Linux drivers!?!? Oh... nope.
I don't want to do your homework for you. I don't think anyone else wants to either. Why don't you tell us what you tried, and we will take it from there :) 
Well i figured out how to select the employees with the ids 20 and 30: SELECT COUNT(employee_id) FROM demo.employee WHERE department_id = '20' OR department_id = '30'; And how to find the min and max: SELECT MIN(salary), MAX(salary) FROM demo.employee; Now Im figuring how to combine them...
Thanks I figured it out
ya i agree that you would want to do my homework... i ended up figuring it out at least i know there are people who are willing to help if i get stuck Thank you :)
Grats! Yep, we're here to help. SQL is pretty much SELEC, FROM, and WHERE.... with a little confusion mixed into it.
I really like N++ for my code scratchpad, what makes Rapid SQL so much better? (never used it so i feel like i may be missing out on something, honest question)
Well, for one, it actually processes my DECLARE CURSOR... statement - which is the only reason at this point that it's better. Otherwise, I prefer Universal SQL Editor - it has some nice outlining features that allow me to expand/collapse lines of code. It also has a results pane visible alongside (below, acutally) the editor window, it has a better database browser.
BTW: can you clairify if you need this to be kept up to date: ONLINE all the time, or done occasionally to keep development DB close to prod looking? check out Idera's SQLsafe tool. its free for 14 days and will get that db in a copy only mode, probably down to 200gig. 
As your title suggests, the copy of the DB will be for reporting purposes? If the source DB is an OLTP DB, have you thought about creating an OLAP DB and creating a data warehouse for your source reporting data? Use SSIS to move and de-normalise the data during silent hours and potentially use Analysis Services to create some analysis cubes? I know this doesn't answer your question directly, but reporting databases should really be OLAP as normalised OLTP's are very unfriendly for reporting/analysis purposes. We have a guy at work that does reporting from a massive OLTP service desk database which partial-replicates during silent hours. The T-SQL he's put together to extract the data would bring tears to your eyes (he's an older guy who hasn't moved with the changes brought in since SQL 2000). Loads of join's all over the place. It's shit in terms of performance and a nightmare to work with, maintain and all that. If the data was extracted into an OLAP DB, he'd be laughing. I'd also ask the guys on [http://dba.stackexchange.com/](http://dba.stackexchange.com/)
Good point, we really do not need everything, maybe 1/3rd of the database? I suppose this would be a bonus when doing replication?
You don't have to wait for a specific time or silent hours. You can run your SSIS packages whenever you want, at whatever interval you want to pull data into an OLAP db. 
it would be a HUGE point. if you are going to go the route of replication, use the GUI to set it up, but have it script everything out. its much easier to re-run the script, to stop/start/enable/disable etc. btw: you don't need 'sa' to run replication, only to set it up. DBowner on the db is sufficient for the account to the dest and source db. 
Those are some serious reporting requirements - 800 GB, must stay completely up to date, and cannot go offline. Transactional Replication is probably your best bet (unless you can figure out a read-only SAN replication to keep the two servers in sync), but if you have a replication "incident", you could be down for 12 hours while you re-intialiase. Can you run reporting out of the existing database? You may need to add some additional indexes, but a beefy enough server should be able to handle the workload. More memory, and a 2008 upgrade utilising Resource Governor would help to segregate the OLTP vs the reporting activity. A couple of SSDs for the more critical indexes would also help, and aren't so expensive if you are saving all the money from having to purchase a new server.
That does it more complicated - it could happen at any time. Do you need to guarantee that you capture every transaction? You could have a dynamic trigger generation that creates a new table with a unique name, and recreates the trigger. It would be easiest to schedule this to run every minute to check if the table has a trigger. If it doesn't, create it and seed the new table with the new data. This might be easier than DDL triggers, and will constantly be checked (and won't interfere with the application if something goes wrong with your code).
Ever since Dell took over, our database went from 500 to 800 GB. It looks like one of the log files is 300 GB right now, which I just brought to their attention. If we use replication, I think we will only be bringing over ~250GB of data. It is very slow when reading from the 2005 server from the 2008 server. That is the reason why we need the database over on the 2008. I don't actually plan on 'reporting' from it, but I will be using the fact tables to do back end calculations.
This has been our Plan B the whole time. Really would like to avoid this if possible. Its not just about bringing the data over, there are mapping/maintenance tables that the users update that would have to be replicated. It just adds a layer of complexity that we would like to avoid if possible.
by your title, it sounds like you want to use this for reporting... in that case, you'd probably want a more denormalized schema
Fixed! The developer got back to me - turns out I needed to change a setting in the editor's options, so that something OTHER than a semi-colon was being used as a statement deliminator (like a "/"), stick one of those symbols at the very END of my Declare... begin...loop.. end loop... end.. code, but leave the semi-colons in place WITHIN the code. Did that, and now it works.
Go into SQL and put this in the select statement: Case when field1 = 1000 and field3 &lt;&gt; 3020001123 then result when field1 = 2000 and field3 &lt;&gt; 230300030 then result when field1 = 2500 and field3 &lt;&gt; 002390400 then result end as "Field" Let me know if you need any additional help. Single quotes are necessary around the 1000 and 3020001123 if they are a string.
http://office.microsoft.com/en-us/access-help/examples-of-query-criteria-HA010066611.aspx
 This is probably a bug in the application. Even in simple the log file grow. If you have a hugh table and do delete * from massive The contents of massive have to be stored in the log until the transaction is committed. Or the situation you are probably seeing is a traction starting and never ending and it just keeps on doing things making the log grow :) 
Access's SQL is... "special," to say the least. There is no case statement - you use if functions compounded together to create the same effect. Anyhow, OP, stop using Design view. Time to learn :) You're looking for something along these lines I think: SELECT fieldsWeCareAbout FROM table WHERE ( field1 = 1000 AND field 3 &lt;&gt; 302000123 ) OR ( field1 = 2000 AND field 3 &lt;&gt; 230300030 ) OR ( field1 = 2500 AND field 3 &lt;&gt; 002390400 )
Tbh that is just the result of bad db planning and lack of validations. Without seeing the tables I would recommend a select/union/ query 
Thank you sir/ma'am - I find your name to be non-indicative of your character :)
If only you knew... ;)
**Holy crap! Thank you SO MUCH for this link!!!!** I'm looking at the published date and assume this is brand new. Way to go MS on this one, this is the *only* way that SQL Server could possibly survive all of the changes that are coming with "the cloud." Edit: *Doh*, guess I shouldn't have deployed our latest batch of webservers with a 32-bit OS. Still, this is so awesome that it's worth re-working those servers.
Yes you can, create a reporting database off the ugly data.
Try joining the two tables. 
If you (I'm assuming you are Mr. Carreon) are looking for some real data sets to play around with, the US Census Bureau publishes its data [here](http://www.census.gov/geo/www/tiger/). Also, there is some data available on the Salton Sea in California [here](http://www.institute.redlands.edu/salton/data/gisdata.aspx). And lastly, the Univeristy of Hawaii publishes some [data sets](http://magis.manoa.hawaii.edu/gis/data.html) as well. In a previous job I tended to some ESRI ArcGIS servers. Interesting stuff.
I'm not sure about your model it seems a little peculiar as your rental should have a cassette id and a date, so you need to join your rental to the cassette table. I'll show you using my design Cassette_Table =&gt; Cassette_ID, Acquisition_Date Rental_Table =&gt; Cassette_ID, Rental_Date Now, we need to join the rental table back to the cassette it belongs to, otherwise the DB will do a Cartesian join - return every permutation of row from one table with row from the other, as in your case. Hence millions of rows. Now before we join, we need to understand what set of results we are looking for, the largest gap before a cassette was rented. * A cassette may be rented out a lot of times, and we are only interested in the first rental (the minimum rental date for each cassette) and how much that differs from when the cassette was acquired. The minimum will use a grouping function, we are grouping minimum rental date for each cassette. Bear in mind this result set will not have a record for any cassette not yet rented out! * One we find this, we want the biggest gap between an acquisition and a rental for any cassette. First rental date... select Cassette_ID, Min(Rental_Date) from Rental_Table group by Cassette_ID Now we can join to the cassette table to calculate the time gap difference for each cassette using this... select c.Cassette_id, DATE_DIFF(c.First_Rental_Date, r.Acquisition_Date) as Gap from Cassette_Table c join (select Cassette_id, Min(Rental_Date) as First_Rental_Date from Rental_Table group by Cassette_ID) r on r.Cassette_ID = c.Cassette_ID If we want the maximum time gap difference for all cassettes, we apply a max clause over this result set... select Max(DATE_DIFF(c.First_Rental_Date, r.Acquisition_Date)) as Max_Gap from Cassette_Table c join (select Cassette_id, Min(Rental_Date) as First_Rental_Date from Rental_Table group by Cassette_ID) r on r.Cassette_ID = c.Cassette_ID If we want to find out which cassette that is, we put it all together and create a list of cassettes and their gaps, find which gap matches the biggest gap. Multiple cassettes could have the max gap. .... select * from ( select c.Cassette_id, DATE_DIFF(c.First_Rental_Date, r.Acquisition_Date) as Gap from Cassette_Table c join (select Cassette_id, Min(Rental_Date) as First_Rental_Date from Rental_Table group by Cassette_ID) r on r.Cassette_ID = c.Cassette_ID) where Gap = (select Max(DATE_DIFF(c.First_Rental_Date, r.Acquisition_Date)) as Max_Gap from Cassette_Table c join (select Cassette_id, Min(Rental_Date) as First_Rental_Date from Rental_Table group by Cassette_ID) r on r.Cassette_ID = c.Cassette_ID) There are more efficient ways to write this last query (using analytics) but this functionality isn't available in all DBs e.g. In Oracle: select Cassette_id, Gap from ( select cr.*, rank() over (order by cr.GAP desc) as RNK from ( select c.Cassette_id, DATE_DIFF(c.First_Rental_Date, r.Acquisition_Date) as Gap from Cassette_Table c join (select Cassette_id, Min(Rental_Date) as First_Rental_Date from Rental_Table group by Cassette_ID) r on r.Cassette_ID = c.Cassette_ID) cr) where RNK = 1 Hope this helps! 
I will have to look into dynamic trigger creation. I've been told by so many people that what I'm wanting to accomplish is impossible, at almost every step I've had. It's almost done, I just have to get over this last hurdle of creating them. Worked on it a tiny bit this week. I created a stored procedure that contains my create trigger statements, and another trigger that runs on drop_table to execute the procedure.
do you want their starting AgeRange or their ending AgeRange?
The data has the age range already in the table so their range is already there. I just need to select the value that is in the table for the last semester that they took a class during that fiscal year
So its fanning on the Year or DateRange? From my original understanding, here is one attempt: Select COUNT(distinct StuID), AR.AgeRange, FiscalYear from tblStatic S inner join (select StuId, MIN(AgeRange) as AgeRange from tblStatic S group by StuId) AR on S.StuId=AR.StuId where Campus like 'World%' and StuMaj = 'LAWSC' and ( enrl_act like 'REG%' or enrl_act like 'SCH%' ) and FiscalYear IN ('09/10', '10/11', '11/12') group by FiscalYear, AR.AgeRange order by FiscalYear, AR.AgeRange
If you wanted to switch it to see what their max age range, just switch the join MAX, rather than MIN
You could also use the No Value function: NVL([ColumnName],'')
Awesome, after talking through some private messages, and these comments I found a solution. it ended up being multiple nested iif statements as access didn't allow case statements. Thanks again for the help
Sweet! You guys rock.
Sounds like its a table of the salary paid to a number of employees over time. If so, then figure out the sum total per employee.
This is correct. Also you can specify a field in place of *. You may not need that for this problem, but it's a good thing to remember in case you want to target a column, like COUNT(DISTINCT lastName) as a bad example. 
Does Access 2007 have views? I think they're called reports or something. 
Ah, yes! Thank you. However, do I still use the built-in sum function to retrieve the sum of the queried column?
The USERGROUPUSERID and USERGROUPID are declared earlier as 
Thanks for this!
Why cast preferred to bit? &gt; ORDER BY Favorite DESC, Preferred DESC, Name That should do it for ya.
Yeah I tried that, no luck. I only put it in the post here for accuracy from the existing SP.
I don't know anything about the Oracle BULKCOLLECT functionality, but you don't really need it - just use a temporary table: CREATE TABLE #to_be_deleted(user_group_user_id, user_group_id); insert #to_be_deleted(user_group_user_id, user_group_id) SELECT user_group_user_id, user_group_id FROM user_group_user_tab WHERE user_id = USERID; DELETE from user_group_user_tab WHERE user_group_user_id in (SELECT user_group_user_id from #to_be_deleted) DELETE from user_group_tab WHERE user_group_id in (SELECT user_group_id from #to_be_deleted) DROP TABLE #to_be_deleted However, there's a BIG caution here - make sure those deletes are as specific as you want them to be. For example, let's say userid1 has user_group_user_id 12 and user_group_id 30. Do you want to delete EVERY record from user_group_user_tab for user_group_user_id 12? Same deal with user_group_tab and every record for user_group_id 30. 
 Try looking at the output of select cast(preferred as bit) from table
okay I'll try, thanks
You were on the right track. The source text file was encoded in Latin-1 Windows charset. Which is apparently quite different from Latin-1 Mac and UTF-8. Lots to learn :) Thanks for the inspiration.
I believe this is in fact the right answer. When you cast [preferred] as a bit, your only possible resulting values are 0 for any 0 value of [preferred] and 1 for any non-zero (including negative) value. The only sorting you'll get on that column is: (all the 0 values, if any, in random order) then (all the 1 values in random order). You get two random piles at best. What range of actual values are in [preferred]?
I removed the bit thing from the equation. The Favorite column is now int. Possible values are 0 and 2 for favorite. The same thing is happening. As long as it is first in the sort order its fine. When it is put second it gives seemingly random results. This is not the case with the favorite field, it does sort correctly in any position.
Thank you, that is very helpful. :f7u12:
Common MySQL queries: http://www.artfulsoftware.com/infotree/queries.php A Visual Explanation of Joins: http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html Library of Free Data Models: http://www.databaseanswers.org/data_models/ Hidden Features of MySQL: http://stackoverflow.com/questions/368858/hidden-features-of-mysql (if you don't use mysql, there are similar questions on the same site for different dbs) WWW SQL Designer for building schemas online: http://ondras.zarovi.cz/sql/demo/?keyword=default SQL Fiddle for testing SQL online: http://sqlfiddle.com/ good luck 
Hopefully you have already solved this but I've run into a similar issue before with MySQL. A client of ours had software in MySQL 4 I believe. We had to go into the My.CNF file and change the max packet size. http://www.jaguarpc.com/forums/vps-dedicated/17657-noob-question-set-max_allowed_packet.html 
Sqlzoo.net is a good one as well
Cursor variables (cv) are pointers to result sets, so you can effectively pass these cvs around to other places in a very efficient manner without copying the whole result set. They can also can be opened for different queries within a single program execution. Your procedure should pass c_dailyStaffingSheet as an "in out" parameter to really make use of it, otherwise the cursor is opened but the cv goes out of scope on returning from it. [More information about cv from a superb book here](http://docstore.mik.ua/orelly/oracle/prog2/ch06_12.htm#SQL2-CH-6-SECT-12.6) 
Yep, I eventually ended up solving it that way, thanks!
Thanks... I did end up figuring it out using subqueries. There was a 1:many relationship in there that I had to split out into two separate 'select distinct' subqueries and then pull the relevant data from those. Thanks!
Yep. There was a 1:many relationship in there that I had to split out into two separate 'select distinct' subqueries and then pull the relevant data from those. Thanks!
Why not make a small table with these fiscal dates? You could then use a case(when getdate() between fiscaltable.start and fiscaltable.end then DATENAME(month, fiscaltable.end) else 'something went horribly wrong' end). I use MSSQL myself, so i hope this will help you at least a bit.
Well then its going to be a LOOOOONG case: case WHEN getdate() between 1/21/2012 and 2/22/2012 THEN 'february' WHEN getdate() between 2/23/2012 and 3/21/2012 THEN 'march' ect ELSE Default END I can't think of something else right now, but maybe there is a SQL guru here that will amaze you with some magical function.
What are you doing with these dates? Lets just say you had write access to the DB, would a table of these dates solve your problem? How would that table look? Do you need to JOIN on the dates? If you can answer this I might be able to make some suggestions.
Well, this is the code the dates are being used in: AND W.WFINDATE BETWEEN DATE '2012-02-22' AND DATE '2012-02-29' I've actually had some pretty creative suggestions from another forum, including this: case when to_number (to_char (sysdate, 'DD')) &lt; 23 then to_date (to_char (trunc (add_months (sysdate, -1), 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') else to_date (to_char (trunc (sysdate, 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') end as begin_date_range, case when to_number (to_char (sysdate, 'DD')) &lt; 23 then to_date (to_char (trunc (sysdate, 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') else to_date (to_char (trunc (add_months (sysdate, 1), 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') end as end_date_range Which does give me the results I'm looking for, but I'll have to admit, I'm not quite sure how to integrate it into my WHERE clause, unless it's a sub-SELECT from dual? I'm also concerned with how this behaves when I have a Fiscal month that spans year-end (For example, Fiscal January of 2012 has the dates 12.22.2011 - 01.21.2012).
This is great! The work is done. You just need to put that into a WITH clause. If you don't know the WITH clause it basically creates a temporary table that lasts for as long as the query is running. You shouldn't need special permission to use it. WITH fiscal_dates AS ( SELECT case when to_number (to_char (sysdate, 'DD')) &lt; 23 then to_date (to_char (trunc (add_months (sysdate, -1), 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') else to_date (to_char (trunc (sysdate, 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') end as begin_date_range, case when to_number (to_char (sysdate, 'DD')) &lt; 23 then to_date (to_char (trunc (sysdate, 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') else to_date (to_char (trunc (add_months (sysdate, 1), 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') END AS end_date_range FROM DUAL) SELECT * FROM some_table WHERE some_table.date_value = (SELECT BEGIN_DATE_RANGE FROM fiscal_dates); Also, I checked and it seems to work just fine when the fiscal date range spans a new year.
Any particular reason to use WITH over WHERE with a sub-SELECT? This is the type of context I'm using it in: SELECT ACCTCORP, WJOB, COUNT(*) FROM IDST_WIP WHERE WFINDATE BETWEEN (SELECT CASE WHEN TO_NUMBER(TO_CHAR(SYSDATE, 'DD')) &lt; 23 THEN TO_DATE(TO_CHAR(TRUNC(ADD_MONTHS(SYSDATE, -1), 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') ELSE TO_DATE(TO_CHAR(TRUNC(SYSDATE, 'MM'), 'YYYYMM') || '22', 'YYYYMMDD') END AS FISCALSTART FROM DUAL) AND (SELECT CASE WHEN TO_NUMBER(TO_CHAR(SYSDATE, 'DD')) &lt; 23 THEN TO_DATE(TO_CHAR(TRUNC(SYSDATE, 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') ELSE TO_DATE(TO_CHAR(TRUNC(ADD_MONTHS(SYSDATE, 1), 'MM'), 'YYYYMM') || '21', 'YYYYMMDD') END AS FISCALEND FROM DUAL) AND ACCTCORP = 09579 GROUP BY ACCTCORP, WJOB And yes, after I posted that I figured out how to run test cases against it, and it works perfectly for Fiscal Jan (which spans the previous year and the current year) and it calculates the correct date on the 22nd and starts the new range on the 23rd.
Thanks for the great insight :)
Yeah, if I needed historical data, that would be perfect, unfortunately this particular table only holds the last 90 days worth of data - which, don't get me wrong, is still an enormous amount of data (millions and millions of rows). We may possibly be constructing an internal data warehouse replacement; I'll definitely keep your suggestion in mind if/when that occurs as I could see that being very useful for historical purposes.
Can you please rephrase the question? I don't understand what you're asking. Are the results different from what you expected to see?
The case statements are replacing the CustomerName and OrderID columns where the grouping values are 1. You have 2 case statements because you are replacing values in 2 different columns, and they both have the same criteria to keep it looking clean. Otherwise you might have something like: "Jacob Customer Total 792.50" instead of " Customer Total 792.50" An alternate version of this would be: select case when grouping(CustomerName) = 1 then 'Grand Total' when grouping(OrderID) = 1 then 'Customer Total' else CustomerName end as CustomerName, case when grouping(OrderID) = 1 then NULL else OrderID end as OrderID, sum(quantity*pricepercase) as Amount from orders group by CustomerName, OrderID with rollup CustomerName OrderID Amount ------------ ------- ------ Jacob 1 305.00 Jacob 2 487.50 Customer Total 792.50 Mike 1 119.00 Customer Total 119.00 Grand Total 911.50 This puts your total titles all in the first column, instead of distributing them between the first two. (Edited for formatting)
That is very insightful. Thank you for such a clear explanation.
Which database platform are you using? The solution depends on it! By the way, "case" is a reserved word in SQL, so calling a table name that is a bad idea.
If you're using MySQL you want to use group_concat(description) and group by all the case attributes. If SQL server, you should implement a clr or xmlpath solution (recommend you google group_concat for sqlserver) Not sure about PostgreSQL or oracle or firebird
Looks like this would work, but it doesn't look like he wants nulls to be returned. He also is using DBTester and VersionID as parameters. From a quick google search, NZ() is the equivalent to T-SQL's ISNULL() or ANSI COALESCE(). I don't know how you do parameters in Access SQL, but it should be similar. select t1.PropID, t1.Version, t1.Client, nz(t2.Item, '') as Fruit, nz(t3.Item, '') as Car, nz(t4.Item, '') as Color, nz(t5.Item, '') as Size from Table1 t1 left outer join Table2 t2 on t2.PropID = t1.PropID and t2.VersionID = t1.VersionID and t2.CatID = t1.Fruit and t2.Type = 1 left outer join Table2 t3 on t3.PropID = t1.PropID and t3.VersionID = t1.VersionID and t3.CatID = t1.Car and t3.Type = 2 left outer join Table2 t4 on t4.PropID = t1.PropID and t4.VersionID = t1.VersionID and t4.CatID = t1.Color and t4.Type = 3 left outer join Table2 t5 on t5.PropID = t1.PropID and t5.VersionID = t1.VersionID and t5.CatID = t1.Size and t5.Type = 4 where t1.dbtester = @dbtester and t1.versionid = @versionid
I'm on my phone but google - explainextended unpivot or explainextended transpose rows. That guy is amazing. 
MSSQL (2008r2) these are all faux column names used for illustration.
I think what you want is an aggregate function to concatenate "description". I know that in 2008r2 you can build your own (user-defined) aggregate functions, and as there is no built-in CONCATENATE (that I know of), this might be your best route. In which case, your SQL would look like: select name, caseid, date, CONCATENATE(description) from case c left join descriptions d on c.caseid = d.caseid where date between @var1 and @var2 and clientid = @var3 group by caseid, name, date There are other, much clunkier, ways to do this (I know, because I've had to do something similar in SQL 2000, which doesn't allow you to build your own aggregates....) -- so if you're looking for another way, let me know :) 
I couldn't discern if he wanted nulls or empty strings, so assumed nulls.
sorry, I'm pretty new to reddit and this formatting is alien to me :/
The equivalent for Oracle would be listagg(), probably one of the most useful functions I've ever learned.
Been a long time since I've done mySQL but in sql server you can select uid, COUNT (DISTINCT CLASS) from table_1 where class in ('ENG','MATCH','SCI','CHEM') GROUP BY UID having COUNT (DISTINCT CLASS) = 4
I think a CASE statement might help you do this in a single query. I brewed up a test version that is untested but it should give you the idea. I may have misunderstood what you wanted but this should at least get you on the right track I think! http://pastebin.com/VM1583PH
I had tried several different CASE combinations and I just tried yours, but on an account that I knew for sure should return 0 when this works, it still returns 1. I think the problem is that in a wash situation (removing &amp; adding the same product in the same transaction) there are two rows - one for each product code (since it has to be a different code for this to be an issue) - technically the CASE statement qualifies as TRUE for either condition.
This is hard to think on without a test table to run on but how about changing the query to something like... http://pastebin.com/RevPFyrc I can't tell from the names which one is the product code so I made a guess that it was ACCTCORP. You might need to change that around, along with the table it's selecting from, but this might help? It's hard for me to wrap my head around this completely without being able to execute tests but maybe I'll give you some inspiration!
Thanks for the kind words. I am curious myself what would end up costing less considering its a million row table. Honestly speaking I think your multiple exist clause is the most efficient. 
Try this... select ID.UID from IDS ID where 4 = (select count(distinct CLASS) from ITEMS I1 where I1.UID = ID.UID and I1.CLASS in ('ENG','MATH','SCI','CHEM'))
What database engine are you using for this? I've not run into a syntax where "ROLLBACK COMMIT" is valid, usually they are distinct statements and cannot be combined. That said different database engines handle transactions differently and there are options within many database engines to change how transactions are handled. For example MS SQL Server has an option (the default I believe) to auto-commit transactions. When this option is enable ROLLBACK is ignored.
It's funny I had just had that same thought and typed this out just before reading your comment :) : Row ID | User ID | Title ID | Sort # I'm definitely going to rearrange my structure tomorrow based off of this new layout. Wasn't sure if I was going in the right direction but after seeing this I know I am. The only problem I need to figure out is if a user deletes, let's say the title in position 3, to move 4+ down 1 but I don't think that should be too hard. Also I don't know if I need the Row ID as a key reference, but I guess it doesn't hurt just in case? Definitely giving this a try. Thanks!
Why didn't you use Access' remove duplicates?
This exists? I didn't know :( :( :(
Access has always had difficulty with these kind of queries. Even if the query is perfectly written and makes sense in SQL, you can still get odd results.. If it runs at all. Later Access versions may have gotten better, but I've just assumed the worst and kept with subqueries. Subqueries - Write an initial query to narrow down your first set of results, then write another query with the previous as data source. Continue on until you get exactly what you want. After you get a feel for Access' SQL nuances, you can cut the whole thing down to 2-3 queries to find practically anything.
Thanks guys, I tried the sub-query thing but Access was being a pain about what I could do with it. I'll give the SQL a shot, and if I can't get that to work I'll try some more sub-query techniques.
 Subqueries are generally used to perform "some other lookup" that the parent query can use. A subquery might return nothing at all (an empty result set), a single value, a list of values in a single column, or an entire result set. The type of result your subquery creates will have an impact on when you can or cannot use it. Some common usage patterns are: 1) Lookup a single value to use as a selection criteria. The subquery might use aggregate functions like MAX(), MIN() or COUNT() to ensure that only a single value is returned. Example: Select all of the WarrantyClaim details for the most recent Warranty Claim Record created for customer "X": &gt; SELECT * FROM WarrantyClaims WHERE ClaimID = (SELECT MAX(ClaimID) FROM WarrantyClaims WHERE CustomerID = 'X') 2) Lookup a set of values to operate against, often using the "IN" or "NOT IN" operators. Using IN or NOT IN with subqueries requires that your sub-query results contain only a single column of data. Example: Select the Item Master details for all parts that have at least 5 Warranty Claims on them. (This is a contrived example that can also be solved with a JOIN) &gt; SELECT * FROM ItemMaster WHERE ItemNumber IN (SELECT ItemNumber FROM WarrantyClaims GROUP BY ItemNumber HAVING COUNT(*) &gt; 5) 3) Grab a full blown result set (sometimes called a "table valued expression") that can be used with a JOIN as if it were a table or view. This is probably the most complex way of using sub-queries. Example: Select all Item Master details and include summarized information about how many Warranty Claims each part appears on. (Like the last contrived example, this can also be solved without the use of subqueries) &gt; SELECT * FROM ItemMaster IM LEFT JOIN (SELECT ItemNumber, COUNT(*) as NumberOfClaims, SUM(ClaimTotal) as TotalWarrantyCost FROM WarrantyClaims GROUP BY ItemNumber) WT ON IM.ItemNumber = WT.ItemNumber There's a lot going on in this last example. The subquery is summarizing a bunch of WarrantyClaims data, grouped by ItemNumber. Notice that I've added an alias name for each column in the subquery. In the JOIN statement, I've also added an alias name for the ItemMaster table ("IM") and I created an alias for the subquery called "WT". The join is then established as if "WT" were a table or view, even though it is actually just a sub-query. If you really like where this whole subquery thing is headed, then you should look forward to learning about "Common Table Expressions", or "CTEs" for short. EDIT: That's a long-ass explanation. Might be a little overkill for Reddit, but what can I say? I love databases :)
"WHERE" clause filters out data a single row at a time. The row either meets all the criteria and so it is included in the query, or it fails the test and gets excluded. "HAVING" comes into play once you begin using aggregate functions like MAX(), MIN(), COUNT(), etc. Aggregate functions evaluate multiple rows of data (every row that has passed the "WHERE" clause) and then returns some summary information about all of the rows evaluated. "HAVING" is your WHERE clause for aggregate data. If a single row does not pass the "WHERE" clause, it will not be evaluated by the aggregate functions. But what if you want to filter out some records based on the aggregate values? Well that's where "HAVING" comes into play. Example query: &gt; SELECT ItemNumber, COUNT(\*) FROM WarrantyClaims GROUP BY ItemNumber HAVING COUNT(\*) &gt; 5 I only want to see Item Numbers that have more than 5 records in the Warranty Claims table. A WHERE clause won't cut it, because that only operates against one row at a time. In this case, we are trying to filter based on the aggregate data, so we use the HAVING clause. EDIT: Formatting
Thank you---and everyone else as well. You guys have given me a bit of a more clear view on the subject, and I appreciate it!
He might not have the disk space to clone the data to the new table...
I ended up going with a NOT EXISTS statement, and it works perfectly. &gt;Your innumerable RCTR* checks are ugly, I do a reverse "in" predicate, which evaluates exactly the same, but is far more readable. It may have been ugly, but your solution only allows for one possible value, whereas mine allowed me to do an IN to allow for more than one value. Just because something's ugly, doesn't mean it doesn't have a purpose.
Errr...your situation only wanted one value...
[Here's a list of DB modelling tools](http://www.databaseanswers.org/modelling_tools.htm) In my opinion, the best on the market is Sybase Powerdesigner (Data Architect), but it's also probably the most expensive. [SQL Power Architect](http://www.sqlpower.ca/page/architect) is free and supposedly very good.
If your database isn't too large and you are working from a single session (this can be emulated of course, using a proxy), you can just do everything in one big transaction and do savepoints after each query. Your database is now versioned :)
You can get the [PostgreSQL Introduction and Concepts book](http://www.cs.earlham.edu/~millebe/CS65/aw_pgsql_book.pdf) (2.5 MB) for free. Note that it is 12 years old.
Reposted in /r/Database
Now you are thinking with versions.
The best way to understand this is by examples... 1. One example of recursion CTE is for traversing hierarchical data. As a simple example, you may have a table which stores employee information and part of each employee's record is a FK link to their boss's employee ID. Each boss themselves is an employee and may themselves have a boss. So say you wanted to traverse the boss structure to show the hierarchy (like below) you'd use a recursive CTE. Feeding the boss back into the loop to find their boss etc. Like below: Jo Bloggs (Director) Jane Roberts (Human Resource Manager) John Smith (Operations Manager) Claire Dunn (Operations Team Leader) Charlie Trent (Operations Assistant) Jane Roberts (Operations PA) 2. Row_number() is an analytic function which returns the ordering number based on an order / partition clause. The partition tells the analytic function the group over which the analytic function will be applied. So sort of like when to restart the analytic function (in this case the row_number count). Say I have a list of employees such that... SQL&gt; select * from EMPLOYEE 2 / ID FIRST_NAME LAST_NAME START_DAT END_DATE SALARY CITY DESCRIPTION ---- ---------- ---------- --------- --------- ---------- ---------- --------------- 01 Jason Martin 25-JUL-96 25-JUL-06 1234.56 Toronto Programmer 02 Alison Mathews 21-MAR-76 21-FEB-86 6661.78 Vancouver Tester 03 James Smith 12-DEC-78 15-MAR-90 6544.78 Vancouver Tester 04 Celia Rice 24-OCT-82 21-APR-99 2344.78 Vancouver Manager 05 Robert Black 15-JAN-84 08-AUG-98 2334.78 Vancouver Tester 06 Linda Green 30-JUL-87 04-JAN-96 4322.78 New York Tester 07 David Larry 31-DEC-90 12-FEB-98 7897.78 New York Manager 08 James Cat 17-SEP-96 15-APR-02 1232.78 Vancouver Tester 8 rows selected. And I'm asked to generate a report list of employees, order the list by salary, into a pay level according to each job description, then I can do that with an order by / partition clause. SQL&gt; select FIRST_NAME, SALARY, 2 row_number() over (partition by DESCRIPTION order by SALARY) as PAY_LEVEL 3 from EMPLOYEE 4 / FIRST_NAME SALARY PAY_LEVEL ---------- ---------- ---------- Jason 1234.56 1 David 7897.78 1 Celia 2344.78 2 Alison 6661.78 1 James 6544.78 2 Linda 4322.78 3 Robert 2334.78 4 James 1232.78 5 
It's an encoding issue. Make sure that you convert to the standard char set when importing.
Many Thanks
Can you give us the error you're getting back from the database? Are you sure that "MyConn" contains the right value? Are you sure that the VB application has privileges to access the file referenced bt "MyConn"?
I don't know what your actual table looks like, and the data type definitions may need a little tweaking, but something like this might work: SELECT AppCode = s2.col.value('@ApplicationCode[1]','varchar(40)') , NumberOfSeats = s2.col.value('@NumberOfSeats[1]','int') , NumberOfTransactions = s2.col.value('@NumberOfTransactions[1]','int') , LicenceByToken = s2.col.value('@LicenceByToken[1]','bit') FROM sessions CROSS APPLY serializedobjects.nodes('/ApplicationLicence') as s2(col)
I'm getting Runtime Error '-2147467259 (800040005)' method 'execute' of object '_connection' failed I started off with a simple SQL statement and it ran fine. It broke when I created the full statement. I have a debug line in there that prints the SQL string. When I copy and paste that into Access it returns the correct result.
Perhaps this will work for you in Terminal: mysqladmin -u user -p &lt; ~/Desktop/PE5.sql NB: I don't use Macs on a regular basis.
I've also tried this way with no luck.
thats correct...excel and access 2010. i believe the problem is with using the datediff and dateadd functions within the sql statement. VB is parsing that string and breaking when it hits those reserved words. Is there any way to comment those out so VB doesnt parse them? 
I'm not positive this is your issue but. I have a coworker that just finished a Access/Excel 2010 application and had some problems with the connectivity b/t the two. He rolled back to 2007 and it was fine. So, again not that much help but it may be worth looking into. 
No, I'm basically saying if the serialized object is XML, then go use [XML functions](http://msdn.microsoft.com/en-us/library/ms190798.aspx). If your serialized object is some other data format than XML, then you're hosed because you don't have any database support for reading / querying into the some other data format. And if you don't understand why, as a general rule, storing serialized data in a relational database is like putting a square peg in a round hole, then I guess you can just stay mad at the internets for not having very helpful answers.
If you just want to open it, use whatever is the mac equivalent of notepad?
If you want to stick with the GUI query interface save the sub-query as a stand alone query in Access, then reference it in a second query. (/r/msaccess).
fyi /r/msaccess
Couldn't you just rename your primary key column? Wouldn't the references get updated automatically? If not I would do it by first adding a new column that should be the new primary key in the referenced table, then add an identical column for the referencing table. Within a transaction I would then : Fill these columns up with the new values. Then go ahead and disabling the foreign key constraint in the referencing table followed up by creating a new reference using the new columns. Rollback whenever there are any errors.
I would guess that your CSV is not properly formatted. Newlines and commas are valid to use in a CSV file, but they have to be wrapped inside "double quoted string values, like this." 
Don't forget to double-double quote any literal double quotes. That would make my last sentence look like this: "Newlines and commas are valid to use in a CSV file, but they have to be wrapped inside ""double quoted string values, like this.""" See what I did there?
Are you new to SQL Server? The majority of these system views have been there since 2005, even 2000 had the system tables that were similar but nowhere near as comprehensive.
I found awesome posts by this Singaporean guy :)
This is my professors table. I am going to try and work with him on fixing it. I spoken to a lot of people about this and it seems as though he needs to work on his schema
This is still using a manual trigger, but you can write your UPDATE trigger as something like: IF NOT UPDATE(UpdateTimeStamp) UPDATE dbo.[dimtable] SET UpdateTimeStamp = GETDATE() , UpdateUser = USER_NAME(USER_ID()) WHERE [idcolumn] IN (SELECT [idcolumn] FROM inserted) [This page](http://sqlserver2000.databases.aspfaq.com/how-do-i-audit-changes-to-sql-server-data.html) might help...
Slightly offtopic but... dynamic SQL runs in the context of the user executing it. So, if you revoked permissions off the table, and did all data access and modification via dbo. stored procedures, the trigger would fail as the dynamic sql portion runs in the context of the user not in the context of the stored procedure owner dbo. This is assuming the owner of the table is dbo.
I'm guessing you're referring to the Database Diagrams feature of SQL Server. These indeed show relationships between tables (a foreign key relationship) using a line between tables, with a yellow key on the side of the referenced table, and an infinity symbol (∞). This means that there can be one key value on the referenced side, with an unlimited/infinite (in theory, but in reality we're constrained by disk limits) number of rows on the table that has the foreign key constraint. If you had the following two tables (with bad pseudocode): Department (DepartmentID, DepartmentName) Employee (EmployeeID, Name, DepartmentID FOREIGN KEY REFERENCES Department(DepartmentID) ) The visualised foreign key relationship would have the key on the Department (each Employee belongs to a single Department), and the ∞ on the Employee (each Department can have an unlimited number of Employees). The SQL Server diagrams aren't as detailed as a proper ER diagram - for example, they don't show ~~1-to-1 relationships, or~~ 1-to-10 relationships, only 1-to-many or 1-to-1. They're not bad for a general idea of what the relationships are, but I'd recommend a proper modelling tool. * Edit - thanks Gamic!
They will show one to one relationships if you key the tables properly. If it's one to one, the key on both tables should be the same, and one table will have a foreign key reference to the other. Both sides of the link will be shown with the key symbol. 
Ah, thinking closer, I do believe you're correct. Thanks for the clarification.
I've seen schemas this screwed up in the real world (cough Salesforce cough). The thing is, just working from this schema you can't really answer the problems you've been given, or you can, I suppose, but you can't really have confidence in what you've been given. If this is an actual schema there's some code on the front end that is doing things in a certain way with specific enumerations and you need to actually go through the front end code to be confident that your sql is correct, and you're going to have some pretty ugly sql code to write. 
It does have a numbers table DECLARE @tblNumber TABLE ( Number INT ) WHILE @Counter &lt; LEN(@CharacterString) BEGIN INSERT INTO @tblNumber VALUES ( @Counter ) SET @Counter = @Counter + 1 END And references using master.dbo.spt_values FROM ( SELECT (N.number + 1) AS Number FROM [master].dbo.spt_values N WHERE N.[type] = 'P' ) N
No problem. I really do appreciate the feedback. I've been meaning to post some of the code that I have written for MS SQL and figured I could just start a quick blog. I noticed [/r/SQL](/r/SQL) and figured what the hell.
I'm frustrated by the fact that he starts the article by saying this somehow relates to interviews. There is very little substance in the post, it's essentially a copy/paste from MSDN, so it ends up being frustrating to read. For the interview's sake, I'd rather ask: what are the pros/cons of each? Should Developers actually care about all of the terms, or are some for Architects and Visio only? 
The second example is the proper syntax. If you run both, you'll notice that you get a No Join Predicate warning on the first execution plan. Please, for the sake of the DBAs, use proper ANSI join syntax :)
And you’ll be forced to write the JOIN clause if you need to switch to an OUTER or FULL join. Best to go with writing out the JOIN, as the others have stated. Creating a table alias (i.e., AS clause) is mandatory of the columns are the same. The DBMS won’t know how to differentiate between without it. But, renaming columns or tables can also be beneficial if names are cryptic. For example, I work on lots of tables that have carried on the legacy limits on name sizes (from way back in the day). Often come across as awful abbreviations. Using the AS clause gives me the ability to rename them as I choose so it reads better.
Thank you very much :) how do you like it? I switched from a CS major to an ISY major (network admin concentration) and was wondering if a DBA job would be as stressful as say a Java developer 
It can be. You do get your in the middle of night calls if something goes south (especially if you have a large quantity of overnight batch work at the organization). Really depends on how well your shop is run (as most jobs do). I got lucky and found a niche doing performance work between apps and the databases. Started out as a programmer and shifted over to DBA focused work when they needed someone who could dig into both sides of the equation. Seems the rift between programmers and DBAs is widening (at least in our shop), and someone who can follow both graph ~~ad~~ and set theory are good mediators. 
Thanks for the information. What is the t1ID/t2ID used for?
Ah. Those are column aliases, which I've only found useful as column headers in query results. There might be other places column aliases are useful, but I haven't yet found the need.
Let's start with the easy stuff: What error message do you get?
Never used reporting services. But if there are "expressions" then that is exactly what you need. The expression should be: QTY * PRICE 
I'm not familiar with Oracle security. But I think that you are going to need the ability to select/insert/update/delete (as appropriate) for each of the tables or databases you need to operate on. I also think you will need the ability to create objects (functions, triggers, storedprocs) in order to implement anything. Finally, the ability to look at view definitions would probably be very beneficial. (This is what I would grant to a new developer on MS SQL)
It's a dupe! This was posted about 4-6 weeks ago. It's also rather redundant.
Important to the database: Primary Key, Foreign Key, Unique Key Important concepts you should consider when designing databases: Natural key vs surrogate key. The rest of the article is mostly fluff.
given most users are comfortable with excel, and your end output can be read in excel, I would just go with excel! you can build gui's in excel with vba as needed too!
You can do it via an expression as suggested above, but I still think it's easier to just do it directly in your SQL query.
&gt; 1-to-10 relationships What are these? Google yields nothing.
The 10 was just an example - you could put any number there, and it would be more properly called a [1-to-N](http://www.google.com.au/search?q=1+to+n+relationship) (1:N). It's more like a 1-to-many, except you're putting an upper bound on the "many". The SQL Server diagrams will show 1-to-many, but if you wanted to say "No more than 10 child rows for each parent row", the diagram won't give you sufficient information.
Oh, I was about to say. You probably shouldn't call it 1-10. How do you enforce a 1-N through a schema? Duplicate FKs?
 select * from job_run_log where job_name like 'RMAN%' and ( ( to_char(run_start_date, 'hh24mi') between '1000'and '1500' and to_char(run_end_date, 'hh24mi') between '1000' and '1500' and 2 between to_number(trim(to_char(run_start_date,'D'))) and to_number(trim(to_char(run_end_date,'D'))) ) or ( (run_end_date - run_start_date) &gt; 6 ) ) ; Although, to my mind this is looking for Monday, not Wednesday.
It returns Monday instead of Wednesday. If I were to run this query with the between time commented out, it returns Wednesdays. If I change it to a 4 it returns Wednesdays. Thanks for the help!
Thank you!
Thanks for your help. I tried your code on Access and it gave me a syntax error (missing operator) in this part of the code: [Regiões do Paraná].[Código] = [Técnicos e Engenheiros do Paraná].[Região] INNER JOIN [Empresas do Paraná (numeros)] ON [Regiões do Paraná].[Código] = [Empresas do Paraná (numeros)].[Região About joining on Código, I really don't understand this, because it is what the Access Assistant made up for me.
Thanks for your help, buuuuut... This is what happened: [screenshot:yourquery ](http://img252.imageshack.us/img252/2967/screenshot2012041314354.png) And these are the values, as you can see they differ: [screenshot:Técnicos e Engenheiros query](http://img198.imageshack.us/img198/5916/screenshot2012041314371.png) [screenshot:Empresas query](http://img638.imageshack.us/img638/6333/screenshot2012041314383.png) The code you made up brings up the same problem made by the Access Assistant. Any clues on this one?
Using Check constraints would work. Imagine the following pseudocode: CREATE TABLE ParentTable (ID int NOT NULL PRIMARY KEY, name varchar(20)) CREATE TABLE ChildTable (ParentID int NOT NULL FOREIGN KEY, ChildRowNum int NOT NULL) CHECK (ChildRowNum BETWEEN 1 AND 10) PRIMARY KEY (ParentID, ChildRowNum) You'd need to check the next possible value for ChildRowNum prior to inserting, but this ensures that you only have 1-10 rows per ParentID (and the ChildRowNum will have a value of 1-10), and you can't have duplicates due to the PK constraint. Alternatively, you could put the logic in the application, or by using triggers on the "many" table to confirm that each insert/update ensures that there's never more than N rows attached to one parent ID. The problem here is that the schema doesn't enforce the rules - triggers can be disabled, or rogue SQL statements can be written to insert more rows. Outside the box thinking - You could store the child rows inside the parent row as an XML value -XSD has the ability to easily use 1:N relationships. Performance and easy querying would be an issue if you needed more than 1:10 or so though. I wouldn't recommend this route though! 
/r/msaccess ? Have you tried it without "DistinctRow"?
you need to join the employee and project tables to the assignment table. what specifically are you having problems with? it should look something like select e.*, p.* from assignment a join employee e on a.blah = e.blah join project p on a.blah = p.blah 
This -- and you'll need to join in the Department table as well, to the Project table. But before we go any farther... do you understand conceptually what's going on here, OP? Throw out some questions if you don't, I'm sure we'd be happy to help.
Is it possible that your data in the old table have duplicates? So you could have something like ID Name Address Zip 1 John Doe Meow 12345 2 John Doe Meow 12345 3 Jane Doe Bark 53421 Maybe selecting distinct CustomerID isn't the best way to do this.
They aren't free but there are already databases of drug characteristics out there. We use [Medispan](http://www.medispan.com/index.aspx) in our medical records systems to alert physicians to drug interactions and such.
Alright, so let's define our first two entities: Drugs and SideEffects. Drugs has a primary key of DrugID, and SideEffects has a primary key of SideEffectID. Now, what's the relationship between those two tables? Let's review your requirements. For any drug, there can be multiple side effects. And for any side effect, there can be multiple drugs. That's called a Many-to-Many relationship. So we're going to need a third relationship table to map these two entities together. Let's call it MapDrugsSideEffects. And it's only going to have two columns: DrugID and SideEffectID. So then we can build a nice little query like so: select d.*, s.* from Drugs d join MapDrugsSideEffects m on d.DrugID = m.DrugID join SideEffects s on m.SideEffectID = s.SideEffectID You can then define your WHERE clause to taste, based on whether you want to specify a particular DrugID or a particular SideEffectID. Make sense?
Ahh thanks for the heads-up, but this is more of an educational project with a focus on specific classes of drugs rather than a full-on commercial project.
This is the way to go, and I'd like to point out how important it is that you understand the relationships between your entities before you start thinking about creating tables. Once you know your entity relationships, the tables will come naturally. The fact that a single side effect can belong to more than one drug is the reason you want to have a hierarchical design. 
What database are you using? SQL server? oracle? Can you paste in the code you are trying that is asking for right parens? Maybe you are putting the parens in the wrong place?
It is probably different dependent upon what RDBMS you're using, but for SQL Server the syntax would be something along the lines of DATEADD(DAY, 14, NextServiceDate).
Select MIN(HPTS), Team From BCS Is that what you're asking for? 
Heya, the easy way is to save that query you already made to get the min of field1, then make a 2nd query that includes the same base table plus the 1st query. Link the table's HPTS field to the min(HPTS) field in the 1st query and you're good to go.
Select MIN(HPTS), Team From BCS Group by Team
use this to get MIN(HPTS) :- Select top team, HPTS from BCS order by HPTS; Exp for my first answer : 1st query for MIN and 2nd query for MAX.
It sounds like you actually need to do an INSERT statement when you click your Update button. Something like this: strSQL = "INSERT INTO tblStudentRewards (UserID, Points) VALUES (" &amp; intCurrentUserID &amp; ", " &amp; intPointsValue &amp; ")" As for the Add/Remove thing, I think that intPointsValue (the value from your Points drop-down) will just be positive for Add and negative for Remove. These are just my initial thoughts, and, without knowing more about your assignment, I can't be totally sure. Just out of curiosity, what DBMS are you using? I only ask because I notice your teacher's query has a *true* literal at the end. I'm not aware of which DBMS supports true/false literals (or boolean data types in general). I know for certain that Microsoft SQL Server does not. 
try to build query at the runtime based on your criteria and create an insert query to import data from your result set to the destination table. It might create a whole lots of coding work, huge mess in database server memory but it work. This kinda job isn't recommend to use it in regular basis but one have to use lousy ways in data migration phase.
Yeah that's how its setup anyway, I'm trying to translate from an object relational schema to an object oriented one. It’s more of a technical exercise and most ways that I’ve tried seem to be drawing up a blank. Using nested tables was the closest way of getting to an OO database with ought having relational tables.
Ah, so I've only ever written inserts for nested tables where the values where hardcoded rather than queried. In your shoes I would probably build a dynamic insert using a simple cursor loop in pl/sql.
Large penile system is good. Be the one to buy beers, send links around, right?Talk funny on fridays. you know? Your skill set is right on the money. Oh yeah, don't fly with drugs, even though you wanna be the cool guy at the seminars/conventions/dinners/sales pitches or lunches.
If you think his usage is incorrect, you must not know the meaning of the word.
Are you talking about when you restore a DB on another server and end up with orphaned accounts? If so, here's the fix for that: http://www.fileformat.info/tip/microsoft/sql_orphan_user.htm
Well here is what I've been told. A couple of weeks ago, he was making some changes to the permissions settings (I'm not aware of what changes were made, how they were made, etc.). After he made these changes, the nightly backups (we use BackupExec for SQL Backup) started to fail with an 'Access Denied' error message. When I saw that, I asked what changes were made. He doesn't really remember, and asked if I could research a way of restoring permissions from a previous backup set.
There is a backup operators role in SQL Server specifically for this purpose. Assign the account that BackupExec uses to the db_backupoperator group in each database (it is a database level thing). Alternatively if you don't care about fine grained security on this server you can assign the account to the SysAdmin role at the server level (note that this gives it admin privileges over the entire SQL instance and is in general a bad thing to do). Your boss has no business touching anything IT related if he approaches it with that laissez faire attitude. If he worked for me I'd write him up for something like this... 
Glad it worked out for you! Thanks for reporting back.
I believe that only returns the number of days between two dates. I'm trying to return the facilityID along with the column1 and column2, most recent date received included, as long as the date in either column1 or column2 is past 90 days. 
Up to date isn't terribly important which is why I sync only once a day. The query is using no locks. I think you pretty much answered my question. Thanks.
You might want to do a DATEADD(D,-90,GETDATE())
So .. what kind of help you need ?
Oh sorry I guess it wasn't that clear. Basically what I want to do is take the assignments table use it to populate a Jtable in my program, once someone selects a row in that table and clicks "Start Assignment" it will then go to the database get the table of questions/answers for that assignment, which will be put into a linked list. The problem is mostly trying to figure out how to set up my tables in such a way that I can get the individual rows of one table to point to their own individual tables. Hope that clears it up, seems like a stupid problem but for some reason having trouble wrapping my head around it.
I'd like to help if you are facing SQL-related problems ( because this is /r/SQL and I'd love to keep it this way). But according to your post and questions .. you are asking about how to design your database, If you want to know how to design your database and tables .. I think you should ask in /r/database. You should read something like this "http://en.wikipedia.org/wiki/Database_normalization"
You don't want each set of questions to be in its own table for starters. Ideally you'll have an assignments table and a questions table (and answers, grades, and students tables). Questions table has all your questions for every assignment with a FK back to the assignment it comes from (many-to-one questions to assignments). Answers table will have a FK back to questions (many answers to one question) and a FK to students so you know which student gave which answer to which question. And grades will either be directly in the answers (preferable it's a one-to-one so that you don't have to do the joins) or if you need lots of info about a grade for an answer then you can have an FK to the answers table from the grades table.
Just throwing out an idea: * tblAssignments: * AssignmentId [int] (PK) * AssignmentName [varchar(255)] * AssignmentTypeId [int] * DueDate [DateTime] * tblAssignmentTypes * AssignmentTypeId [int] (PK) * AssignmentTypeName [varchar(255)] * tblQuestions * QuestionId [int] (PK) * QuestionText [memo] * tblAssignmentQuestions * AssignmentId [int] * QuestionId [int] There will be Foreign Key constraints between: * tblAssignmentId.AssignmentTypeId -&gt; tblAssignmentTypes.AssignmentTypeId * tblAssignmentQuestions.AssignmentId -&gt; tblAssignments.AssignmentId * tblAssignmentQuestions.QuestionId -&gt; tblQuestions.QuestionId 
Thanks
Find some free data sets (the federal government has been releasing some for awhile) and load them up on your own. Then, figure out how to answer some questions that are interesting to you. If you find something interesting enough, it would be cool to publish it.
General's is your problem. Use: '3456 General''s Lane' 
Thank you so much!
 SELECT i.item_id, IF(COUNT(DISTINCT j.item_id) &gt; 0 , 'true', 'false') available FROM items i LEFT JOIN junction j ON i.item_id = j.item_id GROUP BY i.item_id ORDER BY i.item_id Its close to what you've got, but no nested Selects (which are pretty inefficient). Count will only count non-nulls.
You're next step is probably to log onto the report server directly and browse to the reports web site via 127.0.0.1/reportserver (i think that's the right application) login to the site under the user account, and see what the sql error text is. That, or in your web.config enable remote errors (If I recall correctly a text search for "remote" should get you to it in short order) and get the error text on her workstation. Just guessing, it's a permissions issue. Does the report run any stored procedures? Does it create any temp tables? Does it fire any triggers? Look for objects (tables, views, etc) which are touched and figure out her permissions versus yours. 
Like Sylver said, enable remote errors - you can do so via IIS, the web.config, or even Management Studio by connecting to a "Reporting Services" server rather than the normal database connection. Another option would be to log into the report server itself, and look at the log files; they'll be in your Reporting Services directory by default. They're typical Microsoft logs - piles of text that's almost physically painful to read, but if you know the exact time that an error occurred, it should be somewhat easy to find in the logs. Knowing nothing about what you're doing, my first 2 guesses would be: * 1 - How is authentication to your target database handled? Shared datasources with a login stored in the SQL Server? Or Integrated security (as in, the login you use to run the report is also the login used to connect to the database)? * 2 - Are there any formulas or functions in the report .rdl itself that makes use of the UserId (or UserName.. can't remember exactly) global variable? If so, have you tried validating that the functions, uh, function when her username is passed to them? If you can find the actual error text, and still don't know what to do, come back and post it here and I'm sure somebody can help you better :)
Look into COALESCE.
make sure you have the correct execute permission on any stored procedures being called for the dataset. 
 Effectively with replication what you run on the server you are connected to is later run on any other servers. That of course is the basic meaning. There are more complex replication setup's with mssql and other enterprise enviroments.
I will explain MS SQL Replication: You have a master db and a slave db on dofferent servers tipocally. The master is read/write to users. The slave is read only to users When you set up replication both dbs are perfectly in sync. schema and data, this is done automatically through a database rebuild script all objects and the data is scripted out of the master into files and scripted into the slave then replication is enabled. Can also be done with backup/restore. Both the master and the slave must have the same data and schema when you initiate replication. once they are the same you set up subscriptions and the slave db subscribes to the master db. what this means is any changes to the master db are sent down to the slave db, what this means is once the slave db has been created and are in sync, the sql statements that change the master are also applied at the slave. They have to be perfectly in sync to begin with. This is good for creating read only replicas of an insert oriented database. websites and reporting systems dont change data but will conflict with inserts and updates. So these are suitable systems to read data from a subscribers. sql server uses a model analogous to a magazine distributor. you have a publisher - server hosting many publications a publication - a database containing a set of articles / a magazine A subscriber - a slave server a subscription - a request for a set of articles to be.published into a subscription db A subacription db - a place where the publication articles are replicated to. you also have a distributor which manages the process of delivering to subscribers, this enables the publisher to deliver to one location and minimize the overhead of replication. You can enable hundreds of subcribers.
If the product ids were expected to be the same, you could outer join table b to table a. However in yr example they appear to be different?
Hmm. I'll give that a try when I go back to work tomorrow. I was trying to look for something that could "subtract" beginning letters from another column. Thanks for the help
Since you're using access you may have to use vba and loop through your dataset. Make a 2 character arrays and compare each character individualy the ones that don't match get appended to a string. Then after you've completed the lengths of both arrays insert the two string variables into the new table. Hope that helps.
Thank you very much for your detailed descritption. Can you answer me some further questions: 1. What happens or could be a problem when I'm running a stored procedure on the slave which uses temporary tables? 2. What happens if I'm running a stored procedure which inserts values in temporary tables as well as in persistent tables? 3. See question 2 but the sp I want to run is framed by "set replication on" and "set replication off" 
Can you guarantee hat he will not throw it back?
Are you really concerned about the individual character differences, or are you looking for a way to do approximate string matching? If the latter is the case, look into [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance) which is pretty popular and I'm sure you can find an implementation for it in VBA somewhere. If the former is the case then the task at hand will depend on your data. If all of your data takes the form &lt;product_name&gt; &lt;two letters&gt; then your task will be easy. But if you're trying to diff two string like "My Product A" and "AB Products" and get something back like "My s B" then you will have your work cut out for you.
if the table articles arent being replicated you wont have a problem. If the table articles are being replicated then there may be a possibility that you will end up with out of sync data, which breaks replication. Furthermore it depends if you are updating at the publisher Say the replicated value of column cost is 1 and you update it to 5 Publisher value of cost is 1 subscriber value of cost is 5 Then an update statement does something like cost = cost + 2 Publisher value of cost is 3 subscriber value of cost is 7 The purpose of replication is to have data in sync and correct. Therefore Data Modifications are always run at the publisher and replicated down to the subscribers. Therefore your stored procedure should read the value at the subscriber and modify the publisher. You could work around this by creating a second table at the subscriber but that becomes messy very quickly. The purpose of replication is they contain the same values. That is straightforward transactional replication. There are other models with sql server, not sure about sybase but rdbms systems offer similar functionality.
Update: Maybe it would be easier to start at the beginning. Maybe the Tables I ended up with made it harder than it is. (P.s. You guys have been very helpful and I've already learned some things even though they were not exactly helping ) **Breakdown:** * I have a list of 10,000+ products with unique product numbers. * Most of these comes in sets of three or four. for example: osiris a; osiris b; osiris c * * There is one piece of logic built into our system where if the product type B variant is sold out, it automatically goes to product type A variant. (this is not part of the database and shouldn't be, it is done by an external system) * * The main problem is that the field that drives the B to A transfer is manually entered. * * The Osiris B should always go into the Osiris A but if someone misskeyed, it might roll into Hercules VA or any other product. * * I thought it would be easy to run a query that listed all B products, then another query that ran the value from the field BtoA field and alligned it to the right of the B products. This is how I ended up with Example Table1 above. I thought it would be easy to eyeball it with columns next to each other, but there are too many records. * * My next train of though was to somehow subtract one column from the other so that I only see the differences. This would mean if the B to A was right, then I would only see the last digit most of the time. If it was wrong, the difference column would show the long string. Maybe there is an easier way if I modify it or run it differently from the beginning? I thought maybe I could use the InstrRev function inside the select statement but I am completely new to this function. [Instr Function](http://www.techonthenet.com/access/functions/string/instrrev.php) I tried this, but it is crude and doesn't work all the time do to some variation in the fields. &lt;&gt;Left([TABLE1].[PRODUCTNAME],(InStrRev([TABLE1].[PRODUCTNAME],"A")-1)) Maybe I could trim the right side of each string by a few letters then they would "ideally" match perfectly and I could use a not-equal-to? Anyways. Thanks again for all the help. This is a great learning experience and I have really enjoyed this subreddit!
You arent being entirely clear in your specification which is why you are having problems writing this You need to work out a flow and try to be clearer then you can code against that 
loop through datasets is not great advice. If one column contains 1000 rows and a comparison column contains one million that is one billion comparisons.
Yeah, I just did that actually. :)
FYI /r/MSAccess
Oh, didn't know about this, i checked the sidebar! Thanks
Levenshtein Distances (and all similar distances) are not what OP is looking for. That would return an integer representation of how close the two strings are to eachother, not their actual character differences. Depending on the actual expected result, the algorithm could be close enough to impossible its not worth implementing in SQL. UPDATED after mason55's comment (before I had seen it). The examples that were given in the question were: difference("Jupiter VA","Jupiter VB") = "B" difference("Jupiter VB","Jupiter VA") = "A" difference("Orion BA","Ares VB") = "Orion BA" difference("Orion BA","Ares VB") = "Ares VB" But its really easy to find some edge cases and poke holes in those expectations. The desired algorithm is too non-deterministic, for example: * The 2nd letter in both Orion BA and Ares VB is an r, why does it show up in the difference? * How is the difference meaningful if the two texts are different in multiple spots? * example: difference("affect BA", "effect BB") = "e_______B" (where _ is a space) * What about strings of different length? what is the important part to show? * example: is difference("aaB","aaaC") = "aC", "\_aC","a\_C","aaC", "a\_\_C" or "C" - because I can think of non-trival methods to arrive at all of those outcomes. I think the closest thing to what OP wants is a greedy algorithm that just takes the first connected set of differences starting from the end of the string, maybe have a one letter look-ahead that allows the occasional similarity as long as it is alone.
&gt; That would return an integer representation of how close the two strings are to eachother, not their actual character differences. It's trivial to calculate the difference instead of the score based on that algorithm. But yes, actually implementing it in SQL could be a huge pain in the ass.
Try this: SELECT ItemRecordID, Barcode, CallNumber, Author, Title, COUNT(ItemRecordID) AS Items, SUM(Checkout) AS Checkouts, SUM(Renewal) AS Renewals FROM (Select cir.ItemRecordID, cir.Barcode, bib.BrowseCallNo as CallNumber, bib.BrowseAuthor as Author, bib.browseTitle as Title, CASE WHEN irh.ActionTakenID = '13' THEN 1 ELSE 0 END AS Checkout, CASE WHEN irh.ActionTakenID = '28' THEN 1 ELSE 0 END AS Renewal from Polaris.CircItemRecords cir with (nolock) inner join Polaris.BibliographicRecords bib with (nolock) on (cir.AssociatedBibRecordID = bib.BibliographicRecordID) inner join Polaris.ItemRecordDetails ird with (nolock) on (cir.ItemRecordID = ird.ItemRecordID) inner join Polaris.ItemRecordHistory irh with (nolock) on (cir.ItemRecordID = irh.ItemRecordID) inner join Polaris.ItemRecordSets irs with (nolock) on (irs.ItemRecordID = ird.ItemRecordID) where irs.RecordSetID = '45625' and irh.ActionTakenID in (13,28) ) A GROUP BY ItemRecordID, Barcode, CallNumber, Author, Title
Neat, off to go read up on SUM(Case) and how it works :) thank you.
It's a combination of two different concepts. SUM() does exactly what it sounds like - it'll sum up across the entire grouping the data you specify as its arguments. If you're familiar with other programming languages, CASE should look familiar (or "SELECT CASE"). If not, it's just a different way of saying "IF, Then, Else". So in this example we've said take all of the records that have the same itemrecordID, barcode, callnumber etc - go through each set and for every record, if it has an ActionTakenID of 13, add 1 to a total we're counting up - otherwise, add 0. The final total represents the number of checkouts for that ItemRecordID.
There is a reason there are a lot of integer returning string comparison functions and no straight up difference functions. Varying length strings raise too many issues and as I pointed out in my comment above, there are a lot of edge cases. A numerical approximation is the best thing that can be done in anything close to polynomial time. Calculating the actual difference from a distance function is far from trivial. Although I would love to be proved wrong on this. Thats why the diff function in version control systems works line by line and not at a more granular level.
Sorry, I'm asking for a friend (I, myself, know very little about databases). He says he's aware conventional wisdom is to avoid them, but when used properly in the right circumstances, they're the best tool for the job.
Thanks, but I'm not up to the task of mediating a convo between you knowledgeable folks on r/SQL and my friend. Just asking about the book, but thanks anyway! :-)
What your query is saying is that you want to return all rows where the Year is less than the Expiry Year **AND** the Month is less than Expiry Month. This is not equivalent to Current Date is less than Expiry Date. The best way to tackle this problem is to convert the Expiry Year and Month into Expiry Date. You can do this by using the following query select * from Sales.CreditCard where cast( cast(expyear as varchar(4)) + right('0' + cast(expmonth as varchar(2)), 2) + '01' -- Set date to be the first of the month as datetime) &lt;= getdate()
Thank you all. The cast function seems to work, however, I've never worked with it so that's something I'll have to read up on. Thank you!
 SELECT HPTS, Team FROM BCS WHERE HPTS = (SELECT MIN(HPTS) FROM BCS) I think that would do it, will also return all rows where you get the smallest of HPTS. Just take out the top 1.
You're close, just have your logic a bit off __ DECLARE @dateNow Date = sysDateTime() DECLARE @yearNow SmallINT = YEAR(@dateNow) DECLARE @monthNow TinyINT = MONTH(@dateNow) SELECT * FROM Sales.CreditCard WHERE ExpYear &lt; @yearNow OR (ExpYear = @yearNow AND ExpMonth &lt; @monthNow) ORDER BY ExpYear DESC, ExpMonth DESC __ Unexpired is simpler of course: WHERE ExpYear &gt;= @yearNow AND ExpMonth &gt;= @monthNow
Thank you all! You've helped tremendously!
Functional dependency is a lot simpler than it may sound. Let's say I pick a random playing card from a deck and don't let you see it. I ask you what color the card is, red or black. You say, "I don't know, what suit is it?" I tell you, "Diamonds." Immediately you know the card *must* be red. Now rewind. This time you ask, "what number is it?" I tell you, "Eight." But you can't tell me the color because an eight could be red or black. In this scenario, you would say that a playing card's color is functionally dependent on its *suit*, but it is not functionally dependent on its number. More abstractly: If I present to you X and you are able to unambiguously determine Y from just knowing what X is, then Y is said to be functionally dependent on X. It's that simple. As for normalization, that's a pretty broad topic. Have any specific questions about it?
Never mind, I believe the issue is that I'm trying to reference an entry that isn't a primary key, nor a unique non-primary key. I'll have to change the design a bit.
Buy this man a beer, that was a great explanation! 
In MS SQL, Truncate will also reset the identity to it's starting point. 
Buy this man a beer, that was a great comment of praise! (and I agree, praise desserved)
What admiralwaffles said ... and also, it might not be relevant to you but thought I'd mention anyway, Erwin is really good for this, lets you visualise your database pre-production and iron out the structural problems then it will create the code for you automatically. Some might see this as the easy way out but it actually lets you avoid a lot of problems that can easily be overlooked when manually coding everything. I've just started using it a couple of weeks ago (previously i was doing everything by text alone) and so far I'm finding it to be amazing. It is a bit expensive though if you only want to see a visual layout and are not using it to it's potential. I suppose it's all down to what type of company you're working for and how easy it would be to make the investment. There is a limited 20 object, (up to) 1 month trial available on their website so you can try it out before you make a proposal to your bosses. The trial expires on the 15th of the following month though, so it might be best to wait until the right day before installing it. On a side note, why do people downvote frickin everything? the OP is just asking for advice and some muppet downvotes it! 
Instantiate your numbers table, you won't be sorry. A numbers table is useful for many purposes, including many things that would normally take a loop. Then, it's simple: SELECT CAST('12-31-' + CAST(Number as CHAR(4)) AS DATETIME) FROM numbers WHERE Number between (YEAR(getdate())-5 and YEAR(getdate()) 
 Being a DBA isn't just running queries and calling it a day. It involves a lot of installation, maintenance, optimization, backups, and permission. You essentially own the system hence the name database administrator. If you prefer taking data and combining, organizing, and formatting the data for analytics purposes I would suggest researching the role of business intelligence. Their role s to take data and show trends and forecast so the business can close any gaps. This role involves doing a lot of SQL, Crystal report, and some web design. You need to have a good,not great , understanding of accounting and numbers. Personally this sounds like a better fit from the limited info listed here. Edit : http://www.reddit.com/r/SQL/comments/qc3vd/z/c3wgkda
&gt; You don't need to know much programming to be a SQL DBA. Maybe this is true in bigger shops with budgets for commercial ETL software. Smaller places (or even under funded dev projects in large environments) will think you're a god if you can lay down a little code.
I think this is a great book that gives you an idea of the role of a DBA http://www.simple-talk.com/books/sql-books/how-to-become-an-exceptional-dba,-2nd-edition/
If I understand you correctly, the joins between tables t_main and t_user for the second and third users need to be outer joins, as follows (if this was Oracle): select u1.name creator ,u2.name last_updater ,u3.name closer from t_main m ,t_user u1 ,t_user u2 ,t_user u3 where m.creator_id = u1.id and m.last_updater_id = u2.id (+) and m.closer_id = u3.id (+) Because the second and third user IDs may not exist, the joins to your user table have to be outer. edit: formatting
Depending on the type of SQL you're using this may be different. Crouchy's code should work but I prefer to use Left joins instead of "," separated tables. select u1.name creator ,u2.name last_updater ,u3.name closer from t_main m LEFT JOIN t_user u1 ON m.ID = u1.ID LEFT JOIN t_user u2 ON m.ID = u2.ID LEFT JOIN t_user u3 ON m.ID = u3.ID 
A good DBA should be comfortable with at least one scripting language for creating automated or reusable administration scripts.. On windows, powershell makes the most sense, for unix there are many choices. Ultimately I would not hire a DBA who is only willing to use a GUI for all administration tasks. Having said that you don't need to known these things going in, but you'd better be willing and eager to learn.
Try a LEFT INNER JOIN and it will populate the other 2 tables with NULL if the data does not exist 
What do you want to see when no user has updated the record recently? The creator, or some other field? Instead of u2.name you can select ISNULL(u2.name,'n/a') or something similar, to return "nice" data informing you that this record does not exist, instead of NULL that you'd have to code around in some type of frontend.
Thanks for the help! Couple questions 1. What is the primary key for Order Details? transactionID will repeat if there are multiple books (that's why I had OrderID in order details to serve as a primary key). Or is TranID the PK in OD now? That would make sense... 2. Assuming TranID is the PK in OD, is there a way to pass that value automatically to Orders?
Single line query with dynamic output : resulted 6 years from 2012 back ward. SELECT Convert(datetime,cast (year(getdate())-V as varchar(4)) + '1231',112) FROM (Select 0 [c0],1 [c1],2 [c2],3 [c3],4 [c4],5 [c5]) x UNPIVOT ([V] FOR [N] IN (c0, c1, c2, c3, c4, c5)) uPIVOT * result in MSSQL * 2012-12-31 00:00:00.000 * 2011-12-31 00:00:00.000 * 2010-12-31 00:00:00.000 * 2009-12-31 00:00:00.000 * 2008-12-31 00:00:00.000 * 2007-12-31 00:00:00.000 If you don't want current year .. remove c0 from this part like this. UNPIVOT ([V] FOR [N] IN ( c1, c2, c3, c4, c5)) uPIVOT If you want 5 years including current year, then remove c5 UNPIVOT ([V] FOR [N] IN ( c0, c1, c2, c3, c4,)) uPIVOT
I thought he had *two* turntables?
Ok, assuming you meant "SELECT * FROM student WHERE SID NOT IN (SELECT SID FROM CLASS_STUDENT WHERE GRADE IN ( 'A' , 'B' )) ORDER BY SID; (Student table, not customer table) The inner SELECT would return all the SIDs where the grade was A or B.... that'd give you: **2 1 2 4 1 2 1** Then from that list, you're going to look for any SIDs from the STUDENT table, where the SID *ISN'T* in the set above.... meaning you'd have SIDs 3 &amp; 5. Since you're selecting * from STUDENT WHERE the SID is either 3 or 5 (and ORDERed BY the SID number), the result would be: **3 Joe MGMT 5 Jim ACCT** 
When you have nested queries, always start with the innermost query, figure out what it would return, then the outer query will be applied against the data returned from the inner query.
In one of my lower division IT courses, the instructor used this phrase to describe the 3 forms of normalization: "The key, the whole key, and nothing but the key." Referring to the primary key. In 1NF, all data in the tuple (record/row) relies on the primary key. In 2NF, all data relies on the ENTIRE key (even concatenated primary keys), and in 3NF, all data values rely ONLY on the key. For example, if you have an EMPLOYEE table, the primary key may be the employee ID number.... data in that table may include the employee's department number, but if you include the department manager, that's not dependent on the employee's ID number... it's dependent on the department number. Therefore you shouldn't include the employee's manager in the EMPLOYEE table, an additional table relating departments to managers should be created instead. In every phase of normalization, you're going to end up reducing the number of columns in the table and creating additional tables instead.
Is it possible for you to avoid using 'SELECT *'? That's probably what's causing the problem here--it sounds like your query is returning multiple columns named [ID] and your code is just grabbing the last one on the list.
It totally is. I went back and used a more specific SELECT statement and it fixed the problem immediately. I did have to specify Order_Line.ID in the SELECT, though. Never naming columns this way again :-P
I disagree with not naming columns like that. As long as you're consistent, you never have to wonder what you called your primary and foreign key columns.
Agreed. And explicitly referencing table.column is good practice anyways. 
I often have to write a query where I didn't create the schema - you can use alias for column and table. Use this to your advantage and you can make your SQL a lot more readable.
Good point. I will script it out as soon as i have access again to it. Don't have access to it from home :&lt;
That's how I'd do it.
Crystal is a horrible, horrible system. The possibilities are good with it, in theory it can do a lot of fancy stuff that looks pretty, like graphs and charts, which is what managers want to see, which means it's an easy sell to management, but to actually accomplish those results takes far too long and it rarely works correctly. This is why it ends up being used in far too many organisations, and the people actually using the system hate it. I'm sure some people will argue against what I'm saying, as is the way on reddit, I've been cornered into using it a few times though so this is my opinion and no-one will ever change it, Crystal is terrible, bottom line.
It where indeed the quotes, i blame my teachers :f Thanks all :D
To be more helpful: Select table1.product_id From [xxx].[dbo].[table1] inner join [xx].[dbo].[table2] on (table1.product_id = table2.product_id) Where table1.column1 like '%test%' and table2.column1='1234' Wonder if that will format right?... Meh, close enough. I'm assuming that product_id is the same in both tables here. You might even get away with a NATURAL instead of INNER join, and you could lose the on clause if product_id is the only field that have in common.
yes. product_id is the same in both tables. I'll give it a shot thnx.
Hmmm. Ignore my comment about a natural join while you're at it, column1 is in both tables, and from your query, it won't be the same in both. The INNER JOIN with an ON clause will be necessary.
hmmm... it came up blank, but didn't fail. It should be coming up with at least 1 returned value. goddamn I'm an idiot. got it. help if I cut/paste properly.
You could wrap the whole thing in an IF statement declare @param as int select @param = null if @param is not null begin select 1 INTERSECT select 1 end
That is so sexy! There's an extra ) after LEVEL in your example but I got it figured out. Thank you much!
Cursors are slow. Avoid them if at all possible. SQL is designed for set based operations.
Are you using a cursor or a while loop, cursors offer functionality your not utilizing at the cost of speed. I found good speed by throwing the IDs of the rows I need to process into a temp table and using a whole loop to move through the IDs in the temp table. Seeing as I'm on my phone right now I'm not including a code example, but I will tomorrow if you request it. Other considerations... What's the status of you pertinent indexes? I feel like i just channeled Roy from the IT crowd: "have you tried turning it off and on again?" 
If I were you ... I will 1. Stop using Cursor and LOOP. 2. I will remove Transaction : This is simple conditional insert. 3. I'll do Bulk insert ONLY for the items met by the given criteria 4. By doing 3 .. I don't need to worry about dirty insert and rolling back. If criteria is not match .. it won't be inserted from the first place. 5. Truncate Calendar ( Or drop Calendar table and recreate it again using script. ) Because DELETE take more time/resource, I would add a updateflag column and set it to true instead of deleting those rows. If I'm worry about performance, lockup and hardware usage .. I'll split the condition and bulk insert into multiple session. Let's say 5m x 8 times. Or 12 times based on month number. PS : You said you have 800+ different condition and don't want to ended up writing 800 queries .. maybe this is the best way. I did have similar but smaller scale .. which need at least 80~100 queries. I wrote a program to generate the queries based on the data .. and run those 80~100 dynamic queries using that small program. It took time .. but it gives me exactly what I want. But very hard to handover this project to another guy. e.g : having 40 conditions, each condition pair require me to join different tables among 20 different transaction tables with several Master data tables. Based on the criteria, program generate different selection and insert queries .. 
Might not be necessary, but also enclose in an UPPER(): SELECT UPPER(REPLACE(CONVERT(VARCHAR, GETDATE(), 106), ' ', '-')) (I've worked with people who want everything EXACTLY as stated, right down to the case, which can be a pain in the ass but has to be done) EDIT: Sorry, just noticed this post is a fortnight old, nm :]
We had an old search page that pulled across a large number of tables with columns derived from functions etc, and it used a temporary table to UNION all these different results. I'm breaking it up into search fields, and thought replacing the one general search parameter with individual parameters for each search and replacing UNION with INTERSECT (So a search matches all fields rather than any field) but the null parameter is going to prevent me from reusing the code. Regardless, thanks much!
I think my biggest problem is three things. One being the user i'm handing this off to isn't knowledge enough beyond backing up the database, clicking Execute and checking their data to make sure everything happened ok. It's just what i have to deal with. Second is the Duration, the problem with this is, in the Calendar table it's stored as a straight number in minutes (10 hours = 600) but in the History table it's stored as HH:MM:SS. If the duration is 0 then the result must be '', the next part is to now convert that number to 00:00:00 format, which i did like this: '0'+CAST(@duration/60 as varchar(5)) + ':' + RIGHT('0' + cast(@duration%60 as varchar(2)),2) + ':00' The issue being there, if it's above 10 hours I have to chop off the first 0. And if it's 100 i have to keep track of that cid to be processed later. Third is the ctype, there are 6 different ctypes to be processed, each has a resulting cstype that are all different. All my other programming knowledge tells me to do this in some type of loop so i can properly process the duration's and ctypes. But if SQL is that bad at iteration then this could be a long week.
Also works fine in MySQL. If I remember correctly, it can be found under the advanced option in PHPMyAdmin
Added some formatting so I can read it better. SELECT Type, FirstName, LastName, Adress1, Adress2, Amount, Date, CASE WHEN Type = '1' THEN 'Refund' ELSE 'Payment' END AS TypeName FROM Transactions WHERE (Date &lt; @DateTo) AND (Date &gt; @DateFrom) AND (FirstName LIKE '%' + @FirstName + '%')AND(LastName LIKE '%' + @LastName + '%') and SELECT refund.value - payment.value AS Total FROM (SELECT SUM(Amount) AS value FROM Transactions WHERE (Type = 0) AND (Date &lt; @DateTo) AND (Date &gt; @DateFrom) AND (FirstName LIKE '%' + @FirstName + '%') AND (LastName LIKE '%' + @LastName + '%') ) AS refund CROSS JOIN (SELECT SUM(Amount) AS value FROM Transactions AS Transactions_1 WHERE (Type = 1) AND (Date &lt; @DateTo) AND (Date &gt; @DateFrom) AND (FirstName LIKE '%' + @FirstName + '%') AND (LastName LIKE '%' + @LastName + '%') ) AS payment I'm not entirely sure what you are asking for, but I think it might be this: SELECT FirstName, LastName, Adress1, Adress2, Date, sum(CASE WHEN Type = '1' THEN Amount ELSE (Amount*-1) END) AS Total FROM Transactions WHERE (Date &lt; @DateTo) AND (Date &gt; @DateFrom) AND (FirstName LIKE '%' + @FirstName + '%')AND(LastName LIKE '%' + @LastName + '%') edit: took out some fields in my solution that didn't belong. And one comment... Are you sure you really wanted a cross join on that query?
Big thanks for the formating. The query you have provided results in a error with grouping. But I think i gonna check back to this, when i'll have more time. Thank alot :)
I really appreciate your dedication. But in my case its MS SQL 2008 R2. 
Well, if the address fields don't change by date, max() will work by itself. If they do? You'd need a sub-query to grab the date and match by that. If you need me to, I could try to write the sub-query version.
This thread needs to be set on fire.
 SET STATISTICS TIME ON SET STATISTICS IO ON SELECT CAST('12-31-' + CAST(Number AS CHAR(4)) AS DATETIME) FROM NumberTable WHERE Number BETWEEN YEAR(getdate()) - 5 AND YEAR(getdate()) OPTION (RECOMPILE, MAXDOP 0) --- Results SQL Server parse and compile time: CPU time = 0 ms, elapsed time = 0 ms. (6 row(s) affected) Table 'NumberTable'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. SQL Server Execution Times: CPU time = 0 ms, elapsed time = 0 ms. I would highly recommend against this as it has logical reads.
This is extremely ugly but you're right, it has a beautiful query plan. A+
I used 1,000,000 rows for that "NumberTable". I was also thinking, "what if you wanted to go before 1753.." Then I wondered why would anyone practically be doing more than +-100 years.
You might be a bit over your head.
It's a generic concept, the numbers table. It's basically set-based looping. A million rows is excessive, but maybe interesting from a performance POV. I think Brian Knight did an article on this....too lazy to look it up right now. 
I am getting an error, in my from statement. Can't seem to get the 5 table join to work
Hello!
this guy sure is trolling /r/SQL hard tonight isn't shim?
What Olly *meant* to say was &gt; INSERT INTO tblThingsIGot VALUES (1, 1), (1, NULL)
Second this, write the code that will return the data you want to use in your report and just display that. Sometimes it can be easier to aggregate inside the reporttool but most often you don't want to do that. Check out WITH ROLLUP and GROUPING SETS for quick sums for a set. You can also use GROUPING() to check which row(s) are the aggregated rows.
I prefer to name all ids in any table simply as "id". Then, on the other tables where there is a foreign key, I name it as "tablename_id", as in "order_id". That way I can do joins like this: order.id = order_detail.order_id
I haven't seen my style used anywhere outside places I've worked, but I really like it. EVERY column in my databases has a unique name, because every table uses a unique prefix like [prefix]_id with foreign keys as [prefix]_[primaryKey] ORDERS: o_id, o_date PRODUCTS: pr_id, pr_name ORDER_PRODUCTS: op_id, op_o_id, op_pr_id, op_quantity Using this method, you never have to alias or scope tables unless you do a self join. Bonus points for the benefit of short columns and a very obvious key structure. The only downside is the learning curve of memorizing the prefixes. SELECT o_id, o_date, pr_id, pr_name, op_quantity FROM ORDERS INNER JOIN ORDER_PRODUCTS ON o_id = op_o_id INNER JOIN PRODUCTS ON pr_id = op_pr_id WHERE o_id = ?
I would do this. USE (databae name); SELECT * FROM dish ORDER by name ASC; This will order the list alphabetically. Including the USE command can't hurt.
At the place I work, we've adopted the standard that every column for a given table has the same three-letter prefix. Therefore, it's not uncommon to see something like the following: SELECT ShpPickupDate, ShpDeliveryDate, InvTotal FROM Shipment INNER JOIN Invoice ON ShpId = InvShpId -- ShpId is a PK, InvShpId is a FK WHERE ShpId = @ShpId This cuts down on typing when you're writing queries, since you don't have to fully qualify every column name with [table_name].[column_name]. I should point out that this is by no means an industry standard. This is just how we do things.
This is a good starting point, the only thing I'm not really sure on is why you would have ShpID in your Shipment table and InvShpID in your Invoice table, the names would lead me to believe they could hold different values. As for cutting down on time/typing when writing queries, the savings are moot. I usually use select * from Shipment shp inner join invoice inv on shp.shpid=inv.shpid where shp.shpid=@shpid. 
Orders.order
Do you have an issue with making column names descriptive?
Our main database had somewhere around 300 tables and we managed just fine ;)
If you are just working with one database, it makes you life a bit easier because you are only worried about the tables in your queries. Otherwise, I can't see a big difference, unless there is something happening at the low levels that makes one more efficient than the other.
When you do a lot of table joins you don't want a bunch of things like e.id, o.id, p.id... You'll want to know at a glance what the hell it's an id for. I've done it both ways for 10 years and regret all of the times I named a field "id".
This. Also here is a useful article: http://www.techrepublic.com/blog/datacenter/passing-table-valued-parameters-in-sql-server-2008/168
Look up ISNULL in books online. ISNULL(value, replace with) As I recall
I dont think this will work, NULL is not the value inside the table, the table's value is 0.00, it is not until after i execute my query that i get the NULL (idk from where)
 If you're using T-SQL the below will fix your problem: select Acct as AccountNumber ,(select top 1 trandate from gctran_dbfile where acct=a.acct and trantype='Redeem' order by skey desc) as LastRedeem ,(select top 1 trandate from gctran_dbfile where acct=a.acct and trantype='Reload' order by skey desc) as LastReload ,date_issuedc as DateIssued ,a.IssueAmount - a.VoidIssueAmount as OriginalIssuanceAmount ,a.NoOfSale as RedemptionTotal ,a.NoOfReload as ReloadedTotal ,isnull((select top 1 CurrentBalance from gctran_dbfile where acct=a.acct and convert(int,GcTran_Dbfile.TranDate,101) &lt;= convert(int, cast('05/15/2012' as datetime),101) order by skey desc),0) as CurrentUnredeemedBalance from gcbal_dbfile a
McBullseye is right, if you wrapped the subquery into an ISNULL() it would fix your problem. Based on your response, though, it sounds like the real issue is that your subquery isn't returning any data, which is causing the null in the first place. Just from looking at the query, my guess is that there are no dates in the "gctran_dbfile" that are &lt;= '5/15/2012', or at least not once they're converted to an int. Also, when converting a date to an int, the "style" parameter does nothing (this is the "101" you have at the end of the converts). Finally, when comparing a datetime field (which is what I'm assuming "GcTran_Dbfile.TranDate" is since you're not converting it before converting it to an int), if the field has hh:mm:ss then it's not going to be equal to '05/15/2012' (which converts to '05/15/2012 00:00:00' when you cast it to a date). It's for this reason I typically do my date comparisons using strings like this: "convert(varchar(8),GcTran_Dbfile.TranDate,112) &lt;= '20120515' " TL;DR: look at your date comparisons in the select statement. Also, McBullseye is right.
thank you so much! That did work, thank you!
Lol, no problem, figured i'd let you ponder on that for a while before just giving you the answer ;) (teach a man to fish and all).
It was an example! 
Your method was actually the very first way I tried. It solves the issue of counting how much each customer has spent but instead of showing how many rentals each customer has made, it shows how many items are on the rentals they have made. e.g customer 1 made 3 rentals and there was 8 rental items in total on those 3 rentals. I tried replacing the count(*) with count(r.customerno) so that it would count the number of times a customer number is in the rental table but it still counts the rental items. Any idea on a work around? oh and thanks for replying!
&gt;I am not sure how representative my role is of the field as my company has only recently become aware of how important my area is... To be honest with you, I read your post and honestly freaked out that you might work with me. My company doesnt do Gov't contracts. If your company isnt aware of how important BI is they need to read this article about how Target knows when someone is pregnant. http://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/
What about this? (May be sql server specific) select c.customerno, forename + ' ' + surname as 'Name', COUNT(distinct rentalno) as [No of Rentals], SUM(daysrented*costperday) as [Total Spent] from customers c inner join rentalitem ri on c.customerno=r.customerno inner join costume cm on ri.costumecode=cm.costumecode inner join rental r on ri.rentalno=r.rentalno group by c.customerno, forename, surname
notice you have compression on one table? compression can make a huge difference on the situation - especially if the ram is less than the size of the DB+indexes+os overhead. p.s. ram is cheap now - even if you have more ram than database size - it has been found that the extra ram can reduce i/o's alot. if my database is 50gb - throw 72gb of ram at it. compression makes up for the lack of ram and lack of disk iops when you have more cpu (very very common old school setups). pick your poison.
Thanks that worked great! I never actually thought of using distinct within an aggregate function before.
Well, the cheap and dirty method is going to be a cursor and dynamic SQL (note that I did not say 'quick', and your DBA may hate you). First off, create a list of all of the files (dir &gt;c:\temp\filelist.txt), open that text file and strip off all header and footer information so that you just have a clean list of files called filelist.txt Then, this should work for MSSQL 2000 and up. '--' denotes a comment line. backup database [myDatabase] to disk = 'd:\temp\myDatabaseBackup.bak' go use [myDatabase] go -- declare our variables declare @filename varchar(255), @sql varchar(255) -- create a temp table to hold the filelist. The '#' in front means its a temp table in tempdb, not a real table create table #myFiles (fileName varchar(255)) -- collect the file list and store it insert into #myFiles select * from openrowset('MSDASQL', 'Driver={Microsoft Text Driver (*.txt; *.csv)}', 'select * from c:\temp\filelilst.txt') -- declare our cursor to iterate through the files -- it should be noted that iterative functions in SQL should be avoided, if at all possible -- SQL sucks at iterative functions and string manipulations, and we're doing both! declare cur_files cursor for select [filename] from #myFiles -- open the cursor and get the first result open cur_files fetch next from cur_files into @filename while @@fetch_status = 0 -- @@fetch_status becomes -1 when we run out of new records to process begin -- create a string which holds the SQL statement for the current filename set @sql = 'BULK INSERT [dbo].[TABLENAME] FROM ''' + @filename + ''' WITH FIELDTERMINATOR = '' '', ROWTERMINATOR = ''\n''') -- execute the sql statement contained in sql exec (@sql) -- redo from start fetch next from cur_files into @filename end -- cleanup close cur_files deallocate cur_files drop table #myFiles This was a quick, off the top of my head sql script, let me know if you have issues. Change [myDatabase] (leave the square brackets) to your database name. Also, it may be worthwhile to change the script such that it makes a permanent table somewhere which holds the filenames and then checks them off as it completes them. That would take a bit of changing around; but, wouldn't be too hard. **EDIT:** See version two in the reply to myself 
This is kind of confusing. Are you updating a column in a table or are you trying to populate a variable? Are you trying to make this dynamic so you can choose the column that get's updated?
Sylver_dragon has posted a rather excellent way of doing this with just the SQL Server tools you have, but I'd like to point out that this type of work is handled *very* well by scripting languages. Pick one (almost any of them), master it, add another useful tool to your collection.
The more I read this, the more I think he's just trying to figure out how to do PHP string interpolation to use a PHP variable, not a SQL variable.
Pretty sure you're right. The query is just a simple update tbl set a = b where colx = c
Your looking for Parameterized PHP queries and there's a few different ways to do them. Here's the heredoc method. $query = &lt;&lt;&lt;SQL update table set column1 = ? where column2 = ? SQL; $result = your_php_database_object($query, array(&amp;$variable1,&amp;$variable2), DB_ERRORLEVEL_NONE); if (empty($result)) { echo ("no results"); exit; } else { echo "results"; exit; }
Your current PHP line: $updateplayer = "UPDATE players set $equipstat = '$playerstat' where id = '$playerid'"; ... looks correct. Now, what you've done is put a SQL *string* in the $updateplayer variable. You'll need to actually execute the SQL statement against whichever database your using (MySQL and Oracle are common) to make anything happen in the database. As a quick sanity check - have you connected to the database? Using something like mysql_pconnect (mysql) or oci_pconnect (oracle)? For oracle, the quick 'n dirty bit of code that you'd continue with (assuming you've already established a connection to the database): $oracleConnection = oci_pconnect('username','password','database'); // connect to database if (!$oracleConnection) die(print_r(OCIError(),true)); // print error details and die on error $statement = oci_parse($oracleConnection, $updateplayer); // parse the SQL string if (!$statement) die(print_r(OCIError(),true)); // print error details and die on error $rowsAffected = oci_execute($statement); // execute the SQL string As an aside: Build and executing a SQL string this way allows for easy SQL injection (security hazard!) if any of the variables you are putting in the string come from a webpage or end users.
Not sure if this is what you're looking for but I really liked "SQL anti-patterns"
MSSQL BOL ~ Free from Microsoft and my favorite link : http://msdn.microsoft.com/en-us/library/ms130214.aspx
Not enough upvotes. If what you really want is a reference, *this* is the place to go. There are very few questions I've *ever* had about SQL (Server) that were not answered here.
It appears that a default instance should have been created but wasn't. Also, my sql browser service is not on, but don't know how to activate. I've tried searching around for answers but getting dead ends. This is very frustrating. I may soon ditch 2012 Express and just try 2008. At least there will be more support for it. 
I did some Access work a while ago and this statement will work; however, you may get a syntax error. You just use the same statement but capitalize everything. http://office.microsoft.com/en-us/access-help/microsoft-access-for-beginners-part-iii-writing-the-queries-HA010247313.aspx
To do aggregates like Average, Sum, etc. you have to "Group" by a field so in that case you might have to add SELECT AVG(Goals), TeamName FROM Players INNER JOIN Teams on Players.TeamID = Teams.TeamID WHERE Players.TeamID = 20; Group by Players.TeamID 
I know the Rank() over works in Oracle.... I am currently running on a Vertica system which seems to blend rules between Oracle, SQL Server and MySQL. 
Thanks.. ill give that a shot. Good luck with the dentist! Get some N2O!
It must be some mysql table corruption You should make use of [http://www.mysql.fixtoolbox.com](http://www.mysql.fixtoolbox.com)
 WHERE ObjectSubType = 'ICMP' AND (Device_Type &lt;&gt; 'URL' OR Device_Type IS NULL)
haha good catch on the endquote and thanks for this, I'll read up more on that function.
&gt; Device_Type in an isnull() No, don't do that. Don't apply functions to where clause statements. Random reading Links (may be SQL Server specific, should be applicable) http://www.databasejournal.com/features/mssql/article.php/3845381/T-SQL-Best-Practices-150-Don146t-Use-Scalar-Value-Functions-in-Column-List-or-WHERE-Clauses.htm http://stackoverflow.com/questions/356675/using-an-alias-in-a-where-clause http://stackoverflow.com/questions/6319183/aggregate-function-in-sql-where-clause http://stackoverflow.com/questions/1724325/function-call-in-where-clause
If you have an index on Device_Type, and you throw an ISNULL on that, chances are you're going to be doing an Index Scan, instead of an Index Seek. http://stackoverflow.com/questions/799584/what-makes-a-sql-statement-sargable I do apologize that this is SQL Server centered--however it should be universal.
How do you know when your cake day is?
i think on your user page, http://www.reddit.com/user/robbyroo, over where is says you've been a redditor for 1 year, hover your mouse over that part, it should say something like dec 21st for you
Could be the greatest post in this subreddit's history. 
sounds like a target for some SQL injection
But... I don't want to.
Why store the XML when you can easily parse it into rows, and store it the correct way?
Encrypt and Decrypt Passwords in Database
where does firebird stand better than postgres worst than sql server 2005? 
Maybe try adding case statement? case when STFRACT = ' ' then '' else STFRACT end You could try that if the blank spaces are the problem. Looking at your code again now though I think it could be something else. If you data was this. STNUM = 17 STFRACT = NULL STDIR = NULL STNAME = PARKER APT = 10 APTN = 435 Wouldn't you get this output.. 17' '' '' 'Parker' '10' '435 With the ' ' being spaces, you would have 3 spaces there because your concatenation will always put those spaces in there regardless of if a column has data in it. To be more specific 17' '(STFRACT)' '(STDIR)' 'Parker' '10' '435 So are you positive that the fields actually have blank spaces in them? because it looks like your concatenation may just be wrong. You might want to try this [listagg](http://docs.oracle.com/cd/E14072_01/server.112/e10592/functions087.htm) function, it will concatenate columns with a delimiter of your choice (in this case space) and will ignore nulls. A bit wordy but I hope something in here helps!
sometimes you just have to hybrid old school it - i had such a slow sql server (3 disk raid-5 with one partition) - that it was faster to do the dirty ETL work in php including sorting since disk writes were so slow. i even used a SHA hash (sql 2000 didn't have this) to not write the same data since that added too much time when only 50% changed but was not predictable. I tried to do it sql side but it was 100x slower than possible due to the amount of "garbage in" code. I ran the query on the local machine which was 1000x faster than over network due to shared memory/named pipe increase in speed. not very "sql"'ish solution but i was able to overcome a handicap that I could not change (the sql server). I suspect i could now do it with CLR but i wasn't very good at that language back when php 5.0 came out. GARBAGE IN GARBAGE OUT - worst excuse ever
sounds like a botched install - this is why i always use mixed mode - so I don't have to deal with active directory authentication. might move back to 2008R2 developer - SSMS works quite easily after you install properly. Though SQL 2012 works fine with AX 2012 - so it's not that different.
Funny you should mention that, after I had slept on it, I woke up thinking that a case statement might be the answer as well. There's a fairly limited number of situations that should be encountered, so it might be fairly easy and small to write. I've been working on this a while, but never really gave it a lot of intense thought outside of the actual project - I guess writing it out for someone else's consumption helped :)
&gt;I don't think SQL Server has native regex support, but it can be added using .NET and CLR, [1] example. Cool, I'll have to check it out. &gt;You should have used ROW_NUMBER over (partition by .... order by...) for your ROWNUM translation. Depends on how you're using ROWNUM as to whether that works or not (at least, that's what I gathered from my reading). The way that I've been using it in Oracle is to limit my result set to the first row returned. The query in question doesn't actually return any information from that row, only if it exists or not. I used ROWNUM=1 so that it would stop at the first result it found, as it would be possible to return multiple rows from that particular table. I've tried using TOP(1) in MS SQL, but I'm getting both minor and vastly different results when run against the same data set. I haven't had time to go row-by-row and figure out what the difference is.
You might need to use double quotes, as variables are not expanded inside single quotes
Interesting, I never thought of using SELECT NULL What I'm doing is something like this: SELECT &lt;blah, blah, blah&gt; WHERE EXISTS (SELECT 1 FROM ...) Most of the time it's wrapped up in a CASE statement to return either 1 or 0 so I can create flags for records or roll them up into a count by some group or another. Is the SELECT NULL more efficient? I've found that the WHERE EXISTS works in SQL Server; I wasn't aware of the IF EXISTS.
**Exists** only looks for the existence of a row, so there's no point in selecting anything, hence null will suffice. I too used to use "select 1", but moved to "select null" having seen some Oracle articles and some of their v$ view sources doing so, that made me twig that selecting a value must place some overhead (albeit small) on the SQL engine. I think because null in this context is type independent and scale free, it must be slightly quicker selecting null than selecting a value and returning it, wouldn't you think?
Yes, it is a PHP question.
It is if you're still manually concatenating query strings.
It looks like you aren't also grouping by u.fname; it may not be calculating the right totals then.
Without seeing the data in the table or the result set from the query, I can only assume that it was grouping only by last name and returning sums for everyone with the same last name.
Storing XML files in database so you can make backup and also versioning the data
This was my suggestion as well.
I don't feel like writing out a whole huge query cause it's going to be a pain in the ass but these will be the steps: 1. Start with the query as you have it but sort it by p.id, o.product asc 2. Combine with a query that gets a count() of rows where id = p.id of your other query and the o.product &gt; the o.product of the other query. For the results in your post this will return 0, 2, 1, 2, 0, 1. If you were to sort that original query by p.id, o.product you'd end up with this second query returning 0, 1, 2, 0, 1, 2. You might be able to do this with rownums but I'm not positive. 3. Wrap all that in a case statement and only output the first three columns for rows where the second subquery (the one with the counts) is 0. Otherwise output ''. Combining the subqueries in a way that won't kill performance will be a pain but if everything's indexed correctly it shouldn't be too bad.
The easiest solution would be to use Oracle's ROW_NUMBER analytical function to break the result set up into rows you want to hide and rows you don't want to hide. Here's an example: SELECT CASE WHEN rnum &gt; 1 THEN null ELSE id END AS id , CASE WHEN rnum &gt; 1 THEN null ELSE name END AS name , CASE WHEN rnum &gt; 1 THEN null ELSE city END AS city , product , amount FROM ( SELECT p.id , p.name , p.city , o.product , o.amount , ROW_NUMBER() OVER (PARTITION BY p.id , p.name , p.city ORDER BY p.id , p.name , p.city ) AS rnum FROM p LEFT JOIN o ON p.id = o.person_id WHERE o.amount &gt; 0 ORDER BY 1 , 2 , 3 ) ;
Every database product claims to support ANSI SQL in some form or other. However, all vendors end up extending it and adding additional vendor specific features. What I would recommend doing is determining what vendor you'll primarily be working with and then go to Amazon to try and find books that are specific to that vendor that get good reviews. I've primarily worked with Oracle so if you know you are going to be working with that product I can provide additional insight.
Slightly different question - what if you wanted to combine the distinct values of o.product and o.amount into a single field with the values separated by commas? Is that the Coalesce function? 
The [Coalesce](http://docs.oracle.com/cd/E11882_01/server.112/e17118/functions030.htm) function returns the first non-NULL expression. If I understand your question correctly you would probably want an *aggregate* function like [LISTAGG](http://docs.oracle.com/cd/E11882_01/server.112/e17118/functions089.htm#CJABDFBD). This is only available in more recent versions of Oracle. Other string aggregation techniques can be found [here](http://www.oracle-base.com/articles/misc/string-aggregation-techniques.php). 
I've realised that I need to do a division or a double negative. However I can't for the life of me figure out how to "model" my query. And to help those of you that have problems when the column names are jibberish(swedish) what I need to find out is: Find rooms that all teachers that were born in LA have held a lecture in
Imgur is down right now so I can't look at the diagram but the basic logic is going to be this: First, find all teachers born in LA. No reason to look at rooms with teachers that weren't born there. Then find the rooms where they lectured in at least once.
I figured you forgot about this thread. Glad it worked like you wanted!
What's the field u.wp_id? Is that a unique identifier for user? I think you should be grouping by that.
select student.name, result.marks from student, result where student.roll_no = result.roll_no and result.subj= "maths" order by result.marks desc limit 1 Found out one more way as above without the sub query. I am still not sure if same can be done using having/group by without sub query
 select s.NAME, r.MARKS from ( select s.NAME, r.MARKS, RANK() over (order by r.MARKS desc) as RNK from STUDENT s join RESULT r on s.ROLL_NO = r.ROLL_NO where r.SUBJ = 'maths') where RNK = 1 Like your original query, it could return multiple rows if the highest mark was shared by multiple people 
That's probably the way to solve it, but I can't seem to convert it to my tables correctly. My code: SELECT namn, antalplatser FROM Rum WHERE NOT EXISTS ( SELECT * FROM Person, Kurstillfälle WHERE ort = 'Södertälje' AND personnummer=Kurstillfälle.lärare AND NOT EXISTS ( SELECT * FROM Kurstillfälle WHERE rum=id AND )); Any thoughts?
I'm in a similar situation. I'm looking forward to the test that should be released on Monday: http://www.microsoft.com/learning/en/us/exam.aspx?ID=70-461 But I've purchased the training books for exam 70-448 and 70-432 to browse for now. The transition in certification from 2008-2012 is confusing, to say the least!
thanks a lot for your reply, i will check this out
hey i didnt know that you can have a query in the 'from' I will find more info on this, thanks a lot.
Seriously. I had this same issue and solved it very quickly in python. 
Simple! This: ;WITH strings AS ( SELECT 'AS"DF' AS string UNION SELECT 'ASDF' AS string UNION SELECT 'AS|DF' AS string ) SELECT string , newstring = REPLACE(REPLACE(string, '"', ''), '|', '') FROM strings s WHERE s.string LIKE '%"%' OR s.string LIKE '%|%' Returns string newstring AS"DF ASDF AS|DF ASDF So you'll want something like UPDATE [tablename] SET [fieldname] = REPLACE(REPLACE([fieldname], '"', ''), '|', '') WHERE [fieldname] LIKE '%"%' OR [fieldname] LIKE '%|%' Good? 
I don't see why you would need double negative. Do you get the correct results with this: SELECT DISTINCT namn, antalplatser FROM Rum R INNER JOIN Kurstillfälle K ON R.id = K.rum INNER JOIN Lärare L ON K.lärare = L.personnummer INNER JOIN Person P ON L.personnummer = P.personnummer AND P.ort = 'Södertälje'
We are forced to work in MS Access 2003 and that code gives me an error saying: Syntax error (missing operator) in query expression "R.id = K.rum INNER JOIN Lärare L ON K.lärare = L.personnummer INNER JOIN Person P ON L.personnummer = P.personnumme"
Not possible simultaneously. But you can do it beforehand. SELECT [OriginalTable ID], [StringField] INTO [MyBackupTable] FROM [OriginalTable] WHERE CHARINDEX('"', [StringField]) &gt; 0 OR CHARINDEX('|', [StringField]) &gt; 0 MyBackupTable will be automatically created for you.
Thanks everyone!
If someone who has gone through SQL certifications. Do you think it would be better to get certified in 2012 instead of 2008? I know this depends on your business and what is used in house, but is that the only consideration?
I am a 26 year old database developer (3 years out of college). I have spent the last three years working for a healthcare consulting company, dealing with 500+ GB hospital financial databases. Most of my time is spent writing complex calculations (one took me three months, and sits pretty at nearly 2500 lines of pivots, ranks, and other goodies). I have two SQL Server certifications... * 70-432 - MCTS: SQL Server 2008, Implementation and Maintenance * 70-433 - MCTS: SQL Server 2008, Database Development If you are getting a certification just to say you have one (or because it is a company requirement), I would recommend 70-432 - as it is 2000% easier. 70-433 kicked my ass (mostly the portion on XML). I am waiting to finish my MCTS (the Business Intelligence cert) until the 2012 tests come out, so I cannot speak to the difficulty in this one. I do have to say, I learned A LOT studying for these tests. I bought a couple of text books and also got the Transcender, which was immensely helpful: www.transcender.com
I don't currently use SSIS, but I use to use SSRS quite frequently in 2005 and some in 2008. My current job though I don't use either, but that doesn't mean we couldn't for some things. Might have to look into the differences to see if 2012 is the path I should go. Thank you.
USA. I probably should have specified that from the beginning.
Had no idea that existed, thanks!
did you look p the query-plan?
Yes and the first query is more CPU costly. It has an extra Compute Scalar step though, but I cant make much out of it.
That sounds logical. In that *case* I should use cases more often in my queries.
I would argue that you can't compare these queries. They are not semantically equivalent. The first one will return all employees regardless of what type of leave they have. The second will return employees that only have the leave type you are searching for. What are the real query requirements? Once you know the real business requirements try and write some different queries that meet the requirement and then compare their performance profile. Second, I see in one of your responses you mention something along the lines of "I should use cases more then". Applying a general scheme like this is dangerous in my opinion. Generalizations are not always accurate. 
It is a subquery of another query so with a left outer join I still get all the employees (ir)regardless if they have leave or not. And business requirements? Management is too busy with other stuff for something as 'silly' as providing IT with actual specs... I wanted to see my actual, up-to-date amount of leave hours so I made this query (if I have the time for Reddit I could also use it more 'productively') and saved it as a View. I made a LOB/XML thing out of it for on sharepoint and it's now visible on the hour register page (with proper individual security). But updating this page takes longer than other BDC's (with more items), so I dissected the query and looked for where it took long.
Is there an index on [Absence Type Code]? My guess would be that the second query is doing a table scan while the first is doing a clustered index scan and like incitinghatred said, performs the evaluation on every row. table scan = SLOW
I thought you had to use like for text? I see a tiny bit improvement when I use '=' so I'll take it anyway.
 SET STATISTICS TIME ON SET STATISTICS IO ON SELECT [Employee No_], SUM(CASE WHEN [Absence Type Code] LIKE 'Leave' THEN [Quantity Hour] ELSE 0 END) AS [Total Leave Left] FROM dbo.[Company$Leave Entry] GROUP BY [Employee No_] OPTION (RECOMPILE, MAXDOP 1) SELECT [Employee No_], SUM([Quantity Hour]) AS [Total Leave Left] FROM dbo.[Company$Leave Entry] where [Absence Type Code] LIKE 'Leave' GROUP BY [Employee No_] OPTION (RECOMPILE, MAXDOP 1) 
Load two databases up. Run the tests. Note the IO differences. 
In my opinion I'd be cautious and use the artificial keys. I would only go down this route if you place unique constraints on the natural keys, which it looks like you have done. I think this provides a few benefits. First, it makes query development a bit easier because you join conditions between tables will be less complex and easier to maintain. Second, even though something might appear to be a natural key at first glance sometimes things change. I think a surrogate key design allows that flexibility. Again, I would only do this if I have unique constraints defined on natural keys. 
It depends, if you are using a keylength that is 200 characters, you're index is going to be huge. 
There is definitely an added cost due to the fact that these indexes have to be maintained when there is any DML but I would say its always worth the cost for integrity. As already mentioned you can test both configurations, but I wanted to highlight the other things to think about. Finally, given the tables you've provided I would guess they are loaded once and read many, many more times than they are updated. Hope this helps!
possible back door issues as well...
&gt; ...it may be a bit advanced for this board... Them's fighting words :-p
Your natural key proposal is way more appealing than the original useless keys plan. A few thoughts: * Most of your "Natural Key" proposals have less indexes for the RDBMS to maintain than their surrogate key counterparts. * Your "Natural Key" LocationSubdivisions table has the wrong data type for CountryCode * Your LocationCities table should have its primary key ordered in reverse, so: Country Code, SubdivisionCode, CityNameSlug. (The order of composite key fields matters when using "real" storage engines like InnoDB. Additional reading: "clustered indexes") * Your comment "I know varchars in primary keys can be problematic compared to just ints" misses one really important fact: The RDBMS has to enforce unique constraints across your varchar fields anyways, might as well make it enforce only the ones that matter. Just my $0.02.
For the record, I've been working on SQL Server since version 6.0... And today I learned. Thanks!
This is why I love Reddit.
I have just saved this comment. I have a feeling I'll be using it. :)
Also, be aware of the maxrecursion query hint. Default recursion depth is 100, maximum 32767.
Did you ever get around to doing 70-433 - MCTS: SQL Server 2008, Database Development? Thats the one I've opted for, for the time being. Any hints of tips? I've got the Microsoft cert book, any other recommendations?
You'll need to get the current date using [GETDATE()](http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.ase_15.0.blocks/html/blocks/blocks149.htm), but without the time. There's many ways to do that, one of them involves [converting](http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.ase_15.0.blocks/html/blocks/blocks125.htm) GETDATE() to a string and specifying the date format 101 which is the short date with no time (i.e. 6/11/2012). And then convert that string back to a date. That would look like this: declare @date1 smalldatetime select @date1 = convert (smalldatetime, convert(varchar(10),getdate(),101) ) Then you can take that variable and add days to it using [DATEADD()](http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.ase_15.0.blocks/html/blocks/blocks136.htm). update myDB.object set effective_date = DATEADD(dd, 1, @date1) where object_seq = 1 And so forth. 
Thanks, this is really helpful.
Nope, pure .bak file generated by BACKUP DATABASE 
Whoops
I am currently preparing for * 70-433 - MCTS: SQL Server 2008, Database Development Legohax is correct if the business involves quite a bit of MSBI stack. But if the business does not involve SSRS/SSIS, 70-433 should be enough as the 2008 version is a pretty solid database in my opinion, and AFAIK there are not many non-trivial changes in the 2012 version to the core T-SQL except Sequences.
That guy is amazing, whenever I need to do something crazy I search his site first. He has some very 'think outside the box' type solutions which have saved me on a number of occasions.
Thanks for the info guys! Im up to scratch with most of the technical aspects mentioned but there's some good stuff I'll be reading up on. It may sound basic but bear in mind I have been doing this for exactly 6 months with zero SQL experience prior to that.
If you have passion, you will succeed. Just don't lose that passion! Also, report back and let us know how it went. Good luck!
I typically use table variables for relatively small operations and temp tables for large data sets that would benefit from an index since the only indexes you can create on table variables are unique ones.
Table variables are not just stored in memory. They can and will be written out to TempDB as required. 
That won't work in MSSQL. You can't selectively change the datatype of a column on a row-by-row basis. CREATE TABLE #test (col1 int, col2 varchar(100)) INSERT INTO #test VALUES (1, 'current') INSERT INTO #test VALUES (2, 'No data received') INSERT INTO #test VALUES (3, '2012-06-13 16:33:00') SELECT col1, CASE col2 WHEN 'current' THEN 'current' WHEN 'No data received' THEN 'No data received' WHEN null THEN null ELSE CAST(col2 AS datetime) END FROM #test DROP TABLE #test &gt;Msg 241, Level 16, State 1, Line 1 &gt;Conversion failed when converting datetime from character string.
THIS also your hardware matters in this choice, how much RAM, and HDD IOPS, taking into account baseline load on both. Environment matters, SQL benchmarks aren't like programming language benchmarks where CPU load is primary.
Using a "having" clause is the straight forward language provision for this requirement.... your subquery method can suffer from slower performance, though it'd only be noticeable on large datasets. I'm gonna have to try out the self join, I've never done it that way, and I'm curious about the performance in comparison to a having clause
Both the subquery and the self join allow the DB to take advantage of any index on the attribute field, while the group by clause is going to require a full table read. As you say though, it depends on the size of your table (and whether you have indexes).
If it was a pure network issue (I.E. no changes to the server or AD were made) then the DBA is probably full of it. I cannot think of any scenario where a network outage would cause a SQL Server to drop a single login. That being said, SQL may deny an AD integrated login if it doesn't have access to the AD servers, but that would fix itself as soon as connectivity was restored (and wouldn't drop the user from the SQL logins or any DB). 
Sounds like bullshit to me, but it also sounds like the wrong person is answering the question. Well, unless your DBA also runs your domain controller(s).
DBA doesn't run domain controllers. Those admins say no changes were made, but they're also infamous for denying everything.
That's exactly what I was thinking. The resolution was actually adding granting the user permissions, and it was fixed immediately.
It seems possible that he would be denied access if the server couldn't hit the domain controller, but I suspect that would lead to more problems than just this. From the way you've worded it, it sounds like the DBA honestly doesn't know the answer, but is being called on to give some kind of diagnosis so that's the best he can come up with. For what it's worth, a network outage should not *permanently* remove a person's access. Something else would have to change, such as AD group membership or some configuration on the server.
basic idea: split into true and false international shipping tables, with shipper and target. split the falses into country tables, match all the falses based upon id (using rand() tied to the pk, maybe?), any unmatched falses become targets for trues. match the trues together and if there are any left over, have them ship to falses.
**That's a very good point.** We don't have a single AD group added to SQL anywhere though, so I'm thinking he just removed it. His expression when the system went down made it seem like he made a mistake. I have no problem with mistakes, just own up to them, so everybody can understand and we can talk about how to prevent it in the future.
From some of your other comments it may not be his mistake to own. You did mention that your server team likes to deny things. Still I would rather have some integrity, say this is what I changed to get it working but something upstream had to have changed to require this, and let the chips fall where they may. I screw up all the time. I don't have a problem owning up to it when I do. As a result when I say I didn't screw up, people believe me.
This was our most important production database with our most important production user that was removed. Killed the entire company, yikes.
If he restored from backup for whatever reason, the condition that arises when you do this leads to *all* users losing prividges. Of course, he may restored what he thought were all of them but missed or misspelled one. Again, *if* this was the issue. 
Damn it: &gt; Msg 8189, Level 14, State 6, Line 1 &gt; You do not have permission to run 'SYS.TRACES'. I'm going to get another guy who has permission to run it when I'm at work. Thanks so much for this though, this is killer if we can get to the bottom of it!
Thanks alot for all the help guys! I had my first interview this morning and used alot of the stuff you guys have suggested. It's been a productive week! I got a call from the recruitment agency saying the company was really impressed with how I came across in the interview. Having a meeting with the Directors sometime next week to discuss the job further. THIS JOB IS MINE.
There's no need at all for a cursor and they're super slow which (especially for reddit) would be bad. Interesting problem, though. It'd have to go through a proc because I'm pretty sure you couldn't run this through one query since you'd have to update the table you're writing to in order to know if the person was sending gifts to another country and also to make sure they didn't exchange gifts with the same person. And then you'd have a real mess on your hands trying to update the damn thing. 
Essentially you can do the same thing in Oracle. I'd recommend starting with the [Oracle XML DB Developer's Guide](http://docs.oracle.com/cd/B28359_01/appdev.111/b28369/toc.htm). That guide covers nearly everything about what you can do with XML in Oracle. Oracle has a large number of XML related features so you shouldn't have a problem. I'm on my mobile right now so I can't provide any examples but the link above will have plenty. 
Thanks, checking that out now. I know the syntax is actually pretty similar, something like:(From memory so some of it missing but basic structure is there) CREATE OR REPLACE PROCEDURE PARSEXMLTEST (IN_XML CLOB ) IS BEGIN INSERT INTO XMLTEST(USER_ID) SELECT x.userf --INTO V_USERID FROM XMLTABLE('$i/ROOT/users/userf' PASSING XMLTYPE(IN_XML) AS "i" COLUMNS USERF INTEGER PATH '.') AS X ORDER BY x.userf ASC; END PARSEXMLTEST; Is it a safe assumption that the application would be able to pass the full document whereas I'm attempting to test xml documents and parsing through sql developer? If so, is there a trick to test it through an anonymous block? I've tried just setting a CLOB/XMLTYPE variable to the document and I've also attempted setting a bind variable to the document but same string literal too long message. (Though I make no promises I did the bind properly) i.e. (again from memory so probably off a bit as i usually have to type it out and attempt to run to get a parsing error if there's a problem) var v_xml CLOB; (or XMLTYPE - same issue either way) BEGIN :v_xml := '&lt;ROOT&gt;&lt;transaction.....really long doc&gt;&lt;/transaction&gt;&lt;/ROOT&gt;'; CALL PARSEXMLTEST(:v_xml); END / And on execution I get the string literal too long message when the document size exceeds 4000 characters. Thanks again. 
The application shouldn't have a problem. CLOBs can be gigabytes in size. It sounds like you are having client side issues in your test. I would recommend storing your XML document in a file and using the Oracle XML APIs to read it into an XMLType object. Again sorry I can't provide an example at the moment. 
Download MySql, derp around with it for a while. Think up a mini-project to do, like making tables that describe stuff around your house and writing queries that summarize everything. In other words, actually work with it and you won't need to bullshit. If you're just looking for basic knowledge, the type of SQL you're using shouldn't matter.
Or SQL Server Express Edition, since he did mention SQL Server.
I always found www.w3schools.com to be an easily digestible resource for mssql. Give it a whirl. Download sequel server express edition and the adventure works db and have a mess around!
I know the company I work for is doing some interviews for SQL Server DBAs and these were 3 of the most common questions they were asking.
[Here is a similar post from a few months ago](http://www.reddit.com/r/SQL/comments/sgyzi/minimum_skill_set_to_be_a_jr_dba/), and [here's my response](http://www.reddit.com/r/SQL/comments/sgyzi/minimum_skill_set_to_be_a_jr_dba/c4eba31).
I appreciate the responses this should definitely put me in the right direction. Thanks again for your time 
Id also google "sql server interview questions" most companies ask pretty common questions. 
Archivebot
why you didn't eliminate redundant data in original query first
I work as an intern now but I hope to interview for a full-time Junior BI developer at the end of the summer. My boss likes me and has dropped hints for weeks now about what kind of questions he asks in an interview. Your list is very much in line with what my boss has been suggesting. Thanks so much for taking the time to post your thorough answer. I know I'll refer back to it on the weeks to come. :)
Great answer. Ill be interviewing soon just like OP and this info will definitely help. Just being prepared will help with my confidence, which will help overall. Thank you.
Congrats on having a great interview! Good luck, and keep learning! :)
As another poster mentioned, you can do find/replace or use a regular expression. I'd do some benchmarks in your personal environment to see which is faster but I'd imagine that a regex (if you can figure one out) would be faster.
That did it! Thanks!
I have the opposite issue, I have a free form text field with a lot of text and I'd like a new row to be created for each carriage return in that string. Anyone have any ideas? 
Also beware of tabs and multiple spaces.
PostgreSQL: http://www.postgresql.org/
Maybe firebird... http://www.firebirdsql.org/
You can distribute MySQL as long as you also provide the source.
Express should be fine as long as your database is under 10GB. If your DB is for a small business, this should work well.
I just checked and the Index in the substring are correct (1-13 not 0-12) The code that generates the substring uses a hashtable with 13 keys based on 2 letter province code so I know that when the user clicks BC, MB and NS those are always 2,3 and 6 respectively. So really the only thing that can be wrong is the AND/OR. So I am back where I started. What I want is; * Because 2,3 and 6 are ticked by the user and because Changed for 2,3 and 6 are 1 that item should not show up. * If 4 is also ticked by the user, because Updatedbysteph is 1 it should not show up * But if 9 is ticked, because it's 0 for Changed and for UpdatedbySteph, it should show up.
You bet, are you using BIDS to create the reports??? Simply click on Report &gt; Parameters, add two date parameters and then modify your query. ProTip: If you enter a query such as select * from users where hiredate &gt; @StartDate in the query builder it will automagically add a parameter to the report
It looks like you can schedule the test now by following the "Schedule" button on the link in my post, but there doesn't appear to be any training materials available for it yet. The Prometric exam centers list it as available to schedule.
Just to touch on that further... Once you've put the parameters in your query and they appear in the Parameters section on the left of the screen, make sure you right click on each parameter and set them to date/time. Upon running the report users will be able to specify dates from a calendar.
When you add SUBSTRING(Changed,2,1) as a column in your set, it produces '1'? is the same true for SUBSTRING(Changed,3,1) and SUBSTRING(Changed,6,1) ? Just put the rest of the columns you check in your whereclause into the select as columns and check which one is wrong.
I'd recommend instead setting up two columns: "day" and "price," and let each day correspond to a number. Add an "Item" column as well, if necessary. Then query (for Tuesday, for instance): SELECT * FROM Specials WHERE Day = '3' then you just need to change the number in the WHERE clause when building your query (you could also use text for days, but it is better practice to use numerical indexes)
Instead of having a column for every day of the week, why don't you instead make a column called "dayOfWeek"? This would make the query much easier: $query = "SELECT \* FROM Specials WHERE DayOfWeek = '{$dayOfWeek}' AND SpecialPrice IS NOT NULL" Dislcaimer: If you're new to PHP, you *need* to fully understand how to build safe SQL strings before you ever inject a variable into one, as this example does.
Already did that, that's how I made sure that the index was from 1-13 and how I confirmed that my province positions were correct. From the previous example I posted Item ABC has the following strings; changed is 0110010000000 Updatedbysteph is 0001000000000 So based on that * BC; SUBSTRING(Changed,2,1) is 1 * MB; SUBSTRING(Changed,3,1) is 1 * NS; SUBSTRING(Changed,6,1) is 1 * BC; SUBSTRING(UpdatedBySteph,2,1) is 0 * MB; SUBSTRING(UpdatedBySteph,3,1) is 0 * NS; SUBSTRING(UpdatedBySteph,6,1) is 0 additionally * NL; SUBSTRING(Changed,4,1) is 0 * NL; SUBSTRING(UpdatedBySteph,4,1) is 1 And * ON; SUBSTRING(Changed,9,1) is 0 * ON; SUBSTRING(UpdatedBySteph,9,1) is 0 The only time something should show up is when there is at least one result set of; SUBSTRING(Changed,X,1) and SUBSTRING(UpdatedBySteph,X,1) that are both 0 I'll abreviate the substrings by the first letter of the fields here for brevity but lets say you have; C2=1,U2=0 | C3=1,U3=0 | C4=0,U4=1 | C6=1,U6=1 The item should not show. C2=1,U2=0 | C3=1,U3=0 | C4=0,U4=1 | C6=1,U6=1 | C9=0,U9=0 now the item shows because there's one instance of C and U both being 0 I just can't figure out the AND/OR logic to produce that or if it's even possible.
you can use a database to dynamically populate those fields too for many reports on a schedule using a table to feed parameters
Im currently using razor sql for connecting to the database Im suposed to export. I dont have the administrative right to deatatch the database, only to export tables when using razor sql. how should I export the data? and how can I make use of it?
You should have an Orders table that contains all the relevant columns necessary for an order including a Customer_ID that would link to your Customers table. Your customers table of course would have all of the customer's information.
Do you have the access necessary to do backups? If not, then contact someone who does.
The other thing you may be asking about is the CASE statement Select a.col1, Case a.col2 When 1 then 3 When 2 then 8 when 3 then 9 when 4 then 0 else -1 End From tablea a Also, Select Case when a.col2 = 2 And col3='f' and @blue = 'no' Then 1 when a.col2 = 3 and col3= 't' and @blue = 'yes' Then 2 Else 0 End From tablea a 
thanks much, amaxen - these answers are a great starting point for me.
1. Save data in Raw .. which mean without this computed columns. 2. Write a view with all columns + computed columns. 3. Access data via view. in this way you can have very complicated calculated columns .. referencing more than other columns in same table, you can do verification/validation/data extraction from other tables or even other databases.
You should read a book about database design. This is helpful advic.e
This is the correct answer.
openstreetmaps? you'd have to massage the data a bit, but it should be useful for your purposes.
Thanks for the reply. I am actually building a website and I want users to select their location from drop down menu, or even better have suggestions pop up as they type. I can solve this with tags and let users enter their location into database, but I am afraid there must be lot of places that have same or similar names, I know few examples. Does anyone know better way to do this?
I have US cities via zip code and state. That was a bastard to find. If you find it can you reply to my comment so I can have it as well?
If the permissions are setup by AD groups, then anyone with access to AD could have broken the security.
You only have to distribute MySQL if you **modify** the source. Otherwise, mysql.com has you covered.
Postgres is more standards compliant and a little more intuitive to use and configure IMO. It's definitely a good choice. 
sweet. why does the usps charge for their info? $400 bah. any thoughts on geolocation for free? i want to have their ip, dns, AS, geolocation to display on our webstore so i can track anomalies like tor 
the result sets are stored in sql server 2008/r2 Change Data tracking? select * from tableA inner join tableB from noon ? isn't that the point of change data tracking to basically go back in time? Yes boss here's your TPS report group by minute for the last 4 years. Took a minute to run :) 
Microsoft has this database called adventureworks - its a full blown enterprise level store - check it out. 
Not going to have an insanely high workload? Stick with Apache. You may need to use one of the many available mods one day. Changing the config file is a lot less work than changing out your webserver. FWIW, I have a 256MB VPS that does Apache + Mysql just fine, so your performance should be great. After toying around with it for awhile, I do wish I would have went with Postgres or SQLite instead of Mysql.
I'm not positive of the syntax since I mainly work with MSSQL, but I believe this should work in MYSQL. Here is your first query rewritten using joins instead of the sub-selects: &gt;SELECT t1.accountnumber, t2.closeinfo &gt;FROM account t1 INNER JOIN account t2 ON t1.accountnumber = t2.accountnumber &gt;WHERE t1.stage = 'Opened' AND t2.stage = 'closed' &gt;AND t1.date &lt; '2012-06-23 00:00:00' &gt;AND t2.date &gt; '2012-06-23 00:00:00' Here is the query that will let you find accounts opened before a specific date that do not have a corresponding 'closed' record. &gt;SELECT t1.accountnumber FROM &gt;accounts t1 LEFT JOIN accounts t2 ON t1.accountnumber = t2.accountnumber AND t1.stage = 'Opened' AND t2.stage = 'Closed' &gt;WHERE t1.date &lt; '2012-06-23 00:00:00' AND t2.stage IS NULL I haven't tested this anywhere but I think it should do the trick. 
OK - I get that there isn't an actual status of NULL. The 'where t2.stage IS NULL' part is to locate the records that fail to find a match on the left join. The left join is trying to match the 'opened' status with the 'closed' status on the same account. If a 'closed' status isn't found for that account, the join isn't matched on that row and instead all of the data for T2 in that particular row will be NULL. The results are limited where t2.status is null since those are the records you are looking for. I'm not sure why you are getting too many results. As far as I can tell, the date part looks good and shouldn't return anything that was closed. Without knowing what the data looks like it is tough to know why. What I would try is starting the query with 'select *' instead of 'select t1.accountnumber'. That way you can see all of the columns that are part of the query and hopefully you can troubleshoot whats going on from there. 
There's absolutely no reason to use a real table like that. A temp table can have a primary key and indexes. (table variables can have primary keys but no non-unique indexes). EDIT: At least in MSSQL anyways. I'm not sure which language the you or OP are referring to since this post isn't tagged. OP: If you were told to incorporate views into your stored procedures as opposed to temp tables it's probably because you were using temp tables when you didn't need to. i.e. as a temporary storage for a complex query which you then selected from. Perhaps it would help if you showed us an example of a stored procedure where you were told to use more views and less temp tables.
How did the follow-up meeting go?
False. If your purpose for using temp tables is because you're going to be querying them multiple times in a sproc, 1) that data is going to be preserved in memory after the first read from disk unless you're talking about *really* large datasets, and this will happen either way. By using a temp table you're not really improving performance for a second read pass, but you are basically pushing existing pages out of the memory pool with data that's going to go obsolete when the temp table disappears. 2) requerying a temp table is going to be slower than an actual table unless you also build in indexes on the temp table, which will increase the size and and processing load to populate the temp tables in the first place. TLDR: For either reason, building and requerying temp tables is almost always going to be more inefficient than simply directly querying the base tables. If you're working on small datasets this isn't likely to rear its head. But it's a bad practice because sooner or later you'll write a 'small' database that eventually grows up into a large dataset and you'll start having bad problems as a result. 
No. Temp table can do things that views can't. However, it sounds like you are using temp tables when you don't have a good reason to. They can be abused like anything else. A good reason would be to save the results of an expensive query if you need to reuse those results multiple times in a stored procedure. for example, a search function on a website that returns 3 result sets, 50 details rows, a total count, and a count by category. If the query to find the matching rows is reallly cpu intensive, you can trade tempdb IO for cpu cycles, which may be a good tradeoff if you are cpu bound. Should you use temp tables? It depends. temp tables use resources in tempdb, therefore have a "cost" associated with them. Use them when the benefit out-weighs the cost.
What's the point between temp table and real table ?? Both of them are writing to HDD with same IO cost. With real table .. IO could be customized and get better performance if the physical data files are on customized hardware. TempDB on the other hand wouldn't be treated in this way for most of the cases. And using TempDB will effect the entire server performance unless your server serve only one database. For multiple databases environment, using tempDB too much can cause HDD space issue .. unless you specified the tempDB on special storage. e.g : Your SP use (read/write) 5GB on tempDB and 5 more other databases run (read/write) tempDB on the same scale .. all those read/write IO are getting bottleneck on one database (aka) one storage device. If each and every database are on their separate storages .. IO cost will be shared among devices. This could improve processing performance. . If possible, avoid using temp tables and table variable ..
Totally agree with you .. I usually avoid touching tempDB since most of the servers host that database on their primary HDD and all other databases are sharing at the same time ... so for me tempDB is like a huge whore-house.
Look at it this way: Select query done right: 1 read from table. Temp table: Read from original table, write to temp table. read from temp table **plus** the overhead cpu and time that any indexes are going to impose on you. So just logically right there, you're doing three operations opposed to one. Which is more efficient? 
I think you need a temp table like: select A.key, comparison = datediff(ms,b.time,a.time) into #temp1 Then you can select the row with the smallest comparison for the final table.
i didnt even kno you could do that... THIS IS WHY I LOVE THIS!!!!! haha thanks. Will try that, sounds like it will work in theory.
The code I posted in my top-level comment does exactly that ... except I assumed timestamp was just an integer field, so I used plain old ABS(date1-date2) instead of datediff(...). 
First you need to understand RDA ( Relational Data Modeling ) and then you would understand my simple example. **Customer(s) -= Order Header -= Order Lines -- Item/Product** * -= for one to many relation * -- one to one relation Four tables for common order processing. * Customer ( Name, Address, Phone etc etc. ) * Order Header ( Order Date, Delivery Date, Tax, Discount, Grand Total ) * Order Detail ( ItemCode, Quantity, UoM, Price, Discount, (opt) LineTotal ) * Item/Product ( ItemCode, ItemName, List Price, UoM )
A bit off-topic, but IIRC, you will need artificial (integer-based) keys if you plan to use this in SSAS. If this is NOT the case, I would always go with the natural key option. Just feels more natural to me (no pun intended). (20+ yrs DW experience with Teradata...) 
select * from comments a left join following b on b.user_id = a.user_id where b.follower_id in ('who you are following 1','who you are following 2')
This gives you all comments of every user you are following SELECT * FROM comments WHERE comments.user_id IN (SELECT follower_id FROM following WHERE user_id = $my_user_id)
Check this post out. Look into commit and rollback. One thing you might be missing is installation of a SQL DB. http://www.reddit.com/r/SQL/comments/qc3vd/can_you_show_me_a_website_that_will_help_me_with/