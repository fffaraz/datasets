SQL cursors...the calling card of the application developer who was thrown a DB Dev task and told to make it work. I've all-but-banned any kind of non-set-based looping in our production code because of how horrible it always turns out. There are just so many better ways to do something. If you need a loop in your SQL code, you're probably doing something wrong.
I use a cursor for the SPROC that handles the initial user creation for one of our applications, but the security model of that application is very... obtuse. Basically, the stored procedure has to read a list of users from a group in Active Directory. Then, for a new user, the application user id has to be mapped in a single table with their Windows login, and then a record needs to be created in a second table with default security information. Seems good so far, right? Well, you also have to create a server login, create a database user, add the user to a role, create a schema, set the schema as the default for the user you just created, and then create 5 views for the user. Since DML can't do DDL.... it's a cursor. 
Man, thank you for your reply, that is illuminating. I'll check more into this when I get on the laptop. just got mta certified in database fundamentals last week, sql rocks â˜º
You can really never go wrong by focusing on ANSII SQL. It will work on all of the big platforms. Then the only thing you need when changing to a new Db platform is learning the specifics of that platform. While there are many good books on the subjects I'm a really big fan Joe Celkos books on the sibject.
&gt; business requirement states that the script must run nightly It does run nightly, the business requirement didn't say that it has to complete before the other script starts. On a side note, where I work, at major-telephony-company in Ontario, we had a script that a developer wrote that took 5 days to run. In the above scenario, one of our Junior SQL developers looked at the script, got really confused at why a senior SQL developer would write such tripe, rewrote it, had it vetted by the other senior SQL developer, and denied the change. The legacy sql developer cried to management, management said "WHY ARE WE NOT RUNNING HIS SCRIPT. THIS IS IMPERATIVE THE BUSINESS HAS THE INFORMATION IMMEDIATELY AND EVERY MINUTE COUNTS." As a group we told Mr. Point-Haired-Boss that the script which was scheduled to run (against production--not a reporting database) would cause issues because it would run for 5 days. The manager responded that there are many scripts that run for more than 5 days and that is expected. We said no, that's absolutely unacceptable. We've rewritten the script to run in 25 seconds. The PHB decided that our script was inferior in some way. Another manager stepped in and said "Listen, this is not going to happen. We are not running that script against production. They can either run ours, or they can get a letter from the CIO of Technology saying why this is a good idea." A day later it was decided to run our script. They got the results with in 5 minutes of the response being approved. This is what I deal with every day.
Interesting - sounds like that'll work. But one issue is how to grab the email address as a variable to communicate the job(report). Would need to first dump the email contents into a local table and reference against the query. Or setup jobs for specific users in the organization who would run on-demand. Thanks for the tip!
That argument almost makes sense but falls apart in a few ways that I'll list: * It's not my job to teach people who have been working at their job for 15 years. * Why would an employee who makes almost double what you make listen to your input? * You can only hold their hands so many times and show them "new technologies and methods" until your voice box shuts down. If a developer can't understand why a 10 line script with 2 select statements and one insert, works better than their 756 line script that has 30 cursors and 60 temp tables (with delete operations removing items selected into the temp table during the script), everyone has lost.
SQL is a language for querying databases. Essentially you structure a question about a specific data set and send that request to the server. Underneath it is optimized and matching data is returned. Basic query below: SELECT fieldname1, fieldname2 FROM tablename WHERE fieldname1 = some value Explanation: SELECT- List of columns you want returned from the query FROM- the source of the data WHERE- a filter for specific data It can get much more complicated depending on the question you are asking. But that's the basic ELI5. 
Thank you very much for this explanation. This, along with a couple introductory videos has helped me grasp it better
Who is complaining about the language? English can instruct a person to jump off a bridge, nobody blames English for suicides.
SQL is a way to tell the a database what to do with data. You can tell the database to read data, create new data, change data, or delete data.
I don't see how a script taking a full business week is considered "acceptable". What happens if there is a problem with the server or certain records and the script needs to run again? That's an entire business week lost.
SQL is also a language for describing the data (DDL). 
True. And for creating/modifying the data as well. 
Awesome. Would you mind discussing your thought process in choosing a recursive CTE for the job?
To break it down even more for the novices. Let's look at it like an excel file. The file name is the database. The tables in the file are tables in the database. The columns on each table in the file are in turn the columns on a table in a database. so select columnA, columnB from tableA where columnA like '%foo%' Is doing a query against your "file" called mydata.xlsx assuming you have mydata.xlsx opened. This is how it's helped me understand anyways.
If the commands are static, then I don't think it needs to be near that hard. Create a standalone batch file that runs the command you want. If there are 10 different commands that might get called, make 10 batch files. Have a different subject line call each one. If the commands aren't static and you need to get info from the email (for example, get info about a specific customer) then you might need some powershell trickery to grab that info. Or, as someone else noted it may be possible with VBS howeve I don't know much about that. 
I'm glad you mentioned it. The helpful part is where you correct it. That's what would make you a valuable member of this community. 
I've been working with SQL for a almost a year and but still having some difficulties and your explanation really help me out, thanks. 
At its most basic SQL allows you to perform the following operations on a database table: Select - look at tables Update - change data in your table Delete - remove rows from your table Insert - add rows from your table It is a 'declarative language' where you perform set based operations rather than doing things the RBAR way (row by agonizing row) like you would in a for-loop. Basic SQL is much easier to learn than most 'imperative languages' like C. There are no libraries. Most modern variants allow procedural programing (imperative + declarative) but don't get sidetracked and start using while loops( or cursors). SQL is set based.
I found that if you create an index on the primary key with an include of the column required and other required data (ex. timestamp) recursive selects/temp tables were much faster outside a few one-off cases. Edit - You can also be clever with windowed functions in SQL Server 2012 onward using unbounded preceding rows and current rows for filtering. 
You're looking for some sort of export / import utility, but you would need to decide exactly what you want in your clone database. eg. You could script out the schema etc, then use something like integration services to import a subset of data from your tables. You should be aware that different amounts of data from production will probably cause your queries to behave differently, so not a good idea if you're using it for performance testing
Sorry, I should have mentioned in the above post, but yes. I could just script it however I'm really hoping that this is a problem with an existing solution. 
OK, so no then - partial backups work on filegroups, so not unless you have a read only filegroup that you can leave out (eg, archived data) You will need to do a bit of legwork on this I'm afraid ;)
I did the SQL Server MCSA bootcamp [here](http://www.certificationcamps.com/), highly recommended if you use SQL Server. However I would recommend having some background knowledge before stepping into one of these, the bootcamp's very intense and its a lot of information in a short period of time.
SQL is easy. It's pure logic. You can probably learn it yourself with W3Schools and a copy of your database to play with some real world problems. I would recommend you take classes for the specific type of database your company uses, because being a DBA is much more complicated than simply writing efficient queries. 
IBM Optim is there if you're looking for an off-the-shelf very scalable solution (it supports non-declarative data integrity, data de-identification, obfuscation, etc.) http://www-03.ibm.com/software/products/en/infosphere-optim-test-data-management 
I would align your training with a specific product at least initially. I think it gives a broader perspective on the toolset and applicability. SQL Server: https://www.microsoft.com/learning/en-us/sql-training.aspx Oracle: http://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPage?p_cat_id=178 
if you set up file groups, can you manage which data goes to which secondary file. I've found a few articles online but it seems like something you cannot manage directly.
I don't think that's a correct answer, I think you need max(r.stars) in your select list? I don't have access to query this particular dataset so I can't say for sure. And I don't think you want that having statement. The join will eliminate movies that have no ratings. The having statement will eliminate movies whose max rating is 1 star, which is not what was called for.
Do you think with an MTA in SQL Server fundamentals, that would be enough to take part in a boot camp like this? https://www.microsoft.com/learning/en-us/sql-training.aspx 
To be completely honest, you'd pass the bootcamp even with no prior experience (there were people in my class who barely knew what SQL was and passed it) however I think the people who knew a bit before starting took a lot more away from the class than those who didn't.
I believe it. I have some non-work related experience with SQL (home database of books). And I've had an interest in SQL for a few years. It feels good to finally work at a company that wants to help me develop that skill set. Thanks for the input.
So just an update on this. I actually ended up using MS Access and making a form with a linked table to the actual SQL DB. It works well after I tweaked the code in the form I used to add the data to the table. Needless to say, its not the most professional app but its definitely a solid work around to coding one from scratch in VB, C#, or C++.
This is the best book to learn SQL properly: [Beginning SQL Queries: From Novice to Professional](http://www.amazon.com/Beginning-SQL-Queries-Professional-Professionals/dp/1590599438) The important part is to run the examples yourself and play with it. Make up your own queries and figure out how to get them to work. That's what you'll be doing all the time anyway. After that, pick any of the books for your DB vendors SQL dialect. But first, get the fundamentals right. 
What's your role? Database developer, or DBA? Very different training for the two paths - the difference between properly *using* the database, and properly *managing* the database.
Now it's Ok only with numbers, bd updating... What's the problem??....
I don't have a specific role yet. I am just working with the Database/BI groups on some projects. I'd imagine I'd probably get my hands into administration to start and then look at development down the road.
I had one MySQL class in college and that was all the learning I needed to do to be able to properly figure it out. The database I work with is MS SQL, but they are close enough that you should be able to learn either and figure out the query writing. Always learning, always getting better though. My role in my company is ERP specialist and I do a ton of SQL reporting against our live data. It is hosted though, so I don't have to maintain the database itself. I also noticed you mentioned working with the BI group at your job. These guys are going to know a shitload about SQL writing, so bounce stuff off of these guys often.
Thanks for the advice. It's scary how smart these guys are. I'm definitely going to do all I can to make a good impression and soak up as much as I can.
Will you be able to discuss the queries with the people who wrote it ? This type of assignement is not so easy, and it all comes down to experience... There is no universal way of doing it, and I doubt you'll find documentation on it. Nonetheless, some questions and pointers to get the discussion going : * What will be your exact task ? Optimization ? Debugging ? Simplification ? * Do you have a good functionnal knowledge of the databases they are working on ? Then : * Try to analyze the query without any help, only with a general description of what the query is supposed to do * Does the query does what it's supposed to do ? * Does the query return its results in an acceptable time ? * Would you have written the query differently ? * Do you immediately see basic improvements on the query ? (factorization, etc.) * Then you can discuss with the person which wrote the query : basically why did they wrote it the way it is. Avoid criticizing their work at first sight, assess their technical abilities and understand the way of thinking that led them to write this request. * At this point you should have understood everything in the query, and why it was written this way. Now you can write down a critic on the query, and eventually modify or rewrite it. A few precisions : * The pointers above are related to what it think to be similar work I did in the past. Maybe it doesn't fit your situation... * English is not my native language, so don't be harsh on grammar mistakes !
I too would like to know the answer to this. I've been supporting a suite of reporting software and then writing reports for management for almost a decade. My SQL-fu is pretty good, but I don't know how to find a "next step" career. 
You in Philly by any chance? We're always looking for talented SQL developers - and a big part of what we do involves transforming and massaging data.
 SELECT part_no, MAX(CASE WHEN brand = 'A' THEN 'Y' ELSE 'N' END) AS brand_a_flg, MAX(CASE WHEN brand = 'B' THEN 'Y' ELSE 'N' END) AS brand_b_flg FROM part_table GROUP BY part_no ; You want only one record per `part_no`, so the GROUP BY should make a fair amount of sense. For each record, calculate a flag for brand A and brand B. Once we wrap it in a MAX, Y is selected over N.
There are a couple ways to do it. select case when count(case when brand = "A" then 1 else null end)&gt;1 then 'Yes' else 'No' end FlagA or you can probably do it with a Pivot. The real gotcha is that all the scenarios, that I can think of, require you to know which brands and materials you want before so you can specifically return them as a column. A Stored Proc can build a variable number of columns for the resultset, but it will be a bit complex and not report friendly. 
Are you backing up your log files?
Do you have a job board (web page)? I'd love to see what the position name is and what you list as the requirements.
I will send you a private message.
ditto
BI and/or ETL
Also if you're looking for a conversion job PM me. My company is hiring for a remote position. 
They listed the position as a "Senior Programmer" Summary: Develop scripts for use by Implementation Managers to automate upgrades between major software releases. This position is responsible for developing SQL procedures, VB.net processes, and Crystal Reports based on client requests. This person will also contribute to client issue research requests in support of &lt;company&gt; client service teams. Requirements are basically a tech Bachelors and experience with SQL as a MSSQL admin. Don't know what they pay, but it's a nice looking job.
This is a great suggestion and is much more in line with my skill set, thank you! I was brought up to my position by internal promotion so my knowledge of job search/field terminology was limited.
As another alternative, especially if you want to branch out into non-DB programming, would be ERP software development. I'm working on a program called "Infor SyteLine" which is built on top of an awesome development platform called "Mongoose." The whole platform is very database dependent, which makes a SQL background very powerful.
Stop. Read up on recovery models and transaction logs. Start with MSDN include Paul Randal blog http://www.sqlskills.com/blogs/paul/ Increase your knowledge Check your backup strategy is correct, consider file growth settings and your systems usage patterns Make no changes until you understand the problem. Don't be afraid to ask for help. #sqlhelp on twitter and SQLServerCentral or dba.stackexchange are useful places for you I am not criticising, but trying to help
This. Hope for comments, but don't trust them. 
No worries about criticizing me, no ego here haha. I understand the transaction logs, we do daily Veeam backups of the server, and planned on putting back to a full recovery model after. However I should maybe read up on this as well. I am backing up my log files as well, as they go to another server share. 
Investigate transaction log back up and shrinkfile on the log file and then proper resizing and the effect of that process on availability groups ? Better than changing recovery model.
And a great company. Been here 9 years next month. 
Oh! That's kind of similar to what I've done recently in order to load the various file formats we get. I've made a couple of powershell programs to try to automate stuff; one for example to open Excel, modify all the columns to be text only, then import any CSV's which share the folder of the .PS file and save them as .XLSX files. Of course, it's all a bit Frankenstein'd together from other people's examples online, but I get what and how it does what it does. Plus it works. Also, I want your job! Haha Thanks for the example.
this. i know some sql and can do joins..i didnt think this was a big deal but apparently not alot of people can do this shit. 
yep, this. This has been my field for the past 15 years. I work for a consulting company that provides ETL and BI to the telecommunications industry. We have always been busy. There is a TON of data to move and analyze. I have been doing SQL only during all that time. So yeah, a good knowledge of SQL and the concepts of BI and ETL (the latter being essentially what you've been doing already) will keep you nicely employed. 
I've held a variety of jobs in the IT industry for the last 30 years. Regardless of what role I end up in, I always simply say "I'm a programmer". If you're developing solutions from scratch, using a "language", that's programming. So, I've been programming in SQL in for 18 years now. 
I've both worked as a Data Conversion developer and managed teams of Data Conversion developers. As others have already suggested, if you have experience with traditional DC work for one off clients, larger (and repeatable) ETL development is a good next step, especially around the areas of BI (Data Warehousing and other analytics). You don't need to go the DBA route unless you like less coding and more administration. If you really enjoy coding, there are a lot of opportunities for folks who know SQL really well as either Database Developers, Report writers, etc. You can even move towards non-development jobs like Data Architecting, etc.
With SQL Server and strong SQL I'd also suggest looking into Sybase. The few MSSQL DBA's I've hired either came from Sybase or transition into Sybase easily. The biggest technical hurdle is Linux vs. Windows not the dataservers. I suggest Sybase, because while Oracle and SQL server clearly have market share, and nosql gets all the buzz, there is a lot of companies deeply entrenched with Sybase, and Sybase before and SAP now does an absolutely terrible job developing a new generation of DBAs the way MS and Oracle are. TL;DR - Sybase ASE/IQ. It is an easy transition from MSSQL, and Sybase has an aging dwindling workforce and pays fabulously for a good one.
I use SQL every day, Whether it be MS SQL Server, Oracle, IBM Db2 etc.. I work for a bank and data is all over the place. Anyway, I create many SSIS packages for repetitive tasks or anytime i need to creating datasets coming from various sources. As another poster stated many positions state data analyst but can not write straight sql. Usually it is Access or something created by someone else. If you can write SQl you have many opportunities in Banking, Insurance, Health Industry , basically all these industries have a need for report developer. If i have to use excel I will usually embed SQL in the data connections to make refreshing quicker.
If you're starting with administration, check out [Brent Ozar training videos](http://www.brentozar.com/training/) and the [SQLSkills Immersion Events](https://www.sqlskills.com/sql-server-training/)
I would guess it's calculated on the fly against some aggregate of all the answers. They probably distill all the answers down to a much smaller number of categories, with a score for each category and then compare those values. Then you've got some formula that takes the scores for those categories from two users and produces a single value to sort by. You would want to first limit the cardinality as much as possible on concrete, indexable values let keep age range and distance to reduce the number of those calculations required. 
They don't have to calculate all the scores though, only between users geographically near each other. That cuts down the size of the table enormously. Also, I wonder if a probabilistic data structure like a bloom filter could be used here.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Quickselect**](https://en.wikipedia.org/wiki/Quickselect): [](#sfw) --- &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __quickselect__ is a [selection algorithm](https://en.wikipedia.org/wiki/Selection_algorithm) to find the *k*th smallest element in an unordered list. It is related to the [quicksort](https://en.wikipedia.org/wiki/Quicksort) sorting algorithm. Like quicksort, it was developed by [Tony Hoare](https://en.wikipedia.org/wiki/Tony_Hoare), and thus is also known as __Hoare's selection algorithm__. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and variants is the selection algorithm most often used in efficient real-world implementations. &gt; --- ^Interesting: [^Selection ^algorithm](https://en.wikipedia.org/wiki/Selection_algorithm) ^| [^Median ^of ^medians](https://en.wikipedia.org/wiki/Median_of_medians) ^| [^Introselect](https://en.wikipedia.org/wiki/Introselect) ^| [^Introsort](https://en.wikipedia.org/wiki/Introsort) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn4azo8) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn4azo8)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
&gt; If i have to use excel I will usually embed SQL in the data connections to make refreshing quicker. Can you elaborate on this a little further? I am moving into a similar role soon and would love to know where I might pick up skills like this. I am already fairly well versed in SQL, but lacking on the Excel end of things slightly. Appreciate anything you might be able to say about it! 
What do you like to do? Can you communicate well verbally and in writing? Do you have a knack for translating complicated technical details into easy to understand english? Can you break down difficult business processes into simple use cases and functional specifications? Consider becoming a Business Analyst. My best BAs came from MIS degrees, love data, can write SQL with complex joins and cursors and understand the data schema as well as the data itself. These people are worth there salt because they span across the business and technical sides of the business. If you're good, everyone loves you.
Yes!
What does the constraint say? What kind of relationship? Does the value exist in the other table?
With the sql and excel you could go the route of BI which could also be called a data analyst. Use excel as the front end to create executive dashboards.
When you say "SQL queries" do you mean select, etc or whole stored procedures/functions?
Does 0 exist in the other table?
Holy cow THANK YOU m8! I really didnt even expect a response but this is great stuff. Have a gr8 Christmas
Seems likely that your answers to the questions increment/decrement rankings in a series of personality characteristics. More/less religious, liberal/conservative, factors that you value highly in a mate, etc. Writing algorithms to calculate match based on a finite set of int values on the fly doesn't seem that taxing.
Yea, kind of what I was getting at with "limited use", but you nailed it all the way in.
To be honest if this is already in a spreadsheet and if looking up that one value is all you need to so I would probably do it in the spreadsheet using a 2D Lookup of some sort, here is an example I found: http://www.techonthenet.com/excel/formulas/2d_lookup.php Otherwise MickeyFinns has the right idea on the SQL side.
when you are in excel on the menu you should see Connections, It is there where you put your sql code and give it a name. You can have it to sql process that uses temp tables or not or you can have it pull directly from a table. It is up to you. I will do anytime i have repetitive requests where the end user needs it in excel but the data originates elsewhere that would require cut and paste. Once embedded i just click refresh and it automatically updates the worksheet, no need to delete or anything.
As others have said, why would you want to do this in the first place? Perhaps the log table need to be altered to give it something like a GUID or something of that nature. Otherwise, from the sound of it, there is nothing unique about the data in the tables. Now in your Tech_pub table is there anything that you would considered unique such as Pub_Num and Pub_dt, IF so you perhaps could use something like : ROW_NUMBER() over(PARTITION BY TECHPUB.PUB_NUM, TECHPUB.PUB_DT ORDER BY TECHPUB.PUB_NUM, TECHPUB.PUB_DT ASC) as RowNum Again without really seeing the data you are working with and perhaps a better alternative being applied at the table level instead of the view because of performance issues, not sure if this is a viable option.
that returns 2 rows. If i have 100 symbols i need 200 rows returned. i could add an added_when column and filter by that i suppose
You need row_number(): SELECT T1.* FROM ( SELECT SYMBOL , PRICE , RowNum = ROW_NUMBER() OVER ( PARTITION BY SYMBOL ORDER BY DATE DESC, TIME DESC) FROM TABLE_1 (NOLOCK)) T1 WHERE T1.RowNum in (1,2)
This should work Many thanks :)
Side note: depending on the complexity of the table, you may want to partition by more than the SYMBOL (for instance, if there is a symbol that exists in more than one category). Think of it this way, PARTITION = GROUP 
I think I'd lean towards adding the complicated sort logic up after the SELECT, then sort on that column. I wonder if it's treating your integer column 2 like a varchar because the first expression in the case statement is a varchar. You might also look into COALESCE to handle the "use column 1 unless null, then column 2" That might look like: ORDER BY COALESCE(column1,column2) and if you need to add the %-% thing: ORDER BY COALESCE( CASE WHEN column1 LIKE '%-%' THEN column1 ELSE Null END, column2) But that probably doesn't solve why column2 is getting sorted like a varchar.
&gt;I wonder if it's treating your integer column 2 like a varchar because the first expression in the case statement is a varchar. Yes, that seems to be exactly what's happening. Thanks for putting it into better words than I could. Isn't this a bug of some sort or is it supposed to work like that? And as you said, column2 is still sorted as a VARCHAR with COALESCE from what I can see. But I will keep testing, maybe it's something I can use.
Okay, so I can ORDER BY column1(VARCHAR), column2(INTEGER) but when you add a CASE for one of those two columns you simply have to have the same data types for them? I don't really understand why but at least I know what the problem is now. I was puzzled for a while since this seemed so simple. I guess I could add the logic somewhere else.
Yes, that's one way to go around it but I don't know if it's good practice.
I agree. The sorting you're doing sounds really complex, so I'm not sure if there is a good best-practice that captures it. I'd lean towards getting something that works correctly, then post that in another post, asking for "how to do this better". From a "trying to figure it out" point of view, I'd try setting up the ORDER BY to only have the column1 and then only the column2 and make sure each piece sorts correctly. When you use only column2, does it then sort the int like it should? Or does it still treat it like a string? If it does sort it like an int, if you do something like: ORDER BY CASE WHEN 'A' = 'A' THEN column2 ELSE column2 END Just to see if that somehow forces the whole thing to act like a varchar.
2005 supports ROW_NUMBER()
Flyway is fine for getting started. It's easy. Eventually, any migration tool with sufficient power will need to use a programming language. With Flyway, use Java. You may enjoy this too: https://codeascraft.com/2013/01/11/schemanator-love-child-of-deployinator-and-schema-changes/
Glad I could help a little bit. One more thought... could you create a view on top of the table to make some of the transformations (esp. on column1), then use that view as the source in what you're trying to build? Good luck!
I use flyway in java, and I have never had an problems with it. It is configurable and flexible. I have looked for a language-agnostic sql migration tools, but I have never found one that is less complicated than just using a library.
Well, `ORDER BY column1(VARCHAR), column2(INTEGER)` is two columns, not one. A `CASE` expression only has a single value ; it's only one column. 
It's not a bug, it's a feature. As you've found out, sort order is different for different data types. Data types cannot be mixed, so if you have both integers and characters it will sort them all as characters. If you want an integer sort you must use an integer data type, which means all values must be integers. In MS SQL I'd use "order by try_cast(foo as int), try_cast(foo as varchar(n))" to get the results you're looking for. Not sure how to do that in mysql, I don't think they've implemented try_cast. 
Got it, two separate columns. Then even easier: CASE WHEN column1 LIKE concat('%','-','%') THEN column1 END, column2, column3 It makes sense if you think about it. If you mix integers and character data types being returned, like in a case statement, then in most situations the database engine will convert the integers to characters. In other scenarioes it will convert the characters to integers, and if there's a non-numeric character it will fail. Different RDBMS systems do it differently. Even more oddly, the exact same code on the same system can do different things, depending on what data is presented to it. Short answer, don't mix data types. When you have to mix them, be explicit using cast. In your scenario there's no reason to mix them, simply order on column1 (with a case if necessary), then on column2, then on column3. 
Yes, simply skipping the ELSE and sorting by column2 separately works perfectly for my needs here. I should've thought about it earlier but I was a bit confused about the mixed data types for a while and I only discovered this by accident to begin with. It's a bit tricky that you don't get a warning message or anything in MySQL. Thank you for your help!
You can also try - SQooLed.com There are currently only 12 lessons, but more will be coming. It's free! 
I'm teaching database technology as an adjunct for a few years, I'm thinking about using this for my class. Does anyone have any experience using it in a classroom? 
Hey! I am actually the sole developer of this website. I think using it in a classroom setting would be great. If you need anything added to the course for your students, or have suggestions, let me know! I'll be happy to make additions for you. 
I can't remember their names but both mssql and mysql has standard databases with data for playing around with... I wish more courses would use these instead of their own. Anyway, seems like a nice collection of lessons. Although very basic.
You're thinking of Adventureworks (for MSSQL, that is). 
Thanks for the feedback! I will take a look at the standard databases to see if I can use those for the lessons instead. Right now the course is very basic, just wanted to see if people find a tool like this useful. 
I think it looks very useful - I could have done with something like this back when I was learning. Well done :-)
I was sure it was called something win*. But thanks, I know there is one for mysql and possible for postgresql too.
Quantifiable information like size, and specs. I've been thinking of a hard limit. I'm also trying to think of a system for merging/removing categories/features that are too similar ie (Model #/Model No.)
I personally think you are going to have a nightmare from a data perspective if you leave it to the user to have the ability to extend columns etc.. Additionally, what about data integrity type of validations on the front end will you have so you don't have 5 different Motorola company names for instance because each user spelled it differently. Just thinking globally for the type of reporting you might need in the future and if you don't address it in the beginning it will be much harder later on. 
Exactly!
&gt;Each table uses a 'ResourceID' as a unique identifier to identify the PC. I have (incorrectly) tried to join three tables/views - the result being that I get lots of rows returned 'per-pc'. Clearly using a series of JOINS, in this way, is not the correct way to do this. It's quite possible that joining the three views (assuming the `v_` in the names in your query indicates that they're views) is the correct way to do it. However, you may be missing another field (or multiple fields) in your `join`s. Or, you may have to resort to more...interesting...methods, depending upon what you want. For example, it's common now for a single computer to have multiple network adapters, and possibly even multiple video controllers (is that a thing on Windows like it is on Macs?). If you join *only* on the `ResourceID`, you'll get multiple records because of the one to many relationship. Given: * One computer * With 2 NICs * And 2 video controllers If you join on `ResourceID`, you will get two records when you join `v_GS_COMPUTER_SYSTEM` with `v_GS_NETWORK_ADAPTER` - one for each network adapter. If you then join *that* set to `v_GS_VIDEO_CONTROLLER`, you'll get *four* results - one for each combination of NIC and video controller. Try this: take a single `ResourceID` and query each of those views individually. The *product* of the number of records each one returns is how many records you'll get back with the `JOIN`. So I guess the question you need to answer here is this: In what way do the results you're getting differ from what you're expecting? Real quick hit: If you're expecting one record per computer, and you're getting multiple records back but they're all the same, use a `SELECT DISTINCT` to filter the duplicates out. You'll take a performance hit but depending on the size of your sets, it may not be bad. And now that I've gotten to the end and clicked your link, I see that I've basically re-explained the top answer you got on your original post.
Thanks for the feedback. Yes, absolutely correct that I get multiple hits per 'ResourceID', what I see is that I get rows returned based on all permutations of the configuration. By having a laptop PC with wired, wireless, vmware vNics, and then having lots of audio combinations (Bluetooth audio, hd audio,etc) there's lots of duplicates. In my *lack-of-SQL* experience I was trying to use the 'distinct' operand to try and limit the reoccurance of the PC. I think my only option is to run multiple SQL statements - to filter the individual datasets - and then collate the datasets into one set of returned data. I guess my question is then, can this be done using a single SQL script? As I don't have the SQL experience, that you guys would have, I'm purely trying to determine the logic of what I would need to do rather than expecting someone to write the SQL statements out for me. I.e. The *Teach a man to fish..* idea :)
I think I might have answered your feedback, in my response to /u/alinroc. It's definitely a one-to-many result set, due to the combination of values. (I guess it'd be much clearer if familiar with the SCCM tables and views). I know my way around SCCM and the tables &amp; views in SQL, but I am a total novice with SQL as a query language. Thanks for the input 
Ah, so you're getting all permutations because you have multiple one-to-many joins. That makes sense if you think about it. I still don't understand what you are expecting, though. Have you mocked up what you expect the resulting data to look like? Using a union instead of joining will get rid of the permutation problem, but you need to think about what you want the results to look like. 
I looked at your responses and I think what you actually want is an aggregate in a subselect. This would allow you to get a single row for each computer, but see all the info they have together. Something like: SELECT comp.resourceid ,(select count(*) from nic where resourceid = comp.resourceid and MACAddress0 is not NULL AND Description0 like 'Intel%') nic_count ,stuff( (select name from audio where resourceid = comp.resourceid FOR XML PATH('') ), 1, 2, '') audio_list FROM comp I will point out that this data is basically useless to a computer. It's nice to print out and let a human look over, but it will suck for a report/Excel to try and sort or filter. 
Maybe something like below? select m.userid, m.name ,(select number from phonenumbers p where p.userid = m.userid and p.typeid = 1) mobile ,(select number from phonenumbers p where p.userid = m.userid and p.typeid = 2) home ,(select number from phonenumbers p where p.userid = m.userid and p.typeid = 3) work from main m order by m.userid;
Break the bin and gadgets into multiple columns in a subquery and work with those instead. SELECT SUBSTR(bin_location, 1, 1) AS building, SUBSTR(bin_location, 2, 1) AS aisle, SUBSTR(bin_location, 3, 3) AS bay, SUBSTR(bin_location, 6, 1) AS sub_loc, FROM bin_locations Do the same thing for gadgets, and cross join the two. Select the best one (or two, or whatever) per gadget, using RANK or ROW_NUMBER and ordering by the absolute value of the difference ( ABS(bin_locations.building - gadget_locations.building), ABS(bin_locations.aisle - gadget_locations.aisle)....). 
Thanks buddy, worked it into my existing code and it works a treat. Really appreciate the help!
I would consider using a functional index on all of those or a combination thereof and just write a few simple queries looking by aisle, bay, and sub location. I would be willing to accept some sub optimal next nearest locations if it meant a huge decrease in run time. But depending on the number of possible locations it could be a trivial run time anyway.
This will work, but if `PhoneNumbers` doesn't have a unique index on `(UserID, TypeID)` (i.e., if you allow multiple numbers of the same type) then it will fail because a subquery will return multiple results. Here's a solution if you don't have that restriction: SELECT DISTINCT m.UserID, m.Name, pm.Number AS "Mobile", ph.Number AS "Home", pw.Number AS "Work" FROM Main m LEFT JOIN PhoneNumbers pm ON pm.UserID = p.UserID AND pm.TypeID = 1 LEFT JOIN PhoneNumbers ph ON ph.UserID = p.UserID AND ph.TypeID = 2 LEFT JOIN PhoneNumbers pw ON pw.UserID = p.UserID AND pw.TypeID = 3 ORDER BY m.UserID, pm.Number, ph.Number, pw.Number However, generally speaking, it's more correct relationally speaking to do a query like that below, and then do formatting and layout where it belongs -- in the application. Display is an application problem, not a database problem. SELECT m.UserID, m.Name, t.TypeName, p.Number FROM Main m INNER JOIN PhoneNumbers p ON p.UserID = m.UserID INNER JOIN PhoneType t ON t.TypeID = p.TypeID ORDER BY m.UserID, t.TypeName Or if I wanted explicit `NULL`s: SELECT m.UserID, m.Name, t.TypeName, p.Number FROM Main m CROSS JOIN PhoneType t LEFT JOIN PhoneNumbers p ON p.UserID = m.UserID AND p.TypeID = t.TypeIT ORDER BY m.UserID, t.TypeName [Note: In another RDBMS I would use two `FULL OUTER JOIN`s, but MySQL doesn't support those.] Unless it's used for fixed aggregation, the PIOVT type query is not a very good query because it's very difficult to maintain. You have to rebuild the entire query if your phone types change, for example. Do do that dynamically You'd have to do a `SELECT TypeID, TypeName FROM PhoneType` to determine how to build the query, and then dynamically build the query. The *only* time I would do a pivoted query is when I was unable to use another method of formatting and was forced to do it in SQL, such as if I were exporting the list with a direct command or using reporting writing software (e.g., Crystal Reports, which often gives the author little to no flexibility). 
Beware of this query. `FOR XML PATH('')` is a common method of doing this since SQL Server 2005, but it relies on *undocumented behavior* of the `FOR XML PATH()` expression. That makes it moderately less reliable long term than deprecated functions. Microsoft makes no guarantee that any given update will not permanently break this functionality. Other RDBMSs have `GROUP_CONCAT()` (MySQL, SQLite), `LISTAGG()` (Oracle 11g), `string_agg()` (PostgreSQL 9.0), but currently MS has no documented equivalent and based on the numerous submissions on MS Connect, they don't look to be adding anything soon.
Thanks for taking the time to provide feedback.. The 'v_GS_COMPUTER_SYSTEM' does indeed hold one record per PC. The unique id of the PC being the 'ResourceID' value. The 'v_GS_VIDEO_CONTROLLER' view will hold multiple records per 'ResourceID'. Likewise the 'v_GS_SOUND_DEVICE' and 'v_GS_NETWORK_ADAPTER' views can have multiple rows per 'ResourceID'. Having played about with the various queries, and readng the links provided, you're spot on that I was attempting to get rows to become columns - which clearly is bad design. I guess using the SQL report builder would be the best way to tidy up the presentation of the data. The reason for attempting this is that I'm trying to determine what drivers I need for the various computer systems we have in our fleet - so that when it comes to rollout of a new OS that I can reduce the amount of 3rd party drivers being installed. I'll outline the sort of data that SCCM DB is holding, what I'm trying to collate from that data is a report that shows. I may just have to live with the multiple rows and then manually edit it - not ideal, but thats just the way it is. Ideally I'd like to be able to return the following data; [ResourceID][v_GS_COMPUTER_SYSTEM*Model0*][v_GS_VIDEO_CONTROLLER*Devices*][v_GS_SOUND_DEVICE*Devices*][v_GS_NETWORK_ADAPTER*Adapters*] But given there's multiple rows of data per-ResourceID for the Video, Sound and Network controllers. I would need to sort out that data filtering before I can start to consider correlating the data into report rows. TO give some idea of the type of data held by SCCM The **v_GS_COMPUTER_SYSTEM** view holds one entry per PC, but there's quite a lot of information held; SELECT [ResourceID], [GroupID],[RevisionID],[AgentID],[TimeStamp],[AdminPasswordStatus0], [AutomaticResetBootOption0],[AutomaticResetCapability0],[BootOptionOnLimit0],[BootOptionOnWatchDog0], [BootROMSupported0],[BootupState0],[Caption0],[ChassisBootupState0],[CurrentTimeZone0], [DaylightInEffect0], [Description0],[Domain0],[DomainRole0],[FrontPanelResetStatus0],[InfraredSupported0], [InitialLoadInfo0],[InstallDate0],[KeyboardPasswordStatus0],[LastLoadInfo0],[Manufacturer0],[Model0], [Name0],[NameFormat0],[NetworkServerModeEnabled0],[NumberOfProcessors0],[OEMLogoBitmap0], [OEMStringArray0],[PauseAfterReset0],[PowerManagementCapabilities0],[PowerManagementSupported0], [PowerOnPasswordStatus0],[PowerState],[PowerSupplyState0],[PrimaryOwnerContact0], [PrimaryOwnerName0],[ResetCapability0],[ResetCount0],[ResetLimit0],[Roles0],[Status0], [SupportContactDescription0],[SystemStartupDelay0],[SystemStartupOptions0],[SystemStartupSetting0], [SystemType0],[ThermalState0],[TotalPhysicalMemory0],[UserName0],[WakeUpType0] FROM [CM_FIN].[dbo].[v_GS_COMPUTER_SYSTEM] Where I'm looking to only use the following from the 'v_GS_COMPUTER_SYSTEM' view [ResourceID], [Manufacturer0],[Model0] The **v_GS_VIDEO_CONTROLLER** view can have multiple rows of data, per ResourceID, and there's a lot of information held; SELECT [ResourceID],[GroupID],[RevisionID],[AgentID],[TimeStamp],[AcceleratorCapabilities0], [AdapterCompatibility0],[AdapterDACType0],[AdapterRAM0],[Availability0],[CapabilityDescriptions0], [Caption0],[ColorTableEntries0],[ConfigManagerErrorCode0],[ConfigManagerUserConfig0],[CurrentBitsPerPixel0], [CurrentHorizontalResolution0],[CurrentNumberOfColors0],[CurrentNumberOfColumns0], [CurrentNumberOfRows0],[CurrentRefreshRate0],[CurrentScanMode0],[CurrentVerticalResolution0], [Description0],[DeviceID0],[DeviceSpecificPens0],[DitherType0],[DriverDate0],[DriverVersion0],[ErrorCleared0], [ErrorDescription0],[ICMIntent0],[ICMMethod0],[InfFilename0],[InfSection0],[InstallDate0], [InstalledDisplayDrivers0],[LastErrorCode0],[MaxMemorySupported0],[MaxNumberControlled0], [MaxRefreshRate0],[MinRefreshRate0],[Monochrome0],[Name0],[NumberOfColorPlanes0], [NumberOfVideoPages0],[PNPDeviceID0],[PowerManagementCapabilities0],[PowerManagementSupport], [ProtocolSupported0],[ReservedSystemPaletteEntries0],[SpecificationVersion0],[Status0],[StatusInfo0], [SystemName0],[SystemPaletteEntries0],[TimeOfLastReset0],[VideoArchitecture0],[VideoMemoryType0], [VideoMode0],[VideoModeDescription0],[VideoProcessor0] FROM [CM_FIN].[dbo].[v_GS_VIDEO_CONTROLLER] For the above view, I only need the one data column *Description0*, but there are multiple row per-ResourceID For the Audio and network, it becomes more complex as there are lots of rows returned per-ResourceID 
Thanks for that I'll have a look at this and see if I can make it work. 
ok, thanks, I'll maybe refrain from using that then :)
Thanks for the feedback. I've tried to outline what I'm trying to acheive in a response [above](https://www.reddit.com/r/SQL/comments/2qqy44/ms_sql_query_on_use_of_joins_and_unions/cn967f5). I think the complexity is arising purely as a result of each PC having multiple sound devices and network devices. e.g. Vendors such as nvidia providing audio drivers for HDMI with their graphics driver, it complicates things when you use Voip applications such as Lync, which need to have certified audio drivers. When you run the sound device report, you can get 5+ audio drivers being returned. That complicates the process of getting the data I'm after to be returned. For a laptop computer, the network devices query can return up to 10 devices (wireless, vpn adapters, etc). In this case I'd be looking to filter the devices using a filter such as *where ProductName0 like 'Intel(R)%'* - but this would still return multiple items per-ResourceID
You can Google for the other ways to aggregate strings in MSSQL but I'll save you some time, you'll end up using FOR XML PATH.
Not equal
not equals
&gt; I think my only option is to run multiple SQL statements - to filter the individual datasets - and then collate the datasets into one set of returned data. OK, but what will that final dataset look like? If you want to have X columns where X is the number of NICs, that's going to be kind of ugly (unless you start using `PIVOT`). If you're looking to do a report (as in SSRS or similar), then what you're getting will work fine - you can group by computer in the final report. &gt; I guess my question is then, can this be done using a single SQL script? Some implementations (including, IIRC, Microsoft's) of SQL are Turing-complete, so you can technically do **anything** in "a single SQL script". A SQL "script" can be tens of thousands of lines long, run many queries and include lots of logic. If you're asking if it can be done in a single **query**, see above.
Provides the same logic as != but I don't know if it is treated differently by the server.
Instead of using not using a password. (using password : NO) Can you try using user 'root' and password 'root'? Identified is your password. If you changed it to 'root' it should be 'root'.
Is it different in any way from "!=" ?
Literally googled your question: http://bit.ly/1wyko7N But seriously it's the same as != Are you using SQL Server i.e. Microsoft? I believe it's more common there. 
nope. 
Logically it means A is "less than or greater than but **not equal**" to B. 
How is he supposed to know it's called a diamond operator?
wow, are you really joining on a &lt;&gt; condition? That would be weird, and probably not recommended. However if this is just an example, then fine. So just to add to the list of other ways to do this (depending on your SQL engine: a &lt;&gt; b a != b Not a = b might need brackets around that: Not (a=b) 
https://stackoverflow.com/questions/723195/should-i-use-or-for-not-equal-in-tsql
&lt;&gt; is the ANSI Standard Outside of that, they function identically. See [this stackoverflow](http://stackoverflow.com/questions/723195/should-i-use-or-for-not-equal-in-tsql) for some insight 
&gt; The reason for attempting this is that I'm trying to determine what drivers I need for the various computer systems we have in our fleet - so that when it comes to rollout of a new OS that I can reduce the amount of 3rd party drivers being installed. You should just write separate queries for each device type and store the info in an Excel document. There is no relation to NIC and AUDIO, so don't try to force one. 
Neither did I, and I'm a DBA/software developer. After some quick googling, apparently "diamond operator" is a Java term (and I haven't done much Java dev).
There's times when it's needed.
It has its (very situational) uses! For example, if you wanted a list of all permutations of two `products`, it serves as nearly a cross join: SELECT a.product_name, b.product_name FROM products a INNER JOIN products b ON a.product_id &lt;&gt; b.product_id Same thing could be done with "&lt;" instead of "&lt;&gt;" to achieve a combination (assuming a numeric PK).
It also means to poop back and forth, forever, like this: ))&lt;&gt;((
so, what I wound up doing was write a function with this loop: **cursor query** SELECT loc_code FROM locs ml WHERE --find me empty locations AND NOT EXISTS ( SELECT 1 FROM invt cl WHERE ml.whse_code = cl.whse_code and ml.loc_code = cl.loc_code) AND ml.Loc_Code &lt;&gt; In_Loc_Code AND SUBSTR(ml.Loc_Code,1,Wk_Cnt) = SUBSTR(In_Loc_Code,1,Wk_Cnt) AND ROWNUM = 1; **actual code stuff** Wk_Loc_Len := LENGTH(In_Loc_Code); FOR i IN REVERSE 1..Wk_Loc_Len LOOP Wk_Cnt := i; IF Out_Loc_Code IS NULL THEN OPEN GetNearLoc; FETCH GetNearLoc INTO Out_Loc_Code; CLOSE GetNearLOc; END IF; END LOOP; Still not my best work, but what I've found out is that the location definition I gave in my post was incorrect - I could have different formats in different buildings.
What you're probably asking for is a custom traverse path (starting with your original bin 56001A). Try to define a metric LocationDistance( Loc1, Loc2). There might be a way to define a spatial index on top of that.
I have a toy project to create fake data for postgres. It creates plain sql statements, they can of course be changed to fit any RDMBS. It's not "finished", but maybe it helps? https://github.com/sputnik27/jibberjabber
Thanks da_chicken, I actually ran into this problem and it was throwing me back an error due to multiple results. I found that using LIMIT 1 was able to get around it, however looking at your solutions gives me more ideas for how to accomplish it in different ways which is great. Thanks for taking the time to reply. 
i suppose... but as mentioned above. probably easier to do a left outer. 
Take a look at load testing tools that replicate transactions such as HP load runner, IBM rational performance test. You don't need the "load test element" if that's not what you are looking for, just use them to run thousands of transactions to generate volume in the db. Otherwise, as others suggested you can generate volumes directly in the database.
Damn, that is expensive but putting me on the right track I think. 
Specifically it means "greater than or less than" .. it doesn't mean "not equal" I remember watching a training webinar a few years ago where they demonstrated that it was slightly negligibly slower and less efficient than != because technically it's running two computations .. to see if a value is greater than, then to see if its less than, before returning all results. If your data contains values 1 2 and 3, and your syntax is WHERE Value &lt;&gt; 2 you'll get 1 and 3, but not because 1 and 3 are not equal to 2, but because 1 is less than 2 and 3 is greater than 2. If your syntax is WHERE Value != 2 you'll get the same result, but you'll get the result in ... lol 1/3 of a nanosecond sooner or something .. because both 1 and 3 are not equal to 2.
Thanks for your feedback! I agree with your input. I went ahead and made some changes to the tables. Really the intentions for this first course was to show some simple syntax and keep it as high level as possible. I may get into database structure and creating tables in the advanced course. 
Yup, still need to work on allowing all possible answers to be correct. Thanks for the feedback! 
If &lt;&gt; is slower than ! = then they implemented it wrong since they are logically equivalent.
I think I'll just have to live with creating individual SQL queries to grab the necessary data, then export as CSV and manually sort it out with excel. Not ideal, but I think it's probably the simplest option. Thanks for the continued help on it. 
You resolved this in &lt; 12 minutes? I'm new to SQL but wow. 
Thank you. With experience in the language (I've been writing SQL for work my entire 11 year career - most of that working primarily in SQL) you begin to recognize common situations and just know the solutions off the top of your head.
Doing reporting on a CRM instance isn't easy. Start here. http://msdn.microsoft.com/en-us/library/gg328097%28v=crm.5%29.aspx
I know you say you've solved your problem, but you (or someone else with a similar problem) might find these useful [Oracle UTL_MATCH Functions](http://psoug.org/reference/utl_match.html). They use some tricks to compare strings to figure out how similar the strings are to each other.
I'd only use it if it was major performance gains! From a maintenance perspective, I'd probably look back on it in 6 months and wonder what crack the person was smoking and then realize it was my query.
If you don't mind me asking, what is your job title?
It's a premature optimization, certainly. Would be interesting to see how large the parts table would have to be to see any notable gains. I'm guessing it would have to be in the millions of records. 
Cool. Thanks for the continued feedback. Here's to 2015, I hope it's all positive &amp; prosperous for you!
Technically it is "Director of Software Development" But I work for a small company (70 people now, but that is up from about 40-50 only a year ago. 10 people when I started 9 years ago). My primary job responsibility is still writing code.
Left outer join where the right side's PK is null is a technique for finding records in one table that are not in another. When both tables are the same (self join), joining on its PK on both sides will match one-to-one, and the `where` clause will filter out every single record. My example is a completely different situation: when you want all permutations of two records on the same table. Given three `products`: product_id | product_name | :--- | :--- | 1 | A| 2 | B| 3 | C| I want to see each product joined to each other product: product_name_1 | product_name_2 | :--- | :--- | A| B| A| C| B| A| B| C| C| A| C| B| I'm not sure about the efficiency of using &lt;&gt; in a join, but at the volume where efficiency matters, returning all permutations isn't a good idea anyway. With just 1000 products, my query above would return almost [1 million records](http://www.wolframalpha.com/input/?i=1000+nPr+2).
I would think at 200k you'd see MAYBE a few seconds improvement on your query over mine. If it was running hundreds or thousands of times a day that's well worth the expense of the less-readable code. However this particular example seems more likely to be a report that's run once per day (likely even less frequently). 
May I contact you regarding SQL and technical writing?
Yeah you're right. I forgot this was a self join example. My query would return zero rows. Still don't know why someone would want this though. I work with a *freakin' huge* data mart and I get requests that amaze me all the time. Those business folks, especially the really smart ones, have some interesting ideas on how data might be of use to them. 
I don't know anything about how MS CRM 2011 works, but here are the first things that come to mind: 1. Are you sure that you are querying the correct schema and not a table with the same name in a different schema? IOW, "select * from FilteredQuotes" and "select * from dbo.FilteredQuotes" might to resolve to the same object, depending on who you are connected as and what the schema(s) look like in that database. 2. Are you using the same credentials to connect with SSMS as you are using in SSRS? Does the view filter data based on who you are connected as? 3. Are you sure that you are querying the same server/database with your BIDs report as you are when you are using SSMS? 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Object-relational mapping**](https://en.wikipedia.org/wiki/Object-relational%20mapping): [](#sfw) --- &gt; &gt;&gt;*Not to be confused with [Object-Role Modeling](https://en.wikipedia.org/wiki/Object-Role_Modeling).* &gt;__Object-relational mapping__ (__ORM__, __O/RM__, and __O/R mapping__) in computer science is a [programming](https://en.wikipedia.org/wiki/Computer_programming) technique for converting data between incompatible [type systems](https://en.wikipedia.org/wiki/Type_system) in [object-oriented](https://en.wikipedia.org/wiki/Object-oriented) programming languages. This creates, in effect, a "virtual [object database](https://en.wikipedia.org/wiki/Object_database)" that can be used from within the programming language. There are both free and commercial packages available that perform object-relational mapping, although some programmers opt to create their own ORM tools. &gt;In [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming), [data management](https://en.wikipedia.org/wiki/Data_management) tasks act on object-oriented (OO) [objects](https://en.wikipedia.org/wiki/Object_(computer_science\)) that are almost always non-[scalar](https://en.wikipedia.org/wiki/Scalar_(computing\)) values. For example, consider an address book entry that represents a single person along with zero or more phone numbers and zero or more addresses. This could be modeled in an object-oriented implementation by a "Person [object](https://en.wikipedia.org/wiki/Object_(computer_science\))" with [attributes/fields](https://en.wikipedia.org/wiki/Attribute_(computing\)) to hold each data item that the entry comprises: the person's name, a list of phone numbers, and a list of addresses. The list of phone numbers would itself contain "PhoneNumber objects" and so on. The address book entry is treated as a single object by the programming language (it can be referenced by a single variable containing a pointer to the object, for instance). Various methods can be associated with the object, such as a method to return the preferred phone number, the home address, and so on. &gt; --- ^Interesting: [^Apache ^OpenJPA](https://en.wikipedia.org/wiki/Apache_OpenJPA) ^| [^Propel ^\(PHP)](https://en.wikipedia.org/wiki/Propel_\(PHP\)) ^| [^List ^of ^object-relational ^mapping ^software](https://en.wikipedia.org/wiki/List_of_object-relational_mapping_software) ^| [^DBIx::Class](https://en.wikipedia.org/wiki/DBIx::Class) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cnau2ag) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cnau2ag)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
The answer is no. it just makes everything take longer to write out. It's a pain to work in Databases like that.
In most database systems, wouldn't the database name and/or schema suffice for that purpose?
In the relational model *and* in SQL, base tables and views share a namespace for a reason. &gt; It makes things more readable and it's simply nice to know whether you're referencing a physical table or merely a view when writing queries. And you're missing the point of that reason.
&gt; Example: GL_SEGMENTS_TL This is different than the OP's usage, which prefixes database objects with a string that identifies their data type. Your usage is closer to the metadata standards in ISO 11179.
I was enterprise dba for quite a few years (MSSQL). There were several hundred tables, hundreds of 500 line+ stored procs and many views (some of them nested), so its a convention I adopted to save my sanity :) I'd maybe be less inclined to do this on really small projects, but for big sprawling databases, it really helps with readability and performance tuning of the stored procs when you can instantly see whether a view is being hit or the table. 
I prefix views with 'vwTableName' for the reason others have mentioned - it's a lot easier to quickly tell whether you're hitting a view or a table in procs/functions/etc in a large environment. 
How about sth like this: select mid from movie where mid not in (select rating.mid from rating join reviewer on (rating.rid = reviewer.rid) where reviewer.name='chris jackson')
I'm not a DBA, just a dev who specialises more in databases than the rest of the team, and from my (admittedly limited) point of view there's generally no reason to care that much about whether you're hitting a view or a table. Some might cite performance, but in my experience performance issues are as often attributable to poorly indexed tables than overly-complex views. It's good to hear other perspectives though.
`NOT IN` is going to give you poor performance on sufficiently large tables.
&gt; And you're missing the point of that reason. No I don't. In designing BI systems we usually work with 3 to 4 data layers with each their own naming standard. In the final layer that is referenced by the user we won't make a naming distinction between views and tables. However, rarely we use both object types in the same data layer. 
Pro tip - don't use right
Another Pro tip - CTE's are schweeet!
Just use a database that supports schemas. One reason to use a prefix is if you want a view and table to have essentially the same name. Example vwPeople and tblPeople. But I would suggest it's less shitty to use one schema for raw tables (with a People table) and one for views (with a People view). 
http://mockaroo.com isn't bad. 
 SELECT title FROM movie LEFT JOIN ( SELECT rating.mid FROM Rating LEFT JOIN reviewer ON rating.rid = reviewer.rid AND reviewer.name = 'Chris Jackson' ) RatingByChrisJackson ON RatingByChrisJackson.mid = movie.mid WHERE RatingByChrisJackson IS NULL Should solve all the issues above.. One result per movie, and only showing the movies NOT reviewed by Chris Jackson. Got me tempted to fire up my SQL VM and test it.
Who cares what looks more professional.. The question that matters is does it make it easier to work with/clearer what is going on. If things are obvious/easy the other developers will love you for it..
&gt; I know SQL isn't technically a language . . . *SQL* stands for Structured Query Language.
oh. 0_o
The way that you phrased the question shows a lack of understanding. First, I think you should let go of whatever it is you think you know about SQL. SQL is a database language. A database is primarily a bucket for information**. If you were to write a software &amp; did not have a place to store the information you've collected, it would not be all that useful. Moreover, in pretty much any major project, you'll find that different software solutions (often in varying languages) collaborate on work simultaneously. A database provides a centralized place to do that. Then there is the difference between SQL &amp; NoSQL. SQL is a language to manage a relational database. This is one where different groups of information have relationships to other information. So, a business might divide it's customer list from the list of products it sells and have yet another list for which customer bought which product. These are the relationships between the data. NoSQL does not manage relationships. It is however, simpler &amp; faster. So if you wanted to plot a point on a GPS, 3 times per second over the course of a week, you would have 1.8 million rows of data. If you were to need that to operate on a pocket device (like your phone) then efficiency is paramount. Moreover, those points are only related to each other. We don't necessarily need to link them to a separate group. Having said all of that, I think it's becoming clearer why the question is phrased improperly. SQL is not like a car, because that implies that you could just decide on another car. In reality, you can't. SQL is more like the wheels of a car. You can pick the type of wheels that you want, but ultimately (a) you can't do without them &amp; (b) they will not replace the function of the motor. ** this is a gross simplification to get through an introduction. 
Thank you! This is a huge help. So clear. So concise. I realize (and am embarrassed) by my folly. I'm not going to delete of shame, 1) because this answer is so fantastic, 2) So anyone as ignorant (hard to believe) as me can learn from my mistake.
I think an important question would be - who are we selling it to? If we are trying to persuade you to learn SQL then I'd guess I'd point out the following: - Popularity (Consumer). It's difficult to sensibly estimate how many individual SQL databases there are in the world (not that is meaningful by itself) - but well over a billion would be a reasonable start. If your smartphone is Android or iOS then it's likely it has several SQLite databases on it. Your home PC may have applications which store data in SQL databases. - Popularity (Enterprise). An interesting challenge would be to find a business which employs more than 25 people and uses computers but *doesn't* have some sort of SQL database. Whether it be their payroll system, finance application, intranet or whatever their main business system is - there's a very good chance one or more of these will use SQL somewhere within it. - Versatility. If you learn something like C or C++ those are obviously enormously useful skills if you're going to work as a software developer. They have their uses outside of software development, but in a limited scope. SQL is a little different. The number of people who work outside of IT altogether and who use SQL is higher than you might think. Where I work people in Finance, Performance Reporting and Asset Management all write SQL queries on a daily basis. Most of these people do not think of them as "IT people" and in fact some openly admit they don't know a lot about computers at all. DBAs obviously use SQL all the time, but it's useful for non-DB people in IT too (often due to the popularity in enterprise systems as mentioned above). You'd struggle to find another language which is as generally useful as SQL - whether or not you work in IT. - Simplicity. While the number of keywords in SQL continues to grow the basics remain as they have for a long time - SELECT, UPDATE, INSERT and DELETE. That's basically it. And even better the syntax can be reasonably close to spoken English so even if you don't understand SQL as such, it's often possible to identify what a statement is doing from just a common sense reading. e.g - you could *probably* guess what the following (admittedly simple) three statements do: UPDATE employees SET annual_salary = annual_salary + 5000 WHERE name = 'DharmaPolice'; INSERT INTO products (product_code, product_name, cost, price) VALUES ('A100','Acme Widget', 0.59, 2.59); DELETE FROM webhistory WHERE (url LIKE '%redtube%' OR url LIKE '%xvideos%'); - Power. Popularity and simplicity are all well and good, but that doesn't mean SQL is actually useful. Fortunately that isn't the case, and SQL is capable of enormous power when it comes to working with data (which is what lots of people do, every single day they're at work across almost every industry). Having said all that, it's important to realise that SQL isn't a competitor to most other programming languages. Although apparently SQL can be made to be Turing complete through the use of CTEs and Windowed functions (see this [StackOverflow question](http://stackoverflow.com/questions/900055/is-sql-or-even-tsql-turing-complete)), I don't normally describe SQL as a programming language when talking to people. Some SQL platforms allow you to write entire applications (even rudimentary games) only using SQL statements + whatever extensions the platform offers. But that's not really what SQL is *for* and certainly not what SQL is good at. So the question isn't (in most cases) "Should I learn Python/Perl/Ruby/C/C++/C#/Java/etc *or* SQL" it's "What programming language should I learn alongside SQL?". And regardless what you choose, there's a very good chance there'll be library of built in functions to allow you to work with SQL databases. 
So NoSQL stands for not structured query language? 
I'm slowly getting into SQL and I can already see it's 20-30 times faster than Excel's vlookups/indexes/matches on my databases that are each made of 20-30 million "cells" (cells as per Excel's understanding, that's 500k rows of data in 30-40 columns in at least 40-50 files, not to mention smaller files, with 1m cells that I couldn't count even if I wanted to). I'm going to convince the company I work for that it's highly recommended to actually buy a piece of hardware to set up an Apache server to have it even more effective. http://www.w3schools.com/sql/ Use this website as a starter, I couldn't recommend a better place.
&gt; I don't see how it is only a half truth. From the neo4j documentation: A Graph contains Nodes and Relationships[1] &gt; Reread the thread. I said "NoSQL does not manage relationships" and you opposed that. It looks like you're confusing a language and a ~~technology~~ software which can be a combination of languages. Neo4j *does* manage relationships but it is only through the use of Java. My point still stands, *NoSQL does not manage relationships*. NoSQL stores data &amp; a separate software groups it into a meaningful way. I'm not really sure which part is up for debate here. If you want to make the argument that a NoSQL database (alone without the assistance of anything else) can manage relationships, then I'd really want to see an authoritative source corroborate that. **Edit:** &gt; Yes, neo4j is written in Java. Both Oracle and MS SQL Server is written in C and C++, also a separate language. I don't see how the language used to implement the database matters. Now that I've read your message in more detail, I think that we had two breaks in miscommunication. The first is that we began to talk about graph databases which are [distinctly different from NoSQL databases](http://neo4j.com/developer/graph-db-vs-nosql/). We can have that conversation, but it is separate from what I originally stated. The second was that we disagree as to whether Neo4j is a language or a software. I can't find anything online that states conclusively either way.
Wow, this is super helpful. It's great to hear from someone who is still kinda new at SQL and your perspective. Thanks!
the standard toolkit of most DBMS's out there will include a import tool for CSV that will just dump the file into a table. That is not a schema however, and if you are talking about a normalized schema to be "guessed" then I am almost certain you will not find anything since I can't imagine that working very well. If you are just talking about importing a csv into a database table, that is pretty trivial to do yourself, and it should be a function of whatever tool you are using as an interface. Not knowing what DBMS you are talking about, I can't go into specifics of course
not a schema I'd use.... nullable / not nullable, indexing, data types (and lengths), primary key choosing, clustered index / heap... creating a dataschema is a bit more involved than having a quick scan and creating a few columns to hold random data. At least it should be.
Well of course. I'm not going to come across anything that's going to generate 4th normal form. It doesn't have to be perfect to be useful. 
Sure, there are large scale ETL tools that'll do the trick. But I'm going to keep it small. Catherine Delvin's ddl generator looks promising enough that, even if it doesn't do what I want it to, it could be modified well enough. 
Well, the good thing about PDI (and Talend for another example) is that it's free, open-source, and perfectly suitable for small and day-to-day data integration tasks ! 
You...sound like you work for them. #suspicious.
NoSQL isn't any one "thing". It's a lot of different databases that all behave very differently from each other: KV stores, columnar databases, document databases, graph databases, logs. Even just within KV stores, you have a ton of variation between the use cases and behavior of memcached, redis, riak, etc. If you want to get your feet wet, I'd suggest: - https://pragprog.com/book/rwdata/seven-databases-in-seven-weeks - http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf - http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying And also look at "distributed systems" in general. It's not common to use a "NoSQL" database by itself. Usually you'll use it along side SQL, and with a couple other NoSQL databases, to mitigate either scaling issues, or issues inherent with running a distributed system on multiple servers. Well, except for worker queues -- just about any app can benefit from that.
Hi. What makes you think exactly that the static IN list is "VASTLY" better than the custom TABLE cast clause, how did you test? Maybe the data blocks are already in the buffer cache by the time you already run the variant with (@pc1, @pc2)? If you rule that out and it still runs slowly the problem *might* reside with the SPLITINTO_CLOB function which could be un-optimized. What type and length is InCriteria1CSV and can you give a simple example of the values in it? Also, have you looked at the explain plans, is there anything radically different between them (talking about the 2 queries that you provided)?
Using this approach will cause the CBO to assume the subquery IN list size is 4000, which could skew explain plans due to high-ish cardinality. I would use DYNAMIC_SAMPLING hint (mode 2). See Oracle help for more information.
Yeah, that's exactly what I said to myself after reading my post, so I added Talend as another example ! And well... it's an open-source ETL so it's basically free...
Thank you for explaining this. My theory is weak, definitely something I need to work on.
Everyone else has described it pretty well, but this is exactly what I'm asking. I don't know the difference between SQL and PL/SQL, I was just told "Begin learning PL/SQL". /u/UnlikelyExplanations wrote a good explanation. I'm still very interested in learning the difference.
Are you sure there isn't an "or" somewhere in the where clause? where date_of_service &gt; '2014/01/11' and substr(location,1,5)=5495 or substr(location,1,5)=5496 Logical operator precedence will evaluate the AND before the OR, so you'll get: (date_of_service &gt; '2014/01/11' and substr(location,1,5)=5495) or (substr(location,1,5)=5496) -- e.g. all the store 5496 records when you mean: (date_of_service &gt; '2014/01/11') and (substr(location,1,5)=5495 or substr(location,1,5)=5496) When I teach SQL, I always tell students they should get a queasy feeling every time they type "OR" in a WHERE clause, which will only go away when they put parenthesis in to clarify. 
I have a degree in MIS and ended up as a Financial planner for a mass retailer. I do the financials for a group of categories and also help develop excel front ends that hit sql servers i have prebuilt or teradata/db2 servers. My ability to do both the business side and the technical side has made me an extremely important member of the team.
when I remove date of service It pulls the same as with it. 
different tools for different problems
The only way to learn is to do it. Seek out projects and people to help; and then work with the more experienced members of the staff if and when you get stuck.
Exactly! If you have the opportunity to dive in to something, TAKE IT. Most people I know read articles, talk to other admins, but don't actually LEARN until they get their hands dirty and have to clean up the mess. If you don't have a way to do that at work, or don't have a dev or testing environment, consider getting yourself a copy of SQL Server Developer Edition (or whichever dbms you use), install it in a VM, install some sample databases or re-create something similar to your work environment, then spend some time solving problems. Solve them completely, not just barely. Experiment with everything. Use things unconventionally, if for no other reason than to just see what happens. If you're into more admin than sql programming, consider learning PowerShell to add to what you're learning to do in the UI and via TSQL. This all takes a lot of time, so start now. The older we get, the less time we seem to have for this kind of learning. Have fun, and good luck.
That question is a lot like asking if you should learn MacOS or Windows. They're different approaches, they have different strengths and weaknesses. Neither of them are going away anytime soon. SQL is the long held standard. It *will* be around the rest of your lifetime (ask anyone who maintains COBOL code. Once something is in production, it's hard to get rid of). NoSQL has felt "buzzy" to me. Lots of hype around it. It's gaining traction and has potential. Time will tell us that NoSQL is an alternative to SQL rather than a replacement. 
happy cake day!
I know a couple of people that maintain COBOL code on AS400's. The banking industry really hates change. I can't really blame them either. It's stable, mature and really well tested. it might be old, but I so is my granfather. I don't want to kick him out just because he's old.
As all things programming, learning is less about taking courses and more about exercising your skills and researching new solutions. Start some personal pet projects that require you to push your understanding of the system. I suggest trying to make the ill-advised "Database of Everything"; that is, a database ran by an application that can model *any* abstract data. It's a bad idea that teaches you what relational databases are all about.
[Learn](https://ola.hallengren.com/) from the best. Download this MVP's script and try to see what he is doing. Or try to make something difficult yourself. A couple of months back I made a "program" that used google docs as an input. Using some pivots/windowed functions/views/procedures ect I got a nice working program that matches people together on what they entered in the form, all in SQL.
/facepalm Many NoSQL solutions are built on SQL. Furthermore, SQL is method of storing the relationships of data as much as the data itself (hence "relational database"). As such, there are certain types of data transactions/storage patterns that fit SQL's pattern very well, and other types of data that don't fit as well (though, literally any type of data could be stored in SQL). It all depends on what you need to do, but I would wager my salary that SQL will be more widely used than NoSQL solutions for the foreseeable future.
&gt; ask anyone who maintains COBOL code. Once something is in production, it's hard to get rid of I feel like this is a bad comparison. Any modern language can do everything that COBOL could do, and more. The difference between SQL and NoSQL however is that NoSQL does not do everything that SQL does. In effect, COBOL will always be around because old code always sticks around; but SQL will always be around because we actually need SQL. It's not like it's a bad technology that's outdated; there's very little change in that platform precisely because it does its job so well that there's no need for the change in the first place.
That code looks cool enough for a save.
It needs to be two-way. When the primary server goes down, the secondary server is taking on its role and will send any new data it takes on back to the primary as soon as it's available. Stuff like logins and sessions aren't a concern. The only concern is that records on the database aren't lost because the servers aren't syncing properly and that the secondary server can take stand in as the 'primary' in case of a problem. 
MongoDB stores data in JSON/BSON format, which maps conveniently to Javascript data objects. Currently I'm studying the possible use cases for NoSQL for a large firm and (since you're a student) I advise you to look at the broader picture of all NoSQL types, taking into account things like ACID compliance and the CAP theorem. There's a [thorough presentation on Slideshare](http://www.slideshare.net/quipo/nosql-databases-why-what-and-when) that may help you in this respect - it explains the database types way better than I could. On a practical note, PostgreSQL supports JSON as a data type, giving you both NoSQL benefits and RDBMS benefits. When it comes to NoSQL engines, I am more enthousiastic about the features of graphical databases like Neo4J. 
You don't need enterprise for mirroring, only need enterprise if you want to use asynchronous mirroring.
I'd say mirroring is your best bet then. It is an easy fail over process and keeps your secondary up to date. Log shipping is also an option for disaster recovery, but mirroring is an easier failover process. 
SQL in Oracle is the query language. Selecting data, inserting data, creating objects, altering objects. DML and DDL. PL/SQL is a procedural language that sits on top of SQL. It's modeled after the Ada programming language. It can do just about anything. Need to batch process records, write to a file, and send output to the web, that's PL/SQL. T-SQL in SQL Server world isn't 100% comparable to PL/SQL. It's often referred to what is JUST SQL in Oracle. We have tons of resources for free you can consume at our Oracle Learning Library, on YouTube, in Oracle Magazine (print and online, both free), and the Oracle Technology Network. Just Google, or let me know if you want some direct links. You can also check out some O'Reilly books by Steven Feurestein. He's written an entire series on PL/SQL if you're ready to jump in with both feet. 
That wasn't how he was using the term. Pivot is a corporate buzzword which literally means to change direction and work towards a new goal, e.g., I'm going to pivot to NoSQL. It isn't analogous at all to pivoting in SQL.
I agree with most, SQL and RDBMS are not going anywhere. What I'm seeing is a huge difference in how systems interact with SQL. Instead of say System A selecting data directly from the database behind System B I'm seeing services being built to serve the data whether through web services or other means. This way the application is in control of what it serves up and can filter content based on business rules as opposed to someone just selecting everythign from SQL. As a DBA and developer I see the advantages of this, but it sure makes live complicated as hell. So yeah, SQL isn't going anywhere but how you talk to it is definitely changing.
Ahhh Works great.... Thank you very much
If you find a way to rewrite this in MySQL you're good http://sqlfiddle.com/#!4/8b3272/13
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED GO BOOM
I was trying to say he should replace it in his vocabulary with the one I linked, because that's the only way you're going to get any work done with it.
As /u/zbignew said - you can call the command to set your transaction isolation level to read uncommitted at the top/beginning of your query. Not sure which SQL you're using, but if it's SSMS you can even change the default behavior of your query windows to be a READ UNCOMMITTED state, so that you'd have to specifically call a committed read (lock). 
All industry hates migration away from "tried and true" tech. Every time some bug in the new code surfaces that didn't exists in the old system I'm greeted with "well the HP3000 didn't have this issue!" More fun, all the COBAL code was written using a nested looping structure (all table JOINS, etc), so any errors in processing would write the primary key from the database to an error report and continue processing. With SQL, as you know, specific row which causes an error in a bulk operation can't be captured, so we rollback the whole transaction and dump the error to a log. Any time this happens it's "well the HP always told me which record caused an error and SQL doesn't, why can't SQL do this, this is terrible" The answer is because the HP took 10 hours to process what SQL does in 10 seconds, and this is the cost, I guess. I'm ranting, aren't I...
I'm also looking to develop my a SQL knowledge and so far I've seen www.sqlzoo.net which gives some tutorials. It seems more basic depending on how deep you're into it.
I think he means that he wants each page in its own separate PDF file. For that you would need an external program that you input a PDF and it outputs separate files for each page.
Ah, I see that now. Ignore my post!
Yep. Even if by some witchery, all new sql development stopped right this second, the demand for sql programmers would be very large for a very long time. However, that is NOT going to happen. You're good. :)
Check to see if Numeric is a deprecated data type in Access 2010. http://msdn.microsoft.com/en-us/library/windows/desktop/ms714540(v=vs.85).aspx
What is the difference for each page? How many pages? I've worked extensively with ssrs to generate pdfs for clients/customers before and what you are asking for is not possible so you need to re-think your solution. 
I like the enthusiasm.
I think you need to create a subselect using Table_1 with a column you can group on (rownum/10) as myrowgroup 
You have not shared enough info. so much of this depends on things you have not shared: the names of the fields in your tables would have helped. Usually the answer the teacher expects you to give is based on the lecture and assignments given. I recommend you refer to the previous week's assignments and the course materials to try to understand what was expected of you. here are some guesses from an internet stranger: assuming only enrolled students are in student: SELECT COUNT(*) FROM student; Assuming the session unique identifier field is sessionID and the session you are looking for is session 42: SELECT COUNT(*) FROM registration WHERE sessionID = 42; the teacher may want to see you join session to registration and filter by session name... Please read the sidebar for info on more resources for learning SQL.
Im on my phone so I cant format well. Dont forget to add single quotes around the course name. 
Can you use the SSRS library to .SavePdf() ?
Oh that's tempting.
Does mirroring allow me to easily replicate changes to database structure without requiring massive snapshots to be transferred and the delay applying them.
Even if SQL was on the way out, legacy applications would need maintenance for years, plenty of time to learn new technology whilst working with the old. It's not worth worrying about anyway, skills are often transferable.
If you do some searching on some of the SQL and Sysadmin subreddits you can find quite a few reccomendations. I haven't heard of Colaberry but that doesn't mean much. I've been looking around, since work's offering to pay for some additional training, and so far I've been most impressed with http://www.cbtnuggets.com/
This is the answer, theres no way your professor would have given this kind of assignment without teaching something about SQL more than "SELECT * FROM" beforehand. If you're not understanding it you need to read up more on it or speak to your teacher about it.
Ooo! I like that suggestion, and I utilized my original UNION method to get something that works to an extent... I wonder if there are other ways of doing it though. alter table dbo.Example add v_concat varchar(max) update dbo.Example set v_concat = isnull(cast(TRANS_DATE as varchar(max)),'') +isnull(cast(Class1 as varchar(max)),'') +isnull(cast(Class2 as varchar(max)),'') +isnull(cast(Class3 as varchar(max)),'') +isnull(cast(ReceiptNum as varchar(max)),'') select A.* from dbo.Example A join dbo.Example B on A.v_id = B.v_id and A.v_dup = 1 and A.v_concat = B.v_concat and A.ITEM1 &lt;&gt; B.ITEM1 UNION select B.* from dbo.Example A join dbo.Example B on A.v_id = B.v_id and A.v_concat = B.v_concat and A.ITEM1 &lt;&gt; B.ITEM1 order by A.v_id,A.v_seq
&gt;I'm getting a syntax error with this query, but honestly I'm not even sure I'm on the right track. 1. I think you are on the right track. You should essentially pivot the data to get your desired results. 2. Whats the syntax error? You went through so much to post this and that is probably the most important part you left out.
Wow, this is precisely what I was trying to get at. Let me analyze what the heck you did so I can utilize this in situations later. I couldn't figure out the join and you did it eloquently. Thank you!
Good point. I didn't post it because I wasn't sure I was even close to being on base. Error is: &gt; Syntax error (missing operator) in query expression 'competency.comp_id=key_behavior.comp_id INNER JOIN key_behavior_details ON key_behavior.kb_id=key_behavior_details.kb_i'. The last "d" is missing in "id", but I assumed it was a character limit issue.
Likely, since it is Access, the syntax error has to do with the way Access handles multiple joins. See [this stackoverflow](http://stackoverflow.com/questions/7854969/sql-multiple-join-statement) article for details. Essentially, you need brackets. It's really stupid and I think hurts anyone that learns using Access (high availability) and then transitions to anything else (or even vice versa). Edit: Here is the example from the article for those that don't want to click through SELECT ... FROM ((origintable JOIN jointable1 ON ...) JOIN jointable2 ON ...) JOIN jointable3 ON ... I've run into this before a loooong time ago which is the only reason I knew to look for the bracket example.
See my response to /u/Thriven as to what your issue likely is.
That did it! Well, I needed to add the "competency.name" field to the GROUP BY function. Thank you so much for the help. 
Does this make a significant difference for performance? How does this compare to a LEFT JOIN with NOT NULL?
This could give you invalid results if there are duplicate titles. To be more accurate and simplify the query, you could use something like this: SELECT title FROM Movie WHERE mid NOT IN (SELECT mid FROM Rating WHERE rating.rid = 205 )
Microsoft SQL
A dirty read is where you get (as a result of using NOLOCK) data that (for example) ultimately doesn't exist on the table, or has been changed on the table between the time you started and finished reading it, or was altered in a transaction, where you read it mid-transaction and the transaction was rolled back (resulting in, again, the data you read not actually existing on the table). IOW, what you think you get...doesn't really exist.
That's not 100% correct if you're looking for the median in an even number of values, where you should take the mean between the two values in the middle. Example: `MEDIAN(1, 2, 3, 10) = 2.5` Of course, your solution might be good enough for your case...
Whats the problem in making a call to RESTORE FILELISTONLY to get the logical name?
Well if you want to make READ UNCOMMITTED your default state then it's super simple. I'd post a screenshot but I can't access imgur from work. * Select 'Tools', 'Options' * Expand 'Query Execution' * Expand 'SQL Server' * Select 'Advanced' * Change the 'SET TRANSACTION ISOLATION' dropdown to READ UNCOMMITTED Ta-da, you'll always have dirty reads, but you'll never lock a table or database with a transaction. Remember though if you share a query with a co-worker they'll need to either throw in NOLOCK table hints or set the ISOLATION LEVEL at the beginning of the query.
That's true, thank you for pointing that out. In my case it's ok. I have thousands of rows and a very narrow range of possible values so it could not be off by more than 1.
Looking into the msdn articles, it would appear that ROUNDUP() is a DAX function and would not be readily available in SQL Server. As /u/PrehensileTongue suggested, try the Ceiling() function. For more information on DAX functions, [try starting here](http://msdn.microsoft.com/en-us/library/gg399181.aspx)
In your example table there is no primary key. Whats the primary key in the production table? Also it a fairly wide table (lots of columns?). It makes sense why including the v_id in your query would increase the time it takes for joins as v_id and v_seq is probably your primary key on this table. One other way, I can't promise it would be faster is to use TOP 100 PERCENT which allows you to do an ORDER BY in your CTE. Then in your left and inner joins use MERGE (left merge join or inner merge join). Your way is probably faster as sorts can be tedious and yours works but here is the code for shits and giggles. ;WITH seq as ( select TOP 100 PERCENT V_ID ,V_SEQ ,ITEM1 ,TRANS_DATE ,CLASS1 ,CLASS2 ,CLASS3 ,RECEIPTNUM ,ROW_NUMBER() OVER (PARTITION BY v_id,TRANS_DATE,Class1,Class2,Class3,Item1 ORDER BY v_id,v_seq) as v_item ,CONVERT(NVARCHAR,V_ID) + CONVERT(NVARCHAR,TRANS_DATE) + CONVERT(NVARCHAR,CLASS1) + CONVERT(NVARCHAR,CLASS2) + CONVERT(NVARCHAR,CLASS3) as CompositeKey FROM dbo.Example ORDER BY CONVERT(NVARCHAR,V_ID) + CONVERT(NVARCHAR,TRANS_DATE) + CONVERT(NVARCHAR,CLASS1) + CONVERT(NVARCHAR,CLASS2) + CONVERT(NVARCHAR,CLASS3) ), max_seq as ( SELECT TOP 100 PERCENT seq.CompositeKey ,MAX(v_item) as Max_v_item FROM seq WHERE v_item &gt; 1 GROUP BY seq.CompositeKey ,seq.ITEM1 ORDER BY seq.CompositeKey ) SELECT seq.V_ID ,seq.V_SEQ ,seq.ITEM1 ,seq.TRANS_DATE ,seq.CLASS1 ,seq.CLASS2 ,seq.CLASS3 ,seq.RECEIPTNUM ,CASE WHEN MAX(d.ITEM1) IS NULL THEN 'No duplicate' WHEN seq.ITEM1 = MAX(d.ITEM1) THEN 'Same' WHEN seq.ITEM1 &lt;&gt; MAX(d.ITEM1) THEN 'Different' END as LikeDuplicates FROM seq left merge join ( SELECT seq.CompositeKey ,seq.ITEM1 FROM seq inner merge join max_seq on seq.CompositeKey = max_seq.Compositekey AND seq.v_item = max_seq.Max_v_item ) as d on seq.CompositeKey = d.CompositeKey GROUP BY seq.V_ID ,seq.V_SEQ ,seq.ITEM1 ,seq.TRANS_DATE ,seq.CLASS1 ,seq.CLASS2 ,seq.CLASS3 ,seq.RECEIPTNUM ,seq.v_item Many people use left join and inner join and allow the SQL server to determine the best join (usually hash/loop). Merge joins are great though if your datasets are purposely ordered. This allows sql to join them simultaneously rather than scan one table and loop through the other. I'm glad you got it working and your solution was pretty sound based on the original table design.
ROUND ( number, precision). http://lmgtfy.com/?q=ms+sql+round
My interpretation of "a huge mess" means that database name, logical names and physical names didn't really follow any kind of standard. If that's the case then I don't know of a way around using WITH MOVE. Unless the physical location is the same on the source and target you have to do it anyway. If automation is your goal, you can do this all dynamically in TSQL, SSIS or in a PowerShell script.
Classy lmgtfy link... Here's another that easily returns the solution OP was trying. [http://lmgtfy.com/?q=ms+sql+round+a+number+up](http://lmgtfy.com/?q=ms+sql+round+a+number+up) You can likely see where the OP did actually google their use case and was attempting to use the top examples which were failing because, even though they are listed as being for SQL Server 20xx, they're actually a DAX function and aren't built-in functions like round(), floor() and ceiling().
My bad, after checking MSDN I can see now that MS insists that is the function available in MS SQL 2012 server (it causes the same error for me ("'roundup' is not a recognized built-in function name"). Roundup seems to be way more than 'ceiling' though since it's a) supposed to round up away from 0 and b) 'rounds up' to nearest non-integer multiples also. ROUND( PI(), 4) actually gives the same result as the ROUNDUP( PI(), 4) though. 
It is a built-in function available on 2014 SQL Server, apparently.
Looking at the elements view on Chrome it looks like some flavor of [Highcharts](http://www.highcharts.com/demo). Lots of div tags referencing the name and their site looks to have an example chart similar to the netmarketshare.com one.
I was/am attempting to build a restore script that I can execute once; restoring all db's to their correct location on the new server. Using FILELISTONLY means I have to run that against each db *first* to ascertain the logical name before I can build the restore script, which is more work :)
Do you mind putting a sample of the relevant tables?
Sure thing. Would you prefer snapshots or an example like I did above?
You have facility_id in your first table twice, is one warehouse_id and the other store_id? Your other tables would need to reflect that. Also, where is 'QTY' coming from in your desired results?
Sorry, I got a little hasty while writing it all. One would be WAREHOUSE_FACILITY_ID and the other would be STORE_FACILITY_ID. And QTY would technically be STORE_IN_STOCK_QTY while the other IN_STOCK_QTY would be WAREHOUSE_IN_STOCK_QTY. I have this actually written in my code but while rewriting, I skipped over it. The code actually runs and some of the data is accurate but I keep getting ridiculously small counts of records or a crazy Cartesian product, depending on how I change up my joins. I've messed with this 9 ways to Sunday and I can't seem to figure out what I'm doing wrong.
&gt; And QTY would technically be STORE_IN_STOCK_QTY while the other IN_STOCK_QTY woul So your Inventories table is displaying which facility ID? Both? FYI if you're using oracle to copy a data set use CTRL+SHIFT+C to copy the exact column headers as well. You also dont have a WAREHOUSE_IN_STOCK_QTY in your examples, only ALLOWED_QTY. Is that what you mean or are you using a different field?
Its HighCharts. I *think* I looked at HighCharts in the past, but after testing it was no use to me, as their graphs kinda need to be a minimum size to be effective/useable, and that was too big for me I've since found http://canvasjs.com/html5-javascript-pie-chart/ which looks really good, although I've yet to play with it
 select a.item_number, r.region_code, r.reg_desc, a.warehouse_number, a.facility_id , i.in_stock_qty, a.allowed_qty, r.store_number, r.facility_id_2 , i1.in_stock_qty from reg_whs_str_assoc r left join allowances a on r.warehouse_number = a.warehouse_number and r.facility_id = a.facility_id left join inventories i on i.facility_id = a.facility_id and i.item_number = a.item_number left join inventories i1 on i1.facility_id = r.facility_id_2 and i1.item_number = a.item_number order by 1,2; r.facility_id_2 = your second facility_id
What is tpc
We're comparing different hardware and os with the same database on top.
Here is my take, but tested in an Oracle instance. I basically just compare counts of uniqueness between what you have and the inclusion of the item_id. you are only reading the table once albeit with a full table scan. select t.*, case when item_cnt = 1 and class_cnt &gt; 2 then 'different' when item_cnt &gt; 1 then 'same' else 'no duplicate' end test from( select V_ID ,TRANS_DATE ,item1 ,Class1 ,Class2 ,Class3 ,ReceiptNum ,v_seq ,v_dup ,count(*) over (partition by v_id,TRANS_DATE,Class1,Class2,Class3, item1) as item_cnt ,count(*) over (partition by v_id,TRANS_DATE,Class1,Class2,Class3) as class_cnt from Example ) t
Sqlio, iometer, vmware io analyzer.
 SELECT year(t.visitdate) tYEAR, month(t.visitdate) tMON , sum(case when t.custid not in (select distinct custid from table where visitdate &lt; STR_TO_DATE(CONCAT('01,',month(t.visitdate),year(t.visitdate)),'%d,%m,%Y'))) THEN 1 ELSE 0 END) tNEW , sum(case when t.custid in (select distinct custid from table where visitdate &lt; STR_TO_DATE(CONCAT('01,',month(t.visitdate),year(t.visitdate)),'%d,%m,%Y'))) THEN 1 ELSE 0 END) tRETURN FROM Table t GROUP BY year(t.visitdate), month(t.visitdate) 
Thanks, c3js.org and canvasjs.com both look like great solutions, and c3js.org seems to be open source too.
profiler, perfmon
If you get time upload some data sets to dropbox in excel using the exact column names (CTRL SHIFT C in oracle). I'd like to see them along with a proper desired result with matching column names.
I admittedly have not. My actual usage of it is buried in a mountain of barely readable, surprisingly executable code. The next step was to set up a more simplified scenario and try that, just wasn't sure if anyone happened to know off the top of their heads.
thanks lukeatron
The reason why I want to do it in a text document because I am not that familiar with SQL but more with creating batch files. So if there is a way to do that I would prefer it. However your method you explained about making a new stored procedure would work, I would just have to learn a thing or two more about SQL. Thanks for the assistance I might try that approach.
Im looking at the original stored procedure, and I think I will just make a copy of it and modify. Easier than I realized. Thanks for the idea.
The easiest way to put query results in a text file is with `INTO OUTFILE /PATH/TO/FILE`
Q1) In order to do this you must choose the criteria for the other columns to be displayed. For example if you have a People table with first name, and last name and you wanted to select both columns, but only distinct first names, then what last name should be returned for the name "Bob" if the following two rows exist: BOB JOHNSON BOB SMITH Specifying "Distinct" on First name will give you a list where each first name only gets a single record in the resultset. What you CAN do is specify a "rule" of which data to use by using a GROUP BY. In the above example you could do this: SELECT first_name, MAX(last_name) FROM people GROUP BY first_name With this query you will still only have 1 record for each first name, but they will be associated with the last last_name alphabetically of all of the ones paired with the first name.
Nice, thanks
I'll look up CTEs and get back to you if I can make it work. For the second part, I've been using adddate already but it's a lot of typing and makes me query new whereas, I'd like to say something like select...from...where *option &lt; SameOption.nextday 
You made a very good point with the name example. I think you are right, group by will do what I wanted. Thanks
No, I meant that my table name I want to find the data in, is also by the same name as @ticker. for example AAPL ticker query will only run inside a table called AAPL. when I change ticker to MSFT at the top of the query file, I want the remaining queries on the page to use MSFT table. I know I could have put all data inside one table but I just ended up splitting each stock for whatever dumb reason. 
Did you make this? Not to be rude but this is an incredibly simple concept that I think even beginners don't need a video to understand. Also it's not described very well. Would be better if you went into the detail of other wild cards besides the % sign like [0-9], [a-z], _, etc.
the video contains misleading information '%ou%' will find more than just values with 'ou' occuring "in the middle" the video is very poor quality, and the text is not legible
Thank you - this seems to work perfectly - and is very fast. Everything else I tried was very very slow.
The following returns the numbers that exist in the second table, along with the number of times it appears. SELECT c1.Telephone, COUNT(c2.Telephone) FROM ETLTestDB.dbo.C1 AS c1 LEFT JOIN ETLTestDB.dbo.C AS c2 ON c1.Telephone = c2.Telephone WHERE c2.Telephone IS NOT NULL GROUP BY c1.Telephone
 ROUND(@NUM+0.5,0)
Depending on how clean your data is, you may want to use something like REPLACE(Telephone, '-','') to remove the dashes when checking for duplicates.
I would definitely go with CTE's for this. You could also try table variables if you don't like CTE's for some reason. My understanding is that CTE's don't actually return the data until they are called, table variables store the data in tempdb like a temp table, but do not persist after the query completes.
INTERSECT is a set operator that does exactly what you're looking for. select [Telephone] from ETLTestDB.dbo.C1 INTERSECT select Telephone from ETLTestDB.dbo.C2
In a BIRT report (probably most BI reporting) it produces more efficient SQL to dynamically update the querytext to adjust the WHERE than to try to simulate this logic fully in the SQL. However: WHERE ( ( @CompanyCode IS NULL AND @ClubCode IS NULL ) OR ( @CompanyCode IS NULL AND ClubCode = @ClubCode) OR ( CompanyCode = @CompanyCode ) )
I made one to log my daily exercise data. For practice.
Course shows as $9 for me.
Use your music collection, or [That Jeff Smith's Beer database](http://www.thatjeffsmith.com/archive/2011/11/free-beer-or-how-to-get-free-demo-data-from-itunes/).
I've collected traffic information for years via screen scraping one of those national traffic info sites, and information on speed cameras on highways. THe first enabled me to analyse what will be the best time for me to start my commute. This differs per weekday. 
Sports information is a good choice. There are just tons and tons of numbers out there. This will also give you plenty of practice in cleaning up data and getting it into your database. 
Oracle and SQL are two different things. Is 'SQL' SQL language in Oracle Database or is 'SQL' short for SQL Server?
If your Class variable is a number and not a string you will get that, just remove the quotes from the "Class IN ..." clause.
Not a good one. Parent commenters have it right. I'm not a fan of trying to export and import in this case. I would take a backup (not detach/attach) and restore it to a SQL05/08 box. Take a backup of that and restore to your 2014 box.
I'm not sure that implies that. Having sustained context is very useful. All the time I spend manually interacting with a database is through a tool like mySqlWorkbench (or Sql Server Management Studio, etc.) It's like having a blotter pad always handy rather than a short scrolling history. 
You can use notepad++ with sql syntax highlighting if you just have to have a text editor. However, what makes database development bareable is context sensitive code completion. Small things like a dropdown for the columns in a table, that you referenced by an alias, stuff like that. A "dumb" macro that completes keywords dosn't quite do it (at least for me). Sure you can run everything trough the command line, and sometimes thats the better choice, but why whould one want to every time? The religiouse fanatism about what editor to use vs what ide to use, is a bit miss placed I'd say, choose tools appropriate to the task and don't feel smug about using @currentlyHotThing Oh and if you compare MySql Workbench to MSSQL Managmet Studio or Oracle SQL Developer, well, how can I put this.... I'm not happy sitting in front of Mysql Workbench. On a side note, the mySql RDMS is the service interpreting and executing your sql query, the command line you type sql into is just a client sending the sql to the actual server. The command line is just as much of an interface as mysqlWorkbench is.
I still do most of my datamodeling by pen and paper, I only create ERD's as a finalizing step, I too sometimes prefer "low level" ;) oh and anyone will do very well, to not rely on the workbench to do things. Knowing how to query the meta data directly is very useful, even if you have a IDE. Its quite often much faster to generate the code of what you want to do, by knowing where the data is to be found. That and actually knowing how it works. So knowing how to do it by command line is still a very useful thing sometimes. So that is a very good point on not relying on the clicky clicky frontend.
I'd suggest a home budget. Create a table that can be your registry with income and expense and you can tie together categories, payers, accounts, and tons of other stuff to build a true personal accounting system. Then you can use SSRS to do reporting, heck even SSAS to analyze your numbers over time. Yuu can even build what-if scenarios or budgets to compare what you're spending back to what you have. Using SSIS you can even build a system to import in your bank files if you can download them in CSV or some other format. 
I believe to get to 2008 you still need to stop at 2005.
also make sure you have all the correct punctuation in all the correct places.
I have done log shipping from 2000 to 2008. On mobile so I can't check, but I am fairly sure you can go 2000 to 2008. Either way, same process.
I'm sorry, I didn't mean for my last comment to sound snippy or nasty. When log shipping, you are just moving data. When restoring or attaching, you are moving actual objects created in a different ss format. ss 2005 understands the 2000 format but 2008+ only understands 2005+ formats.
Overview of the Transfer Database Task in SSIS: [http://msdn.microsoft.com/en-us/library/ms141204.aspx](http://msdn.microsoft.com/en-us/library/ms141204.aspx)
Thanks for your reply How i can count ID coloum to find the ID with the most appearances? Î™ correction my question maybe is better Any reply?
You can go straight to 2008 and 2008R2 http://technet.microsoft.com/en-us/library/ms190447(v=sql.105).aspx Rule of thumb is you can restore 2 versions up (I had to double check if R2 counted as a 3rd version and it does not) 
Ms sql can use coalesce. Set @code = Coalesce(@companycode, @clubcode, 12) If @companycode is null it will check @clubcode. If @clubcode is also null it will use my default value of 12.
Basically you would have to use dynamic SQL, basically make the SQL statement and store it as a variable then execute it. This is not very popular with my developer friends. http://msdn.microsoft.com/en-us/library/ms188001.aspx for MS SQL
&gt; If you're looking for people who did all 3, you can use a nested and like you show here: event_type = 'Sent AND event_type = 'Open' This is what I'm after, except when I write the code it looks like select customer_id from tablename where campaign_id = 789 and event_type = 'Sent' and event_type = 'Open' But because each record is a different event on a new row, no row has both sent AND open - and I'm left with a blank result. I want to write a query that will select one of the following Present me with a customer ID who has ONLY Sent and not opened their email (customer ID = 456) Present me with a customer ID who has ONLY Sent + Open, but not clicked on the email (Customer ID 123) Present me with a customer Id who has ALL of Sent + Open + Click (Customer ID 789) I have millions of customers and wanting to segment them into 3 different groups. 
Not sure how to do it in mysql, but you would self join this table to itself something like the following (SQL Server) select a.customer_id from tablename a join tablename b on a.customer_id = b.customer_id and a.campaign_id = b.campaign_id and b.event_type = 'Open' join tablename c on b.customer_id = c.customer_id and b.campaign_id = c.campaign_id and c.event_type = 'Click' where a.event_type = 'Sent' and a.campaign_id = 1234 
/u/Muter is after a self join for two of the three situations. Giving him a comma delimited list will just give him a list and won't do any actual filtering. &gt; Present me with a customer ID who has ONLY Sent + Open, but not clicked on the email (Customer ID 123) Self Join (see my other comment, would need to be modified to remove the 'Click' portion) &gt; Present me with a customer Id who has ALL of Sent + Open + Click (Customer ID 789) Self Join (see my other comment) &gt; Present me with a customer ID who has ONLY Sent and not opened their email (customer ID = 456) This will likely be either a not exists or a not in (or whatever is similar). You would take all customer_ids with a Sent and make sure they don't have either an Open or Click event_type. Edit: Looks like the original post was deleted *shrug* take it for what it's worth.
Thanks. I'm still reading replies, I just found it was easier to manipulate outside of SQL. I wasn't sure if the self join option was working as I let it run for about 40 odd minutes before cancelling the query. Deleted because I've manipulated my data in a way I want - no need to waste anyones time :) Thanks for your input though.
Edit, on re-reading you have them in rows not columns so this won't work You haven't mentioned your RDBMS? Could you use a select with a case statement in it? eg SELECT "duration" = CASE WHEN logouttime is not null THEN datediff(minute,logintime,logouttime) WHEN logouttime is null THEN datediff(minute,logintime,lastactivity) ELSE 'Error' END FROM weblog 
QUOTENAME (@Table). Also might check that there's a record in sys.database_files whose name matches @Table. But yeah, this seems like an odd idea. EDIT: ...and I'd include the database name and schema in the static part of the string. EDIT2: sys.database_files? WTF is wrong with my brain. Sys.tables or INFORMATION_SCHEMA.tables...
Name at least three differences between ISNULL and COALESCE. Have them solve a problem using PIVOT/UNPIVOT Have them return identical rows from two different tables Given two strings, have them write a query that returns the number of times that string2 appears in string1, without looping. EDIT: on further consideration, these aren't that challenging... EDIT2: Gold! Woohoo! Thank you very much.
Lineage Table has Name, Parent Add a single column "Siblings" of datatype int and update it with the count of the number of siblings each individual has. Do not use cursors or loops.
http://projecteuler.net - Start solving those using recursive CTEs.
thanks a million!
Thanks so much! I'm sorry I didn't phrase my question better, but this is very helpful. I'll go through it all in my class tomorrow
You might try w3schools.com or tutorialspoint to get you started. Are there specific concepts you are having difficulty with? Do you have programming experience?
I started out using your approach, then started to think about using it for reporting and BI. Both ways will work, how you're going to be using it would determine the better method. He deleted it all for whatever reason, so it's completely moot at this point!
Have you done this [3-minute test](http://use-the-index-luke.com/3-minute-test)? The rest of his site explains a lot of stuff about the different SQL platforms every DBA should know.
Given that what you want is dynamic sql and it appears you are taking a course/class of some sort, there is an excellent article about Dynamic SQL by Erland Sommarskog that anyone using it should likely read. [The Curse and Blessings of Dynamic SQL](http://www.sommarskog.se/dynamic_sql.html)
5/5 =) Thanks, I had forgotten about that site in the sea of stuff in my pinboard.
Here's one approach. create table #test( userid varchar(50) not null, actiontype varchar(50) not null, actiontime datetime not null) insert into #test values ('test1','login', '2015-01-15 00:05'), ('test1','activity', '2015-01-15 00:10'), ('test1','activity', '2015-01-15 00:15'), ('test1','activity', '2015-01-15 00:17'), ('test1','logout', '2015-01-15 00:17'), ('test1','login', '2015-01-15 01:05'), ('test1','activity', '2015-01-15 01:10'), ('test1','activity', '2015-01-15 01:15'), ('test1','activity', '2015-01-15 01:17'), ('test2','login', '2015-01-15 01:05'), ('test2','activity', '2015-01-15 01:10'), ('test2','activity', '2015-01-15 01:15'), ('test2','activity', '2015-01-15 01:17'), ('test1','login', '2015-01-15 02:05'), ('test1','activity', '2015-01-15 02:10'), ('test1','activity', '2015-01-15 02:15'), ('test1','activity', '2015-01-15 02:17') go select l.userid, l.logintime, logout_flag = case when nl.logintime is not null then 'Y' else 'N' end, logouttime = max( allactions.actiontime) from ( select userid, logintime = actiontime from #test where actiontype = 'login' )l outer apply ( select top 1 logintime = t.actiontime from #test t where t.userid = l.userid and t.actiontype = 'login' and t.actiontime &gt; l.logintime order by t.actiontime asc )nl join #test allactions on allactions.userid = l.userid and allactions.actiontime &gt;= l.logintime and allactions.actiontime &lt; isnull( nl.logintime, '2099-01-01 00:00') group by l.userid, l.logintime, nl.logintime 
No problem, happy to help :) thanks for the gold!
Thanks for doing this. Much appreciated. However the table has 3 million rows, and this only returned 92 rows
Going through a test review for even the basic exam can help sharpen or find gaps in your learning. [This study guide for the MTA has a lot of basic questions to go over.](ftp://ftp.certiport.com/Marketing/Mta/docs/MTA_SSG_DbAdmin_individual_without_crop.pdf) CTE's are something that show you have a solid foundation of SQL. [This guy has almost 70 videos over SQL Server, and he occasionally talks in the video how this is a good question to be asked in an interview.](https://www.youtube.com/playlist?list=PL08903FB7ACA1C2FB) I've seen these threads come up before and often people can agree on certain things. One was being how it is hard to determine what is something everyone should know. Someone talked about the importance of CTEs, while the next comment said he never interviewed a person that knew about them. I think the important part is knowing foundations, so that no matter the question, you can competently hold a conversation showing knowledge on SQL.
[DBExt](https://github.com/vim-scripts/dbext.vim) perhaps? I just use psql but [pgcli](https://github.com/amjith/pgcli) is tempting me - when it's matured a little I may conisder a switch.
Website analytic packages (which do not depend on your pages inserting records into your own database) should be able to track this for you. Why re-invent the wheel? Create a Google account for your shop, drop in the requisite JavaScript to trigger their analytics, done.
Not knowing the table, data values or values distribution, I really only can ask 'So?'. In other words, why '92' is not the right number? 
It might be useful to make a site for someone else, because you'll also get experience in how to deal with the real-world case of changing requirements. I recently created a recipe site for my wife. Of course, the more she uses it, the more ideas she has. Learning how to cleanly alter your structures to support growth is a great skill to have. 
What exactly are you looking for? Logically, it looks like you want one row for each name/year combo. If that's the case, try replacing NULL with 0. NULL is the absence of data, not zero, and is not recognized by math functions. 
Can i see your example on what you created?
You could just collage the results identically then compare them.
Hi! I'm fairly new to SQL as well. MySQL is a freeware alternative to SQL Server. It's worth getting to know both of them, but you may want to start out of on [MySQL] (http://dev.mysql.com/doc/mysql-macosx-excerpt/5.5/en/macosx-installation.html). Even still, I write most code (including SQL!) with Notepad++, but [Sublime is also good] (http://www.sublimetext.com/2) but the full version is paid. (When it comes to polishing up your SQL knowledge, you can try out some [real examples with SQLzoo!] (http://sqlzoo.net/wiki/Main_Page). Those are some resources will help, hopefully someone more experience will chime in!
Sorry, I should have been more clear. TechNet is a website and service run by Microsoft. You can download a 180 day trial copy of Windows Server 2012 and SQL Server and run them from a virtual machine. I use VMware Fusion, but I think virtual box would also be a free alternative. 
Install sqlite3 (if you don't already have it). Then you can run it from a bash prompt and have a basic SQL command line. Create some tables, insert some data, and start selecting stuff!
Assuming the third table is your desired results, you want a `FULL OUTER JOIN`, not a `UNION`. SELECT COALESCE(sg.Person,bg.Person) AS Person, COALESCE(sg.Year, bg.Year) AS Year, SUM(combo.Sales) as Sales, SUM(combo.Budget) as Budget FROM sales_grouped AS sg FULL OUTER JOIN budget_grouped AS bg ON bg.Person = sg.Person AND bg.Year = sg.Year GROUP BY COALESCE(sg.Person,bg.Person), COALESCE(sg.Year, bg.Year) ORDER BY COALESCE(sg.Person,bg.Person), COALESCE(sg.Year, bg.Year) In an ideal world, you'd have an actual unique ID to join against instead of the person's name, since names are not guaranteed to be unique. You don't have your full query posted so I can't help you with that. It's been a long time since I've used aggregates with a `FULL OUTER JOIN`. You'll need to verify your results. It might be more accurate to do your aggregates in another CTE, and then `FULL OUTER JOIN` the results of the aggregated queries. Depending on your data, you can easily create a partial cross join doing things this way and inadvertently multiply your sums.
The plan is to add a new column to the table, and have a stored procedure that runs daily and updates the new column with the logged in value. So for any given 'login' record there should be a time logged in column. So removing the group by and changing the MAX to FIRST_VALUE will achieve this (without the update obviously). I haven't seen FIRST_VALUE used before, so i'll have to figure out how it works
This does make sense to me but then that's what my original query (with the group by ) should do, i.e. it should create a result set record per a login record. Since you plan to update only login records, this should suffice, imo.
Maybe its my phone but your WHERE clause doesnt appear to actually evaluate an expression? Use a CTE or a derived table to do your aggregation and then inner join Table to it on the grouping columns. SELECT T2. * FROM ( SELECT COUNT (*) AS RowCount , COL1 , COL2 FROM Table GROUP BY COL1, COL2 ) AS T1 INNER JOIN Table AS T2 ON (T1. COL1 = T2.COL1 AND T1. COL2 = T2.COL2) Sorry for possibly shitty formatting. On my phone. 
Maybe they were afraid the query would run too fast without it?
what was their goal here?!
No harm, no foul?
That is a really good question
Well I do work at a consulting company. So we can always make the query slow then charge the client for us to make it faster... I guess this junior developer is either really dumb, or thinking next-level lol.
maybe it started out as something that made some sense before it got changed to this somehow. Though dealing with jr devs in the past, it's probably exactly what it looks like. 
&gt;As someone who is still breaking into the SQL field. I hope you explained to this junior developer why this was a bad thing. Why? On it's face it's bad - there's no need for an explanation. If your code does something useless, then it's bad, period.
Truth be told, it is a finance guy who knows enough SQL to be extremely dangerous. It is a legacy system that we dont support or care about. I have been tasked with taking the business logic out of that application into a new system, which is when I ran into this.
He meant that as a junior developer, being told that your code sucks is not useful. Being told WHY your code sucks can help you improve.
As someone who will be learning SQL next semester, can you tell me what the problem is here?
Sure! Usually when you have a case statement, you are transforming the data. So an example would be "If Column1 = 'Insurance Refund' Then 'InsRef'" That way we are changing 'Insurance Refund' to 'InsRef'. The way the junior code is written, he is saying if it is 'Insurance Refund' then change it to 'Insurance Refund'. Essentially accomplishing absolutely nothing, except taking up processing power and slowing down the query.
I get that, but I have some junior developers that if I told them why this bit of code sucks would try to improve it, rather than understand that it's completely unnecessary. At least in my experience, some junior devs are really that boneheaded. I have a hat that I make them wear.
lol yep, epic
Makes perfect sense now, thanks! 
what is the difference between selecting directly from a linked server/table vs selecting from openquery?
If I was the owner of the DB, I'd probably do this, but I just use this to get data out, promised our IT director that I wouldn't write back =) Thanks again, I'm planning to get drunk fast tonight as well!
Is the implication here that this code does nothing? If so, that's not true and the snark is unwarranted - this code does 'proper case' (upper case for the first letter of every noun/verb) for status values in a specific set, leaving the others unchanged. It is a kind of 'brute force' approach so depending on the use case it could have been done better, probably. 
you're saying even an alternating caps version will meet the case statement and change it to an uppercase? why would you need to do this and wouldn't it be smarter to handle that type of filtering on the input side?
As someone who just moved from Jr Developer to full time dev, this guy would be the worst person to work under. Having someone give me constructive criticism was a great way to learn.
Performance hit is the least of the concerns. The amount of time spent on that useless CASE is dwarfed by the amount of time doing the actual UPDATE. Computer time is cheap. Programmer time is expensive. Every piece of code has a cost in human understanding. Every piece of code has a cost in maintenance. Every piece of code is a place for errors to occur. 
Oh hell yeah. This is one of the best overall looks at index usage I've seen: SELECT o.name AS ObjectName , i.name AS IndexName , i.index_id AS IndexID , dm_ius.user_seeks AS UserSeek , dm_ius.user_scans AS UserScans , dm_ius.user_lookups AS UserLookups , dm_ius.user_updates AS UserUpdates , p.TableRows , 'DROP INDEX ' + QUOTENAME(i.name) + ' ON ' + QUOTENAME(s.name) + '.' + QUOTENAME(OBJECT_NAME(dm_ius.OBJECT_ID)) AS 'drop statement' FROM sys.dm_db_index_usage_stats dm_ius INNER JOIN sys.indexes i ON i.index_id = dm_ius.index_id AND dm_ius.OBJECT_ID = i.OBJECT_ID INNER JOIN sys.objects o ON dm_ius.OBJECT_ID = o.OBJECT_ID INNER JOIN sys.schemas s ON o.schema_id = s.schema_id INNER JOIN (SELECT SUM(p.rows) TableRows, p.index_id, p.OBJECT_ID FROM sys.partitions p GROUP BY p.index_id, p.OBJECT_ID) p ON p.index_id = dm_ius.index_id AND dm_ius.OBJECT_ID = p.OBJECT_ID WHERE OBJECTPROPERTY(dm_ius.OBJECT_ID,'IsUserTable') = 1 AND dm_ius.database_id = DB_ID() ORDER BY (dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups) ASC I also lately have been adding in this line so I can sort out who's been scanning recently: , dm_ius.last_user_scan AS LastUserScan
should be a initcap() function for this... if I understand what you're getting at.
 CompanyID | employee |Is_job_x | startdate ---|---|----|---- 111| James smith | 1 | 1/8/1995 111 |Jane Doe| 1 |1/12/2014 111 | John Doe | 0| 1/10/2001 222 | Jane Smith | 1 |9/8/1999 222| John Smith | 0 |9/8/1999 
But I'm not performing an aggregation? 
Join the table on itself where Is_job_x = Is_job_x AND a.Startdate != b.Startdate?
Placeholder for future/expected translations?
Thanks for that youtube link, that guy has a wonderfully relaxing voice! :) The study guide is a nice basics re-enforcement read as well. --edit. The chap from the video has some neat questions here... http://csharp-video-tutorials.blogspot.co.uk/2014/05/sql-server-interview-questions-and.html
Okay Mr. Lazy. DECLARE @Login VARCHAR(100) = 'LoginIAmSearchingFor'; DECLARE @Exists VARCHAR(10); SELECT @Exists = 'True' FROM Users AS u WHERE u.Login = @Login; SELECT ISNULL(@Exists, 'False') AS [Exists];
Mr Lazy would like to buy you an adult beverage! But since I can't send you a virtual beer, here's another month of Gold. Thank you.
&gt; where cnt &gt; 1 this part is redundant 
Don't Worry. I got you Covered. one **[Virtual Beer](http://beeroverip.org/)**
It is a proper case function IIF (if and only if) the column/database collation is case insensitive. So, it isn't completely useless, depending on the intent.
At least that's accomplishing something, if ~~a bit~~ very inelegantly.
At least they indented things well... Otherwise massively struggling to figure out why someone would go through the trouble of typing that pretty useless statement! 
I know the university of... Stanford (?) had a free course on databases (where I first was introduced to SQL) but I'm not sure if its still accessible and had some tests etc. Maybe it's just me, but I've not learned much about coding from videos. I've always found that doing the coding was where I learned. I've found w3schools as a good site to learn most coding languages from, if you're interested. 
Not sure why down vote. Update = X lock on row, IX lock on page. Plus writes to t-log and data pages. For all we know this iterates over every row of a 1M+ row table.
Glad you enjoyed it! Thanks for the gold!
ha, actually I formatted it before uploading it, as I was too embarrassed by its sloppiness to put it up the way it was.
This just seems incredibly unnecessary. While the update statement is utterly pointless (given that the intent was not to change the casing of the values) I don't see any value in essentially mocking a junior developer whose very title implies a lack of experience. If this was done by a certified DBA I could see the humor but a novice SQL coder doing this? Not so much. 
Or he is paid by the hour. 
Just download a database system like MySQL, and use that. Also, page numbers basically useless here, we all don't have the same book as you.
cool, thanks, and i just copied and pasted and forgot to take that part out! : ) 
Which it is by default in MS SQL Server.
This is the funniest thing I've seen today. I love it. 
&gt; SSIS can you explain what SSIS does? 
what about Select ISNULL((Select 'True' From Users Where Login = @Login),'False')
This is an interesting problem, quite complex considering how simple the schema is. I'll stick around to see if someone posts a solution. May be I can find some time later to try it by myself, good luck
I think this recursive query will work for you: WITH flightCTE as ( SELECT '-'+idFlight+'-' IdList, destAirport, arrTime, 0 connection, deptAirport+'-'+destAirport path FROM FlightTable WHERE deptAirport = 'LGA' UNION ALL SELECT fCTE.IdList + '-'+ft.idFlight+'-' IdList, ft.destAirport, ft.arrTime, fCTE.connection + 1 connection, fCTE.path+'-'+ft.destAirport FROM FlightTable ft JOIN flightCTE fCTE ON ft.deptAirport = fCTE.destAirport AND fTCE.path NOT LIKE ('%'+ft.destAirport+'%') -- Don't revisit somewhere you've been AND ft.deptTime &gt; (fCTE.arrTime + padding) -- Don't schedule a flight you can't possibly be on WHERE fCTE.connection &lt; 5 -- to limit connections, otherwise there is probably a path that visits all airports in your system ) SELECT ft.idFlight FROM FlightTable ft JOIN flightCTE fCTE ON fCTE.IdList LIKE ('%-'+ft.idFlight+'-%') AND fCTE.destAirport = 'BKK' Plug your sample data into SQLFiddle (set to SQLServer or something that supports CTE) if it doesn't work and I can test and tweak it. This gives the list of flightIds, just join your other tables and order it for your desired output.
I'm not a dba myself... But this looks super helpful! Thanks OP /s
Don't drop all the tables.
Damn, I just realized it's only going to return the final flight. Similar to [path], concatenate the flightids with a delimiter (EX '-flightid-' in case you have 3 and 4 digit ids) . Then you can do the final SELECT as SELECT ft.idFlight FROM FlightTable ft JOIN flightCTE fCTE ON fCTE.IdList LIKE ('%-'+ft.idFlight+'-%') AND fCTE.destAirport = 'BKK'
Also, a good description of the job you're expected to perform would be really helpful. There are many specializations within the DBA realm, so knowing what you're expected to do will be hugely helpful.
Are you forced to use MySQL?
[I found this article a while ago, I'm not sure how helpful you'll find it. But hopefully its better than nothing.] (http://www.essentialsql.com/what-are-ten-things-a-junior-dba-should-know/)
My apologies. I have three points juxtaposed. Let me rephrase. 1) The amount of time spent on the CASE part of the query is measured in microseconds, whereas the DELETE itself is measured in milliseconds. The "performance hit" of the CASE is not worth measuring. 2) The real cost of that CASE is not in terms of how many microseconds were wasted executing the code, but in how many collective hours programmers spend looking at the useless code trying to understand what it does and why it exists. 3) The cost of bugs is the biggest cost, and that code that does nothing is a place for bugs to be introduced.
\#1 Master backup and restore. Nothing sucks harder than learning that your backup/restore plan has a hole in it when you need to execute it. Also, if you master backup/restore, when you do things that might mess up the database (or someone else does), you won't sweat it because you know you can go back. * Practice a variety of scenarios. * Practice with a variety of tools (backup/restore vs import/export) \#2 Familiarize yourself with the data dictionary. I use data dictionaries to write scripts, automate tasks, monitor system performance, find locations of things, and a whole bunch of other things. All of those tasks would be time intensive or impossible without knowing the data dictionary. \#3 Familiarize yourself with the syntax. Learn what statements you can execute, what options those statements have, and what the statements/options do. The 3 main groups - create/alter/drop [object admin], grant/revoke [security admin], and select/insert/update/delete(/merge) [data admin] - will cover most of what's available. There are some standards that most databases follow and some things that will be specific to your database. As for resources - can't help you with that without knowing what database(s) you're working with.
You give to little info to answer. My questions to you : - Why looping - What are the table definitions - What are the index definitions (!!!) - Whats the execution plan - Wtf is @span All I can tell you, calculating the hash key over 10 million rows, in a loop, will take up some seriouse cpu. Oh and that cross apply, will force loop joining to the subquery/function, that might hurt you quite a lot on a large table, so you might want to consider rewriting the entire thing.
Would a pic of the data help? [here's](http://i.imgur.com/f8diTnP.jpg) a pic. Link to the foundation is in the right hand column -- see how #10 has 5 entries, one for each year 2008-2012? I'd want the 2012 amount. Or for instance, #129, two years of data, two types of data, but the date is formatted differently. 
Seriously, we don't need yet another "here is how to select data" tutorial on /r/SQL. Please ride the sidebar. We don't allow basic tutorials here anymore. If you have SQL skills you wish to demonstrate why not make an advanced tutorial? Otherwise please try again at /r/learnsql
@span is how many rows I'm processing at once, basically a fill-in for 500k. As for the big table, it's 76 fields and not normalized (hence the artificial key). None of the filtering is ever done on the PK, even when joining to an equally large table with more information - they join on two fields which have a non-clustered on them (table A in the example, is a subset previously pulled out from the second big table with a clustered on the two join fields), however because of all the other filtering going on it was always using a full table scan instead of any of the indexes we set up. Because each measure uses different filtering, it was determine inefficient to have a separate non-clustered for each combination and an all encompassing index would have 30+ fields included. The subquery and looping forces the clustered index scan which is the most efficient solution we've come up with thus far. Other bonuses of looping in this case, in addition to forcing the clustered index - * can start and stop, pick up where it left off * added debugging messages to track where it is in the loops / how much longer has left * can start analyzing the results while it's still running, make sure I'm not wasting 4 hours on a bad query As for the function, it contains business logic we didn't want to burden the other developers with for simplicity. It's not going anywhere. I can post the two execution plans for comparison sometime over the weekend.
For sure. The query I posted above is my go to for index usage evaluating and like you say it's vital to come back and check that they're actually being used after they're added. Hek even viewing the existing index/table reads vs. writes is a good way to make a decision ahead of adding something to a production database.
Intriguing - i'll look into that. 
The MSSQL example can be done easily with a window function ([`RANK()`](http://msdn.microsoft.com/en-us/library/ms176102.aspx)). [SQL Fiddle example](http://sqlfiddle.com/#!3/242050/6) Why artificially impose the "no `RANK()` or `ROW_NUMBER()`" restriction if you're using SQL Server 2014?
Yes. This is a perfect example of when to use window functions. 
Hey OP, before you shit all over this dude's work, did you go discuss why he punched it out? He might be doing something you don't know about.
Hi Mike, thank you for your reply. I did check with the guy, he said he thought we might need to change the names later, so he put in this code as a placeholder. Not a bad thought, but then he forgot about it, and the code is in the production application now causing performance issues. Thanks again for checking in!
Try just "ECHO ON"
You could try and run a nested UPDATE query to update all values within a specific sub-query. You'll need to already have the flag field created and all set to 0. So for all values that exist in table_2 it would look similar to below: UPDATE Table_1 Set Flags = 1 where ClientID NOT IN (Select ClientID From Table_2) 
Yes, your feedback is helpful. I will do both. Basic tutorials to /r/learnsql and advanced tutorials here. Thank you!
You are right... but it would be more helpful if you had posted a better statement and explained it so that the newbie op could understand what makes it better and why. 
Getting kid dressed to go to the zoo and spent the 20 seconds alloted to shit alone in the bathroom typing that there. It was more like "why are you &lt;clench&gt; correlating &lt;clench harder&gt; your queries? That's horrible (sigh of relief) for performance. &lt;wipe&gt;." Just don't correlate it. I would write it out but formatting code on phone doesn't work well.
try a 'show all' or 'show echo' what does ECHO report back as? 
You can't run an UPDATE and a SELECT within the same query unless they are nested within each other ([nice little primer here](http://www.tutorialspoint.com/sql/sql-sub-queries.htm)). I'd have to see an example of what you want to do to know for sure how to restructure to get it to work. 
Those are monthly extracts. And for each month I am looking to identify customers who were not in last month's one and those who are not in the next month's one (that obviously is one month in arrears) Its banking data so basically newly originated loans and loans repaid flags.
I have managed to solve it. I do not know why but case returned syntax error I used this thingy: iif(facility_ID IN (SELECT facility_ID FROM 201312),1,0) Thanks for help.
* "... something more senior to what I am doing now ..." If you're making presentations and excel models for senior management, you've got access to the best resource for getting a more senior position - senior managers. Ask them if they can give you some of their time to explain the reasoning behind the models you use and the changes they want you to make. If you can start to think like the senior managers, it's not far from becoming one. * "I would like to learn whatever will lead to the most job opportunities over the next 10+ years." No one can really do this. Your best bet is to look at what job opportunities are available in your desired location(s) and see what is currently most popular. If you track it over time, you can get a feel for what's trending up and down. Throw it in a database and write queries against it and you can use it for SQL practice. * "Advice on certifications would be appreciated as well." Certifications are a substitute for basic experience. In most cases, it will give you an edge over someone with a similar amount of experience. It might even get you an interview for entry level positions or positions which require expertise in the area of certification. In my experience on the interviewing side of things - certification wasn't a very good indicator of candidate quality and I'm fairly certain I'm not the only one who holds that opinion. * "... can anyone lay out a fairly straightforward plan ..." Sure - practice, practice, practice. Books are great for putting ideas in your head, but if you don't put those ideas to practice, they will remain ideas and fade away. To practice, find a problem you don't know how to solve, but think you might be able to figure out. Work on it until you either get the right answer or run out of ideas. Then look for other ways to get the correct answer and see how they work and figure out when each might be more useful. Repeat that process until you're confident that you could get a correct or mostly correct answer to any question and explain why the answer to some questions might be impossible.
Writing sql in Access is bloody annoying, e.g. multiple joins all require parenthesis.
Happy it helped. I missed that you're using Access, where indeed case doesn't exist - that was a T-SQL answer.
I agree with just about everything you said except for the following two pieces: &gt; If you can start to think like the senior managers, it's not far from becoming one. I would argue this is bad advice (or maybe just oversimplified for the sake of a reddit comment, if that is the case, ignore and apologies (: ). There is a lot of individual acquired knowledge that goes into (most) senior management positions. Industry Knowledge and Personal Experience both play into transitioning into one of these roles. I'm not saying you're completely wrong, just that there's more to it than "thinking a certain way". &gt; Throw it in a database and write queries against it and you can use it for SQL practice. I think s/he meant which RDBMS should be their primary focus. This is less varied than one would think as there are only a few contenders at an enterprise level. I believe (full disclosure: bias) SQL Server is fairly accessible and easy to learn. If you are able to setup a SQL Express instance on your work machine (this would vary according to your IT policies) and are able to import your excel sheets, you could start to apply your knowledge (practice, practice, practice as /u/in8nirvana suggested) from various sources to make your life easier. Oracle would likely be another strong contender for what to focus on. From a dev standpoint, T-SQL and PL/SQL are close enough to where learning one helps you with the other (always the small things that get yah though). 
I know it doesn't address your question, but to provide you some additional information for future cases in MSSQL. You could have implemented this in MSSQL using XQuery which would have (more than likely) improved your query performance. Using Cross Apply and the xml data type, you could also have done what you're attempting to do in a batch vs one by one like you're attempting to do with a variable. [Check out this article by Pinal Dave](http://blog.sqlauthority.com/2012/04/27/sql-server-introduction-to-discovering-xml-data-type-methods-a-primer/) Other than that, /u/Coldchaos is right, find a way to parse a page as xml as that would likely be more efficient than iterating through a variable as you're suggesting. Edit: Link fail
Depends on the quality of the crm the company has. I work for a large Fortune 100 company and the crm is so lacking any non standard information had to go through a dev. 
I went from data analysis in financial services to data analysis in marketing at a different organization. While the comment about Excel has merit for some tasks, I still use SQL for the vast bulk of my work. In particular, I'm using it to do response rate and lift calculations, analysis in identifying target audiences, and various forms of exploratory data analysis. When you get into performing tasks like basket analysis and affinity groupings, Excel will simply not be able to handle the million+ (100Million+) rows. However, in many situations, using SQL to generate some high-level aggregates that you then put in Excel for nimble ad-hoc analysis can be a great combination. If you want a good starting point for what you can *do* with SQL for marketing, I would recommend [Data Analysis Using SQL and Excel](http://www.amazon.ca/Data-Analysis-Using-SQL-Excel-ebook/dp/B00440DUMM/ref=sr_1_2?ie=UTF8&amp;qid=1421715627&amp;sr=8-2&amp;keywords=data+analysis+with+sql+and+excel). It won't teach you the language, but it will show you the basics of what's possible, and how to implement various solutions once you have a solid, foundational understanding. As for another language, I would recommend picking up R. It's very useful, free, and has great learning resources available. Johns Hopkins offers a free [4 week intro on Coursera](https://www.coursera.org/course/rprog).
&gt;I would like to learn whatever will lead to the most job opportunities over the next 10+ years. Like others have said, it's hard to predict how technology is going to change in ten years. But having experience is a safe bet in helping you get another job. So, I would advice you to look at what kind of jobs you think you would be comfortable working in, and see what those kind of jobs are asking for today. That being said, there are some stereotypes I'll throw out there: * Where are you geographically located? If you live near Seattle and plan to stay there, for instance, MS SQL is going to be a good bet. Look at localized ads for the area you think you'll be, and see what they are hiring for. * Would you like to stick with Windows in a business environment (MS SQL or Oracle, a degree and certifications could be useful), or would you prefer to work in a startup using Linux (MySQL, and who needs certs when we have caffeine)? * As for what to focus on: do you like building things (development), fixing things and setting rules to keep them that way (DBA), or breaking things for others to put back together (Testing/QA/Test developer)? I would note that developers have the most jobs, because companies often try to get away with not hiring DBAs and QA until later. But if you're good at the latter two, there's also less competition and companies may put that as a premium too. tl;dr: Figure out what you like doing today and aim for those jobs, because when you're doing it 10+ years later, you owe it to yourself to be happy there.
This is seriously banal. Why are you posting it?
That is where Pass Through queries come into use. You can write SQL in the native flavor and have it run directly on the server. 
Assuming you are currently employed in a data-centric environment, befriend those who keep the read-only data connections. With a read-only data connection in hand you can start writing practical, ad hoc queries on real data at work. You can start with Access (which you probably already have) and the linked table feature. Once familiar with the schema you can start using Access pass through queries which allows you to start writing queries using the server's particular SQL flavor.
For point #1, I like to run all inserts, updates, and deletes as selects first make sure that the data looks correct and then have a # to verify against - e.g. if my select returns 10 rows and the insert/update/delete does not, I know one of them is wrong (and rollback).
I agree /r/sqlserver might get more responses, but SSRS is too much of a niche component to have its own sub. 
It's almost like they should have their own tech forums run by the company ;) https://social.msdn.microsoft.com/Forums/sqlserver/en-US/home?forum=sqlreportingservices
Most data analysts I know receive data from multiple types of data stores. RDBMS, spreadsheets, flat files, etc. Having the tools in your arsenal to wrangle that data gives you an edge over your competitors. The best data analysts will know a general computing language like Python, SQL and a statistical tool like R. They'll understand data modelling as well as the business end. They can take data from multiple sources and move them into a single source that allows them to relate all that data together. Access is a poor choice of tool for this much of the time if you can get the data into a proper RDBMS instead like SQL Server or PostgreSQL. Much of the data I personally work with resides in the various ERP and other Financial systems we manage and a solid understanding of SQL is mandatory to be able to work with that data. 
There isn't a need for primary keys. There granularity of the tables is definitely a wild card but you can do the following. update a set a.y = b.yy from a inner join b on a.x = b.xx If there are multiple rows in table b on value xx you will get some random results. However, in your original suggestion it may error as the correlated subquery could possibly return more than a single result. I know SQLserver would bomb on this. In your original suggestion Oracle may hash the entire correlated subquery and perform a join but just because you are using Oracle doesn't mean you can write correlated subqueries. Today you may be using Oracle, tomorrow you may be writing for postgresql, SQLServer or MySQL and the optimizer is going to reinterate the subquery till the statement is completed for each line in A. I work for a business that employed 4 oracle developers and the shit they put into MSSQL and MySQL runs terribly. They never knew good code standards because of Oracles amazing optimizer. 
if you're updating a current table with the ID it would look something like the following update a set a.BannerID = b.BannerID from table2 a join table1 b on a.SSN = b.SSN if you're inserting into a brand new table from the imported access db/tables, you would do something like the following: insert into dest_table (SomeData, BannerID) select b.SomeData, a.BannerID from table1 a join table2 b on a.SSN = b.SSN I just really hope you had some data integrity controls around the SSN else this may not work as you hope.
Thanks! I'll give this a shot in a few minutes, after lunch. And yes, thankfully, there were integrity controls. And I've gone through and double checked, as well. I think it'll work. I'll check back in and let you know if it works. Thanks again!
Don't even get me started on the state of the Microsoft forums, they're horrid. Used to be Microsoft had amazingly well maintained Usenet newsgroups, but when Usenet started loosing popularity they along with so many others moved their content into proprietary forums that generally are poorly maintained. 
You sound like you work at a University! If so, hello fellow University DBA.
My job is similar, and I agree on all points. SAS is also a good tool if you're doing marketing analytics because it can do the stats calcs that SQL can't (removing the need to export to excel). But in general, if you're going to work in database marketing (or database anything), SQL is pretty darn useful. 
The standard use case is when you have a stored procedure that you need to run with a bunch of different parameters. You might have a table with all of the various parameter sets you want to run it with, select from the parameter set table, and, looping over that with the cursor, call the SP for each row in that table.
You could use a recursive CTE (if your DB supports CTE) to find all the children then delete them. 
I've never used PluralSight, and I wonder how it compares to other online video sites. I am using [LearnNowOnline,](http://www.learnnowonline.com/) and it is set up really well in terms of telling you which videos are good for which certifications.
I can't speak to the quality on LearnOnlineNow, but Paul Randall and his team of people from SQLSkills all release training videos on PluralSight. 
Almost every instance where a cursor sounds like a good idea, it can be done differently with "set based logic." The idea behind set based logic is that you do operations with whole chunks of data, instead of iterating row-by-agonizing-row. Prior to SQL 2005, cursors were used a lot for recursive datasets, but those can be done now with recursive CTEs. Most times when I see a cursor nowadays, it's the result of a non-db software developer (someone who's familiar with Java or C) having to solve a problem on the SQL side of things, and not knowing the ins and outs of SQL. So they treat tables like 2-dimensional arrays and use iterative logic like they would in their preferred language.
I tend to use cursors for tasks where I'd normally use a programming language to manipulate the data but for whatever reason one won't be available. An example of the last time I used a cusor was where I needed to query 500 remote SQL databases to check a tiny amount of data and the system I was on held the IP addresses of all of those sites. Quick cursor to collect the IP address of the remote site from the DB, it then created the linked server before running off to retrieve the data and moving onto the next. Not the best way to do it but it was super quick and dirty and as I only had SQL available in that environment it fit the bill. But for long term data operations I'd generally not touch cursors if I can help it. 
I think your table only needs an ID (primary key), a NAME and a PARENT_ID. The PARENT_ID can be null (meaning a root level entry) and has "on delete cascade" referential integrity back to ID. Therefore if you delete an ID it will delete all the children.
This. Exactly this. If you haven't yet found another solution please go away and try again (I have said this to a junior on the team.. Fortunately everyone knows my view on cursors). If you haven't already, add a numbers table that's runs from zero to 100k (or higher if you need).. Allows you to easily do 99.9% of what cursors tend to be used for. 
I didn't know cursors were that bad. Are you saying it is best to do something like "set @int = 0; while @int &lt;&gt; max(id_column) BEGIN... set @int = @int + 1..." instead of fetching values from a cursor? And I am assuming there is a performance difference between the two methods.
Absolutely. Lots of articles on it where people have tested and demonstrated the performance impact. We recreated some of these in on test lab to show some of the juniors (and to see it ourselves). On mobile atm otherwise I would like you some interesting reads. Numbers table let's you do it all set based which can make a huge impact. 
imo, one should move into management after a certain amount of time..not being a code/tech monkey
yes..my job is similar to this but i dont claim to be an SQL genius nor do i want to be. 
If I have a stored procedure that implements our backup process, what is wrong with using a cursor over master.sys.databases to supply the stored procedure with the database names? What about a workflow in which two stored procedures are executed against a database, and the second one depends on the output of the first one? How would set-based logic or window functions help me accomplish the task of running that workflow against all the databases on an instance? EDIT: master.sys.databases. I don't know why I keep getting these system tables muddled in my head when I post.
I once had to use something like cursors on a project that would estimate orders and inventory into the future. The situation was like this: EndingInventory = EndingInventory (from last week) - CustomerOrders + Receipts Receipts = FactoryOrders (from 5 weeks ago) as an estimate or ActualReceipts (as reported by the warehouse) - if they exist FactoryOrder = MinInventoryLevel - EndingInventory The "n weeks ago" aspect was what made it hard to do without iterating through the recordset (I can't know "this week until I know last week, etc). It got trickier because there was additional logic (no back-orders, so inventory can't be &lt; 0). There were also multiple products in the the table, so it was difficult to just count "n records back". It was painfully slow, as you'd expect. Oddly enough, this was trivial to set up in an excel spreadsheet (for one product). Given what I know now, I think I could get this to work with a series of sql statements. I'd be super impressed to see it done as a single select statement against the table. 
Being an analyst isn't always a management position though, that's my point, I was an analyst in my 1st, 2nd, and 3rd jobs outside of uni, then I moved up. I'm no longer an analyst.
If execution time only needs to be "good enough", then cursors will be just fine. I've had situations where a cursor took hours to complete what a windowed function could do in minutes. In that case we were stuck with the cursor since it was an older version database that didn't have window functions, but as it only ran once a month, hours was fine.
I hear that. I'm fortunate to work for a company where we host our clients data so our servers are all kept nice and up to date.
It's possible. I would use 12 sub selects (one for each month of the year) using Month(DateDone).
Why would you use a sub select instead of a case?
While it's true that a date table isn't *needed* for this, it'd make the code a lot cleaner than having a whole bunch of `case` statements - you can just do a `count` and `group by` (throw in a `pivot` to get the months in columns).
You could use a case, though that wasn't my first thought.
 CREATE TABLE Derp( x INT NOT NULL, previousX INT NOT NULL UNIQUE, CONSTRAINT PK_Derp PRIMARY KEY (x) ) GO Returns the same error.
Not sure off-hand then.
I am attempting this answer as we speak, hopefully I don't mess it up.
The exact same way. Add `INNER JOIN t3 on t3.field = tX.fieldX` after your current `JOIN`.
Just attempted that and it worked!
It worked great. The only problem I have now is when I export to csv there is one column that 1. I don't need and 2. Throws everything else off. Is there a way to exclude this column in the query? Or exclude it on the export?
as far as the FROM clause is concerned, the NOT IN conditions are not indexable, so the only index that will help is error_date i'm curious about the GROUP BY... what's it for? presumably each accession_id has multiple errors? i'm curious if the GROUP BY will actually collapse anything... thrwo a COUNT(*) into the SELECT clause and see if you get any counts greater than 1
`select *` is not your friend. For a quick, ad-hoc query to find something in a non-production environment, it's fine. For "real" code, always select only the columns you actually need in your output - `select col1, col2, col3 from...`
it would simply be another inner join with the two tables in question.
&gt; Would it be better to have several not equal to's instead of ins? those are evaluated identically it's the NOT part that kills the possibility of an index helping out 
OK, just to clarify, I need to list all errors that I want as opposed to the ones I don't want? So this is no good : and ERROR_CODE &lt;&gt; 'PR1' but this is good: and ERROR_CODE = 'PR1'
&gt; select * from table2 t2 &gt; inner join table1 t1 on t1.ID = t2.itemID I just tried select col1, col2 from table2 t2 inner join table1 t1 col3, col4 on t1.ID = t2.itemID and the error I am getting is: Msg 102, Level 15, State 1, Line 2 Incorrect syntax near ','. 
I think it's applicable to most (if not all) relational databases.
if the number of codes you are looking for is large (compared to all the possible codes) then that won't help you either
OK, let's take a step back here. How much SQL do you actually know? It sounds like you're just starting. A query takes this basic form: `SELECT` fields `FROM` tables `WHERE` conditions So your query should be: select t1.col1, t2.col2, t1.col3, t2.col4 from table2 t2 inner join table1 t1 on t1.ID = t2.itemID
Thanks for the help!
Got it, thanks.
I think I'm missing something from your requirements or your data. This will give you the min price for each comp and the number of categories with Y. Then you can easily see if it has 4 or 3 or less category hits. SELECT t.compprice , cnt.catCount , MIN(t.catPrice) minPrice FROM table t LEFT JOIN (Select compPrice, count(distinct category) catCount From table Where flag = 'Y' Group By compPrice) cnt ON t.compprice = cnt.compPrice GROUP BY t.compprice, cnt.catCount 
1 and 2 are wrong. (1 is true for MySQL, which is just barely SQL, and has the worst query optimizer of all the SQL platforms. Maybe this article is really about "designing" MySQL indexes.) PostgreSQL can use an index on columns that have as few as three distinct values. Two distinct values always (almost always?) uses a sequential scan. I don't think you can provoke an index scan with an index on an expression, but I could be wrong.
What is you DB platform? Are your categories fields in the same table or single rows in a child table? Is your table... T_COMPARE_PRICE (table?) ---------------------------------- ID integer not null,--PK CAT1 varchar(1) not null, -- Y/N CAT2 varchar(1) not null, CAT3 varchar(1) not null, CAT4 varchar(1) not null, PRICE decimal(15,2) null, ..... -- other columns if so it is a simple aggregation after unpivoting your data ... case when CAT1 = 'Y' then 1 else 0 end + case when CAT2 = 'Y' then 1 else 0 end + case when CAT3 = 'Y' then 1 else 0 end + case when CAT4 = 'Y' then 1 else 0 end as FLAG_COUNT, ...
To put a personal addition here, if you have the option this kind of solution should be implemented as multiple tables. One that handles the references and one that handles the data. Parent / child relationships are almost universally better handled as a separate table with keys from the data source instead. It also allows for much more efficient recursive querying to figure out an UPDATE or DELETE structure even if you decide to go with the method of using a trigger. 
I generally prefer explicit joins to implicit joins, much easier to read. Change it to an update statement where you set the field named average to the average value where the months match. Or you can trunc the table and insert both the month and the average at the same time.
what are you counting, number of employees? i suppose you could come back at the start of each month and do a count(*) and compare months. or, if you can create a new table, have a number column and a date column and create a job to insert into new table a count into the number column and sysdate stamp the date column on the first of every month. someone far cleverer than i will probably have a better solution.
Every month you aggregate the number of employees by location and you insert those figures into your new reporting table. That way you have historic monthly figure based totals.
Why not leave your target (SUMMARY) table empty, then: INSERT INTO SUMMARY (Month, Average_Price) SELECT SaleMonth, AVG(Price) FROM SalesTable GROUP BY SaleMonth; This will put all known moths + average sales in those months into your summary table. You may want to consider adding a year value, too, because you only have 12 months to play with if you do not supply the related year. This assumes that the SalesTable has a SaleYear column (which of course may not be the case)... INSERT INTO SUMMARY (Year, Month, Average_Price) SELECT SaleYear, SaleMonth, AVG(Price) FROM SalesTable GROUP BY SaleYear, SaleMonth; HTH
*advice Advise is what you do when you provide advice. EDIT: Downvotes? This word confusion is VERY common in IT and makes me immediately assume you are an underskilled Indian or Asian. I'm just trying to help OP out. 
Nothing to do with SQL. Try /r/SQLServer However, you should really check with the vendor of the profiling tool. Their documentation should tell you exactly what qualifies as time spent waiting for CPU/memory. 
Thanks, I have never seen it on any of our systems. It is probably not a very popular choice for optimizer?
Im using access. The catogory is one single column inside a table so the different catogories would be single rows inside of that column
You could also try using the census Tiger data https://www.census.gov/geo/maps-data/data/tiger.html
what you are looking for is a job schedule once per month (1st/last day?) that run a script that populates another table where you keep your count of employees by state stored by month.
How dangerous would the following code be to run: DBCC SHRINKFILE('logical_ndf_file_name', EMPTYFILE); ALTER DATABASE database_name REMOVE FILE logical_ndf_file_name; It'd be super-tedious, as I'd have to run it over 200 times (Or figure out the T-SQL to iterate over all of the NDF Files)
Here is what I would do in your situation: 1. Create another server, locally or otherwise. This will be your reporting. server/datamart. 2. Create an SSIS package to copy the data you need into a table on the reporting server where you do have rights to create tables ect. At the same time tack on a new reportdate column. 3. Create a SQL Agent job on the reporting server to execute the SSIS package on a monthly schedule. 4. ??? 5. PROFIT!!!!!!1!1!!! If you don't want to create a whole reporting server, an Access DB, Excel, or even Flat file system could be hacked to work as well.
Is it partitioning data based on certain criteria? I have a database with 90 files, one for each day. Every day it creates a new one and drops the oldest. The size may be because the files are created too large.
Well, if that's really the schema in your production database, I pity you. In fact, the hilarious naming convention for "uniqeID" is almost enough reason to believe you. Why don't you go ahead and let us give you the benefit of the doubt by showing us what you have so far. A few sample rows from these tablse as we ll as an example of how you expect the response data to look wouldn't hurt, either.
This sounds like a pivot table.
Pretty sure you can get all the NDF file locations using this: SELECT name, physical_name AS current_file_location FROM sys.master_files WHERE name LIKE 'DATABASE_NAME_HERE%'
Thank you very much for help :)
uhh...this is one of the worst formatted posts I've seen in a while. For one, your question is really ambiguous, and the whole concept of the question you have listed makes zero sense anyways. Do you want sales by customer or just the total sales for a product since that date? **Total sales by product** SELECT Item.itemname, SUM(Item.itemvalue) FROM Item INNER JOIN Sold ON (Item.uniqueID = Sold.uniqueID) WHERE Sold.date &gt;= '01-22-15' GROUP BY Item.itemname
I think the only problem with what people have listed here is that...what if an employee leaves the company? Won't they become a deleted row in the table and no longer exist? How will you overcome comparing employees that have been left out? As in -- Month 1 = 3500 employees Month 2 = 3400 new employees... but 160 employees left the company within Month 1 to end of Month 2. Your table may show 6900, but it should really be 6740. I'm not sure the %error you're looking for, or how big the company is, but know that may throw your figures off if you're aggregating like that.
Databases are used everywhere, security systems, games and programmes on your mobile phone use databases behind the scenes for storing, accessing, reporting on and manipulating data. As a professional software developer with many years experience in different industries, they all rely heavily and centrally on databases. I would go as far as to say that database skills are - in my opinion - one of the most important in any software engineer's skill set.
There are public dumps of the entire wikipedia.
I couldn't said it any better.
I've interviewed people in my industry and some you mention, including recently graduated candidates and it can be surprising how often they lack database skills, which can be a bit of a shock. So yes, sometimes it can appear that certain areas are underskilled. Data is the single most important thing in IT, it is knowledge and that knowledge is often hidden, ready for a revealing revelation if you know how to interrogate it. There isn't a single successful company in the world that doesn't either want that or has it. Don't think database management systems aren't evolving. The likes of Oracle, Microsoft, IBM DB2, Sybase, NoSQL invest billions and billions every year evolving their products and they evolve just as much or more than as other IT sectors. A lot of it is under the hood (improving performance, scalability, SDKs, warehousing, clustering, administration, management), but much of it is keeping up with changing standards or application (SQL standards, native XML or JSON, XQuery, spatial and mapping etc.). Databases are more abstract and more like an engine, so it often doesn't appear that way. However, if you doubt it, just look at the huge number of changes introduced in say Oracle 12c. It would take you a year to read the whitepapers and associated documentation.
 SELECT Name, COUNT(*) as Total from ( SELECT ID, Name FROM NewData UNION ALL SELECT ID, Name FROM OldData) GROUP BY Name
I recommend this link to anyone contemplating storing/querying hierarchical data in sql/rdbms. The "LR Method" is the best solution I've found. Does not depend on vendor specific functionality and/or keywords: http://kamfonas.com/id3.html 
Could be that you are IO bound then. It the storage some kind of raid?
Yea, IO might be the issue. The table is quite big with around 350 million records. There is no raid setup.
Please could you explain how you arrived at 130? 
Try this syntax... UPDATE t_alias1 SET t_alias1.column = t_alias2.expr FROM table1 t_alias1, table2 t_alias2 WHERE t_alias1.column = t_alias2.column
Take out "join table1" from your subquery. Change "on" to "where" and leave everything else.
OP wants the max sum of values over *ANY* 24 hour period, not just the last 24 hours. | D | V | ROLLING_SUM | |--------------------------------|----|-------------| | January, 01 2015 00:50:00+0000 | 50 | 110 | | January, 01 2015 23:30:00+0000 | 60 | 130 | | January, 02 2015 12:56:00+0000 | 70 | 70 | 
60+70=130. the first 2 entries within 24 hours add to 110, but the second 2 add to 130. and the first and last are not within 24 hours.
mysql doesn't accept date formats like that
http://sqlfiddle.com/#!3/04aa5/6 Select MAX(myrows.s24) best From( SELECT s1.foo ,(Select SUM(s2.bar) From stuff s2 Where s2.foo BETWEEN s1.foo AND DATEADD (hh , 24 , s1.foo )) s24 FROM stuff s1 ) myrows This checks every date and SUMs all the dates for the next 24 hours. Then you simply request the MAX from the list of 24 hour aggregates.
IO is most likely the issue here, even before reading these comments, having just read your blurb at the top that mentioned 20 cores, IO immediately jumped out at me as being the bottleneck, not CPU. I use SQL Server, but most of the same principles will apply, processing power is good, but the main bottleneck, especially with lots of data being processed, will be disk IO, followed by RAM.
Yes, I've never encountered an IT professional of any specialization that wouldn't benefit from knowing at least the lingo and concepts of relational database methodology. As ziptime says it's almost always the backbone of other systems. Gathering data, asking the data intelligent questions, and using the data to make informed decisions is what computing is all about.
What about inserting an image in the middle of the list? I want users to be able to make a gallery of images in the order they want. And I was hoping for a good way to store the data so that if the user thought "No, I want this image #16 to actually go third", it would be easy to implement. I guess I could just say "Whatever images you want to add go to the end, and if you want to insert one in the middle, delete all of the ones after that point and re-add them"... but I don't like that idea.
The second column you mentioned will probably usurp the date-stamped column and I shouldn't even add that one then, right?
I'm not sure I follow. When you say "by default that column will be the highest pre-existing value on the table". Which table are you talking about? Because table images_galleries will have a lot of picture #1's, picture #2s, etc.
Nah in SQL the nonclustered index will be a "second" ordering of the data that references the "primary" ordering of the data that your clustered index date stamp creates. 1 primary storage(clustered index) and a reference to a 2nd order (non-clustered index).
Wasn't sure at first what you were looking for it looks like you are just trying to find parks with all road names in common with "New Park". This can be done multiple ways but doing a subquery (as much as I personally dislike them) may be the simplest way to go about it. SELECT Parks.Name FROM dbo.Parks WHERE Parks.Park_No NOT IN -- list of all parks that do not share all the same roads as New Park ( SELECT Parks.No FROM dbo.Parks INNER JOIN dbo.Intersection ON Intersection.Park_No = Parks.Park_No LEFT OUTER JOIN -- all roads/park combinations that do not match New Park ( SELECT Road.Road_No FROM dbo.Road INNER JOIN dbo.Intersection ON Intersection.Road_No = Road.Road_No INNER JOIN dbo.Parks ON Parks.Park_No = Intersection.Park_No WHERE Parks.Name = 'New Park' ) RoadQuery ON RoadQuery.Road_No &lt;&gt; Intersection.Road_No AND RoadQuery.Park_No &lt;&gt; Intersection.Park_No WHERE RoadQuery.Road_No IS NOT NULL ) It's much easier to exclude parks that have at least one different road_no than to try to validate that each park has every road that New Park has. In all honestly a table variable is what I would use but since this is an assignment and I don't know your platform subqueries should be a decent option. This works in three steps: First, find all parks/roads that have at least one mismatch with New Park. Second, select all parks where this is the case. Third, return all parks that are not in the second's select list. This seems like a pretty challenging query for a college course but I guess it helps push you further. Also shoutout to /u/Wepp for pointing out my previous mistake. I hope this helped. 
Since this is an assignment, I hesitate to just write a query that gives the correct answer. So, here are some hints: 1. Your query does not need to include the Road table at all. 2. Your where clause should be used to verify that the candidate (FROM) Park has at least one (EXISTS) Road_No that is shared with "New Park". 3. Your where clause should also (AND) verify that "New Park" does not have any (NOT EXISTS) Road_No that the candidate Park lacks. There's more than one way to write this query. The advice above is for just one way.
&gt; reinventing database functionality to cram it into an OO paradigm Are you talking about object relational mappers, or something else?
This will return any parks that share at least one road with New Park. My interpretation of the assignment is that the query must return only those parks which share *all* roads with New Park. 
&gt; build a clustered index off of that so your default view of the data is based on when it was entered. Even though that will *usually* be the result you get, in SQL Server at least sort order is **never guaranteed** unless you specify `order by`. This is especially important when you join two tables with clustered indexes - you can't make assumptions about which one's ordering comes first.
Alright thanks. I'll likely do something like that.
That might work. Here is the route I was thinking: SELECT Name FROM Park AS Other_Park WHERE EXISTS ( -- at least one shared road SELECT * FROM Park AS NewPark INNER JOIN Intersection AS NewPark_Intersection ON NewPark_Intersection.Park_No = NewPark.Park_No INNER JOIN Intersection AS Other_Intersection ON Other_Intersection.Park_No = Other_Park.Park_No WHERE NewPark.Name = 'New Park' AND NewPark_Intersection.Road_No = Other_Intersection.Road_No) AND NOT EXISTS ( -- New Park doesn't have any roads that Other_Park lacks SELECT * FROM Park AS NewPark INNER JOIN Intersection AS NewPark_Intersection ON NewPark_Intersection.Park_No = NewPark.Park_No WHERE NewPark.Name = 'New Park' AND NewPark_Intersection.Road_No NOT IN ( SELECT Road_No FROM Intersection WHERE Park_No = Other_Park.Park_No)) 
Hey. Thanks a ton for this answer! Could you also tell me how to approach such problems? I found absolutely no progress even after sitting with it for 2 whole hours. Any tips on how you approach such problems would be amazing.
Could you elaborate a bit on how you reached these conclusions?
I believe you.
thank you for the correction. I'm from Spain and english is not my native language, so this is appreciated, especially considering that I use english in my job, and I don't like to have this type of mistakes.
What about if you had more rows, and you had row x and y within a 24 period AND x and z within a 24 hour period?
The way you need to approach SQL problems is take one piece at a time, get it to work then add another piece and get it to work. A problem like you have here in the grand scheme of things is relatively simple but it can still be approached the same way. I know just from thinking about the data that it's going to be way easier to find a park containing at least one road that New Parks does not. You have a query to find this. Now, you want the park name from this query. Now you want all parks that are not in these park names. I completely understand the frustration you went through. I write SQL every single day and have had to deal with very complex queries and I'm telling you that if this is a beginners SQL course that this question, at least based on how you have the requirements written is really asking too much of beginners. As far as any tips I can give you really the only thing I can say is build one piece at a time. There is no shortcut or trick to writing SQL. The best SQL coders I've seen have such a deep understanding of the intricacies of the language that only comes from experience and continued education. In all honesty I had been working for about 4-5 months doing SQL before it really clicked with me and the only reason that happened was because I would constantly try new things and practice as well as study other people's queries. So, in short don't let the big picture confuse you, break it down into simpler pieces, practice and educate yourself on new functionality.
If we want Old Park when it has all of the exact same roads as New Park, we can use window functions and solve the problem in a single pass, like so: WITH ParkRoads AS ( select p.park_no, p.park_name, road_no, ROW_NUMBER() OVER (PARTITION BY i.park_no ORDER BY i.park_no, road_no DESC) AS DescRoadCounter from #park p INNER JOIN #intersection i ON p.park_no = i.park_no ) SELECT DISTINCT pr2.park_name FROM ParkRoads pr1 INNER JOIN ParkRoads pr2 ON pr1.park_no &lt;&gt; pr2.park_no AND pr1.road_no = pr2.road_no AND pr1.DescRoadCounter = pr2.DescRoadCounter WHERE pr1.park_name = 'New Park'; The window function (DescRoadCounter) gives us a checklist, putting each park side-by-side and going down the list to ensure both sides match. We made it descending to ensure the total number of roads is the same (i.e. Big Park has Elm, Oak, and Maple while New Park only has Elm and Oak). If it's acceptable for Big Park to have more roads than New Park, we tweak it just a bit, like so: WITH ParkRoads AS ( select p.park_no, p.park_name, road_no, ROW_NUMBER() OVER (PARTITION BY i.park_no ORDER BY i.park_no, road_no) AS RoadCounter, COUNT(*) OVER (PARTITION BY i.park_no) AS RoadCount from #park p INNER JOIN #intersection i ON p.park_no = i.park_no ) SELECT DISTINCT pr2.park_name FROM ParkRoads pr1 INNER JOIN ParkRoads pr2 ON pr1.park_no &lt;&gt; pr2.park_no AND pr1.road_no = pr2.road_no AND pr1.RoadCounter = pr2.RoadCounter AND pr1.RoadCount &lt;= pr2.RoadCount WHERE pr1.park_name = 'New Park'; Here we still line them up and make sure they all match, then compare total counts with a &lt;= so that Big Park isn't thrown out of the result because it has Maple. Hope this helps! edit:formatting edit2: removed unneeded extra logic in the first query
 SELECT game , club , SUM(goal) AS goals FROM ( SELECT e.game , p.club , 1 AS goal FROM game AS g INNER JOIN event AS e ON e.game = g.id AND e.tipo = 'goal' INNER join player AS p ON p.id = e.player UNION ALL SELECT e.game , CASE WHEN p.club = g.home THEN g.away ELSE g.home END , 1 AS goal FROM game AS g INNER JOIN event AS e ON e.game = g.id AND e.tipo = 'owngoal' INNER join player AS p ON p.id = e.player ) AS data GROUP BY game , club 
As far as hints go, if your intent is to display the score, something like this &lt;guest&gt;, &lt;#Goals&gt;, &lt;home&gt;, &lt;#goals&gt;. To calculate either of th &lt;goals&gt; columns you'll need to add a logic to look at goals/owngoals and figure out which team scored one of those (home/guest). Look for the 'case' expression to help you with the coding the logic. Also, don't forget the 'nil'-'nil' scores. 
make sure you understand the answer too. Ask if you are unsure, but don't just copy paste it, know how and why it works (if it does).
It works perfectly!
the tough part was in deciding to break the problem up into two pieces, and then unioning the results the CASE simply assigns an owngoal to the player's opposing team i suppose it would be possible to combine the separate logics back into a single SELECT, but that, as my old highschool teacher used to say, i shall leave as "efts"
Got it!
&gt; i suppose it would be possible to combine the separate logics back into a single SELECT, but that, as my old highschool teacher used to say, i shall leave as "efts" honestly, I didn't even "parse" it in my head after I saw the username posting it, i took it as granted that it would work ;)
And you can write MSSQL procedures in .NET, but I would stick to SQL for both.
Is it a site wide change they're able to make or just specific to their account? If they're read only access, they shouldn't be able to make changes beyond user settings. Edit: I take that back. Do you have users set up as either of the two listed in the first paragraph of this document? https://msdn.microsoft.com/en-us/library/ms181194.aspx
The only named user with any access is myself. I have set myself up as System Admin and Content Manager. There are no other groups or users with defined access. Could it be something on a server level?
Aren't there two versions available?
Why? Unless you have a new idea, why reinvent the wheel? 
There are many drivers available, but because of limitations in Python, all are untrusted. &gt;PL/Python is only available as an "untrusted" language, meaning it does not offer any way of restricting what users can do in it and is therefore named plpythonu. A trusted variant plpython might become available in the future if a secure execution mechanism is developed in Python. The writer of a function in untrusted PL/Python must take care that the function cannot be used to do anything unwanted, since it will be able to do anything that could be done by a user logged in as the database administrator. Only superusers can create functions in untrusted languages such as plpythonu. (http://www.postgresql.org/docs/9.4/static/plpython.html)
OK, LEFT JOINs don't work because of the column 1, do they? I tried with UNION SELECT e.game, p.club, 0 AS goal FROM game AS g INNER JOIN event AS e ON e.game = g.id INNER JOIN player AS p ON p.id = e.player but it doesn't work :/ where am I wrong? 
Thanks!!
The key difference between sql and nosql is not rooted in syntax - the difference is the kind of data you're storing. The SQL (relational) database is exceptionally suited for what it does, especially with respect to transactional accuracy. Nosql can't, shouldn't and doesn't compete. The reverse is also true. My first goal, if I were in your shoes, and developing a new "database engine", would be to define the kinds of data you're storing - define the expected data types. How you interact with that data will drive the interpretive language used (ansi sql, nosql, roll your own, or other).
&gt;We plan to opensource it under BSD after the end of semester Does your school allow this? Before you start releasing stuff under a particular license (or at all), make sure you're allowed to.
Well my data are in italian, you don't mind, do you? Text really? Not an .xls? 
So, would this be close to the statement I need? UPDATE tblCourseData set tblCourseData.BannerID = tblPersonalData.BannerID from tblPersonalData, tblCourseData join tblCourseData on tblCourseData.SSN = tblPersonalData.SSN; Because I'm getting an error: The objects "tblCourseData" and "tblCourseData" in the FROM clause have the same exposed names. Use correlation names to distinguish them Once I can figure this out, I can actually configure the rest of the damn application and start the user review of some prototypes. Unfortunately for me, I'm not a professional DB admin--I'm just trying to clean up this old one to use in a new application, so I keep running into little issues like this. 
yes, please, text really you posted an image which had what looks like a spreadsheet of sample data i don't need hundreds of rows, just a dozen or so, but please make them comprehensive and realistic, not junk (which i can generate myself) i'm willing to copy/paste them into my database system in order to verify all the different conditions are working
In the meantime, I'm going to be just truncating these tables entirely, continuing on with development, and then will consider integrating these old records into the new system at a later date. I had discussed this with the users, and we agreed that it may be easier for everyone to start the new system from scratch, and maintain the old system as an archive. 
Ah, quite simple. Thanks!
It depends on the requirements of the class but you could try a SQL like database geared to a specific purpose. For example, a database engine designed for data analysis which takes infrequent large datasets loaded from CSV but which is optimised for very fast reads and aggregations at the expense of small inserts, updates or deletes.
okay, i tried the LEFT OUTER JOIN approach but it got really messy so i went back to the UNION and generated "zero" rows from the games table, so that these rows would still show up even if there were no goals scored and of course the zeroes add in with no harmful effect when there are goals SELECT idgame , club , SUM(goal) AS goals FROM ( SELECT g.idgame , g.home AS club , 0 AS goal FROM games AS g UNION ALL SELECT g.idgame , g.away AS club , 0 AS goal FROM games AS g UNION ALL SELECT g.idgame , CASE WHEN p.club = g.home THEN g.home ELSE g.away END AS club , 1 AS goal FROM games AS g INNER JOIN events AS e ON e.game = g.idgame AND e.type = 'goal' INNER join players AS p ON p.idplayer = e.player UNION ALL SELECT g.idgame , CASE WHEN p.club = g.home THEN g.away ELSE g.home END AS club , 1 AS goal FROM games AS g INNER JOIN events AS e ON e.game = g.idgame AND e.type = 'owngoal' INNER join players AS p ON p.idplayer = e.player ) AS data GROUP BY idgame , club results -- idgame club goals 1 Atletico 5 1 Lappele 3 2 Lappele 4 2 Nagane 3 3 Atletico 0 3 Nagane 0 
This is what I use http://www.dylanmorley.com/blog/post/sp_find.aspx And this query for Jobs. http://sqltidbits.com/scripts/search-sql-server-agent-job-steps-specific-keyword-text-or-stored-procedure 
Your choices for handling this include: 1. Use BFILES (sort of like pointers to external file objects not stored in the database) 1. Store UNC path to the file in a string type field. 1. Store in the DB as a BLOB (binary large object). My advice, go with 1. (preferred) or 2. (second choice).
Ok, simplier than I thought Thank you for your help! I don't want you to write more code and lose precious time, if you don't mind I just nees a hint: I'm coding a ranking view. I would use a view that works on this view but don't know how to compare the goals to identify the winner! Which function can I use? EDIT: ~~Ok maybe I just need to use max and other simple function, I'm thinking about it~~ EDIT2: I would write something like this to find the match winner... SELECT Club, Goals FROM ViewResults AS R1 WHERE Goals &gt; any ( SELECT Club, Goals FROM ViewResults AS R2 WHERE R1.Game=R2.Game AND R1.Club&lt;&gt;R2.Club) EDIT3: Ok, without that Club in the select, and with the proper R1. and R2. prefixes, it works! EDIT4: Wow I did it! SELECT Club, SUM(point) AS Points FROM ( SELECT Club, 3 AS point FROM ViewResults AS R1 WHERE (goals &gt; ANY (SELECT goals FROM ViewResults AS R2 WHERE (R1.Game = Game) AND (R1.Club &lt;&gt; Club) ) ) UNION ALL SELECT Club, 1 AS Point FROM ViewResults AS R1 WHERE (goals=ANY (SELECT goals FROM ViewResults AS R2 WHERE (R1.Game = Game) AND (R1.Club &lt;&gt; Club) ) ) ) AS Data GROUP BY Club ORDER BY Points 
&gt;Store in the DB as a BLOB (binary large object). So I've always wondered about this - how does an application retrieve the value? From what I remember, a BLOB can't actually be returned by your standard `SELECT` statement that you might run ad-hoc, correct? Would you use a cursor to load the BLOB, and then return the contents of the cursor to the application for processing (ie. rendering the image)?
BLOBs are pretty straightforward to work with - and they're optimized to be as fast as you would be able to work with the raw files on the file system. Here's some docs to give you an idea of how to work with them with Java http://docs.oracle.com/cd/B28359_01/java.111/b31224/oralob.htm#i1058035 
I'm stuck at how to exclude the "extra" rows (like 12:05 Red and 12:03 Green) in MySQL. If you had a real database, you could make a recursive Common Table Expression (CTE) = WITH clause to skip over the rows that simply confirm the status is unchanged. Finding when each status changes is easy: Select t.id , t.status , t.time start , (select min(time) from table where id = t.id and t.status &lt;&gt; status) end From table t
Yes they are! if you are able to relate concepts and understand how data relates to each other you will be able to discover information where nobody else could. A database could be conformed by your contacts, in a excel file, invoices, so on. E.g For a person in security this could mean the difference between realizing of a breach or not.
Yeah, MySQL does not make it easy. I just made a procedure that basically loops through the records to combine them. I was hoping to not have to do that, but it at least works for now.
i think you might be looking for something like this, i've not tested it but i think the logic is correct: SELECT light_id, time as end_time, t2.start_time as time, status FROM ( SELECT * FROM table_name ) t1 RIGHT OUTER JOIN( SELECT light_id, time as start_time status ) t2 ON( t1.light_id = t2.light_id AND t1.status = t2.status AND t1.time &gt; t2.time ) Basically, you want to create 2 tables and group them on id / status, then select the time which is greater, so the table is populated with the largest endate for a specific status / id. Then just grab what information you need. 
&gt; First off: is this the proper way to store my data? Yes, this is exactly how you should be storing your data. &gt; Also: how am I supposed to fetch say rum and coke by searching for ingredients? A left join? What I'm finding hard is how to build my query. SELECT drinks.name WHERE ... and I'm stuck. If you want to fetch any drink that contains "one" of the selected ingredients, you could do something like this: SELECT `drinks`.`id`, `drinks`.`name` FROM `drinks_ingredients` INNER JOIN `drinks` ON `drinks_ingredients`.`drinks_id` = `drinks`.`id` INNER JOIN `ingredients` ON `drinks_ingredients`.`ingredients_id` = `ingredients`.`id` WHERE `ingredients`.`id` IN ('2', '1') GROUP BY `drinks`.`id` If you want to fetch a drink that contains "all" of the selected ingredients, you could do something like this where the HAVING clause is equal to the total number of ingredients selected: SELECT `drinks`.`id`, `drinks`.`name` FROM `drinks_ingredients` INNER JOIN `drinks` ON `drinks_ingredients`.`drinks_id` = `drinks`.`id` INNER JOIN `ingredients` ON `drinks_ingredients`.`ingredients_id` = `ingredients`.`id` WHERE `ingredients`.`id` IN ('2', '1') GROUP BY `drinks`.`id` HAVING COUNT(DISTINCT(`ingredients`.`id`)) = 2 edit: Here is an SQL Fiddle http://sqlfiddle.com/#!2/9cf71f/3 (the site is slow as shit so wait for it to load)
thanks a bunch! will try it out and get back to you if I run into any issues 
No problem, I edited my post with an SQL Fiddle if you need to fool around with anything.
Try setting it to dirty read so it can see the uncommitted insert. Like this: -- Send mail EXEC msdb.dbo.sp_send_dbmail @recipients='me@companyname.com', --Change to SQL Admins group when in Live @subject = 'Job Status Changed at Prod DB Server', @profile_name = 'Default', @query = 'select * from SystemAdminDB.dbo.JobStatusChange WITH (NOLOCK);' , @attach_query_result_as_file = 1 ; Or this: -- Send mail EXEC msdb.dbo.sp_send_dbmail @recipients='me@companyname.com', --Change to SQL Admins group when in Live @subject = 'Job Status Changed at Prod DB Server', @profile_name = 'Default', @query = 'SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; select * from SystemAdminDB.dbo.JobStatusChange;' , @attach_query_result_as_file = 1 ;
might you have a variable driven by an expression?
Indeed. Thank you for your reply!
Not a problem, it happens to us all :)
I've no idea why you are reinventing the wheel, but take a look at the [SQLite source code](http://www.sqlite.org/download.html) which is C++ for an idea of what goes into what you are trying to do. MySQL is huge and you'll have no chance replicating its functionality during your course. What you are undertaking is no easy task, you have been warned...
That leads me to another question, though. Let's say I create, like you said, two fields where one is the last day of contact with the customer and the other one is the day difference between today and the other field. Indeed, that works. But I have a sales table where I also insert a date (of said sale). So, I need to know **two dates** regarding the customer. The **last sale's date** and the **date in which I last contacted him** (by "contact" I mean something as simple as an e-mail exchange). I want my "Days without Sales/Contact" field to be the difference between **today's date** and the most recent date among the **last sale's date** and the **last contact date**. So, it can't be something as simple as a datediff between date() and date X, but something like the datediff between date() and date X or Y, depending on which is the most recent. Do you know how to do this? Thank you... EDIT: maybe I'm giving myself my own answer, but I think what would solve my problem was if I made the **last contact date** field update automatically when the **sale date** changes. Or should I use another solution?
I'm not entirely sure of the Access syntax, I mostly use MSSQL and postgres myself. Here's what it would look like in MSSQL: DATEDIFF(D, CASE WHEN last_contact &gt; last_sale THEN last_contact ELSE last_sale END, GETDATE()) EDIT: This looks like it might work too http://www.techonthenet.com/access/functions/advanced/iif.php
It sounds like they're using a package deployment model with SSIS Configurations vs a project deployment model. Converting from one to the other likely wouldn't be an option until /u/yoelbenyossef becomes more familiar with SSIS.
You should normalize the structure a bit; your duplicating the name and if a name ever changes all your clothes rows wouldn't have a relationship to them. * Person (PKey, Name, Age, Birthplace) * Clothes (CKey Brand, Color) * Wearing (PKey, CKey) Example: Person: (1, Bob, 50, New York) (2, Jeremy, 21, Toronto) (3, Hal, 13, Houverville) Clothes: (5, CK, Blue) (6, Boss, Red) (7, Tommy, Green) Wearing: (1, 6) (2, 5) (3, 7) -- Bob is wearing the Red Boss -- Jeremy is wearing the Blue CK -- Hal is wearing the Green Tommy Now it should be relatively easy to find what 13 year old's are wearing and what everyone else is much easier. 
Or datepart('yyyy',current_timestamp) 
DATEPART("yyyy", DATE())&lt;=1924 Is that correct? Sorry i'm completely new to this
I have no idea how to test this. It's just an exercise I wanted to try before I enter a University. Do you have any way to test it? For 2) Do you mean SELECT P AGE FROM PERSON P, CLOTHES C WHERE P.name = C.name AND P.AGE = 13 AND C.Color = red ?? 
I am just following an exercise. I finished W3school and was following a university course exercise.
Do you mind if I dm you?
I do not mind.
Won't that throw all my relations out of whack (where records reference IDs of other records, etc)? (I'm not a professional when it comes to SQL) In addition, I've found that when merge replication is stopped and I try to insert a record on certain tables it will give me this error: Msg 2627, Level 14, State 1, Line 1 Violation of PRIMARY KEY constraint 'x'. Cannot insert duplicate key in object 'dbo.x'. The duplicate key value is (247436). Which indicates to me that my identity seeds are desynced between the publisher and subscriber.
Whenever you say programmatically, you probably want a recursive CTE. It also appears that you just want to CROSS JOIN Table1 and Table2 then make a loop of COUNT(CASE WHEN A = B THEN 1 ELSE NULL END)
If storing the image BLOB in a table (which I've seen DB designers do innumerable times), whichever tablespace he specified for lob storage (which would be in Oracle datafiles). I take it you're inferring I think they can't be external lobs? 
The problem with this is the that the statuses repeat, which I failed to show in the sample dataset. So light 1 will go, green, yellow, red, green, etc. I want a time range for each uninterrupted status. 
Was thinking SecureFiles option http://www.oracle.com/technetwork/articles/sql/11g-securefiles-084075.html 
Google is your friend for homework problems. [There's a SO for this exact type of question](http://stackoverflow.com/questions/1572110/how-to-calculate-age-in-years-based-on-date-of-birth-and-getdate)
I'm not exactly sure what you're asking, but can you use INFORMATION_SCHEMA to filter down your choices? This query looks at columns with the same name and data type and only picks one of the matching pair (that table_name clause at the end). SELECT c1.TABLE_SCHEMA, c1.TABLE_NAME, c1.COLUMN_NAME, c2.TABLE_SCHEMA, c2.TABLE_NAME FROM INFORMATION_SCHEMA.COLUMNS c1 JOIN INFORMATION_SCHEMA.COLUMNS c2 ON c1.COLUMN_NAME = c2.COLUMN_NAME AND c1.DATA_TYPE = c2.DATA_TYPE AND (c1.TABLE_SCHEMA &lt;&gt; c2.TABLE_SCHEMA OR c1.TABLE_NAME &lt;&gt; c2.TABLE_NAME) AND c1.TABLE_NAME &lt; c2.TABLE_NAME From your post, it looks like you're just matching up anything that seems to join, but that would probably be a false positive if you pick two columns that were identity fields, for instance. 
As a suggestion for #2, and I believe this is shared across a majority of the sql community, use joins whenever possible instead of using a where condition. The Syntax is clearer helping avoid some potential cross apply situations if you get the where condition wrong. Functionally (depending on the optimizer) they generate the same execution plan though. [Check out this SO on the subject](http://stackoverflow.com/questions/121631/inner-join-vs-where)
The issue, which I think exists in this solution, is still with repeating statuses. If light 1 is green from 12:00 to 12:05. Then red from 12:05 to 12:06, then green again from 12:06 to 12:10, I need the 2 records for light 1|green, the first of which is is 12:00 to 12:05. Using your code, the first Green record will be 12:00 to 12:10, and the second green record will be 12:06 to 12:10. I ended up using a procedure to loop through the data and clean it up. I appreciate the suggestion though!
Sorry, you're absolutely correct. You just still have the extra rows like you said. This is pretty much what I did, and put those rows with start and end times, and then my procedure consolidates the rows. Basically it just finds rows where there exists another row (row next) with the same status and id and where start time = current rows end time. It updates the cur row end time = next row end time, and deletes that next row from the table. It continuously does this until there are no more "extra rows". Thanks again, and sorry for the confusion. This SQL Fiddle is pretty cool, I've never used it before.
I'd say the easiest way to do it is to store the links, to be honest. Just store it as a string. Are you calling it in a front-end at all? Just query it and have it return the value in &lt;a href="&lt;?php echo $row['val'];?&gt;"&gt; or something like that.
Here's one option. SELECT DISTINCT tablename.Ticket_ID, CASE WHEN rewards.Ticket_ID IS NOT NULL THEN 'YES' ELSE 'NO' END AS RewardsCardInd FROM tablename LEFT JOIN (SELECT DISTINCT Ticket_ID FROM tablename WHERE RewardsCardInd='YES') rewards ON rewards.Ticket_ID = tablename.Ticket_ID
Sometimes you can be a bit cheesy with this, in your case YES is larger than NO for character comparisons so you can use the MAX aggregate to solve this. Also happens to work if the YES and a NO are of bits type. SELECT TICKET_ID, MAX(RewardsCardInd) FROM tablename GROUP BY TICKET_ID
Follow up question: `WHERE 'ingredients'.'id' IN ('2', '1')`. What is `('2', '1')` and are those hard coded numbers? The user should be able to filter drink recipes by adding ingredients, which should narrow down which drinks are shown on the page. For each ingredient added, the query is runt to determine which recipes contain said ingredient. If the user searches for only "coke", drink 1 and 4 will yield a hit since they both contain Coke. If the user adds "rum", only drink 1 is now a match. &amp;nbsp; Is this feasible with SQL or should that logic be handled by other parts of my program? If not feasible with SQL, can I somehow extend a query by saying "if the `ingredients`-array is longer than one, append the following to the existing query: '`AND i.ingredient = ingredient[1]`'".
Outside of redesign / normalization, you'll have to pivot. Pivot is the action of turning columns into rows or vice-versa. In this instance, you want to turn each column of each row into its own row. So if you have 40 product lines and 10,000s parts, you'll want to end up with 40*10,000s parts records. The typical way to do this is to cartesian join with a table that contains at least as many rows as the columns you want to pivot (40 in this case or 9 from the image). It'll look something like this: SELECT CASE p40.PRODUCT_LINE WHEN 'Product Line 1' THEN fp.PRODUCT_LINE_1 WHEN 'Product Line 2' THEN fp.PRODUCT_LINE_2 ... WHEN 'Product Line 40' THEN fp.PRODUCT_LINE_40 END PRODUCT_LINE FROM FIT_PRODUCTS fp, PRODUCTS_40 p40;
Only trouble is, you wouldn't want to see anyone on that cruise in a hot tub...
I wish I could convince anyone in my life (management and family) that this is someplace they should send me. Heck, I'd even settle for [SQL Summer Camp](https://sqlsummercamp.com).
Hot tubs aside, getting to just hanging out with the speakers (not to mention the other attendees) on SQL Cruise for an entire week would be (at least for me) awesome.
Stuck on a cruise ship with Burleson? Sounds like a great plot for a bad horror movie.
Yeah, I think somewhere within that 6k span there's some sort of situation where it's assuming incorrect UIDs or whatever it is. I completely understand that it's easier to play that game instead of re-building. Using legacy systems are always, always tough. Good luck with everything! I hope it works out.
I want to say yes. You do sanitize input and change things like &gt; into &amp;gt when inputting into an sql table and do the opposite if necessary when ouputing off of an sql table. That is more the server side scripts than it is anything else thought. For example the current way to input SQL into a table in PHP is through mysqli and PDO both of which actually sanitize and strip out the kind of code that is used in SQL injection attacks. 
Looks like you answered your own question, but your description of the problem is as clear as mud.
You're not the only one who isn't sure. It seems a common problem on /r/SQL that posters can't succinctly and comprehensively describe what they are trying to do 
Yes, but he only used it to explain the Cartesian Product, and not for the actual solutions. I think the LATERAL JOIN is a great feature many haven't used it yet.
Why not PostgreSQL? MySQL is such a piece of shit. 
edit added SQL
I went to the 2013 SQLPass conference. There certainly were some socially awkward uber-nerds, but there were a lot of really cool, not unattractive people. 
Hi, So you ended up moving into a role in programming? Was it semi-accounting related, drawing on your past experience or purely in web development? My impression was dashboarding and the like was mostly Excel-based stuff but maybe I am wrong on that..? Can you expand on what all that involves?
I hate to nitpick, but the terminology used in the intro is not good. &gt;One of the most important tools required to be a successful data scientist is relational databases like SQL. SQL is not a database it's a standard for a query language. &gt; And in addition, they are exceptionally good at storing complicated business data sets as well as allowing for efficient information retrieval. I think "data retrieval" is a better term than "information retrieval" in this case. "Information retrieval" typically means finding information that is not explicitly stated in the data, while queries in "data retrieval" are usually based on explicit data values. Both can be done with DBMS but "information retrieval" requires some additional concepts. &gt;In these posts, I will focus on the MySQL database because it is popular, free, open source, and easy to get started with. But the concepts and most of the commands are easily transferable to other databases. MySQL is not a database, it's a database management system.
You can also use Dynamic SQL which is generally better practice outside a few situations: DECLARE @query VARCHAR(MAX) = ''; SELECT @query = @query + ( SELECT 'EXEC master..xp_cmdshell ''''MKDIR ' + columnHere + ''' ' FROM tbl_locations) EXEC (@query)
My official title is financial systems administrator, which means all I do is fart around in the ERP systems we have making reports and fixing data that someone effed up and can't be fixed through the user interface. (Tasks get stuck sometimes. And people don't know what they're doing.) There's also development work I can do when I have time or is requested to customize some of our system functions or whatever. Being able to understand what accounting and finance wants is pretty critical to getting the job done and I've noticed a lot of jobs with that job title are looking for people that have a CPA and can code. The biggest frustration you'll probably run into is learning a new db and where everything is, where stuff goes when things are completed on the application side, etc, but SQL is all the same. 
yeah, fine, LATERAL JOIN... good luck using that in other databases but seriously, a guy who apparently doesn't know about standard sql's CROSS JOIN? very confidence-inspiring
"is huge, cumbersome, complex, and hard-coded, and i'm not even going to attempt to write an example" Fair enough, I'll let CSV be handled afterwards. But group_Concat was looking the far easier option. Ill start there. Thanks for the push in that direction.
Yeah, I just asked my boss to move to a analyst role and was transfered to the IT department. Accounting has helped very little, my IT classes are what has saved my ass. I thought it was all more excel based, but excel is mostly just a tool that accountants/finance rely on heavily. Think of excel as a programming language. 
If your product names ever contain a comma, group_concat wont escape it for you, so it'll just wreck your output. You can use a different delimiter instead of a comma, but that just shifts the problem I'd really consider just selecting a flat result set, and let your presentation layer massage it into shape. 
When I was hired for ETL/SQL Dev work a couple years ago it was for a "Data Analyst" position. It likely varies depending on the size of company and how they have aligned their titles/career paths.
I'd check the published database's [sysmergearticles](https://msdn.microsoft.com/en-us/library/ms189825%28v=sql.105%29.aspx) table, my guess is there is an object id mismatch. 
Thanks! The subscriber is missing articles from sysmergearticles that should've been applied to it with a new snapshot we took over the weekend. Its as if the snapshot didn't apply. The customer has been editing their DB since Monday morning. What's the recommended fix? I'm concerned that taking a new snapshot will result in lost data. Same with a reinitializing. EDIT: But we do know the snapshot applied because it created the new tables. Just not the entries in sysmergearticles.
Disclaimer, this is going into an area somewhat beyond my experience base. I'd recommend putting a tweet out to the [#SQLHelp Twitter Tag](https://twitter.com/search?q=%23sqlhelp). That said, I believe [this article might help](https://msdn.microsoft.com/en-us/library/ms152466%28v=sql.105%29.aspx); you might want to investigate the **Upload unsynchronized changes before reinitialization** area. Obviously backups/etc should be on the top list of things to do, there might be some manual processes involved. 
 SELECT ticket_id, RewardsCardInd FROM tablename WHERE RewardsCardInd = 'YES' UNION SELECT ticket_id, RewardsCardInd FROM tablename WHERE RewardsCardInd = 'NO' AND ticket_id NOT IN ( SELECT ticket_id FROM tablename WHERE RewardsCardInd = 'YES' );
Hey guys we found the cruise organizer's alt account!
Thanks again! I had overlooked the "Upload unsynchronized changes before reinitialization" option. We'll get solid backups after CoB and try that.
 SELECT `drinks`.`name` AS `drink`, GROUP_CONCAT(`ingredients`.`name` ORDER BY `ingredients`.`name` ASC SEPARATOR ', ') AS `ingredients` FROM `drinks_ingredients` INNER JOIN `drinks` ON `drinks_ingredients`.`drinks_id` = `drinks`.`id` INNER JOIN `ingredients` ON `drinks_ingredients`.`ingredients_id` = `ingredients`.`id` WHERE `drinks`.`id` IN ( SELECT `drinks`.`id` FROM `drinks_ingredients` INNER JOIN `drinks` ON `drinks_ingredients`.`drinks_id` = `drinks`.`id` INNER JOIN `ingredients` ON `drinks_ingredients`.`ingredients_id` = `ingredients`.`id` WHERE `ingredients`.`id` = '1' ) GROUP BY `drinks`.`id` Fiddle: http://sqlfiddle.com/#!2/9cf71f/5 All you have to do is put the ingredient's id that the user picks in that sub query. If you want to select multiple ingredients, simply change the = '1' to IN (array of ids)
There are triggers on every table, but those are required for merge replication, no?
I agree with you all you said, and would like to add 2 small points 1. I think the difference between data and information, is the query written against said data. One can accertain information, by filtering down and transforming the underlying data. The data on its own is just gibberish 2. While I agree that MySQL is a DBMS and not a database, I would very much advocate usage of postgree over Mysql, if one is looking for the reasons given to use MySql. There are multiple reason for me saying that, but i will leave it at this code snipped, that is not recommended to be executed against a MySQL Database : DELETE FROM users WHERE userID = 'MySql comes with really bad default and can do some really bad things'
Yes, it is called pivoting. There's even a special construct for it in MS SQL.
This seems to be exactly what I'm looking for, thanks!
Thanks!
I wanted to find fields in different tables that may be indexes...
If you have a large number of attributes or attributes that can change frequently, you may need a dynamic pivot to make the query a little more manageable. [Found this example for you](http://sqlhints.com/2014/03/18/dynamic-pivot-in-sql-server/)
Will merge work for you? https://msdn.microsoft.com/en-us/library/bb510625.aspx
Not up on my postgres, but something the folllowing should work. Select * from Recipes where recipeid in (Select recipefk from linkingtable INNER JOIN ingredients on linkingtable.ingredientfk = ingredients.ingredientid where ingredients in (ingredientslist)) 
Nope, they are used as a single string and cannot trick the DB into thinking there are multiple commands. WHERE ID = ? = '1234; Truncate table1' This won't return any data but it also won't run the delete as a separate command. Some applications dynamically build their SQL instead of parameterizing it. queryString = "UPDATE EMPLOYEES SET NAME = " + variable1 = " WHERE ID = " + variable2 This type of code is easy to do a SQL injection by inputting a malicious value in variable2.
So, basically, I was right. You want to enable users (programmers!) to not understand the data that they're interacting with. Might as well just use NHibernate or some other ORM. Edit: Good luck with your endeavor though!
This is good to know. Thanks for your help. 
You can't learn it on the command line?
Would it make a difference? I am 100% new to databases and MySQL so I am clueless where to start, all I know is that I have to do it with the command line in WAMP. If theres no difference in WAMP and the command line then ill just go watch tutorials on the command line and SQL. Thanks for replying at least because I dont know if what I just asked is a stupid question and why i being downvoted.
WAMP is a combination of web service programs (Windows Apache Mysql PHP) that are usually pre-configured for ease of installation. All WAMP stacks include the MYSQL command line, and a whole buttload of WAMP packages include the web application PHPMyAdmin to provide a GUI database administration. IMO PHPMyAdmin is absolutely *terrible*, there are much better, faster programs to do what PHPMyAdmin does. The Command line is much more powerful if you know what to do, but obviously does not come with the benefit of a GUI. Ask your Lecturer what he means: The built in command line or the built in PHPMyAdmin. If he says PHPMyAdmin, I ***Highly*** recommend dropping the lecturer or going to the dean. Learning to use PHPMyAdmin does nothing to teach you the primary skills that you learn with the command line. Considering that you interact with a database primarily with text, learning to use a GUI will not help you in the slightest when it comes time to write an application in SQL. You should only use PMA as a tool after you are proficient with SQL in the command line, similar to how you should only use a nail gun after you learn how to use a hammer. I could rant on how using WAMP or the linux cousin LAMP doesn't help you learn the intricacies of the programs you are running or how to trouble-shoot them, but that is a different issue altogether. IMHO you are being setup for failure if you don't learn how to build and configure server software the old-fashioned way. I can give you a crash course on how to use the command line if you'd like?
Well, I'm saying that that *probably isn't a very useful query* because it's very misleading. However, given your description it's about all I could come up with. It's going to be misleading, however, because I'd the above to return this: +--------------+----------+-------+-------+-------+ | datetime | username | infoa | infob | infoc | +--------------+----------+-------+-------+-------+ | '2015-01-31' | 'Alice' | 199 | 'XYZ' | 1 | +--------------+----------+-------+-------+-------+ But those values don't really mean anything presented like that. They're the maximum number or last in alphabetic order within the date, but that's all. They're not really related to each other beyond the fact that Alice had them on that date, but no record in the sample data has those values. infoc = 1 never occurs with infoa = 199. If you only want one per day, then I'd say you should pick a time that is significant. Like, pick the first one or the last one, or pick some criteria that allows you to say, "Show me the most significant record within the time period." I'm saying you sound like you want a group-wise maximum, minimum, or something. But I don't know what makes the most sense for you since your description is too short on details. If you want the last entry in a day, for example: SELECT t1.username, t1.[datetime], t1.infoa, t1.infob, t1.infoc FROM ( SELECT username, CAST([datetime] AS [date]) AS [datetime], infoa, infob, infoc, ROW_NUMBER() OVER (PARTITION BY username, CAST([datetime] AS date) ORDER BY [datetime] DESC) AS GroupRowNumber FROM Table WHERE [datetime] &gt;= '2015-01-31 00:00:00' AND [datetime] &lt; '2015-02-01 00:00:00' ) t1 WHERE t1.GroupRowNumber = 1 (Note: I'm not sure if you can CAST() in the PARTITION BY portion of the OVER () clause like that.)
I can understand the frustration with the question... But please understand... There are often folks that do not have a strong grasp of the technique but they are trying. Specifically I learn best from example and the query examples provided are immensely helpful. At the end of the day I will end up using a variation of these and I will come up with a working solution and learn something new . I do not expect a box solution but appreciate any help. Fwiw... I do tip with gold for helpful examples. More if someone helps me I try to reciprocate. :) 
Good to know for trolling purposes. Write all queries as right joins.
Sorry if I came across as confrontational. That wasnt what I meant &gt; how do you expect the resultset to look when it has multiple rows of infoa, infob etc? This was a genuine question, and by asking this, it usually helps people to understand why you cant distinct on only one column. You mention to da_chicken.. &gt; if there are multiple rows for Alice on a specific day I only want to show 1 and also have a few of the corresponding columns included But if there are 3 rows for Alice, you only want to show one row. But then you need some criteria to decide what values to show for the other columns. Will it just be the most recent row, or MAX(infoa), SUM(infoa) etc?
DISTINCT can be a less-than-performative command. The Sr Developers who review your code are going to challenge you to join on a field and use a parameter to identify distinct records, rather than using the command, if the query is to be repeated .. or more importantly, if the query is part of a nightly job or web service. I've found COUNT(*) to be more performative, and it brings the added bonus of answering the next question your executive team is going to ask. "How many times are people signing in" DECLARE @Today DATE SET @Today = CAST (GetDate() AS Date ) SELECT COUNT(*) AS UserCount, username FROM [Table] WHERE CAST([datetime] AS Date) = @Today GROUP BY username HAVING COUNT(*) &gt; 0 To have the query return for multiple days, just adjust the @Today variable to x# of days ago
You can learn SQL in w3schools and http://sqlfiddle.com As for learning to implement SQL for a web backend, do whatever they tell you to do. 
he said focus on command line with wamp servers
This, the command line for SQL in wamp uses MySQL, important to note as there are small differences from other programs. 
Turning on TDE doesn't make queries or applications work differently. The data is encrypted at rest. Data in the database will still be readable while it is online like normal. However, someone won't be able to take a backup of your DB or your files and restore them on a different instance. Turning it on is pretty easy. Create your master key, your certificate, and then enable it. Depending on the size it may take a while to complete. http://www.databasejournal.com/features/mssql/transparent-data-encryption-tde-in-sql-server.html
The biggest hassle with this is the size of the data files and how long it will take to encrypt. And Ya, the actual data isn't encrypted, but tde only encrypts the files for restore purposes. Had to migrate large databases that were encrypted with tde. Unanticipated delay on the migration timeline.
Yep - this.
Also afaik, a JOIN without any qualification, as well as a NATURAL JOIN follow the same rules as an INNER JOIN. 
I like the venn diagram way of thinking of joins: http://i.stack.imgur.com/GbJ7N.png
A friend told me one day after his database class that the professor told him he would more often be using natural joins than anything else. Sadly, he wasn't joking. I probably spent an hour ranting about what a terrible idea that would be.
I did, as soon as I read over my reply I face palmed so much. Yeah the ON section was the id's. Also I literally just learned about implicit and explicit differentiation the other day so I do need a bit more time to break into it but I feel really rushed and it isn't something I can learn in a week.
One query at a time, you'll get there. :)
Been learning a lot about sql in the last 2 months and came across a similar scenario. But in my case i needed all the information but only on one row per user. My solution was to concat all the infoA information into one column. If this is what you are looking for you can create a solution with a recursive CTE. It's very tricky and i'm still trying to grasp it fully, but a very functional solution. Copying this output from da_chicken to illustrate: +--------------+----------+---------+----------------+-------+ | datetime | username| infoa | infob | infoc | +--------------+----------+---------+----------------+-------+ | '2015-01-31' | 'Alice' |199, 1, 2| 'XYZ', 'ABC, Ã– | 1, 2, 3 | +--------------+----------+---------+-------+-------+ PS: Formating was crap. Assuming there are three rows for alice this is the result: InfoA = 199, 1, 2 InfoB = 'XYZ', 'ABC', 'Y' InfoC = 1, 2, 3
&gt; the professor told him he would more often be using natural joins than anything else. Sadly, he wasn't joking. In 15 years as a professional, I have never seen a natural join. In fact, I hadn't even **heard** of it until a few weeks ago on this sub - and after reading about it, I was thankful that I'd never known about it before.
The issue is, no amount of roll-your-own website/platform will ever replace dedicated developers. Most client's would much rather have something unique to them, something exciting, and powerful. Most cookie-cutter sites don't offer this flexibility. SQL is used just about everywhere. You'd be hard pressed to find a single corporation in existence that doesn't use databases. With the huge emergence of massive dedicated data warehouses for handling business functions, and big data, SQL is everywhere. SQL handles the backends for almost every dynamic site in existence, and that probably won't be changing anytime soon. 
Luckily, even small businesses have an absolutely real need for SQL developers. Many may not even realize the level of control they'd have by implementing it. For example, I started working at my current job roughly a year ago. They were using a horribly designed SQL Server application to managed several large data centers. The application was merely pieced together, but was very prone to issues, errors, breakdowns, etc. Over the course of the past year, I've worked on redesigning this entire system, and building many proprietary applications for the company to use for other purposes. Even small businesses can benefit from having custom tailored applications. Run a trading card store? Why not have someone design a system that integrates with your store inventory to track sales, allow for online purchases, and tracking? Run a contracting business? There are companies out there that make SaaS applications to fit SOME needs of a company, but every industry is different, and often having something specific to them is majorly beneficial.
After going through a lot of online resources so far, [I highly recommend the Stanford course](https://class.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-vid-introduction_to_sql/) She shows good examples, and goes through a lot of different concepts at a good pace. The pages in which you can do the exercises are relatively challenging and if you're ever having trouble, just Google the question directly and there will be somebody's answers posted on github to give you some guidance. 
i used w3schools a while ago.
You can check out [SQLZOO](http://sqlzoo.net/wiki/Main_Page) 
One use case: https://www.periscope.io/blog/use-subqueries-to-count-distinct-50x-faster.html
I enjoy Kudvenkat on youtube. His lessons are pretty straight forward and easy to follow. Plus you'll find yourself saying "ok cool" a lot after.
I personally think that Venn diagrams don't work for joins that well. Let's assume you understand and can imagine what a cross-join is (let's say visually it is a square or a cube with original sets as sides). Try this: imagine the source set A sitting on the left, the cross-join cube (AxB) sitting in the middle, the second set B sitting on the right and the join predicate sitting in the front of the cube. A regular (inner join) consists of parts of the AxB cube after the predicate is applied to cut off unneeded records. A left outer join is the above ("inner join") plus the records from the left set (A) which are not in the 'cut-up cube'. A right outer join is the similar but the extra pieces come from the right set (B). The full outer join is the cut-up cross-join cube plus missing records from both sides. Cross apply is the join where you don't have SQL server build the middle part (cross-join cube) and the predicate works like a function to build the right data set for each record, then cross joins the record to whatever was built and appends to the output. Thus the records that do not produce any right side output drop off from the result. Outer apply is similar to the above but it does the 'left outer cross join', keeping all records from the left record set. The picture: http://imgur.com/8yr9anG 
i think i did it? WITH Query AS ( SELECT * FROM #matching), Numbered AS (SELECT *, ROW_NUMBER() over (partition by [user id], [group name] ORDER BY CASE action WHEN 'Remove' THEN 1 WHEN 'Add' THEN 2 END) AS Rn FROM Query) SELECT * FROM Numbered where Rn = 1 GO
Seconded. if you don't need to return any fields from that child table, a subquery will be faster than joining it in.
It could be better but here it is in SQL Server. Bit of a hack using DISTINCT so it only returns each solution once. WITH positions(i) as ( SELECT 0 UNION ALL SELECT i+1 FROM positions WHERE i &lt; 63 ), solutions(board, n_queens) AS ( SELECT CAST('----------------------------------------------------------------' as char(64)), cast(0 AS int) FROM positions UNION ALL SELECT CAST(SUBSTRING(board, 1, i) + '*' + SUBSTRING(board, i+2, (LEN(board) - i+2)) AS char(64)) ,n_queens + 1 as n_queens FROM positions AS ps, solutions WHERE n_queens &lt; 8 AND SUBSTRING(board,1,i) != '*' AND NOT EXISTS ( SELECT 1 FROM positions WHERE SUBSTRING(board,i+1,1) = '*' AND ( i % 8 = ps.i %8 OR cast(i / 8 AS INT) = cast(ps.i / 8 AS INT) OR cast(i / 8 AS INT) + (i % 8) = cast(ps.i / 8 AS INT) + (ps.i % 8) OR cast(i / 8 AS INT) - (i % 8) = cast(ps.i / 8 AS INT) - (ps.i % 8) ) ) ) -- Perform a selector over the CTE to extract the solutions with 8 queens SELECT DISTINCT board,n_queens FROM solutions WHERE n_queens = 8;
Are your statistics built using 'Sample xx percent' or 'sample xxx rows' perhaps? If so, the statistics can be skewed, even if recent. See if 'update statistics ... with fullscan' will fix it?
I used W3Schools but they really don't go very explicit with stuff. I've used it a few times, it's okay if you're looking to fix syntax but then again Oracle aren't much better tbh.
nice, thanks for porting this zerophase!
Thanks for sharing this. I did not know about foreign data wrappers, and now I'm kicking myself because this would have saved me a lot of time and frustration.
Yep, it's how I would've done it, but not sure why you abstracted **Query** in the CTE, surely you could do it in a single statement? 
I got this now, SELECT voornaam, achternaam, titel, exnr FROM boeken, auteurs, exemplaren WHERE exnr &gt;=3 ORDER BY exnr, achternaam http://imgur.com/wRLmRng There is like Richard Adams Wonderfull live 3 tiems in the list. How is it possible to see only 1 of it?
I know, I dont have any clue about the basics. My teacher said here you go. You can use this .mdb file and you get 5 tasks and tomorrow you need to explain task 1 to the class.
Then your at a terrible school. However I find it hard to believe your expected to learn this in a night. I've told you what you need to learn, I'd get on with it :)
I have 3 tabels. EXEMPLAREN, BOEKEN, AUTEURS. So far as i understand you need to take the colums at SELECT en FROM is where the coum comes from. So if I want the voornaam (first name) of an auteur I need to to SELECT voornaam FROM auteurs. Is this what u mean by joins? Or do I need to use SELECT auteurs.voornaam?
The real future and challenge in IT lies in handling the massive amounts of data that people and businesses generate. Where do businesses store data? In databases. Your knowledge of sql will serve you well for many many years. But don't limit yourself to only sql. Combine it with some general purpose programming language as well.
"Analytics" is a growing area and it's all about being able to sling data around, and much of that requires SQL. So many businesses often have multiple database vendors and non-database sources of data. The need to be able to bring that data together and make it useable will not go away for a long time. So an ability to write SQL in multiple systems will be a valuable skill for some time to come. But as others have said, there's so much more to databases than serving as ~~front~~ back-ends for web applications.
according to the fanboys in /r/programming there might not be a need to know SQL, since ORMs are so awesome and have no downside whatsoever, but SQL is just the syntax to a way of handeling data. Syntax is easy, knowing how to handle large amounts of data is the challange. If you got the thinking down, you can go from SQL to MDX as easy as going from Java to C#. Data is an entirely different realm compared to object oriented programming. I won't make a prediction if in 20 years we will still write SQL, but we will still be handeling data. And the ability to think set based in millions of rows, THAT will not get obsolete in the foreseeable future.
If you're using table variables, consider dumping their contents into a temp table if you can - or enable trace flag 2453 with SQL 2012 sp2. [Table Variable Cardinality](http://blogs.msdn.com/b/psssql/archive/2012/08/22/10053781.aspx) [SQL 2012 SP2 and Table Variable Performance](http://blogs.msdn.com/b/psssql/archive/2014/08/11/if-you-have-queries-that-use-table-variables-sql-server-2012-sp2-can-help.aspx) Edit: missed the note on MAXDOP. [Give your waits a look over](https://sqlserverperformance.wordpress.com/2009/12/21/great-resource-on-sql-server-wait-types/)
SQL is a weird language. Its a great language and there are tons of people that end up in positions where they write SQL more than anything else, but I don't see many "SQL Developer" positions. It's seen as an accessory skill, something that's more or less expected of anybody working on line of business apps. Knowing SQL really well, which implies knowing DBs really well, is one of those things that can set you apart as a programmer once you've been hired. However, it's not likely to be the thing that gets you hired. There's a certain logic in that in the sense that, to be honest, academic SQL courses are fairly worthless. It's like learning French without ever studying abroad in France. Most people never *really* learn SQL until they are forced to work with a large database day in and day out and most people never have access to a complex multi-gigabyte (or terabyte) DB until they start working with one professionally. In short, SQL is an invaluable skill but if you want to get hired you're better off being a "Programmer that knows SQL" instead of a "SQL Programmer". 
I'll second sqlzoo. Together with the book *T-sql fundamentals*, sqlzoo taught me sql.
What program did you use while taking this course? I just downloaded mysql and been trying to create my own local server so I can start doing those tutorials but I don't know if I'm doing it correctly. I feel so overhelmed. :P
That is great, I will definitely use that. 
Are you using SQL Server (MSSQL)? If so you can use [LAG](https://msdn.microsoft.com/en-us/library/hh231256.aspx) that does exactly what you are looking for. 
Yep, rownumber is the shiznit!
LoL.
Well... I happen to be one of those fanboys over in /r/programming, I am still rather fond of SQL, and there are simply certain things I cannot accomplish easily using such things like Linq. And when formatting/correcting old data, I'm constantly hopping into SSMS and manually writing queries to fix bad data.
First off saying a subquery is "better" or "worse" makes it seem as though the two are equivalent, which they are not. One is an Orange the other a Banana, they are both fruits but other than that don't resemble each other. You should do a join on two tables when the joined field (or fields) are of the same "domain". Which is to say they occupy the same value space - this is not same as simply the same data type. Join's will always perform better (in any sane relational DB anyway) than a sub-query to produce the same result set with the exception of the exist/"not exist" short-circuits (already described in another reply). The purpose of a join is to say: "I have two disparate data sets that are common on this one field and I wish to see them combined together aligned by that data value". Basic set theory really. Sub-queries on the other hand are different. They have lots of uses and depending on the DB used and the specific case can perform better or worse than other techniques to accomplish the same result set. I'm afraid the best advice is to try things out and see. There are major and minor differences in DB optimizers that can make one technique superior to another (temp tables, table variables, etc.) As I said there are lots of types of subqueries. Some are used to return a result value in the final result set, some are used like filters - kind of like joins but not coming from a "related" result set per se, some are just used as generators. Lost of sub-queries are nothing like joins but there is a special one that can function like a join which is where, I would guess, your confusion comes from. It is called a correlated subquery. https://en.wikipedia.org/wiki/Correlated_subquery These sub-queries use a value from the "outer" result set to perform the sub-query. They come in handy when needed but do have a performance impact as sometimes (although not always) the only option for a db is to run the sub-query once for each row of the outer result set. Obviously for large results sets that can be a non-starter. 
I hear you man. 
Homework? This comes up quite a bit. If you show us your table definitions (and preferably the data you are using), we may be able to help. In the absence of this, visit [Relational Algebra](http://en.wikipedia.org/wiki/Relational_algebra) and look at the section headed "Division (Ã·)". HTH 
Ask your professor or ta. They know the data you're working with better than we do.
The tool is buggy not the code. Address the tool issue, log a bug or alter the regular expression which performs the syntax highlighting in the editor.
OK, well it must be a problem with TEACHES, what's the PK on that table?
Try this... select * from INSTRUCTOR i where (select count(distinct COURSE_ID) from TEACHES t where t.ID = i.ID) = (select count(*) from COURSE c where c.DEPT_NAME = i.DEPT_NAME) order by i.NAME 
Here's another approach using an anti-join select * from INSTRUCTOR i where not exists (select null from COURSE c left outer join TEACHES t on t.COURSE_ID = c.COURSE_ID where c.DEPT_NAME = i.DEPT_NAME and t.ID = i.ID and t.COURSE_ID is null) order by i.NAME
OK, looks like my hunch about TEACHES is correct, ID, COURSE_ID is not the PK.
Well perhaps the query is correct and the expected answer is wrong, without seeing the sample DB it's hard to know.
you could maybe show us what you tried, and the error message it produced?
It totally is, but it would be a different column. Also, unless this dataset is changing frequently, an `UPDATE` may be the way to go.
Sure, I'll have to paraphrase a bit, but here's the gist of it: EXEC (' CREATE TABLE Test WITH (REPLICATION = HASH(KeyID), CLUSTERED COLUMNSTORE INDEX) AS SELECT [Field1] [Field2] [Field3] (SET CalculatedColumn = CASE WHEN criteriaSet1, THEN CalculatedColumn = ''High'' WHEN criteriaSet2, THEN CalculatedColumn = ''Low'' ) AS CalculatedColumn FROM SourceTable ') Which gave me a syntax error near '=' on line 10 in this example.
That would be fine, you just have the syntax wrong. You can find an example [here](https://msdn.microsoft.com/en-us/library/ms188300.aspx).
Thanks - I figured as much, but do you have any pointers on where to do this, as I'm sure it must be a user fixable issue, but I've been through the tools &gt; preferences options and cannot find whatever controls this functionality. I've looked in oracle's forums for SQL Developer, but have not found an answer.
Did you ever get this figured out? If you're using table variables, even if they have a clustered index the optimizer will still estimate the number of rows as 1. You can try the OPTION (RECOMPILE) after the query to force the optimizer to find the actual number of rows in the table variable. EDIT: disregard, just saw your edit about option recompile / CTEs. EDIT2: take a look at your join conditions? The optimizer makes certain hard-coded assumptions about row counts depending on the operator (&lt;, =, IS NULL, etc.)
/u/r3pr0b8 nailed the syntax for your case statement. If you are running MS SQL 2005 or greater, I would also suggest looking into [computed columns with PERSISTED](http://www.mssqltips.com/sqlservertip/1682/using-computed-columns-in-sql-server-with-persisted-values/) turned on. That way, when the data is updated, it will automatically recalculate your computed column for you. Otherwise, you may find inconsistencies if the rows are updated, but your calculated column is not recalculated.
It likely is not user fixable. Much in the same way that, in SSMS, update appears as pink (function) instead of blue (command). It likely needs to be addressed by the developers of the tool. [Found this thread](https://community.oracle.com/thread/3551327) which has some info on where to submit bugs (it's from April 2014) for Oracle SQL Developer (I'm assuming that's what you're using though) Edit: I realize that update is both a command and function in SQL. My example was for when it is used as a command, it remains pink. [Microsoft has no intention of fixing it](https://connect.microsoft.com/SQLServer/feedback/details/736225/syntax-highlighting-update-keyword-is-pink)
I'd suggest you reconsider doing it all at once. You can use linked server for cross server queries, but the performance is horrible for joins, as the whole table has to be passed from one server to another and THEN filtered on your predicate. It's not meant to be used in the same scenarios as cross database queries. SQL 2008 R2 supports the same compatability mode as SQL 2000 does. Do an inplace migration/upgrade, and you may be looking at as little as a couple of hours work. Test it before hand of course - and if you can, do the migration on a separate server (like a log shipping target).
I can honestly tell you that SQL is not going anywhere. People with SQL skills are in such high demand. Having access to data and being able to use SQL to communicate with that data and answer questions is a HUGE SKILL. People want answers quickly from a database or any data source and SQL is the way to go. Another reason for the demand is most people don't like writing code. I work with some smart people who just hate SQL because they don't like to write code and that is totally fine with me. There is no tool out there that comes even remotely close to replacing SQL. There are some things that try, but they only satisfy a small niche. Tools like pandas which I just dont get the hype around it, make retrieving data and cleaning it up seem like a nightmare compared to sql. If you are not a data scientist I see no need to ever touch a tool like pandas unless you want ass backwards way to select data from a dataset, go for it! Also, if your data does not lie in a mysql database forget about using pandas. 
I've used QuickStart and had them do a customized curriculum. It's about $1000/person for 3 days. You might also check this recent post for some options: http://www.reddit.com/r/SQL/comments/2uj0ft/any_sites_like_codeacademy_to_learn_sql/ 
&gt; The reason for first style is if you need to quickly comment a field out you can do so simply by putting "--" at the line you wish to comment out. You can't comment out FieldOne that way in either the SELECT clause or the WHERE clause. You can't comment out TableOne that way, even if there are multiple JOIN clauses. (shrug) With more columns, you increase the odds that the one you want to omit isn't the first. I've been writing SQL for almost 30 years. The position of a comma doesn't have much effect on my productivity.
can you upgrade? current version appears to be doing better formatting, i don't see the WHEN N'' getting set to WHEN'' when formatting a case statement.
the first style looks the most useful to me, even though it's not exactly what i use. i've rolled my own conventions over time, but honestly that first example has really good ideas. i think a common sql style standard would be great (or at least one for each rdbms), but i've never seen an 'official' one. in my experience, shops either have their own or leave it to developer preference. most of the tricks and formatting i have seen/used are usually there to do things like what you mentioned: easy to comment things out, etc. in the end, they're just shenanigans to make it easier to develop in traditional sql editor (pgadmin, sql server studio, etc.). i'm a heavy vi user, which means i have a rather powerful editor out of the gate, so i can easily modify sql scripts and run them (i use postgres the most lately) with psql (# \i path/to/script.sql).
I've been looking at different standards on the Internet. Some have good ideas others not so much. I'm trying to get the best ones and put them together for my team. We're using MS SQL but I'm trying to keep it open for all platforms.
Worth a try - I'm using 4.0.0.13 at the moment.
It's much more 'difficult' to forget a comma if it's at the start of the line. That's why I use that method. Commenting is not really an issue as the others have pointed out.
Joe Celko has a book on this topic called *Joe Celko's SQL Programming Style (The Morgan Kaufmann Series in Data Management Systems)* Celko has some great ideas but he can be overbearing at times. My advice is to read the book and take what you need. http://www.amazon.com/Celkos-Programming-Kaufmann-Management-Systems-ebook/dp/B006L21AO6/ref=asap_bc?ie=UTF8
I tend to use a mix of both, comma at the end - because hitting enter and commenting isn't a big deal to me, and AND in front of a predicate, because it just feels more readable to me. Of course it could also be that it's the default for SQL Prompt, and I'm just lazy... :)
I use form 2, but I make more use of whitespace between sections. I also prefer my code to be 'flat' and without tabs. This isn't HTML, that kind of hierarchy is not necessary, with the exception of nesting. Select FieldOne, FieldTwo From TableOne Join On Join On Where group By Order By
Personally, I avoid the nesting by using temp tables as much as possible. Nesting can also be a drain on performance I have learned. And I hate them. They are ugly.
what??!! you use temp tables because subqueries are a drain on performance? sheeeit, i'd hate to have to work with you, dude
Here's my style: DECLARE @SomeConstant int = 3 DECLARE @SomeOtherConstant int = 12 SELECT T1.SomeColumn , T1.SomeOtherColumn , T2.YetAnotherColumn FROM dbo.Table1 T1 INNER JOIN dbo.Table2 T2 ON T2.FKColumn = T1.PKColumn LEFT OUTER JOIN dbo.TableWithALongName LN ON LN.PKColumn = T1.PKColumn AND LN.FKColumn = T2.FKColumn WHERE T1.SomeColumn = @SomeConstant AND ( LN.PKColumn IS NULL OR LN.FKColumn = @SomeOtherConstant ) It keeps the table names, table aliases and join conditions vertically aligned and keeps the join types a bit out of the way but still plainly visible. I like to put magic values into descriptively named variables. I find this style makes it pretty easy to digest big complicated queries about as easily as possible.
Thank you so much for this. Helped a ton and it's running now, we'll see after 8 hours or so whether or not it did what I wanted haha. Much thanks though, I really appreciate it.
Thank you for this tip, that's really awesome and I had no idea it existed. It won't be necessary for this particular job since the data definitely won't be changing, but this is great to know for the future. Cheers!
This would mean that the production database would be temporarily offline for the copy? If that is the case I probably cannot implement it, would you have any other ideas?
I suppose we could do that, we were previously concerned with transfer rates with the giant DB, but id we don't restore that one it should be fine. Is there a wizard to walk through that type of scheduled job in SQL or is that pretty much a T-SQL gig. I could walk myself through it, but I am not that proficient.
I rarely care but I do like the format you get from http://poorsql.com There are plenty of SQL formatters online. This particular one just happens to still work through our tough web restrictions at work
Please state your database platform, it makes a difference as syntax and available commands differ between platforms.
I recommend using what works best for your workflow combined with whatever makes the query you're using more readable or maintainable. Quite honestly, for any sufficiently complex query that you didn't write yourself, you will want to reformat it to really understand what's going on. I also recommend the [Poor Man's T-SQL Formatter](http://architectshack.com/PoorMansTSqlFormatter.ashx), which has a plug in for both SSMS and Notepad++. You can pretty easily get it to format close to your preferred style, and then modify it slightly from there. My preferred style is: select t1.FieldOne, t1.FieldTwo, t2.FieldThree as "FieldAlias" from dbo.TableOne t1 inner join dbo.TableTwo t2 on t2.KeyField = t1.KeyField and t2.OtherKeyField = t1.OtherKeyField where t2.FieldFour = 'Value' and (t1.FieldFive = '' or t1.FieldFive is null); My boss prefers lower case keywords, so that's what I use. Our query analyzers all have syntax highlighting so the capitalized keyword thing is no longer relevant. I always use double quote identifiers, probably because I don't work with Oracle. I always qualify every field with the table alias unless I'm dealing with a single query. I always make my join conditions line up on the comparison operators, and I always put the second table first in the join condition. It makes it really easy for me to add the table and then list very quickly the key fields. I also an a big fan of two spaces after the `on`. Even if the new table has to join to three other tables, I can add and later see quickly what the relevant keys are in the given table. For comparisons, I always break on an `and`, and try not to break on an `or` unless the line gets too long. If I need to comment stuff out, honestly it's so trivial to do that I don't really care. Going to this takes like 10 seconds, but is a total waste of time if I don't need the flexibility: select t1.FieldOne, --t1.FieldTwo, --t2.FieldThree, '' "-" [...] where 1=1 --and t2.FieldFour = 'Value' and (t1.FieldFive = '' or t1.FieldFive is null); I also try to do things like always sort `in ()` lists. It makes it so much easier to find the one value you want to remove. Finding a way to quickly transform a list of items into a comma delimited list (especially quoted) is essential. It's easy enough on SublimeText. Pretty much everybody's format method breaks down. Subqueries always look ugly. Too many parentheses always confuses things. At some point, you have to sacrifice your personal convention for readability. When I work with Entity-Attribute-Value tables, I have a way I format the joins to be easier to read for me, for example.
Well YouTube obviously has crazy amounts of videos. I'd recommend downloading SSMS express editions on your PC/laptop, attaching the free databases (northwinds etc) and following along YouTube videos for the fundamentals. If you have money to spare you could consider Lynda.com From a job perspective the titles can fluctuate. However, business /data analyst is a very common search term that I've encountered. I think from an experience perspective you have to open up and consider even simple roles like DBA assistant or the like (not a common name for a position, but extremely small companies tend to be creative with their I.T postings). Basically, any real hands on time out of the confines of the SSMS environment that lives on your laptop, that you can apply to real life is key. Learning the fundamentals of T-SQL and how other variations differ can be something you can learn actively as well as you are getting the experience in. Feel free to p.m if you'd like to discuss more. Good luck!
Microsoft SQL Management Studio
It's hard to beat [SQLZOO](http://sqlzoo.net/wiki/Main_Page). The tutorials give you example SQL statements and then ask to to modify the SQL to achieve a slightly different result. It lets you try out SQL without having to install a database server. You can select from several different popular database engines as well. 
 [Brent Ozar](http://www.brentozar.com) has a pretty decent blog. 
You can't go wrong with downloading a free sql variant and, while going through online stuff, building your own thing. Make a database for holding data about your contacts or stuff in your house or something. You'll learn syntax for querying and also, as you go along, realize where your design sucked and how to improve it. 
Knowing nothing about your table structure, something like this might be on the right track. I think maybe for MSSQL the limit directive goes next to the select. Select table.ID, min(table.date) as oldest, next50.date from table left join (select ID, date from table order by date asc limit 50) as next50 on table.ID = next50.ID group by ID, next50.date
 CONSTRAINT IC4 CHECK((priority = 'low') AND cost &lt; 1000) This isn't saying that when priority = 'low', cost *has* to be &lt; 1000. It's saying that priority *has* to be 'low' and cost *has* to be &lt; 1000. Always. So when this line comes in: INSERT INTO Orders VALUES (10, 'high', 2400); It's checking against IC4 and failing because priority &lt;&gt; 'low'. You need to have IC1-4 as a single check condition and use `OR` statements to match up your appropriate priorities and costs. CONSTRAINT IC1 CHECK (priority='high' OR priority='medium' OR priority='low'), CONSTRAINT IC2_4 CHECK ( (priority = 'high' AND cost &gt; 2000 ) OR --IC2 (priority = 'medium' AND cost &gt;= 800 AND cost &lt;= 2200) OR --IC3 (........ ) --IC4 ) Alternatively, you can keep them separate by defaulting to TRUE (1=1) when priority isn't "in" the constraint's scope. CONSTRIANT IC2 CHECK (CASE WHEN priority &lt;&gt; 'HIGH' then 1 WHEN cost &gt; 2000 then 1 -- since the previous WHEN caught all the other priorities, we don't have to check = 'HIGH' here again ELSE 0 --if it makes it to this, the check should fail. END = 1)
That's a good suggestion. At first I was thinking I want to avoid DBA type of roles but any experience I can get from different ends of the spectrum would be beneficial. Thanks much!
Without there being any particular order to it, and there are way more worth reading : [Glenn Berry](https://sqlserverperformance.wordpress.com/) [Paul Randal](http://www.sqlskills.com/blogs/paul/) [Kimberly Tripp](http://www.sqlskills.com/blogs/kimberly/) [Paul White](http://sqlblog.com/blogs/paul_white/default.aspx) [Grant Fritchey](http://www.scarydba.com/) [Andy Warren](https://sqlandy.com/) [Aaron Bertrand](http://sqlperformance.com/author/abertrand) [Adam Machanic](http://sqlblog.com/blogs/adam_machanic/) 
&gt; SQL 2008 R2 supports the same compatability mode as SQL 2000 does Are you sure about that? The last time I moved a database from 2000 to 2008R2 (via backups), we had to make a stop at 2005 &amp; upgrade the compatibility mode there before moving on to 2008R2. I don't like in-place upgrades in general. If the box is old enough to be running SQL Server 2000, it's probably ancient hardware with an unsupported OS. Just get everything upgraded on a fresh setup.
Well honesty a DBA role becomes a natural evolution from where a data analyst/ report writers role stems from. While the role of an admin can seem overwhelming, it's the SQL experience that does most of the heavy lifting ( as well an integrated knowledge about other processes involved). Don't be surprised if that's the way you find yourself going once you've jumped into a data analytics role. Cheers and happy hunting!
Still need help! I guess with my situation, I'll have to disclose a little more information. I was being a bit vague to try to not reveal too much about my project and used a few different details. I need to limit it to 42 (not 50) and...yeah. Picture shows my query. http://i.imgur.com/Ap3hBDz.png 
Sub queries just act as the component you're replacing so if you're making a join and you want a sub query to act as a table just put the select statement for your sub query inside () and voila you win the internet! Same thing if say you need a variable where clause just put IN() around your select statement.
select id,name,puppies from tablename; select id,name,puppies from (select id,name,puppies from tablename); 
My work is putting this training out to the public while we train our employees, here's more details: http://www.aaronbuma.com/2015/01/free-sql-server-training/ *this is not to push people to my personal blog, just trying to get the word out about free live sql training.
 SELECT TOP 50 * FROM MyTable WHERE LocationID = ( SELECT TOP 1 LocationID FROM MyTable ORDER BY MyDateField ) ORDER BY MyDateField
Start trying to answer some questions over on stack overflow. This will give you experience trying to work out the sql for complex real world situations
In my experience, I started out by using an analysis package called Hyperion/Brio Intelligence (which is kind of like SAS), as well as MS Access. Both allow you to do a lot of query building using visual objects, connecting lines for joins, etc. Getting data from a database and learning about all of the relationships was daunting and it was a long time before I wrote a single line of SQL. Now I do it professionally as a data analyst and consider myself somewhat of an expert at my company. I would also say that it helps tremendously if you are working with data that interests you. For me it's airplanes, airports, and everything to do with aviation. If you have something you're interested in, learning how to retrieve and manipulate the data is so much easier. A lot of people take courses on this stuff, but I've met more people who've just learned it on the job or as something to help them with their own personal projects. It can be a lot of fun if you're learning it because of an underlying interest!
It would be cool if Reddit had some sort of live-broadcast (with recording and chat) capability.
Will these be recorded?
You are a wizard! Thank you so much!
Way cool, I appreciate the comment!
Yes they are and are usually available by the next session. Here is a post on last weeks presentation with links to youtube and slides. Again, I'm not trying to pull users off Reddit, so please ask questions etc on here. http://www.aaronbuma.com/2015/02/aggregating-data-and-aggregation-strategies/ https://www.youtube.com/watch?v=0h2LZtaMXOg
Are you getting a specific error from this?
I love google.
BO? Body Odor? Figure out windowed functions and partitioning/aggregating data. It's easy to gloss over when you're reading a book. But hard to actually execute sometimes. People love reports that roll stuff up and look at counts and summaries of things based on random columns. /u/voshi has a good suggestion for answering questions on Stack Overflow. 
If you want to show the time difference once the due date + duration have elapsed, a case statement like the following may work (not sure what database you're using): SELECT CONCAT(customer.last_name, ', ', customer.first_name) AS customer, address.phone, film.title, CASE WHEN rental_date + INTERVAL film.rental_duration &lt; CURRENT_DATE() THEN rental_date + INTERVAL film.rental_duration - CURRENT_DATE() END AS datediff FROM rental INNER JOIN customer ON rental.customer_id = customer.customer_id INNER JOIN address ON customer.address_id = address.address_id INNER JOIN inventory ON rental.inventory_id = inventory.inventory_id INNER JOIN film ON inventory.film_id = film.film_id WHERE rental.return_date IS NULL AND rental_date + INTERVAL film.rental_duration DAY &lt; CURRENT_DATE() LIMIT 10;
Not familiar, but there is a resolution on this page that might help. http://forums.kayako.com/threads/reports-kql-time-range.30171/
I missed the part where about "that location alone", glad narayanis had the right answer for you.
This is how I'd do it in MSSQL. SELECT TOP 10 c.last_name + ', ' + c.first_name AS customer, --remember, it should probably be comma space, not just comma a.phone, f.title, DATEDIFF(dd,DateAdd(dd,r.rental_date,f.rental_duration), GETDATE()) as DaysOverdue, CASE WHEN DATEDIFF(dd,DateAdd(dd,r.rental_date,f.rental_duration), GETDATE()) &gt; 1 THEN 1 ELSE 0 END as IsVideoOverdue FROM rental r INNER JOIN customer c ON c.customer_id = r.customer_id INNER JOIN address a ON a.address_id = c.address_id INNER JOIN inventory i ON i.inventory_id = r.inventory_id INNER JOIN film f ON f.film_id = i.film_id WHERE r.return_date IS NULL AND DateAdd(dd,r.rental_date,f.rental_duration) &lt; GETDATE() --remember to specify which table rental_date is coming from
For what you need, I'd probably use a CTE ;WITH permissions as ( SELECT u.*, f.FUID, f.Path, p.ACL, CASE (p.ACL) WHEN 1 THEN 'list' WHEN 2 THEN 'read' WHEN 3 THEN 'modify' WHEN 4 THEN 'full control' FROM Folders f INNER JOIN ACLS p ON p.FUID=f.FUID INNER JOIN Groups g ON g.GRUID=p.GRUID INNER JOIN UG_Join ug ON ug.[Group ID]=g.GRUID INNER JOIN Users u ON u.URUID=ug.[User ID] UNION ALL SELECT u.*, f.FUID, f.Path, p.ACL, CASE (p.ACL) WHEN 1 THEN 'list' WHEN 2 THEN 'read' WHEN 3 THEN 'modify' WHEN 4 THEN 'full control' FROM Folders f INNER JOIN ACLS p ON p.FUID=f.FUID INNER JOIN Users u ON u.URUID=p.URUID ) SELECT a.* (SELECT f.*, u.* FROM Folders f FULL OUTER JOIN Users u ) a --for all LEFT JOIN permissions p ON p.URUID=a.URUID AND p.FUID=a.FUID WHERE p.URUID is null --doesn't really matter which thing you choose here, so long as it's from the p dataset Careful, this query could get stupid expensive to run. Then, if you want only Standard Users, add that filter on both the top and bottom queries in the CTE and also in the subselect aliased a in the query.
I did an in-place upgrade just last Friday. Our in-place upgrade was only meant to move the db off to a newer machine via a backup. SQL 2000 had no support for compressed backups, and it was a rather large db to be moving around uncompressed. Regardless, 2008 R2 definitely supports compatibility level 80.
Main reason I present is to learn the subject better
Thank you very much! This makes a lot more sense now.
Thanks, silkykoala. I may take you up on that when the time comes!
&gt; AND DateAdd(dd,r.rental_date,f.rental_duration) &lt; GETDATE() This is where I get a little angry. If the rental_duration is known as an integer, due_date should be a column (or at least a computed one).
Every folder has a permission on it, every folder has at least 1 Admin group with full control. What that being said, this query would return 0 results because there are no folders with no permissions.
What do you mean exactly? [The order of statement processing](http://blog.sqlauthority.com/2009/04/06/sql-server-logical-query-processing-phases-order-of-statement-execution/) dictates that SELECT is one of the last things that is parsed, thusly you can't typically use the alias ('Problem' and 'Formula' here) anywhere else, but instead you should use the actual math you're doing. The exception is in an ORDER BY.
First, the dad joke: you'd be better off with SELECT Answer. A. You can join to a subquery giving you the sum(formula) B. You can use a windowed (unbound) function instead of a standard aggregate. C. As written, it appears that you can replace sum(formula) with yÃ—count(x)-sum(x), and both of those aggregates are at the same granularity as y, so my guess is that those could be obtained by your 2nd subquery.
I tried this query, I get an error: Msg 422, Level 16, State 4, Line 20 Common table expression defined but not used. Line 20: ( SELECT f.*, u.* Msg 102, Level 15, State 1, Line 23 Incorrect syntax near ')'. Line 23: ) a Any Idea's? Also, I'm probably wrong but can the variable p be used twice? INNER JOIN ACLS p and LEFT JOIN permissions p Thanks for the help!
So, to simplify the example even more, this is what you want to do: SELECT 2+2 AS Four, 3*Four AS Twelve But you can't reuse a column in the very same select like that. An ugly quick hack is of course to redefine the formula: SELECT (2+2) AS Four, 3*(2+2) AS Twelve ... but it's cleaner and better to use a CTE somehow like this: ;WITH CTE_Four AS( SELECT 2+2 as Four ) SELECT Four, 3*Four AS Twelve FROM CTE_Four 
Me and my boss will be putting on a free TSQL training on SET Operators, Derived Tables and CTE's on Thursday at 9am PST. https://plus.google.com/u/0/events/ckok7caok4id9lev8k1gkcjn66c Since we are training ALL of our Dev's, so that they can be 70-461 certified, we are giving back to the community by offering it for free live on the web and later download on our blogs, mine's aaronbuma.com. I hope this isn't viewed as spammy, it's free and lots of tips to learn.
Scalar functions are well known for performance issues. I would research the topic before proceeding. 
Thanks for the advice! Do you mean that the execution time suffers more than another type of function or that scalar functions are prone to issues like tihs?
Reddit removed by asterik's. Here's what I'm talking about: (with *= and =* for left/right join) http://sqlmag.com/t-sql/old-join-syntax-vs-new
haha, that is an interesting way to look at it. I looked over a few lessons on it early and really get the vinn diagram reference.
makes perfect sense. I didn't realize you could use a join like that. thanks! SQL is a totally different monster than VBA. It takes a different mindset that I haven't quite figured out.
My DBA insists on using cursors for everything, especially large datasets. The sad thing is that our database is so out of tune that it's more efficient to let a cursor run for a few hours (days once). It's obnoxious.
instead of using the alias formula, re-iterate your actual formula (b.Y-a.X)
Geez... I'm sorry! Sounds like a DBA who doesn't know what he's doing :\
Option A that you described would be the simplest solution for this problem in my opinion. 
They seem to use case-sensitive collation by default (Badge name = 'Great answer' returned no results). Is this common? The three companies I've worked for all used CI collation. 
Sure. The best method is to practice. That's the only way I learned. It also makes it a lot easier to understand the concepts if you can apply it to an example.
Alright. Let me try and explain this. I'm awful at it, and this is an introductory course I'm taking. I have to list customer code, customer first name, customer last name, the invoice number and invoice date of the invoice placed by the customer for each invoice. Then to sort the result by customer last name and first name in ascending order. here is the Data. http://i.imgur.com/LpgXbcb.jpg Here is my work so far. http://i.imgur.com/n57wzRp.jpg I get an error. Apparently INV_NUMBER can't be pulled.
Cant be pulled because its not in your FROM caluse. You need 'FROM Customer JOIN Invoice ON Invoice.Cus_Code = Customer.Cus_Code' This assumes invoice has a cus co field. Btw, that is a crap name for your PK. Should be more like CustomerID or IDCustomet in the real world
It doesn't look like inventory number is in the customer table. You need to join it to the table with the inventory number.
How does that solve this?
this. the problem with textbooks and tutorials is the data is always nice and clean..not full of garbage and other shit in the data along with other quirks setup by people who didnt know better or who just didnt give a shit.
A relatively easy fix is to have the tempdb as the default database for all users.
When I include SUM() in the SELECT statement like that, only one result is returned. There should be a different number returned for each ID. I assume the problem is stemming from the X variable being different for each ID, however it is also used in the formula. 
Re-iterating the formula only returns one result, when there should be a different result for each ID.
my first major/serious SQL development job was for a non profit, working with one primary database, all out of the dbo schema. so it was wicked easy just throwing table names all around. my job after that was with IBM, on a massive data farm with multiple schemas that all served different purposes, and different user permissions by server/db/schema, and different web services using different accounts, each with their own primary needs to update tables in various schemas. I had to un-learn some bad habits and eventually re-trained myself to call out the db, schema and object, as well as teaching myself how to name objects with proc_, tbl_, fn_, vw_, and idx_ in front of them to distinguish their object type. I also learned to stop using SELECT * real quick, and to call out the field names. it was a pain but now its second nature.
Oh, I have used aliases a lot but didn't know there was something called synonyms. It looks very useful. 
This question was asked a few days ago I think... I don't think you really start **knowing** the language until you fill some user requests. Try to answer SQL Questions around here or Stack Overflow. Things that trip me up sometimes are windowed functions, aggregates, or summarizing lots of data in a strange order for a report. My personal pet peeve is people posting obvious homework questions without showing an effort. And the people that think they're doing a favor by giving the answer out. I try to make them show they've at least *tried* before guiding them towards the right answer. 
Just a though, but understanding NoSQL better might indirectly help you learn SQL better.
I like these two: [Simply SQL](http://www.amazon.ca/Simply-SQL-Rudy-Limeback/dp/0980455251) [Database in Depth](http://www.amazon.ca/Database-Depth-Relational-Theory-Practitioners/dp/0596100124/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1423493691&amp;sr=1-1&amp;keywords=database+in+depth)
documentation http://dev.mysql.com/doc/refman/5.7/en/
Check out sp_Blitz. One of the (many, many) things it checks is if there are non-system tables in master: http://www.brentozar.com/first-aid/sql-server-downloads/
NOLOCK should only be used for troubleshooting. If you use it, you will be getting dirty reads and if you don't care about the data, then why are you asking? If you have to use it (let alone rely on it) to avoid deadlocks, then your database structure needs to be looked at (overall architecture, indexing etc) and your queries need to be tuned. To find who is doing the locking, you can use sp_who or sp_who2 to get the SID of the session. If it's holding you up from operating, you can do a 'KILL 13', where 13 is the SID. If that doesn't help, you can try my bosses free tool: http://databasehealth.com/. It will show you the acutal query doing the blocking. I could write up a query for you to use, but it sounds like this might be more helpful. It's similar to my favorite (Confio), but $$$$ less, as in free.
Can you be more specific? I've worked a lot with CouchDB. I don't see how that helps understanding SQL. 
Sorry, I searched but I didn't find any questions that seemed similar to my situation. I work with SQL on my job. So I do write it regularly. But for instance I haven't used joins more complex than inner joins. 
Thank you. I'll check them out. 
i think www.sqlservercentral.com has great newsletters
Got ya, I'll try to make it clearer 
I feel like I got a much better feel for joins after taking a free class from coursera on databases. The introductory material on relational algebra really helped to reinforce joining and filtering operations.
use the [DATEADD() function](https://msdn.microsoft.com/en-us/library/ms186819.aspx) 
This sounds like an overly-complicated solution to a problem you haven't defined here. You're probably going down the path of doing some gnarly dynamic SQL, but if you look for a different approach to the problem you may not need to do any at all.
 DATEADD(hh, 1, StartTime) and DATEADD(hh, 1, EndTime)
You're probably far better off just renaming the tables. There's only 5. You can put the table name as a string into a `varchar` variable but doing anything with that is kind of a pain. You can't use that variable as placeholder for the actual table name in queries or anything like that (without first constructing the whole query into a string and executing as above, which is called dynamic sql). If you know what the data is before it gets inserted into the database, you should probably give it a more meaningful name from the start.
Do you really have to do this with tsql? This seems like something that might be better handled in an imperative programming language. Just curious, why do you need these temp tables as variables? What's your overall goal here?
My goal was to find a randomly generated temp table with a unique column name that I knew beforehand and assign that table to a variable name of my choosing to identify it with. I figured it out with dynamic sql.
How about... ;WITH src AS ( SELECT [Date] = '2015-01-31' , [Facility] = 'Arkham Asylum' , [Metric 1 Numerator] = CAST(502593 AS MONEY) , [Metric 1 Denominator] = CAST(855620 AS MONEY) , [Metric 2 Numerator] = CAST(650239 AS MONEY) , [Metric 2 Denominator] = CAST(999202 AS MONEY) , [Metric 3 Numerator] = CAST(247219 AS MONEY) , [Metric 3 Denominator] = CAST(978974 AS MONEY) UNION ALL SELECT [Date] = '2015-01-31' , [Facility] = 'Mordor' , [Metric 1 Numerator] = CAST(23806 AS MONEY) , [Metric 1 Denominator] = CAST(129134 AS MONEY) , [Metric 2 Numerator] = CAST(440835 AS MONEY) , [Metric 2 Denominator] = CAST(927481 AS MONEY) , [Metric 3 Numerator] = CAST(626508 AS MONEY) , [Metric 3 Denominator] = CAST(270869 AS MONEY) ) SELECT Date, Facility, [KPI ID] = SUBSTRING([KPI ID1], 8, 1), Numerator, Denominator FROM src UNPIVOT (Numerator FOR [KPI ID1] IN ([Metric 1 Numerator],[Metric 2 Numerator],[Metric 3 Numerator])) n UNPIVOT (Denominator FOR [KPI ID2] IN ([Metric 1 Denominator],[Metric 2 Denominator],[Metric 3 Denominator])) n WHERE SUBSTRING([KPI ID1], 8, 1) = SUBSTRING([KPI ID2], 8, 1)
Looks like an UNPIVOT across multiple columns DECLARE @tmp TABLE ( ID INT, Name VARCHAR(5), FieldA1 INT, FieldA2 INT, FieldB1 INT, FieldB2 INT ) INSERT INTO @tmp VALUES (1,'Test1',1,2,3,4), (2,'Test2',5,6,7,8) SELECT * FROM @tmp SELECT ID, Name, Value, Value2, Field, Field2 FROM (SELECT ID, Name, FieldA1, FieldA2, FieldB1, FieldB2 FROM @tmp) p UNPIVOT (Value FOR Field IN (FieldA1, FieldA2) )AS unpvt1 UNPIVOT (Value2 FOR Field2 IN (FieldB1, FieldB2) )AS unpvt1 WHERE RIGHT(Field,1) = RIGHT(Field2,1) The WHERE Clause fixes the cross product issue that is generated, mapping A1 to B1 
Yeah- Query 1 would ONLY return rows in A where one in B exists where LAG(A.DATE) = B.DATE, whereas in Query 2 every row in A will get returned regardless of whether one exists where LAG(A.DATE) = B.DATE.
Probably not, care to enlighten me?
You may want to add some audit checks into the process. Do a count of the number of rows and total a numeric column on the initial file. Do the same thing at the end of the ETL process, accounting for any rows that may have been dropped for quality control reasons. Finally, log this historical data and write yourself a little sanity checker that sends you an alert if a step sees a daily change of more than two standard deviations from the past 30-60 days of data. Depending on the severity, I might even have the load portion of the query stop at a staging table until I can manually assess if something went really wrong, or if it's just an aberration and I'll push it through manually.
Interesting, thanks for explaining. Can you please further explain why the 3 UNION ALLs look like they are doing three separate hits to the database then? http://i.imgur.com/rDRZsIr.png Yet doing the double UNPIVOT suggested by /u/geometrix only hits it once? http://i.imgur.com/7YnmXj1.png
Oh gotcha, ya I definitely would do that. I just wanted to be sure I wasn't missing something, thanks for following up!
Agree with gruffi, if your not using SSIS for this, you should. 
Junior Dev here, so likely adding more questions than answers, but rather than housing the aggregate function in a sub query, could it be dumped into a preindexed staging table, then joining the 7 million record *small* table to the staging table?
How can you skip the excel sheets and move that process directly to the db?
Another better solution for repairing corrupted SQL Server database (MDB) files by MS SQL Database Recovery Tool. This tool can fix any level of corruption, i.e. minor or severe, from SQL Server database files and recover maximum possible data from them. To know more details of this tool visit here:- http://repairandmanage.blogspot.in/2014/06/sql-server-recovery.html
Have you tried CTE's?
Ya usually I let SSRS handle this, but this time I am generating a csv with SSIS using this stored procedure.
Sure, and sometimes the weather is wet and you bring an umbrella.
joe celko does a fantastic job of this. a good book is sql for smarties 
http://amzn.com/0128007613 sorry, should have linked it
You make a good point, but we have to make this assumption as there is no way in our system to tie each call to a sale. I appreciate you spending your time to resolve. I'm trying to think of a select top 1 statement that would apply to each phone number and select the top sorted date diff. I think I can write it for a specific number, but I'm most confused about how to do this for each number. To answer your question, I do have access to the tables. I could set up a temp table or view, but I'd prefer not to edit the raw tables.
My thinking is you may want to add a column of bit type 'Sale' to the communication/call log AND/OR add a unrequired foreign key to the sale primary key. Without directly linking tables it is all just an estimate no matter what you do. EDIT: Can you also give some columns? Maybe there is another way?
 select, phone, agreement#, min(matchtype), product, min(daystosale) as daystosale from ( select phone, agreement#, 'acs' as matchtype, product, datediff(dd,calldate,convert(datetime,sales_date)) as daystosale from ( select sales.*, convert(smalldatetime,cast(salesdate as varchar(10))) as sales_date from sales WHERE product='xx') a inner join ( select * from calls ) b on left(a.address,15) = left(c_address,15) and left(a.city,10) = left(c_city,10) and a.state = c_state where datediff(dd,dateadded,s_date) between 0 and 30 ) a group by phone, agreement#, product A problem with helping with your query is, I don't know for sure from which table which column is. If you want to minimum sales_date after the calldate, I suggest adding salesdate &gt;= calldate to your a = b join. Maybe you can scrap the datediff after you do that.
Partitioning is only available on sql server enterprise. If you don't have that you could create the new results in a separate table e.g. Results_new. Then once you have the results just call sp_rename and change the results table to results_old and results_new to results basically swapping them over. This will take less than a second to do so the table will only be unavailable for a short amount of time. 
Try this one: DECLARE @cars VARCHAR(512) DECLARE @name VARCHAR(50) SELECT @name = c.C_NAME, @cars = COALESCE(@cars,'') + v.BRAND + ';' FROM Vehicle v inner join Customer c on v.C_ID = c.C_ID WHERE (v.C_ID = 1) SELECT @name as CustomerName, @cars AS Cars
there is some decent 2014 licensing info at http://www.brentozar.com/archive/2014/04/sql-server-2014-licensing-changes/. tldr; you can not license only some cores on the server
Assuming a reasonable upper limit to the number of vehicles/customer, you could do something like this: ;WITH VehiclesNumbered AS ( SELECT C_RN = ROW_NUMBER() OVER (PARTITION BY C_ID ORDER BY V_ID), * FROM Vehicle ) SELECT c.C_ID , c.C_NAME , Brand = v1.Brand + CASE WHEN v2.Brand IS NOT NULL THEN ', ' + v2.Brand ELSE '' END + CASE WHEN v3.Brand IS NOT NULL THEN ', ' + v3.Brand ELSE '' END FROM Customer c INNER JOIN VehiclesNumbered v1 ON v1.C_ID = c.C_ID AND v1.C_RN = 1 LEFT JOIN VehiclesNumbered v2 ON v2.C_ID = c.C_ID AND v2.C_RN = 2 LEFT JOIN VehiclesNumbered v3 ON v3.C_ID = c.C_ID AND v3.C_RN = 3
I see no reason this wont work, but i MUST use Coalesce().
You don't need the STUFF, assuming CONCAT_NULL_YIELDS_NULL is ON. SELECT @Brands = COALESCE(@Brands + ', ', '') + Brand FROM Vehicle WHERE C_ID = @C_ID
You're dropping/(re)creating your temp tables with each iteration of the loop, but doing your Results table insert once at the end. I would think that you would either drop/create the temp tables before the loop so that they contain the results for all the values of @db, or that you would be doing the results insert inside the loop if you want a row for each @db. What's @db doing, anyway? I'm assuming you're updating @query to use @db or something...
Thanks for the link. Answers all my questions. 
Using the GROUP BY may give you undesired results- like the matchtype of the record corresponding to the MIN(daystosale) might not be the same as MIN(matchtype), which would just be the first alphabetical value of all the ones within 30 days or whatever. If you want to just get the first subsequent matching sale, I would consider using OUTER APPLY. Something like SELECT s.*, cc.* FROM sales s OUTER APPLY (SELECT TOP 1 * FROM calls c WHERE LEFT(s.Address, 15) = LEFT(c.c_address, 15) AND LEFT(s.city, 10) = LEFT(c.c_city, 15) AND s.state = c.c_state AND DATEDIFF(dd,dateadded,s_date) BETWEEN 0 AND 30 ORDER BY DATEDIFF(ss,dateadded,s_date)) cc (well, that would get you the first subsequent call for each sale, but I don't know your schema, so you get the idea...)
Formatted for easier reading, also looks like the correct answer and is more flexible to change: SELECT Jeremy_Portal.ACCT_NO AS [Acct No], Jeremy_Portal.COMPANY AS Company, dbo.contact.EMAILADDR1 as [Email Address], dbo.contact.[PRIMARY] as [Primary], ROW_NUMBER() OVER (PARTITION by Jeremy_Portal.ACCT_NO, dbo.contact.[PRIMARY] order by dbo.contact.[PRIMARY]) as [rn] FROM dbo.Jeremy_Portal AS Jeremy_Portal INNER JOIN dbo.contact ON Jeremy_Portal.ACCT_NO = dbo.contact.ACCT_NO WHERE (dbo.contact.EMAILADDR1 &lt;&gt;'') AND [rn] &gt; 1; 
thanks! this worked out great
also tried this and it worked, thanks again!
No, I haven't used this exactly. I would most likely argue for in-memory cache as a first approach for any practical project rather than any db-side caching (ACID being of very little concern and for other reasons).
Partitioning requires Enterprise Edition, so be prepared to open your wallet.
that XML stuff is such a hack... it's not "better" except in microsoft circles maybe you could ask microsoft to implement mysql's GROUP_CONCAT function
Does ALL the data change a lot? Maybe your process should Insert new records, update existing records that changed and delete records that no longer exist instead of truncate and insert.
We just had 4 quotes from different resellers for a 16 core SQL 2014 Enterprise license and it is around 100k. 130K if you want SA.
Generally speaking, compression shifts the performance burden from Disk I/O to CPU power. Systems which are short of Disk I/O benefit from compression; systems which experience performance bottlenecks on CPU power may perform worse with compression. I'm a specialist in Business Intelligence and data warehousing and, as such, Disk I/O is almost always the performance bottleneck. Compression is a blessing for us!
Damn, I thought that was the issue, but couldn't find confirmation. That would make complete sense, though. Damn, office bureaucracy sucks. Now I have e-mails to send... Thanks for the help!
Join Table1 to Table2 by both IDs OR-ed together, group by CurrentID, OldID. If you care about showing zeros for the Table1 records where you have neither ID populated you can either UNION those or switch to left join and check whether the join brought something in the count. Something like this: select t1.currentID, t1.oldID, countRef = case when max(t2.referenceID) is null then 0 else count(*) end from table1 t1 left join table2 t2 on t2.referenceID = t1.CurrentID or t2.referenceID = t1.OldID group by t1.CurrentID, t1.OldID
And the data is compressed in memory--so you get to store more data in memory! Bigger caches!
Your second query isn't correct for what you think it is, DATETIME is just a fancy form of a ~~integer~~ float, if you use **'2013'** it will return the same results. To see what I mean run the following: DECLARE @dtest DATETIME SET @dtest= 2013 SELECT @dtest AS IntegerDateComparisonResult SET @dtest = '2013' SELECT @dtest AS StringDateComparisonResult For you second question, you're close, but not quite right, it should be: WHERE ExpirationDateTime &gt;= DATEADD(MM, -6, GETDATE()) AND ExpirationDateTime &lt;= GETDATE() 
That's so weird. I also tried: SET @dtest = 2013.1 SELECT @dtest AS FloatDateComparisonResult For giggles and got '1905-07-07 02:23:59.997' I think I need to create a new table and fill it with some clear sample data to test my queries now. Thank you, and for the second bit too. Seems obvious now that I wasn't closing off the range I wanted. 
For your second question WHERE DATEPART(MM, ExpirationDateTime) &gt;= DATEPART(MM, DATEADD(MM, -6, GETDATE())) would get you all the records where the month *by itself* &gt;= 6 months ago. I.e., if it's currently July all records would be returned regardless of year, because the month is always &gt;= 1. All you really want is WHERE ExpirationDateTime &gt;= DATEADD(MM, -6, GETDATE()) but that would get you exactly 6 months back (like, right now it would give me everything after 2014-08-12 15:00). If want from the first of the month 6 months back, you'd do WHERE ExpirationDateTime &gt;= DATEADD(MM, DATEDIFF(MM, 0, GETDATE()) - 6, 0)
Perfect. Thanks for the help!
You should be using single quotes all the time, Single quotes are used to indicate the beginning and end of a string in SQL. Double quotes generally aren't used in SQL, but that can vary from database to database. http://sqlfiddle.com/#!3/8fb33/3
 Select * from table Order by user_id, date ?
They seem to all be single quotes, but if that is the case, covert() is going to think it has three parameters. The nvarchar(100), an empty string, then another empty string. (in OPs sproc)
Datetime stores the number of days since 1900-01-01 In your case that is 2013 days, 5.5~ years; which is July, 1905.
You are trying to concatenate row values, I would use a recursive CTE, [here's some examples](https://www.simple-talk.com/sql/t-sql-programming/concatenating-row-values-in-transact-sql/).
Ask your TA.
Online class.... I'm writing to the teacher, too. I just figure more info is better. 
That seems reasonable but I'm not sure how to actually do that :-/
Thanks for the idea, but that wouldn't tell me where the switches happens. 
We used to use FOR XML PATH to concatenate multiple rows into a comma-delimited string. I don't have code but you could probably Google for a quick, elegant solution.
Preferably, I'd like to be creating my temp tables, pulling the data, then dumping the results into the temp tables, and then updating the ResultsTable for each iteration. Did I set up my scope incorrectly?
My work wants my boss and I to train our Dev's weekly for our Dev's so that they can be SQL Queries 70-461 certified and we also get to offer it free to the public. This is a weekly event and if you can make it, the videos and materials will be viewable/downloadable from www.AaronBuma.com **I'm not trying to direct discussions away from reddit, come to the trainings and discuss it here.**
Looks like once I add this to the rest of the script, it takes substantial amount of time to run. Is there another way to run this that wouldn't be as time consuming? The files that I'm working with are very large.
Like, r3pr0b8 said, it depends, ;) What language?
Thanks, I'll check it out!
Don't know - try to split the query to use simplistic joins and use UNION to join pieces together? And see if you're missing an index on a table, maybe.
take the results of your dupes query, and join that back to the table SELECT t.UniqueID , t.ColumnX , t.[DATE] FROM ( SELECT ColumnX FROM db.table WHERE [DATE] &gt; GETDATE()-6 GROUP BY ColumnX HAVING COUNT(*) &gt; 1 ) AS dupes INNER JOIN db.table AS t ON t.ColumnX = dupes.ColumnX 
This is perfect, thanks!
 SELECT MAX(UnitPrice) AS MaxPrice, MIN(UnitPrice) AS MinPrice, AVG(UnitPrice) AS AvgPrice FROM Products;
I like CTEs for this kind of thing. WITH CTE_AGGREGATES AS ( SELECT ProductID , MinPrice = MIN(UnitPrice) , MaxPrice = MAX(UnitPrice) , AvgPrice = AVG(UnitPrice) FROM PRODUCTS GROUP BY ProductID) SELECT * FROM Products T1 JOIN CTE_AGGREGATES T2 ON T1.ProductID = T2.ProductID **Edited to clean up code**
Yep that would do it. I don't know why I was trying to make it so complicated. Thanks!
Is there any advantage to running this code vs u/whodiopolis's code?
Got it, I thought you wanted it for each product, as in, the table contained a history. This is definitely what you want for an aggregate of all products
What do you mean by history? My understanding of CTEs now, after coming across them in my book, is that they speed things up when working with specific data that might cause redundancies? I'm still not sure I FULLY understand or would know when to use them over not using them... This is only my first database class and though its nearly at the end I still feel like a newb. 
What /u/1ddqd suggested would return all your product rows with the set's min, max, and averages, instead of 1 row of aggregates like /u/whodiopolis suggested. They have different purposes.
I thought your product table contained lots of different prices stored over time for each product. Something like ProductID 1 has three different prices (3 rows) and you were looking for the Min, Max, and Avg of those 3 rows. My CTE would show you the Min, Max, and Avg for all prices and all products stored over time. As for understanding CTEs, it's like doing "into #temp" - the main difference being that a # temp take is stored in memory for the duration of your connection to the server. A CTE exists only during the runtime of your query.
(4) is odd. The instructions say to use a subquery, so I guess you have to, but this is more obvious: SELECT CustomerID, ContactName, Country FROM Customers WHERE Country NOT IN ('UK', 'Canada', 'USA', 'Australia') ORDER BY Country;
Not over time, just for separate orders. Nothing too crazy. That makes sense though, thank you. And thank you for responding. So basically it's a pass-by-reference instead of a pass-by-value?
[try here perhaps?] (https://social.msdn.microsoft.com/forums/sqlserver/en-US/7c4e63d9-05d9-45cb-a63e-50817f9d087a/polynomial-regression-algorithm) 
Thanks for showing with table data, couldn't figure out how to do it decently on mobile
&gt; WHERE Country NOT IN (SELECT Country Number 4 is odd, but something tells me they'll be looking for: WHERE CustomerID NOT IN (SELECT CustomerID
Thanks for the link. :) I've already gone through this. Just to clarify: I know how to do a polinomial regression. My problem is, that the steps in order to derive the regression equation are very complex (since you have to work with matrixes). My question was more on the technical side. Are there any algorythms someone of you may have used in the past in order to compute such an equation?
It's not. The first comment in this thread is correct. There is no reason why #4 should use a subquery. Using not in('UK', 'CANADA', 'USA', 'AUSTRALIA') makes way more sense. I would provide both answers with an explanation as to why the simpler one should be used. I get that your professor is trying to drive home the idea of subqueries, but writing code with performance in mind definitely makes a difference! I work with extremely large datasources, and something as simple as removing a data type conversion from varchar to nvarchar has made a query that normally runs for 10 minutes return results in less than 1 second. Subqueries don't always have the largest effect on query performance, but I still would try to find alternatives if possible. 
MSSQL select Leads.barcode_id,Answers.result FROM LEADS JOIN Answers ON Answers.lead_id = Leads.Id WHERE Answers.lead_id = 1; This is basically the end result but instead of getting Barcode_id | answer1 | answer 2 | answer 3 | answer 4 E222384 | Test | Test2 | Test 3 | Test 4 I get Barcode ID | Answer E222384 | Test E222384 | Test 2 E222384 | Test 3 E222384 | Test 4 Do you see what I'm aiming for? 
[I think you want pivot]( http://stackoverflow.com/questions/15745042/efficiently-convert-rows-to-columns-in-sql-server) 
Could you help me understand the purpose of the join condition? the instructor noted that I could join multiple tables using a WHERE clause which is what I have used on this statement and all the others. this is the only one causing problems. When i tried to re format using inner join I just got syntax errors that I couldn't fix. It was my understanding that in the FROM section where I put "lab3.country AS co, lab3.country_language AS cl" i could then reference multiple tables together using commands like co.capital and cl.percentage.
He is. Do yourself a favor and start studying outside material for your class. Google the topics of your lessons to avoid other bad syntax habits that your professor may be instilling in unsuspecting students.
Thanks. I just got in to work and will try this out. 
First, this is a bad data design... but you can use this in MSSQL: Sorry that 1st query was wrong 
Can you tell what is wrong with my query? I am still in learning phase so, query design tips would help me.
This was a part of an exercise from Stanford Online Course [here](https://class.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-exercise-sql_social_query_core/). I just ran the database script they've provided. So I guess they wanted us to perform this operation with text datatype only.
Yup. That did it. Thanks a ton. I was unable to figure out why &lt; was not working for me where it was working for everyone else.
Looks like I'm missing something. I get the error: Msg 207, Level 16, State 1, Line 2 Invalid column name 'Role'.
Nope. It returned every pair, even those without mutual interest. This worked for me. WITH T(C1,G1,C2,G2) AS (select HS.Name as C1, HS.Grade as G1, HS2.Name as C2,HS2.Grade as G2 from Likes L JOIN Likes L2 on L.ID1 = L2.ID2 and L.ID2 = L2.ID1 JOIN HighSchooler HS on HS.ID = L.ID1 JOIN HighSchooler HS2 on HS2.ID = L2.ID1 ) select * from T where CAST(C1 as varchar) &gt; CAST(C2 as varchar)
A better way would be just to use the student id instead WHERE HS.ID &lt; HS2.ID
the spool command is OS agnostic SQL Developer supports the SPOOL command Just use a path that the OS will recognize SPOOL /directory/file.csv select /*csv*/ * from hr.employees; execute with the script button - will write a CSV formatted file to that location
For select SELECT * from ACTIONS a INNER JOIN CONTRACTS c on a.SERIAL = c.SERIAL WHERE a.STARTDATE &lt; '2015-05-01' AND a.EXPORTDATE is null AND c.ENDDATE &lt; '2015-03-01' Update: UPDATE ACTIONS a INNER JOIN CONTRACTS c on a.SERIAL = c.SERIAL SET a.STARTDATE = /*some date value*/ WHERE a.STARTDATE &lt; '2015-05-01' AND a.EXPORTDATE is null AND c.ENDDATE &lt; '2015-03-01' Hope this helps...
Is null
Assuming alternate vendor is not mandatory.... SELECT m.CW_id, m.Description, m.model_number, ma.Manufacturer, pv.Vendor as Prim_Vend, av.Vendor as Alt_Vend FROM materials m INNER JOIN manufacturers ma ON ma.Manuf_ID = m.Manufacturer INNER JOIN vendors pv ON pv.Vendor_ID = m.Primary_Vendor LEFT OUTER JOIN vendors av ON av.Vendor_ID = m.Alternate_Vendor
To clarify this, the ISNULL function replaces the first value with the second value if the first value is NULL. So in the example above, CategoryID is being replaced with '' if it is NULL. Then, if the output equals '' you know that it was either NULL or '' originally.
Is this homework or real life? If real life, then whoever set up these tables did a bad job. What happens if there are more than 2 vendors that could supply a product? A good exercise would be to think up a data structure that would allow multiple vendors and priority for each (primary vendor, alternate 1, etc.)
That's not good long term thinking, though. Sure, right now it may be the case that there are no parts with more than two vendors, but can you guarantee that for the next 5-10 years of the business? Data structures should be created with growth and change in mind. It saves hundreds of man-hours over the years.
My work wants my boss and I to train our Dev's weekly for our Dev's so that they can be SQL Queries 70-461 certified and we also get to offer it free to the public. This is a weekly event and if you can make it, the videos and materials will be viewable/downloadable from www.AaronBuma.com I'm not trying to direct discussions away from reddit, come to the trainings and discuss it here.
I love using IPython notebooks + numpy/scipy + pandas for data exploration and presentation. It's great to be able to take a data set, see it on a scatterplot, compute new columns, run a regression, join in another data set, etc. I can then save it off to a notebook which I can send to other people on my team or just have so I can re-run the numbers next quarter. If you're coming from SQL, it will have a very steep learning curve as there's a lot to learn. I would start by learning Python and build up from there. Python on its own is a useful language to have in your tool belt and a good compliment to SQL. It's great for ETL and data munging.
Yes. See the [pymssql](http://pymssql.org/) library.