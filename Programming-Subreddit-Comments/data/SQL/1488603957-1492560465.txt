Been using SQL for 18 years, never even knew it was possible to use a number.
Or even future you a year from now. I'm ashamed to say how many times I'll think, "who hacked this bs together?" Only to check hx and find it was me. 
Names all day. But I like to alias my tables 
Fair enough, especially if 4 is a cast to a bit using sums with a name or something like that
[removed]
It exists for situations in which you don't know column names ahead of time I guess. 
Incorrect. You can alias an expression and then ORDER BY the alias in SQL Server. I just wish you could use the alias in the GROUP BY clause.
Writing SQL for over 10 years and never knew you could use numbers. **If you're writing SQL that anyone else will ever have to read, use the names.** The only reason I can imagine to use the numbers is if you're going to be changing the order of the columns and always want to use a specific column position to sort by but I've never done that.
Oh yeah, for ad-hoc quickies I've done that. For anything more substantial it's no good though. I guess I get it: it's a convenience thing 
Can you group by project id?
Does the table holding the people indicate whether someone is a PM or assistant? Edit: Assuming that it does *and* you have only one PM and one assistant per project, you can `join` to the team member table twice. select P.ProjectName, TM1.PersonName as PMName, TM2.PersonName as AssistantName from Project as P join TeamMembers as TM1 on P.ProjectID = TM1.ProjectID and TM1.PersonType = 'PM' join TeamMembers as TM2 on P.ProjectID = TM2.ProjectID and TM2.PersonType = 'Assistant';
That would be order by, group by should work just fine for what you're doing. Just make sure you keep the rest of what you're selecting in the aggregate clause as well.
This is the way to go
That works, but maybe outer join in case either is missing. I might tend to use a single join and pivot by decoding the columns, or maybe use subqueries to catch cases where there's no or multiple people per job type in a project. But I might be overthinking it. 
If i am writing an automated process, Merge is my go to as You're able to define your own Key to check for records existing in the target table. MERGE allows you to take INSERT and/or UPDATE based on records existing in the target table but not in the source table, and also on records existing in your source table but not in your target table. However for a simple one-time query, a join or a WHERE NOT EXISTS works just fine.
No worries, I figured as much.
Makes sense. This should be a one-time thing but that's good to know for the future.
Try adding a size def to last col https://www.w3schools.com/sql/sql_check.asp
Show your full `create table` statement.
[removed]
I have not used mysql before, but isn't it "numeric", not "number"? 
Nope the datatype used here is NUMBER which indicates an int or a decimal. This script works fine as it's provided by my trainer. I really think the problem originates from the mysql versions installed. 
`IsReserved IN 'Yes','No'` should be `IsReserved IN ('Yes','No')`
Thanks everyone for catching this. Haha i guess using the mysql jdbc driver is gonna be an issue for me since I'm just learning sql. Had no idea syntax would be slightly different
This worked perfectly! Thanks!
first thing you have to learn is to forget about looping databases actually perform most efficiently when you update all rows at the same time with a single command UPDATE sale SET saledate = saledate + INTERVAL '4' YEAR that's all there is to it
your teacher should be shot
did that it seems as though CHECK doesn't work in access 
I will pass that on lol. Do you have any idea how to do it with the for loop?
that too ;o)
Here is a simple way to get yyyy-mm. We use date format 20 for every output --&gt; yyyy-mm-dd hh:mm:ss `declare @date datetime2(3) = getdate()` `select [date] = convert(char(7),@date,20)` 
while /u/fauxmosexual's insert into will work, I prefer an exists for this sort of situation because it's clearer that you're inserting where something doesn't exist compared to the left join requiring you to know what a left join is and what null keys in that left join means. An unnecessary thing for fellow developers, but extra clear when it comes to anyone who's not familiar with SQL reading it. INSERT INTO table2 ( name , number , title ) SELECT name , number , 'Manager' FROM table1 WHERE NOT EXISTS ( SELECT 1 FROM table2 WHERE table1.name = table2.name AND table1.number = table2.number )
I want to be clear that /u/r3pr0b8 is absolutely spot on. This really should be a very simple update statement. I could maybe understand a loop if it was more complex business logic to update the record. I have no clue why execute immediate would be needed for this. Below is an example to create a dummy table and then a statement to update the salesdate by 48 months. I am also assuming there is some sort of unique id for a row that can be used. -- create a sale table with 10 records -- id is the unique identifier for a sale record create table sale as select level id, --level is a psuedocolumn only available in connect by queries -- it will generate number 1 through 10 sysdate+level saledate from dual -- dual is a single column single row table -- usually used to generate or manipulate data/variables -- there is not any data here that I care about, it is all being created in the select portion of the query connect by level &lt;= 10; -- a little bit of trickery to generate 10 records. begin --loop over every record in the sale table for sale_rec in (select * from sale) loop update sale set saledate = add_months(saledate,12*4) where id = sale_rec.id; -- reference the id of the current record when performing the udpate end loop; end
you could use LAST_INSERT_ID() to get the user_id of the most recent user record and insert the subtype record with that as the ID.
Joining them with a `UNION ALL` should do the trick... edit: not sure if MySQL cares about the semicolons missing... SELECT Name, Type, Subtype, Rarity FROM Eternalcards WHERE Rarity IN ('Legendary' , 'Rare') Order BY RANDOM() Limit 1 UNION ALL SELECT Name, Type, Subtype, Rarity FROM Eternalcards WHERE Eternalcards.Rarity = "Uncommon" Order BY RANDOM() Limit 3 UNION ALL SELECT Name, Type, Subtype, Rarity FROM Eternalcards WHERE Eternalcards.Rarity = "Common" Order BY RANDOM() Limit 8
Assuming I'm understanding what you're trying to do correctly, it would probably be best to consider the task data to be separate from test data. You'd need to figure out the count of the different permutations of user\task and user\test separately, only grab the permutation for each with the highest count and then join them back together via the user column.
You may want to analyze it so that you can eliminate the loop and make it entirely a set based operation. Without more details, its not really possible to determine if this is feasible.
Thank you! I finally got it to work today, using a temp table. I had to use a "CREATE TABLE' statement outside of the loop, and then an "INSERT INTO" in the select clause of the loop, with another "SELECT *" to pull from the temp table.
Why not just insert directly into the original table instead of dealing with additional overhead of other temp tables that become irrelevant once the data from those temp tables are dumped into the main temp table?
That's to balance out the times where you look back at some of your own work and think "OMG that's beautiful!!"
That's basically what I'm doing. But instead of other temp tables, I'm using CTEs to pull from all the different sets I need, total it all by some columns by month, and then, essentially, union all the different months of data together. It might be more complicated than it needs to be, but it works for now.
The recruiter gets paid when you get a job so they will make it happen. Without much professional experience you will still be looking at entry level SQL jobs, but the good news is that you are only entry level for about 18 months. The other good news is that entry level pay in SQL should be pretty good. 
Do you work in SQL? Did you go through a recruiter for your first SQL job?
In a cross join you are matching each record in table A with each record in table B. So if you have 10 records in A and 10 records in B you will get a result set that is 100 records (10 X 10). In a full outer join you are matching based on some condition (e.g. ON user.userID = event.userID). So if you perform a full outer join with 10 distinct users and 10 events, where 5 events have a single known user and 5 events have an unknown user (userID is not present on user), you will get a result set with 19 records (5 for known event and user, 9 users without events, and 5 events without users). I hope that makes sense!
An outer join is a join that still has a relationship, it's just an instruction to include both sides of the relationship. If I had a table of products and product categories and joined them together, the result would depend on what join I chose. ....FROM product INNER JOIN category.... would return records where the was a product and category matching ....FROM product LEFT JOIN category .... would give me all of the products, including ones that didn't have a category - all the fields from the category table would be null where there isn't a match. .... FROM product RIGHT JOIN category .... gives me every category, including those that have no products in the category. This is exactly the same as category LEFT JOIN product .... FROM product FULL OUTER JOIN category .... gives me every product and every category. Where there is a match the row appears just as it would in an inner join. Where there isn't a match, there is still a row but all fields will be null on the unmatched type. You would use this type of join when you needed to include all records whether or not a match exists, but the important difference to a cross join is that there is a relationship (way of matching) between records in the two tables. CROSS JOINS are very different in that you use them when there's no relationship between records. They tend to be used as part of more complicated queries to create a base table for other things to be joined on, but to give a simpler example imagine if you sold cars, and you wanted to give your customers a list of all of the possible combinations of car and colour. In that case you would take your cars and cross join them with your colour table, and the result would be one row per colour per car. In the real world you're not likely to use these until you start getting into the tricky stuff.
I just want to work with sql, python, excel. I am having trouble finding work and i dont know why.
What kind of jobs have you been applying for? As I say, data analyst (NOT database analyst, they're generally a bit more different and require more indepth understanding of RDBMSes) positions, especially ones that require some financial experience, would snap those skill sets up. And are you getting interviews, or not even that far? If you're not getting interviews it's probably your resume/covering letters that need fixing.
I have had an interview for a database systems analyst that uses only sql. I have had an interview for an api dev role in .net. most financial positions that use sql require 5-10 years experience. Employers seem to be asking too much out of us recent graduates.
I didn't realise you were a recent graduate, I had thought from your financial experience you'd been in the workforce for awhile. It's hard getting your first real gig after college, there's generally pretty stiff competition for positions that are willing to take a chance on a new grad without much work history. I wish I could give you advice, but all I can suggest is to swallow your pride and your salary expectations. [I just had a google around and imo maybe what you should be aiming for is something like this](http://job-openings.monster.com/monster/116de846-1806-462c-9567-82ec912fb0fc?mescoid=1300087001001&amp;jobPosition=19) or maybe [this](http://job-openings.monster.com/monster/dbd1bb20-c5f4-43d8-becc-5a3080fe510c?mescoid=1300087001001&amp;jobPosition=5) - something that you can do for a year or two just to get something decent sounding on your resume and find ways you can use your SQL/python skills to improve processes and get some practice. I'm not from the US but I would guess that 55k is aiming a bit high as a new grad - might be wrong there though.
&gt;Insert into Table_B (col1, col2, col3, col4, col5) &gt;Select col1, col2, col3, @val4, @val5 from Table_A where col1=@val6
thanks but I don't understand what you mean or what I need to change
This worked perfectly. Thanks. Yes they all count as the same. But i know how to handle that. I Didnt realize that you could triple. IE (PARTITION BY user, task, test). 
there are 2 ways to write data set expressions: 1) the old syntax that implies a cross join by default and 'where' clause is used to put conditions on the resulting set or indicate outer join requirements: "FROM " &lt;data set1&gt;[, &lt;data set2&gt;....] "WHERE " &lt;conditions&gt; 2) the newer ansi standard of writing data set expressions (aka joins): &lt;data source1&gt; &lt;JOIN operation&gt; &lt;data source2&gt; [ &lt;JOIN conditional&gt;] ... Always use only one style in a statement and generally stay away from the old syntax as it is generally dramatically harder to read/comprehend in the case of outer joins. The error that you're getting could be related to aliases not being recognized properly or column SSBSECT_CRSE_NUMB not being in your table referenced by "A".
I like the answer that is already provided, but here is another way in case anyone finds it useful... SET @fromDate = DATEADD(mm,4,DATEADD(yy,DATEDIFF(yy,0,DATEADD(mm,-4,GETDATE())),0)) This subtracts 4 months from the current date, rounds to the beginning of the year, and adds 4 months back on, resulting in the most recent 5/1.
Thanks, that looks pretty clear too actually. The only part I'm confused about is the "select 1", where did you get the 1 from?
Ok, last question. How are you using table1.name = table2.name in that select statement without a join?
Given the level of effort you put into this request, do you *seriously* believe that anyone is going to be interested in applying any greater level effort to step in and try to help? Suggestion: Provide some background on what you're trying to do. Ask good, clear questions and provide the necessary information for others to assess and respond to those questions.
Okay. I wont worry about the test. Hopefully i will be working soon. Thanks mate.
No probs. Aim low for your first contract. You just need to get SQL on your CV, then keep working up and eventually sooner than you think you will be at market rate. 
What rate would you suggest i go for
No idea in the US. But my first contract was for less than my permie job I left. As long as you can bear it fiscally go for any number. Get three months of SQL on your CV and your options multiply, rinse repeat. YMMV as I can only talk for UK market
How much was your rate in the UK
Do you have a sample email? My guess is that you are using unicode data and the coalation is space sensitive. You could create an array of acceptable characters to filter out the delimiters. I would generally do stuff like this in C# assembly.
I'd start by figuring out exactly what those whitespace characters are by using Notepad++ with full markup shown or something. You can then, hopefully, use REPLACE and CHAR/NCHAR to strip them out. E: Sorry, I read properly. If they're unicode strings then that may be why CHAR didn't work. Or it'd just error idunno.
I'd be tempted to try joining src and dst (LEFT OUTER) to a CTE or temporary table, containing your extension array, and using your SUBSTR as an equijoin predicate, then returning using WHERE (srcJoin.Extn IS NULL AND dstJoin.Extn IS NOT NULL) and the inverse. Would probably end up trying a few methods, then comparing execution plans to see what works - not particularly au fait with the query optimiser.
Why are you not convinced? Your query looks like it will work, but it doesn't look very performance friendly.
Cast your integers to decimals of relevant Precision before you do any operations with them
It looks a lot more performant that joining to lookup tables mapping area codes to states. (edit: not that you were suggesting that in particular.) If this is a common exercise, OP might want to go back to their DBA/modeler/ETL folks and add state mapping to the table, or at least break out area code to its own field. 
you need a way to generate the year/month values that you want for your query there are so many ways to do this my favourite method involves a **numbers table**, whereby you have a WHERE clause simlar to this -- SELECT '2015-01-01' + INTERVAL n MONTH AS thedate , yourtable.cnt FROM numbers LEFT OUTER JOIN yourtable ON yourtable.cnt_date = '2015-01-01' + INTERVAL n MONTH WHERE '2015-01-01' + INTERVAL n MONTH &lt; 2018-01-01' edit: gah, brain fart -- it has to be a LEFT OUTER JOIN, not INNER JOIN
`select concat(100 * cast(count(ft.*) filter (where "Realtor_Sale" = 'Yes') / count(ft.*) as float), '%') from "Amazon".facttable ft left join "Amazon".realtordetails rd on rd."realtorpk" = ft."realtorfk" where ft."Project_ID" = 'LFC' and "Can_Week_End" is null` am I missing something here? It returns a 0%.
I would intoxicate myself, thoroughly, then I'd watch some metal videos on YouTube. Once this has been accomplished I would take a break, not too long, like, 20 minutes. Then I would half ass the query and blame the user. 
There was nothing wrong with your original ROUND, it was just that you were applying it to 84 so it wasn't doing anything. Wrap it back up with ROUND(&lt;yourCol&gt;,1). E: As an addendum, concatenating a numeric and a string (I'm not even sure what concatenating two numerics would do) would cause an error - except the DBMS is implicitly casting the numeric as a string. Data types are important :-)
Yes, this is exactly what you need except for one thing. Wrap the two sides of your OR in another set of paranthesis so that the AND is applied in both cases.
pgAdmin
I don't know that partitioning (in a query) does what you think it does. It's for defining the scoping of windowing functions. You'd use it to, for example, generate row numbers per month/product/whatever. 
Using it right now. Very impressed. Thank you again!
I was thinking of the function in the where clause, year(calldate)=2016, that should be replaced with calldate between '2016-01-01' and '2017-01-01' as there probably is some sort of index on that column.
Well step 1 is now complete.
I am the DBA lol, I hadn't considered this. I only have to run this report once a year for the FCC.
I get a "PL/SQL: SQL Statement Ignored" "PL/SQL: ORA-00984: column not allowed here" I'm not entirely sure what you mean by your second point. So you think I should have: INSERT INTO sales_history(CustomerID, OldTotal, NewTotal, TimeOfChange) SELECT SALE.CustomerID, :old.TOTAL, :new.TOTAL, CURRENT_TIMESTAMP ? 
They're pretty good! I found the easiest way to get used to the logic was to relate it back to set theory/Venn diagrams.
That's what I meant, but some googling makes me think I'm wrong. The column not allowed error is saying that you can't use a straight up column in the insert. For each value that isn't a :new or :old, you should select it and store it in a variable. Then you would use the variable name in the values statement. So after your FOR EACH ROW, you would declare your variables and in your BEGIN block you would select and store them. I'm definitely a novice at PL/SQL, but I think this is the correct approach.
Download it from your distro's repo. It's there.
The four I've used have different strengths, which I'll outline: # closed source * If you happen to already have any of the jetbrains IDEs, you probably already have all of the functionality of datagrip built-in. Which I think is the best in terms of efficient/usability in terms of things like auitocomplete and working on a bunch of queries stored in a single text file etc. It's missing a number postgres specific features, but it's good for development, especially if you're already using the IDE for your app code. Otherwise you can use the EAP version for free https://www.jetbrains.com/datagrip/nextversion/ # open source * pgAdmin is the most featureful in terms of postgres specific features, but I thought the interface in both v3 and v4 seemed pretty clunky and/or buggy. I only ever use this for dba/sysadmin type stuff where you need full coverage of all postgres features. * dbeaver - works on lots of DBs, even sqlite. * heidisql - by far the easiest to use, but postgres functionality is very very limited, to the point I'd only use it to run and display queries.
pgadmin3 is in the Postgres apt repo: https://wiki.postgresql.org/wiki/Apt
So your database has a "N/A" value in the database when you have a unknown serial number? I am going to assume the numbers you listed are the row numbers? I would try any shift everything but order by statements to the server rather than the ssrs report (like you count aggregation function). You would put the following in the query: case when serialNum = 'N/A' then 'computer-' + row_number() over(partition by serialNum order by serialNum) else serialNum end AS serialNum_New **E:** Removed isnull in front of serialNum on 3^rd line.
I'm assuming the SSRS report is all based on the values from a table and not a stored procedure. I would create a stored procedure and account for the serial number transformations on that table within the stored procedure. Afterwards, on the SSRS report you would simply point to values to the columns you select from the stored procedure you created.
Thanks. I don't own any JB products, but I was thinking about purchasing a license. I'm gonna play around with pgAdmin since everyone seems to be recommending it. 
Thanks! i just tried doing this in a select statement and got the error "Msg 174, Level 15, State 1, Line 3 The isnull function requires 2 argument(s)." here is what the select statement looked like with my field names in place. select *,case when Serial_Num = 'N/A' then 'computer-' + row_number() over(partition by isnull(serialNum ) order by serialNum) else Serial_Num end AS serialNum_New from *VIEWNAME*
Do you have any other identifying information besides serial number? They can be included in the partition by to reset the numbering so all similar unknown equipments get the same psudo-serial number.
yeah, there is a "Vendor_Item_no" that is our unique number given to all the assets.
looks like i might need to cast something as a diffrent data type. now im getting this Msg 8114, Level 16, State 5, Line 1 Error converting data type nvarchar to bigint.
what do you mean, "connect" two tables? tables are related only by primary and foreign keys that said, you can join on any columns you want, whether or not they are involved in a PK/FK relationship
In very simple terms, the only way to join a table with another table is if there is at least ONE common column between them, correct? So... that is what I meant by "connecting" them. Now, I'm trying to show data that one table has in common with another table, however, I have to do about 4 joins.. which means eyeballing each connection and see what connects with what. I'm certain that this can (and should) be an automated process. It's possible that my understanding is flawed on something here.. so please point it out, if so :)
&gt; In very simple terms, the only way to join a table with another table is if there is at least ONE common table between them, correct? you mean a common column, not a common table doesn't your data model show PKs and FKs?
 SELECT userid , lessonid , videoid , COUNT(DISTINCT CONCAT(scenes,createdat)) AS youneeks FROM daTable GROUP BY userid , lessonid , videoid 
You could write a query against the data dictionary that tells you if the tables have an ancestor/descendant relationship. And if not, it could check to see if they have any ancestors in common or any descendants in common (as would be the case in a many-to-many relationship). On my phone at the moment, but when I get back I can help you write this.
OKC is a great place to find work with those skills. There are a lot of oil and gas shops looking for people who can build reports and with a finance background it will convert fairly easy to at least some of the oil and gas industry terms and practices. I am not the hiring manager but if you are serious I can push your resume forward. There is little chance they will pay a relocation but the salary my offset.
Okay. Can you PM me your contact email?
It may not be what you're wanting, but there are many extensions/add-ons for things like this. We use SQL prompt by redgate which is amazing. It has a ton of built-in snippets. If I type gb then hit space, it puts group by and gives me a small pop up box that has all the group by fields I used in the query, or I can stay on the top option and hit tab and it adds them all. 
if it were possible (it isn't) to do a `select * group by * having count(*)&gt;1` and it actually returns something, then this would be clear proof that the table does not have a primary key who builds tables without primary keys?
maybe its not relational.
if it's not relational, is SQL really the language of choice to pull data?
SQL Fiddle seems to have frequent server problems lately. If you're getting "Create script error," that means something is wrong with their site, not necessarily with your SQL. You might try [rextester](http://rextester.com/l/sql_server_online_compiler) instead. You do have an extra close-paren on your create table statement, though.
Hey dude, I'm away from my PC at the moment so below is all off memory, but try this and see if this gives you what you want - let me know if it's still giving you and error message and I'll look when I'm home :-) CREATE OR REPLACE TRIGGER salesdiscounthistory_trig AFTER UPDATE ON SALE REFERENCING OLD AS old, NEW AS old FOR EACH ROW DECLARE CustID sale.customerid%type; CurTimeDate varchar(50); BEGIN SELECT customerid INTO CustID FROM sale WHERE customerid = :new.customerid; SELECT current_timestamp INTO CurTimeDate FROM dual; INSERT INTO sales_history (customerid, oldtotal, newtotal, timeofchange) VALUES (CustID, :old.total, :new.total, CurTimeDate) END; END salesdiscounthistory_trig; /
You can do this using dynamic sql, if you don't want to type out the column names. In SQL Server you could do the following: SELECT t.name AS TableName, STUFF( (SELECT ',' + c.name FROM sys.columns c WHERE c.object_id = t.object_id FOR XML PATH('') ),1,1,'') AS ColumnList, STUFF( (SELECT ',' + t.name+ '.'+ c.name FROM sys.columns c WHERE c.object_id = t.object_id FOR XML PATH('') ),1,1,'') AS AliasColumnList FROM sys.tables t
Is this for one of those google forms? [You could use google's api to pull the data](https://developers.google.com/sheets/api/quickstart/dotnet) and upload to your database. Alternativly if your spreadsheet is small enough, you could query directly in the program. In addition, you may also sync the spreadsheet to your local hard drive and parse those files as needed.
I may not have been very descriptive, sorry. I have a google spreadsheet that my sales team uses to keep track of leads every day. This sheet has say.. 7 columns - email, name, company, job title, city, phone1, status. it grows and changes everyday. (but the format stays the same) I currently have to download this sheet and upload into sql everyday to add to my master list. Then in SQL I query as needed (maybe to pull everyone in Texas to send them a specialty newsletter, or I grab all the CEO's and send them a special offer. Instead of uploading the csv everymorning with new contacts, I would like to just reference the google spreadsheet I am downloading(in SQL), and have that as a table, and run the query... does that make some sense? Ha, sorry if I am poorly describing the goal here. 
Thank you - What about other html tables on the web? -- IE- ESPN NFL stats? can I call upon those tables in SQL, from a query.. or is this just day dreaming and too simplistic to work ... haha Thanks again. 
You can do that with Google bigquery. You just set up the spreadsheet as an external table, then you can query it. See https://cloud.google.com/blog/big-data/2016/05/bigquery-integrates-with-google-drive
It depends a lot on exactly what you need. Amazon AWS free tier includes DB hosting: https://aws.amazon.com/rds/free/ Azure pricing starts at $5/mo, or you can get a one month free trial: https://azure.microsoft.com/en-us/offers/ms-azr-0044p/
Spinning up an Amazon / Azure DB instance would probably be the easiest, not sure what the free / discounts are for students (using a .edu domain). I heard https://db4free.net/ was pretty good if you're looking for MySQL. They have a limit of 200MB/database (you can exceed it, but it'll get cleared out at random intervals).
Can you use Python? https://github.com/burnash/gspread/blob/master/README.md
Or even this http://ramblings.mcpher.com/Home/excelquirks/get-data-from-google-docs
It isn't a cache exactly, but it can be indexed - and when that happens, an index is created and updated when the underlying data is updated, which allows for faster access. If it isn't indexed or [otherwise materialised](https://en.wikipedia.org/wiki/Materialized_view), no caching goes on, and when the view is queried the query that makes up the view is simply called - the optimiser will treat the overall statement as one big statement with the view as a subquery. They're most useful in simplifying development and granting access to users. If you have users who need to query the database, it can simplify things by having them access views instead of the base tables. That way if the base tables change, the view can be altered to allow for the change without the end user having to alter their applications. In data warehousing scenarios it is common to pre-digest data into views so it can be used by people without an understanding of data warehousing design. And it can be very handy for managing permissions: rather than granting a user full access to the base tables, they can be restricted to views instead, which is generally easier than maintaining row-level permissions. Incidentally, it's not necessarily a subset of a table, it can quite often be a merge of several tables. Your definition doesn't really fit: often, normalising a database results in *more* complicated sets of relationships than what you started with.
My query: select concat(100 * round(cast(count(ft.*) filter (where "Realtor_Sale" = 'Yes') as numeric),1) /round(cast(count(ft.*) as numeric),1), '%') I am doing everything you have kindly suggested and it still returns 84.75....%. 
Actually think about what's happening. At what point in the calculation do you want to apply the rounding?
This is by far the cleanest solution. 
remove the erroneous comma in front of the FROM clause
This depends. You have views which are physical objects stored inside the database but they don't actually contain static data. They simply hold the sql statement which in turn queries the data when the view is called. Lots of people use views for security. You can fine grain who can see what w/ your data. One key note: Views do not support input parameters, whereas inline table-valued functions do. It's important to also understand the difference between views and indexed views: https://www.simple-talk.com/sql/learn-sql-server/sql-server-indexed-views-the-basics/
Well I'm invested now. Please update us with what the real problem is. Sometimes I get error messages on a line that is affected by an error somewhere else. And good luck!
&gt; Parse error: syntax error, unexpected (T_ENCAPSED_AND_WHITESPACE) Yeah this is a PHP error, not a SQL error. StackOverflow link [here](http://stackoverflow.com/questions/17868387/parse-error-syntax-error-unexpected-t-encapsed-and-whitespace), looks like you probably have an extra double quote somewhere. 
Huh. Odd. Try standardizing, and using single quotes for both, like so: function main() { if(isset($_REQUEST['a'])) jump($_REQUEST['a']); } --edit, see you've got it. What was the fix?
Missing a double quote above the line mentioned. Facepalm. Thanks for the quick response, much respect. :)
No problem! Glad it's all fixed. Good luck!
Sql fiddle is a good option 
amen
You should also be able to run a local SQL Server with the learning database provided by Microsoft called adventureworks 
IMO it's the exact opposite. KISS is the way to go. Overly complex SQL for the sake of it is madness. I deal with other people's SQL a lot and the absolute worst to work with is overly complex stuff written by people who either don't know what they're doing or are attempting to show off. 
Try this. Open Management studio. Click File then Connect Object Explorer. Server Type is Database Engine. Server name is . (aka a full stop). Authentication is Windows Authentication. That should connect you to your local server (i.e. your PC) You can then create a new database or restore a backup. 
Using [wildcards](https://msdn.microsoft.com/en-us/library/ms179884.aspx), you can do this to make sure the string starts with a number, which works for the examples you gave. If you have any data that is incorrectly handled by this let me know and I'll fix it. SELECT foo FROM Table1 WHERE DocType LIKE '[0-9]%'
&gt; KISS is the way to go. Preferable, but not always. For example, WHERE IN statements are much more legible, but EXISTS generally run much faster. Creating temp tables and indexing those tables has also saved me huge amounts of headaches over querying/locking resources. 
Where's the Foreign Key TELLER_ID supposed to be referenced to? It needs a Parent table. (e.g. if the tellers are in a table called TELLER and have the TELLER_ID Primary Key then you need to reference that). This will work if the primary table for the TELLER_ID is TELLER CREATE table Invoices (Invoice_id Varchar2(6) PRIMARY KEY, Purchase_date Date Default sysdate, Teller_ID Varchar2(4) Not Null, Total_Cost number(8,2), CONSTRAINT fk_teller FOREIGN KEY (TELLER_ID) REFERENCES TELLER ) 
Ticking time bomb for next poor bastard is my standard operating procedure.
Came here to write this. If often spend 10 mins solving a problem, the two hours thinking how I could have solved it more elega tly/simply
* First.. WHAT DBMS ARE YOU USING? (Really, it can be very helpful if you always remember to mention it) * Second.. you have an opening parenthesis after CREATE TABLE Invoices and no corresponding close parenthesis at the end of your column list. * And third, as /u/KING5TON points out, you're trying to define something as a foreign key without saying what table it comes from. You can't link two tables with a foreign key constraint unless you mention BOTH sides of that equation. My preference is when parenthesis are going to be far apart, always to line up my parenthesis and indent them like I would code in order to keep track of how deep I am. I'm not sure about the syntax of the constraints you are using because I don't know what DBMS you are using. I find it odd that the keyword is constraints plural on the primary key and constraint singular on the foreign key. CREATE TABLE Invoices ( Invoice_id VARCHAR2(6) , Purchase_date DATE DEFAULT SYSDATE , Teller_ID VARCHAR2(4) NOT NULL , Total_Cost NUMBER(8,2) , PRIMARY KEY CONSTRAINTS ON Invoice_ID FOREIGN KEY CONSTRAINT ON Teller_ID REFERENCES Tellers NOT NULL CONSTRAINT ON Teller_ID ) 
&gt; IMO it's the exact opposite. I think OP is taking the piss.
YOu guys must not meme hard enough if you don't think this is hilarious
you can use format( &lt;your int&gt;, '00000#'). Also, you can add 2 instead of 200 and take last 2 characters as-is.
k1 k2.
I prefer to alias according to what role they play in the query. SELECT a.name, kpr.name, kbk.name FROM animals a, keeper kpr, -- primary keeper keeper kbk, -- backup keeper etc. 
I was you. I found a junior database dev job at a big company. Worked hard, and attached myself to the best dev i could find. Attempted to optimize everything i worked on, always asking senior people their opinion when i was done. Learned everything i could there, then moved on to another company. 
Really? I thought the percentage hits in after the division. How can I fix it?
It does, but you're rounding the inputs of the division instead of its results. You should have one ROUND, which is performed after the division.
http://www.studybyyourself.com/seminar/sql/course/?lang=eng teaches SQL concepts through examples. Everything for free, both, course and exercises.
What about CTEs? What about window functions? This is not nearly enlightened enough.
I really felt like I leveled up in SQL when I started writing recursive CTEs with window functions. And when I finally got my head around fucking CLR functions.
Sweet ... I'm a level 2!
I already did it, sorry dude. Turns out, all I needed to do was implement use Visual Studios to make using sql easier. LOL, I got it done pretty quick.
Where's the select? Hans' logic is sound. Perform your operands and then round the result
I didn't know that there was even a debate about the importance of SQL in the data science field. In all the organizations that I have been a part of, sql data munging tasks have been about 60% of my work. You can't really be a data scientist if you can't retrieve and combine data from multiple sources. 
You can use their pokeapi (https://pokeapi.co/) to extract the data and use a database for this. This can help: https://www.periscopedata.com/blog/pokemon.html 
I personally recommend SQL Server 2016 as they have an almost enterprise version for free, check out SQL 2016 development. Postgres and my SQL are free, Sybase, db2, and Oracle also have a free version. My SQL and Postgres are my 2nd and 3rd recommendation and Oracle would be my fourth. A database is a great system to house a pokedex, you will probably do a lot of manual entry though if you don't have coding skills, but you would learn architecture. 
I like it. The code I posted was my quick and dirty investigatory process, but this is more of a universal, long-term strategy. Not to mention I can more-easily add a function to the procedure that imports and parses the raw data. Good stuff!
It's not my code, it's something posted previously in this subreddit but it's very useful when dealing with dirty data. Out of interest they are not 'white space' characters, they are mostly called control characters and each does something different: 00 Null character (this can cause premature varchar string termination) 01 Start of Heading 02 Start of Text 03 End-of-text character 04 End-of-transmission character 05 Enquiry character 06 Acknowledge character 07 Bell character 08 Backspace 09 Horizontal tab 10 Line feed 11 Vertical tab 12 Form feed 13 Carriage return 14 Shift Out 15 Shift In 16 Data Link Escape 17 Device Control 1 18 Device Control 2 19 Device Control 3 20 Device Control 4 21 Negative-acknowledge character 22 Synchronous Idle 23 End of Transmission Block 24 Cancel character 25 End of Medium 26 Substitute character 27 Escape character 28 File Separator 29 Group Separator 30 Record Separator 31 Unit Separator 127 Delete 160 Non-breaking space 
a-Z is not a contiguous range. "not like '%[0-9a-zA-Z]%'" should work for your first condition. The second condition could be done via AND-ing 2 simplier conditions together.
Funny, I looked those up after your post. Good to know they're called 'control characters'. I knew 'whitespace' wasn't right since they don't actually occupy blank space. These days, the hardest part of solving a new problem is knowing what terms you're supposed to be googling.
Are you looking for a front-end too? You could create the database in something like PostgreSQL and then you could hook it up to Ruby or Python or Java.
Your CREATE TABLE does not match the question above it. It has no Student, no Program, and no Date. We also do not know the structure for the tables Spriden and SORLCUR nor their relationship to each other. I am not entirely sure what you want to produce. But it looks like you are trying to find the most recent Program for a Student. In other words, given a Student (Sam) find the Program with the most recent Date. Yes? Or maybe show all Students with their most recent Program for each? 
I see what you mean I have a SORLCUR_ACTIVITY_DATE which holds the date value but has duplicates in it and SORLCUR_CACT_CODE Which has active and inactive values. So basically I want it to create a table and take the DISTINCT from SPRIDEN.PIDM and the most Current SORLCUR_PROGRAM from SORLCUR when the SPRIDEN.PIDM matches the SORLCUR.PIDM. Is that a little better?
I really don't know what do you recommend?
I probably normally would but doing a project through database would disinterest me.
&gt; SELECT * FROM company_profiles WHERE company_name = 'Lorem Ipsum Institute' OR 'Et LLP'; When you build an OR statement, you have to fully specify the condition on the second half, like this: SELECT * FROM company_profiles WHERE company_name = 'Lorem Ipsum Institute' OR company_name = 'Et LLP'; Your second query is correct.
Group by wtf, N[?]
Very strange, I've checked it and it's working for me. Try to search on google: pokeapi and go from there. There are also other poke api's, so when googling, you might find other links even more helpful.
Or Where companyname in ('Lorem ipsum bla bla','ET LLC')
Check out the SQLAlchemy package. It is widely used and well maintained + it supports other SQL dialects as well
/r/businessintelligence, /r/sysadmin, /r/techsupport, /r/erp, /r/tableau, /r/crystalreports, /r/cybersecurity, /r/itsaunixsystem, /r/techsupportmacgyver ...
Aww, thanks folks!
Hahaha, thanks. Kendra headed out on her own - she's blogging at http://LittleKendra.com. (And she's completely awesome, does good training classes too.)
Hey, you can do something like select * from ( select count(*) from....) as a,(select count(*) from...) as b, .... Not 100% sure about syntax, but you should get the idea Edit: typos
Ya, I dig that too man. Again it's more about taking the hard method for some extra marks (affords me mistakes elsewhere in my work ;) I've already worked out my ETL with sql ldr and external tables. Wanted to try hardmode tho. 
I honestly have no idea why I come to this sub to do people's homework for them but anyway: INSERT INTO comments (video, text) SELECT videoid, 'Has not been borrowed in six months' FROM videos WHERE videoid NOT IN (SELECT videoid FROM borrows WHERE dateofcreation &gt;= ADD_MONTHS(sysdate, -6)) Making this work with Oracle syntax has been left as an exercise to the reader.
Hi Snajesz, I see /u/fauxmosexual has provided an answer, but in future I believe people will be more responsive if you show what you've done so far. There's a big difference, even if just perception, in saying "How do I do this?" to "I'm trying to do this; this is what I've done so far and it doesn't work, can you help?"
Thanks /u/fauxmosexual! 
Don't forget to include a link to my comment in your bibliography.
&gt; I might be explaining it incorrectly. yup
Can you explain what you are trying to achieve in more detail? And to clarify: you only have one table in this query?
if you have 2012 running already, you can either detach/attach DB files or backup/restore. Upgrading 2008 instance will (most likely) take more time. 
Are there any online options? I saw some articles referencing an ALTER TABLE INDEXES ON... That seemed to suggest we had some options there. 
Where is this box? If you're constrained and only need the extra space, just plug in an external HD to store your backup data. There's got to be a way to make that work right? 
no sign up require. just captcha 8. Compose a query to find vendors who supply Red Oak with grade C-4. In the result table, show vendor name, material description, grade, and the supplier’s unit price for Read Oak with grade C-4. Sort the result by unit price in ascending order. 9. The management would like to know which order that produced the highest total sales amount (in dollars). In the result table, show the order ID and its corresponding total sales amount as well as the customer name who submitted that order. 10. Suppose that you are a purchasing manager in a manufacturing company. Naturally, you want to know which vendors quoted the best price for each raw material. Compose an SQL statement to show a list of two least expensive vendors (suppliers) for each raw material. In the result table, show Material ID, Material Description, Vendor ID, Vendor Name, and Supplier’s unit price. Sort the result table by Material_ID and unit price in ascending order. Note: if a raw material has only one supplier (vendor), that vendor should also be in the result (output) table [hint: use a correlated subquery].
Ok just would not download on my phone. I will get the db and look at it. Unless it is an odd setup it sounds like a simple select with an ending where clause should suffice. At most a join. I will give you hints and general help but I cannot give you the exact code.
Just do not over think it. Many coders try to make simple things hard or complicated which leads to slow sloppy code
Yeah, to further elaborate, the : character is not needed when creating a table.
Backups are off disk array already. I've looked and the array has nothing but the SQL data on them (I was surprised there wasn't other crud lying around actuality).
I'm investigating that option. It's a legacy platform using a lenovo direct attached sas array (did I say it was a legacy platform yet?). I'm checking to see if we can daisy chain another sas array we have from Dell against it. Your approach sounds like it might help at least and I need to check if theres enough space in the other indexes to see if it'd help. 
If it's MSSQL you can also look at putting in the indexes on another partition if you have another disk available. If its clustered index then you're going to have to find the space for it to reorganise the table. Page compression is probably also out given you don't have enough space to do it.
you're gonna have to spend way longer to ask your question properly, i can't download the database but if you can show snapshots of your data and psot the queries here we will be more willing to help... 
So I have sent you a PM going through the previous code you have as well as a hint to look at the associations or relationships of the Vendor_ID and other tables. You have to delete associations with other tables to get rid of data that would be orphaned upon the deletion of a vendor. The easiest way to look at this is the body. You are a surgeon having to remove an arm. You remove it and what will you do with the hand that is laying on the table? You must get rid of it. Then what about the fingers, they have to go as well. So you have to look at what is being deleted and all associations that are down from it. :) If you do not have the answer by Thursday morning I will help some more, but I do not want to do your work for you. That would not be fair to you if you want to learn.
Show us ten rows or so from each relevant table, and paste the SQL from the query you've built or show some of your working, you can do this by Googling "show sql in access 2013" or whatever version you're using, last time I used Access was 2003 :(
 -- TSQL: Replace GETDATE() with your date column SELECT SUBSTRING(DATENAME(dw,GETDATE()),1,3) Assuming your collation is case-insensitive... otherwise use `UPPER()` around `SUBSTRING()`.
would I not want to do (DATENAME(GETDATE([columnname])),1,3? Also, I am now learning ROWID is a specific oracle function. Would you have any ideas the best way to replace that? Perhaps RANK or something similar?
smellyy, the universal answer to this question is: "optimize so that you make faster your most common operation". Normally, faster equals simpler, but obviously there is not an universal measure of what it is fast or simple. The idea being that if you import once, or it is an operation that you do not care much about its performance, but then you are querying it a lot (something typical for an MySQL system), optimize for fast reads, even if you have to lose some time preprocesing/normalizing/denormalizing. On the other side, if you need to consume data as fast as possible (and you still want/need to use mysql), but your reports are not time-critical you may want to import in a more raw way without complex transformations, and take the performance hit on query. For most relational systems, following the normal forms is the best initial approach- and then denormalize when you can see quick wins. &gt; My goal is to be able to query the data so I can see how much a person has been working If you have that clear from the begining, and you will not use it for anything else, maybe you can have an ideal structure for those queries, and reverse engineer how you can get the tables on that format. &gt; Would it be wise to parse the data and convert each record into 5 new records and include a date field to keep track of the actual date? Based on your comment, I would say yes, that would normalize many of the records and make easier range calculations and sums of items between two dates. something like {code, day, number} if Name is an actual name, put that on a separate table if it is 1:1 with charge codes. If the number of records is two high, you can also create summary tables with totals per week, month, year and per code.
https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all
https://www.w3schools.com/sql/sql_groupby.asp https://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all SELECT PostalCode, Count(*) as CustomersWithSamePostalCode FROM Customers GROUP BY PostalCode ORDER BY Count(*) DESC; 
thanks
What have you tried so far? Which SQL dialect are you using?
There's a link in that stackoverflow post that is definitely an aid to my overall SQL skills and, from what I can make out of it, to this problem so thanks. I'll use it to see if I can work it out on my own, but hopefully someone can aid me if they see this and know what I need.
[Give this a try](https://blog.sqlauthority.com/2010/02/01/sql-server-question-how-to-convert-hex-to-decimal/#comment-59977)
Can you give an example of before and after code?
Average the points column from a subquery where you pull a list of the records you need, order by date desc limit 5. 
That's the gist. You could also use the table to store additional user information if need be. Also, for security you don't want to store the password in plain text. Lastly, be sure to specify case sensitivity on whatever you implement appropriately. 
It's not a super great practice - if I were designing a schema for this, design principles would probably tell me that if all of the spreadsheets were alike in structure, that I should put all of the data into one or a set of tables (depending on the data), with each batch sharing a SpreadsheetID number, and then another table containing a list of SpreadsheetID and metadata (when the import was, who the client was, etc etc). You're quite right in thinking that your colleague's approach is violating a bunch of database design principles. What I meant though in the previous comments, those principles exist for reasons. If your colleague's implementation doesn't need to achieve any of the goals the principles are set out to enable, there may not be a lot of value in doing it the "right" way. If however he is actually maintaining that data store and needs to go back to it to edit it or produce reports from it rather than it just being a throwaway, temporary workspace for the data, you're probably right in thinking that he should do it more properly, *especially* if (s)he is planning to then do some kind of batch cleanup process. That almost certainly would be much more difficult to do with separate tables than it would be with everything set out nicely from the outset. But again, without understanding the structure and lifecycle of the data I don't think anyone can give you a concrete list of reasons and benefits for following established design principles.
Before you build anything that people can access from anywhere that allows them to type in a password watch this: https://www.youtube.com/watch?v=8ZtInClXe1Q
I would write a stored procedure that takes in a UN/PW/IP address and do the following with it: Check to see if there is a matching combo -if so, log it as a success (if you'd like) and return a pass. -If not, log it as an attempt and log the IP of the attempt. --This will allow you to log attempted brute force attempts and possibly lock the user out from the DB side if they try more than "X" times. Doing it in a stored procedure will also ensure that your application has no direct access to the user table and a hacker can't query it directly. You can do this by giving the application user DENY access and explicit EXECUTE on the procedure only. 
Other than doing it individually for each personID (there's in the range of 1,000 I'm querying, from around 60,000 records), I'm not sure how I can limit the records to 5 for each personID and eventID pairing. Here's the full query I have so far, which will obviously not work but give you some sort of idea. The 'date' field I gave in my initial question is actually 'weekend' in reality. SELECT personID, eventID, AVG(totalPoints) AS avgpts MAX(totalPoints) AS maxpts MIN(totalPoints) AS minpts FROM ( SELECT personid, eventID, totalPoints FROM Points WHERE personID IN (&lt;a long list of names&gt;) ORDER BY weekend DESC) LIMIT 5 GROUP BY personID, eventID ORDER BY average DESC
How is it going?
Thanks. I have updated the OP with my oversimplified understanding of the job he's doing. He definitely goes back to edit the tables. And he ends up with tables with different number of columns. IE: Table 1: title column | brand column Blue Adidas Shoes | Adidas Pink Adidas Socks | Adidas Table 2: title column | brand column | color column Nike socks | Nike | unknown Green Nike Hat | Nike | green
Thank you for your insight. I have updated the OP with additional info. He's definitely stuck with a bunch of tables. And they all have different columns.
When I read this originally I missed that you needed the top 5 for each person/event combo. https://www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per-group-in-sql/ This link had a lot of good info. I used a version of the query below to pull the info reliably from a similar table I had in MySQL. In the example, this was not the most elegant solution, though. If you use this one, you should change 2 to 5.. You could use something like this (or something like the other queries that are in the link) as a sub-query and then do the aggregates in an outer query. select type, variety, price from fruits where ( select count(*) from fruits as f where f.type = fruits.type and f.price &lt;= fruits.price ) &lt;= 2; 
And other acronyms, I assume.
Thank you, but I screwed up in the description and added row numbers for the purpose of the example. I don't have actual row numbers in the table. Can I still use the partition function?
We actually have KPI's for our BI team. For example, we track request close rates, time spent on various projects, predictive model accuracy, report utilization, etc. I don't think it's unreasonable to do so. We're big proponents of data-driven decision making, why wouldn't we do the same with our own data? No one's getting in trouble for not meeting quotas, we just use the information to prioritize our efforts and understand if we are being successful or not.
First question: what is the use case for this database? Will the business at large be using it, or is it just something your colleague created to support their work? If your colleague just wants to be able to run analysis on spreadsheet data with SQL, things seem fine as they are. If they wanted to automatically generate a list of required changes to meet best practice, that'd be a bit more of an undertaking. And quite possibly easier not to codify. But something like Mongo or Data Lake would perhaps be better suited.
We have several KPI's that we track. They provide useful feedback and just as importantly, they provide a way to back up our budget requests. There are plenty of useless KPI's out there and some that are more trouble to track than they are worth. What's important is sitting down with both I.T. and operations leadership and determining what is right for your industry and your customers, both internal and external.
/u/ihaxr's solution is *probably* the best, but here's an alternative for you to compare against. select product, price,date from mytable t1 where date = (select max(date) from mytable where product = t1.product) Test with your data/setup to see which one works better.
oh bollocks. How could I've missed that. and it looks quite garbled because I'm worn out... still, I'll improve it. figured it out, it was something else. Thanks anyway pal. Any mods, please close this! :)
When it's implemented, yes, but I think with Crow's foot ERDs only the relational concept of an M:M is shown, using the appropriate symbols.
Basically, the table is simply the school names in column one, and the total number of students started at the school over the time period. So I'm trying to remove duplicate values from the set, while also keeping the table as a count of students, rather than including each unique student in the table. The project was given to me to try and cut down on the size of the existing file, so adding anymore data than how I have structured it kind of defeats the purpose. That's why I wasn't sure if there's a way to add to the WHERE clause in order to remove duplication.
Try Select Min(sessionid) as sessionid , userid From Tablename Group by userid I think this should give you what you're looking for. 
&gt; Also you could do: Select Min(sessionid) as sessionid , userid , count(\*) as sessioncount From Tablename Group by userid Having count(\*) &gt; 1 FTFY
Formatting your SQL doesn't just make it 'pretty', it forces you to look at what you've written in a way that's more likely to bring syntax and "I'm too damn tired to be coding, but I have to get this done" errors. I'm glad to read that you solved it because what you've written here is clearly missing things like conditional sums and a group by condition. 
Uh, I see people going out of their way to make this harder than it needs to be. DELETE Session FROM (SELECT ROW_NUMBER() OVER (PARTITION BY UserID ORDER BY SessionID DESC) AS SessionNumber FROM Sessions) Session WHERE SessionNumber &gt; 1
But what's the true difference? I mean, you either search the business value, or the surrogate key (after the join). So you'd be searching the same thing regardless of the content.
I figured it out. SELECT * FROM mytable WHERE DATE(INSERT(INSERT(DIGITS(EFFECTIVE_DATE),5,0,'-'),8,0,'-') = (DATE '1900-01-01' + (DAYS_SINCE_1900 DAYS))
It seems to me that this was modeled incorrectly from the beginning. I see FILM_CATEGORY to be a parent table to FILM. So as the child table, FILM should have a foreign key to FILM_CATEGORY. The above statement assumes that a film can be in one (and only one) film category. If a film could be in more than one category, then you should bring back the many-to-many relationship. Database relationships represent, in part, business rules. So in order to model this correctly I need to know what the business rule is. It is not just about performance. After the correct relationship has been established, then the decision can be made on surrogate vs natural keys. 
The Car table should have BrandID from the Brand table, not a text field. What is customer_information in the Order table, and would it be better suited to the Customer table? List_of_Cars.... implies you're going to try and stick every purchase into a single field. It would usually be better to replace that with car_id, and have one record in the order table for each purchase. Alternately, if you wanted to have single orders with multiple cars, lose that record entirely and add another table - OrderCar, with the orderID and CarID - each order will appear once in Order, and have one record for each car in that order in OrderCar. Consider approaching your lecturer/tutor for assistance with homework.
&gt; But I don't see how you can do that unless done using quires? Well yes, that is kind of the whole point of using a relational database....
Thank you guys for clearing it up for me!
What OP does not realize is that he is changing the normalization, not just changing the surrogate key to a natural key. In so doing, he introduces data integrity problems. Where is the list of allowable categories if CATEGORY is a child of FILM? It disappears. His suggested solution is wrong.
I might do this for a reporting database but not for an OLTP. You're asking for trouble denormalizing like that.
yes you should the purpose of the FK is to ensure data integrity 1. you will not be able to add a project forecast for a project that doesn't exist 2. if you delete a project, you can either block the deletion if forecasts exist, or delete the forecasts as well, or set the forecast's FK to NULL -- this option determined when you declare the FK
As he says near the end, you can still create a categories table, and enforce correctness with a fk. That's what I do in this situation
FKs do not automatically populate, they simply check values to make sure they're allowed
You could also cast the new date to the decimal YYYYMMDD representation: `YEAR(dateField) * 10000 + MONTH (dateField) * 100 + DAY(dateField)` and compare them.
Love the username
Natural keys and normalisation work well together, so the essential message of the article is not to denormalise but to avoid surrogate keys from time to time. And even in OLTP, you should always consider the option of denormalising *some areas* depending on your use-cases, and your databases' capabilities. For instance, consider the link at the bottom of the article, which shows that an array or jsonb based solution drastically beats a normalised (surrogate *and* natural key) one when using a tagging system: http://www.databasesoup.com/2015/01/tag-all-things.html The article says: Surrogate keys + normalisation are perfect defaults. But from time to time, use natural keys and/or denormalised schemas.
What are you talking about?
Have you tried youtube? https://www.google.com.au/webhp?&amp;ie=UTF-8#q=youtube+sql+joins&amp;*
Quick explanation: write two select statements for each table. And run them both. Select * from table1 Select * from table2
Select TOP 1* would be more advisable. Alternatively, if you're using SQL Server Management Studio, type the table name in the query pane, highlight it, and hit Alt+F1. 
But joins are the bread and butter of RDBMS's! 
When I'm trying to understand common columns between two tables that I can use to join I will start by looking to see if any foreign keys exists between the tables. Alternatively, depending on who designed the database it can be common to name the columns to indicated a relationship. If I have a table called Person which contains a unique column called Id and another table called Order with a column called PersonId I will start with an assumption that I can join the two tables. This is much easier if you have a know set of results from a report. This method will not work in all cases but if you have a good understanding of the database design. The best visual explanation of the various joins: http://m.imgur.com/i2caFd1
90% of the time, I use surrogate keys. They are fast, consistent and easy. But... There are situations where it is not necessary or even good. The category example by the author is not very good, because category have a chance to be renamed and if the master table is big, you will waste a lot of space. A better example would be a tagging table. I just did this the other day. I wanted to add a tagging to records, and first I did use the obvious [item]&lt;-&gt;[item_tag]&lt;-&gt;[tag] design. But then tags are short, and unlikely to change, so I just removed the join table. 
Everything is a tradeoff. You seem to be trading worse performance in x% of the use-cases (querying) for better performance in y% of the use-cases (updating). What happens more frequently? Probably querying. Just like in your tagging example (there's a link at the bottom of the article about a tagging example)
Yes. Another method would just to put the tags into a field in the table. Then you have to figure out which one is more expensive when querying, a join or like '%%'.
Basic data modeling. The fact that this does not make sense to you is revealing. In your example, after you get rid of the many-to-many bridge table "film_category" the resulting relationship between "film" and "category" is upside-down. Instead of Category having a foreign key to Film, it should be modeled such that Film has a foreign key to Category. (This can only be the case if we limit films to a single category.) Films have categories as a basic attribute. The allowable values for that attribute should be in a table of categories. 
That's the point. Normalization and natural keys are separate concepts. The change that resulted in one less join was due to denormalization. It would be better to use a different example that focused only on natural keys to drive the point home. By using a natural key, you can search using the actual business value without having to join to the lookup table, but that does not mean the lookup table can be removed. It still has a purpose there to enforce data integrity. But in your example you not only switched to using natural keys, you also changed relationship.
It's not about surrogate keys being better. The point is that the category table is gone now. There needs to be a table with one row per category to enforce data integrity. The allowable values for category are enforced via foreign key constraint from the film_category table to the category table. Or, if a film can only be in one category, it should be enforced via a foreign key constraint from the film table to the category table directly (without the film_category many-to-many bridge table). 
One major thing you seem to be unaware of is being familiar with the tables. You should study the tables a bit more. To do joins requires knowing the relationship/schema of the tables. Also ON. Learn the importance of ON when doing a JOIN.
Thanks. You gained a lot of respect from me based on how you replied to these comments. You know how it can be on reddit sometimes.
Looking at your history, you've asked about MicroStrategy before so at the very least you've seen it. It seems like you're in a search of a tool to do a job, right? What is the specific business problem that you cannot make MSTR or Teradata Studio (I have not seen this one) solve/address?
Thanks so much! Despite using 2012, it spit back: "'CONCAT' is not a recognized built-in function name." But the second method, using 'CONVERT' worked perfectly. Thank you! 
Not at all, you can ask here or shoot me a PM, whatever you prefer. Just keep in mind I'm not on reddit 24/7!
It still joins to FILM where it is timed over 100 runs in a loop.
This appears to be working, however, for the values I need to add the entire list of ItemNumbers to be converted to Series. It seems merely adding a subquery does not work there. -------- Example: I'd like to pull all of the ItemNumbers and convert them to Series VALUES ((select distinct itemnumbers from itemlist where itemnumber like '%:%') ) 
The table I declared and the insert statements were strictly to create an entire example for you that you could copy and paste into SQL Server and hit F5 to see the ideology in place. For solving your issue, you will have no declare table and no insert statements. The gist of what I'm trying to get across is that the below code is really all you need. &gt;LEFT(test , CHARINDEX(':' , test , 1) - 1) AS series I don't know your schema, or all the columns you need returned, or the table name. Based on your last comment though, this is what I'm assuming will work. SELECT itemnumbers ,LEFT(itemnumbers , CHARINDEX(':' , itemnumbers , 1) - 1) AS series from itemlist
Works! THanks!
Thanks for this. There are a couple things I don't follow, but I'll look deeper over the weekend. I'm sure it's helpful. I'm also going to take a good look at cross-joins and fully understand those before going much further. This is great! 
It looks like it's trying to set the name of the result of `SUBSTR(IDNO,3,4) / 3` to "MOD". Maybe use some parenthesis and keep it all on one line? SELECT (SUBSTR(IDNO,3,4) / 3 MOD 2) FROM EMP;
Hm, when I tried that I got an error- SQL&gt; SELECT (SUBSTR(IDNO,3,4) / '3' MOD 2) 2 FROM EMP; SELECT (SUBSTR(IDNO,3,4) / '3' MOD 2) * ERROR at line 1: ORA-00907: missing right parenthesis EDIT: Also tried with an extra right parenthesis and putting it all on one line 
I use sql assistant and have no issues with it.
Thank you so much for your help. That really helps me understand something for once. As far as the last name and length issue, my list consists of names separated by '/' and 'spaces'. So my list looks like: SQL&gt; SELECT * FROM EMP; IDNO NAME1 NUM_DEC WHOLE_NUM DATE1 ---- -------------------- ---------- ---------- --------- 1111 Doe/John M 111.11 651 12-JAN-96 2222 Jones/Mary 222.22 651 16-FEB-99 2345 Avery Cross 345.67 980 24-JUN-78 1799 Nathan Drake 777.77 700 17-APR-72 6929 Kenny Park 1200 97 30-NOV-84 8898 Jamie Fantano 1510 19 03-MAR-88 1212 Martin King 900.5 30 09-SEP-69 3239 Gavin Blount 400.4 125 03-FEB-93 1972 Daryl Hopcraft 4000.4 215 07-JUL-90 4244 Klein/Ethan 9999.99 190 19-OCT-88 10 rows selected. So to extract out only the last names I think I have to pipe together multiple substring(instring) functions where I take everything before the "/" to grab the Doe, Jones, Klein. Then Pipe that with another function that takes everything after the first space... but how would I exclude the slash stuff from reentering here? That's what's holding me up. 
Try something like this: select name1, decode(instr(name1,'/'), 0,substr(name1,instr(name1,' ')+1), substr(name1,1,instr(name1,'/')-1) ) last_name from emp; Returns: Name1|last_name :--|:-- Doe/John M|Doe Jones/Mary|Jones Avery Cross|Cross Nathan Drake|Drake Kenny Park|Park Jamie Fantano|Fantano Martin King|King Gavin Blount|Blount Daryl Hopcraft|Hopcraft Klein/Ethan|Klein 
You're looking for univot or a cross joint
Unpivot did the trick thanks. That's what I was missing, I've never had to use this before.
Ah, you dropped a closing parenthesis for the first substring, right after the "+1". It's hard to catch at first. I'm pasting it again all on one line: select name1, decode(instr(name1,'/'),0,substr(name1,instr(name1,' ')+1), substr(name1,1,instr(name1,'/')-1)) last_name from EMP;
Oh sorry about that. Thank you so much man, can't thank you enough, would you be okay with explaining to me what the "0" (Zero) stands for following the decode function?
add an index (i know, i know, write-heavy) on `( uid , id )`
write IO I've got buckets of. Will see if I can get that done and report back
As usual the time part, which is the most important, is understated to as close to believable as possible.
Fascinatingly terrible English. No capitalization. Even the screenshot of his bulk emails has uncapitalized subject lines! There's no paragraphs, either, just a double space after *every* sentence. The author consistently writes "a SQL" as well.
That alone would have been enough to turn me off to their pitch. 
I'm going to have to forward this thread onto my teacher because the book outlined in a similar code that : would be required. This was extremely helpful. Thank you!
&gt;today i'd like to highlight a positive outcome we achieved at fomo with SQL -- **perhaps the simplest skill to learn from my list above.** This is why there are so many bad database designs, and poorly performing solutions with embedded queries. There are so many people who overlook the complexity of databases because basic SQL is easy to learn. 
True, the benchmark contained the table (just fixed it), but the join is eliminated from the plan, so it doesn't matter in this case.
While PowerBI is not free to the general public, Tableau has a Tableau Public version. Practice with that, host some of your own visualizations (each user gets a Tableau Public profile with their visualizations), put it on your resume. SQL Server Developer Edition is now free. Download it at home, and learn SSAS Tabular mode first. It's the easiest to get into. SSIS would be the next easiest. SSAS Multidimensional would be the most difficult of the three. You could host some of those projects on GitHub to show to potential employers. R and RStudio are free to download and use. You can learn it and earn verified certificates for an affordable cost from Coursera and DataCamp. These verified certificates can be added "officially" to your LinkedIn profile. SQL Server 2016 now integrates with R. Combine your SSRS skills and R integration. Python is also very popular for data science now. Again, free to download and use, and there are verified online courses for this.
No data warehouse, no real DBA either. It's a small company so we just kind of figure things out as we go. Usually the couple network guys we have handle stuff like backups, restores, permission groups, etc. Setting up a data warehouse would probably be a good place to start though. What does that consist of on SQL 2012? I've heard 2016 made it a lot easier but we probably won't upgrade to 2016 until at least 2020...
Power BI Desktop is free. Try that out as much as you want.
Congratulations! You win!
Especially when you're working with SSRS, you shouldn't be reporting off your actual transactional databases. You should be pulling the important data out into a data warehouse. It prevents locking issues and reduces the workload on your apps. The tables in your data warehouse should be designed in a way conducive to reporting. Look into common data warehouse strategies such as star and snowflake schemas. If you can convince your company to let you start building one, it would be a good opportunity to learn how to use SSIS. 2012 will work fine. 2016 has a lot of additional tools, but I don't think you'll be using any of them anytime soon (in memory tables, columnstore indexing, R integration).
Sgoolgle and teach yourself on the job. Find projects you're currently working on that you can code in sql and remote in or stay late to build them.
I like Pluralsight. Look up their SQL learning paths. Lynda is pretty good too. Both sites should have free trials.
Optimization question here. His query limits his results in a where clause (hitting the shops table twice). Wouldn't it be more efficient to do another join clause that said "and shop_plan_id = 'blah'"? Genuinely curious. 
Well data warehouse is just a database with specially designed tables. Yes it will take awhile but the effort is worth the result. Do some research into star and snowflake schemas and think about how you can redesign your data to fit the structure. Unfortunately, I don't know of any good guides. There are some good sites that discuss best practices that might be useful. 
Even his SQL is in all lower case. Madman!
The subquery is entirely unnecessary but would likely get optimized away. Also, his query is missing two of the columns in the GROUP BY. It wouldn't even run.
PowerBI Desktop is free to use, and a free account on PowerBI.com allows up to 1GB of storage. I would look into that as much as possible, because it now replaces the need to do any SSIS or SSAS tabular models or cubes because that functionality to build a data model and pull from any source is built right in. Learn DAX and you'll be ahead of the curve and very marketable. 
just curious what does "really good at excel" mean to you? also www.sqlzoo.net is very good with hands on exercises.
you have two issues here first, i'm guessing that a project can have multiple expenditure items, and a project can also have multiple events but if you join all three tables in the same query on project id, you will get **cross join effects** so if project X has 4 expenditure items and 6 events, then your FROM clause will return 24 joined rows second, your subqueries are ~not~ **correlated** which is what they should be if you want to return the counts in the SELECT clause i personally try to avoid correlated subqueries because they can be inefficient try this -- SELECT ppa.segment1 , COALESCE(pei_subquery.pei_count,0) "count of expenditures" , COALESCE(pe_subquery.pe_count,0) "count of events" FROM pa_projects_all ppa LEFT OUTER JOIN ( SELECT project_id , COUNT(*) pei_count FROM pa_expenditure_items_all GROUP BY project_id ) pei_subquery ON pei_subquery.project_id = ppa.project_id LEFT OUTER JOIN ( SELECT project_id , COUNT(*) pe_count FROM pa_events GROUP BY project_id ) pe_subquery ON pe_subquery.project_id = ppa.project_id WHERE ppa.segment1 = '181000' notice i've used LEFT OUTER JOINs in case there are projects without expenditure items or events 
Helps to see the table structures. Where are you currently storing the price? 
That gives me EXACTLY what I need. Unfortunately I am fairly new to SQL so I am not familiar with the COALESCE and SUBQUERY, will have to read up on these to understand their application. Truly appreciate your help with this :) 
Vendor Table ---&lt; VendorWidget Table &gt;-- Widget Table Each VendorWidget row will uniquely relate a single vendor to a single Widget type. You can add Price to this table to declare the price charged by that vendor for that widget type. Each Vendor row will point to three VendorWidget rows (because each vendor sells one of each widget type) Each Widget row will point to three VendorWidget rows (because each widget is sold by all vendors) I expect 3 rows in Vendor, 3 rows in Widget and 9 rows in VendorWidget. Enhancement: If you need a history of prices charged by each vendor for each widget type, you could add a further attribute or two to VendorWidget to identify the "current" row (representing the current price charged by each vendor for each widget type) - a "start date" column might be appropriate. With this in place, there may be more VendorWidget rows present than stated above, but the numbers hold true for the "current" versions of each vendor-widget relationship. HTH
What system are you on?
 SELECT *,ROW_NUMBER() OVER(ORDER BY A.id) AS rownum FROM MyTable A ORDER BY rownum Add a top to that and you can get top 10 or whatever you want. 
No, that's not correct, technically. 'derived table' is a result of any query, including ones that have duplicate and unnamed columns. 'Viewed table' is a derived table with a valid rowtype (all columns have unique names). A query in a view definition, subquery in the from clause and table expressions in cte should be 'viewed tables'. P.S. 'Subquery' is any query used within another query in any context and it could return a derived table that is not a 'viewed table', if used in an 'exists' condition, for example. PPS. literally lol. I was just thinking how much I dislike the confusing 'derived table' nomenclature and I just found that MySQL uses 'derived tables' in the sense of 'viewed tables' (yet another thing that's wrong with MySQL): http://mysqlserverteam.com/derived-tables-in-mysql-5-7/ -------------- &gt; A derived table is a subquery that can take the place of a table in the FROM clause of an SQL statement. A derived table that is embedded in the query is sometimes called an unnamed derived table. It is also referred to as simply a “subquery in FROM clause”. -------------------
2 problems: * You are not using connection pooling properly. How you set that depends on the postgres library you are using for node, but you should set it at max 3 connections in the pool, probably. * Your code is creating a new connection for every row inserted (bad), but doing so asynchronously (good). The effect of this is opening "10 or 100" connections at once, each to insert a row, in parallel. Instead, learn how to insert multiple rows with SQL. How that works for you is again dependent on your postgres client library, but [the underlying SQL will look like this](https://www.postgresql.org/docs/current/static/sql-insert.html) (control-F for "multiple rows" without the quote marks).
I have never heard of a 'viewed table,' and can't find anyone else online who uses that nomenclature to describe anything other than an actual view. Do you have any references? The word 'viewed' is no where on the page you linked.
2008 and 2011 iso/Ansi standard terms, cannot find the drafts for 2016 to see if they still use it. 2008: http://www.wiscorp.com/SQLStandards.html 2011: http://www.jtc1sc32.org/doc/N2151-2200/32N2153T-text_for_ballot-FDIS_9075-1.pdf
I don't see any way to edit my response, so here's a note: do NOT use the definition from the MySQL that I linked previously - as many other things MySQL, it intuitively makes sense but is not standard.
WHERE goes between the FROM and GROUP BY statements
first of all, i think your join is incorrect -- as it stands, it's a **cross join**, where each row of tbl2015property will be paired with every row of tbl2015payroll that's probably what you're thinking of when you say "the WHERE statement should be used" you actually don't need a WHERE clause if you use the "best practice" method of joining, using explicit JOIN syntax for example... SELECT tbl2015property.strstate , SUM(tbl2015payroll.curWages) AS SumOfCurWages , MAX(tbl2015payroll.curWages) AS MaxOfCurWages , MIN(tbl2015payroll.curWages) AS MinOfCurWages FROM tbl2015property INNER JOIN tbl2015payroll ON tbl2015payroll.strstate = tbl2015property.strstate GROUP BY tbl2015property.strstate 
I am trying to do this without the INNER JOIN function, and use the old style of joining. Thank you for the reply, though I am really new to SQL and this is for an assignment. 
&gt; I am trying to do this without the INNER JOIN function, and use the old style of joining. Stop right there. Why do you want to do this? The "old style" of joining has been superseded. A **long** time ago. Use `INNER JOIN`. Please. It's more readable, it's more explicit, and it's harder to screw up and get a cross join. Keep your filtering criteria in the `WHERE` clause, and keep your join criteria in your `FROM`.
I agree with you, and would normally do it this way, but again it is not under my control. I didn't even know there was another way of joining, and it is really throwing me off here. 
Also, in this it keeps asking me to estimate the parameter value for tbl2015payroll.strstate and I can not see why it would, I thought it was defined by the JOIN? 
TOP won't work, that just takes the top X over the whole result set. Wrap it and add where rownum = 1 instead. 
https://s-media-cache-ak0.pinimg.com/736x/52/20/c4/5220c492bc4e1a8b9175aba77ed7d091.jpg
Good call, was thinking from my head, not in front of a terminal
Thank you for this. I am using Sequelize.js for my ORM, and it indeed has an option to set the pooling like this: pool: { max: 3, min: 0, idle: 10000 } So I set it to 3, and now everything seems to be working fine. I was able to insert 100 rows in 2 different tables (200 rows total), and I didn't encounter any connection issue. Thank you!
It would be better to use a stored procedure. getLoctionById(id). Then grant the account you're using to connect to the database execute permission to the stored procedure.
Why not do your second suggestion then just build a procedure that builds a dynamic SQL Query based on your type rather than having a query with every type joined in. 
Gotchya. I don't have all the details clearly but another idea would be storing all of these entities in a single table so you just have one join from notification to entity then have an entity type table that joins to the entity table. 
The simple-talk website is great! The over(partition by column) seems like a cool way to combine a group by statement but still retain the data from the individual rows. 
I made a mistake. The first one is the total length of all songs in seconds. That's why I did the sum. So just focusing on the 4th one. The problem I'm having is that I know what I want it to do. I just don't know how to get there. 
Ok, how would you write the SQL for this? Return a result set of every song and the number of times the song appears on a playlist. If you get that correct, you are 90% there, and I can show you the rest if you want.
There's a data profiling task in SSIS that does pretty much just that. https://msdn.microsoft.com/en-us/library/bb895263.aspx and here's a quick googled how-to: https://www.mssqltips.com/sqlservertip/2854/using-the-data-profiling-sql-server-integration-services-ssis-task/ 
It's not a warehouse if you're not housing your wares.
If you can, segregate all the remote queries out [with OPENQUERY](https://msdn.microsoft.com/en-CA/library/ms188427.aspx). This allows to remote machine to handle all it's own information and only provide your local machine with the results. Done properly a remote machine will only have marginal querying overhead and the queries will run significantly faster; usually.
instead of != use &lt;
I have been messing with OPENQUERY as it seems to be the goto but I still can't get the performance that I need. For example, in one case I have a query that creates a temp table from the output of a stored procedure, then joins that temp table to a bunch of remote tables. The stored procedure also pulls some data from the remote server. I think bouncing all of that data back and forth is the problem but I'm not sure how to segregate it all out into "local" and "remote" when it all seems to be touching the remote server. Do you have any recommendations on how to split the two functions and only bring them together when needed?
This is going to be your best bet. The key here is to make sure your remote dataset though is returning the smallest possible resultset... I've gone as far as needing to build out my OPENQUERY dynamically so I could pass in parameters. See [this](http://stackoverflow.com/questions/3378496/including-parameters-in-openquery) for an example on how to do that. 
I think that is the best way using pure sql
Very helpful. Thank you!
SSMS is essentially the GUI access to SQL Server but you can use other third party tools too. SSIS is compatible with some tools outside of SQL Server, although I do not have experience with using other DBMS. MS SQL 2016 is the latest SQL Version, you no longer get the GUI with the default install of SQL. Likewise the GUI can update independently of the RDMS so it's kinda neat. 
What do you mean? Like things that need to be demonstrated as being configured properly in a NIST or PCI-DSS audit?
If you take a query where all the output columns are named, and put it in the FROM clause of another query inside parentheses and alias it, it is then a subquery. You can then pretend it's just an ordinary table in the rest of your query by referring to it by its alias as if it were a table.
This will depend on the process really. For example dealing with huge appends with lots of lookup joins might be better handled within staging tables and SQL scripts. Bare in mind that everything within a data flow in SSIS is performed in memory. Also try keeping your "steps" simple. You can pass data between steps using a raw data file. This is quite efficient if you have lots of separate steps. So one of your next steps after the csv source might be to create a calculated field "derived column". Maybe you want to use a lookup transformation to decode some keys. Perhaps just a simple ODBC destination to a stage table and then perform an insert into a main table. 
upvoted for correctness, but handling pagination on SQL side is not the best practice. Use caching for that.
Thanks! I had to tweak it a bit. SQL 2008 groaned at setting the value in the DECLARE had to change it to: DECLARE @Start INT; SET @Start = 10; DECLARE @Length INT; SET @Length = 10;
Getting into a mess with this I currently have this but it's not running as intended, it works but if a user attempts to say log in even though they've already logged in it will fill out the logged out field - I'd like that to not insert anything @name nvarchar(100), @SIOType nvarchar(10), @sioro nvarchar(10) AS IF EXISTS (select * from sitestaff where SIOType=@SIOType AND SIODate=convert(date,getdate()) and Name=@name and SignIn is NULL) Begin update sitestaff set SignIn=getdate() where Name=@name and SIOdate = convert(date,getdate()) END ELSE IF EXISTS (select * from sitestaff where SIOType=@SIOType AND SIODate=convert(date,getdate()) and Name=@name and SignOut is NULL) Begin update sitestaff set SignOUT=getdate() where Name=@name and SIOdate = convert(date,getdate()) END --ELSE IF EXISTS (select * from sitestaff where SIOType=@SIOType AND SIODate=convert(date,getdate()) and Name=@name and signin &lt;&gt; NULL and SignOut &lt;&gt; NULL) --Insert into sitestaff --(SIODate, --Name, --SIOType) --VALUES --(convert(date,getdate()), --@name, --@SIOType) ELSE IF @sioro = 'SignIn' BEGIN Insert into sitestaff (SIODate, name, SIOType, SignIn) VALUES (convert(date,getdate()), @name, @SIOType, getdate()) END ELSE IF @sioro = 'SignOut' BEGIN Insert into sitestaff (SIODate, name, SIOType, SignOut) VALUES (convert(date,getdate()), @name, @SIOType, getdate()) END 
Yay actually now I've a bit of peace and quiet - I've fixed it :)
You can have a look at this article on [vertabelo]( http://www.vertabelo.com/blog/technical-articles/group-concaton). In MySQL there a built-in function called `GROUP_CONCAT` which is doing something close to what you wish. In the link I mentioned to you, it is suggested a way to simulate that function in DB2.
 SELECT person.IDA , person.Name , person.Age , colors.Color FROM ( SELECT IDA FROM colors WHERE Color IN ('White','Orange') GROUP BY IDA HAVING COUNT(DISTINCT Color) = 2 ) AS these INNER JOIN person ON person.IDA = these.IDA INNER JOIN colors ON colors.IDA = person.IDA gives all colors for anyone who had both White and Orange if you're not interested in all colors that those people have, then just run the subquery by itself!!
here, i reformatted it for you... should make it a lot easier to spot the error -- INSERT INTO users ( ov-nummer , first-name , insertion , last-name , mail , password , woonplaats , straat , huisnummer , hn-toevoeging , profile-img , sect-id , klas-id , pakket-id , rol-id , birthday ) VALUES ( :ov , :name , :insertion , :lastName , :mail , :password , :woonplaats , :img , :sect-id , :klas-id : :pakket-id , :rol-id , :birthday ) by the way, the "you have an error in your SQL syntax" usually says **near** and then points to the exact character in your SQL string where it barfed
It sounds to me like the Order # + SKU is what uniquely identifies the rows of data in your, I assume, composite table.
Index your dimensions on the business key and join or look up the surrogate key when loading facts.
That's perfect, thank you very much!!
that said, i like /u/-null's MAX approach almost as much as the COUNT()=2 approach
and hey, thanks for the gold, much appreciated ;o)
identifiers (e.g. column and table names) are not allowed to contain special characters if they do, you have to delimit them with backticks so instead of INSERT INTO users ( ov-nummer , first-name , insertion , last-name , mail , ... you need INSERT INTO users ( `ov-nummer` , `first-name` , insertion , `last-name` , mail , ... 
Thanks guys, I think I will be going this route.
I think that I would agree, but in the case of it being an order number (one that is useful and not duplicated) then is that really data? The order number in that case would be only there to serve as a primary key. Right?
Thank again, but that was an example dataset to replicate a much larger database. Actually I need that one person have more than one "White" or "Orange" because they have a different description +-----+------+-----+----+-----+--------+---------+ | IDA | Name | Age | ID | IDA | Color | Desc | +-----+------+-----+----+-----+--------+---------+ | 1 | John | 20 | 1 | 1 | White | Desc 1 | | 1 | John | 20 | 2 | 1 | White | Desc 2 | | 1 | John | 20 | 3 | 1 | Orange | Desc 3 | | 3 | Bob | 20 | 11 | 3 | Orange | Desc 1 | | 3 | Bob | 20 | 12 | 3 | White | Desc 2 | | 1 | John | 20 | 15 | 1 | Orange | Desc 4 | +-----+------+-----+----+-----+--------+---------+
That's.... wild. Much like window functions, I imagine it will be a few years before the feature really 'clicks' with me.
the concept still applies, though can IDA 1 have White with Desc 1 more than once?
Take a look at pg_dump. The docs are really good for it.
I feel like there should be a sticky encouraging people to just abandon throw aways and not delete them. When I was learning SQL I asked plenty of bang head against wall questions. But it also helped me learn to see other people like me struggling to articulate what they wanted to do in the same way I was... Where ever you are [deleted] don't be disheartened never give up
Thanks, I am trying: select $ pg_dump name_of_my_database &gt; db.sql https://www.postgresql.org/docs/9.6/static/app-pgdump.html Error: ERROR: syntax error at or near "$" LINE 1: select $ pg_dump postgres &gt; db.sql How can I fix this? 
 SELECT person.IDA, person.Name, person.Age, colors.Color FROM person JOIN colors ON person.IDA=colors.IDA WHERE person.IDA in (select colors.IDA from colors where Color = 'White') AND person.IDA in (select colors.IDA from colors where Color = 'Orange') just another approach. please tell me if this works i'm curious.
Well, SQL Server stores things in 8k pages on a heap, but that isn't necessarily going to be what your I/O is. It'll depend on your source table size (columns, rows, key, indexing, constraints) as well as the destination is; your machine's limitations; the network settings; and any server level configs. I've been working on a full DB archive / port lately and we've been testing this, as well. If you have access, you can use the performance stats dm, activity monitor, your local machine's task monitor (ctrl alt del graphs), and stats I/O to track some of this. If you're super worried about performance, look into SSIS packages and BCP copy. I'm on mobile, but I can root around for sources if any of that sounds helpful.
pg_dump is a command line utility. You need to run it on the servers command like, not inside a Postgres query.
http://portableapps.com/apps/development/database_browser_portable ^ this does seem to support ODBC and it's portable so you don't need admin rights to install it. Be careful with whatever you're doing though. Your post reads like the dba helped you with some prebuilt dsn's for excel reports but they didn't give you any credentials to query the data on your own via an actual SSMS installation. Just an observation though, I could be wrong. SSMS is free and your IT department probably wouldn't mind installing it for you, so long as it's authorized.
For query 2, you shouldn't need any kind of equation in the parentheses. You're already joining where composer = artist so grouping by artist and aggregating songs would be sufficient. For query 4 try a Count Distinct function For query 5 try grouping and using a Having on a count aggregate. 
Try database.net. You can find it on fishcodelib.com
I'm currently on my phone and can't check, but are you running the latest revision of SSMS? Can swear I saw an option for this, if not, check under the SSMS registry tree. Might be buried a bit; usually has all the stuff there isn't a GUI option for though. HKCU\Software\Microsoft\Microsoft SQL Server\
I am on number 6 now. Got the rest working. Here is what i have SELECT sub.BillingCountry, sub.aid, sub.title FROM(SELECT BillingCountry, Title, album.AlbumId AS aid#Count oftime purchased in that country From album natural join track Natural join invoiceline NATURAL JOIN invoice ORDER BY AlbumId) as sub Order BY sub.aid, sub.billingcountry Brazil 1 For Those About To Rock We Salute You Canada 1 For Those About To Rock We Salute You Canada 1 For Those About To Rock We Salute You Italy 1 For Those About To Rock We Salute You Italy 1 For Those About To Rock We Salute You Italy 1 For Those About To Rock We Salute You Norway 1 For Those About To Rock We Salute You Norway 1 For Those About To Rock We Salute You Norway 1 For Those About To Rock We Salute You Norway 1 For Those About To Rock We Salute You Canada 2 Balls to the Wall Germany 2 Balls to the Wall Brazil 3 Restless and Wild Is returned but I can't figure out how to count based on title and country
Hmm, interesting. Best I can tell it really just gives a simplified way of doing this: https://www.reddit.com/r/PostgreSQL/comments/5z7zc9/using_postgresql_solving_gaps_and_islands_with/dewni4u/
Get rid of the select word
(please note the response below is glib af) 1) Repartition your drive to create a new drive letter 2) install Ssms and set installation path to new drive letter 3)???? 4) Profit Seriously, all this time you have spent presumably looking for other ides could have been spent with a ticket in with IT. Save yourself the headache.
Do you mean psql?
What you would want to do is the insert into select statement like the following INSERT INTO table2 (column1,column2, column3, ...) SELECT column1, column2, column3, ... FROM table1 WHERE condition 
Yes
I mean do NOT type it in psql. Type it in command prompt if you're on Windows, or SSH if you are on Linux.
Awesome - thanks!
From an academic perspective, these tables could be cross joined dynamically to a "real" table to have a similar effect like `WHERE true` or `WHERE false` predicates: So, these queries are the same: SELECT * FROM users, dum SELECT * FROM users WHERE false And so are these: SELECT * FROM users, dee SELECT * FROM users WHERE true Same effect, but different style / syntax.
Yes. I assume they'll have the same attributes, so if they're .csv files you can just append them and if they're in SQL you can query them with UNION. The limited row count would probably only become a problem when you're generating data for related tables, as the foreign keys won't map properly.
It said I was wrong but I got the same output, just used a subquery instead of CTE SELECT * , [Life Expectancy Rank (DESC)] + [Population Rank (DESC)] as [Total Rank] FROM ( SELECT name , RANK() OVER (ORDER BY POPULATION DESC) AS [Population Rank (DESC)] , RANK() OVER (ORDER BY LifeExpectancy DESC) AS [Life Expectancy Rank (DESC)] FROM COUNTRY ) Z ORDER BY [TOTAL RANK] ASC
Oracle makes date handling very easy most of the time. Just subtract the two dates `b.datediscountadded - a.createdate`. Result is a number. You might want to consider using `trunc` on both of the columns to get rid of any time portion.
No. It's exploratory research. Will let you know if I find something exciting.
Any article that explains that data is important to organisations is not worth posting here. Delete
 SELECT [id], [field], CASE WHEN [transaction] &lt;&gt;'sell' THEN [value] WHEN [transaction] = 'sell' THEN [value] - [value] - [value] ELSE NULL END Something like that?
Sorry for the late reply, and thank you for your interest! Looks like it works as well as expected: +-----+------+-----+--------+ | IDA | Name | Age | Color | +-----+------+-----+--------+ | 1 | John | 20 | White | | 1 | John | 20 | White | | 1 | John | 20 | Orange | | 1 | John | 20 | Black | | 3 | Bob | 20 | Orange | | 3 | Bob | 20 | White | | 3 | Bob | 20 | Black | | 3 | Bob | 20 | Yellow | | 1 | John | 20 | Orange | | 3 | Bob | 20 | Black | +-----+------+-----+--------+ 10 rows in set (0.02 sec) 
You can also try using [value] * -1 or even just using -[value] You should also protect yourself against pre-existing negative values CASE WHEN [value] &gt; 0 THEN [value] * (-1) ELSE [value]
where not exists
This is my current code: SELECT table1.WORKFLOW, COUNT(DISTINCT table1.ACCTNUM) FROM FAKE_TBL AS table1 JOIN FAKE_TBL AS table2 ON table1.WORKFLOW = table2.WORKFLOW AND table1.ACCTNUM &lt;&gt; table2.ACCTNUM WHERE table1.FILE_DT = '03/20/2017' AND table2.FILE_DT = '03/13/2017' GROUP BY table1.WORKFLOW
SQL Management Studio has Query Shortcuts built into it under Tools&gt;Options&gt;Environment&gt;Keyboard&gt;Query Shortcuts. I use this frequently for sp_help (Alt+F1). https://social.technet.microsoft.com/wiki/contents/articles/3178.how-to-create-query-shortcuts-in-sql-server-management-studio.aspx
First, I'd stop using the old join syntax and use the ANSI join. It makes it easier to read by separating the join logic from the where clause. So instead of : SELECT * FROM Table1 A, Table2 B WHERE A.ID = B.ID AND A.FILTER = 'X'; You use: SELECT * FROM Table1 A JOIN Table2 B ON A.ID = B.ID WHERE A.FILTER = 'X'; Now on to your problem. Using a subquery for your IN clause tends to have really poor performance. You should probably be looking into using a derived table or common table expression. In this case, I used a derived table. SELECT oc.* FROM oilcust oc JOIN delivery d ON oc.custid = d.custid JOIN (SELECT housetypecode , MIN(avgfall) minavgfall FROM housetype GROUP BY housetypecode) h ON oc.housetypecode = h.housetypecode WHERE d.numgal * 2 &gt; h.minavgfall; edit: i give up, not sure how to use spoiler to hide the answer code.
No, this will not work. The data is sequenced for instance: FilmName1: NEW OLD So if a film has at any point had an actor with status new, all data for that film needs to be brought in (in other words all sequences). In the above example that means because FILMNAME1 has a new actor the old actor status will be brought in too. 
thanks. unfortunately this is still not doing what I want. if film contains new actor status should bring in all actor statuses
could you provide me with an example?
Yes. If you provide me with create table and insert statements for the data. Also, the output that you expect from that data.
unfortunately I cannot post too much data on here
Where is the actual SQL in this article? It's boring and trite, frankly.
You can't use the ANSI join? Who says you can't to go the internet for further documentation? A colleague of mine once said: "if it's stupid and it works, it ain't stupid"
also, https://flywaydb.org/ 
Great examples guys! Can someone verify if either of these 2 solutions actually work? 
/u/thats_not_magic can...
In case I didn't explain it well. I have two tables: | prod_name | category_id | | Product A | 1 | | Product B | | | category_id | category_name| | 1 | Category A | I want to pull all items from the product list, but I want to fill in information about certain (not all) products in the prod_list from the category table.. So the output would look like { Product A : Category A, Product B : &lt;null&gt; } 
Partitioned by ID, yes but you're going to get 2 on every row for ID 1 because there are 2 0's with ID 1. What OP is wanting, if I understand correctly, is for sequence to basically be a running count of the 0s by ID, so 1 after the first 0 until the second 0 when it will be 2, etc...
Totally separate things! They compliment each other but totally different. 
That's just not true. Typing ed in windows will open the file in question provided your in the directory of the file. 
Because they do not exist. The current exams are tailored to 2012 /2014 and the 70-461 literally has no difference between 2012 and 2016. Grouping and Windows functions are identical. Stored procedures do not differ and neither do views. It's just marketing and nothing else. Learn the core fundamentals and you will pass any version of the exam. 
What do you mean they don't exist? https://www.microsoft.com/en-us/learning/mcsa-sql-2016-certification.aspx
Any other useful tips you'd like to share?
"We are sorry, the page you requested cannot be found." Focus on T-SQL / not the version of SQL Server.
your xml appears to be not well-formed (&lt;FaultDetails&gt; element)
MSSQL: &gt; SELECT CAST(15.123445564 as decimal(15,3)) &gt; SELECT CONVERT(DECIMAL(15,3),15.123445564)
did u try `round(UnitBalance, 3)`?
Im using a custom DB internally for our company. I have been assigned this task to help out the developers. Im in operations, but don't wanna go run to the DEVs for help every time, so I run to yall.... thanks guys
 SELECT x.ID, SUM(CASE WHEN x.date BETWEEN '6/20/2016' AND '3/22/2017' THEN 1 ELSE 0 END) AS 'Date Count' FROM (SELECT DISTINCT id, date FROM table) x GROUP BY x.id
OP is asking about 70-761, which is 2016. What do you mean when you say that it doesn't exist?
The best thing for you to do is open a ticket to have an ide installed. I can almost guarantee that in an organization with so much beauracracy that you signed an acceptable use policy at some point that said you agree not to do exactly what you're trying to do. 
What is the original data type, if it is already a float type or a numeric type (and you are using SQL Server as stated about, you can use the ROUND function. round(MarketValue,3)
&gt; Try row_number in a CTE or derived table. &gt; WITH recentOrders as &gt; ( &gt; SELECT P.[Order] &gt; , PLS.EnteredDate &gt; , PLS.Price &gt; , PLS.EnteredBy &gt; , ROW_NUMBER() OVER (PARTITION BY P.[Order] ORDER BY PLS.EnteredDate DESC) AS 'Ranked' &gt; FROM #PLog AS P &gt; INNER JOIN #MainPlog AS PLS (NOLOCK) ON P.[Order] = PLS.[Order] &gt; AND P.PLActive &gt;= PLS.EnteredDate &gt; ORDER BY PLS.EnteredDate DESC; &gt; ) &gt; &gt; SELECT Order, EnteredBy &gt; FROM recentOrders &gt; WHERE Ranked = 1; This is close, but I don't think it covers his requirement of &gt; I ultimately need the query to return just [Order] and [Enteredby] user. The criteria is the ***user who first entered the final price***. (final = at most recent [Date]) In this example the desired result would be User 5. I think that as written it only gives the last user to enter any price. Based on OP's example, User4 would be returned for order 1234. The only way I can think to get around this is to use a series of CTE's to break the query down into multiple sub-problems such as Max Entered Date per Order, Final Price Per Order, Use Row_Number to assign ordinal to users who entered the final price ordered by EnteredBy date in ascending order, and then the final non-CTE query would have the Ranked=1 predicate. However, this wouldn't cover the case in OP's example where if 500 was the final price, what user would be returned because all of the three users entered the price at the same minute. Its an edge case, but its still a case. Edit: Corrected the formatting for the butchered quoted code from the reply.
 SELECT column_name.query ('Body/Body/FaultDetails/Body/ClientID') FROM Table SELECT column_name.value('(/Body/Body/FaultDetails/Body/ClientID)[1]', 'varchar(456)') FROM Table The XML included is just an example. I find that any time there's an attribute I cannot get to the child elements.
 select * from CountryLanguage where CountryCode='USA' union all select Code, Language='Thai',IsOfficial='F',Percentage=0.3 from Country where name='United States' I misread the requirements a few time, I hate to admit.
Ahh. That's what I get for not reading the requirements thoroughly.
Just this should work. INSERT INTO users(token) VALUES (:tk) Assuming you're passing in the :tk parameter. You can have something like this though. INSERT INTO users(token) (SELECT token FROM SOME_TABLE WHERE mail LIKE :mail || '%' ) Obviously SOME_TABLE needs changing to a table that has the token field and the mail field. 
I posted a comment going off your work (rather than in the chain), so OP sees it. 
I posted a comment going off your work (rather than in the chain), so OP sees it.
Thanks for the reply! I need to PARTITION BY [Order] as the above returns only one row for the table. A result is needed for each [Order] in the table. 
No worries, here's the fix: WITH CTE_ranked AS ( SELECT b.* , ROW_NUMBER() OVER (PARTITION BY b.[Order] ORDER BY b.[Date] ASC) AS [Rank] FROM #MainPlog a JOIN #MainPlog b ON a.[Order] = b.[Order] AND a.[Price] = b.[Price] WHERE a.[Date] IN ( SELECT MAX([Date]) FROM #MainPlog GROUP BY [Order] ) ) SELECT [Order] , [Date] , [Price] , [EnteredBy] FROM CTE_ranked WHERE [Rank] = 1; Tested with dataset below and looks to be working: if ( object_id('tempdb..#MainPlog') is not null ) drop table #MainPlog; create table #MainPlog ( [Order] int not null , [Date] datetime not null , Price int not null , EnteredBy nvarchar(10) not null ); insert into #MainPlog values ( 1234 , '2016-07-27 15:15:00' , 750 , 'User4' ) , ( 1234 , '2016-07-27 11:43:00' , 750 , 'User4' ) , ( 1234 , '2016-07-27 11:21:00' , 750 , 'User5' ) , ( 1234 , '2016-07-27 10:59:00' , 500 , 'User2' ) , ( 1234 , '2016-07-27 10:59:00' , 500 , 'User3' ) , ( 1234 , '2016-07-27 10:59:00' , 500 , 'User1' ) , ( 3456 , '2016-07-27 11:59:00' , 500 , 'User1' ) , ( 3456 , '2016-07-27 12:59:00' , 750 , 'User2' ) , ( 3456 , '2016-07-27 11:34:00' , 500 , 'User3' ) ;
Can you provide a data sample for all relevant tables?
My sincerest apologies. It seems it is bringing in the correct data. I didn't realize that the sales were as large as the data suggested. BUT, your suggestion (and me querying the data to get you a sample) helped me figure that out.
Try, although this could get expensive pretty fast from a computation perspective: select tickets.name, punches.in_time, punches.out_time, punches.hours , sum( case when tickets.action_timestamp between punches.in_time and punches.out_time then 1 else 0 end ) as count from punches join tickets on true GROUP BY 1,2,3,4 Another option is to create the table you have, then left join the result back to punches on punches.in_time. You are correct that the between clause is limiting the join. Never use a right join :) 
i'm also a novice in microcosmic fluctuators, but you might be looking for "alter table", perharps?
If there is only one record per order then you are probably ok.
Well I'm not that much of a novice that I don't know how to use ALTER to modify a table; I'm looking more for the actual design syntax that would let me do what I want to do with the PositionID column in the table.
**SQL Server** Select ID ,Date From ( Select Row_Number() Over (Partition By ID Order By Date Desc) as Ordering ,ID ,Date From table ) sq Where Ordering = 2 I've just updated the query, as you want the max per ID so I've added a partition by so it'll group on ID and start row numbers at 1 again. 
Just a heads up [retracted because of edit] this will return nothing if there is an ID with only one date, though that may be the desired behavior.
is it withrownum or with rownum also, seeing some other issues in the where clause after
Sorry didn't see comments til just now, had edited before reading. Yeah it return nothing if there's only one date but I guess there could be a valid reason to do that.
So it's hard for us to provide a good answer without understanding your table structures - for example, are any of the tables that you're querying partitioned in any way? One thing I would recommend (assuming you're using Oracle 11g) is getting away from the old Oracle syntax for the sake of organizing your query to be more readable in the future. At least, most analysts that I've worked with would prefer it this way: So in your case: SELECT o.ORDER_ID OrderId ,to_char(h.ACT_TIMESTAMP, 'mm/dd/yyyy hh24:mi:ss') ActionDate ,h.ACTION_TYPE ActionType ,replace(replace(h.MSG_TEXT, chr(10), ' '), chr(13), ' ') MsgText ,h.SHARES_OPEN sharesOpen ,h.SHARES_ACTION shares_action ,h.SEQ SeqId ,h.PARENT_ORDER_ID ParentOrderId FROM ts_order o INNER JOIN ts_order_history h ON o.ORDER_ID = h.ORDER_ID AND h.ACTION_TYPE NOT LIKE 'FIX%' INNER JOIN csm_security s ON o.SEC_ID = s.SEC_ID INNER JOIN csm_security_type t ON s.SEC_TYP_CD = t.SEC_TYP_CD AND t.BLOOMBERG_MKT_SECT = 'Equity' LEFT JOIN ts_fill f ON o.ORDER_ID = f.ORDER_ID AND f.FILL_DATE &gt;= sysdate - 90 AND f.FILL_DATE &lt; sysdate WHERE ( o.TO_TRADER_DATE &gt;= sysdate - 1 AND o.TO_TRADER_DATE &lt; sysdate ) OR f.ORDER_ID IS NOT NULL ORDER BY 2 ,1 ,7; I've had arguments about which is easier to read, but I personally find this much easier to parse and troubleshoot than the Oracle join syntax - your joins to each table are located in the appropriate clauses, so it's easier to spot when you have a bad join.
Okay, I am taking a look at that now and will see if it can do what I am trying. Thanks a ton for the response!
I ran it on a table I made with columns ID and DATE and it does work, only thing I can think with the "where clause" is that the rowNum refers to: ROW_NUMBER() over (partition by ID order by DATE desc ) as 'rowNum'. To make sure that is still in there and this it is labled as 'rowNum' because if you change that you will need to change how it is called in the where clause. If you are still getting errors please send the error and/or select syntax.
Ah, I just realised why he Cartesian join is happening. The brackets around the date filters don't make sense. It should be AND (A OR B) Whereas it's currently.. AND (A) OR B The first date condition is closed before the OR
Thanks for your reply. The link you provided is about extracting the attributes. I need to get the child elements where the parent has attributes.
I'm pretty sure you can work around that with a coalesce.
I agree. As a database purist, one should never delete data, but instead, archive it. Data is knowledge, and knowledge is power. More specific, business intelligence is power. You can use this data to determine different subscription averages over a given period of time, determine what could cause subscription spikes/declines, etc. And what if the user comes back in 30/60/90 days to resubscribe? They would have to recreate an account. I don't suggest deleting this kind of data. If you must, and you're using SQL Server as your backed, look into creating a job that executes a stored procedure to delete data that meets a given criteria. You can set these jobs to run as often at every 5 minutes I believe. Granted, if you run a job that frequently, it could use a lot of server resources, so implement carefully.
Yes, there is a query. Not to sound mean but, if you cannot create that query, you shouldn't be in the database. Find a SQL tutorial before doing anything else. 
&gt; Data is knowledge, and knowledge is power. More specific, business intelligence is power. As my old boss used to say, data is valuable and disk is cheap.
Your design focuses too much on the provided example and only works when there are always the same number of rows per SelectionId (in this case 3). The key to the request is to enforce uniqueness, so unless other specifications are given the best solution will do so without forcing additional restrictions on the data. In that case the solution /u/farhil provided of a unique nonclustered index is most effective.
Depends a little on the data and the tables. If you have a primary key Col1 and many other columns (col2 to col10) this would probable be the most most efficient/ easiest to write: select table1.col1 from table1 join table2 on table1.col1 = table2.col2 and CHECKSUM(table1.col2,....,table1.col10) = CHECKSUM(table2.col2,....,table2.col10) So you would join on the primary key, then a checksum for the rest of the data to make sure there are no differences. If there is no primary key you should just be able to just to a checksum on all columns and join just on that, but I believe it would be a lot less efficient. Did it quickly, but assuming the results of the joined select equals the number of rows then you are all set, otherwise you would probably need to do a full outer join to fine the differences depending on the data/ keys you have. 
Ignoring the complications of duplicate columns with a table, the following query will return all rows that don't match between two tables. It won't be super fast on large datasets, but you're talking about full table comparisons, in which case speed shouldn't really be expected SELECT * FROM @t1 EXCEPT SELECT * FROM @t1 INTERSECT SELECT * FROM @t2
If the 2 tables truly have identical schemas and rowcounts, this will only return rows from table_a that don't have an exact match in table_b SELECT * FROM table_a EXCEPT SELECT * FROM table_b
There is a PK and I've been using a full join in the past to accomplish this. My concern now is that there are over 100 colums on this particular table. 
&gt; speed shouldn't really be expected I'm certain this is a dumb question, but what's the bottleneck generally speaking?
I'm assuming non-indexed fields wouldn't like it?
That makes sense. I work with SQL pretty frequently but am not a developer or DBA by any stretch - just trying to get slightly more technical. Thanks.
Group by all columns, filter those that don't match with HAVING (I'm usually using SUM for this). The pattern is like this. I'm using SUM to see which source was present/missing. SELECT SUM(src), &lt;columns&gt; FROM ( select 1 src, t1.* from t1 UNION ALL select 2 src, t2.* from t2 ) GROUP BY &lt;columns&gt; HAVING SUM(src) &lt;&gt; 3; 
Use SSIS. It's built for doing this.
If you're lucky you have to do 3 clustered index scans with this query. So if you have millions of rows and a hundred columns this might not be in memory and thus has to be read from disk and then the query (which requires this information x3) might also spill onto the tempdb (writing to disk).
Pluralsight.com They have entire tracks dedicated to these exams. I just passed my 70-461 and 70-462 by using the study material there. I'm scheduled for my first Windows Server Cert (70-410 I believe) at the end of April. Will be using Pluralsight to study for that too! :)
Unless you're in Azure and can get on that PolyBase hype, use SSIS.
Thank you used an SSIS package and it worked perfectly.
As a note, if you get the MS Developer access, you get 3 months free of Pluralsight.
TOAD Data Point is my works tool of choice. Allows you to connect to all those databases and has a lot of predictive tools and automation to make everything easier. 
You can look up [Microsoft SQL certifications](https://www.microsoft.com/en-us/learning/mcsa-sql-2016-certification.aspx). Data mining and data analysis are going to be huge. However to be an expert on these, you probably need to go in for some research stuff. IF you wanna work, then I would suggest checking SQL certifications. Specially the following Querying Data with Transact-SQL Implementing a SQL Data Warehouse Developing SQL Data Models Oracle will have something similar. Suggest to check those too.
Go for it. I would say you may or may not need to have schooling (degrees) depending on the employer. I'm hiring for a reporting analyst now and I couldn't care less if you have a degree (or even experience really) I only care that you're analytical and know SQL (whatever flavor it is) I would say very familiar with some of these: T-SQL PL/SQL Crystal Reports (terrible but a lot of employers look for it) SSRS SSIS SSAS Tableau IBM Datastage Having ETL experience for a reporting/analytics job is a huge plus. That let's me know you can take data from cradle to grave. I can give you a CSV or flat file and you can feed it into the DB, write code to parse/validate it and also spit out reports on it. Possible positions: Data &amp; Analytics Analyst Data Analyst Reporting Analyst Data Warehouse Specialist Data integration Specialist ETL Analyst/admin Departments: Purchasing, Marketing, IT, CRM.
Care to elaborate more on SAS? Heard of it. I work as Application Dev (.net/C#). I wanna get a bit more into SQL (warehouse, reporting). I am interested in data analysis
South east US.
I completely disagree, for what it's worth. SAS will continue to decline Folks are learning that it doesn't make sense to spend large sums of money on a product that is worse than free languages/applications. Spend a week learning SQL and get an analyst job. Then learn R or Python (or some other programming language). Edit: Don't pay for classes; you can learn everything for free on sites like code academy, Coursera, YouTube, etc.
I've been in multiple industries and they all have their preferred service. I'm not promoting crystal. I hate it, but it is RDBMS agnostic and gets the job done. A lot of organizations use it for their legacy systems. Sadly, it's more popular than you'd think. Usually when I'd tell a panel I knew crystal during an interview I would see at least one person's eyes light up. More often than not, that's the current "crystal guy" relieved there's a victim he/she can pass the pain on to.
sqlzoo.net is one of the best places to learn SQL (imho), and it's free. An economics degree and a decent grasp on SQL will get you a job as an analyst - then spend your free time learning more about data science and programming. Good, easy skills to pick up: T-SQL (Learn what a Stored Procedure is), Microsoft SSRS, and Excel (Formulas and Pivot Tables)
Thanks!!!
1. DO NOT PAY FOR CLASSES. I repeat Do not pay for classes. 2. GOOGLE and download community version of MySQL and head on to w3schools that has good beginner friendly tutorials on Sql. Play around for 7-10 days through those tutorials and similarly try to execute queries on MySQL. 3. Now that you have some experience with queries go to sql-ex.ru website. Sign up and try solving the problems. If you solve these, trust me you would be 100 miles ahead from your competition. 4. Also you can then download an evaluation copy of Microsoft's SQL server and practice with databases like adventureworks and Northwind. 5. For Data Analysis, you must learn R. Take a look at AnalyticsVidya website that has really (free) cool tracks of learning resources. 6. Head to Kaggle and do data analysis for the Titanic survival competition. 7. I think that will enough for you to get good at Data Analysis. Try to invest at least 2 hours in practicing SQL and R. 
Out of curiosity, if I nail all of this, do you just pay for the opportunity to take the test, pass it and get the cert? 
Yes you can. From where I come, certifications don't matter as long as, you get shit done. Just make sure when you do certification, you are on top of your game. You MUST know inside-out of your certification 
I had zero experience with SQL when I interviewed for my Stats job, now it's basically all I use to pull data. Really wouldn't recommend taking a class on it, since it's fairly easy to learn on your own. If and when you interview, they'll probably hit you with a how would you query the database or pull requested data type question. I can't imagine they'd need an expert in SQL unless they were hiring for DBA. I'd just get familiar with SPSS, SAS, or R.
Well not sure you're going to get "quick help" because then you wouldn't learn anything! I think you are on the right track though and correct about the INNER JOIN being needed so you should work with that. Also, I would pay attention to the requirement "Each employee should only show up once" because I think they are looking for you to show something specifically to handle that as well. You aren't there yet but you are on the right track. Reply back if you make progress or have any more specific questions/ errors.
I figured it out. The xml was using the local-name function.
update, got this: SELECT EmployeeID, First, Last FROM EMPLOYEES INNER JOIN TRANSACTIONS ON EMPLOYEES.EmployeeID = TRANSACTIONS.SoldOrPackagedBy WHERE MemberID = ‘101’ now just have no idea how to make it so each employee only shows up once edit: when i add GROUP BY EmployeeID i get this error Msg 8120, Level 16, State 1, Line 1 Column 'EMPLOYEES.First' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause.
Just as the error says, your select list must also be in the group by clause
Yes, great point!
Out of curiosity, what program did you do and where? 15k seems REALLY cheap.
Does this work? It's hard to say without a sample dataset. SELECT C.Model00 AS Model , A.SMBIOSBIOSVersion00 AS BIOSVersion , COUNT(A.MachineID) AS TotalCount , SUM(CASE WHEN A.SMBIOSBIOSVersion00 = D.BIOSVersion THEN 1 ELSE 0 END) AS MaxVersionCount , SUM(CASE WHEN A.SMBIOSBIOSVersion00 &lt;&gt; D.BIOSVersion THEN 1 ELSE 0 END) AS NotMaxVersionCount FROM [CM_P01].[dbo].[PC_BIOS_DATA] A INNER JOIN [CM_P01].[dbo].[v_R_System] B ON A.MachineID = B.ResourceID INNER JOIN [CM_P01].[dbo].[Computer_System_DATA] C ON A.MachineID = C.MachineID INNER JOIN ( SELECT C.Model00 AS Model , MAX(a.SMBIOSBIOSVersion00) AS BIOSVersion FROM [CM_P01].[dbo].[PC_BIOS_DATA] A INNER JOIN [CM_P01].[dbo].[v_R_System] B ON A.MachineID = B.ResourceID INNER JOIN [CM_P01].[dbo].[Computer_System_DATA] C ON A.MachineID = C.MachineID WHERE A.Manufacturer00 LIKE '%dell%' AND B.operating_system_name_and0 NOT LIKE '%server%' GROUP BY C.Model00 ) D ON D.Model = C.Model00 WHERE A.Manufacturer00 LIKE '%dell%' AND B.operating_system_name_and0 NOT LIKE '%server%' GROUP BY C.Model00 , A.SMBIOSBIOSVersion00
Hey thank you for this reply, I'm a sys admin but a lot of my work is building stored procedures in ssms, turning them into reports in ssrs, or simply adhoc queries for mgmt ( for 4 yrs now), and I've been branching out playing with my own sets (kaggle) with python &amp; pandas lately. Really enjoy the SQL and trying to merge it with a higher level language / trying to find a position that will let me focus on that, rather than wearing many hats. I have a bachelor's in computer information systems so there was a lot of focus on db theory and design. I've been trying to find positions but it always seems like the term "data analyst" is very heavily stats oriented. I'm guessing this term very well may just differ between different companies and their needs? I don't have a degree in stats but I get basic stats concepts and I'm used to aggregating data and using window functions, etc etc. How much would you say you value certs vs actual experience and being able to explain concepts? Thank you for the posting possible positions though! That's really helpful 
Hahahaha yep! I feel your pain.
Ya i'm aware that it's a small database :P Alright then, i guess i'll just send the request to my manager then :S
Hey no worries. I honestly dont really pay attention to certificates when I'm hiring. Not even Microsoft Certifications. They are usually only concept driven and not really practice driven. During my interviews I'll put you right into the lion's den. I'll create a database and objects, each with their own nuance and ask you to either build something like a migration procedure or fix something that's wrong (each scenario is custom for that candidate). If you have 10 certificates but can't solve any of my issues, something is wrong because you don't know how to put it into practice. Your experience would make me want to hire you for: ETL developer or administrator data warehouse analyst/specialist Data Integration Specialist Reporting Administrator System Analyst (with concentration on DB Development) Application Development (with focus in the middle/data tier)
MPA program at New Mexico State. If you land a GA job you get in-state tuition. Full course load (9 credit hours) runs around 2,500 a semester and you get paid around 800ish bi-weekly. Program lasts 2 to 2.5 years depending on what semester you enroll. Edit: Research Methods prof was a beast, but he has since taken a job offer at another Uni. New prof is terrible.
Thank you for your thoughtful reply!
Most welcome. Good luck!
I work in quant finance and we're working on migrating our optimization models from SAS to R and Python right now. Once we do SAS is gone.
I think you might need to give us more information, because it *sounds* like what you're asking for is SELECT Max(cod_day) FROM Ref_day WHERE cod_month = 201607 but if that were the solution surely you would have worked it out yourself. If it was paramaterised, and cod_month could be cast to int, then SELECT Max(cod_day) FROM Ref_day WHERE cod_month = @yourparameter - 1
If you're receiving a parameter of the current month value, why aren't you just using a &lt; instead? SELECT MAX( cod_day) FROM Ref_day WHERE cod_month &lt; :Parameter; That way if you somehow have an entire month without an entry you'd still get a value, whereas if you only go for = &lt;previous month value&gt; you might end up with no row. 
I just wanted to provide another perspective on taking BLOBs out of the database. I prefer to keep files like that in the database, even if it means the database storage needs are higher as a result. For a good discussion on this line of reasoning, look at the responses given by Tom Kyte [on this page](https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:1011065100346196442). When the files are stored in the database, you get all the benefits of data being managed by the DBMS. 
I think you need a computed column that's the max sequence number for that order.. something like: SELECT WorkOrder , Sequence , DateDue , ( SELECT MAX( Sequence) FROM WOTable MaxSeqT WHERE WOTable.WorkOrder = MaxSeqT.WorkOrder ) MaxWOSeq FROM WOTable WHERE &lt;whatever limits you have&gt; ORDER BY 4 DESC, WorkOrder, Sequence
Oh right, very good point. SELECT Max(cod_day) FROM Ref_day WHERE cod_month = CASE right(@yourparameter, 2) WHEN '01' THEN concat(left(@yourparameter, 4) - 1, '12') ELSE @yourparameter - 1 END Again, we're assuming that the contents of cod_month are identically formatted strings that can happily be implicitly cast to ints. 
my last job their ERP system used it, but the newer version of that software got rid of it completely
&gt; you are spending more for your time than it would cost to buy the additional space Reminds me of an old job I had at a small consulting shop. I requested some additional parts for a dev server (RAM maybe, can't remember for sure). My manager said I had to do a formal procurement request document and include at least 3 different quotes. This took two hours for about $70 worth of parts. I also attached a document stating that those two hours could have been billed to a customer for around $180, making the actual budget hit $250.
Right, the only time I have a similar error is when I'm running a query without defining which database to run the query against (which is why I posted code with the use 'WPC-Database' before running your create table for assignment). The only other thing I can think of is that your tables were saved under a different schema than the default dbo and can't be found. You can specify the schema along with the tablename in your query.
I completely agree with his points about backups, recovery, and security. These are both the pros and cons that we experienced during the exchange:: * Our database was able to be shrunk from 1.5TB to 47GB. * Backups that took two days, including the off-site safety copy, now take a couple of hours. * Backup testing, full restores, can be completed in a couple of hours. * Rebuilding indexes took all night, but now only take a little over an hour. * We chose to go with a pathing plan that includes YYYY\\MM\\HH-MM-GUID. Since every file is time related, it makes it very easy to find if we have to look it up manually. * The pointer in the database is actually a fully qualified URL. * We no longer have to worry about a noob running a SELECT * and slowing production to a crawl. Downsides that we have experienced: * Coordinating security with the sysadmins was a pain-point at first. We should have flow-charted the process better before implementation. * Development was not wild about the change because they had to add extra code to pull the employee photos and call recordings into their various applications. * We got turned down for our most recent SAN request because even though the SAN was nearly four years old, it still had plenty of room. * We have to trust the sysadmin team to handle their backups and test them on a regular basis. Final thoughts; It's a double-edged sword when you move from in-db storage to out-of-db storage for BLOBS. While it does make database management much easier, it also opens you up to an additional attack point since you don't have direct control over the SAN and network.
Yep, you nailed it. Was a strange request and took a little bit of time to think about, since I'd normally handle this all within a windowed ranking function and then order by that. This one just needed that extra step. 
Sounds like a good reason to avoid banking and wasting valuable brain space on an deprecated skill.
Can you tell me more about the shape of your data? What are the columns in the base table? What makes it more complex than col1 as Date and col2 as Sales?
How have you got the average at the moment? The answer will depend on how the average is stored.
I'm curious about that long backup time. Using incremental backups, the backup time should be quick unless the blobs are getting completely replaced every day with new files. 
Yes. Test it. Test everything. Merge is often faster and less expensive than update 
Thanks for this!
Thanks goes to Chris Saxon. His work is impressive, and often [quite refreshing :)](https://www.youtube.com/watch?v=Rlgb7LwOiHk)
~~switch your WHERE and ORDER BY~~ ~~select us.email, cm.course_id, cu.role, us.USER_ID~~ ~~from users as us, course_main as cm, course_users as cu~~ ~~where us.pk1=cu.users_pk1 and cu.crsmain_pk1=cm.pk1 and cu.role = P and (cm.course_id like 'SP2-2017' or 'SPC-2017')~~ ~~order by users.user_id desc;~~ /u/r3pr0b8 is correct, I did not look at your query closely enough. 
3 errors in your revised query... see mine
You probably want to read up on The Party Model, and Table Inheritance
 ON dim_dt.dim_ky BETWEEN TO_DATE(empl.begin_dt , 'DD-MON-YY') AND TO_DATE(empl.end_dt , 'DD-MON-YY')
The average day adds about 1,400 of them and busy days can be close to 2,000. And the size varies greatly from as little as a few k to as much as 10 megs each. The killer is that the off-site safety copy is built into the backup process and we can't convince management we need a bigger pipe.
Not sure about this but check out codeacademy and then buy a SQL book. You can learn very quickly. Find something you have passion and create a DB for that. I have one for baseball stats
Could you copy over the table definitions?
Unfortunately the SQL I'm using doesn't support row_number. Oh well, I'll take the concept and use it in the workaround. 
Your begin/end dates might have an hour:minute element to them so it might be worth truncating them: ON DIM_DT.DD_DT BETWEEN TRUNC(EMPL.BEGIN_DT) AND TRUNC(EMPL.END_DT)
 create table empl ( PERSON_KY Number , BEGIN_DT DATE , END_DT DATE , HC Number); create table dim_dt ( DIM_KY NUMBER NOT NULL, DD_DT DATE); insert into empl(person_ky,begin_dt,end_dt,hc) values (1234, to_date('8-Sep-15', 'dd-Mon-yy'), to_date('10-Sep-15', 'dd-Mon-yy'), 1); insert into empl(person_ky,begin_dt,end_dt,hc) values (1234, to_date('11-Sep-15', 'dd-Mon-yy'), to_date('18-Sep-16', 'dd-Mon-yy'), 1); insert into empl(person_ky,begin_dt,end_dt,hc) values (1235, to_date('12-Sep-15', 'dd-Mon-yy'), to_date('15-Sep-15', 'dd-Mon-yy'), 1); insert into empl(person_ky,begin_dt,end_dt,hc) values (1235, to_date('16-Sep-15', 'dd-Mon-yy'), to_date('18-Sep-16', 'dd-Mon-yy'), 1); insert into dim_dt(dim_ky,dd_dt) values (20150908, to_date('8-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150909, to_date('9-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150910, to_date('10-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150911, to_date('11-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150912, to_date('12-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150913, to_date('13-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150914, to_date('14-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150915, to_date('15-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150916, to_date('16-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150917, to_date('17-Sep-15', 'dd-Mon-yy')); insert into dim_dt(dim_ky,dd_dt) values (20150918, to_date('18-Sep-15', 'dd-Mon-yy')); commit; SELECT DIM_DT.DD_DT, SUM(EMPL.HC) FROM DIM_DT JOIN EMPL ON DIM_DT.DD_DT BETWEEN EMPL.BEGIN_DT AND EMPL.END_DT GROUP BY DIM_DT.DD_DT order by dim_dt.dd_dt; DD_DT|SUM(HC) ---|--- 08-SEP-15|1 09-SEP-15|1 10-SEP-15|1 11-SEP-15|1 12-SEP-15|2 13-SEP-15|2 14-SEP-15|2 15-SEP-15|2 16-SEP-15|2 17-SEP-15|2 18-SEP-15|2 Edit: Try doing both following selects and pay attention to the date formats: select dim_ky, dd_dt, to_char(dd_dt, 'dd/mm/yyyy hh24:mi:ss') as to_char_dd_dt from dim_dt; select person_ky, begin_dt, to_char(begin_dt, 'dd/mm/yyyy hh24:mi:ss') as to_char_begin_dt, end_dt, to_char(end_dt, 'dd/mm/yyyy hh24:mi:ss') as to_char_end_dt from empl;
My apologies if my comment came off wrong. Definitely wasn't intended. I should have replied directly, but tagged you instead so you would see what I would have said. Again, SQL is very new to me and I've learned a lot from this community, so I do appreciate your help!
nothing to apologize for your original post just wasn't very clear about the underlying datatypes i'm sorry i said it was misleading... but it kind of was ;o)
 FROM Agents INNER JOIN Engagements ON Engagements.Agent_id = Agents.id
something like FROM agents INNER JOIN engagements on agents.agentid = engagements.agentid is the standard way of doing joins. This is something that you would learn in your first hour with SQL and nearly any example of SQL you could google would include it. Best of luck with your learning journey.
&gt; INNER JOIN engagements on agents.agentid = engagements.agentid Appreciate the help!
Thanks for the recommendations! I'll definitely add them to my toolkit - makes sense. Was trying to prep for a gig with a customer and had no means to build something out for actual testing in the time I had last night. Used the OP code and received an unrecognized syntax at ON - not sure where it blew out so I just dropped the original table and instead did a select * Thanks again!
MSSQL Azure, latest version. Rest of the table information isn't important, it was one table with information on 3 types of "objects", is now 3 un-linked tables. 
Yeah I'm just not understanding this. Don't think SQL is for me. I can't even create this simple problem: "Find the agents and entertainers who live in the same postal code"
I am following a youtube tutorial about SQL and he's using SSMS so its much easier for me to understand if it's the same since im new to SQL. Any ideas why?
Do you have an instance of SQL Server to connect to? If you don't, then SSMS will be essentially useless to you.
It sounds like it would be a lot easier to create a shared id before putting the new data into the tables?
Sigma items_ordered=date_max Or items_ordered=date_min (date_hist) Or something like that. Going off Wikipedia page for terminology. First step is to translate the SQL declaration into a plain words description, then translate that into an algebraic expression.
Max symbol (backward epsilon) Select symbol (sigma) Not sure how to handle the OR but something like: 3MAX ITEMSORDERED(sigmaM.maxitemsdate=d.items_ordered(datehist)) 3MIN ITEMSORDERED(sigmaM.maxitemsdate=d.items_ordered(datehist))
would love to hear your ideas for improvements - always open to ideas and critques
Here are the contents of "/var/log/mysql.d" http://minimael.com/log/mysqld.log 
Have you googled any of this?
Which variant of sql are you working with?
wouldn't I still need one one due to the fact that age is a property of the employee, but department ID is not? 
You need to look at GROUP BY and aggregate functions like AVG(), MIN(), MAX() and COUNT() to get the answer you need. You also need to specify how your tables are joined together. From other questions asked here, it seems there's a persistent habit by academics to use the old, deprecated, harder to read join syntax, which in your case would probably be: FROM Dept D, Emp E, Works W WHERE D.did = W.did and E.eid = W.eid That might be what you've been taught and what your instructor wants to see, but in the real world better practice would be: FROM dept d INNER JOIN works W ON W.did = D.did INNER JOIN emp E on W.eid = W.eid I don't see that you would want age in your where clause at all.
I was just drawing a blank on the Where clause. So this would work? On mobile, so my formatting might be screwy. SELECT D.did, AVG(E.age), min(E.salary)||' '||max(E.salary) as Salary_Range, Count(W.eid) FROM Dept D, Emp E, Works W WHERE D.did = W.did and E.eid = W.eid GROUP BY Dept.did ORDER BY Dept.did 
No, you need to join the tables together
Apart from maybe the concatenation of the min/max salary (I don't know enough about MySQL to comment), that looks spot on. For yours, and your future colleague's, sanity, it's best to get into the habit of using modern join syntax. It makes no difference to what the query outputs, but it makes complex queries easier to understand.
I was at a shop that had XML in a column. Another guy wrote up some functions to read the tags. You'd call readXML(tag desired) and it'd spit back the XML value.
Yeah - IT shared that they query against this field using using the function XMLQuery to read specific tags. I've confirmed that running XMLQuery(tag) works and gives me results. My issue is that I'd like to see ALL contents of the XML field - tag, value, and all. Running a simple query (SELECT XmlField ...) gives me a 0-length string. Hmm, thinking about it, perhaps I can extract the raw contents if I use a specific XML function?
Similar syntax in the language, but how data is structured and stored varies enormously. I've never used H2 but given it's a java based in memory DB I'd imagine the storage and performance implications of its data types would be very different from most SQL database.
That's true. Kind of odd that it won't do anything if there are 249999 though.
in SQL Server SELECT XML_Field FROM Table1 gives you a hyperlink which when clicked shows the XML field. Maybe try casting XML_Field as Varchar(4000)?
Parameterize your SQL. It will tell you the problem. Also use string interpolation.
definitely this.
I would recommend turning this into a script and running it piece by piece to see where the disconnect is. Additionally, assuming this is SQL Server, you could use the INSERTED function to see what is happening.
 Console.WriteLine(com.CommandText); So what does your command ACTUALLY look like? I'd suspect one of your variables of passing in a `,` and you basically running an injection style attack against yourself. Listen to /u/CoderHawk and /u/alinroc. Paramterize your SQL.
You have no transaction begin or transaction commit. Is your database set up to autocommit or is your work being automatically rolled back so there are no updates and no log? Aside from the LIKE issue that /u/fauxmosexual and /u/AXISMGT already discussed the lack of transaction control stands out most to me.
That's a great point. If auto commit is off, none of this is even permanent. 
Unless it's intentional and the transaction control is being performed by the thing calling the stored procedure. But if that's the case I'm not sure how the checkpoint effects things. edit: word
agree with the new table, but totally disagree with the auto_increment primary key you would ~also~ need a UNIQUE constraint on the pair of FKs -- otherwise with the model as you posted it, it's entirely possible for the same user to add ~many~ opinions on the ~same~ post, which OP specifically said was to be avoided it's much easier to dispense with the auto_increment, and make the pair of FKs the PK (unique by design) -- so each user gets at most 1 opinion on any single post
I always create a unique key for the table itself. It's just a design philosophy I follow. Any table should have a unique key that carries no information in itself. A PK that's the combined FKs has information implicit in it. I didn't get into constraints or indexes since the OP specifically asked not to get code. 
I know it doesen't. That is because it is a previous exam from earlier years on our school. Our teacher sent us five of those so we could practice for this years exam. I really need help tho :/
These queries should all be relatively simple so it's hard to provide much guidance without just giving the answer. a. Project Title is only available in the Project table, Hourly Rates are listed in the Project Team table. So you need to find out how to link the tables then find your Average. b. Employee table to find Employee name links to the Project Team table to find their hourly rates based on the Project Code they are working on. c. You have Employee Names and Project Managers... not much else to say here.
I think I got C right: SELECT Project.ProjectManager, Employee.EmployeeName FROM Project, Employee;
Could you correct me on this one please (a)?: SELECT Project Title, AVG(HourlyRate) AS AvgHourlyRate FROM Project, ProjectTeam INNER JOIN ProjectTeam ON Project.ProjectCode=ProjectTeam.ProjectCode;
Isn't something like this done better in the front end code ? 
I'd think you want a table that just stores the review data (PK is review_id, user_id, video_id, review, create_date, modify_date), with a stored procedure that adds or modifies the user and post's review. Then you just have queries to display # Like, # Dislike, Average.
&gt; Any table should have a unique key that carries no information in itself. this is debatable at best, to put it charitably you make that statement as though it were fact &gt; A PK that's the combined FKs has information implicit in it. well, duh... *it's supposed to* 
I misspoke, but you answered my question anyway. Thanks! 
&gt; I'd much rather write an update statement with a single argument in the where clause ("where id = @id") than two arguments ("where user_id = @user_id and post_id = @post_id") i would love to hear your explanation of how the interface is going to determine what that @id value is -- you do realize it has to do a search first, right? using one or the other (or both) of post_id or user_id, right?
That one is close. General recommendations, putting multiple tables in your FROM clause is not necessary and/or incorrect depending on what you are trying to do. Use one table, then join to the other. Another thing you want to consider is aliasing your tables, it would look something like this: SELECT Project Title ,AVG(HourlyRate) AS AvgHourlyRate FROM Project p INNER JOIN ProjectTeam pt ON p.ProjectCode = pt.ProjectCode; Note: I modified your query to include my suggestions, but there is still one thing missing from it. If you were running these queries on actual tables you would see the error and then be able to respond to it. Using something like sqlfiddle would also help you understand how to create the tables and insert data to them.
Thank you for your help! ~~Okay, so for a) SELECT courseCode, Courses.name, studentID, grade FROM Courses LEFT JOIN Grades ON courseCode=code works, except it breaks a course that has no grades. The output should be "CSE-A1110, Programming 1, None, None", but it is "None, Programming 1, None, None". How should I change my query?~~ I should just use code instead of courseCode... I got the b) fixed myself, thanks for help!
The best hint I can provide is look up what is necessary to start using Aggregate functions (ex. AVG(), MIN(),MAX(), etc..) across multiple rows. In this case, you will be averaging multiple rows according to which project they are attached to (instead of just an overall average), so you need a way to tell it which column to group your averages using.
You still need to save it somewhere, right? I think that's what OP's asking.
Your comment definitely wasn't useless. I've learnt something new. TRANSLATE looks really useful. While googling it I found a few UDF versions that I might have a play with.
I've never seen an out-of-the-box solution for SCM-for-SQL. That being said, I've always wanted to put all of my 'CREATE' and 'ALTER' scripts into source control. Ideally, my Test &amp; Production environments would *only* be accessible via deployment scripts that integrated with SCM. "Go get all of the DML scripts for this tagged release &amp; execute them." That way, no objects get to Test without being checked into source control first, and no objects get to Production without building &amp; deploying properly in Test.
In that case , it would be better if he maintains a table in the database that holds postID, userID and some Boolean field "Liked" 
In an attempt to rule out the obvious. Are you literally running the create procedure code and getting 1 row affected? 
much as in writing reddit posts, specificity is important.
Good advice.
Beware of dogs with orange eyebrows.
If you never start you can't fail!
Thanks mate, appreciate it.
This helps! Thanks! 
I cringe saying this to anyone working with SQL but for those that are new to it, Microsoft Access. I have been supporting legacy applications and most of them were Microsoft Access. I still hate it but for those of us that are visual learners, MS Access gives you a visual representation of SQL queries in their table form. Do not JUST use MS Access, use access to help visualize the different tutorials you come across starting to learn SQL the language. ...oh and stackoverflow. (just to add, your post does not really have anything for anyone here to answer so that makes it difficult to provide any appropriate suggestions, if you want to learn SQL in general, start with [tutorials](https://www.w3schools.com/sql/). 
Start with simple queries and build up to bigger ones. Always start by reading and then change to an update. If you are making changes, use transactions if your DBMS supports them, that is BEGIN TRANSACTION, and then either Commit or Rollback. Always use semicolons with interactive query systems (i.e., SQLdeveloper), otherwise you might inadvertently execute something that you shouldn't. 
This should be controlled by your application layer. 
Google is your best friend. Lots of code just sitting there waiting to be inserted into your query. 
I can't think of an elegant, purely SQL solution to this. I think I would end up writing a CLR function in .NET and install it as a UDF. Then you can treat the the string as a char[] and have a static mapping char[] for the translation. 
Neat!
&gt; It's just a design philosophy I follow. Any table should have a unique key that carries no information in itself. I'm pretty sure that this clearly indicates it's a philosophy, not a fact. &gt; &gt; A PK that's the combined FKs has information implicit in it. &gt; well, duh... it's supposed to Which, according to the philosophy I follow is a mistake. Just look at ZipCodes and the cluster they turned into when they could no longer follow their design as zipcodes fragmented and changed over time. Same issue with phone numbers and now social security numbers. When your table's unique key is tightly coupled to a business rule you are in a bind when/if the business rules change. 
It's not covering everything up to the latest features but [here is a comparison of different SQL implementations](http://troels.arvin.dk/db/rdbms/).
Ended up using a UDF version of translate I found [here](https://www.sqlservercentral.com/Forums/124618/T-SQL-Equivalent-to-Oracle-TRANSLATE#bm547594). So thanks again. If anyone else stumbles across this post in the future looking for something similar here is the UDF (In case the above site no longer has it) ALTER FUNCTION dbo.Translate ( @Source VARCHAR(8000) , @ReplaceCharOrder VARCHAR(8000) , @ReplaceWithCharOrder VARCHAR(8000) ) RETURNS VARCHAR(8000) AS /* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Object Name : dbo.Translate Author : UB for DCF, on August05, 2008 Purpose : Like TRANSLATE function in Oracle. Charecter-wise replace in source string with given charecters. Input : Output : returns @Translated_Source string Version : 1.0 as of 08/05/2008 Modification : Execute : SELECT dbo.Translate('ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890', '1234567890', '0987654321') * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */ BEGIN -- -- Validate input -- IF @Source IS NULL RETURN NULL IF @Source = '' RETURN '' IF @ReplaceCharOrder IS NULL OR @ReplaceCharOrder = '' RETURN @Source IF @ReplaceWithCharOrder IS NULL RETURN 'Invalid parameters in function call dbo.Translate' -- -- Variables used -- DECLARE @Curr_Pos_In_Source INT , @Check_Source_Str_Len INT , @nth_source VARCHAR(1) , @Found_Match INT , @Translated_Source VARCHAR(8000) , @Match_In_ReplaceWith VARCHAR(1) -- -- Assign starting values for variables -- SELECT @Curr_Pos_In_Source = 1 , @Check_Source_Str_Len = LEN(@Source) , @Translated_Source = '' -- -- Replace each charecter with its corrosponding charecter from @ReplaceWithCharOrder -- WHILE @Curr_Pos_In_Source &lt;= @Check_Source_Str_Len BEGIN -- -- Get the n'th charecter in @Source -- SELECT @nth_source = SUBSTRING(@Source, @Curr_Pos_In_Source, 1) -- -- See if there is a matching character for @nth_source in the @ReplaceCharOrder String, then replace it with -- corrosponding character in @ReplaceWithCharOrder String. If not..go to next n'th character in @Source -- If a match is found in @ReplaceCharOrder but no corrosponding character in @ReplaceWithCharOrder -- then, replace it with '' (nothing) -- Store the resultant string in a separate variable -- SELECT @Found_Match = CHARINDEX(@nth_source, @ReplaceCharOrder COLLATE SQL_Latin1_General_CP1_CS_AS) IF @Found_Match &gt; 0 BEGIN -- -- Finding corrosponding match in the @Found_Match'th position in @ReplaceWithCharOrder -- if not found then replace it with '' (nothing) -- SELECT @Match_In_ReplaceWith = SUBSTRING(@ReplaceWithCharOrder, @Found_Match, 1) -- -- Now replace @nth_source with @Match_In_ReplaceWith and store it in @Translated_Source -- SELECT @Translated_Source = @Translated_Source + @Match_In_ReplaceWith END ELSE BEGIN -- -- No match found in @ReplaceCharOrder -- SELECT @Translated_Source = @Translated_Source + @nth_source END -- -- Increment the current position for loop -- SELECT @Curr_Pos_In_Source = @Curr_Pos_In_Source + 1 END RETURN @Translated_Source END /* TESTING: SELECT dbo.Translate('ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890', 'ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890', 'abcdefghijklmnopqrstuvwxyz098765432') SELECT dbo.Translate('ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890', '0123456789', '9876543210') SELECT dbo.Translate('', '', '') SELECT dbo.Translate('NOMENCLATURE', 'ABCLE', '123') */ 
Sadly not a very interesting story. I work far a large CRM provider so we do a lot of data migration from other companies databases into our own. I don't know too much about who designed this mess (We've taken one of their customers so they're not overly helpful to us) but what I can gather is they were worried about people with direct access to the DB being able to read the data in this specific table. It is confidential customer data that should have be secured so I can half see the logic but the execution was dreadful. As a side note I've also never seen a database with so many tables &amp; columns with blatant spelling mistakes. Who creates a table called "Invioces" and just leaves it spelt wrong?
yes, mysql has both.
Logically, you're correct, however, SQL doesn't work that way: &gt;Msg 147, Level 15, State 1, Line 4 An aggregate may not appear in the WHERE clause unless it is in a subquery contained in a HAVING clause or a select list, and the column being aggregated is an outer reference. In effect you're trying to compare the current row's salary with the average of the current row's salary... which wouldn't return the correct results. You want a subquery to find the average of all row's salary, then run a query to find salaries higher than that.
you cant have a function in a where clause. You need to use HAVING or a subquery edit: my msitake
&gt; you cant have a function in a where clause Of course you can have a function in a where clause; WHERE LENGTH(str)&gt;5 . You can't have an *aggregate* function in a where clause.
&gt; It's worth noting that the syntax of the former does not work with the example provided in the latter syntax. Sure is a good thing I noted it then.
Your problem isn't sub-queries and joins, per se. Your problem is interviewing. In my previous job if I hired you and asked you to give me a list with first/last name, and salary, of all employees who are above or equal to the average salary, how would you do it? Could you do it? SELECT emp.First_name AS firstName, emp.Last_name AS lastName, emp.Salary FROM Employee AS emp WHERE emp.Salary &gt;= AVG(emp.Salary) That is your first try, and as another user pointed out you will receive an error similar to this: &gt;Msg 147, Level 15, State 1, Line 4 An aggregate may not appear in the WHERE clause unless it is in a subquery contained in a HAVING clause or a select list, and the column being aggregated is an outer reference. So whatcha gonna do? You gonna come on /r/SQL/ and ask how to fix it? You gonna Google? Bottom line: Are you going to come up with the right answer? If yes, you'd have done great in my last company. If no, then your fundamentals are off. Sub-queries and how/when to use them come with time. They aren't learned per se... you write some code that you want to do something, and it doesn't because SQL has its nuances, so you go online and figure it out. Then you see how to actually do it... then you understand why it was needed. THEN you understand how to apply that new learning to tons of other problems you encounter. Are you not interviewing well because they're giving you tests, or are you just not interviewing well verbally? I've NEVER taken a SQL test for a job, and I've interviewed for dozens of them at this point.
in that case, keep the DISTINCT but remove the course and role from the SELECT clause
date functions and date arithmetic are substantially different also, MS SQL is missing the GROUP_CONCAT function (booo!)
That worked, thank you!
&gt;Are you not interviewing well because they're giving you tests Yes. &gt;So whatcha gonna do? You gonna come on /r/SQL/ and ask how to fix it? You gonna Google? That is exactly what I did at my previous job as a Software Engineering consultant. I was never taught SQL in school so I had to learn it on the fly, and especially so as I was doing ETL work. I could be wrong about this, but in an interview, if I don't know the answer to how to build an appropriate query with a proper WHERE clause, wouldn't saying, "I'd research how to get the answer" make me come off as a weak candidate?
..if I could shake your hand, I would. Thank you for that.
I was nailing it until I got to the Joins with nested Selects. I couldn't even comprehend some of the answers after trying for more than half an hour.
Actually, in hindsight, I'm pretty sure /u/dukeMAXIMUM would prefer my one.
Just think of the nested select as a new table, the mechanics of the join are identical
I guess I've never used MAX or MIN with a text string before. Thanks!
Thanks! I didn't realize I could do that with text!
&gt; I could be wrong about this, but in an interview, if I don't know the answer to how to build an appropriate query with a proper WHERE clause, *wouldn't saying, "I'd research how to get the answer" make me come off as a weak candidate?* No - but being afraid to admit that you don't know everything WILL make you come off as a weak candidate. "I don't know" or "I can't recall this right now but when I run into roadblocks, here's what I do to find the answer." is precisely the right thing to say. At least to some people. There are interviewers out there who think you should be more machine that the computer at your desk, but you probably would hate working for them anyway. 
I don't know what any of those things are and I'm pretty sure SQL doesn't have redirects. Sounds like a java question more than an SQL question maybe?
Good news!! I solved it without a cursor! So here is solution you never asked for, but I came up with it so you get to see it any way: declare @replaceString varchar(100), @charRep varchar(5), @newChar varchar(5), @findString varchar(5), @ignoreString varchar(5), @repString varchar(100) declare @i int, @rStringLen int set @replaceString = 'aj;bf;cu;jY;fP;uX' --translation string, semi-colons added for readability, not actually required set @rStringLen = len(@replaceString) set @i = 1 while @i &lt;= @rStringLen begin set @charRep = SUBSTRING(@replaceString,@i,1) set @newChar = SUBSTRING(@replaceString,@i+1,1) set @findString = CONCAT('%',@charRep,'%') set @ignoreString = CONCAT('{',@charRep,'}') update testTable set TextRow = REPLACE(TextRow,@charRep,@ignoreString) -- add non used character to the front and back of each character to be replaced where TextRow like @findString --test table is obviously your table, adn TexRow is the colomn name set @i = @i +3 end set @i = 1 while @i &lt;= @rStringLen begin set @charRep = CONCAT('{',SUBSTRING(@replaceString,@i,1),'}') set @newChar = SUBSTRING(@replaceString,@i+1,1) set @findString = CONCAT('%{',@charRep,'}%') update testTable set TextRow = REPLACE(TextRow,@charRep,@newChar) --replace the new character with the old one set @i = @i +3 end
Listen to this guy. I'll admit, when I interviewed at Microsoft the technical questions were insane, the process was borked, and I was grilled for hours. This despite the fact that I was the only human on the planet that could possibly be qualified for this job... because I had already been doing it as a vendor for 6mo and it involved highly proprietary knowledge that was invented during that time. But that was MS. At every other technical job? They want to know that you know how to find stuff out. Not on every question (don't be dumb) but if someone has 10 years of experience on their resume I'm going to ask questions to show me that their lightbulb is on. Best advice I can give for a question you cannot answer? Take a stab at something similar, that is complicated, that you have mastered. So, I say "How would you do this Common Table Expression in SQL". You can say "I actually don't have direct experience working with CTE's, I understand they are powerful, but when we had a similar problem in C# we handled the data flow like this..." Always steer the interview a little into the areas you are a master. This did backfire on me once though. I was told I was overqualified for a job that would have been paying me double what I had currently been making. They told me I was a flight risk because I would be too bored in their group.
https://blogs.oracle.com/developers/learn-sql-with-this-free-online-12-week-course Still time to sign up.
Based on what? There's no way DB2 is used more today than SQLite.
I bet if you could find out the sizes of Walmart 's DB2 databases, they would be larger than all the SQLite databases around the world. They used to be the second largest user of IBM mainframes after the NSA.
well yea, using sql in conjunction with js
I think for the sake of salary comparison you'll always want to ask for location as well. $100k in the Bay area or NYC is incredibly different than $100k in Georgia, for example. 
That does. I feel silly for not remembering ISNUMERIC. I can delete the NULLs first and then use ISNUMERIC to sperate the INT values from the VARCHAR. thanks so much.
It is not the same. TRY_CONVERT will return NULL if it can't convert it to the specified type instead of throwing an exception. ISNUMERIC may work as well as /u/d_r0ck suggested. The only difference there is that ISNUMERIC will return 1 for decimal values as well. 
I will look into it. I've only take 1 class for SQL and it was 2 semesters ago. I'm trying to get the basics back down. :P I appreciate you commenting and helping me out.
I'm the author of this course. It assumes no knowledge of SQL/databases. So yes, you can do it. Just dive in and get started :)
Each week there's a video introducing the concepts. This is followed by quizzes to help reinforce the points. So you can watch whenever suits you. I'm hosting a live Q&amp;A every 4th week. These will be one-offs. The rest of the content you can use any time.
Unfortunately sql server doesn't let you use the on () command, I tried :) and if you try do a group by or order by it give the same results as using a straight distinct. ON seems to be reserved for JOIN syntax, and I'm not sure if there is an equivalent for it for this usage.
Good point about the decimals
&gt; The HAVING keyword does what you are looking for It actually won't do what he's looking for. `HAVING Value &gt;= AVG(Value) ` will always evaluate as true, because in order for `Value` to appear in the `HAVING` clause, it has to either be in the `GROUP BY` (thus evaluating as `Value &gt;= Value`) or in an aggregate function. And since he wants to get the employees, he has to group by the employee data, which is in a 1 to 1 relationship with their salary, causing any aggregate of `Value` to be equal to itself. Decent explanation of the order of operations, but you didn't address the flaw in his logic, only the flaw in his syntax. Here's a demonstration of why HAVING won't work. Play around with it a bit to get all the employees with a salary that's higher than average. DECLARE @Employees TABLE (ID INT IDENTITY(1,1), Salary MONEY) INSERT INTO @Employees VALUES(40000), (50000), (60000), (70000), (80000), (250000) --SELECT ID --FROM @Employees --GROUP BY ID --HAVING Salary &gt;= AVG(Salary) --Doesn't work SELECT ID FROM @Employees GROUP BY ID, Salary HAVING Salary &gt;= AVG(Salary) SELECT ID FROM @Employees GROUP BY ID HAVING MIN(Salary) &gt;= AVG(Salary) --MAX(Salary) &gt;= AVG(Salary) --AVG(Salary) &gt;= AVG(Salary) --SUM(Salary) &gt;= AVG(Salary)
This is a weird question. Is it possible for you to post your table definitions and an example of the conditions in the second part of your query?
someon one toldme learning MS access is the same as learning SQL BUT with visualization?! is this true?!
Awesome! Welcome to the club. Do this for a couple months then start working on Python! :) Are you starting to appreciate SQL for reporting yet? What tool are you using for data visualization?
Oh, man. That does not sound fun! If the reports are parseable text, this looks exactly like a job for Python :)
Microsoft has a developer login setup you can register for. While you can get almost SQL Server 2016 enterprise (dev ed) to play with there, there are more things that may spark your interest. Like the 3 month free period without credit card pluralsight learning center. Great intro for you imo and it's free. 
Agreed. But the comment indicated that *everyone* who works with SQL relies on Google and Stack Overflow to get their job done, which is just flatly false and gives a false sense of competency to someone who should realistically know where he stands. That's like saying that everybody who wants to speak Spanish has to use Google Translate.
Ok. Does SQL Server allow window functions? If so you could do something like: SELECT * FROM ( SELECT id, city, row_number () OVER (PARTITION BY id ORDER BY id) AS break_id FROM table ) sub WHERE break_id = 1; Basically each set of things (partitions) in the subquery here are assigned a number from 1 to infinity. The where filters out all of the subquery results then that had a row number of 1.
Glassdoor.com too
You would use it as part of the `WHERE` clause: SELECT CandyName FROM Candy WHERE CandyType IN ('Sweet','Sour') You can also use a subquery to do this: SELECT CandyName FROM Candy WHERE CandyType IN (SELECT CandyType FROM Candy WHERE CandyType LIKE 'S%')
CTEs (common table expressions) will help out with this. Stick each subquery in a `WITH` at the top, and Oracle only has to evaluate it once. I wrote this before I read your edit, but it shouldn't be hard to modify: with t1 as ( --subquery1 ), t2 as ( --subquery2 ) --If both subqueries have values, return intersects select * from ( select * from t1 intersect select * from t2 ) --if t2 is empty, return all rows from t1 union all select * from t1 where not exists (select 1 from t2 where rownum = 1) --if t1 is empty, return all rows from t2 union all select * from t2 where not exists (select 1 from t1 where rownum = 1)
Cast is similar to convert, but TRY_CONVERT should return the value as a result of whether or not the convert is successful. If it wasn't, it just returns NULL. This helps because you don't get an Error if the convert can't happpen. This might be anecdotal, but I have found that sometimes I need to Convert to BIGINT before a conversion to varchar.
Fair enough!
Salut! &gt;So thank you so much guys! Once I get a little better I'll try to stick around and pass on what little I know. Once you start treading water after a year (two at most) it's time to look for a more senior position and get a nice big, well deserved raise.
You'll love window functions then, perfect for things like this.
Thanks, appreciate the info! Good idea for DDL moving via SCM also. The idea here would be "Kick off the DDL (db structure altered). Then go get all of the altered DML scripts, replace the old and sit them here on the server"... Then a nightly batch kicks in and executes the updated scripts. Would definitely be an improvement on what we have!
Any particular videos or information that you found to be helpful?
You've had some very clear answers already, and while there are much better, more advanced reference sites for TSQL I'd get yourself to w3schools.com and get your feet wet with the examples there.
Its a good starting place. You can trick it out from there - AND c.system_type_id NOT IN (....) and specify the IDs of data types you don't want like skip 56 (ints) when you are looking for text based columns or whatever. or change the select to SELECT ' SELECT TOP 10 ['+ c.name +'], * FROM [' + t.name + '] WHERE [' + c.name +'] like ''%myVal%'' ;' and then paste those results into a query editor to take a peek at the data.
Any chance you can make a table with Keywords in it? If you can, then you can simply join to the keyword table. That way you can keep it modular and not have to update the query every time you want to add/remove a keyword. Keeps the OR LIKE count to a minimum and you can even add some flare like SOUNDEX.
I think that the kind if expectations you're setting seem fair. They may be on the more lenient side, especially for that kind of salary and benefits package. My thought is that you're throwing some data warehousing concepts at them that a regular DBA wouldn't be accustomed to. In addition, they are most likely not inclined in finance, econometrics, nor statistics, so just the mere terminology could be throwing some people off entirely. They may understand and know how to use concepts on scale you've defined, but they are missing that data warehousing and econometrics background you need on top of their programming skills. 
Thanks. That's a good point though ironically, most of the candidates have been people working in (broadly defined) data science or analytics roles at financial data vendors or other financial firms. For example, one guy I interviewed today even leads a data team at a fund and rated himself an expert, but, when I gave him my level 3.5 question (same basic setup, but give me, as of 4pm every day, the price of the most recent trade, whenever that was, be it at 3:59pm that day or six months prior), he couldn't do it, and, after I walked him almost all the way through it, he was using left joins for no apparent reason. At least he didn't give me the most asinine response I've gotten so far, which is a tossup between "Oh, I wasn't expecting a technical interview," and, "I'd do that in R." 
Maybe he is overrating himself, and he also lacks basic foundations of data modeling and querying. That is a bit alarming if he couldn't justify why he was using the JOINs he chose. 
My thoughts exactly. 
I have the ability to do pretty much whatever is needed, but I'm not sure I'm following your suggestion. Are you saying that the second table should contain unique keywords and the main table should link to that via ID?
Maybe a table with KEYWORD (varchar) and Include (BIT) columns. Then possibly a join on Keyword LIKE '%'+keywordtable.keyword+'%' or even a CHARINDEX() Doubt it would be very efficient though. Maybe a CTE would work a bit better in your scenario.
I can't really help with your problem, but I do enjoy the fact that I'll be looking for a job soon with newly aquired sql proficiency and I understand your example well! Gives me some confidence, so thanks for that!
Good point! I've only really worked with oracle so far, so I'll be sure to put that specifically on my resume rather than just SQL
Either or both is fine, but I'd probably do something like "SQL (Oracle)." SQL is pretty broad, and very, very few people know every variant. In fact, when somebody just puts SQL, the first thing I ask is which systems they've used. The issue is in making specific claims that won't hold up. And please don't write things you don't really know. Somebody might check, and it looks pretty bad when you have Python on your resume, but when I ask you about it, you tell me "Oh, I just started learning it." Like, if it's not at the point where you can use it do the job, it's not something that should go into the hiring decision, so it has no business on your resume.
Reasonable! I will do just that. And just to keep milking you, I've been adding a skill level or comfort level to it, like python - beginner, should I continue that?
Sounds good! Thank you for taking the time to respond!
My pleasure. Feel free to shoot me a PM if you have any more questions.
&gt; We grow over 20 users a month you will not have to worry about reaching the bigint unsigned limit for **A BAZILLION GAJILLION YEARS** i can't be bothered doing the exact math, but i'm willing to bet you a beer that this will occur well after the heat death of the universe
https://docs.microsoft.com/en-us/sql/t-sql/data-types/uniqueidentifier-transact-sql
anyways here's a pivot... Title |Average of SalaryUSD :--|:-- Analyst|77,579.69 Analytics consultant|61,000.00 Architect|123,769.23 DBA|102,067.16 Developer: App code (C#, JS, etc)|80,941.29 Developer: Business Intelligence (SSRS, PowerBI, etc)|92,820.00 Developer: T-SQL|100,383.10 Engineer|111,954.55 Manager|113,000.00 Other|135,000.00 Grand Total|100844.3575 
Let me know if you guys can see this: https://www.surveymonkey.com/analyze/pizkkfqZjJGAWeDTAjlE68PH3zYyhw49a6vuqjRUj0o_3D Some of these answers really surprised me!
&gt; At this point, I'm starting to think that maybe my expectations are off. Having read through your post I don't believe you'll ever find anyone with your process. People in interviews can generally handle about 50% of what they can handle when on the job, in an environment they've become familiar with, people they're familiar with, etc. I'm just looking at this example: &gt; Imagine an asset that has an incredibly volatile trading volume. That is, it may not trade for weeks, and then it may trade thousands of time in a single day, and then one a day for the next week, etc. The point is that the volume is unpredictable. Now, let's say we have a table of all of this asset's trades with two columns, timestamp and price. We want to get the 30-day moving average price, defined as the average of all trades in the prior 30 day period, or NULL if there were no trades in that period. So the result should have two columns, date and average price, where the average price for, e.g., March 30 should be the simple average price of all trades between March 1 and March 30. I would quote something else I've seen: *For every 25 percent increase in problem complexity, there is a 100 percent increase in complexity of the solution.* You are testing several very different things at the same time here: 1\. Their ability to understand your language and jargon 2\. Their ability to handle somewhat complicated sql 3\. Their ability to parse sql presented in a non-sql format (sql queries vs story format from above) 4\. While coming across as likeable to a stranger (you) 5\. In the middle of a series of other tough questions I can make coffee, I can take out the trash, I can talk to a recruiter on the phone, and I can drive a car. They are not hard tasks individually. But ask me to do them all at the same time - and a bunch of simple tasks become to impossibly complex to handle. There's to many level of complexity in your question - jargon on top of story format on top of actual sql concepts. On top of interview stress, in the middle of a questions. Someone who is very familiar with how to do the sql is going to bomb out on the question as they are unable to do the other 4 things and also the sql at the same time.
Holy fucking shit man! This is like asking for half an apple and getting a full basket! Thanks a lot!
That's a fair point. Do you have any recommendations on how to tell if somebody is actually skilled with SQL? Is there a better way to ask the question? Would giving them take-home problems (e.g., through hackerrank) be a better solution? Edit: Also is that really something you'd consider fairly complicated SQL? I mean, it's just a left join and an aggregate, not like recursive CTEs or even subqueries.
These queries won't produce correct results though. If the keyword is *bob*, then it can't be *sue*. Because it's already *bob*.
Looks like you have a single quote in front of personid, but the single quotes should be around the integer/number string.
I would utilize a CTE for easier visualization, with row_number (ordered by whatever makes sense for you), then join that to your main table where rn &lt; 3.
This, and familiarize yourself with the data and relationships in your tables. If you don't have a proper ERD and data dictionary, just select the shit out of those tables. Prepare for lots of trial and error when beginning.
Don't use the quotes that way. Depending on the column type, you might need the numbers in quotes tho
I apologize, I'm not familiar with what CTE refers to. As for doing a join based on row numbers under 3 - the problem is, almost every primary value will have a dozen or more rows - I can use them, but I just need two select two of the many (or will it still pull two for each, but no more?)
Ok, your prior post - what is that you are actually comparing? (What is supposed to be the same or similar in two sources?) Also, quickly glancing the query over, in a similar case I would have asked for new/deleted/updated record categories.
Hey there, I just started learning Access SQL for finance purposes when my boss dropped an 'ambitious idea' on me in Feb. Using your scale, I'd put myself about 2.5. I took a look at your example, and it seemed quite approachable to me...*if I had a comfortable environment to work in and the ability to use Google*. I think /u/GhostBond has a good point about all those compounding factors causing someone to flub that question. I would definitely have trouble coming up with a solution in a high pressure interview, but would give it a solid go on some scrap paper. Perhaps something like 'here's an example problem, I'm going to leave the room for 15 min and give you some privacy to come up with a solution, knock on the door if you have questions' might work?
Have you tried the EXCEPT clause? 
Would that be considered a multiple field primary key?
this did it, thanks.
If you have no clue what advantages normal forms bring and what they are in the first place, how in the world would you know 'clear advantages in ignoring the NF's'? You're ignoring them by the 'virtue' of being ignorant in the first place. (P.S it's rant post, so my responses will be colored accordingly).
Not if that table already is assigned a PK. It would HAVE to be a unique nonclustered constraint, no?
&gt; If you have no clue what advantages normal forms bring and what they are in the first place, how in the world would you know 'clear advantages in ignoring the NF's'? I never said you shouldn't understand NFs. I said that there are clear advantages from time-to-time in ignoring them. All best practices work that way. A particular best practice is the way it is because using that technique produces the most favorable outcomes in the widest array of situations. However, there are always "exceptions to the rule" and after thoroughly understanding the best practice you can learn the right times to ignore it. “Learn the rules like a pro, so you can break them like an artist.” - Pablo Picasso &gt; You're ignoring them by the 'virtue' of being ignorant in the first place. Don't assume ignorance. Ask questions first to understand a persons knowledge and position. &gt; P.S it's rant post, so my responses will be colored accordingly No worries, I totally get it. [And again in full disclosure I didn't read your post only the title so it is highly likely you and I agree.]
Say no to natural keys. We who live in the real world on living systems built upon the quicksand of the shifting requirements of our fickle users know that building our keys on data that means particular things to that hoi polloi is to invite future pain. We know that for every 100 databases in the third or higher normal form, 1 is done that way because it's the best approach and 99 are over engineered projects designed by pedantic architects trying to show off their mastery of academic and usually pointless design principles. These sad people, with their self worth conflated with their intellectual vanity, inflate their own feelings of importance by berating everyone around them with a bunch of useless book learning nobody cares about. In particularly sad and extreme cases they will even rant at anonymous people in a sub reddit to get their ego inflation fix.
Thanks for this! 
I guess you work with MS SQL? 'Clustered by primary key by default' is a microsoft thing. You wont get one in Oracle, for example. 'what even is a nonclustered primary key', really? you realize that invites a question - do you know what a 'KEY' is? And yes, theoretically and functionally nothing SHOULD be different for a 'primary' key other than it being marked for convenience and some defaults associated with it for convenience as well - and that is how it is supposed to be.
"However, some records don't have a good set of attributes that can uniquely identify the entity in question." - this (usually) means that the data design is bad. "How do you uniquely identify a purchase order without a unique ID column" - that's a great question and that actually means that you did, in fact, break or did not even have 3NF for your original/business/baseline data. And the answer (and absolutely correct one) - a piece of business/application data is required - Purchase Order Number or some other kind of visible label/number (as opposed to a synthetic Order_ID created independently of the business workflow). 
Hey - once you'll see at least your second production database kept in 3rd (heck, I want to see the first one you come across that is in 5th form). I do - truly. BUT. something tells me that this comes from your own universe of alternative facts. As long as you know what are you breaking and WHY and what do you SHOULD do from that point onward in terms of data integrity and data QA - BE MY GUEST. The 'sad people' who are saddled with systems overloaded with data entry duplication and are tasked with sorting though the piles of shit collected in the tables cobbled together by the precocious coding maestros who are allergic to learning and seem to be able to re-invent the wheel every day as long as it has new 'pizzaz' - are these the sad people you're talking about? 
So how long, on average, do you think it takes you to "sort through the piles of shit" of having - shock horror - keys of a type that are completely consistent across the database and requires absolutely no knowledge of the context of the data in a relation to be able to understand? Or is it the wastage of those valuable kilobytes of disk space that offends your ivory tower sensibilities?
You are exactly right. I registered online but cannot find the link to the course (Database for Developers). Do you have a direct link to the course? I have already registered.
You are a pretentious asshole. Newsflash: The whole world doesn't operate inside your precious little bubble.
Well you're welcome to your non-sequitur opinion of me or your assumptions on how i see the world. And I did welcome your personal attack with a downvote. Do you have anything of substance to say or your attacks are like soap bubbles - colorful but empty?
I actually agree with you. I consider a surrogate key to be a mild form of denormalization. My solution is to put a unique constraint on the natural key, in addition to a surrogate PK, when feasible. That being said, letting natural keys ripple through a design (via FKs) gets a big NOPE from me.
Can you show what the table looks like?
I see it's a at your own pace, 12 week program, asking 30 minutes a day to keep up with the work (very do able). At the end of this training, should I be in a place where I could be ready to test for the SQL cert? (Sorry I'm a complete nub on this but want to know what is accomplished after this course is over, and it's relevancy to an employer if I start hunting for entry level stuff).
What is the more broad problem you're trying to solve? Why is it that they need to be updated in tandem? This sounds like a design flaw or something that the DB should not be concerned with, but without full details it's really hard to say for sure. 
You need one more recursive join
So you should non cluster fields that get pulled quite often ?
THANK YOU. Of course! :D It's my stupid naming of the attributes coming to haunt me. But I'm still learning. Cheers!
 Hum... If you fork this will oracle attempt to sue for copyright infringement?
Why? If you fork and respect the Apache License terms and copyright [as stated in the repository](https://github.com/oracle/oracle-db-examples/blob/master/LICENSE.md), as to any other open source project, then no need to worry. Feel free to contribute back, also.
You can break the string apart using a numbers tables like this: https://www.periscopedata.com/blog/net-promoter-score.html
Unfortunately I am not very familiar with SQL Server, which is why I didn't try to edit your query. Essentially, you need split_part, which I imagine sure sqlserver had...but I don't know. Regex helps, but you can get away without it probably. Wish I could help more!
No worries, I still appreciate it and it gives me another approach to work with. I wish the DB was 2016 it'd literally be one statement with no work.
Check that year(b.ModifiedDate) -1 is subtracting a year, and not a day. 
I manually created the rule and now feel like I should turn in my admin creds :( Thank you so much!
Check your SQL Server Configuration Manager, but sounds like your services.msc thingy got it set as automatic, it needs to be manual, otherwise it consumes too much resources and no point if not using it.
It's all set to manual on config manager and in services.msc. It's not a big problem, when I'm not on sql sever on my laptop I'm reading articles or watching videos, I'm just wondering what's going on. 
Sounds like that when selecting from sys.sysprocesses - it's using a different server than the one you need to use. It doesn't matter if the owner of the job is sa or not (but it should be), so seems like it's selecting crap. Please do not use COLLATION in the select statement. 
Check the SQL server agent settings, it might be setup to automatically start the SQL server. The setting is to start SQL server if it stops unexpectedly, not sure that this counts but worth a shot.
Your best way will be as Murica suggested and use a numbers/tally table. It gets fairly complicated because you're essentially creating a table valued function with which you CROSS APPLY to to return a row for every delimited value matched with each row of your original table. http://www.sqlservercentral.com/articles/Tally+Table/72993/ The actual code for the function* is in the "Figure 21:" section of the above page. Jeff Moden then goes on to show testing below with this: SELECT test.SomeID, test.SomeValue, split.ItemNumber, Item = QUOTENAME(split.Item,'"') FROM #JBMTest test CROSS APPLY dbo.DelimitedSplit8k(test.SomeValue,',') split Which in your case would be something like... SELECT a.[ID],a.[Question 1], split.ItemNumber, Item = split.Item FROM dbo.Verbatims a CROSS APPLY dbo.DelimitedSplit8k(a.[Question 1],' ') split
I'm getting closer. These queries getting me what I need but separately and I need them together. SELECT t1.LNR, t1.name FROM table1 t1 JOIN (SELECT Lnr FROM table2 WHERE ID = '5') t2 ON t2.Lnr = t1.Lnr WHERE t1.ID = '5' SELECT t1.LNR, t1.name FROM table1 t1 JOIN (SELECT lnr#2 FROM table2 WHERE ID = '5') t2 ON t2.lnr#2 = t1.Lnr WHERE t1.ID = '5' The output of this would be like 1st query - 3 | test3 3 | test3 4 | test4 5 | test5 5 | test5 6 | test6 6 | test6 2nd query 1 | test1 2 | test2 3 | test3 3 | test3 1 | test1 4 | test4 5 | test5 The ideal results would then be... 3 | test3 | 1 | test1 3 | test3 | 2 | test2 4 | test4 | 3 | test3 5 | test5 | 3 | test3 5 | test5 | 1 | test1 6 | test6 | 4 | test4 6 | test6 | 5 | test5
One more question, sorry. If I use the tally table can I see what order in position the word came
 CREATE TABLE TABLE_1 ( id INT, LNR varchar(10) , Name varchar(10) ) CREATE TABLE TABLE_2 ( id INT, LNR varchar(10) , LNR2 varchar(10) ); SELECT t1a.id, t1a.LNR, t1a.Name, t2.LNR, t1b.Name, t2.LNR2, t1c.Name FROM ((TABLE_1 t1a INNER JOIN TABLE_2 t2 ON t1a.id = t2.id) INNER JOIN Table_1 t1b ON t2.LNR = t1b.LNR) INNER JOIN TABLE_1 t1c ON t2.LNR2=t1c.LNR Where t1a.id = '5'
&gt; I would say I'm probably an advanced novice at this point &gt; Is there some type of test I should take to show at what level I'm at in terms of expertise? https://www.microsoft.com/en-us/learning/mcsa-sql-2016-certification.aspx 761, maybe 762, 767, and 768. 
Are you running Windows 10? With fast boot windows doesn't entirely shut down. I've noticed the same thing in my work PC, will test the theory on Monday if you don't beat me to it. I wonder if only restarting pc will do if 
There are no tests. Generally BI (reporting) has little to do with SQL. SQL Developer does all the code writing with .NET. DBAs in most companies don't do any developing, however I've been all 3 of them at the same time and it's a grey area. Find out what you wanna do, and stick with it. SQL Dev in terms of tests, nope, there aren't. I for instance specialise in query optimization and complex scripts, so would go online and do random tests about query optimization and read stuff to see if I have any blank spots. Then once you reach that god level, you can read books and more details articles that probably won't yield much in terms of improvement of what you already know but will help you think about the stuff differently. And I don't believe in certifications. I've got friends who studied hard and got all the certifications and the employer did not care, at the end of the day you gotta show them what you got and whether or not you get along with them ( if you are passionate, knowledgeable etc).
 SELECT source AS "TrafficTypes_Name" , week AS "Week_Ending" , COUNT(source) AS "Traffic" FROM infocentertracker GROUP BY source , week UNION ALL SELECT 'Learn About Us' , week , COUNT(source) FROM infocentertracker GROUP BY week ORDER BY week
Thanks, this is great but I need to have `Learn About Us` in the beginning of each week with total count of the sources. This is what I get with this query: Billboard 3/26/2017 4 Website 3/26/2017 4 Radio 3/26/2017 2 Billboard 4/2/2017 7 Website 4/2/2017 3 Radio 4/2/2017 5 Learn About Us 3/26/2017 10 Learn About Us 4/2/2017 15 Requested: Learn About Us 3/26/2017 10 Billboard 3/26/2017 4 Website 3/26/2017 4 Radio 3/26/2017 2 Learn About Us 4/2/2017 15 Billboard 4/2/2017 7 Website 4/2/2017 3 Radio 4/2/2017 5
&gt; but I need to have Learn About Us in the beginning of each week 1. you neglected to mention that 2. for some reason your ORDER BY is not working -- did you run ~exactly~ what i gave you? because `ORDER BY week` could not possibly have placed those two 'Learn About Us' rows at the end
Thanks for the help! I was thinking that I had tried this but couldn't remember so I used your code and didn't get what I was looking for. When I stopped the query, it had returned 112,863 rows instead of 7..lol. Here's maybe a better look at the data. Table1 ID | LNR | NAME -------------------- 5 | 1 | TEST1 5 | 2 | TEST2 5 | 3 | TEST3 5 | 4 | TEST4 5 | 5 | TEST5 5 | 6 | TEST6 Table2 ID | LNR | LNR2 ----------------------------- 5 | 3 | 1 5 | 3 | 2 5 | 4 | 3 5 | 5 | 3 5 | 5 | 1 5 | 6 | 4 5 | 6 | 5 I think you had a pretty good idea of what I was looking for. Basically I'm only using Table 1 to resolve LNR and LNR2 from table2 to the names. So LNR and LNR2 from the first line of table 2 would resolve to TEST3 and TEST1. These values would be pulled where the ID matches between the 2 tables. 
They do all of their development in EF? Is that common? I think it's easier for people to write raw SQL than it is to write good LINQ tbh.
If you still like the report development aspect of the job, perhaps you are looking into something more along the lines of a SQL Business Intelligence Developer. There are certifications you can take. Microsoft has a few that are great, but I wouldn't say certs are that important to get these types of jobs. Business intelligence (BI) is a pretty hot profession at the moment and very in demand (not as much as data scientists, but you'll work closely with them). Full disclosure: I may be biased because I started as a BI developer and have since moved on to Data Engineering. My career experience has been great. 
True. Using the ado.net interface is a billion times easier to understand than EF. There's no magic. It's all there. It's when you have a lot of ado.net code that you start to question the productivity. That's where EF can help. But there is certainly a learning curve.
Just checking then - if you shut down the service does not stop? That's been puzzling me for weeks but I've only just made the connection when seeing your thread. 
Hi, I am currently a marketing analyst ( mainly sql/ alteryx/ tableau/ excel). I want to develop my career into data engineering and have been working on python and R. Can you describe a little more about the transition that you went through and what was the key thing? I think i am on the transition already, that I am specializing into automations/ETL but I just wanted to hear from someone who already has been there.
Why would SQL snippets in .net code not scale?
I've never met anyone who just writes sql, but I've met lots of .Net developers who really should be able to, but can't do much more than write a simple query. I'd recommend looking at putting another tool on your belt so that you can position yourself better to have more opportunities to write sql. Check out visual studio community edition and look into some sample projects that build a web front end with C# on top of a sql database. There's probably a few AdventureWorks Sample projects out there still. Once you have a basic site running, start writing your more complex sql and rendering your queries onto that front end. You'll improve your back end game while building out the presentation skills. The certifications aren't super helpful unless you want to get into maintenance and systems imo. If you want to write code, you probably have to write a couple different kinds to get more time writing more sql professionally. One way to look at it is go to whatever job search board you like and search for jobs with SQL, stored procedures etc. Then look at what the other skills are that they require. Whatever looks like is the common denominator across those roles should inform your next step.
If the compiled/deployed code calls a stored procedure, the stored procedure and its underlying data structure can be changed without modifying the compiled code. If the SQL is within the compiled code, now the compiled code and database are tightly coupled and making changes to one without impacting the other is much more difficult.
True, but then you're putting business logic in the data layer and it's outside of source control. Our shop uses the procedure method you laid out and also the embedded sql scripts - both get cumbersome/messy very quickly. What I would give to have everything in EF...
SSDT SQL Database Projects. They work like any other projects in VS, and fully support any soruce control that VS does. If you're putting SQL code into your application because of source control then you're doing it wrong. Also, data tells the story of your organization, and if you're putting ALL of your business logic in the application then that means you have to do an application release every time you need to make a change in the database. Code first is the single worst way to approach database development.
There are definitely SQL dev certifications - see the SQL Server MCSAs. 2012 - 14 is very broad, but one of the 2016 streams should be far more relevant. I can't comment as to how much employers value them (probably equivalent to relevant work experience and, if they can be bothered to review it, below a portfolio), but they absolutely exist.
&gt;if you're putting ALL of your business logic in the application then that means you have to do an application release every time you need to make a change in the database. That's actually a really good point that I haven't considered. I've been meaning to check out VS Database Projects but haven't had the cycles at work, I think that might be the direction I'd want to go.
Depends but very unlikely you will be a viable candidate with using sites like cloudcademy and sqlzoo. Even Pluralsight will just give you bare minimal constructs and basics. You should absorb as much info as you can and practice at home with labs. Most all popular RDBMS systems are free including SQL Server for personal use. Good luck!
Yup! So the 3-4 extra seconds it takes to shut down might be worth it for a lot of people. Thanks for the suggestion! Hopefully more people are able to benefit from this. 
Can one participate in this if they're using Mac Os?
The worst part about DB projects is figuring out the nuances with database references, but once you do then database development, and deployments become cake.
i'm not trashing the product, i'm just trying to gain some insight from other users of the product to see if the value it is producing is actually worth its cost compared to other more cheaper solutions. 
Tableau hits a sweet spot for savvy users who want to make dashboards or explore data quickly without coding, SQL or clutter. SSRS is just a reporting tool - it's not for developing interactive dashboards. cognos is a full blown ETL/data modeling/reporting tool that seems to be overkill for most projects and time-to-market is terrible compared to Tableau's quick turn around and feedback. I justify it in terms of: 1. end users can use the tool. IT builds and maintains the servers, extracts and data queries and let the users loose. 2. analysts do their own ETL and combine data from disparate systems and formats usually without IT's help. This let's them get on with their analysis without waiting for IT to (over)-analyze and (over)-engineer a data model for them (without fully understanding requirements). 3. python and R are fantastic tools (both of which are supported by Tableau btw) but who wants to code or write from scratch something that can be done with a few clicks. In short, tableau fits the need of end users to IT to quickly get some analysis done without having to do a bunch of work. Sometimes you just want a play around because you don't even know what the questions is yet. Nearly all other solutions commit you to development cycles that end up not being needed. That said, I've often got stuck trying to do things the Tableau way and I'm always frustrated by the lack of fine-grained control over formatting. I nearly always switch back to SQL to get the data the way I want it. The way to justify it is to figure out how much time and effort it takes to do a single report. Then download Tableau and do the same report. Then multiply by 50 reports from 10 different departments. The math generally works out because end-users can do their own reporting.
My current organization just purchased tableau and is pushing it really hard. My experience is limited but we've really struggled with it. I work with the call center heavily and we used it for about a week and went back to excel and even avaya CMS reporting. The tool itself looks pretty cool and I'm sure you can do some neat stuff, but thus far the customization and drill down ability seem to be lacking. It also locks us in to looking at mostly averaged data, which isn't helpful all the time. A large part of our issues, more than likely, are organizational and not tableau itself. At the end of the day I still prefer using SQL, SSRS and even excel. Those are blank canvases vs tableau feeling like a already outlined picture you color in. 
My usual reax to your earlier question would be to sort if glaze over while thinking and trying to parse the problem...Which could look a lot like a deer in the headlights just totally lost to the interviewer.
Tableu Dashboards, and graphical data representations will give upper management a hard-on. Myself, as a data wonk, developer, etc I prefer hard numbers. So... in our organization upper management bought and paid for tablaeu without asking if we wanted it. I don't really work in it myself, since I am more on the ETL and development side of our team, but I feed data to the people who build stuff in it. They like it. It makes doing data presentation super easy and intuitive. Even though I prefer data, I have seen some extremely slick work, such as enrollment heatmaps based on geolocation data, etc.
Yes it's totally worth the cost if I am spending my company's money. If it were my money, I'd go PowerBI. That said, Tableau is significantly better than PowerBI and I wouldn't consider using PowerBI but for the attractive price. Anything else I've used like Qlik is just plain bad.
I feel like the hype is temporary, and management-induced. And the product will be replaced.
&gt;Also, do people that can meet that skill set routinely see $250,000 annual salaries? Because if so it's time to reinvent myself as a data scientist. If they also know the finance side, have a few years of industry experience, and can get a job at a hedge fund, yes. 
Well damn. With a little work I could handle the sort of technical requirements you describe, but the rest of it would be a bit harder. I'd be like the scene from the Wedding Singer when Adam Sandler interviews at the bank: 'Well, I like money. I have a little, and Id like to have more.... 😀
 CREATE TEMPORARY TABLE IF NOT EXISTS table2 AS (SELECT * FROM table1) In this case, you'd use the derived table liked the first user suggested. 
Excited to see declarative partitioning on the horizon. Too bad Columnar stores won't be making an appearance yet; definitely looking forward to that.
First things first, you're getting the chain of exceptions because of the fact that you're inserting a duplicate row in the "row doesn't exist yet" else clause before doing your initial insert. Remember that when triggering before an insert, the insert is still going to happen afterwards, so that insert will just create a duplicate row in the same table you're triggering on. Which means that it'll invoke this trigger again *before inserting the duplicate row*, and so on. That's why it recurses deeper and deeper until you run out of stack space. Second, you don't need to return after raising an exception. And your exception won't have "plate" defined, that probably needs to be NEW.plate. Here's a "fixed" version of the trigger procedure: CREATE OR REPLACE FUNCTION plate_check() RETURNS trigger AS $$ BEGIN IF EXISTS (SELECT 1 FROM car_rentals cr WHERE (plate, start_date, end_date) = (NEW.plate, NEW.start_date, NEW.end_date)) THEN RAISE EXCEPTION 'unavailable --&gt; %', NEW.plate USING HINT = 'Car is unavailable'; ELSE RETURN NEW; END IF; END $$ LANGUAGE plpgsql; Good luck!
Is the point of the trigger to prevent duplicate entries? 
You'd need to make use of the ROW_NUMBER or PARTITION BY functions (or both, as is common. You can also choose to order by multiple columns like offer desc, *random number*
Forgot to add MySQL in the title, sorry!
You can take a look at some of the analytical functions from SQL Server 2012: LAG() and LEAD(). Pinal Dave has a good [introductory blogpost](https://blog.sqlauthority.com/2011/11/15/sql-server-introduction-to-lead-and-lag-analytic-functions-introduced-in-sql-server-2012/)
Your first query could be: select product, customer, max(date), max(offer) from tablename group by product, customer order by product, customer (This assumes that your data is clean enough so that the highest offer is also the latest offer.) You could get the bid number for the highest, latest bid with: select product, customer, max(date), max(offer), count(offer) from tablename group by product, customer order by product, customer If you want the offer number for every row: select A.product, A.customer, A.date, A.offer, B.count(*) from tablename A, tablename B where B.product = A.product and B.customer = A.customer and B.date &lt;= A.date group by A.product, A.customer, A.date, A.offer order by A.product, A.customer, A.date, A.offer 
the student table contains program number which is also in the program table, that should be ok right?
Sorry I missed that, yes that's fine - just the three tables will do the trick. So once you've identified what tables you're joining, and which fields you're joining them on, it's just a matter of getting the syntax: https://www.w3schools.com/sql/sql_join_inner.asp
select * from students_table a left join programs_table b on b.program_number = a.program_number where program_code = ‘BSC2’ not sure if this is correct, obviously this query would only pull students in the BSC2 program. Would adding and program_code = (select * program_code from students_table a left join programs_table b on b.program_number = a.program_number where program_code = ‘BSC2’) give me the correct answer? 
&gt;not sure if this is correct, obviously this query would only pull students in the BSC2 program. I don't know either! And that's the beauty of SQL. You can check it! Your second question would probably work but I don't see why you need a sub-query. Just do another join and add a second clause to the where statement = 'BSC2' That should work, yes? Why do you need a sub-query? 
ok sub-query because when you said "just add another join" meanig there are 3 joins, i just got completely lost. So i did test the query separately (with the appropriate syntax) and it ran fine. &gt;Just do another join and add a second clause to the where statement = 'BSC2' this confuses me!
&gt;the beginning that you had limited knowledge of MySQL. PS, I don't need to know MySQL and you don't need to know MS SQL to solve problems with SQL. The syntax changes, the logic does not.
i think i got it ! Select Name, Number, Hall_Name From students LEFT JOIN address ON students.postal_code = address.postal_code Where place_name = ‘Toronto’ AND Program_number = ‘BSC2’; 
If you know the operation that just finished, use something like select top 1 * from mom where part = @part and operation &gt; @operation order by operation This is assuming @part and @operation are variables that contain the values of the one you just completed. There's probably a better way to do it based on what information you have but that should give you a start.
I would expect if you ran that, it would give you no results because 'BSC2' is the programme_code but you're applying it as a filter to programme_number. Unless the programme code is exactly the same as the programme number, which I wouldn't expect it to be, that wouldn't quite work. Looks like you're close though, I think your original plan of joining the three tables works better.
https://www.amazon.com/Data-Analysis-Using-SQL-Excel/dp/111902143X/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1491791686&amp;sr=1-1 
Mayhap yes, mayhap no. I don't know the data. Sounds plausible.
Are the times you see the file being modified semi consistent? If so, you could guess when to run SQL Profiler and trace the source of what is touching that file. Also what you could try to do, try this and adjust the part of the where clause below to match the backup file name. Example, my backup file is fakefile.mdf, so I'd put &lt;'%fakefile.mdf%'&gt; down below. SELECT * FROM ( SELECT DB_NAME(p.dbid) AS DBName , OBJECT_NAME(p.objectid, p.dbid) AS OBJECT_NAME , cp.usecounts , p.query_plan , q.text , p.dbid , p.objectid , p.number , p.encrypted , cp.plan_handle FROM sys.dm_exec_cached_plans cp CROSS APPLY sys.dm_exec_query_plan(cp.plan_handle) p CROSS APPLY sys.dm_exec_sql_text(cp.plan_handle) AS q WHERE cp.cacheobjtype = 'Compiled Plan' )a WHERE text LIKE '%SNIPPET OF SQL GOES HERE THAT IS PART OF THE QUERY YOU WANT TO FIND%' ORDER BY dbid, objectID If you can't run profiler, I'd also look at SQL Job history and see what jobs ran / what steps ran during those time windows correllating with the time stamps on the file. You should be able to see when the job began and ended, then you can scour the job and see what step started / completed when. Last ditch effort, get spwhoismonitoring, free t-sql code. Have it log to a table during the time you are investigating, similar to the trace. It may help.
this guy backs up!
Awesome! So, SQL Profiler is just a real time event logger? I set it up to do a trace while I entered your quoted code...The quoted code didn't find anything, but when I went back to Profiler to search for the filename it did show up... It looks like I have a ton of data...OK to let this run from ~5p - 4a? The event usually happens at 3a. I checked back in SQL Management and looked at the Maint Plan for this DB... There is only mention of the three day save/delete process that we have, but nothing implemented 1-15 and/or written to that file. I also checked Task Scheduler and no luck there.
checked Task Scheduler and no luck. I feel that it's a plan somewhere in SQL since the file names are identical to my other (correct) backup files...
&gt; OK to let this run from ~5p - 4a? No, I'd look into a way of automating to start and stop profiler so it doesn't run more than a min or two tops. https://www.sqlservercentral.com/Forums/Topic576752-146-1.aspx
 ORDER BY month("Week_Ending") , rowtype ERROR: function month(text) does not exist LINE 21: BY month("Week_Ending")
Leaving Profiler running constantly is a good way to bring down your server. I only use it in non-production environments, in situations where I'm trying to figure out what queries an application is throwing at my instance. I'll start Profiler, perform the task in the application, then stop the trace and review results.
Exactly this. Use this information to narrow down exactly when it was created, and from that you can cross reference any Agent Jobs, or based off credentials, external connections.
I think proper indentation (or at least my own preference for indentation style) helps display the issues /u/r3pr0b8 pointed out in what you've tried. SELECT EmployeeID , E.FirstName , E.LastName FROM Employee E JOIN ( SELECT PurchaseID -- Why are you selecting this field?? -- There needed to be a comma here between these fields ROUND (SUM(PurchasePrice * Quantity),2) AS TotalPurchases FROM PurchaseItem PI JOIN Purchase P ON PI.PurchaseID = P.PurchaseID GROUP BY EmployeeID ) -- NAME YOUR SUBQUERY! -- ON condition for the subquery it missing. How are you tying this unnamed -- subquery to the employee table??? -- P does not exist out here. -- any reference to P needs to be INSIDE your subquery where P actually exists. WHERE P.PurchaseDate &lt; 01/01/2015 -- You're looking for a purchase date that's less than 1 divided by 1 divided by 2015?? AND P.PurchaseDate &gt; 12/31/2013 -- i.e. this is not how you write a date constant. [MYSQL Date constants](https://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html) Your ERD doesn't give data types, so I'm making educated guesses.
I think you're right. That's definitely a bit more organized. However, it still doesn't return any results in 2008. Works great in 2014 tho... ~~One other note is that I need the Having statement because for whatever reason lots of differences are incredibly small (0.001) and I don't want to bother with them.~~ But I'm probably going to adopt some of the other changes you suggested. Thank you! I can just use: &gt; WHERE HT1.Date_Loaded = @date1 &gt; AND HT2.[Date] &lt;&gt; @date2 &gt; AND ABS(cast(HT1.Amount as float) - cast(HT2.Amount as float)) &gt; 0.1
Hey thank you so much! I'm going to try to fix it. 
Yes, they are. All the differences in formatting, especially between versions of SQL (platforms or otherwise) make them some of most annoying fields to deal with.
&gt;should I just look at Business Intelligence or is this harder to get into than database administrator/database developer? I would suggest that BI would be a much easier route forward. DBA stuff will expect a lot more boring physical layer stuff you're not so keen on, and entry level, *purely* database developer (as opposed to full stack including DB), stuff is hard to find to the point of being almost non-existent. BI developer would be a more likely stepping stone to full db dev, would suit your ad-hoc SQL query/stored proc skills well, and (at least where I am) is a pretty in demand skillset. Spend a couple of weeks teaching yourself some kind of visualisation/reporting tool that fits in with your current knowledge and you'd be a good looking candidate for even intermediate BI roles. Where I live, MS SQL seems to be the preferred platform, maybe look at building your skills in that? SQL is SQL whichever platform you use so it shouldn't be a huge investment in time to get to the point where can put T-SQL on your CV with a straight face. disclaimer: I live in a small country in the middle of fucking nowhere so ymmv on the job market front.
Thank You. Yeah I'm still learning hard. The 2 in the MaxAmt is a typo. Supposed to just be "MAX(TotalPurchases) as MaxAmt". 
 SELECT E.EmployeeID, FirstName, LastName, MAX(TotalPurchases) as MaxAmt FROM Employee E JOIN (SELECT EmployeeID, ROUND(SUM(PurchasePrice * Quantity), 2) TotalPurchases FROM Purchase P JOIN PurchaseItem PI ON P.PurchaseID = PI.PurchaseID GROUP BY EmployeeID ) SumQuery ON E.EmployeeID = SumQuery.EmployeeID GROUP BY E.EmployeeID, FirstName, LastName Is what I tried and got a working result. Sorry about the formatting but the console shown is small so I just group stuff together like that. It's a mess I know.
I'm not sure but I would image that the database uses the standard service account for SQL server to send the file if you are using a sql server login. It's normally named MSSQL$INSTANCE (with instance being the instance name). Check the NTFS permissions for the folder where you have your PDF. 
The file is local to the SQL server. There is a setting in the config of the report scheduling application which uses an UNC path.
Hey, thanks The account does have Execute permission. The proxy account also has appropriate file permissions to the location of the PDF.
just a cool case when syntax tip, you can save a lot of space/typing if you write it like this: CASE [GradeLevel] WHEN 'PS' THEN '30' WHEN 'KG' THEN '29' WHEN 1 THEN '28' etc..
Would EXECUTE AS work? https://www.mssqltips.com/sqlservertip/1227/granting-permission-with-the-execute-as-command-in-sql-server/
&gt;Do SQL Server certifications add value..? I wouldn't sweat the certifications too much. With something like DB development it should be readily apparent to the hiring manager from a sample of your code and talking to you whether or not you have the level of skill you claim. You have enough experience with DB development that I don't think you need to worry too much about getting a certification just to get your foot in the door. &gt;...is there a good chance I'd need to take a paycut for an entry level database position? I can't speak to where you live directly (in Southwest US) but in my area you'd probably be on the high end of salary at $75k for a DB developer. $60k to $65k is a little more standard for where I live. &gt;...should I just look at Business Intelligence or is this harder to get into than database administrator/database developer? BI could be a great direction to go if you don't mind learning a tool like Tableau to build visualizations in. Bear in mind that a lot of what comes with working in BI is requirements gathering and working with stakeholders to get things exactly right -- you may spend a lot less time writing SQL and making visualizations than you might think. &gt; ...is it normal to combine database developer positions while requiring knowing multiple other technologies/general programming languages/application development? I think that it depends a lot on the company and their current staffing as well as the management for IT. Several of the companies I've worked for have had dedicated DB people, a few others have had just 'backend dev' and one had just engineers. Hard to say for sure either way.
I'd recommend against using user-defined functions unless you have a fairly advanced knowledge of SQL. Like a lot of things in SQL, they're deceptively simple on the surface, but can (and will) cause significant performance hits if you don't know exactly how they work. For example, at my last job there was a query that we let run for 23 hours before killing it. After I replaced the UDFs (there were three) with cross applies, the query only took 3 seconds to run.
&gt; The SQL server service is running under a domain account which does have permissions to this file. If I log into SSMS and execute the SP with this account it runs successfully. &gt; Hey, thanks The account does have Execute permission. The proxy account also has appropriate file permissions to the location of the PDF. &gt; The file is local to the SQL server. There is a setting in the config of the report scheduling application which uses an UNC path. All of these things are supposed to be true, and may look like they are true, but at least one isn't true or it'd be working. I've run into this maybe 10 years ago and I'm trying to pull the details back... Which version of MSSQL are you running, and which version of windows server is the server MSSQL is running on using?
You don't need a string. You need to declare an array of string.
Answer revised based on the discussion below: Create table #List (Item nvarchar (200)) Insert #List(Item) values('Item1'),('Item2') SELECT Code1, Code2, Code3 from Table where Item IN (select Item from #list) As far as I know there is no built-in support for an array or list in t-SQL
/u/LinkMetga gave the best answer. A memory table would do exactly what you need.
This is a table variable. It doesn't create a full table and it works better than a temp table
You're not getting the maximum purchase by an employee because you're still using the sum of all purchases by an employee in your subselect. That will not get you the greatest purchase by an employee. You need to group by both employee id **and purchase** in your subselect to get a list of how much a given purchase by an employee totalled out to. **Then** taking the max of that by the employee should give you the result you want. 
Table variables are not "memory tables". * If large enough, table variables *will* spill into TempDB. But they're still scoped to the current batch. * If small enough, temp table can be cached entirely in memory (while also being present in TempDB). They persist between batches (not across connections). Table variables often wreck execution plans because they have no statistics - the query optimizer assumes one record (up to and including SQL Server 2012) or 100 records (2014+) regardless of what's actually in your table variable. Temp tables get statistics and can be indexed just like any other table.
can you elaborate?
Only caveat is that I would make sure you look for missing values in GradeID - that inner join will not produce results for missing IDs, and could cause more trouble down the road if s/he doesn't understand table relationships.
I love you &lt;3 I've worked with too many people that think "table variable, memory, fast!" and "temp table, disk, slow..."
Awesome. Thanks for sharing, too be honest I've always just used table variables. I have an unfounded phobia of temp tables. 
Your phobia is somewhat misplaced. It's table variables that you should be worried about if performance is a concern. Personally, I dislike table variables when I'm writing SQL (ignoring the impact on the execution plan) in large part because they feel clunky when declaring &amp; working with them. The syntax and everything else for temp tables is identical to regular tables, so it's one less variation on things that I have to memorize.
Rather than using a temp or memory table I have done the following in the past: SELECT @List= 'value1, value2, value3 ' SELECT @List = REPLACE(@List, ',',''','''); Select * From Table Where Item IN (''' + @List + ''')
Start with w3schools online, or the WiseOwl video tutorial on youtube
For free : http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can submit exercises online with feedback.
I learnt with codecademy on my own with no prior experience. Was enough to get a senior analyst job and i use it every day
Microsoft provides SQL server material for free through Microsoft Virtual Academy. That is where I started really digging in. [MVA] (https://mva.microsoft.com/) As for choosing a Stack, I currently work with SQL Server and Postgres, and as a result or Wordpress work dabble with MySQL. Any you choose will be good, I would lean more towards Postgres and SQL Server if it were my pick though
Just to ask the question, this is what you are doing to test that account? EXECUTE AS LOGIN = 'Domain\serviceAccountName' EXEC dbo.MyEmailProc; REVERT Second verification question - you are using the SQL Server Agent's account and not the main SQL Server account?
Awesome, thanks :) 
Right, CTEs function alright in SQL Server but if you start nesting them on top of each other they start to get very slow. I've taken a query written with 5+ CTEs, each referencing the one before, and changed them all to temp tables, and watched the performance skyrocket. The other benefits of temp tables over CTEs is that they are persisted for use across many queries, and you can examine their contents directly. Pretty much the only time I use CTEs anymore is if I want to use an analytical function like row_number() and select based off the result. Anything more complicated than the simplest single-subquery and I use temp tables for max readability, performance, and maintenance.
Also former Econ grad who had to teach myself SQL for a job. It's a lot easier than you think, particularly if you are more interested in working in a job that requires you to use SQL as one of your job functions rather than going the SQL developer route. I used MySQL since I was learning on my desktop machine and MySQL is super easy to setup for a local environment. I downloaded a simple data set, much like this: https://github.com/datacharmer/test_db and played around with it. Basically, I thought of a question I wanted to ask using that data and then googled to see how I would go about answering that question. Took about a month to get really comfortable querying and manipulating data. If you want to go more in-depth just grab a textbook. When I was going a bit deeper in database development I used Modern Database Management from Hoffer, but I imagine some people will have a better textbook(s). 
Thank you for that. :) I'm still new and my teacher taught us to write it all the way out, but I'll start doing it that way. I can still read it easily and it's faster. 
Fair play, but no position titled "senior analyst" *should* have accepted that unless you're very good with a lot of non-SQL stuff and mysteriously never learnt SQL properly.
PhD with lots of stats helped
Is there an easy way to do forwards *and* backwards migrations using DB projects? I'm seeing easy ways to deploy your changes but haven't found anything that allows my changes to stay reversible, compared to the Up() and Down() migrations available in EF Code-First projects.
Oracle is offering a free 12 week course right now. https://blogs.oracle.com/developers/learn-sql-with-this-free-online-12-week-course
What is an "old table"? As an application developer, a table is either used by the application or it's about to be deleted. I don't need to remember. Standard protocol is to remove unused tables. 
Aw, crap, I hate it when I get answers that make it obvious I don't know what the hell I'm doing. Can you explain how over() works? And of course, the important bit that I forgot to mention - if we're creating a field of numbers to sort by, I want the first row for each root node to be row number 1.
Nope, we make temp tables that are needed for a few months, maybe a year and then the tables can be dropped. Keeping track of that is hard, was hoping their was a standard procedure we could adopt to set a delete in 1 year meta tag and then i can search for those tables and drop them once a month.
We have the same problem. Part of it is caused by too many people have the ability to create such tables. We are cracking down on privileges first and foremost. If you aren't responsible enough to keep a clean database, we'll keep you out of it. 
I would have thought it should sort the same, so that's odd. Are the fields not fixed width or something? If they are, you should be able to use REPLACE to strip out the forward slashes and periods, then CAST it as an integer (shouldn't really be required) and ORDER BY that. Though it sounds like this *should* have been dealt with by optimising the query (index support and query plan examination) in the source system, which is unfortunate.
I really hope that Amount is not a currency, decimal or integer. Floats are entirely unsuitable for any of those. 
Often times database developers will use temporary tables if needed for complex ad-hoc queries, massaging of data, moving data between tables, etc. We name our tables TEMP_&lt;NAME&gt;_MMDDYYYY, and we generally go through a cleaning session once a year around the Fall as many of our company's business/leadership/special projects are completed by then, which is cause for a lot of the temporary tables we use. Additionally, we do a lot of Research and Development at the company I work for, which also tends to produce a multitude of temporary tables for the short life of the R&amp;D project. 
WITH _CTE AS (SELECT 'abc' AS Str UNION SELECT 'bcd' UNION SELECT 'cde') SELECT * FROM Table T JOIN _CTE C ON T.Col = C.Str is an alternative method of doing it.
Thank you for your insight! &gt; With something like DB development it should be readily apparent to the hiring manager from a sample of your code and talking to you whether or not you have the level of skill you claim. My worry is actually getting to the interview process. Nothing in any of the job titles I've had declares I have knowledge of databases, however it has been predominately what the last two have been made up of. The certifications were more to get recruiters attention to get me to the interview process, I don't have any worries when it comes to the interview itself. &gt; BI could be a great direction to go if you don't mind learning a tool like Tableau to build visualizations in. After a couple of days of research, this seems to be the route I'm leaning towards. Is Tableau widely-used? Or should I learn something else? There seems to be a whole host of different data visualization tools. I've used R in the past for simple data visualizations for a few business presentations I've had, nothing too complex, bar graphs and scatter plots. 
Very nice. SQL Fiddle is broken more often than not. I wish this did MS SQL as well.. maybe later with a "Express" version of the Linux edition can make it's way in? 
If you are looking for an Oracle playground I like using https://livesql.oracle.com. You need to create an Oracle account but it is a reliable service.
Thanks guys! Im glad that there are others like me!!!
Just enrolled fam. thank u so much!!!
thats epic 
tablename_RIP20170417_OwningDeveloperNameIfApplicable Using the date that it is okay to kill the table completely. That or keep a table with a list of the tables and the date they can be safely dropped.
You may want to use power query in excel. Then use power query to divide the hierarchy id into multiplied columns (split by slash). Then you can sort in power query and then remove the hierarchy Id columns . 
Yup. The company I get my source file uses them, I can tell. Rounding issues vary from day to day for a number that should be the exact same. I.e 10.86 one day 10.87 the next even tho I know the value wouldn't have changed. Floats are just approximations. I think I need to call and explain that to them.
Maybe this is easier to understand if you know what the data represents. I should have explained this earlier. Each root node represents a complete SKU, the rest of the hierarchy is the bill of materials that makes up that SKU. SKU is made of level 1 nodes 10, 20, 30; level 1 node 10 is made up of level 2 nodes 10, 10.1, 20, 30; level 2 node 10 is made up of etc, etc, etc. I want to sort the bill of materials for each SKU in their hierarchyid order. I pull multiple SKUs at once from my query, and then I pull different SKUs and add them to the spreadsheet, and so on. I don't want something that iterates through the entire set of query results, I want something that would look the same if I pulled the SKU data by itself or if I pulled the SKU last in a list of 500 SKUs.
I'm having speed issues - these files are 40-60mb each and I often have time to get coffee while waiting for them to calculate. From what I've heard about Power Query, it is likely to make my speed problem worse, not better. Is that inaccurate?
Sometimes, but usually it takes less time. There is a little bit of hurdle to become familiar with power query. But I use it more often than excel functions alone. 
change it so that the CASE is inside the SUM SUM( CASE WHEN invoice_date BETWEEN ADD_MONTHS(SYSDATE,-90) AND SYSDATE AND Amount_Gross &gt; 0 THEN Amount_Gross ELSE 0 END ) AS Prior_3_Months
That did the trick. Is there a reason why it didn't like it in side the case, or is it just a oddity that Should be expected?
Does this help? SQL order: /191987/10/ /191987/20/ /191987/30/ /191987/70/10/70/ /191987/70/10/70/10/ /191987/70/10/70/10/10/ /191987/70/10/70/10/20/ /191987/70/10/70/10/30/ /191987/70/10/70/10/40/ /191987/70/10/70/10.1/ /191987/70/10/70/10.1/10/ /191987/70/10/70/10.1/20/ /191987/70/10/70/10.1/30/ /191987/70/10/70/10.1/40/ /191993/70/10/200/100/ /191993/70/10/200/100/10/ /191993/70/10/200/110/ /191993/70/10/200/110/10/ /191993/70/10/200/120/ /191993/70/10/200/120/1/ /191993/70/10/200/120/1.1/ /191993/70/10/200/120/10/ Same data, sorted in Excel: /191987/10/ /191987/20/ /191987/30/ /191987/70/10/70/ /191987/70/10/70/10.1/ /191987/70/10/70/10.1/10/ /191987/70/10/70/10.1/20/ /191987/70/10/70/10.1/30/ /191987/70/10/70/10.1/40/ /191987/70/10/70/10/ /191987/70/10/70/10/10/ /191987/70/10/70/10/20/ /191987/70/10/70/10/30/ /191987/70/10/70/10/40/ /191993/70/10/200/100/ /191993/70/10/200/100/10/ /191993/70/10/200/110/ /191993/70/10/200/110/10/ /191993/70/10/200/120/ /191993/70/10/200/120/1.1/ /191993/70/10/200/120/1/ /191993/70/10/200/120/10/ 
That's quite distressing. I'm sorry. My initial suggestion would be to just go wild with CASE, INSTR and SUBSTR in a scalar function you can call for each input row, and then benchmark that. I don't think you'll get great performance whatever you do though, unfortunately this isn't likely to be a set based operation. I expect you can use FORMAT to push stuff into 0000.0 format, though you might need to cast it to DECIMAL (5,1) (5 digits total, 1 after the d.p.) or something. http://stackoverflow.com/questions/10581772/how-to-split-a-comma-separated-value-to-columns Here's an example of a function doing something pretty similar.
&gt; doesn't create a full table '#tableName is a temporary table, not a table variable. Check tempdb. There's a very real table there which will persist until the session/procedure ends, unless MSSQL implements things very differently. A table variable is something like DECLARE @myTable TABLE (myInt int, myVar nvarchar(50)), i.e. a table held in a variable. A CTE is the narrowest scoped, as it's destroyed immediately after the statement it's used in IIRC.
You mean like this? That puts it in yet another order: 19198710 19198720 19198730 191987701070 19198770107010 19198770107010.1 19198770107010.110 19198770107010.120 19198770107010.130 19198770107010.140 1919877010701010 1919877010701020 1919877010701030 1919877010701040 1919937010200100 191993701020010010 1919937010200110 191993701020011010 1919937010200120 19199370102001201 19199370102001201.1 191993701020012010 
IMHO use actual temp tables or give each project its own schema. Shit, put that stuff into its own DB. Just keep it well away from prod/application tables.
Create a table in your config/setup/whatever schema containing the temporary table name and an expiration date. Set up a weekly job to return the content of that table where the table name exists.
HELLO, YOU WILL WANT TO USE JOINS OF SOME SORT. PROBABLY INNER OR LEFT OUTER. THIS IS VERY SIMPLE STUFF, SO I'M NOT GOING TO ADVISE FURTHER WITHOUT SEEING AN ATTEMPTED QUERY.
So DISPATCHERID and REQUESTERID are both thrown into a table called CLIENT-T and the name of that column is CLIENTID? Who designed this... I really hate when column names aren't consistent across tables when used as keys. It's infuriating for anyone trying to understand it.
If the DispatcherID and RequesterID both relate back to ClientID then you will need to join Table1 and Table2 on OrderID and then join Client-T twice, once on ClientID and DispatcherID and once on ClientID and RequesterID
How is it that you want it sorted? I see how it is in SQL, and how it is in Excel, but how do you want it in Excel?
So, this is probably off topic. But I was going to look through post/comment history to see how many times OP requested help with SQL, and lets just say OP visits some questionable subs. Is this your homework OP?
I want it sorted the way it is in sql.
OK, so I'm seeing the issue now. So you're going to need to parse each individual ID out and order them to achieve what you're after I think. A simpler way would be to isolate the problem set (in your example its 5th), then parse that out and row_number it, then blend the row_number back in.
I mean if you just partition by the ID it is always going to sort like it does in SQL... 
I would use temp tables. First create a table of ID, Name, Wins then create a temp table of ID, Name, Total Games then join them together. I usually use MSSQL so I am not 100% sure if this advice would pertain to you. 
To get the total wins temp table I would also create two separate temp table one for the wins total and one for the loses table then join those to the ID table and sum the results to get your total matches.
I'm pretty new to SQL myself but want to try and help. I'm only familiar with Microsoft SQL so translate this however you need. I love all the different response I've seen already! ;with Crapo as ( select P.player_id,P.name COUNT(ISNULL(M.WINNER,0) AS 'WINS' ,COUNT(ISNULL(M.LOSER,0) AS 'LOSES' from MATCHES M LEFT JOIN PLAYER P ON P.ID = M.player_id GROUP BY P.PLAYER_ID,P.NAME ) SELECT c.* ,C.WINS + C.LOSES AS 'Matches Played' FROM Crapo C
I've never used NOT EXISTS but probably should get into the habbit more... It's probably the best but perhaps checking where LEN(*)&gt;1 OR (COALESCE(*,-) &lt;&gt; -).... putting this down on paper makes me smash the keyboard in regret. I have to punish myself for not going to the gym so trolls have at it.
100% agree. The company I work for promoted me from within a completely different department based on the GENERAL KNOWLEDGE these videos gained. Befriending other developer's never hurt either :) 
I dunno! Haha I just kind of figured that the player.id and the player. Name were together because theyre in the same table before the join
Wow didn't even notice that. That was probably an error I made just before copying it over to Reddit. It should have been winner and loser to get a final count of games played. Thanks for pointing it out though!
Will the code below work? Tonight is the first time I've started posting in along time and completely forget how to format my code or be of any use. I apologize in advance! Select product,customer,date,offer ,MAX(offer) OVER PARTITION BY (PRODUCT,CUSTOMER,OFFER ORDER BY DATE DESC) AS 'OFFER#' * from table
The issue you are running into, besides missing the **GROUP BY** for *Name*, is that you are counting how many *Winner* and *Loser* values are not **NULL**. Since each of the four players have been in one match, there will always be one winner and one loser. **Alternative 1** -- Qualifies the count to ensure the player is the winner/loser. SELECT PLAYERS.ID , PLAYERS.NAME , COUNT(CASE WHEN PLAYERS.ID = MATCHES.WINNER THEN 1 END) Wins , COUNT(CASE WHEN PLAYERS.ID = MATCHES.WINNER THEN 1 END) + COUNT(CASE WHEN PLAYERS.ID = MATCHES.LOSER THEN 1 END) Total_Matches FROM PLAYERS LEFT JOIN MATCHES ON PLAYERS.ID = MATCHES.WINNER OR PLAYERS.ID = MATCHES.LOSER GROUP BY PLAYERS.ID, PLAYERS.NAME ORDER BY 3 DESC; **Alternative 2** -- Creates two joins to determine count values. SELECT PLAYERS.ID , PLAYERS.NAME , COUNT(wins.id) Wins , COUNT(wins.id) + COUNT(losses.id) Total_Matches FROM PLAYERS LEFT JOIN MATCHES wins ON PLAYERS.ID = wins.WINNER LEFT JOIN MATCHES losses ON PLAYERS.ID = losses.LOSER GROUP BY PLAYERS.ID, PLAYERS.NAME ORDER BY 3 DESC; I'm not sure of the data you would be going against, but here's my concern with the solution provided by /u/Eleventhousand. It aggregates the data on the records available in the *MATCHES* table, but will cause an issue if a player has never won or lost a match. If a player has never won a match, both the *Winners* and *TotalMatches* columns will result in a **NULL** values. If a player has never lost a match, then the *TotalMatches* column will result in a **NULL** value. This can be corrected by combining your logic using a **COALESCE** with his overall logic resulting in: -- /u/Eleventhousand Query Corrected SELECT p.ID, p.Name , COALESCE(w.Winners, 0) Wins , COALESCE(w.Winners, 0) + COALESCE(l.Losers, 0) AS TotalMatches FROM Players p LEFT OUTER JOIN (SELECT Winner, COUNT(*) AS Winners FROM Matches GROUP BY Winner) w ON p.ID = w.Winner LEFT OUTER JOIN (SELECT Loser, COUNT(*) AS Losers FROM Matches GROUP BY Loser) l ON p.ID = l.Loser; All of these examples and a modified data set to illustrate the issues are available [here](http://sqlfiddle.com/#!15/c82e7/3).
Since this is a weekend project, I'm assuming these are likely already on your todo list. Nice work so far. Suggestions: 1. No way to resize result set to view easier. 1. Results of multiple queries are displayed as essentially 1 result set. 1. No way to permalink an example.
Kyle772, can you tell us where you got this data and how you came across this question? Was it a question from a course or something? 
Your issue isn't the case. Your issue is the group by. Any single field referred to the select portion of a select that contains a group by needs to be in that group by list. I'm assuming that invoice date was already in your group by. When the case is changed to have the SUM outside of the case, then you're summing the fields within and therefore you don't need to have them in the group by. Are you really wanting to go back 90 MONTHs? that's what you're doing now with Add_month( field, **-90**) If your field name as prior 3 months indicates your intention then you want to ADD_MONTH( SYSDATE, -3) 
I have no idea why any of the SQL statements that you folks are using have a LEFT join. A LEFT join indicates that I want a row in my results for every row in the table on the LEFT side, whether or not there is a row in the table on the right. That is not the case with your query instructions. In the query instructions, you want the students from some part of Toronto that are in enrolled in program code BSC2. Anything outside of these two qualifiers should **not** be included. That's a JOIN, or more explicitly an INNER JOIN. SO.. result SELECT Student.Name , Student.Number , Student.Hall-name FROM Student INNER JOIN Address ON Student.PostalCode = Address.PostalCode INNER JOIN Program ON Student.Program_Number = Program.Program_Number WHERE Address.Place_Name LIKE '%Toronto%' AND Program.Program_Number = 'BSC2' Either linked table could appear first, and you can keep adding tables with more JOIN statements to link whatever you need to pull in. I went with address next because it was next in the instructions. Also, the where connection to Place_Name needs to be a LIKE because your instructions are that the place name **contains** Toronto which is different from place name IS Toronto edit: dyslexic 
Appreciate the response. Several of the DB's were set to 1MB autogrowth so I'm trying to get that changed. 
I'd be interested in this too, I feel your pain. There is a big gap between the theory and what people actually do. I have been practicing my SQL... But a friend showed me one of his reoccurring queries yesterday and it blew my fucking mind. 
Any lab recommendations? I have a VM running an RDS set up so I can practice but the labs I've found aren't much more advanced than SQLZoo. 
Most every person who gets thrown into the accidental world experiences the same growing pains. The best advice I can give you is a good training course or lots of reading. Find something you don't know while reading an article? Take a moment to read on that. Find something in that article you didn't understand, google that too. Work your way down the rabbit hole. Eventually, you'll learn and pick things up. I'd say the things you're looking at are on the lower end of the intermediate database administrator spectrum. So don't feel bad if you feel out of your depth at the moment. The knowledge will come and you'll never stop learning. Also, I love to read the top answered questions on stack overflow, always learn something new. 
I would start here. https://www.sqlskills.com/ Go through the blogs and the training stuff that is on the site. Once you know what a lock is, a latch makes more sense. A buffer is memory related. For example a buffer cache is just a query result (data pages) in memory. The sqlskills site will help you to put all of this together.
my advice is stay away from those geeky papers, ask specific questions, look for answers you actually understand. You are going to understand more with time, there's no easy way. That "greek" example looks convoluted even after a software engineering degree and 3 years in IT consulting A latch is one bit, like a traffic light regulating access to a road (buffer) which any task can use to get to the disk. Accessing the disk is I/O, the buffer (road) being in an I/O request means it is waiting for data transfer to happen (a task down the road) before another task can go. So basically your task have to wait if the buffer is busy, any long waits (you see from some monitoring interface I guess) may came from disk subsystem problems (server storage isn't a single disk usually) like if traffic go crazy there's something wrong down the road, but saying it in super-technical lingo makes you look cool
&gt; Occurs when a task is waiting on a latch for a buffer that is in an I/O request. The latch request is in Exclusive mode. Long waits may indicate problems with the disk subsystem. Others have given explanations on SQL Server specifics concerning this paragraph and what everything means, if you'd like to know the higher-level Computer Science concept behind why SQL Server implements this in their database architecture, then the word you're looking to google is Concurrency (and in this case, specifically, protecting against Race Conditions). If you're enthralled by such concepts, it may be worth picking up a book or two and going through some C programming to supplement your database learning, as C will teach you a good amount about how memory and I/O operations work at a pretty low level.
Thanks for the suggestion. I finally was able to get it done after a long break from it. The main issue I was having was that each table reference needed to have a where clause for the ID. The LNR values could be used in rows with different IDs and that was causing too many returns. Here's how I did it. select t2.lnr , t1a.name , t2.lnr2 , t1b.name from table2 t2 JOIN table1 t1a ON t1a.lnr = t2.lnr JOIN table1 t1b ON t1b.lnr = t2.lnr2 where t2.ID = '5' AND t1a.ID = '5' AND t1b.ID = '5'
Thanks for the suggestion. I finally was able to get it done after a long break from it. The main issue I was having was that each table reference needed to have a where clause for the ID. The LNR values could be used in rows with different IDs and that was causing too many returns. Here's how I did it. select t2.lnr , t1a.name , t2.lnr2 , t1b.name from table2 t2 JOIN table1 t1a ON t1a.lnr = t2.lnr JOIN table1 t1b ON t1b.lnr = t2.lnr2 where t2.ID = '5' AND t1a.ID = '5' AND t1b.ID = '5'
&gt; If you're enthralled by such concepts, it may be worth picking up a book or two and going through some C programming to supplement your database learning, as C will teach you a good amount about how memory and I/O operations work at a pretty low level. No disagreement and solid advice, but for someone just entering the world of SQL, I would be worried they could spread themselves too thin trying to learn everything. 
Correct, I should have been more specific in my answer. It is the choice of join and the counts chosen. But since we are doing counts of either wins or losses in his original query then, we will count both occurrences of each player in the table. 
Need to know what you're table looks like, bud.
I also noticed that my original answer worked by chance only. That's what i get for doing SQL after a few whiskeys right before bed. I ended up with the same answer as /u/Apoctyliptic. SELECT P.ID ,P.Name ,COALESCE(W.Wins,0) AS Wins ,COALESCE(L.Losses,0) + COALESCE(W.Wins,0) AS TotalGames FROM dbo.Player P LEFT OUTER JOIN (SELECT M.Winner ,Count(ID) as Wins FROM dbo.matches M GROUP BY M.Winner) W ON P.ID = W.Winner LEFT OUTER JOIN (SELECT M.Loser ,Count(ID) as Losses FROM dbo.matches M GROUP BY M.Loser) L ON P.ID = L.Loser
Makes sense, Thanks for the feeback
Read that as 'sqlkills' at first and had a chuckle.
&gt; Also, the where connection to Place_Name needs to be a LIKE because your instructions are that the place name contains Toronto which is different from place name IS Toronto wow, i never even realized this....there were indeed multiple "place names" with Toronto in them...
I'll check this out when I get back in the office tomorrow. Thank you. 
What you want isn't a trigger, it's a *unique constraint*.
I made a [SQL Fiddle](http://sqlfiddle.com/#!7/15294/5) that has a solution. Pretty sure you just had syntax error.
Your NOT IN looks wrong. Firstly, I'd use an alias when doing a subquery like this as otherwise you can fall victim to typo related bugs (or at least it behaving differently to what you want). Even if it works, it's dangerous imho. For example: CREATE TABLE foo (id INTEGER, blah VARCHAR(50)); CREATE TABLE foo2 (id INTEGER, blah2 VARCHAR(50)); INSERT INTO foo (id, blah) VALUES (1,'Test1'); INSERT INTO foo2 (id, blah2) VALUES (1,'Test2'); SELECT * FROM foo WHERE blah IN (SELECT blah FROM foo2); That runs. But my subquery contains something which is very likely an error - there is no blah column in foo2 - but there is in the outer query. Specifying the table name (or using an alias) - i.e. SELECT * FROM foo WHERE foo.blah IN (SELECT foo2.blah FROM foo2); Will at least catch my error as the engine will notice that foo2.blah doesn't exist. More specifically though, NOT IN doesn't work like that in any SQL variant I've used. You want EXISTS as /u/adm7373 suggests. NOT IN usually suggests your subquery will have one column, and then you're checking for the existence of that one column - i.e. SELECT foo.id, foo.result FROM foo WHERE foo.id NOT IN ( SELECT foo2.id FROM foo AS F2 ); 
I use the original title. Sorry about that. :)
Allowing data to be inserted with trailing spaces is still a poor practice. Years ago we ported our application from MS SQL Server to Oracle. Oracle treats the trailing spaces like any other character and we ran in to a few issues with random pieces of data that were loaded with trailing spaces. 
Just don't do this with DB names on SQL Server, unless you want to break the SQL VSS Writer. 
MSSQL and ANSI SQL-92 behavior (section 8.2). Not that anybody should follow this because it makes no sense. This issue has screwed with me on a few occasions, particularly when using ORMs.
The Microsoft KB article linked in TFA directly refers to the ANSI-92 SQL Standard that they (correctly) implement. This is standardized.
That is not at all surprising to me, but I grew up developing web forms &amp; learned that behavior testing some of my apps. For what it's worth, Oracle is a different story.
Depending on what you've done already, I'd go download Brent Ozar's First Responder kit. Run the sp_blitz and see what pops up. Save the list. Research THEN fix things. Run the list again. Show boss that you've fixed items X,Y and Z. 
You aren't getting the `type`. You're getting `parent_object_id` but with the column aliased (renamed in the resultset) to `type`. Run this and you'll get identical results. The `AS` is optional. SELECT name, parent_object_id as type, type_desc FROM sys.tables Or a better illustration: SELECT name, parent_object_id, type, parent_object_id -- missing comma type_aliased_from_parent_object_id, type_desc FROM sys.tables
because there's a missing comma, it assumes that the word that comes after the column name is an alias for that column in your query. It's equivalent to this: SELECT name myname, parent_object_id myobjectid, type mytype, type_desc mytypedesc FROM sys.tables Note that the word 'AS' between the column name and alias is optional and would create the same results. SELECT name AS myname, parent_object_id AS myobjectid, type AS mytype, type_desc AS mytypedesc FROM sys.tables edit: code formatting
this is the reason why the AS keyword in sql should not be optional 
&gt; Not that anybody should follow this because it makes no sense. In a world of CHAR data, it absolutely makes sense.
&gt; Oracle's non-standard behavior is both painful to code for &amp; to support, IMO. I find it refreshing that Oracle treats a character as a... character. Even if it is a space. To me this should be the expected behavior.
Problem is, Oracle only does that for CHARs - where the extra trailing spaces are indistinguishable from 'nothing'. With VARCHAR, the behavior is inconsistent.
You could use a trigger to check for overlapping dates for the same room.
https://asktom.oracle.com/pls/asktom/f?p=100:11:::::P11_QUESTION_ID:42171194352295
This problem is exactly why I picked up the habit of putting the commas prior to the column being selected as it's easier to notice when I make a mistake. SELECT name ,parent_object_id ,type ,type_desc FROM sys.tables
I don't think the free tier includes a Windows instance with SQL Server pre-installed. You'll have to DIY, which is not that difficult and something you should learn to do anyways. As for inserts, no, you'd need to learn how to decrypt your password and login to your instance through RDP, open ports in the EC2 firewall for SQL Server, create a linked server (which you can't do in Express) and assign users and roles to do that securely. It's not plug and play but there are alot of tutorials out there and AWS' step by step written help docs are phenomenal. Note that you can get SQL Server developer edition from Msft for free by signing up for a Developer Essentials account.
Thanks, i'll try this!
Please provide a single example of any human, anywhere, ever, in the history of Earth, with a trailing space​ on their surname. Your argument is a straw man. "If you trim spaces what's to stop you from changing the spelling of someone's name?" Uh, the rules of language for one? Spaces are (with the solitary exception of inter-word spaces like "Di Petro") considered to be separators, not content. Changing the spelling of someone's name is *completely different* than removing a trailing space that was absolutely, 100% entered by mistake. Is it a text input where formatted text is expected? HTML or Markdown or some other precisely formatted input? Then of course don't trim. But if it's a name, address, title, zip code, telephone number, email address, or most other user input, you should absolutely consider truncating the trailing spaces for a thousand good reasons. **Edit**: application layer, not in the DB. Your application should absolutely trim trailing spaces on last names and store them cleanly. The DB should faithfully store and return whatever the application sends it. I was (without being clear) referring to client side SQL, not DB structure. Don't post at 2am, kids.
Is there a particular reason why you need a cloud infrastructure to learn with? As /u/Autoexec_bat mentioned - SQL Server Developer Edition is free with an Outlook/Visual Studio Online account setup and it includes all the tools/features of the Enterprise Edition - for learning, this is your best bet. You can install it locally on your home computer, or set up a VMWare or VirtualBox instance separate from your local OS (recommended). If a cloud solution is mandatory and SQL Server is your desired database engine - it will probably be easier and cheaper for you to check out Microsoft's Azure cloud platform as it offers a free tier for SQL Server, and you can find official Azure to SSMS guides on how to connect easily. 3rd party VPS/Cloud Platforms/Virtualization of Microsoft Products are usually not included in free tiers as the 3rd Party service(AWS, Digital Ocean, etc.) has to worry about licensing both the Windows and SQL Server software for your use.
Yeah, most places I've worked have coding standards so that an alias is preceded with an AS or is contained in [] so mistakes like you mention can be spotted. 
You can overcome this by using char or charindex but the syntax is a pain in the ass. You can prevent this by using LTRIM(RTRIM( religiously during inserts.
Shouldn't an equijoin perform very well (rather than a where not exists, which for all I know equijoins behind the scenes)? 
Sorry, I think you and I agree, I was just confused about what we were talking about. When I say that you should trim the trailing spaces, I'm referring to the application layer, not the DBA doing it the way that MSSQL does it. I agree with the op that this behavior is unexpected and unwanted. I do however believe that you should absolutely be doing this in your application, trimming trailing spaces from the majority of inputs.
You need to first look in the SQL logs to see why the login failed. How did you create the databases and users? That is, did you do so via SSMS, or via some front-end tool? And I'm confused about the timing in this: **it connects to the database and injects all tables but then gives me this error " login failed for user"** How is it connecting in order to create the tables but THEN (aka "later") giving a login error? 
Good question lol. I don't have any big plans for it - I just want to get into database management - AthenaHealth has some interesting jobs that require SQL. Thats a good point tho, fam. I should aim at a specific goal.
Try: Select month, Year, MAX(number) From table Group by month,year
You are basically doing a rank over Select *from ( Select year, month, person, number, rnk= rank() over(partition by month order by number desc) From people )A Where rnk=1 Edit: added missing column
Yes.
Good call, didn't think of that when I added the column
Maybe 
Yes! Thank you very much for your response, I'm learning SQLPlus and didn't know about the RANK() function. Thanks again! :)
Fixed it thanks to u/GingerCurlz answer. Thank you for your help anyway. Good to see so many responses that fast!
Nope.
Some people tried to convince me the answer is, 'yes,' and its name is, 'Tableau.' I lul'd.
I don't know. Can you repeat the question?
I don't think so -- it works well enough, and there's *huge* infrastructure already built around it, and even if the new language beats SQL out of the water, it's going to take *serious* convincing to move people away from SQL and into a land where the infrastructure is sparse.
The example below should be more a bit more useful in explaining how they can be used with single tables. It selects all customers with above-average salaries. SELECT * FROM CUSTOMERS WHERE SALARY &gt; (SELECT AVG(SALARY) AS AVG_SALARY FROM CUSTOMERS); Another common use is identifying records that exist in another table. SELECT * FROM CUSTOMERS WHERE ADDRESS IN (SELECT DISTINCT CITY FROM SOME_GEOGRAPHY_TABLE WHERE CONTINENT = 'Asia') ; Both examples can be solved in other ways, but sometimes it's easier to use a subquery. 
For structured, relational data? Seems doubtful any time soon. 
I think I'd agree, the largest "threat" to SQL is shifting data models. At the moment, it doesn't seem like there's a need to supplant the Relational model, but if if something new is developed that handles transactional or structured data better, then perhaps.
What about Cassandra and mongoDB?
I would say no. SQL isn't/can't go anywhere for the hardcore enterprise relational databases. Even the new big data technologies are finding ways to blend into a SQLesque experience for the user. Cassandra has CQL. I think Spark has its own flavor of SQL. 
Oh I'm sure. Remember that time you saw a stored procedure building 15 temp tables and just shook your head? I've run into more of those than I ever thought I would in the corporate world.
I think the trickiest part will be in 10-15 years when we have even more legacy systems and we'll have to remember best practices by version. 
This is the answer. SQL is the language for the mathematics it is based upon so trying to find a replacement is like trying to find a replacement for arithmetic. 
In your example, you already knew the salary you were going after. You didn't need a subquery to come up with 4500. Could you write me a query that returns all customers who have a salary higher than the mean average salary?
Honest question - what are your misgivings in regards to temporary tables, and why?
I'm not going to lie, I have a few procs within my database which abuse temp tables as well. The thing with them is that I tried other methods of optimization prior to going that route and none worked as well as temp tables. They have their place. I've also worked with people at previous jobs which as soon as the simplest query wasn't performing as expected, they'd resort to temp tables and materializing intermediary steps, etc. It easily gets out of hand. I've never worked anywhere but my current job that the schema and functions weren't described as terrible by the current competent employees. So maybe my view is tainted by the terribleness of my experience. There are just way too many pre mature "optimizations" I've seen which were anything but.
split your task into two steps -- the ALTER alone, with no value, followed by an UPDATE, to set the column values
Definitely use a table, temp or normal and drop. You can't have a query with over 1000 (?) elements in a where.
i'm guessing the error message was "a million values inside an IN list is too many"
&gt; What is the best route to go about doing something like this? definitely temp table
Have you tried creating a user defined function to calculate isExpired and using the function to return the value you want in the column? So you'd have something more like ALTER TABLE Reservation ADD COLUMN isExpired tinyint(1) as dbo.isExpired(date, timein)
again, the AS syntax in an ALTER TABLE statement is not allowed also, i don't think MySQL has any dbo stuff
in this case you can't, because the datetime value would have to be continuously checked every second
We don't have enough info to help you. Please provide the other table and format your code as code.
"Msg 8623, Level 16, State 1, Line 1 The query processor ran out of internal resources and could not produce a query plan. This is a rare event and only expected for extremely complex queries or queries that reference a very large number of tables or partitions. Please simplify the query. If you believe you have received this message in error, contact Customer Support Services for more information." 
How many items do you have in your WHERE clause? You are being asked to look at a million IDs, or there are a million IDs total in the table? Could you also post the entire query?
how do i create a temp table with external values?
I have updated with the infosys table.
Sure. My TSQL snuck in a bit there. The point wasn't to give him actual code hence why i wrote "something more like". The point was to suggest the usage of a function to do the calculation.
how do I import the million records into a temp table? I will google around, but if you know that would be greatly appreciated.
You can't import them to a permanent table is that what you're saying?
Troubleshoot your join by selecting * and removing the GROUP BY clause. This should help you see where the multiplying effect is coming from.
So you're going to need them to create a WORKSPC database where you have rights to create/drop tables, etc. If it is a production server then they could create the WORKSPC database on another server and link it. There are ways to import from Excel to a temp table but I imagine you don't have the rights to do this either. http://stackoverflow.com/questions/12090555/get-excel-sheet-into-temp-table-using-a-script https://www.sqlservercentral.com/Forums/992891/%C3%8Cnsert-into-temp-table-from-excel-file Conversely you could request your DBA imports the file for you.
`sp_` is reserved in SQL Server for system stored procedures. Using it for your own sprocs means you're risking a naming conflict and unless you specify `database.schema` when calling your sproc, the system sproc will be executed. Avoid naming collisions with system sprocs and reserved words entirely, performance issue or otherwise. But as it turns out, yes [there *is* a performance impact](https://sqlperformance.com/2012/10/t-sql-queries/sp_prefix)
Along these lines, its a good idea to include the schema when calling tables, views, sprocs etc. Select * from table; works but select * from dbo.table; requires a tiny bit less work from the server - looking up and verifying the schema isn't free. Its a minor thing but its nice to do if you think about it.
The answer is going to be "it depends" without a lot more information. Are the databases situated where a single query can reach both databases? If so, it would probably be simple enough to do it with straight SQL. If not, then you'll probably want to use an integration tool like SSIS. 
I found USP prefixes to be painful, we ended up with a project that had several hundered stored procedures in the form of usp_[Add/Update/Delete]_[Table/Object] it makes finding the right stored procedure a nightmare. Next project that came around I forced the team to use something similar to [Object/Table]_[Add/Update/Delete] as this makes finding the SP you want much simpler. ie usp_Add_Customer usp_Add_Invoice vs Customer_Add Customer_Update Customer_Delete Invoice_Add.. 
Getting an error on line 3 of this. "SQL Command not properly ended". Gives me a little error swiggly after '1000' and in the beginning of 'From customers C' While I was waiting for an answer I came up with this: UPDATE CUSTOMERS SET CREDIT_LIMIT = (SELECT MAX(AMOUNT) + 1000 FROM ORDERS WHERE CUST_NUM = CUST AND AMOUNT &gt; CREDIT_LIMIT); But I felt like it updated too many rows. 
Its strange how the mind works, I know in the English language i want to "Add" a new "Customer" But when I think about it functionally I'm actually interested in the "Customer" then the action of "Add"
Also, I'd execute the query as follows to see whats going on with that case statement and how it might affect the grouping: select t2.Source, case when week &gt; date_trunc('week', current_date - 1) then t1.source end as Col2 from infosys t2 left join knowledge t1 ON t1.source = t2.Source and project_id = 'LFC'
I don't see any issues with u\alinroc's update statement. My guess is that something was lost between copy &amp; paste. However, I caution against that update statement you came up with because the columns in the correlation aren't qualified with table names or aliases. Also, correlated queries may not perform as well as a properly joined update which could be an issue if you have millions of rows to update.
This is really the right answer. Often when you see new "better than SQL" technologies you can often trace what they are trying to do to a poor understanding of set theory or trying to apply a RDBMS (a set-based data system) to a non set-based issue.
 merge into Customers c using (select c_inner.cust_num, max(o_inner.ammount) max_ammount from Customers c_inner join Orders o_inner on (c_inner.cust_num = o_inner.cust) where o_inner.ammount &gt; c_inner.credit_limit group by c_inner.cust_num) exceeding_customers on (c.cust_num = exceeding_customers.cust_num) when matched then update set c.credit_limit = exceeding_customers.max_ammount + 1000;
 SELECT COUNT(ID) AS cid ,COUNT(DISTINCT surveyanswerbuskey) AS buskey ,factsurveyid FROM BPWAREHOUSEDB.dbo.factsurveyanswer GROUP BY factsurveyid HAVING COUNT(id) != COUNT(DISTINCT surveyanswerbuskey) ORDER BY factsurveyid DESC; BPWAREHOUSEDB is the database, dbo is the schema, and factsurveyanswer is the table. It's basically for finding duplicates, where for example you have unique IDs, but you have the same answer multiple times for the same survey in the surveyanswerbuskey column. Based on your description - it either doesn't return the right result set, or it's taking a lot of time to complete. The former - Probably answers that have whitespaces and it's not treating them as unique, and if it's the latter - use CTE. P.S. Most of the DBAs can't write SQL ^just^saying
So how long does it take just to run the sub-select? &gt; AND Salesperson IN(@Salesperson) This is likely your biggest problem. The solution is to run this for all salespeople and store the data in a permanent table, then just point SSRS to that table so no calculations are being done when the user requests the report. Put the query into a stored-procedure that runs at a regular interval (daily?) -- Might need to be a loop but I'm not seeing that as being necessary here.
Thanks! I'll try that this afternooon. That is essentially making it SARGable right?
You can keep the parameter there, but you don't want it to run the actual calculation. Try this as a simple fix. SELECT SubSelect INTO TestTable To do this you will need to add the Person's name to the group by so it runs the calcs for everyone. Then move the @PARAM to the outer query and instead of having a sub-select jut give it a `From TestTable` -- then see how fast your report moves. There are other steps you can do to improve it, but this will be a big gain.
So you can join your sub query to the outer query using the alias you define at the end. AND sp.Salesperson = dva.Salesperson I think you're worried about the cartesian of records. That should prevent that and only return records where they're the same. Also, I used to do the exact same thing as you. Custom sales report. Here is my recommendation to you. Stop trying to do everything SQL and let some basic tools help you. Cleanse the data in a view and then expose that to Excel. Connect to to the view in excel and build a pivot table off of it. Build slicers and filters off dates and 'Areas'. Next build graphs off of that pivot table and put them on the first page in the work book. Build yourself a nice 'dashboard'. You can move the slicers that control the pivot tables and in turn graphs to the dashboard and hide the source pivot tables in another tab. From there just lock the work book to prevent editing and share it with your sales team. They'll love it. They always love excel things. You'll have reduced query time to local machines and increased user satisfaction. I'm happy to help if you have more questions. 
This is a little outside my realm, but you could publish the SSRS data to a sharepoint form, and then have that sharepoint form spit out a PDF. I am not sure if there is another direct way
Wow..ok nevermind, disregard this...jump the gun too early on this post. Sorry.
Oh wow, that was simple. Thank you very much!
Solid query! &gt; P.S. Most of the DBAs can't write SQL justsaying Ouch
Add the column after your CASE statement: END becomes END, rate 
Not an Oracle guy but at a guess maybe you need to sum the cost before using it as a divisor in profit, or sum the whole lot? SUM(SUM(retail-cost)/SUM(cost)*100) or maybe SUM(retail-cost/cost*100)
I'd suggest the slowness isn't from the concatenation itself, but rather the searching of a calculated field - you won't be able to hit indexes this way. Look at altering how your query is structured so your where clause isn't comparing your search items with a calculated field. If that is the problem, then this would run much faster: SELECT NPA || NXX || LN FROM tns WHERE NPA = 555 AND NXX = 555 and LN = 5555 That query is still concatenating, but not searching with the concatenated field. If that does fix the performance problem I would suggest you really do need to split your search parameter.
SharePoint
If speed is the problem, you could create a composite index on the three fields. If the split query is the problem, you could create a VIEW that combines the three fields together in a new field, then run the query on the VIEW.
Thank you so much. The second one produces an incorrect result because of order of execution, but the first one (though it doesn't provide the correct result) led me down a path of figuring out what was up. I made some modifications, and though ugly, it produces exactly the required result. CREATE VIEW topsellers AS SELECT title, qty, ROUND(profit,0) AS profit FROM (SELECT title, SUM(quantity) AS qty, SUM(retail-cost)/SUM(cost)*100 AS profit FROM books JOIN orderitems USING (isbn) GROUP BY title ORDER BY SUM(quantity) DESC) WHERE ROWNUM &lt;=5; Thanks again for your help! Now I can submit the assignment and enjoy my vacation next week!
&gt; If speed is the problem, you could create a composite index on the three fields. Would that actually work? I was under the impression that as soon as you did a calculation on the indexed fields, you could no longer use the indexes within them?
Is advising people of potential pitfalls "negative"? I spent a decade managing an application which used `DATE` and `DATETIME` in various places with little rhyme or reason, so using `BETWEEN` could easily result in either missing or excess results if you weren't careful, depending on what you were querying. Stylistically I like the clear, explicit meaning that's conveyed by specifying a full date &amp; time for the top and bottom of a range using `&lt;` or `&lt;=` and `&gt;` or `&gt;=`. I also have a penchant for the ISO 8601 date format, for the same reason - it's intentionally designed to eliminate ambiguity.
Thanks for the reply. It does seem like the issue is with searching the calculated field. Concatenating in the SELECT is still fast so I may just be stuck with the split query.
&gt; Is advising people of potential pitfalls "negative"? Sorry, somehow when I read your commend and somehow remembered you saying [`between` is evil](https://np.reddit.com/r/SQL/comments/46figq/simple_ssrs_question_about_parameters/d04sgjg/)... Over a year ago... I have no idea why that's so clear in my memory... Anyway, no, I suppose it's not negative. I'm just confused at how the things listed can be considered pitfalls of `BETWEEN`, when it behaves exactly the same as using `&lt;=` and `&gt;=`. &gt; using BETWEEN could easily result in either missing or excess results if you weren't careful, depending on what you were querying. But in the same places, using `&gt;=` and `&lt;=` could just as easily result in the same problems if you're not careful. &gt; Stylistically I like the clear, explicit meaning that's conveyed by specifying a full date &amp; time for the top and bottom of a range using `&lt; `or `&lt;= `and `&gt;` or `&gt;=`. If you're already specifying full date and time, why not cut a few key strokes and just use `Value BETWEEN '2017-04-01 00:00:00.000' AND '2017-04-18 23:59:59.999'`? Is that not just as clear and explicit? Also, I don't intend to appear confrontational. I see you commenting here a lot, and I almost always agree with what you have to say and consider you to be one of the better contributors in the sub. So I'm just taking to opportunity to have a discussion about one of the few things you've said that I have an opposing opinion about.
Well it depends. If the index is built on multiple fields, or uses a function, the query will use the index if the condition in the WHERE clause fits exactly. For example, you could create an index on multiple columns: CREATE INDEX ndx_a ON table(NPA, NXX, LN); Then use this WHERE clause: WHERE NPA = '555' AND NXX = '555' AND LN = '5555'; Or you could use a function index like this: CREATE INDEX ndx_b ON table(NPA||NXX||LN); With this WHERE clause: WHERE NPA||NXX||LN = '5555555555'; Both these methods will use the index and should be pretty fast.
It's great, but no immediate feedback like OP wanted. :-/
This is a rather strange question and I think the answer would be no unless you are traced it while it ran, but I am not positive. Out of curiosity why has this even come up?
Yes, more tables almost always make for a slower query.
yes you've joined 1 with 2 and 3 with 4, but no connection between these pairs, so you are going to get partial **cross join effects** also, please learn explicit JOIN syntax
http://stackoverflow.com/questions/2051162/sql-multiple-column-ordering looking at the top answer then the first argument will be the first decider and so on? thanks a lot!
I've wondered the exact same thing. Your explanation seems like the most plausible. 