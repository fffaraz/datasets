No. SQL doesn't change *that* much. You'd probably be best studying that book and then doing a quick Google search to see what new features have been added since 2009.
What flavor of SQL and what type of column is `DATE_CLOSED` ? You really shouldn't need to use `LIKE` against a `datetime` column... but it sounds like the `LIKE` is working fine, but it's not stored as an actual datetime... therefore your `MAX()` function isn't working right. Might have to convert / cast your `DATE_CLOSED` column to a `datetime` then you can use `year()`, `datepart()`, `strftime()`, etc... to extract the year out and your `MAX()` function will "just work" against it. I'm betting it's stored as something like "12122015" for 2015-12-12, therefore it's larger than "01012016" for 2016-01-01 when an int, but not a date. SQLLite just because that's what I had on hand: DROP TABLE JOB_HEADER; CREATE TABLE JOB_HEADER (`PART` varchar(3), `DATE_CLOSED` datetime) ; INSERT INTO JOB_HEADER (`PART`, `DATE_CLOSED`) VALUES ('001', '2015-05-01 00:00:00'), ('001', '2015-07-01 00:00:00'), ('001', '2016-11-01 00:00:00'), ('002', '2015-03-03 00:00:00'), ('003', '2016-05-05 00:00:00') ; SELECT PART ,MAX(DATE_CLOSED) AS DATECLOSED FROM JOB_HEADER WHERE (strftime('%Y', DATE_CLOSED) IN ('2015','2016')) GROUP BY PART ; Output: 001|2016-11-01 00:00:00 002|2015-03-03 00:00:00 003|2016-05-05 00:00:00
What is BSwift_MemberPlanQuestion.question defined as?
Hah! I'll have to add them tomorrow. :-)
 Are you sure your Stored Procedure only has two parameters? Could @EndDate be the third parameter?
That's magnificent! Thanks for the idea.
;
The client wants the column header to be the text of the 11 distant questions. The first few columns will be employee identifying data, and the answer to the questions will fill in the table values. I'm just not to sure how to best do it in SQL.
There is no primary key for that table.
Basically, I'm asking you what conceptually defines a record in that table? What are the fewest number of columns -- and those columns' definition -- that you'd be able to identify a single unique record if you were asked to do so.
Syntax error
userid and question.
You can use order by with a column number like: ORDER BY 2 DESC, 1 ASC
[removed]
WHERE (@LIST IS NULL OR List = @List)
you can do it like: UPDATE dbo.tbl_1 SET dbo.tbl_1.Agent = SELECT CASE (ABS(checksum(newID())) % 3) WHEN 1 THEN 'David' WHEN 2 THEN 'James' ELSE 'Steve' END 
Thanks i'll give it a try
Thanks a lot. I'm using Pervasive PSQL and the DateTime column is a Character type for whatever reason. It's actually stored MMDDYY. I will learn how to convert with Pervasive today. But basically, my fundamental syntax with the MAX function should in theory work? Thanks again. 
What RMDBS are you using? SQL Server? MySQL? Oracle? In anything *except* MySQL, this is relatively easy using the `ROW_NUMBER` function.
SQL Server. I've actually never used ROW_NUMBER. I'm gonna take a look and see if I can get a handle on it right now. I've just been trying to familiarize myself with how to accomplish this in a set-based way, because (from what little I understand and have been told) that is SQL's bread and butter (granted you want to look at the execution plan and all that). EDIT: So I'm thinking if I were going to use ROW_NUMBER, I would probably want to put that in a CTE or derived table? Maybe it's impractical, but I was doing this sort of as an exercise in avoiding any sub queries whatsoever.
You're missing the comparison to null at the very least - the query from stackoverflow looks for a single row that cannot be self-joined to a 'next' row. Also, you seem to be selecting something from the left joined table - which again would go against the central point of the stackoverflow query. P.S. I'd also suggest using "NOT EXISTS" as more descriptive/human readable and slightly more instructive for optimizers to pick up the right execution plan (of an anti-join variety, most likely).
Does the service account you're running the SQL service under have permission to the directory you copied it to?
Thanks, I'm going to try writing it with T-SQL in Access in the unlikely case it is just something weird with Pervasive. But yeah our database is weird with dates. Not only are they all characters but they're stored in different formats (YYYYDDMM, YYDDMM, DDMMYY etc.) in different tables. I wish I knew enough about coding to see how the ERP handles it. 
That's annoying.. our ERP is similar as it stores dates as characters but it also includes the century... so it's YYMMDDC... so 99% of the time you have to strip off the last character as it just messes things up. If you can get the format to YYYYMMDD or YYMMDD you should be able to use the MAX function without issue, assuming there are no dates from 19XX when using YY.
Why don't you use SSIS for this? 
Wouldn't it be best to use MERGE in this case? You can use the different matched statements to INSERT/UPDATE/DELETE/do nothing.
I wish I had more time and a db to mimic this on. I don't think the second academic join is needed. Try the following: SELECT a1.ACADEMIC_YEAR, a1.ACADEMIC_TERM, ac1.START_DATE, --a2.ACADEMIC_YEAR, --a2.ACADEMIC_TERM, ac2.START_DATE FROM PEOPLE p JOIN ACADEMIC a1 ON p.PEOPLE_CODE_ID = a1.PEOPLE_CODE_ID JOIN ACADEMICCALENDAR ac1 ON a1.ACADEMIC_YEAR = ac1.ACADEMIC_YEAR AND a1.ACADEMIC_TERM = ac1.ACADEMIC_TERM --JOIN ACADEMIC a2 -- ON **a1.PEOPLE_CODE_ID = a2.PEOPLE_CODE_ID** LEFT JOIN ACADEMICCALENDAR ac2 ON a1.ACADEMIC_YEAR = ac2.ACADEMIC_YEAR AND a1.ACADEMIC_TERM = ac2.ACADEMIC_TERM AND **(ac1.[START_DATE] &lt; ac2.[START_DATE] OR ac1.[START_DATE] = ac2.[START_DATE] AND ac1.SessionPeriodId &lt; ac2.SessionPeriodId)** WHERE p.PEOPLE_CODE_ID = 'P000002781' ORDER BY ac1.[START_DATE] DESC
SSIS or even straight T-SQL Agent Jobs should do. You might be able to get away with pulling small test runs in production, limiting your queries with "TOP 5 PERCENT" or something small to get a sample of how long your downtime may be.
King Henry VIII slept with a big axe.
Yeah I think I misjudged why it's not working. It's not taking 2015 then 2016, it's just taking the biggest number but it's MMDDYY. So it'll take the 2015 date as long as it's the latest month and day. 120515 is a bigger number than 120416. So a conversion should work, I better learn them anyway. Thanks again. 
You could go to the folder on your hard disk that the .bak is going to and temporarily give 'everyone' (use that instead of a username) read/full access, and as soon as the backup is restored, remove that permission, or to do it properly, find the service account name in sql server configuration manager for 'sqlserver' and give it access to the folder with the backup.
? My sister's called Rachel. 
SQL Express is free and you can then download the AdventureWorks databases from Microsoft for testing.
take a look at the HAVING SQL clause which operates in conjunction with group by. So you could do something like https://www.techonthenet.com/oracle/having.php HAVING count(*) &gt; 1
If I do a CAST in my select query, does it convert the actual table permanently or is it only for the purposes of the query? I don't want to mess with the DB in that way in case it hurts the application. 
Thanks for your suggestion. I am not sure if I can get this to work, since I can't really group this stuff. All it seems to be doing is counting the number of times there is a SHIP transaction for each probill and order combo. I need it to only return the probills that are associated with more than one order, along with the orders. Edit: formatting
It's Ellucian's PowerCampus, unfortunately.
 Can you do the detail section as a table instead of text boxes? 
I used SQLite to open the table itself which is where it asks me for the encryption key.
Is your startdate actually a datetime? Because if it is, what you're doing is SELECT * FROM table WHERE startdate &gt;='11/1/2016 00:00:00' AND startdate &lt;='11/30/2016 00:00:00' so you're getting all data up to and including midnight on the 29th. (I'm using sybase date handling because you didn't mention DBMS, and it is the most forgiving.) If you want to include the entire month of November, you need: SELECT * FROM table WHERE startdate &gt;='11/1/2016 00:00:00' AND startdate &lt;='11/30/2016 23:59:59'
I don't know if this would work, but.... What about creating 3 tables? Table 1 would show Rows 1-5, Table 2 would show 6-10, and Table 3 would show 11-15. Modify your SQL to Reset row numbering after 15... CASE WHEN ROW_NUMBER() OVER (ORDER BY somefield) % 15 = 0 THEN 15 ELSE ROW_NUMBER() OVER (ORDER BY somefield) % 15 END AS RowGroupID
Anywho, unless your interest is purely academic, on ms SQL there is 'cross apply' syntax that handles these kind of queries with ease.
Yes, on your resume, you need to stress and highlight the SQL aspect of your job. Don't write an inventory control coordinator resume and add a single bullet point about writing SQL. Elaborate on all of the SQL projects that you've completed. You may also need to look at analyst type jobs at smaller companies that want a "report guy" or "sql guy." Bigger firms with departments full of SQL developers may be more picky. That being said, since you've done so much with SQL and SSMS, could you sneak some SSRS into your skills as well? That would go a long way.
Go check out data analyst / report developer jobs in indeed and dice. Use those terms and post your resume to the major job boards. Don't accept anything under $70k. If you want help looking at a resume I can provide you feedback if you really want. 
Already tried I can see the name of the columns but all the rest are random unreadable signs.
You can also try contracting houses for techies. It can give you the experience you need so you can eventually get into a place on your own if you want.
Thanks, that is encouraging.
http://stackoverflow.com/a/940001/476309 I currently use that pattern in a couple of services running against an always on cluster. One service only has 2 worker processes, but the other has about 100. I haven't experienced any issues with either.
&gt; chkout_dte + case when to_char(chkout_dte + 30, 'D') in (1, 7) then 32 else 30 Thanks. Now, I have one more column to calculate called the pastdue_fees. if return_dte is on or before due_dte then pastdue_fee =0, but if it is any number of days after the due_dte, then it is (return_dte - due_dte)*2. I have the following code but I get an error: Update book_trans set pastdue_fees= case when return_dte â€“ due_dte &lt;0 OR return_dte â€“ due_dte =0 then 0 Else (return_dte â€“ due_dte)*2 end; error: Update book_trans set pastdue_fees= (case when return_dte - due_dte &lt;0 OR return_dte - due_dte =0 then 0 * ERROR at line 1: ORA-01438: value larger than specified precision allowed for this column (should I be using a select case when first and then UPDATE???) Thanks
Job postings are bull. They put a dream wish list together for 1/10 of the salary that individual would attract. Apply anyway. I've no formal training and in the last 5 years have moved from a back office job at a call center to managing the Data operations at my current employer. Doubled my wage in the process. Good luck and let me know if you have any questions. 
Solved it with. DECLARE @advertisementId int; ---- xxxxxxx SET @advertisementId = (SELECT Advertisement_id FROM Advertisements WHERE AdvertisementName = @AdvirsementName) in the middle of the two insert clauses 
Better solution, use the command: IDENT_CURRENT('Table name') DECLARE @advertisementId int SET @advertisementId = IDENT_CURRENT('Advertisements')
Thnaks! what happens if another person insert simultaneously?
Yes, definitely.. you should probably pause your SQL training and start looking into branching into architect and/or developer skills even. This will make you a god
To avoid this in the future, consider putting a GUID/UUID as ID on a table. Then you can create the GUID/UUID at the same time as declaring parameters, so you will know the ID before it's even inserted.
After inserting into advertisement couldn't they just set a variable to @@IDENTITY? 
I don't like @@IDENTITY too much because it gets the last ID inserted from any table in the database, it is too risky for my taste.
I got into the field 3 or so years ago. I learned in a similar way to you. Yes you're qualified. I called an IT staffing firm and they put me into a contract to hire position that lasted 6 months. After 6 months I got hired. If you're in the USA, Robert Half Technology is a decent company. There are no benefits, but they'll place you in a job that you're qualified for and help you negotiate a salary. Many places only hire via staffing firms, it's becoming the norm. They get 6 months to see that you have a functional work ethic, are productive, and have the skills to do your job. As an applicant it's nice because they do most of the job hunting work for you. Plus it's nice to have a 3rd party give you feedback about what salary range you should be seeking. All that said, don't get frustrated, Dec and Jan are terrible months to be looking for a job. 
Why?
[this](http://stackoverflow.com/questions/21896782/using-int-or-guid-as-primary-key) explains it well.
I use scope_identity. DECLARE @NewID INT SELECT @NewID = SCOPE_IDENTITY()
No, no, no. Creating a 16-byte universally unique identifier is *extreme* overkill for this situation. Putting it in an index or primary key is even worse.
Use SCOPE_IDENTITY() instead, which is limited to your current transaction and cannot be affected by others.
Try running just a SELECT statement to see if what you are doing is correct. Something like: SELECT trans.* , CASE WHEN return_dte â€¹= due_dte THEN 0 WHEN return_dte &gt; due_dte THEN (reurtn_dte - due_dt) * 2 FROM book_trans trans;
This was much better than I had hoped for. Im looking for skills specific for my role, so naturaly, some wont show up on this. But most are covered by SQL Developer and we do some light DBA work. So I guess in large parts, SSIS, SSRS, T-SQL, SPs, views, indexes etc. are what we do, but the list ou provided is soo much better. then what I had.
you're on the right track with this -- and ( @tipSubType = HC.tipSubType or isnull(@tipSubType, 0) = 0 or hc.tipSubType is null ) say again why this isn't working the way you want?
Okay, if the user (or application) passes in a value, i need the SP to return records that match the value, but the user can pass in NULL for that column, and I only want records that have a null value for the column. So with this code in place: and (@tipSubType = HC.tipSubType or isnull(@tipSubType, 0) = 0 or hc.tipSubType is null) when the parameter is: @tipSubType = Null I get: gateSize| GateID |id |nozzleID |tipType |tipSubType |shutoff |isSX ---|---|----|----|----|----|----|---- 1.80mm [0.0709"] |23285 |78 |3 |2 |9 |14 |0 1.80mm [0.0709"] |24625 |82 |3 |2 |NULL |14 |0 1.80mm [0.0709"] |25965 |86 |3 |2 |14 |14 |0 1.80mm [0.0709"] |27305 |90 |3 |2 |6 |14 |0 and when the parameter is : @tipSubType = 6 I get: gateSize| GateID |id |nozzleID |tipType |tipSubType |shutoff |isSX ---|---|----|----|----|----|----|---- 1.80mm [0.0709"] |24625 |82 |3 |2 |NULL |14 |0 1.80mm [0.0709"] |27305 |90 |3 |2 |6 |14 |0 What I want is just the null when param is null, and just the 6 when param is 6. I can get it to work for one case but not the other. It has to do with SQL not evaluating a null value as a value, so I can never compare the argument "value" to a null value in the table... or something
Your amazing. All I was missing was the "as." Thank you.
That does not turn a string into a date. It governs output display.
You're tracking two things. You're tracking what people do, and what location they are in. Can people exit the park in the ride area? Id assume they can do that about as well as going for rides in the theater. Each activity should probably join to a location table. Locations like Entrance, RideArea, Restaurant. You can log activities, then query for the activity/location where the max time â‰¤ desired tracking time.
That actually looks like it worked! I'm surprised though that the sub-query allows references to things outside of it (specifically the Events.DateTime). I don't recall something like that ever working in the past but my memory could be completely betraying me. 
Yeah I've changed jobs twice in that time. I'll probably look to change after another 2 in my new position. That should be another double up!
Yeah I can do it, I'm gonna make dinner (1hr) then it will take me an hour or so to do I guess. Need to learn the data, I have no idea why you'd ever use a USING clause, so might need to read up on that quickly. Also it seems like a lot of queries at first glance, but I'm guessing most of them can be mashed in together. I could complete before 4 hours for sure.
Please post the full query, the issue may be in places you least expected. Also, make sure there aren't any warnings after you run the query, sometimes the query stops when, in this case, the CAST fails and bring the data that worked until that point.
Yep that works exactly how you think it would.
Is that just a typo, that you have 3 'y' in the format? 
Maybe there are values that aren't a valid date time? Although you'd probably get an error in that case, unless it's a try_cast. What you could do is take the query without the cast, then add something like this to the where clause and id not in (select id from (select *,cast(StopTime as date) from *rest of query*) as q) Then you'll get the records that don't show up in the cast query, and you might be able see if there's a common abnormality in them Sorry if that doesn't format nicely I'm on mobile right now. Also like the other commenter said, maybe post the rest of your query, for both the cast and the non-cast
You might want to give this a shot. ,CONVERT(VARCHAR(10),StopTime ,101) AS RESDATE
First post on Reddit and jacked up my user name. Ugh..... Anyway if you have Null values in your date column I would try these functions, this should bring back all records. If you have a date of 9999-12-31 then you'll know that it was a null. ,CONVERT(VARCHAR(10),ISNULL(StopTime,'9999-12-31') ,101) AS RESDATE 
Nah, that didn't work :S I ended up just having the end date as DATEADD(DAY,1,'@EndDate) Thanks though!
Does your employer want you to pass the test so that you've passed the test, or do they want you to actually learn the material to pass the test? A braindump will just get you enough to pass the test, you may not necessarily *understand* the material.
Frowned upon by whom, really? I think anyone that values their time recognizes that they are only really useful for 1) passing through recruitment scanning 2) providing some meta-value to your company, I got some Microsoft certs so we get cheaper products etc 3) being impressive to incompetent people I don't believe in "cheating". I believe in efficiently circumventing systems that provide no value, as long as you understand the consequences. Cheating in school is generally dumb. Cheating on certs is generally clever.
Check the OS logs. I'm not familiar with Linux/UNIX, on Windows check the event viewer if you find anything that caused the reboot.
You haven't mentioned the OS, the database platform you're using, or anything else helpful. 
Check for dump files created by the OS (mini dump?).
As others have said, this is very much an OS level issue. What kind of disks? Is this a VM, or bare metal? How is storage attached/presented? Sounds like a BSOD is happening that is triggered by an error/issue in your storage. 
That's exactly the attitude that makes the certification system less valuable. If people didn't use brain dumps, actually took and attempted to pass tests when they understood the required material, then certifications would be an effective way of knowing that someone is well qualified. I have plenty of issues with how testing often focuses on marketing speak rather than testing comprehension, but that's a different issue. "Cheating" is a real problem, as is dishonesty and people working the system. ALL of that said - I think it's crap that an employer would force certification. IMO it should be encouraged and offered, but when it's used that way...then yeah, it's BS.
Yea I agree of course. But I don't learn at all by reading, only by doing. There's no way I can study for these complex tests that uses tech I can't really use in any meaningful way which makes me totally disinterested. So the only realistic way was brain dump, because I was basically forced to get these certs. I'm sure many reason similarly to me in this. 
I agree with the tech you can't really use statement. 70-462 includes the master data management and data quality services tools and I have never met anyone who uses those features.
The larger part of the problem is the certification assessments themselves. The MS SQL ones in particular are stacked full of 'gotcha' questions to the point where one could be an intermediate or senior SQL dev and still fail - they don't reflect a person's ability to actually use the tools in a business setting.
Indexes, hints, or query optimization. Got it. thanks moitroygsbre.
Hi there. I'm not going to be the most definitive source, but if you're interested in pursuing the Microsoft SQL Server certifications, check out this link: https://www.microsoft.com/en-us/learning/sql-certification.aspx There are three different certifications for SQL Server at this level - Database Development, Database Administration, and Business Intelligence (BI) Development. They also have a lower level "Microsoft Technology Associate" (MTA) certification for SQL. It is NOT a prerequisite, but may be useful if you're still starting out. Hopefully someone will have more to share on this, or on non-Microsoft solutions. Good luck!
Came to say this. Study for MCSA, I would suggest 2016, but the book isn't out yet. The MCSA for 2012/2014 is good too. Play with sample databases like Adventure Works. 
Is it the "querying with t-sql" or Microsoft SQL database development course you are talking about 
Get a mentor in the field, not a certification, preferably a local one, who will keep you accountable to them for pet projects. Understanding how to model data in a database and query it is more important than the actual syntax.
Microsoft or Oracle SQL ? 
Thanks for the info ðŸ˜€ðŸ˜€
Stack exchange real world problems (and homework) with practical solutions 
It's called a correlated subquery.
Much obliged. I think I should be able to get something to work from this. Thank you.
A CTE is like a view. It updates the underlying table just like view does so if that field doesn't exists in the underlying table it won't work.
Is there a workaround you can think of? Trouble being, the current query creates 4 temp tables, counts a load of stuff for a number of each, and then drops all 4 tables. I could create the one temp table to insert this info into, but exploring options as I've not encountered it before.
The former. It is a better primer on joins than many others you'll find for free
[removed]
This information is for Microsoft SQL.
OK so my query is as follows: Create Temptable1 (Base data) Create Temptable2 (Assessment data) Create Temptable3 (Treatment data) Create Temptable4 (Treatment2 data) Previously here it would update 11 columns in Temptable1 based on joins with info from Temptable2/3/4. Following this it'd provide me with all of the data from Temptable1 using CASE WHEN to see if things are compliant/non-compliant with policies. Then it'd query TempTable1 for a variety of sum/casewhen on the 11 columns.
tip o' the day -- when you write a blog post for a specific dbms, it's a good idea to put that dbms name in the title so instead of "SQL" your blog title should say "Oracle SQL"
~~Window functions are not part of the most recent SQL standard, 2011. The 2016 Standard was just recently released, and I haven't reviewed it yet to see if they may have added them, but even if they did, there are many RDBMS's that do not support them as of yet, as that would be a new addition to the standard.~~ I had the wrong PDF open. Nonetheless, the fact remains there ARE RDBMS's that don't support windows functions, or don't support them on ALL of the same types of functions.
&gt; Window functions are not part of the most recent SQL standard, 2011 https://en.wikipedia.org/wiki/SQL:2003#Summary &gt; there are many RDBMS's that do not support them as of yet [At least these](https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems#Database_capabilities): - Microsoft SQL Server - MariaDB - DB2 - Firebird - Informix Dynamic Server - Oracle - PostgreSQL - SQL Anywhere - Teradata And MySQL is working on it. Welcome to modern SQL :)
Don't use *, type out all the column names and then include HEX(...) as one of the columns.
Okay, I thought this would be a potential solution as well I was just hoping there would be something more efficient because there are a lot of column names to type. Thanks!
We use Microsoft SQL Server and have access to Management Studio. If you have SSMS, then right-click on the table in the object explorer, the left click on "Script table as...", then "Select to...", and then finally "New query editor window". This will generate a new select statement with all of the columns properly formatted and broken out with commas. Then just customize to your hearts content.
Nice I will check out this option in MySQL Workbench, there's probably something similar. Thanks
Search: &lt;window function&gt; ::= &lt;window function type&gt; OVER &lt;window name or specification&gt; You could also just search for `ROW_NUMBER`, of course. &gt; Sybase ASE 15.x is one Yeah, well ;) But Sybase SQL Anywhere has them... Sure, there are others who haven't upgraded yet. They're not trivial to implement efficiently.
My favorite part about window functions! I was so happy when I discovered this.
Going to look through all of these - thanks!!
OK, I see.
You have two statements. First you delete the content, then you select something Delete t1 From table1 AS t1 Left join table2 AS t2 On t1.metric = t2.metric Where something something Since you have t2 in the where statement it will essentially be an inner join
Just to clarify, there would be essentially 25 columns because it would be FN, LN, HireDate, Q1,Q1A,Q2,Q2A,...,Q11,Q11A. ?
Yeah, it's a weird request but that would be what it ends up being.
What would the code look like for doing it via CASE/GROUP BY? 
Can you give us the three create statements to the tables simplified to the data shown in your query or at least preface the select data with aliases so we can distinguish the data source for each column?
Try this. I only took a whack because I'm working on my 70-461 and the pivot is a great practice Q. I'm curious to input from others what I can do better, I felt this was clunky as fuck. CREATE table BSwift_EmployeeInfo( userid int, hiredate datetime2 ) CREATE table Bswift_MemberPlanQuestion ( Question varchar(255), Answer varchar(255), UserId int ) CREATE table Bswift_Member ( FirstName varchar(255), LastName varchar(255), UserID int) INSERT INTO BSwift_EmployeeInfo VALUES( '1',(GETDATE()-45)) INSERT INTO BSwift_EmployeeInfo VALUES( '2',(GETDATE()-1)) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q1','A1',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q2','A2',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q3','A3',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q4','A4',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q5','A5',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q6','A6',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q7','A7',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q8','A8',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q9','A9',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q10','A10',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q11','A11',1) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q1','A1',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q2','A2',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q3','A3',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q4','A4',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q5','A5',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q6','A6',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q7','A7',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q8','A8',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q9','A9',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q10','A10',2) INSERT INTO Bswift_MemberPlanQuestion VALUES('Q11','A11',2) INSERT INTO Bswift_Member VALUES( 'IP', 'Freely', 1) INSERT INTO Bswift_Member VALUES( 'Mike', 'Rotch', 2) SELECT Q.userid,M.Firstname,M.Lastname,ei.hiredate,A.Q1,A.Q2,A.Q3,A.Q4,A.Q5,A.Q6,A.Q7,A.Q8,A.Q9,A.Q10,A.Q11,Q.A1,Q.A2,Q.A3,Q.A4,Q.A5,Q.A6,Q.A7,Q.A8,Q.A9,Q.A10,Q.A11 FROM ( SELECT UserID, Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11 FROM ( SELECT UserID, Question, Answer FROM Bswift_MemberPlanQuestion ) s PIVOT ( MAX(Answer) FOR Question IN (Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11) ) p) A LEFT JOIN ( SELECT UserID, A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11 FROM ( SELECT UserID, Question, Answer FROM Bswift_MemberPlanQuestion ) s PIVOT ( MAX(Question) FOR Answer IN (A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11) ) p)Q ON A.userid = Q.Userid LEFT JOIN Bswift_Member m ON m.userid=a.userid LEFT JOIN BSwift_EmployeeInfo EI ON EI.userid = A.userid 
Here's a few [examples](http://stackoverflow.com/questions/20111418/sql-server-transpose-rows-to-columns) of ways to do it. 
Once I have the full query how do I automate it and output the result set to a file?
I don't use databases in my career (yet), and I agree, you'll never get fully polished/fluent unless you spend your work hours using something. In the mean time I do EdX courses and practice on codewars.com and other online challenge sites.
You can try the basics by setting up a local SQL Server Express or MySQL instance and moving in some AdventureWorks or the like database. Pretend you're tasked with doing something like bringing back all items in a table based on some criteria, joining tables to bring back other data, etc. In the end, it's like trying to learn a programming language without *actually* doing it. It's pretty tough. There's a lot you can do with SQL and you wouldn't think about most of it, especially if you're just starting out.
You can learn theory, but actually doing it is the way to go. There are free versions of premium SQL languages you can download and install with sample databases and people have posed tasks for said databases so you can get some practice in. I can also guarantee you've had practice in databases. Excel is the most widely used database in the world.
I wanted to do it as a batch but I was having problems with that, looping was pretty simple. I'd love to see a batch solution instead of this shit. DECLARE @Unique TABLE ( ID INT IDENTITY(1 , 1) PRIMARY KEY ,statez VARCHAR(255) ,city VARCHAR(255) ) INSERT INTO @Unique ( statez ) ( SELECT DISTINCT statez FROM testing ) DECLARE @site_value INT; SET @site_value = 1; WHILE @site_value &lt;= 50 BEGIN IF ( SELECT TOP 1 city FROM testing WHERE statez = ( SELECT statez FROM @unique WHERE @site_value = ID ) ) IS NOT NULL BEGIN UPDATE @Unique SET city = ( SELECT TOP 1 city FROM testing WHERE statez = ( SELECT statez FROM @unique WHERE @site_value = ID ) ); SET @site_value = @site_value + 1; END ELSE SET @site_value = @site_value + 1; END; SELECT * FROM @Unique AS u GO
Perfect!
&gt; How do you do this if you don't dabble with databases whatsoever in your career? dabble at home
SQL Fiddle is also an excellent resource. It works in a similar manner to JS Fiddle. You can choose what DB to work against (MySQL, Oracle, SQL Server, PostgreSQL, SQLite), then script some tables, then run queries against them. I use it all the time when I just need to check or validate something. You can find it at http://sqlfiddle.com/
there are plenty of online databases you can play around with :) A lot of tutorials come with sandboxes where you can practice most sql commands, including volatile tables.
This is the best way to screw around with multiple environments.
I don't get the downvotes. I end up having to practice at least 30 min a day on additional skills, upwards to 10 hours a week to stay relevant in IT. :-/ 
Pretty close. SELECT * FROM ( SELECT state ,city ,ROW_NUMBER() OVER (PARTITION BY state ORDER BY state, city) AS row FROM statetable ) AS a WHERE a.row &lt; 2 I'm mad I couldn't figure it out in a single transaction and had to go with a loop now. I blame the fever.
Sounds like a case when concatenate select to me.
would I also need to loop?
Two issues, ROWNUMBER != ROW_NUMBER. Second issue is the over by is required by T-SQL syntax. Still a solid and A+ solution, I did a complex loop to try to resolve it. :-/ &gt; Msg 4112, Level 15, State 1, Line 8 The ranking function "ROW_NUMBER" must have an ORDER BY clause.
yes... google "self join"
Ah, just a flavor issue, then. The dbms I tested on is more like db2, doesn't require order by, and the function is rownumber().
That makes sense, yup, it's flavor then. Also, my condolences on DB2.
sqlite is a file-based database, and takes about 30 seconds to download and install. You can run SQL against it on the command line https://www.sqlite.org/download.html
It varies wildly. Entry level SQL monkey with no strategic or real analytic expectations is probably about $40k. High end DBA or financial analyst probably knows a bit more than just SQL even though they use it daily, and clears $200k.
thanks...but what about the database aspect?
I'm a data analyst in the Chicago area. That sounds very low to me. If you have absolutely no work experience you might get stuck taking those for like a year before you jump to a $70k job, but even then I'd probably aim closer to $30 then $20.
What flavor of SQL? If it's MS SQL I'd focus more on powershell personally.
Yeah, with a year of temping I think you'd be getting a bad deal under $32 an hour. You have a college degree? No bachelor's will make it harder on you, for really no good reason.
I'd need more than "Midwest" to say for sure. What you're describing is very entry level work and in less metropolitan areas $25 might be about right for that kind of work. You'd want to add some analysis or dashboard-creation to the job description to raise it, given you mentioned Python and VB. What you're describing right now doesn't use either of those and is just pure entry-level SQL.
Yes, this. It's all about supply and demand and your market. I have a Python developer with about a year of experience on my team (with no SQL experience) that I'm paying $30/hour. But in my geographic market, that's reasonable.
 select concat(col1, '|', col2, '|', col3) cols, count(*) from tbl group by concat(col1, '|', col2, '|', col3) having count(*) &gt; 1
[removed]
Why would a recruiter say I am only worth $40,000 per year? ... 
"works in SQL", "analyst", and "uses SQL" are vague terms. What are your actual qualifications and experiences? Also, what part of the country are you in? (Be as specific or unspecific as you'd like)
Can I connect with you on LinkedIn and/or PM you? I know recruiters job is to make money off the people they place. I have nothing against the practice, and I want to make sure I am not being undervalued in the market.
Absolute money is a difficult metric since it's wildly dependent on personal expenses, country and location, but I work as a .NET developer and DBA and I make more than 98% of the people in the country (sweden)
5 years
The recruiter I talked to worked in IT for a few year before getting into Recruiting. So how much SQL do I really need to know? I can do joins and stuff like that
Don't trust recruiters. They use you, you use them For basic work? Just stuff like this http://www.w3schools.com/sql/default.asp Intermediate: http://use-the-index-luke.com/ Advanced: https://www.amazon.com/gp/product/190643493X/ref=ox_sc_act_title_2?ie=UTF8&amp;psc=1&amp;smid=ATVPDKIKX0DER https://www.amazon.com/gp/product/0071829423/ref=ox_sc_act_title_1?ie=UTF8&amp;psc=1&amp;smid=ATVPDKIKX0DER 
Not much Salary is dependent on how well you sell yourself to your employer, how well known you are, your previous salary, location and luck I always lie about I know more than I do 
Badly worded. I meant in work interviews if applicable 
If you don't dabble in databases, you probably do dabble in Excel? Try the [Querystorm](https://www.querystorm.com/) plugin and turn your excel sheet into a table.
I think it works on aggregates, lots of waits increasing load on server bringing it crashing down; haven't looked at in much detail but I'm quite sure the brackets execute the code inside over an object designed to trigger a table scan so it runs and hangs for every record in the table
If you had 6,307,200 records it'd run the query for a year on full table scan :-) ~31,536,000 seconds per year (60x60x24x365)/5 = 6,307,200
So you mentioned temping, which isn't as well regarded IMO as a FT hire. Some of the time, FT hires get trained for like 6 months while a temp is in and out by that time.
Small school in upper Michigan. I studied Political Science but came from a programming background since I was a kid. Also ran and founded my own startup for two years and did a lot of free work as a consultant.
To my knowledge, delete will free disk space, but I am an analyst and not a tech guy, so there could be more to the picture. Truncate will empty the table completely and cannot be paired joins and where-statements.
I can't offer advice, but please have en conversation here as other can benefit ad well. 
I will. Even if I have an offline conversation with someone I would totally post a synopsis on here.
Yeah, My research suggests the same. Thanks!
Also what this guy said. And I'd even go one step further, because I suspect that if your website is vulnerable to SQL injection, then it's vulnerable to lots of other nonsense as well. I would strongly recommend a [Web Application Firewall](https://www.owasp.org/index.php/Web_Application_Firewall) (aka WAF). I have been seeing a dramatic increase in attacks from both inside and outside the U.S. I have some sites where I'm just blocking non-U.S. traffic entirely now. I would also suggest blacklisting a broad swath of HTTP User Agents. While they can be spoofed, it cuts down on the some of the casual script hackers who are just playing around (but creating DOS conditions with their scripts).
how much did you make when you started? 8 years is a long time. Congrats. I hope to be there one day.
Where are you located? Also, do you have a LinkedIn? I would like to connect with you and learn more from you, and maybe you can learn more about me...? 
Yeah, I was never trained on the job and I never got any benefits. Temping isn't my first choice, but I need to find work within 2 months time. That's why I am trying to figure out how much I am worth right now, so I don't under/over represent my skill levels to recruiters.
I have experience as a financial analyst and data specialist (1+ years) where I used .NET, SQL, ADO.NET, and then I started picking up Python. I have fallen in love with the Python, SQL, Excel, .NET combination. I know HTML, JavaScript, and CSS as well.
So I should be asking for at least the per capita income in my city though, right? I mean, someone with my skills should be making more money than the 'average' person in my city...?
At my past job we hired someone with more experience with you, but who didn't know any SQL, or .NET, Python, etc., and they were around 50k -- with the expectation that they could learn SQL. I'd say you need another 2ish years to really make the next jump, but fill that time in with doing some free consulting if you can. Just get entries on your resume and references who will back it up. I once was dating a girl who worked for a agency, and she was telling me about this software that they were purchasing that was going to solve all their analytic needs. I did some research and found out it was going to do absolutely nothing that they thought, then I called the company and had them verbally confirm it (pretended I wasn't affiliated with the agency and just asked broad question,) and then I got on the phone with the CEO of the agency and told him that it was a waste of money, but PS I could build a custom solution that met all their needs for half the price. In the end it was a glorified Excel spreadsheet with calculations. But that sits on my resume, and if you call my references they will speak glowingly about how amazing I am at ETL/analytics... and it helped me land a job making 60+ where I really learned about ETL/automation first hand. Now I'm making 80 and looking to make a jump towards 100 in the next 2+ years. Like I said... all how you market yourself.
I am not going to give away free consulting. What I do is valuable and I am good at it. I already know the languages you listed above, fairly well, so If wouldn't suit anyone right If I gave away my services for free to people. I am going to wait for the right opportunity. Going through recruiters seems more and more like a terrible idea. I just need to find the right person who needs my help. Recruiters make like 30% off their placements back anyways, right?
&gt; ETL/automation first hand. Are you talking about Informatica, Hadoop, SSRS, SSIS, SSAS?
We predominantly used SSIS for backend and some Python for customization. Front end was SSRS mostly with abit of Tableau.
They're injecting a CASE statement that includes SLEEP(5) -- If suddenly your website takes 5 seconds longer to respond than it normally does, then they know they've got a working SQL injection. Edit: From what I can see, it looks like they've successfully injected your query with the following: ' OR 3*2*1=6 AND 000749%(CASE WHEN (6864=6864) THEN SLEEP(5) ELSE 6864 END) -- ' Edit2: As u/CODESIGN2 pointed out, this query will likely cause 5 seconds of sleep *for each record* in your table, giving the attacker an accurate idea of how many records exist.
I recommend [this](https://www.cathrinewilhelmsen.net/2015/05/26/preparing-for-and-taking-exam-70-462-administering-microsoft-sql-server-2012-databases/) as a good starting resource. Does it have anything different from what you've done?
The trick is to join your CTE back to the original target table that you want to update. Something like: WITH CTE1 as ( ....), CTE2 as ( ....), UPDATE Target SET Target.Column = CTE2.Column FROM Schema.Table Target INNER JOIN CTE2 ON Target.PrimaryKey = CTE2.PrimaryKey
Always do it for all possible parameters. I can't tell you the number of findings I have reported over the years that would have been stopped by simple validation. It's an excellent first line of defense against a wide spectrum of vulnerabilities. But definitely not a cure-all. Always always always use parameterized prepared queries, and never concatenate some user-supplied input into the query string before you prepare it, either. 
Like I am slightly guessing at that because of the modulo it has to evaluate each record (should guarantee it needs to work it out) I'd love to sit with the author and pick their brains, or maybe get a high level overview of their life * Why not put the SQL skills they have to good use * Why specifically target that website * How much is really to gain from this type of thing (I'd never pay out on principle, I'd simply rather lose everything than give in to a bully)
[You might find this link useful](http://www.w3schools.com/sql/sql_injection.asp)
I worked for two companies that had more than 1 billion dollar worth, and I was only paid between $20-$25 an hour. I worked in finance and since that time I have learned so much more. I would say 2 times more.
Should I give the person interviewing me the pdf I wrote?
Put that shit online dog, give them a link. Make it look like a portfolio. Publish it as a blog or whatever. Start an LLC and make it a business and give yourself a title, boom, in two years you'll have something you've built upon -- and if you can provide references the work will be worth its weight in gold.
what is your experience? what skill sets do you have?
&gt; what skill sets do you have? I want to answer this question accurately, so can you please clarify what you mean?
It sounds like you have a different kind if organizational issue here. Your subscriptions should really be pointed at a distribution list instead of specific email addresses, so your basic entry level IT front line guy can just pop emails in and out of the DL via AD or Exchange. It's also a concern that people are requesting changes that often. Do you have reports that would be better served by allowing users to run them on demand instead?
Do you know. Net? Java? Python? Scala? VBA? ETL? BI? AWS? So on...
Is it just the case? I thought that might get optimised out and the modulo would have forced, or does basically every transformation in the where do that?
Yes to .NET, Python, ETL, SQL, JavaScript, HTML, VBA. ETL with only maybe 500,000 rows of data at the most. Some people say that is not big data.
IIRC, sleep is an Oracle command only (I believe the SQL Server equivalent is waitfor) so this also tell him what DB you are running.
Assuming you're backing up your xaction logs...? Don't log files fill up sometimes when there's a "stuck" transaction running against the database, so it can't actually clear the log? What's `log_reuse_wait_desc` say? 
I agree with /u/macfergusson and /u/Eleventhousand. If there's this much turnover happening, you have to push people into self-service or you're going to go mad. Or you'll be spending far too much money (admin time) managing these subscriptions. Encourage people to just run the reports on demand and for the people who *need* their reports to appear in their inbox every Tuesday morning, show them how to create their own subscriptions. Create a step-by-step document and accompanying screencast and post both to the company intranet. There's accommodating people who need help and then there's enabling those who refuse to learn how to do simple things for themselves. If you're "automating some subscription functions", how are you going to kick off that automation? How much time is it going to take to develop and maintain that tooling (IOW, how long until you reach a positive ROI for it)?
PREMATURE OPTIMIZATION !
It wont turn excel into a database, it will allow you to use an excel sheet as a table. Don't try to use excel as a database. 
Ah, right. Thanks for clarifying 
They're available on the BI Edition (available for 2012/2014). 2016 they got rid of BI edition and its only on enterprise. 
Wouldn't that just shift the problem to individual report permission management? Our website was never set up like that because the company grew exponentially and we were all kind of learning as we went. We have no one with SSRS experience outside of what they've taught themselves here on the job. We still don't have a dedicated DBA, or even anyone with DBA experience for that matter. Our entire IT/dev department is 7 people for a 650-person company... 
Good article, thanks
simply put the intermediate step in a subquery or a CTE: select avg( t.Devices_Nbr) from ( select count(distinct(deviceid)) as Devices_Nbr from asr.tblDeviceLevelLoadData where dateadded &gt; '2016-12-01' and datepart(dw,DATEadded) not in (1,7) ) t
For example, you can use CASE expression, like this: case @parameter when 1 then [Job Title Parameter] when 2 then [My Second Column] when 3 then [My Third Column] else 'N/A' end
Weirdly I started out in a government job as a student. The previous students were morons and there were piles of work that were back logged. I caught that up in less than a month, so my daily tasks amounted to less than an hour of work. Being the government, they tried to figure out something else for me to do. At first they loaned me out to other departments. Eventually my boss was so overloaded with work that she asked if I wanted to learn SQL. 3 years later, I was a wizz at Oracle. Unfortunately, the government was on a hiring freeze and you get fired as a student the day you graduate. So I moved to Chicago and got a job doing basic SQL work. It sucked because they used SQL Server. I got switched over into that after a bit and then they asked me to start writing reports. Voila - SSRS, which I did for a year or so. Then I hated Chicago, so I moved to BFE and got a job doing SQL Server and SSRS. I've been doing the combination for almost 7 years now. Turns out it is a highly requested skill set in the Midwest since I get head hunted all the time, especially for Chicago and the burbs which I refuse to move to. Need a job? I could get you intro level around $40k depending on your skill set right now in BFE. If you like Chicago and the burbs, you can do $45k+ easy. If you don't know SSRS, it's beyond easy. I learned it in 2 days (all of my team did too).
&gt; Then, I have a second SSRS report that allows people to see the full table of the emails, **click a button to remove someone, or fill out their name and email address and hit submit to add them. ** How are you doing this? Any time I've looked into running updates through reports, the general consensus has been that it's a bad idea. Haven't found any good examples of it in action.
finally did the write-up... http://www.sbrickey.com/Tech/Blog/Post/MSSQL_Role_ID_based_Row_Level_Security also, found a much faster/easier alternative, if you're willing... http://www.sbrickey.com/Tech/Blog/Post/MSSQL_Role_ID_based_Row_Level_Security_-_Breaking_the_Rules
Use a variable. 
&gt;I could get you intro level around $40k depending on your skill set right now in BFE. What is BFE? Please tell me more about the opportunity, connect with me on LinkedIn, or PM me!
&gt; Wouldn't that just shift the problem to individual report permission management? Possibly, to a point. But why are you managing permissions on an individual report level in the first place? Divvy them up into folders and assign permissions to the *folders* by AD group. Person changes job roles? Change their AD group membership. And that goes for *lots* of other things in an MS shop - if you set up your permissions by AD group membership instead of individual users, you no longer have to fiddle with access to specific things every time a user moves around.
[removed]
Which rdbms? Looks like Oracle or mysql? 
It's implemented in a NoSQL time-series database that supports SQL - https://axibase.com/products/axibase-time-series-database/
Can you explain the license?
I'm going to agree with /u/Smoresguy We have a developer on our team that started out as an accountant and worked in banking for a number of years. Her education and experience is great at not only figuring out the reason for a business decision, but leveraging that knowledge to catch and correct stupid user requests and catching issues in the system before UAT (User Acceptance Testing). edit: Just an afterthought .... there are also accounts in our building that have some programming experience and it helps them as well. If you go this route though, just remember to be a little more careful. We had one guy figure out how to use a read-only account to create temp tables and speed up his queries. That's great and all, but he did it in an audited production system, which gave us a few weeks of headaches as we had to prove that he didn't do anything to compromise the data.
If you are unable to install your own Oracle db on your local machine, you can try out http://sqlfiddle.com/ and see if that fits your needs (it sounds like it will).
Possibly? ANSI SQL is just an accepted standard, that spans across multiple database types. At my work, we use Teradata for our data warehouse, but we use Oracle (mostly), and sometimes use MS SQL and MySQL as well for the production databases that we source from. At times, we'll need to run queries against the source databases (mostly for auditing purposes). Remembering the different SQL syntax across all these databases can be a bit challenging, so we usually try to write most of our SQL code in ANSI SQL syntax, so that it will run in different database types, so we don't have to rewrite our code when doing audits.
As someone else said, business analyst and data analyst roles both require underlying knowledge of the business, as well as solid SQL knowledge. As someone who went from cost accounting into a business analyst role, and now into a data analyst role, I've been able to apply my business knowledge to every analysis that I do. Product Managers really appreciate an analyst that is always trying to come up with answers and suggestions that will help guide the overall business. I learned SQL on the job in my Business Analyst role, and now Im learning more R and Python. I think my weakness coming from a econ/accounting background is that I only took one statistics class in my undergrad, and a lot of data analysts and data scientists come from more stats-heavy backgrounds. But you can always learn!
[removed]
Oracle 11c should be available to you to download, and you can user SQL Developer for the IDE.
Most definitely. I actually started out as a programmer then got my cert with CFA institute. Makes you doubly valuable IMO.
[removed]
Something like: CREATE TABLE Regions ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, region VARCHAR(30) NOT NULL, parent_id INT NOT NULL ); CREATE TABLE ImmChil ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, region VARCHAR(30) NOT NULL, num_of_imm_children INT NOT NULL ); CREATE VIEW RegionNums AS SELECT Regions.region, Regions.parent_id, ImmChil.num_of_imm_children, ImmChil.num_of_imm_children AS num_total_children FROM Regions JOIN ImmChil ON Regions.region = ImmChil.region; Where RegionNums is the View and Regions is the first table. ImmChil is where num_of_imm_children comes from. Total children being whatever you need to add up to get what you're looking for. 
[removed]
No I just took r3pr0b8' comment at face value. I use SQL server and already use window functions. I assumed based on the comment that the syntax for oricle would be different and didn't read further into it.
short answer: yes long answer: it's mostly the same, other than all the mysql extensions to and deviations from the standard
**Oracle Instances** Oracle's [Live SQL](https://livesql.oracle.com) gives you access to a personal Oracle 12c instance for you to practice writing queries. Oracle's free [11gR2 Express Edition](http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html), gives you a fully functional personal install, and all you need to run Oracle locally at home. [Oracle Virtual Machine Instance](http://www.oracle.com/technetwork/community/developer-vm/index.html), gives you a fully setup VM environment with Oracle install and associated tools. [Oracle Cloud Trial](https://cloud.oracle.com/tryit) is a free trial for all of Oracle cloud services, including DB in the cloud. **Oracle Integrated Development Environment Tools** [Oracle SQL Developer](http://www.oracle.com/technetwork/developer-tools/sql-developer/downloads/index.html) is a free integrated development environment that simplifies the development and management of Oracle Databases. [TOAD for Oracle Freeware](http://www.toadworld.com/m/freeware/43) is an award winning integrated development environment.
Write queries properly : parameterised, rather than incorrectly : build them up as strings - then you won't have these problems. A large proportion of the hacking worldwide uses SQL injection. If your queries aren't parameterised, then you are asking for trouble, it's idiotic. Here's a [previous comment](https://www.reddit.com/r/SQL/comments/2u1q6g/alternatives_to_blacklisting_a_bunch_of_words_and/co4jsek/) I made on the subject.
It was, in the actual query it was 4 y's.
Hey sorry for the late response back. It was a raw data issue, however I am going to practice the clause you provided in case I come across a similar issue. Thank you for the help!
Just curious - whats the difference between this cause and the one below? Most of my work and experience has been in fetching now I'm digging more into data presentation.
[removed]
is there a trigger on the table? are you sure that the command you send from the PHP ends up as is to Oracle? Check your session in Oracle and see what the session is doing and see if Oracle gets the value that you intended. there is no reason for this unless there is a trigger that changes the data on that specifi column or you actually send to Oracle something different than what you think.
My guess would be a charset problem or a problem with the PHP provider for Oracle. What happens when you execute the query from SQL*Plus?
I havent tried that, thanks for reminding me
Ok, Ill check
you'll still need a DB. most of the rdbms'es have free for education versions (and some are totally free!). I'd suggest to install one of the free rdbms (like postgres), upload some open data (from data.gov for instance) into it and start practicing. but why do you need it if it's irrelevant to your career?
Insert into comments. Select * from wine cabinet where booze &gt;1
My thought was something along the lines of [this](https://en.m.wikipedia.org/wiki/Bush_hid_the_facts). ACSII or UTF-8 characters being mistaken for UTF-16 or vice-versa. If PHP is encoding characters in a way that Oracle doesn't expect, it may cause an issue if a magic sequence is hit.
here's what you are supposed to do -- go to your manager and ask him if he's fucking kidding -- primary keys can't have duplicates
Well if it's a 1-to-many relationship, that's fine if there's duplicate foreign keys... A primary key is meant to be unique, but a foreign key is just meant to associate that entry with a set of properties related to another table. Could you ask him for a simple example of what he's trying to achieve? It usually helps if you know what he's trying to have you accomplish. Some sort of data integrity check? 
Fact tables have primary keys based off of the foreign keys associated to the dimensions. Cannot have duplicate facts as well. Perhaps dw has primary keys set to rely disable novalidate ? Either way should not happen. ..
Assuming ID is the key between the tables? SELECT carsprog.id ,carsidrec.addr_line1 ,carsidrec.addr_line2 ,carsidrec.fullname ,carsidrec.city ,carsidrec.zip ,carsidrec.st ,carsidrec.phone ,carsidrec.DBName FROM [EMDM].[dbo].[CURR_CARSProgEnrRec] carsprog INNER JOIN [EMDM].[dbo].[CURR_CARSidrec_CHG00003854] carsidrec ON carsprog.id = carsidrec.id WHERE (carsprog.ai_cohort_yr = '2016' AND carsprog.ai_cohort_sess = 'FA')
I think this might work. Select curr.column_names_you_need, curr_2.column_names_you_need From edmn.dbo.first_table as curr Left join edmn.second_table as curr_2 on curr.id = curr_2.id
google "GROUP BY"
Yup, google SUM and GROUP BY
Done and done, thanks :)
Yes, for sure I'd like to select A and B.ColumnINeed. What is that called? Some kind of hybrid/combo selection, etc.? The name matters, because I will need to look up the equivalent for sqlalchemy.
In Microsoft SQL what you are referring to is called joining. There are several types; left joins, right joins, inner joins, outer joins and combinations of the terms. For example, an inner join will return all of the rows from table A and table B that have a match. I hope this helps. I'm sorry if I misunderstand your question.
I didn't know what the *select* portion is called when there are more than one table names there. I got the syntax to work in sqlalchemy by merely adding parens, like (TableA, TableB.columnINeed). With that library, so far as I've seen, there is no need to specify 'inner join', over just 'join' (perhaps there are some performance benefits, etc.). It returns a list of two-tuples, where one part is the row from TableA, and the second is the column from TableB. Thanks.
It doesn't have a name, it's a completely standard, ordinary use case of a select query.
Oh! I totally misread that. In MSSQL Server there are sys.tables and sys.columns tables, so you can use the table name to find all of its column names: SELECT c.name AS column_name FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID WHERE t.name LIKE '%table1%' But with this approach it sounds like you're going to be generating some dynamic SQL which can get really messy... maybe there's a better way to approach this (If there is, I'm not aware of it).
Thank you ihaxr, I will try this approach. 
It's still just a select. You can select either all(*) or specific columns from any tables in the from portion of the query. It does not have a special name because it is just an ordinary select.
W3 schools is a good start. Also, sidebar
ANSI and ISO publish the universal standard SQL. You can buy it on thier website. Stored procedures are stored in the database and hide complex SQL from the user. You gain performance by placing frequently used SQL queris as stored procedures. Functions are easy start with aggregate and mathematical. You can you tube these. You download an SQL app to practice on the go. Code Academy has three sections of hands on SQL for free.
Just for clarification, only non-aggregated columns are required to be in the grouping clause.
Stored procedured and functios aren't different from procedures and function in c#. Stores procedures are just queries, with logic, flow logic, and perhaps a return statement. Functions, could for instance be logic that takes two parameters and returns the factor of those two parameters. Learning the syntax is whats important, the rest is just dreaming up what you want to do. Bear in mind, just because you can, doesn't mean you should. I've got a couple of SPs that, important excel sheets into table, they aren't necessarily the right way of doing it. But it works, for the purpose. I also have some functions with makes sure that calls from other Users are executed correctly, regardless of what the user wants to input into the function. 
We schools aren't very good in terms of it on showing syntax, not why the syntax is the way it is. Imo. 
&gt;Can I sortof learn both of these at once? Is the SQL common enough to do that or should I only focus on one or the other? Yes, you can absolutely learn both of them at once. There are similarities and there are differences and if you want to be *that guy* you will need to be familiar with the ins and outs of different platforms. If you want to flex your C# skills, build an abstraction layer that allows you to parameterize &amp; execute queries against either platform. &gt;but with SQL, I don't even have a dummy DB to work with Then get on it. SQL Developer is free and full featured. MySQL also obviously free. EDIT: Final thought, stored procedures are *way* overrated. Table-Valued Functions are the way to go, IMO.
In my opinion, if you truly want to be a SQL wizard you need to learn what is happening under the covers. First, learn about bind variables. This is separate from what you are asking, but I suggest it to everyone learning SQL. Improves perf and reduces risk of SQL injection. I come from Oracle, so the terms may be different, but concepts should be similar. Learn about parse-bind-execute, what each step does and how each impacts performance. Know what execution plans are and the impacts of various execution steps (Full Table Scan, Hash Join vs Nested Loops, etc.) Know how indexes and statistics work. A lot of this will be specific to a platform, some will be common. There are some quirks per platform, like in Oracle using a function in a where clause, like to_upper(column), will cause the optimizer to ignore a standard index on that column. I learned mostly via experience and the official DBA class study guides. Learning how to pull diagnostic and tuning information out of the database will be specific to that platform, this is a very important skill. SQL itself is fairly common across platforms, but stored procedures are typically different on each one. I would not recommend learning stored procs until you have SQL down. Until you know how to tune the sql, its probably better to keep your logic in C/C#.
I would also say there's a bit of a mind shift. Business logic does not belong in the database. It's there a lot of times, but a database should hold data, keep the integrity of data, and show proper relations to the data. From there, SQL works best when doing single transactions, not looping. You will need to use loops from time to time, but most commonly, a single big transaction is better. (See googling RBAR, row by agonizing row.) Just a few additions, but all solid input.
Yeah, I asked reddit before I truly thought it through. I did it with !=. Thanks for the reply
Thanks! That's helpful. I don't know exactly what flavor. Some manger asked my manager who can read SQL and I got sent an Excel file with a tab with a CTE in it. They wanted to make sure that the table built in the first part was just a look up and not a limiting factor. At first I thought the implicit joins would make it so but those (+)s were weird and I don't recall seeing them before. Based on what you shared, am I reading things right that the (+) should be on the look-up side of the equation like: FROM accounts, phoneNumbers WHERE accounts.ID = (+) phoneNumbers.AccountID to come out as FROM accounts LEFT JOIN phoneNumbers ON accounts.ID = phoneNumbers.AccountID
you could put UNION between the 2 queries (remove the first ORDER BY). You could make it 1 query using an OR but it gets pretty ugly.
so just to address fanboi flamewars... not much diff from WinForms vs WPF, WebForms vs MVC, or WebServices/SOAP vs WCF... php vs whatever MySQL vs MSSQL vs XYZ... - obviously you're comfortable with the MS stack... MSSQL is going to be an obvious easy fit, since it has ADO.Net built in, etc. - MySQL has multiple engines... the MyISAM engine has the longest history, and the best performance (that i'm aware of), but it also has... we'll call them "quirks" (compared to what most DBAs expect from the DB engine)... feel free to learn either, but I would ask that you START with InnoDB, for a more consistent experience with other DB systems (MSSQL, postgres, oracle, etc). - consider other options too, such as postgres, which is starting to get a big following after MySQL was forked into MariaDB after Oracle purchased MySQL. Postgres is free/OSS, has a lot of the behaviors that DBAs expect from their DB platform, and is seeing an explosion of feature support. - you MAY want to even consider Oracle, which supports both TSQL (the language that you'd be using for all DB platforms thus far), as well as PL-SQL. - If you're playing in the MSSQL world, you can also try playing around with SQLCLR, which puts CLR/IL code into SQL... not helpful for simple stuff like "select * where x", but very helpful for large processes and/or calculations where you'd rather run the logic on the DB server than on an app server. - Since you're familiar with the code side of things, play around with various ORMs (EF, NHibernate, Dapper, etc)... see what code they generate, see how you can improve them (especially when it involves lazy loading, loading related objects, or has complex conditions)
Yep, you've got it.
I retrieve the header's ID to associate it with the data. One-to-many relationship, the table names are literally Product and ProductAttribute. They're both big transactional tables so I think the enabling/reenabling would be too time consuming, especially if the data table is around 1.2 billion rows? Edit: The header rows have no unique identifier before they're inserted, so I really don't have a way to associate header with data without the ID
Depreciated, but still widely used. That's how I learned joins on college in 2011, and I still see it a lot in the workplace. Correct it to ANSI-style joins whenever I refactor someone else's code. 
By "Microsoft sheets" do you mean Excel? *shudders* Are salespeople just manually typing things into Excel? Do you have any kind of POS system? Does the company have any IT? It might make sense to run a physical database server for the company, but it sounds like that's above your level of expertise. What is your job title/role? Who is going to be using this database? 
You would be depressed if you knew how many small retail/restaurant groups that still do this shit. Its usually inexperienced management, not wanting to spend money, or not giving a shit. Usually a combo of all 3. I see it a lot through my job.
My company bought a company for $330 million. They didn't have any computerized records. I was in accounting at the time and everything was on ledger paper in old school ledger books. Everything was done by hand there. It was the late 00's and the company was a telecommunications company as well.
The way I learned it, the (+) was meant to indicate which field should include null values in the result set. The (+) accompanies the field name not the = sign - still not a great explanation but makes it a bit easier to remember (many legacy queries at my company use this syntax)
&gt; Is this MMJ? Lol I was thinking the same thing.
This is great. I will try the 30 day trial of the service. Hopefully I can test if I have the aptitude to do this and also see if I can derive some meaningful information as well. Much appreciated.
I know too much.
I'm talking about medical MJ
Microsoft SQL Server Management Studio 13.0.16100.1 Omg..... You're an absolute hero. Seriously. I've been programming in C, C++, C#, JavaScript, and more and out of ALL of that, this has been the single most frustrating thing so far... This is what stopped me from learning SQL in more detail a couple of months back. And it's solved now, thanks to you. **And "attaching" the mdf worked!!!!** Thank God I don't actually have to restore a backup. So I'm guessing mdf is the actual database binary file that SQL Server uses to store the data?
Glad to see it worked! And yes, mdf files are where data is stored.
Each implementation is different, but if you stick to ANSI SQL, it should work on all platforms.
Use select distinct
&gt; there's nothing wrong with looping https://www.youtube.com/watch?v=yOIUWdJWaec#t=00m06s
How did you get the MDF file if you didn't restore the BAK file? All of these sample databases are distributed as BAKs.
They usually are distributed as bak from Microsoft. Third party sites zip the mdf to share. Prior to 2008R2 versions they zip them with the mdf and log file. I prefer the mdf because it doesn't require a useless installer to run. 
After Googling 'vocalcom dialer' I've decided there's no way in hell I'm going to help get *that* software up and running. Good luck.
For reasons like this. Sometimes the BAK doesn't restore properly. Other times, Microsoft discontinues an older version and updates to a new version. You should take precaution whenever you download the file but those sample databases that used to be from microsoft are now held on codeplex (third party site unless microsoft owns CodePlex then I stand corrected) I believe and Microsoft wants you to use newer versions of SQL Server. The useless installer i'm referring to is on this [page](https://msdn.microsoft.com/en-us/library/8b6y4c7s(v=vs.110).aspx) Step 2. The installer isn't really useless but just unnecessary when you want to do the restoration yourself. Microsoft has you download the file, then **install** it. When you can just restore or attach an MDF. Just giving OP options since he was having issues with the initial setup. 
From what I gather it seems to be native to windows. But I was considering using wineskin to be able to use it as a last resort
I'm using this instead because "IN" can't support the wildcard. This works but it's slow. Also for any future readers, Access needs parentheses after the ON if you're using AND with it. 
Check out SQLPro (I think someone else linked it below). With coupon code [ATP20](http://atp.fm/episodes/201) you can get 20% off. Bonus: It's not just for SQL Server - it covers MySQL, MariaDB, Postgres, SQL Server2005+ and Oracle.
I think TOAD may have a solution for you?
Your query is almost there. I would suggest using the JOIN keyword rather than the WHERE clause to join tables, as it's easier to use and easier to see that it's a join and not a "filter". Try this: SELECT students.column1, students.column2, course.column3, course.column4 FROM students JOIN course ON students.studentid = course.studentid; (Replace the column1, column2, etc with the actual columns you want to show, e.g. course_name or student_name or whatever you have)
I heard about this as well, I may have to give it a shot
Thats awesome. If I give this to my work they may pay for it with the discount. Thanks a ton!
You're looking for the `UNION` operator. http://www.w3schools.com/sql/sql_union.asp (SELECT Name FROM teacher) UNION (SELECT Name FROM student) Keep in mind this will NOT show duplicates. So if a Teacher and Student have the same name, only 1 of them will be listed (assuming you're not selecting multiple columns which will make the two unique). You can do something like this to ensure both are shown: (SELECT Name, 'Teacher' as type FROM teacher) UNION (SELECT Name, 'Student' as type FROM student)
D'oh. Can you tell I rarely use `UNION`s...
You are missing commas. Syntax errors are common so be sure to watch out for them! http://sqlfiddle.com/#!9/e6d8a
Instead of looking for a separate place to type a DOMAIN, have just tried DOMAIN\username wherever you're typing in your user name?
Good catch. I actually discovered this was possible half way through my debacle. Got connected to SQLPro for MSSQL using that
I always think it's much more powerful to have the raw data unpivoted and I would definitely do it the 2nd way. Matrix style will also make it easier to make it dynamic and easy to change.
I'm not sure if I fully understand the problem, and I don't have a MYSQL instance to check for syntax, but I think this is what you're looking for: SELECT * FROM hires WHERE '2017-12-05' between hireStartDate AND hireEndDate OR '2017-12-30' between hireStartDate AND hireEndDate; This selects any rows where the specified dates fall between the start and end dates. 
I don't make the budget =) 
I wish changing the dialers was my call... 
Is Oracle the flavor you want to use? MySQL is the most widely used SQL flavor followed by Oracle and SQL Server. The best way to learn is always hands on in this scenario. There are sample databases out there you can google and download and attach that can give you a good foundation to poke things.
I'm not sure what flavors are out there lol. I've always been kind of confused with the different servers and SQL people use. 
Break this into steps. 3 months from now is relative, every month is relative, every 4 weeks is relative, etc. Every 3 months from what? From now? Find out what now is. Ok, now you need to figure out what months are. Does the three months have 90 days or 88? Figure out from now, what are 3 months from now. Figure out what 6 months from now is. Ok, now you need to do case when manipulation. Case when 3 months then get the value from above we just talked about. Now that you can decide between strings to numerical and they have reference points, add those to your initial reference of today. Briefer explanation: Getdate() = 12/30/16 Find month value of 3 months from today = Jan / Feb / March, calculate the days in each month for the year. If 'Every 3 Months' then take the days you calculated above add them to getdate(), repeat for rest of variables.
Case Statement? CASE &amp;nbsp; WHEN Column = 'Every 3 Months" THEN 1/3 WHEN Column = 'Every 6 Months" THEN 1/6 WHEN Column = 'Every Month" THEN 1 WHEN Column = 'Every 3 Months" THEN 4 ELSE 0 END 
Do you already have experience using the language and need hands-on practice? If so, you need to choose which system you want to use (MySQL, oracle, sql server) and find appropriate database examples through Google to practice on.
I'd get [SQL Server Express](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) and the [AdventureWorks](https://msftdbprodsamples.codeplex.com/) sample databases. YMMV, but in my experience SQL Server is easier to get up and running and easier to learn with than than MySQL or Oracle.
Loss of connection can be two things. A) your client pinged put. B) the server killed the connection. In a, the query may still be running, but you won't get a result, because you're no longer connected. In b, the server killed your process and connection probably because it violated one or more of the rules implemented to limit user load. 
It was A
 2017-12-05 2017-12-30 | | | | 1. S------E | | | | | | 2. S------E | | | | | 3. | S--------E | | | | | 4. S------------------------E | | | | 5. | S--------E | | | | 6. | | S------E S = hireStartDate E = hireEndDate you want cases 2 through 5, which have some overlap WHERE end_time &gt;= '2017-12-05' /* eliminates case 1 */ AND start_time &lt;= '2017-12-30' /* eliminates case 6 */ 
If I am understanding you correctly, you want all courses that have students even if they do not have a teacher. I would recommend a left join from courses to teachers in order to obtain all courses (even if a course does not have a teacher). Such as: SELECT course.coursecode, course.coursename, course.teacherbsn, amount_students, teacher.name, teacher.surname FROM course LEFT JOIN teacher ON course.teacherbsn = teacher.teacherbsn WHERE amount_students &gt; 0; Let me know if I'm not understanding you. Cheers.
Just as a side note, SQL Server Dev 2016 is free and contains everything. 
Don't use Express, use the dev version. [Here](https://blogs.technet.microsoft.com/dataplatforminsider/2016/03/31/microsoft-sql-server-developer-edition-is-now-free/) Adventureworks is outdated, use [World Wide Imports](https://blogs.msdn.microsoft.com/samlester/2016/06/13/so-long-adventureworks-well-miss-you-and-hello-world-wide-importers/)
Nice! Is that actually going to do what they want? Good stuff. Yeah, "or" sucks.
Cool. Yeah, I saw that after I posted and figured either will work for learning purposes. 
An Inner Join is the basic matching of two tables, right? Whatever you join on, if both tables have a record that matches, those two records are mated into one line for your result set. If there are no matching records for a row, neither table's row is added to your result set. In a LEFT JOIN, the first table (your left table) will have all rows in the result set, and the second (right side) table will have only rows that match your join condition added to the result set. If your Left table row doesn't have a matching row on the right table, that portion of the result set is just null, but the left table row is still in the results. Make sense?
Thanks for the help. I've searched around a bit on google and I think I understand how it works now.
Something like COUNT( DISTINCT CASE WHEN items.size&gt;=n THEN itemid ELSE NULL END) AS ItemsofSizeN 
I guess you are looking for HAVING here: SELECT s.setId, COUNT(i.itemId) FROM sets AS s RIGHT JOIN items AS i ON s.setId=i.setId GROUP BY s.setId HAVING COUNT(i.itemId) &gt; 1 EDIT: [reference](https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html)
Wouldn't that discard groups less than two? I think he's looking for filtering on the attribute size in the items table, as opposed to filtering on the number of tuples in each group.
Do you have a fixed set of columns you're selecting from? If so, assuming they are all of the same datatype, you could do something like: SET @result = (SELECT TOP 1 CASE @columnName WHEN 'x' THEN ColumnX WHEN 'y' THEN ColumnY WHEN 'z' THEN ColumnZ END FROM StaticTable WHERE Age = @age) RETURN @result
Check math on 1.66
We're on an older one, going through an upgrade right now (standalone onsite dialer moving to a cloud based dialer). Do you control the hardware? Is it onsite? We had an issue with our dialer this past week due to a memory leak. Outbound dialing would be running at a snails pace. All we basically do is reboot all the servers, and run a script to fire up all the windows services in a particular order. 
&gt; Wouldn't that just be a condition in the where clause? items.size &gt; someNumber? Or Am I misunderstanding something? I'd like to get the total number of items in the set *and* the number of items of a certain size in the same query. &gt;Also, is there any particular reason that you're right joining? Not saying it's wrong it's just seen maybe 90% less than left join. Honestly it's just because I think it looks neater to write Sets before Items.
Ah, that looks promising! I'll give it a try.
Glad to hear! And looks like you got the LEFT JOIN logic down now too. 
You already have your answer gfrom /u/UpUpDnDnLRLRBA, just please keep in mind T-SQL scalar functions are the root of all performance evil. If this is for something performance heavy, consider refactoring into an inline table-valued function and using it with cross/outer apply, e.g.: CREATE FUNCTION GetResultingValues ( @columnName nvarchar(255) , @age nvarchar(3) ) RETURNS TABLE AS RETURN ( SELECT TOP 1 CASE @columnName WHEN 'A' THEN T.A WHEN 'B' THEN T.B WHEN 'C' THEN T.C ELSE T.D END AS Whatever FROM StaticTable T WHERE Age = @age ) GO SELECT Result.Whatever FROM dbo.GetResultingValues('A', 50) Result This is like a parametrized view and SQL Server can apply similar optimization techniques to it, like parallelization, statistics and what not.
I would have used SUM( CASE WHEN items.size&gt;=n THEN 1 ELSE 0 END) AS ItemsofSizeN so there is no need for the distinct
You could be correct. I am creating the query for a dashboard and the specification was "We need to maintain a 98% resolution rate for all accounts over 180 days from discharge" Which I assumed: GETDATE() &gt;= Dateadd(d, 180, a.DischargeDate) took care of. This would mean today's date is greater than 180 days from the accounts discharge date. Does this sound wrong in any way?
Well, you could write a CLR function, and do dirty dirty things in the C# ..... but well.... don't. Other than that, no, it is not possible to execute dynamic SQL or stored Procedures from inside a function.
I'd recommend [The Kimball Group Reader](https://www.amazon.com/Kimball-Group-Reader-Relentlessly-Intelligence/dp/1119216311/). Its focus is on the data warehouse but really, that is *the* technical solution which BI is built around. So if you are going to progress to an architect that specializes in BI, you **have** to know data warehousing in and out. Sadly, the SQL platform kind of lags behind more procedural languages when it comes to the testing and deployment workflows. Not sure why but I'd also like to find more architect level books that give examples of how or what to implement to help your BI environment function smoothly day to day. 
Was this a common habit maybe 10-20 years ago? I see a lot of this when working with older developers. I always ask why they use this and they have no answer. But I also don't have a better solution but I know the existing one is not one I would want to become a habit. I'd prefer a join in this case if it was a better option to get that value rather than a function.
Learn about first normal form, second normal form, and third normal form. Learn that, backwards and forwards, and the rest of SQL will come easy. 
To answer #3 without getting technical: They are different databases made by different companies or groups. Oracle and MS (Microsoft) are the most common commercial products. MySQL and PostgreSQL are arguable the most common free ones. There are also 100% free and legal versions of Oracle and MS SQL Server you can download for personal use. Most of the differences are quite technical, you don't need to worry about them right now. The ones that will affect you directly right now are: 1) The program you use to write the SQL and interact with the actual database and 2) The slight differences in the dialects of SQL they use. For #1) This is largely a matter of opinion. But the opinion that Oracle's SQL Studio sucks is pretty common. We use MS SQL Server at work, and SQL Serve Management Studio is pretty decent to use. Some of us use a third party program called TOAD that I really like. For #2) It's largely a matter of what you get used to. To start, just pick one and stick with it until you know what you're doing. After that, it's not difficult to change to another dialect. For learning.... Once you get the hang of the basics of SELECTING, WHERE clause logic, joins, and the common aggregate functions, *then* start worrying about what's going on under the hood of the DB. Especially indexes and how they affect performance.
Great response, thanks!
Codewars is pretty great. I haven't gotten to the lower katas yet though.
lower katas?
https://msdn.microsoft.com/en-us/library/mt204009.aspx
My translation: you want the member record and an additional column showing the comment count total for the member of a record. **Windowed Function ...** select m.*, count(*) over( partition by m.userid ) msg_count from members m join comments c on c.userid = m.userid where m.networkid = :netid Windowed functions, as supported in Oracle, PostgreSQL, SQLServer (I think), maybe even MySql. 
data tools is an add on you can get from Microsoft that includes tools for SQL server and analysis services. Its good stuff to know if you're interested in doing BI or DW work in the Microsoft stack of tools. 
https://msdn.microsoft.com/en-us/library/mt204009.aspx
Star Schema by Christopher Adamson is pretty good
Do you have any links of sites? I can't find any projects using adventure works 2012. Thanks
I'll assume this is a homework assignment. You'll want to read up on the keyword JOIN AS IN: SELECT * FROM SomeTable a INNER JOIN OtherTable b ON a.somthing = b.something You can keep on joining tables to your query. Also I wouldn't call RoomPeople a pivot table, unless the data in it is pivoted, in which case you'll need to unpivot it to do anything. 
Great, thanks. Sorry I was unclear. Also, why are you grouping by items.id in addition to sets.id?
&gt; I want to create a function I don't think so. Homework, and you don't even know how to ask a question? It shall be answered. SQL Server: CREATE FUNCTION Homework.SolveAllMyProblems ( @User1 INT, @User2 INT ) RETURNS @ret TABLE ( SharedRoomID INT NOT NULL ) AS BEGIN DECLARE @RoomID INT, @UserID INT DECLARE c CURSOR LOCAL FAST_FORWARD FOR SELECT RoomID, UserID FROM Homework.RoomPeople OPEN c FETCH NEXT FROM c INTO @RoomID, @UserID WHILE @@FETCH_STATUS = 0 BEGIN IF @UserID IN (@User1, @User2) BEGIN INSERT @ret (SharedRoomID) VALUES (@RoomID) END FETCH NEXT FROM c INTO @RoomID, @UserID END ;WITH YouShouldLearnStuff AS ( SELECT COUNT(*) OVER (PARTITION BY SharedRoomID) AS cnt , ROW_NUMBER() OVER (PARTITION BY SharedRoomID ORDER BY SharedRoomID) AS rn FROM @ret ) DELETE FROM YouShouldLearnStuff WHERE POWER(cnt, rn) != 2 CLOSE C DEALLOCATE c RETURN END Ok, I feel kind of bad. Reply if you can't find a proper solution and I'll post it.
You don't need joins, if you already know IDs: SELECT RP.RoomID FROM RoomPeople RP WHERE RP.UserID IN (69, 96) GROUP BY RP.RoomID HAVING COUNT(*) = 2 But you should not deal with a database at all if you don't know anything about them. ORM is not an excuse, you can make some terrible mistakes if you don't know what you're doing. SQL is simple enough to learn in a day or two.
Related projects * [ELVM](https://github.com/shinh/elvm) EsoLangVM Compiler Infrastructure * [8cc.vim](https://github.com/rhysd/8cc.vim) C compiler written in Vim Script * [8cc.tex](https://github.com/hak7a3/8cc.tex) C compiler written in TeX * [constexpr-8cc](https://github.com/kw-udon/constexpr-8cc) Compile-time C Compiler implemented as C++14 constant expressions 
Yeah sorry, that's an error. Of course we would just want to group by sets.id. I commented out that bit in the above code.
I don't want total posts to be a value in a database I want it to be calculated within the query. Post could be the unique identifier yes. It's to be used with a commenting system which uses php / MySQL 
If you want 1 row per user per day with a count of their totals comments, then I would do something like; select comments.*, comments.userid, count(comments.userid) as total from comments LEFT JOIN members on comments.userid = members.userid where networkid = :netid GROUP BY comments.postedat desc, comments.userid
I would go with the above, likely that a sum would be better performant than the distinct, although you never know. 
for more Count will give the wrong number as it will count the null value as one row as well giving the wrong number. So if you want to use COUNT than you should have a -1 like `COUNT( DISTINCT CASE WHEN items.size&gt;=n THEN itemid ELSE NULL END) - 1`
Thank you! I was able to do it with COALESCE
While that would work it is not necessary to have a sub query. Just multiplying inside the sum function should work. select companyname, sum(PD.PurchasePrice * pd.UnitsPurchased) netcost, sum(DP.DividendPaid * PD.unitspurchased) [dividendsPaid] FROM dbo.AssetName AS AN JOIN dbo.PurchaseDetails AS PD ON pd.AssetID = an.AssetID JOIN dbo.DividendsPaid AS DP ON dp.AssetID = PD.AssetID group by companyname and BTW the order by does not work in a view.
I had already attempted this previously and your query here gives the same result as my post below - have copied and pasted. Basically the summed values are not correct. Thanks for the input. I had tried this previously and other variants of this and it gives me a distorted answer which I couldn't figure out. Now the totals are inflated. [This is the result](http://imgur.com/a/TmoFL). I cannot figure out why the totals are so skewed. For instance - the row 23 net cost should be 984.94 but it is now 9,849.40. Seems easy enough as it is x10 but then I look at row 3 where the net cost should be 5,139.90 but instead it is 87,378.30. I just cannot understand what is happening.
I think that is because you have multiple rows purchases and dividends per asset and as they are joined you get a Cartesian production. what you need is something like with pd as ( select AssetID, sum(PD.PurchasePrice * pd.UnitsPurchased) netcost from dbo.PurchaseDetails group by AssetID ), DP AS ( select AssetID, sum(DP.DividendPaid * PD.unitspurchased) [dividendsPaid] from dbo.PurchaseDetails group by AssetID ) select an.companyname, pd.netcost, dp.dividendsPaid FROM dbo.AssetName AS AN JOIN PD ON pd.AssetID = an.AssetID JOIN DP ON dp.AssetID = PD.AssetID 
Try removing the GROUP BY and aggregates and look at the detailed data that makes up those totals. Include other fields from the purchase details column as well, and narrow it down to one company/asset if it helps. Maybe there's more coming from the purchase details table than you were expecting - i.e., for Rogers maybe there's 10 rows with variations tracking the status of a purchase but only one 'completed' record, so maybe you need to filter on something else to only pull in the completed data (just an example, not necessarily the case). 
I actually just finished a site aimed at helping people learn sql. It can also be helpful in finding movies on netflix/amazon prime. http://selectstarfrommovies.com
Glad you got it working but... Restoring backups is something you must learn to master. One concept that is hard at first is that you restore into an existing database. When I went to put Adventure Works onto this laptop (for example) I created an AdventureWorks database putting the files where I wanted them to be. I did not run any scripts against the database. I next restored the .bak that I downloaded from Microsoft making sure that I used the option to use my existing files rather than the ones encoded in the .bak files. Worked great.
It's really about a quirk in SQL Server, where parallel processing is disabled for any query that uses any form of a scalar function. It's a true pain in the ass and Microsoft should really work on fixing it.
Thank you! Sorry for the simple question but that was super helpful.
Do you mean you don't want a player to show up if a that player shows up in more than %40 of lineups?
Yeah like for example. If I set the optimizer to display 100 results, I also want to add a constraint that allows me to limit a player to how many results he will be in. Like if i set player A to 40% he will show no more than 40% in results. 
Ah you still want him to show up, maybe someone more knowledgeable than me will have a different answer but to me it sounds like you may to have to do that as a two shot deal with a CTE and PARTITION BY Ranking as I am not sure if HAVING will help in this type of instance. If you want more details than that you may have to post your existing code. Basically your CTE will select every player that fits in whatever that top X result is and rank each iteration of the player from 1 to 100 and generate a "table" of those results, then you would select from that table with the constraint that the player "JoeBob McPunchFace" only shows up when his rank is below 40 or whatever Rank number equates to 40% of the X is. Also I am not sure if mysql supports partition by or Having for that matter.
I second the Kimball Group Reader, but it somewhat assumes that you already understand the material in The Data Warehouse Toolkit. I would also recommend reading up on [Data Vault](http://danlinstedt.com/) modeling.
Table relationships are man made, it's the first thing a person is supposed to think about when designing a database. So the only person that can tell you about whatever the database you are using is the DBA or if he did his job then his notes on the database.
Use the preformatted text in the dropdown when you're making the comments, it solves this
Use the {code} tag
why not just install mysql(or any other rdbms) and start building and manipulating databases? make tables for your cd collection or something. 
&gt; Skype for business Why the hell did they have to name it this? What was wrong with the name "Lync"?
Communicator was awesome - simply because of (bah)(wip). Looked like the sheep was crapping cubes.
Poopin' cubes like a wombat.
[http://sql-ex.ru](http://sql-ex.ru/)
&gt; IMHO, this is a little lazy and may be prone to unexpected data types/sizes in the resulting table. And collations, too, if not all of your tables use the same collation.
[removed]
it's the worst. It also inserts invisible characters 
SELECT INTO is really nice, but it won't copy your keys, indexes, collations or other table attributes. Still, it's hella useful for development purposes.
Which RDBMS are you using? I'll assume it is either MSSQL or Oracle from the use of PIVOT, in either case it would be better to get the Format([Date Issued],"mmm") in the SELECT and when you make the PIVOT just call the alias.
Access.
I think since Microsoft owns Skype now and how Skype is the popular name, they wanted to make communication software more uniform to a singular name and just called it Skype for Business. Problem is is that this "Skype For Business" sucks horribly compared to the real Skype. Mic echo, lag with some actions, lack of message editing, etc.. They did come out with Microsoft Teams which is somewhat better and which my company is starting to use.
top ten -- SELECT payor_id , COUNT(*) as num_sent FROM payments GROUP BY payor_id ORDER BY num_sent DESC LIMIT 10 who paid back a user -- SELECT DISTINCT payee.payor_id FROM payments AS payor INNER JOIN payments AS payee ON payee.payor_id = payor.target_id INNER JOIN payments AS backatcha ON backatcha.target_id = payor.payor_id WHERE payor.payor_id = 'Todd' 
So for number 1 you are starting off on the wrong foot by only looking at payors. You have a "master list" of users so let's start with that. SELECT TOP 10 user_id, COUNT(*) as number_of_transactions, AVG(amount) as average_transaction FROM users u INNER JOIN payments p ON (u.user_id = p.payor_id or u.user_id = p.target_id) GROUP BY user_id ORDER BY COUNT(*) DESC For #2, your specified solution is only going to return records where payor_id and target_id are the same. And I assume that will be nothing since people aren't paying themselves, right? Here we want to start with the full list of payments that each user has made as the payor and then only include that payment if there is another payment with the same counterparty where the roles are reversed. By using "DISTINCT" we will eliminate the duplicates and wind up with a list of each user and all of the other users that they have exchanged money both ways with. SELECT DISTINCT p1.payor_id, p1.target_id FROM payments p1 WHERE EXISTS (SELECT 1 FROM payments p2 WHERE p2.payor_id = p1.target_id AND p2.target_id = p1.payor_id) ORDER BY p1.payor_id 
Your top ten solution doesn't adhere to the requirement. OP specified that we are to count transactions where the user is the payor OR the target and that we are to include the average amount of their transactions. 
No problem. Let me know if you hit any issues with them and I can help you adjust (sorry about the MSSQL syntax - that is just what I am most comfortable in).
thanks for spotting that... i rushed to the conclusion that what OP was missing was the "top" part for your information, `TOP` is not valid in SQlite, use LIMIT
AdventureWorks is direct from MS and sounds like what you want. 
Try to use Stackoverflow databases Here's man how to download and use: https://www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/
Wish this was as a webinar, as I would love to watch it. But im not about to travel to the US for it.
You have to delete the related record (matching KartaID value) from the table dbo.JazykKarty first.
BTW for that example of yours, instead of use (1/3), make it (1.0/3.0) The reason is that MSSQL is making that division return an integer since both elements are integer, so (1/3) = 0, while (1.0/3.0) = 0.333333 since it returning a float. About the division by 0, make ISNULL(A.[2013NR], 1) instead, it will return 1 if A.[2013NR] is null, for MSSQL any calculation with Null will result in Null.
I agree SQL Server documentation is awesome, but finding that specific entry would be difficult if you didn't know it existed.
It has a foreign key defined "FK". That is why you must delete from the child before the parent. What ninjaroach said is what you need to do.
Ok, getting some results now. However, they;re not matching what I'm expecting. Here are a few examples: RollUpCustomer | 2013TCNR | 2014TCNR | 2015TCNR | 2016TCNR | SQL CAGR | Excel CAGR :-- | --: | --: | --: | --: | --: | --: Comp A | 18458 | 23910 | 64735 | 44387 | -1 | 0.34 Comp B | 37016 | 64918 | 50082 | 41821 | 112.0355 | 0.042 Comp C | 10075 | 11028 | 39466 | 41091 | -19 | 0.598 I also realized there isn't any nulls in the table, just zeros, so I changed the formula to: SUM(CASE WHEN A.[2013NR] = 0 THEN NULL ELSE (POWER((A.[2016NR]/A.[2013NR]),(1.0/3.0)))-1.0 END) 
I may have found something. Try not creating the connection object before the loop and then closing it after the loop. I think you're trying to use a closed connection on the second iteration. 
there are 3-4 versions of adventureworks from microsoft all good for different reasons - if you are already looking for MS SQL you have the added advantage of lots of the examples from microsoft material will use those databases or parts of them as references for their examples.
Are A.[2016NR] or A.[2013NR] INTs? If so you'll want to cast one of them as FLOAT. 
Yeah. looks like you are still doing int math. so times it by 1.0 or explicitly convert to float: SELECT * , [Sql CAGR] = CONVERT(Decimal(8,3),POWER(([2016TCNR]*1.0/[2013TCNR]),(1.0/3.0))-1.0) FROM ( VALUES ('Comp A',18458,23910,64735,44387,0.34 ) ,('Comp B',37016,64918,50082,41821,0.042) ,('Comp C',10075,11028,39466,41091,0.598) )a(RollUpCustomer,[2013TCNR],[2014TCNR],[2015TCNR],[2016TCNR],[Excel CAGR])
[table_constraint ](https://msdn.microsoft.com/en-us/library/ms188066.aspx) &amp;nbsp; "ON UPDATE CASCADE" can also be added to the fk index so if the id is updated in the parent table the child table fk id will be updated also. Personally, I always turn on cascading deletes and updates when creating my foreign keys.
Solving this "the right way" would involve creating a table to store a hierarchy of all prerequisites, a recursive CTE to process through that structure, and an aggregate query joined to the CTE output to ensure at least 1 valid combination of pre-reqs have been met. Sounds like a really fun project but also a pretty involved effort.
Yeah, I've been spending all day on it! I went down a road of aggregating "total *AND* prerequisites required" and comparing that to the aggregate of those classes the student passed, and making sure the aggregate of OR combinations of classes they passed was greater than 0, but my code falls apart when there are more complicated combinations. Also experimenting with ROW_NUMBER to identify appropriate groupings. The hierarchy table you suggest is interesting, but I'm not really sure what it would look like. Thanks!
The MS-provided databases AdventureWorks or WideWorldImporters are great, small/medium-sized databases, that are helpful when playing with code to study for their certification exams. The StackOverflow databases contain REAL data from users, a bit more realistic to what you will encounter on the job.
That's a very good idea that maybe I will pursue. Unfortunately, the SECTIONPREREQUISITE table is structured in the way you see above (with OPEN_PARENS, CLOSE_PARENS, etc.), so I would have to figure out how to get that into the more suitable structure that you mention. And thinking of all the different permutations of something like (CR103 AND EG234 AND EG252 AND (EG261 OR EG265) AND (ET371 OR ES371) AND EG292 AND (ET201 OR ES201)) hurts my brain right now.
That's not the same behavior that I get from the same operation - I get the expected GUI. I'm using SSMS 2016 13.0.16100.1, which it tells me is the most up to date.
[removed]
I think the issue is because I'm trying to edit an azure sql db. I can figure out how to create the users, but I thought this would be really simple. http://sanderstechnology.com/2013/login-and-user-management-in-sql-azure/12826/#.WG7JgvkrKM8
If you're asking whether you can assign a user to ONLY have access to certain views within a database, then you can do the following: 1) give the user deny_datareader and deny_datawriter. This will ensure that the user can't even see any databases or objects without your explicit grant. 2) make the user a login in the database you want them to have access to, with specific SELECT access on said views. If you want them to be able to see the view code, VIEW_DEFINITION is also needed. ALTER/DROP are also available if you want. 3) you'll also have to give the user SELECT access on the tables hay the view(s) select from. This is always my approach when a new sys/application user or a contractor user is requested. It's a bit more work, but ensure that that user doesn't wander off to where they aren't supposed to be.
CREATE LOGIN [domain\user] FROM WINDOWS; GO USE your_database; GO CREATE USER [domain\user] FROM LOGIN [domain\user]; GO Then you merely have to follow the same syntax. Because \ is not a standard character for an identifier, you need to escape the name with [square brackets]: GRANT SELECT, INSERT, UPDATE, DELETE ON dbo.Tablename TO [domain\user]; 
[removed]
your formula for CAGR is wrong. cagr = power( (last_balance/initial_balance), 1.0/count(years)) -1.0 what you get is a ratio (aka percentages) and you shouldn't be summing those. 
Meh, you don't need a table variable for that, just use `VALUES`: SELECT Months.MonthNumber , ISNULL(Sales.Amount, 0) AS SalesAmount FROM ( VALUES (1), (2), (3), (4), (5), (6), (7), (8), (9), (10), (11), (12) ) Months(MonthNumber) LEFT OUTER JOIN YourTable Sales ON MONTH(Sales.Date) = Months.MonthNumber GROUP BY Months.MonthNumber ORDER BY Months.MonthNumber 
Was a reorg (any row movement enabled operation) or MOVE done on any of the base tables? Fast refresh tracks by rowid in the mview logs and those operations would change the rowid.
Are you sure this is a postgreSQL database? Postgres backups are usually .sql or .tar files.
Try something like SELECT h.passengerid, r.* FROM History h JOIN Ride r ON r.rideid = h.rideid WHERE h.passengerid = ??? ORDER BY r.date
It's because of the sum function you have. In most cases I have added a union all statement to combine the table with another table with the missing values. I then remove duplicates through an exists statement. SELECT Month([Date]), SUM([Amount]) FROM Table GROUP BY Month([Date]) UNION ALL SELECT Month([Date]), 0 FROM Table GROUP BY Month([Date]) WHERE NOT EXISTS( SELECT Month1([Date]), SUM1([Amount]) FROM Table1 GROUP BY Month1([Date]) WHERE Month1 = Month ) This is probably more complicated than you need but if you are dealing with large data sets this can be helpful.
Ya, I'm convinced now that using staging tables is the correct way to do this. Seems like it'll be a bit more work but better to do it the right way. Thanks for the help all, I'm a noob at this stuff ha (just an IT Specialist that knows basic SQL and how to google)
&gt; just an IT Specialist that knows basic SQL and how to google We all started somewhere.
Given the schema, this should work. But I wonder why one would design the tables this way. It seems you are going to have an entry in history for every entry in ride. These two tables should be merged.
It seems like the History table is being updated with every change made to the ride, such as status, price, etc. As in: for each change made to any columns, a new row is inserted. If that's the case, I understand why the two tables are separate. Reporting on the combined Ride/history table would be sluggish if you just wanted the essential data that the Ride table currently has.
Thanks!
[removed]
get rid of parentheses: A and B and (C or D) -&gt; A and B and C OR A and B and D. A and (B OR C AND (D OR E))) -&gt; A and B OR A and C and D OR A and C and E 
I'm pretty sure Gumtree classifieds does this as well. At the rate that people post ads, generally I get the same ads as I change pages. 
Yep, totally get this, but it's good reinforcement. I'm very much into a verify and confirm thought process. Gotta make sure ya don't mess it up right? But great advice all round. I think I have a solution (copy and pasted to save ya from clickin). Just wanted something more elegant :P Take care ! (solution: I think, I have a (very) simple idea in comarison, but it came out of talking shop with a real smart (rock n roller) I know, and he suggested making another temporary table, and just making a sequence to suit the purpose of "update where sequence number equals" instead of basing it on the rowid. Then, I can copy it back to the original table I was going to house it in, minus the sequence.)
Server: Localhost via UNIX socket Server type: MySQL Server version: 5.0.51a-24+lenny5 - (Debian) Protocol version: 10 Apache Database client version: libmysql - 5.0.51a System memory and Mysql spike at the same moment. It seems it happens around 6PM, but sometime its random. Now it's everyday. hard to define a pattern In the logs i notice many of these : - [07/Jan/2017:13:44:09 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-" - [07/Jan/2017:13:44:10 +0100] "POST / HTTP/1.1" 200 367 "Osclass (v.332)" "-"
mysql usage : https://postimg.org/image/j4emob7fz/
There's never one best book. The best *method*, however, is to use several books at once. Pick any topic - you don't have to go in chapter order - and read about it in all the sources you have; each one will reveal some of the nuances of PL/SQL. Do the practice problems, and play with the sample code to see how things work. Oracle PL/SQL Programming, and the companion workbook from Oracle Press are two sources I like, also the Oracle PL/SQL Programming book by Feuerstein. 
SELECT CITY FROM AREA A JOIN FAMILY F ON A.AREACODE=F.ORIGINAL AND A.AREACODE=F.MOVED From what I understand are you trying to get city when both original and moved match the area code? Sample data and output will help. 
 SELECT Family.ID , original_area.city AS original_city , moved_area.city AS moved_city FROM Family INNER JOIN Area AS original_area ON original_area.AreaCode = Family.original INNER JOIN Area AS moved_area ON moved_area.AreaCode = Family.moved 
I would think your instructor would give you the username and password. However, you are not reading the error message in your png screenshot - it doesn't say invalid username/password, it says your Oracle database server is blocking incoming connections.
So, I just performed the 'shutdown abort' option on http://stackoverflow.com/questions/23743910/ora-12528-tns-listener-all-appropriate-instances-are-blocking-new-connections and now I've got http://puu.sh/tgiYc.png. I've also got lost sleep. :3
New question... Where do I find this tnsnames.ora file? EDIT: Found it! Anyway, it definitely 'helps' that this is a mostly online course and that the prof actually didn't even explain how to do some of this crap but left it to just a reference sheet...
Well done in working it out !
I started as a data analyst and the company taught me sql. I went to off site training and learned from coworkers. Now I am a statistician and build predictive models in python! 
Got a job doing computer support. Boss noticed a proclivity towards programming and got me started supporting VBA early on. Got a chance to try developing and application to be used internally for the company (the program was being used for 15 years) with an Access backend. Was moved to internal development permanently. Was involved in upgrading our call tracking system where the company decided to move to SQL. Learned SQL on the fly.
I started in tech support and learned via books and on the job, because our devs were so backlogged and we need to fix client issues fast. It kind of snowballed from there and I went to more SQL heavy positions.
Had a job, started using SQL. Was working as an insurance underwriter, to charge of compiling a couple small monthly reports for management. Started out using SAS, but eventually I requested access to query the underlying data warehouse directly. Automated the report creation in VB, then left the company.
I learned SQL on my first job as a web developer. When I initially started, Java was all the rage and I was dead set on heading down the path of a Java developer. I'd never even heard of SQL prior to this, so when I encountered the database layer, I was very curious about what was going on there. Thankfully, there was a system architect that was willing to show me the ropes and he pushed me to learn everything I could. Within a couple of months, I was basically the SQL developer for our office and spent almost all of my time writing procedures and queries for reports.
Was lucky to land a job at a place where the position opened up so i kept nagging them until i got promoted to Jr.DBA, then DBA a year later. Been at the same place for close to 4 years; 2 years level 1 helpdesk, 2 years jr.dba/dba. The only experience i had with databases prior to being promoted was MS Access. Overtime i learned the front end application well enough to learn the database backend with a little help. I caught on pretty quick and about 4 months in i was flying solo.
yeah i did a bunch of free online tutorials. I basically just tried to learn how to do queries because thats what i am currently doing on a daily basis. so basically all the ways you can use joins, where, group by, having, case when, with xyz as, etc. the problem i found is you can only truely learn sql when you use it on an actual data set. by that what i mean is i had a good understanding of the fundamentals before i started my new position but i feel i only truely learned how to use sql by doing it on the job
1st line support. I was hired to restart applications and import data sets (drag and drop into the import folder). After I got bored of escalating every time there was an issue with our outputs. I started learning sql to find whether the anomalies were our fault or bad data. This progressed into manipulation and procedure tweaks into becoming the primary sql developer. (Was a small company with 15 ish staff selling a bespoke software product) 
I got a job as a junior web developer - basically maintaining existing intranet applications that were written in PHP and used SQL Server on the back-end.
I told our VP I studied and knew SQL and mentioned that if I could get access to the SQL data that some Excel files were based off of that I was given, I could create far more impressive graphs and reporting. He remembered this and a month or two later, he assigned me to a project that was far beyond my capability. I have worked as a business intelligence developer since then. 
If you run it against the query optimizer, does it recommend anything? IIRC, using a string function on a column, regardless of existing indexes on that column, basically invalidate the indexes. If it were me, I would create computed columns for PO_LINES.PART and ROUTER_HEADER.ROUTER that gets the substring values into a separate column, then index on that column. This option will put more overhead on data insertion/creation but should speed up your join since the substring function will not be performed on the fly in the join statement. This is assuming the bug in the ERP system that creates the data cannot be fixed and you need to handle it via SQL. EDIT: also assuming MSSQL.
I use SQL pretty much every day. Most of our web apps use stored procs to get data from DBs, so web devs are typically in charge of creating/maintaining those. All of our developers have a working knowledge of SQL, so it doesn't make me stand out here. I would seriously question the abilities of a developer if they said they didn't know SQL. It's a pretty fundamental part of any CS education and anyone that has gotten to the point of calling themselves a developer (or applying to become one) and doesn't at least know basic join logic probably skipped a few things along the way.
Started off in another dept building access databases to ease our reliance on excel. Then an opening came up for a SQL junior and as I understood there internal relationships I was given the opportunity, am now a SQL developer with the same company and have been for the last 6 years or so.
Thanks a lot, I'm using Pervasive PSQL which seems similar to MSSQL in my experience but doesn't have a query optimizer. I could try it in Access and see. I'll try following your recommendation. I am contacting the ERP manufacturer for a fix but it won't be immediate. I would guess the indexes are being invalidated. Thanks again. 
I got my first job that used SQL full time via a staffing agency (same agency placed me at my current role also). I called and chatted with a guy there after being fed up at my current role. I was in school for DB work but had no degree in it. He didn't care about degrees. I took a proficiency test in t SQL. They set me up with interviews for SQL based positions. Would recommend especially for into level positions. Into positions IMO want you to know the basics of joins and how set based logic works. Anything beyond that is gravy. If you get a choice, try to be placed where you'll work with lots of other people doing SQL work. You'll then have opportunity to learn from more people.
Started off as a Technician in a small IT team, and worked up an understanding of the systems- started trying to figure out why problems were happening, and how they could be prevented... which led to more understanding of the system, and ultimately, the database. Started analyzing an existing instance of SSRS, fixing existing reports, and eventually writing a lot more. I took SQL classes in college and blew them off... It was hard for me to figure out concepts of SQL without something I knew a lot about to compare it to. When I got my job and learned the system first, figuring out the SQL behind it was a lot easier.
How did you get your first job with SQL through the staffing agency if you didn't have experience in SQL?
Do you have permission to change tables? If you do, make new fields in PO_LINES and ROUTER_HEADER and UPDATE those fields as their substrings, then make a Stored Procedure that makes an update to the data that is still empty, call it before you run your query and use the new fields in the JOIN instead of the substring functions. Also, make an index for the new fields. If you can't call SPs, do the computed columns solution as suggested 
the 'simple/straightforward' (no holidays or whatnot) method could be . [hours for the 1st day of the range] .+ [hours for the last day of the range] . + ([number of days in the range] - 1)*[number of business hours] . - [number of weeks in the range] * 2)*[number of business hours] 
I started as a "systems analyst" doing application/hardware support. After a few months I got involved with the engineering department working on the beta version of our software and then started doing more documentation/QA with engineers. I requested a formal transfer from the support group to engineering and was placed under the reporting team. I had previous background in data analysis through academic research. Then I learned SQL on the job and taught myself. Fortunately there are many free resources online. I now do predicative modeling with R (which I also had previous experience in). 
First job out of college, could at least talk somewhat technical and had some prior experience with Excel functions. No SQL experience. Job was a Business Systems Analyst, used SQL everyday so thanks to a wonderful, patient mentor and time I got much better at it. Learned that I enjoyed writing SQL so naturally researched more and got into some of the advanced topics. 
I remember a problem I encountered similar to this a couple years ago, StackOverflow helped with finding a solution greatly. I believe I made a function that looped through from Start to End date. If you wanted to implement something similar you'd want to do validations such that the iterator wasn't on a Saturday/Sunday, and that it doesn't count the start date or end date as a full date. You're on the right track with Time until close + days between + Time from opening of business. My personal preference is that I'd rather skip the weekend time from being in the calculation than try to subtract after the fact.
I think most versions of MS SQL come with the sample dbs as options if you choose to install them - you can also download the .baks from microsoft here https://sqlserversamples.codeplex.com/
ERP... on Pervasive.. Are you running Epicor?
If you have to deal with holidays, at that point it may be worth looking at creating a data table - if you have a data warehouse it may already exits. In the date table have a value of 'isBusinessDay' as a 1/0 bit where its 0s for weekends and holidays. Then like ichp states you can do the partial calculation on the 1st and last days. For all the inbetween days a simple SELECT SUM(isBusinessDay) * [number of business hours] FROM dimDate WHERE date &gt;= startDate + 1 AND date &lt;= endDate -1 or there abouts
In 1993 I was working for a private investigator who was bringing records online for fraud searches and such. We had a presence on [CompuServe](https://en.wikipedia.org/wiki/CompuServe). Companies could sign up for our service and run searches on businesses and individuals. The back end was originally [Btrieve]( https://en.wikipedia.org/wiki/Btrieve). As we grew we needed something better and SQL Server 4.21 had been released. Transitioned everything to that and never looked back. The internet was available a couple of years later we transitioned everything to that. Been using SQL Server ever since.
Started producing reports via excel and VBA and then taught myself access and later SQL server as it seemed the next logical step to provide more capability and robustness.
As has been said already, substring functions are inherently slow, and joining on a substring compounds the problem further. Creating a temporary table with an aggregated column should decrease the query time, but the best solution (if it is available) would be to create an intersect table.
1. SELECT FIRM, SYMBOL, MIN(QUANTITY) as 'No of Shares' FROM Test.[dbo].[TRADES] GROUP BY FIRM, SYMBOL Does this work?
When you show up to work each day, do you know what you already have to do or do you get instructions? And if its alright would you be able to tell me one real scenario question when on the job? I'm just curious to see exactly what they ask of you because I'm currently learning now and I don't know if I'm good enough or not to start handing out applications.
You're on the right track...rethink the MIN portion....look at the question again 
I had experience in writing queries at the current job, I did not do that full time. Specifically I was given lists of things to process, after the guy that ran it got bored of rerunning it all the time, I got the code, and was to process the stuff on the list. I then asked questions about how to generate the list and began writing my own queries, eventually I decided to study it. By the time I was hired to do SQL full time, I was writing some reasonably complex queries with many joins, some crazy subqueries and some open SQL since there was data in 2 different systems.
Why not build a tally table for dates, then use a simple join and count/group by to do what you're trying to do? You could use a function, but the table would probably run much faster. 
Do those field ever change? If they do this is a terrible idea as eventually someone won't know to run the proc to clean/repopulate them and they'll get out of sync . If they will NEVER change, you can get away with it, but IMO it's not a best practice to do this.
As I understand it, SQL server will still be able to use the index with LEFT as it's still alphabetic/ stored in the same order in the index.
I think a view would work well for this. Create a view on the table and have one of the columns be your calculation. If you don't need something so permanent, you could use a temp table, a table variable, or a common table expression (which one is best would depend on the rest of your use case).
So there's a few approaches you can take: 1) If you want a value in a table that is dynamically set based on the values of the other table, I would recommend checking out Views. A view is a stored query that generates a result set that can then be queried like a table. In this case, you'd want a view like this: CREATE VIEW `myView` AS SELECT `t1`.`primary_key`, SUM(`t2`.`values`) AS `sum` FROM `first_table` `t1` INNER JOIN `second_table` `t2` ON `t1`.`id` = `t2`.`id` GROUP BY `t1`.`primary_key`; After creating your view, you can then do select statements on it as if it were a table, the difference being the "sum" column is dynamically generated based of the values of the rows in "second_table". **NOTE** the queries that generate the view are run **every time** you query the view, so if you have a lot of data you are working with, you can potentially slow your database down. You can learn more about views in MySQL [here](http://dev.mysql.com/doc/refman/5.7/en/views.html) 2) If you are just looking to define some "static" values in a table based on the sum of values from another table column, you can do an UPDATE JOIN like so: UPDATE `t1` SET `t1`.`sum` = `t2`.`sum` FROM `first_table` `t1` INNER JOIN ( SELECT `f`.`primary_key`, SUM(`s`.`values`) AS `sum` FROM `first_table` `f` INNER JOIN `second_table` `s` ON `f`.`id` = `s`.`id` GROUP BY `f`.`primary_key` ) `t2` ON `t1`.`primary_key` = `t2`.`primary_key`; I put "static" in quotes above because you can run this query on a regular interval to periodically update the sum values in the first table to match any changes made in the second. Hope these suggestions were helpful! Let me know if you have more questions!
it varies. sometimes they need something analyzed and its the same thing you did last week just with new numbers. sometimes theyll want you to answer a totally new question with data thats available to you. most of the time you dont have to do anything crazy. I cant really tell what my employer would ask me to do but i think the best way to study it would be to download some type of data like economic data and then put it into several tables. like one containing countries, one with cities, top industries, public spending, etc or something like that. then ask yourself questions such as whats the average % cities in the US spend on public transportation. questions like that. just keep coming up with them and make a list. Then figure out how you can solve them using your knowledge of sql. this will show you quickly what you know well and where you have gaps.
If it is not possible to prefilter the data of both tables, you are gonna add pest to cholera doing that, by additionally taxing the tempDB / memory by dumping both the tables into a temp table. You are not generally wrong, but you are not generally right either. It depends to much on the actual query. 
Usually POs never change, but if they do, just add an UPDATE in that SP in case the current substring is different to the new one. Also, depends how that query will be run, if it is a report then you can make it run the SP before the report opens, if it is a 3rd party system then it is even easier to call it, it all depends, the update is just an option :)
SELECT FIRM, SYMBOL, COUNT(*) as 'No of Trades' FROM Test.[dbo].[TRADES] GROUP BY FIRM, SYMBOL ORDER BY FIRM, SYMBOL 
This should work for the first question if I'm understanding the dataset correctly.
exactly
Yes, this is the correct query for the first question. 
 select count(case t.value when "does not have email" then null when "has a mobile phone" then null else 1 end) as OtherCount from table_1 t Edit: Obviously expand the case statement to match all of your known values. Edit 2: I'm somewhat curious why your "programmer friend" thought it was impossible as this would be a fairly easy/routine exercise in any language.
Use Cross Apply: create table #Test ( a INT ,b INT ) SELECT t.* ,DerivedTable.DerivedColumn FROM #Test t CROSS APPLY( SELECT CASE WHEN t.A = 1 THEN 'Yes' WHEN t.A = 2 THEN 'Maybe' ELSE 'No' END AS DerivedColumn ) AS DerivedTable WHERE DerivedTable.DerivedColumn &lt;&gt; 'No' /* Optional Syntax */ SELECT t.* ,DerivedTable.DerivedColumn FROM #Test t CROSS APPLY( VALUES( CASE WHEN t.A = 1 THEN 'Yes' WHEN t.A = 2 THEN 'Maybe' ELSE 'No' END )) AS DerivedTable (DerivedColumn) WHERE DerivedTable.DerivedColumn &lt;&gt; 'No'
So all your options are stored in a single column? You could create a separate column in your table for each of the options that you have and one for the free text field. This way you can do a count on it where the value IS NOT NULL.
Exactly this! I love using CROSS APPLY for cascading formulas or complex references, there's little to no overhead for it, and it makes more complex case statements or other calculations contained and easy to reuse throughout the query. Great suggestion!
Ah, I get it. Please post your query which has all the options listed when you can. Based on what you attempted already, you might try WHERE column9 NOT IN ('optiontext1', 'optiontext2',...) AND column9 IS NOT NULL. I'm also not near an SQL Server yet, but later today I can try to run a test for your dataset. :)
I don't think there needs to a subquery for this, if it is written similar to select count(case t.value when "does not have email" then 1 else null end) as DoesNotHaveEmailCount , count(case t.value when "has a mobile phone" then 1 else null end) as HasMobilePhoneCount , etc from table_1 t the only conditions should be the reporting requirements (date range, generic filters, etc) and not filtering of specific values.
Only thing worth adding is that it is only asking for Bank Accounts, not every field. So * should probably be seconddataset.AccountNumber or something.
This is not theory but practice. ðŸ˜‰
Would are some other complex SQL declarations besides Joins and SubQueries? What kind of queries do you write daily?
select char(70) , char(117) , char(99) , char(107) select char(121) , char(111) , char(117)
MIN() MAX() other aggregate functions. Being able to handle the common string functions like CharINDEX(), LEFT(), RIGHT(), REPLACE(). I now do some reporting where the complexity lies in getting the data set into an excel (or similar) file in a way that is readable. I do a lot of PIVOT. Now it's more of knowing how to make the data set look correct.
I'm not sure that the distinction between normal forms is something that's really discussed outside of academic settings. The concepts are definitely important to understand, but I don't think I've heard anything but third normal form discussed in professional contexts and even that is mentioned rarely and fleetingly.
Does any of the first 3 normal forms address the issue with redundancy we see here though?
i would just like to point out that if you replace 'Female' with an integer that links to a primary key in the Genders table, **you have not actually reduced any redundancy at all** in fact, you have added needless complexity yeah you might have saved a few bytes, but at what cost??? i'll say it again for those at the back -- normalization **does not mean** eliminating redundancy
&gt; Is it 3rd NF though? The gender does contribute with a fact about the row. What am I missing? yes, the above example is 3NF (probably higher), assuming Name is PK
To add to this, in SQL server you can use DATEDIFF(year, DATE_OF_BIRTH, GETDATE()) &lt; 21 ORACLE would be just SYSDATE - DATE_OF_BIRTH &lt; 21
You need to combine the like queries together. There are 2. You can collapse the case statements into just the where clause so that you are not repeating essentially the same select statement multiple times. Something like this all in one statement would work, though the second would keep the transactions shorter. [MSSQL cause I'm too lazy to translate] --V_COMMANDE Query where COD_STATUT = 1 SELECT C.Nb_Contrats_en_cours ,C.Portefeuille_Contrats ,V.Total_HT_service_en_commande ,V.Total_HT_solution_en_commande ,V.Total_HT_contrat_en_commande ,V.Total_HT_contrat_en_commande FROM ( SELECT TOP 1 NULLIF(P1.VALEUR, 'GBL') AS VALEUR FROM T_PARAMETRE AS P1 WHERE P1.PARAGRAPHE = 'COD_PRTDIRCOMM' ORDER BY P1.VALEUR ) AS P OUTER APPLY ( SELECT COUNT(C1.*) AS Nb_Contrats_en_cours ,SUM(C1.N15 * (MONTH(C1.DAT_FIN - GETDATE()))) AS Portefeuille_Contrats FROM CONTRAT AS C1 WHERE C1.COD_ETAT = 01 --Null means global. Here we will negate the filter if it is global. AND C1.C22 = ISNULL(P1.VALEUR, C1.C22) ) AS C OUTER APPLY ( SELECT SUM(V1.N7) AS Total_HT_service_en_commande ,SUM(V1.N6) AS Total_HT_solution_en_commande ,SUM(V1.N5) AS Total_HT_contrat_en_commande ,SUM(V1.N3) AS Total_HT_contrat_en_commande FROM V_COMMANDE AS V1 WHERE V1.COD_STATUT = 1 --Null means global. Here we will negate the filter if it is global. AND V1.COD_COM = ISNULL(P1.VALEUR, V1.COD_COM) ) AS V With variables and multiple transactions instead: DECLARE @VALEUR VARCHAR(8000); DECLARE @Nb_Contrats_en_cours INT; DECLARE @Portefeuille_Contrats DECIMAL(9,2); DECLARE @Total_HT_service_en_commande DECIMAL(9,2); DECLARE @Total_HT_solution_en_commande DECIMAL(9,2); DECLARE @Total_HT_contrat_en_commande DECIMAL(9,2); DECLARE @Marge_solution_en_commande DECIMAL(9,2); --Get the parameter value SELECT @VALEUR = VALEUR FROM T_PARAMETRE WHERE PARAGRAPHE = 'COD_PRTDIRCOMM'; --If it is global then null out the value IF @VALEUR = 'GBL' BEGIN SET @VALEUR = NULL END; --CONTRAT Query where COD_ETAT = 01 SELECT @Nb_Contrats_en_cours = COUNT(*) ,@Portefeuille_Contrats = SUM(N15 * (MONTH(DAT_FIN - GETDATE()))) FROM CONTRAT WHERE COD_ETAT = 01 --Null means global. Here we will negate the filter if it is global. AND C22 = ISNULL(@VALEUR, C22) --V_COMMANDE Query where COD_STATUT = 1 SELECT @Total_HT_service_en_commande = SUM(N7) ,@Total_HT_solution_en_commande = SUM(N6) ,@Total_HT_contrat_en_commande = SUM(N5) ,@Total_HT_contrat_en_commande = SUM(N3) FROM V_COMMANDE WHERE COD_STATUT = 1 --Null means global. Here we will negate the filter if it is global. AND COD_COM = ISNULL(@VALEUR, COD_COM) SELECT @Nb_Contrats_en_cours AS Nb_Contrats_en_cours ,@Portefeuille_Contrats AS Portefeuille_Contrats ,@Total_HT_service_en_commande AS Total_HT_service_en_commande ,@Total_HT_solution_en_commande AS Total_HT_solution_en_commande ,@Total_HT_contrat_en_commande AS Total_HT_contrat_en_commande ,@Marge_solution_en_commande AS Marge_solution_en_commande
I would work on trying to script the installation of SQL Server and run it *after* you clone the systems.
I have that already scripted but there a couple applications that can be installed beforehand and configured afterward. They have some steps that are bit tricky and it would make my job a lot easier to have it already setup and configured. 
sure, just make another column that is your two sums (in parentheses) with a minus sign between. i.e. (sum1) - (sum2) AS Remainder. SQL will redo the sums, so it will be slightly more processor intensive - but shouldnt be huge unless you are doing this for massive tables. edit: The solution below by /u/KING5TON is also perfectly acceptable - and will actually work on older versions of SQL. My version uses "windowed functions" which were first introduced in SQL2012 if I remember correctly. 
I'd second what /u/ninjaroach said. Script out the installations after cloning the VMs. Out of curiosity - what are you doing that requires instance stacking per VM? Or am I reading your post wrong?
&gt;in fact, you have added needless complexity &gt; yeah you might have saved a few bytes, but at what cost??? The advantage I see is that every gender (or rather gender_id) in the Person table has a corresponding record in the Gender table. I.e. if there are only a limited number of valid genders then those can be defined there. You can be sure that you don't have to deal with the "cat" or "none of your business" genders that people might like to input. Granted, you *can* verify that with a CHECK constraint but I just like the FK approach better. And it saves a lot of space. 
Thanks for this. I have tried this and it is definitely better than it was. I am now having a different issue from my comment above (https://www.reddit.com/r/SQL/comments/5n52ca/help_with_sum_function/dc8t3kd/). I have done it in a previously suggested manner but get the same remaining issue either way (where the totals are by Client rather than Matter). Any advice?
Not a DBA myself but I believe it's a requirement of the applications being installed and they decided on this configuration. This SQL design was before I got involved with this project. My role is automating the install of everything going into the VM deployment. There are some steps involved after SQL is installed that would be easier to include into the snapshot/VM and the constraint is really the time frame that this has to be done in. There are 7 different server VMs that I need to automate the installation of and all have a different application that has a requirement on the database server. The more items I can include into the DB server clone, the better. 
did you know you can set up the relational integrity FK ~without~ using a surrogate integer? so all the benefits of data integrity without the needless complexity and as for saving "a lot of" space, i still say that complexity is far more expensive than gigabytes when i started in IT, a gigabyte cost almost a million dollars... [last year, a gigabyte cost less than 2 cents](http://www.statisticbrain.com/average-cost-of-hard-drive-storage/)
Those are good points. I guess I have just gotten used to the surrogate keys so that's how I tend to think about it.
You can install, but not configure SQL: https://msdn.microsoft.com/en-us/library/ee210754(v=sql.120).aspx Just verify that all of the features you'll be using are supported this way: https://msdn.microsoft.com/en-us/library/cc645993(v=sql.120).aspx This will save you a LOT of time so you don't have to install it from scratch each time.
SQL doesn't like to be cloned. At all. You'll end up with process IDs clashing, issues with permissions with local service accounts vs network service accounts running things. Especially with instance stacking (that is just asking for trouble - and I would **very** strongly question the validity of the need for it) If you can script the -install- of SQL, after the -clone- of the VM, that is your best bet. But cloning a VM with SQL installed with multiple instances on it will turn out poorly, I can all but guarantee - especially if the named instances all have to be different names. While you can change the instance name after install, you will end up with ghost config stuff that will come back to bite you later. I know you are just trying to do your job, but talk to your boss... This very easily could end up in a serious scenario of something going massively wrong and causing unplanned downtime. If this is a new prod setup going live at a later date - take the time and do it clean. edit: check out the link /u/ihaxr posted below about install but not config below. I've never used that myself, but it seems like it could be a time saver for you. 
Did you take out the separate debit/credit columns? That would have an effect on your select distinct. I'd put them back in for troubleshooting. I don't know that the parentheses would cause an issue, but I would add those for clarification. Better luck with the group by method above? 
Thanks for the info. I'm going to go with the scripted install as part of post config. 
This returns the number of days between the two dates minus 2* the number of weeks between the two dates. So if there is 27 days between the dates, this will return 27-(2*4) = 19 My guess is that it is a rudimentary way of determining working days, by excluding weekends.
I don't see the redundancy here. If you have a table with only the two columns name(PK) and gender, then there's no redundancy and the table will indeed be 3NF.
if you replace every occurrence of 'Female' with 2, you would have *exactly the same redundancy*
If you wanted to ensure that only certain values went into that column you could add a check constraint that only allows those values this removes the join overhead and ensures the data integrity 
I'd see if your ERP or application has a calendar table you can cross apply. That way you get non working days, holidays etc.... Hours metrics are hard to do but not impossible. How long do the hours span? More than 24?
I'd do that in Excel, not Access.
Thank you!
Yes. Keep in mind that normal forms do not consider the data types, with 3NF you only need to consider the [functional dependencies (FDs)](https://en.wikipedia.org/wiki/Functional_dependency). The value "female" or "2", regardless of data type or value, would be functionally dependent on the primary key (all values are an FD of the PK). As long as all values in your relation are only dependent on the primary key (and no other columns), your relation is in 3NF (and also [BCNF](https://en.wikipedia.org/wiki/Boyce%E2%80%93Codd_normal_form), which is stricter). Edit: When determining whether a relation is in 3NF/BCNF, you only need look at *one* relation at a time, so in your example it would not matter if the gender had a separate relation with other information. If you want to consider whether there's redundancy/potential anomalies, you need to look at [4NF](https://en.wikipedia.org/wiki/Fourth_normal_form).
Dunno about access... SELECT id, count(id) AS 'Number' FROM table GROUP BY id ORDER BY id ASC That would do what you're asking for. At least in SQL server. I don't know how much syntax varies in access Edit: unless I misunderstood your question 
&gt; specific or strict definitions are not included in the contract. This may be the source of the reluctance to change anything in your reports - they don't want to risk things changing so much that someone ends up getting in trouble. It may be a situation of everyone knowing that there's a ton of ambiguity, but no one wanting to rock the boat. &gt;I was hired to look at this logic and decide whether it is a "best practice" -- and the push back is that doing it the right way is, "too complex." This is really worrying - they've asked you for your expertise, and now are rejecting it.
Does this mean there's no primary key in the database? I'm not too familiar with Access, but is that even allowed? It sure doesn't look standard.
Well nothing is set in stone yet, I just wanted to discuss what was written and what it meant to confirm what I thought about it.
ohhh hahahha, nvm i get it!!!! brilliant! 
please explain. i really want to understand the joke.
This is what I like to imagine Brent Ozar is like when he's off camera/air when I listen to/watch his podcasts. I love you Admiral Waffles.
I think that the attributes nation, industry, and season could be broken out into different tables. All address related attributes in your Brokerage table could be split into an Address table because of these two questions: what if there are multiple brokers in the same building at the same time, and if there are multiple brokers at one building, but in different suites, do you really want to store the general address data (I.e. line1, line2, line3, city, state, zip, etc.) multiple times? It's all identical except the suite. Remember, the goal of normalization is to remove any duplication of data. If you start to see columns of data with identical data, it should probably be split it into another table and related with a foreign key.
If you could go on the DBA StackExchange and answer lots of questions in the same way, I would be soooo happy. Usually it's just a bunch of us arguing over whether or not to use AS before aliases. We need to spice it up.
Thanks for the boost of confidence, ill turn it in and see how I do :) 
&gt; The only thing that could be further normalized would be State replacing a perfectly good candidate key with a surrogate key **is not normalization**
Looks like cascade is a table property, not a function (i.e. it would be defined in a CREATE TABLE statement). If you want to delete a table which doesn't have that property set, you'll need to either alter the table to set it (assuming this is possible retroactively, I don't know) or remove the FK constraint first I think. E: CASCADE actually isn't anything to do with dropping tables. It allows for deletion of rows with corresponding keys from a child table, if rows are deleted from the parent table. So just remove the constraint and delete the table instead I think. 
Should events occurring for other people affect the results? If not, this should return the eventid and personid for when a 6th event in a row occurred: with consecutive_events as ( select eventid, personid, event, timestamp , rnk = ROW_NUMBER() OVER (PARTITION BY event, personid ORDER BY timestamp ASC) from events ) select eventid, personID from consecutive_events where rnk = 6 By the way, if using SQL Server, using identifiers 'event', 'timestamp', 'events' is ill advised
Can you explain me how to I dispay fields with no values using your "setting" ? I don't understand your point when you're saying " This is a setting " ? What are you talking about ? Thanks for the quick response tho
[removed]
You need to change your query from `select *` to selecting the individual columns, and then wrap them in an `isnull()` such as: SELECT isnull(Donnee, 0) AS Donnee , isnull(NumLigne, 0) AS NumLigne 
Think about hierarchy levels. 3NF is kinda similar. For example, you'd drop the date table and, fully normalised, replace it with tables for day, month, quarter, year (running from most to least granular). Depending on how you define seasons, I think they'd sit alongside months or above quarters. The easiest way to understand normalisation is to look at the same data in a 2NF and a 3NF schema. There should be plenty of examples of that around on Google. 
It seems strange that SSIS allows you to run the flow if errors are happening. For the derived column I'm pretty sure the format should be YYYYMMDD and you seem to be trying to do MMDDYYYY. Try this: (DT_DATE)( SUBSTRING([DOB], 1, 4) + "-" + SUBSTRING([DOB], 3, 2) + "-" + SUBSTRING([DOB], 5, 2)) Note that you don't have to convert after, you can just add the cast in front like above. DT_DATE always has a length of 0 in SSIS metadata. You can insert a dataviewer after your derived column to see the output (right-click on the line/arrow that points to the next component and choose data viewer). If your conversions did not work SSIS would give you an error. What is the warning on your destination? Make sure the destination is not set to ignore errors or redirect error rows that would explain why it inserts 800 rows with only null in DOB - because the format is wrong, only rows with null in the DOB can be inserted.
[removed]
You'll just get forced edited. :P 
COALESCE is *marginally* more functional, because you can pass it an unlimited number of expressions chained together for evaluation COALESCE(This, That, 'Undefined') vs ISNULL(This,ISNULL(That,'Undefined')) Useful if you have a lot of outer joins going on.
This is a pretty involved problem! I don't have time for a full prototype, but I think I see how I'd design it. In one CTE, calculate this column for each event: CASE WHEN LAG ( EVENT ) OVER ( PARTITION BY PersonID ORDER BY TimeStamp asc ) = EVENT THEN 0 ELSE 1 END as NewEventBlock This column checks whether your current event is the same as your last one. In your example, this column would be 0 for event 1 (by definition, as it's always 0 for the first row for each person), then "1" for event 3 ("No" is different than "Yes"), then "1" for event 6 ("Yes" is different than "No"), then "0" for event 7 ("No" is the same as "No"), etc. In another CTE, calculate this column off of your first CTE: 1 + SUM ( NewEventBlock) OVER ( PARTITION BY PersonID ORDER BY TimeStamp asc ) Call this column "EventBlock". It basically creates an identifier for each row telling it which blocks of consecutive events it's in. Now, in a THIRD CTE, join your main data set to the second CTE, and calculate this column: CASE WHEN Event = 'No' THEN ROW_NUMBER() OVER (PARTITION BY EventBlock ORDER BY Timestamp asc) ELSE 0 END AS ConsecutiveNos Now we're finally ready for the main query, which thankfully is pretty simple: SELECT DISTINCT PersonId FROM (whatever you called CTE 3) WHERE ConsecutiveNos &gt;= 6
Will need some more information then. Please provide an example of the DOB, is it always in the same format? It looks like SSIS accepts the syntax of the expression - What is the error you are getting? Did the data viewer get you anything or did it fail before it showed? What is the datatype of the destination column you want to insert the DOB into and is it SQL Server?
&gt; there is no guarantee that higher IDs are more recent than lower IDs i love you SELECT p.sku , p.price , m.latest FROM ( SELECT sku , MAX(date) AS latest FROM priceUpdates WHERE date &lt;= ? GROUP BY sku ) AS m INNER JOIN priceUpdates AS p ON p.sku = m.sku AND p.date = m.latest 
Ahh, those are great questions. I should have included what the Dataviewer showed. First off, the DOB columns in the DBF file are all the same. YYYYMMDD. Second, The derived column failed at first but after tweaking the SUBSTRING numbers it worked. Seem the original one was getting YYYYYYDM. Works now. Here is a pic of the DataViewer: http://imgur.com/a/MoGDc The column that it's going into is a DateTime. Here is a pic of the mapping, the Derived Column 1 is what was converted to DT_DATE. I'm mapping it to the DOB column in the table (which is datetime) Pic: http://imgur.com/a/rgS7z Here is also a pic of the last stage of the SSIS package. The dataviewer shows data perfectly fine, but the preview of the last stage shows blank. Would this provide any insight? Pic: http://imgur.com/a/W5chd Thank you so much for all the help!
That's what I was looking for. I knew I'd done something similar in the past but was tripping over the ordering for the join. Great stuff. Thanks.
The preview shows that the table is empty. Did you run the package? Datetime is not actually the same as SSIS DT_DATE. In SSIS Datetime is DT_DBTIMESTAMP which has the format: 1900-01-01 00:00:00 (YYYY-MM-DD hh:mm:ss), but the dataviewer looks correct. SSIS should be able to insert into a datetime column, setting hh:mm:ss to 00:00:00. Assuming you ran the package, you get no errors or warnings but it does not insert anything in the table? If that is the case could you show me the error output of the destination component? By the way, you should work on correctly naming your columns or it will end up costing you more time in the long run, I promise.
Listen to the band Coalesce while you code this... Double win.
Didja stop reading after that?
I will have a play with that this morning. Thank you very much for taking the time to have a look
I already have COALESCE for each count and sum ...
Thank you, for clarifying.
I achieve to change the NULL into 0 by replacing SELECT * by SELECT Donnee, NumLigne, COALESCE(January, 0) AS January, COALESCE(February, 0) AS February, COALESCE(March, 0) AS March, COALESCE(April, 0) AS April, COALESCE(May, 0) AS May, COALESCE(June, 0) AS June, COALESCE(July, 0) AS July, COALESCE(August, 0) AS August, COALESCE(September, 0) AS September, COALESCE(October, 0) AS October, COALESCE(November, 0) AS November, COALESCE(December, 0) AS December But it still shows only the line where there is a value (here Nouvelles affaires avec exploitations) http://imgur.com/a/Ib3Z0, I would like to show the others lines even if the value are all 0
[removed]
I believe you are looking for a union
Just figured out why it was not working... I'm doing this in access and access does not support full outer join. 
What exactly are you looking to get from it? If you don't have any particular needs I'd recommend using MS SQL (Management Studio) just because it's quite good and widely used and has good execution plan displays and context sensitive query editor. 
Postgres if you like open source and truly free. SQL Server if you have a Windows box and don't mind free (but closed) software.
Why not just make sure that you only count the first time someone visits by making sure there isn't a visit with an earlier timestamp than the current visit? SELECT DATEPART( week, x.enc_timestamp)as week, COUNT( distinct x.person_id) as patient_count FROM patient_encounter x WHERE x.billable_ind = 'y' AND x.enc_timestamp between '01/01/16' and '01/31/16' AND NOT EXISTS ( SELECT 1 FROM patient_encounter x2 WHERE x.person_id = x2.person_id AND x2.billable_ind = 'y' AND x2.enc_timestamp &lt; x.enc_timestamp ) group by DATEPART(week,enc_timestamp) order by DATEPART(week,enc_timestamp)
Just remove the word IN, editor error on my part.
If mySQL is not good enough to solve your problems, go to SQL Server. For simple databases doesn't matter which RDBMS you are using. But with large databases a good design for a sql server database can do miracles.
Mastering of queries firstly then relationship concepts. Is it free though? 
Something like this should do the trick: SELECT id , reference , notes , max_date FROM ( SELECT id , reference , notes , MAX(updated) OVER (PARTITION BY reference) AS max_date , ROW_NUMBER() OVER (PARTITION BY reference ORDER BY id) AS rn FROM t1 ) WHERE rn = 1; 
Thank you! I'll be throwing this into my query repertoire first thing in the morning!
row_number() and similar functions that use the OVER (PARTITION BY syntax are some of my favorite tools in sql. Once you get comfortable with them you find uses for them everywhere.
Just to be clear: you've already got a table that has one record per minute, per lot, per space? I think the trick would then be to do a join on less than/greater than on the time. Add the occupied field to your table of time/lot/spaces, default it to 0, and update it something along the lines of UPDATE yt SET yt.occupied = 1 FROM yourtable yt INNER JOIN transactions t ON t.lotnumber = yt.lotnumber AND t.space = yt.space AND yt.time &gt;= t.starttimestamp AND yt.time &lt;= t.endtimestamp
I'm confused as to why you'd want the resulting table, but... Could you write a function that queries whether a certain space on a certain lot is available on a certain minute then iterate through the final table filling in 1 or 0 as a result of the function. Back to my confusion, just the function would seem to serve your purpose without needing the final table.
It isn't efficient but this is a partial solution to help in your quest for an answer: select [Start Timestamp] as [Time], [Lot #], [Space #], [Occupied?] from tableName where (datepart(YEAR, [Start Timestamp]) = 2016) and (DATEPART(month, [Start Timestamp]) = 11) and (DATEPART(DAY, [Start Timestamp]) = 1) and (datepart(HOUR, [Start Timestamp]) &gt;= 10 and datepart(minute, [Start Timestamp]) &gt;= 10) and (datepart(HOUR, [End Timestamp]) &lt;= 11 and datepart(minute, [End Timestamp]) &lt;= 59) -- this should be the ending time stamp you want I tested this using SQL2012
Working with patients you'd normally have an episode ID or spell ID (or epi/spell No). If not you should look at implementing it. Then you can just run a where clause with epi no =1 Doesn't all financial processing use this too? You'd get paid based on the spell, not the episode
We definitely have that level of detail but not sequenced like episode 1,2,3... I'm not certain how we'd implement that but I'm definitely going to look into it. Edit; misunderstood the suggestion.
Did you get it to work?
Yes, those types are all alphanumeric. EDIT: But keep in mind that numbers stored as a char will behave as a character and not as a number.
You've skipped a lot of information in your description and your SQL makes no sense. FIRST what are you trying to GET as a result of this whole thing? Second, what does the temp table look like that we're inserting into? There is no INTO statement so we're not making the temp table as part of this SQL statement, so is there an INSERT INTO #myTempTable (columns) that's supposed to exist before this SELECT??? Third, you're selecting opening a parenthesis and ending the parenthesis after an unfinished WHERE clause???? Forth, as /u/fauxmosexual said, you are referncing alias r in your case statement, yet there is no table with an alias of r in your FROM clause. Fifth, you're returning the exact same thing whether the r.id is 5 or 3, so why have two cases??? 
Thank you!!
Not so much SQL related but the company I work for is still floating a pretty hefty dependence on LotusNotes...
Alas, it is the defacto database in certain industries and markets. I wish we were at least using DB2. 
You work at the same place as me? Lol
In addition to what others have said about SQL being the standard for companies rather than R, I would also suggest that the larger companies tend to use SQL Server and Oracle more than MySQL. So if you're looking for a job in another area of your company, they may use SQL Server/Oracle and might be useful to you to learn one of those.
I would also suggest looking into isolation levels for each of the apps that use that database, and separate read accesses to only use the [dirty read](https://www.ibm.com/support/knowledgecenter/SSGU8G_12.1.0/com.ibm.sqls.doc/ids_sqs_1161.htm) (other DBMS call it UNCOMMITTED) wherever possible. We experienced similar problems using a logged database instead of unlogged: [Default Isolation Levels](https://www.ibm.com/support/knowledgecenter/en/SSGU8G_12.1.0/com.ibm.sqls.doc/ids_sqs_1168.htm)
[removed]
Let's say your data set grows to 1 terabyte, and you have 16 gigabytes of memory on your laptop. R will not be able to load the dataset, because R analyzes data in memory. You can still analyze the data using SQL
SQL stands for structured query language. It is used to query (or to project) data stored in a relational structure. Relations in data is manifested as tables, views, and other objects in your database engine. What you do is you treat the data as sets and can then use set based math to get what you need with a query. What we like about it is that I never have to tell my engine how to do anything just what i want. "How much did we sell for last year" vs "get all records loaded into memory then iterate over each record and add up the AmountRecieved field if the date has a yearpart which is equal to SomeVariable"
it looks like you want the actual rows, along with a count SELECT t.ApplicationNumber , t.FirstName , t.LastName , t.City , m.occurrences FROM ( SELECT FirstName, LastName, City , COUNT(*) AS occurrences FROM daTable GROUP BY FirstName, LastName, City ) AS m INNER JOIN daTable AS t ON t.FirstName = m.FirstName AND t.LastName = m.LastName AND t.City = m.City 
The scores are not super meaningful without being able to assess the questions. Also, given the multi-modal distribution of scores, one cannot help but wonder about correlations between questions. That is, is there perhaps a single essential concept people scoring around 21 just missed? 
Hmm, I thought it was working, but I can see the results aren't as expected. For example, it will say there are 4 occurrences of John Smith in NYC, but I can see there are only 3 John Smith records. Any clue?
I managed to find the questions buried in the article: https://www.analyticsvidhya.com/blog/2017/01/46-questions-on-sql-to-test-a-data-science-professional-skilltest-solution/ And I agree with you, there's surprisingly little actual analysis of this data for a site called 'data science central' or this 'Analytics Vidhya' site were the test is hosted. I didn't go too deep into the test, but I imagine question 5 had a lot of 'wrong' answers, since I believe the code given would result in a error if ran. (They failed to close the quotes.)
If you want to retain all application numbers (i.e., not just have one record per firstname/lastname/city combination) you'd have to do a separate query to do the counting. Something along the lines of SELECT ApplicationNumber, FirstName + ' ' + LastName AS FirstLast, c.counter FROM yourtable yt INNER JOIN (SELECT Firstname + LastName+City AS firstlastcity, count(*) AS counter FROM yourtable yt1 GROUP BY Firstname + LastName+City) c ON c.firstlastcity = yt.Firstname + LastName+City
This appears to be exactly what I needed.
Hey! I'm really interested in finding jobs/internships in data analytics in the Chicago area, any tips or advice? I can shoot you a PM if you'd feel more comfortable talking there. 
The two sample questions were beginner level, and your analysis seemed to have a lot of assumptions... not so much "data science" as bad science.
The issue is that you are returning multiple columns within a NOT IN clause. How does it know which column you are NOT IN. E.g. SELECT * FROM X WHERE Y NOT IN (SELECT A,B FROM AB) Wrong SELECT * FROM X WHERE Y NOT IN (SELECT A FROM AB) May work So to correct your example: Select T_Foo.ID from T_Foo where T_Foo.ID NOT IN( Select T_Foo.ID from T_Foo inner join T_Foo_Ba on T_Foo_Ba.ID_Foo = T_Foo.ID inner join T_Ba on T_Ba.ID = T_Foo_Ba.ID_Ba ) P.S. when using nested queries avoid using the same alias as outside the nested query. An exists would also work well: Select TF.ID from T_Foo AS TF where NOT EXISTS ( Select 1 from T_Foo AS T_Foo2 inner join T_Foo_Ba on T_Foo_Ba.ID_Foo = T_Foo2.ID inner join T_Ba on T_Ba.ID = T_Foo_Ba.ID_Ba WHERE T_Foo2.ID = TF.ID ) Exists is just a bit check, you do not need to return any values from the table so selecting 1 works. Note I have renamed your able aliases to make it clearer.
This is likely incorrect. I think it's more of a trick question because the table should probably be normalized into a Girl-&gt;Flower table and a Flower-&gt;Cost table. If you sum(cost) you end up with a total of all costs where the flower is the same. So if 5 different girls like "tulips" which cost 5 dollars each, your sum(cost) is now 25. ASSUMING that the cost is the same per flower, this should be sufficient: SELECT DISTINCT Flower, Cost FROM Likes Also, You don't need to use "GROUP BY" when you use "DISTINCT" because they essentially do the same thing. Think of DISTINCT as a "group by" with all select values in the group by clause 
I think all you have to do is move the semi-colon in B) to the end. So it would be ... SELECT Flower, SUM(Cost) FROM Likes GROUP BY Flower This would give you each flower, and then the total in the cost column. So this is assuming I'm reading the question correctly, and they want to see a "total cost for all instances of the &lt;flower&gt; records." Your statement would work, but the DISTINCT is unnecessary since you're doing a GROUP BY on the Flower. Now, if I'm misreading the question and they want to see the unit price of each flower, then it would be ... SELECT DISTINCT Flower, Cost FROM Likes 
&gt; ASSUMING that the cost is the same per flower, this should be sufficient: pretty good assumption, considering it's an SQL test... although nothing would prevent Mary from liking roses at $5 each, while Jane likes roses at $4 each
C) would return the sum cost of each group. Right? 
&gt; c) Select Flower, Cost &gt; From Likes &gt; Group by Flower; No, because there is no aggregate function in query (C). It would have to be sum(cost).
&gt; SELECT Flower, SUM(Cost) FROM Likes GROUP BY Flower This is answer (B) in the test. Which, according to the test, is incorrect.
Alright, so I have a Master's in Fisheries Science (heavily into the quantitative parts of ecosystem modeling and population dynamics). I was most of the way through a PhD when I decided to jump ship and get into analytics. I've stuck almost exclusively in the Pharma/healthcare domain, partially because it interests me and partially because I've repeatedly been able to find work there. In healthcare, I've found that SAS is king, and I use it daily. Within SAS, Proc SQL is my bread and butter these days, so a solid working knowledge of SQL, and especially the quirks of Proc SQL, are very useful. This is a bit unfortunate, since SAS is really only so popular in healthcare and finance, but on the other hand those two fields have plenty of jobs so maybe it's worthwhile for you. Also, healthcare seems to be relatively welcoming of weird backgrounds, while I couldn't speak to finance or other industries. Python is very popular in data analytics in general, and it's my current personal project to become more proficient at it. I'd say if you are good or can get good at it, it would probably be well worth your time, but that's just my opinion so take it with a grain of salt. I don't know of anyone working in industry that uses R. It's far more popular in research and academia than outside of it, unfortunately. I'd avoid spending a ton of time learning it unless you have a clear reason for doing so in mind. I spent a couple years using it for my PhD, and it's awesome, but that's just been my experience. I moved directly into a pharmaceutical CRO, and I found that to be a really good stepping stone. CRO's tend to be a lot more flexible and willing to look at what you can actually do as opposed to what's on your degree, in my experience. The trade off tends to be that they can be chaotic, amateurish, and the working environment may not be as good. Overall I found it worthwhile. I also know that people with my skillset have a lot of use in the healthcare claims and billing world. The Medicare Program Integrity contractors are always hiring, and they're a really good entry-level analytics opportunity as well. Get SAS polished up and on your resume, and they might hire you. I moved from there to a private company last year (to do reporting and analytics on healthcare claims data, still), and my old seat is still empty, plus at least a couple more right now. When I decide it's time to job hunt, I polish up my resume and start plugging 'SQL', 'SAS', and 'healthcare' into an Indeed search, and apply to everything that looks interesting within a day or two of it getting posted. There are always new postings up, so even though I probably apply to 20-30 postings before I get one, it never takes more than a month or two, even the first time when I was still a grad student looking to get out. You say that you have limited relevant project work. First off, relevant is a slippery word. If you've analyzed data, then talk about what tools you've used, what questions you've answered, that kind of stuff. Most companies who are hiring entry level data people don't want to pay the $80-$100k that experienced ones cost, and will be happy to take someone who can do the work but needs to be taught some content knowledge for $50k. If you can make that work for a couple years, you're in and then you're experienced and then you're pretty much golden. If you really haven't done a lot of data work at all, I would say you'll need to do some coding and analytics work on your own time, just to get something on your resume. The focus for someone looking to transition to analytics should be to talk about what you can do with data, what kind of questions you can answer. That was a little rambling. Anything you want me to elaborate on?
Not from what I read. In the test, Answer b is ... SELECT Flower, Sum(Cost) FROM Likes; GROUP BY Flower. The semi-colon being the key point. 
It's not asking for sum, I'd say c gets you the right answer. 
Ah. You have good eyes. It looks to me like that's just a typo the OP made as it is obviously incorrect syntax.
C will throw an error as cost needs to be in the group by clause. 
I'm not using Python in a professional capacity at all, but from everything I've read Pandas is a must. Again, grain of salt. I think those are definitely useful concepts, and will be helpful for you. That said, most of the positions I'm aware of involve more data querying/cleaning/summarizing and less model work, at least at the entry level. That said, it definitely looks good and will mark you as someone that companies can develop upwards at the very least. I haven't done any KAGGLE or similar challenges myself. I have one guy on my team that does stuff along those lines and enjoys it, but I don't know enough about it myself to have a worthwhile opinion. I'd say in terms of scope, keep it to elevator pitch style projects. "I created a model to predict Uber prices based on weather and common public events". "I built a dashboard to track and summarize household spending". "I built an application to schedule meetings between you and up to 50 people that avoids all schedule conflicts and ensures you meet everyone within 3 months". They don't have to be world-shakers, just show that you can use the tools to solve problems. I did not get an internship, I moved to a job straight away. I did have ecology internships in college, but I don't know that they were relevant at all.
Group by
Not sure what your response means, but a GROUP BY clause requires that all columns in the SELECT query are included in the GROUP BY clause except when the column is an aggregate like SUM or AVG.
It really depends on the company you'll be working for, but I would not imagine a business analyst would ever need to know PHP. If you ever want to be a developer, sure, learn all the languages/frameworks you can. 
Decimal allows arbitrary precision, but you're paying for it in space and speed. Floats are faster, fixed bit, and almost always good enough. No one 'wants' to change their source data, it's just that in almost all real world examples, your source probably already has a larger margin of error than the float introduces. Recording how long a process takes? If you're asking about floats here, you probably don't have access to instrumentation that can accurately measure to a nanosacond.
Obligatory reading: [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) &gt; what the reason one would use that data type then Sometimes an approximation is good enough &gt; Furthermore, is there any data type that can handle differing precision/scale and preserve the input values exactly? Yes, rational numbers. 
I heard SQL is easy for data scientists
A more immediate resolution, without diving into the complexities, is to import as nvarchar and then convert to decimal(18,2) **correction: DECIMAL(##,##)** after import. While that won't resolve your issue in the long term, it should allow you to preserve the true value.
&gt; Recording how long a process takes? If you're asking about floats here, you probably don't have access to instrumentation that can accurately measure to a nanosacond. Of note here is that where there **is** a need to have that kind of precision the recommended approach is to measure everything at the smallest unit, and use integers to store the values. Thus you would record 10ms instead of 0.01 seconds.
Don't comment if you aren't going to give useful advice.. I had an honest question and don't need a stupid patronizing response. 
&gt;decimal(18,2) That will give him only two decimal places. DECIMAL(28,13) is what he's looking for if he wants to preserve all the digits exactly.
&gt; Yes, rational numbers. SQL Server doesn't have rational number data type but it does have a fixed point data type called DECIMAL or NUMERIC.
This is probably the fastest.
I don't think that's what OP is asking about. The story you linked has web pages that take URL parameters and display them on a web page.
Haha don't drink and Reddit kids.
&gt; It's an analytics database so we need to have daily refreshes This doesn't sound like a development environment to me.
/u/ArguesBoutEverything and /u/ninjaroach are giving you the response you need, not the one you came here expecting to hear. Temp tables are local to the server you're working in and should have no impact on a replication process from another server. But stop calling this a "development" server. if you're replicating from a production server on a constant basis and can't make any changes to the "development" server without breaking something that's important, it's **not** a development server. Definition of "development server": if you can unplug the box, have it unavailable for 2 days, and you don't have people in business-critical positions saying "I can't do my job", then it's a development server. If breaking the replication process to your development server causes important work to not happen, you have a **production** server.
Thanks for all of the upvotes everyone! If you have any suggested improvements to the page, let me know!
&gt; I wouldn't have even posed the question or be given the access I am given if I didn't have a good understanding of the processes. THIS IS NOT A GIVEN. We don't know what company you're at, we don't know the practices there. Therefore it was the absolutely BEST answer to tell you to stop what you were doing and validate your assumptions before you took a single step further when what you've told us already shows you're not using the right terminology for the situation you're describing. That's not patronizing. That's trying to prevent a disaster. 
Why wouldn't you just do SELECT * FROM t_foo WHERE NOT EXISTS ( SELECT 1 FROM t_foo_bar WHERE t_foo_bar.ID_Foo = t_foo.ID) 
Thank you! I'm currently getting the following error when I try to run. Subquery returned more than 1 value. This is not permitted when the subquery follows =, !=, &lt;, &lt;= , &gt;, &gt;= or when the subquery is used as an expression.
I can select TOP 12 before the error comes up. 
Found the problem child. Thank you! 
Sounds like some type of version mismatch between mysqldump and the database engine. The `SET OPTION` syntax went away in MySQL 5.6 and is apparently invalid in [MariaDB 10](https://mariadb.com/kb/en/mariadb/set/) also. It would be interesting to see if the results of `which mysqldump` match up when it's run from the shell and from within the PHP script. I have a feeling that your shell's $PATH points to Maria's copy of mysqldump, and PHP's $PATH points to the copy of mysqldump that came with MySQL 5.5. You may need to pass the full `/path/to/maria/mysqldump` in the PHP script, if you aren't doing that already.
It's not a disaster if I don't implement anything. Again I knew that writing on a slave breaks the replication process. It was a simple question. Does creating a temp table and writing on that in a session do anything to the replication process? Saying "Sounds like you are touching things you shouldn't" Is a shit answer. That's not preventing disaster that's being a condescending douche. 
Does the GLF_OCCUPANCY table need to be a Table? It *seems* like a View might fit your needs better.
I'm open to converting it to a view, but I have some reservations about it. This is a fairly transaction heavy database, and there will be bookings coming from many different sources (Application, Web, External Apps, etc.). In addition, this is going to be part of the determination of the rate a user is going to pay, so it needs to be fast. I don't want to wait on the database compiling the view when we query rates, which could be many times a minute. Finally, users will be able to "override" the occupancy on certain days to ensure they maximize revenue. This, at the very least, needs to be stored.
If you have a high transaction database, that's even more reason to ditch the original design idea. It actually seems like you would be better off aggregating the data in the application itself (via queries) instead of aggregating and storing it in the database. With h proper indexes in place, performance shouldn't be an issue. 
While a possibility, aggregating the data in the application would cause maintenance headaches down the road. The reasoning behind using the Trigger was because there are 5 different applications which could make these changes. Maintaining the queries in all of these would be awful to deal with. We have our core application, an external VB "tee sheet", interfaces to 3rd party online bookings, and our own Web application. I think the best solution we've come up with here is to manually call the procedure after booking changes. At least then we have a single procedure that is handling the calculations, and we just need to make sure we fire it at all the appropriate times.
First time on Reddit and wanted to make my 1st post (such an emotional moment) Just a business bro here glossing over your query... Looking at your field names it leads me to believe there would be multiple rows per ID (and that may by perfectly okay). I noticed there are questions/answers so I'm guessing, for instance, ID1 could have several rows depending on how many times they answered the questions. You're likely going to need to identify unique survey/question encounters by a time stamp - most preferably datetime. There are several ways to skin this cat. One easy solution is to do an inner join on a derived table in which you're only selecting the UserID and Max(SurveyTime) - or whatever your time stamp is called. Your derived table is basically a join on a select statement and it would look somewhat like this... Select UserID ,Derp ,Herp ,Flerp From Table1 t1 Inner Join ( Select UserID ,Max(SurveyTime) as SurveyTime From SomeTable Group by 1 ) t2 On t1.UserID = t2.User ID And t1.SurveyTime = t2.SurveyTime Another solution is to do a Partition By.... This basically adds a row count to your User ID encounters and it would look like this... from there you could take the Max/Min in order to get the first or last time they hit your survey. Row Number() Over (Partition By UserID order by SurveyTime asc) These are just a couple rough examples but I hope it helps. Time to get back to my real passion - drinking beer and watching tv. Cheers!
[removed]
work with messy data. Like Medical Billing Claims data from MSSP
sql is essentially a command line version of Excel
I work sooo much for my current job (well, not a whole lot, probably just 50hrs a week) that I think it'd be really hard to do machine learning side projects in my spare time.. 
I went to school for ms health informatics. I self taught statistics on the side and learned programming at home. My pay difference was about a 1.4x increase going from data analyst to statistician. So it was significant. An MS degree was a requirement to get the job though. 
[removed]
Thank you, yes I have all of the boxes enabled but still am not getting any statement completion or dropdowns of tables. Really not sure why : (
Is your DB Local? I always have troubles with intellisense.
I'm not familiar with the jargon, but no it's not, we have a very large DB hosted and maintained externally. Perhaps that is why?
Oooooh ok. Yeah in the training it looks way more helpful than I'm sure it is in everyday use. Thanks anyway!
&gt;although nothing would prevent Mary from liking roses at $5 each, while Jane likes roses at $4 each True enough, but SELECT DISTINCT Flower, Cost FROM Likes would return both in that situation anyway, and the question doesn't indicate it can't return multiple costs for the same flower (though if it was properly normalised this probably wouldn't happen anyway). Flower | Cost ---|--- Rose | $4 Rose | $5 Tulip | $4 Etc. | Etc.
What version of MS SQL are you running? You can run PowerShell from within a SQL Server Agent job... this is probably going to be the "best" way of doing it... otherwise you could run it from PowerShell remotely or via Task Scheduler on the SQL server, not much worse, but it will require the sqlps module to be installed... or try to run it entirely in SQL... not impossible, but you'll end up using cursors and hidden / undocumented `xp_` stored procedures. Assuming at least SQL 2008 R2, you can run the following PowerShell from a SQL Server Agent job to create a directory / subdirectories... assuming the SQL Server Agent Service account is a domain user and has access to this share. $path = '\\someServer\share\' $result = invoke-sqlcmd -query "select jobNumber from DATABASE.dbo.JobNumbers" cd $env:HOMEDRIVE # can't query UNC paths from SQLSERVER:\ foreach ($folder in $result.jobNumber) { if (-not (Test-Path "$path\$folder") ) { # Folder Doesn't Exist, create it New-Item -Path "$path\$folder" -ItemType "Directory" # Create additional subdirectories New-Item -Path "$path\$folder\jobFolder1" -ItemType "Directory" New-Item -Path "$path\$folder\jobFolder2" -ItemType "Directory" New-Item -Path "$path\$folder\jobFolder3" -ItemType "Directory" } } For a pure TSQL solution, you'll have to do something like this (incomplete): DECLARE @dirPath nvarchar(255) = '\\someShare\path\' DECLARE @dirTree TABLE (subdirectory nvarchar(255), depth int) INSERT INTO @dirTree (subdirectory, depth) EXEC master.sys.xp_dirtree @dirPath,1,0 -- Will return a list of jobs that do not have corresponding folders -- You'll have to use a cursor or a while loop to go through them SELECT jobNumber from database.dbo.jobNumbers WHERE jobNumber NOT IN (SELECT subdirectory from @dirTree) -- Use xp_create_subdir to create the folders -- EXEC master.dbo.xp_create_subdir @dirPath + [jobNumber]
Updated the OP with the version (SQL Server 2012). I'll look this over - thanks for the input.
What's your whole query? Also is hire date a date column? Why convert it to varchar? Edit: you will also want to strip the time off the date if you want the full day 25 days ago to be returned.
You don't want to convert the hiredate to a varchar. If hiredate is already a date field, don't convert it at all: EI.hireDate &gt;= DATEADD(dd,-25,GETDATE())
You should not be using `CONVERT()` on the `hireDate` column if it already has a date datatype. If you wish to keep using `DATEADD()`, then use the following: `WHERE EI.hireDate &gt;= DATEADD(dd,-25,GETDATE())` Else, use `DATEDIFF()`: `WHERE DATEDIFF(dd,EI.hireDate,GETDATE()) &lt;= 25` ----- Note that `DATEADD()` and `DATEDIFF()` function differently.
It's given me issues sometimes but works pretty well normally. Insanely handy to a newb like me. 
You may have a logic error in your `where` clause as well then. But bad data can often be tricky.
Nope.. it'll break and you'll have to seek it out.
https://asktom.oracle.com/pls/apex/f?p=100:1:0 
http://www.w3schools.com/sql/default.asp 
If everybody knows MS SQL, then keep using MS SQL. Why go through the learning curve? All it does is slow things down and aggravate people. In the end, you can produce a fine product using either one. If you can find out why management is leaning towards Oracle, that would help.
Thank you very much. Excellent summary.
That query should work fine assuming you're trying to return only the rows from waRequisition where both its src_sys_cd exists in #rd and its id_wrk_loc_ctry_mapp exists in mpOrderWorkLocationCountry. To comment on this, this is okay as a manual query. But if this is to be in stored code, then performance wise, I'd utilize Inner Joins rather than using Exists in Where clauses.
This is to OP also. If hireDate is a datetime vs date, make sure to check if the date is inside your date range but outside your time. For example, you run the query at 1:00pm, it will go back to 1:00pm 25 days ago and miss anyone hired at 7am.
Python would better align with a data analyst role. PHP is similar but much more web focused rather than mathematical like Python is. So if you want to learn a scripting language learn Python even though most places solely use SQL for reporting
You might want to try doing three statements unioned (preferably Union all) together. 1 for actual, 1 for comparative, and 1 for current. Unless I'm reading it wrong, you could get rid of the case and the ORs in the where. How's the plan look? Are you getting good estimates?
If you join them before grouping, you'll get things counted twice. E.g, from your example data, because each ActID 1050 in tblbudget matches actid in TblSpent twice, the result set would have four rows with ActID 1050. where you probably want two. The trick would be to group each side first *before* joining them, so that each actid would appear only once in each side of the join.
Excellent. Thank you. ðŸ‘
Seriously, what does this have to do with SQL?
Apologies over the time taken to respond, theyve had me on other things. It appears i don't have the LAG function as i'm using 2008. Is there a way to recreate that part without that function? Thanks :)
I don't know anything about certs but having worked in the industry for 20 years, I can tell you to focus on SQL server and/or Oracle. MySQL is useful but not as your primary focus, and you should forget Access even exists.
Thanks /u/alinroc! Interesting. The only reason I was even interested in an SQL cert is because a lot of the jobs I'm trying to apply to now are asking for me to know SQL and maybe R and/or Python. But my degree is in Marketing and i discovered excel modeling at the tail end. So I have no way to PROVE I know SQL because my current employer doesn't require it. Would you suggest a different approach to increase my data skillset and show employers?
Will do /u/GunnerMcGrath! Would you happen to know what employers are looking for in the Data field, from a skill set perspective? 
Maybe do something using SQL and put the project up on github or something, with a link in your CV? I think that certs are shat on a lot but some HR managers don't know how useless they really are so they see "oh he has a cert in SQL, so he must be good at it" (even though certs prove nothing other than you're ability to pass a test), so they are not of *zero* value in trying to get a job.
Here's the thing - passing a test **doesn't prove you know how to do the job**. As luck would have it, this very question came up in yesterday's [webcast](https://youtu.be/OZUNCR6GT0Q?t=2m20s) from /u/brentozar . TL;DW: not worth it :) Go ahead and study for the exams if it makes you feel better, but don't pay money. I agree with /u/fahrenheitisretarded, come up with a project, build it, blog about it, and put it somewhere visible. Show that you *understand* the material, not that you can memorize enough to pass a somewhat arbitrary test.
I'm completely new to SQL, and I guess I have a hard time realizing the potential of it since I only use it to query in my position. Could you give an example of a good basic level project that would show somebody is well-rounded in SQL?
Thank you, I just now learned about semi joins and that's a better idea!
Different database's use slightly different dialects of SQL, so I'd stick to learning one platform. When you get good with that, you can shift to the others without much difficulty. I'd choose MS SQL or Oracle. Both of them have 100% free and legal versions you can download and install. DO NOT choose Access. It's a fine program, and very useful for what it does, but it's not a good place to learn SQL. Also, making anything useful with it is more about knowing VBA than anything else. Also, learn the hell out of Excel. 
Personal project for sure. If you know Python well you can set up an app that pulls data into a SQL db and then you can do whatever with it. As long as the employer knows that you did something actually related to work, that's what they're looking for. Not "does this person remember the syntax for creating a table in SQL?"
thanks ill give some of these a try. These tables are in a firebird DB so i have ran into more issues than i normal. I had thought about making views and that would solve my problem but whenever this system gets an update it refreshes the DB and clears out custom views and functions that are not part the shipped database.
&gt; I believe this is because I am trying to save an XML data type into a variable which is a nvarchar. I believe you're incorrect. That select for XML path is within the STUFF, and STUFF returns character data. So, can you give us the actual statement that gives the error? Your select by itself worked, so what did you add to it?
Thank you, This worked like a charm, but would you know how to write this using LINQ? Edit: var queryAllCustomerTitle = from cust in _titleRepository.Table orderby cust.TitleName == "Other" ? 1 : 0, cust.TitleName select cust; This worked 
You are getting some good advice from others but I'll give a slightly different take. One of the MS SQL exams is on querying SQL. Now take the exam or not - your call - but the [book](https://www.amazon.com/gp/product/0735666059) is a great way to introduce yourself to some advanced concepts like windowing functions. Now other exam books (looking at you administering sql) are steaming piles of crap but the linked one is readable and you actually use what you've learned. The other issue is the old 'you don't know what you don't know.' The book will give you ideas and terms you can use to get better google results.
Some Formatting Issues - Working on!
Thank you all! This got more convo than I thought! Follow up: I know Excel like the back of my hand. Is a Mastery cert useless as well? 
Good thing I don't work for you. I saw your parent account and it's fucking hilarious you are a joke. 
To get all records with a Table2 date greater than or equal to the Table1 date, simply add a where condition like this: SELECT Table1.ID ,Table1.[Date] ,Table1.Information ,Table2.Information FROM FirstTable Table1 JOIN SecondTable Table2 ON Table1.ID = Table2.ID WHERE Table2.[Date] &gt;= Table1.[Date] However, it sounds like you want to limit it to one record per ID. This can be done in a number of ways depending on what columns you're selecting, but in this case I would use a row number partition: SELECT A.ID , A.[Date] , A.Table1Information , A.Table2Information FROM ( SELECT Table1.ID ,Table1.[Date] ,Table1.Information AS Table1Information ,Table2.Information AS Table2Information , RowNum = ROW_NUMBER() OVER(PARTITION BY Table1.ID ORDER BY TABLE2.[Date]) FROM FirstTable Table1 JOIN SecondTable Table2 ON Table1.ID = Table2.ID WHERE Table2.[Date] &gt;= Table1.[Date] )A where RowNum = 1
I have a few suggestions, which may or may not work depending on your environment. * I'm guessing you're using SQL Server, is that right? * What does the explain plan look like? * Could you break it up into multiple steps by materialising some of the queries? I mean, convert some of the subqueries into materialised views, and then query from that? It should make the query simpler because the results of functions are already calculated.
QA doesn't set requirements. End users/product owners do. 
Also interested.
This looks like a fun project. One thing I noticed right away and that you already mentioned are the duplicate joins. This___ left join ( select src_sys_cd , CUST_ID , REQ_ID , COUNT(*) as ReqDistributions_mapped from ( select distinct rd.src_sys_cd , rd.REQ_ID , rd.CUST_ID , ms.SUPL_NAME_MAPP from dbo.waRequisitionDistribution rd inner join dbo.waSupplier s on s.src_sys_cd = rd.src_sys_cd and s.cust_id = rd.cust_id and s.supl_id = rd.supl_id inner join dbo.mpSupplierName ms on ms.ID_SUPP_NAME_MAPP = s.id_supp_name_mapp ) x Could be replaced with a much simpler cross apply. And I just quickly reviewed but there seem to be a couple that could be replaced or eliminated entirely. If it's the speed alone, I'd start with removing those. That will get it down to probably half the time. 
why not calculate number_proficient before you pivot?
I am not to sure who I actually answer to yet, only 2nd week on the job lol 
To be fair, I've seen a few instances where semi-joins caused worse execution plans compared to inner joins. But, this isn't a commonality from my experience - so far. It was really bizarre. *It just depends* on the situation, the environment, the alignment of the planets, and the data among other things.
If you check my other post you'll see what i'm trying to accomplish. The problem is that there are so many joins that have no real use other than to minimize the select, and the execution time is huge.
and how do you show that? bring queries you've written to the interview? I've written thousands of queries in my current job, some of them quite complex, and i'm THE SQL GUY at my current place of work, but I don't know how to *sell* this skill.
This will break when `TitleName` is "zzz". ;)
/u/ben_it has some good points. This does appear to be MSSQL due to the dbo schema being qualified with the table table names and the ISNULL functions. Can you confirm that this is in fact MSSQL? Also, this can't be a select statement for a view because of the "into dbo.BP_workingReqD" line. If I'm correct here, you should be able to simplify this by using a combination of a series of CTE's and temp tables. It just depends on how many rows each aspect of this query are going to return\handle and maybe how the referenced tables are indexed. If you find that you absolutely need some of the temp data to be indexed, you'll need to use indexed temp tables instead of CTE's. Those case statements for the PreID and Payroll columns in the projection are concatenating strings and checking for the existence of certain strings within the concatenated strings. I want to think there's a better way - possibly set based - to handle this. But it depends on the data. You may find that its worthwhile to wrap this into a stored procedure if it isn't already. 
Sorry, brain fart for not putting it in the titles. Yes its MSSQL. &gt;Also, this can't be a select statement for a view because of the "into dbo.BP_workingReqD" line. I added this line to store the original results in a table that I can reference once I rebuild the process to make sure they are identical. &gt;If you find that you absolutely need some of the temp data to be indexed, you'll need to use indexed temp tables instead of CTE's. Currently using this approach. &gt;Those case statements for the PreID and Payroll columns in the projection are concatenating strings and checking for the existence of certain strings within the concatenated strings. I want to think there's a better way - possibly set based - to handle this. But it depends on the data. I agree, but haven't prioritized rewriting these as there are a bunch of other things to get to first. 
Every interview is different. Most of mine have had a technical portion of the interview where a dev lead asks questions to verify that I have the necessary skills. A couple have asked me to write pseudocode for different types of joins and aggregate functions along with providing an explanation. One interview left me alone in a room with a pc and told me to write a bubblesort function in PL/SQL and that I had an hour to finish. 
&gt; A couple have asked me to write pseudocode for different types of joins and aggregate functions along with providing an explanation Had one like this a while ago. There was one question I couldn't quite get worked out - I had an idea of how I wanted to approach it but just couldn't solidify it in my head (the whole exercise was on paper). The interviewer tried to nudge me in the right direction, said "couldn't you use &lt;x&gt; there?" and I said that I had considered it, but it wouldn't satisfy the second half of the requirement as written. He stepped back, said "oh...yeah, you're right, I guess I had the wrong answer myself"
Sorry to butt in, but in my experience, it's SQL, programming, visualisation, stats, business knowledge, process knowledge. Maybe one such skill set might look like this: - SQL Server / Oracle / Postgres / MySql - R / Python - Tableau / Qlikview / Other such data viz or reporting platform - Statistics: confidence intervals, significance testing; statistical testing: linear regression, decision trees, logistic regression, etc; machine learning: k-means, neural networks / SVM - Business knowledge: domain experience, presentation and communication skills, project planning - Process engineering (some knowledge of Kanban / Agile) - Other: web analytics (GA/IBM/Adobe), Hadoop, Hive ("big data") Data field is pretty hot at the moment.
Have an upvote. Who is downvoting this?
Yes i know,was a pain to figure out. I have updared code I'll post when home
Awesome.
sweet 
The `GROUP BY ... HAVING` syntax might help you out... http://www.w3schools.com/sql/sql_having.asp You basically have to select the `userId`, average of `duration` from the `sessions` table, group the results by `userId`, having a count greater than 1.
okay, cool, thanks. also, do you do machine learning stuff?? i was thinking of getting a ms in biostats
UPDATE y SET x = 'beepbeepboopbeep' WHERE a = 'boop' Run it as SELECT x FROM Y WHERE a = 'boop' first to make sure those are the records you want to update. 
That's a British expression. I meant I've been using it for real at work. Very little actual anger. It's not hard to pick up at all.
Folk keep wanting to do this (construct SQL on the fly) and it's always a bad idea. V.frequently it's because you're trying to do too much with one query. Often it's because you haven't thought hard enough about your request. Sometimes you haven't thought hard enough about your schema and your normalization. Your SQL statement is where you commit yourself to a group of columns. Code like this is very fragile, and more or less impossible to understand and maintain.
if you feed the subquery the value 'boop' to search for the `a` column, and then use the `a` column value (which has to be equal to 'boop') to the UPDATE, then as /u/adc90 points out, you might as well just feed that value without the subquery however, /u/adc90's UPDATE is wrong, ~unless~ there's only one table involved here -- your question looked like `Table` and `Y` are two different tables
The [Stanford Introduction to Databases Course](https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about) is one option. In addition to topic-focused lectures there are also both Quizzes and Exercises that can be submitted and automatically checked by the course system. Though the course is five years old, the material is foundational and not likely to become outdated quickly. Also, the age and popularity of the course (enrollment in the tens-of-thousands) means it's likely that you can find other people's solutions to a problem online.
[removed]
The only reason I say this is because once you install sql on your laptop you are going to have to consider the security implications. Also as far as I know there's no real way to remove it short of reinstall the os. I personally have reservations on doing it like that but others may think otherwise. It's a risk/reward scenario so if it has more reward than risk for you go for it. 
That's a big query to chew on. A lot of subqueries need to be broken down and evaluated.. I would rewrite them as CTEs for readability sake. I spotted a few places where you could replace aggregate subqueries with windowing functions. Example: left join ( select src_sys_cd , CUST_ID , REQ_ID , COUNT(*) as ReqDistributions_All from dbo.waRequisitionDistribution group by src_sys_cd, CUST_ID, REQ_ID ) reqdis on reqdis.SRC_SYS_CD = r.SRC_SYS_CD and reqdis.CUST_ID = r.CUST_ID and reqdis.REQ_ID = r.REQ_ID Could likely be removed and replaced with this window function in the top of the query: SELECT COUNT(*) OVER (PARTITION BY src_sys_cd, CUST_ID, REQ_ID) as waRequisitionDistribution 
You can get a free online MySQL database [here](https://mysql8.db4free.net/signup.php). It's a very simple signup. Otherwise, if you're looking to play with MS SQL, you can download [SQL Server Express](https://www.microsoft.com/en-cy/sql-server/sql-server-editions-express). This is a great way to learn SQL Server Management Studio (SSMS).
Thanks. But what do I do with it after making this free database?
You'll be sent a confirmation email. There will be several links, but go near the bottom of the email and click the link that has "confirm" in the URL. After that, it will take you to a confirmation page. On the bottom left of that site, you can log into PHPMyAdmin using the username and password you designated when you signed up. After signing in, click the "SQL" tab on top to open the query window. You can navigate the database on the left.
It's really not a big deal if he's just doing this for training.
 Look for the training database 'adventure works' , there are a bunch of tutorials online that use it. Udemy has a pretty good one, w3schools and code acedmy have free tutorials also. Oh I should say I am talking about MSSQL, not MySql. I think MySql comes with a test db..
[Brent Ozar](https://www.brentozar.com/) has a great blog as well as a lot of free resources, including a real-world database avaiable for download. Just sign up for the email and download the "First Responder Kit". I haven't used any of his paid services yet. The free stuff has been enough for now.
I wouldn't bother building any solution on MySQL. My vote would be Postgres all the way. I've never looked at Pentaho Kettle before, but if you really want to upgrade your nerd skills just write your own ETL scripts (Node, Python, PHP, whatever)
This is an imperfect solution (due to lack of details) but it's a good first stab: With FirstPurchase AS ( SELECT Price, MIN(EnteredDate) as FirstEntered FROM table GROUP BY Price ) SELECT t.[Order], t.User, t.EnteredDate, t.Price FROM table t INNER JOIN FirstPurchase fp ON t.Price = fp.Price AND t.EnteredDate = fp.FirstEntered To improve it, you should likely select MIN(SomeUniqueField) in the CTE and join on that.
Look at Rank/Row Number window functions. There are rough ways of doing it but the window functions are great one line ways of doing it. 
Sorry. I'm using 2008 Microsoft sql server report builder. 
&gt; Pivoting data is loads easier in Excel [SQL Server does pivot](https://blogs.msdn.microsoft.com/spike/2009/03/03/pivot-tables-in-sql-server-a-simple-sample/) in case you need it.
How about supplying the query and tabke structure? 
I have the desktop version installed but I haven't dabbled with it too much yet because I'm sure my company wouldn't support others getting it. I'm ready Ken Puls book right now on Power Query and it's just crazy how great it is
 SELECT product_id , shop_id , history_date , price FROM pricehistory AS t WHERE ( SELECT COUNT(*) FROM pricehistory WHERE product_id = t.product_id AND shop_id = t.shop_id AND history_date &gt; t.history_date ) &lt; 10 
Seconding the ETL bit. Every place I've worked has had its own ETL scripts based in either Perl or Python
for each row in the table, the subquery counts how many other rows for the same product/shop have a later date -- if this count is less than 10, then that original row must be in the top 10 for that product/shop
Postgres++. Kettle and the entire Pentaho suite is actually quite nice, even if a little bit lacking in the UI layer^1. After the initial getting used to period, it actually made more sense to me than SSIS and was my go-to ETL for quick and dirty jobs. ^1 I just checked PDI out, and Spoon looks so much better than it used to.
/u/ninjaroach has a valid point, to elaborate why not MySQL: for now it only implements SQL-92 standards, whereas Postgres, Oracle and MSSQL all evolved at least to SQL:2008 (things like CTEs or Windowed functions; see https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems#Database_capabilities) Also, as said on other comments, ETL tools differ heavily, so you might indeed end up building your own tool yourself. Another suggestion might be to stay with what you already use at work and tinker further, you can build this toolchain on any windows computer with enough capacities for a SQL server installation. I doubt you'll need more for a deep dive than the capabilities that are available on their free developer edition: https://www.microsoft.com/en-us/sql-server/sql-server-editions-developers It installs its tools (including SSIS) with it. Where I work we ended up building our custom toolchain around SSIS with . Connection string is built from server/user config, datasource schema gets read into config db, objects to be imported can be selected and customized (keys, include/exclude columns) and a record is written to db for each configured object. One SSIS package, completely parametrized for use with any ODBC and OLAP datasource, with a web interface to all neccessary configs. This is used for thousands of dataloads daily on a real server, and probably overkill for a bit of tinkering for now, I just wanted to add that you could actually start from within your already known environment.
[removed]
&gt; The PM just changed the design on me yesterday afternoon. Your PM is also the physical data modeler on this project? Wow. I don't know enough about your situation to help, but you or someone technical really should have control over your shcema design.
In reference to the first question (where with a function on an indexed column), maybe the fact that the choices : Good practice â€” There is no major improvement possible. Bad practice â€” There is a major improvement possible. are nonsensical and skewed your results in some way. Surely the better choices are : Good practice â€” There is a major improvement possible. Bad practice â€” There is no major improvement possible. Or am I missing something? 
Your suggested statements don't make sense. The survey's options make sense. Using a function in a where clause when that function statement isn't indexed is a bad practice and can be majorly improved.
I like this. Covers the basics well. Explanations of what you got wrong (for Oracle at least) are well thought out and easy to understand.
I like to use concatenations a lot depending on the data set so instead of row IDs I'll switch it out with a concat. Side question: how many people use Oracle here?
Guess I don't see the problem. It's not at all confusing to me. The question asks if it's a good or bad performance practice. Functions in where clauses are not a good practice and it can be majorly improved by rewriting the query. That's makes the first answer wrong and the second answer the logical choice. Your provided alternative good practice answer implies that queries of that type are good practice, but can be improved. That's only half right, though. It's not a good practice.
Sidebar
I just started using oracle in my sql course
You didn't read the question and you didn't understand my reply. 
The issue is that some may think that with an index on the date column, that there actually will be a possible improvement. This is the subtlety of the English language that I am trying to get across, nothing to do with tuning SQL or indexes. I am sorry if this isn't clear, I work with lots of foreigners and this has confused 2 of them when I showed them the question. Anecdotal data for sure, but IMHO the choices are not clear. I shall be reading the rest of your site with interest. 
I finally see the confusion. Some are thinking that adding the index removes the full table scan and that is the major improvement. Personally I wouldn't consider replacing a full table scan with a full index scan a major improvement, but I get the confusion now.
To clarify, this is a transactional system. There are already checks in place on the source system, but I need to be able to identify other types of bad data that the source system just doesn't check for (and I don't have the ability to customize it to do so). I put more info in the intial post.
I'm unaware of any solution other than having a or several scripts that run daily and point out issues. It sounds like your real issue is that your DB lacks Foreign Keys. I'm in a similar boat. IMO your best prospect is to advocate a total rebuild of the system and let the DB work like a DB should. When you talk with management, I'd suggest using the analogy: We've built this nice house but there is no foundation. It keeps settling and we can keep patching it, but at some point you should actually fix the problem. Sadly that will be expensive.
I don't think a PIVOT would work because you don't really want to rotate the table (For example having a column for Resource 1007, and another column for 1009 and so on). What you are looking to do is slightly different and you have a couple of options. doing 20 left joins on the #temp table, but your probably best off just aggregation with case statements and handle the blank rows how you want. I did this in about 5 minutes in excel and it has the rows as text and just has the empty rows as blank, but you could have them as 0's or NULL's. I haven't fully tested the statement, but have done similar approaches before: Select ResourceType, [Resource], ,MAX( Case when OpCount = 1 then cast ([Order] as varchar) else '' END) as [Order1] , MAX( Case when OpCount = 1 then cast (InUse as varchar) else '' END) as InUse1 , MAX( Case when OpCount = 1 then cast (Mt as varchar) else '' END) as Mt1 ,MAX( Case when OpCount = 2 then cast ([Order] as varchar) else '' END) as [Order2] , MAX( Case when OpCount = 2 then cast (InUse as varchar) else '' END) as InUse2 , MAX( Case when OpCount = 2 then cast (Mt as varchar) else '' END) as Mt2 ,MAX( Case when OpCount = 3 then cast ([Order] as varchar) else '' END) as [Order3] , MAX( Case when OpCount = 3 then cast (InUse as varchar) else '' END) as InUse3 , MAX( Case when OpCount = 3 then cast (Mt as varchar) else '' END) as Mt3 ,MAX( Case when OpCount = 4 then cast ([Order] as varchar) else '' END) as [Order4] , MAX( Case when OpCount = 4 then cast (InUse as varchar) else '' END) as InUse4 , MAX( Case when OpCount = 4 then cast (Mt as varchar) else '' END) as Mt4 ,MAX( Case when OpCount = 5 then cast ([Order] as varchar) else '' END) as [Order5] , MAX( Case when OpCount = 5 then cast (InUse as varchar) else '' END) as InUse5 , MAX( Case when OpCount = 5 then cast (Mt as varchar) else '' END) as Mt5 ,MAX( Case when OpCount = 6 then cast ([Order] as varchar) else '' END) as [Order6] , MAX( Case when OpCount = 6 then cast (InUse as varchar) else '' END) as InUse6 , MAX( Case when OpCount = 6 then cast (Mt as varchar) else '' END) as Mt6 ,MAX( Case when OpCount = 7 then cast ([Order] as varchar) else '' END) as [Order7] , MAX( Case when OpCount = 7 then cast (InUse as varchar) else '' END) as InUse7 , MAX( Case when OpCount = 7 then cast (Mt as varchar) else '' END) as Mt7 ,MAX( Case when OpCount = 8 then cast ([Order] as varchar) else '' END) as [Order8] , MAX( Case when OpCount = 8 then cast (InUse as varchar) else '' END) as InUse8 , MAX( Case when OpCount = 8 then cast (Mt as varchar) else '' END) as Mt8 ,MAX( Case when OpCount = 9 then cast ([Order] as varchar) else '' END) as [Order9] , MAX( Case when OpCount = 9 then cast (InUse as varchar) else '' END) as InUse9 , MAX( Case when OpCount = 9 then cast (Mt as varchar) else '' END) as Mt9 ,MAX( Case when OpCount = 10 then cast ([Order] as varchar) else '' END) as [Order10] , MAX( Case when OpCount = 10 then cast (InUse as varchar) else '' END) as InUse10 , MAX( Case when OpCount = 10 then cast (Mt as varchar) else '' END) as Mt10 ,MAX( Case when OpCount = 11 then cast ([Order] as varchar) else '' END) as [Order11] , MAX( Case when OpCount = 11 then cast (InUse as varchar) else '' END) as InUse11 , MAX( Case when OpCount = 11 then cast (Mt as varchar) else '' END) as Mt11 ,MAX( Case when OpCount = 12 then cast ([Order] as varchar) else '' END) as [Order12] , MAX( Case when OpCount = 12 then cast (InUse as varchar) else '' END) as InUse12 , MAX( Case when OpCount = 12 then cast (Mt as varchar) else '' END) as Mt12 ,MAX( Case when OpCount = 13 then cast ([Order] as varchar) else '' END) as [Order13] , MAX( Case when OpCount = 13 then cast (InUse as varchar) else '' END) as InUse13 , MAX( Case when OpCount = 13 then cast (Mt as varchar) else '' END) as Mt13 ,MAX( Case when OpCount = 14 then cast ([Order] as varchar) else '' END) as [Order14] , MAX( Case when OpCount = 14 then cast (InUse as varchar) else '' END) as InUse14 , MAX( Case when OpCount = 14 then cast (Mt as varchar) else '' END) as Mt14 ,MAX( Case when OpCount = 15 then cast ([Order] as varchar) else '' END) as [Order15] , MAX( Case when OpCount = 15 then cast (InUse as varchar) else '' END) as InUse15 , MAX( Case when OpCount = 15 then cast (Mt as varchar) else '' END) as Mt15 ,MAX( Case when OpCount = 16 then cast ([Order] as varchar) else '' END) as [Order16] , MAX( Case when OpCount = 16 then cast (InUse as varchar) else '' END) as InUse16 , MAX( Case when OpCount = 16 then cast (Mt as varchar) else '' END) as Mt16 ,MAX( Case when OpCount = 17 then cast ([Order] as varchar) else '' END) as [Order17] , MAX( Case when OpCount = 17 then cast (InUse as varchar) else '' END) as InUse17 , MAX( Case when OpCount = 17 then cast (Mt as varchar) else '' END) as Mt17 ,MAX( Case when OpCount = 18 then cast ([Order] as varchar) else '' END) as [Order18] , MAX( Case when OpCount = 18 then cast (InUse as varchar) else '' END) as InUse18 , MAX( Case when OpCount = 18 then cast (Mt as varchar) else '' END) as Mt18 ,MAX( Case when OpCount = 19 then cast ([Order] as varchar) else '' END) as [Order19] , MAX( Case when OpCount = 19 then cast (InUse as varchar) else '' END) as InUse19 , MAX( Case when OpCount = 19 then cast (Mt as varchar) else '' END) as Mt19 ,MAX( Case when OpCount = 20 then cast ([Order] as varchar) else '' END) as [Order20] , MAX( Case when OpCount = 20 then cast (InUse as varchar) else '' END) as InUse20 , MAX( Case when OpCount = 20 then cast (Mt as varchar) else '' END) as Mt20 from #temp Group by ResourceType, Resource 
That's exactly what I needed. I don't know why I didn't think to just join to the temp table a bunch as I've done that on a smaller number before, looking at it too long I guess. I went with your case statements and it's working great. Thank you so much!!
You can make a stored procedure for each issue. Put all the stored procedure names in a table and have another stored procedure to execute them all. Set up a script to execute the proc however often it is needed. The procs then insert into another table or a set of tables. Then have another table to exclude false positives.
Have you looked into Informatica? It can handle some pretty complex scenarios and includes notification capabilities for data stewards.
Not going to give you exactly what to do, as your instructor obviously wants to teach you the ability to read and understand schemas. A few ideas to get you going: * Total how much the person was being paid vs how much they sold (Bonus: subtract the base costs of the item's production) * Lowest sales over hours worked this year * Total how many missed quotas (Bonus: break down by missed amount percent, 10/20/30% etc) For reference, here is the [schema diagram for the AdventureWorks 2008 database](https://moidulhassan.files.wordpress.com/2014/07/adventureworks2008_schema.gif).
I'll look into that, thank you. 
Sounds like my world! Great ideas, thanks. 
Probably OS or hardware. Almost certainly, in fact. I have done quite a few SQL Server installs and upgrades, and the installation program is rather solid.
Sounds like the OS or bad hardware. What hardware is it sitting on top of? What version OS? What version of SQL 2012? 
He meant as in "bonus points", it isn't necessary but consider it will give a better solution.
Just get the most recent record for each staff member where status is A ? edit - like Beefourthree idea for complete solution.
This would only work to find employees that were active on 1-1-2016 I think and would miss hires that happened after that, for example someone that only has a "HIRE" record from 12-20-2016. I think a better where clause would be: where JOB_STATUS = 'A' and ((effective_date &gt;= '2016-1-1' and effective_date &lt;'2017-1-1') or (EFF_END_DATE&gt;= '2016-1-1' and EFF_END_DATE&lt;'2017-1-1')) ****MSSQL guy, sorry if syntax is wrong, but that should be the general logic
I would build an ETL job to either pull the data from access and load it into a table in oracle, or vice versa, then do a join on data that does not tie out between the tables. Easy quick to generate report that way, less code all around.
No need for a for loop, do it in a single query [using a DBLink](http://www.orafaq.com/node/60). Always use SQL over PL/SQL.
Using a temp table is not ideal, doesn't give you much flexibility. Better to have a "staging" type table that you can drop all the data from, then load up again. It all depends on what patterns you are comfortable working with, if you are doing this from scratch the first time, well try it any which way you please. Eventually you'll find a set of tools that work well for you.
Thanks
[removed]
Not sure I follow - why do you need to loop at all? Does this work? SELECT dbo.getinbps95th(interfacetraffic.interfaceid, '2016-12-01 00:00:01', '2016-12-31 23:59:59') AS maxbps_in95, dbo.getoutbps95th(interfacetraffic.interfaceid, '2016-12-01 00:00:01', '2016-12-31 23:59:59') AS maxbps_out95, dbo.getmaxbps95th(interfacetraffic.interfaceid, '2016-12-01 00:00:01', '2016-12-31 23:59:59') AS maxbps_95 FROM interfacetraffic INNER JOIN (SELECT nodes.nodeid, nodes.caption AS node_caption, interfaces.interfaceid AS fuck, interfaces.customername, interfaces.caption AS int_caption FROM interfaces,nodes WHERE interfaces.caption LIKE '%95th%' AND interfaces.nodeid = nodes.nodeid) x ON x.fuck = interfacetraffic.interfaceid ? It would likely be even easier to do this without a subquery, by joining all three tables in a single statement, but I'm not really clear on why you've chosen to do this in a cursor to begin with so I'm not sure.
I think these types of exercises are more to see how you think than whether you can present a 100% complete solution in the allotted time. The fact that you were able to show that you understood the requirements and noted that his solution would have failed should have demonstrated that you knew your stuff.
You'll have to filter on dates, something along the lines of SELECT EEH.*, AA.Name AS Supervisor, BB.Name AS Manager FROM EEH LEFT JOIN EEH AS AA ON EEH.managerID = AA.EEID LEFT JOIN EEH AS BB ON AA.ManagerID = BB.EEID WHERE AA.EFFECTIVEDATE &lt;= BB.ENDDATE and AA.ENDDATE &gt;= BB.EFFECTIVEDATE and EEH.EFFECTIVEDATE &lt;= AA.ENDDATE and EEH.ENDDATE &gt;= AA.EFFECTIVEDATE But I'm assuming here that EEH has one record per assignment, rather than one record per manager (i.e., that it's a many-to-many relationship table). If EEH just has one record per person you're pretty much screwed as the data of historical employee/manager/supervisor assignments simply won't be there.
Don't get me wrong, I passed the interview with flying colors. That doesn't make me less upset with myself that I wasn't able to come up with the solution. Afterwards, it's more a matter of challenging myself to come up with a solution just for the sake of doing it.
If you need to be explicit like that, you could insert the first results into a temp table, then run your second query on it.
No problem... These links may help too: [Compatibility Levels](https://msdn.microsoft.com/en-us/library/bb510680.aspx) [DTS 2000 in SQL 2008 R2](https://msdn.microsoft.com/en-us/library/bb500440\(v=sql.105\).aspx) [DTS designer support in SSMS/BIDS](https://technet.microsoft.com/en-us/library/ms143755\(v=sql.105\).aspx) On the last link, I have some installs that have worked, and some that have not - I don't know why it's so hit and miss, but if you have no DTS packages then you're probably much better off. As it stands, I'm still stuck with major systems on SQL 2008 R2 due to DTS packages that need to be upgraded, and various schema items that SQL 2000 allowed that 2008 R2 (and higher) no longer allow. These can take a long time to deal with, but we're always chiseling away at it.
Your query is no longer available on pastebin.
Thanks guys, that works like a charm. I wasn't familiar with LEAD (and LAG). 
ANSI_NULLS and QUOTED_IDENTIFIER are typically on by default. ANSI_NULLS OFF allows comparison operators like &lt;&gt; and = to be used with NULL. NULL is not a value; it literally means *nothing* i.e. there is no data here. So when you execute a query such as... select * from table1 where field1 is null ... you're asking the database to return rows where there is no data in field1. Logically speaking it makes no sense to compare something to nothing, hence why we say *is null* rather than &lt;&gt; or =. QUOTED_IDENTIFIER OFF changes how the SQL engine interprets text between "...". By default, anything between "..." is treated as if it were contained within [...] thus making it an identifier, even keywords. The best case I can think of for this is during ETL, and replication scenarios, copying databases, parsing XML. I personally use [] for all my object identifiers - seeing as how this is a default behavior for every database I've worked with I can't see much case for turning it off, but it depends on the data I'm working with. 
Indeed, a lot seem to think table valued ~functions ~ are the bomb, but doesn't understand the drawbacks 
[removed]
Pretty good hardware. 16gb RAM, solid Mobo, computer was originally built as a server so it's pretty beefy. Running Windows 10 which I suspect is the problem 
Validate your hardware before you do, otherwise you might end up shits creak without a paddle, and a leaky boat, a few months down the road. Edit - AKA I am surprised installing a new instance didn't work. There might be something wrong with the server itself. 
Unfortunately no. I spent an entire day on the phone once with MS support trying to untangle a screwed uninstall and in the end they gave up and told me to rebuild the machine. So that's what I end up doing whenever I run into this. That's why I don't ever do it with production anymore.
Have you tested the RAM? I would run memtest to eliminate it as a possibility. And you're running bare metal or as a VM?
We are running an Oracle/PLSQL server. How does the select text dual work? I did select 'My comment' from dual and it printed 'MYCOMMENT' back yo me with the quotes and without spacing. Is this normal?
Assuming you don't rehire on the same eid, I think excluding your old leaver population would work. You might want to to_char the dates: select distinct eid from alljobs where eid not in (select eid from alljobs where job_status = 'T' and effective_date &lt; '01-JAN-2016')
I've tried READ COMMITTED as well. The deadlock errors are still there. I set it up as REPEATABLE_READ to test. I will change it back to READ COMMITTED but I do not get how a transaction with just UPDATE statements is causing deadlock issues. I thought the update lock to exclusive lock transition during an update statement would not cause such issue.
UPDATE MYTABLE SET MYCOLUMN=? WHERE ID=? This is the SQL statement in my PreparedStatement. I add 10 of those with different values in a batch inside a transaction and execute it.
Right that would be the column name which defaults to your select text. Underneath should be the real text. But that's probably not what you want if you just want to print directly I guess you're right. Dbms_out is the way to go. Did you see your output with that command and you just don't want to see the "anonymous block completed message?" If you don't see your message enable the DBMS window under "View" and press the Plus button (if you're using Oracle SQL Developer)
My message displayed the way I wanted. I just didn't want the "anonymous block comment" message. Do you know how to get rid of that?
Consider committing after each rather than as a batch of 10, or updating all ten within a single statement (e.g., UPDATE MYTABLE SET MYCOLUMN= CASE WHEN ID = x THEN a WHEN ID = y THEN b WHEN ID = z then c ELSE Mycolumn END WHERE ID in (x, y z). )
If two processes are attempting to update the same rows at the same time you can get deadlock errors. Say one process has a batch updating rows A and B, another updating rows B and A. First process locks row A, but since the other statement is updating row B it holds that lock until it can also lock B and update them both simultaneously. However the other process has locked row B, but can't release it until it can lock and update row A, because again they're all within the same transaction. You're right that a single simple update statement wouldn't cause the lock, but by batching ten of them together into a single transaction you can get locks. 
Could I use an isnull to convert the null to a blank? The company that is having me write this wants all nulls to show up as blank entries?
The problem is probably data in field1 that isn't numeric. SELECT a.field1 from a where isnumeric(a.field1) = 0 Would give you your bad data. 
In that case just change your case statement to take care of nulls: case when a.field1 &lt;&gt;0 and a.field1 is not null then cast(a.field1 as DECIMAL (9,2)) else '' end as Weight1
How will you run the script eventually? SQL*Plus is often used. That should only print the DBMS output like in the DBMS window I described. 
What's the data type on the column you're querying? I suspect it's already a decimal, in which case I would just cast it to a varchar and see if that works for them.
or even case when a.field1 &lt;&gt;0 then cast(cast(a.field1 as DECIMAL (9,2)) as varchar) else '' end as Weight1 but I feel dirty writing that. I think that would return a varchar formatted as a decimal. 
So this works: case when a.field1 &lt;&gt; 0 then cast(a.field1 as varchar) else '' end as Weight1 but then there are a bunch of trailing zeroes after the decimal. So if I can find a way to get rid of those that should do it.
That worked perfectly! Thank you so much!
&gt; I feel dirty writing that. I think that would return a varchar formatted as a decimal. I feel your pain. 
I see you've received other answers, but just to clarify null means "no value", not the word "null", and in most cases null will appear as blank in report tools. By casting the whole thing to text, you are also preventing a report to sum up or average the values, or the user might not have working numbers if exporting the report to excel (the cell might be text instead of number), etc. So in general, null is preferable.
Good to know. In my SQL server management studio nulls come up with "null". Thank you for the heads up
&gt; a.field1 &lt;&gt;0 This is where your problem is. In order to compare values of this column to zero, SQL Server must perform an implicit conversion on the entire column, and fails when it gets to a non-numeric value. If you are on SQL Server 2012 and above, you can use the [TRY_CONVERT\(\)](https://msdn.microsoft.com/en-us/library/hh230993.aspx) function to get around this. On earlier versions of SQL Server you will have to be more creative and add `NOT LIKE '%[^0-9]%'`to your CASE statement to handle non-numeric values.
These scripts will be running within SQLDeveloper for testing. Im simply trying to make a neat and clean display of data for different reports and records
Lo and behold I have fixed the issue. The table had a varchar(50) set as the clustered index. The value of the index was a UUID string. I was using that index for my update statements. I decided I had enough of this and added a Identity int column and set it as my new clustered index. Modified every single line of code that was referencing the old varchar(50) and no more deadlock errors. I even tried it with 4 threads, no errors. 
Thank you for your query/reply... why does the result only return columns based on the function? I think that is my existing challenge-- rebuild through a more efficient query. Perhaps the function (which I have not observed) isn't doing what I think it is, based on past experiences?
Orion's original query does exactly what you're saying, but from reading the query, all it does is create a temp table w/ dates that are supplied as the range, prior to the execution of the request. I think there's a bit more to it than I understand.
Maybe they aren't nulls they could be empty strings which the compiler would still see as values. You could try forcing empty strings to also be null before in mssql this could be done with a null if for example coalesce (nullif (column,''),0) This would mean a value would be returned if there is one or a 0 would be returned if the column contains either a null or an empty string 
Using common language features (e.g., window functions) is hardly "advanced."
It's not effective to do a coalesce in a subquery if you are joining it with a FULL OUTER join, meaning if on Q2 there isn't a "Donnee" to join with Q on then the whole row will be NULL and the coalesce on the sub query will do nothing. You will need to change the first SELECT * FROM (.... To do the coalesce there
I'll try to rebuild the code x_x
 It would be more something like this: SELECT Q.DONNEE, COALESCE(Q2.NUMLIGNE, 0) AS NUMLIGNE, COALESCE(Q2.JANUARY, 0) AS JANUARY, COALESCE(Q2.FEBRUARY, 0) AS FEBRUARY, ...... FROM (SELECT 'Nouveaux contrats' AS DONNEE UNION .......
[ROW_NUMBER](https://msdn.microsoft.com/en-us/library/ms186734.aspx) function will fit your needs perfectly. The PARTITION BY clause will restart the numbering everytime UniqItem changes. select ROW_NUMBER OVER ( PARTITION BY UniqItem ORDER BY UniqSubItem) as RowNumber, UniqItem, UniqSubItem from SomeTable
Just curious, why should it be handled by the report builder and not the query? 
Last update, I just find it insane that a simple change of a clustered index can make this much difference. Removing the old constraint (varchar (50) PK ) and adding a new one (Identity Int) made the application so much responsive with 0 deadlocks. Mind blown.
That's fine, use a Temp table or CTE for your month table.
Post a table creation and inserts script and I'll write it for you.
I'm sorry but I don't think I can get you that with the tools I have to work with... I'm barely able to run simple queries. :(
I dont really get what you mean by packages und text files. Usually you choose an Database System and the only way to interact with your Data is SQL. The reason you use a RDBMS is that you don't care about how stuff is stored and retrieved. You declare what your Data looks like (DDL - Crate Table etc.) and what you want to store, change, retrieve or delete (DML - insert, update, select, delete) on an abstract level. The RDBMS takes care of all the files and persistence for you. There are databases that can be "live" a single data file like sqlite.
&gt; Here is what I did so far. &gt; &gt; `where XANALAPP like 'NO3%'` &gt; `and XNUMECHAPP in (select distinct XNUMECHAPP from XDCURES where XANALAPP = 'TON Haut')` The problem is you need the XDILUTION from that subquery in the `SET` section. There's no way for a `WHERE` subquery to get up there. Try aliasing your target table (which allows you to reference it in a subquery) and moving the subquery to the `SET`: update XDCURES TRG set NUMRES = ( select case when B.XNUMECHAPP is not null then A.NUMRES/B.XDILUTION else A.NUMRES end from XDCURES A left join XDCURES B on A.XNUMECHAPP = B.XNUMECHAPP where B.XANALAPP = 'TON Haut' and A.XANALAPP = TRG.XANALAPP ) where TRG.XANALAPP like 'NO3%' ; What this does is update **all** NO3 records, even if there is no corresponding TON Haut record. If there is none (left outer join), then just update with the existing value. 
Honestly though, the trouble is with `UPDATE`, right? You can `SELECT` the new value pretty easily: select A.XNUMECHAPP, A.XANALAPP, A.NUMRES/B.XDILUTION as NEW_NUMRES from XDCURES A join XDCURES B on A.XNUMECHAPP = B.XNUMECHAPP where A.XANALAPP like 'NO3%' and B.XANALAPP = 'TON Haut' So often, it's easier to just do a `MERGE` instead of an `UPDATE`: merge into XDCURES TRG using ( select A.XNUMECHAPP, A.XANALAPP, A.NUMRES/B.XDILUTION as NEW_NUMRES from XDCURES A join XDCURES B on A.XNUMECHAPP = B.XNUMECHAPP where A.XANALAPP like 'NO3%' and B.XANALAPP = 'TON Haut' ) SRC on ( TRG.XNUMECHAPP = SRC.XNUMECHAPP and TRG.XANALAPP = SRC.XANALAPP) when matched then updated set TRG.NUMRES = SRC.NEW_NUMBRES ; 
so, why not stop using 'pivot' syntax shortcut if you don't find it easy/understandable enough? 'pivot' is a syntactical sugar for the MS SQL - a wrapper over &lt;Aggrgegate&gt;(case &lt;selector&gt; when &lt;X&gt; then &lt;column_X&gt;... end) ... group by &lt;other columns in pivoted result&gt;. Just write out all the elements needed on your own.
Caution: Some of the examples use non-standard SQL.
The description given sounds a lot like a data formatting request. That's the responsibility of the report builder - the database returns the data, the report formats it. Whether or not to display zeros, how many decimals of precision to display, etc. Those are formatting questions.
Keep it in the database. That's what it's for, to hold your data. I can't think of any reason to keep the data in text files. 
[removed]
&gt;recipes \*twitch*
Very nicely presented but the SQL is pretty awful for anything beyond a small database. Good idea though, has potential. 
Huh? Confused to say the least. 
My bad. I totally missed an article before "SQL" in your last statement and parsed it as: SQL is pretty awful for anything beyond a small database. I was extremely confused, but I understand now :)
&gt;Plus it can participate in an Failover Cluster and AG, even along side Windows versions of SQL Holy freaking shit. I'm doing backflips over here. Microsoft, take my (employer's) money!!
To each their own then. Throughout my college degree I felt like modifying code and debugging others code was useless for me. Once I graduated I got a job dealing with SQL and they threw me right in. I actually learn something new every day, and that's not something I thought I would catch myself ever saying. 
You shouldn't need to change your calculations at all, just need to modify the "select *" to instead select the individual columns and coalesce them there. Done quickly but here is the full update to the query: DECLARE @VALEUR VARCHAR(10) SET @VALEUR = RTRIM(CAST(:A_USER AS VARCHAR(10))) -- This value will match the id of the connected person SET language us_english DECLARE @P_A_USER VARCHAR(10) SELECT @P_A_USER = VALEUR FROM T_PARAMETRE WHERE PARAGRAPHE = 'COD_PRTDIRCOMM' DECLARE @COD_GRP VARCHAR(10) SELECT @COD_GRP = COD_GRP FROM USERS WHERE COD_USER = @VALEUR IF @P_A_USER = 'GBL' BEGIN SET @P_A_USER = NULL END SELECT Q.DONNEE,Q2.DONNEE,Q2.NUMLIGNE, COALESCE(Q2.january, 0) AS JANUARY, COALESCE(Q2.february, 0) AS FEBRUARY, COALESCE(Q2.march, 0) AS MARCH, COALESCE(Q2.april, 0) AS APRIL, COALESCE(Q2.may, 0) AS MAY, COALESCE(Q2.june, 0) AS JUNE, COALESCE(Q2.july, 0) AS JULY, COALESCE(Q2.august, 0) AS AUGUST, COALESCE(Q2.september, 0) AS SEPTEMBER, COALESCE(Q2.october, 0) AS OCTOBER, COALESCE(Q2.november, 0) AS NOVEMBER, COALESCE(Q2.december, 0) AS DECEMBER FROM (SELECT 'Nouveaux contrats' AS DONNEE UNION SELECT 'Contrats RenouvelÃ©s' AS DONNEE UNION SELECT 'RDV RÃ©alisÃ©s' AS DONNEE UNION SELECT 'RDV PlanifiÃ©s' AS DONNEE UNION SELECT 'Nouveaux comptes ouverts' AS DONNEE UNION SELECT 'Nouvelles affaires' AS DONNEE UNION SELECT 'Nvl affaires avec exploitations' AS DONNEE UNION SELECT 'CA Nouvelles affaires (â‚¬)' AS DONNEE UNION SELECT 'Affaires gagnÃ©es' AS DONNEE UNION SELECT 'Affaires perdues' AS DONNEE UNION SELECT 'Taux affaires gagnÃ©es' AS DONNEE) Q LEFT OUTER JOIN ( -----------------------------------DEBUT TOTAL---------------------------- SELECT donnee, NUMLIGNE, COALESCE(january, 0) AS JANUARY, COALESCE(february, 0) AS FEBRUARY, COALESCE(march, 0) AS MARCH, COALESCE(april, 0) AS APRIL, COALESCE(may, 0) AS MAY, COALESCE(june, 0) AS JUNE, COALESCE(july, 0) AS JULY, COALESCE(august, 0) AS AUGUST, COALESCE(september, 0) AS SEPTEMBER, COALESCE(october, 0) AS OCTOBER, COALESCE(november, 0) AS NOVEMBER, COALESCE(december, 0) AS DECEMBER FROM (SELECT Datename(month, d1) AS 'MOIS' , Cast(Count(*)AS DECIMAL (18, 2)) AS 'Total_Nouveaux_Contrats', 'Nouveaux contrats' AS 'DONNEE', 'A' AS 'NUMLIGNE' FROM contrat WHERE c22 = ISNULL(@P_A_USER, c22) AND Month(D1)= MONTH(getdate()) AND C27 = 'Nouveau contrat' GROUP BY Datename(month, d1)) AS Total_Nouveaux_Contrats PIVOT(Sum(Total_Nouveaux_Contrats) FOR mois IN (january, february, march, april, may, june, july, august, september, october, november, december)) AS pvt1 ------------------------------------------------------------------------- UNION ALL SELECT donnee, NUMLIGNE, COALESCE(january, 0) AS JANUARY, COALESCE(february, 0) AS FEBRUARY, COALESCE(march, 0) AS MARCH, COALESCE(april, 0) AS APRIL, COALESCE(may, 0) AS MAY, COALESCE(june, 0) AS JUNE, COALESCE(july, 0) AS JULY, COALESCE(august, 0) AS AUGUST, COALESCE(september, 0) AS SEPTEMBER, COALESCE(october, 0) AS OCTOBER, COALESCE(november, 0) AS NOVEMBER, COALESCE(december, 0) AS DECEMBER FROM (SELECT Datename(month, d1) AS 'MOIS', Cast(Count(*)AS DECIMAL (18, 2)) AS 'Total_Contrats_Renouveles', 'Contrats RenouvelÃ©s' AS 'DONNEE', 'B' AS 'NUMLIGNE' FROM contrat WHERE c22 = ISNULL(@P_A_USER, c22) AND Month(D1)= MONTH(getdate()) AND ( c27 = NULL OR c27 = 'nouveau contrat' ) GROUP BY Datename(month, d1)) AS Total_Contrats_Renouveles PIVOT(Sum(Total_Contrats_Renouveles) FOR mois IN (january, february, march, april, may, june, july, august, september, october, november, december)) AS pvt2 ------------------------------------------------------------------------ UNION ALL -- ETC ... -- La fin : ) Q2 ON Q2.donnee = Q.donnee ORDER BY NUMLIGNE ASC 
I actually think I have it done already. Just thought it was an interesting problem and might spur some interesting discussion. I ran a test before I left which finished in about 8minutes, but I still have one tweak to make which should get it down faster. When I ran the job the other night to get the execution plan it took 4hrs.
No kidding! I still haven't gotten a URL for the Webinar, but I emailed them earlier. There are lots of videos on Youtube showing-up about installing and using SQL on Linux. I think they're calling SQL vNext, here's a URL with more info: https://msdn.microsoft.com/en-us/library/mt788653.aspx 
Beginner here... Could this be done in a where clause? Edit: or maybe a Replace function in a subquery? 
Are you sure? Where is applied before the group by operator (see my post above), so it would eliminate groups that I may want to be present in my results set, correct?
Oh my god; you sly devil. I can't believe I didn't think of this! counting any non-blank record as 1 and using sum instead of count. THANK YOU. I wish they had something here like they do at /r/excel where we could verify solutions, because this one is probably the easiest! EDIT: As others have noted, this is no longer truly counting distinct. There are a few solutions in comments farther on down for anyone curious for a solution!
Truncate table SQL.
you're right--I assumed OP was just using distinct in the example to try and limit the number of blank values or something and didn't actually want to count distinct values
select att1, att2, att3, Count(distinct NullIf(att4,'')) from table A group by att1, att2, att3; This might work
You are correct! However, still very much appreciate u/ihaxr's response. It made me realize the issue was with my nesting requiring an extra group, not my methodology of using case: SELECT att1, att2, att3, count(distinct case when att4 = '' then null else att4 end) as Count4 FROM TABLE A GROUP BY Att1, Att2, Att3; EDIT and I appreciate your help and code as well--didn't see it because I hadn't refreshed while I was busy trying and refining all these suggestions :) EDIT 2: Yeah, all the sql syntax differences get a little cumbersome to keep up with between mysql, postgre, T-SQL, PL/SQL... enough to drive one crazy. For MS SQL/T-SQL, I can say isnull() is a function used to replace null values, and since I really want the other way around--to replace '' values with null--I'd want to use Nullif(att4,'') as noted by /u/boxlefty. That case statement would also work too, but I modified in my solution above due to data types (as you noted) being varchar and failing implicit conversion once we go putting zeroes in there :). Thanks again for your replies!
That would probably be a solution as well! Never knew about the NullIf Statement... cool way to reduce my query above to a more condensed version!
yep, because then there's at least one tuple in the set where att4 would now be 0, meaning there's one more unique value to count. So basically the same problem I was having with '' being in there. This will also cause a query to fail if the original variable is varchar due to implicit data type conversion failing, but I didn't specify that my aggregated attribute was varchar so that's my bad. I'd use the solution I posted above: SELECT att1, att2, att3, count(distinct case when att4 = '' then null else att4 end) as Count4 FROM TABLE A GROUP BY Att1, Att2, Att3;
It looks like Mark Litwintschik the author of this article has spammed every subedit he could find: https://www.reddit.com/r/programming/duplicates/5q5n9z/11b_taxi_rides_on_kdbq_and_4_xeon_phi_cpus/ Furthermore there seems to be several puppet accounts voting them all up.
Ask their developers :-) Probably, but the question depends on too many things to be very meaningful.
Thank you very much for all that work you made. The last query didn't work at first but then I spotted some typos. when matched then **update** set TRG.NUMRES = **SRC.NEW_NUMRES** It now seems to run fine. Will check the output and be back to you asap. **EDIT : OMG it works like a charm. You're the man. Thank you!**
My issue is solved already, thank you for your interest!
Hell, we still use fax. Just because a technology is old doesn't mean it isn't going anywhere.
SQL will be around longer than any of us.
Can you post the error message you're getting? Is it possible that the column output is different between the unions? A month might not be represented in one of those result sets, for example.
I would agree, particularly if you have a defined structure like 12 months. I only use Pivot whenver the number of output columns is dynamic.
So I'm with you... the problem is that this is a daisy chain, e.g. select * into #1 from source join source2 select * into #2 from #1 join source3 select * into #3 from #2 join source4 So conceptually it looks like this: * ** *** **** ***** ****** ******* ******** With each step adding new columns, and having more complex functions running. So by the end things are taking a long time to finish running because there is no index on #10. At the moment I'm not indexing any of the #tables along the way, but all of the source tables are indexed. However by the final steps of the process all the joins involve #tables to create the final data set, so there aren't any joins at all. 
OK, so going along with what I suggested, you could add indexes to #1 *after* the data is inserted, but before you select from it to insert into #2. You would still do better (faster processing time) to collect all of your data and write it in one pass, than to iteratively build &amp; chain. Really, you ought to learn CTEs.
I am quite familiar with CTE's... they would give horrible performance with this process. It's too big.
They are very rare and highly paid people who work on legacy business systems the hold up the financial, banking, airline and many other industries.
But long term solution - get better data (consistent string lengths between decimals)
With doublequotes, but case sensitive.
Brackets didn't work, but putting just the column name in quotes worked, thanks for the suggestion!
Does it not just return the same results as if GROUP BY salespersonid was included? e: oh nvm I just read https://dev.mysql.com/doc/refman/5.7/en/group-by-handling.html Apparently it will return the sum of all sales for everyone and whichever salespersonid the optimiser touches first and this, thankfully, is no longer default behaviour for MySql
You'll want to use a combination of [SUBSTR](https://www.techonthenet.com/oracle/functions/substr.php) and [INSTR](https://www.techonthenet.com/oracle/functions/substr.php). with t as ( --these are just examples select '6.222.5405' STR from dual union all select '54.1245.3453' STR from dual union all select '1.125.3' STR from dual union all select '54.122.' STR from dual ) select STR as ORIG_STR , substr(STR, 1, instr(STR, '.')-1) as PART1 --start at 1, go to 1 less than the first "." , substr(STR, instr(STR, '.')+1, instr(STR, '.', 1, 2)-instr(STR, '.')-1) as PART2 --start 1 after the first dot, go the number of characters between the two dots , substr(STR, instr(STR, '.', 1, 2)+1) as PART3 --start after the second dot, go to the end. from t
You're welcome! Thanks for the gold!
Someone who programs in COBOL.
It would be useful if you told us what platform you're using. The tools you would use for analysing performance vary between platform.
Good stuff 
It depends in what you're trying to do but y oh u likely want to download the one that is the tools one under Windows. Unzip the file and then open your command prompt. Change your command prompt's working directory to where the unzipped folder is and run the sqlite3 command with a database name. Then write SQL. SQLite is designed to not need fancy installs and what not so that programs can use them for themselves. 
You might want to try [Code Academy's Learn SQL tutorial](https://www.codecademy.com/learn/learn-sql). It is short, to the point, interactive, and - don't be fooled - you can do all the core lessons for free. The portion explaining GROUP BY is in Section 3: Aggregate Functions.
Hmm that's strange I was expecting you to say a few million records, a few thousand should never be a problem 
I'm not certain on what your desired output is despite your explanation, could be I'm still low on the caffeine this morning. However at first glance, it seems you should do the recursion outside of the CTE in order to actually take advantage, as the union all defeats the purpose of the CTE unless you are going to filter that somehow. I dumped your table example into a temp table to play with. WITH jobs AS ( SELECT j.prnt_id, j.job_id, j.agent_id, j.inhagent, 0 as Level from ##jobsets J) SELECT j.prnt_id, j.job_id, j.agent_id, j.inhagent, (Level + 1) as Level FROM ##jobsets j INNER JOIN jobs as c ON c.prnt_id = j.job_id results prnt_id job_id agent_id inhagent Level NULL 1 123 N 1 NULL 3 456 N 1 NULL 5 123 N 1 5 6 456 N 1 
 WITH CTE AS ( -- Anchor member definition SELECT j.prnt_id, j.job_id, j.agent_id, j.inhagent, 0 as Level FROM jobs j WHERE prnt_id IS NULL -- filter for base case UNION ALL -- Recursive member definition SELECT j.prnt_id, j.job_id, j.agent_id, j.inhagent, c.Level + 1 FROM jobs j INNER JOIN CTE as c ON j.prnt_id = c.job_id --recursively find jobs whose parent is an existing job in the CTE ) select * from CTE
You should be served well by a case expression with a condition: Case when (condition) then (expression) else (expression) end.
Not sure what DB you're on, but in Oracle you can use FIRST. Something like max (balance) keep (dense_rank first order by year, month desc)
yeah there is format your code how the fuck am i supposed to read that
&gt; Change your command prompt's working directory to where the unzipped folder is and run the sqlite3 command with a database name. Then write SQL This seems to be the big piece of information that I was missing. Thanks man, I'll update and see if I solve this.
Thanks, u/ichp. If I'm understanding correctly, then I should do the following? CASE WHEN FTE is NULL then '1' WHEN FTE is &gt; 1 then '1' ELSE FTE END If that's correct, will there be an issue where I'm referencing FTE without the table name before it? Meaning that the FTE field is a column added. Forgive my ignorance here, but wouldn't I need to reference the table.fte or would FTE simply be the reference? Again, thanks for your help. Really appreciated! 
There are records that meet every condition of the where but are not coming through, I cannot explain why. Is there something wrong with my joins that could cause this?
it looks like you would SELECT * FROM CONF JOIN INDUSTRY on Conf.Name = Industry.Name But that is only if you actually need information from the Industry table, otherwise it seems the query you wrote is fine.
Something like this may work, I'd play around with the partition/order though. On mobile so excuse formatting SELECT * FROM ( SELECT * , ROW_NUMBER() OVER (PARTITION BY ANTY_YY ORDER BY ANTY_YY DESC) as year , ROW_NUMBER() OVER (PARTITION BY ANTY_MM ORDER BY ANTY_MM DESC) as month FROM DSNP.PR01_T_DROP_AMTS) x WHERE x.RECIP_SSN_NBR = 111223333 AND x.year = 1 AND x.month = 1
Ah, thanks. I would need information from the industry table, but at the later part of the message it says 'The *VALUE* contains the ID that you would use to join the table' so the relationship is really the name field (not the value field)? That's where I'm getting hung up. Why would the relationship be with the name not the value since the value is what contains the ID?
which leads to the question, why is /u/ToatZimco using a GROUP BY clause if there's no aggregation in the SELECT clause?
Good point. I've heard people argue it's more efficient to use `GROUP BY` all your selected fields instead of `SELECT DISTINCT, but I don't know if that's really true anymore (or ever). Haven't bothered to test it.
It's a bit confusing to me. If you run the example query, &gt; Example: Select Value from conf where name = â€˜industryâ€™ Do you get integers? If so, I guess Conf.Value is a foreign key to Industry.ID when Conf.Name is "Industry" and your join would look like JOIN Industry ON Conf.Value = Industry.ID AND Conf.Name = 'Industry'
You need to give the CTE an arguments list so the child id can become the next parent id. It's a little awkward, but you get used to it.
On most platforms, you won't be able to reference column alias within the same subquery. You wrap your original query as a subquery or you could simply use original expressions/columns, e.g. Case when EMP.hours is null then 1 When EMP.hours &gt;= 40 then 1 Else 0 End as FTE
There's no real reason to do that, it's going to do a sort operation regardless. It's a category 5 mess.
Sqlfiddle is not really meant for large amounts of data. It's very likely submitting that as a single query and could be reaching a max size for a string somewhere, or it could be that MySQL only allows so many values to be inserted that way at once - I know SQL Server has such a limit when executing queries from Management Studio. I'd try your query on an actual server, and if you still run into problems look into the limitations of INSERT/VALUES. Edit: [Link](http://stackoverflow.com/questions/3536103/mysql-how-many-rows-can-i-insert-in-one-single-insert-statement) for the kind of thing I mean.
Is this a question from a course? If so then it's worded in a naff way. It's purely my feeling, intuition, whatever, but this design seems a bit weird. "Value" seems like it's (by name) a non-key attribute, so why is it a foreign key? Hmm. I'd prefer a CLIENTS table, where each row is a client, a CLIENT_INDUSTRIES table, where each row is an association of a client to an industry, given that a client can be involved in many industries and an industry can have many clients, and then a CLIENT_CONFIGS table, where a row is a client configuration (past and present). But yeah the answer /u/shaunc is my interpretation too. Maybe I'm being harsh because we don't have example data or much in the way of context.
Yeah I understand and used sqlfiddle to provide a working mcve. I've tried inserting values and load data through parsing the text file and both methods prove to be incorrect in the final output through heidisql on my local server. I'm honestly stumped. 
Are you able to provide the larger sample via Pastebin maybe? I'm thinking maybe an out of whack delimiter? I often find bringing the data into a spreadsheet and trying the aggregates there can help find problems too.
Thanks! Will give it a shot this evening!
I solved it. The SPRTELE.SPRTELE_STATUS_IND &lt;&gt; 'I' was precluding null values which I needed. fixed.
because I don't know what the fuck I'm doing. No IT background and I teach my self SQL. I figured it out though and solved the problem.
I'd focus on the OLTP side of things to start with, if you'd like a breakdown of the difference between OLAP there's a decent article [here](http://datawarehouse4u.info/OLTP-vs-OLAP.html). The TL;DR is that OLTP is transactional (i.e., you're store front data) and OLAP is analytical (i.e., your year-over-year sales growth numbers that are derived from the store front data). You can just as easily use AdventureWorks OLTP on SQL 2016, you just may not get any examples of SQL 2016's new features in the database.... But, there's still a boatload to learn from what IS there, and you may well find that there's more resources at this point that use AdventureWorks. See [here](https://blogs.msdn.microsoft.com/samlester/2016/06/13/so-long-adventureworks-well-miss-you-and-hello-world-wide-importers/) for details. Regardless, there's nothing stopping you from using both AW and WWI, and if you feel so inclined you can use AW OLAP too! Just restore them all to a server and you'll have 3 sample databases instead of one. If you are planning to build a new database from scratch, then I would highly suggest that you NOT use SQL Management Studio to do this. It's more of a management/admin tool, though you can certainly find products that will help plug it into source control as well. Microsoft released SQL Server Development Tools (SSDT) quite some time back, and it is the preferred tool for database development. It is an add-on to Visual Studio, and it provides supports for Database Projects within VS. It provides deployment tools, schema comparisons, source control integration, etc - and it is ALSO a great learning tool as you can reverse engineer a database (like AW or WWI) into the scripts that would be used to create it. The other route you can go is Entity Framework, where you can have several options - such as designing a database from code. You can dive right in with C#, and there's also plenty of training materials out there. I'd actually warn against this though, unless you're going to have an experienced DB Developer reviewing your code - a LOT can go wrong when approaching data access from an object oriented point of view. I've seen EF code written by *experienced* developers that has made DBAs cry (not to mention the poor database engine). Anyways, here's a [link](https://msdn.microsoft.com/en-us/library/jj590844.aspx) to some tutorials that may use various sample databases - whatever one it needs, just download it and restore it to your instance, you can always delete it if you find it's the wrong one. Edit: Added link to AW/2016 bit.
Try reading the sidebar.
[removed]
Scroll a bit further down below the sections "Posting" and "Format Your Code" and you will see a section for learning SQL. I would just get started with a single SQL flavor -- don't worry about the others unless you need to. They all have essentially the same fundamental statements. When you get into more analytics then the differences become much greater. By then you should be able to go from one to another without much difficulty (and reading documentation). 
If I follow, you're going to want a price table with fund/stock, price, date columns. You'll then join that table to your other table that has fund/stock start date, end date and quantity. DON'T use just the year, you'll end up with some that were both bought and sold in the same year, also it makes it tougher than you need it to be.
I'm trying to value a list of financial hedges. Just to explain what that means, say you are a baker so you buy a lot of flour. If you think the price of flour is going to go up in the future, you enter into a contract to purchase flour at a set price (usually with a bank). Say you enter into a contract to buy 100lbs per month for a year at $4/lb. In 3 months, the price of flour rises to $6/lb. You go to your supplier and buy 100lbs for $600 but since you're in a contract to buy it at $4/lb, the counterparty to the contract (the bank) will owe you $200 so you only spend $400 total. However, if the price dropped to $2/lb then you would buy 100lbs for $200 but then you owe the counterparty (the bank) $200, still spending $400 total. So what I'm trying to do is be able to find out how much you owe or how much you are owed. The issue I'm having is dealing with a large number of contracts spanning different time periods. It's easiest to enter in the contract data as a start date and end date. I don't want to enter 12 rows of monthly data, I would rather enter 1 row and specify that the quantity is monthly. I'm at a conflict between making it easy for the user to input vs easier to calculate but user input has to take priority.
&gt;I'd actually warn against this though, unless you're going to have an experienced DB Developer reviewing your code - a LOT can go wrong when approaching data access from an object oriented point of view. While that can happen, following best practices is a pretty good way to avoid the generation of terrible SQL, like N+1 bad. Coming from someone who does lots of DB Dev work, I think using EF and then profiling to find out if the machine built out some really dumb TSQL, is much more preferable than building the code out by hand from an efficiency POV. Just my 2 cents.
I haven't really set anything up yet, I'm mostly trying to figure out the best way. It sounds like I should just make an entry for each year. It would end up being more work on the input but would be easier to run the calculations. 
I think if you're looking for a database administration position, then yes certifications can help. But if you're doing more data analysis/BI I don't think the certifications matter (although I think Microsoft has an intro cert for basic SQL stuff: https://www.microsoft.com/en-us/learning/course.aspx?cid=40364). 
I can get behind that, though I can honestly say I have never seen anyone write EF code that actually took the time to profile or check on WHAT they were actually doing on the database - my experience on that front is limited to one major project though. It wasn't even usually the machine building out bad code so much, as developers not understanding the way in which they were accessing the data and why it may be bad - writes or materialized selects within ForEach loops, careless use of Include with queries ballooning to 1000+ columns, or 3000+ lines of actual SQL statements, etc. If you're writing that logic in SQL you can very quickly see why it may be bad, as the result sets aren't abstracted away behind some framework. I do see the value in programmatically building out SQL - I tend towards helper/builder libs rather than full blown ORMS though.
I'd definitely say you're reaching the limit of what you can execute via a client like HeidiSQL or Workbench, based on the size of that script. Trying running it via the command line ['mysql'](https://dev.mysql.com/doc/refman/5.7/en/mysql-command-options.html), you shouldn't be restricted by the size of the script there. I'd remove everything but the insert and values parts though, you can run the other part of the query, everything from: ; select @curRank := @curRank + 1 as Rank, ...and on, in your client after the import is done. Another option, you can try [increasing the packet size for the server](https://dev.mysql.com/doc/refman/5.5/en/packet-too-large.html). The 5.5 server default was 1MB, and your script is about 4x that. You can put it as high as 32MB according to that link. Another option, assuming you don't have direct access to a shell or CLI on the MySQL Server, break the values part of the insert up into more manageable pieces. If the max packet size default is 1MB, then split it into 4-5 chunks of inserts. Something like this (but with about 20k rows max between each insert): CREATE TABLE name_table2 (`state_name` varchar(2), `gender_name` varchar(1), `year_name` int, `name_name` varchar(13), `count_name` int) ; INSERT INTO name_table2 (`state_name`, `gender_name`, `year_name`, `name_name`, `count_name`) VALUES ('MA', 'F', 1910, 'Mary', 989), ('MA', 'F', 1910, 'Helen', 473), ('MA', 'F', 1910, 'Margaret', 374), ('MA', 'F', 1910, 'Dorothy', 331), ('MA', 'F', 1910, 'Alice', 313); INSERT INTO name_table2 (`state_name`, `gender_name`, `year_name`, `name_name`, `count_name`) VALUES ('MA', 'F', 1910, 'Anna', 252), ('MA', 'F', 1910, 'Ruth', 247), ('MA', 'F', 1910, 'Elizabeth', 224), ('MA', 'F', 1910, 'Mildred', 198), ('MA', 'F', 1910, 'Lillian', 196), ('MA', 'F', 1910, 'Rose', 187); INSERT INTO name_table2 (`state_name`, `gender_name`, `year_name`, `name_name`, `count_name`) VALUES ('MA', 'F', 1910, 'Catherine', 184), ('MA', 'F', 1910, 'Evelyn', 175), ('MA', 'F', 1910, 'Florence', 168), ('MA', 'F', 1910, 'Marion', 162), ('MA', 'F', 1910, 'Frances', 146); ...That way you can stay within the 1MB packet size if you just execute each of the insert parts one at a time. Hope that helps!
Unless it's just a one-off query/report or something you'd have to hand-key (which is rare- this kind of data is almost always available in some electronic format- another database, an Excel work book, a CSV, whatever) , what I would do is try to keep the data as granular as possible. You never know what you'll want to do with it later, and you can aggregate however you want from there.
We're pretty heavy into JS/React nowadays so we're using a lot of [Knex.js](http://knexjs.org/), I think [this](https://github.com/StackExchange/dapper-dot-net) was the one I was looking at for .NET when we were still using it more though.
What RDBMS is this in? MySQL? SQL Server? Postgres?
Thanks for the help.
Lol good point
just a guy trying to learn here. why was a CTE used instead of just having the DateRanges table as a subquery?
BS in Finance here. Look into positions for Data Analysts and make your path from there. Initially, I was hired because I knew pivot tables. I took on every project I could get my hands on. My first foray into data management was Access, which led me to learn VBA. Next was outgrowing Access and needing to learn SQL, which I did. Next we needed automation so I learned SSIS and lots of DBA-esque type optimization. Decided to try my hand at console-apps, so I learned C# and developed a neat little console/.NET report distribution system. Staying in the grey area in between being a "True" anything (dba, dev, reporting, etc...) has kept my job interesting and lucrative. I've noticed the most valuable devs are those who can interact/mediate with all sides; being a good facilitator between business needs and IT boundaries and being able to understand both worlds is a surprisingly rare quality. I'd slate myself as senior-level SQL dev and intermediate DBA, earning north of 6 figures after ~6 years (and starting at around 40k). My salary trajectory has not been quite as steep as others, but I really like the company I work for and decided several years ago that I didn't want to leave to hit the marks slightly faster.
You're on the right track. As someone else said, experience speaks louder than certificates. Certificates can't hurt to help you build the knowledge though. Sounds like you have more experience pulling data from databases than actually designing them? If so, data modeling is an art in itself that has to be learned and fine-tuned by circumstances that are too numerous to count. The rules change and evolve with the different flavors of databases (relational, triple-store, no-sql, etc) and newer technologies and features emerge every day. Everyone is jumping on the "Big Data" bandwagon and there are numerous possibilities for the future if that bubble doesn't burst. To answer your question, yes, there is a profession in it. Just do your best to be multi-faceted in your skills and keep them fresh with the newest capabilities.
Realistically though, few companies will offer a DBA job to someone with a background in Finance, no certs and no DBA experience. IMO though, the role he describes isn't entirely a DBA, but more of a database analyst/developer with bits of DB admin throw in. 
Sorry I figured it out. It did need a join. This is what I got to make it work. Sorry I was confusing. SELECT S_FIRST || â€˜ â€˜ || S_MI || â€˜. â€˜ || S_LAST â€œS_NAMEâ€, F_FIRST || â€˜ â€˜ || F_MI || â€˜. â€˜ || F_LAST â€œF_NAMEâ€ FROM STUDENT JOIN FACULTY USING (F_ID); 
Something like this is what I do as well. Depending on the size of a company, a department for this is separated in different roles. System, network and database admin; Data Analyst/Scientist, Business Analyst, Solution Architect and Developer roles for ETL, SQL, Data Cubes/Models, Reports/Dashboards; All of them should mutually understand where their fields overlap, and as you described, ideally closely interact with each other on that.
i would take a long, hard look at that fifth table
I work for a pretty large super regional bank. we do have all the typical IT components. Our group has carved out a nice niche of being an agile reporting and solutions group for a specific line of business, but our infrastructure is backed by "true" IT assets. We have our own DBA, server team, etc. I typically face with our internal client and do needs assessment and hack together concept proofs. When needed for prod we hand lifecycles over to my IT components who then produce polished code/objects/etc and finishes documentation and their own SDLC. Most times it doesn't 
True, Ozar did a great piece on why you don't really see Jr DBA jobs most of the time. Data can be incredibly precious, you want to make sure that whoever is handling it takes proper care of it and knows what they are doing. 
Yes and data modeling might be where you want to look. Certifications aren't nearly as important as work experience in this field and you should be constantly expanding your skillset to meet new challenges and do things better or more efficiently. The old adage of "Always be Learning" really does apply here. Why don't you know what SSAS is or much about SSIS? Maybe it's because you don't use SQL Server much. Still, take the time to learn it and learn it well. Anything new that comes to the platforms you work with _learn inside and out and use them_. Each new thing is another check-mark in your favor. There is definitely a future in employment for skilled data professionals. You're well on your way there!
First.. no, that's not too many tables. Second.. start writing your SQL and figure out the joins as you go. That's usually the most helpful way to figure these sorts of things out. You want: SELECT Student.s_id , Student.s_first , Student.s_last , Student.s_class , Term.Term_Desc , Course.Course_Name , Course.credits , Enrollment.Grade FROM Student INNER JOIN Enrollment ON &lt;join condition&gt; INNER JOIN Term ON &lt;join condition&gt; INNER JOIN Course_Section ON &lt;join condition&gt; INNER JOIN Course ON &lt;join condition&gt; WHERE &lt;limits that apply to that full set if any&gt; Now you get to figure out what primary keys in the tables you know must be part of this are foriegn keys in other tables which could change the order of your inner joins. Start with student. Which of the other tables does student tie to? I'm going to guess it ties to Enrollment? Work your way through the list of tables you've been given and plug in the appropriate join conditions to tie them together.
So my position now is data analytics. My company we do much of our automation with ssrs reporting and withh ssis I get the how to use it but my company does insert much data into our database from outside sources besides what ever enter through our website. 
So my problem is I don't really never got to use my finances my first job was in Data analytics because of my excel and analytic school
You could do this with PowerShell script and run it weekly from your computer as a scheduled task. First thing you want to look up is **Export SQL Data to CSV**, from there you can **Upload Files to Document Library Using Powershell**.
Not an expert by any means but I'd recommend visual studio and create an SSIS package. 
Thanks to /u/ihaxr for the helps via PM. I've updated the OP with the final code which achieved the desired result.
What is the error message?
1 - What is the error? 2 - What SQL are you using? MSSQL, Oracle, Postgree...
JSON data? XML strings? in any case, it violates First Normal Form you don't "walk through" it with SQL, you pull it out and do the heavy lifting in application code Wordpress is an abomination
gotta be Teradata because of the NO FALLBACK the error message is probably on the comma after NO FALLBACK
I'm not positive but that looks like a [PHP serialized](http://php.net/serialize) string that describes a 2-dimensional array containing 7 rows of data. (NOTE: Tested with PHP, I cannot "unserialize" this structure, perhaps Reddit formatting has screwed with some of the original data) The rows seem to contain fields with the names "nba_player_cloth_type", "nba_player_cloth_name", and "prev_worn" -- Interestingly enough, the first row does not contain a field called "prev_worn" so this exact data structure is not possible with pure SQL. Perhaps some post-processing code stripped the field out completely because it contained a null value - but that is only a guess. It may have come from SQL, but it is not SQL. It is simply a data structure.
Yeah, powershell is your best bet here. Check out the commandlet invoke-sqlcmd2 and how to send email from powershell. There are lots of how-tos on both. 
Given that youre new to SQL i hate to complicate things but check out an ETL tool of some sort (I use SSIS primarily at work) as they are made for this sort of task. It might be a little more learning than you were imagining, but once you get your query down, you'll basically tell your etl tool to take the output and store it into a flat file. You'll then make a list of what to iterate over so that you can perform that process as many times as necessary to generate the number of files you need. So, in short, get your query figured out. Check out an ETL tool and plug your query into a simple data source to file destination package. Then expand that process to handle the rest of the files you want. If you don't want to leverage an ETL tool, then I imagine you could build a list if tables and iterate over that list. In SQLServer I believe you might be able to leverage bcp to help you do this. Hope that's more helpful than not.
I suppose it also causes headaches when assessing the database from anywhere else, like a web server. It's so much easier to tidy up the column names on SELECT statements though, why do you feel the need so strongly to have spaces in the column names?
Just because you can doesn't mean you should. I use pl/sql to build dynamic queries [Oracle]. Trying to add quotes to strings can make the code harder to read and easier to introduce bugs. Actually, that's about my biggest warning against it is that any program that interacts with the database will be more fragile. Especially dynamic sql, because it won't get validated until runtime. Trust me. Keep it simple. The less things that can break, the better off you are.
Many of the analytics tools, including tablue, use the column names in the database when presenting. The analytics tools assume names are names, and not some db level obfuscated variable name, which I think would be nice I see responses about "hard to escape", but again, that seems a tool issue. If you're generating sql without calling a "escape properly" function, of course things will be "fragile". I think it's unfortunate, but that's ok. I'm not one for any sort of "best practices" without reason, and the biggest reasons I see with this are because it's a best practice not to, which seems to be the cause of the problems/lack of support. So much so that it's seen as a "noob" thing, so of course it would never become adopted/well supported. But the problems seem real, so it is what it is. It's too bad a "human name" isn't in the standard. I'll probably end up using a view, with evil names, similar to [this](https://www.mssqltips.com/sqlservertip/2427/two-options-to-store-user-friendly-column-names-in-sql-server/), which seems very silly.
&gt; It causes headaches around escaping your SQL statement when it's being used in something other than a GUI tool. Can you describe them? Or is it just having to use quotes/brackets? Why is using quotes/brackets considered difficult? In all other languages, delimiting, in some way, is a very normal thing to do. With the stigma around using simple names, I can see why it's not used by experienced people! And for that error, how is that not a bug? If it's not a bug, could it be a deprivation caused by the culture around it, rather than a real reason?
I'm confused why the square bracketed column labels are considered hard to read.
They're not, that's the point. It's a way of turning non-human readable column names into whatever you want. Identical to using "AS".
And, psql and the like have autocomplete. It seems this shouldn't be an issue at the prompt, or when writing in a sane editor. It's 2017, this all just seems so damn silly.
I meant, why not just use them? But, for my case, these tools connect to the database directly, so this would have to be at a dynamic view level or something. Is that true? Would that be performant?
Intellisense only works if your editor is connected to the DB. That's a luxury you as an analyst might have that devs often do not - often, we're writing code to generate SQL to be executed against a remote server, and intellisense isn't going to help. You're looking at your edge use case - which would be in what you call the 'noob' level of SQL use - and for you it makes no difference how your tables are named and wondering why good practice which has been developed for much more advanced use cases exists.
You really sound like you've made up your mind about using spaces in column names already... However say goodbye to writing quick statements- having to wrap everything in square brackets is a royal pain and relying on intellisense is not good practice. 
Surround it with another SELECT (AKA Subquery). SELECT *Case statement based on sub query result goes here* FROM ( *ORIGINAL SELECT GOES HERE* ) *Alias goes here if required* Depending on the RDBMS, you may or may not have to give the subquery an alias.
You put your query in brackets and select from that result set SELECT IF(Users &gt; 0) 1 ELSE 0 FROM ( SELECT COUNT([ID]) AS Users FROM tblUsers )
ah thx
Ty as well! 
I recommend a method like this: https://msdn.microsoft.com/en-us/library/ms187928.aspx
DECLARE @count INT SELECT @count = COUNT([ID]) AS Users FROM tblUsers IF(@count &gt; 0).... Is another unmentioned option
If you have full control of the code and will never need to use anything else then it might be fine albeit bad practice. However some tools dynamically create SQL from column names and I have encountered such tools where it did not encapsulate the column names and as such could not work on the database.
I picked MongoDB and PostGres, thanks a lot! Sorry for bothering you with mundane question.
Entity framework is clever but its SQL for non-SQL programmers and once you hit a performance issue you will find its very tricky to solve. The main issue with it is that unless you are aware of how SQL works and how to code to avoid them its prone to deadlocks when it's scaled up as its not as good at figuring out the lock order as some humans are. So if your knowledge of SQL is good enough to code in entity framework to avoid deadlocks, you might as-well do the SQL yourself. 
Writing SQL that returns the results set you are expecting is one thing. Writing it so that it performs well in a production environment with 1000 other users also querying the same tables is another thing entirely. If you have the training budget then get yourself on a SQL performance tuning course, your DBA will thank you later as it will unlikely to be your code thats causing issues. SQL requires a different perspective to .Net and people who write C# then learn SQL either have to unlearn what they think they know or they end up with terrible code that 'works' on dev but hammers the DB server. 
1. Calculate the month and year. 2. Calculate the start and end date of the month and year. 3. Find the difference between the end and start date. 4. Divide your numbers. [Le Goog](https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=caculate+start+and+end+date+for+current+month+sql+server)
Finding the difference between the start and end date doesn't help me though. That will just give me the total number of days in that month.
Oh gotcha, because "business" days. I had to do a quick and dirty fix awhile ago, I basically just listed the days 1-X, like Monday the 1st, etc. Then I deleted from it where the day = sat / sun, then ran a count on the list and that's your business days. I think a date table is a great solution because you can add a column, isbusinessday. You can always do something similar to what I described though depending on how you need it.
yeah, that can work for fixed dates holidays which you can figure out easily with something like select cast(cast(year(getdate()) as varchar) +'0101' as date) But if you have movable holidays, ie. when the actual holiday is a weekend, or like easter / good friday, which change every year, you probably should be using table to manage those holidays.
This only works if the database is set for English language.
Good point, you could use: -(CASE WHEN datepart(dw, @StartDate) = 1 THEN 1 ELSE 0 END) -(CASE WHEN datepart(dw, @EndDate) = 7 THEN 1 ELSE 0 END) but again, that will only works if your database has Datefirst = 7, which is the default for us/canada. You can view the current value using : Select @@datefirst and / or change it using : SET DATEFIRST 7;
The best nuts-and-bolts SQL book is SQL Cookbook.
Agile as in agile software development? I will be pulling data from the database to analyze (potentially with excel).
Yes agile dev methodology but it sounds like your moving into a data analyst role not so much BA
You're right. Eventually I should implement a table for this. But since we only have the 6 holidays off I was easily able to go forward and find out which dates we'd actually be off. I really appreciate your help with this. It looks like the report is running the way I'd like it. 
&gt; bad practice. But why? :P &gt; I have encountered such tools where it did not encapsulate the column names These are just broken tools, at the same level as those that happily support sql injection. They didn't sanitize foreign input. Surely buggy tools can't be a reason.
You don't need a new table for that, just a different query.
Thanks! &gt; depending on what language you're accessing it from it may be the difference between row.TotAmt and row["Total amount less applicable taxes"] Why did the languages diverge so much?
&gt;what extra value are you adding to your DB in return for making your code more difficult to maintain? Readability, clearly. To some people, readability is their biggest concern. To me, making sure I build a platform that works with 100% of all possible data is the most important thing, so supporting whitespace in a column name is part of that.
&gt; When we see string literals used without a valid reason like being needed for display for some reason, it tells us they don't have a lot of experience Are you referring to 'string literals', "object identifiers" or both?
That's different - you're designing tools to work with any DB that anyone has designed, not designing a DB yourself. When designing a DB for human-friendliness I don't find that camel casing is any more upsetting to users than is spacing.
No books, but never trust anything. Your code might look right, and it might execute properly, but the results will be wrong because of some weird data issue. Get in the habit of analyzing your own data sets and asking them questions, e.g., a parent table is 4M rows and I'm unioning 4 columns to look for distinct values. How many should I expect if there are few nulls? How many come back? Etc.
Thanks and similarly what did you find gave you the best understanding of select, joins etc
&gt; Can you describe them? Others have done a really good job in describing these issues. I'm particularly fond of this example from u/fauxmosexual: SELECT * from [ðŸ‘ŒðŸ‘€ðŸ‘ŒðŸ‘€ðŸ‘ŒðŸ‘€ðŸ‘ŒðŸ‘€ðŸ‘ŒðŸ‘€ john cena JOhN cEnaðŸ‘ŒÂ¯\_(ãƒ„)_/Â¯] &gt; And for that error, how is that not a bug? If it's not a bug, could it be a deprivation caused by the culture around it, rather than a real reason? It's not a bug because the vendor made a decision to depreciate the feature. And the reason for that probably has to do with added complexity that doesn't really give a lot of return. 
Years of work. Sorry bud, I've got no shortcuts for you there. EDIT: My best tip would be to study the relation of data that you're working with. Understand how to correlate two different records from the same table (often a GROUP BY, but sometimes a self JOIN and occasionally something crazier), versus correlating records across tables (generally the "equals" sign, but you can go crazy with JOIN predicates too)
Sorry, I don't really see that as a problem. Do you want camel casing in your object names or not? Because there's a huge difference in Oracle &amp; DB2 between: create view AccountingAssignmentDimL_MVL and create view "AccountingAssignmentDimL_MVL" If you're on SQL Server, that might not seem like such a big deal to you. But for the rest of the world, its the difference between SCREAMING_ALL_OBJECT_NAMES_IN_CAPS and allowing "Mixed-case, case sensitive &amp; special characters" in your object names. EDIT RE: Nested Views. I hate them.