I am in the same boat as you. So far I'm enjoying this book. http://shop.oreilly.com/product/9780596009762.do
Not sure what 'EDIT_DATE' is, but your logic is checking to see if it's closer to the present than 30 days ago, 60 days ago, etc. by using '&gt;', the greater than operator. So if EDIT_DATE is today, they're all going to be true. If EDIT_DATE was 31 days ago, all but the first will be true. It sounds like you want to see if EDIT_DATE is '&lt;' LESS THAN 30 days ago, 60 days ago, etc.
You also have a comma after the last column (alias 'BACKLOG_6') that will cause you grief.
Ent_date = entry date, aka when work commend. Edit_date is when work ended. So I'm wanting to find all entries that are 30 days unpaid from edit date, 60 days, etc. My background is mainly JS, does sql not end after a case is met? How should I go about this, doing 30-59; 60-89; etc? 
Thank you! A quick browse looks promising!
Understanding database design, theory, normalization/denormalization, how the SQL engine works/processes queries (for optimization), and how to create / test indexes for performance is probably the "next step". I've seen a lot of people that know how to join data from multiple tables to get their result, but often don't understand how to build a new schema from scratch... they end up either duplicating data or making it so normalized that it becomes very difficult to work with. I also know a ton of developers that can write great SQL, but it ends up performing like utter garbage because they don't understand how to build the indexes required to make the query work right. They'll end up just using the SQL Server tuning advisor to build out a brand new index when they could have just added an additional column to an existing index.
Dice has been polarizing. Their profile setup was easy and imported my resume, which was great, but trying to use their website in any capacity after that point is terrible. Going to dice.com, I cannot figure out how to get to my profile again. I had to just bookmark it.
Yep, had that at my desk when I first started working in SQL, pretty good little diagram of diagrams. 
Np. Feel free to send me any recommendations you have. 
I didn't even think about putting the listener in the my cert, maybe I should have thought of that. I have have each side of the AG with it's own cert and SSL is required on both. Every app connects fine using the listener. I am using SharePoint on one instance and various web apps and systems management databases on the other.
SQL 2016 has the ability to read JSON natively which would probably make this a lot easier: https://docs.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server I don't suppose you're using 2016 or have the ability to upgrade?
Thank you I'll take this advice on board as I move forward. Its very much appreciated. 
I will if I come across any! Only just started seriously thinking about it. 
I will admit, it took me a while to set it up also... But once I got it straightened out, it was easy sailing from there
Everything is done through SSMS V.17.3. I've never used JSON. 
... just use CASE expressions.
I have no idea what you are trying to do, but coalesce function returns the first non null value. So you could coalesce(B,C,D), coalesce(C,D) or something like that. 
&gt; They'll end up just using the SQL Server tuning advisor to build out a brand new index *internal screaming*
If you are looking to match the man with every woman and then also list the contact ID if they have a contact request try: select men.name as 'Men', women.name as 'Women', contactRequest.contactRequestID as 'Contact ID' from men join women on 1=1 left join contactRequest on contactRequest.manId = men.manID and contactRequest.womanID = women.womanID
Sorry didn't ready everything correctly. It appears you are looking at the favouriteWomen table for matches and then listing if they had a contact request. Below is what I got. Very similar to what you had just a couple changes on the join types. select men.name as 'Men', women.name as 'Women', contactRequest.contactRequestID as 'Contact ID' from men join favouriteWomen on favouriteWomen.manID = men.manID Left join women on women.womanID =favouriteWomen.womanID left join contactRequest on contactRequest.manId = men.manID and contactRequest.womanID = favouriteWomen.womanID
Hey thanks for trying but this doesn't do what I am after. This snippet is showing a list of favourite women of two men and in the third column it's showing if contact request was submitted for any of the women in the favourites. The third column is where I struggle to get the desired outcome as outlined in the above table.
The query you provided was almost there, just need the clause to connect contactRequest to favouriteWoman. In your query change left join contactRequest on favouriteWomen.manID = contactRequest.manID to left join contactRequest on favouriteWomen.manID = contactRequest.manID and favouriteWomen.womanID = contactRequest.womanID 
YES!!! This does the trick! Thanks heaps my friend!
Here is a better optimized version of the query. Instead of starting with the men table, you should start with the favouriteWomen table and join off that. select men.name as 'Men', women.name as 'Women', contactRequest.contactRequestID as 'Contact ID' from favouriteWomen left join contactRequest on favouriteWomen.manID = contactRequest.manID and favouriteWomen.womanID = contactRequest.womanID join men on men.manID = favouriteWomen.manID join women on favouriteWomen.womanID = women.womanID 
Is there a particular reason why to start with the favourites table first? There are other tables that I need to join to get the full query I am after and all of them are related to the 'men' table and ultimately the query gives you all the information 'about the men' so I thought starting there would make sense. Sorry, not really sure maybe a stupid question. I am quite new to SQL.
Thanks. I will try posting again with better details. 
Actually using them. Again, sorry for my lack of detail. 
Coalesce is what I'd suggest. I'd do it like UPDATE a SET ColA = COALESCE(ColA,ColB), ColB = COALESCE(ColB,ColC) FROM Table a; UPDATE a SET ColC = NULL FROM Table a WHERE ColB = ColC;
Thanks. How would that WHERE statement work with 40+ columns? Just add ORs? 
you set the last column to NULL when it's the same as the previous column. You'd run the top query as many times as you need to to shift the data to the left. Potentially 40 times. Each Time you run it you can have the second script be column 40-(times ran), or you can add another query to clean out the column that is now duplicated (best option IMO)
it's super fast without the order by
SELECT puff, puff2, pass FROM rotation WHERE turn=‘mine’
Due to the physical structure of a database, ORDER BY can be a pretty heavy operation, even when you're ordering close to an existing index structure. Here, you're doing just the opposite and enforcing randomness (by design, I'm sure, but it's still a heavy operation for the DB engine). Strictly speaking, ORDERing is a presentation function, and should be handled at the application, if possible. If you're pulling your results straight from SQL Developer, and need to randomize the rows, I think you can simply expect low performance. Sorry to be the bearer of bad news...
Something like this: select a.customer_id, a.room_type, a.reservation_date from bookings a where a.room_type = 'double' and not exists (select 'x' from bookings b where b.customer_id = a.customer_id and b.room_type &lt;&gt; 'double') 
Is it too late to change your data structure approach? You seem to be treating columns as arrays/lists. In SQL though, records are "same-ish" (they share the same metadata) while columns are "different" (column metadata is distinct, even if the data type is the same). Essentially, within a SQL query you always need to handle columns in a context of a single row on one-by-one basis.
everything you own join the box to the left 
Whatever you do I hope you made a copy of everything on that drive before you did anything... Having no knowledge of mediawiki, I googled "mediawiki database schema" https://upload.wikimedia.org/wikipedia/commons/9/94/MediaWiki_1.28.0_database_schema.svg Looks like much of the stuff is stored in binary format, which is going to make it a bit difficult for someone with no PHP and SQL knowledge to extract. My suggestion is to fix whatever is broken on the PHP page and once you get the wiki site back up then you can copy out your data as needed. You seem to be confusing "can't run database" with "I have no idea what a database is to begin with so my database may actually be working fine but my website isn't configured properly". I'm pretty sure it's the latter here, but it could be anywhere in between. Try connecting to the database with MySQL Workbench. If you are able to do that then you know the database is available and the issue is on the website configuration. If not, you will need to keep troubleshooting from there. Sorry, this is about all I can do without knowing this website and backend. 
The query is optimized based on the requirements you provided in the post. It's not optimized based on the unknown requirements in your head that aren't in the post though. Joining the men table may be better but since you didn't provide that information surely /u/addonKurt didn't know this beforehand right? ;)
Hi. I wish. It's not my data. The column is the result of a CASE statement parsing a string for a wildcard match. So up to 40 potential, distinct wildcard matches, and THEN, only the non null returns are supposed to be shifted to the left. Kill me. 
Are you able to alter the "CASE statement parsing a string" then? So that you apply the wildcard match sequentially and produce a number of records instead of a number of columns.
Iunno why you had to put something on blast like this. Not cool. 
Hmm so no easy file-editing-to-get-the-text-back. It's definitely the "website" configuration as that is what throwed all the errors previously. The current issue is that it's just handing me a blank page when I try to open the panel in the browser once MYSQL and Apache are running. And that apparently is a PHP error but without error code nothing can be done so I'm lost there. Too bad, it seems like all text is lost then ._." I should have thought about that before moving it to the new computer. But thanks anyways! At leats I can stop trying to find my written content in any of those thousands of files now, haha. 
I still think you are making an incorrect assessment about the status of your data. There could still be data in the database even though the website shows a blank page. The two issues could be completely unrelated. But you won't find out without either a) learning how to connect to a db using the MySQL workbench (not the same as the admin page) and some basic SQL or b) fixing the website configuration.
You are mixing up the join logic and the select logic. SELECT CASE WHEN A.userid IS NULL THEN XML.userid ELSE A.userid END AS userid, CASE WHEN A.userid IS NULL THEN XML.fieldID ELSE A.fieldID END AS fieldID, CASE WHEN A.userid IS NULL THEN XML.fieldName ELSE A.fieldName END AS fieldName, ........ FROM [File Data] AS XML LEFT JOIN [TableA] AS A on A.UserID = XML.UserID WHERE XML.FileID = 11 * XML is the 'anchor' table, its going to be the list of all the user IDs etc. * You LEFT join that to TableA on the UserID column which will give you XML data and any matching records from TableA. * IF a UserID exists in TableA but not in the XML file, you ignore it. * IF a UserID is in XML but is NOT in TableA, all TableA values will be NULL * So if the data Matches, A.UserID will not be null. Or you could say if A.UserID IS Null, then it only exists in the XML file 
I worked with MS Dynamics GP for a few years. Mostly creating custom crystal reports and analytics that were accessed via a 3rd party EDI module inside of GP. I'm very familiar with the data structure, etc and consider myself pretty highly skilled in tSQL. I work as a DBA(Analytics) now for another company, but would love to put in a few hours a week for some extra cash. PM me please and I can give examples work.
This thread isn't going to do your work for you. How about we start here: * What do you have to start with? * What have you tried so far?
I would build this into a Stored Procedure and pass the ClientID as a parameter. Then use IF statements and have custom queries per ClientID. That would be the easiest way, and keep you from having to use WHERE CASE, or manually modifying the query each time.
SELECT sname, cno, pno, qty FROM Company, Orders, Parts ORDER BY pno, sname ; That is what I have so far but I get an error that says it can refer to more than one table.
That would potentially mean hundreds of custom queries, which is precisely what I want to avoid. What I'm thinking is having 1 sproc per calculation, where each calculation's where clause is dynamically built based on the values stored in a parameter table, and in some cases also building the SELECT dyanmically for cases where its an identical calculation but one client requires it to be a datediff() between X and Y, and another requires it to be between X and Z, so something like datediff(@field1, @field2) where the parameters are set by whichever client is going through the loop.
Take a look [here](http://sqlfiddle.com/#!6/279ae9/24). I put comments in the actual code.
That actually sounds very efficient. I have idea though how to go about that. I've seen other 3rd party procs use EXEC @sql to do this, but not sure myself. 
Yes, absolutely agree with that hence why I said that it may have been a stupid question:) Is there a rule of thumb on optimizing queries? What exactly gets optimized? I assume it has some impact on the speed of processing/resources used but I guess this only becomes significant with high number of tables/rows in a db? I should really take a step back and read up more on the fundamentals.
Yeh, I have no idea how to do it either. So it's going to be a learning curve, lol. End goal is to just have to maintain 1 sproc per calculation, and then the parameter tables. Clients can submit their parameters in Excel format, we use SSIS to suck them up before using it to trigger the SPROCS. Automate the whole fucker.
Not for the gold - if your rules are relatively simple, home-grown rule systems (and you're building one of those) are generally sufficient. Once you'll get to an advanced stage or run out of patience to support the zoo (ymmv, but seems inevitable to me), you might want to look into Rete-based rules engines (such as drools). 
Sorry, I guess I wasn't clear. I think you need less than ('&lt;') instead of greater than ('&gt;'). If the work ended on August 31st, then it ended 64 days ago. Think of it (EDIT_DATE) as the 243rd day of the year, and today (SYSDATE) as the 307th day of the year. So EDIT_DATE is 243, and SYSDATE is 307. Is 243 LESS THAN (307 - 30)? Yes. Is 243 LESS THAN (307 - 60)? Yes. Is 243 LESS THAN (307 - 90)? No. ...and so on. So you're really close... you just need to change your "greater thans" to "less thans", like this: SELECT A.PROPOSAL, A.SORT_CODE, A.SHOP, A.ENT_DATE, A.EDIT_DATE, (A.ENT_DATE + 30), (SYSDATE - 180), (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 30)) THEN '1' ELSE '0'END) AS BACKLOG_1, (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 60)) THEN '1' ELSE '0'END) AS BACKLOG_2, (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 90)) THEN '1' ELSE '0'END) AS BACKLOG_3, (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 120)) THEN '1' ELSE '0'END) AS BACKLOG_4, (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 150)) THEN '1' ELSE '0'END) AS BACKLOG_5, (CASE WHEN (A.EDIT_DATE &lt; (SYSDATE - 180)) THEN '1' ELSE '0'END) AS BACKLOG_6 FROM SERVER WHERE (A.ENT_DATE + 30) &lt; A.EDIT_DATE AND A.STATUS_CODE = 'CLOSED' AND A.ENT_DATE &gt; (SYSDATE - 360) AND A.ORDER_TYPE = 'XX' AND A.PROPOSAL = '0000000' ;
Don't tell me what to do, you're not the boss of me.
Is there a time limit?
No time is not a factor in this example
My advice for dynamic SQL is this: CREATE TABLE #Test (a varchar(max)) Declare @SQL varchar(max) Set @SQL = ‘Your dynamic SQL here’ + INSERT INTO #Test SELECT @SQL + ‘End of loop SQL’ This will allow you to see what is being built, and trouble shoot it a load easier than trying to just execute it and read the error message.
So this will store each query iteration of the LOOP as it executes with the dynamic variables?
Well, if you add EXEC(@SQL) to it, yes. I’d start off just writing each iteration to a table, selecting the iterations back, then copy that to a new query Window / further down your current window. From that point, it’s much easier to see where things have gone wrong!
No, I mean like... is your code going to save each iteration of the SQL as it executes (not the results of the query, but the query itself)... or just dump the sample data in a #table? 
I won't lie, I only skimmed the question and I don't claim this is a good idea but..... Assumption - you are just getting back a single result, ie just for client X, its not called on the entire data set. Assumption - a client can only have 1 formula, they can't choose Calcs 1, 2 and 8. create a new table - &lt;clientID, specialFormula&gt; Create Sproc that takes in the clientID. Sproc looks up the clientID from the table and saves it to a variable If @specialFormulaNumber IS NULL THEN -- Default formula ELSE IF @specialFormaulaNumber = 1 THEN -- First one off Else IF -- etc etc END By having null be the default you only maintain special ~~snowflakes~~ clients. Its still clunky because you'll have unique formuals but at least they'll be in one place.
The query itself. 
Hmmm... this might work for the instances where two calcs are basically identical but only look at X to Y, or X to Z. I'd have to maintain two copies of the code as opposed to one that dynamically takes the column names. Not sure how I feel about your suggest. I appreciate your honesty about not reading the question. I am going to open a beer and debate giving you gold. I'm at 49.5/50.5 on the fence right now.
Yep, that's a great idea.
Thanks for the gold dude. You’ll get there in the end!
Oh, I know. My goal tonight is to get as much ideas flowing as possible for tomorrow when I really dive in. I have some primitive ideas that I'm sure will evolve. I started writing it in my head yesterday and "think" I have some ideas worked out, but it's all going to come down to testing so your suggestion is really worth the $3.99, plus I like supporting Reddit.
Yeh, ok, I like your idea. Coding everything to behave as the default unless there is a flag. 
Thats like 5/7 right? A pro of that solution is as long as the answer back to the caller is consistent you can write whatever query you like. IF .. = 1 can can be completely different from IF ... = 2. And when another formula request comes along you can just add a new ELSE IF block, toss the records in to the snowflake table with the new ID and you are good. Pro #2 is that you aren't dealing with dynamic sql - either you have to store how to create the dynamic query somewhere or you are getting a column list and then have to validate there is no injection, bad names etc. A con is that this is very rigid. You can't add more columns on the fly because it could break whatever else is calling it. Another con as I think you brought up is its not very DRY - you'll likely have repeated code in the blocks as I bet they overlap a fair bit.
&gt; Pro #2 is that you aren't dealing with dynamic sql Pretty fucking persuasive pro. I'm currently debating taking your advice, but still using dynamic SQL as well. I won't really know until I'm in the trenches.
Thanks for the gold. So what was the beer that pushed the choice over the edge? Now, because I don't work in your environment I'll make another suggestion you SHOULD (maybe) ignore but, well you've got a beer so what the hell. You could turn this into a user defined function and call it inline. SELECT blah, ......, dbo.udf_SpecialCalc(ClientID) as Uptime, .... From tableWithClients Heres the thing, user defined functions can perform like absolute trash killing query performance. It can make what was once a set based query have to evaluate Row by Row (REBAR). You should avoid functions in WHERE clauses and JOIN clauses, but there you go.
I'm pretty sure the data is still there but I can't access it since the configuration seems to be broken in some way that cannot be identified (by me. Blank pages just don't tell anyrthing and the error logs write nothing new so it's a hidden issue) MySQL workbench won't work (i think?) as I don't know what username &amp; passwort the database uses. The tutorial I used was re-written and no longer includes that part soooo that possibility is gone :/ Other wise I might have been able to do that.
Do you get an error? Try hard-coding a username into the query and see if it returns results... I don't see anything wrong with the query itself (although if you add 4 space to each line of the code, it'll format it nicely like below): $sql = "SELECT * FROM major_req AS mr LEFT JOIN academicrecord AS ar ON ar.CourseID = mr.course_id LEFT JOIN course AS c ON c.CourseID = mr.course_id LEFT JOIN department AS d ON d.DeptID = c.DeptID LEFT JOIN subject AS s ON c.subject_id = s.subject_id WHERE ar.CourseID IS NULL AND ar.UserEmail='".$_SESSION["username"]."'";
It wasn't the beer, I just had a few minutes to step away and actually think about what you were proposing. You mean user defined such as an example where an SSRS report is passing the parameter and letting users pick what they want? Won't work, and yeah that can turn into a disaster... but also works great. For this it isn't feasible, I'm more looking to create a cube that I can plug into Tableau that will let users pick all sorts of variables... so long as the parent process is calculated using their specific exclusions, etc.
I'll help you a little bit - in order find the maker who makes the largest number of products, you need to COUNT the models, grouped by maker.
It's not the default? Have you tried the wikimedia site?
ar.CourseID is your JOIN predicate. So if it's NULL, you won't be returning any data from the academicrecord table. Since your UserEmail is in that table, you won't ever have records from it in a LEFT join. You might be able to do FULL OUTER JOIN, but might also get records returned that you don't want.
For your first question - Since you are working with aggregates, you'll want to have a GROUP BY somewhere in your query. As groupings go, you group by what is the same between them. For example, if I wanted to find how many people have a certain last name, my group by would look like GROUP BY [Table].[Last_Name]. Since the people's last name is the same, everybody with that last name will be grouped together and counted. In case it is helpful, your second question is easier to answer than your first one. Maybe take /u/TWISTYLIKEDAT 's guidance and tackle question 2 as a warm up to question 1.
Could also be COUNT(*) in this case? No idea if different model types would be considered a separate product or not.
Okay, what's the actual request? You seem like a smart individual and just following orders from some marketing drone that has no concept of how technology even works. This may be easier done in some sort of presentation layer like Excel or Tableau instead of the data retrieval layer.
It seems like count is only counting the number of variables not the number of products 
I replied
When you select data from multiple tables you have to create a link between tuples to product a single record. There will be identifying values in each table that determines which record in table1 belongs to which record in table2 and table3. You then use `joins` or `where` clauses to create the data links to return complete records.
Have you tried it?
Have you tried something like googling "sqlite create sequence"?
hard to guess what you're trying to return perhaps you meant to have the UserEmail condition as part of the join? SELECT * FROM major_req AS mr LEFT JOIN academicrecord AS ar ON ar.CourseID = mr.course_id AND ar.UserEmail = '".$_SESSION["username"]."'"; LEFT JOIN course AS c ON c.CourseID = mr.course_id LEFT JOIN department AS d ON d.DeptID = c.DeptID LEFT JOIN subject AS s ON s.subject_id = c.subject_id WHERE ar.CourseID IS NULL 
Yes, with [SSIS](https://www.red-gate.com/simple-talk/sql/ssis/moving-data-from-excel-to-sql-server-10-steps-to-follow/). Or using [OPENROWSET](http://blog.learningtree.com/using-openrowset-to-read-excel-worksheets-from-sql-server-part-2-linked-sql-queries/), or similarly [OPENDATASOURCE](https://docs.microsoft.com/en-us/sql/t-sql/functions/opendatasource-transact-sql), but you'll need to write a query or a procedure that does it. 
If you were to 'Select * from Product Order By Maker, Type', you might see that Maker1 makes three models of PC's, two Laptops, and one Printer for a total of 6 products; Maker 2 makes 2 PC's, 1 laptop and 2 Printers for a total of 5 Products, etc. So your 'Select Count(model) ... Group by Maker' query should give you counts of 6,5,... Mind you, this is just a *little* help. 
You'd have to write some script that watches the folder and executes SQL when it sees a spreadsheet.
I looked up the tutorial again but the changed it and it no longer includes that section. Too bad I guess :/ I was hoping to be able to extract the text directly from some file but it seems databases just don't work like that, ha.
ANSI-SQL is the standard that should be transferable to any variant, so that’s a good foundation. T-SQL is Microsoft’s variant, which is probably the most widely used. PL/SQL is Oracle’s, and is another good option. If you learn one, you’ll be able to quickly pick up the others.
&gt; T-SQL is Microsoft’s variant, which is probably the most widely used nope, they're in 3rd place -- https://db-engines.com/en/ranking/relational+dbms
It is possible to save it as a csv then upload into a table. I use ms-sql server. 
I am comparing the major requirements(courses needed to graduate for a specific major) against the academic record(courses completed) and returning all courses in the major_req not completed yet for a specific user
https://sqlzoo.net/ is what I used to get a handle on it. Learning any one will allow you to move amongst the others (you might be missing features like PIVOT or something like that).
https://sqlzoo.net/ is what I used to get a handle on it. Learning any one will allow you to move amongst the others (you might be missing features like PIVOT or something like that).
That’s bey interesting. I was going on personal experience and nothing more in depth than that. It would be interesting to split this by county and industry.
That ranking is partially based on google trends, website mentions and linkedin profiles, and has the top 3 all within a very close margin, so I wouldn't rely on that as a definitive source.
This only shows the number of installations, not the amount of code written. I would suggest (with only anicdotal evidence) that a significant amount of the MySQL installations are as a backend DB for some appliance type application such as WordPress and that there are many smaller companies doing analytics, reporting, warehousing, etc. on SQL server and that there are more people writing in T-SQL on any given day than other variants. I invite anyone to find evidence in support of or to refute this baseless statement. 
I found this Microsoft course for t-sql rather good. And it's free unless you want to pay for the certificate. https://www.edx.org/course/querying-data-transact-sql-microsoft-dat201x#!
so did my suggested change to the query do that for you? 
yes that works thanks, I didnt know you could use the AND inside a join
[removed]
Tanks for this. Does this course cover Azure SQL? I've found some nuances in azure like cross-database queries that I wanted to learn more about.
I would suggest T-SQL or Postgress. They are both reasonably standard complaint (Postgres sticks very close to the standard, MS as always tries, but with backward compatibility everywhere). MySQL is garbage and will get you into bad habits, and Oracle is a bit too... 'special' at times.
I don't think we're at the point where you can afford to ignore Oracle yet, but we're getting there. Having said that, try to learn Microsoft T-SQL first, and then Oracle PL/SQL. I would recommend PostgreSQL's PL/PGSQL variant as well, but I'm not seeing a ton of job positions in that area yet. I suspect I will see tons of them in the future though.
I believe both Sql Server and Azure and both Microsoft and are t-sql but both have different management suite features (I think?) They concentrate on t-sql in SQL Server but do cover some points which they state are not relevant for Azure (as opposed to the other way round which you want) They use Adventure works sample database which I think you can get for either. It is free, so you can start it to see what it is about. Also they have a discussion for which the lecturers and other Microsoft staff respond to, which might help you. 
People here are saying PL/SQL. That is not SQL, that is the programming language used by oracle that incorporates SQL within it. PL comes from procedural, meaning its a programing language with ifs loops and functions - not a structured query language.
I didn’t know that group_concat was a thing until I read your flair, so thanks for that 
MySQL only, but existed long before other database systems came up with a quote unquote equivalent FOR XML PATH? are you shittin me??
Yes, you could write an OSQL statement that performs a bulk import on the file. https://docs.microsoft.com/en-us/sql/tools/osql-utility
Gotcha, I only have experience with t-sql in MS SQL and not that much at that.
Yeah. I've done it in Excel, and as I noted, I know how to do it in SQL...I just kinda want to do it a different way because I think the approach would be generally valuable. But it seems the effort overwhelms the payoff, so maybe not. Thanks. 
Line 30 looks like you need a `'` before `Department: '`... DBMS_OUTPUT.PUT_LINE('Department: ' || e_DepartmentNumber || ' ' ||
I would also recommend SQL Server Express and TSQL. It's super easy to setup and you can run it in very lightweight environments. I was running SQL Server Express LocalDB with LINQpad on my tabletPC with 2 gigs of ram for example. And you'll find SQL Server is very popular. I just finished a book (T-SQL Fundamentals) on SQL querying and got a certificate from Microsoft's EdX course on it. But if you want to be a developer, you'll need to learn a lot more than just querying. I would recommend reading Itzik-Ben Gan's books to learn these things (like T-SQL programming) after you get a good foundation in querying.
SQL is standardized since 1986. One of the best online course. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
I fixed that, but still got the same error. Unless I made another mistake somewhere else... https://imgur.com/ULwpfOG
Remindme! 2 days
I will be messaging you on [**2017-11-07 00:34:23 UTC**](http://www.wolframalpha.com/input/?i=2017-11-07 00:34:23 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/7atctg/ms_sql_database_mapping/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/7atctg/ms_sql_database_mapping/]%0A%0ARemindMe! 2 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Line 25.
Shit. I'm using Putty, which forces me to retype everything each attempt. I can't go back and edit a line after I hit enter. Will it at least execute correctly once I quit making typos? It's so frustrating because I can't even tell if it will work due to the mistakes I'm making.
Type it all in notepad then copy/paste it into putty lol
I tried that, but it kept giving me all the employees from all the departments, without even prompting for user input
 A B Alpha 100 Beta 100 Charlie 200 Delta 200 Echo 200 You'd have to do a SELECT on just B using the COUNT(B) function and squash it down with GROUP BY. B COUNT 100 2 200 3 Then join this back to a SELECT from the original table using B as a key in both.
Toad has a feature called “Code Map” which I’ve used for similar purposes. It scans the DBA_SOURCE table (Similar to sys.comments) for keywords and builds a dependency map for you. Not sure if it’s available in the SQL Server version of toad though.
Many database systems provide some kind of "audit" functionality. Usually, you can simply select the users and tables that should be audited and then it will log the SQL commands and changes that are made from then on. I'm not sure how this works in MS SQL, though. Alternatively, it sounds like something you can do with TRIGGERs. Create a trigger for inserts, deletes and updates for each table and let those triggers write into a logfile or table.
They both look like they have valid syntax. That said, they are missing primary keys. I also avoid 
https://stackoverflow.com/questions/14232075/how-to-give-input-to-session-of-putty-login-using-cmd
Ew. You definitely don’t have to work that hard to edit your sql - copy and paste it into a file. Put this at the beginning: UNDEFINE in_department ...if you don’t want Oracle to “remember” the user input from the same session. Let us know if it works now.
I'm very new, please no hate. Won't SELECT column A, column B FROM tablename WHERE B='value' ; work? 
That is fantastic info and gives me a good basis to go off of. Thank you!
Would you mind posting the query? I am alittle lost
So I am not a huge fan of how the schema is setup on this database for class. But here is what I have to work with.... Does this look right? ALTER TABLE customer_rentals ADD CONSTRAINT pk_customer_rentals PRIMARY KEY (item_rental_id); ALTER TABLE financial_transactions ADD CONSTRAINT pk_financial_transactions PRIMARY KEY (transaction_id); ALTER TABLE rental_status_codes ADD CONSTRAINT pk_rental_status_codes PRIMARY KEY (rental_status_code); ALTER TABLE transaction_types ADD CONSTRAINT pk_transaction_types PRIMARY KEY (transaction_type_code); ALTER TABLE customers ADD CONSTRAINT pk_customers PRIMARY KEY (customer_id); ALTER TABLE accounts ADD CONSTRAINT pk_accounts PRIMARY KEY (account_id); ALTER TABLE payment_methods ADD CONSTRAINT pk_payment_methods PRIMARY KEY (payment_method_code); I cant really tell from the examples when you add the constraint where you either determine if its a primary key or a foreign key. is the PK_Whatever or FK_whatever the table or the attribute needing modified?
Subquery select a,b from table1 where b in ( select b from table1 group by b having count(*) &gt;1 ) http://sqlfiddle.com/#!9/426548/2
Thank you very much
I looks valid but id add primary keys.. a store ID for each table ect. 
That's pretty much... Everything that could be said about such a vague question. Welll done good sir. 
That would pick all rows where b is the value you put in the WHERE clause. What OP need is when there are multiple rows with the same data. E. G. There are two rows where B=2, three where B=3. If only one where B=4 then do not need to pick that row. 
Couldn't we use SELECT DISTINCT and find all distinct value and use that result set with WHERE NOT B='value' ; ? 
Since this is MS SQL you can simply create a database trigger (as opposed to a table trigger). It's actually very simple, Google it
Why not just use [Oracle SQL Developer](http://www.oracle.com/technetwork/developer-tools/sql-developer/overview/index.html) instead of PuTTY?
Solar Winds and SQL Sentry both make pretty good tools. They're not free but they're not crazy expensive. If you want free then as others have said just build triggers. Be aware that the more judicious you are with your triggers you run the risk of losing database performance.
If you don't include primary keys and you decide to scan your VARCHAR fields, you run the risk of a full table scan. That can really affect your performance. Either put a clustered index on there and include it in every query, or just put a primary key on.
Yes it's possible, and fairly easy. SSIS is going to do this more easily, but as u/grizzlyhamster/ suggested you can also do this via syntax. You should talk to a data architect about this before you do it. If you guys are small and you don't have a data architect, then talk to the business unit and your network/systems/security folks. Some points to consider. The folder containing the file needs to be secured so other scripts can't accidentally access it. The process needs to make sure that only one valid file at a time is in the source folder. The process should probably archive older versions of the Excel file so you'll need to include some kind of script to move and rename the file with date/time at the end. Create a shitload of error handling and test for all. What if the file doesn't exist? What if the file name is wrong? What if the file accidentally has some kind of read only setting on it? What if a new column is added? What if a column is missing? What if the values in a column are wrong (text in a date or int field, etc)? Are any fields not allowed to be NULL? Do you need to convert empty fields to a NULL or to some other default value (replace blank int values with zero, blank dates with today's date, etc)? Doing that last step will make troubleshooting easier when it fails. And believe me it's gonna fail. Several times a year, always at an inconvenient time.
So I'm looking at your original thread and I wonder if you're solving the wrong problem. Is your Tableau data source a live connection? If so, is it reading from either transactional SQL data or real time replicated data? Is it possible to create a Tableau data extract? The data is fixed. It only updates on a schedule. And the extraction can be pretty slow depending on the size of source data. But once it's extracted, you can just create a calculated field in Tableau. Or hell, hundreds of them. In fact, learn how to make them, teach your reporting team, and tell them to do it themselves. This way you avoid the social/psychological side effect of accidentally training the reporting team to think about the way the data is constructed, in a manner different from how it's designed. Plus, you avoid having to avoid supporting a bunch of red code. Dynamic SQL is cool but it's a pain in the ass to maintain. *** Please don't bother gilding me. I've got like a years worth because I tell stupid jokes on AskReddit. if this post helps, donate my gold to someone else. The Lounge is awesome, find someone who will contribute.
Yes. This is the main reason I don’t do DB triggers/audits. The performance aspect. I think they touched on it in this episode of office hours: https://www.brentozar.com/archive/2017/10/video-office-hours-20171011-with-transcriptions/ 
No, it is an extract. &gt;But once it's extracted, you can just create a calculated field in Tableau. Or hell, hundreds of them. Hard no. Not a good way to design your reporting interface. We currently have dozens of workbooks with same/similar calculations and are working to standardize these and migrate all of the real work to SQL. Tableau is literally not capable of calculating the things I am calculating with this project. &gt;Dynamic SQL is cool but it's a pain in the ass to maintain. Not nearly as much of a pain in the ass as Tableau is. 
This is more of a lab thing, not prod. Performance isn't important to me, or for this. probably should have specified that :)
Thanks everyone for the replies. I'll look in to DB triggers and see what I can do.
[removed]
Ahh, then go nuts! Also you may want to search for a tool that helps verify dependencies. That might help get more results.
For resources, I'd look at the following articles: Transaction and error handling, specifically to use as a refresher and educator for transactions and how SQL Server handles them. http://www.sommarskog.se/error_handling/Part1.html The same guy again, but now on Dynamic SQL. http://www.sommarskog.se/dynamic_sql.html
&gt;Transaction and error handling, specifically to use as a refresher and educator for transactions and how SQL Server handles them. LOL, do I look like a DBA to you? I'm just planning on deploying this and letting them fix the things that break. That's a corporate best practice, right?
I would argue that transaction knowledge can carry across multiple IT disciplines, especially for anyone who develops in SQL. It pertains to the ACID principles, so it goes well with architecture or engineering processes. If you needed to maintain things in your database or see how things are performing, it plays a role there too. 
&gt;ACID principles Can't hear you over my GIGO chants. I'm self righteous enough to blame others for errors. But no seriously... I don't really touch things like transactions. I only touch the data after that level of development. My code is more aggregating it, modeling it, etc. From that perspective the concept of a transaction is irrelevant, and there, "should," never be an error in the first place.
&gt; Then use IF statements and have custom queries per ClientID. &gt; That would potentially mean hundreds of custom queries Not if the bulk of the queries are defined by variables at the beginning and then the If statement passes arguments creating the where clause to execute the sql. I like the idea you had about the parameter table and 1 sproc per calculation. One thing to throw out there if you can do this in SSIS, SSIS has the ability to handle dynamic things exceptionally well with expressions. It's easier, fast, and efficient. 
&gt; SSIS has the ability to handle dynamic things exceptionally well with expressions. It's easier, fast, and efficient. Can you elaborate? I am a novice with SSIS. I know how to use it put a SQL package in, but that's about it... I don't really take advantage of any of it's native functions, just package my SQL and call it a day. It seems like you're saying here I could have better performance building this in SSIS and having it handle the parameters than if I just did everything in SQL first and packaged it?
As always, it depends. One random but cool idea, is to store the text of how you want the SQL to look like. Then you could chop that long script up into "programmable" columns. So perhaps your select is the same for everyone except one company, you could put their last column in a separate column, with the rest of the select clause in the first column. SSIS can put text together from the columns, so it could build that select statement with or without that last column, and then store it as a variable, which can then be executed. That would essentially do all of the dynamic SQL work in expressions. Or you could have SSIS just manage the execution of the procs against the parameter table and put the query together from those outputs and execute it. Expressions are pretty much black magic in SSIS. I love them. Just make sure not to make it into spaghetti code... then it's an unbelievable nightmare. 
You might be flying a bit over my head. Each SELECT is identical in terms of schema, in as much as the "housing" table that I've built receives all of the data from each calculation. Values that might not exist in one query are SELECT NULL AS to accommodate the schema. The variance I have is between different date values in the SELECT, so it's like `SELECT datediff(col1, col2) as 'Field'` vs `SELECT datediff(col1, col3) as 'Field'` between different clients, and then in the WHERE it might be something like `where col1 not in (var1, var2, var3)` versus where col2 not in (var1, var4, var9) and col1 not in (var2, var6)` So the final output is coded to be identical, and the calculations themselves are identical, they just need to dynamically pick field1 for clientA, etc.
Works for me. Your code is right, maybe refresh the page. 
I'm a college student, so it's just what they have us use write SQL code in. They also use it to work in the Linux OS.
SSIS receives an ID input for the Client. &gt; Client_ID Int Unique Primary Key - Identifier for clients. You can either store the next part as a variable built into SSIS or you can store it in a table and query it. My example assumes you are querying it. SSIS calls a stored procedure that returns the base query from a parameter table. SELECT D_BaseSQL from Parameter_Table WHERE Clause_ID = 1 &gt; D_BaseSQL Varchar(Max) - Contains the primary SQL to be used. Ex: SELECT Column_A ,Column_B SSIS will continually execute procedures until all of the clauses are obtained. SELECT D_BaseSQL from Parameter_Table WHERE Clause_ID = 2 **Return** FROM Example_Table SSIS will be storing the results returned from each select that had occurred. BaseSQL1 = (SELECT D_BaseSQL from Parameter_Table WHERE Clause_ID = 1) BaseSQL2 = (SELECT D_BaseSQL from Parameter_Table WHERE Clause_ID = 2) ... Now to fill in the gaps where it changes by client. You will need to query the Client_Rules table, here's a rough idea I had for the table. Clause_ID | Client_ID | Key_Rank | Clause ---------|---------|--------|------ 1 | 1 | 1 | ,DATEDIFF(Col1,Col2) 2 | 2 | 1 | ,DATEDIFF(Col3,Col4) 3 | 1 | 2 | WHERE Col1 NOT IN(Col3,Col4) 4 | 2 | 2 | WHERE Col3 NOT IN(Col1) Your next expressions would look like: VarSQL1 = (SELECT Clause FROM Client_Rules WHERE Key_Rank = 1 and Client_ID = (Passed from expression)) VarSQL2 = (SELECT Clause FROM Client_Rules WHERE Key_Rank = 2 and Client_ID = (Passed from expression)) The Key_Rank changes each time. You would have SSIS concatenate those expressions so it would look like this: BaseSQL1 + VarSQL1 + BaseSQL2 + VarSQL2 &gt; -- This is BaseSQL1 SELECT Column_A ,Column_B &gt; -- This is VarSQL1 ,DATEDIFF(Col1,Col2) &gt; -- This is BaseSQL2 FROM Example_Table &gt; -- This is VarSQL2 WHERE Col1 NOT IN(Col3,Col4) SELECT Column_A ,Column_B ,DATEDIFF(Col1,Col2) FROM Example_Table WHERE Col1 NOT IN(Col3,Col4) SSIS can then execute that as a T-SQL command from there. No idea how good the idea is, just wanted to try to give a detailed answer on how to use SSIS Expressions on that as an example.
It's interesting. I work with some SSIS wizards that I'm going to discuss with. My own personal thoughts (since this is my deliverable) is to provide it in straight SQL and then if/when it becomes necessary to reimagine in SSIS have them do it. At the moment the execution times are so low that I don't know if my group would want to give ownership to the other group in terms of being able to make modifications without having to open tickets, etc. Your general example of the rule table is exactly how I'm proceeding currently.
It says "Wrong answer. Too many rows." :(
Ok I solved it by manipulating the query. Final answer was: Select name, continent from world x where population/3 &gt; ALL (SELECT population FROM world y WHERE y.continent=x.continent and y.name&lt;&gt;x.name)
I don't get it, your first one works. https://imgur.com/a/p8ZKq Your second one doesn't https://imgur.com/a/IITwn 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/LL4FOiB.png** **https://i.imgur.com/LzGpEJV.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dpeetvk) 
 SELECT Users.* FROM User_Addresses INNER JOIN Users ON Users.ID = User_Addresses.ID WHERE User_Addresses.zipcode = '90210' 
SELECT DISTINCT user.id, user.name, user_addresses.zip FROM user INNER JOIN user_addresses on user_addresses.zip = user.zip WHERE user_addresses.zip = 90210 GROUP BY user.ID
Crap!!! Sorry I meant number 10 :x
Thank you! This worked out perfectly.
I'm not a SQL expert but from my research I think you are right! Thanks for the input.
I'm pretty sure you can pull this off with a windowing/analytic function but the one you need escapes me at the moment.
DENSE_RANK with a Partition By the week? 
You're still thinking like a programmer. SQL works best when doing things in sets. What you want to do is pull your date into the select and group by it. I know you want by week, so you'll have to play with tree datepart function to get what you want for weeks. In the past, I have used a CTE with the modulo (%) to get weeks to line up with a business calendar. Hope this helps. 
Access is often used for this purpose. It's also very limited and potentially not worth learning. I would recommend SSMS and a bit of SQL though. It's free. If you want a "no code" solution (would not go there but it's your funeral) then we would probably need to know more about your environment and requirements.
you *can* do a while loop with a parameter and increment the parameter up by week if you really want a For loop type of logic; I think you could accomplish this with better performance using a recursive CTE though: https://technet.microsoft.com/en-us/library/ms186243(v=sql.105).aspx
Thanks for your quick response . I want no code solution. The database is stored in a server. I did set up some report with ssrs for my coworker to revive information (read only).I want my coworker (no coding experience) will able to update the table like Excel spreadsheet with limited function such as he can't create a new column or change the data type of the data.
Yep, my rule is, if you're doing a loop in sql, you're doing it wrong. I'm sure there are occasions where they're necessary, but I'd say in the last 10 years I've probably never done one, and rewritten ones done by others to improve performance significantly. Database engines are not optimized for loops like that. 
Those types of things can be limited by security constraints. The types of operations your user can perform is somewhat unrelated to the software they use to perform those actions. But Access should solve your issue. You can set up an ODBC connection to the SQL database. Access's default UI is very Excel-like, but it can also be customized. Pretty straightforward if you are just looking for a way to allow a user to update data.
Check out row_number() if you combine that with count and vendor state, it should just take you a single join and a single subquery to find the top performers in each state.
The only reason I use loops are for batch processing. That way i can commit after each iteration and not blow up the transaction log. Other than that, loops aren't good for much. 
Thanks! I heard of Rank() as well, but if possible I'd love to keep this to the material the professor has taught. I'm trying to figure out how he wanted us to do it.
The other issue is we don't have access. Will ssms do the same lol?
You can update tables with SSMS.
An alternative to Ssms (which would probably be the right way to do it via a stored procedure), would be to just gold the data in a csv file and import that file each night via SSIS. That is a fairly common solution for master data although not the prettiest. A similar solution can be made with sharepoint lists and flow. 
I'm on mobile, but try this. Get a count for each vendorid and state. Now, figure out the max for each state based on those counts. Join the results back to your vendorid, state, count dataset on state, max(count). Join that back to your vendors table to get your vendor names.
But it seems gave him too much power to the database. Can I limited to the table access only?
Yes... 
Are you allowed to use windows functions? I feel like this would probably work: &gt; "Which vendors have the most invoices in their state?" Select VendorNames, VendorStates, Count(Invoices.VendorID) OVER (Partition by VendorStates) From VendorNames inner join Invoices ON VendorNames.VendorIDs = Invoices.VendorIDs
This works! What I had to add was joining on **both** State and count = max(count). Thanks! Just curious though, any reason why joining on just the count wouldn't work? I guess that would allow for same counts on different states, and that wouldn't be good...
Yeap, that. :)
Depends on the RDBMS, but one example is SQL Server 2008, some of the aggregate queries don't scale well and a loop would be better, but windows functions can negate that case scenario now. Two examples that are still current, maintenance and dynamic SQL. If you need to do index maintenance, cursors / loops are good there. If you need to do dynamic SQL (see index maintenance possibly) this is another good use case. It's all niche, best tool for the job. Definitely best to think in sets 99% of the time.
can confirm... now 30 years writing SQL, never needed a loop calendar tables, on the other hand...
 SELECT TIMESTAMP ,* FROM TABLE1
I tried that. Just tells me that I have a missing expression. 
you have the proper aliases if you are joining?
I am not using any aliases. I am just trying to look at the entire table but would prefer to have the time stamp as the first column
Since I'm not allowed to just create objects, I did all my calendar needs in a cte.
you could select it in the navigator/object explorer, and just move the timestamp to the front.
Just tried with an alias and it worked. Thanks for the help! :)
Added an alias and it worked. All set now! :D
glad to help
Thank you! I apologize for not replying sooner, this is all done from a work computer and I was off over the weekend. How would I handle UNIQUE_IDs with multiple uploads. That's what I was attempting to do in my code. Thank you again. 
Use JOIN ... ON ..., not JOIN ... WHERE.
NEWID() [generates a universally unique ID](https://docs.microsoft.com/en-us/sql/t-sql/functions/newid-transact-sql). Each ID would be unique. Is that what you're asking about?
sorry I should have prefaced I'm not using traditional sql but rather CCL. They are essentially the same with minor differences. I was just asking which is more efficient.
in SQL server I would choose the table with the best index on ORDER_ID for my join conditions. So choose the table where it's the clustered index.
I apologize, I don't think I made it clear in my original post. I already have a table called FORMS, within this table a have a multiple fields including UPLOAD_FILE(text) and UNIQUE_ID(int). UNIQUE_ID is the primary key, starts at 1 and auto-increments at every entry. Every time someone creates a new form a UNIQUE_ID is created and all the files uploaded are stored in the UPLOAD_FILE field. As you can see in the string above, or below I'll post it again, 2 different files are uploaded to this particular UNIQUE_ID. We can make UNIQUE_ID = 12345 for clarification. How would I ago about inserting that UNIQUE_ID into the name of every upload file for that UNIQUE_ID? [{"name":"C:\\Users\\username\\Documents\\Uploads/abcd_udod4v5p.jpg","usrName":"abcd.jpg","size":1013329,"type":"image/jpeg","searchStr":"abcd.jpg,!12345.txt,!:sStrEnd"},{"name":"C:\\Users\\username\\Documents\\Uploads/12345_o6sq520f.txt","usrName":"12345.txt","size":288,"type":"text/plain"}] 
I use the first version when it's an "inner join" chain so that it's easier to keep track of what's being included by making the joins reference the fewest number of tables. But your DB engine should optimize them to the same query plan.
Depends on the RDBMS I think. In most SQL, join is a synonym of inner join, in which case, the query optimizer should recognize that those are equivalent queries and it shouldn't matter. All that varies is readability (see other commenter's JOIN ON comment). If join implies left join, then I'm honestly not sure if they have exactly equal output. I would suspect not. Also, if your RDBMS's query optimizer isn't very mature (e.g. you aren't using oracle, ms, my SQL, postgres etc) then it might not be treated as the same thing. Tl;dr I don't know enough to comment on your specific RDBMS, but consider A)readability B)left join vs inner join C) query optimizer. 
It sounds like what they're actually asking for is a data analyst who happens to have finance background. It never hurts to commit to the interview process, but if they're stressing SQL, you have ALOT to learn and it cannot be done quickly. It also highly depends on the level of SQL experience they're expecting. 
Duplicates will still go into that list. Let's say column B has values of 'A','B','B','C'. "SELECT DISTINCT ColB FROM Table" will pull 'A','B','C'. 
I think may have answered it myself. See my crudely drawn illustration.
True. The position is probably a long shot but I'm just trying to position myself for the interview as best as I can. I intend to keep working on understanding SQL in the coming months. From what I've gathered, the position is geared towards financial analysis as you're still supporting other teams but it also spends time working with big data and having the ability to write your own SQL queries is a beneficial skill to have. I could probably rephrase my question above to, "What kind of SQL questions can one expect in a 45 minute interview?" The position start date wouldn't be until after the New Year so I feel that gives me time to get a better understanding. Getting through this interview is my goal. 
The problem is that you are entering data into your table. For the most part, when we are manipulating tables -**the data that we are manipulating is already put into the system**. It's relatively easy to think of a table solution for data that already exists and relatively difficult to insert new data into a system without using "tricks" like loops and such. Anyways, I would do it with a staging table. I've created an example that *you should be able to modify* to run your command on so that you can get your result. IF OBJECT_ID('tempdb..#Revenue_by_date_Temp') IS NOT NULL DROP TABLE #Revenue_by_date_Temp CREATE TABLE #Revenue_by_date_Temp ( [ID_column] [int] IDENTITY(1,1) NOT NULL, [Start_Date] date NULL, [End_Date] date NULL, [Revenue_Total] money NULL PRIMARY KEY CLUSTERED ( [ID_column] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] declare @date_val as date set @date_val = '01-02-2016' While @date_val &lt; =cast(GETDATE() as date) BEGIN INSERT INTO #Revenue_by_date_Temp ([Start_Date], [End_Date]) SELECT @date_val, CAST(dateadd(dd, 6, @date_val) as DATE) set @date_val = CAST(dateadd(dd, 7, @date_val) as DATE) END SELECT * from #Revenue_by_date_Temp 
If you add two rows (2 files) to the FORMS table, and they have the same value in the UNIQUE_ID column, then it cannot be the primary key, by definition.
ANSI 89 joins for life. Haters gonna hate. 
you should be doing all your db modifications using a tool like flyway or liquibase. then you can run a command and it shows you the changes
I've not run across what you're looking for. In my humble opinion, there's little need to have SSMS on your core SQL infrastructure. The DAC can be used for major issues, and if you have a network down scenario, how are you getting onto the server to launch SSMS? As a DBA, I'm pretty fluent in using Powershell and SQLCMD to do support work if needed. With Microsoft separating SSMS from the SQL Install media, I think it's a sign of the times, if you want to install SSMS on a server, you're going to have to do a perform a second installation.
You are correct. They are not two rows with the same UNIQUE_ID, they are 2+ files that were uploaded in the same form and therefore share the same UNIQUE_ID. Yes, I would like to rename all the files within SQL. I think that's what your code did, except it did it for when only one file is uploaded per UNIQUE_ID. What I attempted to do in my code was to find how many different files were uploaded by finding the number of "{." Once that was done, for each substring between "{" and "}" I replaced anything between "/" and "_" with that UNIQUE_ID. 
You're reading my mind. I swear I've read it a hundred times you shouldn't install management studios on your core SQL Server, but I can't for the life of me find those articles. 
&gt; you shouldn't install management studios on your core SQL Server If you're running Windows Server Core, you *can't* install SSMS as there's no GUI in the first place.
Security is not constrained by the choice of client but rather the permissions granted to the user account. IOW, regardless of how you connect to the SQL Server instance, you'll have the same privileges.
I'd say the easy ones do reflect SOME real problems. But with tech companies the issue becomes that you can be dealing with HUGE amounts of data. With that said, you'll need to think about ways to filter data down. I would look up WHERE and HAVING examples to help build filtering skills. This will come in handy in technical interviews where they ask you for just data on a particular day or region. The other thing you want to stress is joining data. Even if it's just to replace the id of something, you're gonna run into it a lot. Understand joins and how you can apply them with in more complicated scenarios - like having to aggregate filtered queries from two databases. 
I’ve run into issues creating and maintaining maintenance plans on ssms installations that weren’t the exact version on the server itself, having ssms on the server eliminated this problem by always having a way to do this.
Here's my approach to problems like this: 1. Get a table filled with integers. I have one with a range from 0 to 10000 - you can always add more. It's useful to have a table like that in your database. (You can [fill it with crossjoins](https://stackoverflow.com/questions/21425546/how-to-generate-a-range-of-numbers-between-two-numbers-in-sql-server), or a while loop) 2. Get the difference in weeks between those two dates. 3. Select ints from 0 up to that number (so from 0 up to 53 in your case) 4. Use DATEADD to gate the ranges week ranges from using those values. 5. Left join with your orders table. The order date should be between those two dates. 6. Group it using those weeks. You should get something like this: DECLARE @dateFrom datetime = '2014-1-9', @dateTo datetime = '2017-1-9' SELECT w.weekNo, w.weekStart, w.weekEnd, SUM(subtotal - discount + additionalcost) amountSum FROM ( SELECT val + 1 weekNo, DATEADD(week, val, @dateFrom) weekStart, DATEADD(week, val + 1, @dateFrom) weekEnd FROM [Helper].[Ints] i WHERE val &lt; DATEDIFF(week, @dateFrom, @dateTo) ) w LEFT JOIN [dbo].[Orders ] p ON p.[date] &gt;= w.weekStart AND p.[date] &lt; w.weekEnd WHERE datecanceled is null and ... GROUP BY w.weekNo, w.weekStart, w.weekEnd ORDER BY w.weekNo The last weekEnd date will go above the range, so you can adjust it with a CASE 
Not sure I'm completely clear on what you're asking. Do you mean: SELECT min(BusinessEntityID) OVER (PARTITION BY PostalCode ORDER BY SalesYTD DESC) You can use the over clause with an aggregate function without the whole query being grouped.
If Hitler was an SQL developer he would have used ANSI 89 joins.
Good to know. Will keep hammering away at it then. Do you have any sources/practice problems that you would personally recommend? I've tried downloading the Amazon databases I've seen recommended on here but I find myself spinning my wheels thinking of challenging queries. 
True, I meant that perhaps less literally which is dumb because of the ambiguity. Re-phrasing: &gt;you shouldn't install management studios on your production SQL Server servers.
That's what I needed! Thanks!
Ok. Create some bullshit data. Literally. Sometimes you will have to write a query in SSRS and you don’t want that query hitting the entire table. You want it to create a table from sales with only January’s data. Then understand joins. This SUCKS to do it this way but it’s the only way I learned. Recreate a report that you normally would run at the office but with bullshit data. Take a CSV file and load it. Learn the date formats OMG they fucking suck. 
I would however argue that you should have an admin jumpbox with ssms installed should you need to connect remotely. Running ssms over a vpn sucks.
I work with databases daily and I hope to never again touch multi-table joins. It sucks because I guess that means staying away from deep data analysis jobs, but there are lots of other career paths with databases.
I don’t know about that. I’ve worked with operations analysts and because they don’t know what the accounting side means, they don’t know what the numbers they are analyzing mean. Two stories. 1) Working with ops analyst and his numbers are CRAZY high in these two accounts for this little tiny office that is mostly an allocation cost center but there are two accounts that aren’t allocated. District office expense and salaries and burdens. DO expense is supposed to be the salary of one person and salaries and burdens are for two admins. $2.5mm a year. WTF? How? I asked him and he said, “Accounting told me to ignore those numbers.” ... wut... Those are the FIRST things I’d look at then. And they were the highest two spend accounts in his entire report. Completely ignored. They had office lunch every day. There were iPads, Christmas parties and none of it was in G&amp;A. 2) Another allocation cost center but it had every other asset cost center in it. Why? Answer was that is was for incidentals that could be used anywhere, small inventory stuff like bug spray. Really? Can of bug spray is gonna be used from California to Florida? Well it’s a penny per month most of the times so NO ONE is looking at it. I start looking at the months where it’s $2 a month. What’s the difference? Arguably those items are large enough to make it to inventory right? Turns out the guys in the field were using the cement vendors to pour them pools and driveways and using that allocation to spread out the cost because it was immaterial. 
This guy accounts. Not sure if I miss it...
&gt; I hope to never again touch multi-table joins. And I'm over here banging out sql on 20 tables in my queries because Oracle normalized the fuck out of their ERP product. There's something like 4000 tables.
The board BARELY looks at costs. They look at G&amp;A first, expense second, and capital third. So people will try to hide G&amp;A in expense and they will try to capitalize expenses any chance they get. 
I don't think you'll find a lot of things online for those more challenging queries... intermediate anything is always sparse with info. I did find this link though, it'll help test your skill on those filtering operators: https://sqlbolt.com/lesson/select_queries_with_constraints 
Haven't had a problem with vpn... and I connect to severs globally. My issue with VPN is that ours blocks udp 1434 so named instances are a pain. Never issue a backup command via TSql through SSMS on a remote server, the data flows through the SSMS connection.
I also like to put the first table's column to be joined first in the join clause. Example: SELECT * FROM U JOIN C ON U.ORDER_ID = C.ORDER_ID JOIN MT ON U.ORDER_ID = MT.ORDER_ID //I would prefer to use U in the last join instead of C since U.ORDER_ID is the key I joining the other two tables with.
Ah alright. Thanks guys! 
Not sure why they would include a where statement but they are trying to dump user information.
My brother.
Pretty sure that's part of the exploit. Similar one here: https://securitytracker.com/id/1009851
Is the pn_uid part a php thing or an SQL thing?
It's a postnuke thing.
And the md_users?
That's exactly what I was thinking today too! I know it's best practice to not have it installed and I know I've read it before, but I can't find anything that actually states it now! 
&gt; With Microsoft separating SSMS from the SQL Install media, I think it's a sign of the times, if you want to install SSMS on a server, you're going to have to do a perform a second installation. I think that in itself is a very strong indicator as you said, along with the fact that SQL was one of the first apps MS made compatible with Server Core, but getting others to think that way is proving to be a challenge.
It'd be a table in the database they're targeting. This exploit seems to have knowledge of the database schema. I'm not sure why they're selecting all 0's—this wouldn't return any useful information—but it could be that they're probing to see how many columns the source query is (the UNION ALL will only be successful if the subquery you're UNIONing against has the same number of columns).
&gt; I hope to never again touch multi-table joins That's a pretty fundamental use case of SQL. There's no way you could say you had even basic understanding of SQL without being comfortable with joining multiple tables, much less have an entire SQL career path.
I believe this is centered around the UNION ALL statement. This will return all the column names of the user table, thus revealing more table structure. This could also give away certain naming schemas that are used in addition to possible primary keys (compound or surrogate) and foreign keys. In reality, a hacker may not know how many columns are in a specific table, so that's why they have so many 0 listed -- to select the same amount of columns required by the UNION ALL syntax. This would be a trial and error process usually.
Probably stored in the session variable. It's been a long time since I wrote php but they are basically hitting a page and giving it a querystring variable expecting it to return results. pn_uid is probably primary key, it may have a user account in the database matching the service account. It may also have the same password for the machine account as the app account. It's probably looking for some known exploit in this module. The module may not even exist. It may only work if the module is in debug mode. A lot of "hacking" is casting a large net and seeing if anything responds. It's not like the movies where you have a specific target and it goes off like Oceans Eleven. This attempt at an exploit may have completely failed. You can reattempt it using postman and see what your results are.
I work with SSIS on a somewhat daily basis in mssql. I don't know if you'll actually find the answers you're looking for unless we have some people I'd classify as data architects lurking this subreddit. Those people would be looking at efficiency in ETL for fast transactions that would supply up to date data on demand and would likely know the efficiency of each platform. Most data warehousing or integration across platforms that I've worked with has all been reporting related or for business application needs that don't really demand second to second transactions and most times are efficient enough with nightly or bi-daily transactions. Depending on the use case any of the platforms can be used as a base, it's genuinely just about the house preference or developer preference. If you're in a primarily windows environment with biztalk capabilities, you're likely on a Microsoft platform. Where if MQ is your file workhorse you're probably more of a Linux/oracle house.
Know your left joins and group by’s. I’m in a similar role at a smaller company and that’s the majority of what I use. I’m sure there’s other stuff you’ll need to know, but you just need to pass the smell test to get your foot in the door. 
I've worked mostly on SSIS with MSSQL but have pulled info from Oracle with SSIS. I usually connect to MSSQL through an ODBC connection, so I'm guessing whatever I've done would parallel fairly closely to if I was putting my data into Oracle vs MSSQL. When I did pull from Oracle, it was no different (in terms of SSIS operations) than working with another database. "The Enterprise" department used Informatica to do their ETL into and out of Oracle. They supported a few hundred processes daily.
Select SupplyCode, Description From Supply Where SupplyCode Not In (Select SupplyCode From Group By SupplyCode)
&gt; I need to find out: &gt;&gt; Select the supply code and description of the supplies that have never been used on a job. &gt; I have (so far): &gt;&gt; SELECT Supply.SupplyCode, Description from Supply right outer join JobSupply &gt;&gt; ON Supply.SupplyCode = JobSupply.SupplyCode &gt;&gt; WHERE JobSupply.SupplyCode IS NULL You're close. You just need to swich from `right outer` to `left outer`. `A left outer join B` means "Return all rows in A, even if they don't match to B", or "return everything on the *left*". `A right outer join B` means "Return all rows in B, even if they don't match to A", or "return everything on the *right*". [This chart](https://imgur.com/a/vUill) might help. See the two diagrams on the left.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/hlp2gQJ.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
&gt;&gt; Select the first name, last name of the staff member who has taken the most number of training courses. &gt; I have: &gt;&gt; SELECT FirstName, LastName FROM Staff &gt;&gt;GROUP BY FirstName, LastName &gt;&gt;HAVING MAX(TrainingCredits) &gt;=ALL(Select SUM(count(TrainingCredits)) from Training) &gt; Again obviously wrong as I should only have one row of results. No idea what I'm doing here other than knowing I need to find the max of a sum of a count, and that i need to use a subquery or several to find it. But nothing else about the syntax structure for this makes any sense to me. Let's start with that subquery: `Select SUM(count(TrainingCredits)) from Training` first, you're aggregating `count(Training.TrainingCredits)`. So if there are 10 records on the `Training` table, you're going to get 10 here. `sum(10)` is... just 10. So the subquery is returning only one value. The `ALL` isn't doing anything. In the main query, the `Staff` table should only have one record per staff member. So grouping by FirstName, LastName at best will do nothing. At worst, it will combine two staff members who happen to share the same name. Instead I'd break this into two parts. 1. How many TrainingCredits does the staff member with the most TrainingCredits have? 2. Who has the number of TrainingCredits returned by part 1? Let's say the answer to #1 is 20 credits. Can you find the staff member with 20 credits? Then, just replace the hardcoded "20" with a subquery for #1. All you should need is the Staff table. 
From what I've seen, the methods are fairly similar between ETL tools just the implementation differs. You have some mechanism to select data out of a source, another to modify that data (usually in memory), then drop that data to a destination. I'm more curious why you are asking about the "distinct differences" as I think that's incredibly vague and unlikely to get an actual, helpful, response.
Join instructor to courses via teaches. In your query group by the instructor, and count the courses.
[removed]
SQL can be times challenging to learn but it just starts to click after a while. All your challenges so far seem to mainly syntax errors. You'll improve with time, but honestly it seems you just able to google accurately enough yet. I don't write SQL much anymore but I've written enough that a google search gives me the answer I'm looking for straight away. I didn't read through all the comments, but from what I read there where plenty of users willing to help. If you get stuck again, this sub is pretty useful! 
Select Distinct Teachers, Count (Distinct Courses) From Teachers as T Inner Join Courses as C on C.key = T.key Group By Teachers Order By Count (Distinct Courses) DESC
How do you write outer joins? Do you use the (+) notation, or explicitly define your left joins? I've been in shops with a mashup of both, a la: SELECT t1.ID, t2.Name, t3.Address FROM table1 t1, table2 t2 LEFT JOIN table3 t3 ON t3.t2_id = t2.id WHERE t1.id = t2.t1_id; Either way, you should definitely adopt ANSI-92, lest your children be born with tails. 
&gt;Hard work has its rewards! On the last day of each month the staff member who worked on the job with the highest individual sale total in that month wins a $25 Tim Hortons gift card! Select the first name and the last name (one column) of that staff member. Like fucking what? I can't even begin to think of how to do this. If you can't begin to think about this you're going to have a really tough time doing this for a career. Real talk. Start by taking out all the unnecessary information and break it down to what you are looking for. This statement can be rewritten without all the fluff language: "Select the *firstname* and *lastname* column from the *staff* table" (you can surely do this select) "Select the *max* *total* from the *job* table" Then you can work backwards. Find the max total from the job table. This will give you the staff ID. Using the staff ID, select the first and last name from the staff table. Now join the two queries together on the staff id column.
from the article -- "You might want to read that double negative again – it is ugly English, but good logic." ;o)
Yeah. I read that part many times but I can’t wrap my head around it when translating it to SQL on paper or in my head.. if you get it I’d love to hear your explanation.
 SELECT DISTINCT pilot_name FROM PilotSkills AS PS1 WHERE NOT EXISTS (SELECT * FROM Hangar WHERE NOT EXISTS (SELECT * FROM PilotSkills AS PS2 WHERE (PS1.pilot_name = PS2.pilot_name) AND (PS2.plane_name = Hangar.plane_name))); “There ain’t no planes in this hangar that I can’t fly!” Exactly as it says, ugly English, but good logic. * Innermost query checks for the existence of any records in the pilotskills table matching the pilot_name and plane_name * Middle query checks for records in the hanger that *don't* meet the above criteria * Outermost query returns those pilots that have *no* planes in the hanger that they can't fly 
&gt; When I use right joins it's usually because I'm lazy and don't want to retype my tables to be swapped from the left join I made the mix up on. Fairly certain this is the case for a majority of right joins :)
I'm not 100% sure about the internals of an Oracle database, whether or not it mandates a table prefix, but it does sounds kind of weird indeed. Maybe it has something to do that you alias the table? (serviceplans p) Anyway, normally it would only make a difference if you select from multiple tables. So if you'd do Select * from A Join B where a.id = b.id You would get the results of both A and B in 1 joined row Select A.* from A Join B where a.id = b.id You would get the results of just A
On the second one focus on writing a query from the StaffTraining table first. Make sure you're getting the StaffID with the most TrainingID's before you worry about joining to the table with FirstName and Last name. The key is in breaking up the problem into smaller steps. To find StaffID with most TrainingID's you could use "Top 1", "Count(TrainingID)" (leave out distinct), Group By StaffID, Order By Count(TraininID) DESC. Those are the elements I would use, but it's probably a good idea for you to think through them so when they test you, you'll be able to figure it out.
The mandating of a table prefix is interesting, because if I do a select * on the table, it works fine, no alias needed. There's something about the (PlanPrice + 1) that mandates a table prefix. I get what you're saying about the join. Select * would be the same as select A.*, B.*. But since you specify A.* only, you'd just get the results of A.
Did you only include a portion of your query? Something is not adding up.
What did you feel like was not adding up? Strictly speaking, no I did not include the full query, but the only addition is an IN statement. select p.*, (PlanPrice + 1) as IncreasedPrice from serviceplans p where planid in ('I1001', 'L2001', 'W1001') Thanks for the tip on markdown language.
Oh just that you were referencing other queries but I suppose that was theoretically. Seems like a bug or quirk of Oracle. I don't have the same issue in MS SQL. 
from a dynamic SQL statement, you can insert into a temp table that has been created outside...here's a quick example: declare @SQl nvarchar(max) select top 0 name, database_id into #output from sys.databases set @SQl = 'insert into #output select top 10 name, database_id from sys.databases' exec (@SQl) select * from #output
It's an Oracle thing. Always trips me up when I switch over from T-SQL.
[removed]
That's really cool to me that you can create temp tables on the fly like that. Unfortunately, the third party reporting software doesn't seem to allow it... just returns blank results when executing dynamic SQL. There's a listbox control that allows me to define the SQL. This populates the listbox: SELECT TOP 10 * INTO #output FROM [PEOPLE] SELECT * FROM #output This doesn't: EXEC('SELECT TOP 10 * INTO #output FROM [PEOPLE]; SELECT * FROM #output') Not sure if I have any other options.
Due to context scoping, a temp table created inside a dynamic SQL statement no longer exist after the statement is executed. This is why I had the temp table created outside. Also, recordsets returned by a dynamic SQL statement are often not considered for a lot of plugins due to the fact it's a different context that the main session. Try this: SELECT TOP 0 * INTO #output FROM [PEOPLE] EXEC('INSERT INTO #output SELECT TOP 10 * INTO #output FROM [PEOPLE]; ') SELECT * FROM #output
Thank you guys for your time.
I can accept that. Appreciate your answer.
&gt; edit: I officially fail at SQL formatting in reddit. Four spaces before the code Very fancy code here
So i feel like I'm getting really close here so far I have: SELECT MONTH(Date) 'Month Number #', FirstName, LastName, MAX(Total) 'Highest Month Total' FROM Job inner join Staff ON Job.StaffID = Staff.StaffID GROUP BY MONTH(Date), FirstName, LastName But I'm getting more than 1 entry per month, which obviously is not right there can only be one Staff member with the highest total each month. But I can't pin-point what I'm doing wrong...
I use (block select)[https://blogs.msdn.microsoft.com/davidlean/2009/09/18/tip-ssms-how-to-block-select/] in SSMS to insert the four spaces before I paste my code here.
[removed]
Hey, so this works (in the reporting software)! SELECT TOP 0 [LAST_NAME] INTO #output FROM [PEOPLE] EXEC('INSERT INTO #output SELECT TOP 10 [LAST_NAME] FROM [PEOPLE];') SELECT * FROM #output But this doesn't... DECLARE @ViewName NVARCHAR(64) DECLARE @Sort NVARCHAR(8) DECLARE @DynSQL NVARCHAR(MAX) **SELECT TOP 0 PEOPLE_CODE_ID, LAST_NAME, MIDDLE_NAME, FIRST_NAME INTO #temp FROM PEOPLE** SET @DynSQL = 'INSERT INTO #temp SELECT *, ROW_NUMBER() OVER(PARTITION BY PEOPLE_CODE_ID ORDER BY REG_GROUP_PRIORITY) FROM (' DECLARE cursor1 CURSOR FOR SELECT [GROUPVIEWNAME] ,[SORT] FROM [WEBREGISTRATIONGROUPS] [wrg] WHERE [ACTIVE] = 'Y' OPEN cursor1 FETCH NEXT FROM cursor1 INTO @ViewName, @Sort WHILE @@FETCH_STATUS = 0 BEGIN -- Add the select code. SET @DynSQL = @DynSQL + 'Select PEOPLE_CODE_ID, ''' + @ViewName + ''' AS [REG_GROUP_VIEW], ''' + @Sort + ''' AS [REG_GROUP_PRIORITY] from ' + @ViewName FETCH NEXT FROM cursor1 INTO @ViewName, @Sort -- If the loop continues, add the UNION ALL statement. IF @@FETCH_STATUS = 0 BEGIN SET @DynSQL = @DynSQL + ' UNION ALL ' END END SET @DynSQL = @DynSQL + ') x' CLOSE cursor1 DEALLOCATE cursor1 PRINT @DynSQL EXEC Sp_executesql @DynSQL SELECT * FROM #temp (LAST_NAME, MIDDLE_NAME, FIRST_NAME are just used as place holders... I assume I could create the temp table outside the dynamic SQL scope, if I want more control over the structure, with the same result?) Returns results in SSMS, but not the reporting software.
you should comment out the print command for testing with your reporting software, as it may throw it off. Is LAST_NAME the same field format as REG_GROUP_VIEW? Also, is there any reason your temp table has 4 fields, but your dynamic statement only has 3? that might also cause issues...
Thanks both of you, I&lt;ll keep that in mind next time I have something to paste around :-)
There's actually a fourth ROW_NUMBER() field defined in @DynSQL's first assignment. I got rid of print - no dice.
Could you replace the row_number clause with a static string such as 'abc'? Not familiar with your third-party reporting software, so I do not know where the code is parsed or how unrecognized commands are handled. Since row_number () is quite probably not the same format as FIRST_NAME, it may or may not perform proper auto-casting. But yeah, I'm starting to run out of ideas... 
I could do that, but I've now broken the query down to the basics: DECLARE @table NVARCHAR(64), @DynSQL NVARCHAR(256) = '' SELECT TOP 0 [LAST_NAME] INTO #output FROM [PEOPLE] DECLARE cursor1 CURSOR FOR SELECT 'PEOPLE' OPEN cursor1 FETCH NEXT FROM cursor1 INTO @table WHILE @@FETCH_STATUS = 0 BEGIN SET @DynSQL = @DynSQL + 'INSERT INTO #output SELECT TOP 10 [LAST_NAME] FROM ' + @table + ';' FETCH NEXT FROM cursor1 INTO @table END CLOSE cursor1 DEALLOCATE cursor1 EXEC(@DynSQL) SELECT * FROM #output Which works in SSMS, but not third-party... I think it has to do with the variables or the cursor, because this works: SELECT TOP 0 LAST_NAME INTO #output2 FROM PEOPLE EXEC('INSERT INTO #output2 SELECT TOP 10 [LAST_NAME] FROM PEOPLE;') SELECT * FROM #output2 Thanks for your help, in any case. Very insightful!
Your description sounds like a star schema. How are you adding data to this database or is it purely for analytical purposes?
I’ll take a look into star schema, but would the pathways even need to be their own entity? All of the points connect by a hubs. I kind of just want to ignore the “pathways” since it’s all just a network of points. There’s no database as of yet, but hopefully there will be someone soon. Data will most likely be uploaded in batches of excel files. At least at first. Looking into automating some things with python but that’s way, way down the line. 
Awesome stuff! I'm still grasping at it a bit but this definitely helped me out.. 
I don't know what you are taking about when you say pathways. My guess is that they should be foreign key relationships to other tables.
I use the (+) notation for right and left join. It's easier, looks clean, and way less typing. I also find it easier for debug. However you cannot use it to do a full outer join, for example a.empid(+)=b.empid(+) is not allowed. For that I'd bust out the FULL OUTER JOIN ... ON ... syntax. But I have yet to use a full outer join the real world. Select ALL THE THINGS!!!!! But I left join all day. I actually started with ANSI 92 joins and then "regressed" to the 89 style. 
Yeah... I think we're going to need some more tangible data or proper SQL terminology to answer this. What's a pathway? A workflow? What's an "exit to a pathway"? A user selected option? This all sounds like application logic (If&gt;Then), not at the database level. 
If you're struggling with double negation rather than the sql part of it, try unwrapping the levels by saying 'i'll want every record in the result set unless it is bad, and a record is bad if &lt;x&gt;'. "X" will have one less negation level in it.
Put the earliest date first, you have it second. Enclose your dates in single quotes. Like this: '2012-11-08' Also, if you want to do a BETWEEN, but the data has time stamps, just add one day to the end of the query. 
The date range only apply to order number like 59% You should use parenthesis
did you mean WHERE (a AND b) OR c OR d or did you mean WHERE a AND (b OR c OR d) examine the difference carefully and then remember this rule -- "when mixing ANDs and ORs, use parentheses" 
Mixing ORs and ANDs always trips me up because I can never remember the order in which they're applied, so I use parentheses to make it explicit: WHERE ( (vendor_code='XX' AND item_number LIKE '105%') OR item_number LIKE '704%' OR item_number LIKE '59%' ) AND orders.date_of_order = '2017-11-8' The exact syntax of the date delimiters might vary based on which platform you're using.
Try this as your query: SELECT ~~TOP 3~~ sum(LineItems.QuotedPrice*LineItems.QuantityOrdered) AS Total, Customers.CustomerID, Customers.CustFirstName, Customers.CustLastName FROM (Customers INNER JOIN Orders ON Customers.[CustomerID] = Orders.[CustomerID]) INNER JOIN LineItems ON Orders.[OrderID] = LineItems.[OrderID] GROUP BY Total, LineItems.OrderID, Orders.CustomerID, Orders.OrderID, Customers.CustomerID, Customers.CustFirstName, Customers.CustLastName [ORDER BY] (https://msdn.microsoft.com/en-us/library/bb208913(v=office.12).aspx) Total LIMIT 3; Try that?
Gives me an error on calling LIMIT, says it's missing an operator: [Error](https://imgur.com/5k76zkg)
It's the [Replen] in the where cause. It doesn't exist there yet. SELECT PartNum , PartDescription , ProdCode , ClassID , (CASE WHEN LEN(ISNULL(userchar3, '')) &gt; 0 THEN 1 ELSE 0 END) AS Replen FROM PART_Part AS Part WHERE Part.ProdCode IN ('0071.HIY', '0070.HIY', '0100.RMT') AND TypeCode = 'M' AND (CASE WHEN LEN(ISNULL(userchar3, '')) &gt; 0 THEN 1 ELSE 0 END) = 1 Or you can use a cross apply to get it to a usable column. SELECT PartNum , PartDescription , ProdCode , ClassID , Replen FROM PART_Part AS Part Cross APPLY ( SELECT (CASE WHEN LEN(ISNULL(userchar3, '')) &gt; 0 THEN 1 ELSE 0 END) AS Replen ) x WHERE Part.ProdCode IN ('0071.HIY', '0070.HIY', '0100.RMT') AND TypeCode = 'M' AND Replen = 1 
Or a nested query SELECT * FROM ( SELECT PartNum , PartDescription , ProdCode , ClassID , (CASE WHEN LEN(ISNULL(userchar3, '')) &gt; 0 THEN 1 ELSE 0 END) AS Replen FROM PART_Part AS Part WHERE Part.ProdCode IN ('0071.HIY', '0070.HIY', '0100.RMT') AND TypeCode = 'M' )a WHERE Replen = 1
I had a feeling it didn't exist yet. Thanks!
Is any certain one more "proper"?
Something like this should work. SELECT TOP 3 * FROM ( SELECT sum(LineItems.QuotedPrice*LineItems.QuantityOrdered) AS Total , Customers.CustomerID , Customers.CustFirstName , Customers.CustLastName FROM Customers JOIN Orders ON Customers.[CustomerID] = Orders.[CustomerID] JOIN LineItems ON Orders.[OrderID] = LineItems.[OrderID] GROUP BY Customers.CustomerID , Customers.CustFirstName , Customers.CustLastName; ) a ORDER BY Total DESC 
Access is giving me a "syntax error in FROM clause". Any idea why? I removed what appears to be a transient "a" from near the bottom. I apologize.
Not a transient 'a' it's an alias
[Here's what MSAccess is spitting out:](https://imgur.com/t7ZIEed) Sorry to keep being a bother.
stray semicolon i think
Did you google that error or ask your microsoft rep?
Ahh, well thanks for trying to help! I appreciate it. Maybe I can try to debug it.
[removed]
The Microsoft rep asked me to "Bing" it...
And, did you "bing it", "bing it"?
Using the 89 syntax, how do you denote a condition that is part of the join and not the where clause? Do you enclose the whole thing in parentheses, like ( a.empid(+) = b.empid and b.deptid = 5 ) I'm the opposite of you, I learned on 89 and switched to 92 shortly after. I personally find it easier to understand as join conditions are clearly separated from filter conditions. I work on the SQL Server side and they have completely abandoned 89 syntax starting with SQL 2012, which if memory serves, used an asterisk to denote the left or right side of an outer join. I won the battle to get our coding standards updated to use all 92 syntax, but lost the battle to use just "join" for inner joins. It was a battle I was content to concede, as we were able to win the greater (syntax) war. 
/u/aplato is saying move the semi-colon after the last entry in the GROUP BY clause to after the ORDER BY Total DESC statement.
Access is a bit dumb. One way around it might be to save the subquery as its own query. So save this as, e.g., query1: SELECT sum(LineItems.QuotedPrice*LineItems.QuantityOrdered) AS Total , Customers.CustomerID , Customers.CustFirstName , Customers.CustLastName FROM Customers JOIN Orders ON Customers.[CustomerID] = Orders.[CustomerID] JOIN LineItems ON Orders.[OrderID] = LineItems.[OrderID] GROUP BY Customers.CustomerID , Customers.CustFirstName , Customers.CustLastName; Then the next query would be just SELECT TOP 3 * FROM [Query1] ORDER BY Total DESC
You do t have you're query ordered, so the top 3 are just random. If your order it by the total descending then it will show you the top three with the highest totals.
The outermost parentheses are not needed, but both with and without is valid syntax and will do the same thing. In the example shown you have two things going on, which are 1) b.deptid has to be 5, and 2) the right join from table a to table b. Joining in itself can be a "filter" in a sense, so maybe that's why it started out that way by sticking it in the where clause. A lot of my queries can join 20+ tables, so using the 89 cuts the typing down considerably. Microsoft and Oracle are two different beasts. Oracle does a good job of acting like Microsoft doesn't exist. Interestingly enough, you can view most of the SQL code within Oracle's ERP and I don't think I've ever seen the JOIN ON syntax on any query or view. Some of the code is 30 years old but even the more moderns stuff still has joins in the where clause. Agree on "join" instead of "inner join". Why type it all out when inner join is implied unless otherwise specified? Keep the fight going because that's for people that need to be reminded that they are doing an inner join, I guess. But if you're not explicitly typing "right" or "left" or "full", then what are you doing? Oh yeah, inner... 
I don't have access to an Oracle box at present, but the results using ANSI-92 syntax with conditions in the join clause vs. the where clause are absolutely different. Consider the following (silly) example: DECLARE @Employee TABLE (EmpID INT, Name VARCHAR(100)); DECLARE @EmployeeDept TABLE (EmpID INT, DeptID INT); INSERT @Employee VALUES (1, 'Moe'), (2, 'Larry'), (3, 'Curly'), (4, 'Shemp'); INSERT @EmployeeDept VALUES (1,1), (2,1), (3,2), (4,1); SELECT e.EmpID, e.Name, ed.DeptID FROM @Employee e LEFT JOIN @EmployeeDept ed ON e.EmpID = ed.EmpID AND ed.DeptID = 2 WHERE 1=1; SELECT e.EmpID, e.Name, ed.DeptID FROM @Employee e LEFT JOIN @EmployeeDept ed ON e.EmpID = ed.EmpID WHERE 1=1 AND ed.DeptID = 2; Note the different results based on where the filter is placed. Contrived though it may be, but can this same scenario be replicated with ANSI-89 syntax?
AND should always have higher precedence than OR. I don't really have an issue remembering it but A comes before O could be an easy way of remembering it.
Hmmm, I'm not an expert in ANSI 89 compliance but here is how you would do it the "oracle" way while avoiding the word "join" like the plague. https://i.imgur.com/BQTqs6r.png I've unioned the output of the two queries together plus added the oracle version and separated them with red boxes for clarity. The blue highlighted subquery is the "join with condition" alternative. I don't think you can do this without a subquery (or a CTE) while avoiding the word "join". Nothing to do with the use of the (+) syntax anyway. In my query I still specify the left join using that method but to limit the "from" data to a subset has to be done in a subquery as far as I know. Also in case you're interested, you always have an "oracle box" available at https://livesql.oracle.com which I use all the time for proof-of-concept examples when I'm questioning things. An account is required but it is free, and you can even store objects like views and tables. I'm sure it's very limited but it's a pretty cool tool to have. 
Hi. I am not sure that I understand the question. What do you mean by table cell? Do you mean an html tableV What program are you using? Are you asking how to insert line breaks into a long string returned by a sql query?
Sorry, I'll try to explain further. I'm referring to data in a MSSQL Server 2016 DB. The lines above are coming from a single cell called "StringData", and from a table called "MultipleStatisticData" that resides on that SQL DB. What I would like to attempt is to write a query that will put line breaks into that string after the semi colon if that's even possible. Hope this helps, let me know if you need more details.
While you have indeed successfully avoided the word "join" like the plague, you've also managed to give me AIDS. So, thanks for that. 
My suggestion is first learn MySQL. Several database platforms use [SQL](https://www.youtube.com/watch?v=7Vtl2WggqOg), but a slight variation on it—each tend to have a slightly different syntax. Microsoft SQL and MySQL are two of the most common database platforms on the web.SQL Server is slightly older than MySQL.
 UPDATE MultipleStatisticData SET StringData = REPLACE(StringData,":",":" + CHAR(13) + CHAR(10)) 
Jet SQL is dumb and unfortunately can't just string joins together like normal SQL languages. You have to nest additional joins in parentheses. Search for how to join multiple tables in Access to see some examples. 
 SELECT SUBSTRING(Call_date, 1, 13) AS date_hour , COUNT(Call_ref) AS cc FROM Issue GROUP BY date_hour
&gt; with topmost record of the subdivision topmost? based on what?
please show which columns are in which table
Thank you very much
If the scores for each time are being split across rows then it means something in your GROUP BY is splitting them out as such. Take a look at what you're grouping on and try and conceptualise how the grouping is working that would cause that. Also once you've fixed that you're going to run into another problem so I'll give you a hint for that as well. Think about what would happen if no team scores a goal. 
Not enpugh info. Show table structures if you can. r3pr0b8 make a valid point - you need to define "topmost".
upvote for wonderful coding style, including indents and leading commas
Thank you!
Thank you!
Thank you!
Thank you!
I wasn't sure if I could post screenshots between the text, so the link I put down contains the tables at the top of the page
i am so sorry, i forgot to check your link okay, here you go, a solution ~without~ using CASE SELECT game.mdate , game.team1 , COALESCE(goals1.goals,0) AS score1 , game.team2 , COALESCE(goals2.goals,0) AS score2 FROM game LEFT OUTER JOIN ( SELECT matchid , teamid , COUNT(*) AS goals FROM goal GROUP BY matchid , teamid ) AS goals1 ON goals1.matchid = game.id AND goals1.teamid = game.team1 LEFT OUTER JOIN ( SELECT matchid , teamid , COUNT(*) AS goals FROM goal GROUP BY matchid , teamid ) AS goals2 ON goals2.matchid = game.id AND goals2.teamid = game.team2 
Here's a sqlfiddle demonstrating all of the above scenarios if anyone else doesn't want to do real work right now: http://sqlfiddle.com/#!4/4dd0d/8
Also, you may want to look into a different function besides substring - probably something like DATEPART to extract the hour value from the date. But, if what /u/r3pr0b8 provided gets you what you need, then that should work as well.
something like this should do the trick. You don't need the first table since your only asking for employee name which is already in the last table you mentioned. Regarding your second point: since you're pulling subdivision from a table that requites division, there will be no cases where division is null and subdivision is not. I think this may have been a typo of "if there is only one subdivision" Also, I'm assuming topmost subdivision is alphabetical in this case. select a.employee_name, b.division, b.sub_division from empdivmap a left outer join ( select division, sub_division, row_number()over(partition by division order by sub_division desc) as rn from division_table) b where b.rn=1
Because you have two options, either doing a like with double % or adjusting the column to be non-sargable, you will need to look at a query plan, beware of caching, and measure wait statistics to see which performs better. I would recommend investigating columnstore indexes and full text search and see if those could be better solutions for you in the long run. 
Thanks, that sent me on a quick [rabbithole into *S*earchable *Arg*uments](https://www.sqlinthewild.co.za/index.php/2016/09/13/what-is-a-sargable-predicate/) (formatting to help anyone else wonder what *sargable* stands for). I've been doing joins on expressions routinely and I wonder what the upshot would be if I wasn't cannonballing the indexes. If i'm understanding correctly, it would be better to create a subquery like SELECT CharIndex('text',Field_name,1) as New_col, * FROM Table WHERE [all my other filters that don't require expressions] and filter *that* with WHERE New_col = 1 Does that hold water?
Because we are not converting the column on the table, we are converting the value we want to compare against the table, we can use the index if one exists. Here's a text example. Let's say I want to find all invoices by Bobby, and the column was poorly designed from the getgo. The data looks like: Invoice by Bobby or Invoice by Debby or Invoice by Jack. To make this sargable, I would do this: DECLARE @PersonNameVarPassedByApp varchar(50) SET @PersonNameVarPassedByApp = 'Bobby' SELECT Col1 FROM TABLE WHERE InvoicesByPerson = 'Invoice by ' + @PersonNameVarPassedByApp Now you can use your index! There are times where you just can't do that though. I recently was trying to design a search function that could search for places that a web app would use. One use case though was some places may be spelled... weirdly, like 'K E N T U C K Y'. Well, the best way to search now is to perform calculations on the column and using LIKE '%%', effectively not using the index. If you use full text search, you create indexes on words. If you had a varchar column and it had sentences like Invoice by Bobby or More widgets needed, each of those words become indexed and you can specifically query for those words using contains or freetext where clause qualifiers. 
if x is the same as what? another field?
I don't really understand you and on mobile. Can you put it all in a where? Where X = yourfield and (y&lt;&gt; yourfield or z&lt;&gt; yourfield)
i updated with a table. it may help
i updated with a table. it may help
Thanks for the comprehensive reply. That will save me some re-work had I just run off on my assumption!
Functionally identical to what r3pr0b8 suggested, you could also write your GROUP BY like this: GROUP BY SUBSTRING(Call_date, 1, 13)
It would be helpful if everything was laid out and matched. Your query references column names that are not in your sample table. Also, you are asking how to get X is the same, Y is different, and Z is the same initially. Then below that you are simply asking for X is the same but Z is different. Also, you want to pull rows 100-xyz and 100-abs even though 100-abs does not exist in your sample data. Some possible solutions are below but not necessarily correct without clearer description of the data and what is being requested. -- Get records where X has records where Y &amp; Z are different. SELECT t1.X , t1.Y , t1.Z FROM crim t1 INNER JOIN crim t2 ON t1.X = t2.X AND t1.Y &lt;&gt; t2.Y AND t1.Z &lt;&gt; t2.Z AND t1.COUNTY = T2.COUNTY WHERE t1.COUNTY = 4 ORDER BY T1.X; -- Get records where X has records where Z is different. SELECT t1.X , t1.Y , t1.Z FROM crim t1 INNER JOIN crim t2 ON t1.X = t2.X AND t1.Z &lt;&gt; t2.Z AND t1.COUNTY = T2.COUNTY WHERE t1.COUNTY = 4 ORDER BY T1.X; 
Woahhhh this hurts my entry level brain. 
Thanks for the advice. Will give Adventure Works a shot and work on the joins and group by's. 
Thanks for the link. 
Got it. Will give it a shot. 
yeah, but it works, right? try running the subquery by itself (yes, there are two subqueries, but they are actually identical, and i expect the database optimiser to realize this, and run it only once) the subquery produces a table of results -- examine these results to become familiar with what they subquery is doing okay, so the subquery produces a table of results, which is then joined to rows of the game table based on matchid and whether you want the team1 or team2 goals i will leave you to figure out why COALESCE is required with LEFT OUTER JOIN
solution verified
This won't quite work because of the colons in both the dates, and the first line. It would split those into separate lines and look like this: Total number of lines that match search criteria: 4. Lines that have search string: 2017-10-27 14: 28: 14,583 ERROR [ajp-0.0.0.0] This is an example of the data in my string : 2013-10-28 13: 00: 41,120 ERROR [ajp-0.0.0.0] This is another example of the data in my string : 2013-10-28 13: 37: 09,007 ERROR [ajp-0.0.0.0] This is yet another example of the data in my string : 2013-10-28 13: 37: 09,007 ERROR [ajp-0.0.0.0] Final example of my string : You'd need to change it slightly, and this is still tenuous at best. The input data is not great here. UPDATE MultipleStatisticData SET StringData = REPLACE(StringData , ": " , ": " + CHAR(13) + CHAR(10));
Depending on your requirements this will not work. It will accurately give you the calls by each hour of the day, but it will not give you the max, min, or other meaningful statistics on your calls per hour. For example you might receive 100 calls between 2pm - 3pm, and 100 calls between 3pm - 4pm, but you might receive 180 calls between 2:30 and 3:30... so if the purpose of your data is to look at call volume in order to project staffing requirements, then what you need to do is generate a moving window and run a loop that calculates every 60 minute 'window' as it moves through the day in increments of 15, 5, or 1 minute (depending on how accurate you want to be). I might have some old work that does this but I'd have to dig through archives to find the query. 
Move having insure the parens()
 SELECT book_id FROM ( SELECT book_id, count(book_id) AS 'issue_count' FROM issue_return GROUP BY book_id HAVING issue_count = 3 ) AS T This results in, +---------+ | book_id | +---------+ | 1 | +---------+ works, but again if I use any of max or min here it gives , SELECT book_id FROM ( SELECT book_id, count(book_id) AS 'issue_count' FROM issue_return GROUP BY book_id HAVING issue_count = 3 ) AS T results, Empty set (0.00 sec) 
union query
won't a union query create potential duplicate rows? for example: give me all users created or updated in the last 1 day 1. user signs up 2. user updates profile
book with maximum issues -- SELECT book_id , COUNT(*) AS 'issue_count' FROM issue_return GROUP BY book_id ORDER BY COUNT(*) DESC LIMIT 1 book with minimum issues -- SELECT book_id , COUNT(*) AS 'issue_count' FROM issue_return GROUP BY book_id ORDER BY COUNT(*) ASC LIMIT 1 depending on your database platform (which you neglected to mention), you may need slightly alternate syntax for LIMIT, e.g. TOP 1 for MSSQL, etc.
if you're worried about that, just use UNION instead of UNION ALL
Oh awesome. Thank you. If I'm also doing pagination via cursoring (e.g., last seen id method), i'll need to keep track of last seen created id and last seen updated id? 
you would have to answer that, not me
&gt;HAVING issue_count = MAX(issue_count) I don't like this code. I don't specifically know what to tell you about why it's not working but my guess is that you haven't yet defined what the max or min is: select a.* from ( select book_id, count(book_id) as issue_count from issue_return group by book_id ) a inner join ( select max(issue_count) as max from ( select book_id, count(book_id) as issue_count from issue_return group by book_id ) a ) b on b.max = a.issue_count Also look at your original post, it seems you are using MIN() in both examples where I presume you meant to use MAX(). If one works for MIN() but not for the other then I would suspect it is a bug. In MSSQL I would use a row_number() or something to select the max or min to remove (or include) duplication where the values are the same for two id's, but here is a piece of code that I imagine would work: with cte as ( select book_id, count(book_id) as issue_count from issue_return group by book_id ) select book_is, issue_count from cte where issue_count = (select max(issue_count) from cte)
Yes, the delimiter is also used as data in some of the fields. To parse it correctly we need to know the data better.
I can actually modify the symbol at the end of the string. I would be a simple change to 1 line in the powershell script. So let's say I changed it to semi-colon. Would that help the formatting? Also, if I run the query that you shared, would that be a permanent change?
If you can't do your own homework and in fact can't even seem to put any effort in trying before you ask others to do it for you, how in the fuck do you think you're ever going to pass a test or work in the field?
i fished it but i just want to check 
a. Report those payments greater than $100,000. SELECT checkNumber, paymentDate, amount FROM Customers WHERE amount &gt; 100,000; b. List the products in each product line. SELECT productLine FROM Products WHERE Products; c. How many products in each product line? SELECT COUNT(productNumber) FROM Products; d. What is the minimum payment received? SELECT MIN(amount) FROM Payments; e. List all payments greater than twice the average payment. SELECT checkNumber, paymentDate ,amount, customerNumber FROM Payments GROUP BY checkNumber, paymentDate ,amount, customerNumber Having count (checkNumber) &gt;2 ; One to many relationship f. How many orders have been placed by Herkku Gifts? SELECT employee_id, job_id, salary FROM employees WHERE last_name = 'Herkku Gifts'; g. Who are the employees in Boston? SELECT employeeNumber "Department Code", COUNT(*) "No of Employees" FROM employees GROUP BY department_id; i. List the value of 'On Hold' orders. SELECT orderNumber FROM orders WHERE status; j. Report the number of orders 'On Hold' for each customer. SELECT orderNumber, status, customerNumber, customerName FROM Customers, Orders WHERE customer LIKE %On Hold% Many to many relationship k. List those orders containing items sold at less than the MSRP. SELECT orderNumber, producCode, FROM Products, WHERE m. List the products ordered on a Monday. SELECT orderNumber, orderDate FROM Orders, WHERE DATENAME (DW, Orders.orderDate) = ‘Monday’ n. What is the quantity on hand for products list don 'On Hold' orders? SELECT orderNumber, orderDate FROM Orders, WHERE DATENAME (DW, Orders.orderDate) = ‘Monday’ 
a. Report those payments greater than $100,000. SELECT checkNumber, paymentDate, amount FROM Customers WHERE amount &gt; 100,000; b. List the products in each product line. SELECT productLine FROM Products WHERE Products; c. How many products in each product line? SELECT COUNT(productNumber) FROM Products; d. What is the minimum payment received? SELECT MIN(amount) FROM Payments; e. List all payments greater than twice the average payment. SELECT checkNumber, paymentDate ,amount, customerNumber FROM Payments GROUP BY checkNumber, paymentDate ,amount, customerNumber Having count (checkNumber) &gt;2 ; One to many relationship f. How many orders have been placed by Herkku Gifts? SELECT employee_id, job_id, salary FROM employees WHERE last_name = 'Herkku Gifts'; g. Who are the employees in Boston? SELECT employeeNumber "Department Code", COUNT(*) "No of Employees" FROM employees GROUP BY department_id; i. List the value of 'On Hold' orders. SELECT orderNumber FROM orders WHERE status; j. Report the number of orders 'On Hold' for each customer. SELECT orderNumber, status, customerNumber, customerName FROM Customers, Orders WHERE customer LIKE %On Hold% Many to many relationship k. List those orders containing items sold at less than the MSRP. SELECT orderNumber, producCode, FROM Products, WHERE m. List the products ordered on a Monday. SELECT orderNumber, orderDate FROM Orders, WHERE DATENAME (DW, Orders.orderDate) = ‘Monday’ n. What is the quantity on hand for products list don 'On Hold' orders? SELECT orderNumber, orderDate FROM Orders, WHERE DATENAME (DW, Orders.orderDate) = ‘Monday’ 
Removing duplicates is an expensive operation, but there is only one to find out if it results in better performance.
Have you tested those yourself? At a glance, some look incorrect, but eithout the data, hard to say. Hint, would you need to use distinct when using count()?
no because i use mac |:
What can I say?
a foreign key must reference either a PRIMARY KEY or a UNIQUE KEY your first two tables are missing these
&gt; Would the code below be right? what happened when you tested it? ™ also, note in your Shoe_Size table, if you make ShoeID the primary key, then each shoe can have only one size
Oops, rookie mistake. Thank you
There are plenty of databases that work on Mac.
If you can change the delimiter to a unique character (; or ~ for example), then you could use something like this to split the lines: UPDATE MultipleStatisticData SET StringData = REPLACE(StringData , ";" , ";" + CHAR(13) + CHAR(10)); or UPDATE MultipleStatisticData SET StringData = REPLACE(StringData , "~" , "~" + CHAR(13) + CHAR(10));
This is a simple group by gotcha. When you're using aggregate functions, you have to say what you're aggregating by. If you just ask for max timestamp, you'll get the max timestamp regardless of user. You've grouped by user, so you'll get the max timestamp for each user. But when you add inorout to your query, you're asking for all of the inorouts, but only some of the timestamps, which of course doesn't work. What you need is to collect the user and max timestamp as you've done, then write another query that gives you the InOrOut for that user and timestamp. 
This is probably an NLS_SORT issue which is derived from NLS_LANGUAGE. Show the parameters for NLS_* on both systems and compare. The issue is probably with the regex of [^ -~] which says something like "not in the range of space to tilda", but due to the sort and language parameters, the starting character of "space" does not precede the ending character of "tilda", so you get ORA-12728 message about having an invalid range within the regular expression.
Is this a homework problem?
Awesome! Thank you for your help and pointing me in the right direction. I have a cunning plan...
It may be as easy as putting an "alter session set nls_language" statement at the start of your script. Alternatively you could check the language then adjust you regex per-language case. Example: select case when (SELECT USERENV ('language') FROM DUAL) like 'AMERICAN%' then 'some regex code that works with your language' when (SELECT USERENV ('language') FROM DUAL) like 'DUTCH%' then 'some other regex code that works with other languages' end from dual; Glad it helps!
maybe change CURSOR getTrain IS...; to: getTrain CURSOR FOR SELECT * FROM Train_Status WHERE TrainNumber = TN AND TrainDate=DT; 
If this IS homework, and I AM to hire you at some point in the future, I'll just point you in the right direction (you're almost there). You just want to SELECT the Customer(s) who is/are NOT IN the set you just defined. In the real world, if all of your customers except one call five minutes before you close, the problem is not with that one customer. I think that's what makes this smell like homework.
This definitely sounds like a homework problem lol. A developer or DBA on the job would be like "gtfo" if someone came to them with this problem. That said, allow me to do some kid's homework instead of doing real work: DECLARE @Customer TABLE (CustomerID INT, CustomerName VARCHAR(100), ContactID INT); DECLARE @Caller TABLE (CallerID INT); DECLARE @Issue TABLE (IssueID INT IDENTITY, CallerID INT, CallDate DATETIME); INSERT @Customer VALUES (1, 'Apple', 100), (2, 'Microsoft', 101), (3, 'Facebook', 102), (4, 'Google', 103); INSERT @Caller VALUES (100), (101), (102), (103); INSERT @Issue VALUES (100, '2017-08-12 19:55:00'), (100, '2017-08-12 19:56:00'), (100, '2017-08-11 19:57:00'), (101, '2017-08-12 18:55:00'), (102, '2017-08-12 19:59:59'), (103, '2017-08-12 20:00:00'); SELECT c.CustomerID, c.CustomerName, i.IssueID, i.CallDate FROM @Customer c JOIN @Caller ca ON c.ContactID = ca.CallerID JOIN @Issue i ON i.CallerID = ca.CallerID WHERE 1=1 EXCEPT SELECT c.CustomerID, c.CustomerName, i.IssueID, i.CallDate FROM @Customer c JOIN @Caller ca ON c.ContactID = ca.CallerID JOIN @Issue i ON i.CallerID = ca.CallerID WHERE 1=1 AND LEFT(CONVERT(TIME(0), i.CallDate), 5) BETWEEN '19:55' AND '20:00';
An order shouldn't be for a given productid but rather a given shoeID. If you feel like carrying productid as a denormalized fk that's fine. What I'd do is kill the idea of shoe size and call the thing shoe attributes. Like, think of converse, specifically Chuck's. They have two products, high top and low top. And then these products have a bazillion colors and sizes. In fact, I'd make a show size and a color lookup tables and make the shoeattribute table the intersection of these 3 keys, productid, colorid, sizeid. If you want a surrogate for identity to carry into the orders table that's fine, rather than the composite natural key.
I am new to working with databases on the enterprise level and the way this table is structured is extremely convoluted to grab information out of. I was wondering if there was any way I could filter the main table everything joins to similar to the way i filter the joining tables. I figure this would be much more efficient than having them join on all tables then filter later in the where. Any help would be appreciated.
If you're on Firebird v3.0 or later: SELECT user, timestamp, InOrOut From ( SELECT User ,TimeStamp , InOrOut , row_number() OVER (PARTITION BY user ORDER BY timestamp desc) RowNum FROM your_awesome_table WHERE datestamp = '20171031') x WHERE x.rownum = 1
I understand on a high level but I sure can't figure out how to do it in SQL. If I understand, I need to keep the select statement above but store the result in variables. Then I use those variables as criteria for a second select statement. The problem is Firebird seems to only allow variables in a procedure, which is leading me down a rabbit hole that seems to be overkill. Am I heading in the right direction?
SQL engine (and optimizer) will move operations around and 'where' conditions could be executed before your joins and your tables are not going to be joined/accessed in the order written. An execution plan will show you the order of operations and also will indicate the most expensive steps.
What is your RDBMS and version? You might be able to do this with a `LAG` function. Is Company just a text field or a unique key?
 select a.user_id, a.last_time, b.inorout from first_query a join my_awesome_table b on a.user_id = b.user_id and a.last_time = b.timestamp. First_query is the query that generated your table of users along with max timestamp. You can make that a temporary table, or a subquery, or a cte, or a physical table, whatever you want. There are many ways to achieve the result you need, but I think this is the most intuitive. 
CTE?
you could change it to ... FROM (select * from dbo.StatProfile where dIntervalStart BETWEEN @Start of Month and @YesterdayEndDat) AS PRO ...
It's ver 1.5. :-(
So correct me if im wrong, you mean I should make a table for shoe size and a table for color, a intersection table calling it shoe attributes where all 3 of the primary key (shoe size, color, product/brand)? Relating to the shoe size primary key, would it be unnecessary to add another column listing the shoes size if I were to just set the primary key 1,2,3,4,5,6,7... as the size? Sorry, it's kind of hard to see without diagram/code examples. So the whole general thing I'm trying to do is where the employee is able to see the customers making what type of purchase.
I am getting the syntax error in the first Declare line itself. Any idea why?
Why are you using a subquery in the column? ,CASE WHEN temptablealias.Order &lt;= 5 THEN 'Yes' else 'No' end as FirstFive 
The subquery, as written, returns same number of rows as the temp table has. Did you forget a 'where' clause for it?
There is a where clause. It's just another column in the temp table
If #temptable has more than one row you are returning more than one value and you can't do that when a subquery is used as an expression. E.g. if you have SELECT thing, (SUBQUERY), other_thing FROM some_tables Then (SUBQUERY) can only return one result. If you have more than one row in #temptable you are returning more than one value with that query. e.g. you are returning Yes, No, Yes, No, No, No, Yes etc.... Which won't work. If you need to get this Yes or No value from #temptable then you need to limit it somehow. E.g. (select CASE WHEN SUM(temptablealias.Order) &lt;= 5 THEN 'Yes' else 'No' end as FirstFive from #temptable) 
https://docs.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql
That's how it was set up in an example I found on case statements. I haven't messed with those too much so I was googling around. It'd be pretty hard to share the schema over the five tables I'm touching. These things have dozens of columns. The select statement is clocking in at just under 500 characters before I even start listing the joins and where clauses. I'll see if I can cook up some simplified version.
OK, so, the bad news is version Firebird v1.5 doesn't support temporary tables, subqueries, or CTE's. Maybe I'm being paranoid but I really don't want to create a table every night on this old POS computer. We're looking into upgrading the front-end app so I'm going to pursue that instead. The good news is I finally understand what you were getting at. The join make it click for me. I learned something. I really appreciate your time.
That's how it was set up in an example I found on case statements. I haven't messed with those too much so I was googling around. It'd be pretty hard to share the schema over the five tables I'm touching. These things have dozens of columns. The select statement is clocking in at just under 500 characters before I even start listing the joins and where clauses. I'll see if I can cook up some simplified version.
There is a where clause. It's just another column in the temp table
It's not in the Subquery though. Obviously #temptables has more than one row and the subquery is returning more than one value. The subquery needs limiting someone to return only one value (e.g. like ichp says with a constraint in the WHERE clause that limits it to just one row) or it needs rewriting to join #temptable to the other tables and just remove the subquery. 
I think you're tracking. Let's think of tables as fitting into two major categories. Tables that hold reference information, and tables that hold transactional data. A wise old SQL architect once argued that reference table data has more to do with schema than data. He had a valid point. No transaction can be created without first having these reference tables populated. For example: StatesLKP has like 50 records in it (+4 if we add the military codes, +? for the territories). Before you ever add a customer's address, you're going to need that StatesLKP table to be populated. If one doesn't use reference tables, the front end data-entry could be freeform and I could enter a customer's state as ZZ and that would be acceptable... the form wouldn't puke and say "that is not a valid state". Ok, now going back to shoes. I'm probably going to model this TOO MUCH for your assignment. That's the way it goes. You end up modeling the world, haha. BrandLKP - BrandID int identity(1,1) primary|BrandName varchar(25) not null BrandName will hold values like Nike, Adidas ProductLKP - ProductID int identity(1,1) primary|BrandID int identity(1,1) fk_BrandLKP|ProductName varchar(50) not null ProductName will hold values like Samba, Pharrell Williams, Forum Hi, Forum Low ColorLKP - ColorID int identity(1,1) primary|ColorName varchar(250) not null|RGB varchar(15) null|HSL varchar(17) WidthLKP - WidthID int identity(1,1) primary|SizeWidthCD varchar(2)|SizeWidthDSC SizeLKP - SizeID int identity(1,1) primary|SizeLengthCD dec(4,2)|WidthID int fk_Width Shoe - ShoeID int identity(1,1) primary|ProductID|SizeID|QtyInStock int|QtySold int|Price dec(6,2) Customer - CustomerID int identity(1,1) primary|CustomerName varchar(500) - This could be very wide with customer properties OrderStatusLKP - OrderStatusID int identity(1,1) primary| OrderStatusDSC varchar(100) Order - OrderID int identity(1,1)|CustomerID fk_Customer|OrderDate datetime|DeliveredOnDate datetime|OrderStatusID fk_OrderStatusLKP|OrderPrice dec(10,2)|Deleted bit default=0 OrderItem - OrderItemID int identity(1,1)|OrderID fk_Order|ShoeID fk_Shoe|Qty int|OrderItemPrice dec(10,2)
That's what I'd do too. 
To your question about ID vs other columns, that's a religion debate. On the one hand there's the idea that, if you can identify a natural key, than use it. On the other hand there's the idea that you should always use identities, GUIDs, etc. and NEVER rely on it to hold any meaningful value. Don't forget half sizes! And maybe those Wide shoe sizes.
Thinking "Why *AM* I using a subquery" got me there. I think. Just took that out and left the case statement and it appears to be working. Thanks! 
SQL server 2014. The Company is a text field.
Here's a base of what I _think_ you're trying to do... http://sqlfiddle.com/#!6/cc3b40/1
Well its only 12 files so I would brave it out and use the wizard; alternativly, if the format is the same, you could use some powershell code to merge the 12 files into a single file. If this is something you expect to be doing frequently, consider making an SSIS package or custom program to do this for you.
Each file is a different year, so thats probablly not possible to merge the 12 files into a single one
You're filtering on two different columns in your example. The most likely scenario is that one is [properly] indexed, while the other is not. Keep in mind too that the JOIN/ON clause occurs before a WHERE. You're starting with a smaller dataset by including the statement in your ON rather than getting all the data first, and then applying there WHERE to it. See here: http://blog.aajtech.com/blog/sql-programming-understanding-the-logical-order-of-operations-in-select-statements/.
You would probably be able to script it out by parsing word document (with powershell?) and then running create table statements and bcp.exe (with appropriate parameters) based on your parsed data. I see that as the only way of avoiding defining tables manually. Even imp/exp wizard would ask you to define column sizes because otherwise you would run into data truncation error most likely
The latest version of SSMS (17.3?) has a flat file import wizard that works pretty well. You could try with OPENROWSET too. I would probably just bite the bullet and make an SSIS package as /u/manojk92 suggested and once you have (tediously) defined your source you can just switch the source file and run the jobs pretty quickly.
There's a "Suggest Types" button to auto detect the fields... Don't use it. Trust me. :)
You didn't specify precision.
cant even tell if the starting positions are even correct Sex should be at 149 but 149 is blank
Fixed width or Ragged? If you have no delimiters you don't have much choice but to figure it out. Hope you save the package once you're done so you don't have to do it 12 times!
You ever hear of the unix tool, `cat`?
Have you confirmed that whatever @var1 and @var2 actually are is convertible to datetime? Eg, what does DECLARE @Client nvarchar(255) = (SELECT Client FROM @ClientList WHERE ID = @Loop); DECLARE @Var1 nvarchar(255) = (SELECT Var1 FROM [AnalyticsMapping].dbo.[SLA_Variables] WHERE Client = @Client AND SLAType = @SLAType); DECLARE @Var2 nvarchar(255) = (SELECT Var2 FROM [AnalyticsMapping].dbo.[SLA_Variables] WHERE Client = @Client AND SLAType = @SLAType); SELECT ISDATE(@var1); SELECT ISDATE(@var2); tell you?
Nope but I’ll check it out 
&gt; How can I get the 4 results from above with all three tables? that's not going to be possible, unless you can define how each of the four rows from b is supposed to decide which one of the four rows from c it's supposed to match up with ‏
First, quit your job or change your role. You shouldn't be working with this technology at all. Files belong on file systems... SO, if you really have to put anything in the database at all, it should be where the files are located. If that doesn't work for you for some reason, you could look at FILESTREAM, but you probably shouldn't be messing with that either. Better to ask the question, "Why the hell do I want to put these files into a table?"
Also, why is everyone assume the "txt" file is delimited? Because the OP mentions "columns"? How about you describe to everyone what structure the files are; is the data comma delimited? Tab? Fixed length? Frog balls? If the files are delimited properly, importing it can be easy... or it can be major pain in the ass with type conversion issues. 
Ok so just subquery it out. That makes sense. All in all that would do the same thing.
Give this a quick try: SELECT DISTINCT * FROM a INNER JOIN b ON a.batch = b.batch INNER JOIN c ON a.batch = c.batch WHERE a.batch = '123';
Glad to see you figured it out. Any reason why you have to dynamically build the sql statement in this case? I’ve found building sql strings to be a real pain to maintain.
Ill check that out, I didn't know it changed the order of the tables around during the joins. I was really hoping for a much more procedural hookup, that way I could manipulate exactly what data I need to go in appropriately. The tables I am shuffling this down to are going to be sub 200 rows, maybe I could subquery or query to a temp table?
Yeah I guess I should have said what should be 308 columns based on the data elements on the word document with the fixed length info
Not quite following what you’re trying to do. Sounds like you might want to create multiple recipes and then save only one of them later on? What if you create a recipe_holding table that stores the temporary recipes and once you decide on one of them, insert it update that record into the main recipe table?
adding DISTINCT is not going to solve it there are 1 * 4 * 4 = 16 different combinations, and they are all unique
Yup. Just got done trying it. And a few other ways. I don't see a solution for four results.
That is a lot of data. Are you sure you want it all in one table? It might also make sense to think about how you might normalize it, and use text manipulation tools to get it into shape before importing into your table or tables.
i feared my explanation might be lacking imagen the following scenario, i have 10 recipes in the tables as described. The person planning next week's menu picks 3 of them but has/wants to adjust them slightly BUT just for next week. The base recipes remain the same but the "changed" ones need to be saved somehwere and fixed so the calculation what you need to buy. These "changed" recipes belong to the MENU of that week and the calculation of prices and vendors is done. (those prices and vendors come from the Product table) Does this make i a bit clearer? 
There are hints you can place in the query, you can do 'apply' type joins and/or subqueries that cannot be moved by optimizer, etc. The data changes though, so most of the time you're better off workign with optimizer, not against it.
Ok, so you're saying it's fixed length? Use the word document that has the columns, types, and lengths, as a reference for a bcp format file (you can google how to use bcp with format files). But honestly, I highly doubt going down this route is going to be beneficial. You are most likely not going to correlate this data with anything else and are simply just looking for a simple way to report on what's in the file. It would be less work to import the data into ElasticSearch using logstash with a grok filter for your fixed width delimitation. Once it's in there, it's very easy to report on with Kibana... no SQL proficiency required. Forcing it into SQL Server is going to be extremely time consuming and -very- frustrating, and only worth the effort if you're also going to modify the source to start using SQL Server -- or you're going to invest into a robust ETL solution to automate the process. Lots of "why" questions are popping up with this one... 
I usually don’t deal with this we got data this way it’s usually in excel format or csv 
Yes, that is clearer. I’m thinking you can pull the 3 recipes into another table. Tag some kind of unique ID to these records so they don’t conflict with other temporary recipes by other users. Make the changes accordingly to the temporary recipes and calculate prices. Probably put a date field in there so after that week you can deactivate or delete the temporary recipes. Also attach the menu ID to these records. The base recipe tables remains untouched. 
For a "sandbox" or test environment I would: Install docker and follow [this guide](https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker) This enables you to spin up temporary instances of SQL Server with the intent of testing. What you do with it after that is more complicated. You'll want to research tutorials or sign up for some computer based training. Microsoft even has free "Virtual Labs" for SQL Server that don't require you to have your own environment: https://technet.microsoft.com/en-us/virtuallabs?id=IMUmyf7VAbE
While this is little strange, I would certainly not be telling Op to quit his/her job or change roles. That's pretty harsh. I don't know what magical shop you work in, but I get problems an order of magnitude more insane and we solve them. Your foot in the ground crap is what typically pisses me off and it's also that attitude that has benefitted me significantly. I've seen 10-20% raises every year in my ten year career because I solve shit problems like this with out hesitation. Op SSIS, or power shell, if you have to do this all the time with the word doc and txt files maybe look into writing a console app in C#. All of this is doable. 37 gigs is going to take time to process but batch over night and it won't matter.
Thought I did say fixed length in the original post. 
If it has to be in the database, you're stuck with using any of the mentioned methods (bcp, ssis, wizard, etc), but there is no magic to take the metadata you need from the word doc. You'll have to transpose that by hand into a format file or refer to it when manually setting the column widths within the wizard or in SSIS. Either way, you're looking a lot of tedious work. However, it's a lot less work with bcp. 
Then what would you do if someone gave you 6 gig txt files based on years. that were from and EHR system and need to run reports off them for market research. 
It doesn’t have to be database but that’s the first thing that came to me to actually view the data since nothing seems to want to open a 6 gig txt file without delimiters. 
Since we're now in a pissing contest... my 20 years just pissed on your 10 years face. Not sure what relevance measuring our experience like porn cock is, but whatever... It's obvious that the OP doesn't have the experience or technical aptitude with the platform to solve this "shit problem" that you could do without hesitation. Sorry if I've pissed you off with my attitude, and I do hope that it lands you another raise! 
If all you're doing is "limited to creating new tables and running queries", then you have a few options. You can download the developer edition of SQL Server and SSMS for free. https://www.microsoft.com/en-us/sql-server/sql-server-downloads If you don't want to install MSSQL on your local machine, you can always download a developer virtual machine from Microsoft and install MSSQL on that. I don't have Windows 8 or higher, so I can't install the latest versions because of min OS requirements. But, my machine will still run VM's well enough. So, I'm using these VM's to do some training.
I would request that the source that gave me the data would export it into a format more easily consumed. If that's not possible, I would be forced to invest my time into designing an ETL solution to handle it. If it was beyond my capabilities, I would ask my employer to hire the personnel with the skill-set and experience to complete the task. As I mentioned, BCP is probably going to take the least amount of work for you.
We did that this morning but they didn’t seem to happy about it so these txt files might be all we get. 
OP, u/r3pr0b8 gave you a great answer to your question. But, I think you may want to consider why you're doing it like this. Your tables b and c technically should be consolidated to a single table to prevent the scenario that you're asking about. On the other hand, if tables a, b, and c are supposed to be arbitrary examples of actual tables that you're working with and can't change, then something is missing from your example that may help resolve the problem such as additional columns that would allow you to make the joins more selective.
Wow dude. I wasn't pissed you just gave pretty crappy advice. And I could care less how long you've been in the industry. I gave my years of service as a frame of reference. Calm down. This isn't like the hardest thing to do. If op has some programming experience which all sql devs I work with do, it's maybe a day or two of coding to build a simple app to process this. 
Maybe it's crappy, or maybe it's the best advice. The OP could be in no-win situation. Let's assume some of the files are jagged and will never import at all. Should the OP burn weeks of time asking for help or trying to develop their own software to handle a problem? I don't think there's an issue with saying, "This is beyond me and I can't make it work". OP has also mentioned that the data is Electronic Health Records. Lots of compliance to consider and whether or not the OP can make the call to ignore certain data for the sake of a successful import. Just seems like a really bad situation to be in if you don't have the necessary experience. Many people have already given the advice and mentioned the tooling and methods to try. At this point, short of doing with the work *for* the OP, I think the convo is pretty much done.
Thanks! Yes these are arbitrary tables. They're Dynamics GP tables that I can't change. So I can't consolidate anything unless I copy this data to a new table.
Thank you so much for your help! For the ColorLKP, the ColorID would be a primary in Shoe right?
Are you saying you have never worked with a flat file before? He just doesn't want to have to manually enter the column header information so he's asking for an easy way to do it. He has the column definition. This is very common.
my guess would be that 'strongly correlated subquery' means that you use columns from the current record set context? Basically, have a 'where' clause in it with some conditions using your columns from your main (outer) query.
Thanks, I pulled my head out of my ass and figured it out. 
 FROM StatProfile PRO INNER JOIN StatDimensions DIM ON PRO.DimSlot = DIM.DimSlot AND PRO.IntervalStart BETWEEN @StartDate and @EndDate
Awesome, got the Virtual Lab working - thanks!!!
Long story.
This seems promising: SELECT *, c.[Row Number] - Row_number() OVER (partition by c.[Company] order by c.[Row Number]) as [CompanyGrouping] FROM dbo.[Your Company Table] as c I think you need an uninterrupted sequence in your c.[Row Number] column for this to work, but even if you do not have one, you can use a CTE to use the Row_Number() window function to create one. The Row_Number() window function is going to look like this given your data set. I am on my phone, so I will take some liberty here: Row Number Company NewRN CG 1 Company A 1 0 2 Company A 2 0 3 Company B 1 2 4 Company A 3 1 5 Company B 2 3 6 Company B 3 3 7 Company B 4 3 8 Company A 4 4 9 Company C 1 8 10 Company D 1 9 11 Company A 5 6 NewRN would be the isolated Row_Number() function call in the expression whereas CG is the full expression representing what I named [CompanyGrouping]. While not a perfect order, I think this gives you a unique grouping that represents what you were trying to achieve and it does it in a set based way. The math will create group contiguous clusters. If you really want to get an order similar to your desired ranking, you could chain a CTE on top of this and use MIN(c.[Row Number]) OVER(Partition by [CompanyGrouping]), which will allow you to order it sequentially. If you also need this to be an unbroken sequence, you could add another CTE to dense_rank this MIN value. 
Is it a flat file?
I have no idea of the complexity of your software, but 10 weeks migration for SQL Server to Oracle seems to be calling for a disaster (unless it's a trivial app, of course). Management needs to sit down an evaluate the options: 1. Back to SQL Server 2. Forward with Oracle 3. Migrate to something other You cannot reasonably expect an answer to that question from the community here. I've added another option because I thing the effort you need to put into finishing the Oracle migration might be high enough so that another database (e.g. PostgreSQL) might also be an option (especially when you consider the long term license cost). However, that third option really depends on how much the already invested 10 weeks are relative to the total migration effort (which you don't know yet). You should definitively *not* blindly follow the advice to inline the views into the queries. Although it might help in some cases, there are also many cases in which it doesn't change anything. Also for materialized views: The are mostly a DWH feature. Sure then can help in other scenarios as well, but don't do it blindly (they have drawbacks too). Obvious self plug: I'm offering services in this area: https://winand.at/ 
&gt; "You cannot reasonably expect an answer to that question from the community here." Of course. Fortunately I don't have to make these decisions. :) I'm more looking for suggestions on how to communicate bad news, and maybe "he don't forget you can bla bla" recommendations Nearly every report queries an Oracle standard view with aliased friendly field names
&gt; I'm more looking for suggestions on how to communicate bad news, and maybe "he don't forget you can bla bla" recommendations Just stay at the facts (IMHO). &gt; Nearly every report queries an Oracle standard view with aliased friendly field names That's not a problem at all. A problem **might** occur if the view does more than the query need and Oracle is not able to optimize the unnecessary part away. Other than that: an SQL Server indexed view (I guess this is what you meant by "covering view") is basically a table. Querying a table will be faster than doing the manual work in many cases. Although the materialized views are probably the closest feature on the Oracle side, I'd personally first look whether the query in question can be optimised manually.
&gt;making bad choices (missing NOLOCK hints etc) Missing? You should never add those unless you're querying system tables. You should instead use Read Committed Snapshot Isolation Level (which Oracle has on standard) if you're afraid of locking and user partitioning on your tables. Reading uncommitted data is *bad* architecture everywhere and no other RDBMS even allows it. &gt;How do I communicate to a manager, that an arrogant short sighted VP screwed up? How to best explain this bad news? Never mention the VP, state the bad news as best as possible, make a list of all the things. They'll follow the stink back to the source themselves and you won't have bad mouthed a person. &gt;Should we roll back and suck up the 10 weeks and licensing? This can't be done in 10 weeks. &gt;Should we forge ahead with Oracle? If so, any tricks to suggest? Tell your Microsoft representative you're going with Oracle, they'll probably give you a sweet deal on those licenses. &gt;materialized views Are just about the same as Indexed Views in MSSQL
Our views are typical / stereotypical SQL covering views Select fieldA as friendly_fieldA, fieldB as friendly_fieldB and so on from table as viewname_table That's it I mean yeah we also have views that do more complex stuff but we use aliased covering views in lieu of tables. And we have query plans that are randomly like "mm, nope, just gonna table scan now" .. because of SSRS to Oracle problems, not db server side problems that execute against those covering standard views.
Oh heck, you are using SSRS to access on Oracle DB? Didn't even know that works...well seems like it doesn't :/ As far as I can tell from what you say here, the views should not be the problem. 
I want to rearchitect the entire db because there are tons of tables with no primary keys and many more with nonclustered primary keys. Everything is so static and dodgy. But anyway, this is why we use NOLOCK everywhere. We run simple replication, with realtime results, copied from transactional data, into tables with millions of rows and no clustered PK, then read from those tables for reports at exactly the same time. (Oops) So we need NOLOCK everywhere. Plus I want to report on the confirmed order not the almost confirmed order. So I'm ok omitting unfinished transactional data. *** I'm with you I think we should refactor reports at this point but it's a huge undertaking. I feel like index hints on the covering views is pointless. The greatest offender is a table 100+ fields wide, hundreds of millions of rows long, nonclustered primary key. The covering view is a straight select statement, with aliased names on pretty much every field, different ordinal rules (alphabetized by view field name). We have hundreds of reports that interrogate that on different string lookups, date, monetary, .. sometimes we can inner join to a dimensional table but a lot of the queries involve table scans. So while we had it tuned for MS now we've got hella table scans in Oracle and SSRS reports randomly crash. Should we just use Oracle stack reporting systems and migrate our reports? The only other solution I can think of is edit every report to use the right index as a hint at the start of your select statement. This is doable seeing as how a lot of reports are duplicated across multiple environments
&gt;So we need NOLOCK everywhere. While you might think you really do, you don't. Read up (or don't since you're migrating to Oracle anyway) on read committed snapshot isolation level. I just finished implementing this for a large company handling huge amounts of live exchange data. Also using replication, also needing multiple readers and realtime results. The execution engine in Oracle is also smart, but has different criteria than MSSQL for using indexes vs a table scan. Usually it's just a statistics thing, are those up to date in Oracle? I wouldn't recommend hard coded index hints unless you can really see a big difference in the execution plans. Does your company have any Oracle DBA's? They should really be helping in this migration.
Get some certifications? That at least shows you're proficient in using tooling and sql from whatever vendor the cert is for. Pick a real world business problem. Time tracking, inventory, payroll, scheduling, asset management, etc. Build something to solve it. They all have pitfalls that are easily overlooked until its too late and you gotta redo a bunch of your work. Its a good way to practice for freelance work and develop a portfolio.
Oh sure it works! :) (Except for when it doesn't) To be fair it's not definitely the views that are problematic it's definitely the SSRS engine randomly generating and fixating on bad query plans. It's weird man. Queries run fine in Toad or SQL Developer. They run fine when testing Query Designer in SSRS. You upload to the SSRS server, click on the report and the report sucks and the query plan suddenly does full table scans for no reason. iono .. it's weird.
I mean them today for the first time.
Depends on what you want to do as freelancing. There's a difference between developing SQL as DBD or BI reporter or maintaining SQL as DBA. Being able to write SQL is just half the code. Writing "performant code" is hard, you should be able to debug code, creating and effectively using indexes. [1](http://use-the-index-luke.com/sql) Anyway, you will never *feel* ready to do this. So my answer is, just do it and learn on the fly.
Yes
&gt;some real freelancing work I was under the impression that freelance work is limited here. Most places looking for that will just outsource it to India. If you think you're ready go talk to an IT recruiter and try to get a full time gig writing SQL. Ready to me is stuff like INNER vs LEFT JOIN and how GROUP BY works, and the difference between WHERE and HAVING
when you spell it beginner /s i found it was when i began consistently helping others with their asks. imho im always a beginner. there are always people ahead of me and being hungry is s better mindset. tbis is why i hate the bullshit gamification people put on their resumes now like sql: 85% python: expert java: statusbar
&gt; I have no idea of the complexity of your software, but 10 weeks migration for SQL Server to Oracle seems to be calling for a disaster (unless it's a trivial app, of course). I've schedule a server upgrade for that amount of time (including testing iterations of course). OP's VP is apparently attempting to put their company under.
Sweet I'm not a beginner! I like you! Real talk, I'm a total beginner. I'm scared of things like subqueries, certain formatting, and as Gen says above debugging, etc.
&gt; I was under the impression that freelance work is limited here. I feel like freelance work with SQL is extremely limited, but it could just be the location I'm in. The issue with SQL is that we are professionals holding and touching someone's data. In this age, data is going to be a huge commodity going forward. Would you trust someone who learned how to query from a website for a few months handle all of your payroll data for the company? How about ensuring all of your patients records are secure? Would you trust that they could recover your data in the event of hardware failure? Can they write SQL code for the applications that will scale, maintain over time, be easily readable and fixable by another individual later, and not break your database? There's a reason there are few to near no junior jobs in SQL minus development, and those are scarce. Data is big stakes. You can hire someone to write a crappy website, scrap it, and be out a few grand. You hire someone who crappily maintains your data, your business is gone. [Ozar's take](https://www.brentozar.com/archive/2009/04/how-to-get-a-junior-dba-job-part-1/) [Another](https://www.brentozar.com/archive/2016/01/get-first-full-time-dba-job/) [A post that I mostly agree with and how it ranks levels of skill in SQL](https://softwareengineering.stackexchange.com/questions/181651/are-these-sql-concepts-for-beginners-intermediate-or-advanced-developers)
If you know how many numbers users can chose from, a drop down list would be better I think. You can save your concatenated string as a variable and include the variable in your query directly.
&gt; Anyway, you will never feel ready to do this. So my answer is, just do it and learn on the fly. I agree completely. Although when you become exposed to more and more environments, you'll become more comfortable. Seven years and whenever I enter production to do work, my heart rate still goes up.
I'm glad you got it! If you need any help on this project, feel free to PM. I also feel like for posting questions that complex here, it may benefit you to start a git and upload scripts and then people can be granted permission to assist when you share here. Bonus for you is exposure to git which is a handy skill if you don't have it already. 
Not a bad idea. I'm actually redesigning the process to work similarly to what we discussed, and then implementing the dynamic SQL from there. So at the moment I have a few mapping tables such as: CalculationID, ClientProfile, ClauseID, then ClientID, ClientProfile, then CalculationID, CalculationDescription. So instead of passing just one field I'm going to pass larger chunks of the query which at a later date can be modified to be combined with the base SQL if it ever gets rebuilt in SSIS. Current plan is to create a single SPROC LOOP per Calc, then one SPROC that triggers each child and functions as a "Global." Lots of fun. Thanks for the offer.
Stop thinking of a subquery as a subquery.... pretend it's just another view you're plugging into the where clause. But instead of using the view name, you're putting the SQL in. It certainly looks more imposing, but logically they're the same thing.
Ok, I'm with you...make my control a drop-down that includes all possible Pay Groups, and pass that to the SQL SELECT statement. I'll test it today, thank you. Hopefully no problems using the control as a field name. I'm using it within the WHERE clause without issue elsewhere in the database, I imagine it's the same. Thanks again!
Yes just like that! Share your query if you are having issues with syntax. I recently went through something similar, passing a int variable to the sql is easiest syntax I found. So if int dropDwnValue = dropdown.value Then query is: ("SELECT * FROM [X] WHERE [Y] =" + dropDwnValue) Research sql injection also, if you are ever considering using a free text field to pass a value to a query.
Here's code I have in a sproc that checks for which node it's on. DECLARE @rc INT; SET @rc = ( SELECT count(*) FROM sys.availability_groups_cluster AS AGC INNER JOIN sys.dm_hadr_availability_replica_cluster_states AS RCS ON RCS.group_id = AGC.group_id INNER JOIN sys.dm_hadr_availability_replica_states AS ARS ON ARS.replica_id = RCS.replica_id INNER JOIN sys.availability_group_listeners AS AGL ON AGL.group_id = ARS.group_id WHERE ARS.role_desc = 'PRIMARY' ) IF @rc = 0 BEGIN --Not Primary PRINT 'Not the Primary'; DECLARE @name NVARCHAR(128); SELECT @rc AS Node_Status ,Getdate() AS RunTime ,'Exited' AS Job_Status END; If you do the dest / path, make sure it's network based. You'll probably need to declare and set the name of the server and build that path dynamically. To backup to a different server: \\Servername\c$\backup\
Sounds like a solid plan. When doing something like this, I like to write it out and whiteboard it. Then I like to think about it, take a break, re-analyze it, etc. Question it, question it all, think about it. - Can I maintain this going forward? - Am I documenting this as I go along? - Could someone look at this without documentation and figure this out? - Am I notating my procs clearly and are the fields and table names and proc names appropriately named? - Does a diagram showing how it works make sense? - What if I need to change something, how easy is that? Asking these questions in the design phase and also having a well thought out design document and having people review it could be a life saver down the road.
That or subquery I guess could work.
In your case, I'm guessing that script is actually kicked off directly on the secondary or on all nodes simultaneously? In my case, the job is running on the primary, but only the backup portion should run on the secondary, and because the backup has to happen at a specific point in a process running on the primary (which can vary by several minutes or more depending on workload and the day), I can't just schedule it. The problem isn't so much 'writing a sp that makes backups only on the secondary replica' as 'writing a sp that runs only on the primary replica but can kick off/wait for completion another stored procedure/backup job on a specific secondary replica. Basically the use case is: big DB offsite in production, number crunching happens at the end of day, need to take a backup between some sets of number-crunching and that backup needs to happen at another physical site where the file can be put to LTO6 tape for long-term retention. Currently we use rsync/cron for the backups (we just duplicate the previous day's backup on the other side and run rsync against the new full backup, which reduces the copy time by ~50-70%), but it would be much cleaner to just have the backup occur on the replica at the site with the tape drive directly.
Dumb question and I won't be looking at this until next week, but if I define a @var in sproc_1, can it be passed into sproc_2 if sproc_1 is firing sproc_2?
&gt; In your case, I'm guessing that script is actually kicked off directly on the secondary or on all nodes simultaneously? It's ran on all nodes, it's just a piece of my main script as I did not want to post its entirety, but it essentially kills the job if it's not the primary because it will error out otherwise. &gt; 'writing a sp that runs only on the primary replica but can kick off/wait for completion another stored procedure/backup job on a specific secondary replica'. To me, that sounds like daisy chaining procs or jobs, or use of triggers, or possibly resource governor. Maybe all three? http://www.itprotoday.com/microsoft-sql-server/altering-execution-priority-using-resource-governor
If sproc_1 is just: Exec sproc_2 sproc_2 will not have any inclination of your @var. If your sproc_1 is like this: Exec sproc_2 @var Then it will use the value you passed it. There are two other ways you can possibly do it depending on how you write this and design it. You can specify with OUTPUT or you can use RETURN calling the variable. https://stackoverflow.com/questions/2996277/how-to-use-a-value-from-one-stored-procedure-in-another 
Also random note, for the dynamic bullshit I've had to write before, I found this really helpful making the procedures do several things. 1. Having a parameter that is @Test or something. Essentially it executes print statements, not the dynamic SQL. Fantastic life saver and easily helps troubleshooting. 2. Log it and have the setting to lower or heighten logging similar to the @Test variable. 3. Documentation and design docs are your friend.
The answer is specific to your environment. We can't tell you if you have jobs running powershell that will or won't break. Assuming you haven't touched many things with powershell, I would say it would go relatively smoothly. 
Ah, so we could have the stored procedure designed to make sure it only runs on the secondary replica, then use a trigger so that it kicks off when the main stored procedure does some kind of 'update' operation to note that it's ready for backup, then have the 'backup' stored procedure that was triggered update another value that the first process looks for before continuing? 
That's definitely a way to do it and pretty much what I'm suggesting. I would test the hell out of it before implementing it and seeing how it can break or if any unintended behavior occurred. I haven't had to do that before, but that's what goes through my mind first.
I am not worried about the powershell side. I am worried about some sort of SQL thing i am unaware of. I have a feeling it is a 2 minute job but i just wanted to make sure i wasnt going into blind. 
If you have jobs in SQL using powershell, that's what I'd be concerned about.
yes~!! when a subquery is used in the FROM clause, it's called an **inline view** or **derived table** -- because that's what it does, it provides a customized table (the result of the subquery) to the FROM clause just like any other table
Doesn’t matter; fake it til you make it. SQL problems are ALWAYS google-able!
I will double check this. Thanks
Could you theoretically do something such as: exec sproc_2 @var, @var2, @var3 I'll check your links out next week. Thanks again.
I've been doing that in my head for a week now, and ironically doing it a lot while I sleep. 
I have nothing useful to add I'm afraid, just driving by and I couldn't believe what I was was reading. This is beyond shocking and I'm blown away at the insanity of moving from MS to Oracle to save on licensing costs lol. I mean, I'm sure that's not news to anyone here but wow. Just wow. Sorry for your situation. It sounds supremely unpleasant. 
Even if you're not using a "free text field", this pattern is vulnerable to injection. Parameterized queries. Every day. All the time.
For now I'm just looking for recommendations. And to validate whether or not what I heard was true. The notion of using standard covering views on every table, with a non-dbo schema for the views .. I mean, this harkens back to the 70's doesn't it? Developers and DBAs manage the tables and indexes and giant sweeping changes to structure are invisible to users because they just access views. I thought that was really just best data practices 101. The DBAs I sat with yesterday were like "no you run the risk of using different query plans" .. really??? If yes then this is seriously big and bad news and we need a plan now. Yesterday. Before the holidays hit. I'm hoping that the DBAs are wrong but they're more knowledgeable about Oracle than I am. So I'm asking the internet (while trying to be vague enough to not give my employer away)
You could, but once you are in sproc_2, you can't reference those variables outside of it. So you'll need to declare variables in sproc_2 and assign them values passed.
Yeah, the goal is to have a smaller set of data since there are so many joins. These tables are massive, I work for a huge company and the phone system we use stores data in a crazy way so it creates more entries than necessary. Im trying to limit the data as much as possible pre getting them to be joined together so I don't waste time joining on records I know I am not going to need in the first place.
the only reason the counts would be different is NULLs can you confirm this is what you're looking for?
you want this instead of a WHERE clause -- FROM TABLE_ONE A LEFT OUTER JOIN TABLE_TWO B ON B.PLAN_KEY = A.PLAN_KEY AND B.STATUS &lt;&gt; 'C'
Uhm... I don't think a running total is what you're needing. You just want a sum of credits for the current term.
Yes, sorry, I should have specified that's what I'm doing.
Thanks for your reply! My SQL is not that strong. Any pointers on how I achieve sum of credits for the current term? 
nevermind... t2.TERM &lt;= t1.TERM) AS RunningTotal seemed to do the trick. Thanks for pointing me in the right direction. Much appreciated!! 
Thanks! That seems to have done the trick. I assume this means that the purpose of the original statement was to limit the table prior to joining, rather than limit the end results?
https://blog.jooq.org/2017/04/20/how-to-calculate-multiple-aggregate-functions-in-a-single-query/ 
something like that ;o) an outer join's ON clause says "try to find rows that satisfy all of these conditions and return NULLs in those columns when no such row exists" by leaving one of those conditions in the WHERE clause, you are effectively changing it to an inner join, because there will never be an unmatched row where the NULLs will satisfy that condition
 SELECT COUNT(field1) AS count_field1 , COUNT(field2) AS count_field2 , COUNT(field3) AS count_field3 , COUNT(field4) AS count_field4 FROM yourtable HAVING NOT ( COUNT(field1) &gt;= COUNT(field2) AND COUNT(field2) &gt;= COUNT(field3) AND COUNT(field3) &gt;= COUNT(field4) ) then the answer is 
&gt; n uninterrupted sequence in your c.[Row Number] column for this to work, but even if you do not have one, you can use a CTE to use the Row_Number() window function to create one. &gt; The Row_Number() window function is going to look like this given your data set. I am on my phone, so I will take some liberty here: Thanks :)! This worked perfectly.
First step to troubleshooting: format your query so you can read it. select art_nom , mag_loc , art_coul from magasins , articles where (select mag_loc from commandes join lig_cmd on cmd_num=lcd_cmd join magasins on mag_num=cmd_mag where mag_loc like 'casa%' ) ; A few issues here. * Don't mix comma joins and explicit `JOIN` syntax. Just use `JOIN` it's the new/best way. * `WHERE` clauses need an expression. Your first WHERE has none (just a subquery). What are you trying to compare here? Basically SQL is asking you: WHERE *WHAT*? I am guessing you need something like `WHERE mag_loc IN (your subquery here...)` * Consider aliasing your columns and/or tables so you can avoid confusion, especially when you are using the same table/column names multiple times in a query for different purposes. Hope this helps!
lol
I'm afraid I'm more of an MS guy myself. All the Oracle work I've done has resulted in me wanting to tear my hair out and wonder why even the simplest things seem difficult. I'm also more of an architect. Covering views seem reasonable to me, depending entirely on what you're trying to achieve and they don't tend to get in the way of query plans (on sql server anyway). Doing a mass migration to Oracle just sounds like a very expensive move, and the fact that you have no reason to do it other than to 'save money' on licenses is just something I can't get my head around at all. Everything about it is more involved. Support is more expensive, developers and DBAs are more expensive, licenses are more expensive, and unless you're a java house... Everything else your doing won't play with it as nicely as sql server. Not to say Oracle doesn't have its place, but it's place, certainly these days, is large scale back ends that have to be super robust for companies that have a lot of money. It's just such an expensive option I don't see why anyone would do it if another rdbms would meet their needs. 
&gt; But I'm not seeing AS AS is an optional keyword the `a` alias is right after the subquery's closing parenthesis before the GROUP BY
It's very easy to miss, but look after the last parenthesis in your example but before "GROUP BY 1,2 ORDER BY 1,2. See the "a" there. That "a" is aliasing the subquery in the FROM clause so that it can be referenced in the SELECT clause. When you put a subquery in the FROM clause, you are basically asking SQL to treat the results set of that query as its own table - but that table has no name! So you need to add an alias after in (in this example, "a") so that you can reference the columns coming from that subquery in SELECT, JOIN, WHERE, or GROUP BY clauses. 
^ Great tips here OP. 
'a' is a so-called "table alias" that can be given to any part (operand) of the "FROM" clause. It is a bit confusing since a result of any query is called a "derived table" and you can use other query result sets in the "FROM" clause. Sometimes these are also called "subqueries". In your example, this internal result set: SELECT dep_month, dep_day_of_week, dep_date, COUNT(*) AS flight_count FROM flights GROUP BY 1,2,3 is treated as an input ("derived table") to your outer query and it is given a "table alias" A. 
 SELECT a.dep_month, a.dep_day_of_week, AVG(a.flight_count) AS average_flights FROM ( SELECT dep_month, dep_day_of_week, dep_date, COUNT(*) AS flight_count FROM flights GROUP BY 1,2,3 ) a /* a is right here*/ GROUP BY 1,2 ORDER BY 1,2;
&gt; This can't be done in 10 weeks. &gt;Tell your Microsoft representative you're going with Oracle, they'll probably give you a sweet deal on those licenses. /u/johnwalkersbeard this is probably the only two bits of advice you need here.
thanks for confirming my reply
Oh wonderful! Thank you so much, I had no idea that all that was going on or needed :) Thanks for your help and comprehensive explanation! 
I would say you're preaching to the choir and I would also hope this doesn't devolve into a MS vs Oracle debate like Win vs Mac I'm trying to view it positively as an opportunity to expand my skills and marketability. But yes this migration has been humbling in that me, a senior level developer, am having to ask for help with date parameters, finding SPIDs (or SIDs for Oracle apparently), generating and reading query plans (or explain plans), index hints and so on. I feel like the migration to save MS licensing was penny wise and pound foolish. We'll have to pay Oracle licenses eventually, and aren't they more expensive? A variety of reports randomly poop and I'm swimming in tickets now .. 13 on a Friday when I usually have 2 or 3 I think the greatest frustration as a BI person isn't just, how much money are we spending on development and support compared to licensing savings .. but also, how much money are we NOT making, by focusing on post migration production support, rather than crafting some big fancy new toolkit just in time for our customers big holiday push. That's the bigger frustration. I know there's better things I could be doing other than trying to rapidly ramp up to a senior level of skillset commensurate with my title, while ops guys bug me every half hour "is it fixed yet? Got another one for you.." Like I say though, I'm just trying to find universal fixes to this problem of SSRS reports and their underlying queries randomly dropping their decision to use an index and instead perform multiple full table scans on our biggest tables. A colleague pointed out that it could be related to a case insensitive index on date field, being in the wrong schema. So we'll try that, perhaps. Case sensitivity? Really? Okay.
Thank you for your help! :)
Ah! So is there a reason a person would or wouldn't use AS?
Personally I use "AS" when aliasing columns in the SELECT clause and don't when aliasing tables in a where clause. 
Appreciated. It's done though. I've heard through the grapevine that VP guy just doesn't like MS anyway. Like, in principle. The migration from MS to Oracle on 10 client sites in 10 weeks already happened. And it was grueling. 60-80 hour weeks for two and a half months. There's just no way we're migrating back though. I'm sure of it. Plus, it's mid November. We could never meet the same timeline. Holidays and that end of year mad dash to "use it or lose it" and burn through PTO.
this is consistent with Oracle syntax, which *doesn't allow* the AS keyword on tables
Salary and location? 
DC metro area. I don't know for sure salary. It depend on skills. I know other SSE make about 140K
In your defense, you don't start to see aliasing used like this until you get past basic SQL.
If both parameters are required, why allow them to default to NULL? Also, what should happen if neither parameter is supplied? And do the parameters need to be OUTPUT parameters? SQL runs start to finish to you should raise an error before the SELECT query. If you combine this with TRY - CATCH, raising an error will jump the script to the CATCH statement: ALTER PROC orders_by_dates_sp ( @startdate DATE = NULL, @enddate DATE = NULL ) AS BEGIN TRY IF ( @startdate IS NULL OR @enddate IS NULL) BEGIN DECLARE @errMsg VARCHAR(200); IF @startdate IS NULL SET @errMsg = ISNULL(@errMsg + ' ', '') + 'Please provide start date.'; IF @enddate IS NULL SELECT @errMsg = ISNULL(@errMsg + ' ', '') + 'Please provide end date.'; RAISERROR(@errMsg, 11, 1); END; SELECT o.OrderID, CONVERT(VARCHAR(12), o.OrderDate, 107) OrderDate, CONVERT(VARCHAR(12), o.ShippedDate, 107) ShippedDate, od.ProductID, c.CompanyName FROM Orders o INNER JOIN OrderDetails od ON od.OrderID = o.OrderID INNER JOIN Products p ON p.ProductID = od.ProductID INNER JOIN Customers c ON c.CustomerID = o.CustomerID WHERE ShippedDate BETWEEN @startdate AND @enddate ORDER BY ShippedDate; END TRY BEGIN CATCH DECLARE @ErrorNumber INT = ERROR_NUMBER(); DECLARE @ErrorLine INT = ERROR_LINE(); DECLARE @ErrorMessage NVARCHAR(4000) = ERROR_MESSAGE(); DECLARE @ErrorSeverity INT = ERROR_SEVERITY(); DECLARE @ErrorState INT = ERROR_STATE(); RAISERROR(@ErrorMessage, @ErrorSeverity, @ErrorState); END CATCH;
I think you're going to be forced to do multiple queries, whichever way you tackle it from since you have the two date columns being used. I wonder if putting the first dataset into a form of temp table and then count off that with multiple queries would consume less resources on the server?
If you are using sql server you can write select top 20 percent * from...
 SELECT anydate , COUNT(CASE WHEN type='o' THEN 'humpty' ELSE NULL END) AS num_ordered , COUNT(CASE WHEN type='d' THEN 'dumpty' ELSE NULL END) AS num_despatched FROM ( SELECT 'o' AS type , date_ordered AS anydate FROM mytable WHERE date_ordered IS NOT NULL UNION ALL SELECT 'd' , date_despatched FROM mytable WHERE date_despatched IS NOT NULL ) AS u GROUP BY anydate 
Here is a rudimentary work up for you.. create table #tmpDates (date_ordered date null, date_dispatched date null) insert into #tmpDates (date_ordered, date_dispatched) values ('11/11/2017','11/11/2017'), ('11/11/2017','11/15/2017'), ('11/12/2017','11/15/2017'), ('11/12/2017',null); with disDates as (select date_dispatched uniqDate from #tmpDates union select date_ordered uniqDate from #tmpDates) select disDates.uniqDate ,count(ord.date_ordered) totOrders ,count(dis.date_dispatched) totDispatches from disDates left outer join #tmpDates ord on disDates.uniqDate = ord.date_ordered left outer join #tmpDates dis on disDates.uniqDate = dis.date_dispatched where uniqDate is not null group by disDates.uniqDate drop table #tmpDates 
In SQL, records are conceptually same- ish (they share the same metadata) while columns are distinctly different - each has it's own metadata even if it is the same type. You are trying to create a common/ single granularity from 2 different columns so you will need to un- pivot the data first. You can do that by any number of methods, some of which have been given by prior answers. 
Thanks u/r3pr0b8 ... this works exactly as I needed. Theres a lot more involved than I thought, I'll need to check through this to understand it properly ... but a quick implementation is working perfectly. Cheers!
Thanks u/ichp, I've realised there was more involved than I originally thought. I've got a working solution now from the answers given
Thanks for taking the time to write this up u/phuque_ewe, much appreciated. My limited sql knowledge means I'm struggling to follow and implement, but I'll learn more from working out how this works! cheers.
Thanks for the quick reply u/Ogoshi_. From the solutions posted I now realise there was no 'simple' method I was overlooking.
In my opinion, use the first one because it is clearer. Read it: "I want the things from TableA that aren't in TableB." I'd use the second if there's a situation where the performance is very much better, but I can't think of any, and I'd comment the hell out of it.
Replying to myself, under your condition where TableA and TableB are identical, in Oracle at least this would also be clear, though depending on the indexes and row count and such this would probably not be as efficient: select * from TableA minus select * from TableB;
In tsql you could use “except” in place of the minus
You could use a full outer join instead of combining results of 2 left outer joins.
The first one is more efficient. Let me break it down for you: In example two, you have to query the full set of both tables. The results are received and then you are excluding the nulls from your final set. The two joined tables *probably* dump into memory so that the where clause can eliminate the nulls from the final results (not smart enough to know for sure how this works). In the first example, you have your data from table one, and even if table two is just doing a serial read (table scan) and it comes across your job criteria in the second row, that's it, it's done. It had found that the record existed and thus did not meet your criteria and its on the processing the next row of data from your main clause. tl:dr - ex. 1 is faster because you don't have to read the entirety of the second table whereas in the second example you would have to. 
You're right I could but I need to extract it out separately within our application that is tied to the database so I don't mind doing two queries.
Put SET STATISTICS IO ON SET STATISTICS TIME ON When I have compared both, the outer join wins on large tables.
Is there a difference between ex. 1 and a logically equivalent "NOT IN"? I believe there may be a blog post about this that I've read somewhere with tests done and, in a properly set up environment, they're all equivalent if you only actually select fields/columns from table A.
NOT IN is more limited... You can only use it with one list of values, whereas a NOT EXISTS can have whatever criteria a subselect can have. I have also seen the optimizer choose bad plans using IN and NOT IN under certain circumstances and was able to confirm with Microsoft. Given EXISTS is more powerful and has less issues than IN, unless I am writing quick and dirty ad-hoc sql, I generally just use EXISTS. 
Use whichever is more readable to you. I prefer the first one as the intent is more clear and it ensures that rows from TableA won't be duplicated.
I am not seeing that result. I tried on SQL 2014 and 2008 R2 with some very large OLTP tables. What order are you running these statements in? Are you accounting for the data being cached? Do you have exactly the same criteria in both queries? What version of SQL are you running? 
As noted, EXCEPT is the equivalent in T-SQL. This is indeed less efficient due to a key functional difference: with OP's examples, the comparison is done on a limited subset (the key column(s)), with the minus/EXCEPT, all columns returned must be the same order/datatypes and the comparison is done on all columns. 
Isn't the first one a "correlated subquery"? Shouldn't something like the following be more efficient? SELECT * FROM TableA WHERE TableA.id NOT IN (SELECT id FROM TableB) 
It will depend on table definition including indexes as well. 
I just wanted to let you know that I got 30/30 points on my test. Thank you guys for your advices you were very helpful :) https://imgur.com/a/fKFqH
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/UTVitkH.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dppb6aj) 
Run it both ways, capture time &amp; IO stats and review the execution plan. Depending upon the indexes, FKs, data volume &amp; distribution, you may get different results. There's no single definitive answer.
Can you share exactly under what conditions LEFT JOIN will outperform NOT EXISTS? If you are seeing this in an environment, please share as much detail as you can about your scenario so that we can reproduce it. In this world of subjectivity, one of the absolute best things about working with SQL Server over the last 17 years is that everything has an explanation. If you can't explain it, it means you just don't understand it yet. The way you understand something is by isolating the conditions that reproduce the behavior and then sharing that with other experts so they can also reproduce it. We are all scientists. I can say that I have never seen a LEFT JOIN out perform an equivalent NOT EXISTS in 17 years of working with SQL Server professionally. The vast majority of time, I have seen them perform the same or, in a minority subset, the NOT EXISTS outperforms the LEFT JOIN. While all the evidence I have points to your statement being dubious at best, I would love to learn something new if it can be proven. 
There is a difference and if I'm not mistaken an IN would process the entire table. I'm racking my brain trying to think of why it does that - the only thing I can think of is that query optimizer isn't smart enough to know that once you've received a hit not to continue. The EXISTS function is inherently different than IN. 
There's a couple responses below that cover how IN works. Hope they cover what you need! 
My hypothesis is that these results are unrelated to the differences between LEFT JOIN vs EXISTS. Instead, if we were dig into the scenario you used to come to this conclusion, my bet would be on this being attributed to general factors such as the data being buffered for one statement and not the other or the statements not being equivalent (e.g. the filter criteria is not identical). 
I would hesitate to claim this. My advice is always to test and experiment. Keep your tests in a reference library and improve them as your knowledge improves. Everyone can do this: write three statements with identical criteria using LEFT JOIN / NOT EXISTS / NOT IN and compare the execution plans and IO stats. You should be able to get them to be identical. In the case of the LEFT JOIN, it is important to return the exact same output (only table A). Repeat the experiment on large and small tables. They should still remain equal if equivalent. Where things begin to differ typically is in how developers use and write these. The problems I have seen with NOT IN were not simple scenarios - they involved a mix of factors that made it difficult for the optimizer to make the right choices. Specifically, I believe it was in a statement using a table variable in a join and for some reason, because table variables don't have stats, the optimizer chose a significantly worse plan with the NOT IN vs equivalent EXISTS. Not typical and very edge case. I mostly recommend EXISTS because it's just more powerful/flexible than IN it's easier to make optimal queries, and there are no scenarios I have found where the reverse is true. 
Keep in mind that SQL is a declarative language - you tell it what you want and the principle is that the optimizer decides how to do it. The caveat is that the optimizer operates on the principle of 'good enough': it has a finite amount of time to figure out the 'best' execution plan. The more complex you make your statements, eventually you will hit what I call 'the wall', where the optimizer can no longer figure out objectively what the best plan is in the time it has. Why is this relevant? Because for the most part, you can write NOT IN / NOT EXISTS / LEFT JOIN statements so the execution plans and IO stats are identical. Most of the differences you will see in the real world come down to how developers write their statements - they introduce inefficiency without meaning to. In my experience, it's just easier to do with IN / NOT IN because of the inherent limitations. 
As far as the "why" - you are correct, I do hesitate to say why because I can't remember. But as far as the fact that EXISTS is more efficient than IN, that is a true statement. 
In this case, it can result in an identical query plan and execution time. See above edit.
In this case, it can result in an identical query plan and execution time. See above edit.
In this case, it can result in an identical query plan and execution time. See above edit.
[removed]
This can be true, but for the query at issue the query plan and execution time can be identical if the fields used to match/not match are both NOT NULL in SQL Server. See edit above.
For a fairly simple query like this I will concede the point. Table size would play a large factor as well. 
Absolutely, exists is more efficient and flexible for logic not so simple.
I'm not 100% certain but I'm pretty sure he could also make the first example more efficient by only selecting the ID inside the IN because then it can just do an index table lookup and never actually even go to the table. You're just throwing that data away anyway. But perhaps the database engine is smart enough to optimize that case anyway.
We've been debating below about whether IN was as efficient as EXISTS. For a simple query like this I think there is no distinction - the EXISTS should be hitting the same index as IN. But for more complex queries, EXISTS is the better method. 
I am mainly speaking of this statement: "if I'm not mistaken an IN would process the entire table". I do know that to be true and, indeed, all the evidence I have is that this isn't true. There is nothing innate to the IN statement that forces it to "process an entire table". The optimizer is going to try to use an index if one is suitable.
Sorry, you are correct. It could apply to a table or index. What I meant is that it would have to process the full set, whether table or index, and not just stop like the EXISTS does. 
It's worth mentioning that the syntax EXISTS (SELECT * FROM ...) is not completely intuitive. It's funny - when you get into programming SQL, one of the first things you learn is don't use SELECT *, yet in this case the SQL Customer Advisory Team (CAT - which is made up of SQL rockstars) advocates _FOR_ SELECT *. The SQL Optimizer knows that EXISTS doesn't return output, so the EXISTS command "shortcuts" when it finds the first matching row by halting further matching for that row. It used to be that the SELECT * syntax helped the optimizer find the right index, but in all modern day versions of SQL Server, you can put virtually anything in the SELECT without impacting the execution plan. A lot of developers I see hardcode a number (like 1, 777 for EXISTS, 666 for NOT EXISTS, etc...) but I like SELECT * for consistency. In any case, that's why NOT IN isn't necessarily more efficient here, even though it looks like it might if you are trying to read into the syntax.
Agreed. I do advocate for a general practice of "pattern solving" vs "problem solving". In problem solving, a person simply tries to obtain a specific result from a given problem by creating a solution that meets the stated requirements. In pattern solving, the main concept is that problems are all just combinations of repeating patterns, so if you take the time to understand what _patterns_ makeup the problem you are trying to solve, and then you try to solve those patterns in the best way you can (via research and the scientific process), you can save your solutions to those patterns, building out a repertoire, then eventually problem solving becomes pattern matching, recall and tailoring - which is a lot quicker, produces higher quality solutions and, in my opinion, a lot more fun (because you are always solving the hardest problems). This entire thread is about pattern solving to me more than just problem solving (the original problem posed by the OP).
Is it possible to create ##tables in Sproc1, and have Sproc1 pass variables as we discussed to Sproc2, which then utilizes the ##table.... then come back to 1 and pass variables to Sproc3 which also uses the ##table?
Interesting, although I am still not sure _"it would have to process the full set, whether table or index, and not just stop like the EXISTS does."_. The only difference I could see is if duplicate values exist within the IN expression. To be honest, I don't know for **sure**, but I'd bet fake internet points that an IN with duplicates would shortcut the same way as EXISTS.
Agreed. I have so many queries to change. I love these times when I learn a better practice.
This I challenge: _"Table size would play a large factor as well."_ Show us this alone changes the execution plan in a meaningful way for a subset of constructs (IN / EXISTS / JOIN) and not the others. Table size most definitely affects performance, but I have yet to see an IN / EXISTS / JOIN behave differently based only on table size. When tables grow, what performance really comes down to is proper index usage and statistics, which are independent concepts from the discussion.
It might in a simple query (see other comments below that support that and have links) but the more complex it gets, EXISTS should be more efficient. 
I was in the midst of rebutting what you wrote when it became clear that I may be wrong in what I said in regards to this specific example. 
Are you sure that database is even still exists? Please put your code in the coding container like this So we can read it easier.
As far as I can tell from this script you're trying to drop a database that does not exist - as the message tells you. So that won't work. First create the database before running the script or skip the 'drop database' command.
I did like you guys said. But when I do SELECT * FROM muziekdatabase; it says: sg 208, Level 16, State 1, Line 1 Invalid object name 'muziekdatabase'.
It is, but I believe it be written as a self-contained subquery like so: SELECT * FROM TableA WHERE TableA.id NOT EXISTS (SELECT * FROM TableB) With EXISTS, I don't think it makes a difference on performance based on what's queried. Also EXISTS uses two valued logic (true, false) whereas IN uses three valued logic (true, false, and undefined) So you can get into some tricky situations with using IN as opposed to EXISTS when the rows evaluate to undefined.
If you indent a new line in a comment with four spaces it will change the text from: this to this You should edit and reformat your post so that it's readable. A simple way to do this is to copy the code into a text editor (or right in SSMS,) select it all, then hit tab.
You need to make the ID column an identity on the table, the database will auto-increment it.
I got it working thanks to you guys! Thanks a lot you guys truly are my saviours!
&gt; But when I do SELECT * FROM muziekdatabase you don't select from a database, you select from a table
Relax, no need to become pedantic. I agree I was a bit shorthanded there, but I was on my mobile. Where exactly did I insinuate that a LEFT JOIN would outperform a NOT EXISTS? If the comparison is done over more than the Primary key fields and column values are to be compared that would require a table scan (going with the assumption that key columns are part of an index) I would reckon a NOT EXISTS may have a clear advantage. I'm more of a BI man myself who likes to think in datasets and would prefer the LEFT JOIN option, especially since in his example I don't think the NOT EXISTS will perform any better. 
Thanks very much for that. It has helped me understand the structure and other functions a lot better.
As soon as I saw that join, I thought it's not a true patient stay granularity. What you need to do, conceptually, is get a table/ result set that is truly at a patient stay ( i.e. exactly one record per) and a way to link your patient_table to that new result set. So I will suggest looking for that " true" patient stay data source or looking at your data in the patient_ table and defining the rule for which records can be " stitched together" if that true source cannot be found. If it's the latter case, you'll need to write the relevant query.
Since you're using MS SQL Server, you probably have SQL Server Reporting Services available. Put your queries into stored procedures (because I despise queries embedded in the report), build &amp; deploy the reports, link to the data source, and create a schedule to run them, format the results as Excel and send via email. https://docs.microsoft.com/en-us/sql/reporting-services/subscriptions/e-mail-delivery-in-reporting-services
Probably the simplest way to manage this would be to use sp_send_dbmail. There is some setup required to allow that sp to work, but you can hand it a query and a list of recipents and it will send out the results of that query as a csv file attached to an email. [Here's an article that should start you in the right direction](https://www.brentozar.com/archive/2014/10/send-query-results-sql-server-agent-job/).
Thanks for this. The "true" patient stay table isn't available to me, unfortunately, which is what has prompted me to make this post! What i'm struggling with is how to write said query to determine this true stay.
Your very first block of code should be change to a drop if exists command... Instead of drop database xyz.. For the first time running it just comment out the part of the code that says drop database 
With the subquery the optimizer might decide to tun the subquery every time for each row. Might be very bad. What you want here is a hash join between the tables. This is why i suggest the join option with the hint use_hash.
Depends on the query. No reason to create an sproc and a table to store simple aggregate data, on the other hand you might need to create a psuedo-cube that the report can sit on top depending on how many parameters you are using, etc. From there though it can be easily delivered by email in Excel format.
Unless you are clinically qualified, I do not think determining how to put those together is your decision to make. Look at the data, figure out the cases that you have, think of possible ways to do it, see if there are any of edge/exception cases. Then present your findings to whoever is in charge for clinical/hospital coding and make them accountable for setting up the rule. Once you get the rule set and signed off by business/clinical, you can code accordingly.
The way I’ve done it has been to do an external connection in excel to the sql database. Either execute a stored procedure or run your own query in Excel. Then once it’s done refreshing, use vba to manipulate however you see fit. 
Just comment out the line that drops the database, like this: -- drop database muziekdatabase When you need to re-run the script later and need the database to be wiped out and re-created, remove the inline comment (--). 
Depending on your version of SQL Server, you can probably add this line above the "drop database" statement. if db_id('muziekdatabase') is not null drop database muziekdatabase 
Toad Data Point. Requires a dedicated workstation or server. Plus you can automate reoccurring activities.
You never specified the database schema for the `ingredients` table, which is the most crucial part of this work. It could look something like this: CREATE TABLE ingredients ( ingredients_id INT(32) NOT NULL AUTO_INCREMENT, category_name VARCHAR(128), subcategory_name VARCHAR(128), ingredient_name VARCHAR(128), PRIMARY KEY (ingredients_id) ); The choice of `VARCHAR(128)` for your text fields is somewhat arbitrary: a maximum of 128 characters for those fields felt like a reasonable assumption. You can adjust as necessary. With respect to your question, focus on the definition of `ingredients_id`: it's a non-null integer with the `AUTO_INCREMENT` property, and it's also a primary key for the table. The `INSERT` statement that you wrote should be fully compatible with this table schema: because your `INSERT` statement didn't specify a value for `ingredients_id`, MySQL uses the `AUTO_INCREMENT` definition to assign a value for you. One last comment about how you prepare your `INSERT` query. I would remove the following bindValue call (on line 6): $statement-&gt;bindValue(':ingredients_id',$ingredients_id); PHP is likely throwing an error on that line because 1) the `$ingredients_id` variable is undefined in this scope, 2) You never referenced `:ingredients_id` in the `INSERT` statement.
When you can describe the difference between 3NF and BCNF. Oh, and when you stop using surrogate keys like identities and auto-numbers.
Lol - you didn't understand a single thing I was talking about did you? My first reply to this assumed he was talking about an unstructured text file - not a delimitied file. Regardless, like I said, we've given him the information to move forward. I just don't feel he'll be able to figure it out is all. So, you can eat a bag of dicks too :)
SELECT COUNT(*) FROM `table` where `colA`='X' AND `colB`='Y';
Depends on the complexity and size of the report: Something simple and relatively short the schedule a job to call a procedure that emails it. If it has multi tabs and is a bit larger use this as a great opportunity to create and schedule your first SSRS report. With this solution you can set it up so excell reports are archived to a share to maintain a historical record. Or is it possible that you use SQL to automate the manual loading of data they are doing: your chance to learn new stuff and save someone from having to load it every day and impress your boss.
Assuming that Flight has a column for flightno, I think you could use the query you have above as a sub query, and join it to Flyer WITH flyer_trip as ( SELECT Flyer.FrequentFlyerID, Flyer.Name, Trip.FlightNo FROM Flyer LEFT JOIN Trip ON Flyer.FrequentFlyerID = Trip.FrequentFlyerID) SELECT * FROM flight LEFT JOIN flyer_trip on flight.flightno on flyer_trip.flightno ;
Default both parameters to some weird date long long ago, then check to see if either or both of them still have that weird date. If any of them do you can can wrap your block of code in a check to see if they matched your weird value and if they did don't do any work BUT display your custom error messages.
You may want to pay a bit more attention in class.
Also, don't be scared to get the answer you are looking for threw the use of multiple independent queries. Ex: create a few smaller #Temp tables that meet the criteria for the source table. then do the same for another #Temp table. Then join those two smaller tables rather than doing everything with a query where you've joined two massive tables. Usually runs faster and is easier to trouble shoot.
I noticed that too, all it takes to get MaunaLoona's code to work would be to add a space after : so he's replacing ": " with ": " CHAR(13) + CHAR(10))
Unless I'm misinterpreting the problem, there's no need for a subquery. SELECT Flyer.FrequentFlyerID, Flyer.Name, Flight.* FROM Flyer LEFT JOIN Trip ON FrequentFlyerID = Trip.FrequentFlyerID JOIN Flight ON Trip.FlightNo = Flight.FlightNo
As it’s an assignment question I think it’s best to give you direction rather than the answer, Check out the AVG and HAVING functions;)
Don't hard code your average. Write a query to calculate it. It would be instantly out of date as soon as a row is added, removed, or changed.
Have you tried backticks?
Yeah you assumed he had a 6gb file with no delimiters even though he said so in the OP. Sounds like you're a bit disgruntled or something. Chill out
That did it, cheers!
Happy to help!
Excel has...interesting...ways of interpreting CSV data unless you're *very* careful in how you open the CSV or import the data, and even then, I've seen it get things wrong. Another method which exports directly to Excel format will be easier/safer for non-technical people to deal with.
You'll need to make sure you are outputting and inputting the variables, but all sprocs should be able to access the ##table like it's a standard table. When going from Sproc1 to Sproc2, you'll want your variables declared and set. Example: Sproc 1: exec sproc_2 @var, @var2, @var3 Sproc 2: Create proc as sproc2 (@var varchar(10), @var2 varchar(10), @var3 varchar(10)) Exec sproc3 @var, @var2, @var3
I'm planning on creating a sample set of what I'm doing to upload ans show you for additional input. I think for these purposes it should be fine. As far as the ##table goes, is it fair to think of this in such a way that if ##table1 is created by sproc1, then it will stay alive so long as sproc1 continues to execute sproc2, 3, 4, etc.? However if I were to create a ##table2 in sproc2, then it will cease to exist as soon as sproc2 finishes? I can always just create real tables and have them all dropped at the end of the process, but I was curious whether ## would work or not... however now you've made me curious about whether or not #tables would work in sproc1 or whether they'd be not found. I'll test it myself but apparently this morning I'm locked out of my database and waiting to see whats going on. 
Like this? SELECT COUNT(*) FROM [TABLE] WHERE colA = X AND colB = Y
Why does his posts gets up voted all over the place? He posts the same thing in all subs.
I'm pretty sure you can use ##table1 as you think, but I'd personally use a real table. The main reason is that you can just truncate it or drop it, but I'd do that check at the beginning of your package, not at the end. If you need to troubleshoot and see what went wrong, you can see the past results. If you you drop it at the end, you got nothing there to check later.
The only reason I'm adverse to using permanent tables is the database where this lives is already seriously cluttered beyond all belief. These tables I'm talking about are just segments of larger tables that I'm selecting into because subsquent sprocs will all be hitting them, such as: insert into ##orderhistory select * from dbo.orderhistory where date between getdate()-366 and getdate()-1 create index blah on ##orderhistory([blah]) Then sproc2, sproc3, and sproc4 might hit ##orderhistory and use [blah] as a join to a params table, etc. sproc5-9 might hit ##saleshistory. My thoughts here were that the larger tables (some of them are views, etc.) are huge and if I connect my children queries to them they take much longer than segmenting first -- and this is especially true if there are multiple sprocs that need the same segment. Could easily just dump them into dbo.tables and drop them at the end if there is a benefit to it, but I was worried about the scenario of someone else locking up those tables (for whatever reason) and it then delaying or erroring out the sprocs that are trying to hit them. Other users can't lock up what they can't see.
&gt; My thoughts here were that the larger tables (some of them are views, etc.) are huge and if I connect my children queries to them they take much longer than segmenting first -- and this is especially true if there are multiple sprocs that need the same segment. That goes into SET theory and Transaction levels, depending on how it's written, it could be bad or good. You could write it to be one big transaction with billions of rows. You could write it to be 1 or 2 transactions with a few thousand, it just depends. &gt; Other users can't lock up what they can't see. Unless they also accidentally declare and say the same ##table in their statement, which seems unlikely. Just be sure to let SQL Server name the index on your temp table, it will name it a unique name which is important. If I make ix_index on ##temp or @@temp, and someone else goes to make ix_index on ##Supercraptable, it will error out because ix_index already exists in tempdb.
Well bear in mind that each iteration of the loop is going to hit either the ##table or its parent, and there can be two levels of segmentation based on a variety of factors, so ##table1 might segment only on date, ##table2 might segment on date + another random field... and then each loop will have additional segmentation going on based on the client rules table. Considering that the final state is going to be ~20-40 sproc loops, and each one is likely to run for ~100 clients... it made sense to me originally to pare the tables down and create custom indexes based solely on my logic then simply drop the tables and get rid of them as they have no other purpose. &gt;Unless they also accidentally declare and say the same ##table in their statement, which seems unlikely. Fair point. I had given some thought to giving the ##tables a dynamic naming convention that changes on each run and then somehow having the dependent processes know what to look for, but that struck me (at least at this point in the development cycle) as overkill. &gt;Just be sure to let SQL Server name the index on your temp table, it will name it a unique name which is important. If I make ix_index on ##temp or @@temp, and someone else goes to make ix_index on ##Supercraptable, it will error out because ix_index already exists in tempdb Interesting. How do I let SQL name the table? I can see the use for doing this absolutely, but this shouldn't happen because in a production environment no one should be making ##tables or doing anything at all other than system triggered procs, etc. 
When you stop asking this question. But no, seriously, if you want to know when you're no longer a beginner it's when you are able to come here and start helping beginners out. As a beginner I used to come here and try to answer every single question I read, even if I wasn't qualified to answer it. Sometimes I'd research them and try to solve them on my own if I had free time, and then I'd read the correct answers to see how close I was. Over time I was actually able to start helping people... and the benefit is that you really start to internalize these concepts when you try to teach them to someone else.
&gt; I had given some thought to giving the ##tables a dynamic naming convention that changes on each run For the sake of maintainability and troubleshooting, I think that may over complicate it. Remember you're already working with complicated solutions on a complicated problem, keep it as simple as you can and as straight forward as possible. &gt; Interesting. How do I let SQL name the table? I can see the use for doing this absolutely, but this shouldn't happen because in a production environment no one should be making ##tables or doing anything at all other than system triggered procs, etc. Just the index! It gives a unique name to the table too and now as I type this, I don't know if you can have the same global table created more than once, I am pretty positive you cannot. But you can obviously create the same transaction level temp table multiple times across various transactions. For creating the index and letting SQL create it, I can't recall how to do it. There was a great example in my 70-461 book which I don't have on me. But I think from googling, you do it the same way you'd do it with a table variable. https://stackoverflow.com/a/17385085/5149122
Sorry I meant the index not the table, lol. I'll give it a Google. I'm still locked out of my database because for some reason people in the office can't connect, but people who are remote on the VPN can, so I'll be oging home soon. 
What are backticks?
Also called the Grave accent: `
Question. If Patient 00001 is admitted in January and stays for 2 weeks, and then is admitted again in August for 1 month, will he still have the same patient ID? You could calculate this but you'd have to come up with some rules where you take something like a row_number where 1 = 'admitted' and n = 'discharged' where n is the row before the next line which would be admitted. If the ID changes then you'd simply need to take the datediff between the dates for row_number = 1 and max(row_number). If it doesn't then you'll need to segment your calculation to exclude people who have not been discharged and are still under care. You could do this as a loop on a per patient basis to segment each stay and make the necessary calculations but I imagine there is a simpler way to go about it.
I don't see why you would need a loop. Just join the table with itself on a.person_id = b.person_id and a.date_end = b.date_start and go from there (if there are other reasons why a patient might end and start care on the same date, then you'd have to filter, but you get the idea).
Where does the original posting say "I have 12 6GB fixed width delimited plain text files"? Sorry, didn't read that anywhere. Maybe my decoder ring is broken. I merely asked for clarification as to how the files where delimited, and if they even were, if you bothered to read anything. And yes, I am quite disgruntled; I really don't like you, your friends, your family, or your colleagues. You are the distinct reason that I hope humanity fails.
This is actually how the majority handles database issues anymore. Relational theory is a lost art... to the point that people are saying when you can tell the difference between an inner and outer join, you're an expert. I have yet to meet a "modern" developer who has heard of relational algebra or predicate logic. And this is exactly why more and more projects are moving away from using relational engines and to NoSQL solutions. It has nothing to do with one providing anything better of the other, it's that developers aren't bothering to learn relational theory anymore. It's too "difficult" or "time consuming". Just google it.
I did not go to school for anything computer related. I learned all that on my own, but i did have a very good foundation in symbolic logic which i can attest to it being a huge help when writing accurate queries. I think it’s important to understand math and logic generally, which is why most good high schools teach these things. You don’t have to have computer school to learn them well enough to apply them in this context. Obviously a person who literally knows nothing will not succeed, but being a determined googler is in many respects better than having a good foundational education because when tech changes as it always does, the googler is ready to take it head on. Can’t say that’s the case with everyone.
Kind of confused here broham: CREATE PROCEDURE [dbo].[sp_tab_SLA_01_Global] AS BEGIN TRY BEGIN TRANSACTION DECLARE @CalcID int = 1 EXEC [dbo].[sp_tab_SLA_02_Onboarding] @CalcID COMMIT TRANSACTION END TRY CREATE PROCEDURE [dbo].[sp_tab_SLA_02_Onboarding] AS BEGIN TRY BEGIN TRANSACTION DECLARE @CalcID int INSERT INTO dbo.CalcID SELECT @CalcID COMMIT TRANSACTION END TRY
Using NOT EXISTS will result in a Left Anti-Semi Join without filter, whereas using the LEFT JOIN with IS NULL approach will require an actual join and filter (for IS NULL) operation. These concepts are covered in relational algebra (research: join/equijoin, semijoin, antijoin). 
what a great question!! one trick i like to remember is that if you're doing any aggregation on only one table but there are several tables being joined in the query, you can do the aggregation in a subquery and thereby simplify the outer query
Using NOT EXISTS will result in a Left Anti-Semi Join without filter, whereas using the LEFT JOIN with IS NULL approach will require an actual join and filter (for IS NULL) operation. These concepts are covered in relational algebra (research: join/equijoin, semijoin, antijoin). 
Keep it simple. Subquery for nested aggregation. CTE when you need recursion. Of course, it really depends on a ton of other requirements -- there's no special rule of thumb really. 
Hard to say in general. Helps if you have an example.
First, you should be using the preferred backup replica configuration: https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/configure-backup-on-availability-replicas-sql-server Then, using the sys.fn_hadr_backup_is_preferred_replica function, you can determine whether or not you're on the preferred replica to perform the backup. If that preferred replica is a secondary, ensure you are using the COPY_ONLY option for the backup. 
Should also clarify, you can't issue the backup command on a primary node and expect it to backup on a replica node. If you need something like that, you'll most likely have to issue the statement remotely through a linked server connection (OPENQUERY) or some other similar solution. If that's what you need, it sounds dirty enough to externalize the whole process into powershell or use some other orchestration external to your database servers.
The thing is I don't have one specific example, but whenever I'm trying to solve a problem with sql it takes me a while to figure out the best method to use. I'm hoping there's some kind of rule of thumb, for example I've noticed that when the question has multiple levels to it, like "biggest number per state" or something like that, a cte usually works. I'm basically looking for advice similar to that
Thanks! Yeah, that's about where I'm at right now, I guess I'll get better with experience.
Pretty much all tasks can be done in variety of ways in sql. Subquery is just a result set different from your main - use it whenever you feel it's easier achieve your results in a step-wise approach; Correlated subquery is a result set dependent on the current row context - think of it as a fancy function. CTEs are a more modern way to write subqueries and also the only way to get to recursion in sql. If you need data from two result sets A and B and you need the columns side-by-side - it's a join of some kind. If you need them stacked (records from A, then records from B) it is a union. Watch for/track your row granularity (hopefully that's a concept that your school course went over) - joins can increase granularity, 'group by' decreases granularity. Pivot (rows to columns) decreases granularity, so it's a 'group by' type of operation; 'Un-pivot' (columns to rows) increases granularity, so it is a (cross) join-type of operation. Union could also increase granularity.
I didn't learn any of this in school either. Knowledge only costs attention -- and paying attention is free. 
I'm a SQL Server guy. In my world, the semicolons used that way would throw an error. Just to be sure that's not the issue, write four DECLARE statements (one for each variable) instead of one, and see if that works.
&gt; SET StringData = REPLACE(StringData , ";" , ";" + CHAR(13) + CHAR(10)); Is there a way to use this in a SELECT statement?
SentryOne's Event Manager has a really nice view of your scheduled jobs and you can zoom it in and out. https://cdn.sentryone.com/help/ug/#Calendar_View.html If you aren't already using SentryOne, it's a bit pricey just to get this one feature, but look at your existing monitoring solution to see if it has something similar. And if you don't have a monitoring solution...it's worth getting one.
It all comes down to how efficient your process is, and understand that what is efficient today may become inefficient over time as your data grows. For example you might write a view such as: select * from ( ) a inner join ( ) b on a.id = b.id left join ( ) c on c.id = a.id And you're very happy with it. You could rewrite this as a CTE such as: with a as ( ), b as ( ), c as ( ) select * from a inner join b on b.id = a.id left join c on c.id = a.id In reality sub-queries almost always perform less efficiently than simply segmenting your data into #tables and then doing simple joins such as: select * into #a from a select * into #b from b select * into #c from c select * from #a inner join #b on b.id = a.id left join #c on c.id = a.id And in addition you can create indexes on your #tables to help with performance. The SQL engine isn't always smart enough and forcing it to execute things in sequence as opposed to concurrently can result in a huge performance gain. As a general rule of thumb I avoid sub-queries for complex processes and break them down into #tables. It just largely depends on exactly what you're trying to do, how much data you're trying to do it with, etc.
Yeah, that was my conclusion. Basically, four options: 1. Continue doing backup as normal on primary node, then use rsync to copy backup to main location for LTO archival 2. Used linked servers to execute backup on secondary node 3. Use SQL triggers: job on primary node writes a flag to a table indicating it is waiting for a backup, then blocks until flag is cleared; sql trigger catches the flag, and kicks off a stored procedure (that only runs on the secondary using the function you mentioned) which runs the backup and then clears the flag, letting the process continue on the main node 4. Use an external program: job still writes flag and waits for it to be cleared, but an external service intermittently checks for it, runs a backup if found, and then clears it afterward.
SQLJobVis
&gt; Would you trust someone who learned how to query from a website for a few months handle all of your payroll data for the company? How about ensuring all of your patients records are secure? Would you trust that they could recover your data in the event of hardware failure? Can they write SQL code for the applications that will scale, maintain over time, be easily readable and fixable by another individual later, and not break your database? &gt; There's a reason there are few to near no junior jobs in SQL minus development, and those are scarce. Data is big stakes. You can hire someone to write a crappy website, scrap it, and be out a few grand. You hire someone who crappily maintains your data, your business is gone. When you put it like that, it's crazy looking back and imagining how any of us got jobs. Oh wait, that's right. The company gave me the keys to the kingdom and said don't F anything up. I feel bad for people trying to get their foot in a door these days and touching a production database. With all the data leaks lately, I'm sure most companies are locking down their data as tight as possible, I can see the same thing in my own organization. 
Actually, why are you looking to do this to begin with? Are you looking to offload the work of a backup to a secondary due to performance issues? Otherwise, this all sounds overly complex for no reason. If you have some process that, for some reason, you may need to rollback its work, use a marked transaction to make it easier. Or, if you just simply want the backup in an "archive" location, use backup mirroring (MIRROR TO). 
For performance reasons, we have to colo the production database near to particular clients, but we also need years of retention and the databases are too large to use cloud storage, so we've opted to use LTO tape. Problem is, it's a lot harder to set up an LTO storage system and arrange for tape storage/delivery than it is to spin up a SQL instance in a remote cloud environment. So we want to have the primary database node be in one region, but be able to have the retention backups put to tape at our primary physical location. Copying the backups over the internet, even if differential, takes about 20 hours, so it would be more ideal to have the backups occur at our primary site (LTO-drive adjacent).
Use a stored proc to execute each query... or setup SQL Server Agent Jobs... that way you just need to execute / start the job. If you go the SQL Agent Job route, you might want to setup Step 1 as a blank command, go into advanced, set "On Success: Quit Job". Then Step 2 will be your actual SQL code... this prevents someone from just running "Start Job At Step" and kicking off the query on accident... you have to manually tell it to start on Step 2.
[SSIS](https://docs.microsoft.com/en-us/sql/integration-services/sql-server-integration-services)... literally what it's made for :)
&gt; I feel bad for people trying to get their foot in a door these days and touching a production database. If I knew what I know now and my end goal was to do SQL work / administration, I would have done a two year technology degree and trying to find an internship in IT. Something technical, like sys administration or coding / development. In my free time if I wanted SQL Server, I would have obtained my MTA. After graduating with a two year degree, preferrably in computers or technology. I would have hoped one of those internships would land a junior job in IT where I would ask to shadow or assist the DBA folks on hand. Hopefully after six months to a year, they would allow some minor work there and hopefully in two years I'd be allowed to do more. Afterwards you could jump base or aim for a promotion for a DBA role and I would have worked on my DBA Certs. After that 2nd job or the promotion for DBA work and certs, you would be at the mid-range to low high range gigs for DBA. Like most folks though, I became an accidental DBA essentially and just stuck with it. Had a jack of all trades IT gig -&gt; Sys Admin -&gt; DBA.
You'll want to use WHERE NOT EXISTS rather than NOT IN. This will give you full customization of the subquery. Maybe something like... SELECT DISTINCT VendorName , VendorCity , VendorState WHERE NOT EXISTS(SELECT VendorName , VendorCity , VendorState GROUP BY VendorName , VendorCity , VendorState HAVING COUNT(*) &gt; 1)
Yeah, not sure why they did it that way... there was a similar post for this in MySQL that did the same thing... string concatenation: /r/SQL/comments/767e9h/issues_trying_to_select_unique_results_in_mysql/ SELECT vendor_name ,vendor_city ,vendor_state FROM Vendors WHERE (vendor_city, vendor_state) NOT IN ( SELECT vendor_city, vendor_state FROM Vendors GROUP BY vendor_city, vendor_state HAVING COUNT(*) &gt; 1 )
I would say the big problem is why is the application in front of the database not doing this to begin with? Otherwise, you options would be stored procedures and possibly SQL Server Agent Jobs as ihaxr stated.
I admit to being pedantic. I _am_ learning to scale it back; please bear with me. On the flip side, this line of work does benefit tremendously from a detail-oriented mindset, so there's at least a silver lining. Regarding your point about where you insinuate the left join outperforms NOT EXISTS, note that you were replying to a response refuting that very same point from /u/millhoused: _"Put SET STATISTICS IO ON SET STATISTICS TIME ON When I have compared both, the outer join wins on large tables."_ Finally, I want to comment on this part of your response: _"If the comparison is done over more than the Primary key fields and column values are to be compared that would require a table scan (going with the assumption that key columns are part of an index) I would reckon a NOT EXISTS may have a clear advantage."_ There is a misconception here that you can test and validate. You say PK here and allude to a correlation with an index, but to clarify, there are two types of indices that apply here and the differences between them are relevant. Knowing how they operate is going to help clear up the misconception that you are going to get a table scan when your criteria spans the PK as well as other columns. OK, first, creating a PK automatically creates a clustered index in SQL Server. If you write a query against a large table and your criteria includes the clustered index columns without adulteration (aka SARGable criteria), in addition to other filter criteria, SQL will still perform a clustered index seek. Your additional filter criteria will be included in a Predicate, which essentially operates on top of the results returned from the seek. Why does this work? In a clustered index, all non-key* attributes are included at the leaf level of the B-tree, so SQL has direct access to them. In a non-clustered index, which is also possible if the architect/designer elected to differentiate the clustered index from the key column(s), it depends. If the 'additional filter criteria' acts on columns defined as part of the index, or as INCLUDEs, and you aren't using any additional columns in the output not contained within the index, that is called a covering index and you will get the same behavior as the clustered: a seek with predicate. If not and the table is large, plus the joining recordset is relatively small, you will likely still get an index seek but with the added cost of a bookmark lookup against the clustered index in order to retrieve the values needed by your predicate and output. Finally comes the table scan: you will only get one if you have a small table to begin with, or you are trying to join in a dataset that is relatively large (making the scan cheaper than a bookmark lookup), or... Most commonly, your criteria is not SARGable. All that said, it's worth noting that sql behavior will be the same here regardless of LEFT JOIN, NOT EXISTS or NOT IN. Hope this helps demystify this for you. Let me know if you have any follow up questions. 
I'd recommend moving away from tape as much as you can. Investigate backing up to Azure BLOB storage instead: https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sql/virtual-machines-windows-use-storage-sql-server-backup-restore SQL Server can backup to it natively. It's also cheaper to backup to than LTO -- however, the performance is really limited by your internet pipe. And it's really nice in that it takes care of your offsite requirements in one operation. 
We use RedGate SQL Monitor... I'll check and see if they have anything. Thanks for the heads up, I wasn't aware they would do stuff like that.
We're backing up just shy of 820GB/day in backups, and need to keep each day's backups for 7 years, so that would end up being in the neighborhood of $40,000/month at the end, which is currently more than we spend on tape + professional tape storage in a year...
Thanks. I saw some recommendations for SSIS in my initial Google searches. I will definitely give it a shot
That's an odd assignment actually. Your statement, except for your where clause, effectively meets the requirements and is far more efficient than using a subquery or arbitrary union. Seems like your class is teaching you two compose inefficient SQL. But yeah, shobbsy has it right - it's an assignment, so no one should give you a straight answer ;P 
Thanks, I'll check it out
On my phone. I would not accept their solution from a professional. It is nonSARGable criteria. The fastest solution requires the use of a CTE and count(*) in a windowing function, since it only requires one pass at the dataset. See this: ;With cte1 As ( Select city, state, name, count(*) over(partition by city, state) as cnt From dbo.[mytable] ) Select city, state, name From cte1 Where cnt = 1; You can also just do it with a CTE and join or a derived table: ;With cte1 As ( Select city, state From dbo.[mytable] Group by city, state Having count(*) =1 ) Select t.city, t.state, t.name From cte1 as c Join dbo.[mytable] as t on c.city = t.city and c.state=t.state This requires two passes at reading from your table. 
You need to correlate it and eliminate the name from the inner group by, but note that this pattern is going to be slow because you are asking sql to perform the aggregate for every record. 
This syntax does not work in MS SQL Server.
update statistics on the DB (run gather_database_stats, https://docs.oracle.com/cd/B28359_01/appdev.111/b28419/d_stats.htm#i1036194) and re-run the first one. If you are still seeing unexpected results - post them here.
What do you mean?
I mean rather than using UPDATE, can I somehow incorporate the REPLACE(StringData , ";" , ";" + CHAR(13) + CHAR(10)); in a SELECT statement and get the same output? 
&gt; UPDATE MultipleStatisticData &gt; SET StringData = REPLACE(StringData , ";" , ";" + CHAR(13) + CHAR(10)); I get the following when I try to run this: Msg 207, Level 16, State 1, Line 2 Invalid column name '; '. Msg 207, Level 16, State 1, Line 2 Invalid column name '; '. 
Select t.* From table_name as t Left join ( Select * from table_name Where tag_id in ('18','19') )s On t.user_id = s.user_id Where t.tag_id ='15' And s.user_id is null
Sorry for formatting. On mobile. 
I’m sorry I can’t help with your first question, I’m not a MySQL person. On your second question, imagine a table called “employee” and a table called “employment history”. (This is a very simplistic example, just trying to make the point). The employee table has a unique key, let’s call it “employee id”. It also has some other stuff like name, birthdate, etc. The employment history table also has the “employee id” from the employee table (which is now called a foreign key) and the dates of employment for each employee. If you drop the employee table first, you just removed the reference that allows you to go back to the employee from your employment history table, right? That’s a referential integrity problem, you literally can’t reference back to employee since that key doesn’t exist anymore. That’s bad. So delete your employment history table first (here we’re assuming this is the only table that has an “employee id” foreign key), and then you don’t create these kinds of problems. I know the intent is to drop both tables so it seems like it shouldn’t matter, but in practice it does. Hope that helps.
Thank you so much for your help! In the example you provided i would first have to drop the employement history table and then the employee table. Then i would have to create the employee table and then the employment history table. Am i getting it so far? 
That’s exactly right. :-)
This worked! I kept on getting into a bad loop where there was something in the SELECT I couldn't put in the GROUP BY, I forgot all about JOINING to circumvent that problem. Thanks!
You absolutely can. It would be a normal SELECT syntax, with the selection being the REPLACE statement. I'm on mobile and can't type it out at the moment so let me know if you have questions. 
Got the issue resolved, cheers though. 
Azure data factory v2 is basically distributed cloud ssis, v1 is just a scheduled bulk load, so you might be able to solve the problem with out of the box azure tools.
 SELECT foo.* FROM table_name foo LEFT JOIN (SELECT * FROM table_name WHERE tag_id != 15) bar ON foo.user_id = bar.user_id WHERE foo.tag_id = 15 AND bar.user_id IS NULL Basically, we have a subquery where we build a table of all the rows except those with a `tag_id` of 15. Then we join the original table to it (on `user_id`), and find all the `user_id`s that didn't end up in the join. That means they have a `tag_id` of 15, and no other `tag_id`.
Conceptually, start by considering which “thing” people have (or don’t have) access to. Let’s say “department” in this case. You’ll need some kind of user access table that defines which users have access to which departments. The view essentially inner joins user access to the other table(s) on the departments defined when user = login id. You can implement various levels of complexity by using a security key to represent different access combinations (i.e., 101 = department 10 at company ABC; 201 = department 10 at company XYZ, etc.). Up to you. If you go this route you need to create security key columns on each of the tables used in the view. We implemented Oracle VPD a long time ago, which is the same principle but different totally execution (more like the second example you quoted below). It worked well in a data warehouse environment but it would have been hell on an OLTP system (imagine all of the keys/combinations that would need to have security assignments associated). But it does work when the scope of analysis is limited to a handful of tables.
Ah, those should be single quotes not double quotes. I updated my original post.
This is good stuff. I am going to add one thing: be like the optimizer and always write the most efficient SQL you can in the time you have. Compromise only when you absolutely have to (immovable deadlines / other priorities). It's going to force you to learn more and more about how SQL Server _works_. It's a deep topic, but data skillets are in demand and that demand is only growing. By focusing on efficiency, you will ultimately answer your own question by finding the patterns each of the options excel in. Taking an example from above, granularity / cardinality is key to solving the problem, but if you can understand how SQL Server is going to take your SQL and fullfill your request, you will know how to manipulate it to do the least amount of work in order to achieve the result you are looking for. This is how you build business critical systems that scale. I will say that asking questions like this is a great step along the path. Godspeed. 
We used log shipping from Racksapace to AWS (EC2) then simply cut over. Later migrated to RDS. Effectively zero downtime, a requirement. 
 SELECT table_name.* FROM table_name INNER JOIN (SELECT user_id FROM table_name GROUP BY user_id HAVING COUNT(tag_id) = 1) q ON table_name.user_id = q.user_id WHERE tag_id = '15' 
When I first had to decide between cte and sub queries just remember that cte is easier to reuse. Sub queries were easier to use once. Once you do more queries where one or the other is needed you’ll figure out based on your coding style which is more effective in your situation. I could be way off from the good folks’ advice on here but that’s what worked for me. 
Were the stars added to format for reddit? They look like almost as much of a problem as using a 14 (nearly 15) year old piece of SW
Overhauling the entire backup strategy is probably a priority. Producing a daily full of an 820GB database straight to tape is not sustainable and hasn't been a viable strategy in 15 years. Differential and incremental strategies exist to lessen these exact issues. In this case, moving to a differential model, and backing up to disk to sweep to tape later should be part of the initial plan. There are so many other technologies out there to address this problem, but it sounds like your shop may not be willing to spend the money.
But I need a base to start on at what point is that base ready 
For someone writing reports I'd say (recursive) CTE's and grouping with rollup or cube. All the DML statements, in what order they are executed. For a developer you will also need to learn some DDL statements besides the DML ones, know how indexing works (how does a clustered index differ from the non-clustered index?), performance tuning, query plans, the different kind of loops ect. For a DBA, you also need to know about mirroring, replication, backups, some powershell/bash/python, failover clustering, high availability ect.
Does MS Access allow you to run `Create table` queries in the first place, or does it require that you use the GUI designer? You can execute this type of query via OleDb (see https://msdn.microsoft.com/en-us/library/bb177893\v=office.12).aspx) but I've never tried right in a query window.
Formatted query (don't know what OP was trying to do with the asterisks) СREАTE TАВLE Emplоyees (E_CОDE СHАR (4) NОT NULL , E_LN СHАR (30), E_FN СHАR (30), E_DATE DАTE, E_WEXP INTEGER, СОNSTRАINT [Index1] PRIMАRY KEY (E_CОDE) ); Note this was copy/pasted directly from the post source, it looks like mixed ASCII/Unicode or something.
This is more about the effects of indexing than it is general performance improvement.
Something like this: SELECT FirstName, LastName, COUNT(*) as RowCount GROUP BY FirstName, LastName HAVING COUNT(*) &gt; 1
 SELECT name FROM users GROUP BY name HAVING COUNT(*) &gt; 1
On mobile, so sorry about the formatting. select t1.name from table t1 inner join table t2 on t1.name=t2.name where t1.role=1 and t2.role=2
True. My biggest performance headaches always come from over-parallelization. I did learn that like statements that end in a wild card can still use text column indexes efficiently. I always thought using like and any wild cards invalidated all indexing.
So, that is exactly what I want... Except, it doesn't deal with time zones, and (seemingly) only shows jobs that conflict. My hourly TRN log backup only shows up for 5 hours a day, when we are doing other maintenance. It only does one day at a time, and the zoom function is not user friendly. In theory, that is what I want, just some more features. I'll keep looking, or figure out how to use the data to build in excel.
Doesn't your one and only line of the procedure - "INSERT INTO...VALUES..." - need a semicolon at the end, too?
You probably want to use (the less secure) SQL authentication, in which access is managed completely within SQL Server, and has little to do with Active Directory or computer permissions. There's more information [here](http://codedevstuff.blogspot.com/2014/02/how-to-set-up-sql-server-2008-local.html):
will answer with the usual. It depends on what you want to accomplish. do you want to work for a large enterprise? then only knowing SQL and being a data analyst would work. or do you want to do reporting and business intelligence? then getting good at the ETL aspect will help. or do you want to get into big data? then learn HDFS and things like pig and hive. do you want to design websites? learn a language to go along with things that will help with full stack web development alongside HTML and CSS. do you want to do DBA work? learn a system (oracle, ms, mysql) and how to optimize it depending on the OS. Optimization in queries etc. also permissions and how to automatically track storage space etc. Learning a language and being able to present the data is always helpful to the business side of an enterprise. what seems to be your passion? i enjoy the ETL side, moving lots of data around and preparing it for reporting or analysis. I personally cant stand doing the analysis part and prefer to have other people that are better at speaking and telling a story handle that part. I dont really like DBA work in my company, there are too many restrictions (the developer side of me talking). I like developing star schemas and making a usable DB for analysts to use. 
All Oracle?
There's a free query out there that can do what you're looking for, I think it's awesome. [Visualize the timeline of your SQL jobs using Google graph and email](http://www.sqlservercentral.com/articles/Agent+jobs/127346/)
I am not 100% sure what that does and since that database is not mine, I prefer not to alter it. As for results, I getting 25 records of created_date 17-Feb-04 and only 1 with 22-Sep-17 using the first query. All the others produce only the one record between the date range.
Should work on other databases too. At least on ms sql it's pretty much the Same. Mysql doesn't Support in Versions lower than 8
Thanks a lot m8! I will read about ETL and HDFS, then consider them. Have a nice day :) 
That looks like exactly what I want! Thanks for the link!
&gt; I do support for a software company and the problem I'm running into on rare occasions is a new order_sub_line record will be inserted AFTER the original records were inserted into the temp tables So while the procedure is running, someone or something makes an alteration to one of the underlying tables and that record does not get accounted for in the procedure because it already ran the select? To me, that's what constraints are for, it's enforcing data integrity so it can't be deleted out after. I would force a table lock with that stored procedure so if a process is trying to alter it, it's going to have to wait it's turn. Essentially turning parallelism off because things rely on order. You can also use try and catch error handling, log all errors to another table and a final stored procedure can be called to work on the results from that table. Example: One record failed to delete for constraints, it logs to the table. Then when it's finished with the rest, a proc is called, dynamic SQL likely runs in place, and it tries to do another delete with the records from that table. Instead of killing parallelism for this instance or adding new mechanisms, you could change it from being set based to RBAR based, but that's not best practice. It's not going to scale well either. But this allows for a high commit environment. So this would become trigger based in my opinion. You could also temporarily disable and re-enable constraints, but that's bad practice and by far the worst idea. The last thought is that the tables being accessed and the way the deletes and inserts working here needs to be re-architected and evaluated from the ground up. 
Yeah, obviously, wanted to highlight the code. They weren't used in Access.
&gt; Does MS Access allow you to run Create table queries in the first place, or does it require that you use the GUI designer? No idea. This is my first ever experience with creating tables through an SQL query. Judging by the text of the message, it doesn't allow it, but we are not meant to use the GUI designer. &gt; You can execute this type of query via OleDb If I manage to wrap my head around it, lol.
Excellent! Thanks!
Thanks!
Oh, my bad, I didn't format it correctly in the text box on Reddit, it's meant to look the way you've posted it. Still doesn't work though, still the same error.
I don't want to say this test is misleading... but it's kind of misleading. All of the questions are focused on the index but the answers are focused on the query. Spoiler: [Like the first question:](#spoiler "Is the following index a good fit for the query? and the correct answer is that the query needs to be changed, even though the question clearly asks about the index")
Not super familiar with Access, but do CHAR data types take a length specification? Try it without maybe? Or try TEXT or VARCHAR? (I usually use the UI when I'm using Access)
It's been a while since I've used Access. Depending on the version, I think that once you're staring at the query window, you'll pick the 'Query' dropdown, the 'SQL Specific' flyout, and choose 'Data Definition', since this is DDL, not DML. You should be able to type your query there.
Check your logic :) IF object_id('tempdb..#CLAIMS') is null DROP TABLE #CLAIMS If the `object_id` of `tempdb..#CLAIMS` IS NULL... which means it _doesn't_ exist... then drop the table? That doesn't sound right...? You probably want to check if it `IS NOT` null... then drop it: IF OBJECT_ID('tempdb..#CLAIMS') IS NOT NULL DROP TABLE #CLAIMS SELECT 1 a, 2 b,3 c INTO #CLAIMS
&gt; do CHAR data types take a length specification? They do, well, supposedly - can't test it, but lengths are featured in examples found online. The issue seems to be not with the body of the statement, but with the statement itself, for it does not start with 'DELETE', 'INSERT', 'PROCEDURE' etc. &gt; (I usually use the UI when I'm using Access) Maybe that's what I'll have to resort to.
Thanks for the idea. Tried it just now, didn't work, threw the same error.
[removed]
My next set of comments are not related to what you are doing or if there are better ways to doing it, but in regards to your questions directly. The best out of them in my opinion is 2.) and then C. EXISTS has very specific criteria to it and is not the same as WHERE IN. So you need to be careful about that, EXISTS can make your query very different later, make sure you understand it. &gt; that putting certain pieces of the where clause in the join make queries more efficient. Sometimes, this is mostly relatable and used with a left join, I believe it does not add anything with an inner join and it becomes harder to read. &gt; Does it make sense to always include certain joins and clauses in hopes they get optimized away in order to use the same plan/index? Probably not, no. There's a small one off situational outlier chance I'm sure, but no, I would not recommend that. &gt; Also, does the order of clauses and joins affect query plan caching? Nope! :) &gt; That is, if I switch the order of the AND statements, are those 2 different plans? Maybe, but it's not because you switched the order of the AND statements, it's because SQL Server decided there was a better plan to use. 
&gt; The last thought is that the tables being accessed and the way the deletes and inserts working here needs to be re-architected and evaluated from the ground up. I 100% agree because this is so janky. So, locking the table is not an option, there are multiple types of orders and the other orders need to be accessed and updated while we are purging these other order types. It's not a big deal if the delete and archive job that calls this simply "skips" the new records that are getting created and catches them on the next days scheduled delete. I think I've come up with something though as a 'band-aid' fix because I do not have time to rewrite the whole job in my position as a lowly support tech. Here's what I came up with... --original delete statement delete oh from Exactadb.dbo.order_header oh inner join @orders o on oh.order_Id = o.id --my band aid fix --get all the new child order_sub_lines from relevant orders --that were NOT inserted into the original temp tables, but have been added since --then delete everything except the records tied to those with new_sub_lines as ( select order_sub_line_id from order_sub_line where order_sub_line_id not in (select order_sub_line_id from @sublines) and order_sub_line_id in (select order_sub_Line_id from order_sub_line where order_line_id in ( select order_line_id from order_line where order_id in ( select order_id from order_header where order_type = 1 and order_category = 1))) ) delete oh from Exactadb.dbo.order_header oh inner join @orders o on oh.order_Id = o.id where oj.order_id not in ( select order_id from order_line where order_line_id in ( select order_line_id from new_sub_lines)) I think this should work but I need to test it 
That's news to me, I have usually just used permissions in SQL. Typically you have an instance level account which just means you can access the server gui of the SQL Client. Then you have accounts associated to a database that you can tie to the account. Then you can get more granular from there, but this can vary based on RDBM. For your example, it sounds like this to me: - Table1 = table you want to access - View1 = view you have to access the table - sproc1 = stored procedure you have to access the view ALTER Table1 ADD Identifier_Stripe INT The Identifier_Stripe would have an ID ranging from 1-X, the view would basically equate to: SELECT * FROM Table1 WHERE Identifier_Stripe &lt;= 4 The number at the end is the level of permissions you want them to see. Say we had a person table, perhaps their ID and favorite_color are level 1, pretty minimal security risk. I would say that's a 1. Then let's say they had SSN too, that to me would be a 10. So the view definition above could only view data with the security level you defined, from 4 down. Then they are saying you should execute a sproc or some function to call the view. I feel it would do something similar. I could be getting this all wrong. It's not uncommon in the real world to use VIEWS or Programability options to restrict access and ability to see data. I have not seen the "striping" like I showed above, so I'm not exactly sure what that is. &gt; A view implements a filtering clause that checks whether the current user or database matches the striped column value. But that sounds a hell of a lot to me like what I mentioned.
I don't see how it could work better unless it was faster. The issue as I understand it is that during the call of the proc and it's transaction, another transaction took place. Rare, but occurs. I still foresee this occurring because it's still in the same ideology. I would also re-write your in clauses to be joins instead for readability and maintainability personally. This is definitely a band aid fix as you stated. I would look into other options long term, but I believe you are doing that already. 
&gt; database schema and SQL Do you mean architecture and the ANSI language? SQL has a "standard" language or syntax, and it would work in most platforms. If you have a specific type you want to learn, that changes a lot. Architectural ideas have a lot of key concepts, and that in itself is pretty broad. If you have any more examples or info, I could try and find you some info.
For a full time SQL developer, yes. For a TAM, absolutely not. TAM you may need to know 2, 3 (maybe), 8, and probably 9.
 SELECT count(distinct purchaseID) , count(distinct CASE WHEN sku='bike' then purchase_id else null end) FROM purchase_item
How would you write it so that it displays like this: purchase_has_bike | Count TRUE | 2 FALSE | 6 This is where I'm struggling
remove the unnecessary parentheses in your ON clauses, they aren't required in any database then add unnecessary queries to nest your tables... these unnecessary parentheses are required by Access SELECT i.invpart , i.pname , i.onhand , q.quosupp , q.onord FROM ( inventory i INNER JOIN quotations q ON q.quopart = i.invpart ) INNER JOIN suppliers s ON s.supsupp = q.quosupp ORDER BY i.invpart , q.quosupp
 SELECT CASE WHEN pi.purchase_id in (select purchase_id from purchase item where sku = 'Bike') then TRUE else FALSE END as Purchase_Has_Bike, Count(distinct purchase_id) as Count FROM purchase_item pi GROUP BY CASE WHEN pi.purchase_id in (select purchase_id from purchase item where sku = 'Bike') then TRUE else FALSE END 
There are a few ways of doing this but the easiest is probably: select s.route_number, vessel_name, f.year_built from sailings s inner join fleet f on s.vessel_name = f.fleet where f.year_built IN (select MIN(year_build) from fleet)
Wow. This is a weird post. No need to get all gendered here. Anyways... If you want to learn SQL or anything, start off by finding tutorials on YouTube or just google them. They will tell you how to set it up, what you'll need, etc. 
four spaces before each line generally trigger a code block, which should auto-highlight for you.
Yeah. Like to how to model a real world example, from modeling to schema design to SQL. From idea to logical to physical design. There are plenty examples on Query (the SELECT parts). I am looking for the CREATE, Constraint, and type parts.
would this not just output one row for each route number? im using sqlite and top 1 i believe is the same as limit 1
wow that is so helpful thanks alot any thoughts on the second one ? I am trying to edit per your first solution but having a hard time thanks
Just as a heads up, [schema](https://stackoverflow.com/a/11618350/5149122) in MySQL, physically, a schema is synonymous with a database. You can substitute the keyword SCHEMA instead of DATABASE in MySQL SQL syntax, for example using CREATE SCHEMA instead of CREATE DATABASE. Some other database products draw a distinction. For example, in the Oracle Database product, a schema represents only a part of a database: the tables and other objects owned by a single user. ^ Taken from the link. WE3 schools has good [examples](https://www.w3schools.com/sql/sql_constraints.asp) for syntax.
Thanks for the reply! I tried doing a second join, but I kept getting a syntax error and "missing operator" or "JOIN expression not supported." Here is the code I tried: SELECT Flyer.FrequentFlyerID, Name FROM (Flight LEFT JOIN Trip ON Flight.FlightNo = Trip.FlightNo) INNER JOIN Flyer ON Trip.FrequentFlyerID = Flyer.FrequentFlyerID WHERE MilesPerFlight BETWEEN 340 AND 560; (That last part is from something else in the problem). When I don't have parenthesis, I get syntax error (missing operator). I looked it up online and found that I'm supposed to use parenthesis when I'm doing multiple joins(?), so I added them and then I got "JOIN expression not supported." I've also tried it with Flight.* (like in your code) but that didn't fix it either. I've double checked each individual name and the relationship, so I'm not sure what's going on. Any ideas? Once again, thanks for your help! 
W3schools.com they have an interactive SQL tutorial that I found really helpful. 
You don't want the parentheses you added, so get rid of those. I just re-read your original post and noticed you're using Access. Two potential quirks that could result from that: Access sometimes needs parentheses around join conditions, and it sometimes doesn't like combining left and inner joins. Try this: SELECT Flyer.FrequentFlyerID, Flyer.Name FROM Flyer LEFT JOIN Trip ON (Flyer.FrequentFlyerID = Trip.FrequentFlyerID) LEFT JOIN Flight ON (Trip.FlightNo = Flight.FlightNo) WHERE MilesPerFlight BETWEEN 340 AND 560
got it thanks again 
First, you're going to need to wrap date values in single quotes just like the way you did with strings. Also, the default format for dates in MSSQL is YYYY-MM-DD HH:MM:AS. It does sound like the date values are being stored in this instance. You can check this by executing the following query: Select * from information_schema.columns where table_name='Trip' If the datattoe for that column is a varchar instead of daytime, then the values are stored as strings instead of native datetime cakes. If they're strings then you should add a cast in your predicate.
SSIS can [monitor a folder for new files](https://stackoverflow.com/q/21436377/1324345) and does a very good job of doing bulk file imports and data transformations.
Do you love to take exams for others, too? 
Just ones where I get to show off how awesome I am and/or provide me with the imaginary internet points that I base my self worth on.
Thanks for your help!
The top 1 just grabs the oldest year_built from fleet, it doesn't restrict the the actually query, that said I re-read your original comment again and I think I misunderstood what oyu are asking for, are you looking for and maybe the data itself, can a vessel_name have more than year_built date in the fleet table (intuitively a ship can only be built once)? Are you looking for a way to find all of the routes and the original built date of a vessel? If so you had the right idea (normally I would use a window function for this but apparently sqlite3 doesn't support that): select s.vessel_name, s.route_number, y.year_built from sailings AS s inner join ( select f.vessel_name AS vessel_name, MIN(f.year_built) AS year_built from fleet AS f group by f.vessel_name ) y on s.vessel_name = y.vessel_name 
This soounds relatively simple, shouldn't something like this work? SELECT T1.FlightNo, T1.FlightDestination, T2.FlightDate FROM FLIGHTS T1 INNER JOIN TRIP T2 ON T1.FlightNo = T2.FlightNo WHERE T2.FlightDate &gt;= '20100101' AND T2.FlightDate &lt;= '20100228' AND T1.FlightDestination = 'SLC' OR T1.FlightDestination = 'LAX' Just check the type of your dates and compare it to a date format that is compatible. 
Whelp, believe what you want but some studies have shown that by believing [certain group] are better at a given mental task you will, on average, perform worse at it, even if you are part of [certain group]. Anyways - SQL isn't like many other programming languages. It changes very slowly. So an 'outdated' resource is about 95% as good as a 'modern' one. If I was learning from scratch I'd start off by downloading &amp; installing MySQL + MySQL workbench. You'll want to learn and understand relational databases so you should watch [https://www.schneems.com/ut-rails](this course) by Richard Schneeman. It's fantastic and while it is oriented toward Ruby on Rails it's a great introduction. Once you've got the basic theory under your belt it's time to dive in with a bit of practice. Head over to [https://sqlzoo.net/](sqlzoo) and run through the exercises. If you get stuck search google / stackoverflow for answers. Don't just copy &amp; paste. Try and understand what you're doing! Take good notes, it will help you remember things later on. After you've done all this, and you feel confident that you like the work go to Udemy / Udacity and search for a few courses in things you're interested in. I'd try out the courses in the data scientist track on Udacity. You can view them for free, and if you pay some money you can get a certification (though how much its worth is still up in the air). Ok - now time to really step it up. Go to [codewars.com](codewars) and work your way to at least 5kyu in SQL. It's technically Postgres, not MySQL - but the core concepts are the same. Read through [https://www.amazon.com/Joe-Celkos-Thinking-Sets-Management/dp/0123741378]("Thinking in Sets"). Again - take notes! You want to remember this stuff. Finally practice building a few databases of your own using workbench. Once you've got that down, practice building them using the command line. Now you're ready for a basic job using SQL :) good luck!
Do you know if there's a significant negative performance impact of using procedures with SQL/PSM in MySQL? I would use them, but i'm unsure about how much it will slow down my system.
By misleading, do you mean that indexing is actually independent of the queries?
Why are you checking existence in the first place? Temp tables are dropped at the end of your session. Global temp tables are named with double # eg tempdb..##claims Concurrent sessions calling the same procedure automatically use a put a identifier on the end of the table name in TempDB: #claims_000000000309 &amp; #claims_00000000030A 
Stitching, knitting are in a way programming. [Margaret Hamilton](https://www.nasa.gov/feature/margaret-hamilton-apollo-software-engineer-awarded-presidential-medal-of-freedom)'s code got us to the moon and back. Look at that picture, see all that code? A woman wrote that. Kimberly Tripp and Kendra Little are well known SQL experts in the community. SQL is a different from other programming languages, it's a language designed to work with databases (querying and storing data). If you want to create apps, websites or games you should find another language.
&gt; shouldn't something like this work? no, it shouldn't what you wrote is actually evaluated as follows -- WHERE ( T2.FlightDate &gt;= '20100101' AND T2.FlightDate &lt;= '20100228' AND T1.FlightDestination = 'SLC' ) OR T1.FlightDestination = 'LAX' and this clearly is ~not~ the right solution
[Manage Your MySQL Databases With SQLyog](http://blog.sqlyog.com/manage-mysql-databases-sqlyog/)
You might also need to distinct/group the results.
I think you are working outside of the scope of SQLite at this point. SQLite was designed for a single user, deployed in a local manner to host small datasets. SQLite was not designed with networking in mind. That said, is a remote environment out of scope? Could RDP into a server and access the file? Otherwise you'll want to write a web server that handles the interactions using some kind of simple front end. I'm not experienced with web servers but some Googling may help you.
[removed]
No, I mean the question asks if the index is good, but the answer is: change the query. It isn't clear from the instructions that changing the query can be the answer.
 SELECT R.owner_id FROM R INNER JOIN C ON C.id = R.car_id GROUP BY owner_id HAVING COUNT(CASE WHEN C.color = 'red' THEN 'humpty' ELSE NULL END) &gt; 0 AND COUNT(CASE WHEN C.color &lt;&gt; 'red' THEN 'dumpty' ELSE NULL END) &gt; 0 
 for future reference this is called a many to many relationship. So I would say something like "cars and owners has a many to many relationship using 'relationship' as the intersect table". I would rename the "relationship" table to "car_owners" to make this clear. As for your question, remember that you can select from the same table multiple times. Select only red cars and then select cars that are not red. 
Um, did you take the same quiz? &gt; Most questions ask whether the index is the right one for the query (good fit) or not (bad fit). It is even explicitly mentioned that changing the query is valid. &gt; If you think about changing a query to improve performance, the new query must still return the same result. That includes all columns if select * is used. 
Maybe there is a translation issue. &gt; Most questions ask whether the index is the right one for the query (good fit) or not (bad fit). Yeah I read the same thing. The question asks if the *index* is right for the *query*. Where does it say "is the *query* is right for the *index*"? May be clear to *you* since you wrote the test, but not clear to me.
Format your queries, it will be a lot easier to read and refactor them.
What are you trying to write to the table? The end time remaining? Why not just calculate the remaining the time?
This was my first thought. Why write every second? You only need to write to the table when the data changes. Doesn't the table store the end time for the bid? What could possibly be updating once per second? This also seems like a poor approach if you intend to scale the app.
Yeah well the "writing every second" idea was about that on the client side the remaining time will not be changeable (by editing the html behind) but thinking about it the query is enough too. So I gues i'll need to do a query every few seconds to get back the remaining time?
I usually deal with the back end development (and haven't done much on mobile) so take this suggestion with a grain of salt: Maybe the approach should be to cache the remaining time on the client. When the timer is up, compare the *current time* to the end time on the server. If the query successfully (i.e. current time &gt; end time) reports that the bid time has ended, then continue with the transaction. Else, you probably know the user is cheating or the timers are out of sync. Then refresh the cached timer data on the client. 
Good idea! I'll try both approaches and see. Thank you
hmm, I can't seem to get this working. Do I need to do a nested SELECT? 
Just one thing about [datetimes](https://stackoverflow.com/questions/8153963/sql-server-2008-and-milliseconds) though...
Something like this: MONTH( OrderDate ) &gt;= 9 ? "4th Q" : MONTH( OrderDate ) &gt;= 6 ? "3rd Q" : MONTH( OrderDate ) &gt;= 3 ? "2nd Q" : "1st Q"
Could you post the code you are writing and the error it is producing? Also how are you implementing this in the SSIS package? 
As written, your requirements are ambiguous, as a month value of "10" is greater than 9, so it should be in the 4th quarter, but 10 is also greater than 6, so it *could* also be in the 3rd quarter. I have no idea what "IF the month is &gt; 3 6 then the column value is 2nd Qtr" means at all. Here are the relatively standard fiscal quarters: Q1: 1/1-3/31 Q2: 4/1-6/30 Q3: 7/1-9/30 Q4: 10/1-12/31 Here is what it sounds like your desired quarters are: Q1 = 1/1-3/31 Q2 = 4/1-5/31 Q3 = 6/1-8/31 Q4 = 9/1-12/31 If my assumption of your desired quarters is correct, either of the below solutions will work, the first uses a CASE statement, the second uses IF/ELSE IF: DECLARE @OrderDate DATE = '2017-06-01'; --assume this is queried from your "Order" table DECLARE @Month TINYINT; DECLARE @YourQuarter TINYINT; SET @Month = DATEPART(MONTH, @OrderDate); --Option 1, using CASE SET @YourQuarter = CASE WHEN DATEPART(MONTH, @OrderDate) &lt;= 3 THEN 1 WHEN DATEPART(MONTH, @OrderDate) BETWEEN 4 AND 5 THEN 2 WHEN DATEPART(MONTH, @OrderDate) BETWEEN 6 AND 8 THEN 3 WHEN DATEPART(MONTH, @OrderDate) &gt;= 9 THEN 4 END; SELECT @OrderDate AS OrderDate, @Month AS Month, @YourQuarter AS YourQuarter; --Option 2, using IF IF @Month &lt;= 3 SET @YourQuarter = 1 ELSE IF @Month BETWEEN 4 AND 5 SET @YourQuarter = 2 ELSE IF @Month BETWEEN 6 AND 8 SET @YourQuarter = 3 ELSE SET @YourQuarter = 4; Now.... if you can, or are actually supposed to use the relatively standard fiscal quarters, you can just let SQL Server take the wheel, a la: DECLARE @OrderDate DATE = '2017-06-01'; DECLARE @Month TINYINT; DECLARE @StdQuarter TINYINT; SET @StdQuarter = DATEPART(QUARTER, @OrderDate); SELECT @OrderDate AS OrderDate, @Month AS Month, @StdQuarter AS StdQuarter;
Plenty of women in our IT department utilising a varuety of languages. Ability to think in a logical programmatical way is person based, rather than gender, in my experience. 
Post your code.
How are `$firstname`, `$lastname`, etc... being set? Those need to be pulled from the `$_POST` variable... `$firstname = $_POST["firstname"];` Since you're not even hitting the SQL request, this isn't really a SQL question/issue.. I'd check your PHP logic is valid and your variables are all being set properly.
This makes little sense to me. Why not just create the SQL with the joins? You will need to perform the join somewhere, whichever way you do it. Even if you write a proc to create it for you, you still need to tell the proc the details of the relationship between TABLE A and TABLE B, in each case, in order that it can create the join for you. 
The vessel can only be built once. What i'm trying to find is the oldest vessel on each route and display its name year and the route its on. the issue i'm currently having is if two vessels are built at the same time and have the same route only one shows. Thank you for the help!
yeah all of them are set above that code in same file: $message=""; $firstname = filter_input(INPUT_POST,'first_name'); $lastname = filter_input(INPUT_POST,'last_name'); $DOB = filter_input(INPUT_POST,'DOB'); $gender = filter_input(INPUT_POST,'gender'); $height = filter_input(INPUT_POST,'height'); $weight = filter_input(INPUT_POST,'weight'); $pm = filter_input(INPUT_POST,'pm'); $fa = filter_input(INPUT_POST,'fa'); $notes = filter_input(INPUT_POST,'notes');
I'm guessing this is the issue, but I would also look into using WHERE Flight.FlightDestination IN ('SLC','LAX') instead of the OR clause. It is easier to read and they give you the same result. 
Exactly what I needed. Thanks!
Cool! Thanks!
I figured it out! This was my final code: SELECT Flyer.FrequentFlyerID, Flyer.Name FROM (Flyer LEFT JOIN Trip ON Flyer.FrequentFlyerID=Trip.FrequentFlyerID) LEFT JOIN Flight ON Trip.FlightNo=Flight.FlightNo WHERE (Flight.FlightOperator="United" OR Flight.FlightOperator="Contintental") AND (Flight.MilesPerFlight BETWEEN 340 AND 560); 
I got an error saying it couldn't find the information_schema file when I tried that. Is there another way for me to check how the date is formatted? What are all the ways to format dates? I swear I've tried so many ways but none of them are working: different MM/DD/YYYY orders, with and without quotes, with dashes instead of slashes, etc. 
does not work? okay, replace TotalOrderAmount with the SUM expression in your ORDER BY 
You're using Microsoft SQL Server, correct? And SQL Server Management Studio? Can you take a screenshot and post that in a reply?
I changed it to SELECT SUM(UnitPrice * Quantity) AS TotalOrderAmount, ProductID FROM ProductsOrders GROUP BY ProductID ORDER BY SUM(UnitPrice * Quantity) DESC; Still no luck. It just outputs them in the same order. 
I’m using Access
By replication do you mean snapshot/transactional replication? Some people use replication to refer to any type of data moving from A to B. If you're using it as a read only copy you should just use log shipping with a STANDBY/READ_ONLY secondary unless you need real time updates. 
That changes things. Your post title says [MS SQL] which isn't the same thing. Not my expertise. Try right clicking on the table and see if there's a way to get the table definition. Maybe design mode or something like that.
My suggestion is do a subselect Do: select * from ( "your first query here ) t1 Order by sumofproducts
Yes, transactional. I didn't want to go down the snapshot route because I believe it's a full snapshot each time. I'd like as close to real time as possible for the analytics side. I wasn't sure once I made a publication of the database if it would need another 200GB. I don't want to start running out of disk space.
Have you thought about setting up a database snapshot? There are some restrictions / limits of it, though. https://docs.microsoft.com/en-us/sql/relational-databases/databases/database-snapshots-sql-server
I guess it depends on your reporting needs. I've dealt with replication enough to try and recommend against it unless necessary since it can be very finicky. If you have scheduled reports (hourly) you can run log shipping hourly a few minutes before they run so that will be basically up to date. If you have to go the transactional route, the subscriber tables will be the same size as the publisher tables. If you replicate all tables, the database will be the same size on both sides. Whichever database you point replication to, it will either drop and recreate the tables or truncate the tables when you initialize it. After that everything that happens at the publisher happens on the subscriber. A possible caveat though is if you want the database to be read-only, replication doesn't allow a read-only subscriber since all of the transactions are essentially re-run at the subscriber so it needs to be able to insert/update/delete. Other options like mirroring/log shipping just restore the most recent log backups so there is no change happening, allowing it to be read-only.
Can you provide some output here? I am failing to see why this one doesn't work. 
Friendly heads up, the way this is written, it's a little more accurate &amp;specific to refer to this as a derived table. Having worked with them extensively before the days of CTEs, I generally recommend and prefer CTEs in place of them because they read top down and allow you to chain/nest in a much more readable way. 
ok, i see what you are saying. i have tried things like mysql and MS SQL Server but they seem super complicated, i know sql's syntax and how to make and query a database but it is the server part that is difficult for me. 
Your FROM clause refers to the table Passenger but your inner joins do not refer to valid table names. 
&gt; In semi-pseudocode Going to guess their actual code is different from the example and they arent able to translate the working example to their original query.
Might be too late to the game, and maybe this isn't what you mean (I'm new to this still, maybe I'm over simplifying your question) but in MSSQLS you can "ORDER BY &lt;col #&gt;" for your results. Let's say you have 4 columns, and your total is column 2, you could "ORDER BY 2 ASC" to order by derived values. I know this works for normal derived data, like from CASE WHEN statements, but I believe it also works in functions as long as you have that many columns (ex. Can't order by column 6 if you only have 4 columns of results, same as you can't order by a field you haven't called in your SELECT). 
The store procedure expects 3 parameters being passed in when it's being called. The 3 parameters than are set to those other parameters. It's really not needed as far as I can tell. It's redundant. I would need to see the rest of the procedure to say anything else. 
Have a read about parameter sniffing. Setting the local variables with the passed in variables can circumvent issues with parameter sniffing, so it could be because of that. In terms of the code, as others have mentioned, it is otherwise redundant. 
Hi, try something like this: SELECT week_table.date+numbers.n as DAY_DATE, week_table.itemnr, week_table.value from ( select 0 as n union all select 1 union all select 2 union all select 3 union all select 4 union all select 5 union all select 6 ) numbers cross join week_table where year=2017 and week=46 and itemnr=454; 
&gt; Still no luck. It just outputs them in the same order. sorry, i'm not buying it... there's something else going on
you keep using the word "dataset"... a ~lot~ i do not think this word means what you think it means 
Interesting solution. It sort of works (select week_table.date+numbers.n as DAY_DATE needs to be - and not +). But I do get duplicate rows with 0 values. And the result date isn't in the desired format (2017-11-11 is 20171111, I suppose this is because we do calculations on it?)This is what I get (3 date example): DAY_DATE|ITEMNR|VALUE :--|:--|:--|:--|:-- 20171111|454|67 20171110|454|67 20171109|454|67 20171111|454|0 20171110|454|0 20171109|454|0 Is there any way to adjust the query to get proper output with the date format and remove the duplicates? Also any idea why the duplicate rows with 0 values? I'm not familiar with cross joins at all, but I'd love to learn. DAY_DATE|ITEMNR|VALUE :--|:--|:--|:--|:-- 2017-11-11|454|67 2017-11-10|454|67 2017-11-09|454|67
as someone who practices this shit for more than 13 years. this short book seems too daunting for what is actually explained. after all those pages of text we learned how to join two tables and 2-3 DML statements? I think all this can be explained in a much more simple, short and easy on the eye text.
The 0 results you receive probably because there are rows with these values in the database. You have to decide what to do in these case and which value to take. A cross join simpliy combines all rows from one table with all rows from the other table. that means in your case. you are combining 7 rows (with the numbers 0 to 6) with each week in your week table. to format your date, you can use str_to_date (string/number, format) function. Just change week_table.date-numbers.n TO str_to_date(week_table.date-numbers.n, '%Y%m%d') 
Thanks for your comment! Are you talking of the excerpt or the book? The excerpt doesn't contain any DML statement. And the joins are explained on a few pages only. From my experience as a trainer it's good not just to write the plain syntax of a join but to give some more explanations and examples. Not everyone is probably as familiar as you with programming and technical stuff and for those this book is intended to be written. 
thanks for that! I will check for the word "dataset". You have any suggestions which word is more suitable?
Thanks for the explanation. Appreciate it.
Your where clause has no expression. It's just a subquery. Are you missing a parenthesis here? SHRTATC_CRSE_NUMB_INST)) &lt;--
You're welcome. If you are interested in other SQL stuff you can also check out my blog: http://bi-solutions.gaussling.com
I totally agree with the idea of not just showing a joind syntax and thats it. What im saying is that I think that this book is over complicating things. it says Learn SQL in easy way. this is not easy in my opinion. This is your book and you can choose to tell it however you want. but the days of reading text over text and theories are over. This is much less complicated in my opinion. https://www.w3schools.com/sql/sql_join.asp
When bringing a new subject up for the first time you should spell it out instead of explaining the acronym. Example, you have "We use ER models (ER = entity relationship) " should be "We use Entity Relationship Models (ER models)". By the way they are usually referred to as diagrams and not models "ERDs" at least in my experience. A small nitpick but hopefully helpful.
I'm ok with the destination database being the same size, I couldn't figure out if it needed a ton of additional space on the source. Also, I guess I could just make the users read only on the destination database.
In case others are wondering, I found this blog post from someone who completed the course when it first came out: http://carstenfuehrmann.org/dbclass-summary/ This guy has a computer science background but wanted a more formal/academic understanding of databases. He successfully completed the series in 9 weeks and spent about 14 hours per week on it. He also added that it could take others more or less time depending on their background and/or how much they want to delve into the material.
Nothing changes on the source space-wise. The only change in space is where the snapshot is kept
sqlite allows this query to pass? You have to group by vessel name as well, unless I'm missing something. Does sqlite assume you want min vessel name too or something? Strange.
I think what happened is that I clicked the "sort" button on the top of the output table, so whenever I would run the query it would automatically sort by ProductID instead of TotalOrderAmount in the output that was displayed
Yeah that's a big wall of text. Doubt anyone will come and debug this for you, but maybe someone feels exceptionally helpful today. My recommendation is to a) format this and b) resolve each error individually until they are gone. 
I would do this: Create a #temp table. Populate #temp table with select statement. Insert into...select * from #temp table. Output #temp table. Maybe wrap in a transaction for integrity if needed.
Yeah, /u/Cal1gula is right... probably not gonna debug that for you in depth. But when you DROP TABLEs, shouldn't the table names be wrapped in single quotes?
the term used in relational theory is *tuple* however, since this is supposed to be an easy way, say **row** instead
WHERE might not work, but the logic wouldn't either. The YEAR of the [SnapshotDate] will never be 2018 when the [Snapshot Date] is '11/05/2017'. What is the plain English here? You want the number of Snapshots in 2018 with a CloseDateKey of 0 and a ChargeOffDateKey of 0, but how does the Sunday before last (11/05/2017) come into play?
Don’t know much MySQL but in MS SQL i would do this: If OBJECT_ID(‘tablename’) IS NOT NULL DROP TABLE ‘tablename’; You can write a line for each table in your query, its probably gonna take a short while but i usually so this in my test environments so i don’t have to restore backups all the time. Not sure if this exists in mysql tho...
Sorry for the confusion. I meant to put '01/05/2018' for the snapshot date. So, when the snapshot's year is 2016 or 2017, I have these static lists put out for each branch number. But whe the year is 2018, instead of inserting a static number of accounts for each branch, it will look at how many accounts are existing with the criteria of Closedatekey = 0 and basically do a count (or in this case, because each record will be a 0 or 1, it will sum up the 1's). And in my code, I will actually replace the static snapshot date with a DATEADD which looks at the snapshot date from the 5th day of the corresponding year 
Can you just add the criteria to your case statement? When YEAR(Snap.[Snapshot Date]) = 2018 THEN ((sum(CASE When SD.CloseDateKey = 0 and SD.ChargeOffDateKey = 0 and Snap. [Snapshot Date] = '11/05/2018' THEN 1 ELSE 0 END)) I'm not sure if this is what you're looking for because I can't tell what you are trying to accomplish.
I'll give this a try in the morning and let you know
good point. Thanks!
yeah, row might be better. Thanks!
yeah, maybe I could remove some things from the join chapter. Thanks!
I think the sum(....) won't work in a where. Before you use sum() you have to group and then you can use sum in the having clause....
Yeah, I was hoping to use a Where in a Select somehow. I think I might accomplish this by doing the query with the static numbers and the one based on snapshot separately, and use a Union All to accomplish what I need. Hopefully...
Not sure about DB2 but this resource is awesome and free: https://www.brentozar.com/training/think-like-sql-server-engine/1-clustered-index-21-minutes/
I've never set up an Any rule *or* used root to access a database, so I'm not sure how to diagnose the set up you've got right now if you're committed to it - I've always created a system user for each database I create, so that PHP scripts I run just assume *that* user and don't have access out-of-scope. I would create another user, say "peopleuser," and then give it all privileges to the db "people."
Doesn't look like you are logged in with any user at all actually. The user should come before the @ so the error literally says Access denied for user ` ` @ hostname `localhost` to database `people` Try here: https://stackoverflow.com/questions/21714869/error-1044-42000-access-denied-for-root-with-all-privileges
&gt; I've never set up an Any rule or used root to access a database, so I'm not sure how to diagnose the set up you've got right now if you're committed to it - I've always created a system user for each database I create, so that PHP scripts I run just assume that user and don't have access out-of-scope. Not committed by any means. I'm only using this for a coursera course and won't have any impact on my professional work (for now). I'd like to be comfortable using these concepts within the next 12 months so I'm starting to dabble in it now. 
Well that makes a lot more sense, thank you. I'll take a look and give you a shout if I'm still confused. 
&gt; Can you guys ELI5 indexes grab a white pages phone book... ask them to list all the phone numbers of people whose last name is Farquhar (equivalent to an indexed search) now ask them to list all the phone numbers of people living on Chestnut Street (equivalent to a table scan)
This is great, thanks! I get the single column indexes, that makes logical sense. But our DBs are full of indexes made up of several columns, like Product ID + Customer ID + Order Date. For those indexes, do you need all 3 of those columns present in order to call the index? And those 3 columns have to be in the WHERE clause, right?
You can view more detailed error messages here: select * from USER_ERRORS where lower(NAME) = 'get_itinerary' Off the top of my head, I believe an exception has to be at the very end of a block. So either move it to after "END LOOP; CLOSE cpn" or make the inside of the loop a sub-block and put it at the end of that. Also, you'll need to declare opn.
OK, so I wanted to provide more of an explanation to my problem, and provide some details on the solution I found. First, some info on my data structure. My company just finished building a data warehouse this year. Up until June, we only had month-end snapshots of data on new accounts, but some of our reporting goes off the starting number of accounts based on like the 5th of January of the current year, and compares YTD numbers after that. So, to build this reporting I didn't have a Jan 5, 2017 snapshot, I only had January 31st. So, I had to simulate having the 5th of Jan data by looking back at some existing static reporting, and manually input the "start" numbers for this year and last year for each of our branches. Going forward however, I can point to the 5th day of the year. So, I had a situation where I wanted to combine the tables containing static outputs, and a table which will dynamically point to the 5th of the year starting in 2018. What I had to do is create about 5 temp tables and do a Union All to sync the two together in one continuous table. Then, since we use the BI tool Tableau, I had to insert these temp tables as Initial SQL in there to bypass Tableau's inability to process CTE's and temp tables Overall, quite a productive morning.
OK, so I wanted to provide more of an explanation to my problem, and provide some details on the solution I found. First, some info on my data structure. My company just finished building a data warehouse this year. Up until June, we only had month-end snapshots of data on new accounts, but some of our reporting goes off the starting number of accounts based on like the 5th of January of the current year, and compares YTD numbers after that. So, to build this reporting I didn't have a Jan 5, 2017 snapshot, I only had January 31st. So, I had to simulate having the 5th of Jan data by looking back at some existing static reporting, and manually input the "start" numbers for this year and last year for each of our branches. Going forward however, I can point to the 5th day of the year. So, I had a situation where I wanted to combine the tables containing static outputs, and a table which will dynamically point to the 5th of the year starting in 2018. What I had to do is create about 5 temp tables and do a Union All to sync the two together in one continuous table. Then, since we use the BI tool Tableau, I had to insert these temp tables as Initial SQL in there to bypass Tableau's inability to process CTE's and temp tables Overall, quite a productive morning.
OK, so I wanted to provide more of an explanation to my problem, and provide some details on the solution I found. First, some info on my data structure. My company just finished building a data warehouse this year. Up until June, we only had month-end snapshots of data on new accounts, but some of our reporting goes off the starting number of accounts based on like the 5th of January of the current year, and compares YTD numbers after that. So, to build this reporting I didn't have a Jan 5, 2017 snapshot, I only had January 31st. So, I had to simulate having the 5th of Jan data by looking back at some existing static reporting, and manually input the "start" numbers for this year and last year for each of our branches. Going forward however, I can point to the 5th day of the year. So, I had a situation where I wanted to combine the tables containing static outputs, and a table which will dynamically point to the 5th of the year starting in 2018. What I had to do is create about 5 temp tables and do a Union All to sync the two together in one continuous table. Then, since we use the BI tool Tableau, I had to insert these temp tables as Initial SQL in there to bypass Tableau's inability to process CTE's and temp tables Overall, quite a productive morning.
I had done that originally, but when I put DECLARE opn VARCHAR(50); above "CREATE OR REPLACE PROCEDURE," and then run in iSQL, I get: CREATE OR REPLACE PROCEDURE get_itinerary(givenresid IN flight_reservation.rid%TYPE) * ERROR at line 3: ORA-06550: line 3, column 1: PLS-00103: Encountered the symbol "CREATE" when expecting one of the following: begin function pragma procedure subtype type &lt;an identifier&gt; &lt;a double-quoted delimited-identifier&gt; current cursor delete exists prior 
&gt; select * from USER_ERRORS where lower(NAME) = 'get_itinerary' [Here is that output](https://imgur.com/a/4PlH7). I had done a much simpler SHOW ERRORS before and got essentially the same result; that end of file/EXEC were coming too early. To me, that reads that I didn't properly state the end of the procedure and it's coming up on an unexpected natural end, but I can't see where I've failed to define the end of the procedure? There is a BEGIN and END.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/VOLDPOv.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dpyue4w) 
There's also perverse workaround that involves MERGE with a match condition that will never pass, so it only ever inserts, see below: WITH cte_s (UserName, CustID, UserPassword, EmailAddress, FullName, Account, SuperUser) AS ( SELECT cc.ContactName, ci.CustID, 'D8675309$$%a', cc.C_Email, ci.CompanyName, 102, 0 FROM ReportData.dbo.Customer_Contact cc JOIN ReportData.dbo.Customer_Information ci ON cc.CustID = ci.CustID WHERE cc.C_Email IS NOT NULL ) MERGE WebReports.dbo.RPG_Users t USING cte_s s ON 1=0 WHEN NOT MATCHED BY TARGET THEN INSERT (UserName, UserPassword, EmailAddress, FullName, Account, SuperUser) VALUES (s.UserName, s.UserPassword, s.EmailAddress, s.FullName, s.Account, s.SuperUser) OUTPUT Inserted.RPG_Users_ID, --the ID you would get out of SCOPE_IDENTITY() s.CustID INTO @NewUsers;
Looks like it's reading in the "EXEC" as part of the "CREATE"... What sql client are you using? Try removing the EXEC line. You may need a forward slash (`/`) at the end, but try it without first.
In a procedure, you don't use the keyword DECLARE. It would go after your IS: CREATE OR REPLACE PROCEDURE get_itinerary(givenresid IN INT) IS cursor cpn is SELECT pname FROM passenger INNER JOIN flight_reservation ON flight_reservation.pid = passenger.pid WHERE flight_reservation.rid = givenresid; opn VARCHAR(50); BEGIN
iSQL*Plus 9.2.0.6.0. I think its a few versions outdated, but it's a university. Interesting, without the EXEC at all - and moving the declared opn from your other comment, it says "Procedure created." Then in a separate query with the EXEC, it returns "Jeff" (the correct value). Thanks! Now for the rest of the task, where this needs to be beefed up a lot more...
I've had to do this once or twice in about a year of working with procedures, hasn't been super common but it's a nice tool to keep in the back of your head. Has to do with SQL caching the execution plan. As already stated, check out [Parameter Sniffing](https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/). In my experiences it's been incredibly helpful on reducing the execution time on procedures that work with variable sized parameters.
Okay, so this is strange. Got it to work with your feedback, but then when I added the other cursors/lookups required by the task, the procedure still creates and executes successfully, but just returns...blank. Nothing. I'm using the same givenresid (1) for my testing, so I know there's definitely data, and again, the SQL statements themselves all work fine... do I have too many cursors / am I not opening them correctly? https://pastebin.com/2nR31dXA
I've tried this and I'm missing something with the OUTPUT still... BEGIN CREATE TABLE #t( CID char(6), CName varchar(50), PWD char(12), CMail varchar(150), CoName varchar(100), Acct int, Super int ) INSERT INTO #t SELECT ci.CustID, cc.ContactName, 'D8675309$$%a', COALESCE(cc.C_Email, cc.C_Fax + '@reed.fax'), ci.CompanyName, 102, 0 FROM ReportData.dbo.Customer_Contact cc INNER JOIN ReportData.dbo.Customer_Information ci ON cc.CustID = ci.CustID WHERE ContactName NOT LIKE '%DO NOT%' AND ContactName NOT LIKE '%NO UP%' AND ci.CustID NOT IN ('002194', '006291') DECLARE @NewUsers TABLE ( ID int, CID char(6) ) DECLARE @RPGU TABLE ( UID int IDENTITY(1,1), UserName varchar(50), UserPassword varchar(50), EmailAddress varchar(200), FullName varchar(50), Account int, SuperUser bit ) INSERT INTO @RPGU (UserName, UserPassword, EmailAddress, FullName, Account, SuperUser) OUTPUT SCOPE_IDENTITY(), #t.CID into @NewUsers SELECT CName, PWD, CMail, CoName, Acct, Super FROM #t SELECT * FROM @RPGU DROP TABLE #t END 
Sorry, I don't know why it won't take the formatting of the last couple of lines of code. 
&gt; For those indexes, do you need all 3 of those columns present in order to call the index? no this index would be utilized whenever you provide a WHERE condition that requires a specific value for one, or two, or three of those three columns, *working from the left* recall the white pages, which are in sequence by last name, then first name or initial if i said find all guys named Todd (last name unspecified), then the index is ignored and you're back to table scan (reading the whole book)
What RDBMS is this for? MySQL? The problem has to be in your WHERE clause. &gt; ... WHERE `type` = 5 AND `spec_db_auth`.`name` = `us_id_table`.`id` AND `us_id_table`.`uid` = `aulis_id`.`uid` ORDER BY `name` The error implies that the problem is that the collation doesn't match between the columns you're comparing between separate tables. You probably need to find out how to discover the collation of each of these columns so you can handle it accordingly. I googled the error and got several stack overflow results. 
the question is where I have to put it, because putting it after each select elements didn't work.
 SELECT us_id_table.uid, us_id_table.db_name AS 'elop' spec_db_auth.name, aulis_id.name AS 'suin' FROM spec_db_auth, us_id_table, aulis_id WHERE type = 5 AND spec_db_auth.name = us_id_table.id AND us_id_table.uid = aulis_id.uid ORDER BY name A little bit of code formatting goes a long way to finding syntax errors, and much of the time it's a missing comma - you don't have one after `'elop'`.
 SELECT us_id_table.uid COLLATE utf8_general_ci, us_id_table.db_name AS 'elop' COLLATE utf8_general_ci, spec_db_auth.name COLLATE utf8_general_ci, aulis_id.name AS 'suin' COLLATE utf8_general_ci FROM spec_db_auth, us_id_table, aulis_id WHERE type = 5 AND spec_db_auth.name = us_id_table.id AND us_id_table.uid = aulis_id.uid ORDER BY name doesn't work for some reason.
I'm not too familiar with MySQL. But, I'm seeing alot of clues by googling. I think you should get a better handle of the problem before you try to resolve it. MySQL has the INFORMATION_SCHEMA. I recommend that you execute Select * from INFORMATION_SCHEMA.TABLE where table_name in('table1','table2', etc) As well as a similar query to INFORMATION_SCHEMA.COLUMNS. Both views have collation type columns per the MySQL documentation. I think you need to understand what collations are in play and what doesn't match, and what doesn't match what you see in dev. I'm on mobile so I can't really type out better queries.
Try putting the `COLLATE` in the where clause. Transformations applied in the SELECT don't automatically apply to those columns in the WHERE. 
Got it, really appreciate your help!
Try this at the beginning: set serveroutput on size 1000000
I'll be damned, that did it. Could you give me a quick explanation of what that changed? Does it change the output buffer or something? This is for a college course, and the instructor never went over this (that said, I'm probably not following best practices by having so many cursors)
Could you post a small sample set of the data with rows and columns? The structure is unclear from your description or I'm just not following.
Not that person, but it basically enables DBMS OUTPUT, or at least allows you to enable it. https://docs.oracle.com/cd/B19306_01/appdev.102/b14258/d_output.htm#i1000634 “Typing SET SERVEROUTPUT ON in SQL*Plus has the effect of invoking DBMS_OUTPUT.ENABLE (buffer_size =&gt; NULL); with no limit on the output.”
Yeah. He should probably mock up an excel sheet and maybe post a screen of it or something. 
Well first I would write a query that generates every two-bus route. We're gonna have two route tables, route1 and route2. All you need to know really is the one stop where the two buses meet. So route1.Stop = route2.Stop Also we can't do this at route1's first stop: route1.POS&gt;1 And we probably shouldn't allow this for route2's last stop, so we're gonna get correlated route2.POS&lt;(select max(POS) from Route r2end where r2end.Num=route2.Num) Now, somewhere on route1, prior to the current stop, it has to have a stop at Craiglockhart Exists (select * from Route r1Craig where r1Craig.Num=route1.Num and r1Craig.POS &lt; route1.POS and r1Craig.Stop = (select min(ID) from Stops r1stops where r1Stops.name='Craiglockhart')) And somewhere on route2, subsequent to the current stop, it has to have a stop at Sighthill. Exists (select * from Route r2Sight where r2Sight.Num=route2.Num and r2Sight.POS &gt; route2.POS and r2Sight.Stop = (select min(ID) from Stops r2stops where r2Stops.name='Sighthill')) So that's how I would do it, but I don't want to get out of bed on a Saturday morning to actually try it. Also, pretty sure I broke most of the rules i learned in query optimization class :-/
Off topic: You should play around with SQLBolt.com, Hackerrank.com, and codewars.com. They're much better than SQLZoo.
Char
Thank you, I need lots of practice so I will try those next.
Varchar (size)
Char, varchar, nchar and nvarchar. Varchar is likely to attend Allan your alphanumeric needs 
So the key here is your route tables. What is routeA doing? With the stop join, we're limiting down to just routes that contain our origin, great. RouteC is doing the same with the destination. But what is routeB doing? routea.num=routeb.num routeb.num=routec.num So by easy maths routea.num=routec.num Wait what? Let's revisit your aim : * find routes going to Craiglockhart * find routes going to Sighthill * filter to the routes that intersect i.e. have a common stop You have the first two happening, but you're combining them wrong. I think you'll need another join beyond what you have already. RouteA: get the routes with the origin stop RouteA-1: get all the stops that are on the routes in routeA RouteB: get the routes with the dest stop RouteB-1: get all the stops that are on the routes in routeB And now combine them! Find the stops routea-1 and routeb-1 have in common Eg ON routeA1.stop = routeB2.stop ... You can ignore the extra POS filtering from the other commenter, I got the correct answer without any of that. 
Sql_variant. *runs away*
Sqlzoo is easily better
I already completed everything except this question lol, this question is the final boss I keep dying at.
* In a query that's only pulling data for a single region, the second ("wide") option would perform very slightly better. All that it saves you relative to the first ("tall") option is an index seek to find the rows for the appropriate region, which is very quick. * In a query that needs to pull data for multiple regions, the first option would perform significantly better and eliminate the annoyance of writing queries that UNION together data for all of your regions. (I don't think UNPIVOT exists in SQLite.) Overall, I'd strongly recommend the first option. But you'll want to put some thought into how to index the table - consider the column order in your clustered index, whether to add nonclustered indexes in addition to the clustered index, and whether it makes sense to have a ROWID based on your anticipated usage.
That's probably going to be handled in Rust, not in SQL. You'd probably be better off posting in a Rust forum instead of a SQL forum.
The same way you do it with any other language - use prepared statements through your drivers and it should be handled (or at least 99% handled). https://github.com/sfackler/rust-postgres And from the readme: https://github.com/sfackler/rust-postgres#statement-preparation
Consider future maintenance. If you ever add or remove regions, you'll have a lot more work to do with the second option. With the first option, you can more easily create indexes to make your queries run faster. 
Trying, so stuck :( I did find the answer online though, so I am going to try to work backwards.
Oh, sorry I should have clarified. I did have SET SERVEROUTPUT ON; above the code originally to make DBMS_OUTPUT work. What I didn't have was the SIZE parameter for 1M. Wondering what the SIZE did.
Never ever ever use string manipulation to create SQL statements. 
I have to disagree here. When I work with the app/mid tier teams I always assume they are doing 0 validation on their side. It may seem a bit excessive, but if their code fails to catch something, at least I know my code will catch it. My team does validation/mitigation at all tiers (from text box to stored procedure).
Yeah, I guess I'm more of a developer than a DBA (small company, wear lots of hats) and philosophically I've always thought of input sanitization as the responsibility of the application layer, not the database layer. Probably a sign of healthy skepticism that both of us consider it our responsibility though. Side note: I recently deployed a public Rails application that (as I realized at 2 AM one night) would allow you sysadmin access to SQL Server with a simple '; in the username or password of the login form. The next morning I went early and tried '; create table problem ( oh_no varchar(max) ); select 'hi and almost had a heart attack when I refreshed SSMS and saw that a new table called "problem" had been created. 
Haha I get where you’re coming from. After I went from developer to DBA I’m just known as the “sky is falling” guy. I’m a cross between chicken little and Branch from Trolls. That sounds like a rough way to wake up in the middle of the night. Glad you caught that!
the guys in the backend don't do inserts and call stored procedures? (never worked in a big project)
Which back end? The middle tier or the database? If database, then yes, my team (database) only allows an application to call stored procedures or functions to get or set data. No other transactions allowed. We validate and assume the apps and middle tiers are stupid and can’t validate. Everyone does (middle and app tiers too). Some call it excessive but we hold down the fort at all tiers.
It should be noted that what's actually doing the injection protection is the parameterization, not the preparation. PostgreSQL doesn't really make a distinction because of how it's implemented, but a lot of languages and RDBMSs do. If you're concatenating strings to build your query and calling a Prepare() method, it's not making it injection-safe. 
you're 72 hours away from a 16-minute presentation and you need ideas? whoa
&gt; I'll **happily take** input/ideas &gt; on how you Can show a &gt; *deep understanding* of SQL I hope you have some understanding of SQL to being with... Answere some these questions and you might have a presentation. What are the parts of SQL language? What are the parts of SQL querry? Is SQL a programming language? How is SQL different from a Programming Language? What does the order of this list signify? * FROM * ON * OUTER * WHERE * GROUP BY * CUBE | ROLLUP * HAVING * SELECT * DISTINCT * ORDER BY * TOP There .. that should cover 16 minutes.
I'm guessing you'd probably get some points for briefly going over ACID principles and normal forms if it's a RDBMS course.
http://use-the-index-luke.com — A guide to database performance for developers. (disclaimer: my thing. It's the free web-editon of my book "SQL Performance Explained").
Have you not been provided a brief or outline of what they are expecting (or a scoring matrix) or anything like that? If so, that is your starting point - ensure you cover each point they want. As for general presentation tips, practice your timing at least once so you know what 16 minutes feels like. Say it aloud - reading and speaking times are often different. Try not to just read from a piece of paper/the screen. Often just bullet points of the very general headings you are to discuss on the slides then speak the detail. Have a good understanding of what points you need to cover and a feeling for what detail. If this is a strict 16 minutes timescale then have a time keeping device on you and be aware of sections you can either go into more detail or less if you find you are going to finish too early or if you are going to run out of time. 
Use a CASE statement
Hmm yes introduction, middle, end/conclusion
This guy is a riot, thanks for bringing him into my life. 
This is spam.
I have one table with attributes (ingredient0, ... ingredient7), and another table with attributeas (ingredient). I want to get count for each tuple representing how many of its ingredients are in the second table. 
What’s a cube/roll up?
just sum the coupon amounts for the items you are going to be calculating, put that in a temp table and then in a final select get the actual results you want. (you can do this other ways like sub-queries) create table #couponamount ( item, sumcoupondiscountamount ) insert into #couponamount ( item, sumcoupondiscountamount ) select item, SUM(coupondiscountamount) as sumcoupondiscountamount from coupondiscount where item in (select item from itemlist) group by item select il.item, il.soldprice, ca.sumcoupondiscountamount from itemlist AS il inner join #couponamount AS ca on ca.item = il.item 
You need a count of combinations? or a list of combinations? A count would just be math. It's been many years, but if my memory is right, (Ice Cream Flavors) * (MixIns) * (Toppings). 2 * 5 * 4 = 40. If I understand your data correctly, a list of Combinations could be a cross join. You could do a Count(1) on the cross join. Doing the math would be more efficient than querying the database. I'd look at how many times the query will run and how the app uses it.
Thanks. I need a list of unique combinations with a count for each one. I'm not overly concerned about performance since the query would only occur once per event once all records are created.
Thanks. I need a list of unique combinations with a count for each one. I'm not overly concerned about performance since the query would only occur once per event once all records are created.
A list of unique combinations, the count would always be 1, they are each unique.
Currently in a Business Intelligence Analyst position, I do not use either. However I hear of it being quite common in other work-places. It is going to heavily depend on what company you are with.
The party organizer doesn't need to know who wants what, so InviteId will not be in select list. Therefore, one request for Topping IDs 4, 7 and 19 would be the same as another. It should be assumed, however, that each sundae comes pre-assembled, so simply knowing the requested quantity of each item separately is not quite enough. 
I’m in an operations role. We have a lot of data from a lot of systems. Python/Pandas skills have been a life saver in my role. Often I can’t wait for enterprise solutions to meet my analytic needs. I need something quick and now...I’ve automated several tasks of pulling, mostly Excel, data from different places. I’m excited to brush up on my SQL and combine it with Python to take it to the next level. 
Not an answer to your question, but what stats courses are you taking? I've considered doing the same.
I'm in ops and have written some basic R scripts to essentially save me 100s of hours of excel work. I believe the tools can be useful in any role with a slight technical skew. 
R is being used in a lot of jobs that usually require something like Stata, because R is freeware. 
I have used Python for automation of random stuff, set one up to search shared drives for specific report files, then kick off a load to get the data into SQL. Did another to build a project tracking tool with GUI. My next project for python is to build a GUI search tool for someone using a postgre backend. For R, I have only used it for stats type predictive modeling. My company looks down package installs, and the R packages aren't locked down. I have to stay basic with Python
What exactly do you mean by "host my database on a web page"? Do you just need to display data?
mostly youtube, coursera and udemy. 
Yep, just need to display the results of queries 
so you will need to have some way of querying and rendering the data. If you are familiar with python, that might be the easiest way. I'm more familiar with django but that might be a bit much for your need. I would suggest flask. A quick search pulls up a few links to doing just what you asked. [Python to display MS-SQL](http://www.c-sharpcorner.com/article/create-python-flask-web-application-and-display-sql-records-in-browser/) [flask-display-database-from-python-to-html](https://stackoverflow.com/questions/45558349/flask-display-database-from-python-to-html) [simple-tables-in-webapps-using-flask-and-pandas-with-python](https://sarahleejane.github.io/learning/python/2015/08/09/simple-tables-in-webapps-using-flask-and-pandas-with-python.html) If it were up to me, it seems like the first link might be the path of least resistance. Hope it helps. 
Write a python method to query and return the data from your SQL instance. Expose it through an API and just use jquery to call it with an Ajax request.
D3 is your friend. https://d3js.org
&gt; CUBE | ROLLUP In simple terms it's like sub totals for each level of grouping. 
I am not sure if that works. A constraint is just some kind of condition that has to be met in order to input data in a column. But you want to change a value based on a condition. What might work is that you create a database trigger that checks for the people attending each time you insert a new row. When it reaches 20 then you can change the ticket price. If you like you can check out my blog for an article in which i describe how to create a trigger in oracle: http://bi-solutions.gaussling.com/how-to-create-a-database-trigger/
Hmm. I know an Oracle solution. And I've read that MS SQL from version 2017 on also offers this function: SELECT TOPPING_COMBINATION, Count(invite_id) FROM ( SELECT listagg(topping_id,',') WITHIN GROUP (order by invite_id) AS TOPPING_CoMBINATION, invite_id from tbl_icecream group by invite_id ) x GROUP BY TOPPING_COMBiNATION; The function in MS SQL is called: String_agg. Or what you could also do is something like this: SELECT TOPPING_01, TOPPING_02, TOPPING_03, TOPPING_04, TOPPING_07, TOPPING_19, COunt(invite_id) From( SELECT SUM(decode(topping_id, 1, 1, 0)) AS TOPPING_01, SUM(decode(topping_id, 2, 1, 0)) AS TOPPING_02, SUM(decode(topping_id, 3, 1, 0)) AS TOPPING_03, SUM(decode(topping_id, 4, 1, 0)) AS TOPPING_04, SUM(decode(topping_id, 7, 1, 0)) AS TOPPING_07, SUM(decode(topping_id, 7, 1, 0)) AS TOPPING_19, invite_id FROM TBL_ICECREAM GROUP BY invite_id ) x GROUP BY TOPPING_01, TOPPING_02, TOPPING_03, TOPPING_04, TOPPING_07, TOPPING_19
For the 2nd solution you have to add columns for each Topping Id you have,
I'm not in data science, and don't use python daily more like weekly and use it for automation, and pivoting (because i don't like writing pivots in SQL).
Interesting. Can you provide details on how you integrate your code into your workflow? How do you run the python scripts, and on what data, xlsx/db/csv? 
 SELECT one.ingredient0 , one.ingredient1 , one.ingredient2 , one.ingredient3 , one.ingredient4 , one.ingredient5 , one.ingredient6 , one.ingredient7 , COALESCE(two0.ingredient,0) + COALESCE(two1.ingredient,0) + COALESCE(two2.ingredient,0) + COALESCE(two3.ingredient,0) + COALESCE(two4.ingredient,0) + COALESCE(two5.ingredient,0) + COALESCE(two6.ingredient,0) + COALESCE(two7.ingredient,0) AS total FROM table1 one LEFT OUTER JOIN table2 two0 ON two0.ingredient = one.ingredient0 LEFT OUTER JOIN table2 two1 ON two1.ingredient = one.ingredient1 LEFT OUTER JOIN table2 two2 ON two2.ingredient = one.ingredient2 LEFT OUTER JOIN table2 two3 ON two3.ingredient = one.ingredient3 LEFT OUTER JOIN table2 two4 ON two4.ingredient = one.ingredient4 LEFT OUTER JOIN table2 two5 ON two5.ingredient = one.ingredient5 LEFT OUTER JOIN table2 two6 ON two6.ingredient = one.ingredient6 LEFT OUTER JOIN table2 two7 ON two7.ingredient = one.ingredient7 
Well like I said, automation. So I have scripts that run every x minutes/hours/days via task scheduler or cron, scripts that run on new FTP uploads. Some of them just clean/change (pivot) up the data or transform them in the correct format (whether that is XML or CSV). Some of them also upload files and/or email reports to customers or suppliers. The craziest script I have is an xml parser that: * looks for xml's in an error folder * parses the data from that file * runs SQL queries based on the data and error message * Starts up a GUI program (it has no API) in a VM and uses pyautogui to "manually" insert the data from the XML message or emails the responsible department that they FUBAR'ed
Mdm or ssrs?
If it can be just a html page in your computer, then import the query result into excel and save it as html :) 
I’m a data engineer that is also involved heavily in the backend of our cloud environment. I use python quite a bit for ETL tasks. It was either that or learn JavaScript. Depending on your goals (whether you want to focus on BI or go more into back end architecture), learning a programming language can really give you a leg up. From what I’ve seen, AWS and Azure are changing the way people do things, and now 90% of architecture and infrastructure up there is code of some kind.
If you’re just looking to display data, just go with PowerBi, it’s free and works with Express. You can be up and running in a day.
Stop giving terrible advice
Fellow BI'er. I see it used more in actuarial work. My cousin is a credit modeler for a bank and makes use of it.
What exactly is "terrible"?
OP barely knows how to code and you're suggesting that D3 will solve his problems? First off, he would still need some sort of backend to query the DB (this is his biggest problem and D3 doesn't solve it). Second, he doesn't mention needing to graph the data, he can just put it in a table, so there's no need for a graphing library like d3. Third, even if he did need to graph the data, D3 is so complicated that countless wrapper libraries have been built around it to simplify it for novice developers, so any of those are probably a better fit than D3 for someone who "[doesn't] know ASP or PHP, but [is] familiar with Python, although would prefer not to have to code too much."
This is like someone walking into AutoZone asking what kind of oil to put in their car and the person behind the desk selling them on a super complex DIY paint job.
Don't use ID values for ordinals, sorting, time spans, or anything at all regarding the data other than a value that uniquely identifies the row in the table against the other rows. Most likely the server was restarted and the sequence for the values was reset (on Oracle I believe this is determined by the CACHE value). Someone also could have run a query to manually restart the ID as well.
Ah, ok, thanks.
Link to screenshot: https://ibb.co/fvs2Cm Thanks for helping out though!
You can use a SUM in a SubQuery to total up discounts for an item. But with a billion rows, that would most likely be pretty slow. Or, you can do a join, and have the Discounts as a SUM OVER PARTITION BY Item#, and use a SELECT DISTINCT in your top level select. Pseudo-code: Select Distinct MainTable.Item, MainTable.ItemPrice, Sum( DiscountAmount) Over (Partition By Item) as TotalDiscounts From MainTable Join DiscountsList ON MainTable.Item = DiscountsList.Item Or you can use the temp table to pre-aggregate the discounts. With a that many rows, I'd also be wary of this method.
Unless you need to store UniCode Data... like Chinese/Japanese/Arabic characters. In which case the Nxxxx's are required. But they take up additional space - so its really up to what you are going to be storing, and if you think youll need UniCode data.
Based on the limited description from OP, i thought OP wanted an offline way to view and access the resultset. Yes, D3 is complicated but it has good ootb library support for doing a bunch of things.
I've set up a few transactional replication environments. If you want near-real time reporting it's one of the best ways to go. I'd just restrict your users to read only on that database. Also, generally most people don't need ALL the data replicated. So if you know what your target tables used for your BI Reporting are - you can probably save some space on the subscriber.
S: drive is most likely a network drive, mapped for your user. S: drive is most likely not mapped for the SQL Service user account. Try with a UNC.
The S: drive is a network drive which has all of the SQL data files on it, I've attempted to copy the back up to the same drive but I have the same issue. 
"Network drive" only applies to the user you are logged in with, hence my post. Since the SQL Server service account most likely is not your user... the S: drive is probably not there.
You're suggesting one of the hardest javascript libraries when he only knows python. you're throwing him off in the deep end. Any d3 wrapper would have been a better option, like nvd3 or metricsgraphics.
I'll sell you my SQL Saturday presentations that can take anywhere from 15 min to 1 hr. Only asking for $15,000 a presentation.
Sigh. Yes, you are right. I stand corrected. 
How are those IDs generated? If it is driven by a sequence, that can be dropped/recreated with a starting number lower than the current sequence nextval.
In my current position, I am almost exclusively using SSMS and SSRS, and of course Excel as needed. How about you?