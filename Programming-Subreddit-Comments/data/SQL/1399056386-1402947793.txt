No problem. I know how hard it is to find legit Informix help. Most people haven't even heard of it.
apart from you have to put the fields in the partition by list! swings, roundabouts...
I've actually run into this problem too, and I couldn't come up with a great solution. Below is the code I use, but it requires variable names to be 8 characters or less which is very limiting. Otherwise it works like a champ for hundreds of thousands of records. I wrapped it in a macro, since I run it a decent amount. It first drops a temporary table and a backup table if they exist. It then uploads all the data into a tmp table. It renames the previously existing table to a backup table and renames the tmp table to the destination table. The purpose of this is to replace existing data with new data without having any downtime. Sometimes the loads can take a while, so that's why I use the tmp table. Not the best solution, but it works for me. %macro loadData(input, output); proc dbload dbms=odbc; dsn="sdsdatam"; sql drop table if exists tmp_&amp;output.; run; proc dbload dbms=odbc; dsn="sdsdatam"; sql drop table if exists bak_&amp;output.; run; proc dbload dbms=odbc; dsn="sdsdatam"; sql create table if not exists &amp;output. (id bit); run; proc dbload dbms=odbc data=&amp;input.; dsn="sdsdatam"; table=tmp_&amp;output.; limit=0; load; run; proc dbload dbms=odbc; dsn="sdsdatam"; sql rename table &amp;output. to bak_&amp;output., tmp_&amp;output. to &amp;output.; run; proc dbload dbms=odbc; dsn="sdsdatam"; sql drop table if exists bak_&amp;output.; run; proc dbload dbms=odbc; dsn="sdsdatam"; sql drop table if exists tmp_&amp;output.; run; %mend loadData;
try isnull(convert(varchar(12), eval_date, 120), '') Your problem is a implicit type conversion, by converting the dates to varchar, im quite positive, you wont have that problem I have to ask thou, you want to "work" with the dates, so you convert it to varchar? That doesn't sound to good to me, please tell what exactly you want to do there. /edit : the code snippet is valid for MSSQL, I had a loooong day, no idea if the codepage for the convert is working on your DBMS, you still got to convert the date to a varchar, and then wrap it in the isnull
haha i love your name. and thanks that makes sense now
isnull() on TSQL will always return the first arguments datatype, so your guess is quite correct. Thats also the big difference between isnull and coalesce, coalesce will return a "everything fits" datatype. @OP, next time please include the DBMS you are using, there is quite a difference between MSSQL, Oracle, Postgree, Mysql and Access.^the ^last ^two ^suck ^anyway
Simplified your issue is the following basic expression: NULL + 1 = NULL Suggestion: ISNULL([2009],0) + ISNULL([2010],0)... ISNULL will replace the NULL values with 0 in the addition to allow it to complete without evaluating a NULL+X scenario 
ah ok, reports is fine, I was afraid you'd be converting dates to string, in order to do string manipulation to do logic on dates. 
My solution has one table scan, versus a nested join on itself (multiple joined scan on table), so will be faster. You only have to list all the fields, rather than type a long join predicate list. It's not swings and roundabouts at all, mine is a superior solution. Try partition by *, in case SQL Server supports it (I'm away from a computer and have never tried it).
sir, im not very bright...
this is a catch 22... I don't want to give out stupid ideas, but i want to explain to you what i ment... I'll give it a shot. Datetime is a datatype. A datetime value is not what you see when you do a select to that column in managment studio. That would already be a conversion from the datetime value to a string. That also depends on the culture settings your server / your connection is running on. There are people out there, that do calculations on datetime values, using a conversion to a string, and then for example cutting out the 2 characters that represent the day in said string, convert that to integer, do math on that integer, that was cut out of the string that used to be a datetime, convert the result back to a string, concatinate that back into the string you got from the initial conversion from datetime to string, and cast the whole thing back to datetime. Sounds stupid right? It kind of is... please never do that. But there are people out there that do shit like that ...... 
 case when appointmentdate is null then @status_id = 0 else max(appointmentdate) &gt; leaddate then @status_id = 2 else some other scenario --when you're all done just go to end end
oh ok I gotcha. i am a lowly report writer so my SQL knowledge is miniscule but thank you for the explanations and info
oh dear.
I'll make you a deal. Your part : - write an sql script, creating some example dummy tables, containing the neccesary fields, including the datatypes you'd have - include a query populating those tables with some sample data - include the indexing that touches the relevant tables &amp; columns My part : - I'll be happy to help you Deal ?
Hah- I'm learning that! Do you have any experience using SSIS with Informix? If so, mind if I ask another question?
Celko, is that you? ;)
since I have no idea whom you are refering to, I'll have to answer no ;)
Can you clarify what you're doing here? It looks like you're attempting to search based on an input (i.e. searchInput). For what it's worth, you should format your code a little more cleanly so the community can parse it more easily. SELECT Name , TelephoneNumber , Mobile , Mail , Company , Title , Department , Name -- Note: It looks like you're selecting this twice , SN , UserAccountControl , MSEXCHHideFromAddressLists FROM 'GC:// " &amp; objADsPath &amp; "' WHERE sAMAccountname LIKE '%" &amp; searchInput &amp; "%'" -- FIXME: I think your single quote is misplaced at the end of this line
Are any of the fields being populated? Youre mixing up forward slashes and backslashes in your INFILE path. Maybe not a problem, but I'd check I imagine MySQL is treating the date as the 12th of the 22nd month, seeing its invalid, and so setting it to NULL. You need to change your LOAD DATA to include an explicit field list. But instead of specifying the date field, specify a @date variable instead. Finally at the end, you can tack on some SET statements to process the @date how you want it See http://dev.mysql.com/doc/refman/5.1/en/load-data.html
Let's say you're not allowed to use 'null'. How would that change your design?
It's available as self-study; I'm looking at a career change too and just started it. So far it seems like a pretty good intro course.
Thanks for the help. It gives me something to try on Monday. A query through PCFILES seems to be much slower then on the DB or a SAS library. As a point of reference. How long is it for you to bring a table of 1mil rows in SAS?
staging table with all VARCHARS works fine you can then use STR_TO_DATE to convert month/day/year to dates
you're designing an EAV (entity-attribute-value) scheme, and it is notoriously difficult to pull information from -- google this i don't see any problems in using the design in your raw input
I would just look up the two ids in separate statements and put them into variables. That makes your insert very simple. You may need to check if that combo already exists in the table if it needs to be unique. Alternatively you could bind the ids to the drop down and skip the look up by name. 
dear OP, please see sidebar: When requesting help or asking questions please prefix your title with the SQL variant/platform you are using within square brackets like so: - [MySQL] - [Oracle] - [MS SQL] - [PostgreSQL] - etc While naturally we should endeavour to work as platform neutrally as possible many questions and answers require tailoring to the feature set of a specific platform. 
Duly noted, thank you.
Not sure what DB or interface you're using, so I'll keep this generic. The drop-downs need to have the UserID &amp; WhaleID as the value that gets stored in their respective variables. In essence, you want the drop-downs to *show* the names, but pass the IDs. Then your insert query will simply use those variables. Right now, I don't see where your insert query gets told about the values selected by the user So it will read something like: "INSERT INTO tblWhalesSpotted(WhaleID, UserID) VALUES (bindWhaleDropDown, bindUserDropDown)" - honestly not sure what your variable names for the values selected in the drop-downs are here. One more suggestion - Once you get this worked out, add a DateSpotted column to your WhalesSpotted table. It adds a huge additional dimension to your data gathered and you probably already have the information.
This is a bad design. Why not read coalesce on the values to get what you need?
If you need to just update it once right now for all existing records, here is how: UPDATE groupTable SET isActive = 1 WHERE groupEndYear IS NULL If you need to it to just update by itself whenever the value of groupEndYear is updated then you would have to drop the column and add it as a computed column: ALTER TABLE GroupTable DROP COLUMN isActive ALTER TABLE GroupTable ADD isActive AS CASE WHEN GroupEndYear IS NULL THEN 1 ELSE 0 END Computed columns are not stored in the database and are calculated on the fly, so they might affect performance. But the performance hit could be very small depending on the size of the table.
Why not just drop the active flag altogether? Its essentially duplicate data. You already know if a group is active, because endyear IS NULL, so just use that.
If you want to stick w MS SQL Server, then learn the hell outta TSQL and SSRS, and also consider C#, as it's pretty complimentary. You can write websites in C#, as well as database sprocs. PowerShell is a good option too.
You can also store the value by adding "PERSISTED" to the end of the alter table statement. Edit: you can also use this column in indexes if it's persisted
Possibly the best troll account name ever
This is the correct answer. Don't create a separate column for something that can be derived from data in the already existing columns at any time. What if someone did an update and set the active column to true on a record with an end year? Or set the active column to false on a record without an end year? Now your database contains inconsistent data.
I'm not sure I understand the significance of the zz table so I'm assuming it's a table with random job_ids in. SELECT Z1.job_id FROM zz AS Z1 INNER JOIN job AS J1 ON Z1.job_Id = J1.id INNER JOIN "user" AS U1 ON J1.user_id = U1.id WHERE U1.id NOT IN ( SELECT J2.user_id FROM job AS J2 WHERE J2.user_id = U1.id GROUP BY J2.user_Id HAVING COUNT(*)&gt;=U1.max_jobs ) I don't have Postgres installed at the moment, but that works with SQLFiddle. There's probably a much more elegant way of doing this but in the real world I'd probably have a summary view I'd use in a query like this. 
thanks
i hardly ever use the command line but it's the easiest way see [Executing SQL Statements from a Text File](http://dev.mysql.com/doc/refman/5.0/en/mysql-batch-commands.html)
create your database then mysql -u &lt;username&gt; -p &lt;databasename&gt; &lt; file.sql
I'm currently downloading all your data, including several (hundreds?) email adresses. You might consider not sharing this? EDIT: sorry, said hundreds, meant thousands. EDIT2: no, I'm not planning on doing anything with it. Just making a point about data security. EDIT3: nice, SMS history as well!! Scandinavian? Nope, Icelandic... :) EDIT4: it's getting better!! Usernames, address information, and the sin above all sins: plaintext passwords!! 
The contents from the folder are from [This](https://www.google.is/search?q=vodafone+iceland+hacked&amp;oq=vodafone+i&amp;aqs=chrome.3.69i57j0l5.3824j0j7&amp;sourceid=chrome&amp;es_sm=122&amp;ie=UTF-8)
Meh, already thought so. Getting someone stressed out because of something like this is a lot of fun, though. I once copied a folder from a friends computer (loads of personal stuff) and told him I downloaded it from the internet. He was freaking out and trying to fix it for three days until I told him.
Try removing the where clause. If it works that way, use "Having COUNT(ON_OBJECT_AO_ID) &gt; 1" instead.
You're trying to use an aggregate function without a GROUP BY clause. You need to move the part in the WHERE clause with the COUNT function to a HAVING clause. But, to use that, you must have the GROUP BY. The HAVING clause is essentially the WHERE clause for GROUP BY.
Still would need a GROUP BY to allow the HAVING clause.
Use a procedure, with 2 inputs of datetime start and datetime end. hardcoding changable values is silly.
Thanks bro. I will Google this method up. Feel free to share any useful links if you have. Thanks again :)
Hey can you tell me through what means I can have my procedure prompt the user to enter data. I can research my way through the rest. My googling advised to make an interface using Access. I'm I on the right track ?
Create linked table and do it in access... Edit: if you have specific questions, please let me know. I saw that you are dealing with High ED data, i deal with that data from the government side all of the time. So, please let me know via PM, or something if you need assistance on getting started, or suggested analysis. 
Yeah my guess is access is going to be the least cumbersome way to do it. You could build an SSIS package to load the file into a SQL table, but that will require a little more work.
I'm kind of confused by what you're wanting to do... Couldn't you write a macro if all you're trying to do is automate a process?
http://i.imgur.com/zIdZBCm.jpg
Not really an answer to your questions, but if you want to delete all the entries with a timestamp for a particular month, you can say, for example delete from cars where year(timestamp)=2000 and month(timestamp)=12 to delete all the entries with a timestamp in December 2000.
I don't know, and it's completely idiotic. The error message indicates that it knows what's wrong and what you need to do to fix it, but sql refuses to run your code. I just want to say to MSSQL thanks for being idiotic.
&gt; deleting by a whole year at a time If you include the "and month(timestamp)=12" part, you would only be deleting a month at a time, as with your original delete statement.
What are you connecting to the instance with? Are you just running queries through SSMS or are you getting errors running queries through an app?
Mostly in Agent jobs, owned by a user with sysadmin, scheduled or ad-hoc. I can paste the same code into an SSMS query and execute without error. It also happened today using sp_msforeachdb. The procedure errored in an SSMS query on 2012, but ran on 2008R2.
Are any of your queries using filtered indexes on the queried tables by chance?
So I'll be working with **huge** data sets involving classroom data in a higher learning institution. I want to be able to look at interesting data correlations between race, class, gender, prior grades, attitude, etc. That's why I want to use SQL and some database "thing", but I'm confused as to how to start.
Excel cant handle really big data sets anywho. At least not in xlsx format. You could just make a huge ass pivot table though hahaha. For the future look into postgres, python, and pandas. There are nice suites, so to speak, available to use with python for analysis. 
I almost immediately said, 'No', because I *knew* they didnt, but decided to check anyway. Filtered indexes. Filtered indexes everywhere. Apparently a coworker added tons of them during the migration and didnt tell me. Fortunately I didnt spend an embarrassing amount of time on this... Anywho, thanks for the second pair of eyes -- much appreciated!
No problem! I learned something myself. Check out this article for some more reference... http://blogs.msdn.com/b/sqlprogrammability/archive/2009/06/29/interesting-issue-with-filtered-indexes.aspx
&gt; where 1=1 Looks like dynamic sql of some flavor.
Well, there's always storing the whole *goddamn* in memory and plowing through it. Otherwise you might have to parse the urls into every possible substring and then throw an index on that, which could be an enormous amount of space.
You can query a sheet via SQL using VBA so long as it's a plain tabular data with headers in row 1. It's useful if you're working entirely within Excel but not much elsewhere. 'Requires reference to Microsoft ActiveX Data Objects Library Dim adoRS As ADODB.Recordset Set adoRS = New ADODB.Recordset Dim adoConn As ADODB.Connection Set adoConn = New ADODB.Connection adoConn.Open "Provider=Microsoft.Jet.OLEDB.4.0;Data Source=c:\workbook.xls;Extended Properties=""Excel 8.0;HDR=Yes;"";" adoRS.Open "SELECT * FROM [SHEET NAME$]", adoConn, 3, 1, 1 Do While Not adoRS.EOF MsgBox adoRS.Fields("column heading").Value adoRS.MoveNext Loop adoRS.Close adoConn.Close Note: don't query a sheet within the same workbook executing the code. [There's a memory leak.](http://support.microsoft.com/default.aspx?scid=kb;en-us;Q319998) Instead, have one Workbook query a second. 
As an alternative, consider why you want to do this (I'm not saying that I know, but you should). Is this something where you will know what is being searched for before hand? If so, run the queries and store the results in a new table, then when someone/something else runs the same query just run against the results table. If you are constantly adding new urls, then just update the results table as you add new records as well (via a trigger, or front end, or whatever works for you). If it may be different every single time and has nearly infinite permutations then you might want to consider working with the dataset outside of SQL. If you are for some reason forced to use SQL (and this is extremely unlikely, seriously, are you SURE you considered everything?) then an in memory table to temp table join is probably going to be the fastest solution you can get as /u/Mamertine said. This is not a highly optimizable query. You're going to want to consider how and why are you are trying to achieve the results and then make a plan to do so efficiently. 
&gt; where 1=1 I have seen this 'technique' used as a way to line up your and/or where clauses. Why? I'm not really sure. I guess it sort of keeps things looking nicer?
Not sure if this will help, but all student data should already be residing in a database due to regulations with retention and privacy. Can you get access to the data you need in its natural for in the database/data warehouse? It would make what you are trying to do very easy. But if you do have to work from Excel sheets, please make sure the data is encrypted at all times if you are working on a laptop. If you are working on a desktop, please do not walk away from it without the machine locked. Student data is very serious when it comes to privacy, and you do not want to be the person who let private education data out in the wild. 
MSSQL has a full text search option that you can install separately and index a column for searching.
I use WHERE 1=1 a lot. It's just so that you can comment out any of the where clauses easily, generally for troubleshooting. 
I work with Oracle, so I would do a function based index. It looks like you could do a functional column and index that though, or build the insert that strips out the site then search by that: select substr(site,instr(site,'.',1)+1, instr(site,'/',instr(site,'.',1),1)-instr(site,'.',1)-1) from (select 'http://www.reddit.com/r/SQL' site from dual); SUBSTR(SIT ---------- reddit.com 
just chiming in as a Institutional Researcher - regulations don't require any data on classroom activity *yet* (because honestly, who the fuck knows what the DoE wants) This sounds like some interesting Big Data research though - I'm working on a similar tool at my school. ninja edit: I'm working on something similar ( i think ) and I am using R, SPSS, and Tableau, if that helps.
Yeah, Excel has a hard limit of one million rows.
I'd just use SSIS to import the file to table, but that requires an installation of MSSQL.
I completely overlooked that. Yes, in order to group, you need the GROUP BY.
True about actual classroom data. I guess I made an assumption that the data mentioned involved grades, demographics, and other personal information. If it just has to do with classroom activities then sure it does not need to be encrypted and protected... but then again, that is a really good habit to get into.
I use it during development so I can comment out the AND clauses and not worry about accidentally leaving out a WHERE
Drop the first % and use the absolute url. You shouldn't have that many urls.
Thanks for all the advice! I'm looking into an SQL guide ("A Primer on SQL") and I'll see where that will take me. I'll message anyone if I need to, and I probably will. Thanks again!
1. Make your Excel workbook an ODBC data source. http://www.cyberfella.co.uk/2012/06/11/windows7-odbc/ 2. Then you can query it with any SQL system that can speak ODBC. 
I believe it's licensed per core per instance. It's fairly complicated, thus the reason you're not finding any prices listed. Edit: to give you a ballpark idea, I think one of our licenses for an active passive cluster is around $25,000 a year. I don't know a bunch of important stats on those machines. I know they're pretty beefy though.
(CONTINUED) Create table #students_in_LA ( table structure duplicates first instance ) INSERT INTO #students_in_LA SELECT DISTINCT XPT_SIS_TEST.dbo.SCHOOL.SCHOOL_TITLE AS SiteShortName, 'N' + RIGHT('000000' + CONVERT(varchar(7), XPT_SIS_TEST.dbo.STUDENT.STUDENT_ID), 6) AS BarCode, STUDENT_ENTRANCE_RELEASE.SCHOOL_ID AS DistrictID, PERSON_1.LAST_NAME AS LastName, PERSON_1.FIRST_NAME AS FirstName, STUDENT_ENTRANCE_RELEASE.YIS_CODE AS GradeLevel, CASE WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 2) = ('CS') THEN ' ' WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 1) = ('A') THEN (PERSON_EMPLOYEE.STAFF_ID) ELSE ('A' + PERSON_EMPLOYEE.STAFF_ID) END AS Homeroom, PERSON_1.MIDDLE_NAME AS MiddleName, ' ' AS NickName, PERSON_1.GENDER_CODE AS Gender, ' ' AS Patron, 'A' AS Status, ' ' AS AcceptableUse, ' ' AS PolicyOnFile, REPLACE(CONVERT(varchar, PERSON_1.BIRTH_DATE, 101), '/', '') AS BirthDate, 'N' AS IsTeacher, ' ' AS AccessLevel, 'N' + RIGHT('000000' + CONVERT(varchar(7), XPT_SIS_TEST.dbo.STUDENT.STUDENT_ID), 6) AS UserName, PERSON_1.PinNo AS Password1, ' ' AS Email1, ' ' AS Email2, XPT_SIS_TEST.dbo.PERSON_ADDRESS.ADDRESS_LINE1 AS AddressLineA1, XPT_SIS_TEST.dbo.PERSON_ADDRESS.ADDRESS_LINE2 AS AddressLineB1, ' ' AS AddressLineA2, ' ' AS AddressLineB2, XPT_SIS_TEST.dbo.PERSON_ADDRESS.CITY AS City1, ' ' AS City2, XPT_SIS_TEST.dbo.PERSON_ADDRESS.STATE_CODE AS State1, ' ' AS State2, XPT_SIS_TEST.dbo.PERSON_ADDRESS.ZIP AS ZipCode1, ' ' AS ZipCode2, SUBSTRING(XPT_SIS_TEST.dbo.PERSON_PHONE.PHONE_NUMBER, 2, 3) + REPLACE(RIGHT(XPT_SIS_TEST.dbo.PERSON_PHONE.PHONE_NUMBER, 8), '-', '') AS PhoneNumber1, ' ' AS PhoneNumber2, ' ' AS PhoneNumber3, ' ' AS PhoneNumber4, XPT_SIS_TEST.dbo.COURSE_SECTION.PATTERN_CODE AS PatternCode, XPT_SIS_TEST.dbo.STUDENT.STUDENT_ID FROM XPT_SIS_TEST.dbo.PERSON AS PERSON_1 RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT ON PERSON_1.INST_ID = XPT_SIS_TEST.dbo.STUDENT.INST_ID AND PERSON_1.PERSON_ID = XPT_SIS_TEST.dbo.STUDENT.PERSON_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_PHONE ON PERSON_1.INST_ID = XPT_SIS_TEST.dbo.PERSON_PHONE.INST_ID AND PERSON_1.PERSON_ID = XPT_SIS_TEST.dbo.PERSON_PHONE.PERSON_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_ADDRESS ON PERSON_1.INST_ID = XPT_SIS_TEST.dbo.PERSON_ADDRESS.INST_ID AND PERSON_1.PERSON_ID = XPT_SIS_TEST.dbo.PERSON_ADDRESS.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE AS STUDENT_ENTRANCE_RELEASE LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_EMPLOYEE RIGHT OUTER JOIN XPT_SIS_TEST.dbo.PERSON ON XPT_SIS_TEST.dbo.PERSON_EMPLOYEE.INST_ID = XPT_SIS_TEST.dbo.PERSON.INST_ID AND XPT_SIS_TEST.dbo.PERSON_EMPLOYEE.PERSON_ID = XPT_SIS_TEST.dbo.PERSON.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.COURSE_SECTION ON XPT_SIS_TEST.dbo.PERSON.INST_ID = XPT_SIS_TEST.dbo.COURSE_SECTION.INST_ID AND XPT_SIS_TEST.dbo.PERSON.PERSON_ID = XPT_SIS_TEST.dbo.COURSE_SECTION.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL ON XPT_SIS_TEST.dbo.COURSE_SECTION.INST_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.INST_ID AND XPT_SIS_TEST.dbo.COURSE_SECTION.SCHOOL_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.SCHOOL_ID AND XPT_SIS_TEST.dbo.COURSE_SECTION.SCHOOL_YEAR = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.SCHOOL_YEAR AND XPT_SIS_TEST.dbo.COURSE_SECTION.TERM_CODE = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.TERM_CODE AND XPT_SIS_TEST.dbo.COURSE_SECTION.COURSE_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.COURSE_ID AND XPT_SIS_TEST.dbo.COURSE_SECTION.SECTION_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.SECTION_ID ON STUDENT_ENTRANCE_RELEASE.STUDENT_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.STUDENT_ID AND STUDENT_ENTRANCE_RELEASE.SCHOOL_YEAR = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.SCHOOL_YEAR AND STUDENT_ENTRANCE_RELEASE.SCHOOL_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE.INST_ID = XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.INST_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.SCHOOL ON STUDENT_ENTRANCE_RELEASE.SCHOOL_ID = XPT_SIS_TEST.dbo.SCHOOL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE.INST_ID = XPT_SIS_TEST.dbo.SCHOOL.INST_ID ON XPT_SIS_TEST.dbo.PERSON_PHONE.PHONE_TYPE_CODE = 'HOME' AND XPT_SIS_TEST.dbo.STUDENT.INST_ID = STUDENT_ENTRANCE_RELEASE.INST_ID AND XPT_SIS_TEST.dbo.STUDENT.STUDENT_ID = STUDENT_ENTRANCE_RELEASE.STUDENT_ID WHERE (STUDENT_ENTRANCE_RELEASE.EnrlTyp = 'R') AND (STUDENT_ENTRANCE_RELEASE.RELEASE_DATE IS NULL) AND (STUDENT_ENTRANCE_RELEASE.SCHOOL_YEAR = '2014') AND (STUDENT_ENTRANCE_RELEASE.SCHOOL_ID IN ('0085', '0181', '1391', '1251', '0021', '0221', '0051')) AND (XPT_SIS_TEST.dbo.PERSON_ADDRESS.ADDRESS_TYPE_CODE = 'HOME') AND (XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.TERM_CODE = '2') AND (XPT_SIS_TEST.dbo.COURSE_SECTION.STATUS_FLG = 'A') AND (XPT_SIS_TEST.dbo.COURSE_SECTION.START_DATE &lt; GETDATE()) AND (XPT_SIS_TEST.dbo.COURSE_SECTION.END_DATE &gt; GETDATE()) AND (NOT (XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL.ADD_DROP_FLG IN ('D', 'G'))) AND (NOT (XPT_SIS_TEST.dbo.COURSE_SECTION.PATTERN_CODE IN ('B1'))) AND (XPT_SIS_TEST.dbo.COURSE_SECTION.DEPARTMENT_CODE IN ('LA')) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_EN.STUDENT_ID FROM #students_in_EN)
Create table #students_in_ESE ( table structure duplicates first instance ) INSERT INTO #students_in_ese SELECT DISTINCT SCHOOL.SCHOOL_TITLE AS SiteShortName, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS BarCode, STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID AS DistrictID, PERSON_1.LAST_NAME AS LastName, PERSON_1.FIRST_NAME AS FirstName, STUDENT_ENTRANCE_RELEASE_1.YIS_CODE AS GradeLevel, CASE WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 2) = ('CS') THEN ' ' WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 1) = ('A') THEN (PERSON_EMPLOYEE.STAFF_ID) ELSE ('A' + PERSON_EMPLOYEE.STAFF_ID) END AS Homeroom, PERSON_1.MIDDLE_NAME AS MiddleName, ' ' AS NickName, PERSON_1.GENDER_CODE AS Gender, ' ' AS Patron, 'A' AS Status, ' ' AS AcceptableUse, ' ' AS PolicyOnFile, REPLACE(CONVERT(varchar, PERSON_1.BIRTH_DATE, 101), '/', '') AS BirthDate, 'N' AS IsTeacher, ' ' AS AccessLevel, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS UserName, PERSON_1.PinNo AS Password1, ' ' AS Email1, ' ' AS Email2, PERSON_ADDRESS.ADDRESS_LINE1 AS AddressLineA1, PERSON_ADDRESS.ADDRESS_LINE2 AS AddressLineB1, ' ' AS AddressLineA2, ' ' AS AddressLineB2, PERSON_ADDRESS.CITY AS City1, ' ' AS City2, PERSON_ADDRESS.STATE_CODE AS State1, ' ' AS State2, PERSON_ADDRESS.ZIP AS ZipCode1, ' ' AS ZipCode2, SUBSTRING(PERSON_PHONE.PHONE_NUMBER, 2, 3) + REPLACE(RIGHT(PERSON_PHONE.PHONE_NUMBER, 8), '-', '') AS PhoneNumber1, ' ' AS PhoneNumber2, ' ' AS PhoneNumber3, ' ' AS PhoneNumber4, COURSE_SECTION.PATTERN_CODE AS PatternCode, STUDENT.STUDENT_ID FROM XPT_SIS_TEST.dbo.PERSON AS PERSON_1 RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT ON PERSON_1.INST_ID = STUDENT.INST_ID AND PERSON_1.PERSON_ID = STUDENT.PERSON_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_PHONE ON PERSON_1.INST_ID = PERSON_PHONE.INST_ID AND PERSON_1.PERSON_ID = PERSON_PHONE.PERSON_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_ADDRESS ON PERSON_1.INST_ID = PERSON_ADDRESS.INST_ID AND PERSON_1.PERSON_ID = PERSON_ADDRESS.PERSON_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE AS STUDENT_ENTRANCE_RELEASE_1 LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_EMPLOYEE RIGHT OUTER JOIN XPT_SIS_TEST.dbo.PERSON ON PERSON_EMPLOYEE.INST_ID = PERSON.INST_ID AND PERSON_EMPLOYEE.PERSON_ID = PERSON.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.COURSE_SECTION ON PERSON.INST_ID = COURSE_SECTION.INST_ID AND PERSON.PERSON_ID = COURSE_SECTION.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL ON COURSE_SECTION.INST_ID = STUDENT_COURSE_ENRL.INST_ID AND COURSE_SECTION.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND COURSE_SECTION.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND COURSE_SECTION.TERM_CODE = STUDENT_COURSE_ENRL.TERM_CODE AND COURSE_SECTION.COURSE_ID = STUDENT_COURSE_ENRL.COURSE_ID AND COURSE_SECTION.SECTION_ID = STUDENT_COURSE_ENRL.SECTION_ID ON STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID = STUDENT_COURSE_ENRL.STUDENT_ID AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = STUDENT_COURSE_ENRL.INST_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.SCHOOL ON STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = SCHOOL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = SCHOOL.INST_ID ON PERSON_PHONE.PHONE_TYPE_CODE = 'HOME' AND STUDENT.INST_ID = STUDENT_ENTRANCE_RELEASE_1.INST_ID AND STUDENT.STUDENT_ID = STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID WHERE (STUDENT_ENTRANCE_RELEASE_1.EnrlTyp = 'R') AND (STUDENT_ENTRANCE_RELEASE_1.RELEASE_DATE IS NULL) AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = '2014') AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID IN ('0085', '0181', '1391', '1251', '0021', '0221', '0051')) AND (PERSON_ADDRESS.ADDRESS_TYPE_CODE = 'HOME') AND (STUDENT_COURSE_ENRL.TERM_CODE = '2') AND (COURSE_SECTION.STATUS_FLG = 'A') AND (COURSE_SECTION.START_DATE &lt; GETDATE()) AND (COURSE_SECTION.END_DATE &gt; GETDATE()) AND (NOT (STUDENT_COURSE_ENRL.ADD_DROP_FLG IN ('D', 'G'))) AND (NOT (COURSE_SECTION.PATTERN_CODE IN ('B1'))) AND (SUBSTRING(STUDENT_COURSE_ENRL.COURSE_ID, 1, 7) IN ('7910110', '7910111', '7910112')) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_EN.STUDENT_ID FROM #students_in_EN) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_LA.STUDENT_ID FROM #students_in_LA) 
 Create table #students_in_3rd ( table structure duplicates first instance ) INSERT into #students_in_3rd SELECT DISTINCT SCHOOL.SCHOOL_TITLE AS SiteShortName, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS BarCode, STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID AS DistrictID, PERSON_1.LAST_NAME AS LastName, PERSON_1.FIRST_NAME AS FirstName, STUDENT_ENTRANCE_RELEASE_1.YIS_CODE AS GradeLevel, CASE WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 2) = ('CS') THEN ' ' WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 1) = ('A') THEN (PERSON_EMPLOYEE.STAFF_ID) ELSE ('A' + PERSON_EMPLOYEE.STAFF_ID) END AS Homeroom, PERSON_1.MIDDLE_NAME AS MiddleName, ' ' AS NickName, PERSON_1.GENDER_CODE AS Gender, ' ' AS Patron, 'A' AS Status, ' ' AS AcceptableUse, ' ' AS PolicyOnFile, REPLACE(CONVERT(varchar, PERSON_1.BIRTH_DATE, 101), '/', '') AS BirthDate, 'N' AS IsTeacher, ' ' AS AccessLevel, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS UserName, PERSON_1.PinNo AS Password1, ' ' AS Email1, ' ' AS Email2, PERSON_ADDRESS.ADDRESS_LINE1 AS AddressLineA1, PERSON_ADDRESS.ADDRESS_LINE2 AS AddressLineB1, ' ' AS AddressLineA2, ' ' AS AddressLineB2, PERSON_ADDRESS.CITY AS City1, ' ' AS City2, PERSON_ADDRESS.STATE_CODE AS State1, ' ' AS State2, PERSON_ADDRESS.ZIP AS ZipCode1, ' ' AS ZipCode2, SUBSTRING(PERSON_PHONE.PHONE_NUMBER, 2, 3) + REPLACE(RIGHT(PERSON_PHONE.PHONE_NUMBER, 8), '-', '') AS PhoneNumber1, ' ' AS PhoneNumber2, ' ' AS PhoneNumber3, ' ' AS PhoneNumber4, COURSE_SECTION.PATTERN_CODE, STUDENT.STUDENT_ID FROM XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE AS STUDENT_ENTRANCE_RELEASE_1 LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_EMPLOYEE RIGHT OUTER JOIN XPT_SIS_TEST.dbo.PERSON ON PERSON_EMPLOYEE.INST_ID = PERSON.INST_ID AND PERSON_EMPLOYEE.PERSON_ID = PERSON.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.COURSE_SECTION ON PERSON.INST_ID = COURSE_SECTION.INST_ID AND PERSON.PERSON_ID = COURSE_SECTION.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL ON COURSE_SECTION.INST_ID = STUDENT_COURSE_ENRL.INST_ID AND COURSE_SECTION.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND COURSE_SECTION.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND COURSE_SECTION.TERM_CODE = STUDENT_COURSE_ENRL.TERM_CODE AND COURSE_SECTION.COURSE_ID = STUDENT_COURSE_ENRL.COURSE_ID AND COURSE_SECTION.SECTION_ID = STUDENT_COURSE_ENRL.SECTION_ID ON STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID = STUDENT_COURSE_ENRL.STUDENT_ID AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = STUDENT_COURSE_ENRL.INST_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.SCHOOL ON STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = SCHOOL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = SCHOOL.INST_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.STUDENT INNER JOIN XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE ON STUDENT.INST_ID = STUDENT_ENTRANCE_RELEASE.INST_ID AND STUDENT.STUDENT_ID = STUDENT_ENTRANCE_RELEASE.STUDENT_ID ON STUDENT_ENTRANCE_RELEASE_1.INST_ID = STUDENT.INST_ID AND STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID = STUDENT.STUDENT_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON AS PERSON_1 ON STUDENT.INST_ID = PERSON_1.INST_ID AND STUDENT.PERSON_ID = PERSON_1.PERSON_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON_PHONE ON PERSON_1.INST_ID = PERSON_PHONE.INST_ID AND PERSON_1.PERSON_ID = PERSON_PHONE.PERSON_ID AND PERSON_PHONE.PHONE_TYPE_CODE = 'HOME' FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON_ADDRESS ON PERSON_1.INST_ID = PERSON_ADDRESS.INST_ID AND PERSON_1.PERSON_ID = PERSON_ADDRESS.PERSON_ID WHERE (STUDENT_ENTRANCE_RELEASE_1.EnrlTyp = 'R') AND (STUDENT_ENTRANCE_RELEASE_1.RELEASE_DATE IS NULL) AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = '2014') AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID IN ('0085', '0181', '1391', '1251', '0021', '0221', '0051')) AND (PERSON_ADDRESS.ADDRESS_TYPE_CODE = 'HOME') AND (STUDENT_COURSE_ENRL.TERM_CODE = '2') AND (COURSE_SECTION.STATUS_FLG = 'A') AND (COURSE_SECTION.START_DATE &lt; GETDATE()) AND (COURSE_SECTION.END_DATE &gt; GETDATE()) AND (NOT (STUDENT_COURSE_ENRL.ADD_DROP_FLG IN ('D', 'G'))) AND (NOT (COURSE_SECTION.PATTERN_CODE IN ('B1'))) AND (STUDENT_ENTRANCE_RELEASE.YIS_CODE IN ('09', '10', '11', '12')) AND (COURSE_SECTION.START_PERIOD_ID = '3') AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_EN.STUDENT_ID FROM #students_in_EN) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_LA.STUDENT_ID FROM #students_in_LA) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_ESE.STUDENT_ID FROM #students_in_ESE) 
thanks a bunch
Create table #students_in_7th ( table structure duplicates first instance ) INSERT into #students_in_7th SELECT DISTINCT SCHOOL.SCHOOL_TITLE AS SiteShortName, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS BarCode, STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID AS DistrictID, PERSON_1.LAST_NAME AS LastName, PERSON_1.FIRST_NAME AS FirstName, STUDENT_ENTRANCE_RELEASE_1.YIS_CODE AS GradeLevel, CASE WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 2) = ('CS') THEN ' ' WHEN SUBSTRING(PERSON_EMPLOYEE.STAFF_ID, 1, 1) = ('A') THEN (PERSON_EMPLOYEE.STAFF_ID) ELSE ('A' + PERSON_EMPLOYEE.STAFF_ID) END AS Homeroom, PERSON_1.MIDDLE_NAME AS MiddleName, ' ' AS NickName, PERSON_1.GENDER_CODE AS Gender, ' ' AS Patron, 'A' AS Status, ' ' AS AcceptableUse, ' ' AS PolicyOnFile, REPLACE(CONVERT(varchar, PERSON_1.BIRTH_DATE, 101), '/', '') AS BirthDate, 'N' AS IsTeacher, ' ' AS AccessLevel, 'N' + RIGHT('000000' + CONVERT(varchar(7), STUDENT.STUDENT_ID), 6) AS UserName, PERSON_1.PinNo AS Password1, ' ' AS Email1, ' ' AS Email2, PERSON_ADDRESS.ADDRESS_LINE1 AS AddressLineA1, PERSON_ADDRESS.ADDRESS_LINE2 AS AddressLineB1, ' ' AS AddressLineA2, ' ' AS AddressLineB2, PERSON_ADDRESS.CITY AS City1, ' ' AS City2, PERSON_ADDRESS.STATE_CODE AS State1, ' ' AS State2, PERSON_ADDRESS.ZIP AS ZipCode1, ' ' AS ZipCode2, SUBSTRING(PERSON_PHONE.PHONE_NUMBER, 2, 3) + REPLACE(RIGHT(PERSON_PHONE.PHONE_NUMBER, 8), '-', '') AS PhoneNumber1, ' ' AS PhoneNumber2, ' ' AS PhoneNumber3, ' ' AS PhoneNumber4, COURSE_SECTION.PATTERN_CODE, STUDENT.STUDENT_ID FROM XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE AS STUDENT_ENTRANCE_RELEASE_1 LEFT OUTER JOIN XPT_SIS_TEST.dbo.PERSON_EMPLOYEE RIGHT OUTER JOIN XPT_SIS_TEST.dbo.PERSON ON PERSON_EMPLOYEE.INST_ID = PERSON.INST_ID AND PERSON_EMPLOYEE.PERSON_ID = PERSON.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.COURSE_SECTION ON PERSON.INST_ID = COURSE_SECTION.INST_ID AND PERSON.PERSON_ID = COURSE_SECTION.PERSON_ID RIGHT OUTER JOIN XPT_SIS_TEST.dbo.STUDENT_COURSE_ENRL ON COURSE_SECTION.INST_ID = STUDENT_COURSE_ENRL.INST_ID AND COURSE_SECTION.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND COURSE_SECTION.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND COURSE_SECTION.TERM_CODE = STUDENT_COURSE_ENRL.TERM_CODE AND COURSE_SECTION.COURSE_ID = STUDENT_COURSE_ENRL.COURSE_ID AND COURSE_SECTION.SECTION_ID = STUDENT_COURSE_ENRL.SECTION_ID ON STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID = STUDENT_COURSE_ENRL.STUDENT_ID AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = STUDENT_COURSE_ENRL.SCHOOL_YEAR AND STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = STUDENT_COURSE_ENRL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = STUDENT_COURSE_ENRL.INST_ID LEFT OUTER JOIN XPT_SIS_TEST.dbo.SCHOOL ON STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID = SCHOOL.SCHOOL_ID AND STUDENT_ENTRANCE_RELEASE_1.INST_ID = SCHOOL.INST_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.STUDENT INNER JOIN XPT_SIS_TEST.dbo.STUDENT_ENTRANCE_RELEASE ON STUDENT.INST_ID = STUDENT_ENTRANCE_RELEASE.INST_ID AND STUDENT.STUDENT_ID = STUDENT_ENTRANCE_RELEASE.STUDENT_ID ON STUDENT_ENTRANCE_RELEASE_1.INST_ID = STUDENT.INST_ID AND STUDENT_ENTRANCE_RELEASE_1.STUDENT_ID = STUDENT.STUDENT_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON AS PERSON_1 ON STUDENT.INST_ID = PERSON_1.INST_ID AND STUDENT.PERSON_ID = PERSON_1.PERSON_ID FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON_PHONE ON PERSON_1.INST_ID = PERSON_PHONE.INST_ID AND PERSON_1.PERSON_ID = PERSON_PHONE.PERSON_ID AND PERSON_PHONE.PHONE_TYPE_CODE = 'HOME' FULL OUTER JOIN XPT_SIS_TEST.dbo.PERSON_ADDRESS ON PERSON_1.INST_ID = PERSON_ADDRESS.INST_ID AND PERSON_1.PERSON_ID = PERSON_ADDRESS.PERSON_ID WHERE (STUDENT_ENTRANCE_RELEASE_1.EnrlTyp = 'R') AND (STUDENT_ENTRANCE_RELEASE_1.RELEASE_DATE IS NULL) AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_YEAR = '2014') AND (STUDENT_ENTRANCE_RELEASE_1.SCHOOL_ID IN ('0085', '0181', '1391', '1251', '0021', '0221', '0051')) AND (PERSON_ADDRESS.ADDRESS_TYPE_CODE = 'HOME') AND (STUDENT_COURSE_ENRL.TERM_CODE = '2') AND (COURSE_SECTION.STATUS_FLG = 'A') AND (COURSE_SECTION.START_DATE &lt; GETDATE()) AND (COURSE_SECTION.END_DATE &gt; GETDATE()) AND (NOT (STUDENT_COURSE_ENRL.ADD_DROP_FLG IN ('D', 'G'))) AND (NOT (COURSE_SECTION.PATTERN_CODE IN ('B1'))) AND (STUDENT_ENTRANCE_RELEASE.YIS_CODE IN ('09', '10', '11', '12')) AND (COURSE_SECTION.START_PERIOD_ID = '7') AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_EN.STUDENT_ID FROM #students_in_EN) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_LA.STUDENT_ID FROM #students_in_LA) AND STUDENT.STUDENT_ID NOT IN (SELECT #students_in_ESE.STUDENT_ID FROM #students_in_ESE) AND STUDENT.STUDENT_ID NOT IN(SELECT #students_in_3rd.STUDENT_ID from #students_in_3rd) Insert into #students_in_EN SELECT * FROM #students_in_LA Insert into #students_in_EN SELECT * FROM #students_in_ESE Insert into #students_in_EN SELECT * FROM #students_in_3rd INSERT INTO #students_in_EN SELECT * FROM #students_in_7th SELECT DISTINCT (all but student_ID) FROM #students_in_EN ORDER BY DistrictID 
No one is going to bother to read that impregnable wall of nightmares. If you want help, you're going to have to put some effort into making that legible first.
What do you recommend?
Prices aren't published anymore. You have to contact a reseller. http://www.brentozar.com/archive/2014/04/sql-server-2014-licensing-changes/ gives a quick overview of some of the changes with regards to licensing for 2014 
Pastebin.com is a popular way of exchanging chunks of code. It will give you the syntax highlighting but you need to manually add linebreaks and indents and stuff, if that stuff isn't already there.
mine eyes!!! zey are bleedink!!
You will probably get better traction on this question in /r/SQLServer.
thanks, posted it there.
dang. okay thanks
Haven't worked with SQL in a while, but it will depend heavily on whether you're running in a virtualized environment or not. I know a while back our Oracle Database and WebLogic licenses got confusing because each VM technically had access to like 80 cores so Oracle was trying to get us to license an 80 core WebLogic when its guest was constrained to around 4 cores.
From what you describe, it sounds like you've already got duplicate entries in table 1? I'm not going to go through all this code, so this may not be the best solution, but it should work. Paste this DELETE after your final insert. It will remove any duplicate entries of STUDENT_ID. If there's a certain copy you'd like to keep, you can update the ORDER BY clause. What ever comes first in that sort is the one that will stay in the table. &amp;nbsp; ;WITH [cte] AS ( SELECT ROW_NUMBER() OVER (PARTITION BY [STUDENT_ID] ORDER BY 1) [row_num] FROM #students_in_EN ) DELETE [cte] WHERE [row_num] &gt; 1
Sorry not sure. Pricing is different in different regions and with different customers and their agreement levels. I've seen pricing ranging from $8k -&gt; $25k per core. 
Yes, just as they all said its just for easier commenting : where 1=1 --and date &lt; '1/1/2013' and car = 'green' instead of : where -- date &lt; '1/1/2013' --and car = 'green'
If one were to use Oracle to do the kind of searches that OP talks about, one would probably have to use Oracle Text (CONTEXT indexes). Either that, or change the application logic so you don't have to do those searches.
Yes, context would be the next choice, although without knowing the data and how it is being queried, it's hard to say if that would be the best choice either. More an interesting exercise than a valid proposal. :)
Of course it is. The devil is in the testing and analysis. Otherwise if it were that simple, I'd probably be out of a job. :D
 http://pastebin.com/Ar8nD1Hn 
It's in the ballpark of 7000 per core (without SA). Standard and BI still have the option of going per server (9000 for BI, 2000 for Std) + CALs (not sure on the cost of those, as they are usually rolled into EAs)
Sorry but this is bad advice If you have dupes, take some time to understand it, and decide which ones you want to keep, rather than just indiscriminately deleting the first dupe the db engine happens to stumble across.
&gt; which is why I disclaimed by saying it may not be the best solution. Fair enough, you did. Sorry :)
You could try Coalesce(A.Name1,B.Name1,C.Name1) as Name1 From TableA A Left JOIN TableB B ON A.key = B.key Left JOIN TableC C ON A.key = C.key but I think the UNION suggestion may be the way to go. As for this to work every record would have to be able to JOIN to tableA.
Is it possible to index the tables then use joins? You could dump all, index, and join, no?
Did you sell out SQL experience/knowledge at your interview ? Or are they aware of the fact that you are a beginner with databases ? If the latter, then it's their responsability to teach you. So don't worry too much :) That being said, try to download and install Oracle Express, and find a tutorial thats guides you through the first steps of using this database. That shoudln't be too difficult !
I suspect if they know they can train you, you will cost less. If they know you don't have experience then the best you can do is jump in with both feet. It would be hard for you to connect to an Oracle database if you don't have the software installed on your personal machine. Consider downloading MySQL (free) with workbench to practice on. SQL is SQL regardless of the tool. Syntax may be slightly different but it could help you practice basic SQL. Good luck .. Work hard and it can be a great experience!
So true ... I know that is how I got my start in SQL and since have trained many people...
You are correct, there is no set it and forget it, as well as over-indexing can be a problem. I suggest you take a look at these [Index Related DMV](http://technet.microsoft.com/en-us/library/ms187974.aspx), they can shed alot of light on your current index usage. 
If you have limited time let me tell you what to do. It'll cost you 25 bucks but it's well worth your time and money. Sign up for Lynda.com, and watch the sql courses by Simon Allardice . There's like seven hours of videos explaining general sql concepts then more specific training after that. After you watch all the sql training videos on Lynda.com you can cancel your account or whatever, you will have gotten your money's worth. Then go to pluralsight dot Com and sign up an account there. Pluralsight is way way more in depth with tutorial videos on anything and everything in the development world. All the programming languages are there, tons of sql videos, etc. The reason I say go to Lynda.com first is because those videos by Simon allardice are phenomenal. They helped me tremendously. I pay for a membership on both sites currently. They're an unbelievable resource for tech training. I am a big proponent of teaching yourself and those two sites are very helpful in that process. Good luck. 
Ha, no worries. I should've explained why it wasnt the best in my original post. Thanks for picking up the slack
No problem!
&gt; Consider downloading MySQL (free) with workbench to practice on. Oracle Express is free. MySQL is not a particularly good tool to use for learning SQL, especially with its default configuration. MySQL is a particularly *bad* tool to use for learning Oracle. &gt; SQL is SQL regardless of the tool. Yes and no. Oracle's CREATE TABLE statement takes about 50 pages to print.
Well, where you need your indexes depends on your data usage patterns. You being a developer, I'd recommend you delete all your indexes (and statistics) and then run through your processes for a day or two, and then run the 'missing indexes dmv' and apply some of those that are a good fit. Also take a look at sp_blitz.
Who is printing the create table statement? In general terms if the OP doesn't know SQL at all, then any tool they can download and use will help them. And, more importantly understanding that there are a number of tools that do similar things (again generally speaking) so the OP gets that a tool is just that, the logic is what is truly important to learn. Oracle, SQL Server, DB2 all have the same basic function. 
Dude, honestly it doesn't matter what you do between now and Monday I can promise you two things: 1. You won't know a fucking thing that's going on around for you for at least a few weeks. 2. Nothing is going to change it. You're going to be taught by someone to do specific things and expected to be able to do them again, plus be able to extrapolate what you're doing when you do them so that you can prepare yourself for more advanced tasks that you will then be taught to do and expected to be able to do them again. You're going to want to take notes. Stay VERY organized. I recommend getting a copy of 3M's Post-It Notes. I abhor the ones that come with Windows and there's a lite freeware version that's floating around out on the web that doesn't have any annoying trial aspects to it. PM me if you need a copy. This program is amazing for keeping track of login credentials (not passwords!), making notes of what database specific tables are located, etc. Get SSMS or something like it as soon as you start the job and save every single query you run. Every. Single. One. Document as much as you possibly can. Think of one of your jobs as preparing documentation for someone else to take over your job who also knows nothing. Basically prepare documentation that will allow your replacement to do your job if you get fired, or better yet promoted. This will both increase your worth and help you learn what you're doing / be able to do it without help. Notes are not the same as documentation. Make absolutely **sure** you set up a comprehensive email filing system as soon as you can. Make folders for everyone you routinely email and then copy emails from them into those folders whenever the email is no longer relevant or part of your "current" workload, aka, a top priority. This will allow you to find "back burner" emails/items very easily when they become a top priority in the future. You can keep everything in the SENT folder where it is. Make sure you also create an OVERFLOW folder where you can dump everything that's in your INBOX if you get overwhelmed. Then you can work your way through this overflow box while still paying attention to incoming emails / responding to time critical requests, etc. Don't wait two weeks and then do this because the longer you wait the harder it is to organize everything, whereas if you do it right away it will become a habit. Don't forget to make a folder for HR, either. I recommend you keep a naked desktop: That is no icons, or as few as you can get away with. Organize your start menu into categories such as ACCESSORIES, INTERNET BROWSERS, DATABASE PROGRAMS, etc. Spend some time configuring Windows because it sucks out of the box. Get yourself a start bar with small icons, a minimalist start menu, etc. I like to keep a WORKING folder on the desktop that has different folders for different projects. When I finish a project, or when a project is no longer a top priority (aka I'm not working on it) then it gets appropriately filed on network drives. I never move files from the network drive to my WORKING folder, I copy them. Make sure you keep backups of backups! This is especially true of any queries you write!! You do **NOT** want to find out that for some stupid reason or another a file was overwritten and key components of a process/query were lost and now you need to spend an entire day or two going back to the person who taught you how to do it in the first place to rewrite it. Any time you are going to make changes to an existing file, or report, or piece of code, or whatever... make sure you have a backed up version and then even after you make the changes and know that everything turned out fine? Keep the original version backed up in an archive for an extra 7 days, or 30 days, or indefinitely if space isn't an issue. You'll be surprised how often you'll write something, it'll work great, a few months go by and you remove a section so you can write a new section, it'll work great, a few more months will go by and a new requirement necessitates that you write a new section that is *similar* to the section you removed... but you don't have a fucking backup because you're like me and figured you'd never need it again and hell you wrote it in the first place, so it couldn't have been that hard to reproduce, right? Wrong. Instead of being the guy who has to go ask someone to help you a second time you get to be the guy who spends 4-6 hours rewriting something you already wrote. Basically you need to turn your work computer into a temple of organization. A sanctuary. You need to keep a notepad with you at all times and write as much down as you possibly can. You want to do something between now and Monday that will actually help you? Fucking Google how to take notes, practice taking notes, look into information about how to improve your reading (you're going to be doing a lot of reading in the future), etc. I mean... you probably still want to spend as much time as you can reading about Oracle, but don't overlook the real skills you're going to need. Subscribe to the related sub-reddits. Spend free time looking at what problems other people have, and what solutions they come up with. Try to come up your own. Try and understand the ones that others come up with. Ask questions if you don't understand something. Start frequenting some related forums and taking time after work to increase your total understanding. Oh, and for Christ's sake learn how to behave in meetings. Don't ask a million questions that are duh to everyone else in them. Ask someone afterwards if you don't know what's going on, but don't be afraid to speak up if something contradicts something you were told/taught. Just be ready for things to move on quickly and to be quiet when they do. You won't be promoted based on how you behave in a meeting, but you can get fired. Just be polite, and cordial, and pay attention to how other people behave. Learn from them. 
So you don't want to count the outbound links to facebook 'like' pages... is that the goal here?
Check out this resource, learn SQL the hardway. http://sql.learncodethehardway.org/book/ SQL really isn't that bad, once you have the concepts down it's just different puzzles that need to be solved.
I hire someone with a good attitude and personality that I feel can learn the job given lots of training and hand holding. Eventually they become completely independent. It has worked well so far. 
^ THIS GUY ^
Not sure what flavour of sql you are using but should simply be the case of 1) adding an email definition to the view section after all other column defs. 2) add the actual email column name to each select statement, again after all other column refs. 3) possibly optional add a def. customised label 4) Check code refs to the view that select all columns can handle the new addition. Note:Access to tables and views should never be allowed via raw sql as changing the table/view defin can break your apps. Note2: The reason we add the new column last is a precaution. All existing code refs to the view expect can expect the the other columns in their pre existing order.
http://jaxenter.com/assets/Code2013.jpg
You don't really need the CASE statement if you're careful with your regex. INSERT INTO mytable (last_name, first_name, suffix) SELECT substring(namefield from '^[^,]+') AS last_name, regexp_replace(namefield,'(^.+, | JR$| SR$| II$| III$)','','g') AS first_name, substring(namefield from ' (JR|SR|II|III)$') AS suffix FROM mytemptable last_name is everything from the beginning that isn't a comma, suffix is JR,SR,II or III at the end that comes right before a space, and first_name is namefield with the beginning up to the comma-space and any possible space+suffixes stripped out. EDIT: if you also wanted to use a substring for first_name for consistency, this works but it feels more complicated to me: '^.+, (.+?)(?: JR| SR| II| III)?$'
If these are the only rows, regex will work. But take note that names come in all sorts of varieties. See [Falsehoods Programmers Believe About Names](http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/).
https://www.coursera.org/course/db Too bad there is only self study option though.
Well, I've pulled the statement from the IBM Client Access Operations Navigator. I've been attempting to create a separate view within it but keep causing errors. 
What errors are you getting and can you show me what sql you are submitting
The procedure makes perfect sense to me. I am trying to figure out how to get prompts so that the user can enter date values and feed these date values into the procedure. My research has led me to "Embedded SQL", however the sample C programs I have found don't compile. I am familiar with SQL and C but not embedded sql. Is this a good route for solving my issue ... or would you advise any other methods. And if so any good resources for embedded sql ? Thanks bro
Thanks for the tip. 
I'm told the emails are also in CMSDAT.CCFL.REEMAL I tried this and the error I get: &gt; 'Message: [SQL0206] Column REEMAL not in specified tables. Cause . . . . . : REEMAL is not a column of table *N in *N. If the table is *N, REEMAL is not a column of any table or view that can be referenced. Recovery . . . : Do one of the following and try the request again: -- Ensure that the column and table names are specified correctly in the statement. -- If this is a SELECT statement, ensure that all the required tables were named in the FROM clause. -- If the column was intended to be a correlated reference, qualify the column with the correct table designator.': CREATE VIEW INTERNET.UPSTEST ( INVNUM , ORDERNUM , CUSTPO , ORDERDATE , SHIPTONUM , SHIPTO , ADDR1 , ADDR2 , CITY , STATE , ZIP, EMAIL ) AS SELECT DHINV# AS INVNUM, DHORD# AS ORDERNUM, DHCPO AS CUSTPO, DHIDAT AS ORDERDATE, DHSCS# AS SHIPTONUM, DHSNAM AS SHIPTO, DHSAD1 AS ADDR1, DHSAD2 AS ADDR2, DHSAD4 AS CITY, DHSAD5 AS STATE, DHSPOS AS ZIP, REEMAL AS EMAIL FROM CMSDAT.COIH UNION SELECT DHINV# AS INVNUM, DHORD# AS ORDERNUM, DHCPO AS CUSTPO, DHIDAT AS ORDERDATE, DHSCS# AS SHIPTONUM, DHSNAM AS SHIPTO, DHSAD1 AS ADDR1, DHSAD2 AS ADDR2, DHSAD4 AS CITY, DHSAD5 AS STATE, DHSPOS AS ZIP, REEMAL AS EMAIL FROM CMSDAT.CCFL EXCEPTION JOIN CMSDAT.OIH ON DCORD# = DHORD# UNION SELECT DHINV# AS INVNUM, DHORD# AS ORDERNUM, DHCPO AS CUSTPO, DHIDAT AS ORDERDATE, DHSCS# AS SHIPTONUM, DHSNAM AS SHIPTO, DHSAD1 AS ADDR1, DHSAD2 AS ADDR2, DHSAD4 AS CITY, DHSAD5 AS STATE, DHSPOS AS ZIP, REEMAL AS EMAIL FROM CMSDAT.OIH WHERE DHIDAT &gt; (CURRENT_DATE-1 MONTH) UNION SELECT DCORD# AS INVNUM, DCORD# AS ORDERNUM, DCPO AS CUSTPO, DCODAT AS ORDERDATE, DCSCUS AS SHIPTONUM, DCSNAM AS SHIPTO, DCSAD1 AS ADDR1, DCSAD2 AS ADDR2, DCSAD4 AS CITY, DCSAD5 AS STATE, DCSPOS AS ZIP, REEMAL AS EMAIL FROM CMSDAT.OCRH EXCEPTION JOIN CMSDAT.OIH ON DCORD# = DHORD# WHERE (DCSCUS IS NOT NULL AND DCSCUS &lt;&gt; '') UNION SELECT DCORD# AS INVNUM, DCORD# AS ORDERNUM, DCPO AS CUSTPO, DCODAT AS ORDERDATE, DCBCUS AS SHIPTONUM, DCBNAM AS SHIPTO, BVADR1 AS ADDR1, BVADR2 AS ADDR2, BVCITY AS CITY, BVPRCD AS STATE, BVPOST AS ZIP, REEMAL AS EMAIL FROM CMSDAT.OCRH EXCEPTION JOIN CMSDAT.OIH ON DCORD# = DHORD# JOIN CMSDAT.CUST ON DCBCUS = BVCUST WHERE (DCSCUS IS NULL OR DCSCUS = ''); LABEL ON COLUMN INTERNET.UPSTEST ( INVNUM IS 'Invoice Number' , ORDERNUM IS 'Order Number' , CUSTPO IS 'Customer P/O Number' , ORDERDATE IS 'Invoice Date' , SHIPTONUM IS 'Ship-to Customer' , SHIPTO IS 'Ship-to Name' , ADDR1 IS 'Ship-to Address1' , ADDR2 IS 'Ship-to Address2' , CITY IS 'Ship-to Address4' , STATE IS 'Ship-to Address5' , ZIP IS 'Ship-to Post Code' , EMAIL IS 'Email Address') ; LABEL ON COLUMN INTERNET.UPSTEST ( INVNUM TEXT IS 'Invoice Number' , ORDERNUM TEXT IS 'Order Number' , CUSTPO TEXT IS 'Customer P/O Number' , ORDERDATE TEXT IS 'Invoice Date' , SHIPTONUM TEXT IS 'Ship-to Customer' , SHIPTO TEXT IS 'Ship-to Name' , ADDR1 TEXT IS 'Ship-to Address1' , ADDR2 TEXT IS 'Ship-to Address2' , CITY TEXT IS 'Ship-to Address4' , STATE TEXT IS 'Ship-to Address5' , ZIP TEXT IS 'Ship-to Post Code' , EMAIL TEXT IS 'Email Address' ) ; 
A common hidden resource hog is pagination. by chance are they complaining about a page or pages that have a report with "X-Y of Z" pagination? if so what is the max rows for that report? I have seen people enter "999999999" so it gets all data but that can cripple an app. The "Z" in that pagination scheme changes the query from a top N query, to read the entire dataset just to return the number of rows.
Ok the email column only exists on CCFL. So my bad for telling you to add it to all selects. The second select statement (the one referencing CCFL) is the only one that should ref the actual email column. All the others will need a dummy ref try REEMAL AS TEXT. Apologies as I'm unfamiliar with this version of sql. 
&gt; All the others will need a dummy ref try REEMAL AS TEXT. I still get a Column REEMAL not ins specified tables. Error. &gt; Apologies as I'm unfamiliar with this version of sql. No worries, me either :)
Can you run the CMSDAT.CCFL select statement stand alone to se what it returns. You'll need to remove joins to other tables. 
+1 for [SQLZoo.net](http://www.sqlzoo.net) - this is a great place to start.
Haven't checked it out yet but, http://www.sqooled.com/
Correct, you have to use a GROUP BY when aggregating in your SELECT statement. select max(age),Name from table group by name You use HAVING when you are going to limit the results to your aggregated result. Such as : HAVING MAX(age)&gt;18 If you aren't aggregating, you can just use there WHERE criteria: where age&gt;18
You're right on target with the first part, but just a little clarification on the second. The WHERE clause filters rows BEFORE grouping occurs. The HAVING clause filters rows AFTER grouping (and aggregation) occurs. So, if we had a table like this: | Customer_id | Transaction_Amount | 1 10.00 1 -10.00 2 10.00 This query: SELECT Customer_ID, sum(Transaction_Amount) from aTable WHERE Transaction_Amount &gt; 0 GROUP BY Customer_id Would yield this result: 1 10.00 2 10.00 This query: SELECT Customer_ID, sum(Transaction_Amount) from aTable GROUP BY Customer_id HAVING Sum(Transaction_Amount) &gt; 0 Would yield this result (Since the total transaction for customer_1 = 0, that row is not returned): 2 10.00 Make sense?
I would use a subquery to rank the rows and then pull the top one for each asset: select AssetId, Caption, ROW_NUMBER() OVER (PARTITION BY AssetId ORDER BY CASE WHEN Caption LIKE '%nvidia%' THEN 1 WHEN Caption LIKE '%intel%' THEN 2 ELSE 3 END) AS Rank from tblVideoControllers group by AssetId Join to that by asset ID and rank = 1 and you'll get NVidia first, Intel second, something else 3rd. Edit: Forgot to partition.
Aggregation is generally the intended purpose of a GROUP BY, but I've worked with a number of people who (for reasons I cannot comprehend) use GROUP BY as replacement for SELECT DISTINCT. Confused me the first few times I ran across it.
 Select vc.AssetId , Caption = Case When vc.Caption like '%nvidia%' Then 'Nvidia' When vc.Caption like '%intel' Then 'Intel' Else vc.Caption End from tblVideoControllers vc ( nolock ) group by vc.AssetId , Case When vc.Caption like '%nvidia%' Then 'Nvidia' When vc.Caption like '%intel' Then 'Intel' Else vc.Caption End 
&gt; Then, is the only purpose for GROUP BY to use some aggregate function on the group For the most part. By itself it can also function as a `DISTINCT`, although it's generally considered best practice to use `DISTINCT` when you need distinct records. Most RDBMSs do not allow fields in the `GROUP BY` clause unless they're in an aggregate function. MySQL is somewhat broken in this regard as they have transparent "extensions" to `GROUP BY` that most DBAs I know (myself included) consider broken. &gt; Also, HAVING is just a quick way of filtering the table before the grouping begins e.g. getting rid of anything with an age of above 24 - therefore saving having to do this as a nested query? Sort of, but the way you put it is exactly backwards. The `WHERE` clause happens *prior* to the `GROUP BY`, and therefore it happens before any aggregate functions have been calculated. So you can't use an aggregate function in the `WHERE` clause (unless you use a subquery). `HAVING` is done after all that but before the `ORDER BY` that does sorting. If you want a list of all songs that appear on more than one album and how often they appear, for example: SELECT song_id, COUNT(song_id) AS "song_count" FROM album_song GROUP BY song_id HAVING COUNT(song_id) &gt; 1 
SQLfiddle 
[MSSQL: Nolock is not recommended](http://www.sqlpassion.at/archive/2014/01/21/myths-and-misconceptions-about-transaction-isolation-levels/) If you need to read tables without locking them use the transactional level of READ COMMITTED. Google "Nolock phantom records".
 BEGIN TRANSACTION UPDATE dbo.somedamntable SET value = 1 WHERE value = 2 ROLLBACK --- 12 records updated If that is your expected output BEGIN TRANSACTION UPDATE dbo.somedamntable SET value = 1 WHERE value = 2 COMMIT The rollback on the first statement will run and back out your changes in the event you update 2 million records. The second one will run but commit your changes.
It's buggy also. I normally ignore it.
Like '%bleh%'
I'll watch out for it, thank you. 
I've not used Intellisense much as we only migrated to 2012 from 2005 at the back end of last year. I used SQL complete because of that. Being a junior DBA I like the way SQL Complete gives you syntax hints. As the video was a learning SQL video I'd recommend it bugs or not.
I am referring to the ability to page through a report dataset. just like how the o's in gooooogle used to work. the first page of the report is 1-15, the second 16-30, 31-45, ect... well to display those numbers apex provides a few "pagination schemes" that you can pick from in the report attributes. The "x-y of z" scheme can beat up your system since it displays the total number of rows in the dataset (1-15 of 3,000). The problem with this is you are making your database work harder to get the "Z" part of the scheme. for smaller datasets this is not an issue, but when you start looking at serious reports than it can cripple the page.
Ok whats probably really needed here is a join to CUSTON.UPSCUST.UPSEMAIL in each select statement. Are there fields to logically join to this table from each of COIH, OIH and OCRH. What are the table definitions for each table?
https://www.youtube.com/watch?v=X9Jl2I-hwfE
^^^ what he said. Those are wildcard characters and can be very handy in situations like this.
if you are just mucking about, you can download the latest SQL Server for free and use it. 
You're right on the schema: CREATE TABLE Property PropertyID= PK.int, not null CREATE TABLE PropertyContact PropertyContactID =PK.int, not Null PropertyID =FK.int, not null LocationID =FK.int, not null ContactRoleID = FK.int, not null 
So is my query what you need or do you need something further than a list of all LocationIDs that have a ContactRole of 1 or 6 but not 7?
Using lng instead of long it does at least pass through an SQL Syntax Checker. I believe this is the issue. 
...Now I feel stupid. It worked. Tanks a lot!
No problemo. Best way to learn is diggin' in and makin' mistakes! keep at it!
It works in the sense that it pulls any PID with a ContactRole ID = 1 or 6 and not 7. But, I need to find only the LocationIDs that are on PIDs where that LocationID is **both** a 1 and a 6 -which is why I initially had created temp tables to inner join LocationIDs- and then return all other PIDs that have that LocationID as a 1 or a 6 (and not a 7). did that make sense? 
Forget trying to learn SQL recipe/tutorial style. SQL is a mix of relational algebra and relational calculus. And knowing these is essential in writing good queries (or formulating queries at all). One of the best books on learning SQL the right way is [Beginning SQL Queries From Novice to Professional](http://www.apress.com/9781590599433) by Clare Churcher. This will cover the basic nicely. After that, you might want to dive into the manual of the database of your joice. ***TL;DR:** Learn the basics well and exercises a lot!
Personally I'd make yourself a separate box and buy SQL Server Development Edition. I know it's Microsoft but it's the one I've seen used most often. Then join sqlservercentral.com and read the forums, do the question of the day and ask the awesome people there for help when needed.
I might just add: Learning the language is one thing, developing the experience of how data will trip you up is quite another. As you well know I am sure reality is never as clean and straightforward as examples from a book.
To learn, it doesn't really matter how you write your queries, IMHO. This is also nice: http://data.stackexchange.com/stackoverflow/query/new
That's not a bad idea, I actually have a perfect machine for it sitting there just waiting to be used. Though I don't know exactly why I like the idea...is it basically to act as its own server on my own wifi? I know very little about that. But I've been interested in that if that's the case, setting up my own intranet of sorts.
You're absolutely right. I was reading about relations and anomalies and 1NF's, etc. and I can see where/how things get jiggy pretty quickly. 
If I am understanding correctly you ware looking for something where the user select A date/Time and all the flights_Nbr near that time are selected? Perhaps the way to do it is to setup a range. such as: Declare @Departure date Select Flight_NR from flights where deptime between @departure -1 and @departure +1 this would list all flights -1 or +1 from the date they entered. Another way is to assume they know the Departure and Arrival location and incorporate those. 
Media storage? Email archive? Or, if all else fails mess about with adventureworks (the sample database) I thought you might have enough bits of kit about to make a box that you can mess up as much as you want.
:) This will be fun, thanks again. 
Please don't ever consider security not a concern and don't post code snippets around like that if you know they are bad. Other people see them, think they are normal, and use them.
* 1. You don't need to purchase any sort of MSSQL product, they already have a free one that you can download and use. MySQL and PostgreSQL, and Oracle all also have freely available versions. [SQL Server Express](http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx#fbid=i3RVNqQ070e ) [MySQL Community Edition](http://www.mysql.com/products/community/ ) [PostgreSQL](http://www.postgresql.org/) [Oracle Database Express Edition](http://www.oracle.com/technetwork/database/database-technologies/express-edition/overview/index.html) * 2. Get yourself a good book on sql. http://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1399662580&amp;sr=1-1&amp;keywords=tsql+fundamentals While this one is tsql specific it is extremely well written and the author makes note of anything that isn't part of the ANSI standard. * 3. Try and learn not to think in terms of looping over data but instead in terms of working with sets of data. This will become more clear once you've used it a bit.
It's not very elegant but something like this? http://sqlfiddle.com/#!6/53b14/1 
If you want to work in MSSQL download the [express copy of 2012](http://www.microsoft.com/en-us/download/details.aspx?id=29062). I'd suggest the SQLEXPRADV_x64_ENU.exe file so that you have all the features. If you want a prebuilt test database that has test data in it, download the [Adventure Works](http://msftdbprodsamples.codeplex.com/releases/view/93587) database. If not you can [create your own database](http://technet.microsoft.com/en-us/library/ms186312.aspx#SSMSProcedure) in SQL Management Studio Express.
thanks for the reply! i guess my confusion is in where this information is stored. am i storing the database info on my harddrive? if i choose to make my own database where are those tables going to end up in relation to my computer directory? thanks again for the info, i will look into Adventure Works as an idea to learn
Here's something I make use of quite a bit. SELECT CAST(DATEDIFF(D,0,GETDATE())AS DATETIME) This returns 00:00:00.000 as of today's date. Substitute this into your query to grab all SCHEDULED_FOR that fall on the desired day. SELECT * FROM vAppointment_Email WHERE CAST(DATEDIFF(D,0,SCHEDULED_FOR)AS DATETIME) = DATEADD(day, @DaysOut, CONVERT(date, GETDATE())) Hope this helps!
TSQL is so awkward dealing with this. It seems like there should be an easier way yo truncate the time off a date. This will work: Cast(convert(varchar(20), getdate(), 101) as date time)
Yes. New databases will get stored on your hard drive. You can define where when you create the data base. The default is something buried in program files like C:\program files\microsoft\microsoft sqlserver\msssql\... I don't remember exactly what it is right now.
Looks like that worked. Thanks a lot! I was so pissed when I couldn't get this to work on my own and I'm glad I don't have to do this within program code. 
As /u/drunkadvice says the databases are stored locally on your file system you do not even need to be connected to the internet after the initial install. Usually the database files are saved as \*.mdf/\*.ldf files and you can specify where in your file system to put them when you create the database. Though the default should be fine to start if you are just learning.
Hi /u/flipstables, could you elaborate on what you mean by non-sargeable? I am a bit of a novice myself. 
I've just today accepted a job moving from all reporting to all sql database administration work. I really appreciate these suggestions 
You're dead-on, I have no idea what indexes are! Based on your description it sounds like a way of pre-baking column data so that queries can act on them more efficiently. Would you be able to point me to a good reference on indexes? I'm working on a project now that sounds like it could benefit from them! 
Oooooh SNAP. We got them... we got lots of them. What's the maximum any one could bring back? Well shit... too many! It doesn't just cripple the page, though, the whole damn system takes a hit and just grinds to a halt. Users locking themselves and others and the load on the box spikes. Very strange... I'll have a look into pagination schemas. Maybe there's some way to monitor which ones are used most often and take the longest. It's a killer, for sure! Thanks for taking the time to reply!! Today Im About To Learn: Pagination.
I'm a quantitative analyst; using t-SQL (in the form of MS SQL Server) is about 90% of my job, if not more, and most of the other 10% is Excel. I had 0 SQL experience before that book, which I used to cram for interviews. I wasn't terribly confident about anything when I started this job, but my boss was really happy with my progress. I was actually rather surprised with how well that book prepared me, given its size and silly title. It gives a good foundation of how relational databases work, which goes MILES toward writing good SQL. Edit: And you're quite welcome! :) I love subscribing to this subreddit and helping people in any way I can. 
I'm an analyst and when I started I had a bunch of queries written for me by the SQL developer. I knew nothing about programming except for the Pascal class I took in highschool over a decade ago. The hardest part was that all the queries I had were written in different styles. It took a shamefully long time for me to find the pattern of select -&gt; from -&gt; where because some of the authors capitalized every keyword, others wrote all on one line, some started new lines in different places than everyone else. Basically no one was declaring variables and setting them, rather they were just writing huge formulas mid line over and over again. Somehow I learned joins before knowing anything about what I was doing. It was like an animal learns command/response through rote memory. Supposedly there is a difference in mental approach to problems between SQL coders and everyone else. Here you're working on entire sets of data all at once, where as C or Java developers are trying to build things procedurally. That might be bullshit I heard, but the point is you are working with entire tables all at once, selecting out of them, combining them, summing, ect.
You need some time to sober up to remember?
I love seeing my user name referenced when I'm sober. Especially when its a decent answer and not incoherent ramblings
Sure, two free resources that I used are: http://use-the-index-luke.com/ and http://www.red-gate.com/community/books/inside-sql-server-query-optimizer (sql server)
Cast getdate as varchar(12) in your where, this flattens all records to 00:00:00. if you're on newer SQL you can cast to DATE
My comment was a little unclear, I meant to say the intellisense was buggy. I haven't used SQL complete.
you're not looking for a "partial string match" at all dates and datetimes are ~not~ strings use the DATE() function, luke 
Evernote or similar works great for keeping track of queries. I've found search + tagging to be pretty useful for finding them again, and you can also paste in result sets &amp; it respects syntax highlighting. Whenever I write a query, I'm pretty anal about formatting. Select ... From ... Join .... Where ... And .... That way you can easily retread them later. Whenever I mail an excel document out, I put the SQL that generated it on tab 2. I don't know how many times this has saved my ass. There's a bunch of public SQL data sets floating around, you can load them into oracle express &amp; query them using SQL developer. 
SQLfiddle 
Thanks! 
Hey. I just read this for the third time, and have forgotten to thank you up until now. I appreciate you putting the time and effort into this explanation. If you're speaking of a computer program 3M developed called post it notes, I would definitely like to take a look at it if you have the file. I realized a couple days ago that all the preparation in the world isn't really going to help me. Like you said, I'm going to be asked to do specific things. I have taken in a good amount of information, understanding why certain commands create certain outcomes, and I think that is good enough leading into learning what I'm actually going to be doing. I do have one question about backups of files. Typically I imagine I'll be backing up many versions of the same file, all with different twists in the code, and I'm unsure how I'd develop a file-naming system to make it easy to go back and know what I'm opening. Is that just trial and error, and even then, still end up opening wrong files from time to time? Thank you so much, again.
http://s000.tinyupload.com/index.php?file_id=04494775426655317288 If that link doesn't work then let me know. As far as filenaming goes... learn to use multiple [directories] and then multiple version numbers. BIWAREHOUSE_update_query_v1.28 goes in the \SQL Queries\BIWAREHOUSE\Update Query folder. When you make an update then toss the older version into a *\Archive\ folder and leave only the most recent version there. edit: Good luck.
1. Is your pathing correct? Can you actually execute sqlite3.exe? 2. Is ex4.sql exist in the same folder as sqlite3.exe? 3. Does ex3.db even exist? 4. Fuck, what is your error? "Don't do shit" doesn't tell me shit. 5. Yes you are learning the most pain staking way.
I think that you are mixing things up. Database storage and query result display are two separate things. Here we are only talking about the client that runs the query (and thus displays the results). You wouldn't have this issue when using another client. I guess that you're using SQL Plus with Oracle ? Anyway, here is a quick explanation about strings and database storage (even if your issue doesn't look like to be related) : If your column is specified as varchar2(255), and a record has the value 'AA' for this column, it implies that the dedicated storage for this column of the record will be 2, BUT could be up to 255 if needed (i.e. if you want to store more characters). This differs from a column specified as char(255), in which 255 characters are reserved for each record, no matter the number of characters really used in each record (hence the "var" in varchar).
Thanks very much. I am using sql plus with Oracle yes. What I meant is that on 10, if I do a select on the column, it would give me this output: CO -- AA BB On 11: COLUMN --------------------------------------------------------(255 length) AA BB Hope this explains it better.
That's actually a syntax error In some databases. 
Apparently I'm working the wrong terminology (sorry, new to this and was using the office-speak). We're working with SSIS here
Yes I did. I still prefer this method. Again. Opinions vary
That sounds reasonable.
To expand a little bit on your actual problem: the OR operator doesn't work like that. It will get interpreted as SELECT * FROM Assets WHERE (ID = 9) OR (10) That is, the "ID =" part doesn't get extended to the 10, it's treated as an entirely separate part of the where clause. Now it turns out that any non-zero value will be treated the same way as TRUE, so what you wrote is basically the same as SELECT * FROM Assets WHERE (ID = 9) OR TRUE and because of the way OR works, this simplifies directly to SELECT * FROM Assets WHERE TRUE which will, as you observed, select the entire table. TL;DR SQL doesn't know what you mean by that
I missed that detail....
&gt;I wouldn't really recommend using temp tables in ETL like this. SVTR's comment is correct in its entirety. However, I recommend you take heed to his first statement and fix your process using temp tables.
Like the guys said, you need to be careful with the operations how you define them. Sort of like pemdas with math: 2+3*4 = 14 and not ~~24~~ 20. SELECT * FROM assets WHERE ID = 9 OR ID = 10 OR SELECT * FROM assets WHERE ID IN (9,10) the former is a long way to do it, and for larger statements very inefficient. the latter uses IN, which means "give me results where the the filter matches the specified list" and you can list things longer than that. edit: i cant math. 
I may be doing a bad job explaining this. the results from below already do what my query was doing (return all Properties wth LocationIDs that Match a ContactRole (1 and 6) and that property does not have a ContactRole = 7. But i need to find all the Properties with LocationIDs from that first query (regardless of their ContactRole) IFF that Property does not have a ContactRole = 7 associated. So, imagine the first query being an inner join of LocationIDs that have a ContactRole= 1 and 6, returning those Property Records as long as their is no ContactRole = 7. Te next query I'm trying to do is EVERY Property NOT on that original list that has either a ContactRole =1 or 6, but still doesn't match a 7. They have to match the first time, so i need to do the first part of the query, but afterwards, i can just make an inference that if they've matched before, they probably match again but we just have an incomplete record (i.e. has ContactRole =6, but doesn't have ContactRole =1 or vice-versa) Sorry if this is still unclear. 
I find it easier for readability to use the latter, so long as it is the same field it might as well be part of the same expression.
Luckily SQL maths for us! SELECT 2 + 3 * 4 Returned 14. Between that and Microsoft Word spelling/grammaring for me. Me noe nede schuul!
Ah yes, good old MS. Word. http://2.bp.blogspot.com/_o9IAU8j81lY/SbdWXlOyz8I/AAAAAAAAAJg/9rgfAY4t7rQ/s320/C1EE1_microsoft_word_write_letterz_n_.jpg
It is very unclear, I have read your post/s multiple times over. You are saying : Requery the data from query1 and query2 with the same query. 1. I needed to find all Buildings that have LocationIDs as two separate Contact Types (1, 6) but that building does not have another Contact Type (7) 2. But now I need to run a right outter join off of the LocationIDs and return all Buildings that have either ID (1 or 6) but not (7). 3. Te next query I'm trying to do is EVERY Property NOT on that original list that has either a ContactRole =1 or 6, but still doesn't match a 7. 
Whoever designed your tables used "ID" instead of "AssetID" which means you'll get lots of ambiguous field names when you start making joins, you forgot a semi-colon at the end, You used a "*" instead of specifying fields and didn't use a "LIMIT" which means you will be vigorously flogged by your DBA if your database is large and you run this on it. Oh and that stuff everyone else said about the extra "ID =" too.
This. Using Or makes it harder to edit for future use. Lets say you need to change it later to include 11, just add it inside the parenthesis and you're good. Alot easier than adding another condition.
 I noticed that you mentioned an alot, Jimbomacaz. Here's a picture of one for you: http://i.imgur.com/kTKRLlk.gif --- --- Hate me? Love me? Respond with an angry/loving comment! I read them all! ^^In ^^case ^^you're ^^really ^^dense ^^and ^^don't ^^get ^^the ^^joke, ^^'alot' ^^isn't ^^a ^^word.
Thanks, I was hoping to avoid using the 'column' formatting. Have like 200 reports some with 20 fields to convert.
Welcome to the real world, kid.
As others have mentioned: ID = 9 OR ID = 10 ID in (9,10) But don't forget: ID BETWEEN 9 AND 10 (or if you have a longer range and need to exclude e.g. one number: ID BETWEEN 9 AND 12 AND ID != 11 )
I figured it out. I just needed to take your suggestion of Intersect, turn that into a temp table, run another query that pulls every properties into a temp table, then right outter join based on LocationIDs in the first query where Property is NULL. Thanks for the suggestion and sorry, I need to do a better job explaining if I'm going to ask for help!
At the very basic level you are writing logic to join two arrays (then to join a third or fourth or fifth... to the resulting array) then select and filter to get the data you want. If two things have at least one thing in common, you can use SQL to join them. The FROM clause is the most important part where you join your tables. After that the WHERE clause adds filters to only return the rows you want. The SELECT is where you decide which fields/columns from the rows you want to actually see. If they use it a lot in your department, ask for read only user login and just play with your business use cases. Find all the employees last names. Find out which office each employee works at. Find out how many people are at each office. You can start at W3Schools for basic reference and Google for any questions and you'll find someone else that's hit it before. 
SQLZoo.net
[Microsoft Virtual Academy](http://www.microsoftvirtualacademy.com/) is free to anyone.
Thanks a ton!
Thanks...do they have Excel courses on there? Or do you know where I could find them?
Sqlservercentral.com sign up to the mailing list and read up on the qotd. Also epic forums. 
Brent ozar has some good tools for that
Yeah.. I started testing with other LocationCodes and just pasted in what I had.. thanks!
http://schemaverse.com - Learn SQL by playing a space battle game
Quick and dirty USE &lt;DbName&gt; EXEC sp_helpfile
Another approach would be to left JOIN to the table 3 times and use a coalesce statement every time you wanted a field from that table. Like: SELECT Coalesce(a.field,B.field,C.field) FROM SomeTable S LEFT JOIN Table A on criteria1 LEFT JOIN Table B on criteria2 LEFT JOIN Table C on criteria3
It would help if you included what the error messages say. Anyway, my guess is that you need to add your new column to both the SELECT and GROUP BY clauses, since there is a Count()
What is the subject of the first lab you have not completed?
Thank you so much for replying. Creating tables is basically what it deals with. I have to use two databases that came with the book named "Premiere Products" and "Henry Books." The lab requires me to Create a table, and add rows to the table. Then I have to delete the tables to add records to it, and run the script file for each database. 
Thank you!!!! God bless
Thanks for the reply, unfortunately I am home from work and can't get a screen capture or the error. I tried adding the column to both the select and group by clauses, but it still wouldn't work :( One thing that I tried was to simply change, T_NAM.TS_USER_01 /*Test.Application*/, to T_NAM.TS_USER_02 /*Test.Application*/, When I do this, the query runs and appears to return the data. So for whatever reason, it seems to freak out only when a new column is added.
How many labs do you need to do in order to pass?
You could be right, I am not sure, it is an internal query from HP Quality Center
Even though my professor isnt too great of a teacher, he is pretty lenient on his grading. the assignments dont have to be perfect but i have to basically do about 6 labs by next Monday in order to pass. 
Thank you, I will check those out!
Give an outline of each one of them.
I will teach you for free. edit: im going to schedule a desktop sharing with OP to demonstrate with ER diagram means and how to write a SQL
Virtual machine?
Doesn't the book itself have all the information you need to complete the labs?
I have spent the past 3 years writing queries for HP QC. Looking at this query, I would take a guess that the count() result will always be 1. I make this judgement from the fact you are grouping on a Time and a Date field and then a date time executed field. I think it would be highly unlikely that a groups of tests would occur at the exact same time. But I digress. When in trouble with these count queries, just start removing fields until you get a count with your new field. When you get the count value, start adding back the other fields one at a time. It is indeed Oracle that is the database.
Make the fieldname list the same in the Insert and Select statements Insert into SAT_RESOURCE (fieldname1, fieldname2) SELECT fieldname1, fieldname2 FROM [dbo].[DIM_DATE] AS d INNER JOIN proberenDWH.[dbo].[SAT_RESOURCE] AS r ON r.load_date = d.date_name WHERE r.load_date = d.date_name 
Thanks! No further comments are needed. It works! I've got this now; USE proberen Insert into FACT_MAIN (DATE_ID, LCR) SELECT DATE_ID, LCR FROM [dbo].[DIM_DATE] AS d INNER JOIN proberenDWH.[dbo].[SAT_RESOURCE] AS r ON r.load_date = d.date_name WHERE r.load_date = d.date_name 
Yea, but I would prefer to get it running native 
+1 on this !
that WHERE clause is redundant
Good call, I did see that the total was always 1 no matter what I changed. Thank you. Any other tips for Hp QC?
It's an epidemic. Kids are just coming up to reddit and asking for people to do their homework for them. There was a post in a vb subreddit not long ago for someone offering pay for their assignment. Why is it so hard for people to just do a little searching for help?
Dude, first of all: Calm down. You can do it, you will do it, everything is going to be ok.
I stumbled across this when i was looking for a basic tutorial, really well explained and nicely splitted. I second this.
Also note: DROP * FROM &lt;anytablename&gt; is usually NOT a good idea.
Then you're fine dude, it was similar for me - in fact it still is since i started only 1 1/2 months ago. You will aquire knowledge over time, there are trainings and of course you can always teach yourself. The only thing the company wants from you is, that you want to learn.
Have you considered joins? In your FROM clause, you can assign a variable name to the table, and use that in your SELECT * (Replacing * with your specific columns you want): Such as: SELECT a.col1, a.col2, b.col1, b.col2 FROM table_1 a JOIN table_2 b ON a.col = b.col WHERE bla bla bla Here's a decent article on joins: http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/
 SELECT it.* , ca.category , im.url ,im.id AS 'img_id' , si.ASIN AS 'similar_asin' , si.title AS 'similar_title' FROM items it --link categories to item by their product_id INNER JOIN categories ca ON it.product_id = ca.product_id --link images to item by their product_id INNER JOIN images im ON it.product_id = im.product_id --link similar_items to item by their product_id INNER JOIN similar_items si ON it.product_id = si.product_id --We only want to return out result set when the product_id is 1 WHERE it.product_id = 1 
 'FileNameColumnName' is a column name you supply, that will contain the name of the file. It will be added to the output columns of the file. You can then used derived column to massage it if you need to.
Well I couldn't get the FileNameColumnName function to work for me. I tried using a derived column and it worked just fine, I'd never tried that before. Thanks.
Yeah - I'm still learning joins but as I said in the OP, I posted this query that I tried. This is what you are talking about, yes? &gt; SELECT items.*, categories.category, images.url, images.id AS img_id, similar_items.ASIN as similar_asin, similar_items.title AS similar_title FROM (`items` JOIN `categories` ON (items.product_id = categories.product_id) JOIN `similar_items` ON (items.product_id = similar_items.product_id) JOIN `images` ON (items.product_id = images.product_id)) WHERE items.product_id = 1 &gt; 
I'm still learning joins and unions, but can you explain to me how inner join and join differ from your statement and the one from my OP? &gt; SELECT items.*, categories.category, images.url, images.id AS img_id, similar_items.ASIN as similar_asin, similar_items.title AS similar_title FROM (`items` JOIN `categories` ON (items.product_id = categories.product_id) JOIN `similar_items` ON (items.product_id = similar_items.product_id) JOIN `images` ON (items.product_id = images.product_id)) WHERE items.product_id = 1 
I want to output it to HTML. Since every product will have multiple images, categories, and similar products - I am going to end up having 1 single row, with many columns having multiple values. That's why I wasn't sure if what I am trying to do is possible. I don't care if they aren't in a single row, but ,my result set for a single query was coming back with 60+ rows, where it could've been 13 had I done them individually. I didnt know if there was a way to bring this down using JOINS.
There is not. That's the cardinality of your query.
Thank you! Since I'm new to the JOIN statements, I was frustrated not knowing ... and after literally hours of googling with no result, I realized there might be an SQL subreddit.
 the word INNER is optional
the problem here is that you are seeing cross join effects for a given item, it may have 2 categories, 3 images, and 4 related items if you join these tables on the item id, you will get 24 rows, as each category joins with every image, and each of those joins with every related item best solution here is separate queries
If you do this, make sure the database is offline before you copy.
With the exception of temp db. This database is recreated at the start of sql server. 
What sort of thing do you want it to say? You tell us what you want it to say, and we can make it look like SQL. 
Either just "Happy 25th Birthday (his name)" or "Happy Birthday (his name) I love you"
"JOIN" and "INNER JOIN" are equivalent, I just tend to write-out "INNER JOIN" because it's a bit easier to understand what's going on if there are other type of joins going on. 
Select "Happy Birthday" from birthdays where name = 'boyfriends name' and birthday = SYSDATE;
 SELECT 'happy birthday' , name FROM loved_ones WHERE BirthDate = 'May 14' --(or whatever it is) And Relationship = 'Boyfriend' Though that might imply you have other boyfriends.
The relationships are horrible across the tables for Requirements / Test Cases / Test Execution / Results. I always struggled with the relationships in the more complex queries. I created a spreadsheet listing Tables and Fields, then colour coded the cells across the relationships. This made it easier when compiling the queries rather than relying on mental gymnastics. I cannot see any reason why adding the additional field would cause issues, unless you missed the comma! 
Set a variable in an expression task to your path. Then configure an expression on the flat file source to set the connect string property to the variable.
 SELECT 'happy birthday' , name FROM loved_ones WHERE DATEDIFF(dd, '5/14/1984', BirthDate) = 0 And Relationship = 'Boyfriend' He'll be horrified if you store a date as a string. 
Thanks!
I understand now. Thank you for putting it in simpler terms than what I could find through google.
Thanks for the input. That's what I ended up doing and it's all working great now.
Not sure if I understand your data structure, but this might get you started. Update tblItem Set flag = 0 From tblItemCategory Where tblItem.ID = tblItemCategory.ID And tblItemCategory.Category = 'N/A'
he'll also mention that the DATEDIFF is wrong
 Drop table RELATIONSHIPS (Don't do this.)
What is the nature of the error?
 INSERT INTO relationships (name, relationship, status) VALUES ('his name','boyfriend','love') or INSERT INTO relationships (name, relationship, status) VALUES ('his name','boyfriend','Happy Birthday!') edit: format
I'd like to think that this would do what you are asking... SELECT Category.Name, CASE WHEN Values.Name is not null then 1 ELSE 0 END AS FLAG FROM Category LEFT OUTER JOIN Values ON Category.ID = Values.ID
&gt;perphaps with some kind of delay on the trigger No. Triggers are ment to be executed inside the same transaction the insert / update / delete runs, otherwise, the tiggers would be kind of useless (they are very often used to raise errors on logical restraints, or to update other data, both you really need to be transactional comliant). as /ujust_dig_in said, post the error message, also, please, very please, with a cherry on top, tell us what dbms you are using. It actually does kind of matter. It by the way, would not be a bad thing to actually know what the trigger causing (maybe) is doing. Sourcecode is an other very nice thing. If we don't know what you are doing that is raising some kind of error, we cant tell you anything. If we only have the error, well, the best possible answer I can actually give you right now : Do it right, and it will work. tldr: If you want help, provide us with an as complete picture as you can. Otherwise we will have to guess, or not respond. Oh and before I forget, ha, this is actually a case where I kind of have to spell it out: Learn enough to even talk to us DB nerds, on a level, that WE don't have to put in effort to understand. You are asking US to *for free* answer somthing that is a typical consulting kind of matter. When asking us for a favour, at least read the sub's rules which contain the essence of my entire post. Otherwise, offer us money (yes, knowlage kind of gets payed), and WE will put in effort to understand you. No money, YOUR effort. /meta, what the fuck is up with today? Between the I dont want to pay for something that clearly should be payed for help, and the dumb fuck collage student asking for his homework, today is a pretty shitty day in this sub aint it
In light of the information given, I would say the answer would be Yes. :)
 SELECT 'Happy Birthday ' + name + ' to you!' FROM loved_ones WHERE BirthDay = GETDATE() Quietuser had a pretty good idea. This is a bit more simple and correct.
I actually found a youtube video that showed me most of what I needed to know. A variable for the filename is exactly what I needed. https://www.youtube.com/watch?v=38H4MlCPjr8
There's also no need for BEGIN/END. But I like the idea.
I think OP may want to do a compare against 'N/A', but it isn't clear. SELECT Category.Name, CASE WHEN Values.Name = 'N/A' then 1 ELSE 0 END AS FLAG FROM Category LEFT OUTER JOIN Values ON Category.ID = Values.ID
indeed. later when I have sometime I'll do all those points. I just had to post quickly at that time and was within kind of panic
maybe I did and couldn't solve my problem? are you retarded? I came here as a last help method, cause I'm not being able to get it done by any means. but I get your point
Calling me retarded doesn't endear you to us. You post this with no indication of what you did except for a questionable file on a Google drive. We aren't here to do your homework for you. At this point you act like an asshole who couldn't be bothered to do his or her own homework in time and wants someone to do it for them. 
Bit of an ambiguous answer. Can you just post your schema? SHOW CREATE TABLE BIGDATA; SHOW CREATE TABLE SMALLDATA; BigData needs an index on Reference. SmallData may need an index on (Reference, Lookup) How often is data inserted into the table? If you are inserting nightly but then its read only for the rest of the day, you could benefit from a large query cache. What storage engine are you using, and have you optimised my.cnf?
What's the usage pattern like? Are all records accessed equally? How are your "bunch of disks" laid out (single RAID)? I don't know if MySQL supports partitioning, but that might help
I am sorry for not being detailed enough. I am talking about MySQL. I have two seperate databases that reside on the same server. The code for the trigger: http://pastebin.com/tttfTHSN There's no specific error message (just a regular vbulletin database error page). Nothing to be found in the error log either. 
I don't know since there's no specific error message on either the web page or in the error logs.
How are they related?
I called that just because of your atitude generalazing me just because I have a doubt. I have about 80% of the work done just this part is giving me a hard time. Sorry for trying to find help under pressure
You simply posted what you were doing and and a .bak file with no other information about it. You didn't indicate what you've attempted, where specifically you're stuck, or anything else. Asking for help under pressure doesn't preclude you from leaving out details. Leaving out details makes it look like you want someone to do it for you. Providing a file and asking people to install it on their box is just sketch. End of story. I'm not battling with a kid over this anymore. If you want help demo what you've done and then you might get some.
sorry about all the trouble then. I created a new post, lets see if this one is a bit more clear http://www.reddit.com/r/SQL/comments/25mcnw/joining_subqueries/
 SELECT ROW1, ROW2 FROM TABLE T1 WHERE ROW1 LIKE '543%' AND NOT EXISTS ( SELECT 1 FROM TABLE WHERE ROW2 = T1.ROW2 AND ROW1 NOT LIKE '543%')
It's difficult for English speakers to decipher this, but I'm taking a guess with : select parti.SIGLAPAIS, res.CLASSIFICACAO, count(*) MEDAL_COUNT from RESULTADOS res join PARTICIPACOES parti on res.IDPARTICIPACAO = parti.IDPARTICIPACAO where res.CLASSIFICACAO &lt;= 3 group by parti.SIGLAPAIS, res.CLASSIFICACAO If you need the country name, use this as a sub select select p.NOMPAIS, m.CLASSIFICACAO, m.MEDAL_COUNT from (select parti.SIGLAPAIS, res.CLASSIFICACAO, count(*) MEDAL_COUNT from RESULTADOS res join PARTICIPACOES parti on res.IDPARTICIPACAO = parti.IDPARTICIPACAO where res.CLASSIFICACAO &lt;= 3 group by parti.SIGLAPAIS, res.CLASSIFICACAO) m join PAISES p on p.SIGLAPAIS = m.SIGLAPAISE
OK that is so much more clear. I will take a look and see if I can help you.
As long as they are also in your group by statement you can partition your count. Select Country ,medaltype ,count() as medal count From foo Group by Country ,medaltype That's not the exact table names but it should point you in the right direction. Good luck!
Sorry for the language transaction. This is exactly the ''kick off'' that I needed, with this I can do the rest of the question. Thank you so much! Have a good day kind sir
Thank you! Have a good day
dude... it should fit on a cake... 
You haven't provided anough information, your problem is too vague. 1. Which version of MySQL are you using? (In versions earlier than 5.5.6, some tables will queue DML actions due to multi-threading.) 1. What is the PK of z.wp_users and what indexes exist on it? 1. Are there any foreign keys on it? 1. Does this table have any triggers on it? 1. Could the insert error be a duplicate key error? I'm not a MYSQL expert, but my advice would be to declare some sort of [**handler**](http://dev.mysql.com/doc/refman/5.1/en/declare-handler.html) for errors in the trigger and log it. 
Investigate not using a trigger.
Oh i feel bad for you reading all these posts lol. I suggest you keep it simple: SELECT 'Happy Birthday ' + name + ' to you!' FROM Loved_Ones WHERE isTrueLove = 1 1 = True, typically True False is stored as a Bit, 1/0 for True/False. This also assumes that you have only 1 true love :) (and he is it)
Thanks. I really just needed confirmation that there wasn't some concept I wasn't grasping. I'll just chalk this one up to sqlzoo being stringent on what it accepts as an answer.
That's what table partitioning is for! ...cake partitioning? Yeah I need to stop. 
Agreed that it is functionally the same, however, consider a more complex column in place of count(c.actorid)...like if the column you wanted to ORDER BY was a CASE statement with several 'WHEN's, there were calculations and/or multiple functions being used in the column, or possibly all of these things! Then ORDER BY &lt;alias_name&gt; would be far easier/more efficient to manage. Giving you the benefit of the doubt though if that's exactly what sqlzoo was looking for, then they probably should drop a hint to reference the alias not make an assumption there.
:D 
I am currently reading [this documentation](http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc32300.1570/html/sqlug/sqlug160.htm) as your question caused me to realize i don't have as solid of an understanding of this topic as I would like. I know it is sybase but I imagine the information is mostly universal.
So if it doesn't have the (+) it will be enforced like any other where clause. e.g. status = 'A' means regardless if it exists in this outer joined table or not, it needs to have a status of 'A'. * (+) means add it to the on clause because it doesn't have to resolve to true * not (+) means add it to the where clause because this must be true weather the data exists or not.
I've done more advanced sorts that aren't part of the select list before, some of which are quite handy (granted, I work in SQL Server, not MySQL) The best example I can think of is comparing two lists in a full outer join, where you want to be able to visually and easily see what's missing in each list. SELECT T1.val1 ,T2.val1 FROM Table1 T1 FULL OUTER JOIN Table2 t2 ON T1.ID = T2.ID ORDER BY ISNULL(T1.Val1, T2.Val2) 
I do see your point on the ease of management if the sort was a complex case/formula, where maintaining/editing it in one place would be advantageous.
i tried this, was harder to get into than i thought. ended giving up on it because i had no idea what i was doing 
Union all?
nice. Union All. That seems to be it. I figured it was something stupid. Thanks!
OP is regretting her decision of asking us.
Yeah... i think so
Nevermind, I believe it is the DISTINCT command.
Thank You, this structure is what I was looking for!
Also if you want to know how many people are in each city, for example, you can use GROUP BY: SELECT City, Count(*) Records FROM Employees GROUP BY City ORDER BY City
I will have to try this out. Thank you for the feedback!
Of course, it might make more sense to do city and state, but you get the idea. Have fun. 
Both answers are right here. Normally use DISTINCT if you just want to remove duplicates from a list and only get unique records back (you can sometimes also use the keyword UNIQUE for the same purpose). SELECT DISTINCT City FROM Employees Use Group by if you want to have some aggregate values from your data: SELECT City, COUNT(*) AS NumEmployeesInThisCity FROM Employees GROUP BY City Note you can use other aggregate functions: SELECT City, SUM(Salaries) AS TotalSalaryForThisCity FROM Employees GROUP BY City You can use numbers in the group by which correspond to the columns output in the select, and you can group by multiple columns: SELECT City, PayGrade, AVG(Salaries) AS AvgSalaryForThisCity FROM Employees GROUP BY 1,2 ORDER BY 1,2 You can use a WHERE to filter the records that go into your group: SELECT City, PayGrade, Sum(Salaries) AS SalaryforPayGradeandCity FROM Employees WHERE EmployeeCategory = 'Technical' GROUP BY 1,2 ORDER BY 1,2 Finally you can also filter items based on the group aggregate values: SELECT City, PayGrade, COUNT(*) NumEmployees FROM Employees WHERE EmployeeCategory = 'Technical' GROUP BY 1,2 HAVING COUNT(*) &gt; 5 ORDER BY 1,2 Hope this helps a bit with the examples. 
i love you
Thanks for the reply, I like your view on not over-complicating SQL. I often see very complex SQL statements, usually in Stored Procedures, which become a headache to maintain.
de nada!
Isn't that why they're in SPs? Because the DBA doesn't want developers messing with them? 
Before I try to answer your question... You need to remove your unnecessary correlated subquery. SELECT vakklas.klasid, vakmodule.vakid, COUNT(moduleopdracht.opdrachtid) FROM moduleopdracht INNER JOIN vakmodule ON vakmodule.moduleid = moduleopdracht.moduleid INNER JOIN vak ON vak.vakid = vakmodule.vakid INNER JOIN vakklas ON vakklas.vakid = vak.vakid INNER JOIN userklas ON vakklas.klasid = userklas.userid LEFT JOIN usermodulecomplete ON usermodulecomplete.moduleid = moduleopdracht.opdrachtid WHERE userklas.userid = 5 GROUP BY vakmodule.vakid,vakklas.klasid I think you want the 4th column to show moduleopdracht.opdrachtid that have been completed? Correct? What field determines if the module is already done or completed? SELECT vakklas.klasid ,vakmodule.vakid ,COUNT(moduleopdracht.opdrachtid) ,SUM(CASE WHEN moduleopdracht.CompletionStatus = 'Y' THEN 1 ELSE 0 END) as Already_Done FROM moduleopdracht INNER JOIN vakmodule ON vakmodule.moduleid = moduleopdracht.moduleid INNER JOIN vak ON vak.vakid = vakmodule.vakid INNER JOIN vakklas ON vakklas.vakid = vak.vakid INNER JOIN userklas ON vakklas.klasid = userklas.userid LEFT JOIN usermodulecomplete ON usermodulecomplete.moduleid = moduleopdracht.opdrachtid WHERE userklas.userid = 5 GROUP BY vakmodule.vakid,vakklas.klasid 
Also a good reason.
This mostly from the analysis/BI standpoint, which I am not sure is the perspective from which you are asking, but I was thinking about this very issue the other day, so I'll go for it. Also, I don't really have much to say about how SOLID applies to SQL, but maybe something about OO more generally. I often think of my ETL processes as encapsulating logic in a similar way to OO principles. For example, say you have a concept that isn't necessarily built into the application you are pulling data from, that you want to report on, like, diabetic patients (I work in healthcare.) There might be a fair amount of logic in determining which patients are actually diabetic, you might need to check for a number of ICD-9 codes in the diagnosis table, maybe you also check for certain lab results that would indicate a diabetic patient, maybe you have similar data coming in from other systems that you need to check as well, etc. One could pull all of those tables into your datamart (they're probably already there for other reasons) and include all of that logic every single query that references diabetic patients. The problem this creates is quite similar to the problems OO code helps solve. This means that any Jane or Joe that wants to write a report that even references whether a patient is diabetic or not, has either reinvent the wheel and think carefully about how the concept of a diabetic patient ought to be defined or paste in some trusted SQL from another report, which is usually what happens. Now imagine you have a bunch of queries built up over years of analysis, and all of a sudden a diagnosis coding convention that comes out (or you discover that your previous version included pre-diabetics or was somehow wrong.) You then have to hunt through all those queries and change the portion of them that picked out diabetic patients. If you had built that concept into your ETL process, a couple things would be different: 1. Your queries would be more readable for humans. Instead of 40 lines of code, you would see "INNER JOIN diabetic_patients ON diabetic_patients.patient_id = patients.id" 2. If you needed to make a change, you can make that change in one place, and it will propagate to all the other queries you have written. This to me seems quite a lot like what you get when you write good OO code.
/u/y_u_no_Nguyen_yet, the Reddit Grammar Police Bot has detected an error in the description of your link: &gt; Hello, Thank you for taking a look at this. &gt; Currently I am working with one table, but I plan on joining in about 8 more tables with the same information. &gt; I have economic indicators for a specific country for 18 years, there are 1300+ indicators, but not all of them are used. If the year does not have a value for that indicator it is reflected as "NA". I flagged all "NA" records and added the flags. The problem is the code is very long and I'm looking for a better way of writing it since I have to Join in about 8 other tables. &gt; My end goal is to cross reference all the tables and choose only indicators that have less ~~then~~ [**than**] 10 total "NA's" from all 8 countries from their 18 year respective periods (All 18 year periods are different for each country) &gt; Thank you! Here is the code: &gt; select Indicator_code, &gt; SUM( &gt; (Case When YR1961 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1962 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1963 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1964 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1965 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1966 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1967 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1968 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1969 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1970 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1971 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1972 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1973 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1974 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1975 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1976 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1977 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1978 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1979 = 'NA' Then 1 else 0 End)) AS "# of NA's" &gt; From ARGDATA86 &gt; GROUP By Indicator_Code &gt; Having SUM( &gt; (Case When YR1961 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1962 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1963 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1964 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1965 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1966 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1967 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1968 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1969 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1970 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1971 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1972 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1973 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1974 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1975 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1976 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1977 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1978 = 'NA' Then 1 else 0 End)+ &gt; (Case When YR1979 = 'NA' Then 1 else 0 End)) &gt; &amp;lt; 10; As indicated above, we recommend that you use that have less [than] 10 instead. 
Off the top of my head, assuming you can't change the data, How about just concatenating the fields and using string length?
I put the below together, but then was not sure it is correct looking at your coding again.. Your current table structure would help.. a lot.. as I cant quite fiqure out how a returned value of YR1977 has a value of 'N/A'. SELECT Indicator_code, Sum(countofNA) AS SumOfNA FROM ( SELECT Indicator_code, count(*) ascountofNA FROM ARGDATA86 where value1 ='N/A' group by Indicator_code union all select Indicator_code, count(*) from ARGDATA87 where value1 = 'N/A' group by Indicator_code union all select Indicator_code, count(*) from ARGDATA88 where value1 = 'N/A' group by Indicator_code etc....for each addtional table ) GROUP BY Indicator_code having Sum(countofNA) &lt; 10
PL/SQL will return NULL for an outer join with missing relation, not N/A 
What would be the best way of showing the table structure? 
Here is an SS http://i.imgur.com/9dDLaPM.png
Here is another of the data http://imgur.com/dKFM4IL
Non-ansi compliant SELECT BigData.DataPoints FROM BigData, SmallData WHERE BigData.Reference = SmallData.Reference AND SmallData.Lookup = 'value' While MYSQL doesn't comply to all most recent standards for SQL. I would strongly suggest moving away from non-ansi compliant joins. Ansi compliant SELECT BigData.DataPoints FROM BigData inner join SmallData ON BigData.Reference = SmallData.Reference WHERE SmallData.Lookup = 'value' If your original code does run, great. However, one day you be testing an upgrade to a more recent version of MySQL and suddenly all your code is taking forever to run. If MySQL no longer supports non-ansi compliant queries, your join in your where statement will be ignored create a Cartesian and then filter based on the join in your where statement. MSSQL 2005/2008 will drop support for non-ansi compliant queries the moment the database is raised out of SQL 2000 compatibility mode. Unfortunately, from 2004-2013 we had a litter of oracle developers developing for SQL 2000 internally. We literally cannot upgrade our databases beyond the 2000 compatibility. As DBA's this prevents us from using new statistics and indexes and having to deal with this sub 2008 standard. If you get out the habit now, it will pay off immensely in the future when MySQL and Oracle adopt the new standards. **EDIT** Especially when you are talking "Big Data" which is pushing new and new versions of SQL servers. You don't want to be on the bleeding edge of technology using joins that compliance standards ruled inefficient back in the 1980s.
 DECLARE @outercursorcount int SET @outercursorcount = 0 --outer cursor code open SET @outercursorcount = @outercursorcount + 1 --Inner cursor --Inner cursor deallocate --outer cursor deallocate SELECT @outercursorcount Sorry, I rarely write cursors so if I don't look up the syntax of how write a cursor I can't remember how they are built. I'll spare you the lecture on cursors. 
Can you try this? select Indicator_code, COUNT(NULLIF('NA',YR1961) + COUNT(NULLIF('NA',YR1961) + COUNT(NULLIF('NA',YR1962) + COUNT(NULLIF('NA',YR1963) + COUNT(NULLIF('NA',YR1964) + COUNT(NULLIF('NA',YR1965) + COUNT(NULLIF('NA',YR1966) + COUNT(NULLIF('NA',YR1967) + COUNT(NULLIF('NA',YR1968) + COUNT(NULLIF('NA',YR1969) + COUNT(NULLIF('NA',YR1970) + COUNT(NULLIF('NA',YR1971) + COUNT(NULLIF('NA',YR1972) + COUNT(NULLIF('NA',YR1973) + COUNT(NULLIF('NA',YR1974) + COUNT(NULLIF('NA',YR1975) + COUNT(NULLIF('NA',YR1976) + COUNT(NULLIF('NA',YR1977) + COUNT(NULLIF('NA',YR1978) + COUNT(NULLIF('NA',YR1979) AS "# of NA's" From ARGDATA86 GROUP By Indicator_Code Having COUNT(NULLIF('NA',YR1961) + COUNT(NULLIF('NA',YR1961) + COUNT(NULLIF('NA',YR1962) + COUNT(NULLIF('NA',YR1963) + COUNT(NULLIF('NA',YR1964) + COUNT(NULLIF('NA',YR1965) + COUNT(NULLIF('NA',YR1966) + COUNT(NULLIF('NA',YR1967) + COUNT(NULLIF('NA',YR1968) + COUNT(NULLIF('NA',YR1969) + COUNT(NULLIF('NA',YR1970) + COUNT(NULLIF('NA',YR1971) + COUNT(NULLIF('NA',YR1972) + COUNT(NULLIF('NA',YR1973) + COUNT(NULLIF('NA',YR1974) + COUNT(NULLIF('NA',YR1975) + COUNT(NULLIF('NA',YR1976) + COUNT(NULLIF('NA',YR1977) + COUNT(NULLIF('NA',YR1978) + COUNT(NULLIF('NA',YR1979) &lt; 10;
You start. What do your answers look like? what do you not understand?
Cannot see a way around this. Your data is denormalized (values for all years occur in a single row), and you need to refer to every value to achieve your goal. The only thing I can offer is to normalize the table(s), but then you would need to do something similar to what you are currently trying to avoid in order to do the normalizing. If it is available, perhaps you could look at PIVOTing your data in a view? 
yeah i know that cursors arent ideal, but for the routine thats running its needed. attaching accounts to repair centers (1 account gets 48 repair center links) so it cursors through 355 new accounts linking to all repair centers. thats like 16000 transactions.. and thats why i want a counter, for debugging, see if im getting what i expect. Thanks for the tip!
You are joining Courses to Course Modules. One of the tables is a one to many of the other. Since Course code is being grouped, it would seem to be the "one" in the "one to many" relationship. So if you are joining courses on course modules and looking at only courses that return greater than 5 rows... I can't give the whole answer so I'll leave some blanks in the text. "The query is returning courses that have a minimum of 5 ______ in the table ________"
And followed up with: OP... are you posting here just simply asking people to solve your exam question.. or have you attempted it yourself? What don't you get that you want explained?
I have attempted it, but i don't think it's correct: SELECT ModuleCode FROM CourseModule WHERE CourseCode IN (SELECT CourseCode FROM Course WHERE Title = 'BSc Computing Science');
Correct. You could also assume that CourseModules is a one to many relationship to Courses based on its naming convention. Smart developers Prefix their one to many relationships with the parent table in a one to many relationship. CourseModules is almost completely worthless data if you aren't associating it back to its parent table.
Thanks guys, unfortunately the amount of indicators(1300+) is a lot less than the amount of Years i'm looking at (18). I guess this will have to suffice. 
This is an unpivoting exercise, you need to rotate data from columns into rows. This situation has occurred because your database isn't designed correctly. It isn't normalised. If you are using Oracle 11g you can achieve what you want by using UNPIVOT as follows: select INDICATOR_CODE, count(*) from ( select * from ARGDATA86 unpivot include nulls (YEAR_VALUE for YEARS in (YR1961, YR1962, YR1963, YR1964, YR1965, YR1966, YR1967, YR1968, YR1969, YR1970, YR1971, YR1972, YR1973, YR1974, YR1975, YR1976, YR1977, YR1978, YR1979)) where YEAR_VALUE = 'NA') group by INDICATOR_CODE having count(*) &lt; 10 
I am going to steal this and save it. :)
OK, this makes it very simple. Add another column in the spreadsheet for count of NA [countOfNA] Count the number of NA values across the row =COUNTIF(E2:W2,"NA") You then only have to query the [countOfNA] value to determine if that indicator meets the requirements. 
By repair centers, this is a table that needs a record? Or is a user account in sql that needs access to a database? 
table(s)
I have played with KNIME in the past. It would no doubt be capable of this. OT, but it's stunning what's out there at no cost.
I love that Knime lets you see your tables at every step along the way. Really helped me get a grasp of the way pivoting works (and to see and learn from mistakes too). Can't remember if it sums automatically when it pivots, but if OP replaced his data with flags at an earlier step (NA --&gt; 0, all else --&gt; 1) then it might just be Add All Fields and they'd automatically sum by year. 
 SELECT CM.ModuleCode FROM CourseModule CM JOIN Course C on C.CourseCode = CM.CourseCode WHERE C.Title = 'BSc Computing Science' 
I'm willing to bet money on it ... If you post the table structure, and sourcecode, we will come up with a no cursor at all solution. I can't guarantee that it would be faster, but, what I am willing to bet money on is, that it will be faster than a nested cursor in a cursor*. *might include fixing fucked up data models thou 
That's a nice link. I remember when I was in school I had a hard time finding info on ER diagrams.
 Select ID, Birthdate, DATEDIFF(year, BirthDate, GETDATE()) as Age from users You should never store age but rather calculate it based on the birthdate vs current date everytime you pull the data. 
Thank you! But somehow it is required for us to store the updated age on the Age field. I'm thinking it should be an event or something. Not sure.
Trigger maybe? It's a dumb requirement and I would suggest pushing back against it as whoever wrote and then accepted it has little idea of good database practices. 
It's nothing as simple as calculating age, but I have a table that stores a bunch of metrics for our case tracking system. I have a job that runs every night at 2 AM to transaction update all the records. You should also be able to do this with a View, but I'm guessing the reason for this dumb requirement is legacy reporting and they don't want to go find and update queries to point at the new object. 
Something like this (http://technet.microsoft.com/en-us/library/cc645912(v=sql.105).aspx) perhaps? 
Is this (http://technet.microsoft.com/en-us/library/ms135739(v=sql.105).aspx) what you are talking about?
If you can afford it or like me get your company to pay it then Pluralsight is the way to go. I'm doing SQL server 2012 right now and the teacher is awesome.
Generally, you shouldn't store calculated columns. If you are up to date with SQL Server, you can create a computed column. This is a virtual column that is not stored and is ideal for your situation. http://technet.microsoft.com/en-us/library/ms188300.aspx
Add as computed col. This is a more accurate age calculator, since one doesn't age simply because the year changed, as this sample demonstrates. create table #bd ( bd date ); insert into #bd (bd) values ('5/18/1935'); insert into #bd (bd) values ('5/25/1935'); alter table #bd add [Age] as (cast(datediff(yy,bd,getdate()) - (case when getdate() &lt; dateadd(yy,datediff(yy,bd,getdate()),bd) then 1 else 0 end) as tinyint)); select * from #bd
Keep in mind that this won't reliably calculate their actual age because DATEDIFF with a YEAR parameter will calculate the difference between the years component of the dates and is not looking to see that there is a full year between. For example DATEDIFF(year, '2013-12-31', '2014-01-01') returns a value of 1 even though there is only a single day between whereas DATEDIFF(year, '2013-12-30', '2013-12-31') returns a value of 0. If you are just looking for a rough approximation this may work for you, however if you need to be more exact you can end up with a lot of off-by-one values. A closer (but still not perfect) approximation is FLOOR(DATEDIFF(day, 'yyyy-mm-dd', CURRENT_TIMESTAMP) / 365.25)
Yep.
Look out for the next offering of Stanford's free online course, ["Introduction to Databases"](https://class.stanford.edu/courses/Engineering/db/2014_1/about). 
Thanks, I've made an account and will have a look around on it later.
remove under investigation from the where clause and add a case to the select. (SELECT PCMS_CHDOC_HISTORY.CH_DOC_ID, MIN(CASE WHEN PCMS_CHDOC_HISTORY.STATUS = 'UNDER INVESTIGATION' THEN PCMS_CHDOC_HISTORY.DATE_AMEND ELSE NULL END) AS DATE_RESPONDED FROM PCMS_CHDOC_HISTORY GROUP BY PCMS_CHDOC_HISTORY.CH_DOC_ID) SQ2 
You could create a code snippet. If you have tools like SQL Promt or similar, that should be quite easy. Other than that you can create snippets in vanilla SSMS as well, http://technet.microsoft.com/en-us/magazine/jj554304.aspx
Excellent! I've just run away from work to avoid having an SQL-related aneurysm but it looks good and I'll try it tomorrow. Thanks very much for your time! 
uff, I got a LOT of snippets. A fair amount of queries to the DMV's, queries to look stuff up in the datamodel, really, a lot of them. For me it ranges from a simple ij for INNER JOIN to huge queries digging trough the DMV's. I use SQL Prompt thou, I'm not sure that the SSMS internal snippets are as flexible. 
&gt; It's a dumb requirement The.End. 
Not using "year" as an argument isn't about accounting for leap-years. It is about reliably returning an accurate age. Using "year" can and will give you off-by-one results when calculating age with DATEDIFF: Billy Example was a New Year's baby, born on 2000-01-01. He has had his 14th birthday as of today, 2014-05-19. DATEDIFF(year, '2000-01-01', '2014-05-19') returns 14 Jimmy Example was born later in the same year on 2000-10-01 (Mr and Mrs Example don't waste time). He has not yet had his 14th birthday. DATEDIFF(year, '2000-01-01', '2014-05-19') returns 14 Your industry may not require accuracy when it comes to age so "year" may suffice. Granted, I can think of many situations where this is not relevant. However, anything that involves calculating age thresholds such as majority, retirement or anything else that is bound by some age-related regulation is important that you don't include/exclude people accidentally by this nature of error.
Oh boy, might want to try /r/sysadmin ? I dont think detatching, and reattaching to a new SQL Server would be a good idea. Probably best you can do is make a backup, and restore on the new instance? I think you will have to change the reports/access MDB connection strings for this. It sounds bad, but maybe shutdown the DB; take note of what breaks, restart the DB, and prepare a transition plan and take your time; you dont want to mess this up if it is critical to the business in question.
http://www.databaseanswers.org/data_models/ I've used this before just to get an idea; but experience and the end-goal matters a lot in database design. I've heard this book is good, but havent purchased it yet: http://www.amazon.com/Data-Model-Resource-Book-Vol/dp/0470178450/ref=pd_sim_b_2?ie=UTF8&amp;refRID=0CQ088ZFPFKKTMR9CKQ7
Yea, I was hoping for an easier way, but I guess not.
You are correct. Took me some time to find it but the last time I wrote something calculating age was the following. SELECT DATEDIFF(DAY, Birthday, GETDATE())/365 Its still wrong though. SELECT DATEDIFF(DAY, '2000-05-20', GETDATE())/365 As its still returns 14. Using 365.25 is wrong as it returns 13.99. I'll have to revisit this one.
Just stop the SQL services on the old server, copy the MDF and LDF files over (don't forget your full text indexes!), attach the DB to the new server, and power off the old server. Give the new server the old server's IP. You can add it as a secondary if you need the new one to retain its current IP. Alternately, if you are absolutely sure that all clients/apps access the old server by NAME (not IP) then you can skip that and just create a CNAME record in DNS. Make sure you delete the old A record first. You could use cliconfg to set server aliases but DNS is easier and you don't have to edit each client/app. If the client pings the old name on the new IP, you're all set. The biggest gotcha will be the SQL user accounts. You will need the password(s), and you'll have to delete the user from the databasename\security\users folder after attaching it. Then create the same user with the same password under servername\security\logins, then use the user mapping section to grant access to the DB (probably dbo). If you don't delete the user from the database first you will get an error. That should effectively make everything work identically to the original server. If you have SSRS you might have to redeploy your reports and add permissions but that shouldn't be too much trouble.
Would there be a certificate you could get so you can apply to jobs that need SQL? Or do you need a BA in computer science or something? 
^ This 100%. I did the same thing. Hardest part was scripting the logins and passwords. Name the server the same as the old one and give it the same IP as stated above. This (http://support.microsoft.com/kb/918992) will script out the logins and passwords. Check [this out too] (http://stackoverflow.com/questions/5983676/t-sql-copy-logins-users-roles-permissions-etc) I've used the above method and everything worked better than I thought it would
You actually shouldn't have to script everything out if you move all the sys tem databases as well. Once you restart the system it will automatically upgrade all the system databases. Only trick is the all databases thst were moved have to be in the same path as before. Weve done this move on about 30 clusters ee virtualized recently. Dns change, same ip, repointed the SaN drives to the new server (physically moving all the mdf and ldfs work exactly the same) If you already have other dbs on the new server this obviously won't work.
I agree with everything except naming the server the same as the old one, that's not necessary these days with DNS being the primary name resolution protocol and in order to do that you would have to perform the additional step of removing the old server from Active Directory first, renaming the new server in AD or wait until the old sever is decommissioned completey before you even build the new server. Renaming servers after SQL is installed causes all kinds of problems. It's not necessary as long as the old name resolves to the IP of the new server.
Sorry, I forgot to mention that you wrap it all in the FLOOR function. It will become SELECT FLOOR(DATEDIFF(DAY, '2000-05-20', GETDATE())/365.25) You're looking at a date that is one day away from the current day and there have been a few leap years since that are unaccounted for. You are also using integer division. Appending the .25 provides a closer approximation for leap years and forces the implicit conversion that will result in a date that is fractionally very close to a day away from 14. The FLOOR function gives you the age in years that most people are expecting. Another easy way to see the difference is forcing the implicit conversion, but not accounting for leap years by SELECT DATEDIFF(DAY, '2000-05-20', CURRENT_TIMESTAMP)/365.0 which gets you 14.005479, so without accounting for those leap days every four years, it would appear someone has already had their birthday but shouldn't have because we know in this case that it isn't until tomorrow.
SCCM has a lot of reporting built-in already, have you had a look at them?
Hello nepobot, just wanted to say thank you again. Tried it this morning and bam! headache gone, the birds are singing and the sun is shining. I'm so happy right now I'm expecting all my colleagues to break into song and suspiciously well choreographed dance :)
Begin tran Rollback Is my default template :)
Ok that got a chuckle out of me. I agree! It would be nice to have vetted data structures. Oh and thanks for the gold!
GROUP_CONCAT() doesn't even exist in SQL Server yet. 
I really liked http://www.w3schools.com/ and http://sqlzoo.net/ for the basics! 
I think my answer on proper procedure would be "don't do it". You are going to have a lot of fun on that project, and in the futur...
I think the fact that there could be multiple results returned could be a reason. Aggregate functions need to be scalar (meaning, there's only one answer). Think of all the aggregates. They can only produce one result. 
If we allow for the result to "suck", everything is possible.... @op, at least consider postgree, mysql has some real issues that will come back and bite you in the short / mid / long term
Mode is found easily using standard SQL and as it isn't a scalar result which aggregations (group by) operations need to be - it isn't included like you want. Here's one way to do MODE in standard SQL: select AMOUNT, OCCURS from (select AMOUNT, count(*) OCCURS from qryData group by AMOUNT) where OCCURS &gt;= ALL (select count(*) from qryData group by AMOUNT) So, for example (using ORACLE CTE for the dataset): with qryData as (select 1 as AMOUNT from dual union all select 5 from dual union all select 3 from dual union all select 1 from dual union all select 7 from dual union all select 2 from dual union all select 0 from dual union all select 3 from dual ) select AMOUNT, OCCURS from (select AMOUNT, count(*) OCCURS from qryData group by AMOUNT) where OCCURS &gt;= ALL (select count(*) from qryData group by AMOUNT) Results: |AMOUNT | OCCURS | |-------|--------| | 1 | 2 | | 3 | 2 | If you want the AMOUNT values in one column, write a pivoting routine or use your native RDBMS pivot routines if they support it.
Don't be cheap. This is a bad idea.
&gt;If we allow for the result to "suck", everything is possible Unless he needs CTEs, or window functions, or CROSS/OUTER APPLYs, or custom CLRs, or proper stored procedures, or...
Your title is one of those phrases that should give any DBA pause and possibly a small shudder. 
Possibly? Small?
Everyone learns differently. I used the [Teach Yourself SQL in 21 days book](http://imgur.com/h7k6w3d). It might not be the best book out there but it really got me started with the basics. I also just jumped in and started writing SQL every chance I got. I worked in a group that did a lot of ad-hoc reporting and analysis so there was always a request for "this or that" report. I had colleagues that would take shortcuts by taking raw output and pasting it into Excel. They would use excel to combine tables, create pivots, create summaries, DeDup and varoious other things that are more fun in SQL. I decided that I would write everything in SQL and would never use excel unless under a time crunch and just had to get something out now. Even if I didn't know how to do it, I would use book and the internet to figure it out. It took me longer to write those first queries but I got faster and better. My skills grew while my colleagues stayed the same. That was all about 17 years ago and today I don't think there is a query/procedure/view I couldn't write. It might take several hundred/thousand lines and all day, but I'll get it. For better or worse I've become the SQL expert. I don't get to do it as much as I would like (I'd write queries all day long if I could). Unfortunately, competence in one area seems to mean that you're capable of doing other things and they keep promoting me.
Get yourself a SQL teaching guide, read it whilst going on SQLZoo.com and practising, then when you're getting better create your own test projects on SQLfiddle.com and practise some more. As far as SQL goes, there's no substitution for having a go.
its right along the lines of "backups? well, we got raid 5..." ain't it? But hey, it could be worse, it could be Access. I'd say we stock up on popcorn, chips, and the alcoholic "it help me cope" beverage and then link this thread to /r/webdevelopment. Lets have some fun ;D
I'd do it with a SELECT top 1 WITH TIES Amount, count(*) ... ORDER BY count(*) DESC Should be faster, since there is no selfjoin involved. Anyway, @OP, one (should) writes SQL to scale and to perform. So the evolvment of computing power is a really bad argument when it comes to SQL. 
This. I detest vendors that have these requirements; **we are paying** for your product and you're telling me your programmers can't use DateTime.Now?
TOP doesn't exist on every db platform. I think my solution should work for all the main vendors.
Oh, please do!
I don't think you will find a list of steps required as most of them require manual work and analysis of the current db. Hire a MySQL DBA to help or the likelihood of failure is 100%. 
You would need to use two views unless you queried on the view with an extra condition like WHERE dept = 'sales' .
One would assume you have indicators within the data somewhere to determine which data belongs to which department. You do not mention what environment you are viewing the data in, or which DBMS you are using. But given all things equal, the answer to your question is; Yes.
Is the data stored within the same table?
Once you hire a MySQL DBA the likelihood of failure only drops to about 80%. 
I am using Oracle Application Express This is what I have, but how would I write it to include HR and have Sales and HR to see only the defined data in their respective departments? CREATE VIEW sales_view AS SELECT firstname | | ' , ' | | lastname AS "Employee Name", dept AS "Department", position AS "Position", salary AS "Salary" FROM employee WHERE dept='SALES' ORDER BY lastname
This book is a great starting point and reference!
Well yeah. You can make the groups your users are in as part of the view. You can choose to do an ='s or an EXISTS or a join or something. What database is this? Oracle, mysql?
If you put the sales people on a group called sales, you can say where dept = (select the group of the user)
&gt; mysql has some real issues that will come back and bite you in the short / mid / long term Like fucking foreign keys not being enforced with the out-of-the-box configuration! (Still annoyed by that one. Was fun to realize two months into a project.)
Haha! I'm trying not to scare them too badly.
LOL, given the shortage of good MySQL DBA's I must agree. 
I loved this... it's no longer free though for SMSS 2012+.
Yeah I think we can all agree it's basically Hitler.
Check this link out (MS SQL specific): http://msdn.microsoft.com/en-us/library/ms131057.aspx You can create your own aggregate function in CLR on that platform. I created a MEDIAN() function once, and it shares one problem MODE() would have, which is that as each row is fed to the function, you need to keep a table of each value in memory, and it became a performance/memory hog for large computations, and SQL 2005 had limits to the amount of memory a CLR UDT could use (8KB), so it would just fail on a large enough query. You'll notice nearly all the built-in aggregates only require storing one or two values as it's being computed so are very efficient. As already mentioned, MODE() also runs into the problem of being a table-valued function since you can have more than one answer. While the performance issue is an issue, the table valued part is kinda a showstopper...
User based filtering.... Your view would have a predicate based on the USER environment variable. So you set up a new table of departments and their users then in your view join to that table and have a filter based on USER. or Fine grained access control .... http://docs.oracle.com/cd/B19306_01/network.102/b14266/apdvcntx.htm or.... use user based private synonyms pointing to different views, each filtered accordingly.
Yep, totally agree, PostgreSQL would be a much better solution than MySQL for this. It's also free but unlike MySQL, it natively supports referential integrity, much more advanced SQL support including SQL the new types, window functions, common table expressions, all new additions to ANSI join syntax, full trigger support, has T-SQL equivalent in PGsql, advanced index support indexing functions including functional, index-only scans, partial, functional and multiple-index-counting, cost based optimizer, asynchronous as well as synchronous replication, has richer administration functionality including those needed for the task and I believe is more scaleable. It is far superior to MySQL on just about any metric and much more aligned to MS SQL, the task will be easier and less of a nightmare. Going with MySQL will be "fun" for him. Mwhahahahahaha! 
Shitler more like
MySQL has backups Including incremental.
This depends on the length of time the query takes to run: * If it is very fast, it's probably okay to leave it * If it is somewhat slow, but only sparsely required, probably okay to leave it * If it is somewhat slow (or very slow) and/or regularly required: -If the data is required on the fly, do the calculation on the application server, not the database -If the data set is too large for the application, you may want to consider using a lower resource transaction log shipping configuration and build a reporting server (unlikely) Edit - I personally dislike storing aggregate data unless absolutely necessary because it has a bad habit of being neglected or forgotten when a process is updated; meanwhile people continue using it like it is accurate, only to find out weeks on that they forgot to update the stored procedure/job/etc. 
PG is definitely the way to go if they can't afford MS-SQL. That said, PG really needs to fix that CTE performance penalty.
I said decent. Dump everything to a script file is not a backup at all. Binary file copy is, but I didn't see much about page level checksums and there such. The differential backups you say MySql has, is writing out its log file, which then becomes your differential backup. That is not a decent differential backup. On a restore I sure as hell do not want to rerun every single transaction since the last full backup. I want to have page level differential backups that I can take on a regular basis, and then have a real log backup to do point in time recovery (which you can't do in MySQL either, hence &gt;real&lt; log backup). Backups in Mysql are gimped to an extend that makes me say that it does not natively have decent backup capabilities.
http://pgexercises.com/
Page level incremental backups. "Incremental backups work because each InnoDB page (usually 16kb in size) contains a log sequence number, or LSN. The LSN is the system version number for the entire database. Each pages LSN shows how recently it was changed. An incremental backup copies each page whose LSN is newer than the previous incremental or full backups LSN." tp://www.percona.com/doc/percona-xtrabackup/2.1/xtrabackup_bin/incremental_backups.html?id=percona-xtrabackup:xtrabackup:incremental Just saying because most people think they can just install MySQL and run with the default settings and use mysqldump as a backup (locks by default). Then when it sucks they blame MySQL. The MySQL guys have done themselves a great disservice by promoting that view. 
I'd say it also depends on what DBMS you are running. There are DBMS's out there that can calculate running totals on the fly without going bananas. For example MSSQL since version 2012 using sum with an over(order by). If your DBMS does not have functions like that, you are going to end up with a query internal rowcount of n! , with n being number of rows summed up into the cumulative value. And that can hurt a LOT on a big table. In that case it would be better, a lot better actually, to calculate it on the fly. You would not have to worry about row n-1 value changing, forcing you to update the cumulative total for all n+m rows. 
Yes, of course there is a binary/transaction log backup as well as point in time recovery. This is the exact reason I told the OP to hire a real MySQL DBA. 
Is this for a production application database or a reporting/DW database?
I agree completely, it was a general case as they the DBMS wasn't mentioned. Cases like this are a constant battle of accuracy verse performance; the bigger the data set becomes the more sacrifices from the former have to give way to the latter. The options as I see them when this is the case are: * Use slightly older data to maintain performance (report server/etc) * Upgrade the hardware to limit bottle-necking * Lock out/high priority the query on the database (bad practice) * Store the aggregate data (bad practice) 
Thanks. I added some more info. Few questions: Why should I do calculation on application server instead of database level? When is a data set considered too large to warrant need of a reporting server? Other than that, I will probably go with calculating on the fly because that is obviously less work and less maintenance.
No, it takes care of non deterministic functions like all other db's with binlog functionality do. Why would you need to sift through thousands and thousands of lines of SQL? 
This is the key question. If it's a transactional/Application oriented database, you want to avoid storing computed values of any kind for all of the reasons others have listed. If it's a data warehouse/reporting/BI oriented database, storing pre - aggregated data is part and parcel of that process. You just have to have a roburobust ETL that can account for historical corrections. Whatever it is, I would shore up that logic on selecting your PreviousCumulative. Even if your subquery logic is just SELECT Cumulative FROM Table GROUP BY Cumulative, Entry Date HAVING max (EntryDate) You'll at least get the most recent entrydate to compare againts.
If it's slow, this is where materialized views become useful. In SQL Server, assuming you don't have self-joins or outer joins (and some other criteria) you can create a view with clustered index including all of the columns to get a similar effect. You may want to see how much this impacts your write times though.
&gt;Why should I do calculation on application server instead of database level? Short(ish) answer: * The application server can handle multiple threads without interrupting the data source; if an aggregate function locks data then you can run into database contention which can cause performance problems for everyone, instead of the individual. * Application servers generally have the option to use drive caches more freely to account for memory heavy solutions. * The data pulled to the application server will be point-in-time and you won't run into the problems with changing values mentioned by Svtr &gt;When is a data set considered too large to warrant need of a reporting server? Entirely depends on the type of data being stored and how it's being used. If reporting is causing noticeable performance hits to the live system, it is probably time to consider an alternate configuration to handle reporting. They are generally not expensive as they don't have to be as rigorous as a normal server; seen CAD level workstations ($3-4K~) used for this purpose, the more up-to-date the information required the more you are going to spend. If reports only need to be accurate to the last day/couple hours it can be added to the maintenance jobs/etc. &gt;Other than that, I will probably go with calculating on the fly because that is obviously less work and less maintenance. Very much so, until you reach tables with billions of rows and need multiple aggregate functions to run. 
Interesting article!
[MS SQL], right? it's kinda obvious from the content, but you should always identify your platform in /r/SQL (please see sidebar)
Well, I figured it out again. ~~It seems that the LEN() function is returning the length of the records after removing the first 55 characters rather than just returning the length,~~ I just had to change it from -3 to -58. *edit* - Thanks r3pr0b8
i've had to cleanup a mess that another DBA had done when they didnt back up the encryption key. It was about 4 years ago, and to my recollection the main issue was that they lost the data sources (and possibly the subscriptions), not the reports. It was not as bad as i would have thought it was when losing the encryption key.
I use a SQL Script that scripts out all the users. For the SQL account it will script the account, but it does not hash out the password properly. so i am able to easily script in the user accounts, but the SQL account password need to be reset.
the problem with w3schools is, it's not very good at differentiating all the various dialects of sql on the w3schools MID() page, it says... &gt; Note: The equivalent function for SQL Server is SUBSTRING(): with no mention of the fact that only some platforms support MID() but despair not -- SUBSTRING() is standard sql anyhow, back to your problem... the reason it's not taking anything off the end is because the 3rd parameter of the function is **length** not position what you said is "start at position 55 and take everything *for a length equal to 3 characters less than the entire value*" see why it's not working now? 
Is your SSIS import going to be a one time thing or an ongoing process? If it is one time, then use the import/export wizard by right clicking on the database in SSMS Object Explorer &gt; Tasks &gt; Import Data... Also FK constraints have an [optional cascade delete and cascade update option](http://msdn.microsoft.com/en-us/library/ms189049.aspx#TsqlProcedure) to handle the cascading that you can apply if needed.
[Murach's](http://www.amazon.com/Murachs-Server-Developers-Training-Reference/dp/1890774693/ref=sr_1_1?ie=UTF8&amp;qid=1400707240&amp;sr=8-1&amp;keywords=murach+sql) really helped me when I was starting out.
I have no choice in the matter. I'm just in charge of the data migration. I really dislike Mysql to put it mildly. However, it must do as it's told.. and move the data...lol You know.. because we're all hold magical powers apparently. 
Make your way to stackoverflow.com whenever you get stuck. People there are really helpful. Just be sure to search first since it's very likely your question has already been asked and answered.
I wish! lol I'm a SQL data transition specialist (DBA really but that title costs too much..lol).. and learning MySQL (hate.it.) I'm on the team in charge of the 'project' at this point. And there doesn't seem to be any clear leadership. I kind of have an idea of how I want the actual technical aspect is gonna go down. I just wanted to have a step by step outline in place so that I can herd these cats and get everyone on the same page. 
Arent the subscriptions just Agent jobs, if so, a simple msdb restore will fix those. Datasources are a whole other issue
&gt; 0 == 'foobar' that one, the silently thrown error message, that you have to set session params to even get as an error message (it will still execute, cause hey, why should a transaction be rolled back when I encounter an error, that would be just inconveniant wouldnt it) equating to true... imagine that on a DELETE FROM customers WHERE customerID = 'oh my frontend just fucked up' Even thinking about it gives me chills
To start I have used: http://www.sqlcourse.com/index.html Once you have a little experience under your belt I liked http://sqlfiddle.com 
Exactly. I personally find the silent coercion of NULLs in NOT NULL columns to zeroes or empty strings to be even more frightening. Oh, you don't have a value, let's just call that 0 and move on, shall we? Why 0 and not 34.6? It's just as correct. The type coercion is a terrible idea, but at least I understand it in a lolphp sort of way. This bespeaks a fundamental lack of understanding of constraints and why they are used. I can't tell you how many times NOT NULL has caught issues in ETL when e.g. a data provider has changed their formatting slightly, but if I was on MySQL, we'd just have bad data with no warning until we generate a report.
I can't even tell how many times I had the argument that something being NULL equals information. Some field being null tells me "I have not the frist clue dude", and that, I consider that to be information. (I have to deal with data migrations of 3rd parties). &gt;lolphp sort of way First, that made ma laugh, cause I understand it as you ment it, an insult. Second, I consider PHP a scurge that will (and has) prevent me to take a job, exactly for the "data types, who need datatypes" thingy. I like datatypes. I really do..... /edit: I think the issue is, the multitude of really, REALLY bad design desicions that have been taken on MySQL. Shit like silent type conversion errors, null, not null, 0, whats the difference, and all the other little things. MySQL is just all arround BAD. I can never trust MySQL to tell me the trouth about the data I'm querying, I cant even trust it to persist the data as I tell it to. Even if they fixed all the shit they built in as I suppose a "feature", I can't trust it, can I? I am not in the habbit of checking the servicepack of the DB to accertain if boolean arithmatic logic will work. And that actually is an issue on mysql, christ even typing it out it makes me cringe. A DBMS that I can not trust to handle data. Nuff said. (I'm waiting for the village idiot to throw buzzwords like "webscale" and "mongo db", then I'll really get upset) 
Thanks guys for the awesome help, I literally am trying everything you guys are recommending. I can say out of college it's INSANE working in security now I know why they ask for so much experience. Luckily my boss has trust in me and wanted me for this opportunity. I'd like to say again thanks so much!
I'd need half an hour to wrap my head arround this one, and I've delt with a LOT of "never should have been done". Just these quotes : &gt; was initially hitting the 1024 column limit &gt; It's kind of weird how it is output visually &gt; Access sees it(...) Makes me say, dont. Just dont. The first one, 1024 column limit, just dont. I can say honestly, I have done a lot to my SQL servers. I have made them throw the "to complex to generate execution plan, fuck you too" error, I have done a lot. I have however never hit the 2^10 column limit. My head hurts too, and I do SQL and MSSQL Server all god damn day 
There's not going to be any formal white papers, because there won't be any real difference to talk about. It doesn't matter where the cores are. They are still the same distance from main memory, which is the bottleneck. If you're dealing with high-end numa machines, there are some issues to consider, but otherwise it's irrelevant. 
Derived data should never be stored. It's your job to explain why it's a terrible idea to store a birthday and provide them easy access to the derived data. You can use a view, or provide a function and use examples. Every example I've seen here to calculate an age is wrong for some edge cases. If you ever see code with 365, or 365.25, it's wrong. If you ever see DATEDIFF, it's (most likely) wrong. Calculating age is really easy to do, but really hard to do correctly. If you're trying to do something fancy to handle leap years, you're doing it wrong. The correct way to calculate age: Subtract the current date's year from the birthdate's year (yeardiff). If the current month is greater than the the birth month, you're done, if it's less than, subtract 1 from yeardiff and you're done. If the months are equal, subtract the current day from the day of birth, if it is negative, you're done, otherwise subtract 1 and you're done. This algorithm is agnostic to leap years, it will get the right answer without having to have special cases. In pure MS SQL it's: SELECT year( GETDATE()) - year(BIRTHDAY)+ CASE WHEN month( GETDATE()) &gt; month(BIRTHDAY) THEN 0 WHEN month( GETDATE()) &lt; month(BIRTHDAY) THEN -1 else case when day( GETDATE()) &gt;= day(BIRTHDAY) THEN 0 else -1 end END; 
Actually, Microsoft CRM gets the birthdate from the database then calculates the age. I'm not sure how the CRM developers did it but my task was to reflect an updated age in the database. Yes I have explained it to my immediate superior why calculated values should not be stored in the database but the clients insisted on having an updated age in the database. To solve the problem, I will make a scheduled job every time the CPU time is idle to update the age column. I haven't done it yet because I have no Server Agent permission in the database. Thanks for showing me how to properly compute the age.
I'm afraid I'll have to defer to an Amazon search. I have a couple good SQL books but they are for much older versions of SQL so aren't the best if you are starting from scratch.
Thank you
thanks for the info friend
Remember, joins are not expensive. Normalisation, as extrajoss, says, helps you avoid duplication and generally makes it easier to maintain accuracy. You need to use some common sense too, though. Normalising names, for example, is probably taking it a bit too far.
Lots of different ways to set up all depends on your requirements for - availability, performance etc. You may want different frequencies between jobs. Backup daily, reindex weekly. You could set up a log transfer to another server and back that up. I haven't used ms sql in over 10 years so am sure it's come alone way since and has a plethora of options.
&gt; there are other ways that data gets into a database, besides through "the front-end application" What are those other ways? Why should data get into a database without any front-end application or some piece of code which runs business logic? &gt; and what about taking an app out of production, into development, changing it, and promoting it back via change review, testing approval, and various bureaucratic hoops, just to add a designation? when it's so much easier just to insert a new row in the designation table? Why would anyone take their app out of production to add a row in designation table even if their data is denormalized? If you look above, designations table still exists in denormalized case. The only change is that instead of storing 'Designation ForeignKey' in Employee table, 'Designation Title' is stored. The 'Designations' collection is always loaded from Designation table when user is entering data.
At least going from Access there is a built in method for converting, but he said he's going to MySQL, not sure if theres going to be any tools to help.
&gt; If you look above, designations table still exists in denormalized case. The only change is that instead of storing 'Designation ForeignKey' in Employee table, 'Designation Title' is stored. ah, okay, that's much better than what i thought the importance of FKs cannot be understated with FKs in place, it's fine for the front-end app to do some screening, because the database will still ensure the integrity of the data being passed in 
Not exactly what you're looking for, but [this webcast](http://www.idera.com/resourcecentral/webcasts/sqlserver/performance-implications-of-parallelism/thankyou?rt=webcast&amp;q=sql%20server&amp;t=sql%20server%20webcasts) covers how to tweak parallelism settings to better performance. Given the other replies in this thread, it could be useful to you.
NEVER EVER EVER rely on your application layer to ensure your DB's data integrity. It's wrong on so many levels, and if you are relying on it you're doing things wrong. Databases can be changed through scripts, schema evolution, administration, APIs added later. What ever route, data integrity is paramount, your business can depend on it. It's why there are FKs, referential integrity action, null controls, type sizing (strings, numbers, etc.), field, row and table level constraints, trigger based validation etc. in DBs. In your case, what are you going to do when one day your boss says something like "baba1478" there are 40,000 personnel clocking in records that need changing - there was a bug in the system that meant the clocking in timestamp was using the wrong timezone. You'd write a script to correct it, because your application layer wouldn't have such functionality - it's unforeseen. That script runs outside of your application layer and any business layer integrity you've written to circumvent poor design. Never assume such things won't happen, because they do, and often. DBs are normalised for good reasons. It keeps data size, management and integrity issues to a minimum. Queries with joins are easy to write and DBs are designed for them. Description fields such as your example being made convenient are foolhardy, it'll end up with you firefighting and having to do continual maintenance. I've worked on poorly designed legacy systems, with EXACTLY your sort of design decisions, absolute never ending nightmare.... seen it, had it, done it, bought the T-Shirt and hated it. 
yeah thats what I was afraid of. I guess I just wonder at which point enough is enough, so was trying to find to see if there was any benefit falloff from racking up sockets, esp with the new pricing schema. ok thanks
ill take a look at that, thanks!
thank you! I will take a look
I don't believe so. They are all defined as "INT" and I've converted them to a (NUMERIC,8,2)*100 in a derived column. The error appears to be simply the fact that my source contains nulls and the table doesn't allow them. I don't see how though b/c there are NO blank cells in my data; but when I do allow nulls (as a test to figure out the problem) there are nulls in my data fields in some places where there are zeros in the source data. *headache ensues*
Save the excel file to a CSV or tab delimited TXT. Open with notepad and check for nulls or blank rows at the bottom. Excel files are notoriously flaky for importing data. edit: One specific scenario I can recall. An excel file had a "phone number" field. The first row had 1234567890. The second row had 456-123-7890. Based on the first row the Excel file decided the column had integer data. Because the second row had "-"s it nulled out the value since it wasn't an integer.
I'll totally try that out! Thank you!
That's a different question. I don't think it makes much sense to go bigger than dual-socket in most cases and never more than quad-socket. You need to keep the ratio of cpu-disk-network consistent if you want to scale linearly. The easiest way to do that is keep adding similar servers as needed. You can get big boxes with lots of scalability in all those areas, but they get increasingly exotic and expensive. I don't think that's the case with virtualization, but if you're putting SQL loads directly on the hardware, don't over-think it. The new licensing makes socket (and frankly server) count irrelevant.
That stinks, I use it all the time!
Since MODE() would be generally used for reporting (OLAP) purposes, scale is not as critical. And if the task needs to be completed regardless, then wouldn't it generally be implemented better at the database core level versus at the user defined level? Its not like the MODE can be calculated differently for different situations, it is a standardized statistical metric. Also, there was a time where we considered certain tasks / functions awful for performance reasons yet today are trivial. An example would be hashing a string into an MD5 real time for a record set of 30k rows. This was a consideration I had to make recently and after comparing the run time with and without the MD5, the difference was a mere 3 seconds. (this was with PHP)
Precisely!
You offer a decent solution, its just disappointing that nothing is built in considering that the MODE of a set of values is a standardized metric, it is only determined in one way.
[edit] This assumes that I read the example correctly, and we only have one table with one column. I'd use the same approach, I think they were fishing for indexes. If Employee.First_Name is one big heap, that's inefficient as hell, so look for an index first and if one doesn't exist, create one. Then, SELECT DISTINCT First_Name INTO zzfn FROM Employee DROP TABLE Employee CREATE TABLE Employee (First_Name VARCHAR(32)) -- Or per spec ALTER TABLE Employee ADD CONSTRAINT [PK_Employee_First_Name] PRIMARY KEY CLUSTERED (First_Name) INSERT Employee SELECT First_Name FROM zzfn Transaction optional depending on the scenario. I prefer to run these one at a time instead of as a full batch, verifying my temp working set (zzfn in this case) before doing anything else.
actually, you can take that exact course right now on coursera.org, which is what im doing. jennifer is a great instructor and im learning very efficiently. i made a reddit post about this the other day over in r/learnprogramming. heres that post: http://www.reddit.com/r/learnprogramming/comments/25seee/best_free_online_resource_to_learn_sql/ and heres the link to the current stanford class on coursera (its self-study with no start or end date): https://www.coursera.org/course/db SQL is easy and the most important thing is to understand the logic and process of how databases work than to know the actual SQL language/syntax... the latter is much easier than the former. good luck!
There really is no BETTER way. If there was a unique identifier in the table as well (EmployeeID int identity(1,1), then you could do something like this... DELETE emp FROM dbo.Employee emp JOIN ( SELECT EmployeeID, ROW_NUMBER() OVER (PARTITION BY FirstName ORDER BY EmployeeID DESC) as RowNumber FROM dbo.Employee ) rn ON rn.EmployeeID = emp.EmployeeID AND rn.RowNumber &gt; 1 ...but there isn't - so your way was best.
&gt; An example would be hashing a string into an MD5 real time for a record set of 30k row You know that in a datawarehouse you are looking at 30 million rows rather than 30 thousand rows right? If you are talking OLAP, we are talking BI solutions anyway, and there you do not query the dwh all the time, you build cubes, and you have a lot of analytical functions in those systems, since they are built to do stuff like that. You also do know, that the usability of the application querying the database will start suffering quite a bit once you go above lets say 500ms (depending on how long the UI needs)? 30k rows is NOTHING. And you always have to factor in (data) growth over time. O(n) is just as bad today, as it was 20 years ago. Also, 3 seconds for 30k values is a lot. Thats why skalar function calls in a query are one of the best ways to shoot oneself in the foot.
Back up the table for reference as always DELETE FROM EMPLOYEES FROM EMPLOYEES E INNER JOIN ( SELECT ID, FIRST_NAME, LAST_NAME, OTHER_FIELDS, ADD_DATE, ROW_NUMBER OVER(PARTITION BY FIRST_NAME, LAST_NAME, OTHER_FIELDS ORDER BY ADD_DATE ASC) as INSTANCE FROM EMPLOYEE ) DUPS on E.ID = DUPS.ID AND DUPS.INSTANCE &lt;&gt; 1 If you run just the following code you will get a list of what the inline query generates. SELECT ID, FIRST_NAME, LAST_NAME, OTHER_FIELDS, ADD_DATE, ROW_NUMBER OVER(PARTITION BY FIRST_NAME, LAST_NAME, OTHER_FIELDS ORDER BY ADD_DATE ASC) as INSTANCE FROM EMPLOYEE This will give you your list of employees. The Row_Number method gives you how many occurrences based on the columns you specify in the the Partition by. ORDER BY ADD_DATE would give you the oldest instance of the account first and wouldn't be subject to deletion. Generally when systems start duping rows, usually you want to get rid of the duplicates and keep the oldest values. Here is the killer question for you... Do you have other tables that are referencing the employees by an ID value? If so, these tables need to be mapped to the old values. 
It would be a real stretch to try to make a case for that. You have a 6-core option and an 8-core option. Everything else is rounding error. There might be some utility in a single 6-core cpu in a 2-socket server, giving you an upgrade path. The cost/performance is the same in any case. In this scenario you're just assessing if you want padding for future growth. Personally, I think if you bottleneck a sql server on cpu you are doing something wrong. Put any extra budget you have into RAM or storage first. 
Thanks for the answers. I was hoping that there was not some function or process I was ignorant of which would have been the home-run answer. I'm feeling better about my reply now.
I tried to import a txt file instead and it is giving me a bunch of data conversion errors... sigh. This thing is such a pain haha. 
[MySQL] ? [Oracle] ? [MS SQL] ? [PostgreSQL] ? etc ? 
its for a DR site. I want it to be able to function relatively well but I do not need it to be on par with production. and you're right, im not worrying about bottleneck on cpu -- i was legitimately wondering if there was any performance issues on that. sort of the more you know.
No specific format was specified for the example. I believe they use Oracle though.
This is *not* a best practice, but it works really well if there few duplicates (&lt; 100) and an index on the duplicated column. It's a nice example to understand why it works better in this scenario. SELECT First_Name INTO #Dupes FROM Employee GROUP BY First_Name HAVING COUNT(*) &gt; 1; WHILE @@ROWCOUNT &gt; 0 DELETE TOP (1) FROM Employee WHERE First_Name IN ( SELECT First_Name FROM Employee WHERE First_Name IN (SELECT * FROM #Dupes) GROUP BY First_Name HAVING COUNT(*) &gt; 1);
As far as Oracle is concerned, the answer is simple: it depends (on quiter a few things in fact, both technical and business-wise). First of all who uses a one column table anyway?! It's not even unique for chrissake! Secondly, very quick question: does the table need to be online or not when this brilliant re-organization takes place. If it needs to be online then you issue a DELETE FROM employee where ROWID NOT IN (SELECT min(rowid) FROM employee GROUP BY first_name). Not very quick at all but effective. If on the other hand it doesn't need to be online then you CREATE a secondary table with the same structure as the first and you INSERT APPEND SELECT distinct first_name FROM employee (or you can use SELECT first_name FROM employee GROUP BY first_name, it is basically the same thing). A real-life example might contain parallelism as well. Then you drop the first table and rename your second table back to its original name and there you go, your one-column table is now all nice and cleaned-up and still with no purpose to it.
lol bro you way overcomplicated this. The table only has FirstName in it. No ID, LastName, OtherFields... It's an over-simplified interview question - not a real life situation he is trying to solve.
Exactly why update it every time. Just store what you need and calculate when you query the data.
I figured it out. The original formatting that I copied into my formatted file for the DB; was pre set to extend on FOREVER down the column. Thus the "nulls" haha. Thanks so much for you help!!
Could you not simply use the [ODBC Foreign Data Wrapper](http://wiki.postgresql.org/wiki/Foreign_data_wrappers) and create some [Materialized Views](http://wiki.postgresql.org/wiki/Materialized_Views).
In Oracle, the quickest way would be... delete from EMPLOYEE where ROWID in (select RID from (select ROWID RID, row_number() over (partition by FIRST_NAME order by ROWID) RNUM from EMPLOYEE) where RNUM &lt;&gt; 1) or create table EMPLOYEE_TEMP as ( select FIRST_NAME from (select FIRST_NAME, row_number() over (partition by FIRST_NAME order by ROWID) RNUM from EMPLOYEE) where RNUM = 1 ) / truncate table EMPLOYEE / insert /*+APPEND */ into EMPLOYEE (FIRST_NAME) select FIRST_NAME from EMPLOYEE_TEMP / commit / drop table EMPLOYEE_TEMP /
[SSIS to staging table on POSTgreSQL database.](http://slalomdotcom.wordpress.com/2011/03/14/connecting-to-a-postgresql-instance-using-sql-server-integration-services/) Initiate an upsert (update/insert statement). Update records that don't match based on primary key. Insert new records where the primary key doesn't exist. The logic of your statement should follow the constraints of the table. Do not drop constraints on a table. (I have seen this done more than once, its a nightmare to troubleshoot when it breaks).
Personally, I would test their method, I'd bet a few bucks it's wrong.
For the purpose of the question, I assume no other tables. The parameters stated that there is only one table with one column to focus on. Real world, yes, there are more factors, but this example is hypothetical.
Thanks so much for all the information! 
From pov of the interviewer, I would put more stock in being asked questions detailing the scenario than getting the "dump it to temp table, select distinct * back into the table" answer. Its not rocket science after all. Questions like "is there a PK", "what DBMS", "when was the last backup" 
update MyTable set MyColumn = 'whatever/00001' where MyColumn LIKE 'whatever/&lt;underscore&gt;000&lt;underscore&gt;' note, the underscore _ is a wildcard for ONE SINGLE character and the percent % is for ANY AMOUNT of characters. I'm typing the word &lt;underscore&gt; because the redit editor seems to interpret it as italics or something if i embed it. Example of data that would be found by the where clause includes whatever/10001, whatever/20001, whatever/30007 if the "whatever" in your example is also variable, then put an underscore for each character position there or use a %. i'm not sure what your source data looks like. 
heh. after reading this, i may have made it more confusing to you, with all my assumptions. not sure if you want to KEEP the 'whatever' part and just replace the numbers in the 13th position..? 
Yeah, I noticed that the other names didn't appear when they should have, since they are between F and S alphabetically. I've found a handful of errors so far :(
Assuming you want to discard the initial '/' from your data, something like: update your_table set whatever_field = right(whatever_field,len(whatever_field)-1) where whatever_field like '/%'
I was just able to get the query to run. I was missing the comments column in the second query. The query runs successfully now, but the results aren't what I hoped for. Basically the UNION returned 20 rows from the first select and 20 rows from the second select for a total of 40 rows. So I guess using a UNION isn't the correct solution to the problem. What I'm trying to do is end up with 20 rows that includes all of the columns from the first select AND the interest, storage, and average columns from the second select. Thanks again for any help! 
Well, the UNION is working properly but I don't think you want a UNION in this case. You want to join the two. Looks like you want to join @CostsFormatted and CI_Item ON itemcode. When you select, just select the colums from CostFormatted you need and the columns from CI_Item you need. 
update myTable set MyColumn= '0' Where MyColumn like 'whatever/1%' Is what I thought about using - something similar to that. The "whatever" part of my query is actually a constant throughout the field and it's the end part of the value that counts. I also tried your version of with set MyColumn= '0' Where MyColumn like 'whatever/_1_' To no effect. :( Also yes the whatever needs to stay with only the '/1' needing to be changed.
Okay, I figured it out. I got rid of the union, used my orginal join, and did a select distinct instead of a normal select. Thanks again for everyone's help! 
What about using replace. REPLACE([COLUMN],'/10','/00'). Depending on how large the sequence numbers get this should work up to 10999. You could always use '/1' to go to 9999
if you use the LIKE command, you must use % or _ in the value you are comparing to. set MyColumn = '0' where MyColumn like 'Whatever/1%' but if you want to KEEP the part of the string that comes before the slash, then you have to do some string extractions.. like if Mycolumn = 'Smith/10001' and you want to change it to 'Smith/0001' then you have to use Substring or Left to extract the keeper part of the string (i'm not sure what sql language you are using so LEFT may not be available) There are many permutations of that kind of thing so I don't want to right that code and confuse this answer by going in the wrong direction. 
There is no official order. Order of difficulty is: * Easy: Administering SQL Server * Medium: Data Warehouse * Hard: Querying Just for a frame of reference, I have spent 95% of my work days for the past 5 years querying SQL Server, and the Querying one was still the hardest.
I got my MCSA in SQL Server 2012 last March by attending a boot camp. It was two weeks of pretty intense study and if I hadn't been working with SQL Server for a year prior I would have been completely lost. Microsoft actually has a training class for each exam. Below is an outline for the courses. We did them in the order that they're listed and IMO 463 was the hardest, but probably because I had the least experience with it. http://www.cedsolutions.com/906/certification/microsoft-mcsa-sql-server-2012
I would do / I did the t-sql one first. That to me, and I guess to you as well, was the easiest one. Its a relative term thou, there where a couple of things that can screw you. The administration one can be tough if you haven't done much in that regard. Be sure to know very well how backups work, and what the best practices are. You also should have a very good grasp on always on and replication. High availability was more than just one question in my exams. I did that together with the T-SQL one, but the upgrade exams are 2 parters where you have half the questions of 2 of the regular exams (and you have to pass both). So I had T-SQL + Admin in one exam. As for the BI one, well, I saved it for last, since that I have the least experiance with. And to me at least, that one is where I really have to ,and am, spending time on.
to me it is almost reversed, I'd say it absolutly depends on personal real life experiance ;)
That's the thing though - I have no experience with administering SQL Server, but that exam was CAKE. ALL I do is write complex queries, but I struggled with the Querying one. Same with the other five people at my company who are taking these exams. 
What would be the correct format then? 
All 4 entries are between F and S
Oh wow duh. So it should be returning all 4 entries then. Got it. 
Case of your millage may vary ;) What I can say with certainty is that I'm growing to hate the BI one. I really do...
I'm a MSSQL guy, not sure if this is correct syntax for MySQL. SELECT RegionPlayer.Regionid, LastSeen.Key, LastSeen.Name, LastSeen.Seen, LastSeen.Vacation FROM RegionPlayer JOIN Players ON Players.Key = RegionPlayer.Playerid JOIN ( RegionPlayer.Regionid Players.key, Players.Name, MAX(Player.Seen) OVER (PARTITION BY RegionID) as Seen, Players.Vacation, FROM RegionPlayer JOIN Players ON Players.Key = RegionPlayer.Playerid where ( RegionPlayer.Status = 1 ) ) LastSeen on RegionPlayer.Regionid = LastSeen.Regionid where ( RegionPlayer.Status = 1 ) GROUP BY RegionPlayer.Regionid DESC HAVING SUM(CASE WHEN Players.Vacation != 1 AND Players.Seen &lt;= (NOW() - INTERVAL 8 DAY THEN 1 WHEN Players.Vacation != 0 ) AND Players.Seen &lt;= (NOW() - INTERVAL 58 DAY THEN 1 ELSE 0 END CASE) = 0 
Get rid of the app.
Will do, it's terrible.
SQL Quick Tutorial in the Play Store
I haven't taken the other exams, but the querying exam was... easy? I hated the XML queries, but everything else seemed pretty easy to me.
http://ask.sqlservercentral.com/questions/7080/export-results-of-a-query-into-a-csvtxt-file.html Are either of these satisfactory?
What dbms? Also, what data type is x?
dbms: xampp, type of x: float
Are there any apps like this that are worth getting? 
Change the field to DECIMAL type rather than FLOAT and it should work. There are problems with comparison of floats for equality. This is due to internal implementation of floating point arithmetics which are stored in the form of 32 bit numbers constructed as a mantissa and exponent. This means internally it might be something like 3.3999999999999986, whereas your check will be decimal and exactly 3.4, they aren't the same. Decimals are exact, but they are slightly slower.
Now when I do that it says "data truncated for column 'x'" and leaves the field with a truncated value. So if I put in 'x'=3.4 I get a value of 'x'=3. This is really pissing me off. I changed 'y' to a decimal field an it truncated the value as well.
Two reasons: 1. `BETWEEN` doesn't look at wildcards. Only `LIKE` does that. That means `'S%'` is literal. 2. `'S%'` is before `'Sameer'` since `'%'` is ASCII character 37 and `'a'` is 97 (and 65 for case insensitive comparisons).
Thanks! So then the would the most efficient way to get these values be (assuming we don't know the exact names for which we are searching): SELECT * FROM emp WHERE LEFT(name1,1) BETWEEN 'F' AND 'S' ? Or is there a better way? I have heard adding calculations to the left of the conditional can mess with the query optimizer but I can't think of a better way...
I...I just can't see how a boot camp will help because its so much material so quickly. It sounds like a cram - you may pass the exam. But, what have you really learned? Maybe this is another mileage may vary per person situation...
**Don't compare floating-point numbers just for equality.** (The Elements of Programming Style, 2nd ed, Brian Kernighan and P. J. Plauger, McGraw-Hill Book Company, 1978, p118) **Go ahead and compare floating-point numbers for equality, as long as you know what you're doing.** AQuietMan First, think about integers. When you assign a value like 1.5 to a variable of type integer, that variable won't contain 1.5. That's because the value 1.5 isn't in the set of values represented by an integer data type. Instead, the integer value closest to 1.5 will be assigned to that variable. Likewise, when you try to assign the value 3.4 to a variable of type float, that variable will end up containing the float value closest to 3.4. On my computer, that's 3.400000095367431640625. And that doesn't match "where x = 3.4", because MySQL interprets that bare 3.4 as being of type numeric(2,1). You might think that there should be a type promotion, so that when you try to compare a numeric to a float, the numeric should be promoted to a float. But apparently [MySQL can't cast to a float](http://dev.mysql.com/doc/refman/5.7/en/cast-functions.html#function_cast). PostgreSQL can, and an expression like "where f = cast(3.4 as real)" matches the rows in the table. 
What did that cost you, if you don't mind the question.
So how should I modify my select statement to reflect this? I'm confused.
I don't know about "better" but you can do this too ([SQL Server](http://technet.microsoft.com/en-us/library/aa933232.aspx) at least): select * from emp where Name1 like '[F-S]%'
I think this cracks it. It works in my MSACCESS db I created based on your schema. I bypassed the date calculation by just putting day counts in, so you would need to adjust accordingly. Also change Name to PName to keep MSAccess happy. I read a contradiction in the spec, you firstly say you only want the latest player, then you wanted the latest players on holiday and not on holiday. I am only querying the latest player of the region where no players have recently played within the parameters set. select pname, seen, region --get the player information from players p, regionplayer r where p.playerid = r.playerid and exists ( select 1 --get the player ID of the most recently seen player from players p2, regionplayer r2 where p2.playerid= r2.playerid and p2.playerid = p.playerid and p2.seen = (select min(p3.seen) from players p3, regionplayer r3 where p3.playerid = r3.playerid and r3.region = r2.region and r3.region in --get the region where the count of players equals the count of unseen players for each region ( SELECT unseen.region from ( SELECT 'unseen players' AS Expr1, r.region, Count(*) AS playercount FROM players AS p, regionplayer AS r WHERE (p.vacation = False AND p.seen&gt; 8 AND p.playerid=r.playerid) OR (p.vacation &lt;&gt; False AND p.seen&gt;58 AND p.playerid=r.playerid) GROUP BY r.region ) unseen , ( select region , count(*) as playercount from regionplayer group by region ) players where unseen.region = players.region and unseen.playercount = players.playercount ) ) ) 
MySQL doesn't support casting to float. 
If they are different entities, put the data in different tables. You can do the generic table approach but you miss out on a lot of advantages of the relational model and queries become a lot more complicated. To take a simplified version of what you're talking about you could have three tables : - Contractors - Customers - Employees (Let's assume that your attributes recorded for each of these three are quite different but with obvious overlaps - name, date of birth, title, etc ). Or, as these are all people you could have one table (Persons) and then have the questions specific to those entities in an entity table, 2 tables instead of 3 sounds simpler, right? OK, but let's say you want a list of all employees in the Sales division who earn more than 50000 a year but who joined after 1st January 2014.. Before, you did: SELECT E1.name FROM employee AS E1 WHERE E1.annual_salary &gt; 50000 AND E1.join_date &gt; '2014-01-01' AND E1.division = 'Sales'; Now you're doing something like: SELECT name FROM person AS P1 INNER JOIN person_attribute AS A1 ON P1.person_id = A1.person_id AND A1.attribute_type = 'annual salary' INNER JOIN person_attribute AS A2 ON P1.person_id = A2.person_Id AND A2.attribute_type = 'join date' INNER JOIN person_attribute AS A3 ON P1.person_id = A3.person_id AND A3.attribute='Division'; WHERE P1.person_type = 'employee' AND A1.numeric_value &gt; 50000 AND A2.date_value &gt;'2014-01-01' AND A3.string_value = 'Division' That's only a simplstic example but you get the idea. You also lose the ability to use some of simpler check constraints. 
I don't know what the policy of this subreddit it, but I will try. I remember when I was at uni, on got stuck doing group assignments Out of curiosity what uni? I am guessing Melbourne based.
Yes, it's a Melbourne based Uni, I would prefer to not name names :)
Oh well then I am sure you know the pain ;) Everyone in the tutorial decided to group up with other students outside our tutorial....I dont know anyone outside the tute, so I had to resort to course forums to get in one. The leader of the group left because she was a postgrad, and im undergrad, cant group together....and the other two members just never ever replied to a single email or forum post or txt msg, and it got past the deadline for getting into a diff group. Course convenor more or less told me to suck it up and do it solo...great.
lol, I would not have got though uni if it was not for postgrad students, I always tried for evening classes, local postgrad students, who want to learn. To many day students don't care.
Some more details just to confirm. This is the last part of the case study, data taken from 'hand written' documents by the faux company to be put to database. https://fbcdn-sphotos-a-a.akamaihd.net/hphotos-ak-prn2/t31.0-8/1795296_10204044097721410_5704325791598689079_o.jpg Much appreciate this Duncan, I ought to buy you a slab :P
This is sort of what I was explaining I had done previously. Some of the problems with this is that you don't get to type the data, you have to set any constraints in the application model, and you might as well forget about indexing a specific object &gt; property. Basically, at the bottom level of this method, you have one table filled with attributes and values. So where you would have columns of data in a normal table, you have rows. My previous work with DBs/Tables designed like this was not very fun, and I don't think it was very fast either.
Yes, this style of querying / table design is what I was trying to explain at the bottom of my post. Obviously I failed to do that, but your example was perfect. Your example also gave me a rather ugly idea that I hadn't considered previously, where you could create a view to a table like this that is an object level view. To use your example: SELECT name, A1.value AS 'annual salary, A2.value AS 'join date', A3.value AS 'Division' FROM person AS P1 INNER JOIN person_attribute AS A1 ON P1.person_id = A1.person_id AND A1.attribute_type = 'annual salary' INNER JOIN person_attribute AS A2 ON P1.person_id = A2.person_Id AND A2.attribute_type = 'join date' INNER JOIN person_attribute AS A3 ON P1.person_id = A3.person_id AND A3.attribute='Division'; WHERE P1.person_type = 'employee' AND A1.numeric_value &gt; 50000 AND A2.date_value &gt;'2014-01-01' AND A3.string_value = 'Division' You can see my changes just in Select statement in the first two lines. You could compile that into a view and then just treat it like a table. You could even index it. Besides being really ugly in the back end, can anyone think of any reasons not do to this? Is there a better way?
From an Oracle point of view I'm not sure you can give it a shortcut name to make self reference easier. If you want to order by or group by it you're going to have to copy the whole if statement as if it were any other select statement. 
Nevermind. Wrong application my bad. 
You need to give your column name an alias. You can do this for any column, even ones where you don't perform any kind of function at all (like if the column name is too vague or too long). However, when you are performing most any type of conditional function (like if or case), you will definitely need to use an alias. For specific examples of someone using aliases with if statements in a SELECT clause, see this: https://stackoverflow.com/questions/5951157/mysql-if-in-select-statement
MYSQL 5.5.32 create table TEST ( VALFLOAT float, VALDECIMAL decimal(10, 2) ) ; insert into TEST (VALFLOAT, VALDECIMAL) values (3.4, 3.4) ; select * from TEST where VALFLOAT = 3.4 ; Record Count: 0; Execution Time: 35ms select * from TEST where VALDECIMAL = 3.4 ; | VALFLOAT | VALDECIMAL | |----------------|------------| | 3.400000095367 | 3.4 | Record Count: 1; Execution Time: 1ms Exactly like I said. USE DECIMAL!!!!!!!!!
&gt;Is there a better way? Yes, don't do the generic attribute table. Stick with multiple tables, one per entity. So if you have to create Object A to Z then have 26 tables . In my example above have three tables for each entity (Contractor, Employee, Customer) works much better (imho). It's certainly easier establishing rules (i.e. every employee must have a division assigned, must have a salary level, etc). It's only if you want to make it truly dynamic (i.e. users can create new entities via the application) that you should go with generic attribute tables - because you don't have the luxury of making schema changes to accommodate every new entity. edit: Obviously if your different objects/entities have the same attributes then I would keep them in the same table in most cases. 
I think you're looking for a case statement: select pr.Key, pr.Regionid, p.key, p.Name, case when p.Vacation = '0' then ((((UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(p.Seen))) - 691200)/(24*60*60))) when p.Vacation = '1' then ((((UNIX_TIMESTAMP(now()) - UNIX_TIMESTAMP(p.Seen))) - 5011200)/(24*60*60))) else 'ERROR' end as 'alias goes here', p.Vacation FROM Players as p Join RegionPlayer as pr ON pr.Playerid = p.key WHERE ((p.Seen &gt;= (NOW() - INTERVAL 2 YEAR )) and (pr.Type = 'o') ) and (pr.Regionid = 819) ORDER BY p.`Seen` DESC edit: this is MSSQL syntax, but my experience is that MSSql is very similar to MySql
Entity Relationshiop Diagrams are stupid simple, they take like 5 minutes to make. Here is a perfect image of what I used the first time I made an ERD. Hope this helps, http://wc1.smartdraw.com/resources/tutorials/images/erdexample.gif
I would say you'd be correct if I had zero prior knowledge. I didn't have an problems with the querying or administrator exams and actually learned quite a bit in the classes. I struggled with the BI material because I hadn't used SSIS previously. I'd only suggest a class like this if you'd have some self study prior to attending and were looking for something to fill in the "you don't know what you don't know" gaps.
Im on my phone so msg if u need help
I need someone to look over this and tell me if there are any issues with it please. Edit: The last link had some mistakes, I think this is closer to where it should be. https://scontent-b-sjc.xx.fbcdn.net/hphotos-prn2/t1.0-9/10336717_10204051693031288_3687005116779725510_n.jpg 
Thanks Wally, Ive got the ERD drawn up but I need to know if its correct according to the assignment requirements. The tutor wanted to see everyones before they began coding....but mine got left too late for that :( Edit: The last link had some mistakes, I think this is closer to where it should be. https://scontent-b-sjc.xx.fbcdn.net/hphotos-prn2/t1.0-9/10336717_10204051693031288_3687005116779725510_n.jpg The identity between booking and payments, or booking and activity performance, are they correct? It looks strange.
Customer - Payment crow is the wrong way around, absolutely. Any others? Edit: ok....im finding more. Will post update shortly Edit2: updated https://fbcdn-sphotos-a-a.akamaihd.net/hphotos-ak-prn1/t1.0-9/10300219_10204052430569726_2969533151154732644_n.jpg
The only thing jumping out at me is that multiple staff can be assigned to work on a scheduled tour. Your schedule is currently only permitting one attending staff member. Since there is only one tour leader, but many tour assistant staff, you can leave the staffid in the schedule, but change it to tour_leader_staff_id (or something like that) - then create another table to track the assigned assistant staff to that tour, also joined to the staff table.
I'm not following you, the current ERD has that one or more staff can be assigned to a tour by way of schedule. The staff ID identifies what staff member is attenting, and staffStatus identifies if they are a leader or an assistant. Can I not just enforce one leader through a constraint check, whilst allowing many assistants?
Yes, I see your point, now that I realize how you've got an adventure-&gt;tour-&gt;schedule. I missed the big picture there, and now realize schedule entity is your staff schedule. Nothing else jumps out at me Chuckles. I hope you do well.
Although... I wonder if the booking to tour crow is correct. It would seem that a booking requires a tour, meaning the symbol on the tour side shouldn't permit zero. Further, I wonder if, since the booking has a tour key, that the tour side crow shouldn't permit many - since there aren't duplicate tour keys? In fact, I think if you just reversed the crow on those entities it would make more sense.
Good spot! Changing it now :)
Delete statement generation is incorrect. delete from T_TEST where PARENT_ID = ?,NAME = ?; should be delete from T_TEST where PARENT_ID = ? and NAME = ?;
Ive all but managed to cobble this thing together, however I am having some issues with my implementation of linking booking to customer, for some wierd reason I am getting duplications when attempting to issue the following query; select tour.advkey, booking.startdate, booking.customerid, customer.name from tour inner join booking on tour.tourkey = booking.tourkey inner join payments on booking.bookingKey= payments.bookingKey inner join customer on customer.customerid = payments.customerid; with the following output; ADVKEY STARTDATE CUSTOMERID NAME GR1 2013/09/25 1145 Rose Tyler YV2 2013/09/17 1444 Mickey Smith YV2 2013/09/17 1444 Mickey Smith YV2 2013/09/17 1274 Martha Jones GR1 2013/09/18 1333 Amy Pond GR1 2013/09/18 1334 Jack Haskness MRG1 2013/09/18 1299 Harold Saxon MRG1 2013/09/18 521 Donna Noble MRG1 2013/09/18 521 Donna Noble MRG1 2013/09/18 521 Donna Noble Here is my table create statements; create table tour( tourKey varChar2(8) NOT NULL, advKey varChar2(8), startDate varChar2(10), price decimal(5,2), CONSTRAINT PK_TOUR PRIMARY KEY (tourKey, startDate), CONSTRAINT FK_TOUR_ADV FOREIGN KEY (advKey) references adventure ); create table booking( bookingKey Number(10), customerId Number(10), tourKey varChar2(8), startDate varChar2(10), CONSTRAINT PK_BOOKING PRIMARY KEY (bookingKey), CONSTRAINT FK_BOOKING1 FOREIGN KEY (customerId) references customer, CONSTRAINT FK_BOOKING2 FOREIGN KEY (tourKey, startDate) references tour ); create table payments( paymentKey varChar2(20), bookingKey Number(10), customerId Number(10), ammount decimal(5,2), datePaid varChar2(10), paymentNo number(2), CONSTRAINT PK_PAYMENTS PRIMARY KEY (paymentKey), CONSTRAINT FK_PAYMENTS1 FOREIGN KEY (customerId) references customer, CONSTRAINT FK_PAYMENTS2 FOREIGN KEY (bookingKey) references booking ); create table customer( customerId Number(10) NOT NULL, name VarChar2(30), address VarChar2(50), CONSTRAINT PK_CUSTOMER PRIMARY KEY (customerId) ); Any tips? /u/fromyourscreentomine /u/theduncan /u/incrediblemouse /u/wallyflops 
Eh, nevermind, I submitted it anyway, thanks for the help guys. Ill get at least 80 out of this im hoping.
...and I suck for taking forever. https://www.simple-talk.com/author/joe-celko/ He's infamous for replying to any halfbaked question with: ddl, insert stmt and 'what is your problem?' around the web. He was mostly right all the time; sql mechanics haven't changed so if you search usernet, go deep into internet's archives, hilarity ensues... and learn a lot as well. I did...
Perform a select asterisk instead of specific columns, just to review the results. Look for the one thing that makes the row different. If I had to guess, I'd stab at the notion that you likely have more than one payment for those with duplicates there.
I submitted it at ~4:00am this morning just before passing out. Online submission. The ERD is worth 50%, deriving schema and network diagram from that is 20%, 10% is in coding the db and inputting correct data, the remainder is queries....so I should be fine.
Nice article! I'm left wondering how this applies specifically to Java developers aside from the requisite dig at PHP. I was expecting a focus on Hibernate, maybe. Aside from the couple that are only relevant to a specific engine, these tips look good for all developers, data analysts, DBAs.
Thank you, I corrected it
3 seconds for 30,000 rows may be a considerable addition to run time depending on the situation. For the task at hand, it was an acceptable performance hit in order to meet the business needs in a timely fashion. Background if interested: A process to generate and queue up newsletters for delivery. We have approximately 40 newsletters that get sent on the hour depending on time zone between 3pm and 6pm and the subscriber list for each newsletter has to have an MD5 hash of the users email address for the ad calls that are contained within the email. This was a new requirement to pilot a new ad partner. Re: OLAP vs OLTP... I know the difference. I also know that just because a database vendor creates a feature, doesn't mean it should be able to be used in all situations. Which means its up to the developer and db admin to know when to use which tools for which situation. Also, statistics are generally performed on samples of a population, which means if you have 30MM rows, you won't actually need to query all 30MM to get a proper sample. I also know that in situations where I really do need the mode of the full 30MM rows, I will bake that into my ETL layer outputting a fact table with that information. I may not be an expert, but from job to job, I am generally the only dev on my team that knows what EXPLAIN means and how to use it, nor do I settle for slow queries just because they work.
An honest answer can be years. Depends on your current exposure to shell script writing, network comprehension, database development and administration competency. People move into a DBA role from being on the network admin side and learning database admin. The best DBA's I've seen were database developers turned admins. There are many paths, but there are a number of skills required to be competent. Unless you work in a small place, this will be a position that could take some time getting into. 
&gt; Well I wanted to ask how long does it actually take to become database administrator? I have difficulties with some foreign keys and writing queries that require more than 2 tables (with joins) .... Its gonna be quite long, probably very long. If you are thinking about working as DBA, you shouldn't have difficulties with those basics. I don't know how long does it gonna take for you to be able to use joins without much thinking but you need at lest 5 to 10 times of the time you took for those basics. And also .. if you are working on ERD and normalization .. then you shouldn't have problem working with foreign keys. Lots of DBA predates intellisense and generally don't need that to do their stuff.
I replaced an old Stored Proc that went line by line and now I have this beautiful set of SQL functions and a CTE to calculate time OnHold from a history table. I do it in 1/4 the time, but it looks like all I needed were window functions. Guess I know what I'm trying out tomorrow to see if it's faster.
A Notes table with [TrapID, Date, Note] would suffice from what I can tell. You would not have to flag a Trap as having a specific year, as that would be determined by the Dates in the Notes table. As for updated Trap information, just 'effective date' each record, e.g have a date value related to the start of the new data for each Trap. Depending on how much data is likely to change for a Trap, an additional table to hold that data would be something to consider. An example of the data would help. This is perfectly doable in Access.
I've been doing DBA work for a little over 10 years... I'm still learning new stuff all the time.
I don't like your permissions. You should make a separate table for permissions, then you can simply do where Cntry IN (select up.cntry from userpermission up where up.usr_id = usr.id) If you are going to stick with the single string, then you need a clear delimiter (semicolon?) So you don't confuse 'United States' and 'The United States of Zimbabwe'. Then you can do this: where user_permission LIKE Concat('%',Cntry,';%') EDIT: Actually, you should probably delimit at the beginning and end (or just start the string with a ';'). My LIKE could return both 'United States;' and 'The Martian United States;' The new where would be: where user_permission LIKE Concat('%;',Cntry,';%') 
For the changing trap data, any time a trap is changed out with another trap type, a new entry would be made as if it was a totally new trap and appropriate CYXX check boxes would be checked. For the CYXX check boxes, I want to leave those in there, however, they will not be used to sort out the traps but I think I will need them so that we can determine the list of traps applicable to a contract year to make sure no trap is left unchecked. In case you didn't see it in the other post, [here](http://i.imgur.com/jkc0LGx.png) is the table I am now using for the visit information. All the fields there are required by my boss for numerous filtering reasons. [Here](http://i.imgur.com/ZznTTtX.png) is a small example of the data that would be put in. I ultimately want to have this in an idiot-proof form that can be filled out by someone with minimal computer experience. I hope this gives you a better picture of what I'm trying to do. Thanks for you input.
Unless there are some special circumstances, you should probably normalize the DB: have a country reference table and then your 'user permissions' table record would have the user id and a country code.
what if I have multiple countries for a user, I guess this is my biggest issue and not knowing how to incorporate says Canada has an ID of 101 and United Kingdom has 102, User1 needs to be able to access both of these and user 2 only Canada
thank you, should I be doing my table such as username cntry user1 ;canada;united kingdom user2 ;canada;
united kingdom should also have a semicolon at the end, but yes. . If you are making a separate table for permissions, each row is a single user-country pairing: **username** **cntry** user1 canada user1 united kingdom user2 canada And it's even better if you use unique ID numbers so your strings can't get confused. So you have tables User(*ID, Name,...), Country(*ID, Name,...), Permission(user_ID,Cntry_ID)
What type of SQL Server are you using, versions, manufacturer, the word.
BACKUP LOG with TRUNCATE_ONLY This breaks the backup chain though, I would suggest you do another backup after issuing this command
Run DBCC SQLPERF(LOGSPACE) to see how big the log file is, and how much of it is being used. If the log is full of data &amp; you absolutely can't backup the log, then truncate it. Take a full backup immediately afterward though and institute log backups to manage the filesize in the future (or consider changing the recovery model) If you're on SQL 2008 upwards then 'backup log with truncate_only' won't work. Switch the database into simple mode, then back to full to truncate the log. Try shrinking the log again, using the 2nd option - 'shrink to size' If it still doesn't go anywhere, then use the following to query what's going on: select log_reuse_wait_desc from sys.databases 
Use this: http://sqlfiddle.com/#!4
thanks so much, i got it doing exactly as needed. I really truly appreciate your time and patience. 
For me the key to a good DBA is understanding what physically happens with a database. This kind of understanding goes way beyond SQL syntax and database design. For example, knowing FULL OUTER, LEFT OUTER, RIGHT OUTER, INNER, and CROSS JOINs is not fully understanding joins. NESTED LOOP, HASH, MERGE are physical join concepts which can mean the life or death of a query. Indexing OrderDate does not necessarily make "SELECT * FROM Order WHERE OrderDate = '2014-03-01';" faster. It might make it faster if there are 3 orders on that date. It might not be used at all if there are 3,000 orders on that date. It might actually slow down the query, especially if statistics are out of date. As far as getting to that level of understanding, understanding SQL syntax and database design is the first step. After that, I've gotten a lot of milage out of reading Microsoft documentation, attending expert presentations, asking questions, and testing hypotheses until I can understand, predict, and manipulate how a database will physically react to business demands.
That'll do it, the only thing I would highlight is that the log will be shrunk to 1mb which may not be enough for your app. I don't know what your situation was, but consider shrinking the log to a larger size, if 1mb is too small it will just mean that it grows again (without getting into VLFs etc ... ) Best of luck! ;)
yeah - make it a gig maybe ;)
dumb question maybe.... why videos? I so much prefer a well written document to some youtube video. I don't want judge or attack people that do prefer videos, I just don't get why.
Lets see.... 1st : WHERE DATEDIFF(mi, tblLaesning.forventetSlut, tblLaesning.faktiskSlut) &gt; 15 thats quite bad, its a non sarg query on a trigger 2nd I have no clue what DBMS you are using, but why in gods name, are you writing a trigger and not reference the trigger internal "tables" that hold the updated / inserted / deleted data? 3rd this is the answer you are looking for : AND NOT EXISTS (SELECT FL.LaesningID FROM tblFejlLaes FL) You see it yourself? "semester finals" please tell me you see it yourself... 
Well, I got it working, but not thanks to you. Sorry my first semester SQL isn't up to your standards. :)
The CYXX boxes are a very poor design. The Trap records should have a StartDate value and and EndDate value. You would determine the years of operation by querying those values. You could also have a CurrentRecord flag to assist in querying the current valid records. The alternative is to query the latest StartDate value for a record. The StartDate EndDate values would also give you a record of when historical changes were made to the records. This is how it is done in industry. edit: I keep reading your [original post](http://www.reddit.com/r/MSAccess/comments/26hrvm/comparing_different_years_data/) over and over again, but cannot grasp the issue with *Unfortunately, everything Ive read states that you cant essentially run a query and then add more data to it.*. It makes no sense. You add data, you run a query. The query returns results selected. Add more data, requery. OK, more reading. If you have child records for Trap information additional to visit information, you should have a Main Trap table with effective dates (start and end date), this contains data of the trap that never changes, then have a TrapInfo table with data that is likely to change also with effective dates. You are then able to independantly query Notes or TrapInfo against the main Trap record. [Trap] [TrapInfo] [TrapNotes] 
&gt;but not thanks to you if you are not able to understand that EXISTS (SELECT FL.LaesningID FROM tblFejlLaes FL) will always return true if that table is not empty, don't bitch about it. Basic, so very god damn basic. Your attitude is not well placed. Btw, its not about SQL, its about learn your god damn material, wikipedia homework generation 
I'm well able to understand that, but I couldn't spot the mistake (could be due to looking at SQL all day, or other reasons). And in regards to attitude, I can see my response could be interpreted as "Attitude-y", but I was merely pointing out that simply quoting what's wrong doesn't always help. :)
I don't do well learning through reading. Don't know why, just don't.
&gt; And in regards to attitude, I can see my response could be interpreted as "Attitude-y", but I was merely pointing out that simply quoting what's wrong doesn't always help. :) I prefer to point to the spot, instead of grabing someone by the hair and smashing their face into the answer. You do not learn by being given the answer streight up. You learn by thinking about what the issue is, and me telling you what the issue is, I consider to be help. I'm not answering the "hey can someone help me with my homework" posts for the reason of giving out the answers. I do it to help the person to learn what they are missing. If you want streight up sourcecode, I'll be happy to send you my hourly rate, you are going to have to deliver a shit load of pizza for an hour of my time thou, that much I can promise you.
Thanks you!
I have no idea, personally. I read a lot of books on the topic. To not read could be a real hindrance when learning how to code. The only other option I know of is training classes. These are many times light on reading and heavy on instruction and exercises. I would highly recommend taking some classes on reading as well, though. Not being able to read technical documentation is going to be a problem, especially if it carries over into reading code. Writing code without reading code is a recipe for disaster. Good luck!
Yes. He is asking for a spreadsheet, or chart, or movie, or something, that lists the major RDBMS with the various releases and what came with those.
That sounds like more than a paragraph. Do post your final answer. 
Nah, I'm coming along real well with actual application. I just need the videos for pointers. I'm doing all sorts of wacky testings on my own personal stuff. Getting better at my job too.
By "SQL Standard Versions" it sounds like he is more concerned with [ANSI SQL standards](http://en.wikipedia.org/wiki/SQL#Standardization) than vendor releases. He definitely wants it compared to vendor release versions, but my interpretation puts ANSI standards as the main variable to progress/compare along.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 20. [**Standardization**](https://en.wikipedia.org/wiki/SQL#Standardization) of article [**SQL**](https://en.wikipedia.org/wiki/SQL): [](#sfw) --- &gt;SQL was adopted as a standard by the [American National Standards Institute](https://en.wikipedia.org/wiki/American_National_Standards_Institute) (ANSI) in 1986 as SQL-86 and the [International Organization for Standardization](https://en.wikipedia.org/wiki/International_Organization_for_Standardization) (ISO) in 1987. Nowadays the standard is subject to continuous improvement by the Joint Technical Committee *ISO/IEC JTC 1, Information technology, Subcommittee SC 32, Data management and interchange* which affiliate to [ISO](https://en.wikipedia.org/wiki/International_Organization_for_Standardization) as well as [IEC](https://en.wikipedia.org/wiki/International_Electrotechnical_Commission). It is commonly denoted by the pattern: *ISO/IEC 9075-n:yyyy Part n: title*, or, as a shortcut, *ISO/IEC 9075*. &gt; --- ^Interesting: [^Microsoft ^SQL ^Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) ^| [^SQL:2003](https://en.wikipedia.org/wiki/SQL:2003) ^| [^IBM ^SQL/DS](https://en.wikipedia.org/wiki/IBM_SQL/DS) ^| [^Language ^Integrated ^Query](https://en.wikipedia.org/wiki/Language_Integrated_Query) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cht0zcb) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cht0zcb)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Pluralsight has some of the best SQL Server training available. They have classes from the SQLSkills.com team and a bunch of other highly respected individuals from the SQL Server community. Not free, but totally worth the subscription. 
haha what? Please explain?
 I would like to share a list of Microsoft exam-related articles and tips, at 100Questions Exam Portal (http://www.100qns.com) . MS 70-410 measures your ability to install, configure servers, roles and features, as well as configure Hyper-V. You will also learn to configure network services and Active Directory; create and manage Group Policy. Good first volume to prepare yourself for the exam. MS 70-867 are for IT professionals who configure or support Windows 8 computers, devices, users, and associated network and security resources. The networks with which these professionals typically work are configured as domain-based or peer-to-peer environments with access to the Internet and cloud services. The other good thing is that you can test your MS knowledge and prepare for the exam via the exam library, which contains free-to-try exam questions. The Microsoft reference section could be accessed from the drop-down menu, and because the portal covers several topics, it could be confusing initially due to the enormous amount of data, but if you spend some time navigating the contents you might be well-rewarded! Another useful website is Exam Fight (http://www.examfight.com) is also a new and relevant site leveraging on gamification to make learning fun and enjoyable - definitely worth a visit! Susan Williams 100Questions @ http://www.100qns.com
I guess an automatic downvote function isnt the best way of putting it but basically the number of upvotes and number of downvotes aren't the actual numbers, they are "fuzzed" so that a bot which is programmed to downvote or upvote posts can't detect when it's been shadow banned. In other words you can trust the total count but you can't trust the upvote or downvote count. It's possible your post could say 10 upvotes and 5 downvotes when really there's just been 5 upvotes. EDIT: [here's](http://www.reddit.com/r/explainlikeimfive/comments/z4o44/eli5_why_reddit_autodownvotes/) an ELI5 I found on the topic.
What a dumb assignment. This is for grad school? There is very little merit to it, if you ask me, unless you're going to have to write some universal database-agnostic SQL or something- the important thing is just to know that there are various "flavors" of SQL and some features do not/only exist in certain DBs. Anyway, [this](http://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems#Database_capabilities) might be a good place start...
Or you could change this to: CASE Name WHEN 'Other' THEN 'zzz' ELSE Name END
Try RLIKE instead of LIKE. RLIKE uses regular expressions
Awesome!
Thank you!
Just wait until you get to execution plans, those will blow your mind. If you're in BI, take a Kimball course if you can. It will save you years of mistakes. 
depends on the higly on the collation
another developer stung by the dubiousness that is w3schools 
This channel also has some nice plain-jane tutorials. https://www.youtube.com/user/KatieAndEmil/videos
Why? Also, using a window function like ROW_NUMBER() would make this a lot easier.
&gt; Also, using a window function like ROW_NUMBER() would make this a lot easier. Show and example :-)
easiest way to do this from SSMS (management studio) is to right click on your DB object then click tasks, then export data. Create a text file, the wizard will need the file to be created already, simply a file with nothing in it somewhere on your machine. from the export data wizard it should be a simple matter of following the prompts, the destination will need to be set to flat file destination. Also a tip for this, if your text fields in the table are stored as nvarchar, then you will need to export as Unicode, if varchar then non Unicode will work. Somewhere in the wizard you will be given an option to export from a table or a query, select query then you will be able to paste your query into the box. 
Hey, thanks everyone. You were all a big help. Kind of a strange assignment, I know. 
Are all transactions and their data stored in the database? If so, you can create a report to be run when requested that would pull every 200th record from a table and get your results. --Transaction table CREATE TABLE Table1 ( ID INT NOT NULL IDENTITY PRIMARY KEY, Col1 VARCHAR(50) NULL, Col2 MONEY NULL ) --Insert test data. DECLARE @a INT = 0 WHILE @a &lt; 5000 BEGIN INSERT INTO Table1 (Col1, Col2) SELECT CHAR((ABS(CHECKSUM(NEWID())) % 22) + 65) + CHAR((ABS(CHECKSUM(NEWID())) % 22) + 65) + CHAR((ABS(CHECKSUM(NEWID())) % 22) + 65), ABS(CONVERT(MONEY, CHECKSUM(NEWID()) / 10000000.0)) SET @a = @a + 1 END --Get every 200th transaction. SELECT ID, Col1, Col2 FROM Table1 WHERE ID % 200 = 0 --or with ROW_NUMBER if you do not have a good identity or transactions can be deleted. SELECT T1.RowNum, T1.Col1, T1.Col2 FROM ( SELECT ROW_NUMBER() OVER (ORDER BY sT1.Col2) RowNum, sT1.Col1, sT1.Col2 FROM Table1 sT1 ) T1 WHERE T1.RowNum % 200 = 0 ORDER BY T1.RowNum
Two (easy) ways I can see to do this: * Figure out about how often you would need to do a transaction dump and just set a timed job * Perform an insert trigger that does(requirement of TBL_KEY must be used on every transaction): *** IF(COUNT(TBL_KEY) % 200) = 0 -- Run transaction log dump ELSE -- Insert normally *** It should be noted that this is normally bad practice, but it is not completely terrible since the total row count insertion is relatively low (80~ an hour) and the space requirement for storing the transactions for an entire year is probably cost prohibitive; as per IAmAJerkAME suggestion, if you just store each one on the fly on a low resource server then it could be a viable solution as well. 
MSSQL guy here but I think this is what you want. EDIT: misread TWICE see below AGAIN Select t2.columnname, t1.column_name1, t1.column_name2, t1.column_name3 From ( SELECT column_name tablename2 Where column_name like ('%variable%') LIMIT 1 ) t2 RIGHT OUTER JOIN tablename t1 on T2.column_name is null This should give you a dataset of either t2.column_name = value t1.column_name1 = null t1.column_name2 = null t1.column_name3 = null --1 record returned-- or t2.column_name = null t1.column_name1 = value t1.column_name2 = value t1.column_name3 = value --Many records returned-- Keep your schema the same (number of columns). Don't use Select *. 
Thanks, I think CASE statement might be the direction I need to look 
All past transactions are in the database, but it's being updated 24/7. But my question was , maybe, not very clear. I don't want every 200th transaction afterward, I need something that takes the instant average of a set a value anytime there's been 200 transactions gone by since the last log. A bit like if someone asked you to check the outside temperature every one hour for the next 6 months.
If you go with a solution in the database a trigger would probably be the way to go. That is if the application doing the insert can't be changed to preform this before the insert. The trigger could insert a tracking row with some calculated values into a separate tracking table. Something similar to this as /u/Coldchaos noted: CREATE TRIGGER Table1_INSERT ON Table1 AFTER INSERT AS BEGIN SET ANSI_WARNINGS OFF; SET NOCOUNT ON; DECLARE @CalcualtedVal INT; IF (SELECT COUNT(*) FROM Table1) % 200 = 0 BEGIN SELECT @CalcualtedVal = I.Col2 * .05 FROM inserted I; INSERT INTO Table1Tracking (MyCalculatedValue, CurrentRowCount) SELECT @CalcualtedVal, COUNT(*) FROM Table1; END; END; GO I still think it'd be possible to do it off the underlying data with just a select, but I may not know all the business requirements.
I updated my statement.
perfect! Thank you so much
What exactly is this report supposed to do, what are the specs? It's not very clear looking at the code, I'm afraid. Also what version Oracle are you using? How big are these tables (number of rows) and do you have any indexes on those columns? Just at first glance, you are selecting sum values into varchar2 type variables so you should probably change that and secondly, you should also assign an alias to the second column in your cursor: NVL(b.BLT_WEIGHT,NVL(b.WEIGHT, l.BILLET_WEIGHT)), dbms_output.put_line can be heavy on the CPU so if you have a lot of lines that could be taking up resources as well. Overall it would probably work much better if you could re-work the logic to put it into a single statement, using the appropriate indexes (and partitions, if it's the case for that).
Right...I'll make sure I use as little triggers as possible and document it. Thanks, you're not such a jerk after all. :~) 
I'm ashamed of how long it took me to grasp the concept of what you were trying to say. Would the current record flag just be a Yes/No based on whether or not the trap is still being checked? In your experience, would it be worth adding the extra field or would it be better to just query the traps that do not have an EndDate value? [This](http://i.imgur.com/gC0kHFws.jpg) is what I have now. Regarding that quote, that was supposed to read: "Unfortunately, everything Ive read states that you cant essentially run a **MAKE TABLE** query and then add more data to it **AND STILL HAVE IT AUTOMATICALLY UPDATE WITH REPSECT TO THE TABLE THE MAKE TABLE QUERY WAS PERFORMED ON**." This, fortunately, is a moot point now with the way I'm approaching it. My brain wasn't firing on all cylinders when I wrote that the first time. &gt; OK, more reading. If you have child records for Trap information additional to visit information, you should have a Main Trap table with effective dates (start and end date), this contains data of the trap that never changes, then have a TrapInfo table with data that is likely to change also with effective dates. You are then able to independantly query Notes or TrapInfo against the main Trap record. &gt;[Trap] &gt;[TrapInfo] &gt;[TrapNotes] Per my table names, this would be: [Trap_Info] - Information that never changes for the lifetime of the trap [Trap_Notes] - Notes that apply to a trap not specific to a particular visit, such as "area under construction, equipment served out of order, contact customer for specifics, etc.," that will have start and end dates of their own, but don't necessarily apply to the life of the trap [Visit_Info] - All information visit specific In regards to this, will it be possible to run a query/report pulling information from all 3 tables simultaneously with the following output format? Building 1 Info Trap 1 Info Trap 1 Notes (if any) Visit 1 info Visit 2 info Visit 3 info Trap 2 info Trap 2 Notes (if any) Visit 1 info Building 2 info Trap 1 Info Trap 1 Notes (such as area under construction, which would result in no visits being performed) Building 3 info etc..... I can create a better visual with actual information if necessary. Also, I guess it would be good to have a similar notes table for the buildings themselves, in case a building is being renovated. Thanks again for your input. It is helping me a lot. EDIT: I'm not asking how you would go about making a report/query look like that, I'm only asking if it is possible before I start running around figuring out how to do it.
It outputs tons per hour based on the bounds set as start_time and end_time User enters start time and end time and the reports calculates the tons per hour for each billet between them. Sorry I forgot to add that. Version 10g The tables are huge. Over a million rows. No indexes that I know of. Will do. I assume number is the way to go? I was using the dbms_output for troubleshooting. I just got rid of it and the script runs but with no output at all. So I guess that's good but how do I make sure it ran correctly? 
What is the relationship between those 2 tables? "1 to 1" or "1 to many"? You could probably use an index *at the very minimum* on the RF_OUT column of the X_BILLETS table and another composite index on the three join columns of the X_LINE_UPS table. Create the index in the order of column selectivity (so the column with the fewest number of distinct values should come first and so on). Make sure your statistics are up to date (use dbms_stats.gather_table_stats for that). Now to the select itself: if I understand you correctly, you would like to see a sum calculated for every billet that turns up in the X_BILLET table in the specified interval and each billet (whatever that is) should only appear at most once per calculated hour, correct? If I understood correctly then I think this select should help you produce the output you need (you need to replace the :start_date and :end_date parameters in the select with your own dates): with t as ( select :start_date + level/24 as date_ from dual connect by level &lt;= 24 * (:end_date - :start_date) ) select x1.date_, x1.blt_no, sum(x1.blt_weight)/2000 as tons_per_hour from ( select x.blt_no, (case when x.blt_weight is not null then x.blt_weight elsif x.weight is not null then x.weight else (select l.billet_weight from x_line_ups l where l.ord_num = x.ord_num and l.ord_item = x.ord_item and l.lup_num = x.lup_num) end) as blt_weight, t.date_ from t inner join x_billets x on t.date_ &lt;= x.rf_out and x.rf_out &lt; t.date_ + 1/24 where x.rf_out between :start_date and :end_date ) x1 group by x1.date_, x1.blt_no; As a note: putting your report inside of a PLSQL procedure will most definetly not get you any performance benefits, quite the contrary actually.
Are you given the original and discounted prices? Do you know what the discount rate is? Is it different for each item in the list? It it static? 
 I am told that the discount rate will be different for each supplier. They told me to normalise the following tables to 3nf: Supplier details(name,adddress,approved purchase limit) Invoices submitted by the supplier invoice number and date Items on the invoice item_id,quantity,unit proce, total price, discount amount) Also, there is a discount rate agreed with each supplier for certain items (item_id) And then it tells me to produce a query that will identify items on an invoice where the discount has not been applied correctly (or not at all) No need to go to easy on me, I'm used to programming in other languages so feel free to get technical. 
Ok, so let's start with normalizing the tables. I'm not the best at this so someone may correct me somewhere down the line, but in 3NF, all fields must only relate to the primary key, and nothing else. supplier_details supplier_id name address approved_purchase_limit ---------------------------------------------- supplier_discounts discount_id supplier_id discount_amount item_id ---------------------------------------------- invoices invoice_number supplier_id invoice_total date ---------------------------------------------- items item_id unit_price ---------------------------------------------- invoice_items invoice_items_id invoice_number item_id quantity ---------------------------------------------- Ok, with me so far?
sure am, I'm about to hit the hay now (UK) so I'll reply to the rest tomorrow or in a few hours if I can't sleep. I normalised mine slightly different, I'll post them in the morning, thanks for the help so far!
So now that we have the table normalized, what we need to do is go through the invoice, and see what the total is, and what it should be. The total is provided on the invoice, we're going to assume that this has been provided by whoever entered the information, and that it was not computed. To generate the proper total, what we will do is join every single together. However, it will be joined in the following fashion. From the `invoices` table we need the `invoice_number` and the `supplier_id`. Next, we need to generate the total. This will be done by using an aggregate function that joins the `invoice_items`, `items`, and `supplier_discounts` tables. Within that function, we will `SUM` all the items within `invoice_items`. But first we need to generate the price of each item. This is done by using the `discount_amount` from the `supplier_discounts`, and taking it from the `unit_price` within `items`. Once you have this, you'll multiply it by quantity. After you've summed all the items, you will have the true total, and you can compare it to `invoice_total` from within `invoices`. I'm still at work, but when I get home in the next hour, I'll actually create a quick test project to see if I can hash it out rather quickly.
&gt; *I'm ashamed of how long it took me to grasp the concept* Don't be. Things are only obvious after they are explained and conceptualised internally. We can't know what we don't know, this is why you came to this subreddit. I am pleased to be able to help you with the breakthrough in understanding. 1. I have seen examples of flags and enddates, but you really don't need the flag. An enddate is effectively the flag. e.g *select * where enddate = ' '* is as good as *select * where flag = 1* 2. Yes, you can absolutely get the report you are after with the data structure described by ordering the data by Building / TrapInfo / TrapNotes / TrapsVisit 
SQL is a set based language. Why are you creating a report with a cursor?
Using VBA, there should be an onChange() event for the checkbox that will trigger something, You could trigger an SQL UPDATE on the EndDate value to enter the current date. It might be pertinent to disable the Check_Status once checked - maybe with a confirmation alert that the Trap will not be editable there after.
Absolutely. Using the onChange event you can determine the value selected, and it if the closure status then run a Sub() to ask if they want to continue and enddate and then disable the drop down. Just google "msacccess VBA sub functions" for more info on subroutines to accomplish more complex processing in VBA applications.. 
Yes http://www.youtube.com/watch?v=aylQNvVDPsg Consider though that it will reside with each worker that uses it. Alternatively you can [share the database ](http://office.microsoft.com/en-au/access-help/ways-to-share-an-access-database-HA010279159.aspx#BM3) and include a login field to start that can then be used as an identifier for records inserts and updates. There are 4 fields used in industry for this at the record level for each row. AddedBy AddDate LastModifiedBy LastModifyDate
Ah, thanks.
[Oracle APEX](http://www.oracle.com/technetwork/developer-tools/apex/overview/index.html) is probably the closest thing to what you're looking for. Not really the best framework IMHO, attempting to map a DB schema to a front-end is OK for smaller, forms based apps, but isn't scalable in the least.
Index your foreign keys and RF_OUT then analyse your indexes. Always use SQL over PL/SQL, explicit cursors are slow compared to queries. You don't need nested loops, you can rewrite your query using a single query and a windowed analytic clause sum(...) over (.... range between ... and ....) 
As others have already answered, this is not a thing. Depending on the complexity of the project you are trying to build, a possible approach would be a framework like Django, which ships with a full ORM. Basically, you don't have to write any SQL at all, but befine models of what your data should look like and let the ORM take care of the rest. From there, you can basically do whatever you want with that.
Looks like I've got more reading to do. Thanks
I was guessing. It's over 16 million but I will certainly try out your suggestion.
I'll try this out and see if it gets me what I'm after or at least down a better path to what I need. Thanks much.
 SELECT * FROM targetlocation WHERE Region in ('region1', 'region2') AND ( Flag = 'yes' OR anothercolumn = 'yes' )
not sure if "SQL Server Developer Edition" is going to do you any good with a MySQL database the UI will need to be done in some language like php 
I'll give this a run :) Thanks!
Actually, on a LAN, the UI could be built with MS Access.
 UPDATE YourTable SET columnX = 99990000 + columnX That's assuming your column is an integer or something. If it's a string it would be UPDATE YourTable SET columnX = '9999' + columnX 
The closest thing I can think of is PHPMyAdmin. It needs to be installed on server (usually the same computer that MySQL is installed on) and you access it through a web browser.
it also may be worth mentioning that you are talking about 10 rows. You have a table with 10 rows and you want to update a column based on its existing value.
[This is what we say to SQL Server 2012 Licencing](https://www.youtube.com/watch?v=NHh0rf0ojEc). My recommendation is just use Postgres for the back-end. MySQL isn't a bad choice necessarily, but the way things are headed it will be in the next few years, so you're better off just jumping now.
OP wanted a web site, though
What a mess of a query. What SQL platform do you use? (MSSQL, Oracle, MySQL, ect...) To give you a quick and dirty answer, all you need to do is add this change: [...] ELSE 'All Other' END ) AS "CATEGORY BUCKETS: ", --&lt;&lt;-- Add a comma here IM.SALE_GRP AS SALEGROUP --&lt;&lt;-- Add the field here FROM STORE_DISTRO SD [...]
I think this is what you are asking for DECLARE @seed int = 999 UPDATE m SET mytable = u.ColumnX FROM mytable m JOIN (SELECT primaryid, columnX + CONVERT(nvarchar,@seed + ROW_NUMBER() OVER (ORDER BY columnX)) as ColumnX FROM mytable) u on m.primaryid = u.primaryid If columnX is an int DECLARE @seed int = 999 UPDATE m SET mytable = u.ColumnX FROM mytable m JOIN (SELECT primaryid, CONVERT(nvarchar,columnX) + CONVERT(nvarchar(@seed + ROW_NUMBER() OVER (ORDER BY columnX) as ColumnX) FROM mytable) u on m.primaryid = u.primaryid Edit: changed seed to 999 because row number starts at 1 
Just an fyi. This only works up to 10000 rows. At 10001 it will concatenate 900001 with 10000 which is a drastic jump in numbering.
Which DB platform?!
I guess, I better rephrase. I need to concatenate 999 to 1001 so on and so on. How can I do this?
You mention MySQL in the title but ask about SQL Server, two totally different things. I think SQL Server is overkill for this kind of scenario to be honest, i don't see that the benefits can possibly outweigh the costs (and MySQL is a bit crap). SQL Server **Developer Edition** cannot be used in a production environment, it is *purely* for building and testing things in, you would need a fully fledged production license, which is costly. For what you propose, only a few tables in a small database, go for a free option. I mean technically, you *could* use developer edition, but I think your employer might have something to say about it, especially when it comes time for an audit. Postgres is the sensible option. It is opensource, free, and better than MySQL. You can do everything you need to do with it, and you'll be operating on the right side of licensing laws. **edit:** Think I may have offended people by saying MySQL is a bit crap hence the downvotes. Compared to the big guys like SQL Server and Oracle, it is very primitive and quite lacking, I don't see that there's any arguing that. It's also lacking compared to other free options like Postgres, but for some reason is more popular, I guess because it gets bundled with so many things. Kind of like Betamax and VHS, or HD DVD and Bluray, the technically superior loses to the better marketing.
Are you using query timeouts in the application? Having a maintenance job blocked by a user process isn't bad, per se, but letting a user process keep a query run overnight is probably an issue. Also, you can kill just the spid in SQL Server without killing the application, and (if the application doesn't choke on losing the connection) the user may never notice. Is your rebuild process online or offline? Is the user application changing structure (DDL), or just data (DML)? Edit: when it says "corrupt the database", that's pretty near impossible to pull off. Running transactions get rolled back when their process is killed (which might take some time). Edit edit: what ERP are you using?
MySQL will work great for this. PostgreSQL isn't bad either, but there's more noob-ish oriented documentation out there for MySQL. Both MySQL and PostgreSQL are free. Microsoft's SQL Server is not. MySQL will run on Windows, but you honestly should run it on Linux like it was meant to be. If you have zero Linux experience, it may take a few days to get it figured out, but setting up a Linux LAMP stack (Linux, Apache, MySQL, PHP) is an extremely well documented process and if you can follow directions, you can do so easily. Now, for the task you've set yourself upon, most of the work will be writing the PHP code. Check out http://www.w3schools.com/php/php_mysql_intro.asp and see if it looks like something you can work on comfortably. If so, you're well on your way.
Thank you!
If that doesn't work, find out who/what is taking the locks out. *** SELECT es.original_login_name + ' (MachineName: ' + es.host_name + ')' AS ConnectionInfo, DB_NAME(tl.resource_database_id) AS DatabaseName, es.logical_reads AS LogicalReads FROM sys.dm_tran_locks AS tl INNER JOIN sys.dm_exec_sessions AS es ON tl.request_session_id = es.session_id ***
Wow! (I really wish I had picked this program up 4 years ago when it was suggested to me.) A lot of the potential problems they list won't apply to my database at this time because it will reside on a single NAS box and will only be used by one person at a time. [This](http://www.msaccesstips.com/2006/11/microsoft-access-security/) seems like it might work for what I'm trying to do. &gt; There are 4 fields used in industry for this at the record level for each row. &gt;AddedBy &gt;AddDate &gt;LastModifiedBy &gt;LastModifyDate Are these fields normally appended to the right of the records you are keeping track of? I was thinking of something more along the lines of [this](http://i.imgur.com/F0vbEOu.png). This way, we have a record of every change that was made, not just the addition of information and the most recent change. I got the idea from another program we use here at work. The preliminary relationship I built for it is [this](http://i.imgur.com/5ZODcRl.png). Am I severely over complicating it for what I want to do? With this, I would be able to run reports for all changes made to a particular visit, all the way up to all changes made to a particular building. I know it seems a little excessive but we want a way to keep track of who all makes what changes. By the way, does every field name in Access have to be unique, or only fields in the same table? EDIT: Fixed pictures.
Yes, and you are asking for it to increment correct?
Yep, Access is pretty good for low cost fast developement. But for large data sets and multi user use I would recommend moving up to a proper client/server model, which you can use Access for the front end for cheap front end interface developement : http://www.techrepublic.com/article/using-access-to-build-a-front-end-for-sql-server/ . The audit fields are generally placed with the last fields. It makes for easier viewing when looking at the raw data. They are generally placed on every table in industry for auditing purposed. The last few images you have uploaded are so small as to be impossible to see information clearly. The child table records would hold the ID of the parent record ID allowing cross queries against any or all of the associted data. This is what databases are built for. e.g the Trap record would have the id for the Building record, and the Trap child records would have the id of the Trap. You would not normally include the building record ID in the Trap child records - although you could if you want to, after all it is YOUR database, but bear in mind that gives another level of data integrity to manage.
if its enterprise then rebuild with the ONLINE=ON option
I updated the pictures. I'll have to let my mind chew on those last 2 paragraphs for a little while before I can thoroughly respond. 
You'll likely need to add a comma and that same field to after the GROUP BY line too.
Access has its uses. Depending on what sort of network, or rather if not going over the internet they are using it could work well and is easier to implement than php or the like.
Java
And if you're not on Enterprise Edition, just switch to a reorg. Typically, a reorganize is good enough.
i got a few line into the tutorial and then stumbled on this part -- &gt; So if you wanted to sort employees by first and last name you could state &gt; SELECT FirstName, LastName FROM Employees ORDER BY LastName; shit, man, get your act together 
&gt; If it's a string it would be... no it wouldn't well, okay, in microsoft's weird alternate universe, but not in **SQL** 
i think that only works in one vendor's database offering, not in **SQL** dear OP, please next time identify your platform (see sidebar)
Oh no, java will create the interface and commands to control your SQL database. I just finished a small class mostly on sorting algorithms.
Oooh ok! Thanks!
No problem, good luck though. I switched degrees because I hated Java. I'm learning SQL now.
 SELECT a.id, a.name FROM news a JOIN newstags b ON b.articleid = a.id AND b.tag IN ('funny', 'sports') HAVING COUNT(b.tag) &gt; 1
you forgot the GROUP BY clause
How badly is the index fragmented each night? How big is the table and the index? One way to reduce the amount of locks/failures of the job is to check fragmentation prior to rebuild. If its under a certain threshold, reorganize it. If it is over a threshold, rebuild it. If its under 30% most DBA's will let it fly. Especially if its a small index.
Ah.. stupid typo! Thanks for pointing that out... post fixed... poop in a group... :) 
Hi, In general AND's are evaluated before OR's, just as multiplications are looked at before additions are in arithmetic. You can use parenthesis to "adjust" the order. So 5 + 2 * 3 = 5 + 6 = 11 can be made to be (5 + 2) * 3 = 7 * 3 = 21. Alot of this and more is explained on how to use boolean logic in SQL: http://www.essentialsql.com/get-ready-to-learn-sql-4-query-results-using-boolean-logic/ 
You are a SAINT
If newstags PK is (ID, TAG) and you are explicitly looking for more than one matches : select * from NEWS n where 1 &lt; (select count(*) from NEWSTAGS t where t.ARTICLEID = n.ID and t.TAG in ('funny', 'sports')) 
Thanks for that, I was actually looking in that direction as well with a subquery. The other solution gave me the right direction in where to go with the sqlalchemy statement which had worked for me :). conditions = [] conditions.append(News.id == NewsTag.newsid) filterlen = len(filters) -1 # filters is an array brought in by request.GET['filters'] filters = tuple(filters) conditions.append(NewsTag.tag.in((filters))) conditions = and_(*conditions) news = db.query(News).join(NewsTag, conditions).group_by(News.id).having(func.count(NewsTag.tag) &gt; filterlen) 
Whoops, I'm using MySQL.
thanks for the advice everyone. i found the solution by lukeatron to work well for my situation. i forgot to mention that the dropdown list that I am using this SQL statement for was going to have dynamically-added items by an admin user of the application, so input is uncertain. this discussion did bring up a concept i didn't really think about: taking into account non-ascii characters. are there any other concepts like this that i should know, but I might haven't learned that can contribute to a SQL statement's overall usability? Thank you!
Look at Pentaho PDI. It is an Open Source ETL tool. 
Why not just use the ExtractValue call and pull it out with XPath?
For every row in the the outer Orders table, select the sum of OrderAmt for every row with the same or smaller OrderID. If one assumes these records were inserted in date order, than this would give you the running total over time. If that's not a safe assumption you could do `WHERE two.OrderDate &lt;= one.OrderDate` in your sub query.
 [one].OrderID changes with every row, since you're selecting from [one]. the subquery is executed for every row returned, so two.OrderID doesn't really change, but the evaluated result of [two].OrderID &lt;= [one].OrderID does for every row. Your sub-query is in-essence, its own Scalar function, that is executed for each row, taking [one].OrderID as a parameter. 
If XML is not an option, you can look into PATINDEX to find the starting and ending characters of the desired string and tie that into a SUBSTRING command. If you want to get the data back as an XML segment rather than just specific values you can also do(may need some small changes as I don't have access to SQL atm) this has typically had the lowest cost if youre going to deal with a lot of data: SELECT trn.query('EndDate') FROM XMLColumn.nodes('/ScheduleDefinition[1]') AS nodes(trn);
As a side note, PATINDEX only works if you can guarantee only one instance of a tag. Which with these scenarios rarely happens. Otherwise you are stuck using recursive UDFs to get all the instances out which is much less efficient than the above mentioned solutions.
To visualize this even more, [here is an example spreadsheet.](https://docs.google.com/spreadsheets/d/1OCiX2CETGi0AP9RpyFPsg09jxhUdGxi1rWGGH7f0YX4/edit?usp=sharing)
There's a very relevant Stack Overflow answer here: (http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags).
Quick warning, this method is commonly known as a "[Triangle Join](http://www.sqlservercentral.com/articles/T-SQL/61539/)." (See /u/IAmAJerkAME's spreadsheet to see what I mean by triangle.) A "triangle join" is essentially a cross join, cut in half. For every row it processes, it has to do an additional select for all of the rows previous to it. The article I posted gives the formula to figure out how many rows you would need to process: (X^2 + X)/2 + X That's fine if you have 20 rows in your table, you're only processing 230 rows (!). If you have 25,000 rows in your table? You will be processing 312,537,500 rows. If you're running SQL 2012, you've got some options. There are some new windowed aggregate functions that make this amazingly simple. Your code would look like this: (Edit: /u/lukeatron brings up a good point, the original code assumes all OrderIDs were inserted sequentially. I've updated my code here to use OrderDate as the ORDER BY instead.) SELECT OrderId , OrderDate , OrderAmt , SUM(OrderAmt) OVER (ORDER BY OrderDate) AS 'Running Total' FROM Orders ORDER BY OrderDate If you're running SQL Server 2008 or prior (Or some other SQL solution) your choices are considerably more complicated. I won't get into it here, but Jeff Moden has an article on using the "[Quirky Update](http://www.sqlservercentral.com/articles/T-SQL/68467/)" to handle running totals in SQL 2008 and prior. It's incredibly interesting, but I recommend a cup of coffee (or four) before trying to read it.
check to see if any of the titles have a leading space
Yeahhh you're right. I looked into writing my own script and it's actually not very difficult. It will simplify things in the long run. https://docs.python.org/2/library/xml.etree.elementtree.html 
Thanks for the reply! Some questions below, I am not trying to be controversial, just legitimately asking. &gt;Aid readability and maintainability, as sections can be modularised and separately extracted and tested. I could not disagree more. I have an extremely hard time reading code with CTEs in it. This is 100% a matter of preference, I realize that. &gt;Can act as a view without the overhead of storing in metadata. Not really sure what you mean. The purpose of a view (I thought) is to restrict access to underlying tables, or to modularize complex pieces of code. How does a CTE help with either of these? &gt;Enables easy usage and querying of complex derived data. Sounds like a view or function, how does this relate to a CTE? &gt;Allows normalised access : reference resulting CTE sub sections multiple times in the same statement, which prevents multiple scans of the same data. Indexed #temp tables will out-perform a CTE every time on larger data sets, so why use a CTE?
It sounds like you have alot of reasons NOT to use CTEs and the reasons you have listed all seem to boil down to your first response statement: This is 100% a matter of preference, I realize that. * Disagree - Readability is better any day of the week, once again preference * Why create a permanent structure (VIEW) if you only need it in this specific use case, or when you don't have permission to create views, you can simply use a CTE inside your ADHOC query, or Stored Proc. * Refers to the above mentioned complex underlying data which a view can hide, same as above, onetime only, no access to create. * OP must be referencing smaller datasets if they work for him. 
&gt;I could not disagree more. I have an extremely hard time reading code with CTEs in it. This is 100% a matter of preference, I realize that. Then who ever is writing your CTE's doesn't know how to format. I'll take this over anything else any day of the week: WITH TMP_SOMECTE AS ( SELECT COL1, COL2, COL3, COL4, SUM(COL5) COL5 FROM DB.SCHEMA.TABLE1 WHERE COL1 = BLAH AND COL2 != BLAH3 ) SELECT A.*, B.COL5 FROM DB.SCHEMA.TABLE2 A JOIN TMP_SOMECTE B ON A.COL1 = B.COL1 AND A.COL2 = B.COL2; CTE's are modular, and when properly formatted are much easier and require much less overhead. Yes, the same task could be completed by creating a session based or global temp table, but there is significant overhead required for both. The CTE only exists at run time, and is in-memory only. Once the query completes, the CTE no longer exists. This is the distinct advantage of a CTE over a temp table. &gt;Not really sure what you mean. The purpose of a view (I thought) is to restrict access to underlying tables, or to modularize complex pieces of code. How does a CTE help with either of these? Not quite. A view can be used to restrict access to underlying tables, but most views are actually used to flatten two or more tables into one, so that the end-user doesn't need to use complex joins to achieve the same result. However, the query in a view is executed every time the view is accessed, which creates overhead. Oracle does have the materialized view, but I don't think SQL Server has an equivalent. &gt;Indexed #temp tables will out-perform a CTE every time on larger data sets, so why use a CTE? Because the #temp table continues to exist for the entire session? Whereas the CTE only exists during run-time? Also, indexed #temp tables will _not_ out-perform a CTE if the #temp table is so large it must be written to disk.
Not sure why I am being down voted ... anyway ... This will out perform your CTE query all day any day due to it being indexed on the queries you are joining on. ESPECIALLY if it is a large dataset. Yes it will be written to disc, but if it is SO large that it needs to be, then the joins will see HUGE benefits from being indexed properly. IF OBJECT_ID('tempdb..#SomeTable') IS NOT NULL DROP TABLE #SomeTable SELECT Col1, Col2, Col3, Col4, SUM(Col5) as Col5 INTO #SomeTable FROM dbo.Whatever WHERE Col1 = 'Red' AND Col2 &lt;&gt; 'Red' GROUP BY Col1, Col2, Col3, Col4 CREATE CLUSTERED INDEX ix_SomeTable ON #SomeTable (Col1, Col2) SELECT * FROM dbo.Whatever2 a JOIN #SomeTable b ON b.Col1 = a.Col1 AND b.Col2 = a.Col2
It won't win if the dataset is small. That CREATE NONCLUSTERED INDEX statement costs. You're likely being downvoted because you admitted your close-mindedness to the CTE. Kinda bodes poorly for anyone making an impression on you.
&gt;Indexed #temp tables will out-perform a CTE every time on larger data sets, so why use a CTE? not really. To fill the temp table, you have to do the select first, write the data to the temp db, and then index it (or have the index beforehand and have the index updates on insert). With the CTE the data will be "on the fly", its better to think of CTE's as subqueries than temp tables. So you can't really say that a temp table will always be better for performance compared to a CTE. However, you are quite right in that sometimes the good old (indexed) temp table can do wonders for performance. I also have to say that I do like CTE's for readability. They are quite nice for debugging too I'd say. That part is absolutly personal preference thou.
&gt; Whereas the CTE only exists during run-time? Not even that, the CTE should be fully inlined into the query using it, so its not even an object per se.
I agree with you. I don't use them except in rare instances. Usually if I need to build data sets in chunks I'm going to need to persist the data for testing and later for debugging. Maybe the data I live in is especially crappy but I'm always tweaking things and re-evaluating whatever the query pulled. 
Yep, agree there. sqldiaries flat out said that his CTE would out perform an indexed temp table on a large dataset though. We have our product installed at 80 different clients who have drastic ranges in database size. We have to code for the large clients, knowing we may take a 1.25 second query to 1.26 seconds if its a small client. If I was so close minded to CTEs, why would I have made this post? I wanted to understand WHY anyone would want to use CTEs, and I honestly haven't heard one good reason yet.
&gt; seem to have a boner for using CTEs &gt; I could not disagree more. These sorts of comments and phrasing, combined with your argumentative tone is probably what is pushing forth the impression that you are close minded. It seems like you are looking for people who agree with you rather than genuine different opinions, based on the way you are responding. 
CTE's make logically complex problems much easier to solve. Especially when dealing with big data. Working in the healthcare industry, I regularly have to join data from upwards of 20 different areas of our systems for ETL purposes. Each of those areas has scores of tables. Using CTE's allows for joining tables within specific areas to get just the data needed from that area, then joining the separate CTE's together for final output. In order to do this without CTE's, I might be looking at 40 joins in a single FROM block. Good luck following that logic without scratching your head. Additionally, CTE's help compartmentalize these types of situations. For instance, if there is one particular block of data causing performance issues, I can easily validate that by commenting out the CTE and corresponding joins to validate the issue. Additionally, I can run whatever is within that CTE alone, and tweak and modify it to improve performance. Try reading a joined query with 10 or 20 nested subqueries - good luck. I would take a CTE any day over that mess.
&gt;Yep, agree there. sqldiaries flat out said that his CTE would out perform an indexed temp table on a large dataset though. I have a table that is properly indexed on both the primary keys as well as on occasional (read: used often enough to benefit from an index) join columns that is over 600 Million rows. The CTE that I wrote to aggregate data from that table and then join it to another table for grouping is faster, by far, than the indexed temp table.
I agree with the OP. CTEs have their place don't get me wrong but I prefer temp tables any day. 1. I find temp tables to be easier in management of data in a stored procedure especially if data needs to be called multiple times. 2. Large data sets (millions of records) CTEs will never perform better than an indexed temp table. Look at your execution plans. If they are performing better than chances are you indexed your temp table badly. 3. Supporting applications that use CTEs in complex stored procedures can be a huge pain. My first reaction is fuuuuuuuuuuuuuug when I see a CTE in a sproc that I have to pull apart. Its so much easier to walk through code when I still have the data in the temp table to find the anomalous data issue. I only use CTEs for recursion and dynamic temp table creations. I might use them for really small datasets where I only need the data for a single pull. Otherwise temp tables are the way to go for me. 
If you think about it, that makes sense. The query optimizer is going to take the query with CTEs and figure out the best way to process it. Worst case scenario, it is going to pick a plan that spools the entire recordset (and will write to disk if necessary), reorders it, and then uses that to process further. Even better, the query optimizer might find a better way to process the query depending on a number of factors. By dropping the same data to a temp table, you never give the optimizer a chance. That being said, the optimizer isn't always right, and dropping to a temp table occasionally makes sense. Also note that this isn't unique to CTEs. The same is equally true of views and subqueries, both of which have their own advantages and disadvantages.
Surprise surprise... CTES 4 LIFE
CTEs have permissions advantages. You can use them without any kind of CREATE rights. Quick story: I was once working in a support environment on some DB2 where I absolutely could not create any kind of database object. CTEs became my answer to temp data, portability, and reusable code.
As with everything, there are tons of ways to accomplish the same thing. I use them in a "cascading" manner. With A as ( select from table where) , B as ( select from A where) , C as ( select from B where) Select from C where It helps me to think through selecting my sets. And I don't like having to define tables and then do stuff with them. I'm an on-the-fly kinda guy.
Excellent point. When I work in sybase (with no CTE support) I run into the same issues and solutions with subqueries/views. Often, dumping to a temp table is the only way to override the query plan generated by the server if it is making a mistake.
The lightbulb moment is when you realize sorting is a cultural thing not a mathematical thing if you aren't talking about numbers. How is a chinese phone book sorted? Databases manage this by specifying collation. Collations effect character sets and sorting. ASCII is not necessarily sorted the same as unicode and there are some collations that treat characters with umlauts as different than their plain brethren and some that do not.
Readability. Which is very subjective. Performance of CTEs, views, tmp tables, etc should always be benchmarked. Products vary, even across versions and the query optimizer may or may not handle a specific workload well in all cases. Write according to your preference, the test and optimize as needed.
This is my most common reason for using CTEs. Healthcare industry as well.
My take is that you shouldn't smack down a developer for using something you're uncomfortable with. Factoring and performance standards are worthy of smack downs. Cloning yourself across a team will stifle innovation and make you susceptible to competition.
&gt; but I don't think SQL Server has an equivalent Indexed Views. The Enterprise level has some additional features in addition.
Brent Ozar's 'Accidental DBA' resources are a good start for under-resourced DBAs. http://www.brentozar.com/archive/2013/07/announcing-our-free-accidental-dba-6-month-training-plan/ You can also try http://dba.stackexchange.com with questions and if you have the free time, attend your local community SQL Saturday events. http://www.sqlsaturday.com/
thank you for the reply!
This is a stand alone SQL 2008 R2 Server with SSRS installed. We've accessed security for this fella via the web interface as well as SSMS using the ReportServer URL. 
XML is sorta the IT boogieman, scary to start but pretty harmless when you look at it in detail. There are many horror stories from people creating hack solution because they didn't want to bother understanding it than just doing it right. I've personally started using it because it's easier to understand and maintains the context of the information better, infinitely more so than a CSV or flatfile. Glad you took the leap. :)
Comparing CTE to an indexed temp table is just apples and oranges. I see this comparison a lot, and I don't really understand it. They are 2 different strategies that mostly depend on the operations you are doing.
Thirded. I have to write some horrific queries that while doable without CTEs, are a huge headache to maintain that way. If you want to get good at complicated sql, work in health care. 
This right here. I have occasionally seen some huge improvements with CTEs over various other means of coalescing data and it always comes down to crazy stuff in the query plan. It won't necessarily execute the way you wrote it. Sometimes it performs better by drastically rearranging the evaluation of the predicates. Sql server seems to produce more effective plans when with CTEs against the logically same thing in one big query. Insisting on temp tables and the like is basically early optimization. 
FYI, your first example is exactly equivalent to your second. The first is just shorthand for the second.
What you did is called an implicit joint (as you are implying the join in your where statement).It is always better to use explicit joins. There's no efficiency gain either way, but it's easier to debug/read explicitly joined queries.
...and the first is easier to read and quicker to understand IMHO.
Definitely agree. I was just pointing out, for no reason other than potentially educating some one who might not know, that the db engine will expand an `IN` clause into the "OR column equals..." format internally before parsing out the query plan.
Not really, just looking for a legit reason to use them, which I have yet to see. Personal preference is not a legit reason. Outperforming #temp on small datasets is not a legit reason for what my company does.
This is a legit reason, however we don't have that problem where I work. All developers have full rights to create.
This goes back to personal preference, which I am seeing a lot in this post.
They are two tools that can be used to accomplish the same thing, how can you not understand the comparison? One is going to be superior to the other in any given situation. Typically CTE is better for smaller data sets, or data sets that do not benefit from an index - while indexed #temp tables will be superior for big data and complex queries.
We have code in our application that has 22 cascaded CTEs in a row. This is the hardest thing in our application to read for me. Also, if there is a problem in the 13th CTE, it isn't easy to debug. I would rather have everything in temp tables so I can query each one individually, instead of having to run the query every single time with certain things commented out. Just very, very annoying to debug/read.
The original join is on employee.empid with dependent.depempid This isn't evident in the morern join notation??
I had my tables switched. Edited.
I don't disagree with anything you said there. I guess we have only one source of data (raw files) to populate our database - and a properly designed database. So when we need to pull data from 40 different places, its properly architected tables, not multiple ETL sources. So I really don't have that problem. So yes, I can see how that is useful for you, thanks for answering the question!
&gt;and a properly designed database Not sure what you are trying to imply with this statement. There isn't anything improper about extremely complex designs. Sometimes, it is just necessary. Most people have trouble even grasping the enormity of the data involved in things like healthcare. If you even tried to put all that data into a simple table structure that was accessed in real time your operations would slow to a crawl. Thus the ETL work. We replicate the data regularly into a star-schema simple format that enables simpler access for reporting and analytics. However up-to-the-minute reporting still must be done via the transactional tables.
I would add One major benefit of a CTE over a #tmp table is the CTE does not need to utilize tempDB. This minimizes unnecessary I/O . A CTE will act exactly as a sub-query (check your execution plan if you don't believe me). Personally I think its a pain in the dick to troubleshoot multiple CTE tables, I'd rather have 100 joins and subqueries than CTE's. 
Essentially you will be the person responsible for the company's data model, and Enterprise data. You will design the schema's, translating requirements from the business executives into a domain model. You will work with internal software developers on applications that access that data. You will have to ensure the databases can scale effectively without performance bottlenecks.
I was implying that if someone had poorly architected designs, they may have to pull in data in chunks with CTEs instead of being able to just do it in JOINs. I was NOT implying that you had a poorly designed database, sorry if it came across that way.
Data Architect is probably defined differently from place to place. To me, the most important and understated part of my job is defining and enforcing standards. Projects can live and die by did they adhere to good standards or not.
&gt; They are two tools that can be used to accomplish the same thing, how can you not understand the comparison? Actually, they aren't used for the same purposes. Temp tables are used to temporarily store data to **share**. CTEs are inline **subqueries** that you can't share. Therefore, asking whether to use a temp table vs CTE (in my opinion) doesn't really make sense. Not to mention that you can't use a temp table everywhere you can use a subquery (like views or inline table functions). Yes, there's a subset of operations where you have a choice between a temp table or a CTE. But generally speaking, they both solve very different problems. You can't use a temp table everywhere you could use a CTE and vice versa. How would you pass information from one stored proc to another? A temp table is one way. A CTE doesn't even make sense. But how would you write a parameterized view (inline table function) with a lot of subqueries joining various tables? A temp table isn't even an option. A comparison that makes more sense is CTE vs subquery. Or temp table vs table variable.
The closest I've ever been to a useful enterprise architect (data or otherwise) is my current VP. Their most visible contribution (from my POV) is alignment of the data domains with the business interest and vision of the flow of the data through the business process, including a kind of a logical 'plug-in' architecture for absorbing new in-kind or different varieties of data through a (relatively) well-defined process. And, of course, the buzzwords are just that, process for process sake is a waste, so 'usability' of the architecture measured in specific result for us was to shorten up a new client ramp-up time from 1 year to 4 1/2 months.
yeah, madness, amirite? the good thing about standards is, there's so many to choose from what was your point again? 
It's more than just the data models...way more. It also entails developing and implementing policies, process management, and data standards to ensure the entire thing functions as desired.
The role has nothing to do with performance.
Knowing what can be done is what's important. For the "how," you'll learn the dialect you work with well. For others, there's documentation.
There's actually only two standards in all of that: - The `ROW_NUMBER()` based solution - `OFFSET x ROWS FETCH NEXT y ROWS ONLY`
Yes to your question. Good luck. You seem to have a good grasp. Regarding the additional IDs, my suggestion maybe was not clear. Rather than independant IDs for each table, three records would suffice. The table recordID , a child table recordID, and a reference to the table the child recordID belongs to. I am back at work now, not spending my days cruising Reddit, so forgive the delay in response. 
I really appreciated this additional context. Thanks!
What version of sybase and what type? Sybase ASE 15? Sybase IQ?
Yessir, you are right.
Completely agree, data structure has one of the largest impacts on performance and is the least flexible once implemented. An Enterprise Data Architect is essentially an engineer for databases; they don't much construction, but good luck building a skyscraper without them. 
I've been dealing with this in another database system, namely firebird. In that system they allow for hex and ascii literals in the ANSI sql statement. So something like REPLACE( field , x'0' , ' ' ) will work. Short of that, i've also had problems with non-unicode data being saved somehow to the database; and the frontend application barfing upon reading it. I've given up trying to fix it in the database; and resorted to creating a custom app that scans through the tables, selects for the string columns, try to interpet the strings into clean UTF-8, and update the database back. That strategy may work for you.
This is a discussion of "Enterprise Data Architects". It's best that you just lurk here.
was there a question?
Company I used to work for implemented a [batch-based replication system](https://github.com/wayfair/tesla) using change-tracking on 2008r2. IIRC, change-tracking will create change tables that store changes in a specified format and the tool above scrapes that data to create batches of changes to be applied to slave systems, but you could look through the source and find out for sure. Edit: me markdown pretty some day
Does the account you are using have the necessary permissions to perform these actions in the server?
Have you tried the KEEPNULLS hint for bulk insert?
mind you, if this is a JOKE, it's hilarious. :) 
That was my train of though as well -- IBM still offers certification on all versions though. And it seems like most books cater towards V9, which has been around since 2006/7. I was hoping there was something about this I just wasn't understanding properly.
Heh, yeah I know, Didn't mean to imply you were the OP. Just tucked it under the existing thread since we're all pretty much asking the same question: WTF is this! :) I thought for a sec it *might* be some new scripting language for Crystal Reports that I've never heard of.. Still, pretty hard to read eh? Maybe it looked better when he pasted it. 
It's Microsoft Retail Management System. 
well, i don't see any actual sql in there, so i can't help you
Hmm. Okay, thanks. I know that the system uses SQL, but I don't know SQL from a hole in the ground. 
Dude, I don't know shit about computers. I manage a small business that uses Microsoft Retail Management System. I'm being forced to try and learn how to customize reports because the software sucks. One of these reports came straight from that piece of shit software. All I want to do is reconcile my fucking credit card transactions, but this $50,000 software can't tell me which credit card transactions are from what register batch. If I have a discrepancy, I just have to do it all with paper and a pencil. It's frustrating as hell. You'd think Microsoft could put together a useable point of sale system. If I want to get support from Microsoft, we have to pay $7,000 to re-up our service contract. It's such horse shit. And I didn't buy the software, so don't tell me I should have chosen something else. I'm stuck with what I have. When I found one of these reports online, I thought it was going to be perfect. But, it doesn't list credit card transactions that were deposits on work orders. It only shows cash sales. It makes no fucking sense to me. I know this system uses SQL, but how it uses it is a mystery to me. 
That really sucks! You probably have some sort of Sql Server behind MS RMS. I see a few CASE statements in there, so it's sql based...to some extent. Maybe it's some sort of pseudo SSRS mockup code? If someone else here had that merchandise system, they might be able to help you. My company uses an old version of Epicor CRM, so I'll be of no help. /r/sqlserver might have someone who knows what you have here...or you might try tech net, but don't lead with a bunch of spaghetti code like that. Name your system and the reports you are trying to combine (I assume both were standard reports). You'll get more helpful responses.
Although this is some sort of reporting service, not SQL, I think the SQL statements can be reconstructed though it is tedious. For the first report I believe it does something like this: SELECT Register.Description, Batch.BatchNumber, Batch.ClosingTime, Batch.PaidOut, Batch.RegisterID, TenderEntry.Description, TenderEntry.Amount, Customer.Company, Customer.Name, Customer.AccountNumber, [Transaction].Total, [Transaction].Time, [Transaction].ReferenceNumber, [Transaction].Comment, [Transaction].TransactionNumber FROM Batch INNER JOIN [Transaction] ON Batch.BatchNumber = [Transaction].BatchNumber INNER JOIN TenderEntry ON [Transaction].TransactionNumber = TenderEntry.TransactionNumber LEFT JOIN Customer ON [Transaction].CustomerID = Customer.ID LEFT JOIN Cashier ON [Transaction].CashierID = Cashier.ID LEFT JOIN Register ON Batch.RegisterID = Register.ID We would need to similarly extract from the second report. Now, I think there is a possibility of being able to work out the table structure for your database, and even a way of combining the queries (particularly as it appears you can join VisaNetAuthorization to TenderEntry via TenderEntry.VisaNetAuthorizationID = VisaNetAuthorization.ID), but I would question if there is a simpler method here. Does your report writer offer any GUI, somewhere you can drag and drop fields onto a page and put in grouping, sorting and selection criteria? You may find this a more straightforward approach.
Why did it fail? Did you get a permission error or run out of log space? 
Access: no full outer join (without the lame workaround I built)
yeah, you're kinda screwed then. Even if we WERE able to help you with a little SQL, the next time you had difficulty you'd have to go through this again. What you really need is consultant who can come on site, find out what you want and design it for you from scratch or mod what you have above. This is not SQL problem per se, so this subredit might not be too useful to you. Frankly i've been doing VB and SQL for about 30 years and I can't make heads or tails of it based on what you've posted. For the best support, you have to look for a computer consultancy in your town and say I have this software named XXXXX it runs on Windows 7. It is a Point of Sale system It is based on this: (Ms Money?, Quick Books? something else?) I need help customizing some reports and maybe a little training. I'd like to make it do *this* but right now it's only doing *that*. You don't have to go back to MS necessarily and pay the big fees, unless this was a complete custom job written by them. There are small time consultants who are familiar with most popular tools out there. If you go to an established computer consultancy firm you'll pay for it. Easily $1000/day. A single man shop might do it for much less. I feel you pain man. It's hard to be thrust into your situation, (inheriting a poorly documented system that doesn't do what it should) and not have any IT support on staff. 
Whoa whoa whoa whoa. Let's kill the idea of this concatenated key you've got going. What you want is: create table Grades ( S_ID NUMBER(3) NOT NULL, COURSE_NO VARCHAR(6) NOT NULL, GRADE CHAR ); You'd then put your FKs on your Grades table, pointing back at the Student table and the Course table. Tables can have 0 to many foreign keys. The foreign key is always on the table that is using the ID, not the keeper of the ID. Since Students and Course are keepers of their respective IDs, and Grades just uses them, the foreign keys would go on Grades. But, let's talk about primary keys. Numeric keys, like using a [sequence](http://stackoverflow.com/questions/6389641/does-oracle-have-auto-number-data-type) will make the joins faster. If you're not worried about the performance, than don't worry too much about it.
&gt; Whoa whoa whoa whoa. Let's kill the idea of this concatenated key you've got going My Instructor named the key S_ID + COURSE_NO on the spreadsheet, so I'm not sure what he means by that. I don't think that table is meant to have 3 keys. Also, the key that has both names should be the primary key, and I can't do that if they're separate keys.
You can have a composite primary key consisting of more than one column. I'm currently on my phone, but look up composite primary keys, and you'll see what I mean.
I've created that, and my Faculty and Course tables are working now, but I'm having trouble creating the relationships. This is what I have for my student table: CREATE TABLE Student ( S_ID NUMBER(3), S_LAST VARCHAR(8), S_FIRST VARCHAR(6), S_ADDRESS VARCHAR(17), S_CITY VARCHAR(11), S_STATE VARCHAR(2), S_ZIP NUMBER(5), CONSTRAINT Student_S_ID_pk PRIMARY KEY (S_ID), CONSTRAINT Student_F_ID_fk FOREIGN KEY (F_ID) REFERENCES Faculty (F_ID) ); Oracle is saying "ORA-00904: "F_ID": invalid identifier".
You don't have an F_ID column on Student. Why are you trying to create that relationship? Faculty should go to course, no?
Faculty has the F_ID column. The relationship is between Faculty and Student, where Student is the "many" side. Edit: I guess I just answered my own question. I had to make F_ID in the Student table. The database was created with no errors. Thanks for the help.
But why is Faculty related to Student? Shouldn't it be related to a course? Also, you need to understand "relationships" and "foreign keys." Relationships are, for all intents and purposes, logical. They're meaning we assign to data. Foreign keys are a constraint on a database that ensure values in one column exist in another column in the database. That's all they are. Why would the student have a faculty ID column on the table? What does that mean? If we said Student #302 has a F_ID of 250, what does that mean?
&gt; But why is Faculty related to Student? Shouldn't it be related to a course? I don't know. It doesn't make sense to me either, but that was the assignment, so that's what I did.
yes and no, mathematically they are equivalent. 12.5 is 12.50. You don't have a data issue, you have a display issue. When you go to display it, use a formatter to add 2 decimal places. The oracle documentation is here. http://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements004.htm You want something like: to_char(you_number,'FM9999999999.99'); 
What database are you trying to do this on?
That would be true, and is called a composite primary key as you said.
Sounds to me like the author was implying that two *candidate* keys can together form a primary key, which is true, but, depending on the schema, could even potentially knock you out of 2NF. How you're interpreting it also makes perfect sense, though, too. I'd contact your instructor if possible to see if you can get the ambiguity cleared up.
a PHPMyAdmin database
So most likely MySQL?
Yes, it is MySQL
I'm guilty of making a five part primary key...
Yep. At what point do you quit being relational and just go star schema?
so you want **FORMAT(numbercolumn,2)** note this will also use commas for thousands
yes, it's true.
[Why?](http://media.tumblr.com/tumblr_m726858b4m1rsy50k.gif)
I ran that with this query: UPDATE `UNIVERSITY` SET `note_needed` = FORMAT(`note_needed`,2) but I'm pretty sure I've done that already as it returned 0 rows updated
you might be misunderstanding that the FORMAT function is actually a display function as /u/abdadono1 said, you have a display issue if you want to store the formatting, you need a character datatype, not a numeric datatype, but then with a character datatype, you introduce all sorts of other issues 
I passed the Querying exam and am studying for the Administration one at the moment so I don't believe I have a frame of reference to rank the other two exams in terms of difficulty; however, I will say the Querying exam was not easy; it wasn't terribly hard either rather it was challenging. I felt like they purposefully obfuscated the code examples turning what should have been a relatively easy query into a bit of a monster. You need to have a firm grasp on syntax, best practices, and the finer details about how the different parts of a complex query interact with other parts. Even though the Querying exam was tough, I feel the recommended Training Kit did a good job of preparing you for the exam. In reading the book cover to cover, I not only learned a lot but I also learned how much more there is to know. ;) Good luck!
yes- no effect
my guess, too, but no support from DBA group
yeah....i emailed him the script to play with, he didnt seem too happy to support it i would just like an Integration Service package, but its not installed.
no msg, doesnt fail, the view fails (0 rows)
I love ReSharper for our C# work. This could be a nice compliment on the SQL side.
If you want to display a date, you should cast it to a varchar to ensure a proper format. Use the [CONVERT](http://technet.microsoft.com/fr-fr/library/ms187928.aspx) function.
that wont mess up future entries into that column?
What do you mean exactly ?
the column is a list of test drives, listed as ex:"2014-06-01" but when I put them in the DNN template I am using, it comes out as "06-01-2014 12:00:00" So I understand what you mean by converting it to nvarchar, but I wanna make sure that after I do it, the column will still be the same as before in SQL
Is there a data difference between the 2 environments? If the tables are populating, and the data they are populating with is valid, then the bulk copy succeeded. That really only leaves join conditions and where statements in the view query that are causing there to be no valid records for the query because of data differences between the 2 systems. Did you try running the SELECT from the view on its own (as a query) to validate that it gives results?
Well, as long as you only "SELECT" the values, all the content in the database will stay unchanged. And that's exactly the point of deferring the format operations to your presentation layer : the db stays in charge with the data storage, and your layer can then display them in the format you want.
GARGH! you're right...forgot to get data for active_users....it was empty
:)
When it's a BI system...
This is great news... I've already been using PyCharm (and more recently IntelliJ) for the database console. It works fairly well, though has a few issues - like very long refresh times for the intellisense in a large schema (it's much slower than sql prompt), and a few quirks around some JDBC drivers for MSSQL. But the cross platform, multi-db vendor, and support aspects trump all of that easily. The best aspect of using any Jetbrains product is that they're actually responsive to bug reports. I've already had them fix several issues I've run into with the database console, generally within a few days of reporting them. On the flip side, I've got bugs I've opened with SSMS/SSIS that still don't even have MS comment on connect several years after the fact.
W3schools.com
I used Lynda.com and w3schools to get off the ground with SQL.
http://philip.greenspun.com/sql/
Not hugely familiar with SQLite, but on every database I've worked with, you need to specify you column names with an INSERT INTO, and run a separate one for each record INSERT INTO RESTAURANTS(name,address,id,owner,lattitude,longitude)values("Malek Al Shawarma", "Urdesa Central Ave. Victor Emilio Estrada y Guayacanes", "2384162", "ARABE", "-2.165357", "-79.911271") 
Also, check the data types for your records. Decimals and INTs shouldn't require the quotes.
already specified the columns, same problem... last two params I rather keep them as string for several other issues with the app I'm developing...
sqlfiddle
Yeah, you need to do one record at a time.
SQLite 3.7.2 and the statement I'm building is supported from version 3.7.11... Hmm... Oh well... Thanks for the help!
&gt; but on every database I've worked with, you need to specify you column names This isn't necessary in SQL Server if the columns aren't in the same row, but highly recommended.
I know older versions of SQL Server, you couldn't do more than one row at a time. In SQL Server 2012, you can. It's probably a similar situation in SQLite.
Since you didn't specify DBMS I did the below solution for MSSQL; others have similar solutions. *** &gt; Get open session list *** EXEC sp_who2 *** &gt; Kill a Session (T-SQL) / SPID From Above *** KILL SPID WITH STATUSONLY --After verifying KILL SPID *** The advantage of WITH STATUSONLY is that it provides feedback if transactions are in a rollback state. Be careful about what you stop with this though Microsoft highly recommends avoiding the following Command list: * AWAITING COMMAND * CHECKPOINT SLEEP * LAZY WRITER * LOCK MONITOR * SIGNAL HANDLER 
Your schedule data is XML, so here's an example of how to extract data from an XML sample using XMLTable : select NUMBEROFTIMES, PERIOD, TO_TIMESTAMP_TZ(RULESTARTDATE_STR, 'YYYY-MM-DD HH24:MI:SS.FF TZR') as RULESTARTDATE from xmltable('/scheduler.rules.FrequencyRule' passing xmltype('&lt;scheduler.rules.FrequencyRule&gt; &lt;numberOfTimes&gt;-1&lt;/numberOfTimes&gt; &lt;period&gt;86400000&lt;/period&gt; &lt;ruleStartDate&gt;2013-09-09 01:00:00.0 EST&lt;/ruleStartDate&gt; &lt;/scheduler.rules.FrequencyRule&gt;') columns NUMBEROFTIMES integer path 'numberOfTimes', PERIOD integer path 'period', RULESTARTDATE_STR varchar2(100) path 'ruleStartDate' ) t | NUMBEROFTIMES | PERIOD | RULESTARTDATE | |---------------|----------|-----------------| | -1 | 86400000 | 09/09/2013 01:00:00.000000000 -05:00 |
Oh my god you're amazing. How didn't those guys know that? That was it, thanks so much.
A couple of additional tips to build on this. One, you generally don't want to kill any spid of 50 or under, as those belong to the system. And two, you can get some insight into what a particular spid is doing (before killing it) with DBCC INPUTBUFFER(spid) This will give you a snapshot of the contents of the batch that spid is running. There are better ways to do this on 2005+ but that's beyond the scope here.
If you came across connection pool stuff in your search, was in the context of .NET? Because connection pools are not a MS SQL thing. The connection pool running out means there is an application with too many connections open for it to open another one. This is usually caused by very high concurrency or exceptions happening between the connection being opened and it being closed and there is no clean up code to close the connection. The real question is what/who is leaving those connections open? If you're talking SQL Server, you can open SSMS, right-click on the server and choose Activity Monitor to see all the current connections and see if they are actually executing anything. If they're just sitting there and appear to be idle you can get some more information by scrolling over to the right and seeing what computer the connection came from. Remember, the activity monitor only takes a snapshot of what is happening every few minutes.
And even then... 38 connections is not much at all and shouldn't be anywhere near any kind of limit right? 
Where you have 'AMERICAN', put the name of the column the info is in. It will then use the contents of the column as the reference text.
thanks, I tried that and it's not giving me any rows back. I don't understand how to integrate two tables without a join using contains. Here is my full query. table b is my master table and table a is the table with only supplier names in it that I want to match with supplier names in table b SELECT score (1), a.supp_nm, b.supplier_name, b.supplier_code FROM f_supplier b, navi_supp a WHERE contains (supp_nm, 'fuzzy(supplier_name, , , weight)', 1) &gt; 0; So here I am trying to search supp_nm (in table a) within supplier_name column (in table b)
Also - I just ran the following query again: SELECT DB_NAME(dbid) as DBName, COUNT(dbid) as NumberOfConnections, loginame as LoginName FROM sys.sysprocesses WHERE dbid &gt; 0 GROUP BY dbid, loginame And this time - I got 47 connections. Not sure why it keeps going up. Is this related to my issue?
I figured it out! WITH sc AS (SELECT a.supplier_code, a.supplier_name AS c_supplier, b.supp_nm AS n_supplier FROM f_supplier a, n_supp b) SELECT supplier_code, c_supplier, n_supplier, UTL_MATCH.jaro_winkler_similarity (c_supplier, n_supplier) pct_match FROM sc GROUP BY supplier_code, c_supplier, n_supplier HAVING UTL_MATCH.jaro_winkler_similarity (c_supplier, n_supplier) &gt; 87 ORDER BY 4 DESC;
Yup, the SQL standard has always said you can leave the column names out, and every RDBMS I know about follows this. But, as you said, it's highly recommended not to use this shorthand.
Consider answering your own question, maybe someone will come across it in a few months! :)
My 2 cents : Ditch the definition of those 65 screw ups (if it wasnt a screw up you'd have 1 schema). Define a schema that fits your business requirenments. I'd do that to large extends independant of those 65 different schemas, and than migrate the data to YOUR data schema. If you are looking to press a button and have 65x crap that you got sent in 65x excel file, into one coherant data model, forget about it. It won't ever work. I've done a lot of "data managment" which was exactly that problem many years ago when i started out on the DB trail... trust me, you are going to import 65 different excel files, after looking trough how they are structured for each and every one of them. If you take shortcuts here, you are aming at your own foot with a quite large caliber. 
The thing is, I either need to insert the count in beginning or update it after the initial insert. Plus, its per row. For example: 0001 CS101 A 1 0001 CS102 A 2 0001 CS103 B 3 0002 EN101 A 1 0002 CS101 C 2 And so on
Right. The default is 100, but could be overridden.
Those could be normal. Also every connection/query window you open adds to the count. There really are very few instances when you can't tell who connected and from where. You should be seeing all of that. There are also a few system connections that are always open. Where do observe this error? Event log on a server? Pasting the actual exception message (without the stack trace) would help. SQL error messages are usually fairly precise. Also, did you check the max connections setting in the server properties? What is the value?
I think what you are looking for is the row_number function select *, RN=ROW_NUMBER() OVER (PARTITION BY (column 1 name) ORDER BY (whatever order should be)) if order is solely by the first column you have here, then you can just drop the partition by part and order by the first column
This is correct for that expected resultset (assuming you're attempting to generate that last column) PARTITION BY is required, I think he meant you can drop the ORDER BY clause edit: OP, are you trying to write this count back to the original table, or are you just trying to display the result?
 select changes.uuid, changes.name, count(*) from (Select distinct UUID, name, value1, valuen, shasum from table where date between ? and ?) changes group by changes.uuid, changes.name having count(*) &gt; 1
The connection pool limit is specific to an application, so your e-mailer program can have up to 100 concurrent connections by default. The number of connections to the SQL server from other apps, like SSMS, shouldn't matter unless the server is under memory pressure. If possible, you need to get a snapshot of connections around the time the application usually fails. You can do this by taking the script you were running before and putting it into a SQL Agent task that runs every minute or so around the time the e-mail program fails logging the results of the query to a table somewhere. If this is an in-house application and get to the code, I would make sure someone reviews the process of doing the SQL connections because it is appears to be done incorrectly. If it is in fact correct and it really is consuming that many connections due to concurrency, then adding the Max Pool Size option in the connection string should be done to increase the size of the pool. Secondarily, to ensure that the connection issues have nothing to do with SQL server, ensure that the Max Server Memory option is set something reasonable on the SQL Server so that the OS has enough memory to function.
87/100. The ERD was flawless. Thanks chaps :)
Make sure that closing the connections in code always happens. [Good article on it here](http://blogs.msdn.com/b/angelsb/archive/2004/08/25/220333.aspx) As for changing the timeout, you can do it from a query(example below sets the timeout to 30 seconds): EXEC sp_configure 'remote query timeout', 30 ; GO RECONFIGURE ; GO 
Thanks!
Make your joins LEFT OUTER JOINs. You were correct in your first guess: followings.followable_type can't be both = 'Outlet' and = 'Person' at the same time when using an INNER JOIN. (Just using 'JOIN' implies 'INNER JOIN', so there's no difference between them.)
I am assuming the result set is going to be rather small, I would just use a cursor for loop. CREATE OR REPLACE TRIGGER CleanupAfterCategorieDeletion BEFORE DELETE ON CATEGORIE FOR EACH ROW DECLARE v_deletedcatname CATEGORIE.NAAM%TYPE; v_abonneeid MAILABONNEE.ID%TYPE; v_abonneefirstname MAILABONNEE.VOORNAAM%TYPE; v_abonneelastname MAILABONNEE.ACHTERNAAM%TYPE; v_abonneeEmail MAILABONNEE.EMAILADRES%TYPE; v_mailbody nvarchar2(200); BEGIN /* get name of category that is being deleted */ v_deletedcatname := :old.NAAM; /* Get id of all abonnees that are subscribed to the category that is being deleted */ FOR cur_abonee in ( SELECT MAILABONNEEID FROM CATEGORIEABONNEMENT WHERE CATEGORIENAAM = v_deletedcatname; ) LOOP /* Get abonnee name and email */ SELECT VOORNAAM, ACHTERNAAM, EMAILADRES INTO v_abonneefirstname, v_abonneelastname, v_abonneeEmail FROM MAILABONNEE WHERE ID = cur_abonee.mailabonneeid; /* Concat multiple strings to create the body of the mail */ v_mailbody := CONCAT('Beste ', v_abonneefirstname); v_mailbody := CONCAT(v_mailbody, ' '); v_mailbody := CONCAT(v_mailbody, v_abonneelastname); v_mailbody := CONCAT(v_mailbody, ', uw abonnement is opgeheven voor categorie '); v_mailbody := CONCAT(v_mailbody, v_deletedcatname); v_mailbody := CONCAT(v_mailbody, '.'); /* Send mail to abonnee with information about deleted subscription */ sendMailAbonnee(v_mailbody, v_abonneeEmail); END LOOP; /* Delete abonnee subscription to deleted category */ DELETE FROM CATEGORIEABONNEMENT WHERE MAILABONNEEID = v_abonneeid AND CATEGORIENAAM = v_deletedcatname; /* Clear categorienaam for affected Nieuwsbericht rows */ UPDATE NIEUWSBERICHT SET CATEGORIENAAM = '' WHERE CATEGORIENAAM = v_deletedcatname; EXCEPTION WHEN NO_DATA_FOUND THEN v_mailbody := ''; END; As a side note, for send mail procedures I find it most helpful to actually just insert the email into a table and then circle back around with a job to push the emails. This makes sure if there was an error in the processing then emails were not sent on actions that did not see completion. This just takes advantage of commit/rollback.
OK, but then why is it working where `outlets.id` `= commentable_id` and `= statable_id` at the same time? EDIT: The LEFT OUTERs do produce the desired result. But I just can't figure out what the difference. EDIT 2: I just tried changing the `outlets` to LOJ's as well, and that returned a list of results that is approximately double the length of the `outlets` table itself. So, while it's progress I don't think it's a general solution.
 SELECT CUSTOMER_NAME, ADDRESS, COUNT(DISTINCT CUSTOMER_NAME||ADDRESS||CUSTOMER_ID) IDS FROM &lt;TABLE&gt; HAVING COUNT(DISTINCT CUSTOMER_NAME||ADDRESS||CUSTOMER_ID) &gt;1 GROUP BY CUSTOMER_NAME, ADDRESS;
But in the first one am I not also setting a restriction on `outlets.id`, the main table? Why is that not also being treated like a `WHERE` clause?
In the first one you're matching records in `outlets` with records in two tables that have some restrictions placed on them. In the second one you're matching a restricted set of `followings` (with no members) on records in two tables.
It does, and I think this is the right approach. I realized that my choice of `statistics` was giving "misleading" results because each `outlet` has a `statistic`. Which caused this to return a full copy of my `outlets` table. However choosing a better test table (e.g. `followings`) it's actually yielding consistent, complete, and unique results.
So, here's an explanation of how these reports work and how they relate to SQL. I wish I understood this stuff more. http://www.mindfiresolutions.com/working-with-ms-rms.htm
FTFY. Index all the tables and all the columns.
Be careful. If you index too much... index takes up space. Run out of space.... Obviously depends on RDBMS and what data you work with. We had a database failure near year start cause by indexes taking up too much storage space. 
 update table set &lt;rowcount_field_name&gt;=ROW_NUMBER() OVER (PARTITION BY PersonIdentifier ORDER BY (whatever order should be)) from table 
If you just want to get a list of all tables, regardless of schema, you can start here (should be platform agnostic): select TABLE_NAME, COUNT(distinct TABLE_SCHEMA) from INFORMATION_SCHEMA.TABLES group by TABLE_NAME from that, take your list of tables that you want to keep and do: select TABLE_NAME, COLUMN_NAME, COUNT(distinct TABLE_SCHEMA) from INFORMATION_SCHEMA.COLUMNS group by TABLE_NAME, COLUMN_NAME Figuring out your duplicates that don't match exactly will be a manual effort, but this will at least get you started with the most common objects.
lmgtfy
You might look into [Server Level Auditing](http://technet.microsoft.com/en-us/library/cc280386(v=sql.110\).aspx)
This query did not find fact that carol only began to exist, but i can work with this because it is fast. Thank you so much!
I just realized that there is a problem. If it changes then changes back, it will only count once. I look at it some more later and hopefully have a better solution.
you seem to have gotten further than i did on this. here is OP's response to one of my posts from above. It may give you more info.. &gt;So, here's an explanation of how these reports work and how they relate to SQL. I wish I understood this stuff more. http://www.mindfiresolutions.com/working-with-ms-rms.htm
You're *maybe* looking for [PIVOT: http://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx](http://technet.microsoft.com/en-us/library/ms177410%28v=sql.105%29.aspx) Edit - You can also do this: SELECT your_table.phone_num, MAX(CASE WHEN t.RowNum=1 THEN t.category END) as Pn1, MAX(CASE WHEN t.RowNum=2 THEN t.category END) as Pn2, MAX(CASE WHEN t.RowNum=3 THEN t.category END) as Pn3, MAX(CASE WHEN t.RowNum=4 THEN t.category END) as Pn4, MAX(CASE WHEN t.RowNum=5 THEN t.category END) as Pn5, MAX(CASE WHEN t.RowNum=6 THEN t.category END) as Pn6, MAX(CASE WHEN t.RowNum=7 THEN t.category END) as Pn7, MAX(CASE WHEN t.RowNum=8 THEN t.category END) as Pn8, MAX(CASE WHEN t.RowNum=9 THEN t.category END) as Pn9, MAX(CASE WHEN t.RowNum=10 THEN t.category END) as Pn10 FROM your_table INNER JOIN (SELECT phone_num, category, ROW_NUMBER() OVER (PARTITION BY phone_num ORDER BY category) AS RowNum ON your_table.phone_num = t.phone_num FROM your_table) AS t GROUP BY your_table.phone_num 
&gt;Too many schemas, tables, and differences to do it manually (Manually going through every field in every table in every schema) I wouldn't say that 65 tables is too many. It is probably the only way you can be sure you won't automatically mess up your data. How do you eat an elephant?
This is great. I was trying to do something similar to this earlier, but I couldn't quite get it. Thank you!!
Well of course. Just allocate me a nice fat slice of that FusionIO and we'll be best friends!
Truth is, people write crap code and especially crappy SQL code. I have heard people say ,"You don't need to write good code because computers are fast enough to run anything nowadays". It is jaw dropping stupidity at its finest. I have seen the following code written: Insert into Sometable Select primarykey, 'a' as keyvalue1, 'b' as keyvalue2 from table1 tab1 where primarykey not in (Select primarykey from table1 where somevalue = '1' and primarykey = tab1.primarykey) and primarykey in (Select primarykey from table1 where somevalue = '2' and primarykey = tab1.primarykey) Insert into Sometable Select primarykey, 'b' as keyvalue1, 'a' as keyvalue2 from table1 tab1 where primarykey not in (Select primarykey from table1 where somevalue = '2' and primarykey = tab1.primarykey) and primarykey in (Select primarykey from table1 where somevalue = '3' and primarykey = tab1.primarykey) It's absolutely f'n mind blowing. 2 inserts that could be one select/insert. Both statements have correlated subqueries looking for a match on the primary key in the same table. It'll run fine if you have a small database. Anything over 100,000 rows and you will see exponential slowness on the size of the tables. Always try to aim for fewest select statements as possible to get the results you are looking for. Use CTE's or table variables with primary keys (clusters the results in memory). No developer writing DML code or reports should ever allocate a cursor, EVER! On the opposite side of the coin. Storage Admins vary greatly. You have the ones that understand MPIO,vluns and storage and then the ones that know enough of MPIO, VLUNS and managing volumns to get something barely working. I have had SAN experts come in on projects and say ,"You don't need to split your luns for SQL! This HP EVA 4400 aggregates data across all disks in the controller!" No SAN experts, you do still. They are lun aggregates. If I have a 2TB partition across 20 disks and my database is 400gb, my database is striped across 5 disks, not 20. They aren't file aggregates but lun aggregates. Split the luns and IO improved by 350%. Who would have thunk that? Generally what wins the argument is pulling statistics for wait times from SQL server. If PAGELATCH_IO is at the bottom of the list, there isn't a disk problem.
I have the strangest boner right now
By sharing it with your family?
That's what I'm here for.
One bite at a time 
I don't care if it changes and then changes back. These state changes are going to be examed over months, so if someone tries something for a day or two then changes back, I don't care. It is the long term value change that I care about. I also thought finding things that never existed on day y and now exist on day x would be easy, but that is no simple query either. I only added the shasum as a sql trigger, it calculates the shasum from value1 + value2 ... valuen. I thought examining one hash would make the query quicker. But I am apparently not saving that much. 
also, i don't necessarily need between two dates, but the delta of one date and another date. I am the php dev on this project. sql is my weak point, I don't know how you guys do it. 
I still can't convince my "Data Protection" team that they should turn off DDupe for our compressed backups &lt;sigh&gt;
A million times this. Despite datasets growing more and more the average developer doesn't seem to understand basic query does and don'ts and would rather start fucking about with NoSQL and Memory Resident solutions instead of just fixing their god damned shitty code.
should it not be INSERT INTO table1 (column1, column2) values ('data1','data2') etc
Maybe something like this using a Union where you'd get all the items that aren't in each others tables select gpromed.itemnmbr as ItemNumber, gpromed.itemdesc as ItemDescription, impromed.productname as ImpromProdName, impromed.productcode as ImpromedProdCode from gpromed left outer join impromed on impromed.productname=gpromed.itemdesc where impromed.productname is null union select gpromed.itemnmbr as ItemNumber, gpromed.itemdesc as ItemDescription, impromed.productname as ImpromProdName, impromed.productcode as ImpromedProdCode from impromed left outer join gpromed on gpromed.itemdesc = impromed.productname where gpromed.itmedesc is null
Woah... the results I got for this are frightening but that worked perfectly. thanks.
Sure, sorry about the formatting though.
It's okay. I see that stuff well. You made one error, if you wanna locate it yourself, that would be fun. hehe. It's the same error I make constantly. It's in the very end of your query. e and m. hehe. I was just coming to the conclusion that I needed to do this. Still though, thanks.
http://i.imgur.com/kGARb0e.jpg
I ese it nwo!
If you're going to be using different databases in the future this helps a little bit. Now I just read that it's DBO's and not databases. I'll leave this here, but for DBO's just remove .dbo from both queries and it should work the same. declare @db1 as varchar(20), @db2 as varchar(20), @exec as varchar(8000), @table as varchar(30) set @db1 = 'db1' set @db2 = 'db2' set @table = 'table' set @exec = 'select '''+@db1+''', * from '+@db1+'.dbo.'+@table+' union all select '''+@db2+''', * from '+@db2+'.dbo.'+@table+' ' execute(@exec) Change the * to the tables you want Change the union to an except and remove the ''+@db+'' if you want to see the unique differences There's a lot of options depending on what kind of analysis you are doing.
Thanks very much, I do appreciate this.
You can put an alias on tables. See the below example SELECT impromed.productname as ImpromProdName, impromed.productcode as ImpromedProdCode FROM impromed vs SELECT i.productname as ImpromProdName, i.productcode as ImpromedProdCode FROM impromed i You can also put spaces in your column aliases if you put them in brackets (or apostrophes) SELECT i.productname as [Impromed Product Name], i.productcode as [Impromed Product Code] FROM impromed i I use this when I'm handing something to clients that are familiar with the references in the interface of our software (they export SQL to Excel and work from there when we do this).
This is pretty much what I am doing at the moment. Appreciate it!
fetch_row will return an enumerated array, thus the id would be in $row[0] see: http://www.php.net/manual/en/mysqli-result.fetch-row.php
What you're looking for is a unique constraint. ALTER TABLE customers ADD CONSTRAINT uc_city UNIQUE (city) That's for SQL Server. Not sure what db you're using but it should be similar.
I edited it - I changed the values when I pasted it. Ignore it, wasn't the issue
It was in $row[0] - I must be tired. 
Hah, happens to the best of us. I surprise myself with the dumb mistakes I make with PHP when exhausted
Just completely replaced.
truncating.
I'd be interested in hearing the story about how you can't use TOP.
Right click database, choose **Generate Scripts...**. Select your tables. Click the button for advanced options and set **Script Data = True**. Works great if it's just a small amount of data. Example: http://blogs.msdn.com/b/davidlean/archive/2009/09/20/tip-ssms-script-your-entire-table-including-the-data-a-hidden-gem.aspx 
Support for TOP(expression) wasn't added until SQL Server 2005 I believe SQL Server 2000 does support TOP, but only as an integer or percent, ie, no paranthesis Just use select TOP 5 person_id etc
maybe add a link on the sidebar so we don't forget it?
You can do that, but I see most design do this: 1. Make the primary key an identity key (an integer that auto numbers) 2. Create a Unique Key constraint on the key YOU WANT to make the primary key.
Smart :) - thanks, done !
Generating SQL scripts is likely going to generate SQL of the dialect that that specific RDBMS uses. It's not going to be portable, sometimes not even portable between different versions of the same RDBMS. It is possible to create a [Linked Server in SQL Server to an SQLite DB](http://www.mssqltips.com/sqlservertip/3087/creating-a-sql-server-linked-server-to-sqlite-to-import-data/) which you can run `INSERT` statements and the like through to pull the data in, but you may need to recreate the schema completely. For MySQL, I would probably export each table to CSV and then import them in MySQL Workbench. You can do the same kind of think with SQL Server, too, but this method is more error prone. 
Good to see something like this finally come to fruition, there's been a pretty large demand for it. Best of luck! edit: After going through a few of these, I have a few points: * a sandbox mode would be quite nice so that we have an environment to play in * The English is a bit off in some places, you might want to consider hiring a native speaker to maybe proofread it? * Maybe try changing the theme a bit? Filtering down searches to try and find your ideal girl seems kinda weird and it singles out a lot of people 
Personally, I'd like a tab-indent chatbox for this to really be useful, if you are to showcase table SELECT statements. Nobody really writes SELECT Name, Gender, Job, City, Phone_Number FROM Clients The "right" way to do it would be SELECT Name , Gender , Job , City , Phone_Number FROM Clients Reddit doesn't even have a tab indent chatbox =( 
Thank you so much
Thanks for the terrific resource :)
In the section where it was checking to see if the girls job was in the set of jobs {'artist', 'musician', 'designer'} there was only one acceptable order, this may cause some confusion if someone decides to not copy it word for word
Thanks a lot! 1. Well, we thought about sandbox mode and founded pros and cons (50/50) = ) That's why decided to leave 'linear gameplay' 2. Absolutely agree. We are not native speakers. Will fix our text soon 3. It's just a story for fun (I belive that people understand, that it's a bit impossible to find a girlfriend via SQL query). But we will add one more lesson soon with another (more realistic) case. Thanks a lot for your feedback 
Thank to you ;)
Thank you. Looks great
Fixed, Thanks a lot
Btw, We add tabulation + multiline input + you can use 'enter' key to proceed your query if it ends with ";" symbol ;-) 
I hadn't considered postgreSql. I use mssql at work and mysql for Web Dev projects and the like because it seems well suited to lightweight Linux servers. I'm at work now but I'll post some of the sql when I get home. 
PostgreSQL works anywhere that MySQL works. The biggest benefits I can think of off the bat are out-of-the-box ACID compliance with massive recovery ability (think InnoDB master files for every DB with binary logging; but better, on initial startup) and CTE's (Common Table Expression(s)). CTE's also work with SQL Server, and for that matter 'SQL-wise' MSSQL/PGSQL are nearly identical in a lot of ways, so most queries just port straight over. I strongly suggest taking a look at PGSQL especially if you do *anything* with full-text searching, or have a need for a massive amount of data in a single column (hstore, json, and the upcoming jsonb are really wonderful data-types). MyISAM is faster in some situations, but if you're using InnoDB and have a need for ACID; just use PGSQL and save yourself a lot of headache. If by 'Web Dev' you mean something that only needs speed and doesn't matter if you lose a bit of data (e.g.: Wordpress), then don't sweat it MySQL is a fine choice. :) I'm not trying to knock it but after being a psuedo-DBA for a few months now I'm just floored by what I didn't know that makes my life so much easier with PGSQL and it's amazing functions. A lot of the other popular DBMS (SQL Server and Oracle espeically) offer a lot of the same functionality, but nothing is quite as nicely wrapped up in one wonderful package as PGSQL is. Also I should be able to help you a lot with that SQL. I just finished converting a DB2 database to PGSQL so I have a lot of regex fresh in my mind....
The problem I have with NoSQL solutions; referring to MongoDB as it is something I have built a small test system for, is the utter lack of enterprise/security structure that comes standard with almost all RDBMS; Instead focusing on out-of-the-box functionality. Personally I feel like it is an attempt to make a broad solution for very specific problems and are just riding the wave of Buzzword Managers. It has a place, but it shouldn't be part of every project.
I have no direct experience with this problem, but NTLM will not pass credentials (hence the second request) you likely want to investigate using Kerberos security as it allows for double-hop authentication.
You really don't want to use NTLM if you want to stop users from being prompted. You want to use Kerberos. Kerberos is quite easy to set up. When a user logs onto the domain they are given an authenticated token that will allow them to access resources without requiring them to re-enter their credentials. Add the following to your &lt;Authentication&gt; element in the RSReportServer.Config. &lt;AuthenticationTypes&gt; &lt;RSWindowsNegotiate/&gt; &lt;RSWindowsKerberos/&gt; &lt;/AuthenticationTypes&gt; Next step is to see if you have an [SPN setup](http://social.technet.microsoft.com/wiki/contents/articles/717.service-principal-names-spns-setspn-syntax-setspn-exe.aspx) for the SSRS box. SPN's are service principle account names. Normally, these are created at install for most services such as SQL. However, for SSRS and some where circumstances.... these aren't always created. Most of the time the SPN will be setup using the SSRS service account. setspn -s host/ssrsservername DOMAIN\ServiceAccountName Make sure you set up one with the fully qualified domain as well setspn -s host/ssrsservername.domain.com DOMAIN\ServiceAccountName The -S operator first checks to see if there is an SPN setup for that host. If there is it will give you a message saying ,"Duplicate found. Object not updated". You can use setspn -L DOMAIN\ServiceAccountName to see all SPNs being delegated under than username. Make sure the service account is set up for delegation. [When you view the account in active directory check to make sure the following is set.](http://blogs.technet.com/blogfiles/askds/WindowsLiveWriter/20fb3a1ffed5_E0B5/image_thumb_7.png) [Not all browsers support kerberos and NTLM by default.](https://www.pingidentity.com/support/solutions/index.cfm/How-to-configure-supported-browsers-for-Kerberos-NTLM). However for the most part, most of the browsers will look at the internet configuration in control panel. Your site must be in the "Intranet" group and cannot be in the "Trusted Sites" group. Make sure you roll out a group policy that adds the SSRS site to "Local Intranet". Then roll out a group policy to unmanage the local intranet sites. This will force the entry into local intranet but then it can be managed by the users moving forward. I have done this set up so many times I could do it in under a half hour. It takes about a day for proper replication of the group policy and its rollback. Doing it for the first time can be a learning experience.
Thanks. I'm going to give this a crack. I did see this on one of the search results and wanted to investigate a little more before I implemented. I appreciate it. I'll let you know if it does the trick.
Yeah, a number of search results mentioned the double-hop issue. I wasn't entirely sure whether my situation qualified. Thanks for the heads up.
awesome thanks!
You're absolutely right, it does have its place, and it should not be a part of every project. I think what happens is that so many people just love the idea of a schema being defined entirely by the code, instead of defining a schema along with the code, or before the code. It makes some things easier. RDBMS have been in development and use for the last ~40 years. They're tried and tested, and I'm sure a number of fantastic minds dedicated to studying data storage have examined all different methods of storage. They're two different tools that do similar things, in my opinion. You can hammer a nail in with a ball peen hammer, but that's not what it's meant for and you'd probably be better off with a claw hammer. It goes the other way, too. Just use the best tool for the job.
Perhaps we can build an FAQ?
Thanks for the help, the elaboration, and the story. The worst is when people will allow their ego to get in the way of better, more efficient systems and processes. We're actually dealing with a nearly identical situation here because some of our networking guys (which I am one of) improved upon a SQL query someone else wrote. It improved search times dramatically, but they're mad 'cause... who knows. Anyway, thanks again. Once I take care of some other things, I'm going to give your suggestions a whirl and then I'll update. So if it works, this thread won't end up being one of those infuriating search results with a either no conclusion or a "NVM FIXED IT".
Agreed. It's usually with sad resignation that I resort to this because the next best option is a ton more work. Usually I'll use a mix of regular table and the "entity-attribute-value" pattern. Put the relational stuff in the regular table, with a bunch of them null if they have to be, and then relate your attribute table back to your main table for the variable schema parts. This way you get easy querying on the parts you're most likely to query on and you can get let the software layer deal with the complexity of constructing your complete entity. If you're frequently joining on the variable part, you should probably rethink your schema.