If you plan to work on Microsoft ambient, take a look at their sql server training kits. You can either buy the book itself from amazon or just google for the pdf file.
Create your own projects for sure if you can think of any. Depending on what vendor you pick, will change your path. But its all very related. 
Python has been emerging in its uses for machine learning. SQL has largely stayed the same since 1986. Not a lot of young people getting excited about it. Second, SQL is a tool that webdev / python / node.js / R / SAS can all use, it doesn't create pretty results on it's own, it is essentially a powerful hard drive. More difficult to get people excited then when you get to show them in 20 minutes I went from a few lines of code to this awesome 3D visualization. Just my 2 cents though... 
In your WHERE clause you‚Äôve misspelled caliber as cailber btw 
Yeah I see your points. I just think a lot of people are missing out by not caring as much about SQL/DBs üòû 
Oh man.. lmao damn fat fingers getting all excited lol. Thank you! 
What about: SET @MACHINE_NAME = %COMPUTERNAME% (assuming this is windows)
In SQL Server, it would be SELECT COUNT(DISTINCT Name, OS) FROM yourTable I think that's the shortest way to do it, but if that isn't the right syntax, then SELECT COUNT(*) FROM ( SELECT DISTINCT Name, OS FROM yourTable ) a &amp;#x200B;
You might need to figure out how to actually put the data in. In my mind, I put it in once and don‚Äôt put it in again. I check to see if it already exist. Give me an example of what you are doing exactly.
File month ago gets loaded to database 1. File this month gets loaded to database 2. Both tables are compared for changes.
I'm late to this party. I'm a director in a tech company and I just SQL daily, but it's nowhere in my job description. I'm entirely self-taught (and I occasionally write some terrible queries - heh). But in the kingdom of the blind, the one-eyed man is king. You specifically asked the question: Is this the right move for me or should I go back to school for this specifically? While incredibly unsatisfying, I would say that it depends. What you need to be able to do to specialize in data analysis is to be able to demonstrate that you can do data analysis. If you can learn and then demonstrate that you can do it, then you'll be fine. But if you don't have a platform to show off your skills, then it may be that the only way to prove you can do it is with some kind of certification or degree. Honestly, when I'm hiring, I care more about the abilities than the piece of paper. But I do understand why people go that route, especially when trying to move into the field without the opportunity to gain experience and exposure through actual daily work. Good luck! I know I for one would love to hear more about your journey and how it works out.
Sorry man, I just don‚Äôt have enough info on your setup to be of any help. I haven‚Äôt seen an easy way to deal with misspellings before insert.
Mysqlskills.com -VERY useful blog.sqlauthority.com - great blog and reasons why Brentozar.com -SQL GOD http://www.sqlservercentral.com/ -good articles and references https://littlekendra.com/ -SQL GODDESS she used to work with Brent Ozar Olahallengren.com - super functional scripts http://tsql.solidq.com/-Itzakh Ben-Gan Another SQL God Various materials I've pulled over the years https://drive.google.com/folderview?id=0Bw2VLO8kPlGlYWY1MGZiYzQtYjM4Mi00M2U0LTk3NDQtYWFlYThlZDc1ZTJj
ALSO it's good standards to FQ your column calls in your select statement when you have joins. That way, the database knows the exact table and field to seek/scan. So, that means you write a SELECT statement like SELECT Model.FieldName, Caliber.FieldName...
SQL is sexy in a mature way. Development is a dujour sexy. üòÇ
Thank you! I‚Äôll give that shot
Unfortunately, your observation is right in many ways. I'm actually fighting this situation. E.g. I've just given a talk at JavaZone about some new features of SQL that many people would love to use, if they knew about it. Here is the recording: https://vimeo.com/289497563
Is it doing something like rsync?
Having two or more values: select k, count(\*) from (select k from A union select k from B union select k from C) u group by k having count(\*) &gt;1 &amp;#x200B; &amp;#x200B;
Hi, you can practice the exercises available at [https://www.stratascratch.com/sql.html](https://www.stratascratch.com/sql.html) and [https://www.stratascratch.com/python.html](https://www.stratascratch.com/python.html) at your end. But you would really want to use the complete access by signing up for our highly economical plan that comes at only $19 per month. The features include: Built in browser-based SQL editor and visualization tool Over 70+ free datasets to analyze Ability to upload your own Excel/CSV datasets and files Connect using your favorite data analysis tools like SQL, python, R, Tableau or any other analytical tool that connects to a database Share your datasets easily with others. You can find more at [https://www.stratascratch.com/pricing.html](https://www.stratascratch.com/pricing.html) Please let us know if we can help!
Is this Access? If it is: watch out, you have to put each join into ( ) So with 2 joins: From table ((Join table2 on xxxx ) Join table3 on xxxx)
The problem is that this might not work for me since my application uses a library to send the commands. So I am not in control of what gets generated without writing my own DB driver.
That sounds logical, yes. I will try to replace the field name and report back.
I replaced the column "interval" with "remind\_interval" and now my SQL statement works. Thanks for the help. Little background / trivia: I was using MySQL Workbench 8 to test the command and it always pointed at the first bracket to be an error but the error message told me something different. On my notebook I have MySQL Workbench 6.3 installed and it told me today as I tested it again, that "interval was unexpected", effectively telling me that a column named "interval" isn't a good idea‚Ä¶ Wondering why 8.0 couldn't tell me that‚Ä¶ But oh well.
If you're using MSSQL you can do it easily with CTE: https://www.mssqltips.com/sqlservertip/1520/recursive-queries-using-common-table-expressions-cte-in-sql-server/
thanks
It depends. What do you use the data for? How clean do you need it? How many resources (time) do you have? You mentioned that you get new records every month. Do the rows have unique ids? You can keep track of your corrections and hash the original field to detect changes. If the field changes, you need to invalidate your correction and re-validate it.
Use the LAG/LEAD window functions if your RDBMS supports them.
Unfortunately it is. What our professor recommended for our intro to DB class. I will also give this a shot. The error is saying I‚Äôm missing in operator in my join statements. Maybe this is it. Thank you! 
&gt;unistr('\\00A0') Beautiful, thank you. We did it reddit!
Why would you subtract the ID from the capacity? Which column in Storage contains values related to capacity? &amp;#x200B; Needs more info.
The primary key; Product_ID is in a different table, all of the Product_ID's that you see are FKs.... Take the example with a grain of salt...a better example would be: Chocolate_Room_Capacity - Current Inventory, where; Current inventory was found using: SELECT * FROM Storage WHERE Product_ID NOT IN (SELECT [Product_ID] FROM [Goods Movement Details]); The storage values aren't stored in a single column, so column 3 is chocolate rooms storage capacity, 4 is Dry Goods Stores capacity and 5 is Freezer Rooms capacity.
You can perceive it however you want mate, if it's that simple you should be able to find the answer. However, the database was created by myself...
you dont need it if you can derive it and your tables follow normal form. but youre not deriving anything if youre select * from a table. there is no aggregated value in current_inventory. however now youre saying that the values are stored across multiple columns? i think you may need to provide a sample table because that sounds highly unusual.
I was able to figure this out. It turns out I was trying to make it way too complicated. See what I did below. &amp;#x200B; Batch file: sqlplus user/pass@host @filename.sql %COMPUTERNAME%.txt &amp;#x200B; SQL file: SPOOL &amp;1 append
Here's of the SQL for current inventory per storage type... SELECT Storage_Type AS [Storage Type], count(Inventory_ID) AS [Inventory Count] FROM Storage WHERE Inventory_ID NOT IN (SELECT [Inventory_ID] FROM [Goods Movement Details]) GROUP BY Storage_Type Which gives the output. Storage Type | Inventory Count ---|--- Chocolate Room | 2 Dry Goods Store | 1 Freezer Room | 1 Here's a copy of the [Warehouses](https://i.imgur.com/U9fxz1j.png), [Storage](https://i.imgur.com/o6y6jqt.png), [Goods Movement Details](https://i.imgur.com/jHPZZUz.png). Hopefully this helps, sorry if it comes across as a bit of a mess 
did it work?
This did not work, but thanks for the reply. I figured it out, though. See my other comment.
ok thats helpful. now can you tell me if each room type capacity is equal across all warehouses? i also need you to refine the question a bit after you know the answer. are you looking for capacity excess for each warehouse or overall across all warehouses?
SQL is the declarative programming language used to describe, control, interact with, and program in a relational database management system. Most implementations are Turing-complete. It supports complex logic including looping and branching/flow control. SQL encompasses three sub-languages: * Data Manipulation Language - your `select`, `insert`, `update`, `delete`, `merge` * Data Definition Language - used to create and manipulate the objects in your RDBMS - `create`, `drop`, `alter` `truncate`, etc. * Data Control Language - used to manage security, transactions, etc.
There is more to it than that, though yes, a query can give you a filter of data in a table. &amp;#x200B; Where it get more powerful is when what you need to do with the data way more complex than that. When you have multiple tables that take about different aspects of the data, SQL allows you to combine that data in multiple ways to generate answers to business questions that are much more complex than simply filtering a table. &amp;#x200B; For instance, if you have one table with information about customers, another with information about sales people, and another with the all the sales ever and which sales person sold to what customer, you could build a query to tell you "What percentage of sales in 2015 were made by salespeople to local customers (in the same county as they are), broken down by how long the salesperson has worked for the store?" (i.e. does a salesperson build a powerful local network of customers the longer they work there, or does it really not matter?). 
No it's not, the idea is that in the event that capacity does change you can update in the table rather than have someone have to go through X amount of queries and change the values. Capacity excess for each warehouse would be great, overall free capacity doesn't seem beneficial. Sorry mate, you're 100% right, my skill has been been transfering concepts in to words.
What RDBMS are you using? Post your create statements for users and views. Might be a syntax issue and/or it could be a grants issue. Are you using a course-provided solution or did you install the RDBMs by yourself? Usually with course-provided material you not have to immediately start granting privileges and creating users as that sort of thing comes later in a course. 
So if a job says I need knowledge of sql queries I just need to know the commands to sort through the table data right? 
What I am currently doing is joining tables by record_id. Another thing I am using SQL for, is storing a relatively large text scraped from MS Word. Excel would be bad for too long texts.
wut????
It depends on what the job will entail. "sorting through the table data" is a fairly simplistic description of what one would do in a query, overlooking analytical functions, pivoting/unpivoting data, rolling up &amp; aggregating data, etc.
Do a learning exercise for me, it's good practice and you'll get some exposure to users and system grants. Remember that for grants you have two types: system privileges, and object privileges. What you are interested in is granting a system privilege to create a view. So here is an example. --log in as system, then create a user called testacct create user testacct identified by testacct; grant create session, create table, create view to testacct; --Then log in as testacct using password testacct, and try to create a view Create or replace view testview as (select sysdate as testdate, 'hello' as teststring from dual); --Then select from the view select * from testacct; Now you've proved that giving view grants solves the issue of not being able to create views. But what about the HR user? From what I'm seeing, the HR user cannot create a view. So let's do the grant. From the system user, type: grant create any view to hr; Now log back in as the hr user and try to create the view. If you cannot create your view in your post (I did not check it for syntax), try creating a simple view just like we did with testacct and see if that works. Let me know how that goes 
People tend to get excited about things that are new and/or flashy. So the latest 10mb js library complete with backdoors and cryptomining code that can render a fancy animation using html5 blockchain AI in just a few lines of code generates more headlines. Its loud, flashy and probably dead in a year or two. To be honest its fine by me, the web development world is saturated and moves very fast whereas SQL is a tried and tested language that has not changed that much. You can find an hire a reasonable web dev for a lot cheaper and easier than the same level of skill with SQL. Just means more jobs eith the caveat that there are fewer sources of information that spoonfeed you. 
Curiosity should drive this decision. It‚Äôs easy to understand ‚Äúwhat‚Äù you need to learn in order to do a (fill in the blank job here) job. What will drive your proficiency is practice and practice will be driven by your curiosity. Find a topic you are passionate about or interested in and scour the webs for data. Find yourself a way to manipulate (query) the data then find yourself a way to visualize it. Let curiosity be your guide.
If I had a list of suppliers and wanted to shows all of the information in the table for only the suppliers in the USA would my string be something like; Select * From suppliers where country=USA this was in one of my samples and it says the string is invalid. Any ideas what would make this work?
it did not. I am probably doing it wrong. SELECT Make_ID, Caliber_ID, ModelNum FROM [Model] INNER JOIN (([Make] ON [Model].[Make_ID] = [Make.Make_ID] INNER JOIN [Caliber] ON [Model].[Caliber_ID] = [Caliber].[Caliber_ID]) WHERE Caliber.Caliber = '9x19mm'; 
[https://pastebin.com/YAmuSmLQ](https://pastebin.com/YAmuSmLQ)
YES!!! thank you!! and now i see how it is suppose to go. Much appreciated! 
Is this something with the newer versions? WAAY back in the day when the BA's only knew how to use Access and sometimes needed help with their queries; I didn't know how to use the Access GUI and would just parse their sql and/or delete it all and write my own, lol. I remember there were a few things it was temperamental about syntactically but not all those parens... else I would have told all the BAs to figure it out; I don't have time for the parens, lmfao.
There's even a Access-Runtime now... pls don't ask and not even consider to use it. thanks.
Select * from test table minus select * from production table
For SQL Server, there are a few options, see: [https://www.mssqltips.com/sqlservertip/2779/ways-to-compare-and-find-differences-for-sql-server-tables-and-data/](https://www.mssqltips.com/sqlservertip/2779/ways-to-compare-and-find-differences-for-sql-server-tables-and-data/) &amp;#x200B;
Does it support CTEs?
Then you should be able to do something like WITH mylist(code) AS (VALUES ('01'), ('02'), ...) SELECT ... FROM ... WHERE foo IN mylist 
Here is a basic CTE that is a list of things so you can see how they work. It's oracle but you get the drift. with myCTE as ( select 'dog' animals from dual union all select 'dog' animals from dual union all select 'dog' animals from dual union all select 'zebra' animals from dual union all select 'mouse' animals from dual union all select 'birdy' animals from dual union all select 'horsey' animals from dual union all select 'kanga' animals from dual union all select 'kanga' animals from dual ) select animals, count(*) from mycte group by animals order by count(*) desc, animals; You can have one or more CTEs that effectively become in line views (a virtual table for all intents and purposes), and then you can join it or use it within an in clause, or whatever you want to do. They are super handy. It's also known as a defactored subquery. Instead of having the temp table in the from clause and aliasing it there, you're moving it up top. The optimizer doesn't really care which way you do it and I've never seen a case where a CTE performs worse or better than an in-line view. 
Something you should look at are Table Valued Parameters. I can look more at this next week but I bet that can give you some ideas. The idea is you can essentially define a table type, pass it a list, and you can cross apply your TVP to essentially pass lists as arguments. 
SQL for dummies I guess.
 I hear the website sqlzoo is really good for hands-on experience. Not super sure, as I'm just a beginner programmer. 
For USA addresses, you can get the address lookup API from the USPS.com. As a side job to fixing the mis-spellings, you can fix the addresses. I would assume there would be similar services from other major post services. https://www.usps.com/business/web-tools-apis/welcome.htm
Personally, under sql server, I build a dynamic sql statement using concat_ws and calculate a md5 hash on it. Then it‚Äôs just : Select ... From table1 Inner join table2 On table1.id = table2.id Where table1.hashcolumn &lt;&gt; table2.hashcolumn. 
Hey there, I may not be following you entirely, but have you tried grouping by the fruit type? Do you have any code that you've already written / can simplify to fit this example?
You should take a look at your where clause. Do you need to do anything with USA in order to identify it as a string?
 SELECT Location , Dates , *code here for apples* FROM LocationsList JOIN DateTable on blah JOIN FruitSales on blah This is the rough outline of my issue. The actual query itself has 7 different types of joins, so it's not really going to help to throw it in here as it'll be just as confusing. As for grouping by fruit type I don't see that helping in the subquery. Each day/Location already has one record per fruit type, with the total sales per record, and that's what I want.
WiseOwl SQL on YouTube 
I'm not sure if you even need a subquery. I was thinking something like &gt; SELECT Location , Dates , Fruit type ,Sum(sales) FROM LocationsList JOIN DateTable on blah JOIN FruitSales on blah GROUP BY location, date, fruit type 
It‚Äôs pretty solid and you don‚Äôt need to sign up
Thinking of picking this book up! Sounds like a good recommendation!
Would you think that this is necessary even if I'm just playing around as a hobbyist? I see the purpose but I'm surprised that Microsoft doesn't have a 'community' version or something.
That's what I was thinking as an option, thanks!
It's not that there's not a version you can use -- SSMS is free. It's just that SSMS only supports Windows. If you don't have Parallels or something else that lets you run Windows software on your Mac, then an Azure VM is a perfect solution to give you a way to play around with it.
There are some courses on Coursera that you could look at. I audited portions of one from the University of Colorado that went from basic SQL into learning about ETL tools, etc. Some of the material is a little dry, but there are some pretty useful items in it. There were several others available last I looked. Including a series from Duke University that looked pretty good. 
Get Visual Studio, get SSDT. Do a data and schema compare. Save your brain.
Pivot out fruit type? Pivot(max(qty) for fruit_type in ([apple],[banana],[cantelope]) I don‚Äôt memorize the whole structure of a pivot query, but a simple google search should get you there.
Wrapping banana stems tightly in cling wrap will make them last three to five days longer. *** ^^^I'm&amp;#32;a&amp;#32;Bot&amp;#32;*bleep*&amp;#32;*bloop*&amp;#32;|&amp;#32;[&amp;#32;**Unsubscribe**](https://np.reddit.com/message/compose?to=BananaFactBot&amp;subject=I%20hate%20potassium&amp;message=If%20you%20would%20like%20to%20unsubscribe%20from%20banana%20facts%2C%20send%20this%20private%20message%20with%20the%20subject%20%27I%20hate%20potassium%27.%20)&amp;#32;|&amp;#32;[**üçå**](https://np.reddit.com/r/BananaFactBot/comments/8acmq6/banana/?st=jfof9k8d&amp;sh=acd80944)
 Maybe it is better to start with a tool ... # T-SQL Fundamentals (Ben-Gan Itzik)
I figured out myself, thanks for the help though. SELECT Warehouses.Location, Warehouses.[Chocolate_Room_Capacity] - Count(IIf([Storage_Type]="Chocolate Room",1,Null)) AS [Chocolate Room], Warehouses.[Dry_Goods_Store_Capacity] - Count(IIf([Storage_Type]="Dry Goods Store",1,Null)) AS [Dry Goods Store], Warehouses.[Freezer_Room_Capacity] - Count(IIf([Storage_Type]="Freezer Room",1,Null)) AS [Freezer Froom] FROM Storage, Warehouses WHERE Inventory_ID NOT IN (SELECT [Inventory_ID] FROM [Goods Movement Details]) AND (Storage.[Warehouse_ID] = Warehouses.[Warehouse_ID]) GROUP BY Warehouses.Location, Warehouses.[Chocolate_Room_Capacity], Warehouses.[Dry_Goods_Store_Capacity], Warehouses.[Freezer_Room_Capacity]
How can I check?
Sql ex ru
 SELECT DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = 'tbl_name' AND COLUMN_NAME = 'col_name'; change the names and column to your own and it should tell you. 
run this query -- SHOW CREATE TABLE tablename
Write a report using SSRS
It might be related to parameter sniffing: https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/
You can do it that way or you can control it with various query hints like optimize for. Best to try a few approaches and see what works best in your situation. 
Yup, if you dont have ctes I could see s9mething using a while loop and insert into a table with a join could work but it would be ugly. 
There are no parameters that I see in your queries, so Parameter sniffing is out. &amp;#x200B; I think what you're running into - the CROSS APPLY is executed first and so you don't have the FromDateTime and ToDateTime alias's with values yet. That's why your execution plan is likely different (you don't have that so it's hard to guess). But the index seek seems to confirm that. On the faster query, you can see it's using both the values to filter Start and End, and on the slower query the only filter you have is the Customer ID. &amp;#x200B; Logical Query Processing Order [https://www.itprotoday.com/microsoft-sql-server/logical-query-processing-what-it-and-what-it-means-you](https://www.itprotoday.com/microsoft-sql-server/logical-query-processing-what-it-and-what-it-means-you) &amp;#x200B;
Or ensure the SQL Browser service is started and also ensure UDP 1434 is not blocked by any firewalls.
I'm a sqlite user, heavily. Been looking forward to window functions since it first appeared on their draft changeling. Having never used them before, I've read the documentation but was unclear on usage, until I saw this. Great explanation, thank you! Been using recursive ctes since sqlite implemented them. One of my big projects is a complex directed graph problem. Before I used ctes, my code was heinous. But, it's surprisingly clear now. Good times. 
There are no parameters in the example provided, so it can‚Äôt be a sniffing issue. 
This is EXACTLY what I‚Äôm looking for - thank you so much! I‚Äôll give it a try now!
Just checked [sqlite.org](https://sqlite.org/index.html). Looks like the version that includes window functions [literally came out today](https://sqlite.org/releaselog/3_25_0.html).
The issue you are encountering is that one use of your table valued function is deterministic, while the other is non-deterministic (look those up). This can drastically affect performance. Just remember that programming in SQL is not the same as other languages. SQL is a set based, instead of object oriented. The concept of abstracting sets of code into functions that you can call is an anti-pattern in SQL. My recommendation is to replace your use of a function with a subquery, temp table, or CTE. Let me know if incorporating the logic of the function directly into your query improves your performance. 
Thanks. Didn't notice yet :)
Heh. I'm building sqlite-jdbc with it as I write this. ... probably going to end up upgrading my main work project to this on Monday. As you say: turns out, I have a *lot* of self-joins, and most of those should go away.
If you are getting an error message prefixed with ORA, like ORA-9999, then it is definitely not anything to do with sql developer and I've ever seen an ORA error be a false positive. In other words, it only tells the truth and it appears that the system account does not have the grants to create a user. To be honest I've only upgraded existing databases to 12c and 18c but I've never started a native 18c installation. Typically the system account would not lack these grants, but they are always doing "best practice" security with each new release so I guess it wouldn't surprise me if everything was totally locked out from the very start. So check system privileges. As user 'system', type SELECT * FROM DBA_SYS_PRIVS where grantee='SYSTEM'; Do you see privilege "create User" here? System can self-grant this privilege. grant create user to system; SELECT * FROM DBA_SYS_PRIVS where grantee='SYSTEM'; Now Do you see privilege "create User"? create user potato identified by potato; You should be able to create potato user at this point. Let me know how it goes 
Another thought... if you cant get any of this to work, you might want to consider using a pre-built oracle virtual machine. Example: https://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html This one says it has Oracle linux 7, 12c, and other tools. That's what I use at home when I'm tinkering with stuff. It's all free for non commercial use. 
Just do this Where A not in (c,b) and b!=c
Select Title,sum(Id) From (Select Title, count(ID) From database ) as e 
 where rownum &lt; 100; Should do the trick
I agree 100%. I'm 32 and have been using SQL on a daily basis for about 8 years. I find that the analysis that you can get out of SQL tends to excite folks like CFOs or Controllers, and less the people who tend to make YouTube videos. Personally, I found SQL to be a "how did I get by without knowing this" type of thing before I learned it. Then again, my background is stat and econ, and I find a lot of data science videos where the person doing the analysis doesn't seem to actually the model to a deep degree. 
My rusty coin: learn them anywhere (youtube, datacamp, even mysql help is clarifying enough), but be sure to setup your own DB and play the shit out of it. Modern studies are theoretical/playground style, you might come out of it with ‚Äúi know what is it, just can‚Äôt do anything‚Äù. Learn by doing. 
Depending on your dedication, you can probably learn SQL for free with w3 and YouTube. Many people think they need to pay shiny looking sites like Lynda etc when in reality it's not about learning it's about understanding. Understand the basic fundamentals (selects, joins, what the heck a database is for) and you learn the pieces you need as you go. One, two or ten courses are useless if you're not applying what you learn in the real world. 
Just as an aside- many libraries provide free access to Lynda.com. https://lifehacker.com/here-s-how-to-access-lynda-s-learning-database-for-free-1820761688 
what is a DB ?
SO I logged in at SYSTEM and tried SELECT \* FROM DBA\_SYS\_PRIVS where grantee='SYSTEM'; I got an error of ORA-00942: table or view does not exist &amp;#x200B; I then tried the grant create user to system; &amp;#x200B; I then got the same error of ORA-01031: insufficient privileges &amp;#x200B; I'm going to try reinstalling SQL DEVELOPER using the walkthrough my program gave me. If that doesn't work them I'm going to try the pre-built oracle virtual machine. I really do appreciate your help and I will update you once I figure it out. 
w3 ?? tell me more ? 
This... I was (un)lucky enough to learn on the job with a terrible, old, disorganized assortment of DBs. If it wasn't for all the stuff I had to google to get any data that made sense, I wouldn't know anything. 
It depends on your learning style and what aspect of SQL you want to learn first. For basic data retrieval this is one of the best free interactive websites I've found, [https://sqlbolt.com/](https://sqlbolt.com/) Also, this recent article gives you a nice overview on SQL - [https://www.datacamp.com/community/tutorials/sql-reporting-analysis](https://www.datacamp.com/community/tutorials/sql-reporting-analysis)
Database
Interested in hearing an answer to this one...
Alright, let's hear it 
Haha shit, just realized the way I wrote that is like an offer. It's a question to see who does make cash on the side üòÇ
This is confusing to me... why would the system account not have access to DBA_ tables? I even read the manual for 18c and it says that system holds the DBA role (like it always has), but I wanted to see it with my own eyes in case there's some new feature I'm not aware of. Here's a screenshot from the manual. https://i.imgur.com/Bv3fCS3.png Also, if you want to skip sql developer, open up a terminal and connect to sqlplus, example: yourlinuxprompt&gt; sqlplus '/as sysdba', then try any dba operation. You are really logged on as system? If you type 'sho user;' does it return system? And do you get the same experience if you log on as sysdba? Honestly at this point a VM might be better just so you can get started, then as you learn you can crack the case on what the issues are with your other installation. 
Papa bless cloudsinsky. Also how does one take a break from the internet. I'm getting very dependent on it.
Yep. Definitely review your contract and clear it with your employer first. I have to do the same even for open source contributions/projects and blog posts about code I do in my spare time before publishing them.
Not yet - but I‚Äôm trying to setup a business to purely act as a SQL consultancy! So if anyone wants to hire an experienced DBA and database developer, hit me up.... ha
Check permissions. If its not that, check event viewer. May have the directory it's looking in a better error. Looks like you installed in root of C, may be looking for it in one of the Program Files folder. 
I'm sure it could, but as long as you remain relatively anonymous and don't share company IP/secrets then I wouldn't worry.
I teach databases and SQL as an adjunct at my Alma mater. 
Permissions are okay. What logs should I look for in the event viewer?
assuming that i'm responsible about non-compete stuff, and that it never got in the way of my day-to-day stuff... i'd guess that employer wouldn't really care that much. but SQL evening/weekend work seems unlikely to find, and not an ideal scenario anyway... leaving daytime work... and no, i won't harm primary employment for a steak dinner or so.
Not on a PC ATM, but I believe it should be under application events. Try again and then refresh right after. 
I have yet to find any SQL work that doesn't already have 100 applicants from a third world that can under bid me to pennies on the dollar.
&amp;#x200B; Ran &amp; refreshed and still nada in there (or anywhere else). Does show an app crash for pgservice.exe from earlier today but it doesn't line up with my attempts to uninstall. Shit. Maybe this is the universe's way of telling me to take my Sunday off. 
We display the order in the UI and it's editable, which triggers back-end reordering. I was thinking that having the row number reflect actual displayed number keeps a certain amount of integrity between the front and back end.
I do small amounts of ETL work with MSSQL and Oracle. Most of it is for DWs and DMs
MS Access is a terrible database, but a decent front end for other database engines. It can talk to remote database servers over ODBC Then you can make some nice forms for staff to edit data
Had to save this post as it‚Äôs so cool to read people‚Äôs stories. I‚Äôve just hit my 1 year anniversary working as a Data Analyst after a career change from boring office admin last year. Sadly I‚Äôve never used SQL in a work environment as we were trained to use Alteryx but I‚Äôve always wanted to have a job that is SQL based. Currently learning T-SQL and hopefully will land a job that utilises SQL in the next 6-7 months when my contract ends.
The w3schools website was very helpful when I was getting started.
Oops! You're right, I missed those two. ü§¶‚Äç‚ôÇÔ∏è I'll edit my comment so other people won't be confused. Thanks for pointing it out and you're welcome Glad I helped.
The easiest way would be to do it with Python scripts. Build some sort of standardize Excel template that the users could populate then place in a specific location. Have an hourly/daily/weekly Python script run, check that location, read the data in the Excel doc, then connect to the database and update the records as needed.
Thanks, I might try the Python route as Im looking for ways to develop my Python skills, and I doubt they would let me build a web app.
Have you tried anything at all? This just reads like a copy-pasted homework question.
There should be a review type code in the interviewer_review table as well as a reference table with descriptive values. If a different interview type implies a different set of review questions than the interviewer_review table needs to become a table with interview ID as well as "review Type Cd", "review question XX ID" and "review question XX Response ID" with a reference table to join to for both questions and answers based on the type of interview it was. IMO ...
what do you mean by "review Type Cd?" I have never seen "Cd" before.
looks like school started again
_cd typically stands for code, for when the de-normalization of the data takes up little enough space to make the join to a key reference table redundant enough to remove from daily queries. (think of provider type cd = pcp versus provider type key = 001 with a join to get the value. etc) Its a pain but what you need to do (IMO) is take the one to many relationship and understand that in your interviewer_review table each column should be a "key" that joins to another reference table (or a _cd where an abbreviation has value). Now you can create any number of "interview ID" "Reviewer id" "review type CD (code)" reviewer response key" to create any number of different situations.
Yeah at my job we use SQL server for everything normal, but use Access purely to get the data in as the form designer is quite nice.
Strftime is probably what you are looking for. https://www.sqlite.org/lang_datefunc.html
I still use access 2010 and frequently build access data projects for a simpler option than linked tables in access. Beung able to connect directly to Sql Server backend and easily build forms, reports and easily whip up some vba code? Access data projects are the best. I frequently still use Sql 2008 r2 for early development of the stuff I write so that I can easily and directly use ms access instead of Ssms to create objects. For some things access data projects are super useful. I also miss Data Access Pages. I used to use the office web components to create sick ad-hoc reporting tools. At safeco we had seventy thousand different reports using office web components. Also at Expedia, they had over 60,000 different reports that would use office web components to provide ad-hoc reporting solutions that were super simple. I just hate how Microsoft had introduced So many different tools for the database world and then they just flaked out and killed the product. Whether I'm talking about PerformancePoint or the Data Analyzer product they had bought. Or whether we talk about notification services, even DTS. I think that it is a shame when Microsoft builds a terrific product and then five years later they build a completely different product with no backwards or forwards compatibility. Killing off Access Data Projects or Data Access Pages is lame. But the change they had between DTS and SSIS? I would literally pay 10 thousand dollars to be able to follow the same workflow that I did with data transformation services. Back then you could use the gui to copy some data, do a couple of operations. But then you could take that DTS package and you could SAVE IT AS A SCRIPT. That means that DTS was just a code builder and it was drop dead simple to save a dts package into vbscript or Javascript. I thought that there was a 3rd option for exporting dts as a script. But I knew the dts object model very very fucking well seventeen years ago when I first started writing enterprise level etl, schemas and business intelligence tools. In my first year doing etl, I was clearly about ten times more productive in DTS than what I am in SSIS.. I am decent with SSIS, but I've just never gotten into the SSIS object model to the same level I was using dts. For example, if you built a package in dts that would statically import from textfile101 to tablexyz, then it was drop dead simple to take that script and add a couple of loops and some error handing. I still to this day miss writing etl in Sql server 2000 data transformation services. Scripting was just amazing in DTS. We had the multi phase data pump back then. Good times. 
Change Full Join to inner join
"no data found "
Left outer join on t1. CuID = t2. CuID
thank you! now i get &amp;#x200B; **CUNAME OID** Cu1 (id01) Cu2 (id02) Cu3 (null) &amp;#x200B; &amp;#x200B;
Thanks for the mention and glad you liked my article! Good points on understanding the industry and the business. That often takes a long time to really learn.
Awful website, the navigation bar at the top not shrinking is just terrible UI design. Ignoring that, can you please explain what this article adds in terms of value over say the top couple of links in google for 't-sql shrink database'? Because this looks to be more simplistic, less informative and just pure blogspam to be honest. I'd say that [this article alone](https://www.brentozar.com/archive/2017/12/whats-bad-shrinking-databases-dbcc-shrinkdatabase/) makes your posted article entirely pointless.
Using what database? Oracle: select to_date('01/01/1970','mm/dd/yyyy') + numtodsinterval(1456342438,'SECOND'), TRUNC(to_date('01/01/1970','mm/dd/yyyy') + numtodsinterval(1456342438,'SECOND')) from dual; (https://stackoverflow.com/questions/35613644/convert-unix-timestamp-to-date-and-datetime-sql-oracle)
A couple questions, I know you are trying to join data, but in your example you don't have anything to join on. The reason you are getting data back from that Full Join, is because is bringing all of the rows back regardless of the join criteria. So... 1. Is this a real example? 2. What exactly are you trying to accomplish? 3. Have you checked Inner Joins instead of Full Joins? (Assuming there's a value in which the tables can join. [Here's a more realistic example](https://snag.gy/RFApLB.jpg)
Based on a quick read it sounds like you want the following tables: * Customer (name, address, billing info, etc.) * Services (list of unique services available) * CustomerNumbers (one to many relationship with Customer; lists the IMEI number for each phone number, lists the SIM serial number) * CustomerService (link customer number to Services, one-to-many, to group services with each number) Does that sound like what you want?
So many inaccuracies in the article along with some unsaid important information about the effects of shrinking a database. First, a truncate does not ‚Äúdelete‚Äù all data in a table. A delete statement without a where clause does that. A truncate statement is a metadata operation. It deletes the rows pointes to the data, not the data itself. Second, what do they mean by resetting indexes? Have you reset your indexes in your database recently? =P Third, truncating a table does not release unused disk storage. If it did, you wouldn‚Äôt need to shrink the database! What you do need to know about using these statements is... Truncating a table does not update the statistics. As the table gets reused, the performance will suffer because the query optimizer has an incorrect view of what data is actually in the table. After truncating a table, best practice is to run an update statistics statement. Shrinking a database is usually a bad idea because usually normal use age grew it to that size. It is just going to grow back to that size, but now you get to enjoy the performance hits of when the file grows (unless you use instant file initialization). Shrinking a database creates massive data fragmentation. This is going to wreck your performance. If you must shrink the database, you better be rebuilding your indexes afterwards.
So if you can access mysql on the server locally (via mysql command), you can type mysql [database name] &lt; file.sql and that will execute the file in the database specified. Or are you talking about actually accessing the mysql command line? 
The first part, I want to put the command line in a batch script for less clicks. I keep getting 'mysql' is not recognized as an internal or external command error. 
I'm not sure, I have Heidisql on my desktop, though whenever I'm loading data, the work gets done on my boss' laptop.
Ok. What may be the best idea is to figure out where the heidisql database is. It sounds like it may be on your boss' laptop. You will need connectivity to this if you want to connect to a database on his laptop. Alternatively, its on a server somewhere that your boss is accessing. You need to find the credentials and IP/port for this to connect as well as installing the heidisql client on your computer. The other alternative is that you're trying to load a database onto your computer. Again, you would need to make sure you have both the server and client software installed. 
Hi, First of all thank for your comment. Yes we are working on design issue and hopefully we reslove it by today itself. Other thing is you might be not found this article helpful and it doesn't mean that my article is not good. Give me your one article link and I will suggest you better article that was posted by someone else. So please don't compare anyone article with other articles. At last please try to find some positive things and focus on that way rather than spread negatives. And last Your comment is totally pointless and senseless to me...
Writing a script to loop through a table and export the files from. Your blob table into a folder? It wouldn't be that hard to do. What languages do you write? What is the shape of the tables inside the mdf file? Is this mdf file attached to Sql server or has it lived outside of a database server? 
Oh look, your article has 0 upvotes and the only other person commenting here is talking about how crap your article is. As I said and others seem to agree. You have no business writing and posting articles about SQL. Your articles are dangerously lacking in information and just crap.
I have connectivity to my boss' laptop via Heidisql. I'm not sure the steps to put mariadb/heidisql commands in a batch script.
&gt; Nobody is invited you to review my article You did when you posted it here. Just take a look at the other comments, nobody has said anything good about your article. Your article is dangerously lacking in information and could cause serious issues for someone who does not know better. Could you please explain what your article adds in terms of value that Brent Ozars article does not?
Right do yourself a favour. Follow this link: https://www.reddit.com/r/SQL/duplicates/9gj6nv/reduce_sql_server_database_size_after_trucate/ Click comments on each of the posts you have blogspammed round reddit. Find me a comment that is 1) Not written by you 2) Not pointing out how shit your article is. Now when you have not found any positive comments and only negative ones, ask yourself this question: Why is everyone saying my article is rubbish? I will give you a big fat clue. Because it is.
I think this is what I was looking for. If this doesn't work it's probably just got to be broken down. Thanks!
I think other people are not that much interested in my article. They reviewed and share feedback. But I don't know what's worng with you man. Better to give your opinion here just go kids zone buddy.
Oops just saw this...I would recommend the following too: For basic data retrieval this is one of the best free interactive websites I've found, [https://sqlbolt.com/](https://sqlbolt.com/) Also, this recent article gives you a nice overview on SQL - [https://www.datacamp.com/community/tutorials/sql-reporting-analysis](https://www.datacamp.com/community/tutorials/sql-reporting-analysis)
Thank you!
I'm looking to figure out how to build it from a database design point of view. Using databases is new to me. I do have some experience with them but it's limited to WordPress and vBulletin, so actually designing the database is something I'm not up to speed on. 
This is what I'm looking for yes, I'll need to research how the tables link together though so when I search customer is pulls CustomerService details for that customer only. 
I would recommend downloading [SQL Code Smells from RedGate](https://www.red-gate.com/simple-talk/sql/t-sql-programming/sql-code-smells/), while it doesn't tell you WHAT TO DO, it will share the most common mistakes in database design. It's *only 71 pages for free* and I strongly recommend it from a DB lifespan point of view. Also, it's really good to understand [Normalization](https://en.wikipedia.org/wiki/Database_normalization), I bet that a couple articles afterwards you will be bomb at it. Extra: [11 Rules on Designing DBs from codeproject](https://www.codeproject.com/Articles/359654/important-database-designing-rules-which-I-fo)
Try this: `SELECT strftime('%Y-%m', MillisField / 1000, 'unixepoch') FROM MyTable`
I took the Visual Basic frontend, Microsoft Access backend based asset inventory system of a company with 750.000 employees into the modern age with a responsive web frontend, SQL Server backend based inventory system. Build an API for inserting new assets/changing existing assets through a client that gets run on PCs after they're imaged. They also needed a client for technicians to be able to print paperwork to use when bagging hard drives in their health care division, as they legally must retain drives for a number of years or indefinitely. Across several contracts, this was worth over 500.000,USD and now I have their support contract.
Only comments on the article are giving better options(and showing it's not a one size fits all approach), so not sure why it's titled "best practices". 
Are 50-100 tables above the typical number of tables used in a working environment? Thanks!
Any time! Hope it works.
I'm just confused for most of them, I'd assume that the first question would be something like this: SELECT dtoohey.ITEM.Description, dtoohey.ACCOUNT.TreatmentDate FROM dtoohey.ACCOUNT JOIN dtoohey.PATIENT ON dtoohey.ACCOUNT.PatientID = dtoohey.PATIENT.PatientID JOIN dtoohey.ACCOUNTLINE ON dtoohey.ACCOUNT.AccountNo = dtoohey.ACCOUNTLINE.AccountNo JOIN dtoohey.ITEM ON dtoohey.ACCOUNTLINE.ItemNo = dtoohey.ITEM.ItemNo WHERE dtoohey.PATIENT.GivenName = 'Betty' AND dtoohey.PATIENT.FamilyName = 'Eggert'; While I don't have much idea on how to do the other three questions
for the second question I'd assume it would go something like Select ProviderNo WHERE PatientID &gt; SELECT AVG(PatientID) FROM *)
F
What is the logic that makes this table?
could you post a few rows from each table that are illustrative and the sql you currently have?
Will do first thing tomorrow as I am not at work. Thanks for your response! Much appreciated
I think a simple UNION would work. SELECT email, expessed_interest_at FROM form_interest UNION comment_interest;
i tried, but the point is to output the minimum of 'expressed_interest_at' for 'dixi@example.com'. I don't know how to do it.
SELECT distinct(ci.email), min(ci.expessed_interest_at) FROM form_interest fi right join conversation_interest ci on fi.email = ci.email group by ci.email
Because it's blogspam
This makes no sense. Table 2 and 3(results) are identical, yet there is one record that has a difference in the timestamp. What's your desired outcome (in words). The table definitions you gave don't help towards that. 
You need to provide more info. If there is a conversation_interest record for an email address, will there always be a form_interest record beforehand, or can they exist independently? Personally I would not use email address as a Primary Key either. Email addresses can be reused, or you may wish to treat similar emails as the same user... ie hello+test@gmail.com and hello@gmail.com belong to the same email address. Create a userid for each sales lead, and work from that. 
Just post your schema, and what results you are trying to get Trying to explain it all in English just makes it more difficult to help 
Its homework.
This is probably returning null, or at least not converting properly: CAST(Data_Updatedate AS DATETIME(3)) Try this instead, STR_TO_DATE(Data_Updatedate, '%m-%d-%y %h:%i:%s')
Thanks for your reply, I re worked my example in an edit to the main post. Does that look like it captured basically what you were saying?
Maybe it doesn't know how to stop? Try a WHERE clause. Use the one in your sub-query. Yay redundant WHERE clauses. If this suggestion works, rewrite it using a single sub-query, not for performance but just on principle
Yes, assuming the functions/string delimeters are correct for your flavour of SQL, that will do what you want it to. If you want to be double safe when you're doing your update, begin your statement with begin tran and end it with rollback (again, details may vary: you haven't told us which flavour of SQL you're using) This way you can see how many rows it is going to update before it does. Once you're happy it's correct, change the rollback to commit and run it again.
Knowing your platform would be useful. I think your problem *may* be platform dependent: you can't always use a parameter at that point in the query. You could do it with dynamic SQL like : DECLARE @sqlstring (varchar 1000) SET @sqlstring = ';WITH CTE AS ( SELECT TOP (' + @ticketquantity + ')Status,Order_Id FROM Ticket WITH(UPDLOCK,ROWLOCK,READPAST) WHERE Ticket.Class_Id = ' + @ticketclassid+ ' AND Ticket.Status = 101 AND Event_Id = ' +@eventid+ ' ) ‚Äã UPDATE CTE SET Order_Id = 1,Status = 100' exec(@sqlstring) 
I think there are some paid tools (maybe with a trial version) that can extract a single table, so you don't have to restore the whole database. I can't help more, because I'm a SQL dev, not a Database admin.
Ok thanks anyway. Do you think it's common to have databases that size? I've seen forum posts call 300GB large, but I don't have any other experience to draw from.
Which database software do you use? Is your backup a SQL backup or a backup of the files that make up the database?
for `DATEPART` you'll have to use `DAYOFWEEK` or `WEEKDAY` (why there are two functions for this in MySQL is anyone's guess)
Not a performance hit at all if written correctly. The API acts as an ESB. Hell it would be way easier if the rest of the apps I deal with were as easy as the API. It would be a lot more consistent and easy to monitor. The only thing I‚Äôve found is that I‚Äôve had to code so that the procedures always complete instead of returning exceptions. The exception is caught as a varchar parameter and returned to the API. If the API sees that the exception parameter is populated, it assumes something went wrong and returns that error to the API. The database also logs the exception for debugging purposes.
Do you have any replication strategy? 
The problem is the time it's taking to restore. Over a day for a full restore to the only machine that has space (a VM in Azure). The IT manager (my manager) is currently on a drive to push everything off-site into datacentres, but the problem seems to be the transfer speeds between the solutions.
I doubt that without further details anybody can help you.
We do not. Although I'm sure replication is a good idea generally and we should get on that - we're trying to restore from a week ago. The table lost a load of data (probably due to a process or an accident from one of the DBAs) and we need to get it back from when the data still existed.
You might want to at least tell people which SQL it is if you're looking for help. Microsoft/MSSQL? Postgres? MySQL? And what do you mean by "product"?
For an "average" database, yeah it's big. But that's including "all" SQL databases on earth, including 1 page wordpress websites. Maybe it needs to be that big, maybe it's badly designed. It's like asking "is this piece of string long?" ... it depends what you're comparing it to. If it's not obvious to you by looking at it, then nobody can explain to you how to determine that without accessing the database themselves to look into it properly, as well as knowing all the business requirements etc. So if you don't have anyone in-house to make a judgement, you'll probably need to get a database consultant in.
You can get some help from here: [https://www.nucleustechnologies.com/sql-backup-recovery/](https://www.nucleustechnologies.com/sql-backup-recovery/) It is a SQL backup recovery software, which can help you to restore data to Live SQL or Batch file from SQL backup files. 
What specs is the temp VM that you're doing the restore on? What storage option? What version of SQL is the database? MS 2012? MS 2016? 
reverse engineer with visio 2010 will generate the schema you are looking for. Red gate tools is expensive but can do the same thing.
Google Data Dictionary software. There are a few tools that can do this. SQL Spec is a decent one. However, querying the INFORMATION SCHEMA of the database should be enough. 
I do, but working in a place that dealt with defence for a few years has got me into bad habits. Apologies. We're using Redstor to do the backups and it does the SQL backups via the SQL Writer Service and then out to redstor's datacentre. The SQL database is Microsoft SQL Server 12.0 and contains a 1.5TB BI database. This is hosted in one datacentre and the backups are in another datacentre. The machine we're restoring to is in Azure running the same version of SQL Server. One of the tables in the BI database lost a large number of records at some point in the last week and we're trying to restore this particular table to its previous state - it should've been a static table according to our BI person, so we're unsure what happened. I understand that the connection between the backup datacentre and the Azure machine is a massive bottleneck, and I'm sorry for not supplying very much info.
run queries against the information schema database to get table layouts
I answered a similar question just now elsewhere in the thread - you probably missed it: We're using Redstor to do the backups and it does the SQL backups via the SQL Writer Service and then out to redstor's datacentre. The SQL database is Microsoft SQL Server 12.0 and contains a 1.5TB BI database. This is hosted in one datacentre and the backups are in another datacentre. The machine we're restoring to is in Azure running the same version of SQL Server. One of the tables in the BI database lost a large number of records at some point in the last week and we're trying to restore this particular table to its previous state - it should've been a static table according to our BI person, so we're unsure what happened. I understand that the connection between the backup datacentre and the Azure machine is a massive bottleneck, and I'm sorry for not supplying very much info. As somebody not well versed in SQL I was looking to see if anything we're doing is out of the ordinary or if there are any glaring issues that I'm missing.
I would recommend this. &amp;#x200B; I would also look at the stored procedures and views. You can right click their name and view dependencies.
&gt; Over a day for a full restore to the only machine that has space Gotcha. In my case, I was able to perform the restore inside the same rack on the same network, so the restore only took an hour or so. Not much you can do about it at this point, short of adding a server/drive on the same physical network as your existing server. Down the road you'll want to look at your overall recovery solution.
There are ways to do this on SQL Server with filegroups. It wouldn't be unusual to break up a database into multiple filegroups. You can divide the tables into different file groups which let's you determine which file a given table is physically stored in. Then when you do a restore, you can then choose to restore just the filegroups that contain the tables that you need. You do always need to restore the Primary file group, IIRC. [Here's the MS doc on piecemeal restores](https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/piecemeal-restores-sql-server?view=sql-server-2017).
 SELECT u.email, MIN(u.expressed_interest_at) AS expressed_interest_at FROM (SELECT * FROM table1 UNION SELECT * FROM table2) AS u GROUP BY u.email;
Forgive me if I am missing something, but why do you have the f_scan_sdid clause in the 1st but not the 2nd, and why are you doing that weird date thing? Greater than or equal to and less than or equal to 2018-09-18 is . . . 2018-09-18.
Agreed. In the real world, people also forget the definition of Foreign Keys. So no software will be able to make the relationship, in example: BookId in table1, to Id in the books table, and such.
You could try a tool like http://schemaspy.org/ to explore the database.
select TABLE_NAME, COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH from INFORMATION_SCHEMA.COLUMNS order by 1, 2 
I spend a lot of time using SQLite specifically. I wrote a tool called [sqlite3todot](https://github.com/chunky/sqlite3todot), which renders sqlite3 databases to a directed graph [ERD type thing]. There's an old example [here](https://icculus.org/~chunky/stuff/sqlite3_example/sqliteschema/complexexample.png). The point I found most useful about this tool is that it allows for grouping of tables. Note the example there has stuff called "input" and "output". When I'm exploring a new database, I'll usually start to group up tables into things that look like logical groups. If I see a bunch of tables that are metadata about people, I'll shove them all into a group, re-render, and see what it looks like.
one can only hope it's called BookId. I've seen "ParentId" more often than I care to remember... and no, there's no "Parent" table... it's just a reference to the ID of its logical parent in the "gee the business knows that [x]'s parent is [y]" kinda way.
##r/PowerShell --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/853qg2/ignore_list/)
Visio should (or, used to at least) create a database diagram.
Python + SQLAlchemy + NetworkX. Some database have built-in GUIs for RDB analysis. When you know how to use SQLAlchemy to reflect a database, that ORM really shines. Add it to NetworkX for graphing and you've got a simple script that could apply to many database in exploring relationships among the database. The more SQL Specific approach is really exploring the information schema as u/r3pr0b8 mentioned. You get a lot of detail from the database that way and it's agnostic to whatever database system you're using.
I always try to see a diagram if not I‚Äôll run queries. If using sql server management studio (free), you can right click and create diagrams. 
Start here: https://wiki.postgresql.org/wiki/PostgreSQL_for_Oracle_DBAs
For that, since they are separate paths, you'd need to break the nodes out and parse from there: SELECT OfferStartupCosts, OfferNoLoadCosts FROM OPENXML (@idoc, '/ScheduleOffer',2) WITH (OfferStartupCosts XML, OfferNoLoadCosts XML) After that, you'll need to [Query](https://docs.microsoft.com/en-us/sql/t-sql/xml/query-method-xml-data-type?view=sql-server-2017) or Value them out depending on what you need. 
the sql server query is sadly using a not so great "feature", that say that the "day of week" part is dependent of the language and regional settings of the current session. If, for example, you set your session to "french", DOW 1 is Monday. In US English, it's Sunday. Also, the cast(xxx as date) might behave differently depending the "set dateformat" value. You might want to set it explicitly before doing the cast() Look at "set datefirst" and "set dateformat" for more infos. You can see your current settings with "dbcc useroptions" &amp;#x200B; Otherwise, as a guy that spend 5 days a week in sql server, your interpretation of the sql server query is correct. It selects rows where "data\_updatedate" (I suppost it's a string)casted as a date is older that 3 month and which the day of week is either "Sunday or Monday" or "Monday and Tuesday", depending the dateformat I wrote before. Again, given that the "AND ((DATEPART(dw, data\_updatedate) + @@DATEFIRST) % 7) NOT IN (0, 1)" is doing an implicit conversion of the supposed varchar field to a datetime (via the datepart) you might get results that doesn't match the expectations. Sadly, it's been 19 years I haven't played with mysql, so I cannot help you much on that side. Maybe someone else can chip in.
Are you forgetting the AS statement? So SELECT nm.[named field] FROM [table] AS nm; ?
I would make the ingredient talbe reference the product table. Give the ingredient table a foreign key to reference products to get a one to many relationship. For example: Product: cake [Primary Key: 50] Ingredient : egg [Primary Key: 1, Foreign Key(references the Product Primary key): 50, Quantity: 30] Something to this extent. Or that's how I would go about it.
I'd say a join table between `ingredient` and `product` makes more sense. A lot of products probably have butter, eggs, and flour. The supplier of butter (there's a supplier fk in the ingredient table) isn't going to change between products.
No my boss is paying me to do this. 6 figures
Do you get an error? If it's an incorrect syntax error, try adding AS before the table alias. 
Maybe something like this? [https://www.google.com/search?q=sql+join+table&amp;rlz=1C1CHBF\_enUS812US812&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwigosba5cXdAhUjqlkKHflvCf0Q\_AUIDigB&amp;biw=1280&amp;bih=579#imgrc=VwIjdDxe-8qy4M:](https://www.google.com/search?q=sql+join+table&amp;rlz=1C1CHBF_enUS812US812&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwigosba5cXdAhUjqlkKHflvCf0Q_AUIDigB&amp;biw=1280&amp;bih=579#imgrc=VwIjdDxe-8qy4M:)
You would need to convert the video / image into binary and store it in blob fields using a general purpose programming language. &amp;#x200B; It's not typically a great idea to story video files a database though. Usually a better idea to story paths/links to the actual video files that are held externally. You will likely have trouble at some point with any size-able video files. &amp;#x200B; [https://wiki.postgresql.org/wiki/BinaryFilesInDB](https://wiki.postgresql.org/wiki/BinaryFilesInDB) [https://www.postgresql.org/message-id/51494DB187D98F4C88DBEBF1F5F6D42302043789%40edb06.mail01.enterprisedb.com](https://www.postgresql.org/message-id/51494DB187D98F4C88DBEBF1F5F6D42302043789%40edb06.mail01.enterprisedb.com) &amp;#x200B;
"I have the prompt for" was what gave it away, right?
I call it a join table too, but for academic purposes (since it appears we're helping with homework) it's the implementation of a many-to-many relationship.
Thats what i asking, how to store the files else where and have them linked to my database
LibraryIndex, LibraryTag, LibraryCode?
This. I hate that so many people cheat on their homework and the internet is the great enabler. 
Generally UID, UUID, or GUID could be used. For an ID like the YouTube ID, I would choose ‚ÄúUID‚Äù (‚Äúunique identifier‚Äù) because UUID (‚Äúuniversally unique identifier‚Äù) and GUID (‚Äúglobally unique identifier‚Äù) have more defined meaning.
So much this. Students shan‚Äôt use their resources!
Why don't you look at the relational model of an ERP system? 
That's okay, I'm kind of a novice myself but let me try to explain based on the data that I can see. The question is asking you to return only the **shipments** that contain an item with a value ‚â•1000. In the four rows you are looking at from SHIPMENT_ITEM, there are four **items** with a value ‚â•1000. But now look at the ShipmentIDs listed associated to those 4 items in the SHIPMENT_ITEM table. One of the items valued at ‚â•1000 is in ShipmentID # 3. The other three items valued at ‚â•1000 are in ShipmentID # 4. So to answer the original question, there are only two shipments that contain **at least one** item valued at ‚â•1000. Even though ShipmentID # 4 contains three items valued ‚â•1000, the shipment would only be counted once.
Yeah I don't fucking believe you.
Why would your job have you creating a sample DB that's not real world realistic? No "small bakery" is creating a database of ingredients. This is homework.
I use SQL to help automate some of my daily work, and for that it cultivated a massive amount of interest. Also, use stored procedures to create power bi dashboards from a reporting standpoint. SQL is what has me interested in data and this new found passion over the course of 2-3 years in my young professional life.
Data Diagrams physically alter the schema. If you add a FK relationship in the diagram, it will lock tables and create it in the database. Not saying it's a bad choice, but may not be able to option in all environments.
&gt; I was wondering, how would I go about the case where the product has multiple ingredients? If you have a many-to-many relationship, like where many ingredients go into a product and many products use the same ingredients, you create an additional table to store the mappings: Ingredients ------------- IngredientID (PK) Name Product ------------- ProductID (PK) Name ProductIngredients ---------------------- ProductID (PK, FK) IngredientID (PK, FK) Quantity UnitOfMeasure (ie: tsp, tbsp, 1/2cup, cup, etc) 
lol seriously cannot take a joke
Is StackOverflow for programming courses also considered cheating?
 delete from myTable where crt_timestamp between timestamp'2018-09-17 00:00:00' and timestamp'2018-09-18 23:59:59' and colID = 'myIDvalue' and someOtherCol = 'somethinghere'; But whatever statement you create, be sure to copy the WHERE clause and run it in a SELECT first, to see how many (and which) records you'll be affecting: select * from myTable where crt_timestamp between timestamp'2018-09-17 00:00:00' and timestamp'2018-09-18 23:59:59' and colID = 'myIDvalue' and someOtherCol = 'somethinghere';
It becomes more interesting the more you use it. Especially when you have a large sample DB to play with (with lots of tables).
At this point you, since you have to stalk a profile and took that job statement seriously lol
Searching for things to help you understand things is different from just flat out asking people what to do.
Once you have a real world problem that isn't easily solved by simple queries, you'll find yourself getting much more interested in learning it and figuring out new better ways to solve problems.
Thank you. That explanation has been very helpful. This says MS SQL @@DATEFIRST is 7 in US (by default, unless set explicitly or by language as you are saying): https://docs.microsoft.com/en-us/sql/t-sql/functions/datefirst-transact-sql?view=sql-server-2017 &gt; For a U.S. English environment, @@DATEFIRST defaults to 7 (Sunday). And while I can't find a @@DATEFIRST equivalent in MySQL, this tells me MySQL developers would rather have Sunday as 1 (The 16th of Sept in 2018 was a Sunday): SELECT DAYOFWEEK('2018-09-16') Gives me 1! 
The payoff comes when you start finding ways to save yourself time. Nothing feels better than crossing off and laying to rest a monotonous and repetitive part of your work day. 
I don‚Äôt find SQL itself personally interesting, but rather the things it enables me to do. I work in analytics and the analysis is the part that drives me, SQL is just one of the many vehicles to get there. 
I see UID, I think User ID.
If you were to look up a set of rows by category, for instance, the primary key would be useless. The additional keys enable the table to be joined or searched on these other attributes efficiently, using a b-tree search rather than a full table scan.
Convince them that SQL will help them achieve their goals. 
Thanks for the recommendation. Is there a way to use the same database they use in the video?
Yea, I do have to user it for, user, orders, shipments, items, assemblies... and more. I'm thinking KID for key ID. Since the field functions as a key...???? ¬Ø\\\_(„ÉÑ)\_/¬Ø
I'm thinking KID for key ID. Since the field functions as a key...???? ¬Ø\\\_(„ÉÑ)\_/¬Ø
hahaha dude you're so right. i've totally been there but it gets better. i think what sparked interest for me was seeing how different problems get solved using SQL. I use the Microsoft stack and lately I've been fascinated by views and table-valued functions. I recently reverse-engineered some legacy code where the original author pulled data from a staging table (which was loaded via an ETL package), joined it together with another one containing business rules and a 3rd holding static mapping values to create an export dataset to reconcile purchase orders, all using a function that accepts parameters, which can be joined onto itself to create an aggregated final result set. very neat! prior to seeing this model I would have probably used a snowflake arranged group of tables with various dimensions and a stored procedure that required storing the calculated values somewhere, which would have required a process to run and compute those calculations...a much less elegant design.
CTEs, query hints, error handling, transaction processing, coalesce.
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
Never heard someone say ‚ÄúI‚Äôm getting fired‚Äù in place of ‚Äú I‚Äôm failing my Intro SQL class.‚Äù
Unpopular opinion here but no rosy feeling will come from any advice given to you as long as your attitude towards SQL remains as it is. I would say change your attitude. Make a conscious effort to enjoy solving problems with SQL (challenging exercises can be helpful here). You have to cultivate the interest in the subject yourself. &amp;#x200B;
Won't this delete all the records which meet the criteria? I'm guessing OP wants to keep one, and delete the rest.
If youre looking for a way to get a row per date, you could join the table to itself 3 times, joined on date, with each table join for a different database.
use the PIVOT operator
I tried that, but I think I messed up on the syntax. 
You are a godsend! Worked like a charm.
Instead of retrieving information like a well, retriever, look into SSIS and start building something. 
I find performance tuning in SQL an art and for me when my work day requires getting a query that runs 15 minutes to finish in 5 seconds is a happy day. learn the more complicated stuff as well like recursion and hierarchical queries or even aggregate functions and window clauses are fun. (to me at least) but you know, everyone has different likes. mybe you just really dont like it or dont like working with data. also I find that programmers in general see sql as a toy and find it stupid or useless. so maybe because its not a programing language you dont find it interesting / challenging? 
What's a query hint? Is that the same as OPTION(OPTIMIZE FOR Condition)?
Oh, the rows are 100% duplicates? This might work: https://stackoverflow.com/questions/15492004/delete-top-2-in-access DELETE FROM (SELECT TOP 1 * FROM Table1 WHERE .....)
Yes
So they're creating indexes on those columns.
I'm guessing that's why the two were named `icategory` and `igroup`.... to imply they're indexes.
Pivot, rollup, rank, row_number. Lots of excellent functions really!
`as` is optional in most SQL dialects. Funny enough, with the (ancient) Oracle server I sometimes use at work, `as` will break the query if used in a from clause, but works fine in the select clause.
Predictable and proven examples aren't very exciting. It was relatively boring for me until a query I used successfully on every other dataset ran many hours without returning the result set. That's when I really started to learn about SQL.
What type of server is the pass-through connecting to? What does the error say? That query looks fine for MS SQL Server, but there's also no reason to specify a table alias when you're only using one table--just drop the alias from the from clause, and from the columns in the select clause. `SELECT [Named Field] FROM [Named Table]` If possible, connect directly to the server in its client (e.g. SSMS) and write / run the query there, and then copy and paste into Access. Keep in mind that the Access editor is worse than Notepad, and hates tab characters. (the cursor will render in the wrong location, making editing confusing and difficult).
Subqueries (both self contained and correlated.) It won't hurt to read a good SQL book too.
Huh. Good to know!
Yep. A simple select query is pretty boring. Working from table to table looking at keys and relationships in order to join in some data to your query that doesn't have a straight join, that's much more interesting. I also (as someone who's only been working professtionally with SQL since June) find working with temp tables kinda fun, and window functions.
Oracle. I am using aliases because there are multiple tables and the `Named Table` name is too long. I don't even know how to properly write it on a passthrough query. I tried `[Named Table].[Named Field]` and it does not freaking work.
TRUTH. Mine does not work if I added `AS` in the `FROM` clause. this is so confusing. It does not help that I do not have any other SQL s/w or editor I mainly use Access and so the errors are so broad and vague it's downright unhelpful.
How can a table name include a space?
If you're doing analysis etc I'd recommend getting a handle on indexing, especially related to how to structure your queries to best use the available indexes. 
Okay, that's probably it. It looks like Oracle uses quotations, which is the ANSI standard. `SELECT nm."Named Field" FROM "Named Table" nm` https://stackoverflow.com/questions/6468337/oracle-sql-syntax-quoted-identifier
The best way to get interested in SQL is when you have to use it to solve a real problem or find something of real benefit that would otherwise have been left on the table.
Thanks. I am needing to grab the full Schedule offer and write data from each node into a table. Would there be a way to do this with one query?
For you to be able to search for something, someone had to ask the question first 
SQL for Dummies is a really good starting point. Rather than immediately teaching you how to start programming it starts with database hygiene an making sure you don't trash the place while learning it. After that it's better to find something interactive, something that shows you immediately what you're doing while you're doing it. I haven't found the best choice for this yet.
select * from TABLE where [Order Number] in (select [Order Number] From TABLE where (DUE_DATE &lt; SYSDATETIMEOFFSET ()))
Sorry, I‚Äôm not sure if this is a public demo database but what is a ‚Äòpurchase order line‚Äô?
https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/ This starts off beginner friendly 
I don't know what system you're using but here's something that might work in SQL Server if object_id('tempdb.dbo.#PurhcaseOrders') is not null drop table #PurchaseOrders create table #PurchaseOrders( OrderNumber int, Lines int, CoNumber varchar(3), Customer_Name varchar(10), Item varchar(2), Qty_Ordered int, Qty_Ready int, Due_Date date, ) insert into #PurchaseOrders values (1551, 1, 'V42', 'WAL-MART', 'AB', 1, 1, '9/17/2018'), (1551, 2, 'V42', 'WAL-MART', 'BC', 1, 1, '9/17/2018'), (1551, 3, 'V42', 'WAL-MART', 'CD', 4, 4, '9/30/2018'), (1551, 4, 'V42', 'WAL-MART', 'DE', 2, 2, '9/17/2018'), (1551, 5, 'V42', 'WAL-MART', 'EF', 2, 2, '9/17/2018'), (1551, 6, 'V42', 'WAL-MART', 'FG', 1, 1, '9/17/2018'), (1551, 7, 'V42', 'WAL-MART', 'GH', 1, 1, '9/17/2018') select p.* from #PurchaseOrders p inner join ( select distinct OrderNumber, min(Due_Date) as EarliestDueDate from #PurchaseOrders group by OrderNumber ) p2 on p.OrderNumber = p2.OrderNumber where p2.EarliestDueDate &lt; GETDATE()
Purchase Orders are used by companies to order goods and services from another company. Basically the opposite of a Customer Order. Each of which contain line items of what was purchased (or ordered by a customer). For instance, Purchase Order 10001 (The overall Purchase Order - Header level), this will have lines: 001 - Cool Widget 1, qty 1, price 342.67 002 - Warranty Service 1 year, qty 1, price 27.50
Assuming created date is unique for each subset of records, you can create another table that mins(created date) and groups by your duplicate values. Then join this table back to your original table on and delete anything where the created dates don't match. so for : Id Name created_date 348 John Doe 9/19/2018 943 John Doe 9/20/2018 452 John Doe 9/21/2018 189 John Doe 9/22/2018 delete from table left join ( select Name min(created_date) as dt from table as creation group by Name ) on table.name = creation.name and table.created_date = creation.dt where creation.Id is null
Hi. Not sure if I was being unclear. Created date is not unique. I have a set of say 500,000 records across 25 columns. I will use duplication across 5 of the 25 columns to determine whether the records are duplicate. That is, if 2 or more records have the same: - Name - Creation Timestamp - Regular ID - Assigned To Then those are all considered duplicate, no matter if they have differences in 'Category', 'Root', or any other fields. So I want to delete all duplicate entries such that there is only one left. Problem is, Access does not have row_num() / PARTITION ON capabilities. The records all also have a unique primary key ID.
[SQL Code Smells](https://www.red-gate.com/simple-talk/sql/t-sql-programming/sql-code-smells/)
I like livinglifelazily's answer, however I prefer CTEs over derived tables for readability reasons. (personal preference) ;with POList as ( select OrderNumber from #PurchaseOrders where Due_Date &lt; GetDate() group by OrderNumber ) select #PurchaseOrders.* from #PurchaseOrders inner join POList on #PurchaseOrders.OrderNumber = POList.OrderNumber &amp;#x200B;
Watch my talks about SQL: - [Ten SQL Tricks that You Didn‚Äôt Think Were Possible](https://www.youtube.com/watch?v=yuuhkHORzfM) - [How Modern SQL Databases Come up with Algorithms that You Would Have Never Dreamed Of](https://www.youtube.com/watch?v=wTPGW1PNy_Y) You'll never want to code in a 3GL again.
Get an open source database running on your box at home, then create a database and import system to pull in all your emails. The data mining you can do just kinda dicking around with an email database as it grows over time, with spam and such, is kinda neat. Who emails you and how often? Can you predict the sender based on their use of language? Normalize the From addresses into a contact management database. It's a lot to chew on. I, admittedly, have been doing this for a couple decades, so my email database is a little insane (I've got almost every one I've sent or received since the early 90s.) But I've found some interesting diversions in teaching myself some data mining with it. 
udemy business intelligence and data anlaytics
SQL For Beginners form Oreilly Media. It's how I got my humble beginnings.
I started learning with the data camp‚Äôs sql tutorial.
Piggy backing on this. Is there anywhere that gives some kind of certificate that could be added to a resume? It's ok if it's paid.
Thank you!!! It worked perfectly. I was way over complicating the query.
In my opinion, the only certs that hold real value are difficult and not entry level certs. There are folks who obtain these certs without prior experience, but it's hard work. When I see certs listed on a resume that are not from a reliable vendor like Oracle, Microsoft, etc, I mostly discredit them. The takeaway I get is that you had the initiative to spend at least 5 minutes and possibly a few dollars to add the cert to your resume. &amp;#x200B; So there are certs you can add to your resume, but expect to put in 60+ hours of effort and $150+ into it. 
And for someone that wants to delve past the basics, and has yet to do so in his job (but who will need to in the near future), where would one find these difficult certs?
 SELECT CASE WHEN LEN(phone) &gt; 2 THEN 1 ELSE 0 END AS lc , COUNT(DISTINCT bi_tickets.ticket_id) AS Transactions , SUM(bi_tickets.Gross_Line_Total) AS Gross_Sum , SUM(bi_tickets.Gross_Line_Total) / COUNT(DISTINCT ticket_id) AS Average FROM customers INNER JOIN bi_tickets ON bi_tickets.Customer_ID = customers.id WHERE {{DATE}} AND customers.phone &lt;&gt; ' ' GROUP BY lc ORDER BY Transactions DESC 
I made my best attempt at adopting the code to my database but failed miserably - I'm defintely saving this for future use though when i better understand it
I would look at Oracle and SQL Server, those are the two major players who have certificates. I found the SQL Server certs difficult but fair, they are definitely passable with enough perseverance. 
Correct, when you buy 5 things at a store and see 5 items on the receipt each item is a 'line'. So because you're the customer they would be purchase orders lines in your POV and they are customer order lines in the POV of the store &amp;#x200B;
What college are you attending?
sounds silly but this manga guide to database helps me get the grips of database, and this is coming from someone with zero knowledge about sql and databases in general https://nostarch.com/mg_databases.htm
So the link table ProductIngredients will contain every single product right? So there will be repeating ProductIDs and repeating IngredientIDs? i.e. Sandwich Eggs Sandwich Bread Sandwich Cheese . . . does a link table imply repeating of the PKs? Why would IngredientID be a PK, it does not uniquely identify a product? Shouldn't it just be an FK?
Yes; 'key' is a synonym for 'index'.
I know you have a working answer but maybe for next time... How about something like where isnull(columnA, columnB) != null ?
Just to simplify your logic you don't need your third constraint. 3 is a subset of 1 and 2. Once you've checked for 1 and 2 there won't be any additional rows that are 3. All 3's are also either a 1 or a 2. You can't have a row that is a 3 that isn't also a 1 or a 2. Am I sounding redundant yet? So are your constraints. :P
www.w3schools.com www.stackoverflow.com https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-vid-introduction_to_sql/
Get your own database on the server to use as your sandbox if you can. Being able to create derived tables and your own views is very useful for my team. Also: union, union all, except, indexing, user defined functions, stored procedures (w/parameters), jobs, and SSIS packages.
When you say "joins", does that include left joins and outer joins?
What you were doing before is called a "cross join". Basically, every row in faculty is matched with every row of department. Do ypusee how your first results show every person works in every department? By specifying a column to join on ( DeptCode) you only match where the values are identical. More commonly you would write this as: SELECT .... FROM faculty f INNER JOIN department d ON f.DeptCode $ d.DeptCode
Forgot to mention that I do work with subqueries. Pretty often too. Will have to look into the difference between self contained and correlated though. Thanks!
Seems to be a popular answer. I'll definitely take a look into them.
It has a character show u all things about database??
Thank you! I actaully ran into another issue... again, i know I am close, but cant seem to get rid of duplicates 
You can do this using case statements. Think of all the different ways the time periods can overlap and have a check for each one.
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
It's TOO basic for someone who's just starting to learn SQL today? ...... Don't ask for help with resources if you're going to poo poo on the advice you receive.
The OR keyword means any rules already applied are ignored and only the rules after the OR word are evaluated together. You can use Parentheses to contain multiple statements so they are evaluated together like in BODMAS. A working version of your first example would be something like this. SELECT DISTINCT fname , lname , deptname , rank FROM faculty, department WHERE (schoolcode = 'CSE' OR schoolcode = 'HSH') AND faculty.deptcode = department.deptcode ORDER BY lname, fname ASC It should also be noted that using , to represent joins is not the current standard (and hasn't been for a long time). It tends to make queries with multiple joins difficult to read, if possible you should stick with using the Join Keywords. 
Hey u/zarco98, check out this [link](https://stackoverflow.com/questions/9113204/t-sql-how-to-select-values-in-value-list-that-are-not-in-the-table) there are a couple of simple solutions there.
Thank you!
Sounds like homework so I'm not just going to give you the answer but I'll try and point you in the right direction. Look at what you're asking to do, you're asking to compere the list of cod codes you're looking for to a result set from a query. There would be any number of ways of doing that if that list of codes were in a table even if it was just temporarily.
It's not homework actually. I knew the concept of what I was trying to do (basically a left join where B value is null) but I was struggling when trying to create a table variable with those list values. Solved it with a few find and replaces in Management studio. Thanks for your answer anyways!
Just checked out rollup yesterday and it's pretty neat. 
I'm going to use this: [https://wiki.postgresql.org/wiki/User-specified\_ordering\_with\_fractions](https://wiki.postgresql.org/wiki/User-specified_ordering_with_fractions). It accounts for insertions, swaps and reordering without constant expensive renumbering.
Try it with a `join` instead of `in` and see if it runs better. Also, https://www.reddit.com/r/SQL/comments/9hdbdq/ms_sql_could_you_help_me_with_this_probably/e6b60el/
You're right, I should have pointed that out, but the poster has asked specifically about the variables so I was showing them the correct syntax. I used varchar(max) because I have no idea what their actual codes look like and for a one off ad hoc query it doesn't really matter. 
Self-taught.
Unfortunately JET won't allow case statements. core SQL or nothing :(
http://www.devguru.com/content/technologies/jetsql/home.html That Jet? 
You need to aggregate before you do the left join, usually with a subquery. 
Thank you! I'll give that a shot.
Do you want to keep the duplicate rows? If so, try using sum operator with window function. Or use a case with dense rank to assign an integer and sum 0 when it's not the first occurance. Otherwise I think you'll have to fix the subquery. 
It's basically to pull people into the query with no gifts, but who have proposals.
"The version of SQL created by Microsoft is called Jet SQL and it is the database engine behind Microsoft's Access. " Yep! That's the one.
I'd find the IDs in gift that are also in prospect (if I've understood your structure correctly) as that would give you the list of people who have either a gift or a proposal. Then join that onto gift and people to get your aggregated value. &amp;#x200B; I think.
Try adding top 1 to your select in the subquery. 
That did the trick! I realize I should probably rewrite it more correctly, but this works so I'll go with it. Thanks again!
By ODBC do you mean as ODBC data source in Windows? I think you need to download ODBC driver for MySQL. Either way you use MySQL syntax. For switching between different SQL dialects I have often used http://troels.arvin.dk/db/rdbms/ as a reference. However it might be out of date a bit. MySQL documentation is probably your best friend.
Yes I do mean the ODBC data source in Windows and I did download the MySQL driver. The login/host credentials I was provided by one of our developers (not an expert in DBs) didn't understand what I was trying to do, so he couldn't explain why our local MySQL server wouldn't authenticate with the information he provided me.
Awesome. I will look into these. Thanks for the recommendations!
I follow you I think. I believe you almost got it. Move the proposal join into a WHERE EXISTS follow by an or g.id is not null. It can be read as, I want the people that have gifts (g.id not null) or they have proposals (WHERE EXISTS) 
Maybe firewall or rights issue. Difficult to be sure without more information about the configuration.
Oh right of course! Thank you!
I just came across \[this talk by Curtis Poe\]([https://www.youtube.com/watch?v=y1tcbhWLiUM](https://www.youtube.com/watch?v=y1tcbhWLiUM)) about figuring out basic database design. Nothing terribly in depth, not a SQL tutorial, but a walk-through about how to take a simple word problem and break it down into a rough database design.
The code I used the refence this view is: SELECT d.DEPARTMENT\_NAME, l.STREET\_ADDRESS, l.POSTAL\_CODE, [l.CITY](https://l.CITY), l.STATE\_PROVINCE, c.COUNTRY\_NAME, r.REGION\_NAME FROM DEPT\_DETAIL\_VIEW WHERE REGION\_NAME = 'Europe';
What advice would you give to someone who doesn't know about that configuration?
**sqlite** doesn't have datetime or boolean types. https://sqlite.org/datatype3.html
Codewars has some sql problems. It's free.
OK, I'll check that out!
Performance anxiety is 100% real. &amp;#x200B; You've got the experience but maybe you just interviewed poorly? Did you talk out the process during the interview or code silently?
See if this attend your need. SELECT cast(reverse(cast(101566100 AS BINARY(4))) AS BINARY(4)) That will return: 0x94C60D06 , all you need to do is remove the "0x" if you don't need it
Well, because it's his, of course..
You might be good at SQL, but you also might be good at using the data you are used to and that masks some deficiencies. In other words, maybe you should at least review some of the concepts that you might not have to deal with because they've been something you haven't dealt with in quite some time.
Does this look correct? : https://imgur.com/a/fOqxoEt
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/0Oh6QNJ.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
How does this look? https://imgur.com/a/fOqxoEt
Nope... Recipes should have both ProductID and IngredientID as PK and FK. Otherwise you'll only be able to have a single record for "Cake", and therefore only a single value for Ingredient. By making both ProductID and IngredientID PKs, the PK is the combination of those two values, so you can have many ingredients in a single Product. Also, your Sales/ProductQuantity tables.... the most commonly used design for sales orders is to have Orders/OrderItems tables. Where the Order contains the header records (customer, date, how the order was paid for, etc) and the OrderItems table contains the item details (productID, quantity, price, etc). Orders ---------- OrderID (PK) CustomerID (FK) Date OrderItems --------------- OrderID (PK,FK) OrderItemID (PK) ProductID (FK) Quantity ItemPrice Discount 
I'll save you a hell of a lot of clicking. https://docs.dbatools.io/#Reset-DbaAdmin If you're using SQL Server and *don't* have dbatools, you're missing out.
Doesn't setting the database to Single-User Mode come with it a good number of dangerous scenarios that can possibly happen? SQL Server isn't my RDBMS of choice, but from my understanding doing this should be only used as a last resort from some of the SQL Server DBAs I've spoken with. 
thanks, I fixed the double HAVING. But I am still not able to make this illegal SQL command.
From the subquery, I need a list to pass back to the main query which will do more reporting. This has to be done via a subquery or perhaps through stored procedures or manually (i am trying to avoid these two routes)
It all depends on how busy your server is. Probably you should review memory allocation settings and perhaps CPU and IO affinity if you have a really busy system to avoid performance surprises. For lesser loads, the SQL Server should cope with other applications on the same server.
You need to copy the backup to the database server's local storage before restoring.
I store columns of data on my chair.
&gt; I need a list to pass back to the main query which will do more reporting should've said that at the beginning what you posted was SELECT ITEM_NUM FROM TABLE A WHERE ITEM_NUM IN (SELECT ITEM_NUM ... which **clearly** is begging for simplification next time, post your actual situation please 
Standing desk.
Is there some tutorial on how I can accomplish this using visio 2010? Thanks!
CASE is ANSI SQL.
It's Russian and kind of Spartan, but it's really damned thorough and addictive ... http://www.sql-ex.com/ 
In my instances, I rename the SA login to something else, set the password to a 20 character random string, and then throw the password away. I use an added security group to administrate the SQL instance and if shit every hits the fan, I just deploy a new installation and restore databases.
i really liked this one when i was first starting out
Hopefully [this](https://github.com/dbeaver/dbeaver/issues/2764#issuecomment-357040867) helps.
Ohhhh thanks!!
What kind of questions did they ask? What type of SQL queries did you write? I also thought I was good at SQL. It turned out I was good at writing crappy code that managed to work in a messed up backwards environment. I did not know about performance tuning, statistics, benefits of certain patterns, etc. 
&gt; Doesn't setting the database to Single-User Mode come with it a good number of dangerous scenarios that can possibly happen? It does, but as you rightly pointed out you're only doing this in a situation where you're completely hosed if you don't get admin access, have no other means by which to get it, and you put the system into and then back out of single-user mode _very_ quickly. Which is where the PowerShell function I linked to comes in - it accomplishes this *much* faster than a human would.
yes for sure test/performance aniety. when my co-workers come to me with a problem and are behind me or watching me troubleshoot i either start stubling or forget where to look for things. it's totally different when I'm on my own. it's really frusterating!!
Which rdbms? 
Woops sorry, postgres. PSQL.
Thank you very much for your answer :) 
Sure can. Started learning the syntax yesterday. While it is similar, it‚Äôs still different.
And yes I can connect to our db via an SSH tunnel
Did you try [the doc](https://www.postgresql.org/docs/current/static/functions-matching.html)? Is very good.
What is even the point of this? Once you find the reg ex column then what? For the most part, I would think that column is useless to you. 
Ahh alright. So I want to limit the right of the user as much as possible. Any idea how I can search through the database in the easiest way? There a lot of tables and I don¬¥t know yet, which ones we are going to use for Power BI.
Correct. This article is a watered down version of the microsoft page https://blogs.technet.microsoft.com/sqlman/2011/06/14/tips-tricks-you-have-lost-access-to-sql-server-now-what/ Basically a pointless article.
I almost feel like blocking an entire domain is excessive, especially if that user changes and decides to create quality content and becomes more of a community contributor than spammer. &amp;#x200B; I think a combination of downvotes + ability to report low quality blog posts to be removed is a better solution but obviously has a higher administrative overhead and possibly negatively affects user experience. &amp;#x200B; So maybe somewhere in the middle? If the user is seen as someone who contributes to the community and generates higher quality articles, then they can have permission re-granted under stipulations and can be removed from the banned domain list?
Not sure which RDBMS you‚Äôre using. If SQL Server, then INFORMATION_SCHEMA.tables or sys.tables. If Oracle, DBA_OBJECTS. What I‚Äôve done is built views for PowerBI to access. This ensures that even if columns are added to a table, PowerBI will not see them unless I allow it.
You probably can use SSIS and Cozyroc to essentially do a full load and truncate nightly. Here's questions I'd ask and think about. * How frequently does the data need to refresh? * What kind of load is my full load and truncate having on the read only and read / write instances? * Does this affect other users? * What is the isolation level? You say read only, I hear read uncommitted, are you sure your data will be accurate? * How important is this accuracy? * Is working with IT to get Read access through Tableau out of the question? For data warehousing, see Kimball's books. I'm working my way through the data warehouse book and studying for the implementing a data warehouse with SQL Server currently. 
So create a local database and use SSIS to copy data from the VPN database into the local database, and connect Tableau to the local database. In the local database take advantage of columnar storage etc to use cases which are batch-write/read-heavy. Don't import data that is irrelevant from a reporting/analysis perspective. How you do the ETL is highly dependent on the structure of the production DB. If rows have timestamps, then you can build SSIS packages to only bring in the day's changes. If not, then it could get a bit more painful, but it really depends on your connection speed, quantity of data, and acceptable data lag for the data warehouse.
1. The sample data does not match the sample result set. The dates don't match. 2. The query does not match the sample result set. The query will produce unique Store/Business Date combinations, but result set has two rows with the same combination. There is something you didn't share accurately, and it's crucial to understanding the problem. This is the output of your sample data and sample query. Store|Business_Date|Cashier Name|a|b|c|d|e|f|g -:|-:|:-|:-|:-|:-|:-|:-|:-|:- 100|2018-09-18 00:00:00|Toya|0|2738.74|8.52|-2.74|0|20.5|0
How current does your data need to be? Daily? Hourly? 
Sorry, I fixed it. But you are right, that is the result when I pivoted and I expected 2 rows for the result. Any idea why?
&gt;especially if that user changes and decides to create quality content and becomes more of a community contributor than spammer I agree some method for them to get out of the blacklist should exist but I would not hold my breath that https://www.reddit.com/user/roytrilok91 will start generating good content considering they thought https://ms-sqlserver-dba.blogspot.com/2018/09/how-many-cpu-support-by-sql-server-web.html is something worth putting online. I do agree that this is a good route forward: &gt; So maybe somewhere in the middle? If the user is seen as someone who contributes to the community and generates higher quality articles, then they can have permission re-granted under stipulations and can be removed from the banned domain list? Problem is these users just post their blogspam, offer no discussion and just water down the good content that this sub has. If you look at the particular user I posted all they do is post crappy blog articles here and at /r/sqlserver Literally no comments and only crap blog posts. I'd suggest we just ban the user except they will just roll another account, hence domain blocking seems to be the only method to get rid of these persistent spammers.
Here's a sneak peek of /r/SQLServer using the [top posts](https://np.reddit.com/r/SQLServer/top/?sort=top&amp;t=year) of the year! \#1: [Microsoft¬Æ SQL Server¬Æ Notes for Professionals book](http://books.goalkicker.com/MicrosoftSQLServerBook/) | [30 comments](https://np.reddit.com/r/SQLServer/comments/7nv6uj/microsoft_sql_server_notes_for_professionals_book/) \#2: [Rebooting tonight...Wish me luck!](https://i.imgur.com/VdbPnQK.png) | [23 comments](https://np.reddit.com/r/SQLServer/comments/83x3d5/rebooting_tonightwish_me_luck/) \#3: [Sorry if this post goes against sub rules, but I just nailed my 70-461 exam in the first try and am pretty damn happy about it](https://np.reddit.com/r/SQLServer/comments/7c0kis/sorry_if_this_post_goes_against_sub_rules_but_i/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
Daily would be enough. We're not talking about maybe 200,000 rows, 300 or so columns across 30 tables.
I have set up a Python script which connects to a remote DB, pulls data and inserts into a local DB, then does an analysis and sends me an email with the results. I have a Jenkins job scheduled to run the script daily. This sounds like it could work for your situation.
&gt; Problem is these users just post their blogspam, offer no discussion and just water down the good content that this sub has. &gt; &gt; I agree some method for them to get out of the blacklist should exist but I would not hold my breath I agree on both counts, which is why I think a domain block is fine as long we have the ability to give folks redemption. I don't think they would begin to change how they interact with the community, but I'd rather give them that opportunity than a hard ban. I'll add to this that I have a bias as I post my website on Reddit and other community forums too. I work hard on that content and I try to keep it relatable to the subject without spamming or posting overly frequently. I just wanted to note that to users who do not post their own material on this sub, I do come from a biased perspective so my opinion may need to be taken with a grain of salt from the community. 
Domain block. If the user starts making worth while content then they can contact the admins and get their domain approved after showing admins their good content. 
It's a great time to learn! 
This is going to come down to a cost of needs and wants. Can you afford the storage to duplicate data? I would STRONGLY recommend to pull data down with either full refreshes or some way to compare the read only to your new copy to ensure it's a 1:1 each pull. Incremental will help, but the read uncommitted piece makes it troublesome. Once you have a copy of the read only data, you can begin to work on a data warehouse piece to aggregate and mine the data. The data set you have pulled down is likely structured for Online Transactional Processing (OLTP) and not structured for Online Analysis Processing. Meaning it will perform well for writes but reads may take a hit. You want your data warehouse to perform quickly and efficiently for reads. Once you have a solid warehouse created, you can begin the next piece which is to cube the data so you can begin to really slice and dice it in all sorts of dimensional ways. I would highly recommend the Kimball book still, it's a relatively light read. (Comparing to the 4,000+ pages of technical manuals I have combed through in the last year.) If you can't afford the duplicated storage, I'd probably look towards how well the structure performs with your environment as a straight copy from the vendor as sorting out details as to why things are different or inaccurate is going to be much more difficult without a static copy on site that you control. It's much easier to say X is my data set from Y, X &lt;&gt; Y, that's where the problem is. Or perhaps X is my data set from Y, and Z is my DW, X = Y, so something between X -&gt; Z is the issue. If the data set does not perform well when you bring it over, you will need to perform transform logic between the extraction and load rather than perform that after the extraction and load. This would essentially be configuring a DW from their data set. 
Thank you so much, I just picked up Kimball's book for further reading.
Man I was soooo hoping you were right. Exact same error unfortunately.
use the PIVOT operator
It's your else. You have to choose one data type or the other. The first two are strings while the last is an integer. You can cast it as a string line this. ELSE CAST(Column AS NVARCHAR(n)) n being the number of characters for the field. 
Thanks so much, trying now. Solved, you save me. Thank you wizard!
What book are you using?
Is there like a super basic example of this? I'm familiar with doing this at a file-level in Python, not in the Database.
SSH tunnel is opened by the Workbench itself right? Just checking, had forgotten that it could do that. Since ODBC driver can't create tunnel on its own you need to create tunnel yourself using Putty. This configuration is somewhat dependent on your setup, usually always google it. [This might help](https://blog.devolutions.net/2017/04/how-to-configure-an-ssh-tunnel-on-putty). Use the SSH credentials for this and simply keep Putty open in the background. If the tunnel is successful then you should be able to connect to DB by specifying localhost:&lt;tunneled_port&gt; as a server address in ODBC datasource config and then use this datasource when the SSH tunnel is open. Use DB credentials to connect to ODBC datasource. Trickiest part is probably setting up the SSH tunnel... Hope it helps! 
Who in their right mind would think that SQL is dead or a relic? This post is just a way to get pageviews on the blog. 
More specifically I think what's going on here when you execute the query and SQL translates what you've written into what is going to be executed... You're saying `ELSE &lt;field&gt;` and in your head this is the last thing you're doing, and the first two things you're doing are strings of text. What SQL is probably doing is saying something like, `USE &lt;FIELD&gt; UNLESS CASE =` and because that field is an integer, it then tries to put a string into it, which means it tries to convert the string to an integer but because you are not using strings which can become integers it no longer knows what to do and errors. So you `cast(&lt;field&gt; as varchar)` and now everything going into your new case column will be the same datatype. Working your way through `cast()` errors is an important skill. Sometimes you will have a very large dataset and try to cast or convert something only to find out that there are a few rows of data that won't work... so you have to find them and establish logic that will omit them, or case them into something else. 
I see now what you meant by "try casting the reason code as varchar in the final part of your case" now that another user was a tad more specific (he actually mentioned the ELSE which I didn't catch was what you meant as well) and that turned out to be the exact fix. Thank you for your help.
good point, i'll edit the post with sample data &amp; expected output
Has the user received a warning like "you will be blocked if you continue to post (crap)"?
Probably the same people who keep telling me desktop computers are going away.
would a HAVING statement do the trick? Something like having count(student_id) &gt; 1? 
That is exactly what I figured out thanks to the help of this sub. And it explains what was an incredibly confusing error message. When it told me it couldn't convert it to a int my brain said "I'm ***not trying*** to convert to an int I'm trying to cast as text!" Now I see that it was trying to put in integers where I had just made it change specific integers to text, and the error made more sense.
Got it worked out using just IF's. SELECT BB.reldate, exception, start, end, eeid, mins, attuid, domainID, location, navaya, currentsupervisor , iif(start &lt; CDate("02:00:00 AM"), BB.RelDate-1, BB.reldate) as StartDate , iif(end &lt; CDate("02:00:00 AM"), BB.RelDate+1, BB.reldate) as EndDate , DateValue([StartDate]) + TimeValue([start]) AS StartDateTime , DateValue([EndDate]) + TimeValue([end]) AS EndDateTime , iif(StartDateTime &gt; eframe, null, iif(EndDateTime &lt; sframe, null, iif(StartDateTime &lt;= sframe, sframe, StartDateTime))) as FinalStartDateTime , iif(FinalStartDateTime = null, null, iif(EndDateTime &gt;= eframe, eframe, EndDateTime)) as FinalEndDateTime FROM BB_IEXKEYED as BB left join OTFrames as CC on BB.RelDate = CC.RelDate WHERE BB.termdate is null AND CC.RELDATE IS NOT NULL and BB.Exception in ('Overtime','OT Break')
For full results... SELECT * FROM TABLE WHERE StudentID IN ( SELECT StudentID FROM TABLE GROUP BY StudentID HAVING COUNT(StudentzID) &gt; 1 )
Thank you!!
I used to get really frustrated when I started because in my head everything was properly coded, but I would get an error, and I'd eventually get so frustrated I'd go to my boss/mentor telling him that something was fucked up. His response was always the same thing, "Do you think that you just found a unique error in SQL, or do you think that your code is wrong?" It was painful but in the end I learned to systematically break my queries down when I had errors to find where the error was coming from, and then attack that central issue. If you run something and it comes back with an error... its your fault... its always your fault... its never SQL's fault. Most of the time.
I put my desk computer in the cloud. Couldn't be happier. 
If you can't rely on consistent folder names, can you read the date the folder was created? 
I'm not sure I fully understand, but use using your working query as a function same time and effort?
You have to make the tunnel from the remote host ip up to your localhost and then connect with ODBC as the data source would be on localhost (Step 2 at the link I sent).
So basically the concept of time here is just a start of a "week" and end of a "week". At the target table, everything should be aggregated to the KEY1 &amp; KEY 2 column. I need to aggregate sales from time_period_0-&gt;time_period_1 and insert that value into column AGG_TIME_PERIOD_1, aggregate sales from time_period_1&gt;time_period_2 and insert into AGG_TIME_PERIOD_2...etc.
Even though you‚Äôve now got your code working with the CASE statement, In general, I recommend joining to the config table, so that as the config table grows/improves over time the output of your code will improve with it, and your will always be on the most recent version. Saves having to manually update hard coded CASE statements in numerous queries whenever the business‚Äô configurations are updated.
Partially understood but this is rather technical, and I am unsure if I am doing this correctly. Feeling a bit confused and overwhelmed. 
Script task? You could make a call out to cmd.exe that traverses the tree, I would imagine (done using an "Execute Process" component.) I wouldn't know the command, but if you figure that out, you can capture the result. [Stack Overflow question for CLI](https://superuser.com/questions/428088/find-a-directory-folder-with-cmd-without-knowing-full-path)
The creation date of the folder may help, or you could search the folder that contains today's date in the name. I'll consider a script for this (powershell or similar), it will probably be easier. You can move the file somewhere and process it with ssis from there 
I see now. I have to agree with the other comment that PIVOT is your answer, unless you want to somehow dynamically change the column names in a loop of some sort. I honestly can't think of an "easy" solution to your scope. 
Those are pretty low quality posts. That picture Steve Jobs is especially irritating and discrediting on a SQL Server blog. With that said, I've started my own blog that contains similarly low quality post. Right now, it's place holding ideas and snippets that I've found through my time. Eventually it'll build up to a library of my own creation. I want my site to be useful to myself and other people if they come across it. I wouldn't post it anywhere, unless I spend time doing an actual write up. This is the type of post that deserves a down vote, not a rally call for a ban. It might be a shit post, but at least it's on topic.
Interviewed poorly and after 2 sql tests where i just froze, I'm going to b practicing actual pen and paper query writing. As i said previously, I had managed to write some pretty cool stuff at work but it was a while ago and i had ZERO experience with writing sql under pressure. So I was used to being able to get by with approximate knowledge and always being able to look things up. Not a good training. I'm fixing that. None of the problems they gave me are that hard. I just need to practice and to develop a systematic method. &amp;#x200B;
too embarrassed to post them. They were obviously basic questions, but i went into interviews with zero practice. that was dumb. working on that!
I think i'll be ok once i've actually practiced. It was dumb af of me to go into those interviews with zero practice even though i haven't written much sql in at least 1 year.
Sorry if I was unclear. I've already written the SELECT statement. What I need to do is show it in [relational algebra](https://www.tutorialspoint.com/dbms/relational_algebra.htm) as well. Could I just use the {T | Condition} notation on that page and say the condition is that T exists in a given table, or is there a specific symbol to use for IN?
I sent you a pm. I am not up to speed on this but sent you a link to a pdf that may help.
No problem. Glad to see you got it.
You can, but in these cases you'd typically add a surrogate key (most likely some form of numeric identity) 
Probably people who use MongoDB by default for everything, without even considering the use case. 
&gt;I work hard on that content and I try to keep it relatable to the subject without spamming or posting overly frequently. Yeah, that's vastly different from the articles and users in question. &amp;#x200B; Automod can always comment on a post before it removes it from public view to let the poster know what is going on at which point they can message modmail to see about getting their content white listed. For sites like blogspot, I honestly think white listing specific subdomains (aka content creators) would be the way to go.
To be fair, I believe the mods of /r/SQLServer have filtered those posts from public view unless you go through the users post history.
There's SaaS/PaaS products out there that could accomplish this for you without the scripting. One of our vendors accomplishes the reverse of what's described (connects to our database and pulls from selects) with a product called Talend.
&gt; backwards That is a reasonably useful skill in and of itself in some ways!
#2. Collation doesn‚Äôt matter. You want to use NVARCHAR and NCHAR strings instead of VARCHAR and CHAR. I can‚Äôt speak to your Azure question as I have no experience with it. 
Much thanks.
What have you tried thus far and what was the exact error? Have you tried the wizzard? https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html. 
Read only != read uncommitted. A vendor would generally not present data uncommitted data to any select statement, unless a client connection forces it to do so. It's an RDBMS he. 
&gt;This is the type of post that deserves a down vote, not a rally call for a ban Disagree. A lot of these blogspam writers have incorrect and dangerous advice. Someone new to sql could see the posts and think its useful information. They add no value so there is no reason to let them continue to post. 
Have you *seen* some of the other attempts to make a query language? They are such a disaster. There's a reason SQL has stood the test of time, and it will continue to do so.
To be honest, I would really appreciate if people made a better query language for a change. SQL is a pretty low target. I think the only reason some better query language has no chance of adoption is that database vendors themselves don't even support the SQL standard from 20 years ago.
You ought to be asking this in a sub that's more focused on those front-end and middleware technologies/frameworks. People whose primary focus is databases (many of the people in this sub) aren't going to have much to say on those. You wouldn't ask your plumber to help you decide what car to buy, would you?
FIGURED IT OUT AND GOT THE LINKED SERVER SETUP!!!!
Try subquery factoring. I can work this out for you if you give me the ddl and dml of some sample data.
Thank you so much for replying! I don't have a table to import anything into. I don't have any schemas available to me. Is that what I'm doing wrong? I guess I was assuming that if I imported data as a table it would make a table for me. I'm trying to use the data import function and when I was using the .txt file I got this error: ERROR: ASCII '\\0' appeared in the statement, but this is not allowed unless option --binary-mode is enabled and mysql is run in non-interactive mode. Set --binary-mode to 1 if ASCII '\\0' is expected. Query: 'ÔøΩÔøΩA'. Then I changed the character encoding to UTF8, and I got the following error: ERROR 1064 (42000) at line 1: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'AU AF TI SO LA DT DE ID AB C1 RP EM RI OI FU FX CR NR TC Z9 U1 U2 PU PI PA SN EI' at line 1 That error stayed constant when I tried to upload the file as an .sql file, a .csv file, and a ,json file. Thank you again, any guidance would be appreciated! &amp;#x200B;
I asked in this sub because majority of people who are into DBs must have experience with deployment of DBs too. And no I wouldn't ask my plumber to help me buy a car but my mechanic won't be a bad choice for this question although he doesn't build cars.
In tsql I usually use a cross join / for xml / group by. Not sure with an option with oracle but try if it might be easy peasy
Does empty mean null, or a 0-length text or blob value?
Safer to detach, move, then reattach. Single_User mode still maintains a connection which may prevent you from moving files. 
Can you post a screenshot of your Mysql workbench environment, especially the objects in Navigator? A bit like the first screenshot in the link I posted. So that we can get an idea if you have a db, etc. Also, is it sample data that you have? Could you show what the records actually look like? You shouldn't need a table; the wizzard can create one. I have to say, it is often a bit of trying different settings until you got it going. (field enclosing, line separator, field separator, encoding) &amp;#x200B;
Thank you kind stranger!
I‚Äôll give it a look!
At that data size you might just want to write a quick python script. 
Post create table statement pls. 
I wish I knew how to. I'm just doing a very small project and want to keep filtering in Excel to a minimum, but I may have to do that instead. I was hoping to be more proficient with SQL, but I appreciate your point about it not being good with column oriented stuff. Thanks!
A blob is one of the [sqlite datatypes](https://www.sqlite.org/datatype3.html) (And standard sql type in general) used to hold arbitrary bytes of data. Assuming the column only holds TEXT values: SELECT sum(length(some_column)) FROM some_table; If that's 0, every string in `some_column` is empty. 
I‚Äôd like to know of some as well, would be very helpful. All I‚Äôve found so far is codewars.com.
Is there a way to do it for all of the columns all at once, instead of individually? Thanks for this.
You can just do a max() for each column and then the ones that are NULL or blank only have those values in it. E.g. select max(col1) as col1 ,max(col2) as col2 from table Since you have a lot of columns, the easest way to come up with the statment would probably be to do a File -&gt; Export to SQL and then export just the schema (no data). Then you can modify the query with a regex string. Basically just delete everything that isn't the column names from the top and bottom. That will leave you with a text file similar to: col1 col2 col3 etc. If you then do a regex... I like to use Notepad++ in Windows or Notepadqq in Linux for this as it makes it a little simpler to play around than using perl or something for me at least. At anyrate, it should be something along these lines: String: \(.*)\ Replace with: max\(\1\) as \1 That will take just the string and replace it with max([col name]) as [col name] 
OK, thank you for the Select statement above. I appreciate it!
You could also wrap that in a CTE if you want to get really fancy but that would require a few regexes or a lot of patience building out the query. with t1 as ( [Query from above] ) select case when col1 is not null or col1 != '' then 'col1' end as null_column from t1 UNION select case when col2 is not null or col2 != '' then 'col2' end as null_column from t1 That might take a hot minute to execute and would take a few minutes of building the query but would give you a tall format that is what you originally wanted. Basically the output would be: col1 col25 col66 etc. If col1, 25, and 66 only had NULL or '' as the value. ('' is actually two single-quotes). 
This will take me a bit to try out. Thanks, I appreciate your help! I'll let you know if this works. I might have to ask you some questions as I am not a database person...I only know a few simple Select and Join statements.
Sure! And if you're doing this a lot, it woudl behoove you to also learn some basic Python. I actually was just talking about this in a thread in /r/Ubuntu where someone was having a similar workflow importing a CSV file. SQLite is awesome for basic manipulations and if you're importing a file like this often and are trying to do QC, what you can do is write a script that specifies a particulra folder that will import the CSV, do the QC, and then export it to the SQL Database. It sounds fairly daunting and it might take you a weekend or two to fix it up but once you start doing some basic scripting in Python you'll wonder how you lived without it lol
Here's a sneak peek of /r/Ubuntu using the [top posts](https://np.reddit.com/r/Ubuntu/top/?sort=top&amp;t=year) of the year! \#1: [Linux now powers 100% of the world‚Äôs 500 fastest supercomputers. -](http://techexeconline.com/linux-now-powers-100-worlds-500-fastest-supercomputers/) | [70 comments](https://np.reddit.com/r/Ubuntu/comments/7daeli/linux_now_powers_100_of_the_worlds_500_fastest/) \#2: [Me after 2 days of Windows 10](https://images2.imgbox.com/ab/46/zo3WMpgP_o.png) | [136 comments](https://np.reddit.com/r/Ubuntu/comments/7h57hd/me_after_2_days_of_windows_10/) \#3: [WPA Vulnerability? What WPA vulnerability?](https://i.redd.it/txn0rm8b6asz.png) | [51 comments](https://np.reddit.com/r/Ubuntu/comments/76uq2y/wpa_vulnerability_what_wpa_vulnerability/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
I prefer to avoid script tasks, where possible. In SSIS 2016 + SQLServer 2016, there's a bug where script tasks will blank, and all code associated is lost. Ever since, I've either put my script tasks into compiled executables, or attempted to find Windows CLI utilities to circumvent. Nonetheless, the script task is going to be easiest (Assuming you don't share my misfortune) in terms of skills to write + maintainability.
Dude, I'm dealing with the same shit. I'm trying to get postgresql running on my windows machine and it wouldn't ever start. So, okay, why not try a reinstall. Nope, can't do that either, same reason you gave. Terrible terrible stuff. 
I feel like SQL is a huge skill and recently I‚Äôve found myself in a position of re-doing peoples‚Äô old manual process stuff in Access and transferring it to SQL.. efficiency is a fun side effect. Healthcare. Small team. 90k. 
If you post a sample CSV file I can write something for you. It should only take like 5 minutes. On the topic of SQL, what you really need is an `UNPIVOT`, which turns columns in to rows, but sqlite doesn't support it.
I‚Äôm now a ‚ÄúSystems Integration Analyst ‚Äù . I used to be both a Data Analyst and SQL Developer. All good titles. Fun to tell you cause I am just as confused but getting somewhere!
Try the uninstall procedure u/callmetom found, I was able to find a fix for my root issue without uninstalling so I never went through with it: https://www.postgresql.org/message-id/559D58FD.8090100%40hogranch.com It's frustrating bullshit, I agree. Not looking forward to the day I do have to uninstall postgres. I think we'll leave that for IT to deal with ;) 
I‚Äôm now a ‚ÄúSystems Integration Analyst ‚Äù . I used to be both a Data Analyst and SQL Developer. All good titles. Fun to tell you cause I am just as confused but getting somewhere! 
I just did individual Select max() for each column I saw in the spreadsheet form that looked like it had lots of empty rows, and that seemed to work. I don't have a regex program on my Mac, otherwise I think your suggestion would've worked. Thanks again!
[Enki](https://www.enki.com/) has some SQL stuff that I've found to be largely OK. 
Probably not the best way but you could always export it to CSV and do a test to columns in Excel. Maybe create an INSERT in VBA. Don‚Äôt have any experience with Oracle. Sorry if this is a bad suggestion 
This is great advice! I write down similar "fetch" questions when I have a moment at work - a lot of "I wonder what happened in this year, or location, etc. How do I find it?" When I have some free time, I try to solve those issues. Not only does it help me help myself at SQL, it lets me talk to various SMEs to understand whether or not I'm interpreting the data correctly, and also letting me get to interesting insights that may not be asked for initially. I find that managers tend to love this kind of work as long as you do it on the side, or in your spare time as well. One other thing that helped me develop my SQL skills a lot is to work with journal tables to understand window functions better. This isn't always a viable option, depending on the environment, but I thought that it was incredibly useful in shaping how I thought about data and queries.
sqlworkbenchj is nice
You can use [VisiData](https://visidata.org) for this, on either the sqlite table or the csv file. Here is a [video](https://www.youtube.com/watch?v=prdyXM6-FSc) showing how to do something just like that. Basically you use the `I` (describe) command, and then select those with the number of null rows the same as the total number of rows. Pull off into their own sheet with `"` and then clean it up and save it off.
https://dbeaver.io/ is pretty good. Datagrip is pretty nice, and depending on what other tools you would utilize from JetBrains it can be worth it. Also, can often get deals (25-50% off) on it in addition to the decreasing yearly price.
As much as I can imagine the position you are in. I would strongly suggest that you do it yourself. Else you will be realy mad at you at the end of the term when you have an even bigger pile of work and have to redo this sheet anyway to rewind the material.
Hey, blauefarbe, just a quick heads-up: **realy** is actually spelled **really**. You can remember it by **two ls**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
Wow, DBeaver looks promising, and it has Community Edition as well. Of course, the only downside is, it is Java, not native Mac app.
Not sure if I'm understanding the problem here, but these are the steps I would take: 1) List all entities 2) Create an ERD 3) Create tables and relationships from ERD 4) Populate static data 5) Create all tickets that are to be allocated, with a bit flag of "IsAllocated" and set that as 0 when inserting. Is that it? Why would you/the system only create 5000 to start with?
Now() is your issue here? 
Why pre-create them at all? Store a limit for the # of tickets that can be sold, and when someone wants to buy one, create it in a "staging" table that only lives for the duration they're allowed for the checkout process (like what most ticketing websites do; you have 5 minutes from clicking the button to complete your purchase). If someone attempts to buy when you've met your limit, turn them away. You'd have to do something like this if you pre-created the ticket so that you don't double-sell the same ticket anyway.
As a sql developer were you actually building the databases or still just simply querying the data?
I work in the medical field. My company is very large, you'd know the name, and my division has about 60ish people in the business intelligence department. My queries are pretty involved because I do a lot of state specific compliance work and dashboard building. If you are familiar with this scale, I am a PF4. If you want to always be able to find work, get some experience working with medical or insurance data. Claims, enrollment, ICD10, procedure codes, pharmacy... that kind of thing. Think about anything your doctor would use for billing. There is a ridiculous amount of work out there. Good luck
thanks so i should not create the tickets at once
It would really depend on the business rules and use case. What's the purpose of creating 10000 tickets ahead of time? What does a ticket consist of? Are there any obvious drawbacks?
From what I understand, it‚Äôs a ticket reservation system (think concert show with assigned seating), and not a big tracking system.
Appreciate it, and sounds like I‚Äôd want to hone in specifically on a BI type of team. Thanks for the reply 
I know what my idea of a ticket reservation system is. Pre-Generating tickets when an event is created doesn't seem right to me. If OP is going to be designing one, these are things that he needs to think about.
In my case it‚Äôs been more about building data solutions including views and stored procedures for more process purposes rather than reporting purposes which is what I did as a data analyst. 
The Exam Reference is probably the best resource out there. The Kindle Edition is only $16 as well and comes with a decent amount of sample questions. &amp;#x200B; [https://www.amazon.com/Exam-70-761-Querying-Data-Transact-SQL-ebook-dp-B06Y21QGGQ/dp/B06Y21QGGQ/ref=mt\_kindle?\_encoding=UTF8&amp;me=&amp;qid=](https://www.amazon.com/Exam-70-761-Querying-Data-Transact-SQL-ebook-dp-B06Y21QGGQ/dp/B06Y21QGGQ/ref=mt_kindle?_encoding=UTF8&amp;me=&amp;qid=)
Because?
Nor me. If there's a venue with a set number/layout of seats then generating records for them and linking them to events and customers would make more sense.
You can find a sample test here: [https://us.mindhub.com/search?query=70-761&amp;categoryId=11284](https://us.mindhub.com/search?query=70-761&amp;categoryId=11284) Also, I'm not sure where Coldchaos saw a price of $16 on Amazon. I just went there and the Kindle version was $25. I would imagine this book is quite good though. The author is terrific.
Check out this post: let me know if you need any resources on specific topics if that thread isn't enough. It's primarily for the 762 but I also list my 761 resources and experience. https://www.reddit.com/r/SQLServer/comments/956hcs/hurray_i_passed_70762_and_earned_my_mcsa
Question #1: How do the backups work?
You should probably find out what being the "SQL Database guy" means. Like, there is a hell of a lot of difference between: * Maintaining databases * Importing data into databases * Using SQL to report on databases * Building databases etc. Like, are your databases used for the backend of a website? Are they reporting databases? Are you doing research and you need to import data from sources? Are you going to use the data to derive insight from them? Are you going to be building dashboards that rely on the database to report the same things on an ongoing basis? Databases have a ton of different uses. I'm imagining the role you play is going to be reporting it unless your company has, erm... a lot of faith in your ability to learn. "SQL knowledge requested" to me means that they probably have been reporting out of it but they want someone who can pick up basic SQL and do reports off of it because doing a direct ODBC from Excel isn't cutting it anymore or something.
Also relevent: do you know SQL? Because that's where I'd start. Tons of basic SQL stuff for learning online. Just knowing the syntax is going to help a lot if you don't know it already. You can learn it in a weekend, it isn't complicated. There's a steeper learning curve for more advanced stuff but basics are super simple.
The much more important question here is what kind of scale and feature set you're looking for. RFID, barcodes, and SMS are just different methods of transmitting short strings - no standard SQL database is going to have a major advantage over any other for that kind of task. I don't blame the companies you've been asking for not revealing the implementation details of their internal systems, they paid good money to have those made, and are not obliged to tell you anything about it :/
Follow up question: Do they work?
I‚Äôll take a look this week. I appreciate the info. Thanks!
Dev or Infrastructure? Completely different areas. 
Sick of seeing this crap
I like seeing content like this and support general practices for writing queries, but one note: &gt;1) Define all the necessary fields for SELECT instead of using SELECT * This isn't always true. For example lets say you start with a query such as: select a , b , c , d from table Objectively even if the table only has 4 columns, it is probably better to spell them out as opposed to using `*`, however: select * from ( select * , stuff from ( select a , b , c , d from table x ) y ) z Here you can see that when using recursive sub-queries, etc., that you may want to use a `*` because otherwise you are going to be spelling out the same basic list multiple times in a row, and this makes it much more of a pain in the ass to go in and make changes. Instead of making them in one place, you're making them in 10 places. Also, you might want to consider creating a view that is a `select * from table` so that new columns automatically show up and filter into front end applications like Tableau. So now if you update a stored procedure and add a column, you don't have to go adding it in multiple places.
How exactly did you get past the interview?
Thanks for your comment. Unfortunately, this case I did not take into account
Just looked, it was because I have Prime. 
Considering how the SQL engine optimizes queries based on the the columns SELECTed, I would highly advise against using \* in any select. IF EXISTS (SELECT 1 FROM dbo.tblRefTable WHERE &lt;&lt;Indexed Columns&gt;&gt; = @Value ) SELECT &lt;&lt;Index Included Columns&gt;&gt; FROM dbo.tblRefTable WHERE &lt;&lt;Indexed Columns&gt;&gt; = @Value SELECT Count(1) SELECT &lt;&lt;Indexed Included Columns&gt;&gt; FROM dbo.tblRefTable FOR JSON AUTO, INCLUDE\_NULL\_VALUES, WITHOUT\_ARRAY\_WRAPPER 
I told them I took a class back in college! 
I'm not sure what the point of not using distinct in. Using the example in the article, if I need a distinct list of names and ages, then getting a full list of names, ages, and addresses doesn't help me. I'll have to do post query processing to get the information I need. I agree that you should only use it if you really need to, but if you need it then you need it and should use it.
What happens when, due to poor data quality, another record has the same values? Unique results for one set of columns are not the same thing as distinct results for another. If you want uniqueness, guarantee it by selecting key column/s or generating GUIDs. If you want distinctness (for example to populate a dimension), use DISTINCT.
Dude...it's just a simplified example for a potential alternative to an expensive DISTINCT operation. It won't always work, but a lot of the time it can. If you're working with a small dataset, it might not matter, but for larger result sets, it could potentially offer a large savings.
I would like you to demonstrably show me how using a `*` in an outer parent will impact execution speed.
It's not an alternative though! The whole point of DISTINCT is to return unique rows for a *given set* of columns.
I can't think of a reason any popular RDBMS would be materially better than another for this.
&gt; It's not an alternative though! If it accomplishes the same thing, then yes it is an alternative. What's your point here? You could make the same argument for any SQL feature. "It's not an alternative! The whole point of a CURSOR is to iterate through a given result set" --Ignoring the fact that 99% of times cursors are misused, and there is almost always a way to do the same thing as a cursor but faster. &gt; The whole point of DISTINCT is to return unique rows for a given set of columns. Obviously, but it's very slow at what it does. So, if there are available alternatives, they should be used. DISTINCT is misused all the time. There are valid cases for using DISTINCT, but when writing queries, DISTINCT is usually slower than any other available alternative. So best practice is to avoid using DISTINCT. 
My point is that I honestly can't think of a scenario where this could achieve the same thing as DISTINCT while not being bad practice (i.e. a cheap way to get rid of mystery duplicates). I'd very much appreciate an example.
Case 1 - Extra clock-ticks are extra clock-ticks Execution Start Parse Query -Result Set has dependencies -Execute Subquery --Result Set has dependencies --Execute Subquery --- Check indexes for columns are specified --- Store temporary result A -- Cartesian , 'stuff' with temporary result set A -- Store temporary result B (A+Cartesian) - No where clause to filter - Store temporary result set C (B) - Serve Result Set Case 2 - Possible ambiguous column collision. Failed execution impacts performance. &gt;Also, you might want to consider creating a view that is a &gt; &gt;select \* from table &gt; &gt;so that new columns automatically show up ... SELECT * FROM ( SELECT strName FROM dbo.tblTabTable_A ) Table_A INNER JOIN dbo.tblTabTable_B TableB ON Table_A.sysTabTable_BId = TableB.sysTabTable_BId
Looking at it again, yeah you're right, that's a terrible example, and the provided query is not an alternative. I'm not sure what the OP was going for there.
wat
Decent. I'll be sure to note that on my cover letter. Though in my last job i was a project manager for our "time collector" which was basically just a sql database. Learned a lot. Learning how to actually do the work now.
&gt; Please help, I am ~~on probation at my job~~ [a liar and shitty student](https://www.reddit.com/r/SQL/comments/9gzwid/how_can_i_create_this_table/) and about to ~~get fired~~ fail a course
&gt; Considering how the SQL engine optimizes queries based on the the columns SELECTed Specify which "SQL engine" you're talking about here, because Oracle, SQL Server, Postgres, etc. all work differently.
&gt;With a large number of records and rows in the table, defining all the necessary fields will greatly speed up your query. [It's not always so clear-cut](http://www.scarydba.com/2016/10/17/select-not-hurt-performance/). The bigger issues with `select *` are: 1. Column order changing in the table and the application not being prepared for that (grabbing fields by ordinal instead of name, for example) 2. Grabbing a crapload of columns your application doesn't need (extra data over the wire) 3. Not being able to make use of covering indexes.
[removed]
&gt; DISTINCT is misused all the time. I would say that 95% of the time that I come across distinct in the wild it is being used because people have written garbage joins that are returning cartesian products.
lol
I expect there are better ways to do it, but I'd try something like: `DECLARE @actorXid int = 555` &amp;#x200B; `WITH cte_actorXshows AS (` `SELECT` [`t.id`](https://t.id) `AS actorXshow` `FROM` `tvshow AS t` `INNER JOIN` `cast_person_tvshow AS ctv` `ON` [`t.id`](https://t.id) `= ctv.tvshow_id` `INNER JOIN` `person AS p` `ON ctv.person_id =` [`p.id`](https://p.id) `)` &amp;#x200B; `SELECT` [`p.id`](https://p.id) `FROM` `person AS p` `INNER JOIN` `cast_person_tvshow AS ctv` `ON` [`p.id`](https://p.id) `= ctv.person_id` `WHERE` `NOT EXISTS (` `SELECT` `1` `FROM` `actorXshow` `LEFT OUTER JOIN` `tvshow AS t` `WHERE` [`t.id`](https://t.id) `IS NULL` `)`
Sometimes we have to fall and learn to pick ourselves up. Well I thought something was a little off, confirmation on this business case. 
Are there alerts on place if the backup fails? If so, are they working? What is the minimal acceptable data loss? What about acceptable time to recovery after disaster? See [Brent Ozar‚Äôs site](https://www.brentozar.com/archive/2016/07/updated-high-availability-disaster-recovery-planning-worksheet/) for great info on this.
They're doing different things. Query one is returning every row from zipcensus where the value of the state column is &lt; 100. Query two is returning every row from zipcenus where the value of state *occurs* &lt; 100 times in the state column.
Everything is solid advice here... But i strongly recommend a friendly approach to #5. I've seen some managers go out of the way to make The Sql Guy (me) look bad. Don't take it personally... These managers were burned out from the SQL guy before me. After a few months they became my strongest supporters. They knew how bad things were and how I took their feedback and built off it. Goodluck at your new job! You can do it!!!! 
I agree and shook my head a bit at this example.
Congrats! Hope you do well...
pre regenerating seems like the best fit for both worlds for concurrency,but i have been thinking of maybe having a max no of tickets that can be generated at a time lets say 4000 then increasing it when it gets almost finished is this better?
can someone expand on when to ever use EXIST over IN and vice versa? i didn't understand the short blurb
space consideration some people/org may put high ticket numbers that they may never sell but what is your suggestion i was thinking of maybe an algorithim that can create tickets in batches of 1000 whenever it gets around 90 % sold
Your strategy is solid, you can also left join the product table on ProductID with the OrderDetails for the name of the product. Also important, the exercise asks only for the most sold product, not for a list. Order your results DESC by SUM and LIMIT the results by the top 1 answer. What you do with it thereafter depends on how far you want to narrow down your result. SELECT [d].[ProductName], SUM([c].[Quantity]) AS SumQty FROM [Customers] AS [a] LEFT JOIN [Orders] AS [b] ON [a].[CustomerID] = [b].[CustomerID] LEFT JOIN [OrderDetails] AS [c] ON [b].[OrderID] = [c].[OrderID] LEFT JOIN [Products] AS [d] ON [c].[ProductID] = [d].[ProductID] WHERE [a].[Country] = 'Germany' GROUP BY [d].[ProductName] ORDER BY SumQty DESC LIMIT 1 This will give you the Name of the product and the quantity sold (Boston Crab Meat, 160). If you want to follow the exercise word by word and only give the name of the product you can use: WITH [ctea] AS (SELECT [d].[ProductName], SUM([c].[Quantity]) AS SumQty FROM [Customers] AS [a] LEFT JOIN [Orders] AS [b] ON [a].[CustomerID] = [b].[CustomerID] LEFT JOIN [OrderDetails] AS [c] ON [b].[OrderID] = [c].[OrderID] LEFT JOIN [Products] AS [d] ON [c].[ProductID] = [d].[ProductID] WHERE [a].[Country] = 'Germany' GROUP BY [d].[ProductName] ORDER BY SumQty DESC LIMIT 1 ) SELECT [ctea].[ProductName] FROM [ctea] This will give you only the name of the top 1 sold product in Germany (Boston Crab Meat).
Yes, absolutely. Great point. I really meant to Find out whether you have someone who can be your guide/mentor, partner, or if it‚Äôs someone who is a bit lost and needs your help.
high level: they are placed in a temporary queue so someone else can't buy the same seat. real life: they are scum and sell most of the tickets to stubhub at a higher price . 
With EXISTS, you can make a where clause that checks another table for existance of a certain row. Simple example to find Customers that also have an entry in the Account table with the same CustomerId: SELECT * FROM Customer.Customer c WHERE EXISTS (SELECT TOP 1 a.AccountId FROM Account a WHERE a.CustomerId = c.CustomerId) This could also have been done with an INNER JOIN instead, which would be preferable, but I'm just making an example. You can write any subquery within the EXISTS (), so you can use it to perform a check on a table where there's no real join condition. Likewise, you can do a NOT EXISTS to make a query ensure that a certain entry in another table does not exist, which can sometimes be more readable than doing a LEFT JOIN and checking that one of the joined columns is null.
SQL is just one of many technical skills that are needed for this position and I do know SQL - but as you know that is different from knowing everything their is about managing and repairing databases in organizations. To satisfy your curiosity - one of the technologies they need is critical to them and I know it as well as anybody out there. Really hoping to get more tips from those who live SQL and database management consulting. I will find out more about my specific responsibilities soon.
This is not a very good guide. My critical comments: 1) First, do not stress writing * when performing ad-hoc queries. An ad-hoc query is as query you typically write and execute by hand, for example in SQL Server Management Studio as you develop, write a one-time data extract, or "dig around" in logs or such. If you're being a good boy and just doing a TOP 1000 or similar to restrict your queries, it doesn't matter that you select all columns. Use * as you please. When writing an SP or data layer code which will ultimately go into production, then yes, only return necessary columns. 2) HAVING and WHERE are two different things, so I don't see how this is an issue. The first query doesn't even work - you can't perform the HAVING on a column you haven't done a GROUP BY on. It should have been: SELECT age FROM users GROUP BY age HAVING age &gt; 25 This will give you a list of unique age values, where age is &gt; 25. Not a very usable example, but eh. 3) Again, comparing two different things. A good case for using DISTINCT is when you're just interested in the ID's of an entity (for example customers), but your SELECT statement has a JOIN (or more) that results in "duplicate" rows of the Customer. Let's say you have an SP where you just want the ID's of some customers and insert them into a temp table, then do further work with them. Your initial SELECT statement that gets the ID's of the customer JOINs on another table, for example ContactPermissions, because you want customers that have one or more permissions set. Then you'd do a: SELECT DISTINCT c.CustomerId FROM Customer.Customer c JOIN Customer.ContactPermission cp ON cp.CustomerId = c.CustomerId If I didn't do a DISTINCT, I'd get unnecessary extra duplicate rows. I can't just select more columns to narrow it down like in the (bad) example). 4) Doesn't work in SQL Server, what SQL that that first statement work in? JOIN and WHERE is not the same. A more legitimate question is: ON clauses vs. WHERE clauses. There are cases where they result in the same, although usually you want at least one ON clause. Assume this query: SELECT * FROM Customer.Customer c JOIN Common.PhoneNumbers pn ON pn.CustomerId = c.CustomerId WHERE pn.Type = 'Mobile' This could also be written as: SELECT * FROM Customer.Customer c JOIN Common.PhoneNumbers pn ON pn.CustomerId = c.CustomerId AND pn.Type = 'Mobile' I prefer putting it in the WHERE clause for readability, but if you google "sql server join vs where clause performance", there is a lot of hardcore theory to read. 5) The "bad practice" example should have been a INNER JOIN on the address table, and it would have been a lot more readable.
oops - I need to slow down. "there" not "their"
I like seeing this type of content on this sub. Being self taught I always missed out on the "best practice" and "real world" type of material. 
Sorry for the late reply, but yes, class would be a suitable key for all ticket. My point with the date fields was that each ticket would have its own unique dates so the fields should be listed on the ticket table. 
95%? Dang. I thought *we* had some half ass people writing queries, but I don't think I've ever seen DISTINCT used to clean up a cartesian product. 
Our database is effective dated so joins are much more complicated for people who aren't used to it - which is most everyone in the org outside of the oltp group who owns the data model.
Which version of Oracle do you have? short answer is yes but the process is a little more involved - bunch of step between copying the physical files and the SQL Developer connecting to it. &amp;#x200B; this link is wil help: [https://searchoracle.techtarget.com/answer/How-do-I-copy-an-Oracle-DB-from-one-server-to-another](https://searchoracle.techtarget.com/answer/How-do-I-copy-an-Oracle-DB-from-one-server-to-another)
One of our big db's is like that. They created a "Current" view that, so you can just use those to build the 99% of queries that don't need historical data. One of the guys down the aisle from me thinks they Current view was a dumb idea because "all you have to do is put "end_dt = '9999-12-31'" in your query. Um, OK... sure. Just put that in for every table in every query... no thanks.
Unfortunately there is no CASE in JET. There's one called "switch" that's kind of like a case clause. Good example here: https://stackoverflow.com/questions/772461/case-expressions-in-access
I don't know what DB engine author is using but you are certainly correct in t-SQL land. Especially regarding #2 - in SQL server that code will never run without an aggregate function. Trash quality blogspam as usual.... I don't know why people are downvoting you, your comment is more accurate and informative than the article!
Hi, thanks so much for the help. I changed it a bit to make it work since i don't have WITH statement in mysql. I ended up with this SELECT DISTINCT p.id FROM person AS p WHERE NOT EXISTS ( SELECT 1 FROM ( SELECT ctv.tvshow_id FROM cast_person_tvshow AS ctv WHERE ctv.person_id = 1 ) AS cte LEFT OUTER JOIN cast_person_tvshow AS ctvInner ON cte.tvshow_id = ctvInner.tvshow_id AND p.id = ctvInner.person_id WHERE ctvInner.person_id IS NULL ); but i'm getting Unknown column 'p.id' in 'on clause' Im guessing the p table isn't in the context of that not exists statement. I'll keep trying. Thank you!
Different DB or just schemas? That query would work if it's just a different schema granted that you have the appropriate rights. If it's really a different DB instance, then you need to setup a [database link](https://docs.oracle.com/cd/B28359_01/server.111/b28310/ds_concepts002.htm#ADMIN12083).
Apologies, I wrote that when I was really tired and it's incorrect - you're right in saying that p.id is not in context. Hopefully you get the general idea of what I was trying to do, 'cause it's tricky to write SQL without sample data (feel free to create some on [SQL Fiddle](http://sqlfiddle.com/) and link it) to test on. This should hopefully work: ```sql SELECT DISTINCT p.id FROM person AS p WHERE NOT EXISTS ( SELECT 1 FROM cast_person_tvshow AS ctv LEFT OUTER JOIN ( SELECT ctv.tvshow_id FROM cast_person_tvshow AS ctv WHERE ctv.person_id = 1 -- actor x id is 1 ) AS cte ON ctv.tvshow_id = cte.tvshow_id WHERE ctv.tvshow_id IS NULL AND cte.person_id = p.id ); ``` How did you get code blocks to work properly with Reddit markdown? Use the markdown editor instead of the rich one? I could never figure it out, embarrassingly.
It was the permissions! Thanks :)
The answer hasn't changed from the last 4 or 5 times you've asked. What answer are you looking to hear?
Naw you didn't do anything wrong, I just made a real mess of this. The first two snippets I posted make no sense, and this one is really ugly. ```sql SELECT pout.id from person AS pout WHERE NOT EXISTS ( SELECT t.id AS showID ,p.id AS personID ,ctv1.tvshow_id AS inShow ,cte.tvshow_id AS actorXInShow FROM tvshow AS t CROSS JOIN person AS p LEFT OUTER JOIN cast_person_tvshow AS ctv1 ON ctv1.person_id = p.id AND ctv1.tvshow_id = t.id LEFT OUTER JOIN ( SELECT ctv2.tvshow_id FROM cast_person_tvshow AS ctv2 WHERE ctv2.person_id = 1 ) AS cte ON t.id = cte.tvshow_id WHERE ctv1.tvshow_id IS NULL AND cte.tvshow_id IS NOT NULL AND p.id = pout.id ) ```
Your CPA and finance analyst experience should serve you pretty well. Are you particularly trying to move away from financial analysis? "Job ready" is a bit tricky, because even within the titles you've given, duties vary a lot! It'd be helpful to start looking at particular job listings if you've not already. I'd say you'd want to comfortably be calling yourself an advanced SQL query writer, though. There is a lot of overlap between BI and BA/DA, but a lot of differences too - it's all quite fuzzy. I'm in BI, so I'm generally more concerned with user requirements/acceptance testing, database/ETL development (YMMV) and delivery platforms like Tableau, Power BI and SSRS. From what I understand, a BA/DA would typically focus less on producing models which drive regular, operational reporting and more on relatively ad-hoc statistical analysis to answer specific business questions. Lots of time spent sourcing data, interrogating data, modelling, calculating statistics. I imagine sourcing would generally be done with SQL, but there's stuff like Hadoop too. Analysis and visualisation typically in R or Python.
Have you created all the tables being referenced for inserts? I'm tsql you call things by schema / object. Ex dbo.insert_course_delegates It also looks like your procedure is kinda of similar to a cursor in tsql. You'll want to look them up.. Last thing.. If you can keep triggers out off the table please do.. Its hard to spot issues and usually triggers are the things people forget about... Making it the last thing we look for. 
Good ole sub queries coming to the rescue 
Hi! Thank you for responding. I am actually trying to get back into Financial Analysis actually. I automated myself out of my job when I built a reporting package for my boss after 9 months and she chose to just have a contractor (@ 60% of the cost) take it from there since all you needed to do was basically copy/paste data into the workbook and it spit out everything you needed error free. My goal is to learn SQL and then really sharpen my Financial Modeling &amp; Tableau skills. I'd like to pick up statistical analysis on a job if possible but I don't have a strong background in statistics at the moment. 
Thank you! I already bought the Udemy Course for $5 awhile back but I'm going to start with the 14 Mini-Courses from Stanford. &amp;#x200B; I think I have a road map: SQL --&gt; Finding Financial Data --&gt; Setting up a database --&gt; Learning more intricate/advanced Financial Models --&gt; Building a Portfolio using SQL/Excel modeling. 
Hi I've ben a Data Analyst/ETL developer for most of my career and now am a Data Architect. Ans 1 - If you know how to join 3 or more tables, know how to use Union, Except and I intersect,know case statements, views,inedexes, primary keys, data modeling (ER vs Dimensional) I would say you can get a job. It's more about passing the interview and then you'll learn a lot of the more complex stuff on the job. That's how I started and my SQL fundamentals were not good at all. But I learnt it on the job very fast and after a year my level was intermediate. Ans 2 - I never built a portfolio so but maybe pick something of interest to you and build a simple data mart with ETL jobs that load the dimension and fact tables. Can be as simple as 1 stage table, one type 2 dimension table and 1 fact table. Ans3 - Read on Dimesional Modeling and 3nF modeling. You should be able to interpret data models with ease. Read up on General Data warehousing concepts Pick a popular database like SQL server and read the documentation. Pick a popular ETL tool and read the documentation and try using it for the project I suggested in ans 2 PM me for any help or questions you might have. Your experience with accounting def helps as many of the BI reports deal with company financials and profitability. 
Oof, feels overwhelming starting from the bottom! I will definitely PM you down the line when I get further along. Thank you so much for the assistance. 
don't over think it. a familiarity with most of those things rather than expertise is what is needed to start. also, what is valued very highly is business knowledge in an area like banking, insurance etc. that will make up for what you lack in "data" skills.
What type of work?
Hit me up.
Work with SQL full time in Toronto. Let me know if you want help.
The homework was only assigned today.
Windsor Area here. Worked for FCA, Dominos, Green Shield and Ground Effects Ltd as a Database Administrator. Hit me up
1) on being Job Ready, I've been working with SQL for 10 years and am constantly learning new stuff. You won't wake up one day and think "Oh, I'm Ready for this!" It's a slow burn that you build up by working with it every day and getting that experience. Put yourself out there, find a company that is willing to help you transition.
SQL DBA for a fortune 10 company. Let me know if I can help. 
It's a dime a dozen. My employer primarily uses Greenplum (a MPP derivative of PG), and my previous employer used Teradata primarily. It really depends on what kind of employment you're looking for - syntax is largely the same between all platforms until you get into scripting and very advanced features.
I'll remember that. I understand the syntax fairly well and can always just use the internet to fill in the gaps in my knowledge. Thanks for that
More people eat at McDonald's, but Morton's is better
ETL stands for extract, transform and load. It's a concept that means you extract data from a source database, transform the data into whatever you need and want in your data warehouse and then load it into your data warehouse. The transform part could be simply things like cleaning up bad data, all the way to reorganizing it into different structures (like going from a relational transactional database into a dimensional reporting database.) Lots of tools can be used to do ETL... stored procedure/SQL, SSIS, Informatica, etc. 
To me, ETL means two things: Running jobs and running transformation pipelines An example of a job is a script that checks if a new file exists then downloads a big file from say a vendor over FTP. Kind of like a bash script An example of a transformation pipeline is a program that opens a file, and reads out records one at a time. Does some formatting, parsing, lookups for each record. Then writes each record to a database. Kind of like a Unix/bash pipeline Generally, ETL is how you load your data warehouse. A data warehouse often contains data from many sources, such as email, GIS, operational db, vendor data
Perfect, thank you for the explanation! I use SSIS and wasn‚Äôt sure if it would be considered an ETL tool.
You're welcome. If you have any questions or anything, feel free to message. Been doing it about ten years now, but I know i still have a ton to learn! 
If you understand the database concepts well, learning the specific syntax of one DBMS over another is largely trivial and can be googled. "I want to rebuild this index", "I want to partition this table", etc.... If you know what you want to do, the commands to do it can be found easily and the learning curve is small.
replication with mysql is a little less arcane imo but pg is my fave bar none. pg is love; pg is life
You say that, but things like referential integrity or cascade deletions or nulls in a not null column still dont work in mysql. Mysql was born to the php and js developers who cant figure out if == is correct so they created ===. PG follows real db requirements just as id expect from microsoft or oracle. Not just in different leagues, mysql is a steamy pile of turd with a whole bunch of hacks polishing it up to look like a something useful.
Posting to come back to this 
My typical rule of thumb. Dumb data dump, MySQL. Specific Enterprise database feature, PostgreSQL. Basically, MySQL it's good enough if you just need a place to store data and later retrieve it. Especially if a lot of your logic ends up in the front end. PostgreSQL is definitely a step up, but at the same time has more maintenance. You'll find you have a lot more flexibility with your data, but at the cost of more complexity. If you need that, you can't go wrong with pgsql.
I always thought it was the inverse..
Any tips for breaking into stored procedures? Haven't explored those just yet.
Indeed. The DBA postings I saw recently were paying significantly less than data engineering and with less strict requirements (20-30% less).
Is it more realistic to go into database management?
I feel similarly. I've been in love with pgadmin4 too. Everything feels very modern and easy
I am sorry, I found someone, but If it doesn't work out, I will let ou know. thank you for your message!
I am sorry, I found someone, but If it doesn't work out, I will let ou know. thank you for your message!
I am sorry, I found someone, but If it doesn't work out, I will let ou know. thank you for your message!
The other source is very often another database. Not just text or API sources.
I'd absolutely consider it an ETL tool. SSIS is our primary ETL tool at work.
Just to add to the other answers, the transform portion is often used to standardize, as well. If you extract from multiple sources, they may have different ways of recording data - Genders, for example. During the transform part of ETL you would map these source system values to a standard set so that reporting from your datawarehouse is using the same values regardless of source.
https://www.reddit.com/r/Database/comments/9iqxvj/database_management_or_data_engineering/
no... same reason that GUIDs are often discouraged for PKs... they ruin the B-Tree's fragmentation. an int based IDENTITY is the fastest performing PK for inserts and maintenance. For a fully defragmented table, the column type won't really matter (data size aside - "just big enough" is ideal, ints are smaller than GUIDs are usually smaller than text, and a 5mb index would perform slower than a 2mb index). But tables rarely stay fully defragmented... so the big question is how much pain do the new records add... GUIDs (when generated with newid() as is generally recommended since it's random) ruin B-Trees, so performance tanks... random anything else (text/numbers/etc) are the same - keep in mind that a GUID is just a 128-bit number. unfortunately, security is often the reason for random (newid() for example)... so sometimes you take the good with the bad.
Database Administrator and Database Engineer are ambiguous and will shift meanings with every person you talk to. What you should look for are the desired skills and traits, match those with what you have currently and what you would like to do. I'm a Database Engineer and have had the past position of Database Generalist. In all, I'd say both roles are actually Database Administrator. I develop, administrate, tune, adhere to security, report and analyze data, etc. It's almost like there is a bucket, and that bucket is called "work involving the database" and that's what I do. If I had a niche though, I'd say my niche is in efficient architecture design and making things go fast. So don't look to the title of the role, look to the responsibilities and opportunity of the role. It is significantly harder to land an entry level database job in most circumstances because data is one of the most important pieces to an organization. I'm not going to hand the keys to the kingdom to a jr dba, they will need to prove themselves in lower environments and slowly get promoted up to production. I'm going to do the same thing with a senior, maybe much faster though. One trait I'd recommend for all folks to learn is high availability and disaster recovery. This is how you keep your job when things go bad, everything else after that is gravy. Again though, you may be strictly a 100% development DBA. Maybe you just do reports all day. I'd still recommend to focus on learning those skills though, it will only make you more valuable to your current organization and others looking to hire. 
I'm doing an ad hoc ETL at work today. It's fairly simple and just requires a bit of busy work. So we received a dataset from a 3rd party vendor. Their dataset is sent to us in 4 csv files. The structure of their data does not exactly match how we have it stored in our data warehouse; different headers, a couple of columns are missing... small easy stuff. Then finally separate the data by a conditional column. My work flow will be: Load the 4 partial files into staging tables. Join staging tables. Restructure the dataset to conform to our data warehouse. Partition dataset into about 12 tables based on that conditional column. commit;
When you say "random text", do you mean a random series of characters automatically being generated, or do you mean actual data? Like order ID numbers/codes? If you DBMS auto balances the index tree, then whether the keys are digits or strings, you still get log n lookups.
The first rule about fight club is not to talk about fight club. The first rule of ETL is to refer to it as wizardry and magic, and to explain that it costs a lot of money. That's it. No further detail. Ever.
In my experience, which is fairly new and less than a lot of people here... in my experience titles like DBA are "specialized niches" within the data science field. There are a lot of weird titles but here is a list of titles that get progressively more specialized: 1. Analyst - Bottom of the barrel. Typically uses Excel, and SQL to pull data and do some reporting. May use tools like SSRS/Tableau to develop some front end reports, or SSPS to look at distributions. Does not work with ETL usually. 2. Developer - Senior analyst/Engineer. Typically uses SQL heavily to do work to prepare data for consumption by tools like SSRS, Tableau, or to generate more complex models that are dependent on having specific structure to your data. Works with ETL. Also BI Developers, creating relational tables and working with BI Architects. 3. Integration Specialist - Focused on ETL, some light DBA work, monitoring jobs to make sure jobs are not stalling, looking at some jobs that take too long to run and resolving them, making them more efficient, looking at execution plans, etc. 4. Modeler - This title deviates from the path and moves towards the business. It's a highly intense mathematics role. In terms of "analytics" this would be the top of the food chain. 5. Data Scientist - Nebulous title unless you are working on things like machine learning. Generally a management position over other analysts/developers/modelers. Very little "science" involved in most positions, but how the data is structured for modeling/advanced analytics is very important. Does not work with ETL usually. 6. Architect - Focused more on the "backend" of the analytics food chain. They are not designing how databases are architected, but how the analytics environment is set up, how data is stored/managed. Similar to a DBA, and similar to a data scientist, but looking at the full scope of the process from ETL to front end consumption and making decisions relevant to making the process more efficient/compatible. May be doing more work with the IT group relative to how a website is collecting data, designing requirements for future collection, etc. 7. DBA - Top of the IT food chain in data management. Servers, backups, permissions, and some assistance in monitoring/optimizing processes. May work with ETL, or may simply work to implement the requirements designed/given to them by the ETL team. As I have found the type of work a DBA, or a BI Engineer, or a BI Developer does is different than the type of work "analytics" do. BI is about creating a warehouse to store data... analytics is about taking that data and "doing things" with it, and then creating new database objects that will store the results of a statistical analysis/predictive model/etc. You could have a DBA Architect, or an Analytics Architect. Similar jobs in the middle of the work flow, but the other ends are not similar in my experience. Certainly there are DBA's who do that kind of work, and there are probably analytics people who fuck around with servers and backups. From those roles you get into senior management. Director level, VP, all the way up to things like Chief Technology Officer, Chief Analytics Officer, to Chief Data Officer. A CDO is going to be mainly focused on DBA/BI type work. A CAO is going to be looking primarily at modeling/analytics. A CTO might be looking at both, plus website management, and managing other tools like AS400's, whether the company uses Outlook or Lotus Notes, etc. Like I said there are a lot of overlap between titles, and a lot of companies have no idea what the appropriate title should be. For example, currently my official official title is a very nebulous, "Senior Analyst," however it is understood by all that I am an "Analytic Developer" or "Analytic Engineer", and I am referred to as "the developer" on calls. My "title" in my email reflects this, even if my title with HR does not. I work with other "Senior Analysts" who have never used SQL, and who can't program in any language. HR would probably assume that we are equal, but there are some organization differences. I report to a different management structure, and based on who I report to and my compensation package I am a "Senior Manager," and in meetings with other "Senior Analysts" I am generally deferred to as though I have the same title as their boss, who are "Senior Managers." In reality I am currently working as our company architect. You'll find this a lot when you start a new job and after a few weeks suddenly go, "wait a minute... this isn't what I was hired to do, but this is what they want/need from my position." This isn't a bad thing, and generally it will give you some direction in terms of where you want to move. For example, if I stay with my current company then my next promotion should have the term "architect" in it, which then opens up future roles moving towards more specialization. What I'm trying to say is that none of this progress or work is really helping me become a DBA. I'm probably qualified to get a "junior DBA" role, but it would be a lower position than the one(s) I am qualified for. If I really decided I wanted to switch tracks and become a DBA then it might make sense to take a step back and start moving down the DBA track. Maybe if it were a small company I could function as their "real DBA" but it would be a hands on learning experience. So to me there are three areas of specialty in the data world: DBA (IT, servers), Analytics (statistics/modeling), and ETL (extracting, normalizing/transforming data so that it can be loaded into the database.) Depending on the size of your company, the size of your data, and other factors there may be a huge amount of overlap between these jobs, and specialties but I would say that the skills that are necessary to be successful in these niches are fairly different. You could be the best DBA in the world and command a huge salary... but you probably haven't learned shit about statistics, distributions, or creating predictive models. If you wanted to become a modeler you'd have to start at the bottom of the chain. Conversely a high end modeler might not have the foggiest clue how to back up a database, or do basic DBA related tasks. ETL tends to be more shared skillsets to a degree, but the difference between an amateur and a professional is huge. You know say you want to load some credit data that is over 100M rows of data and needs some heavy cleaning/transformation to make it ready for your modeling/analytics guys to dig into. Not saying a DBA can't do it, not saying a developer can't do it.... but if you haven't done it before, or if your company does a lot of this... a competent Integration Specialist is worth their weight in gold. Between BI/Analytics/DBAs you have a lot of similarity, and a lot of differences, and very rarely will you ever find a company that understands this when it comes to the job title they are broadcasting. 
Let me give you a real-world example: We use a HR software that is SaaS. Every day I connect to their FTP and download a flat text file that is huge and unruly but it has everything I need. Next step is to put this raw data into a staging table. The point of this expertise is just to get it into my RDBMS and nothing more. Next step is to use the staging table and put new records into my normalized structured tables in my warehouse, which is a schema that I designed for this purpose. That completes the *EXTRACT* portion. Now that my data is nice and tidy, I can make this data flow into other applications. HR data is a pretty good source because it has to do with attributes about employees who work here or no longer work here. It's great data to pass on. But to pass it on, we need to *TRANSFORM* the data to be compatible to the other system's record layouts. An example would be, maybe my database stores employment_status as "A" for active and "T" for terminated, but maybe the system I want this data to flow to accepts "Active", "Terminated", or "Leave". We use SQL to transform this data, perhaps using a bunch of case-when statements, and we make data that conforms to their record layouts. So let's say we've done that. The next step is to *LOAD* the data. This can be a combination of things, maybe it's FTP or sFTP, maybe it's copying files from windows shares to samba shares, or maybe it's over DB links and the data is going to another database. In the end, the data you transformed is loaded and used to by next system. So ETL is not something that belongs to SQL or any one process. It is many OSes, processes, databases, scripts, and protocols, working together to make data flow from one business process to the next. The goal for me is to do this automatically, with sanity checks and error trapping along the way. Typical sanity checks that I program into my scripts are things like: Did I get all my files? Are any of them zero-byte files? Are my record counts in each file within a threshold that is acceptable? Did we get a partial file? Did the FTP transfer work successfully? Am I on the right host? Am I logged in as the correct user? And for failures, I have a method of keeping track. Maybe my ETL process is 10 steps. For each step, I have checks to see whether it completed successfully based on my sanity checks and then I will make a decision based on the type of error. Maybe I stop the process and e-mail at what step it failed at. Maybe I wait 20 seconds and try again. Maybe I stop trying after 5 minutes and send a warning e-mail. Do I try to recover, or do I just stop? How you design an ETL process is up to you, but the more iron-clad you make it, the better off you are. Back when I was a novice, a simple problem of a failed FTP resulted in a zero-byte file and I let those changes cascade throughout my systems, totally wrecking everything. The zero byte file had essentially nulled out users in all of my other applications. You live and learn! For every simple process there are 100 "what if" scenarios you need to account for, but sometimes you don't know what you don't know until it happens, and hindsight is always 20/20. 
Could you add a GROUP BY on the column values that are the same for each set of 6 records?
I want to expound upon this... it doesn't have to be directed into a data warehouse. It can literally be used for just cleaning bad data, and putting it back into your database/table.
I hadn't considered that, I've tried inputting as suggested however I subsequently get the following return: &gt;Column 'PLSupplierAccount.SupplierAccountNumber' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. I'm not sure how to otherwise adjust, as I still want to be able to identify what Accounts the Contacts are associated with, and I'm not sure how I can include it within the Aggregate/Group. To show how I've attempted your suggestion I've included the MAX in the CASE columns as an example and the GROUP BY function as displayed below. WHEN PLSCV.SYSContactTypeID = 2 THEN MAX(PLSCV.ContactValue) ELSE '' END AS ConctactEmailAddress, -- FROM and JOINS seen in code segment in OP. GROUP BY PLSC.PLSupplierContactID &amp;#x200B; &amp;#x200B;
Thank you for the detailed response!! Is it easier to get a software engineering job? Doesn't look like people hire a lot of junior dbas or data engineers
Thank you for the answer!! I'm learning Python on Codecademy and plan on taking a data engineering course which focuses on python and postgresql. Am I wasting my time or should I just aim for a software developer position?
Maybe try: MAX(CASE WHEN PLSCV.SYSContactTypeID = 0 THEN PLSCV.ContactValue ELSE '' END) AS ContactTelephoneNumber, For each of your CASE statements, then GROUP BY: GROUP BY PLSA.SupplierAccountNumber, PLSA.SupplierAccountName, Sal.Code, PLSC.ContactName, PLSC.FirstName, PLSC.MiddleName, PLSC.LastName, PLSCV.ContactValueCountryCode, PLSCV.ContactValueAreaCode, PLSCV.ContactValueSubscriberNumber 
backups &amp; restoring databases quickly 
I would recommend you find the specific amount of Lantus in a vial (100 units/mL) and then how many vials are in cartons. Then put a price per quantity column in. That's your comparison. &amp;#x200B;
Well there's also pens, which is a different form entirely and I'm not sure the math would be the same for insulin volume in that case, if the actual injector costs were calculated into it. But also for a teaching moment, why should I do it that way? Because it's simpler? Or for some other reaso that 
Database engineers are different from data engineers, is this what OP is asking about?
Thank you, that's mostly done the trick. I was getting 4421 rows, I'm now getting 1346. I should be getting 897 but that is still considerably better than it was.
It sounds like you are confused about the job definitions of data engineer and DBA. DBAs do manage databases as some of their daily responsibilities.
I didn‚Äôt know they made lantus in pen form. It‚Äôs long acting insulin. I thought the pens were for the quick acting insulin. Irrelevant either way. Just thought it was interesting. So... here‚Äôs my professional opinion. And I can say this because I‚Äôm a DBA for a pharmacy. Lol. Our tables have a product ID which then has a column for the NDC which is the national drug code. We then have a cost that we purchase from wholesale, and all of our stores have a price they sell at. Each drug is listed in a new row based on quantity. So for example row 1 is sildenafil 50mg. Row 2 is sildenafil 100mg. Etc. we tie in on the Product ID most of the time. But the NDCs stay the same. It‚Äôs just the price differences are there. It sounds like all of this will be in one table anyway. The reason I recommended breaking down by cost per quantity of item is to show a fair assessment of prices. 
One thing to keep in mind, some people believe that it's best to follow a ELT standard. Yes you can do many transformations on the fly inside of SSIS, but I would much rather have my staged data be unmodified. Instead of doing transformations in SSIS I do them in SQL Server whenever possible. 
Also, bigints CAN max out if you are dealing with high-frequency transactions, in which case guid's either become necessary, or multi-column-keys become necessary. On those datastores I typically opt for guid - the performance impact is negligible on modern hardware and rdbms's.
The max for BIGINT is about 9.2*10^18 if you're using only the positive numbers. That's 29 BILLION records/second for 10 years. You're not going to max out a BIGINT primary key.
How about hashing it, then convert it to a string? In SQL Server: select master.dbo.fn_varbintohexstr(hashbytes('sha2_256',cast(newid() as nvarchar(128))))
This is great info, thank you for sharing your thoughts! There's one clarification that I'm not sure is a typo or not: In answer 1, you mentioned that we should know how to use union, Except, and I intersect. Did you mean that as a separate list of: * Union * Except * I intersect or is it Except intersect and I intersect, or something else? I ask because google shows that there's no "I intersect." Thank you again! I'm going to go searching for these different concepts you listed in the answers. :)
Surely you can't mean Extract, Transform, Load? It's the very act of doing that with data.
Well I do want to be able to manage a database and not a lot of programming. But I heard Database administration is a dying career. I am thinking of going either into database management and probably work my way up to a management position(project management)
That's really helpful! What made you go from system administration to database management?
https://docs.microsoft.com/en-us/sql/t-sql/statements/set-datefirst-transact-sql?view=sql-server-2017
&gt; What made you go from system administration to database management? Honestly, I fell accidentally into database work. I intended to shift into database or development because in both scenarios you are "writing code" and "creating things". I think system administration is pretty neat now and I could see myself going into system and enterprise architecture and design someday. I could see myself going into development too. I just like technology and want to learn everything. At a point though, you accumulate so much education and knowledge in a certain subject. I knew I liked database, Solarwinds made me a job offer I couldn't refuse in database and since then, it's been "golden handcuffs". Don't get me wrong, I love doing data work. If I wanted to up and shift into development or system architecture or networking, it's going to be a long road back to where I am now compensation wise. I figure once you get so advanced in one realm, you either stay there or you get to a point where you can begin to incorporate new skills at other jobs that can segway into something new. Like if the next job I worked they had a need for someone to work on Python on the side, that would be a great segway for me to enter the development arena with a strong foundation in data and data architecture and who knows where that could lead.
That's nice! Have you ever thought of going into project management? Or is it not likely?
oh look, another shitty blogspam by a shitty blogspammer. [Here's a better and less useless article in every fucking way.](https://blogs.msdn.microsoft.com/blogdoezequiel/2010/05/31/sql-server-and-log-file-usage/)
I mean, I had it happen, but OK. I am sure I don't exist and everything is just a simulation.
it CAN happen... which is why guids are 128-bit... and you can generate guids incrementally instead of randomly to keep your bTree's defragmented. but if you recognize that GUIDs can be sequential, the diff between int32/int64/guid/text is about sizing, not about fragmentation / performance.
I understand where you're coming from. My end goal is to go in management, especially project management. But it's hard to get an entry level role so I'm going to try to getting into software engineering or databases and work my way up
A good project manager doesn't need to have the technical aspects down but I think it is helpful to understand what it's like. Business analysts are a good stepping stone to project management in my opinion and in some cases, are one and the same. I think it would be easier to focus and work into project management rather than work up through IT. It's also good to note that project managers don't need to specialize in IT projects, but there are some who do. 
I was thinking of focusing on project management but it's hard to get an entry level job. IT is a fall back plan for me
You cant encode a 64-bit value in less than 64-bits (I mean you could in many cases, but there will always be the case where it needs 64-bits). So your choices are to simply make it humanly readable (convert to characters), and forget about imposing a length limit. Alternatively you collapse the GUID into some *unique* string based identifier, and you then have a lookup table in your database which maps the short string back to the full 64-bit value.
Like any retail scenario, each sellable unit has a sku associated with it. The sku/product lookup table has manufacturer, product name, volume, gross wt., net wt., etc. There will be a productXdistributor table that has the sku, for a given distributior, for a given price (, and you may have to put date ranges on there). Sometimes date ranges aren't necessary, if you push the MSRP/list price and the actual price down to the transaction line item. 
&gt; no... same reason that GUIDs are often discouraged for PKs... they ruin the B-Tree's fragmentation. GUIDs are still numbers dude so they still work just fine. Nothing breaks with the B-Tree's. The data just isn't "naturally" ordered.
I‚Äôm relatively new to SQL and working with DB. And have been using the inner join operator. I have not seen the ‚Äúin‚Äù operator before. When would I it? (Not trying to steal the thread from OP, just curious is all). 
It CAN'T happen unless you just skip almost all numbers. Guid is so large to have near 100% uniqueness in all circumstances, thats all. Sequential guids just allows you to replicate table easily in cost of real uniqueness. In all other cases bigint is enough if u don't need to hide order of ID for security reasons.
Blowing out a guid is near impossible in today's tech. Blowing out a bigint is possible, just involves a lot of data (or row inserts and deletions).
By far the best description on here. Thanks!
You have to spent near 600 years if you will insert 1 BILLION rows in a second. Please show me at least one DB that can provide such speed and task that can geneate such big amount of data.
Yes. At that point, you're convincing the company that you're a good fit. Worry about the salary when they ask, or when the offer comes in.
Salary: Yes. Also, it shows your hand so its bad for negotiating. Someone actually did something similar, exception during the phone screen and top-end was 10K *less* than what we thought their floor was. Remote: Probably not, at least not if I was reading it. 
You must not understand how things work to be able to make a statement like that. The "natural" order is about insert order. When normal auto-generators run they provide numbers sequentially. There is no real logical reason why they need to do that other than the fact that it prevents them from having to check if a previous number was already provided. None of that in any way effects the storage or read performance. Only the generation of the numbers. GUID's are just 128 bit integers. That's it. The fact that you "generate" then in a random order doesn't mean that once they are stored the B-tree can still be used to find the one you are looking for. Perhaps you should study relational calculus a bit more before spouting off in forums.
What I typically use it for... I have a list of Order #'s that I need to pull for Accounting from a historical table that isn't on a Production server... I can run something like this: SELECT * FROM Invoices WHERE InvoiceNumber IN (00123, 00124, 00125, 00126, 00127) Pretty simple, but isn't very useful for tons of OrderNumbers... so.. say we have a list of customers who we need the orders for... Accounting will provide us some Customer Numbers: SELECT * FROM Invoices WHERE InvoiceNumber IN ( SELECT InvoiceNumber FROM Invoices WHERE CustomerNumber IN ('c12345','c54321') ) Granted you could rewrite this by doing: ... WHERE CustomerNumber = 'c12345' OR CustomerNumber = 'c54321' but when you start dealing with hundreds or thousands of values it gets easier to either to select them into a temp table or a new table then just do something like: WHERE CustomerNumber IN (SELECT CustomerNumber FROM #TempTable`)
Outstanding. Thank you for the information and provided examples!! 
I suck at wording things. GUIDs are just 16 pairs of Hex values [0-9][a-f] (Base 16) I'm trying to get a [0-9][a-z][A-Z] (Base64) representation of that I have it working in C#, but the conversions in SQL are squishing the 64 bits down to 32bits somewhere 
GUID example --689d3450-283a-4262-921e-2c4d8367ddee Proper 64bit representation (C#) --4bp3Lq8bTwM5FHSWaN4oXc my busted SQL code --59BLw0Yc2D1
&gt; Perhaps you should study [...] before spouting off http://sqlblog.com/blogs/davide_mauri/archive/2014/04/06/guid-fragmentation-and-performance-the-eternal-flame.aspx the Microsoft MVPs agree... random value insertion causes fragmentation of the B-Tree, which causes additional IO operations to perform the same lookups, which affects performance.
big question is whether rebalancing the index b-tree can be performed without impacting table usage... SQL Standard Edition requires the table to be completely blocked during a nonclustered index rebuild... Enterprise Edition supports "online" (during activity) index rebuilds, but comes at a cost... can't speak to other DBMS's.
Python and R are very similar. The best way I can explain the difference is that there are more suitable uses for Python when it comes to ETL, and R seems to be more suitable for statistical programming. Both can largely do what the other can do, but there are some strengths. It really depends on where you want to go and what you want to specialize in. &gt;Am I wasting my time or should I just aim for a software developer position? I don't think you're wasting your time at all. But you are combining skills that might be best suited for the ETL/DBA/Integration side of the niches and specializations. That doesn't mean you can't double dip and get your statistical game up, or that you can't use Python for statistics, or that you can't just use SQL for analytics. It really just becomes a matter of where you want to go, what you want to do, and where you see your career in 10+ years. If you want to be a DBA for example, or go down that "road" then I would imagine Python is much more valuable than R, but SQL/Oracle is going to be more valuable than Python, and tools like SSIS are very valuable to learn. Personally I am going down the analytics path, so its difficult for me to give you specific advice relative to a specialized niche that I am only familiar with at a cursory level. Within analytics itself there are sub-niches, marketing analytics, process excellence, machine learning, risk assessment, etc, etc, etc. Then you have even more nebulous niches like "process automation." They all use the same tools and skillsets, but they have different objectives... are you looking at a call center and doing analytics to determine what practices are most efficient in order to increase customer services? Are you looking at sales data and trying to make the company more money? Are you taking manual processes and automating them to save time, or allow the business to downsize a department?
Sorry, my bad. I saw "imgur-like IDs" and assumed your main goal was to summarise them into much shorter strings I'll go back and reread the question :) 
Just a follow up, I cut my teeth on SQL Server but later moved to other platforms. I wish I still had an SQL Server instance to test this on What happens if you define @cBase as a BIGINT? My hunch is it's this statement which is causing problems, causing @bigintdata to prematurely evaluate to 0 SET @bigintdata = @bigintdata / @cBase 
Why's it have to be remote?
Here's a good example of a borderline shit blog post, linked to by the blog author: https://www.reddit.com/r/SQL/comments/9iafz8/5_best_practices_for_writing_good_sql_queries/ As I commented in the thread (with downvotes of course), the quality is of the blog post is low, and a lot of the information is straight up wrong. Yet, it's by far the highest voted post on the front page of /r/SQL right now. What gives, is it 95% newcomers here who innocently think that blog post is gospel? Grandma's notion that "if you don't have anything nice to say, don't say anything at all" is not valid when it comes to technical information.
It‚Äôs just what I am looking for. Driving to and from to do a job that could be done at home makes no sense to me.
Lots of mortgage companies offer this for their tech folks. Not right away usually, but after initial training. 
Good to do...in a week? [https://www.udemy.com/the-complete-sql-bootcamp/](https://www.udemy.com/the-complete-sql-bootcamp/)
Wish I could find something like this. I'm a data analyst and work from home 99% of the time and I'm really not that busy. 
with that, cBase is constant : @cBase int = LEN(@charSet) It just lets you dynamically specify the character list you want to use, for example, you could remove the zero from the char list and it would be base 61 instead of 62 then. I think the issue might be coming from the Modulo operation against bigint :\ Also, SQL express doesn't have a trial period if you wana run it locally :P If I figure it out I'll post the solution 
Perhaps you should study anything about at least binary search tree and balancing of it, if you don't want to understand b-tree. And then something about clustered keys and how data is stored in rows. And about data pages and fill factor.
I just wasn't sure if it was accidentally typecasting the result as an INT when doing SET @bigintdata = @bigintdata / @cBase &gt; Also, SQL express doesn't have a trial period if you wana run it locally :P I used to develop a bit on SQL Express (and deployed desktop software using MSDE, its predecessor). Great product. But I moved to Mac so not really an option anymore :) 
Are you a member of PASS? The BI Community of PASS is growing like mad. That community is the first place I look when we are hiring. [PASS](https://www.pass.org/) 
I don't think it's possible to run out of bigints but I'm willing to keep an open mind. How did that happen in your case?
I learnt it reading the Oracle rdbms documentation since that was the database I started my career with. all popular databases will have their documentation available that will go through the basics and more. you can find them all online. sql server, postgres alternatively you could just Google "sql basics" or something of the sort and use the resources available online. good thing about the database documentation is that it is structured and covers many of the data modeling and warehousing concepts I suggested as well. if you can grok a lot of it you'll do well. I spend a lot of time building dimensional warehouses so I read the data warehouse toolkit by kimball early on and still use it for reference. I also read the etl toolkit and it was very helpful when working in a data warehouse. 
Thank you very much for the advice! I was thinking about the DBA/ETL and Business Intelligence careers as well. I think what I'll do is keep on learning and keep applying for PM jobs. If I get a PM job, I'll go that route and if I don't get a PM job, I'll just go the technical route
Ahh you're right, now I remember bashing my head against that (wrote this a few months ago). So I guess I am stuck manipulating varbinary :|
You know take my advice with a grain of salt and listen to other people. That's first and foremost. But secondarily it sounds like you don't really have a goal, or end game in mind. Which is OK, but it just kind of sounds like you don't care so long as the money is right. PM job, analyst job, ETL job, DBA job... if the money is right, you don't really care, because you'll take any of those jobs. My advice to you is to **stop** portraying yourself that way. Pick something. Decide where you want to go. You can always go back and switch tracks. When I interview someone I want the person that knows what they want to do, and if what they want to do is the job we have available... they get huge points from me. That's the guy I want. I don't want the smartest guy in the room, or the guy with the most education, or the most accomplishments. I want the guy that knows what he wants, and is hungry to get there. I want the guy who wants the job *we have*, not just any job so long as the money is right. I mentioned in my original post that taking a junior DBA job would be a step back for me. It would be. But when work is difficult I think about how much "easier" it would be to go down that track and take a step back. I don't want to say it's a "back up plan" in the context of one being better than the other, but it isn't the track I wanted to pursue originally and switching over would probably be a minor set back in terms of career development. In reality it probably isn't easier at all, and it has a bunch of annoying problems that I just don't know about because I'm on the outside looking in. What I'm trying to say is that I'm not interested in those kind of roles. They are not geared towards the specialty or niche that I've been working towards. All of them can command ridiculous salaries depending on where you work, and what you can do, and all of them can be highly profitable to do freelance. For me the frustrating thing is when I go into a room with a DBA with 30 years experience, but who has 0 understanding of analytics, statistics, or what we want to do with the data they are maintaining... and trying to explain to them why I need to do certain things a certain way, and that I don't really give a fuck about their rules, normalization, or any of the DBA type shit they care about. Those are very difficult conversations to have. What I'm more trying to impress on you is that the general field of "data" has many little corners to it, and you should be aware that knowing something about one corner does not mean you know much about other corners. Perhaps you will become a specialist in multiple corners. Everyone makes a decent living. It is a humbling field where you are always learning new things. I much prefer it to my previous IT work in software development, and IT. I've bounced all over the IT field since I was a kid from network engineering, to security, to hardware, to programming, to web, to e-commerce, and now to data. I like data. I'm not looking for PM jobs even if they pay twice my current salary. Fuck that. What I like about data the most is that it leverages such a diverse set of skills. For example, I used to do analytics for a large scale call center. I happen to have a lot of experience working with phone systems, hacking phone systems, etc. Reading phone system data was intuitive, and then writing algorithms to interpret and statistically analyze that data was also intuitive. By the time I was done working on that particular project the vendor was deferring to me as a subject matter expert, and I was participating in high level meetings with their telephony engineers about integrating specific "features" which would allow us to more effectively collect data --&gt; which would then be used for more in-depth statistical analyses --&gt; which would then lead to further requests for new features, tests, etc. I mean I can't tell you how to install that phone system, or turn everything on... but once you do I am probably one of the top people in the world at looking at the data and troubleshooting bottlenecks in your customer service experience. That is a **very** little niche specialty, but it can also lead to amazing things. You can greatly improve quality, you can massively grow sales, you can do so much with data that if you find the right company that aligns with your particular skill set... and you design and build all of their statistical models, or all their ETL, or all their data systems... I mean you aren't exactly irreplaceable but replacing you is an extremely difficult process. You can basically charge whatever you want in those cases. $500? No problem... go do six hours of work and we'll call you the next time we have an issue. Or you try to get a salary job and just automate the hell out of it until you just hang around looking at the stock market all day waiting for something to break.
You could represent the GUID using two BIGINTs Depends how you plan to use these strings, but you could split the GUID into a and b strings where a is the top 64 bits encoded as base64 and b is the bottom 64 bits encoded as base64 Then using imgur as an example imgur.com/a/b Then to find what resource its trying to locate, you just decode a and b, concatenate them and form your GUID 
You're right. I graduated college a year ago and not sure what field in IT I want to specialize in. I learned so many different things but I'm still picking what interests me. I think I will try to get internships and see what really interests me. Thank you for your advice!
Central table that serves as a "hub" for multiple child tables... Can't be more specific. Anyways, that table supports cross-linking of records and is time-stamped everytime a record is updated. Additionally, as an audit function, this table has every touch logged in an audit table. Updates, deletes, inserts, all tracked. The system processes millions and some days billions of transactions per day. Most if not all of those involve either updating or inserting records into this hub table. Every touch results in an audit record. The audit table had a bigint for a primary key. It maxed out. Honestly, I would have designed the entire thing differently, but I wasn't here 15 years ago. Hell, I was just graduating high school when this was designed. My quick fix was to change the primary key over to a guid field. Design wise I wouldn't have had a pk for the audit table aside from base table primary key + timestamp. But that ship has sailed 15 years prior.
That's a lot of data. 8*10^63 bytes for the PK alone. That's dozens of exabytes.
nope, why do you ask that?
I work at Salesforce. The largest "location" listed for our employees (behind San Francisco) is "remote". Look at salesforce.com/careers, and PM me if you have any questions on your search. Source: 6 year employee working remotely, elbow deep in DBs alldayerrday.
I forgot to mention we archived/pruned the table to offline compressed backups every few years. But for audit purposes we couldn't recycle the keys that were pruned. Thus the switch. That system has since been retired/replaced with something entirely different and more efficient.
Could use some more detail but I'm guessing you will want to look into the Replace() function. For SQL Server: https://docs.microsoft.com/en-us/sql/t-sql/functions/replace-transact-sql?view=sql-server-2017 
Update: after some help, it was pretty easy. Just added: ```(SumofActual_Price-SumOfAcutal_Cost) AS Margin```
I tried UPDATE Stores SET StoreName = CASE StoreName WHEN ‚Äò%Inc%‚Äô THEN ‚Äò‚Äô END: and it deleted every record...
There must've also been a large percentage of keys that were reserved but never created/stored, such as when a transaction is rolled back against an auto-incrementing field.
Aetna hires a lot of full time telework sql developers, their job search even has a telework only option.
Oh indubitably there were gaps too, tbh I never dug deep enough. I was just "the fixer" called in to find a stopgap solution. Bought them 3 years before replacement.
I tried UPDATE Stores SET StoreName = CASE StoreName WHEN StoreName like ‚Äò%Inc%‚Äô THEN Replace(StoreName, 'Inc', '') Else StoreName END: and it deleted every record...
I would agree that with the future of cloud infrastructure, traditional DBA tasks will disappear. I would argue, however, that DBAs will always be needed. Their skill requirements just might change. If anything, would say the role of a "database manager" will be consumed by the DBA position. And if you want to get into project management, you don't really need any of the data skills to do it. Just start doing it.
In this case, you have to work carefully because you have to remove Inc if it is only a single word like 'Walmart Inc' not as 'Easy-day Inca'. Basically single replace command if enough for this like as UPDATE Stores SET StoreName = IIF(Right(StoreName,3)='Inc‚Äô, Replace(StoreName, 'Inc', '') ,StoreName) Hopefully it will help.
It might take you 10 years. The most your'e exposed to the better. Ideally you want to end up somewhere that leverages all of your past experiences in some unique way. Companies like that are hard to find, but you have 10 years.
Considering the last word "Inc" has a white-space in the column value, just by doing a simple REPLACE will do. But you need to make sure it does not replace "Inc" pattern occurring with another word, like: "Walmart **Inc**ome". &amp;#x200B; UPDATE Store SET StoreNames = REPLACE(Store, ' Inc', '') WHERE StoreNames like '% Inc'
The cover letter is the wrong place to put this information. It's not what the cover letter is for - which is to provide a synopsis of why you think you'd fit the position the best and why you're interested in the position in the first place. That being said, mentioning the salary or any benefits early in the conversation doesn't make you entitled, nor should you worry about salary only until the time of an offer. The offer is usually the last step in the process and by that time you've already went through several rounds of interviews. I am always upfront with the recruiter/HR personnel as to the target salary I'm looking for and any specific benefits I feel would be appropriate for the position. I usually bring this type of stuff up after the first phone interview with HR when the position is better explained. There's no reason to waste HR's time, the hiring personnel's time or my own time if the salary range they've chosen for the position is on a different planet from what you're expecting. It's unfortunate that so many people have a stigma regarding talking about salary during the hiring process, while waiting for the company to make an offer. From my own experience, companies worth their weight in salt are far more likely to be upfront with salaries/benefits and are willing to negotiate such. Essentially, you are a company yourself, and need to market yourself and negotiate as such. You offer a service in return for some monetary receipt.
But...but...shouldn't we be preemptively scaling all of our infrastructure and services as if we were make-believing that we're Google? That's the cool thing to do nowadays isn't it?
&gt; which causes additional IO operations to perform the same lookups Which only matters until the B-tree is balanced again. Which, these days, happens quite frequently. Yes, it is true the B-tree is slightly less performant for the edge leaves that have yet to be integrated... but a simple rebalance makes it perform just as good as a B-tree of longs or ints. 
https://www.reddit.com/r/SQL/comments/9irjsd/if_i_manually_set_the_primary_key_is_the_lookup/e6mzmbz
Unfortunately turning the initial `SELECT` into `SELECT DISTINCT` didn't make a difference to the number of rows returned. I went back to the query and associated tables and realised my error. PLSCV.ContactValueCountryCode, PLSCV.ContactValueAreaCode, PLSCV.ContactValueSubscriberNumber The 3 lines above were a broken down version of `PLSCV.ContactValue` when `PLSCV.ContactValue` is referring to Telephone/Fax/Mobile as determined by `PLSCV.SYSContactTypeID` so I removed them from the GROUP BY and am using just the `PLSCV.ContactValue`. Thank you so much for your help.
Lots of data engineer jobs at weworkremotely.com
&gt; you could remove the zero from the char list and it would be base 61 instead of 62 then. This shows you fundamentally misunderstand how baseX encoding works. You will not deduce a correct implementation until you correct your understanding here. Here's a question, how many base64 characters does it take to store 64 bits? How many base62 characters?
If you insert non-sequential keys in b-tree, it's leaf level have to be moved on disk to another location, not just rebalance of search nodes. And if it is clustered key for a table, that means DB has to rewrite all content of table again. It's never should be on frequent basis. I had few years ago a table for external requests with GUID clustered PK. At first at one random day in a week daily maintenance script figured out that fragmentation is above 30% and called index rebuild. Then number of requests started to increase with time and full rebuild was done every day at night. And then one day sql server just decided to rebuild it during the day automatically while inserting new row, and DB hanged for 20 minutes, great! &amp;#x200B; And now you are trying to tell me, that non-sequential key in b-tree is good? Maybe only if your table has less than 10k rows. Even blogpost in the link above tells you how dramatically speed decreases if tree is fragmented just in case of selecting.
oh wait i see now. thank you!
[removed]
[removed]
Because it's a subscription based website...
What if the value wasn't a number, but a date?
MAX(DateColumn) also works fine.
Late response, but based on your recent posts you're still dealing with this issue. Sorry I didn't see this comment earlier. &gt;an organiser may need 10,000 tickets to be sold won't the insert be abit expensive on the database? Not really. Sure, it's a more expensive insert than typical, and you wouldn't want to do it frequently, but it's not like you'll have dozens of events being created every minute. I'll demonstrate. First things first, you need a numbers table. Run this: CREATE TABLE IntNumbersTable(n INT NOT NULL PRIMARY KEY) ;WITH numbers AS ( SELECT n FROM (VALUES(1),(1),(1),(1),(1),(1),(1),(1),(1),(1)) AS Vals(n) ) INSERT INTO IntNumbersTable SELECT ROW_NUMBER() OVER (ORDER BY a.n) FROM numbers a, numbers b, numbers c, numbers d, numbers e, numbers f, numbers g This script will create a table called IntNumberTable and fill it with 10,000,000 records, starting from 1 and ending at 10,000,000. **This is very computationally expensive and may take a few seconds to a few minutes to complete (depending on your computer). This is not the example I want to show you.** Now we'll create a ticket table. This will have a minimal amount of data for this example, but the number of columns won't significantly impact its performance. CREATE TABLE Tickets(TicketID UNIQUEIDENTIFIER PRIMARY KEY DEFAULT(NEWID()), EventId INT NOT NULL, LastReservedTime DATETIME2(4)) The only information needed to populate this table is the ID of the event and the number of tickets we want to insert. To insert 10,000 records, we simply need to insert the EventId in the table 10,000 times. We can easily insert 10,000 records at a time using a subset of the number table we created earlier. SET NOCOUNT ON; DECLARE @DebugTimeStart DATETIME2(3) = SYSDATETIME(); DECLARE @EventId INT = 10; DECLARE @NumberOfTickets INT = 10000 INSERT INTO Tickets(EventId) SELECT @EventId FROM IntNumbersTable WHERE n &lt;= @NumberOfTickets PRINT(DATEDIFF(ms, @DebugTimeStart, SYSDATETIME())) On my machine, I get a result of &lt;50 when executing this query. Meaning it takes less than 50 *milliseconds* to insert 10,000 records. It slows down when inserting more records, to at most a few seconds, but performance is acceptable for intermittent use, which is what you be expect. &gt;a User may purchase 1 ticket..i delete that from the quantity and maybe create a lock of time in the order table if it expires i can return the quantity I *very strongly* dislike this approach. **Very strongly**. It's a reasonable conclusion to come to, but will set you up for endless headache in the long term. I explained why in my first comment. You should **not** update your data based on the current state of your data. Let your data describe itself. Don't try to track, maintain, or update the number of available tickets, simply calculate it. Using the Tickets table above, let's see how we can do that. Before we start, let's add a reservation duration column to the Tickets table, that way the duration is specific to each ticket ALTER TABLE Tickets ADD ReservationDurationInSeconds INT NOT NULL DEFAULT(0) The first step in the process is going to be checking how many tickets are available. We'll create a view: CREATE VIEW AvailableTickets AS SELECT EventId, COUNT(1) AS AvailableTickets FROM Tickets WHERE (LastReservedTime IS NULL OR DATEDIFF(SECOND, LastReservedTime, SYSDATETIME()) &gt; ReservationDurationInSeconds) GROUP BY EventId My results look like this: EventId | AvailableTickets -|- 10 | 10000 The important bit there is the `WHERE` clause. I'm checking the last reservation time for the tickets. If the ticket's `LastReservedTime` is within the ticket's reservation duration, it is excluded from the results. In our case, no tickets have been reserved, and all columns will be null, so every ticket will show as available. Let's fix that. Next, we'll write a script that reserves a number of tickets for 10 minutes. DECLARE @EventId INT = 10; DECLARE @NumberOfTicketsToReserve INT = 3; DECLARE @ReservationDurationInSeconds INT = 600 -- 10 minute reservation duration UPDATE TOP (@NumberOfTicketsToReserve) Tickets SET LastReservedTime = SYSDATETIME(), ReservationDurationInSeconds = @ReservationDurationInSeconds OUTPUT Inserted.TicketId WHERE (LastReservedTime IS NULL OR DATEDIFF(SECOND, LastReservedTime, SYSDATETIME()) &gt; ReservationDurationInSeconds) AND EventId = @EventId In this query, I'm accepting three parameters: The event id, the number of tickets the user would like to reserve, and how long their reservation will last. The `WHERE` clause in this query is the same as the previous one, meaning that *the only tickets eligible to be reserved are tickets that are not* ***CURRENTLY*** *reserved*. Next, I want you to notice the `OUTPUT` clause. This selects the TicketIds that you just updated. In my case, it returned TicketId | -| D69056C6-9794-4B54-B035-000AF8F4FCBD| 47B63F76-F13C-4950-8CFE-0014E4A2AC57| 7FE465B8-AD0B-4805-AE66-00167C24E538| You can send these values to the users, these are the tickets that they have reserved. If and when they purchase these tickets, you can update these rows' "LastReservedTime" to be equal to the event date, that way until the event is over, those tickets cannot be reserved or purchased anymore. Now, run that query, and then run the previous query (the one that counts the number of available tickets). After I did so, the result of the query was this: EventId | AvailableTickets -|- 10 | 9997 I didn't have to update the available tickets, because just through the action of reserving them my query was able to tell that those tickets were unavailable. Now, real quick, let's testing something else. The following query is just the above two queries combined in a special way. First things first, we're going to reserve three tickets for two seconds each. Then, we're going to check how many tickets are available for that event. After that, we're going to wait for three seconds (`WAITFOR DELAY`, so that the tickets should no longer be reserved. Then we check how many tickets are available again: DECLARE @EventId INT = 10; DECLARE @NumberOfTicketsToReserve INT = 3; DECLARE @ReservationDurationInSeconds INT = 2 UPDATE TOP (@NumberOfTicketsToReserve) Tickets SET LastReservedTime = SYSDATETIME(), ReservationDurationInSeconds = @ReservationDurationInSeconds OUTPUT Inserted.TicketId WHERE (LastReservedTime IS NULL OR DATEDIFF(SECOND, LastReservedTime, SYSDATETIME()) &gt; ReservationDurationInSeconds) AND EventId = @EventId SELECT * FROM AvailableTickets WAITFOR DELAY '00:00:03' SELECT * FROM AvailableTickets Your results window should look similar to this: TicketId | -| D69056C6-9794-4B54-B035-000AF8F4FCBD| 47B63F76-F13C-4950-8CFE-0014E4A2AC57| 7FE465B8-AD0B-4805-AE66-00167C24E538| EventId | AvailableTickets -|- 10 | 9994 EventId | AvailableTickets -|- 10 | 9997 Note: Mine is showing 9994&gt;9997 because the 3 tickets I reserved for 10 minutes earlier are still reserved. If yours shows 9997&gt;10000, that's fine, their reservation has probably expired. Also your TicketIds will be different. The point of this demonstration was to show that you don't have to do any extra work to maintain the number of available tickets. ***THIS IS NOT A COMPLETE SOLUTION*** The tables and queries above do not fully protect a reserved ticket from being purchased. If I reserve ticket id `7FE465B8-AD0B-4805-AE66-00167C24E538` for 10 minutes, when that reservation expires the next person to reserve a ticket will end up reserving the same ticket. Then if I try purchasing it, I would purchase their newly reserved tickets. Since I'm thinking of it, I'll offer you a solution to that too. First, we should sort the tickets by `LastReservedTime` ascending, that way we're reserving tickets in this order: Never been reserved &gt; Oldest reserved. This doesn't solve the issue, but it makes it less likely to occur, and has the added benefit of making your reservation effectively last longer. To fully solve the problem, we need some way of saying "this ticket was reserved by this user". However, since a user may not be logged in, you may not have a user id to associate with the record. In that case, you can use a randomly generated token that the user will send along with the purchase request so that the server can validate that the reservation belongs to them. Let's implement that. ALTER TABLE Tickets ADD ReservationToken UNIQUEIDENTIFIER Then we're going to change the reservation query. DECLARE @ReservationToken UNIQUEIDENTIFIER = NEWID(); ;WITH t AS ( SELECT TOP (@NumberOfTicketsToReserve) * FROM Tickets WHERE EventId = @EventId ORDER BY LastReservedTime ASC ) UPDATE t SET t.LastReservedTime = SYSDATETIME(), t.ReservationDurationInSeconds = @ReservationDurationInSeconds, t.ReservationToken = @ReservationToken -- New OUTPUT Inserted.TicketId, Inserted.ReservationToken -- New WHERE LastReservedTime IS NULL OR DATEDIFF(SECOND, LastReservedTime, SYSDATETIME()) &gt; ReservationDurationInSeconds Now, each ticket that you've reserved will have a unique code associated with it. The user will receive this code, and store it along with the ticket information in his cart. Then, when he tries to purchase the item, your query will validate that the reservation token sent by the user matches the one on the ticket. **IF THEY DO NOT**, the ticket has been reserved by someone else, and the transaction *MUST* be aborted, and new reservations need to be made. I've run out of characters (10,000 character limit) and time. I've spent over 4 hours writing this, building this data structure, and testing it, all for you. Please spend at least as long trying to understand what I've shared with you. I would really appreciate that.
&gt; You can represent any number in any base You can, but you're not doing it even remotely correctly. Go ahead and fix the 64 instead of 128 bit problem you also have and you'll see. I wouldn't say grumpy, I just came back to the thread and saw your responses continuing to travel down the wrong road and tried to give you a heads up. Good luck.
If you want your DBA to hate this is a perfect starting point. It's a good segue to running over his dog.
Koch Business Solutions lets you work remotely. So I hear. ;)
I'm sorry I don't understand. Why is this bad?
The GIFs are pretty cool for helping to explain joins, I personally have a script that creates similar tables and gives result sets for each type of join to visualise it. Thanks for sharing.
Bordering on self promotion: I covered this topic on my blog. Check it out.
Do you happen to have a link to this?
SQL is a set based query language. It's built upon a database engine and optimizer that is meant to determine what the database infrastructure is in place and it "should" make the fastest process to handle the request if the query is written appropriately. Adding a cursor is like telling a Nascar pit crew as the car comes in you are going to give instructions 1 step at a time and nothing will be performed in parallel until the previous step has been completed. Sorry if I came off vehemently opposed to your idea but 99.999999% of the time there is little justification for a cursor to do what you are proposing to do. Cursors seem like second nature to people who come from scripting or object oriented languages. If you want interate over a dataset you use something like a for/foreach/enumerator. This is wrong in the SQL world. A SELECT in itself is a foreach statement. It can be combined with something like an OUTPUT statement (pretty sure ORACLE has his). To where you INSERT &lt; SELECT &gt; OUTPUT &gt; DELETE. This performs minimal reads of the database table. In a cursor, that "minimal read count" is multiplied by N rows in the cursor. So rather than walking away with 12000 logical reads you end up performing 167,000,000 reads. At that point you are hoping things are cached in memory. Then there is the fact that one set up minimal reads is alwayd ran in serial. You wont take advantage of parallelism. You will however tax the CPU consistently over a long period of time, constantly blocking in flight processes and locking the source tables for that cursor. There are times and places for cursors but as a novice or intermediate sql developer you should abstain from them till you know the implications of them. As a Senior BI developer I have written 3 cursors in the past 8 years. 2 of them were for migrations that never persisted on the server. 1 was a management task unrelated to data or etl processes. Don't write them. If anyone here proposes them for reporting or etl tasks, they are dillusional.
This looks great but seems to be outside my skill set at this time. I will try to learn it but knowing what to change in that coding is difficult when I've never used python.
It's on the https://SelectCompare.com You will have to go to the blog section, there is a few posts about the setup and querying.
Month end reporting
I don't have any formal training in cursors, I'm just trying to learn on my own at work and trying to make scripts that can be run for certain tasks that require the least amount of input to use again. One example is a cursor that updates multiple tables based on the result of a cursor. It's useful because I'm only having to enter the values once and run one statement rather than doing a separate statement for each update and insert. It usually doesn't take very long, usually within a couple seconds if there aren't a bunch of values. If we were only using one value, we could just declare it a variable and be done with it, but with multiple values a cursor is needed. Is this the wrong way to look at things?
Access is a simple option
There are two aspects to a database in this case. You have the engine which is based of a service, this is what hosts the tables / database and runs in the background. Then you have the GUI, this lets you interface with the database. MySQL does have a command line (most databases do) that you can use to connect to the instance and run commands, like create a database or run SQL on tables. It is not recommended to do all of this via command line, I recommend finding a GUI tool. Workbench is nice if it's compatible with your MySQL, PHPMyAdmin would be a close 2nd in my opinion. Once you have a tool to access your database with, things become easier. Are you that far currently?
Yes, I agree. For some reason the business version of office 365 I have does not allow me to download access. 
No I'm not there. This is the first time I've attempted to do anything in SQL outside of code academy. I will need to do this the easiest way and then over time discover superior ways of doing it imo.
Would you share that script please? :)
Hey, Thriven, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Have you installed MySQL already? I would focus on installing MySQL and then work on installing workbench. https://dev.mysql.com/doc/refman/5.6/en/osx-installation.html https://dev.mysql.com/downloads/workbench/6.0.html
Why did I not think of that. I'll give that a shot later, hopefully it does.
I downloaded workbench and it's not intuitive at all imo. I am stuck on getting past the source selection and target selection. I also do not have ODBC Administrator. 
If you can get to the home page in workbench, you should see MySQL Connections. You can hit the + icon to add a new connection. I'd recommend to connect via TCP / IP. From there you need to plug in the IP, username, and password. The user / password would have been set up in the beginning during the MySQL install. 
Export excel spreadsheet to csv, then import the csv to sqlite. Sqlite works on both OS. sqlite&gt;.mode csv sqlite&gt;.import c:/sqlite/city.csv cities
Getting a couple of errors. In workbench, it is saying "authentication plugin 'caching password ' not working and image not found. Unless I'm closer than I think, this is going to take a lot of back and forth.
Ok, I'll give this a try shortly. Do I need to use the DB Browser for SQLite?
you can use it via command line, don‚Äôt need to use any db browser
I didn't listen to this video, but I skimmed it. See how familiar this all looks for you. I don't have much IOS experience, so it's hard to know how much of this is an IOS issue vs understanding MySQL. https://www.youtube.com/watch?v=aY6LiTbfckA
Ok, understood.
Hackerrank has sql problems
Thanks!
Something like this: Select Car.TID, Car.TripState, Car.Fare as CarFare, Plane.Fare as PlaneFare from TRIPS as Car join TRIPS as Plane where Car.TID = Plane.TID and Plane.TravelMode = 'Plane' join TRIPS as Train where Car.TID = Train.TID and Train.TravelMode = 'Train' where Car.Fare &gt; Plane.Fare
Might help to lay it out logically by splitting the problem in two queries and then combining them: First find the [MAX()](https://www.w3schools.com/SQl/sql_min_max.asp) of 'Fare' where 'TravelMode' equals 'Train' Next, you want to return all values of 'TID', where TravelMode is equal to 'Car' and 'Fare' is bigger than (put first query here)
Thanks, I working in production support and most of the time we just run scripts that developers send us, but I try to make things more efficient if I see an opportunity to improve something. I think what you're suggesting would work, it looks like you're storing the numbers to backup and delete in backup_Move, and then you would insert and delete anything in the backup_Move table? Would you eventually want to truncate backup_Move so it didn't get too big? I suppose that would work, but we're still putting (12345,12346) in two places. The reason why I wanted to use a 2nd cursor in the first place was so that I could put the values in the first cursor and we'd only need to put numbers in once place. It's not a deal breaker, but my goal was to minimize the amount of places that someone would need change something before they run it. 
I will be messaging you on [**2018-09-27 09:25:47 UTC**](http://www.wolframalpha.com/input/?i=2018-09-27 09:25:47 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/9j4y9e/update_sql_editor_now_lets_you_save_tag_rate/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/9j4y9e/update_sql_editor_now_lets_you_save_tag_rate/]%0A%0ARemindMe! 12 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I'm sorry, can you be more specific about how I would use this? In your example, where is c_input coming from? 
When I was learning SQL, I printed this diagram and put it on the wall in my office, it helped me so much. https://i.stack.imgur.com/UI25E.jpg
you don't need to worry about the processing load of millions of two number averages. that'll be less than a tenth of a second on an aws tiny instance
right but how to DO it? I have no idea how to even write a query to do an average like this. 
It sounds like you want all records where the due_date is greater then today. If so this should work for you.: Where due_date &gt; GETDATE()
Are you trying to get the averages of every possible pair, or just the adjacent ones? e.g. (1,2) (1,3) (1,4)... N^2 output rows or just (1,2) (2,1) (3,4) (4,3) ... N output rows? I don't know about it being "cheap", but it'd be pretty easy to make a derived row pair_id that's just the nearest even number (e.g. id 1 and 2 both have pair_id 2) then join the table on itself where pair_id matches but id doesn't match. I definitely wouldn't put that in real-time processing, but if you're doing some batch calculation job it does the trick pretty effectively. Don't forget to have an index on `pair_id, id` though. Will probably help. 
well i don't know where the rest of the query comes from, and there's some defects in your example, but something like (and i'm sure this is buggy, i haven't tested it) ```sql insert into tablename(col, cols, colz) values(thing, otherthing, (case mod($rowid) when 0 @rowid+0.5 else @rowid-0.5 end); ``` honestly it should probably be a default or a computed value in the table declaration instead, though (or just computed in the query) the short answer is "let the engine handle it, don't do this yourself" ```sql create table foo(a integer, b integer, z as (a + b / 2)); ```
It‚Äôll likely be used to pull reporting/data that is otherwise difficult to find anywhere else. And using looker to visualize it. Some things really stand out once you put the data in an appropriate graph. Use the data to tell stories 
\- jackdb \- popsql
Seconded. Please share!
Could you explain a little more with an example? It sounds like you‚Äôre talking about a left join. 
Use the original duplicates query as a sub-query and inner join back to your original tables to get the records of each duplicate set where your condition is true. Something like this: SELECT s.external_created_At, s.account_name, s.email, s.account_id FROM ( SELECT sc.external_created_At, sco.name AS account_name, sc.email, count(sc.email), sc.account_id FROM salesforce.contact AS sc JOIN salesforce.company AS sco ON sc.company_id = sco.id GROUP BY sc.email, sco.name, sc.external_created_At, sc.account_id HAVING COUNT(sc.email) &gt; 1 AND sc.account_id = 258 AND sc.email != '' ) S INNER JOIN salesforce.contact AS c ON s.email = c.email AND s. external_created_At = c. external_created_At AND s.account_id = c.account_id INNER JOIN salesforce.company AS co ON c.company_id = co.id WHERE c.Created_By_us = True 
is the due_date column actually a datetime or is it potentially a string data type? Maybe try casting due_date to a datetime to see if that helps... something like: WHERE cast(due_date as datetime) &lt;= SYSDATETIMEOFFSET() 
I would love to see something like this in MS SQL Server. Great idea!
 WHERE due_date &lt;= date'2018-09-26' 
I suggest you jump right in and download MS SQL Server Express Edition 2017 and SSMS. It's a free edition of SQL Server from Microsoft that lacks many enterprise features and limits you to 10GB of storage space but is great for a build-your-own-playground environment. You can download it here: https://www.microsoft.com/en-us/download/details.aspx?id=55994. These instructions are for 2016 will work for 2017 as well: https://www.bu.edu/csmet/files/2016/09/SQL-Server-Express-Installation-Guide-V01.pdf The download includes SSMS - the GUI for interfacing with the databases and writing SQL - and a few other utilities, like the SQL Server Import tool. Once you get all setup and logged into the server, create a database and start uploading spreadsheets to it. When you upload a spreadsheet it will create a new table automatically based off of the name of the worksheet. I'm skipping over a lot of details here but honestly this is the way I learned not just the basic of SQL but database structure and administration. Also it was my first real interaction with enterprise grade software, which has helped me greatly at work.
1st solution: add this line to the HAVING clause. It will count the number of times `created_by_us = TRUE` and filter out records where it is never true. `AND sum(case when created_by_us = TRUE then 1 else 0 end) &gt;= 1` Million dollar solution: add this line to the HAVING clause. It will determine the first date that `created_by_us = FALSE` and the first date that `created_by_us = TRUE` and filter out records where one or the other doesn't happen or when the first true comes before the first false. `AND MIN(case when created_by_us = FALSE then sc.external_Created_at else NULL end) &lt; MIN(case when created_by_us = TRUE then sc.external_Created_at else NULL end)` Please make your check payable to jjoonnnnyy :)
Do you mean if there are two results in the second table, you just want the first match?
Honestly THanks so much for this i am forever grateful
I'll save you up for succesive queries i ain't the best currently a newbie but aspire to be like you
Strictly speaking I don‚Äôt think there is an official structure as to how to document SQL coding in layman‚Äôs terms. Best thing to do is break it down naturally and be sure to explain the critical features that your audience is looking for. Maybe not how for/while loops or left/inner joins work but what is produced as the result and why/how the result is critical to get the job done. 
Thanks... I did what you suggested and kept triggers out of it and the migration worked and I found a workaround to do what the triggers were meant to do :)
Yes
Yes
hey! we support MS SQL Server, you can give it a [try here](https://www.seekwell.io/reddit).
Ok, if you mean what the code behind the exit button then it's this: Private Sub Form\_Close() Dim intCourseRunId As Integer DoCmd.RunCommand acCmdSaveRecord intCourseRunId = Form\_frmCourseMainForm.txtCourseID With Form\_frmCourseMainForm .Requery .txtCourseID.Visible = True .SetFocus .txtCourseID.SetFocus End With DoCmd.FindRecord intCourseRunId Form\_frmCourseMainForm.txtTitle.SetFocus Form\_frmCourseMainForm.txtCourseID.Visible = False End Sub &amp;#x200B; Apologies if I misunderstood, this is all new to me.
Do you know what the actual data is that your code is attempting to insert into the database?
Yes, it's a form which enters delegates against courses that are being run.
That's not the question I asked. The error you're getting says you're attempting to put invalid data into a `datetime`field in the database. **What is that data?**
Access may be trying to update it regardless because Access is gonna be Access.
Ok do you know if this can be stopped?
Remindme! 10 hours
Thank you. I'm going to do this today with the Mac. I actually got the PC working with Access and was using it last night. I think using access and this will help me to learn multiple programs at once since I have no experience in Access.
I was able to get access working on my PC last night. I guess Access for Mac does not come on the same office 365 subscription. Thank you.
If they're looking for specific documentation regarding the business logic behind a particularly reoccurring report - then I would suggest just grabbing a template for a User Story and using that - it's the closest thing you're going to get. Perhaps slapping the Explain Plan and the code in an appendix at the bottom of the document.
dear OP, you forgot to mention which platform you're on this problem is a lot easier if you could use GROUP_CONCAT or LISTAGG or STRING_AGG function
no, this will give a count of 1 for every item
Writing style mainly, I‚Äôm only a junior college and kinda thought maybe a word document outlining the process is a tad simple
Thank you I‚Äôll check it out!
Appreciate your response, but this does not count the different part groupings.
You're welcome. If you run into any issues along the way or have any questions feel free to message me and I'll help if I can. 
Yeah I realized it wasn‚Äôt what you were looking for after i posted. Look into the STUFF() function. Should get what you want. 
I personally do not like sub queries as they often break sargability and have to be run in its entirety before the join clause is applied filtering. I've seen sub queries cause it to completely ignore indexes and table scan millions of rows for a select top 10 on a view. The tables did have correct indexes so it was not that. You should split it up a bit, a CTE would be useful or a temp table. You should also be able to use the LAG or LEAD functions to get the 1st and 2nd phone number in one hit rather than two. Also don't use select *, if you need all columns currently then type them out so that if additional columns are added in the future its not having to ignore pointless data.
It seems that Azure Data Studio is the new name of SQL Operations Studio as written [here](https://azure.microsoft.com/en-us/updates/azure-data-studio-is-now-available/).
Kind of a personal question, but how much are you making now and how many years were you an analyst?
I read it as he wants to filter out rows that are in the future. But, going back and reading it, its ambiguous. 
Thanks for the advice. This is one-time data pull, so performance isn't really an issue. Would you have any pointers on how to query 2nd max date from the phone_list table grouped by ID? If I could start at least with this I can go from there. I'm having a few issues using top with aggregates. I'm on sql2008, so no LAG or LEAD for me. Thanks again.
I tried this and it's still not filtering on only old orders. Any other suggestions? &amp;#x200B; Select \* FROM CUSTOMER\_ORDERS WHERE cast(due\_date as datetime) &lt;= SYSDATETIMEOFFSET()
&gt; Would you have any pointers on how to query 2nd max date from the phone_list table grouped by ID? Join back onto same table excluding 1st date where there is no greater date except the 1st date. Also do it in a cte or temp table, it will help keep the code easier to follow. 
Easy peasy my man, DENSE\_RANK() is your friend here. Here's an example. DECLARE @Customers TABLE (FirstName NVARCHAR(24) , LastName NVARCHAR(24) , ID INT IDENTITY(1,1)) INSERT @Customers VALUES ('Mr','Darcy') , ('RX','Sarsaparilla') DECLARE @PhoneList TABLE (ID INT , [Type] NVARCHAR(12) , Number NVARCHAR(7) , [Date] DATE) INSERT @PhoneList VALUES (1,'Mobile','1111111','2018-01-01') , (1,'Home','2222222','2018-02-01') , (1,'Office','3333333','2018-03-01') INSERT @PhoneList VALUES (2,'Mobile','4444444','2018-01-01') , (2,'Home','5555555','2018-02-01') , (2,'Office','6666666','2018-03-01') ;WITH Results AS( SELECT a.ID , a.FirstName , a.LastName , b.[Type] , b.Number , b.[Date] , DENSE_RANK() OVER(ORDER BY b.[Date] DESC) [Rank] FROM @Customers a JOIN @PhoneList b ON a.ID = b.ID) SELECT ID , FirstName , LastName , [Type] , Number , [Date] FROM Results WHERE [Rank] IN(1,2) ORDER BY ID,[Date] Results will look like this... &amp;#x200B; |1|Mr|Darcy|Home|2222222|2018-02-01| |:-|:-|:-|:-|:-|:-| |1|Mr|Darcy|Office|3333333|2018-03-01| |2|RX|Sarsaparilla|Home|5555555|2018-02-01| |2018-02-01|RX|Sarsaparilla|Office|6666666|6666666| &amp;#x200B;
I'll try this. If you have something better il try that as well
Easy peasy, my dude. RANK() is your friend here. USE master; GO DECLARE @Customers TABLE (FirstName NVARCHAR(24) , LastName NVARCHAR(24) , ID INT IDENTITY(1,1)) INSERT @Customers VALUES ('Mr','Darcy') , ('RX','Sarsaparilla') DECLARE @PhoneList TABLE (ID INT , [Type] NVARCHAR(12) , Number NVARCHAR(7) , [Date] DATE) INSERT @PhoneList VALUES (1,'Mobile','1111111','2018-01-01') , (1,'Home','2222222','2018-02-01') , (1,'Office','3333333','2018-03-01') , (2,'Mobile','4444444','2018-04-01') , (2,'Home','5555555','2018-05-01') , (2,'Office','6666666','2018-06-01') ;WITH Results AS( SELECT a.ID , a.FirstName , a.LastName , b.[Type] , b.Number , b.[Date] , RANK() OVER(PARTITION BY b.ID ORDER BY b.[Date] DESC) [Rank] FROM @Customers a JOIN @PhoneList b ON a.ID = b.ID) SELECT ID , FirstName , LastName , [Type] , Number , [Date] FROM Results WHERE [Rank] IN(1,2) ORDER BY ID,[Date] DESC Here are the results... &amp;#x200B; |1|Mr|Darcy|Office|3333333|2018-03-01| |:-|:-|:-|:-|:-|:-| |1|Mr|Darcy|Home|2222222|2018-02-01| |2|RX|Sarsaparilla|Office|6666666|2018-06-01| |2|RX|Sarsaparilla|Home|5555555|2018-05-01| &amp;#x200B;
Thanks!
I still lose the hour of 3rd the prior day
What do you mean by ‚Äúwe already have a lot of select statements‚Äù?
I'm curious: does anyone here actually use cross joins? I've been writing SQL for a few years in various capacities (simple querying, DBA type work, etc.) and have never run into a use case. When are these useful?
The select statement isn't formatting the date per se. It is casting the date to a string with a specific format. So no, there is no automatic show all dates in this format when I happen to cast to strings. 
Creating sample/test data is the one useful thing I've come across (every possible scenario based on current data).
yep, no way around it. You'll need to modify every cast / convert to specify the format.
Consider this. We have a table of members of a council (Table: CNMBRNAM). We also have a table of measures up for consideration (Table: MEASTOVC). A program adds and removes members from CNMBRNAM. A program adds and removes measures to MEATOVC. We now need something that will generate a table that we will fill in on how each member votes on a measure, a CHAR(1) that is blank (or null) if they have yet to vote, Y for 'Yea', N for 'Nay', and A for 'Abstain'. We can built that table quite easily with a cross join. CREATE TABLE STATMBRMEA AS ( SELECT C.MBRNAME, M.MEANUMB, ' ' STATS FROM CNMBRNAM C, MEASTOVC M ) WITH DATA Now a table exists for having all items currently in the docket for the council to consider, with their current position STATS set as blank.
You give almost no examples or data or structure to work with in your initial question, then you ask for something better when someone takes the time to give you an answer? &amp;#x200B; /u/krankie's answer works correctly.
You could say that. It joins all rows in the target table.
I've used it rarely. Assume a situation where you want to create an output where each row is joined onto a list of numbers, for example a number of days or amount of items (a bit abstract, hope you get my point). In an SP, I create a temp table: CREATE TABLE #days ([days] INT) INSERT INTO #days ([days]) VALUES (1), (2), (3), (4), (5) Then you can make a query that does a simple CROSS JOIN #days, and all your rows will be "duplicated" with 1, 2, 3, 4, 5 as the value of [days]. Hard to give a really good example, but that's one I've used.
Not realy inner since it will never filter away data cause it always matches no matter what. More like a left join. 
I think you forgot to PARTITION BY DATEPART(YEAR, dt) in the CTE2. I fiddled with it, rewrote a bit of it, and made a working example: DECLARE @DateStart DATETIME = '2018-01-01'; DECLARE @DateEnd DATETIME = '2020-12-31'; DROP TABLE IF EXISTS #dates CREATE TABLE #dates ([date] DATE) INSERT INTO #dates SELECT TOP (DATEDIFF(DAY, @DateStart, @DateEnd) + 1) Date = DATEADD(DAY, ROW_NUMBER() OVER(ORDER BY a.object_id) - 1, @DateStart) FROM sys.all_objects a CROSS JOIN sys.all_objects b; ;WITH tuesdays AS ( SELECT d.date AS [date], ROW_NUMBER() OVER (PARTITION BY DATEPART(YEAR, d.date), DATEPART(mm, d.date), DATEPART(WEEKDAY, d.date) ORDER BY d.date) AS rn FROM #dates d WHERE DATEPART(WEEKDAY, d.date) = 3 ) SELECT CAST(DATEADD(month, DATEDIFF(month, 0, d.date), 0) as date), tuesdays.date FROM #dates d JOIN tuesdays ON tuesdays.date = d.date AND rn = 1 WHERE tuesdays.rn = 1 GROUP BY CAST(DATEADD(month, DATEDIFF(month, 0, d.date), 0) as date), tuesdays.date The first statement is just to fill a temp table with all dates from start to end, I like doing that. The databases I work with usually have "date" table which is pre-filled with all dates from like 2000 to 2040, it's handy to have.
The last place I worked had many identical DBs. Each DB had a table with the company info in it (but had 1 row). For reporting I had to Cross Join to that table to display the company name/ info on reports. SELECT C.CompanyName, s.Date, s.Sales FROM Sales s CROSS JOIN Company c Where s.salesDate between something and otherDate
Because you keep using less than or equal to (&lt;=) instead of greater than ( &gt; ) OR greater than or equal to ( &gt;=)
It's common to use cross joins in data warehousing development. There are special types of dimensions called junk dimensions that are filled with low-cardinality flag type attributes, that don't have much in common. Cross joins are used to populate these dimension tables with every possible combination of every flag to use in the dimensional model. For example, pretend that your facts for a specific table can either be {Active, Inactive}, {Open, Closed, In Process}, and {Promotional, Regular}. Instead of creating three mini, narrow dimensions and cluttering up the list of tables, we use a cross join in our ETL and create a single dimension that has twelve rows - every combination of the three attributes above.
Thanks! Very helpful. 
Thanks! Very helpful. 
Woo boo! 
For analytical reporting cross joins are very common. Think if you want to count the number if patients per hour of day. What if there are hours where the are no patients, hence no rows of data to fill the gaps. You would have a data set with missing hours of a day. Using a base table that is cross joined with every possible hour of day to your very left that is then joined with patient data. Now even if a patient isn't seen in an hour you have a row that simply says zero.
I don't do a huge amount of it, but I do write a bit of doco for non-technical clients on their general IT stuff. A few tips: * Use bullet points (or numbered lists) where ever possible. It makes stuff easier to read in general, by making it obvious what are separate things. It also makes it more obvious to yourself as the writing if you redundantly repeat things. * don't assume the reader knows what acronyms mean, always include the full words too (in the first instance at least) * try not to include too many details that aren't relevant to the user, because then they'll either lose interest or just not read it at all. or if they do read it all, they'll have trouble remembering the the important bits, because it's much harder for them to tell which are and are not the important bits, so you don't want to dilute what they remember with stuff that doesn't matter. if there are some extra details you want to include, put them in a clear separate section so that it's more obvious what the main facts are vs the optional extra details. &gt; thought maybe a word document You want to ensure that nobody is ever looking at the old out of date version. So use something online like Google Docs or a wiki. I set up wikis in the past, but I prefer Google Docs these days because it's quick and easy to edit (so I'm more likely to keep it up to date, even when I busy with other stuff), and lets you insert a table of contents easily based on the headings (you can also get a URL to paste into emails etc to each heading section). Also if somebody really wants to take an offline copy or print it they can (easy to backup too)... but for the most part they'll always be looking at the latest version online. Optionally you can allow comments if you want people to mark any sections they find confusing or anything like that.
I don't do a huge amount of it, but I do write a bit of doco for non-technical clients on their general IT stuff. A few tips: * Use bullet points (or numbered lists) where ever possible. It makes stuff easier to read in general, by making it obvious what are separate things. It also makes it more obvious to yourself as the writing if you redundantly repeat things. * don't assume the reader knows what acronyms mean, always include the full words too (in the first instance at least) * try not to include too many details that aren't relevant to the user, because then they'll either lose interest or just not read it at all. or if they do read it all, they'll have trouble remembering the the important bits, because it's much harder for them to tell which are and are not the important bits, so you don't want to dilute what they remember with stuff that doesn't matter. if there are some extra details you want to include, put them in a clear separate section so that it's more obvious what the main facts are vs the optional extra details. &gt; thought maybe a word document You want to ensure that nobody is ever looking at the old out of date version. So use something online like Google Docs or a wiki. I set up wikis in the past, but I prefer Google Docs these days because it's quick and easy to edit (so I'm more likely to keep it up to date, even when I busy with other stuff), and lets you insert a table of contents easily based on the headings (you can also get a URL to paste into emails etc to each heading section). Also if somebody really wants to take an offline copy or print it they can (easy to backup too)... but for the most part they'll always be looking at the latest version online. Optionally you can allow comments if you want people to mark any sections they find confusing or anything like that.
&gt; And now you are trying to tell me, that non-sequential key in b-tree is good? No... merely that it is not "bad" or non-functional as you maintained.
I actually used one the other day, we needed to get a set of records from a table and join it up so that every user in the user table was joined with ever dataset in the other and a cross join was exactly what was needed ..it's a rare thing to use but it is used 
My manager had a vague need that he couldnt articulate that turned out to be a cross join Dont know the data or what he was trying to do, but he was happy with the results, so... yeah I dont know either lol
Well, for one thing, in the second query, what are you wanting to not be in the sub query, you have to pick a single column...plus NO IN isn‚Äôt a thing.
Azure Data Studio [http://www.sql-datatools.com/2018/09/azure-data-studio-in-sql-server-2019.html](http://www.sql-datatools.com/2018/09/azure-data-studio-in-sql-server-2019.html) 
&gt; it will never filter away data that's what I meant with the no "conditions"
They're useful to initialize variables without using stored procedures in MySQL. SELECT * FROM table CROSS JOIN (SELECT @var := 0) r 
He said he can do it better without converting. I was asking for his suggestion not knocking him. 
Depends.. some SMB's want to work with large data... sql EE is what theyd need, but the price aint cheap (15k / 2 cores)
i've done something like this WHERE CASE WHEN @casesenstive = 1 THEN COL3 ELSE '%string%' END COLLATE SQL_Latin1_General_CP1_CS_AS LIKE '%string%' AND CASE WHEN @casesenstive = 0 THEN COL3 ELSE '%string%' END COLLATE SQL_Latin1_General_CP1_CI_AS LIKE '%string%' 
It's common to have a numbers table in analytics work, they're usually created with CROSS JOiNS, here's one example https://www.red-gate.com/simple-talk/sql/database-administration/creative-solutions-by-using-a-number-table/
So, I think you can do a number of things, but if you want to stick with sql the string concat functions in some fashion is likely going to be your best bet. Here is a sample of something I may try in your shoes: WITH CTE (order, item) AS (select distinct order, item from Table) select itemList, count(distinct order) from ( select order, STRING_AGG(item, ',') as itemList from cte)a group by itemList Now, something to note here, order matters. Because A,C,B =/= A,B,C. So having a clustered index on order, item is likely going to be required. A note on line 3, that too is important. Since order 2 in your example has multiple rows for one item (this whole thing can be cleaned up if that was a typo). 
You can set the date format for a session by using SET DATEFORMAT &lt;format&gt; https://stackoverflow.com/questions/331002/change-default-date-time-format-on-a-single-database-in-sql-server But I am curious why you are using so many "select getdate()" queries in the first place. Cant you just use a single method in your app to give you that once for all the things? If its really important, you might want to consider just adding a date field with the date you actually need on the table you are reading from.
I have done some things like this, but never in a large system, so take this with a grain of salt. Also, if possible, a sample set of data can be used to greatly increase accuracy in the comments. Please include that next time ;) select CA5.[customer-no], COO.[open-order-total], CASE WHEN CA6.[customer-no] is not null THEN CA6.[activity-datetime] ELSE CA5.[activity-datetime] END as [activity-datetime], CASE WHEN CA6.[customer-no] is not null THEN CA6.[activity-type] ELSE CA5.[activity-type] END as [activity-type] from ( --gets all of the latest statuses per customer SELECT CA2.[customer-no], CA2.[activity-datetime], CA2.[activity-type] FROM ( Select max(CA1.[activity-datetime]) AS [activity-datetime], CA1.[customer-no] from [customer-activity] CA1 group by CA1.[customer-no]) b join [customer-activity] CA2 on CA2.[customer-no] = b.[customer-no] and b.[activity-datetime] = CA2.[activity-datetime])a)CA5 left join ( --gets the customer-no, max activity-date, and activity type of the ready/collects SELECT CA4.[customer-no], CA4.[activity-datetime], CA4.[activity-type] FROM ( Select max(CA3.[activity-datetime]) AS [activity-datetime], CA3.[customer-no] from [customer-activity] CA3 where [activity-type] in ('ready', 'collect') group by CA3.[customer-no]) b join [customer-activity] CA4 on CA4.[customer-no] = b.[customer-no] and b.[activity-datetime] = CA4.[activity-datetime]) c )CA6 on CA6.[customer-no] = CA5.[customer-no] left join [customer-open-order] COO on COO.[customer-no] = CA5.[customer-no] Again this is the best I could do given the requirements without a dataset to see. A quick explanation: CA5 and CA6 queries are used to get the latest activities PER CUSTOMER. But the CA6 specifically looks for those 'ready' and 'collect' activities. The existence of one of those activities is the point of the case statements at the top, where i look for the existence of a CA6 record that is not null to use. Otherwise I use the CA5 record which includes all customers.
Might be more efficient to cross apply select a.id, b.id, (a.value, b.value)/2 as average from table a cross apply table b where a.pair_id = b.pair_id and (a.id &lt; b.id or a.id &gt; b.id) I try to avoid inequality statements as well. But an inequality would work just as well in my example. 
I switched to sql_latin1_general_cp1253_ci_ai and have had better luck. Now I'm running into other random odd characters in their medical names. It's not helping that half their users enter with Portuguese accents, half without, then another subset of users used ? in place of the accented a 
First off, moving to 2014? Why? 2017 has been out for a year and 2019 was just announced. 2016 should be the minimum migration target right now. Second, instance stacking sucks. Avoid it if at all possible. Third, you've got (tested &amp; verified working) backups of everything, right? Don't mess around with manually trying to migrate things to your new instance, *especially* the system databases. It's great that you've done a lot of reading, but you're going to make it far more complicated than necessary. The good news for you is that there's a PowerShell module that will take care of moving *everything*. https://dbatools.io/ and the function you're going to key in on is [`Start-DbaMigration`](https://docs.dbatools.io/#Start-DbaMigration). It will migrate _everything_ to your new instance - your databases, security, linked servers, databases, Agent jobs, configuration settings, DBMail, **EVERYTHING**. [Just watch](https://www.youtube.com/watch?v=C2G_0gxz5jc) But once you've completed the migration, you're not done. You will need to: * [Rebuild all indexes on the new instance](https://www.sqlskills.com/blogs/erin/do-you-need-to-update-statistics-after-an-upgrade/) * Update your databases to the latest [`CompatibilityLevel`](https://www.spiria.com/en/blog/web-applications/understanding-sql-server-compatibility-levels) after testing that setting in a non-production environment. `dbatools` has you covered there with [`Invoke-DbaDbUpgrade`](https://docs.dbatools.io/#Invoke-DbaDbUpgrade). This **will** affect how your queries run, usually for the better but there are cases where you may take a hit. Hence the testing.
&gt; It's not helping that half their users enter with Portuguese accents, half without, then another subset of users used ? in place of the accented a Hm, this is a long shot, but that may not necessarily be the users. ? is the default substitution character in SQL Server when there's no other form for a character that is unsupported by the character set. Other unsupported characters can have diacritics removed. It's most commonly seen when converting an NVARCHAR to a VARCHAR or with unicode characters in a VARCHAR literal. Other characters will have diacritical marks stripped. For example: select 'ƒÑ', N'ƒÑ', 'ƒ≥', N'ƒ≥', 'Êº¢Â≠ó', N'Êº¢Â≠ó' With my English US culture, I get: A ƒÑ ? ƒ≥ ?? Êº¢Â≠ó The [standard Portugese diacritics](https://en.wikipedia.org/wiki/Portuguese_orthography#Diacritics) all *should* work fine with the above, but other characters clearly don't. I almost wonder if the application is correctly storing the data or if it's using code page 437 or something else suitably ancient. 
I do as for longer columns, put the comma on the start of the next line and don't use anything for tables: select long_column_name as lcn ,col2 from long_table_name ltn inner join tab3 on ltn.col2 = tab3.col2 
I prefer `as` 1. It's the ANSI-92 SQL Standard 2. It helps make a visual distinction from equality tests elsewhere in the query
Usually as... but I can see where if your select has some aggregation in it you might want to use =. Are you doing aggregating, converting, cases, etc in the selection?
I'm not 100% sure, but I think the source data is from Oracle going into a T-sql environment. I didn't know about the question mark substitution. That's very helpful. 
I claw my eyes out everytime I see =, and now I'm blind and itshard to tpye without mainkg msitaeks! 
That depends on what your ultimate goal is. What is the query doing? What is executing the query? This answer will change pretty radically if its a webserver that is being initiated by customers or if you're doing data munging for an anlytic file, for example.
Yeah, I could easily see that causing an issue. I would check to see [what code pages the Oracle instance is using](https://community.microstrategy.com/s/article/KB16101-How-to-determine-the-character-set-or-code-page-of-an) and compare them to SQL Server. 
Me too!
I'm mostly in Oracle, so AS because the equal syntax doesn't exists. I wish it did... I hate how cluttered it can get with long expressions: SELECT product_name, list_price, CASE category_id WHEN 1 THEN ROUND(list_price * 0.05,2) WHEN 2 THEN ROUND(List_price * 0.1,2) ELSE ROUND(list_price * 0.08,2) END as discount, CASE WHEN list_price &gt; 0 AND list_price &lt; 600 THEN 'Mass' WHEN list_price &gt;= 600 AND list_price &lt; 1000 THEN 'Economy' WHEN list_price &gt;= 1000 AND list_price &lt; 2000 THEN 'Luxury' ELSE 'Grand Luxury' END as product_group, ROUND( (list_price - standard_cost) * 100 / list_price , 2 ) as gross_margin FROM products ORDER BY product_name; Finding the `discount` field at a glance can be difficult (I went lazy in this example and grabbed a few fields from a tutorial, but it's not uncommon for me to have 200+ lines of sql, at which point you pretty much have to ctl-f). This just seems _much_ cleaner to me: SELECT product_name , list_price , discount = CASE category_id WHEN 1 THEN ROUND(list_price * 0.05,2) WHEN 2 THEN ROUND(List_price * 0.1,2) ELSE ROUND(list_price * 0.08,2) END , product_group = CASE WHEN list_price &gt; 0 AND list_price &lt; 600 THEN 'Mass' WHEN list_price &gt;= 600 AND list_price &lt; 1000 THEN 'Economy' WHEN list_price &gt;= 1000 AND list_price &lt; 2000 THEN 'Luxury' ELSE 'Grand Luxury' END , gross_margin = ROUND( (list_price - standard_cost) * 100 / list_price , 2 ) FROM products ORDER BY product_name;
coming from sql developer, i don't use either. 
&gt;Union doesn't attach the data from two tables to a single row, like you might think no one thinks this and union is not a join.
I use AS or simply a space (in T-SQL). SELECT * FROM TABLE T WHERE T.ID = 5
Me too, AS is optional, and I omit it.
I use as or a space. Usually a space for table designators and as for fields.
Oooh love playing this game, so here's how I would format it. #Rate\_my\_query. SELECT p.product_name, p.list_price, CASE p.category_id WHEN 1 THEN ROUND( p.list_price * 0.05 , 2 ) WHEN 2 THEN ROUND( p.list_price * 0.1 , 2 ) ELSE ROUND( p.list_price * 0.08 , 2 ) END -- AS discount, CASE WHEN p.list_price &gt; 0 AND p.list_price &lt; 600 THEN 'Mass' WHEN p.list_price &gt;= 600 AND p.list_price &lt; 1000 THEN 'Economy' WHEN p.list_price &gt;= 1000 AND p.list_price &lt; 2000 THEN 'Luxury' ELSE 'Grand Luxury' END -- AS product_group, ROUND ( (p.list_price - p.standard_cost) * 100 / p.list_price , 2 ) -- AS gross_margin FROM products AS p ORDER BY p.product_name I've got fingers crossed that someone likes it.
For SQL Server, I used to prefer strings (table.column as 'ColumnName'), but I read it's discouraged for some reason. I like how the red text pops out. Lately I'm using: table.column AS [columnName] This also goes in thread with how 99% of the code in the database I'm working with is written, and consistency is king. If I get a job where it's all string aliases, or = rather than AS, continue doing that. There's nothing worse than "smart" new developers who start doing things a different way, without changing all the old code at least to streamline it.
Are you trying to SELECT and actually display 1 million+ rows in the GUI? Why? Why not filter it before you dig through the data manually? If you mean you're writing queries that work on tables with 1m+ rows, and you get that except, it reminds me of how I used to get that annoying error sporadically in SSMS as well, until a few months ago. I can't remember precisely what I did, but I think I upgraded to the latest version of SSMS (17.6), and also started working primarly against 2017 databases, and I haven't seen the error in months.
Working with 1m+ rows and reading out 1m+ rows to the query results window in ssms are not the same the thing. 
What alternate ways would you suggest for querying large databases using SQL? I‚Äôm working off a very large data lake atleast for the time being. I‚Äôve never used SQL operations studio, so I‚Äôll look into that but not sure I can get the right enterprise license for it. In this instance I can run it to a file and import to PowerBI which handles it fine for dashboarding. But for general day to day reporting and analysis I use SSMS and run into this error constantly when working on particular types of analysis. Is there a better way I could be doing this? 
Its completely avoidable. No one can look at a million rows, it's useless to displaying it. 
Why don't you limit your queries to top 100 or top 1000?
Always try to use the ansi standard ..as. It's does everything the top comment said and... Knowing the standard will help you alot if out applying for jobs. Generally if you at least know why you prefer one over the other it gives you a leg up. So keep on asking questions its great to learn üëç‚ò∫Ô∏è 
Hey, ramborocks, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Pretty sure ops studio is free:) it's using alot of c# extensions for new features and is constantly growing. It starts as a barebones ssms. Alot more info can be found on their git page. Sorry I'm on my phone and waiting for dinner or I'd try and help more. Keep on keeping on üòé
Could you pivot so the count of items are for each order? I think pivot goes back to before 2008 r2. If pivot doesn't work maybe a sum case count when then 1 else 0 type of statement will work. Group by orderid. I'm at a bar and on my phone so I can't remember give great details. 
I actuality use a whole print off of my code and it's comments. Usually I'm only showing my code to directors or top management and my way has been totally exceptable. If they want you to follow a format they should establish a baseline of expectations. Anyways, starting out this way has made my comments useful for jr developer's reviewing my code. It's helped me make Useful comments too!! End of the day... They should tell you what they want. If they don't... Print the code off with useful comments and call it a day lol
Definitely practice abtract reasoning tests and make sure you can write basic search ans join queries, I'm sure that's all you'll need.
The STUFF() function deletes a part of a string and then inserts another part into the string, starting at a specified position. All the stuff is doing is adding commas better the column names. Just do print @cols to see this.
The first part of this uses the for XML method to concatenate a list of categories. The second part uses dynamic SQL to concatenate a pivot function; the list of categories is used in creating the correct pivot command. The dynamically generated SQL command is then executed to return your results. 
This. In SQLServer you can do: select top 1000 from table Which will finish the query, but only display the first 1k or so results. This will be a lot faster if there aren't any sorts on it. If you're trying to munge around in the data trying to find something, you can normalize the things that you're looking for by creating tables that are subsets of the data. Or utilize temporary tables so that you aren't putting everything in memory. Like: SELECT col1, col2, col3 into tmp.table1 from lake1.table2 where col2 like '%XYZ%'; select col1, count(*) from tmp.table1 group by col1; is going to be way faster than putting the whole thing in memory. Note that this isn't a temporary table. I tend to make permanent tables myself because I'll use them for a week or so and need them to persist just perhaps not permanently. 
Yep. I usually see the STUFF(...FOR XML PATH('')...) syntax in places where someone is using MS Reporting Services to build things that aren't reports (form letters, for instance). Using it for the columns in the PIVOT is a neat trick I hadn't thought of. The downside to that, though, is that you're asking the maintainer to do the heavy lifting later on. I'd be temped to comment that unless it was used all over in the code base, just so when I had to fix my own code six months later I didn't have the "wait, whaaaat?" moment.
Thanks for your help. Yeah I have been making heavy use of it over the years and in all my experience I have never had an issue with and perhaps that is why I never took the time to actually understand it. I never had to debug it. It wasn't until today that I even ever wondered "how does this actually work."
I've seen someone use it for dates in a temp table as a way to not use variables so they could run sections of code easier.
Same here, and I align all of my aliases to make the SQL more readable
And if you do that, anyone who comes along later can easily add the AS quickly if they want it. 
If you have the ability to create a table, that would be a much better way to go. A permanent or temporary table will work, but permanent is preferred (so that you don't have recreate &amp; refill the table every time, and others can see/use your table). The table would look something like this: Min_Age | Max_Age | Age_Bucket ---|---|---- 0 | 19 | '&lt; 20' 20 | 29 | '20-29' 30 | 39 | '30-39' 40 | 49 | '40-49' 50 | 59 | '50-59' 60 | 255 | '60+' If the buckets could potentially shift over time, and you need to maintain historical accuracy, you can even add two more columns for the start and end date. Here's how to create and fill the table (I don't know what flavor of SQL you're using, so I'll write it in SQL Server. There may be some small syntax differences): create table dbo.Age_Buckets ( Min_Age tinyint not null , Max_Age tinyint not null , Age_Bucket varchar(10) not null , primary key (Min_Age, Max_Age) ); insert into dbo.Age_Buckets values (0, 19, '&lt;20') , (20, 29, '20-29') , (30, 39, '30-39') , (40, 49, '40-49') , (50, 59, '50-59') , (60, 255, '60+') ; And here's how you'd use the table in your query: select ad.ID -- Whatever you need from this table , ad.Age , ab.Age_Bucket from Age_Data as ad inner join Age_Buckets as ab on ad.Age between ab.Min_Age and ab.Max_Age ;
Sometimes you need to know how many rows a query is returning as you are tweaking it. Yes I could count(*) but that‚Äôs just not efficient when you‚Äôre creating something and running it constantly to see how it‚Äôs working. All it takes is one result to go too high and the whole SSMS will crash due to this error. Really didn‚Äôt come here looking to debate this, just wanted to know about this error and specific ways of alleviating it. 
Probably can do it with a combination of a: case when statement, floor and ceiling after multiplying times .1 to move the decimal, after you get the result multiply times 10 to move decimal back, and subtract one from the ceiling result. This will be all in your case when statement and you can use the floor result and ceiling result as your range in your between or &gt;=, &lt;= condition. Your 'then' portion can also use the same floor and ceiling logic to create a vchar string that displays the range with a hyphen in between.
I've never seen a join done like that before, that's super useful. I did something similar to what you suggested, but instead of using min_age and max_age, I created a table with Age and Age_Bracket, where I have values 1-200 in age and the corresponding Age_Bracket with it. I would then just join on age. Your solution is much better, and I will definitely s
If i'm running through something quickly I won't use either, I'll just add the column alias in plain text after the column selection. This is probably bad practice, but it gets the job done. If i'm preparing code that will generate a query I'll then throw over to publisher, then I'll throw double quotes around the alias and use friendly names. Come to think of it, I never use 'as' for aliasing. Is this bad? 
When my company does SQL assessments for entry-level positions, the assessment typically consists of needing to do some basic joins, subqueries, and aggregations. Most often, I see people stumble with the syntax, not selecting the right columns, joining on the wrong keys, forgetting to add GROUP BY... Things they probably have learned, but haven't had enough practical experience with different datasets, so when they're asked to do this in an interview with a brand new set of data, people frequently forget.
Yeah, you can join on any Boolean (true/false) expression. I mostly end up using it for date ranges, but it's handy for stuff like this too.
If you're on a newer SQL Server version, you can use the R function "cut" cut(x, c(0, seq(20, 70, by = 10)), right = F)
I know. As I said i was on mobile so it was inconvenient for me to format the entire query 
This is a better solution than a function, because calling a function in a query can lead to severely bad performance - some links: https://www.mssqltips.com/sqlservertip/4689/understand-the-performance-behavior-of-sql-server-scalar-user-defined-functions/ https://www.mssqltips.com/sqlservertip/2727/removing-function-calls-for-better-performance-in-sql-server/ For people coming from OO/procedural programming, it's important to remember that is it not a good idea to try to build up a "library" of re-usable functions and call them throughout your database. It just doesn't result in good performance, broadly speaking. If you need data re-used like in this example, look into creating tables with fixed master data, and if the data is dynamic (can change from user/system input), consider setting up a scheduled job that reads data from a master table/database and denormalizes it into another speed-optimized table, which you can then use to join on in your queries where performance is important.
Sorry, my point was that you only need to check one condition, instead of having an `and` in the expression.
People used to have jobs putting out gas lanterns and scrapping shit off the road. Isn't it nicer having electricity and plumbing? Don't react, adapt. Everything will get sorted out if you make a plan to adjust to new technology. Technology isn't the problem, poorly regulated economic systems and entitlement are what cause new technology transitions to be so hard for a lot of people. 
Is that volunteering, also how do you find companies that do this?
No you'll get paid. Look up Robert Half Tech. They're a huge national firm. 
Which country is this?
How did you get into Databases and SQL I'm always interested how others did it
I think that data professionals, particularly those of us who make sense of data, are going to be the most immediately impacted by ML/AI as that's a space that very naturally lends itself to those kind of techniques. I'm not worried about my job security though. I'd be the change in the next ten years is going to be bringing ML techniques to bear on existing problems, which means more specialisation in our skill sets. It's going to mean that the value of data increases by order of magnitudes, so the demand for us is going to increase even as the knowledge barrier for entering the field rises. And I don't think the AI that can answer any and all questions by automagically understanding natural language questions and then contextualising an organisation's entire set of data assets to deliver reports is something that I'm going to see before retirement, or at all. I might be being optimistic, but I think we're very fortunate to be where we are right at the time when all of this stuff is blowing up. I'm sure that in ten years I won't be spending nearly as much time building individual reports, but I'm equally sure the things I know and am learning are going to be very valuable for many years to come.
&gt;Does anyone have any fears that AI could wreak havoc on business intelligence jobs? No. AI can do a lot of cool things. But only within fairly narrow boundaries. It's *really good* at playing a specific game. Or it's *really good* at identifying pictures of cars. Or it's *kinda good* at driving a car. But once you get into a fuzzy area... like figuring out how a particular data set is useful to a business, AI isn't all that great. &gt;end users having tools to just ask question in natural language, no matter how complex and crazy and AI just spitting out a report I will be quite surprised if that happens in the next 50 years. AI is capable of some cool stuff. But the further you get from an environment with clear boundaries and strict rules, the worse it gets. Keep up with your skill set and you'll be OK.
Is backup with compression enabled?
My role is split between BI and data science. I think both fields have a bright future for several reasons. There's been decent self serve analytics and visualization for non technical decision makers for more than a decade. It isn't very popular. The problem isn't ease of use; it's trust. Decision makers are comfortable challenging and vetting analytics until they are satisfied all the details have been considered. If they're creating analytics by themselves, or with the help of AI, there's nothing to challenge or vet. AI, same as any computer, can only do what you tell it to do. Whether AI is consuming data or a developer is writing SQL, someone needs to fully understand if the results are correct and why.
If developers are building reports ad-hoc for users *today* then they're already obselete. Self serve cubes have been the norm for a long time.
As far as I can tell, the whole AI fad is... well a fad. There is no algorithm that can make algorithms. And if a human has a hard enough time making a BI solution, a "thinking" computer won't be any better. What we have today is a large selection of AI driven frameworks and libraries (all of which are being pushed due to invested money). I won't say "don't worry about it", but what you should worry about is an industry push towards a more generalized "programmer" type role. I have no idea why you are posting this to /sql (not saying that is a bad thing, I just don't know your motive or skill set), but if the only language you know is sql, then you need to branch out. At the last large corporate type job I had, the DBAs and BI folks knew nothing but SQL and were pretty horrible at anything else. Even worse, they really didn't "know" SQL. They seemed to all have their own USB thumb drives full of random SQL queries that they had collected over the last 10+ years of their career. For every large task they were given, they'd spend a couple of days digging through their old query files until they found something similar, and then spent another week beating on it until it did. Don't be that guy.... If you want to be a guru (or even useful), you need to actually learn how to do something.
haha, I was thinking the same thing. Most of the customers I'm familiar with don't even know what they want in the report and the only way to get them to agree on something is to threaten to charge them $200 for every hour they make me sit in a meeting.
I would consider my sql skills to be intermediate as I can write complex queries to transform the data any way the user wants. I wouldn't call myself advanced as i'm not very good with performance tuning or understanding execution plans etc.... My team uses a combination of cognos, tableau, SSRS, tabular models and SSIS for ETL related tasks. I do have a small background in c# and python but hardly get a chance to use it. Yes, I am very familar with people who work in BI that just copy and paste code and have absolutely no clue what is going on like you describe. When you tweak the requirements just a bit, they're lost since they just copy and paste. They become exposed rather quickly. 
Hey, j1888jji, just a quick heads-up: **familar** is actually spelled **familiar**. You can remember it by **ends with -iar**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
&gt; I would say at least 50% of the time, the requirements we're given are off by a little or sometimes a lot because the end user doesn't know what they want and they ask the wrong questions so we spend hours looking for stuff they don't need or want. It usually takes 3 or 4 times before we actually get them what they want which involves many meetings &amp;#x200B;
Something like: select datepart(mm,dates.MaxDate), count(1) from ( select user_id, max(date) as MaxDate from MyTable group by user_id ) dates group by datepart(mm,MaxDate) order by datepart(mm,MaxDate) Code is untested and you'll likely have to adapt it for vertica anyway but it should be a good starting point. 
I believe you are talking in the context of primary keys. Composite means more than one field makes the row unique. So a composite key has more than one field. A Unique key can have 1 or more fields. So, all composite keys are unique keys but not all unique keys are composite keys.
Composite key takes multiple fields to make a unique key. For example, if an employee fills out one and only one timecard each day, then employee + work date could make for a unique key since you expect multiple from the same employee and multiple from the same work date, but not multiple of the same combination of both. I'm assuming unique key in your context just means it only takes one field to make the key.
&gt; all composite keys are unique keys No, composite key does not have to be unique.
Unique key is the unique identifier(can be one field or can be concatenation of more than one fields) that you can link two or more tables together. Composite key is you concatenate several fields together to form a unique key. 
If your data warehouse is well developed and has good documentation, it can work. You won't ever be completely free from answering questions and helping users, but you can definitely free a lot of the workload. This is especially true if your power users are technically competent and can pass on the tribal knowledge other analysts.
Worth re-checking. On some data backup compression can save a *lot* (ours is 1.2tb on disk as a db, 200gb backed up)
WhereScape Red, but maybe pricey. 
What you are looking for is called change data capture, there are different techniques and to for that. It depends on your source systems and data
Type 2 SCD
Tool looks very promising. They don't have their prices online (I'll research about the product more before deciding). But do you have any vague number regarding $$ ?
The best advice with the least attention paid to it. OP, Google those words. It's the SQL data model for exactly what you're describing
That‚Äôs a pretty straight forward table. Are you sure you can‚Äôt cobble something crude and insecure in python that‚Äôll ETL it for you?
Why use flag columns at all? Why not just have "AreaID" as a single column? So all of their health goals are AreaID = 1, all work goals are AreaID = 2, etc.
Its priced per developer seat, but pricing is super dependent on situation and functionality, thats why they dont post it. They have a reputation for not letting pricing ever be a barrier to purchasing if there is a good technical use case. Youre probably looking at low to mid 5 figs regardless, but the ROI is ridiculous.
Personally I would just put in the hours and develop a proper audit (sure its a pain in the ass to write, but at least that way you know the results are reliable and not based on a potentially buggy third party tool) remove insert/update permissions from the table and make sure inserts/updates are possible only via stored procedures which ensure create audit records are created Alternatively you could add triggers to the table which will compare the before and after data and create an audit record if its changed. I prefer to avoid triggers if I can though
Could a Goal have multiple Areas? If so, you have a many-to-many relationship and will need a join table ("GoalAreas"): Areas -------- AreaID (PK) AreaName Goals ----------- GoalID (PK) GoalName CreatedTimestamp CompletedTimestamp GoalAreas ---------------- GoalID (PK, FK) AreaID (PK, FK) 
Thanks PilsnerDk for the help and suggestion. Tried both and it worked out very well. 
Does this help? http://www.vertabelo.com/blog/technical-articles/creating-pivot-tables-in-postgresql-using-the-crosstab-function
you had me at "A goat herder tending his modest collection of livestock patiently guides them through an alpine meadow." this is a classic description of the sql developer trying to get his queries through production quality review
Uhuh, as a long time user of HeidiSQL, one thing you're missing for sure is comparison of features across different engines. Heidi was made with MySQL in mind and it definitely does lack in features for both SQL Server and Postgres. Last time I checked editing pg functions was a nightmare for example. So while things work out mostly fine for MySQL, and it's actually my editor of choice when dealing with MySQL/Maria, don't expect the same kind of experience with other engines. That said, I think both drivers are marked as experimental. 
`len(CompanyName) AS whatever`
Good man :)
Yep, my company bought RED, totally worth it.
I don't believe it's possible to change a property of the document from the command line when you open the file. Maybe if you explain your situation someone here could help find a solution. 
Min(struct(timestamp,col2)).col2 
I tried a few alternatives and stuck with dbeaver for postgresql. [https://dbeaver.io](https://dbeaver.io) 
You can also skip the 'AS' part and it will still work.
Can't help you, but my spidey senses tell me you're working with something awful of a setup. Godspeed.
If it's that "hardcoded" and you don't need a generic solution, you could SELECT from the two tables and do a UNION (ALL). SQL Server example: DROP TABLE IF EXISTS #transpose create table #transpose (id int, purch_d7 int, purch_d30 int) insert into #transpose (id, purch_d7, purch_d30) VALUES (1, 10, 25); insert into #transpose (id, purch_d7, purch_d30) VALUES (2, 5, 15); SELECT t7.id, t7.purch_d7 AS [purchase_count], 'purch_d7' AS [purch_window] FROM #transpose t7 UNION ALL SELECT t30.id, t30.purch_d30 AS [purchase_count], 'purch_d30' AS [purch_window] FROM #transpose t30 ORDER BY 1, 2
Ditto
Mybuttholecolumn = alias.field
First query unique identifier and min(datefield) and store in a temp table. Then join your original table on unique identifier and date.
Yeah depending on what SQL your using there should be functions you can create so instead of a case statement it just calls the function
You could even have a trigger in the DB itself... upon insert to the main table, insert the data and metadata about the change into your audit/change table.
While I second the impression that you probably have a really unpleasant setup.... this could be reasonably managed as a hack if your spreadsheet is xlsx - just unzip it and rewrite it and zip it back up. Xlsx is actually pretty easy to manipulate from other languages. I‚Äôm not condoning this behavior but technically it would be doable.
Coalesce (field1, field 2....) 
Are you asking someone to complete a test for you? 
Reported. 
No idea if the best, but I use DBeaver on OSX. Only thing I miss is native Excel exports, but tab separated works as well.
May be a challenge but look into the excel interop. 
In case this error still persist, then you [repair access database](http://www.databasefilerecovery.com/ms-access-database-recovery.html) with a reliable option. But first know the causes behind it. Know more: [http://www.databasefilerecovery.com/blog/access-database-corruption.html](http://www.databasefilerecovery.com/blog/access-database-corruption.html)
dbeaver is good in essence. its missing a lot of PLSQL things and real compilation with it is a bitch. and also some more advanced stuff other IDEs can do.
[removed]
If "nothing" represents a null then you can use coalesce which will return the first none null field. If it's actually text then you'll need use a case statement CASE WHEN column1 &lt;&gt; 'Nothing' THEN colum1 WHEN column2 &lt;&gt; 'Nothing' THEN colum2 WHEN column3 &lt;&gt; 'Nothing' THEN colum3 ... ELSE 'Nothing' END as NewColumn You could have done something like this in the first place, maybe you didn't know you can supply as many WHEN THEN's for a CASE statement as you want? It'll return the THEN of the first WHEN it finds to be true and not bother to even evaluate the rest making each new WHEN THEN an implicit ELSE. 
Thank you for answering &amp; I apologize about the delay. Had some things come up Friday and was out of town all weekend. To answer your question, the reason we are moving to 2014 is because that is the volume license level that we purchased. We were exercising downgrade rights to 2008r2. 2014 is the latest version we can run before needing to make a purchase. Management wants to avoid spending on this for now. I'd love to avoid instance stacking, and in almost all cases... this is true. Only our development environment has more than a single instance installed at this time. I should have known Powershell had a thing for it. =| Looks like my job today is reading over https://dbatools.io Thank you for taking the time to respond. I appreciate it &amp; gives me a direction to move towards.
Ha! Glad you liked that bit.
Fantastic feedback! We'll update the article with some info on that.
Another reason why SQL Server is one of my favorite Microsoft products.
Try throwing in a `;`? Just the semicolon. This is a statement terminator. Sqlite should spit back an error, and get you back to the main prompt.
SquirrelSQL or DBeaver should both work with SQLite as a graphical front-end. 
It was on a 3 year. They chose not to renew software assurance when it came up. Due to that, if they wanted to renew the license, it would be purchasing 2017 + SA. They saw the price tag and said 'nope'. Not the end of the world, most of this isn't being actively developed (well... barely)... it's more of a migration project to get the base OSs up to date and extend the value/life of the existing license for the next few years. I've looked into dbatools a bit today and believe I am going to have to host the instance on a different server than what I had planned on (target/destination change). I'd rather avoid an issue with renaming instances as a lot of what I've read this morning/afternoon seems to paint an ugly picture for the process and afterwards/cleanup.
3 minutes and 50 seconds.
Why not put this in the sqlserver sub?
Not sure if there is a MSSQL equivilant but Oracle has Live SQL online where you can easily run scripts and learn quickly without the hassle of setting up your own server. You'll want to create an account and save any schema statements like creating tables since it'll wipe it clean after each time you log out. But if you save them and have created 10 or so tables you can simply click the script you saved and it'll create it all again for you when you start back in. It was very helpful for me to learn it and I started using it almost exclusively for my first SQL class because of how easy it is to just pick up and start working. 
Isn't it as simple as just doing this: select * from new_table except select * from old_table You get all the stuff from the new file that is either different, or not part of the original file, no?
Yep -- a goal can have multiple areas. But where is the list of areas that a goal belongs to? Is it in GoalAreas? For instance if goalID 1 belongs to AreaIDs 1, 2, and 3, would there be three rows in GoalAreas? Thanks for your help!
&gt;As a very strong rule of thumb, if you have to change your database schema everytime you introduce new data (in this case adding a new Area), then its a sign your database is poorly designed. &amp;#x200B; Awesome, thanks for this information I'll keep this in mind!
Damn good questions. I guess I'd have to add a UserID column to goals. But then all the users' goals would be in one table. Is that OK? That table could become ridiculous after a while. Well, I mean, let's be honest no one's ever going to use this app! But it's good to do things the right way! I can't imagine that twitter has one massive "tweets" table for instance. Or does each user get their own database maybe? Yeh updating goals is another thing to think about. I actually have no idea right now, but I think a latest flag sounds better since they might have just updated a typo or wording or something. Thanks for the thoughts!
Yes. That's why the primary key of GoalAreas is both GoalID and AreaID (the pairing of the goalID and areaID is unique, but either value on its own might be duplicated). GoalAreas ------------- GoalID AreaID ------------- 1 1 1 3 2 1 3 6 3 2 4 3 Say that AreaID 1 is "Health". Then Goals 1 and 2 are in the Health area. Area 3 is "Finance", then Goals 1 and 4 are in the Finance area.
Yes it does do that, but it doesn't do 2 things; 1- Identify the difference between a changed row and a new row (if that's not important to your business need, then no biggie), and 2- It doesn't identify deleted rows at all, and that's usually needed. Plus like I said, that just gives you a dump of changed records. If the business need is to track changes over time an in SCD, you have to build the queries to archive the current changes records then add in the new ones with the correct start dates. None of that is terribly difficult though. Added/deleted are two simple WHERE ID NOT IN (SELECT ID FROM ...) queries. SCD maintenance is another two queries. If you want to keep certain columns current without driving a history record addition ("Current Age" for instance), you need to run a separate UPDATE query for those matching columns. Again, only if that's a business need.
You would miss rows deleted in the new table with this approach. If deletes are possible a second query to look for those would be necessary.
Touche.
select item, price sum(quantity*price), sum(quantity), from table group by item, price 
It doesn't identify the difference, but it shows you which rows are different, no? You would get any extra rows, and any rows which have changed, correct? &gt;Plus like I said, that just gives you a dump of changed records. If the business need is to track changes over time Yeah, no, I get this part. The `EXCEPT` is just the first piece to looking at differences, figure out what you want to track, etc.
Yep. I misread your original comment and wanted to make sure I understood EXCEPT properly. Thanks.
something like select item ,price ,sum(quantity) as amount ,from t group by item ,price with rollup
Because they can belong to multiple areas.
DBBrowser should be good for sqlite. I haven‚Äôt touched on the CLI for sqlite, but you might have to commit the query if you‚Äôve done changes.
Impossible to say for sure without more information, but I doubt that's your bottleneck. Try running it without that clause and see what happens. Or better yet, paste the query and [the execution plan](https://www.brentozar.com/pastetheplan/).
You could use Sum(quantity) over (partition by...) for different aggregation levels
I love this article in regards to NOT IN vs LEFT JOIN vs NOT EXIST. https://www.sqlshack.com/t-sql-commands-performance-comparison-not-vs-not-exists-vs-left-join-vs-except/
Sorry no clue what an execution plan is. This query just took 4 hours to run and export which seems insane to me. I know very little about SQL or coding at all. I am just good at writing excel formulas so this was just given to me since the prior owner switched roles. I edited the query into the post
Solution verified
I shall check this out tomorrow, thanks. Looks like I have opened a whole new can of worms haha.
Your database is not normalized. You should have transaction and transaction details as a one to many. You could make a cte with distinct transactions then join to the whole table on seq Id.