&gt; ` That worked :). Of all of the combo's i tried i didn't think to drop 'AS'. I ended up moving the mysql db to oracle 11g which makes 'as' work normally again but i tried renaming the output columns in my old connection as you suggested and all is good with the world. Thank you very much :)
the use of "dbo" as a qualifer in a table name should be a huge giveaway that this is Microsoft SQL Server, which has unique and proprietary date functions so those links don't really help
 SELECT additional01.text1 AS ad , additional01.date1 AS plandatum , CONVERT(CHAR(5),additional01.time1,8) AS zeit , additional01.number1 AS reihenfolge , additional01.text2 AS erledigtstand , addresses.company1 AS firma1 , addresses.firstname0 AS vorname , addresses.lastname0 AS name , addresses.street0 AS stra√üe , addresses.zip0 AS plz , addresses.city0 AS ort , addresses.country1 AS bundesland , subtable01.date1 AS besuchsdatum , subtable01.notes1 AS besuchstext , subtable01.text3 AS n√§chster-schritt , subtable01.text4 AS besuchsstand FROM dbo.additional01 INNER JOIN dbo.addresses ON addresses.id = additional01.superid LEFT OUTER JOIN dbo.subtable01 ON subtable01.superid = additional01.id
Ok thanks.... I should have specified equi joins. The rest of my post stands true though for the OP. He effectively has a key across two tables that need cleansing. 
No, you can strip punctuation and whitespace in a subquery. It's totally worth it to do so before your join if you're dealing with a large-ish number of rows. 
Oh that could work. Basically extract the entire column in a subquery, scrub it of the punctuation, then use that to join on? 
 CONVERT(VARCHAR(5),Time1,108)
You could not have possibly provided any less information.
You can use convert to varchar and substring. Once in a varchar you have all your typical string functions like replace/left/substring, in case you need to manipulate further. SUBSTRING(convert(varchar, Time1,108),1,5)
Lol sorry. I guess I just am looking for a link do get sql server on win7? Basically I'm familiar with writing queries, I have managent studio.. I just need to make a database?
he could have. He could have left off the OS
i clicked on your clickbait title and read your article loved your explanation at the end -- "It's not about the star, stupid!" well done
Not off the top of my head, but I like the explanations used in this poster. http://2.bp.blogspot.com/-wuinSTn-X4A/UwHmmceDQqI/AAAAAAAAJFo/5EjPg-LpAJc/s1600/Sivakumar_Vellingiri_Normal_Forms_Poster.Jpeg Note that this poster takes it beyond 3NF, which can be thought of as a comfortable standard to aim for in most cases. I didn't mention it in my original post, but if you can describe what the goal/benefits of data normalization are, as well as examples of time you would want to store normalized data vs denormalized data, that's bonus points in an interview candidate. (I know you're not OP, but still)
Install sql server express if you haven‚Äôt. Then download a sample db like adventureworks. You should be able to find some instructions for loading the db into sql server. It‚Äôs only a few steps
[Here is a good StackOverflow thread on stripping non-alphanumeric characters from a field](https://stackoverflow.com/questions/1007697/how-to-strip-all-non-alphabetic-characters-from-string-in-sql-server). If you can't make a UDF, try this: Stuff(@yourfield, PatIndex('%[^a-z0-9]%', @yourfield), 1, '') Keep in mind that using a function like this in a join is going to prevent you from using indexes and force a full table scan. Better to create a temp table or subquery if you can.
# Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better # Because nobody has created anything better
1. Because nobody has created anything better 2. Because nobody has created anything better 3. Because nobody has created anything better 4. Because nobody has created anything better 5. Because nobody has created anything better 6. Because nobody has created anything better 7. Because nobody has created anything better 8. Because nobody has created anything better 
Aren‚Äôt we in the SQL subreddit? Why are you justifying its use to the group of people that already use it?
Note: I'm not Markus Winand, I just like his articles.
ah... okay, thanks for clearing that up
Thank you 
If that bothers you, I suggest you stay out of the Python subreddit. 
Redis is the most-loved database and rust the most loved language, wtf does that even mean? they probably dont have 1/10 the userbase of other languages, how can they be "most loved"
The article is misleading and the title is bad. Please read/understand the material fully before posting. A quote from the parting section (the key observation being "software stops working"): &gt;Besides the performance issues mentioned above that are not caused by the star (asterisk) itself, the star itself might still cause other trouble. E.g. with software that expects the columns in a specific order when you add or drop a column. However, from my observation I'd say these issues are rather well understood in the field and usually easily identify (software stops working) fixed.
Here's my eight reasons why I use it: begin for idx in 1..8 loop dbms_output.put_line (idx || ') 100k+ salary'); end loop; end; /
Currently learning Jet SQL in Access, I keep asking myself why....
please see the sidebar -- &gt; When requesting help or asking questions please prefix your title with the SQL variant/platform you are using within square brackets like so: &gt; &gt; [MySQL] &gt; [Oracle] &gt; [MS SQL] &gt; [PostgreSQL] &gt; etc
I can see the benefit in returning less stuff. Example: I have a varchar column and I don't want all of it, I want part of it. I could do left(column,4) and the result would be less intense than (column) because of the network wait. Unless of course the cpu was more intensive performing the left. However if you are doing a manipulation on a column and then joining it, it should be the same plan as if you were to do a like operator. I would need to look at the query plan in both instances, but it makes the column no longer search argument usable, so I would anticipate the same performance in both minus the less data transmitting over the net.
thanks for the heads up. Will delete and repost with proper tagging.
yes... use the [to_char function](https://www.postgresql.org/docs/9.1/static/functions-formatting.html) 
If you're using MS SQL Server 2012 or newer, you could try using the [FORMAT function](https://docs.microsoft.com/en-us/sql/t-sql/functions/format-transact-sql): select ... FORMAT(cast(ADDITIONAL01.TIME1 as time), N'hh\:mm') as "Zeit", ... from (("dbo"."ADDITIONAL01" "ADDITIONAL01" ...
If you do the string stripping in a sub, the predicate turns into an equality check between hashable values. Hash joining is going to be a lot faster than nested loops (with a fuzzy comparison at each step, to boot)
Would something like this work: CREATE TABLE #tbl1 ( [user_id] INT, [valid_from] DATE, [valid_to] DATE, [used_for_transaction] INT ) CREATE TABLE #tbl2 ( [user_id] INT, [transactionID] INT, [transactiondate] DATE, [coupon_used] CHAR ) INSERT #tbl1 ( [user_id], [valid_from], [valid_to]) VALUES (1, '2017-01-01', '2017-12-31'), (1, '2017-05-01', '2018-04-01'); INSERT #tbl2 ( [user_id], [transactionID], [transactiondate], [coupon_used]) VALUES (1, 1000, '2017-06-14', 'Y'), (1, 1001, '2018-01-06', 'Y'); UPDATE #tbl1 SET [used_for_transaction] = t2.transactionID FROM #tbl1 t1 JOIN #tbl2 AS t2 ON t1.[user_id] = t2.[user_id] AND NOT EXISTS ( SELECT 1 FROM #tbl1 t WHERE t.used_for_transaction = t2.transactionID); SELECT * FROM #tbl1 DROP TABLE #tbl1; DROP TABLE #tbl2;
Oh man. Great argument. You got me convinced. It doesn't at ALL seem like you might have an emotional reason for feeling this way.....
That's silly. When there's something better for the purposes SQL is used for, I'll use that. I didn't stick with VB6 my whole career.
I would only left join twice, not 4 times. Probably an even more elegant way of doing it...but that would at least take out some of the repetition. pnull.manufacturename = cnull.manufacturename as the last line seems like it would get the same results as what you are doing. 
Won't that exclude results where a materialid isn't in the partaml table at all, though? 
&gt; Because nobody has created anything better They did like 10 years ago. It's called MONGO. And it's webscale. SQL is slow as a dog. Mongo is fast because of sharding, the secret ingredient in the webscale sauce. You just turn it on and it scales right up.
Right, but I want to exclude rows where the materialid is present without a matching manufacturername. I don't think a simple left join will let me see the difference between "absent" and "present without a matching manufacturername", will it? They'll both just show nulls. 
Thank you. Completed that too now. I have 2 hours left. Wow.
Thanks. Yesterday I looked into normalization, but coming from Javascript and dealing a lot with JSON, I understood the benefits of it. I mean, obviously it's better to have the same information with less code and sliced into multiple modules.
Title is self explanatory. Click through to see the code examples.
It has its use cases but it is terrible for other things, particularly in the corporate world. Sharding can be done with many systems, from SQL server to Riak and a bunch of others. Neo4j has something pretty good going with their graph databases, in my opinion. Someone mentioned something to me about relational calculus a while back but I haven't read into it yet.
Do you have some sample data? Its a bit hard to tell without seeing the structure of the data but is there a reason you can't just: Select * from bom b left join partaml p on b.parentmaterialid=p.itemnumber or b.childmaterialid=p.itemnumber where p.itemnumber is null And maybe use DISTINCT to filter duplicates?
LOL. Super simple 5 questions, but the one before the last one I screwed up unfortunately. I needed to get the average price of a product. Immediately I thought "eaaasy, I'll just use AVG()", but the price given in the table was the price *per product*, so I had to solve it at first with pure maths. I was super nervous and amazingly fucked it up in the end. Only solved it with their immense help. However, still no clear answer. But they said, if it was so negative that they know they wouldn't employ me, they would have said it right away in that moment. Same thing for the positive side, and that's the reason I have to wait another week. Anyways, thank you all for your help. Some of you guys will definitely get gold. &lt;3
I'm suggesting keeping your where the same except for the last line, so you'd still be telling it to give you nulls or where it is matching on mfg name. 
If you can query your MySQL with linked server using openquery: select * from openquery(&lt;linked server name&gt;,'&lt;sql query&gt;') For example: select * from openquery(mysql,'select name,surname from person') This might help as well: https://docs.microsoft.com/en-us/sql/t-sql/functions/openquery-transact-sql
As a SQL novice, what I've observed from reading most of the questions on this subreddit is the ability to know how to use the tools to solve or answer certain things. Sure, running 'SELECT * FROM table 1 where x = 'y'; seems pretty easy but once you need to mangle and shape the data to produce what you want, it seems the rabbit hole goes very far indeed.
What is the size of your parameter? It might be that you're using nvarchar(20) and your list is 50+ chars. Something else to look at is your where clause, are you using a split sting function for your parameter or simply saying WHERE field in (@param)?
Thinking in sets is the key.
It's not so much a limit on the number of items in the selection criteria (although a very long `in (a,b,c,d)` is going to make a mess, performance wise) but the size of the parameter itself. The solution (and it's a very messy one) is to use a Table-Valued Parameter. You have to create a Table User Defined Type, populate that table variable from your select list, then pass that table into the stored proc and (for best results) `JOIN` to that table variable or preferably insert those values into a temp *table* and `JOIN` to *that* (because table variables cause crappy estimates and execution plans). IIRC, [this is pretty much *the* source for the solution](http://gruffcode.com/2012/06/21/using-table-valued-parameters-with-sql-server-reporting-services/) and I'm pretty sure it's the one I used when I had to implement this.
Looks like there is your problem, try making your parameter larger.
What happens when you add another 150 clients? Then another 500?
This. The developers I work with can do things in C# or Python that it would take me forever to figure out, but I write a SQL query for them and they look at me like I'm some kind of wizard. Beyond understanding joins, aggregates, string functions, transactions, cursors (and why they should be avoided!), etc. as far as specific things (as a T-SQL dev) I'd suggest knowing how to read the query analyzer, understanding the different kinds of indexes and compression, inline functions, windowed functions, recursive CTEs, PIVOT/UNPIVOT, CLR, how to query XML... yeah, just basically know everything there is to know about SQL ;)
Yeah the field names are not consistent though. I have field names all over the place. AECC Trans, Distribution, Underground, xxxx Fiber and then there are the several variations such as AECC Trans (H2O) and (RR) etc. for water crossing, railroad crossing, etc
I'd add that knowing the right questions to ask and the least resource intensive way to get the answer you want is really important. Understanding the impact indexes can have on a query, or why your query with a WHERE clause that includes a wildcard on the left of a string is slow, or knowing under what circumstances it's ok to use WITH NOLOCK (protip: the answer is never. Do you want dirty reads? Because that's how you get dirty reads.) are some examples of understanding how you'll get your question answered by the DB.
Alex, I'll take things "Things that make DBAs cry themselves to sleep at night" for $600.
Hey, can you explain why using WITH(NOLOCK) gives dirty reads? I t was my understanding that it is pretty much a necessity when querying a live database.
[This](https://softwareengineering.stackexchange.com/a/181657/256008) is pretty much my go to answer.
Nooo, NVARCHAR(4000), not MAX! Also, noooooo in general at not normalizing your data
Can we inner join between linked servers and how it works, btw ty for the response
That query hint is basically telling the DB engine not to wait for a lock on the table before returning the result set. On a live database you want a lock when you read so you can be sure that you're not reading dirty or inconsistent data. For example, there could be an application writing to the same table you're trying to generate a report from. If the application is in the middle of an uncommitted transaction, you run a query WITH NOLOCK, and then that transaction gets rolled back the resultant set you get from your read is dirty -- it contains writes that weren't committed. This of course begs the question of if you could roll the dice for the sake of speed and risk a dirty read intentionally. I've done this (as I think most of us have) but the key is understanding what you're risking and how you can mitigate it. If I'm querying a data warehouse that's populated with new data once a day I can read uncommitted to my heart's content but if I'm trying to read financials from a live transactional database that's hit with fifteen thousand writes a second I'm putting my job on the line by not using a lock.
Cool! Will google all the stuff you mentioned, step by step. Thanks! 
Oh, that's a great answer. Thank you!
You didn't specify which flavor of SQL, but if it's MS SQL Server and it's version 2012 or newer, you could use the [FORMAT](https://docs.microsoft.com/en-us/sql/t-sql/functions/format-transact-sql) function along with a bunch of cleanup, like this: SELECT FORMAT(CAST(REPLACE(REPLACE(REPLACE(REPLACE(phonenumber, '(', ''), ')', ''), '-', ''), ' ', '') AS BIGINT), '###-###-####')
There is no line in the sand and it can get as complex as you make it. SQL is built on easy building blocks - SELECT FROM WHERE GROUP BY HAVING ORDER BY. But with sub queries, aggregates, functions, and a whole lot more, it can get very complex in a hurry. When most people say they are going to "learn sql", usually what they end up doing is banging out a few SELECT FROM WHERE statements which leads to the fallacy that SQL is just super simple. But that's just textbook examples focusing on pulling data via DML and specifically the select statement. [SQL does a whole lot more](https://www.geeksforgeeks.org/sql-ddl-dml-dcl-tcl-commands/) than selecting, and in addition since sql is based on set theory and data sets (union, intersection, minus), sometimes we need an iterative approach with variables that change based on conditions, so there's a whole other approach called [PL/SQL](https://www.geeksforgeeks.org/sql-procedures/) which gives you that power. I work on Oracle's flavor of ERP and all of the front end is coded in a mix of java and sql. Some common business questions can be answered using various levels of easy-to-hard SQL, such as: When was the last time user "JSMITH" logged on? - An easy one-liner select from one table. What was the last purchase order for, and the line summary of the order? - A simple two-table query using an sum aggregate But what if I need more complex info for someone who is doing due diligence on purchasing contracts? What were the highest and lowest three prices we ever paid and the date the order was placed when ordering product xyz from any vendor that we ever bought it from, broken out per vendor, that was approved, received, paid, and not returned or cancelled, less the discount amount when payment terms were met within the vendor's discount window? - An intermediate to advanced query involving about 10 tables, mixed join types, subqueries, ranking, and partitioning functions. But it's the second example where advanced SQL really shines. It's all just text, all just information, but if you tried to answer that question using some other method, it would get hairy very quickly considering there's millions of orders, invoices, and payments in the database. You can't do it with Excel due to size alone. The rabbit-hole goes forever deep depending on what question you need answered with respect to what considerations. But to ask a complex question in the first place means that you need to have a vast database. Textbook examples of a 3-table HR database with 100 records isn't going to enlighten you. If you want to look at what Oracle calls "expert", why not look at the table of contents of one of their official books, such as "SQL Certified Expert Exam Guide 1z0-047"? To be an "expert" means you know how to use and apply the most commonly used DML, DDL, DCL, TCL along with some less-common things, CUBE ROLLUP, anyone? 
Thank you, it helped a lot &lt;3
Ok that makes sense. I work for an executive search firm with a database of around 1mil people records. We have many different users querying and updating the database throughout the day, usually all dealing with a few main tables (people, companies, etc.) Is it worth the risk for us to use NOLOCK since we have so many people relying on timely queries it seems that waiting for a locked table might cause frustration? Also, we do not use any reporting that requires real-time 100% accurate data. I guess I will have to look a little closer at the issue,
According to some job specs, being able to join tables. 
If you just want to ignore everything after the first paren in the name, this will do it: SELECT rtrim(left(Name, len(Name) - charindex('(', Name))) as NameFormatted , sum(Miles) as Miles FROM YourTable GROUP BY rtrim(left(Name, len(Name) - charindex('(', Name)))
You could use the logic to remove all non\-numeric characters, then add the dashes in at the correct places. As long as there are 10 digits in there, this should work. The actually code depends on the flavor of SQL.
Oh shoot! Yeah, should've specified that it's MS SQL 2014. Would you happen to have an easy script for that one?
Added source data. I think I understand what you're saying now - I was forgetting that you only get nulls when you get a mismatch as a JOIN condition, not when you get a mismatch as a JOIN condition.
I know a decent chunk from each of the 3 points, where do I stand ü§î
 select distinct b.parentmaterialID, p.manufacturername, b.ChildMaterialID, c.manufacturername from dbo.BOM b left join dbo.partaml p on b.parentmaterialID = p.itemnumber left join dbo.partaml c on b.ChildMaterialID = c.itemnumber where p.itemnumber is null or c.itemnumber is null or p.manufacturername = c.manufacturername; Something like this? -------------------------------------------------------------- My test data use TSQLV4 create table BOM ( parentmaterialID varchar(20), ChildMaterialID varchar(20) ) insert into BOM(parentmaterialID, ChildMaterialID) values('item5', 'item6') insert into BOM(parentmaterialID, ChildMaterialID) values('item6', 'item7') insert into BOM(parentmaterialID, ChildMaterialID) values('item7', 'item8') insert into BOM(parentmaterialID, ChildMaterialID) values('item6', 'item3') insert into BOM(parentmaterialID, ChildMaterialID) values('item5', 'item4') create table partaml ( itemnumber varchar(20), manufacturername varchar(20) ) insert into partaml(itemnumber, manufacturername) values('item4', 'supplier2') insert into partaml(itemnumber, manufacturername) values('item5', 'supplier6') insert into partaml(itemnumber, manufacturername) values('item7', 'supplier4') insert into partaml(itemnumber, manufacturername) values('item7', 'supplier6') insert into partaml(itemnumber, manufacturername) values('item8', 'supplier3') insert into partaml(itemnumber, manufacturername) values('item8', 'supplier6') select distinct b.parentmaterialID, p.manufacturername, b.ChildMaterialID, c.manufacturername from dbo.BOM b left join dbo.partaml p on b.parentmaterialID = p.itemnumber left join dbo.partaml c on b.ChildMaterialID = c.itemnumber where p.itemnumber is null or c.itemnumber is null or p.manufacturername = c.manufacturername;
In general, I would say avoid correlated subqueries and rewrite subqueries to joins as much as possible. Even mayor databases will currently not rewrite certain types of subqueries. Without a specific use case, I would say it is pretty hard to provide more detailed tips. Maybe you could elaborate a bit more about your setting? 
I think this might work, but it needs to be debugged on a larger dataset. WITH cte1 AS ( SELECT c.user_id , c.valid_from , c.valid_to , t.transactionID , c.CouponOrder , ROW_NUMBER() OVER (PARTITION BY c.user_id, c.CouponOrder ORDER BY t.transactionID) AS CouponSeq , ROW_NUMBER() OVER (PARTITION BY c.user_id, t.transactionID ORDER BY c.CouponOrder) AS TransSeq FROM (SELECT * , ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY valid_from) AS CouponOrder FROM coupons) AS c INNER JOIN transactions AS t ON t.user_id = c.user_id AND t.transactiondate BETWEEN c.valid_from AND c.valid_to WHERE t.coupon_used = 'Y' AND c.used_for_transaction IS NULL AND NOT EXISTS (SELECT * FROM coupons AS c2 WHERE c2.user_id = t.user_id AND c2.used_for_transaction = t.transactionID) ), cte2 AS ( SELECT * , CASE WHEN a.CouponSeq - a.TransSeq &lt; 0 THEN NULL ELSE ROW_NUMBER() OVER (PARTITION BY a.user_id, a.CouponOrder , CASE WHEN a.CouponSeq - a.TransSeq &lt; 0 THEN 1 ELSE 0 END ORDER BY a.CouponSeq - a.TransSeq) END AS FinalSequence FROM cte1 AS a ) SELECT a.user_id , a.valid_from , a.valid_to , a.transactionID AS used_for_transaction FROM cte2 AS a WHERE a.FinalSequence = 1;
One of my specialties is query optimization, and it's a very nuanced subject. I hesitate to give general advice because every query is different. The only fundamental principles are obvious and too general, summed up as "make efficient use of resources". Not very helpful. Also, optimization is more about what the database is doing and less about how the SQL is written. Of course SQL can be rewritten to affect what the database does, but not in the sense "always wring SQL this way so the database does this well" because that's just not the case. It takes a good understanding of how databases physically work, combined with the experience of optimizing 100s of queries, to develop a good knowledge of what to look for and what techniques can be effective in manipulating execution plans. If you can get share a good and bad execution plan, I can explain the differences and what's going on. Otherwise, I'm happy to answer any questions you might have.
If I'm reading your request right, you just need to alias your columns (the `AS` SQL tool).
&gt; where do I stand ü§î As a knowledgeable person with gaps in your education. (I'm in a similar hole filled boat!)
Interviewed a guy who said he was an intermediate to high end in SQL knowledge. He immediately followed that up with saying he'd have to google to check the differences between a left join and inner join. You can guess how the rest of his interview went.
I'm sorry I didn't explain well enough. Notice on the image there are 2 rows per emplid - I blacked them out but should have just put dummy emplids in there. One row contains a plan_type of 50 and the other of 51. I need both of those on one row, but with a new column header.
So something like: ```CASE WHEN this_col = some_value THEN select_this_column WHEN this_col = some_other_value THEN select_this_OTHER_column ELSE null END```
Learn to read execution plans. Read [Use the Index, Luke](https://use-the-index-luke.com/)
Ok, to show even better, the first two rows of the first image should be merged together and look like this: https://imgur.com/QDGI2ib
I edited the original post with better screenshots of what I have and what I need.
I'm happy enough though. I'm consistenly called upon for my data manipulation skills. I've tried a bit of php and web design, not my thing. I'd write SQL and manage database all day long.
Do this in your presentation layer, not in a query.
Okay.. I see. I'm on mobile so bear with me. Where (p.mfr is null and c.mfr is not null) or (c.mfr is null and p.mfr is not null) or p.mfr = c.mfr I tend to not like a lot of ors, so I'm still thinking about it - but that should work with still just tje two joins. 
There are eight different species of bear! The North American Black Bear, the Brown Bear, the Polar Bear, the Asiatic Black Bear, the Spectacled Bear, the Giant Panda, the Sloth Bear, and the Sun Bear!
Lol! After I edited it like 1200 times, I noticed I probably used the wrong bear. &lt;sigh&gt; I give up ;)
The Black Bear can be found with black, brown, gray, silvery-blue and cream fur coats!
My personal steps when hitting a slow query: * Am I missing an index. EXPLAIN is your friend. Nothing is performant if you have forgotten an index. * Does this query really need to be fast? Can I batch it in a nightly cron job? Do the input parameters constantly change? * Are the right indexes being used. Perhaps I need to specify which index needs to be used.. * Can I break this query into two or more smaller, faster queries? There is alot of power in putting CREATE TABLE in from of a SELECT and caching an intermediate result. Then index that result, then do the next part(s). At this point, I ask "How important is this?" Because once you get past these basic methods, you will either need to learn alot, or get access to someone else who has learned alot. I will say that the only way to start getting good at query optimization, is to start doing query optimization. But there can be a lot to gain by taking the steps above which amount to "avoid optimization if you can". 
Eh- not so much asking the right question, because it may often be others asking the question, but knowing how to go about answering the question. I.e., you're asked to write a query to retrieve the top 3 products for each category by month and you whip up a SELECT DISTINCT with a CROSS APPLY. It took you 10 seconds to get the answer which might have taken a noob all morning to figure out. This really only comes from experience encountering all sorts of questions and recognizing commonalities among disparate problems. If you're working as in-house developer for a company outside of the IT sector, domain-specific knowledge goes a long way toward "wizard" status- your familiarity with the databases behind your company's systems (ERP, CRM, etc.) means you immediately know exactly which tables and columns to join to find the answers, and you'll probably amass a collection of frequently used queries, parts of queries, scripts, and other tools that enable seemingly godlike powers of information manipulation.
Nope. Informatica is a application I use to ETL data from transaction dbs to staging dbs to report dbs.
Can you direct me to readings or guides to understanding and applying this to my databases? Where did you learn it? Thanks!
&gt; I would not recommend mongo Mongo DB is badass rockstar tech.
https://en.wikipedia.org/wiki/Extract,_transform,_load ETL is a a process for getting data from one system into another system. Getting an Excel file imported into a SQL database can be considered ETL. Typically I import data (flat file, Excel, CSV, etc) into a temporary staging table within my destination database (typically MS SQL). I then use T-SQL to modify the imported data (transform it) to what I need for my purposes. An example of a transform is converting values like 'M' in source file = 'Male' in destination. This is a simple transformation. I then use another TSQL query to load this transformed data into my destination table. So that process might involve using scripting languages like powershell, bash scripts, etc. It could have parts with full blown programming languages or custom routines written in something like C# or Java. It can involve T-SQL or PL/SQL or any other SQL type query language. 
No doubt, it all is when used in the proper context with the proper planning!
ETL is just a general term that means Extract, Transform, and Load. It's used to describe lots of big business scenarios where large volumes of new data per day (such as sales) are taken (the extract), flattened/de-normalized (the transform) and put into a larger database / data warehouse for reporting purposes (the load). How exactly this is done is different across companies. Much of the time, SQL is involved in the extract part. There's lots of software that is specially built for ETL purposes.
What database are you using? You basically want to write a case statement to check for length of the string, if less than 13 concatenate with leading zero else keep the same string. Pseudo code: Case when length(yourdatestring) &lt; 13 then 0||yourdatestring else yourdatestring end
I‚Äôm asking for the code to change the record on sql, I‚Äôm using MS Access. And I understand the pseudo code I‚Äôm unfamiliar right with sql so I was hoping someone can go through the actual sql code with me.
So use iif instead of case, len for length, "+" or "&amp;" to concatenate
 RIGHT('000000000000'+CONVERT(varchar(12),IntegerField),12)
I wouldnt have thought to drop the AS either... and i see reddit chanhed my back tocks to single quotes. Im wondering if it did that to you as well in your original post. Only reason i recomended back ticks is i saw single quotes in your post...
THAT is a very good point... one i hadnt thought of. So just store the actual timezone amd a TS of the UTC and let maketime do the hard work
No. ETL is a thing you do. The big emphasis of ETL - "Extract, Transform, Load" - is that these three jobs are thought of as separate. This matters when you're talking about huge databases consuming huge amounts of data, especially when the rate of data isn't even. If it matters a lot that you consume everything, but if it's okay that answers you're giving can be on data that's slightly delayed, then an ETL system very well may be the right choice.
No it is a process of applying business rules to data. It can is often done using purposes build applications like Infomatica or SSIS. However, it can be done in any language. Extract data, apply business rules, insert data. 
This might work: ON CASE WHEN X -1 = 0 THEN Y -1 ELSE Y END AS year_solved
Try CASE WHEN (X-1) = 0 THEN (Y-1) ELSE Y END AS year_solved
 ON (CASE WHEN x -1 = 0 THEN y -1 ELSE y END) = year_solved The brackets aren't required but it shows what the issue was. You were making the assignment inside the CASE statement. 
This seems to work. However, how could I `AND` in a secondary `CASE`. This is where I'm having trouble. I need to have two `AS` statements. See a better example below; ON CASE WHEN (x -1 ) = 0 THEN Y -1 ELSE Y END AS year_solved AND CASE WHEN (Y -1) = 0 THEN 4 ELSE (X -1) END AS quarter_solved AND c.product = s.product As you can see, I need to `join` on two cases, each with an `AS`. Can this be accomplished? The `IF` I have setup works fine so I would imagine there is a `CASE` synthax for that. 
Perfect - thanks!
 1 2 3 4 5 6 select case my_num when 1 then 'one' when 2 then 'two' when 3 then 'three' else 'four or more' end from my_table;
select case my_num when 1 then 'one' when 2 then 'two' when 3 then 'three' else 'four or more' end from my_table; 
The way I would do it, and I'm not sure if its the most efficient way, would be to join two queries SELECT * FROM (SELECT *, [CASE STATEMENT 1]) AS A INNER JOIN (SELECT *, [CASE STATEMENT 2]) AS B ON A.YEAR_SOLVED = B.QUARTER_SOLVED
ETL is my bag, baby. As others already said, not a language but a process. The process can be anything you want, using whatever tools and languages and operating systems and protocols you want. Here's a real world example of ETL: A company's HR software runs in the cloud (aka someone else's computer). But the company needs this data for other uses. They want it in-house and it needs to get archived every day in some sort of snapshot fashion, and in addition, all other applications in the company need this data every day so that the other applications know when a user has become inactive, or newly rehired, or it's a new employee, or they changed a personal attribute such as a phone number/address, etc. So what we do is create an ETL process. The ETL process takes an extract from the online HR software, usually via plain text report that is FTPd to a secure place nightly. From there, we can use the DB's host operating system (example: linux) to create a scheduled job via shell script that grabs the file each day. It can also load the raw the data in database staging tables. At this point the "Extract" of ETL is complete. We have raw data in-house! Now onto Transform. We will use the data in the staging tables to create our snapshot view of what the data looked like on that day. We will also transform our data into different record layouts that the other application needs. With the transformation complete, onto "Load". For "Load", we want our data loaded into other applications and we also want this data warehoused in our DB. So from there, each individual application starts it's own ETL process, picking up this ETL process as the starting point. For a large company, ETL is often an orchestrated event that has dependencies and timings. So to automate it requires a great deal of care, covering all the "what if" scenarios. What if the original data source was only partial, like the file transfer failed? What do you do? What would happen to your business if you blindly continued? A good ETL process will have a lot of sanity checks. Are you on the right system? Yesterday your record count was 52,000 but today it's only 48,000, so should you continue? Today you got an alphabetical character in a numeric field. Does that crash your ETL process leading to a cascading event of catastrophic failure throughout your systems? Or have you coded error traps for that? ETL is often both a proactive approach and a reactive approach. You try to think of all the scenarios up front, but then as you enter production and shit blows up, you say "huh, I never thought that would/could happen", and you trap for it and redeploy. Wash, rinse, repeat until you have an unbreakable iron-clad ETL process. I love ETL. For me, ETL is a combination of shell script, sql loader, sqlplus, rsa key exchanging, FTP, sFTP, and crontab. Some scripts are just a few lines. Others are 20+ pages long because of the unfortunate circumstances that could arise if I let bad data into my system. One mistake and it could cost the company 100s of manhours to fix. 
I would not use a function for this, what you are looking for here is dynamic sql.
I clearly have no idea what i'm doing because i'm not sure what it means. Can you give me an example of what you are looking for? 
 &gt; 1! 1! = 1 
So a function has some limitations, I don't believe you can edit the view using the function and if you can, it may be cumbersome. Dynamic SQL is basically constructing a SQL Statement auto-magically. I can't regurgitate as well as other people have, here's a good link: https://www.mssqltips.com/sqlservertip/1160/execute-dynamic-sql-commands-in-sql-server/
1! / 0!
Perfect, thank you!
It means your sub query returns more than one row. Which is not allowed for the coalesce function. 
If I do that, then I get `Unknown column sale_date` as I'm selecting it from no where.
Thank you for your insight. Another commenter provided a link to Use The Index Luke, which I'm going to spend some time digesting to better understand query execution overall, then delve into some of my less-efficient queries to see what I've learned
I'd take this with a huge grain of salt because I don't know much about your situation, but from what you describe (no need for 100% accuracy) you're probably fine to risk dirty reads.
Run SELECT QUARTER(sale_date) FROM sales on its own. You'll see that it returns multiple rows, one for each record in sales, but when you put it in the COALESCE function you're telling it to expect a single value. So you have to figure out what single value you are trying to get. I suspect you may want something like this: SELECT COALESCE(x_quarter, CASE WHEN QUARTER(sale_date) = 0 THEN 4 ELSE QUARTER(sale_date) - 1 END) FROM sales
&gt; FROM someTable I'll look into this. My query in the OP is shortened. My actual `FROM` is a sub-select with a join. I was hoping there would be a more straight-forward way to go about this. Thanks.
Why not just query the view with the needed values... create view vbatmanrocks as SELECT A.CATEGORY1, A.CATEGORY2, B.SUM(DOLLARS) FROM ALPHA A LEFT JOIN BETA B ON A.KEY=B.KEY GROUP BY A.CATEGORY1, A.CATEGORY2 UNION SELECT A.CATEGORY1, 'TOTAL' CATEGORY2, B.SUM(DOLLARS) FROM ALPHA A LEFT JOIN BETA B ON A.KEY=B.KEY GROUP BY A.CATEGORY1 SELECT * FROM vbatmanrocks WHERE QUARTER_YEAR IN( @Q1, @Q2, @Q3, @Q4) 
Is cost a factor? 24/7 availability with no maintenance windows is going to cost a lot.
I've used that method of dumping the results in a temp table in other instances, but sometimes I wonder if I would be able to capture this kind of thing in a view. With your method I get the error: The metadata could not be determined because statement 'EXECUTE(@sql);' contains dynamic SQL. Consider using the WITH RESULT SETS clause to explicitly describe the result set. but it works with: SELECT * FROM OPENQUERY(localserver, 'DECLARE @sql VARCHAR(256); SELECT @sql = [database].[dbo].[gen_dyn_sql](); EXECUTE(@sql) WITH RESULT SETS ( ( NAME NVARCHAR(64) ) );') AS x; Thanks!
... but this works: CREATE VIEW v_dyn_sql AS ( SELECT * FROM Openquery(localserver, 'DECLARE @sql VARCHAR(256); SELECT @sql = [database].[dbo].[Gen_dyn_sql](); EXECUTE(@sql) WITH RESULT SETS ( ( NAME NVARCHAR(64) ) );') AS x ) SELECT * FROM v_dyn_sql
Neat.
There's a little camera icon next to 'See attached image'. If you click on that you should see the image.
Thanks for letting me know. I just fixed it.
change it to: order by concat\(SUPPLIER\_ID, COMPANY\_NAME\) desc
not a single one of these mentions MSSQL or Oracle. This article is niche to say the least.
 SELECT * FROM ( SELECT CONCAT(supplier_id, company_name) as 'i' FROM accountmanagers ) AS subquery ORDER BY LEN(i) DESC 
I want to order by length or the resulting concat though 
Do you have users who need access to your DB at all times?
will try tomorrow. Thanks alot
bittesch√∂n
Thanks a lot, this worked! select emplid, name ,sum ( case when PLAN_TYPE = 50 then hours end) as sick_hours, sum( case when PLAN_TYPE = 51 then hours end) as vac_hours from ( SELECT A.EMPLID, A.EMPL_RCD, A.DEPTID, B.NAME, C.PLAN_TYPE, TO_CHAR(C.ACCRUAL_PROC_DT,'YYYY-MM-DD'), SUM(C.HRS_CARRYOVER + C.HRS_EARNED_YTD - C.HRS_TAKEN_YTD + C.HRS_ADJUST_YTD + C.HRS_BOUGHT_YTD - C.HRS_SOLD_YTD - C.HRS_TAKEN_UNPROC + C.HRS_ADJUST_UNPROC + C.HRS_BOUGHT_UNPROC - C.HRS_SOLD_UNPROC) Hours FROM PS_JOB A, PS_EMPLMT_SRCH_QRY A1, PS_PERSON_NAME B, PS_LEAVE_ACCRUAL C, PS_EMPLMT_SRCH_QRY C1 WHERE ( A.EMPLID = A1.EMPLID AND A.EMPL_RCD = A1.EMPL_RCD AND A1.OPRID = 'xxxxxxxx' AND C.EMPLID = C1.EMPLID AND C.EMPL_RCD = C1.EMPL_RCD AND C1.OPRID = 'xxxxxxx' AND ( A.EFFDT = (SELECT MAX(A_ED.EFFDT) FROM PS_JOB A_ED WHERE A.EMPLID = A_ED.EMPLID AND A.EMPL_RCD = A_ED.EMPL_RCD AND A_ED.EFFDT &lt;= SYSDATE) AND A.EFFSEQ = (SELECT MAX(A_ES.EFFSEQ) FROM PS_JOB A_ES WHERE A.EMPLID = A_ES.EMPLID AND A.EMPL_RCD = A_ES.EMPL_RCD AND A.EFFDT = A_ES.EFFDT) AND A.DEPTID = '120029' AND A.EMPLID = B.EMPLID AND A.EMPLID = C.EMPLID AND A.EMPL_RCD = C.EMPL_RCD AND C.COMPANY = A.COMPANY AND C.ACCRUAL_PROC_DT = (SELECT MAX( D.ACCRUAL_PROC_DT) FROM PS_LEAVE_ACCRUAL D, PS_EMPLMT_SRCH_QRY D1 WHERE D.EMPLID = D1.EMPLID AND D.EMPL_RCD = D1.EMPL_RCD AND D1.OPRID = 'xxxxxxxx' AND ( D.EMPLID = C.EMPLID AND D.EMPL_RCD = C.EMPL_RCD AND D.COMPANY = C.COMPANY AND D.PLAN_TYPE = C.PLAN_TYPE )) AND A.EMPL_STATUS NOT IN ('D','T') )) group by A.EMPLID, A.EMPL_RCD, A.DEPTID, B.NAME, C.PLAN_TYPE, TO_CHAR(C.ACCRUAL_PROC_DT,'YYYY-MM-DD') ) group by name, emplid ;
works like a charm. THANK YOU SO MUCH
Why do you say you only seem to be able to export 7 or 8? What error message(s) does it give you?
Having a second column should work. SELECT CONCAT(SUPPLIER_ID, COMPANY_NAME) as 'i' ,LEN(CONCAT(SUPPLIER_ID, COMPANY_NAME)) as 'iLen' ,COMPANY_NAME as 'c' FROM AccountManagers ORDER BY iLen I'm guessing you can't use calculations in the ORDER BY on aliases as the calculation needs to be processed as part of the SELECT step before they are assigned. So even if you tried to order by LEN(c) in this case you'd get an error. As such this does what it would do if you *could* do that, or if you did the CONCAT in the ORDER BY as suggested by the other poster. 
Yup, we're in healthcare, so 24/7 in US and we have some parties in EU/Asia as well.
Thank you! That worked. I ended up changing the fill for the tablix to 'khaki'. I had only changed the fill for the row and fields.
That is not normal behavior. Are you sure the disk is not running out of space? Or that there is no limitation on file size at the OS level?
It is not a space problem. The input card can only handle so many characters. SET ACCTLIST=('A1234', 'B2345', and 98 more account numbers) only the first 7 or 8 will be exported.
And if I'm not in the proper subreddit, please let me know. Also, if you can think of a better place to post this to, I would very much appreciate it.
Guys, I did it. I got the job. &lt;3
Are you sure this isn't a batch limitation? I hate batch with a passion and avoid it when possible, so I do not know details about its variables. With considerable Db2 experience, I do not see how it could be on the Db2 side. I have seen inlists with hundreds of values with no problem, and Db2 provides good error messaging.
You have the wrong context. That was me writing ‚Äútry the following:‚Äù
nice reply
If you want something simple, you may want to try PowerBI or SSRS. I administered SSRS for a while and it was pretty easy to setup and maintain. PowerBI has been awesome for my team. Setup took about 2-3 days and it‚Äôs still running strong. Licensing is odd, but they should have something g that meets your needs. PowerBi.com And of course there‚Äôs the ever popular Tableau and Qlik. Here‚Äôs the magic quadrant for BI. https://www.sisense.com/gartner-magic-quadrant-business-intelligence/
You might be able to get more people to help provide insight on your error if you edit your post to show your query in code format. Are you using MS SQL, MySQL, DB2, etc? Just by looking, it seems your error is on this line regarding the constraint you are trying to add: CREATE TABLE customerorder ( orderID char(4) NOT NULL, prodID char (9) NOT NULL, quantity varchar(9) NOT NULL, CONSTRAINT prodorders PRIMARY KEY (orderID, prodID), CONSTRAINT fk_orderID_customerorder FOREIGN KEY (orderID) references prodorders(orderID), CONSTRAINT fk_prodID_customerorder FOREIGN KEY (prodID) references product(prodID));
I'd say start out with SSRS since you're probably already licensed and familiar with the MS tools. It can be a little quirky because it's so flexible. 
https://www.microsoft.com/en-us/learning/course.aspx?cid=40364 https://www.coursera.org/learn/data-analytics-business/#
this works too!
this works! Could you explain why? 
This is essentially how availability groups work, except it takes care of dns for you, and you can have more than one secondary server. There is a single IP (listener) for the group, which points to whichever node is the ‚Äúprimary‚Äù at any given time. Also, the secondary nodes are readable, so you can use them for read-only workloads like reporting and taking backups. Pretty cool stuff. 
 Try something like SELECT * FROM Cars WHERE Cost &gt;= (SELECT AVERAGE(Cost) FROM Cars)*0.9 AND Cost &lt;= (SELECT AVERAGE(Cost) FROM Cars)*1.1 Would ideally avoid two sub queries (though the query engine is probably smart enough to just run it once anyway) by using a table valued function, and storing the average in a variable.
&gt; Are you using MS SQL, MySQL, DB2, etc? MS SQL &amp; DB2 don't kick out errors that start with `ORA-XXXXX`. It's Oracle or MySQL.
&gt;Error starting at line : 44 in command - INSERT INTO prodorders VALUES ('4', '30-DEC-2017', '04') Error report - SQL Error: ORA-00001: unique constraint (RPARRY.PK_ORDERID) violated 00001. Your primary key on `prodders` is `orderID` and you already have a record where `orderID` is 4 You didn't ask about this, but I'm going to tell you: your schema has at least one terrible flaw. You're storing dates as `varchar2(30)`. If they're dates, ***STORE THEM AS DATES***. Oracle has [a whole class of data types](https://docs.oracle.com/cd/B28359_01/server.111/b28318/datatype.htm#CNCPT413) devoted to it and you will avoid an enormous group of problems by using it.
Do you need very up to date data? If so then SQL replication would be an option. If your customers are happy with data that is a day or so out of date then either some data warehouse option or just restore the live DB to the reporting server and allow whichever frontend reporting app your users have to call the reporting SProcs. All three options have benefits and downsides so worth researching each.
This. You really want to try and minimise the number of moving parts in your code. You could totally use a procedure to generate dynamic SQL and drop/recreate the view, but that's seems to be an overcomplication when TVFs exist - so long as whatever is consuming this is able to determine the appropriate parameters and call it.
Many different ways to skin a cat so to speak but this seems to work. You may find the performance would work better if you pre-computed the upper and lower bounds in the CTE... create table #Cars (car varchar(100), price decimal) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',10) insert into #Cars values('brrm',11) insert into #Cars values('brrm',13) ; With CTE_Averages AS ( select car,avg(price) AS AveragePrice from #Cars GROUP BY car) select c.*,ca.AveragePrice from #cars c join CTE_Averages CA ON CA.car= C.car and C.price between CA.AveragePrice*0.9 and CA.AveragePrice*1.10 drop table #Cars
Take a look at the HAVING clause. It should provide you what you‚Äôre looking for. 
In general, being able to read query plans and having some understanding of how they are determined and how the database engine works will be very helpful.
This is the most maintainable and best performing solution posted here so far. It also does not introduce the risk of SQL injections which dynamic SQL does. 
...HAVING COUNT(*) &gt; 9
Is the most recent value of opendate going to be the most recent record? If not, is there another way to determine what the most recent record is for the accountno? 
Awesome thank you
If the most recent opendate will be the most recent record, create a second query like so: Select accountno, Max (opendate) from etc etc Do an inner join on the accountno and opendate from your new query to the one below. This will restrict your records to the most recent for each accountno and it won't matter to the query what the name is
This 100%. There can never be an 'always do this' answer as it always depends. This week I took a view that was taking several minutes to return a top 1 * from it and added 40 joins to make it approx 600 times faster. The original view was grabbing every ID from a table in a left joined sub select with no where clause, cross applying it by 40 records so that in order to deliver the top 1 it had to crunch 41 million records. Adding 40 joins to a query would under 99% of circumstances be a terrible idea. The joys of cleaning up after object orientated programmers who don't understand SQL.
You are correct. It is a batch error. I was able to get around it by pointing to a dataset rather than have the account numbers in stream. Thanks for your help!
Here's a fun one from last week: /*800 lines of code*/ WHERE ca.ChargeActiveClusterKey IN (SELECT ChargeActiveClusterKey FROM cteRecordSelection) AND ca.[Status] &lt;&gt; 17; --10 hours 30 minutes - /*800 lines of code*/ WHERE ca.ChargeActiveClusterKey IN (SELECT ChargeActiveClusterKey + 0 FROM cteRecordSelection) AND ca.[Status] &lt;&gt; 17; --1 minute 25 seconds Nested loops were happening in the wrong order. Instead of looking up a couple thousand records from `ca` it was running the `cteRecordSelection` subquery (with 20ish tables) a couple million times. You know when you filter one table and the query transitively filters a joined table with the filtered column in the join condition? My hypothesis is that the query was sniffing something inside the subquery, contributing to the poor join order, and adding the `+ 0` broke that link.
Yeah thats a really weird one. Sometimes you got to break the sargability to make it faster. The optimizer is clever but not infallible.
Performance tip. Use count(1) instead of count(*). This way sql does not need to lookup and load unneeded data in his ram, so it has more available ram left for other things
Without seeing any code it is hard to provide direction but I will say it sounds like you might be selecting/adding the header data into a record type with a number field or trying to to_char the header data into some format. Trying to insert headers into that record type or format the headers will not work because they are not numbers/dates. Wherever the code is looping over the dataset to output the CSV is it not possible to add a line before the loop which outputs the desired headers? I would say make the record type all varchar2 types but it is helpful to have the raw data in the appropriate type so formatting is possible. I often run across code where devs attempt to do everything in one shot and it becomes a fickle process to debug or extend. Try breaking it up into several steps and establish a workflow of transformations. At that point you could almost create a control table with how to format and transform data and you would not need to manually write CSV exports anymore. It would just be configuring some control tables and you are good to go! I typically do something like this: 1. Gather data to export 1. Set a headers record with the values needed for this export. Typically this is an array because it allows the column number to be flexible 1. Run transformations and validations on the data as needed 1. Write the header record of the CSV if desired 1. Write the actual CSV data with formatting as needed
Congratulations!
Do you have a good article / podcast / resource to digest about that in particular? I may need to update my post, but I'm thinking it would be a HA OLTP setup. So people in Asia / America would be pointed towards the primary at all times for R/W right? 
I'm not sure why you'd create the trigger for a specific list when Microsoft already includes functionality for that with **DDL_SERVER_LEVEL_EVENTS** and **DDL_DATABASE_LEVEL_EVENTS**. 
Yes I figured as much. I just wanted OP to format his code so it is easier for people to read and also his title says 'SQL Developer' so hopefully he/she is aware of troubleshooting steps he/she can take. And also I don't think MS SQL has data types of varchar2.
Of course! Unfortunately, some of us just kind of stumbled into SQL usage instead of starting from the ground up. There are advantages to both methods, but either way you're going to struggle if you don't understand these things at some point
the parentheses are not required, nor desired google "noise vs signal"
this is nonsense COUNT(*) does ~not~ require any column lookup, if that's what you meant the optimizer is free to choose any index that will speed up counting
Thank you. I will have to explore this option, I am not super familiar with varchar. I thought about casting all the numbers as text, but didn't want to do that (also not sure if it would fix my issue). The actual query itself is in several steps to convert the data/do calculations appropriately. Any suggestions on where I can read up on VarChar? An extremely parsed down version of the code is below: select 'itemno' as itemno, 'OnHand' as OnHand from iqms.arinvt union select arinvt.itemno, arinvt.onhand from iqms.arinvt where itemno='10-00000000'
I think I'll probably stick to SSRS, but I am definitely interested in learning PowerBI; so maybe I'll play around with it. Thanks for the links!
Thank you so much! This is what I am doing. :)
Sounds good! If it helps any, I‚Äôm my team set up PowerBI using SQL server Express on my Surface Pro. The engine and the PowerBI gateway/service ALL ran contained on my surface. It was pretty great (and free). So if you want to just play around, feel free to.
Well, nothing like up to the millisecond, but something within the last couple of minutes, sure! We do SQL replication for one portion of our business. The new web application won't require replication (at least that's the hope). Can SSRS not call SProcs and their results quickly? I just need PDFs to generate from current data. I thought that was possible with SSRS?
Worked like a god dam charm. Actually used the max YRMO
So, in this case, no You can use parentheses on conditional join functions, but thats super dirty. You usually just want to put those in the where clause, as the SQL order will filter those out first, and you will usually have a more efficient query.
 SELECT * FROM [Records] WHERE [Location] IN ( SELECT DISTINCT Locations FROM.... ) AND [datetime] BETWEEN @StartDate AND @EndDate
Not to be an ass, but the other two answers aren't really helpful, and don't really account for the full story. There is no technical difference in this *specific* context - HOWEVER: Where it gets meaningful is when you have nested conditions. For example, if you had a situation where you were joining on multiple criteria, such as this: FROM t1 LEFT JOIN t2 ON t1.xxx = t2.xxx AND t1.yyy = t2.yyy OR t1.yyy = t2.zzz Without parentheticals, this will likely NOT perform as one would expect. Depending on the desired behavior, you have to add parentheticals to explicitly dictate behavior such as: FROM t1 LEFT JOIN t2 ON (t1.xxx = t2.xxx AND t1.yyy = t2.yyy) OR t1.yyy = t2.zzz or such as: FROM t1 LEFT JOIN t2 ON t1.xxx = t2.xxx AND (t1.yyy = t2.yyy OR t1.yyy = t2.zzz) Because of this, my preference is that I write my code to include parentheticals so that the formatting is consistent regardless of whether there are nested conditions or not. It also helps with readability in my personal opinion.
And i'm kicking myself with how obvious it is now that I see it. Thanks
same. after a few subqueries I get pretty lost as it is. 
Took me like 3 hours of wtf before I learned this the hard way. 
All true only I would say when they‚Äôre unnecessary it actually hinders readability not improves it. If I see a parenthetical I‚Äôm now looking for a nested condition, if you always use parentheses even when there is no nested condition it‚Äôs wasted space and just more likely to throw me off as a reader. You‚Äôve clued me in to something that doesn‚Äôt exist in the query. It‚Äôs minor either way, but I have to point out the other side of your argument.
At that point, its really the same as the tabs vs spaces debate IMO. Note: Tab Master Race!
okay, got it next, i would like to ask you about your tables the query looks like it's working with two tables -- vw_DepartmentEquipment DE vw_DepartmentEquipment.EquipmentAssignments EA but something smells here is it perhaps that you are trying to self-join a single table?
Same table. To give some more detail each record is a machine log with: ID, Type, Time, and Person (Plus some junk i need) The start date and end date will depend on a specific type of log occurring, and i need to check everything that occurs between those instances (which are difficult themselves to find since I can't just use the type, but also some of the other data) I could just create a table of those, but it'd be duplicate data. 
no those are definitely two different tables with different information in it(I didn't design the DB)
&gt; which are difficult themselves to find so what chance do i have?
those can't be the actual table names, though
I've got the query for finding them already written, its just not a simple one, and it's the one referenced earlier for setting the start and end date. If it helps to offload those key dates/times into another table I can do that, but I'm not sure what i'd do with it.
You don't always want to put conditional join functions in the where clause, it can lead to lost rows: [SQL Left Join losing rows after filtering](https://stackoverflow.com/questions/18750464/sql-left-join-losing-rows-after-filtering)
Thank you, I will apply this to my server I appreciate it
sounds like it could be as simple as making that query a subquery (or two, if the logic for start and end is different) AND [datetime] BETWEEN ( SELECT ... -- @StartDate ) AND ( SELECT ... -- @EndDate )
which way is the one-to-many relationship?
Assignments is joined onto base equipment. That really isn't the big part of this though that should be the focus...
Glad it worked! There may be a better way but this has always worked for me after much trial and error
Noob here just wondering if the sample database on the website posted can be converted to PostgreSQL? I have PGAdmin and data grip all up and running with the fake dvdrental database, but I thought it would be really fun to mess around with real movies, actors, directors, etc. Any help is appreciated.
I only use parenthesis for nested conditions like (x=1 and y=0) ; (x=1 or x=2).
Got it, I've been trying to learn Python lately. And TRY / EXCEPT statements are a thing. Thought maybe I could do that in Python
Learned the hard way to ALWAYS have parentheses whenever I have an Or anywhere.
so each assignment can have multiple equipments? or each equipment can have multiple assignments? your join condition on "instance" isn't very helpful
 create table candidates ( firstName varchar(10), lastName varchar(10), resume varchar(10) ) insert into candidates(firstName, lastName, resume) values('f1', 'l1', 'f1') insert into candidates(firstName, lastName, resume) values('f2', 'l2', 'f1') insert into candidates(firstName, lastName, resume) values('f3', 'l3', 'f2') insert into candidates(firstName, lastName, resume) values('f4', 'l4', 'f3') insert into candidates(firstName, lastName, resume) values('f5', 'l5', 'f7') select c1.firstName from candidates c1 where -- names mentioned by another (excluding self): f1, f2, f3 exists ( select 1 from candidates c2 where c2.resume like '%' + c1.firstName + '%' and c2.firstName &lt;&gt; c1.firstName ) or -- names who mention another (excluding self): f2, f3, f4 exists ( select 1 from candidates c2 where c1.resume like '%' + c2.firstName + '%' and c2.firstName &lt;&gt; c1.firstName ) 
I'm on mobile so can't write out a query but why not a sub query? Select...Where person_ID NOT IN (select person_id....where x NOT LIKE %h%)
Sometimes I use parentheses in joins for readability: FROM t1 LEFT JOIN t2 INNER JOIN t3 ON t2.xxx = t3.yyy ON t2.aaa = t1.bbb Valid but confusing FROM t1 LEFT JOIN ( t2 INNER JOIN t3 ON t2.xxx = t3.yyy ) ON t2.aaa = t1.bbb Better readability though parentheses are optional.
When I see parentheses all over the place I assume whoever wrote the code doesn't know the order of operations and is sprinkling them all over the place just to make sure it does what they want.
For sure. I will never forget the half an hour I spent troubleshooting a query saying to myself "this is impossible". Now when I have an OR it looks like this: WHERE (A = 1 OR B = 2)
You say that like it's a bad thing to be clear and specific, and it sounds weirdly snobbish.
That's been my impression so far. I'm all for adding parentheses for clarity as you can see from my other comment this thread.
You're going to have a hard time and terrible performance without using auto-increment. Why do you think you can't use it?
Its just for hobby, trying to learn SQL in my spare time. I'd like to understand both ways of working. So i'd like to know how it works with autoincrement and without. 
Is forcing people to use migrations and keeping them in source control really that hard?
Try posting to /r/samplesize too!
Hello eduard93, Thank you very much for the feedback. I will include you feedback into my research thesis. Hope you can complete my survey. Thank you very much!!!
Hello DataDouche, Yes I did. Hope you can complete my survey. Thank you very much !!!
What would sorta be neat is if they built in temporal functionality into the system tables. How cool would it be: *** SELECT t.name, c.name, st.name FROM sys.tables AS t INNER JOIN sys.columns AS c ON t.object_id = c.object_id INNER JOIN sys.types AS st ON st.system_type_id = c.system_type_id FOR SYSTEM_TIME AS OF DATEADD(dd, -14, GETDATE()) *** 
Are the two fields your multiplying int fields?
Well I'm trying to show the total amount each customer spent in 2009
Not sure I understand
What's the actual error message? What line is highlighted? Also, learn how to format your code when you post it. It's not that hard.
Int is a data type, short for integer. 
This looks a lot like a homework assignment. How about giving us what you've got and we can help where you're going astray?
Yes, in that case the above code is what you are looking for. You need to aggregate the orders in some way to group by customer. What you are multiplying just gives order total for each order. So, you are trying to select one customer name, and a bunch of order totals, which can't be represented in one record. You need to sum the order totals. That allows them to show as one record by customer.
SELECT * FROM .CustomerID.year=2008&gt;2009 WHERE UNION (sales / region) GROUP BY answers[0,1,2,3,4]
Do you need it in one query? What specifically is causing you trouble?
Nobody should ever use the "other" way of working though. I had to do it once on a platform without IDENTITY. Declare an int variable, set it to MAX(PK), assign that + ROW_NUMBER() to the PK field in your insert rows. Was horrid.
Ah, gotcha. Thanks.
For Christ's sake, just use prepared statements already.
You can't call out an alias in the same query its established. Since its been established in the subquery, you can now use it in the main query.
i gotta upvote this this is the most creative and egregiously bad faux-sql i have ever seen
The other top comments have already told you "what" you need to do to make it work. Namely, add some kind of aggregation on your calculated total. Here's the reason why that's necessary. Imagine I have a table with three columns; movie_id, actor_id, salary. The primary key is (movie_id,actor_id), so each row represents a unique movie/actor pair. That is, each movie may appear more than once, and each actor may appear more than once, but their can only be one entry in the table for Elijah wood starring in The Return Of The King. Now, imagine I wrote a query: Select actor_id, average(salary) from movie_casting group by actor_id; This would return one row for every actor, and during the group by aggregation, there would maybe be 10 different salaries that it would take the average of to generate the value for that single return row. Now imagine instead I wrote Select actor_id, salary from movie_casting group by actor_id; It still NEEDS to return one row for each actor, but during the group by aggregation, how does it know what to do? There are 10 different salaries for that actor, and it's supposed to return a single value for salary. Imagine how frustrating it would be if your boss told you "tell me the salary for Elijah wood". There are 10 answers to that question. So you ask the follow-up "in which movie?" But he insists. "Tell me the salary of Elijah wood". So you try again, "okay, well do you want to know much he's made total in his life? Or how much on average he makes in a single movie?" But once again your boss says "tell me the salary of Elijah wood". So finally you relent and just pick one of the 10 answers at random. This is an example of an undefined behavior. MySQL lets you do it, and it relents, and picks a number out of a hat for you. Other RDBMS are very strict and don't let you ask ambiguous questions. PostGRES INSISTS that you are precise. You must tell it if you wanted the sum of the salaries, or the average, or just add salary to the group by, so it can give you all of the distinct values for each actor! In your case, you might be getting 30 different order totals for each customer ID in 2009. In this case, the DB has NO IDEA what you want to do. You insist that you only want one row per customer id, but don't tell it how to resolve one to many relationships.
Do you enjoy SQL injection attacks? Because that's how you get SQL injection attacks. There's a solution for this already. It's called prepared statements/parameterized queries. Your Postgres library probably already has support for them. And so does your MySQL library, where you should *also* be using them.
If you're going to post a screenshot of text, at least limit it to the relevant portion of the conversation and explain what the statement is. Don't make people hunt and guess at what you're referring to.
Can you share tabke definitions?
UNION * FROM distinct (up-vote)
I've been trying to think of a good use for this, and can't. Generally when you want to build queries and export data, you'd use an IDE (ex SSMS) that provides schema and permission context along with the query writer. If it's something that needs to be run regularly, Reporting Services already solves that. Don't get me wrong, it's neat. I've just become jaded after seeing applications like this being given to end users with instructions to 'copy and paste the query from this txt document into the field and your morning report will show up on your desktop'(*not withstanding having to give regular employees direct database access*) or professors expecting students to blindly build queries without any schemas using only things like this.
Thanks I get that. I use SSMS or excel‚Äôs ODBC functionality to run larger queries but if I just want a quick way to pull the top 10 rows from a table this has been useful for me. 
You may want to clean up the code or use the format that reddit has. As is, the code is kinda hard to read. 
Your parenthesis seem to be misplaced. Try wrapping both of the full equations in parens: WHERE (priceinusdollar * (subquerey) &lt; 150) OR (priceinusdollar * (other subquery) &lt; 150)
You have what can be summarized as where (amount &lt; 150) or (amount &lt; 150) Your condition is wrong. I suspect what you want is where (amount &lt; 150) or (amount &gt; some-other-number) 
Thanks! That seemed to be exactly what the problem was. It seems so simple looking at it now, can't believe I wasted 2 hours on it.
Yeah that was a copy error on my part. Spent too much time working on the formula that I ended up making some mistakes while re-writing it. Thanks for calling it out. I've edited my post
I totally get the "I've stared at this too long and am missing things" point of writing code. Sometimes you've got to take a break and go do something else for a while. I see from another post that you've now got it working. :) I can't begin to count the number of times I've missed things that were right in front of me. I've found that sometimes I find my mistakes by explaining it to someone else. This is where Rubber Duck Debugging comes from - explain your problem to the duck, and sometimes just in the process of explaining it, because your thought process is different while doing so, you'll realize where you've gone astray.
The problem of parenthesis is infuriating, completely get it. What type of editor are you using friendo?
I think you should join the product and currency tables rather than do all this sub-selecting, which is hard to read and hard to think about.
You could use OLAP functions to compare to rows. LEAD will ompare the current row to the next and LAG the current row with the previous.
&gt; what I'd like is a query that shows me the differences in values between the tables you forgot to tell us what these tables look like but yeah, it should be possible 
I'm using Microsoft SQL Server Management Studio. Found it a little more user friendly for beginners than Oracle. In a previous class we were using MySQL, but this class only allows MS SQL or Oracle.
I agree with this. If you had joined the product and currency tables, you wouldn't have had so many nested parentheses to keep straight. The query could have been rewritten like this: SELECT Product.product_name, FORMAT(Product.price_in_us_dollars * Currency.us_dollars_to_currency_ratio, '$0,0.00') AS price_in_canadian FROM Product JOIN Currency ON Currency.currency_name = 'Canadian Dollar' WHERE price_in_canadian &lt; 150 OR price_in_canadian &gt; 300; An autoformatter helps prevent this kind of error because the indentation won't match what you expect, and SQL Server Management Studio seems to have one built in‚Äîsee [this Stack Overflow answer](https://stackoverflow.com/a/10628242/3239078) and [this documentation page](https://docs.microsoft.com/en-us/sql/relational-databases/scripting/manage-code-formatting?view=sql-server-2017).
$16 per user, per month. 
Make it a type-2 table and then comparing history will be easy. 
Yes! Using it every day. Although I know creators.
I think you can do this with a correlated sub query, no windowing function necessary. select transaction, item,price, price2,saledate, (select min(saledate) from #items i2 where i1.item = i2.item and i1.price = i2.price and i1.price2 = i2.price2) as StartDate from #items i1
Hi, Thanks for the reply! This works for the very first occurrence of the price for the item. What I'm looking for is when an item's price first appears in the previous transactions and is the same as all of those proceeding transactions. In the example table, the 1340 price for the Yellow item would show a date of 3/31/18. For transactions 137 - 139 it should be 4/8/18. 
Were you able to find anything that runs well alongside the Stanford class? 
Sure thing Buster Keaton.
Learning takes time, don't be discouraged. Those two hours are not wasted.
Can you copy/paste your query here as text? I'm having trouble seeing everything in your screenshot.
SELECT PRODUCT_DESCRIPTION, WEEK_IN_MONTH, TOTAL_AMOUNT_SOLD, round(TOTAL_AMOUNT_SOLD/(SUM(TOTAL_AMOUNT_SOLD) over(partition by PRODUCT_DESCRIPTION ))*100,2) AS WEEKLY_PERCENT_SOLD FROM ( SELECT REPLACE(REPLACE(prod_desc,'this is the famous' , ' '), 'size XXXL', 'size 14.00')||':PRODUCT_ID ='||p.prod_id PRODUCT_DESCRIPTION, DECODE(calendar_week_number,'13','First week','14','Second week','15','Third week','16','Fourth week' ,'17','Fifth week') WEEK_IN_MONTH,t.CALENDAR_WEEK_NUMBER, SUM(QUANTITY_SOLD * PROD_LIST_PRICE) AS TOTAL_AMOUNT_SOLD FROM TIMES t JOIN SALES s ON (t.time_id=s.time_id) JOIN products p ON (s.prod_id = p.prod_id) WHERE t.time_id BETWEEN '1-APR-2000' AND '30-APR-2000' AND p.prod_id IN (10,300,540) group by calendar_week_number, REPLACE(REPLACE(prod_desc,'this is the famous' , ' '), 'size XXXL', 'size 14.00')||':PRODUCT_ID ='||p.prod_id HAVING SUM(QUANTITY_SOLD * PROD_LIST_PRICE) &gt;0) ORDER BY PRODUCT_DESCRIPTION, calendar_week_number;
What are the questions? 
The solution is to use auto-increment. Essentially any solution you attempt will introduce race conditions thanks to concurrency, so your code will need to account for getting key violations on this INSERT statement. The auto-increment feature was added to RDBMSs precisely because of concurrency issues. Many RDBMSs support a SEQUENCE, which is a incrementing value that isn't connected to a single table field and functions much the same as auto-increment (indeed, it's how some RDBMSs implement it) but MySQL is not one such system. 
&gt; To be honest I am an extreme noob and dont know what its even asking me basically, any chance you could do me a big favour and tell me some of the answers? - &gt; I am an extreme noob and dont know what its even asking me basically, any chance you could do me a big favour and tell me some of the answers? - &gt; Any chance you could do me a huge favour by doing a couple for me? - &gt; Any help is appreciated as I am struggling a lot as I missed a lot of college time due to a family bereavement. Just looking for some answers as I am struggling bad. 
You never finished your story about what that rogue account was doing. 
Hi, sorry for the late reply. would you know how PERCENTILE_CONT recognizes which column is the frequency column? does it look for the string "frequency" or "freq"?
[According to the documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#percentile_cont) it follows: *** PERCENTILE_CONT (value_expression, percentile ***
Any other suggestions is greatly appreciated. Thanks 
oh fuck off like, have you nothing better to do? Loser
loser
Sure. You just need to: 1. Stand up instances of both Postgres and SQL Server 2. Get the database up and running in SQL Server 3. Recreate the schema in Postgres (likely not a 1:1 data type mapping, so you'll have to work that out) 4. Write up an ETL process of some sort to pull the data out of SQL Server and insert into Postgres
This looks awesome. I wanted to try it out. Setting it up the test connection says its good. But then when I actually try the connection it says it cannot connect to null. Anddddddddd demo over. 
And the physical dimensions don‚Äôt matter? Only the weight? A washing machine is a different size than a stapler.
[This is what I came up with.](http://sqlfiddle.com/#!18/48be5/3) There are some edge cases that won't work properly (rounding issues) or aren't optimized (extra loads), but it should give you some ideas of how to approach the problem.
Well the physical dimensions are related the the calculations of total pallets. Every item has a *std cartons per pallet* which is later used to calculate the total pallets: Total pallets = (Quantity ordered)/(Units Per Carton) / (std cartons per pallet)
This is a great reference - right mix of theoretical stuff, how-to, example usage, and output. Thanks!
Thank you very mush we need more advanced topics. 
Sum(count()) - what ? Never knew, thank you!!
The real secret sauce in that example is the over clause. Sum wrapping a count will just return the count because they are both aggregate. OVER turns the sum into an analytic function which enables the running total 
Very cool, cheers for the additional info.
Too bad OVER(ORDER BY) doesn't work in 2008 :(
Not my choice, but it's in the 2018 budget. So hopefully soon. We have to upgrade our ERP at the same time because of compatibility. 
At least it's on the cards!
&gt; he field is created allowing Null, but I get an error because it's a Foreign Key foreign keys are definitely allowed to be null you must be doing something else wrong, but i can't see it from here
No, because that is a terrible idea. Null leaves room for errors, especially in the case of foreign keys. Make an item in your second table called No Referral Required. have people choose that when no referral is required. 
Thanks for the input. Alternatively I thought I'd get rid of the Foreign Key field and express that connection differently, but that holds the problem, that I didn't give the Foreign Key a name, and all commands I found would need that, and the SHOW CREATE TABLE command doesn't help (it only shows the beginning of my CREATE statement and cuts off halfway through).
√¨ don't think `([InitialDate] like '&lt;=GETDATE()')` is going to do what you think it will what values for this column do you want to accept? only today's date? why would you make that an input field? just default it 
yes, ditch the check constraint altogether 
&gt;you are wrong, there most definitely is a way to do that My no was a no you shouldn't, not a no you can't, should have specified I guess. &gt;oh, please Null foreign keys break 1NF because null is not a value and therefore cannot create a proper link. http://sqlfiddle.com/#!18/a32d1 This is specifically designed to fail getting all records The point is to explain that (WHERE/ON) X=Y is not (WHERE/ON) X=Y It's **Where X is not Null and Y is not Null and X=Y** Most people don't think of it that way and they run into issues because they choose to use nulls in places they should not. 
Null FK fields most certainly do not break 1NF on their own (or any other NFs for that matter). Null FKs are a common, accepted and preferred practice for optional ('N to 1 or 0') relationships for single column key relationships. Although it's not conceptually different, it gets technically tricky with compound keys though so it's a rule of thumb not to have nullable compound FKs.
I'm sorry, but this is simply wrong. Using NULL values is absolutely allowed if you do not specify the NOT NULL clause on the column. It is used in cases where a relation between entities is optional. An example could be the relationship between departments and departments managers. A department that currently has no manager would be indicated with a NULL value in the according foreign key column of the department table. 
I would think so too, from a mere logical perspective. Like in my example, you can have a situation, where you want to feed in data from another table, but optional. I created the table for the specialists like this: CREATE TABLE Specialists (sId int not null, sSpeciality char(255), sName char(255), sAddress char(255), sPhone char(255), Primary Key (sId)); And the one for the treatments like this, including the Foreign Key: CREATE TABLE Treatments (tId int not null, tName char(255), tPrice float(10,2), tSessions int, sId int, Primary Key (tId), Foreign Key(sId) references Specialists(sId)); Do you see any issue with this? This is part of the final project of my database course, so I'm just at the beginning of understanding all of that. 
A FK can only link to a PK. A PK cannot be null. A null FK necessarily represents the lack of a link. If you want to represent an optional link, a null FK is a valid and fully constrained way to do so.
no major issue, just trivial stuff like VARCHAR(255) for a name instead of CHAR(255), and not using FLOAT for price oh, and if a treatment might require two specialists, you're stuck, aren't you putting the relationship into a separate table is actually the "correct" way to do it (some modellers say you should never need to use NULL) so if a treatment has not specialist, instead of a column with a NULL value, you have another table where that row doesn't exist
Condition: https://www.w3schools.com/sql/sql_where.asp Get the top 5: https://www.w3schools.com/sql/sql_top.asp 
Ah, just use a group by and count() then order by the count desc: http://sqlfiddle.com/#!9/ad2eb2/5 SELECT Location , Feeling , COUNT(Feeling) as Total FROM Locations WHERE Feeling = 'Cold' GROUP BY Location, Feeling ORDER BY COUNT(Feeling) DESC LIMIT 5 Remove the LIMIT 5 and replace with SELECT TOP 5 or whatever your SQL flavor uses for the first X results
There are some great examples in here! My one hesitation is that this kind of feels like "if all you have is a hammer, everything looks like a nail". Other tools are far better for doing much of this. Things like R, Tableau, Power BI, Python, even Excel would be more efficient at doing some of this analysis. I love SQL, but it is not the right tool for everything. Use what works for you, but I think it's good to be willing to branch out too.
sum(case...) over (partition order) is useful as well in some cases
1. Don't know. 2. I'm not sure about your delivery pipeline, but we build and deploy our dependencies alongside our SSIS packages. Granted, we have no third party dependencies, it's simply worth noting that the GAC isn't the only option for you to load and utilize the DLL. You could always dynamically load the assembly, as long as you know its relative location. The point is, there are assuredly options beyond the GAC. 3. Your script must finish elegantly for any error reporting to take place (to the best of my knowledge.) This means the script itself must manage and report exceptions back to the DTS runtime without bubbling the exception up to the runtime. try-catch everything, finally to clean up, and make sure you have a path to report the exceptions. I'm not the end-all or be-all of SSIS development, but that's my two cents. If it's garbage, well, you didn't pay.
Doesn't that violate normalization? If each artist can be classified under multiple genres (rather than at the song or album level) how would I indicate that without a m:m relationship?
I am assuming you have an Artist table and the primary key is ArtistID. You also have a Genres table and the PK is GenreID. The artists and genres should be listed once and only once in these tables. In your composite table MusicArtists_Genres, it's sole purpose is to link Artists to Genres. You only need the two PK columns from Artists and Genres (ArtistID, GenreID) in this table, and individually they are Foreign Keys however *each unique combination* of these two columns together makes up the Primary Key of this new composite table. The whole point of normalization is to avoid redundancy. It's ok to have the same GenreID listed multiple times in this table as a Foreign Key. But each combination of GenreID + ArtistID must be unique.
/u/MaunaLoona is saying that your MusicArtists_Genres table doesn't need to have a composite key. It can have GenreID as a PK and then something like GenreName. In this situation, your Songs table could use ArtistID and GenreID as FKs. This will remove your need for a composite key. Here's an image for reference: https://imgur.com/pVylDQ2
&gt; If each artist can be classified under multiple genres (rather than at the song or album level) Don't classify the artist(s) by genre - do it by the genres of music that they've performed.
can you share the code, it will help me in my project. thanks 
I work for a logistics company You need to factor in vendor/client, rush orders, and fragile orders. I also recently introduced a concept I borrowed from a call center called "single touch". In the call center, if we had an urgent need to contact a customer, we discussed EVERYTHING we needed to discuss with that customer during the one call (rather than discuss the urgent matter in the morning, then call back throughout the day/week for other matters). Similarly, if we're shipping a rush job to a client, might as well pick and pack the others. I was stunned that no one had thought of this before I started.
Maybe stupid, but what's the purpose of adding them to "C:\Windows\Microsoft.NET\Framework\v4.0.30319" as well?
ahem. NOT ( a AND b) == NOT a OR NOT b
Good question. I just realized that was the script that runs on our developer laptops. For deploying to the server, the dll is coppied to C:\Source\ZPEDW.EtlSharedLibrary and this script is run: copy "C:\Source\ZPEDW.EtlSharedLibrary\ZPEDW.EtlSharedLibrary.dll" "C:\Windows\Microsoft.NET\Framework\v4.0.30319\ZPEDW.EtlSharedLibrary.dll" "C:\Program Files (x86)\Microsoft SDKs\Windows\v8.1A\bin\NETFX 4.5.1 Tools\gacutil.exe" -i C:\Source\ZPEDW.EtlSharedLibrary\ZPEDW.EtlSharedLibrary.dll pause I'm actually not sure if there is a specific reason to copy the dll to the .Net folder. Maybe it easier to see in VS. Maybe it's just a common location.
The 1NF does not require a primary key to be defined. From Wikipedia: &gt; A relation is in first normal form if and only if the domain of each attribute contains only atomic (indivisible) values, and the value of each attribute contains only a single value from that domain.
thanks! i'll try this and report back
Same amount of data, approximately. Rows/columns/numbers of tables/cell values. And physical storage size of both databases is ~ 5.3gb. 
Fragmentation can potentially double required reads. The data being available in the cache can make a big difference. Clustered column stores can be slower for SELECT * queries. Sometimes a query will trigger statistics to refresh, delaying execution significantly. Blocking of course, but you've ruled that out. None of those explains such a simple query taking 11 seconds consistently. If the databases on on the same instance, I have no explanation. If they are different instances on the same server, it's possible that one instance is starving the other for resources.
You might want to check the indexes on the tables you are querying. Also your IT department may have given preferential bandwidth/spooling space to one DB.
Good answers. I‚Äôll throw in another one. Possibly different hardware (disk drives). This is if the DBs are on different disk drives and maybe one is SSD. Update stats probably wouldn‚Äôt hurt either with the index fixes.
Firstly, If you're looking for a career in pure SQL, you'll never find it. That said, knowing SQL opens a bunch of doors. Database Admin, Data/Software Engineer and other careers related to database management if you prefer working on the software side \(which, I'm assuming you are\). [Indeed.com](https://Indeed.com) provides a good start. Just search SQL and you'll see a BUNCH of openings. SQL is but a tool really for manipulating and accessing data.
I'm not a big fan of Java but I did find python tolerable. Would python pair well with sql? 
As others have said, you will never be able to get a job working with only SQL. That said, if you can pair that knowledge with something like Visual Studio, that may be a great asset. I've been learning SQL on the job and the guy who I work closest with is like a SQL wizard but he pairs his knowledge with Visual Studio and god knows what other tools to be one of the most essential people in our ~120 man team. Just knowing a vast amount of SQL is a great start though.
Any other skills I should work on with sql? I believe I've heard python gets used with data analysis lot,is that true? 
You can get a job with SQL being the primary skill you bring to the table. Of course it helps to know more but you gotta start somewhere. I've made a career off of t-sql. It can take you far. Later you can add python or C# if you want. If I had to pick a simple useful skill to package with SQL on Windows it would be PowerShell. If Oracle, DB2, etc. then you'll be *nix based bash. Look for jobs with SQL in the description. For entry level you might look for: business analyst, systems analysis, programmer, DBA, anything BI (Business Intelligence) related, etc. I use Dice.com for tech jobs. Good luck!
I am 100% the wrong person to ask. My skills with SQL are pretty basic. My only leg up on some people around here is that I actually see it used in action at my place of work, so I know it has worth and is a worthwhile endeavor to learn. I hope someone else could answer other programs to learn in tandem with SQL for you because I'm loving my time working in and learning SQL and would love to make a career working with it like you would.
Python and R are heavily used for data science, but you learn it all at the same time. Learning python independently doesn't particularly help.
I'm going to disagree with the others here. I'm a one trick pony. I write SQL. That's about all I do. Sub parts of that are that I do SSIS and data warehousing. Most places don't care about certifications. Find a recruiter/agency like Robert Half. Work with them find you a job. They get paid when you get paid. Look for terms like data analyst, ETL
Same here. I write reports all day in a hospital/clinical environment. I‚Äôve never done any SSIS or warehousing. 
Here are a few positions I‚Äôve done some research on hiring for my team. The salaries indicated are (from my quick researching) the averages for Southeast USA. YMMV. Data warehouse developer 70-90K Data integration developer 60-80K database Developer 60-100K data analytics developer 50-70K Business Intelligence Analyst 60-85K Data/Database analyst 50-75K ETL Administrator 55-80K database engineer 65-100K
There are a TON of great SQL-only jobs. I‚Äôm only scratching the surface but...... (Business) Data Analyst is a solid role where you only need to know a bit of SQL to get an entry level job. Every Enterprise needs SQL Data Analysts to write reports and make data visualizations etc.. If you have a solid grasp of pure SQL you can pretty much pick up any specific dialect easily (Oracle, MSSQL, PostgreSQL, MySQL, MariaDB, etc...) If you wanna dig deeper into a SQL procedural language you can do some more hardcore server-side development. I am a full-time database developer myself using PL/SQL (Oracle), but there‚Äôs tons of folks that do the same with T-SQL (Microsoft), SQL/PSM (MySQL and others), or PL/pgSQL (Postgres). DBA‚Äôs similarly know a lot of SQL, though they will typically know a scripting like or two like Bash, Python, Perl etc.. 
This is not at all correct. As AXISMGT said, Look at these... Data warehouse developer 70-90K Data integration developer 60-80K database Developer 60-100K data analytics developer 50-70K Business Intelligence Analyst 60-85K Data/Database analyst 50-75K ETL Administrator 55-80K database engineer 65-100K Some of these are different titles for the same job, but I personally know a handful of people in each of these roles. I am a database developer myself and have been on a team with senior ETL developers and there‚Äôs over 100 pure SQL data analysts at my organization. And it‚Äôs good money! These salaries are not exaggerated. As a standard Dev you will never find a job with only SQL, but as a data [insert title here ] you will definitely be able to be a SQL-only person with use of visualization tools like Tableau, SSRS, etc... 
I can chime in as I just recently got hired as a Management Information Systems graduate. The main entry-levels positions you are going to look for are Data/Business/System/Technical/BI Analyst. I just graduated back in December and I can tell you that this field is very competitive. Just be confident and know your skills. That being said, if you are just learning sql, sqlcourse.com is a very beginner/fun place to start. I would recommend you supplement learning SQL with other Data-related skills such as R or Python. If you have questions, just ask them here and someone can help! Good luck!
This exactly. You can be ‚Äúprimarily‚Äù SQL, but to get ahead you should learn another language. R/Python for data analytics, data engineering, data science, etc.. C# for MSSQL in a .NET environment definitely. There‚Äôs a lot of opportunity out there!
Well, yeah. Tableau and SSRS are both different programs. I'm designing an SSIS package with SQL right now, but SQL skills alone gives me no insight on how to develop an SSIS package. I needed additional training in Visual Studio to do that, which I specifically mentioned.
Adding onto this... some Tech Support jobs also use SQL (such as mine). 
I'm certainly not a competitive applicant. I will only have a bs. Should I think about a different direction or is a non competitive bs in information systems still OK for finding a job in this area? 
So you're learning C#? Visual Studio is just an IDE bro
Yeah python can be used to build pretty much any piece of software 
Honestly, in my opinion, certifications for SQL don't do much for you at all. They might do a bit for DBAs, but they do nothing for SQL developers, so don't bother. Additionally, SQL doesn't necessarily have anything to do with Excel macros. Excel macros are written in VBA and don't always include SQL. On the other hand, I applaud you for pursuing a major in the business college with a minor in Comp Sci. Those are two very different realms.
Forget Excel macros. I say that as someone who began their career programming macros. Learn SQL and never look back.
If I'm planning on becoming a financial analyst when I graduate would Microsft start supporting SQL/ Java Script to make the language inside excel more powerful. I'm basically trying to become the best at excel as possible in the coding sense. 
Yes--correct, this is the setup that I have currently if I am understanding your description. What I am not clear on is the best way to populate these types of tables..just manually? https://imgur.com/a/wdRS7s3 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ci9TVUW.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dxlx36l) 
SSIS is just a method through which to implement SQL and it is a specific use case, that being of ETL. There are plenty of jobs that require only and IDE, and whatever variant of SQL. I use only SQL in my current job, and we use VS literally for TFS and that's it because that combo solves the problems we are trying to solve. 
I thought that's what a linking table solved--necessitating only updating once if a new genre is added to a given artist..?
Well you have a few options: 1) Business Intelligence. Learn some Tableau or SSRS, some Pivot Tables / OLAP, and read up on data warehouses 2) DBA. Pick a database brand. Learn all the administrative stuff. Backups, restores, indexing, performance, DDL. This route takes a while 3) Database Application Developer. Learn a web back end, and use a database that way. This is a sneaky way into becoming a DBA 4) Database Systems Programmer. Pick an open source database, like PostgreSQL, and start hacking it like Julian Assange
I have not once met an Excel expert in business. All the experts do their expert thing using something better than Excel macros. For data analysis, the SQL expert will outperform the Excel expert all day long. For visualization, the Tableau expert will outperform the Excel expert all day long. For little apps, the programmer will outperform the Excel expert all day long.
So basically for hard core data analysis learn SQL and ill be gold 
Yep. :)
Ah, that makes sense for what you need. Didn't realize that songs could be categorized into multiple genres. Your diagram makes sense, but AudioArtists_Genres should have no keys since neither the ArtistID nor the GenreID originate from that table. Here's another article to reference what you seem to be looking for: https://en.wikipedia.org/wiki/Associative_entity
**Associative entity** An associative entity is a term used in relational and entity‚Äìrelationship theory. A relational database requires the implementation of a base relation (or base table) to resolve many-to-many relationships. A base relation representing this kind of entity is called, informally, an "associative table" (though it is a table like any other). As mentioned above, associative entities are implemented in a database structure using associative tables, which are tables that can contain references to columns from the same or different database tables within the same database. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Try asking this in r/businessintelligence I think you‚Äôll get so good information there. 
I would highly recommend getting MCSA certified. It opened up a lot of doors for me and I came from an engineering background, so I didn‚Äôt have any computing qualification. The roles you look for it up to you, but search for junior + sql. They‚Äôll likely be a mix of administrator and developer jobs. Another reason to get that certificate is if the company that hires you has a Microsoft Partnership or is aiming for one, then your qualification counts towards it. 
&gt; still OK for finding a job in this area? Define "area": Skill wise - You should be able to find a job with your degree. It may not be the highest paying, but you be working in the data space in some capacity. Geographically - You may or may not have to find a better market for your skill set, depending on your goals. 
I'm guessing you'll probably find it's either a missing index, or poor statistics. Some stuff you can do to visualise the difference Check your Execution plans, do they look the same, is something different? To do this in the SSMS query window, press Ctrl+M to enable actual execution plan, and then run the query, you'll get an additional tab which shows how SQL completed your request. Another thing you can do is to add SET STATISTICS IO ON; SET STATISTICS TIME ON; at the top of your query, this will show how many pages were read, and how much CPU time was spent on the query. 
That‚Äôs not a bad move. Thank you!
Just fyi, the word you want is "eluded". 
Thanks
Yes I've used access. The program is absolute trash. 
`Field - 20000000 AS FormattedField` 
Access is good for creating relatively small local database apps. You just need to know some VBA and spend the time on forms and reports to bring it to life. 
For OP, I don't think SQL certs are the way to go. Certs at most just help you land an interview, that's the only impact I've seen from its own value. The value I received by studying for the cert improved my confidence and filled in my knowledge gaps. I would argue if you are a tsql developer (not dba), a few of the Microsoft certs can be really beneficial from the perspective of the material you need to learn and review. 
&gt; after talking to some insiders they agreed that getting a SQL certification is very powerful. Many of the "big names" in the SQL Server community (the ones you see speaking at 6+ SQL Saturdays per year, Summit, etc.) hold no certifications. Those certs prove that you can pass the test - much of what you're tested on, you actually *don't* use in most SQL Server tasks, or at least not frequently. &gt;I would mainly want to get this so I could code macros in Excel easily. A SQL Server certification will give you **zero** knowledge about coding Excel macros. If you want to learn how to write Excel macros, take an Excel class and get *that* certification.
Seconded, have been a Data/Testing Analyst for 3 years and use SQL every day (technically T-SQL but pretty much the same thing).
So Arists_Genres wouldn't have a composite key made up of those two FKs? How do I ensure each row is unique otherwise?
that is the most straightforward solution ever, in my mind i was contemplating left right charindex and all that stuff.
It's actually not quite that straight forward. I am making the assumption everything is a 10 digit number that begins with 2. A more robust solution for the last X digits would be: Field % /*Mod*/ 100000 AS Last5Digits Where 100000 has as many zeros as you want digits.
Do you have some null names? that may not match either case.
Ohh thanks! I feel really stupid now :)
If you want to spend your live writing VBA, maybe. But who's "certifying" you? Unless it's Microsoft, I'm not putting much stock in it. BTW, VBA sounds like it's going to be phased out in favor of [JavaScript-based Apps for Office](https://technet.microsoft.com/en-us/library/jj219429.aspx). Maybe you could get out ahead of the curve on that one.
It's Microsoft approved through certiport. Only one that is Microsoft approved. And yes I was thinking about that too I'll def look into it. 
Sorry, I misunderstood. You have the correct idea already. I didn't realize that the foreign keys could make up a composite key!
I think I am in over my head here. I have been looking through your SQL &amp; I can't grasp the overview of it fully. I'm not sure what the cte references are. I was thinking I could "loop" through the output of the order once I derive the *[total weight]* and *[total pallets]* for each item, then add each item to a new order until nothing will be capable. Basically cutting &amp; pasting from the original (using insert &amp; update statements to move). I'm assuming my thought process is totally naive and needs a lot of work though?
Change the field name to rent_sale Don't think you can use a / in a querie
Tried it. No good. Thanks anyway 
Remove the filter, just join and do a select *
What happens if you add an ORDER BY? (timestamp/etc). Also, check if your other database has any indexes disabled.
Fragmentation can cause much worse performance than that. In the worst case, you're replacing a tree with a search function in the order of O(log(n)) to one of O(n).
Well that's pretty simple, you don't need CTEs, just a `GROUP BY`: SELECT Owner.OwnerID , Owner.LastName , COUNT(DISTINCT Ownership.PropertyID) AS NumberOfProp , SUM(Ownership.PercentOwned * Property.AppraisalValue / 100.0) AS TotalValuation FROM reddit_8dg6ft_owner Owner JOIN reddit_8dg6ft_ownership Ownership ON Ownership.OwnerID = Owner.OwnerID JOIN reddit_8dg6ft_property Property ON Property.PropertyId = Ownership.PropertyID GROUP BY Owner.OwnerId, Owner.LastName; [Working example](http://sqlfiddle.com/#!18/8b761/1) (sorry about formatting, I'm in a rush). 
I knew it was as simple as that, but couldn't figure out how to write it up. Every time I can't understand something and see how the query is actually done, it makes so much sense haha. Thanks so much!!
Good catch. I was speaking more to page fullness than fragmentation (though they go hand in hand), which if your pages are half empty your table is twice as big. Could you clarify what you mean about search function performance? I don't think fragmentation measures nor affects navigating a search tree. More that hardware is unable to take advantage of sequential reads during a scan.
 SELECT Customer.* FROM Customer INNER JOIN [Order] ON [Order].CustomerID = Customer.CustomerID AND [Order].[RENT/SALE] = 'rent'
Pretty sure the way we handle our pricing table is by doing something like this: DECLARE @OrderDate datetime = '2018-06-01' SELECT TOP 1 * FROM CurrencyLookup WHERE @OrderDate &gt;= CurrencyLookup.Date ORDER BY CurrencyLookup.Date DESC
I meant seek not search (*had binary search in my head when typing it*). 
 WITH cteN1000 AS (SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS N FROM (VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(10)) AS n10(N) CROSS JOIN (VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(10)) AS n100(N) CROSS JOIN (VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(10)) AS n1000(N)) , cteMonth AS (SELECT DATEADD(month,n.N-1,d.MinDate) AS [Date] FROM (VALUES(CONVERT(date,'2018-01-01'),CONVERT(date,'2018-12-01'))) AS d(MinDate,MaxDate) CROSS JOIN cteN1000 AS n WHERE DATEADD(month,n.N-1,d.MinDate) &lt;= d.MaxDate) , cteCurrency AS (SELECT * FROM (VALUES('EURO',CONVERT(date,'2018-01-01'),CONVERT(money,1.01)) ,('EURO',CONVERT(date,'2018-03-01'),CONVERT(money,1.02)) ,('EURO',CONVERT(date,'2018-07-01'),CONVERT(money,1.03)) ,('YEN',CONVERT(date,'2018-01-01'),CONVERT(money,10100.0)) ,('YEN',CONVERT(date,'2018-04-01'),CONVERT(money,10200.0)) ,('YEN',CONVERT(date,'2018-10-01'),CONVERT(money,10300.0))) AS c(Currency,[Date],USExchange)) SELECT ct.Currency , m.[Date] , CONVERT(money,STUFF(MAX(CONVERT(char(8),m.[Date],112)+CONVERT(varchar(10),c.USExchange)) OVER (PARTITION BY ct.Currency ORDER BY m.[Date] ROWS UNBOUNDED PRECEDING) ,1,8,'')) AS USExchange FROM cteMonth AS m CROSS JOIN (SELECT DISTINCT Currency FROM cteCurrency) AS ct LEFT OUTER JOIN cteCurrency AS c ON c.[Date] = m.[Date] AND c.Currency = ct.Currency;
Exchange rate once per month, really? You can get hourly for a single currency for free from https://currencylayer.com/product 
&gt; And say you wanted to write a process where any missing month is filled in you are solving the wrong problem the real issue here is simply to find the exchange rate for any given date see /u/ihaxr's solution for the approach 
I am completely aware that I am solving the wrong problem. It isn't an option for other reasons out of my control.
I am aware.
I tried it but still no good. I'm starting to think that it's Access causing the problem. Thanks for trying.
I have it doing what I want. This is going to have really poor performance in the next phase but it will do for now: WITH CTE AS ( SELECT X.MthYearApproved , X.CURR_CODE , Y.USDXUNIT AS 'Deconvert' , Z.USDXUNIT AS 'Reconvert' , ROW_NUMBER() OVER(PARTITION BY X.CURR_CODE ORDER BY X.MthYearApproved ASC) AS 'RN' FROM ( SELECT MthYearApproved , CURR_CODE FROM ( SELECT DISTINCT CAST(MthYearApproved AS date) AS 'MthYearApproved' FROM Table1 ) A CROSS JOIN ( SELECT DISTINCT CURR_CODE FROM Table2 ) B ) X INNER JOIN ( SELECT CURR_CODE , USDXUNIT FROM Table2 WHERE CURR_DATE = (SELECT MAX(CURR_DATE) FROM Table2) ) Y ON Y.CURR_CODE = X.CURR_CODE FULL OUTER JOIN ( SELECT * FROM ( SELECT CAST(DATEADD(mm, DATEDIFF(month, 0, CURR_DATE), 0) AS date) AS 'CURR_DATE' , USDXUNIT , CURR_CODE , ROW_NUMBER() OVER(PARTITION BY DATEADD(mm, DATEDIFF(mm, 0, CAST(CURR_DATE AS date)), 0), CURR_CODE ORDER BY CURR_DATE DESC) AS 'RN' FROM Table2 ) D WHERE RN = 1 ) Z ON Z.CURR_CODE = X.CURR_CODE AND Z.CURR_DATE = X.MthYearApproved ), CTE2 AS ( SELECT * FROM CTE WHERE Reconvert IS NOT NULL ), CTE3 AS ( SELECT A.MthYearApproved , A.CURR_CODE , A.Deconvert , A.Reconvert , B.Reconvert AS 'ReReconvert' , A.RN , B.RN AS 'RN2' , ROW_NUMBER() OVER(PARTITION BY A.MthYearApproved, A.CURR_CODE ORDER BY B.RN ASC) AS 'RN3' FROM CTE A CROSS JOIN CTE2 B WHERE A.MthYearApproved &lt;= B.MthYearApproved AND A.CURR_CODE = B.CURR_CODE ) SELECT MthYearApproved , CURR_CODE , Deconvert , CASE WHEN Reconvert IS NULL THEN ReReconvert ELSE Reconvert END AS 'Reconvert' FROM CTE3 WHERE RN3 = 1
&gt; still no good sorry, not familiar with that error message try it without `AND [Order].[RENT/SALE] = 'rent'` -- this should tell you lots
Apologies. Just wanted to see if anyone had a progressive list what they started out from and advanced to. Is this all entry level material?
1. I am really sorry, that is awful news. 2. Go find meetups and networking events now, right now. Start going. 3. Maybe start talking to recruiters, it sounds like you want a break in between. I'd like to stress that it's easier to find a job when you have a job. 4. Start googling and talking to the people you networked with. What flavor of SQL is predominant? What systems are being used? What are the jobs you will be applying for locally and what does the scene look like? Use that to start. If I just started learning DB2 and mastered it, I'd have been so screwed. There are NO DB2 jobs for 50 miles of my place. Typically speaking, MySQL, Oracle, and SQL Server are the top three, still do your research. 5. Once you pick a flavor, you stick to that flavor. (Initially, there is a lot to learn.) Now once you've done 1-5 and have some knowledge and experience, form some good questions and what did and did not work well for your learning and what was missing. 
Thank you. This is very useful. That's why I wanted to point out that I have zero knowledge. I don't know what I don't know so I have no idea where to start. Knew nothing about DB2 other irrelevant parts of SQL. Well, sure it's not completely irrelevant but maybe not useful for someone in my position and at this time. 
 DECLARE @table TABLE ( letter VARCHAR(1) ,number INT ); INSERT INTO @table VALUES ('A',1),('A',2),('B',3),('B',4),('C',5),('C',NULL); SELECT DISTINCT LETTER FROM @Table WHERE letter NOT IN ( SELECT DISTINCT LETTER FROM @table WHERE NUMBER IS NULL );
Depends on what you need it for, maybe a function would suffice? CREATE FUNCTION fnCurrencyExchangeForDate ( @currency CHAR(3), @date DATETIME ) RETURNS TABLE AS RETURN ( SELECT TOP 1 usexchange , date FROM reddit_8dgvmn WHERE date &lt;= @date AND currency = @currency ORDER BY date DESC ) Given an index on the table it performs an index seek for each row with a `CROSS APPLY`. 
Ah that makes sense. That seems so simple, I just didn't think about it like that. Thanks.
Personally I liked sqlbolt to get a taste of SQL, then you can go to SQL Zoo or the interactive textbook which provides alot of problems to practice and get better. 
Hey, ConfusedAcademic1123, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Did you do steps 2, 3, or 4 yet? Because that's the first things you need to do before you go any further with learning SQL if you want to get a job in it. Now the next process piece, you are in full on vacuum mode. You should hit a bunch of the learning material in the side bar and try to learn as much as you can. It won't all make sense but you'll get some good exposure. Once you've met some folks, watched some videos / read articles, I'd say you can begin a light learning path. https://softwareengineering.stackexchange.com/a/181657/256008
So counting a column doesn't include null values? That works. I was trying to think of an aggregate function I could use, but the closest I could think of is min() would work if min(number) would pick null as the smallest value. I figured sum() or count() would just treat null as zero. Thanks.
And now watch 20 other people write the query in a different way, some better and some worse. 
&gt; So counting a column doesn't include null values? all aggregate functions work that way -- the only exception is COUNT(*) which doesn't count values, it counts rows instead
&gt; where any missing month is filled in and takes the oldest record available with exchangeRates (currency, date, usexchange) as ( select 'EURO', '2018-01-01', 1.01 union all select 'EURO', '2018-03-01', 1.02 union all select 'EURO', '2018-07-01', 1.03 ), dateList (dateOfExchange) as ( select cast('2018-01-01' as date) union all select dateadd(mm, 1, dateOfExchange) from dateList where dateOfExchange &lt; '2018-12-01' ) select e.currency, d.dateOfExchange date, e.usexchange from dateList d cross apply ( select top(1) er.currency, er.usexchange from exchangeRates er where er.date &lt;= d.dateOfExchange order by er.date desc ) e 
I agree with u/mercyandgrace. Skill wise you will be fine, but it is a good thing you mentioned area, as many of these SQL positions will be located in more popular metroplexes (SF,LA,DAL,CHI). It might be more suitable to find an internship to acquire experience if you currently have none. I can tell you for a fact that as a Database Analyst Intern, while I did not earn a lot, my learning and knowledge paid dividends.
Absolutely. But a huge part of the point was to provide people \(especially people not really familiar with relational math\) a *method* by which they can work out many kinds of clauses. You're also correct about the nullability issue, and I might add a footnote to pay attention to it. But again, focusing mostly on the method.
So, back to my original question..when one has a junction/linking table, what's the best way to update it efficiently?
I just finished the sql boot amp course on Udemy. Jose Portilla‚Äôs course. I had zero knowledge of SQL and now feel comfortable with basic queries, joins, and databases overall. I have tons to learn still but that was a good starter. W3schools has all the same info in text form if you can like to read to teach yourself rather than hear and see it done. Good luck on the job hunt!
Www.sqlzoo.net was very helpful to me. Starts very basic and helps to gain a better understanding of database structures.
Feel sorry, man. Good choice. According to the last [Stack Overflow survey](https://insights.stackoverflow.com/survey/2018/#technology) SQL is one of the most popular language. Should be enough in 3 months to still have good knowledge. SQL is a declarative programming language. Unlike Java or C++, you do not need to tell the database how to get the records but what record you'd to get. This means that SQL can be grasp quite quickly. Go for practice oriented tutorial. The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. 
First create the missing rows (dates) using a numbers table. Then fill in the missing exchange rates using a subquery.
Using a date table the first thing I would do is insert the missing dates. Then I‚Äôd look into the LEAD/LAG function if you are using from SQL Server 2012 onwards. There may be something that can be done with that. 
has anyone tried the measureup practice exam for this cert?
I'd steer clear of W3 Schools. Whilst it hits high in the Google results it is almost universally abhorred by It Pros due to the bad practices it pedals. If I receive a CV with any reference to W3 Schools training / certification it is a black mark. [Stack Overflow](https://stackoverflow.com/questions/tagged/sql) is a much better option for an online resource / reference.
Well that's interesting to know. Thanks for this. I've been dabbling in sql for a little over a year now, was about to try and get a certificate from w3. 
Makes sense. Min,max,sum clearly don't use null, count only counts rows that have a value for that column unless you use count(*). Avg means sum(column)/count(column), not sum(column)/count(*).
&gt; count only counts rows that have a value for that column COUNT only counts ~values~ that have a value for that column ;o)
I'd definitely run both and see what the query plan is for them. Set statistics io and time on and run em both. See if either could use an index and which is the most efficient.
set statistics io on and set statistics time on is pretty cool, I don't usually look at that, but looks like a good way to judge query efficiency. I'm at the point where I'm just happy to get the result I want rather than be concerned with speed, but it's probably still a good thing to keep in mind. The elapsed time actually seems to vary, it's not always the same if you run it again. Logical reads stays consistent though, I assume lower is better. SELECT letter FROM test GROUP BY letter HAVING COUNT(number) = COUNT(*) seems to be the most efficient method, it has one logical read versus 7 and 13 in the other two.
I think you just need to compare prices during the insert. create trigger TR_Title_Update ON Title for Update as if @@RowCount &gt; 0 and update(SuggestedPrice) BEGIN insert into LogPriceChange(ChangeDateTime, ISBN, OldPrice, NewPrice) select getdate(), inserted.ISBN, deleted.SuggestedPrice, Inserted.SuggestedPrice from inserted inner join deleted ON Inserted.ISBN = deleted.ISBN where deleted.SuggestedPrice &lt;&gt; Inserted.SuggestedPrice END return
&gt; The elapsed time actually seems to vary, it's not always the same if you run it again. Logical reads stays consistent though, I assume lower is better. The ideal way to test is to do a restore of your prod environment to another server and then run the query after flushing cache and memory. After the first run your server generates plans and puts pages into memory, so it's good to do a few runs with "cold" runs where plans and pages aren't in memory. Then it's good to do a few "warm" runs where it is in memory for comparison.
&gt; seems to be the most efficient method thank you
Check out The Accidental DBA, it has some good info as well.
You will get as many displaynames as exist related to your: WI.WorkItemDimKey = AssignedToUser.WorkItemDimKey In other words, if your AssignedToUser.WorkItemDimKey has multiple records associated with WI.WorkItemDimKey, then you'll get that many dupliated rows.
Could you post dummy results? From what you're saying, it seems like one incident ID can have multiple affected and assigned users? 
&gt; However, I am getting duplicate listings for "Incident.ID". This occurs when the "AssignedUserInfo.DisplayName" changes from analyst to analyst, or similarly when "AffectedUserInfo.DisplayName" changes. I don't understand why both of them would show in one query, though. because you put them into a SELECT DISTINCT clause not sure if you realize, but DISTINCT applies to all columns selected so if they change, then yeah, the different changes will all show separately if you want only one row per incident returned, then you'll have to do something like this -- SELECT Incident.ID , DateAdd(HOUR,-8,Incident.ResolvedDate) AS ResolvedDate , DATEADD(hh, DATEDIFF(hh, getutcdate(), getdate()), incident.CreatedDate) AS CreatedDate , MAX(AffectedUserInfo.DisplayName) AS [Affected USER] /* pick one */ , Incident.Title , COALESCE(IncidentQueue.IncidentTierQueuesValue,'Unassigned') AS [Support GROUP] , MAX(AssignedUserInfo.DisplayName) AS [Assigned USER] /* pick one */ FROM ... GROUP BY Incident.ID , ResolvedDate , CreatedDate , Incident.Title , [Support GROUP] 
Are you looking at ADB a role? I would start by downloading sequel server and Oracle ( Any Oracle sequel developer)Both are free to download. The Wrox Series of books for sequel server are awesome. Next is the versions you wanna take a look at. Right now sequel server 12 is the industry standard. As for Oracle it all depends. Another field you can look at is bi analytics. Something like Tableau, Oracle, ect. Take a look at the Gartner Magic square to see what is being used. If you have any other questions let me know. I'm what's known as a generalist, I've been the IT field now for 18 years. 
Yes perfect! Thank you! I was being thick since in my class we have been doing everything with (If exists). Sometimes I get too zeroed in on one way!
Noted, thank you! Its the lesson we are learning in my Database class so I don't have a choice at the moment. I've been seeing that a lot in my searches for this answer via google. I'm using MS SQL 2012
Umm... Reporting Services?
Wait.... This actually works! Mostly. So I originally switched to distinct because I was getting a lot of duplicate rows, probably due to sloppy joining. It reduced some of the duplicates but not all. So after populating the from field of your query, duplicates are gone! However, (and I hope this makes sense) if the assigned or affected user changes, the query shows what the value originally was, not what it is currently. I can get it to show the current value if I add "WHERE not like" and exclude what the value of the original affected user was, for instance. Any idea why that might be happen? 
&gt; Any idea why that might be happen? because i used MAX() which simply returns the ~higher~ display names, not the earliest or latest or whatever to get the latest, you''ll need a **window function**
&gt; They are names, not numerical values. names can be sorted, right? just like numbers so MIN(name) is the lowest name, and MAX(name) is the highest name
Try CONVERT(VARCHAR(MAX), 127, GETDATE()). Probably got that wrong off the top of my head, but should give you an ISO standard string that Excel should be able to parse.
To flesh out what someone else said..... Go to Indeed.com, search for SQL, read through a bunch of listings and find out what DB's they mention. MS SQL uses the Transact SQL (T SQL) dialect Oracle uses PL/SQL I believe MySQL and PostGRESQL use PL/SQL, or close variants of it. You can download 100% free and 100% legit copies of all of those databases. (For MS SQL get the Developer edition and track down the Northwinds database). Figure out which one people near you are using, and get a copy. Your local library should have a lot of SQL books. They may not have the most recent editions, don't worry about that for now. Check some out and start working through one you like. Also, don't worry about "picking the right dialect". It's pretty easy to pick up a new one. They're about 95% identical. And learn about stored procedures. 
Thanks. I'm having trouble getting this to run. Do you know where my column needs to be identified in your statement? I couldn't get the script to take when I included the GETDATE()). This runs, but results in my same issue: CONVERT(VARCHAR(MAX), table, 127) Thanks again.
Gotcha - I suppose I was over thinking that bit. I think I'm going to take the lazy way out and explicitly define what I don't want coming up with "NOT LIKE" statements. There's only a handful that it could possibly be so it should work in this scenario, though definitely not best practice. Thank you for all your help! 
Sorry, it's late here and I'm just on mobile. Look up msdn for "tsql convert datetime". I probably have the params around the wrong way. Replace GETDATE() with your column name.
That did the trick. Never heard of "STUFF" before. Thought that was a joke! Thanks a lot :D
A nice elegant solution. 1 table hit, no nested queries,CTEs or temp tables. I'd be surprised if anyone can find something that is faster performance than this. 
https://docs.aws.amazon.com/redshift/latest/dg/r_Numeric_formating.html literally the second result from googling "redshift add commas to INT". 
Certificates... MCSE, MCSA if your going the microsoft route. What do you want to do 5-10 years down the line? DBA? Data analyst? T-SQL Programmer? I work heavily with SQL and my route with no college degree was functional tester &gt; technical tester (started with C# and SQL here) &gt; 3rd line support &gt; a mix between developer and 3rd line and my current employer is wanting to make me a DBA. If you can get a support role you will be exposed to both customers/users to understand what they need (very important) and hopefully some coding stuff. You may need to work your way up through 1st/2nd line but if you are upfront in the interview about wanting to work with SQL and they say no then ask if that would change if you proved yourself and how long that would take. A company with a lot of reporting is likely to be SQL heavy. This website was recommended to me by another redditor and is spot on https://www.brentozar.com/ Sign up to some courses and do the exercises. Sign up for one of the free courses, get a local SQL install with maybe the northwinds database and start mucking around. Re-write stuff to make it faster. See if you can think up some cool code and post it somewhere online and link it on your CV. If your going to do that it needs to be good or interesting code though. Maybe a naughts and crosses game that you move the pieces by calling a proc... I might have a crack at that one. Finally, get some ability in a non-database language. In windows your options would be powershell, C# or VB. SQL can do a lot of things but some things it cannot and some things it can do but its far easier and quicker to do with another tool. Understanding the areas around it will only add to your skills.
You don't think I saw that? Of course I did. Please, do not think I'm that fucking stupid where I wouldn't Google search this first. ```SELECT CAST('3621731641' AS BIGINT)``` Returns the same thing with no commas inserted. 
Don't do it in your database at all. Save it for the presentation layer, and use the thousands separator character that's used by the user's selected regional settings (using platform-native number to string formatting functions). Example (PowerShell): [int]$i = 123456789; $i.ToString("N",[System.Globalization.CultureInfo]::CreateSpecificCulture("sv-SE")) 123 456 789,00 $i.ToString("N",[System.Globalization.CultureInfo]::CreateSpecificCulture("en-GB")) 123,456,789.00
Thank you my good sir 
Could you post the execution plans? It's possible that the query is under/over estimating the rows being accessed, and some other kind of hint (like `OPTION (FAST XXXX)`) might convince it that the table index is preferable.
"STUFF" is magical. You can have lots of fun with "STUFF".
Thanks for review. Never used W3 but will be more cognizant of the bad practices taught by these schools. 
DB Browser is real quick and easy. Obviously not as in depth as others but it‚Äôs free so that‚Äôs cool. 
Add SQL flavor and code to your post. 
Do you regularly update your statistics ?
How can we properly berate you without knowing what you did wrong. 
It depends on where your data is stored currently. Do you have an existing table that has the artists and genres already listed in a Songs table or something similar as /u/maunaloona suggested? If so you could do something like this with a temp table ... --get each unique pairing of artistid and genreid and insert the records into a temp table SELECT ArtistID, GenreID INTO #temp FROM Songs GROUP BY ArtistID, GenreID; --then insert the records from the temp table INSERT INTO MusicArtists_Genres SELECT * FROM #temp Or if your data is in something like an Excel spreadsheet or a text file, you can use the import wizard to upload it into the database, then manipulate it as needed from there.
Where is your data coming from? If it's from some external source then you'll need to match genres and artists to what's in your db and then populate the linking table.
SqlStudio is good
I am using Transcender. The site has changed i guess. Good stuff for learning. Kinda expensive though https://www.kaplanittraining.com/certification/microsoft?examlisttaxonomy=categories&amp;examlistpropertyName=category&amp;examlisttaxon=%2fcertification%2fmicrosoft%2fsql-server
I‚Äôve used visual studio/reporting services a bit but wasn‚Äôt sure how it stacked when compared to other services 
It's currently stored in two tables for artists and genres, respectively. This all lived in a spreadsheet but I've done a one time import to populate the main tables. I'm trying to ascertain the best way to continually add a few rows of data over time outside of just manual INSERT statements. A temp table sounds reasonable. I'm thinking of using an Access front end too as a more user friendly means of input. Thanks for your help.
Select * from tableyouwant. I imagine What your doing now is right clicking the table name and selecting top 1000. 
What you want to do would be way easier with a normalized table.
There is a distinct difference between writing a difficult query, and hacking together some mess because a table doesn't even follow first normal form. I mean, it might be difficult to nail some boards down with an ice pick, but if you do it, it doesn't mean you're a good carpenter. So I don't fault you for moving it into python.
In MSSQL at least, I think you could accomplish this by creating a string splitter function then using a CROSS APPLY. Also worth mentioning that it would be much easier if there was a separate table where you have a row for each status and date. 
thanks
Without the query, and the table / view definition, as well as the statistics on those indexes and the execution plan, all we can do is wild guessing. Things I'm guessing : - parameter sniffing - bad statistic (i'd actually look at them, and not blindly update with fullscan) - bad statistics on some other tables you are joining too, have a look at estimated vs actual row counts 
And graduation year = 2018
How are values in an integer type column pipe separated?
All languages I've used have some sort of DB abstraction layer. Including VBA. If you want to do mySQL imports and stuff visually, use mysql workbench. But SQL is for working with data in databases. It's great for that. For what you're doing, postgres sounds like a better fit as it is much more advanced.
I don't know what your concerns are with bulding insert statements as a string. I think anyone who has interacted with a SQL database programmatically has done this at one point or another. I did a project one time where I took JSON formatted data from a REST api and pushed it into a SQL database. I built each tables insert individually. I probably could have turned what I had into an abstracted table loader...but for the project I was doing it wasn't worth it. Here's a link to the GitHub: https://github.com/trooper454/PS2BI/tree/master/ps2bi_etl/extraction Now Im not scrubbing the data coming in, but I knew it was coming from a sanitized source. If you are worried about the quality of the data coming in, just load it to a table where every data type is varchar(1000), put every loaded value in single quotes, and escape single quote by doubling it (i.e. I would insert the text string Frank's dog as 'Frank''s dog' or Robert 'DROP TABLE STUDENTS;-- as 'Robert ''DROP TABLE STUDENTS;--' with no harm to my students table). Then insert the data from the load table to a table with proper data types. If you wanted to get fancy you could use a cursor to insert rows in the normalized table one at a time and use SIGNAL/RESIGNAL to catch error rows and put them in a "dirty data" table for manual correction/review. A lot of what we are talking about here is ETL processing, or extract, transform, load. It's how data is acquired, sanitized, and combined in the business intelligence/data warehousing world. If you are looking for more detailed info I'd look into ETL tutorials. Fair warning though 99% of them will be about database to database transfers. "ETL from flat file" might be a good place to start searching.
I've used ORM in C# before (LINQ specifically). Maybe it's just me but I found it more annoying than just building SQL strings. 
it would look something like this... select s.student_id as 'Student ID', s.enrollment as 'Enrolled', t.styleofmusic as 'Music Style' FROM dbo.student as s INNER JOIN dbo.teacher as t ON s.studentid = t.studentid As you may notice, that doesn't work. The tables don't have related data. Do you have a column that links the information between the two? If you do, then the inner join will connect those for you. 
1- We don't know what flavor of sql you're using. 2- Don't use natural joins. Use inner/left/right and specify columns to join on. 3 - How can you join on teachers if both tables don't have a teacher columns? 4- `WHERE`clause should be after join
Assumptions: * teacher is a table with some unique identifying column such as teacher_id * student is a table with some unique identifying column such as student_id * band is a table with some unique identifying column such as band_id * every student has at least one teacher * every student is associated with at least one band * every teacher has at last one student * style of music is identified as 'genre' and is populated in the band table. Other notes: Converted string matching to lowercase because without seeing the data I'm not sure what case it is. Strings are case sensitive so this removes the case sensitivity. Looks like you're using Oracle, need single quotes around your % wildcards. When you join these tables, you need to join on columns that have alike fields that represent uniqueness in each table, such as an ID. So you need to replace the bracketed text with whatever that colmun is for each table respectively. Think primary keys and foreign keys. SELECT student.name, teacher.name, band.genre FROM student INNER JOIN teachers ON student.student_id = teacher.teacher_id INNER JOIN band ON student.student_id = band.band_id WHERE lower(band.genre) LIKE '%rock%' AND lower(student.enrollment) = 'yes'; 
I've used ORMs in PHP, perl, ruby, and I think Java. What it really comes down to is that the ORM has a distinct methodology and set of conventions. If you are happy working within those guidelines, the ORM can be more productive and feel more intuitive. But if you're not aware of or don't want to follow them, it can be a lot more work to use an ORM
Lol yes. The most fun I have at work is dropping all the "important" stuff and spend a day coding, figuring out solutions to problems that no one else in the organization can do. People think I'm a wizard.
Nah don't do this.
yep. I'm glad I discovered that I like writing SQL. As long as I can figure out where the data is stored, people think I'm some kind of genie. Now if I could only teach people how to articulate what data they want to see...
Add an ORDER BY clause to your query where you cast the varchar to datetime: select taskDescription, dateCreated, userId from task order by cast(dateCreated as datetime) desc
MSSQL
Absolutely, wrenching data trends hidden in tables is one of the few things that gets me up in the morning. :) 
I'm really happy to read this. I have a very strong excel, vba, access background and just applied to a position working with sql. Wish me luck! ! ! 
ditto. and dont discount powershell
SQL is a love of mine, unfortunately I‚Äôve recently moved into a job where I‚Äôm using Salesforce all day and SOQL is horrendous and I don‚Äôt get to use SQL anymore :( *sad violin music*
&gt; Anyone just love coding in SQL all day long? I used to. However, I've written SQL every day of my life for the past sixteen years. It can get old after a while. One of the keys is to stay open to new features and syntax that are released, in addition to branching out into other languages. If you enjoy SQL, you may also enjoy R and Python.
can you give me an example of a bad practice from W3 school?
I disagree with Robert Half. They're just about the worse agency out there and they'll gauge you.
I am also interested in the answer to this question :)
I am also interested in the answer to this question :)
Every day!
why?
That's the direction I'm moving. I'm getting setup in the fundamentals of data analysis where SQL and it's tools are the bedrock then I will look to Python and R.
My code would keep you busy üòÅ
There really is no short answer. Did an Information Systems module in college which had SQL and database design as subject's, developed a flair for it straight away. As with many courses ANSI I standard SQL is used (Oracle). On graduation I pursued any career that involved SQL. Over the years, I just self taught myself the different flavors but definitely gravitated to a solid career in SQL SERVER as it's so feature rich. 
They're a big company with many branches. I'm not surprised that some places suck. My interactions with them (most of the time) we're very positive. OP, as always YMMV.
If I could just unload all of my other responsibilities I would just write SQL all day long. I'm a director now and have a couple of DBAs and a BI Analyst writing reports for me these days but I still get called in to train and take on some of the more difficult jobs. I love it for the shear joy of solving problems that I never thought I could. I swear I learn something new once a week and I still think I have a ton to learn.
Yes. I love T-SQL!
Yeah, I have a friend who told me many horrible things. ie., they price gauged him for a job position doing highly technical work, and paying him slightly above minimum wage, and lying about how they can't pay higher. In fact, they were pocketing the difference so they can make better profits. They were constantly hounding him for his network, so they can get his friends under the robert half network. after he got full time, they constantly tried to get him to quit for contractor jobs that paid way less, trying to pull him back int other RH network. they constantly lie about there being no good jobs, so they could get him to accept these crappy jobs that no smart person would take, but they want to fill these positions so they can get paid. they put fake job listings on their site, and when you say you want those, they slide you a low tier job instead. the list goes on and on
as long as you know more than the basics you are good 
I don't know SQL yet, but as an accountant I make simple reports from data, and a lot of times, people don't know what they want to see until you put some of it in front of them, and they can say "like this, but different here..."
Yes. It will order by dueDate from oldest to newest. If you want to order by dueDate from newest to oldest use the following: select dueDate, userId, description from task order by cast(dueDate as datetime) desc 
http://www.sqlservercentral.com/stairway/
I would think DROP important stuff Would lead to a rather unpleasant day at work. 
I worknim accounting but spend all my time trying to code every task I have. One day I will do no work.
Thank you, appreciate it!
Makes complete sense. Thanks for taking the time to explain.
...and I must have other issues, because this definitely works in phpmyadmin but not when I input the php script ;(
THANK YOU!!!
Avoid big data. When ur batch jobs take hour and then run, SQL gets exhausting.
Anyone using SparkSQL/HiveQL/Presto?
What have you tried and/or what error arr you getting? 
You want to find all tickets for a movie, you need to go via their sessions. A ticket has a sessionID and a session has a MovieID. You could try and join the three tables using those columns. Show your work if you need more help.
I‚Äôd love to be able to replicate SF into SQL Server but that request has been denied several times already now :( 
Yeah I love the tasks that are 100% SQL. The sad part is I don't get to spend much of my work days on them since I finish them comparatively fast. I wrote a script last week that creates tables and a procedure for archiving data (delete output into) recursively through the tables' FK relationships. This was to minimize impact of table scans and index management on standard edition (no partitioning). That thing was frikkin sweet.
me 
Yes. I started learning SQL in a major capacity only about eight months ago. It's all well and good when you're learning the easy stuff and succeeding almost every day. I thought I was a hot shot until my superiors noticed and started throwing incredibly complex 10,000 line queries at me and started expecting me to learn more complex commands, lol. I'm officially over that hump where I think I know everything to realizing I know nothing. It's daunting and I'm getting some SQL fatigue, to the point where I've been planning on taking some time off for a few weeks now just to relax. I can't wait until I understand more and feel more comfortable with more advanced commands and theories behind coding in SQL.
It's so satisfying when you realize that information and data is the real key to power, and you hold the keys to utilizing that power. It almost feels like being a supervillain. XD Do you feel this way? It's a sort of power trip in a way.
Did your education take you much beyond simple joins like this? He might be referring to how simple it is. It's like graduating with a mathematics degree and then putting 5*6=30. That said, it's a cute idea and you can do whatever you want. A lot of people who do this for a living might feel disdain for a person who thinks they are a hot shot and just received their degree. That's my feeling. But none of those people will be at your graduation so again, do what you want.
my education does take me beyond this simple join, but I'm trying to keep it simple so that my friends and family who do not know sql can understand it. And a graduation hat is only so big for me to do anything complex. I'm opened to any suggestions.
There are a few different ways to deal with this. Make an SSIS package that compares the 2 tables and adds/updates items from one to the other when out of synch. Make a SQL agent job that runs the package on a schedule of your choosing (hourly or daily) IMO the most correct approach. Have a script that does the same thing the SSIS package is doing in open query. Schedule that script in a SQL Agent job. Create a Trigger that fires when a row is added or modified from the SQL Server table to the MySQL table. IMO the worst option. I assume you can get it to work, but I bet you'll be writing open query. Most people (myself included) dislike triggers, because they sometimes don't work as expected causing your systems get out of synch.
I just noticed your query wouldn't even work lol. Not in the way I've been using SQL, anyway. I'm relatively new myself. That said, I think it accomplishes what you want to do. I say go for it.
How would you make it work?
I'm not familiar with using ":" after where, join, etc. And I believe you would have to join on a certain column from each table, you can't just join tables like that. But maybe you've been taught a different version of SQL from me. I would be really interested in hearing how I'm wrong, though.
Yeah I found a 3rd party app that did this but it did not comply with our security policy.
I have been working pretty heavy with PowerShell. When I found Invoke-SQLcmd my world changed. Now, I am not sure which I love more, PowerShell or SQL, but I do know if you put them together it is like Reese‚Äôs Peanut Butter Cups!
It's weird knowing our customers behaviour better than they know it themselves
I wish I could do SQL all day but I'm also the guy who needs to fix other's computers when they stop working. 
Kill your vendor/architect. 
I'm assuming that most of the desirable knowledge for jobs in your field would be SQL querying? If so, for Microsoft SQL (TSQL), I'd suggest starting with an interactive course e.g. CodeAcademy and then pursuing the 70-461 qualification, or whatever the most recent equivalent is. Passing that should make you very competent. It starts from the basics, and builds up to IIRC quite advanced querying. It's also an accredited qualification you can put on your CV and verify. 
Python like SQL and PowerShell for me is quickly becoming indispensable to the point that I don't think I could do my job without it.
I am understanding... to a degree. I'll usually try to at least set up the framework of a report even if for example the numbers are totally bunk. I've found most people can pretty easily set the goal posts after you've shown them the ball is in play and you can give them a clearer idea of what is being done and how it's getting finished. However, if I'm constantly having to run you down and interview your ass because you don't have a grip on how to even begin to give me enough actionable information to go to work without an interrogation, I'm going to grow to dislike you. That sort of thing is forgiven a couple times, but it's also a huge huge slam on the brakes every time we need to ask a question.
I saved the plan. Is there an attachment option?
You could share the file with dropbox.com, or share the xml on pastebin.com.
It depends on your requirements. If it can be read\-only, you can look into SQL replication. We do this in our production environment at work, and the replica server is used for all user reports. I never see more than a 10 second delay in transactions between the 2 servers \(its usually around 1\-2 seconds\). [https://docs.microsoft.com/en\-us/sql/relational\-databases/replication/sql\-server\-replication?view=sql\-server\-2017](https://docs.microsoft.com/en-us/sql/relational-databases/replication/sql-server-replication?view=sql-server-2017)
If you're using SQL Server or Oracle, you'd use **DROP CONSTRAINT** not **DROP FOREIGN KEY**.
It's definitely a power trip. Probably what people at Facebook feel with all that information. I bet the allure to use that information for nefarious or non-nefarious deeds is tempting and I I kinda feel for the people caught up in the scandal. You have that information and you want to put it to use just to see what you can do.
I thought so too. damn.. but thanks for your try! 
google the difference between single quotes and backticks very common noob mysql mixup
&gt; single quotes and backticks tried both, and double quotes: different errors, but same outcome 
In a word, no. SQL is useful, but it's like working in text files in 1997. I miss proper debugging and project management the moment I open SQL Server Management Studio. I really hope SQL Server/SSMS get some TLC soon - they're great tools, but the user experience is frustrating.
Don't start by just moving data from one system to the other whole-hog. You'll be pounding square pegs into round holes immediately, and, though you will have to do that eventually, it's a bad place to start. Essentially every data-driven system can be reasonably expected to have some kind of built-in data import and export system. Nobody in their right mind is re-keying all their data when they buy a new system, so this is more or less an essential component. The first place you should be looking is Prestashop's documentation. Generally, it's not up to the customer's systems analysts to figure out how to get the data into the new system. Instead, the new system has defined import formats, and you build views in the systems that the data is coming from to create output in the required format. A casual look at the Prestashop site shows [the user guide appears to talk about it](http://doc.prestashop.com/display/PS17/Import) and there are [numerous commercial modules for managing data import and export](https://addons.prestashop.com/en/451-data-import-export). Essentially, you'll write a query from SAGE that you use to generate a file (usually a CSV) that then gets imported into Prestashop. That way Prestashop and not you are responsible for ensuring that all the required data is all present. 
Coding in SQL is one of my favorite parts of my current job. Our reporting and visualization application sucks so bad that it makes it doubly refreshing to work in clear, deterministic SQL.
If you're accountant who makes simple reports from data, I can strongly recommend learning SQL - but I suspect that's already occurred to you, if you're here But yeah, with both reports and software development, a quick mockup can save you a lot of work
please show the exact query and the exact error message when you tried it with backticks
Save this on your phone. When starting out, it's the ultimate cheat sheet for SQL Joins. Just think of the two circles as the two tables. The part where they overlap is the stuff that matches the "ON" line, and red is what will be included https://i.stack.imgur.com/UI25E.jpg
So i have Table A that has a relation of one-to-one with table B. Table A always has values on column Val And table B only sometimes, but if table B has Values i want that to be the outcome of the query. Suggestions ? and thx in advance
Unfortunately what you're trying to do is more complex than what is expect to find in one link. I develop SQL full time. I'd expect doing one of my suggestions to take me in the 8 to 12 hour range to do. Doing cross server between database systems can get messy. How big are your tables? Is it possible to manually open the tables and just compare them and update/insert the differences?
Ty for the response, is it possible to send data from SAGE (via CSV), to prestashop database even the columns being completely different ? 
The tables are big, but most of them i dont need to work with them, the only i needed was an implementation of a two way synchronization between a Microsoft SQL server database and one MySQL database. The synchronization has to be bidirectional and whenever any table updates the other tables updates too. The only 3 tables i need to do this, is the Client, Products at this moment.
I'm thinking it was written for aesthetic understanding for their peers and family. 
lol that's what I figured after looking at it a bit more.
I'm assuming you did the select: from: where: join: etc as an aesthetic view for your peers and family so they can better understand what is on your cap. My only recommendation is to do the new form of join, not the old. SELECT STUFF FROM TABLE A INNER JOIN TABLE B ON A.ID = B.ID 
Venn diagrams should not be used to describe joins, but to describe set operations: http://sqlblog.com/blogs/dejan_sarka/archive/2014/01/10/sql-set-operators-set-really.aspx
 SELECT user_id, username, count(friendsinfo.friend1) AS friend1total, count(chatinfo.friend1) as MsgCount FROM friendsinfo RIGHT OUTER JOIN logininfo ON friendsinfo.friend1 = logininfo.user_id OR friendsinfo.friend2 = logininfo.user_id ????????? chatinfo RIGHT OUTER JOIN logininfo ON chatinfo.friend1 = logininfo.user_id OR chatinfo.friend2 = logininfo.user_id group by user_id, username
D:
But im trying to link another table (chatinfo) to logininfo table, I thought this was correct?
Dude, awesome. Thanks so much :)
Sorry, i was testing formatting. I updated the comment with my actual comment haha.
You need to put the join first, e.g., SELECT user_id, username, count(friendsinfo.friend1) AS friend1total, count(chatinfo.friend1) as MsgCount FROM friendsinfo RIGHT OUTER JOIN logininfo ON friendsinfo.friend1 = logininfo.user_id OR friendsinfo.friend2 = logininfo.user_id RIGHT OUTER JOIN chatinfo ON chatinfo.friend1 = logininfo.user_id OR chatinfo.friend2 = logininfo.user_id group by user_id, username
What I'm trying to do is to join friendsinfo and logininfo and then join chatinfo and logininfo so that friendsinfo and chatinfo have nothing to do with each other, then to return the sum of the friend1 field that appears in both friendsinfo and chatinfo tables (poor choice of column headers I admit on reflection)
Other than the weird question marks it looks like chatinfo is aliased. 
u/KrevanSerKay (Don't know if you see this post if I don't tag you xD)
Ah that makes sense if it's just to add complexity. In any case, you need to specify left, right, or inner join for your chatinfo table. That's your problem
The User_ID and Username are both in the logininfo table. To give some context im basically creating a social messaging site for a side school project just to learn some PHP SQL etc... and I'm making an admin page just to expand on my SQL knowledge and I want to be able to print for each user how many messages they have sent and received and how many friend connections/friendships with other users they have if that makes sense and I'm trying to make it complex because complexity gets marks :D
You get notifications every time someone replies to one of your comments, so I saw it :)
&gt; while you might have some legit reason for using it, I have never seen right outer join being used. Nearly everyone sticks to left or inner I only use Right Outer Join when I write the tables in the wrong order for a Left Outer Join and I'm too lazy to reverse them :) Really, it's just a matter of style whether you use left or right.
What is the purpose of count(1)?
Woops, formatting got screwed up on that post. Updated. If you want to know how many rows are in a table, you can do select count(*) from table_name What that does is it retrieves all of the fields for every row (thus the * in the parenthesis), and counts the number of rows. This lets us do stuff like select count(distinct user_id) from table_name In this case, it retrieves the user_id for every row, then does an intermediate calculation to return a count. When you don't specify any filtering behavior (like distinct), all you're looking for is the total count of rows... In that situation, do we really care about all of the data in the row? In that case, we can say "For every row, return the number 1". Don't bother loading any data off of disk, or wasting IO transferring it around. Just get '1' for every row, then tell me how many '1's you saw! -------------- In my solution above, we did an aggregation, in this case, we knew the first row would have friend1's username and user_id, and behind the scenes the 3 messages associated with his chats would be associated with that row. All we care about is the total number of those messages, so we can simply ask for how many there were associated with that specific user_id. 
That depends on what you mean by "columns completely different." Typically when two well-designed systems represent the same entity, they do so with similar information about them. Now, I have worked with SAGE products in the past, so I know that "well designed" may not be the entirely correct term for their system. However, customers should have some unique ID or account number, a name, an address, a primary contact, etc. The point is that you use the CSV to take the data from SAGE and present it to Prestashop in a format that Prestashop can understand. Your view or query in SAGE which you use to build the CSV is where you handle all the conversions you need. Mostly it depends on how your business is going to use these systems. 
In the text post it appears after a join condition but before "RIGHT OUTER JOIN", not a valid alias syntax.
SSMS is iterating over the objects returned by SMO in a particular order (that order is somewhat a black box, but it can be inferred from the resulting script) and appending the `CREATE` statements for each of those objects as it hits each. So as it walks down the heirarchy, as it finds things that can be scripted out, it generates those scripts. It's not generating the scripts in the order they were run against each object, but a default constraint (for example) may script out differently if it was part of the original `CREATE TABLE` or `ADD COLUMN` vs. an `ALTER TABLE ALTER COLUMN` where it was added.
Hahaha exactly
If you want atomic scripts for each object including dependent objects such as indexes or constraints, you can select to generate a single file for each object. If uou want, you should then be able to concatenate those files together.
This is very informative. Thanks for that. So even if it's being generated I can do a clean up of the generated file so that the original file will contain the extra constraints? Instead of following how the SMO returned it by. Or would you recommend not to change the generated file. Basically the work involves creating a new "create" sql for new devs to be able to run on their own local machine. So we just want to provide them with a script of the db that everyone else has. 
I'll have to look into that. I heard that it was a bit outdated but I imagine a lot of the base knowledge is the same. We actually have free access to that (While I still have my job lol) 
Gotta check this one out? Guessing this is paid? If so, how much? 
I appreciate it man. I'm not sure what role I am looking for. You mind if I message you sometimes with some questions? 
Thanks for this heads up. Everyone has used a variety of resources and I'm going to have to carefully look over what will be the "best" option. 
With supply chain, I believe most of it would be SQL querying. Thank you so much. I was looking for something like this as well. Just an idea on where to start ("e.g. CodeAcademy) and what I should drive towards ("Pursuing the 70-461 qualification") 
The SQL Agent service account will frequently be used by automated process to access network resources. Generally speaking, the SQL Server service account doesn't need to access network resources, but there are exceptions. I don't know about types of accounts to give a recommendation though.
Yes, its paid. They have a 3 day free trial currently so you can check out the material without a commitment. I think its around $30 to $40 per month range if i remember correctly but its been a couple years since i had a membership.
Yeah check that one out, and also check out colt Steele‚Äôs MySQL boot camp. I just started that one and while his humor is a little meh, there is humor and it feels more real. There is no dialogue other than SQL dialogue with Jose‚Äôs course. Have fun!
I have no idea honestly, the book has helped me a bit for sure, but I do a bit of everything so I haven't the proper time to dedicate to DBA business.
Doesn't matter, your rewritten script and the one SSMS generates will both have the same effect, provided you don't introduce any bugs when rewriting it. The goal is to create the table with all the constraints, not necessarily to have a neat looking script.
see i read that as "it doesn't matter if you clean code. just matters that it works"
&gt; i read that as "it doesn't matter if you write clean code. just matters that it works." And to a degree, that's correct. Your DDL code isn't like the code inside a stored procedure or trigger or function - you're not going to be running it thousands of times per second. What really matters is "if I run this script, will I get the database object that I expect?" &gt;of course it's necessary that if someone read this script 5 years from now they are able to understand what is happening. SQL, and especially the DDL portion of SQL, does not change much over time. Given that people are still running versions of SQL Server that were released a decade or more in the past, this really shouldn't be a concern.
The code matters if people are going to be editing the database based on that code, but at that point it would be better to throw Visual Studio at the database, import the whole thing into a project, and get it in source control.
thanks for understanding. yeah it is in source control. im trying to use the generated code to have a starting point to the clean up. instead of grabbing all the scripts in source control and putting them together and removing the junk. 
If you're wanting to start source control fresh from a database, it would still probably be easier to let Visual Studio handle that. Create a new database project, run a schema compare, use the database as the source and the project as the target, and tell it to update the project, and it will set everything up in your new Visual Studio project.
it's not a static script. this script will be updated in the future. and being updated by other teams as well. it's source controlled as well. i understand your point. but its ran very often i can tell you that, for a variety of reason out of my control or new clients. i think the indication that im currently doing the work to gather the scripts and make one create script warrants enough not to make it a "black box" script.
How exactly do you code in SQL? I‚Äôm a beginner, but I‚Äôm looking at it from a generating queries standpoint. I don‚Äôt know if more is involved if you‚Äôre a DBA or something.
If it's clustered sql instance I think it's a gmsa or something.. which is group managed service account.. if the first instance is soley for just SharePoint ( which it should be... Don't mix any other dbs ), they sql agent really won't be used alot. SharePoint will have it's own farm account that will be sys admin and will handle all permissioning when it is installed. Etc. ( WFE Connecting to the SQL box.. ) . And will set up roles etc and depending on how many features SharePoint has installed , different accounts should be used... From a SQL DBA standpoint a SharePoint to install is one of the simplest installs etc
https://docs.microsoft.com/en-us/sharepoint/install/initial-deployment-administrative-and-service-accounts-in-sharepoint-server
https://blogs.msdn.microsoft.com/markweberblog/2016/05/25/group-managed-service-accounts-gmsa-and-sql-server-2016/
Also you don't need to set up maintenance jobs for a bunch of the databases. I usually think they are just content databases... But let me find an article for ya!
I would nest the selection: Select a.* from (your select stmt {add the tableb.datefield}) a Where tableb.datefield between ‚Äò1604‚Äô and ‚Äò‚Äô1804‚Äô; Something like that....
As presented, your code is nonsense for the reason you think it is, but since the lines begin with `--`, they're also just comments.
With out knowing the data its hard to say what the intent was but I'd guess, if its example code, that the single apostrophe is just a mistake. It looks they are building a string so replacing a null with an empty string, a double apostrophe, is the most obvious thing I can think they were doing. But as I said its just a guess.
command: alter table ¬¥table1¬¥ drop foreign key ¬¥table1_ibfk1¬¥ error: #1146 Table 'project db.¬¥table1¬¥' does not exist. if I omit the backticks for the table: alter table table1 drop foreign key ¬¥table1_ibfk1¬¥ error: #1091 Cannot delete '¬¥table1_ibfk1¬¥'. Does this column or key exist? I worked around now, setting up the whole table again, but I'm still wondering why it didn't work.
If the format on the data field is always the same, you can use RIGHT() to get the numbers from the field, and CONVERT() to convert the numbers to a date. Then select the range you‚Äôre looking for.
&gt; error: #1146 Table 'project db.¬¥table1¬¥' does not exist. okay i tried [googling that error message](https://www.google.com/search?q=error%3A+%231146+Table+does+not+exist) and plenty of results showed up 
I just came across a beginners programs in sql and ended up learning sql basics, join, aggregations, sub queries and little bit of data cleaning in last two months. Now I bought a book (Sams Teach Yourself in 10 Minutes) and querying all day every night. SQL is like love at first sight for me, Now I think I would have learned this 5 years ago. SQL is awesome so far and my plan is to study a little python also. Currently I am 28 working in IT pre sales Dubai. I hope I can change my career to any analyst job or sql related job.
I will try to explain with more details. So, we have an store using Prestashop that created automatically an database at MySQL. And we have an database at SSMS that is being used with Sage 100c. The goal of our job is to make the MySQL database being connected with the SSMS one, and when someone adds an customer the database from sage is automatically updated with the customer added on the shop and vice-versa. We need to do this for the customers and the items, the problem is that on the shop side giving the example of the customer we have the table like this https://imgur.com/uWKoIBA and at the other side of Sage 100c we have the table clients looking like this https://imgur.com/uOM2eeL . Both of this tables have diffrent columns, my first question is, do i need to have 2 similiar if not equal databases? Do i need just to sync the equal columns (ID-ID, NAME,NAME) and where and how can i sync just the columns that i want to ? 
My guess, then, is that `ISNULL(field_2,')` should be `ISNULL(field_2,'')` and so on to handle null values in those columns during the update. That makes the most sense.
SELECT DISTINCT OrderNo, ItemID WHERE orderno in SELECT orderno FROM Table1 WHERE (SUM (match) / COUNT (match) = 1 ) GROUP BY OrderNo, prj
Thanks friend. Was my guess as well, except that prior that prior DBA usually had clean and perfect code. I was right to question in this case, though usually I would not be.
CHARINDEX (...,....,....) &gt; 1 YOU'RE BEST FRIEND
I'm not an oracle guy, but I think you're using Trunc() incorrectly. I believe you need to tell it you want the result in years. on 3. Post_Code and PostCode are likely the same column minor thing instead of using 6576.5 the future person changing/ reading your code would appreciate it if you did 365.5*18. Truth be told, they're both wrong and could return people as 18 the day before their 18th birthday or exclude someone on their 18th birthday (leap years). BirthDay &gt; (dateadd(Year,18,getdate()) in SQL or datediff(year,birthday,getdate())&gt;18, there are other ways to do it. 
If you can give us some information about the DB schema it might help.
Yes you‚Äôre totally correct on the first two, thanks for that!! As for your third statement, I personally never thought about that scenario. Truth be told, i‚Äôve never heard of the suggested way to calculate age, yet reading it in theory it seems flawless! Thank you for the help! Any clue on the last one?? 
I intentionally ignored that one. I don't think your design can support answering that question. It'd take me some time that I don't have this morning to suggest a better way. :( sorry
So caveat, I'm not positive I'm reading your EER correctly, but --i date is 'yyyymmdd' here SELECT * FROM student WHERE Start_Date &gt; '20180701' ORDER BY Start_Date --ii. unclear if 'musicians' include students as well as employees, will assume not --additionally, there should be a way to connect a student directly with their assigned teacher, --a foreign key on the student table that goes to the employee ID or teacher.id depending on how you wanted to do it SELECT DateDiff(Year, Employee.Date_of_Birth,GetDate()) AS Age, Employee.First_Name FROM Employee GROUP BY Employee.First_Name, Employee.Date_of_Birth HAVING DateDiff(Year, Employee.Date_of_Birth,GetDate()) &lt; 18 ORDER BY Employee.First_Name; --iii Teaching.ID would be a foreign key pointing to employee.id, If there are matches that employee is teaching. SELECT COUNT(*), Employee.Post_code FROM Teacher JOIN Employee ON Employee.ID = Teaching.ID GROUP BY Employee.Post_Code --iv Yeah your query works, although there isn't something that denotes it as 'current' or not. Also it is ORDER BY like you have above --V This would change depending on what foreign keys you add. Is Student.style a key pointing Lesson.style? SELECT Student.First_Name+' '+Student.Last_Name AS [Student Name], Lesson.Style, Employee.First_Name+' '+Employee.Last_Name AS [Teacher Name], MONTH(Lesson.Date) FROM Employee JOIN Student ON Student.TeacherID = Employee.ID JOIN Teacher ON Teacher.TeachingID = Employee.ID JOIN Lesson ON Lesson.TeachingID = Teacher.TeachingID GROUP BY Student.First_Name, Student.Last_Name, Lesson.Style, Employee.First_Name, Employee.Last_Name HAVING MONTH(Lesson.Date) = MONTH(GetDate())
Totally understandable, thanks for your feedback anyway! 
Thanks for the encouragement :). Yep, I got into basic programming with Rails last year, but then wasn't sure where to go with it. Taking a coursera class on SQL now to see if it's more easy to combine with accounting skills I already have.
thank you
You'll likely run into problems using `where not in` if the column you're evaluating contains nulls. Not sure if this is your problem but it's something to consider
try NOT EXISTS instead of NOT IN
Wow! This is amazing help, thanks for your time! All your suggestions are extremely helpful and i'll take them into account! Especially with the foreign key and calculating the age!! For question five, it seems very confusing and technical, so i'll try and understand it row by row!
The main performance problem is you use the same table too many times. I'm also wondering what happens if a course has more than one prerequisite. It seems that your query would return the course if a student has satisfied one prerequisite but not another. This is my take. SELECT courseNumber FROM courses LEFT OUTER JOIN course_prerequisites ON course_prerequisites.courseNumber = courses.courseNumber LEFT OUTER JOIN student_courses ON student_courses.courseNumber = course_prerequisites.preRequisiteCourseNumber AND studentId = @STUDENT_ID_HERE WHERE courses.courseNumber NOT IN (SELECT courseNumber FROM student_courses WHERE studentId = @STUDENT_ID_HERE) GROUP BY courses.courseNumber HAVING COUNT(*) = SUM(CASE WHEN course_prerequisites.courseNumber IS NULL OR student_courses.courseNumber IS NOT NULL THEN 1 ELSE 0 END);
I believe your original query has a bug in it. It'll return courses where the student has satisfied *a* prerequisite, but not *all* prerequisites. This query should do what you want, quickly: DECLARE @studentId INT; WITH IncompletePreReqs AS ( SELECT DISTINCT p.courseNumber FROM course_prerequisites p LEFT JOIN student_courses s ON p.preRequisiteCourseNumber = s.courseNumber AND s.studentId = @studentId WHERE s.courseNumber IS NULL ) SELECT c.courseNumber FROM courses c LEFT JOIN student_courses s ON c.courseNumber = s.courseNumber WHERE s.courseNumber IS NULL AND NOT EXISTS (SELECT p.courseNumber FROM IncompletePreReqs p WHERE p.courseNumber = c.courseNumber) I used this setup data: CREATE TABLE courses (courseNumber INT) CREATE TABLE course_prerequisites (course_number INT, preRequisiteCourseNumber INT) CREATE TABLE student_courses (courseNumber INT, studentId INT) INSERT INTO course_prerequisites(preRequisiteCourseNumber, courseNumber) VALUES(1, 3), (2, 3), (4, 7), (5, 7), (6, 7) SELECT * FROM course_prerequisites INSERT INTO student_courses(courseNumber, studentId) VALUES(1, 1), (2, 1), (4, 1) SELECT * FROM student_courses INSERT INTO courses(courseNumber) VALUES(1), (2), (3), (4), (5), (6), (7), (8), (9), (10)
Thanks for the tip.
You're right, thanks.
I'm actually trying this in MS Access (forgive me). I don't believe the WITH keyword is supported. I'll try to work around that.
Its a good habit, and depending on if you have null values, it will likely perform better. here is a good article on it http://weblogs.sqlteam.com/mladenp/archive/2007/05/18/60210.aspx
Fancy. I like this. Since you've been using this, how does this compare with the "new normal" of outer apply select top 1? SELECT p.pm_project_recid , note.most_recent_notes FROM PM_Project AS p outer apply ( SELECT top 1 n.most_recent_notes from pm_note n where n.pm_project_recid = p.pm_project_recid order by n.pm_note_recid desc) note
Try this? DECLARE @studentId INT; SELECT c.courseNumber FROM courses c LEFT JOIN student_courses s ON c.courseNumber = s.courseNumber WHERE s.courseNumber IS NULL AND NOT EXISTS ( SELECT p.courseNumber FROM course_prerequisites p LEFT JOIN student_courses s ON p.preRequisiteCourseNumber = s.courseNumber AND s.studentId = 1 WHERE s.courseNumber IS NULL AND p.courseNumber = c.courseNumber ) 
OUTER APPLY with a TOP 1 it essentially forced to use a loop join, and requires a perfect index to be efficient. The string sorted aggregate can use hash aggregate, stream aggregate, hash join, merge join, or nested loops (though less efficiently than the ideal OUTER APPLY). I've found the cpu cost of the string functions to not really be a factor.
Would not using Extended Events and piping it a file be better?
Thanks again, but MS access does not allow a JOIN in a sub select query. IT SUCKS
Hey guys, I dont understand "Before Insert On PropertyForRent referenceing new as prop "
It really depends on what you are trying to accomplish. In the case outlined in the article I wanted to be able to replay the trace on a different server running a different version of SQL Server to get an idea of what issues might come up if we were to upgrade. I'm also a huge fan of Extended Events but in this case the replayable trace option was more appealing.
Awesome, thanks!
You could have the Extended Event do something very similar, but you also get much better run speed/time analytics around plan comparisons; on top of a significantly lower performance impact vs Trace. For example, changing the WHERE condition on a query will use the same cached execution plan in most cases, so you can group on plan hash to see which executions are taking up the largest total resource amounts/etc. Honestly I haven't found any day\-to\-day operation that need Trace anymore, as of 2012 extended events are just more functional and the systems don't come to a crawl if someone decides to run the nightly report in the middle of business hours. 
Assuming that string is always at the end of your "date" field: `where reverse(datecode) = '4081'` Be advised that performance-wise, this will suck because it's not SARGable. But neither are any of the other options.
This sounds like a custom integration, and frankly is one of the hardest things to not screw up. I like having a mapping tables in the middle, one per entity (customer, item, etc). The mapping table has separates fields from both sides. Such as Sage_NOME, Prestashop_firstname, and Prestashop_lastname. Then you have 4 scripts. 1. A script to write from the mapping table to Sage. 2. A script to write from the mapping table to Prestashop. 3. A script to convert Sage fields in the mapping table to Prestashop fields. 4. A script to convert Prestashop fields in the mapping table to Sage fields. Finally you have Status table with a status for every step in your scripts. Like this: StatusID|StatusName -:|:- 1|New from Prestashop 2|New from Sage 3|Ready to load into Prestashop 4|Ready to load into Sage 5|Loaded into Prestashop 6|Loaded into Sage 7|Mapped Prestashop customer names 8|Mapped Sage customer names 9|Mapped Prestashop ... 10|Mapped Sage ... ...|... Then you can write your scripts to be fairly robust, and skip over records that would fail the step. Like this: UPDATE m SET Prestashop_firstname = SUBSTRING(m.Sage_NOME,1,CHARINDEX(' ',m.Sage_NOME)-1) , Prestashop_lastname = SUBSTRING(m.Sage_NOME,CHARINDEX(' ',m.Sage_NOME)+1,32) , StatusID = 8 /*Mapped Sage customer names*/ FROM CustomerMapping AS m WHERE m.StatusID = 1 /*New from Prestashop*/ AND CHARINDEX(' ',m.Sage_NOME) &gt; 0 /*Can be parsed into first and last name*/ AND CHARINDEX(' ',m.Sage_NOME) &lt;= 32 /*First name will fit into field size*/ AND LEN(m.Sage_NOME) - CHARINDEX(' ',m.Sage_NOME) &lt;= 32 /*Last name will fit into field size*/;
Yep. Their stairway to SSIS saved my bacon last year. Hope they serve you well.
Well I'll be.... That worked. If you don't mind - what is it that the ISNULL portion is doing with the blank quotes?
Do you have any indexes on your tables? Can you post the execution plan (http://pastetheplan.com/)? Have you broken your query down to its components to find one that's taking too long? For that matter, have you considered using temp tables to reduce the data that has to be processed?
Fair warning:I am a complete novice at this. I tried showing the execution plan but got an error "SHOWPLAN permission denied in database". I originally wrote the query in three sections (jobs, sales orders, employees) and then combined them at the bottom with a where statement. They all run ok except for the employees section which takes 8 seconds by itself. When I add the employee section in, the query jumps to taking over a minute. I do not know enough about indexes or temp tables to answer the other two questions...I have some researching to do Thank you!
This should work: where at.TRANS_POST_DATE &gt;= convert(varchar(10) , getdate()-7, 101) and at.TRANS_TYPE in ('cr', 'crx') and (_EBILL_TYPE not in ('ebill') or _EBILL_TYPE is null) The reason this wasn't performing the way you expect it to is because of the way where/and/or work. Your WHERE clause was being interpreted as: where (A, and B, and C) or D. Unless I'm misunderstanding you want: where A, and B, and (C or D). Ty using and coalesce(_EBILL_TYPE, 'this is no longer null') not in ('ebill'). ISNULL (SQL server) and NVL (Oracle) are some DBMS specific functions that will accomplish similar tasks but can not be used everywhere.
Without an execution plan, I can only guess. `DISTINCT` is a crutch, and is usually a symptom of joins that could be improved. `FOR XML PATH` is never going to be as fast as normal queries. Almost every table is referenced more than once, which could maybe be reduced. The final joins don't look like use unique criteria, which could inflate the dataset just to reduce it back with distinct.
I think OP wants to exclude 'ebill', while including NULLs.
 SELECT * FROM [Table B] WHERE ID = 1 UNION ALL SELECT * FROM [Table A] WHERE ID = 2
&gt; I tried showing the execution plan but got an error "SHOWPLAN permission denied in database". Go to your DBA and make him give you permissions to view execution plans. There's no reason for you to not have this if you're developing queries. If I were in your position, I would take each subquery and load those into temp tables. Then do proper `JOIN`s between them instead of those pre-ANSI SQL92 joins you're doing (`from table1 join table2 on table1.field1 = table2.field)`). Extra benefit of temp tables: you can index them (for starters, create indexes on the fields you're using to `JOIN`). Temp tables may look like more work is being done but they can ultimately help SQL Server make better decisions about how to run a query, resulting in less system load and better performance. You can find out what indexes exist on your tables with `sp_help TABLENAME`. But I suspect the issue isn't the underlying tables but the fact that you're `JOIN`ing these derived tables.
So I'm trying to exclude the _EBILL_TYPE of ebill, but include all others and all NULL. The suggestion from u/developer80 retrieved the output I was looking for. The TRANS_POST_DATE is just a date spit out in the following format: 2017-07-18 00:00:00.000 The goal is to pull all data from the week prior to the report being run and I think this is a way to clean it up?
 SELECT TableA.ID , COALESCE(TableB.Val,TableA.Val) AS Val FROM TableA LEFT OUTER JOIN TableB ON TableB.IDOther = TableA.ID 
You need to completely revise how you wrote this. I tried formatting and am just getting lost. You are doing way too much at once and need to simplify your approach using #tables. I honestly can't even understand what you're doing here with the commas. I assume it functions like some kind of join but just break it down into sections and join the sections. Here is what your partially formatted code looks like: select distinct substring(orders.customer,1,len(orders.customer)-1) as Customer , jobs.prodcode as Dept , jobs.partnum as Part , jobs.jobnum as Job , jobs.assemblyseq , LEFT(Operations.Operations,CHARINDEX(',',Operations.Operations + ',')-1) as Current_Op , operations.Operations as Operations_Remaining , jobs.ReqDueDate as Due_Date , jobs.Status as Status , employees.Employees as Active_Employees from ( select jobhead.jobnum , joboper.assemblyseq , jobhead.PartNum , jobhead.ProdCode , cast(jobhead.ReqDueDate as datetime) as reqduedate , case when jobhead.ReqDueDate&lt;getdate() then 'Late' else 'On Time' end as Status from erp.jobhead as jobhead inner join erp.joboper as joboper on jobhead.jobnum=joboper.jobnum where jobhead.JobClosed=0 and jobhead.partnum &lt;&gt;'non-fab tool' and jobhead.partnum &lt;&gt;'fab tool' and jobhead.partnum&lt;&gt;'ws-ss' ) as jobs , ( select part.partnum , (REPLACE(REPLACE(((select distinct (CAST(OrderHed.OrderNum AS VARCHAR) + ',') as [Calculated_OrderNum] from Erp.OrderHed as OrderHed inner join erp.orderdtl as orderdtl on orderhed.ordernum=orderdtl.ordernum where orderhed.openorder=1 and orderdtl.OpenLine=1 and Orderdtl.partnum= ltrim(rtrim(part.partnum)) FOR XML PATH(''))) , '&lt;/Calculated_OrderNum&gt;',''),'&lt;Calculated_OrderNum&gt;','') ) as [Calculated_Orders] , ( REPLACE(REPLACE(((select distinct (CAST(customer.name AS VARCHAR) + ',') as Customers from Erp.OrderHed as OrderHed inner join erp.orderdtl as orderdtl on orderhed.ordernum=orderdtl.ordernum inner join erp.customer on orderhed.CustNum=customer.CustNum where orderhed.openorder=1 and orderdtl.OpenLine=1 and Orderdtl.partnum = ltrim(rtrim(part.partnum)) FOR XML PATH(''))) , '&lt;/Customers&gt;',''),'&lt;Customers&gt;','') ) as Customer from Erp.Part as Part) as orders , ( SELECT jobasm.jobnum , jobasm.assemblyseq , substring( ( SELECT distinct empbasic.name + ',' FROM erp.labordtl as labordtl inner join erp.empbasic on empbasic.empid=labordtl.employeenum WHERE labordtl.Activetrans=1 and jobhead.jobnum=labordtl.jobnum and jobasm.AssemblySeq=labordtl.AssemblySeq FOR XML PATH('') ),1,len(( SELECT distinct empbasic.name + ',' FROM erp.labordtl as labordtl inner join erp.empbasic on empbasic.empid=labordtl.employeenum WHERE labordtl.Activetrans=1 and jobhead.jobnum=labordtl.jobnum and jobasm.AssemblySeq=labordtl.AssemblySeq FOR XML PATH('') ))-1) as Employees FROM erp.JobAsmbl as jobasm inner join erp.jobhead as jobhead on jobasm.JobNum=jobhead.JobNum where JobReleased=1 and JobClosed=0) as employees, (select distinct jobhead.JobNum, JobHead.PartNum,jobhead.ProdCode as ProdGroup,joboper.AssemblySeq as ASM, substring((select ',' + concat(JobOper2.OprSeq,':',JobOper2.OpCode) from Erp.JobOper JobOper2 where JobOper.Company=JobOper2.Company and JobOper.JobNum=JobOper2.JobNum and JobOper.AssemblySeq=JobOper2.AssemblySeq and JobHead.JobClosed=0 and JobOper2.OpComplete=0 order by JobOper2.Company, JobOper2.JobNum, JobOper2.AssemblySeq, JobOper2.OprSeq for xml path('')),2,9999) as Operations from Erp.JobHead as Jobhead inner join erp.joboper as joboper on JobHead.Company=JobOper.Company and jobhead.jobnum=joboper.JobNum where JobHead.JobClosed=0 and JobOper.OpComplete=0 and jobhead.partnum&lt;&gt;'FAB TOOL' and jobhead.partnum&lt;&gt;'NON-FAB TOOL'and jobhead.partnum&lt;&gt;'WS-SS') as operations where jobs.partnum=orders.partnum and jobs.jobnum=employees.jobnum and jobs.assemblyseq=employees.assemblyseq and operations.jobnum=jobs.jobnum and operations.ASM=jobs.AssemblySeq order by ReqDueDate Here is what an example looks like if you break it down into chunks: begin select jobhead.jobnum , joboper.assemblyseq , jobhead.PartNum , jobhead.ProdCode , cast(jobhead.ReqDueDate as datetime) as reqduedate , case when jobhead.ReqDueDate&lt;getdate() then 'Late' else 'On Time' end as Status into #step1 from erp.jobhead as jobhead inner join erp.joboper as joboper on jobhead.jobnum=joboper.jobnum where jobhead.JobClosed=0 and jobhead.partnum &lt;&gt;'non-fab tool' and jobhead.partnum &lt;&gt;'fab tool' and jobhead.partnum&lt;&gt;'ws-ss' end begin create index idx_indexname on #step1([field1], [field2]) end begin select blah into #step2 end begin create index idx_indexname on #step2([field1], [field2]) end begin select blah into #step3 from #step1 a inner join #step2 b on a.field1 = b.field1 and a.field2 = b.field2 end 
A personal favourite was re-writing a query that had a cursor ( ! ) and getting the execution time down from 15 minutes+ to 30 seconds...
Is that what those commas are doing? Joining subqueries? Where does the criteria exist to join on or does the comma function like a cross join?
You have to reformat the query to see what's going on. First derived table: ( SELECT jobhead.jobnum, joboper.assemblyseq, jobhead.PartNum, jobhead.ProdCode, CAST(jobhead.ReqDueDate AS DATETIME) AS reqduedate, CASE WHEN jobhead.ReqDueDate &lt; GETDATE() THEN 'Late' ELSE 'On Time' END AS Status FROM erp.jobhead AS jobhead INNER JOIN erp.joboper AS joboper ON jobhead.jobnum = joboper.jobnum WHERE jobhead.JobClosed = 0 AND jobhead.partnum &lt;&gt; 'non-fab tool' AND jobhead.partnum &lt;&gt; 'fab tool' AND jobhead.partnum &lt;&gt; 'ws-ss' ) AS jobs Second derived table: ( SELECT part.partnum, (REPLACE(REPLACE( ( ( SELECT DISTINCT (CAST(OrderHed.OrderNum AS VARCHAR)+',') AS [Calculated_OrderNum] FROM Erp.OrderHed AS OrderHed INNER JOIN erp.orderdtl AS orderdtl ON orderhed.ordernum = orderdtl.ordernum WHERE orderhed.openorder = 1 AND orderdtl.OpenLine = 1 AND Orderdtl.partnum = LTRIM(RTRIM(part.partnum)) FOR XML PATH('') ) ), '&lt;/Calculated_OrderNum&gt;', ''), '&lt;Calculated_OrderNum&gt;', '')) AS [Calculated_Orders], (REPLACE(REPLACE( ( ( SELECT DISTINCT (CAST(customer.name AS VARCHAR)+',') AS Customers FROM Erp.OrderHed AS OrderHed INNER JOIN erp.orderdtl AS orderdtl ON orderhed.ordernum = orderdtl.ordernum INNER JOIN erp.customer ON orderhed.CustNum = customer.CustNum WHERE orderhed.openorder = 1 AND orderdtl.OpenLine = 1 AND Orderdtl.partnum = LTRIM(RTRIM(part.partnum)) FOR XML PATH('') ) ), '&lt;/Customers&gt;', ''), '&lt;Customers&gt;', '')) AS Customer FROM Erp.Part AS Part ) AS orders Then `jobs.partnum = orders.partnum` in the `WHERE` clause is specifying the join criteria between them. Without specifying the join criteria in the `WHERE` clause, the comma join will be a `cross join`. It *could* have been written as: (first query) as jobs JOIN (second query) as orders on jobs.partnum= orders.partnum which would make it even harder to read, but it's ANSI SQL92 syntax. 
OK, that's more or less what I thought it was doing. I'm going to bet that's exactly what his problem is. Each derived table will be maximum size before it shrinks down in the final where, etc. Thanks. 
[Searching this subreddit should be able to help you. This gets asked A LOT](https://www.reddit.com/r/SQL/search?q=learn&amp;restrict_sr=on&amp;include_over_18=on&amp;sort=relevance&amp;t=all) [Or here](https://www.reddit.com/r/learnSQL/)
I'm all about the temp tables. They're so good in situations like this.
Nice. I actually did the opposite for someone. Yes, I used a cursor to make the query run faster. But it was a one off scenario and I automated the query to run in the off-hours. 
In my limited experience with SQL they are simply unbeatable, and any time I see some ridiculous complex query that has multiple sub-queries, functions, cases, etc., I just assume it was written by someone who knows less than me. Which is saying something considering I don't rank my SQL knowledge very highly. At my first SQL job, where I was working with some heavy hitters, that was the only way we flew. Time and time and time again I would write something unnecessarily complex, which took too long to run, and which could easily be optimized simply by breaking it down and leveraging #tables. It sucks when you finish a query and it does what you want but you have to literally start over to rewrite it in order to process it efficiently, but it was a habit that was drilled into me and I have seen the benefits of doing it enough to know that it is a best practice.
In my limited experience with SQL they are simply unbeatable, and any time I see some ridiculous complex query that has multiple sub-queries, functions, cases, etc., I just assume it was written by someone who knows less than me. Which is saying something considering I don't rank my SQL knowledge very highly. At my first SQL job, where I was working with some heavy hitters, that was the only way we flew. Time and time and time again I would write something unnecessarily complex, which took too long to run, and which could easily be optimized simply by breaking it down and leveraging #tables. It sucks when you finish a query and it does what you want but you have to literally start over to rewrite it in order to process it efficiently, but it was a habit that was drilled into me and I have seen the benefits of doing it enough to know that it is a best practice.
Read everything by Itzik Ben Gan. I like Benjamin Nevarezs stuff too but that is a little more towards the intermediate/advanced level.
is this supposed to be a self join? JOIN Product_Listing ON Product.Listing_ID = Product_Listing.Listing_ID
Your problem is with the WHERE clause. You if you use IN the subquery must return only the row you want to search. You also want to make sure you select the correct column to compare, which you aren't doing. ORDER\_ID IN \(SELECT Customer\_ID, Inventory\_ID.... should be ORDER\_ID IN \(SELECT Order\_ID
Its a bit confusing syntactically, but that join seems correct \(assuming his schema matches\). **Product**.Listing\_ID = **Product\_Listing**.Listing\_ID 
If \_EBILL\_TYPE is indexed and you want to continue to use that index, \(I believe\) you can use a union. One union will check where \_EBILL\_TYPE != 'ebill', the other will check where \_EBILL\_TYPE IS NULL.
I tried the method you mentioned: WHERE Customer_ID IN (SELECT Customer_ID FROM Orders GROUP BY Customer_ID, Inventory_ID HAVING COUNT (Order_ID) &gt;3 AND COUNT (INVENTORY_ID) &gt;3); However, Now I get an error stating: Ambiguous column name 'Customer_ID'. Looking up the error it says it is because I am referencing the column twice? 
It's possible your IN clause is returning nulls which will throw off your results. Try changing it to an EXISTS
I agree, this does seem possible.
&gt; But #temp tables not a best practice. For advanced analytics, ETL, and modeling I think that I disagree with you. You might be right about everything you're saying, but that doesn't change the way I've seen things done by senior modelers. They *all* use #tables to segment long processes. &gt;If implemented too aggressively across an environment, tempdb itself can become the bottleneck. That's true, but why you need to schedule things appropriately, no? &gt; It also sacrifices the ability to reuse execution plans, which can be a consideration if a script is executed hundreds of times a day. Using #tables appropriately creates a static execution plan that doesn't allow SQL to interpret what to do, it just does what you tell it. The optimizer isn't smarter than a human is.
You need to specify which table you are looking at for Customer_ID in your WHERE Customer_ID IN (...) statement. Qualify it either as Customer.Customer_ID IN( .. ) or Orders.Customer_ID IN (..) 
\^\^ Correct answer. Sorry I didn't look to see if you were already using it. Since you're already using the using the Orders table you'll probably want to alias it: Orders.Customer\_Id IN \(SELECT o.Customer\_ID FROM Orders o GROUP BY o.Customer\_ID, o.Inventory\_ID HAVING COUNT \(o.Order\_ID\) \&gt;3 AND COUNT \(o.INVENTORY\_ID\) \&gt;3\); 
The table BookOrderLine likely does not contain any records in the BookOrderNo column which match the PK of the BookOrder Table. If you use a LEFT JOIN instead of INNER JOIN, it will return all rows from the BookOrder table with a NULL in the QuantityOrdered column for those which do not have a match in the BookOrderLine table.
Ah, definitely makes sense. Got what I needed -- thanks!
is the SELECT inside the IN() returning null?
Yes. I think so. I've removed it and was able to get results, just not limiting the query based on the parameters I set.
Just run that inner query by itself to check. NULL is not equal to anything. Even itself. So when you say IN(x) and x is NULL then it will not match. 
&gt; I am trying to run a query to see where a product was purchased more than 3 times by more than 3 people Unless I'm mis-interperting, I think you want to replace everything from the where clause (I.e. remove Where and everything else after it) with a join to a subquery, i.e: JOIN ( SELECT Customer_ID, Inventory_ID, COUNT(*) OVER (PARTITION BY Inventory_ID) as MoreThan3Custs FROM Orders GROUP BY Customer_ID, Inventory_ID HAVING COUNT (Order_ID) &gt;3 ) MoreThan3 ON Customer.Customer_ID = MoreThan3.Customer_ID AND Customer.Inventory_ID = MoreThan3.Inventory_ID AND MoreThan3.MoreThan3Custs &gt; 3 The group by/having does the customers who have ordered the item more than 3 times, and then the partition by (which executes after the having has filtered) finds the more than 3 customers 
So I think this is a **great** topic of conversation and I want to get that out of the way right up front, because I understand what you're saying but I think this is the cause of a lot of confusion between DBA's and analytic developers. The two are simply not the same, and many times we will need to break or redefine what you normally use as a best practice in order to more closely align with general concepts like normalizations. &gt;I'm thinking of analytic queries that are run on demand. Everyone and their and their mother runs their reports at 10 AM, and if all reports use #temp tables in some fashion it can be a problem. Absolutely agree, but at this point in the conversation we need to at a minimum delineate between either environments, or types of jobs. We may write an algorithm to create a "cube" or "mother data source" which will be quite length, need to be run on set intervals, and need to be scheduled in such a way as to not impact other users or jobs. With multiple environments you can accomplish this by running the job itself on one server, and then simply copying the results. These types of jobs can take months or years for us to write, and they are (in a best practice) rarely updated. The rest of the time we are writing on the fly code to test hypotheses and develop models/algorithms which might eventually one day become mother sources. These types of queries tend to exclusively be based on our "cubes" and when we do join, it tends to be a best practice to segment the data as much as possible before doing so (especially true with remote servers, or open query, etc.) &gt;It's not static but the statistics are definitely accurate, unlike many sub-queries. The statistics are not smart enough to see where I am trying to go with a query or model. We are applying our own statistics to the data, and how it comes together is totally irrelevant to our purposes. Take your example up above about using PK's, which is totally valid, but: What are you going to do when I need to aggregate data multiple times into segments, then join those segments in multiple ways, then join the results of that process to other data which has primary keys, but which aren't being used because we're only interested at dates, financial information, census data, or aggregating the data and then joining? Now you theoretically could take those types of queries to create new tables which do have primary keys and then base the queries themselves on joins which involves keys, but they would become nonsensical to a statistician or analytical developer. In a best practice we are not letting SQL project what it thinks is a best execution plan because we are testing multiple paths and writing the one which is most efficient for each step, and then forcing SQL to execute them in serial. This might not *always* work, but it's more accurate to say that it might not *always* work to a huge degree of improvement. You absolutely could rearchitect your entire database to work around these "exceptions" (which to us are all we do) or you could allow for sprocs to do complex transformations that allow us to do complex queries on the fly, while at the same time having jobs scheduled to generate the data sources we're interested in looking at.
For what it's worth, Oracle supports function based indexes. What may not seem "sargable" in SQL Server may be in Oracle. 
What database is this for? The query feeding your IN clause is selecting two columns (Customer_ID and Inventory_ID). As far as I know, you can't do this. You can only have one column in the select clause. In addition, your essentially says give me all orders where Order_ID is in theis list of Customer_ID's (or Inventory_ID's). I think you should change it to Order_ID. Try this: SELECT First_Name, Last_Name, Country, Address, State, Zip, Product_Name FROM Orders INNER JOIN Customer ON Orders.Customer_ID = Customer.Customer_ID INNER JOIN Amazon_Inventory ON Orders.Inventory_ID = Amazon_Inventory.Inventory_ID INNER JOIN Shipment ON Amazon_Inventory.Shipment_ID = Shipment.Shipment_ID INNER JOIN Product ON Shipment.Product_ID = Product.Product_ID INNER JOIN Product_Listing ON Product.Listing_ID = Product_Listing.Listing_ID WHERE ORDER_ID IN ( SELECT Customer_ID, Inventory_ID FROM Orders GROUP BY Customer_ID, Inventory_ID HAVING COUNT (Order_ID) &gt; 3 AND COUNT (INVENTORY_ID) &gt;3 );
Does coalesce perform any better than isnull, or are they about the same?
Thanks for asking this question! I never had any idea that there was a name for this. I only knew about the concept through plenty of trial and error experience.
One of the best online course. For beginners and for free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
 NOT EXISTS maybe?
From the information given that seems to be correct :)
I'd say that is accurate, only other possibility might be with Item_Quantity if there is a similar column in Item_dimension like number in stock, but that is going to get more complicated.
You just need square brackets around the database name. So replace this: ‚Äò + name + ‚Äò With this: [‚Äò + name + ‚Äò]
Not like that. Use `quotename` as it‚Äôs safer. `Quotename(@name)`
While you *can* do it with T-SQL, there's an easier way with the [dbatools PowerShell module. ](https://dbatools.io/) Test-DbaDatabaseOwner -ServerInstance YOURSERVER | foreach-object{set-dbadatabaseowner -ServerInstance YOURSERVER -Database $PSItem.database -TargetLogin $PSItem.TargetOwner} Test-DbaDatabaseOwner returns all databases where the owner is not the given desired owner (by default, `sa`). `Set-DbaDatabaseOwner` does what your T-SQL here attempts to do - change the owner of the database. Even better, you can pass a collection (array) of SQL instances to `Test-DbaDatabaseOwner` and fix this on **all** of them in one go.
While I like the use of quotename in this case, can you explain why it's "safer"?
For the same reasons that writing your own string-escaping functions usually leaves you with a SQL injection vulnerability. http://www.sommarskog.se/dynamic_sql.html#quotestring
This is incorrect for several reasons. NOT IN performs a semi-join, which does not increase the record count. An INNER JOIN increases the record count, by the number of qualifying transactions in this case. This query counts items that have non-sales, but they may also have sales and shouldn't be counted. This query also fails to count any items that don't have a transaction of any kind, but would still qualify as not having a sale.
I'm certainly going to try the alternate methods mentioned here, but I'd also like to figure out why the wrong value is pulling into my query results. Does anyone notice anything glaring? Keep in mind, I'm a beginner, I could be doing something fundamentally incorrect here.
Pretty much this, I had to do this years ago, it can be a bit of work, but it's totally doable. main thing is just matching data types but luckily postgres syntax is very similar to SQL.
Thanks, good explanation.
Thank you!
Thank you!
Ah, your right. Thanks.
Thank you!
You're not collecting the output parameter properly. You need to tell SQL Server where to put when it's output by the stored proc. Declare @Thiscustseq int = null; use test_app; exec [dbo].[GetCustomerSequenceSp] '123' , @ThisCustseq out; select @ThisCustseq;
How does quotename handle the bracket in the name differently? I did some testing and it doesn't seem to do anything special with it, just puts square brackets around the whole thing. It seems like SQL injection wouldn't really be a concern for a script being manually run directly on the DB. Anyone with access to run the script could just change it anyway.
u/KrevanSerKay You might be able to help on this 
Move the opening parenthsis so that it's after the ON. So 'ON (......' and not '(ON.....'
[Wrapping the string yourself can result in an invalid name](https://stackoverflow.com/a/40616945/1324345) for the object. &gt; It seems like SQL injection wouldn't really be a concern for a script being manually run directly on the DB. Anyone with access to run the script could just change it anyway. Unless someone has *previously* compromised the instance, it wasn't detected, and that compromised data/object caused this script to do something else dangerous.
Thank you, fixed :)
What exactly do you mean by "how to do year over year"?
you can't do two one-to-many joins in the same query without getting **cross join effects** put the counts into subqueries also, you can't do those ORs without counting them separately, so you have to double the joins SELECT TblA.user_id , TblA.username , COALESCE(B_friend1_count,0) + COALESCE(B_friend2_count,0) AS B_friend_total , COALESCE(C_message1_count,0) + COALESCE(C_message2_count,0) AS C_message_total FROM TblA LEFT OUTER JOIN ( SELECT TblB.friend1 , COUNT(*) AS B_friend1_count FROM TblB WHERE TblB.requeststatus = 0 GROUP BY TblB.friend1 ) AS B1 ON B1.friend1 = TblA.user_id LEFT OUTER JOIN ( SELECT TblB.friend2 , COUNT(*) AS B_friend2_count FROM TblB WHERE TblB.requeststatus = 0 GROUP BY TblB.friend2 ) AS B2 ON B2.friend2 = TblA.user_id LEFT OUTER JOIN ( SELECT TblC.friend1 , COUNT(*) AS C_message1_count FROM TblC GROUP BY TblC.friend1 ) AS C1 ON C1.friend1 = TblA.user_id LEFT OUTER JOIN ( SELECT TblC.friend2 , COUNT(*) AS C_message2_count FROM TblC GROUP BY TblC.friend2 ) AS C2 ON C2.friend2 = TblA.user_id 
The P1 columns would show the total quantity, revenue, and margin for 1 year for each item a customer bought, and the P2 columns would show the values for the items the customer bought in that year. If an item was bought in one year, but not the other then the values in the P1/2 column would be null for the year the item wasn't bought
Assuming that the last line in his procedure is: SELECT @CustSeq Shouldn't the procedure be outputting that? I don't think he's trying to stuff the output of the proc into a variable. 
Figuring out how to organize the data is the crux of my issue :( My expression don't have a date column (because when it's included the records are duplicated for each invoice date), so I'm not sure how I'd go about joining the P2 table to the P1 table.
I'm in love with powershell. I will explore this module, thanks!
Direct link to the article: https://threadreaderapp.com/thread/987602838594445312.html 
There's other ways of doing it, but the UPDATE is probably the easiest... this type of manual entry doesn't come up a whole lot in SQL... typically you'll have a CSV or some other data source to query to do the bulk import, then you just update it by joining the tables: CREATE TABLE Friends (`ID` int, `Name` varchar(5), `Email` varchar(16)); INSERT INTO Friends (`ID`, `Name`, `Email`) VALUES (1, 'Joe',''), (2, 'Bill',''), (3, 'Bob',''), (4, 'Frank',''); CREATE TABLE Emails (`ID` int, `Email` varchar(16)); INSERT INTO Emails (`ID`, `Email`) VALUES (1, 'Joe@domain.com'), (2, 'Bill@domain.com'), (3, 'Bob@domain.com'), (4, 'Frank@domain.com'); UPDATE Friends INNER JOIN Emails on Friends.ID = Emails.ID SET Friends.Email = Emails.Email or some poor administrative assistant gets the task of update everything manually via a web-GUI or some other application that writes to the database.
Thanks mate! 
This is sql not python 
What kind of data from the real companies? Anything we can use for SSRS or SSIS learning?
Are you asking about table structure? There would be many ways to do it. The most basic would presumably be a 'User' table and a 'User likes table' that has both 'Liker' and 'Likee' columns that are both foreign keys to the user_id
That's pretty much what I was asking. Dunno how I didn't think of that, it's been one long brain fart of a day. Cheers for the help:\)
Their purposes are different. While you can use them for certain overlapping functionality, and can usually craft a method using windowed functions for most aggregate operations \(*if not all, I can't think of one where it isn't possible to do so but I'm a couple drinks in*\), it's not advisable to do so. Windows functions are built and optimized around maintaining row specific contextual information \(AKA they don't rollup the information\) meaning that utilizing them for aggregation will be orders of magnitude less efficient than GROUP BY even if you're crafty about it. Want aggregated operations around a defined criteria? GROUP BY Want running totals / rankings / etc? Windowed Functions
Firstly (to get it out of the way) - "standard Sql" is a misnomer as there are multiple SQL standards (named after years) and SQL is evolving. Secondly, given advanced enough implementation, the answer is "yes". "Partition by" clause borrows the main (imo) function of reduction of granularity of the "group by" clause and with some tinkering you can implement pretty much any kind of grouping sets and rollups. Now, a better question is - why in Date's name you'd want to do that? There are plenty ways that SQL is not a terse language and it wasn't meant to be one to begin with.
How does that make sense when I set @CustSeq to a value and then select @CustSeq at the end of the proc. Results should be the value of @CustSeq.
Because you declared `@CustSeq` as an output variable. It's a pass by reference if you're familiar with that from other environments. If you want `@CustSeq` returned directly as the resultset of the stored proc, remove `@CustSeq INT = NULL OUT` from the definition of the stored procedure.
Sorry, but that's not my point. If I select the variable after giving it a value, why am I returning null. Adding the select at the end was a testing method. For example, if I said SET @CustSeq = 1 SELECT @CustSeq, it would return 1. What I don't understand is why is returns NULL when the query itself returns 1.
discount code for us in this thread?? :)
You're not calling the stored proc correctly. You have to pass it a second variable (`@ThisCustSeq` in my example) by reference so that it knows where to put the value for its output variable.
I think you're essentially asking how to pivot data. A generic and more or less vendor-neutral approach is to group by your relevant granularity (accountnum, name, itemid, salesunit in your case, I think) and then do relevant aggregates over conditionals (e.g. sum( case when &lt;condition that makes it a P1QTY&gt; then &lt;P1QTY expression&gt; end) as P1QTY). You're on MSSQL so you have the PIVOT keyword/sugar available and obviously there are other ways to pivot data too.
Maybe a bi-directional like table would be pretty fast to identify matches, and with a nice check constraint would be well defined. CREATE TABLE Likes ( UserID1 int NOT NULL CONSTRAINT FK_Likes_UserID1 FOREIGN KEY REFERNECES User, UserID2 int NOT NULL CONSTRAINT FK_Likes_UserID2 FOREIGN KEY REFERNECES User, UserID1Likes bit NOT NULL CONSTRAINT DF_Likes_UserID1Likes DEFAULT (0), UserID2Likes bit NOT NULL CONSTRAINT DF_Likes_UserID2Likes DEFAULT (0), CONSTRAINT PK_Likes PRIMARY KEY CLUSTERED (UserID1, UserID2), CONSTRAINT CK_Likes_UserID1UserID2 CHECK (UserID1 &lt; UserID2) ); CREATE NONCLUSTERED INDEX IX_Likes_UserID1LikesUserID2Likes (UserID1Likes, UserID2Likes); SELECT * FROM Likes WHERE UserID1Likes = 1 AND UserID2Likes = 1;
Um, no. If my proc (with the same parameters) only said select 'test' it would return test... I should be returning the results of the select, regardless of the out parameter. Give it a try. 
"Mastering" Excel will not pay significant dividends in learning SQL.
LOL, fuck Excel. If you want to "master" SQL (not that I have) then you need to learn to **hate** Excel. Learn to do everything in SQL, even the things that are much easier to do in Excel (like grand totals for pivot tables.) Then after you learn enough SQL to get lazy you can go back to using Excel to make life easier for certain takes all the while bitching about a technology like Tableau.
is this worth the money? is codeacademy generally good?
Use power query in excel and you will find that a lot of things are quite similar. All kinds of joins, group by etc are to be found in both. Traditional excel and sql are two different beasts though.
&gt; "standard Sql" is a misnomer I don't agree. Newer standards replace previous once. When not referring to a specific version, "standard SQL" just means the current release IMHO.
Mind if I ask how did the subreddit help? Are you guys gonna be offering a discount? 200 is a bit steep for some of us :/
Window functions themselves cannot change the number of rows of the result (like group by does). But window functions can always get you the same numbers‚Äîbut for all rows. If you want to see an exciting new standard SQL feature that can replace GROUP BY as well as some parts of OVER, have a look at MATCH_RECOGNIZE. I've slides about this here: https://www.slideshare.net/MarkusWinand/row-pattern-matching-in-sql2016
Listen here now. Groupings by sets is the shiz niz. I learned it... Then forgot it... Then learned it again forcing myself to implement it.. Which I did.. But report builder gets that so info and makes it look pretty. 
Thank you for the response it will help a lot! 
Access would be more transferable. Excel can help you with t-sql concepts. However, query writing in a database is a whole different ballgame. Like someone mentioned in a comment before this, sql is more about data structuring and relationships and leveraging that for data analysis - excel, not so much 
Will these run every 6 weeks? Can't fit it in this go around, but would like to in the future. 
I think having a good handle on Excel is an important office skill. I don't think it should matter whether you deal in IT or other things in general. If you are an office worker who uses Excel multiple times a week, I'd recommend a basic -&gt; intermediate skill set. You say you are already an intermediate user, then I think you need to focus on what you want to do. If you are going report based, check out PowerBI or some of Excel's functions or some other reporting tools too. If you want something more powerful for analyzing, python / r may be good. If you want help with administrative things, powershell / python / c# would be good.
&gt; you must select ... no, we don't -- **you** do give us your best effort and we'll critique it for ya 
Im pretty sure first one is right, but idk about other ones
Thank you so much! That's exactly what I needed :-D
 with table1 as ( select 1 field1 from dual union all select -1 from dual union all select 2 from dual union all select -2 from dual), table2 as( select 3 field1 from dual union all select -3 from dual union all select 4 from dual union all select -4 from dual) SELECT field1 FROM ( SELECT field1, 0 p FROM table1 UNION ALL SELECT field1, 1 p FROM table2 ) a WHERE &amp; p = CASE WHEN &amp; p = 0 THEN 0 ELSE 1 END AND field1 &gt; 0;0 Result Field1 ------ 1 2 3 4 
Throwing my vote in for DB Browser too Sqlite Studio is probably slightly more complete, but adds a bunch of stuff you probably don't need
i personally think that this course is worth $500...
we currently don't offer discount codes unfortunately. but i will talk with marketing/product and see what we can offer in the upcoming months.
No problem, thank you for your answer. I just finished the data science source and really liked it, just to give a bit of feedback :)
The distinct queries are wrong because they eliminate duplicate values. In fact everything about the distinct queries is the devil.
(also work at Codecademy) - We just transitioned to a new payments system that doesn't support discount codes yet, but we're working on this. Will reply back here when we have it set up. 
How would this course compare to that of the Stanford's DB5? I'm currently going through that one right now.
you talk about "distinct rows" but your example uses ~columns~ perhaps you could show some sample data of what you want
1. Learning new schemas is always tricky, most systems will have a diagram to show the relationships between tables. If they don't have this it would be a good exercise to do. There are most likely views written by other analysts that you can either report on or look at to help understand the joins. 2. Shouldn't be a huge issue, have a look what other members of the team have done. 3. I started my career doing data entry into the system I now write SQL/reporting on, so I understand the underlying processes and what can be leveraged from the system. Try and shadow some of the operational guys and ask them what they want from reports etc. 4. I'm not great with numbers, but that's why we use computers! Most important thing is learning about how percentages are calculated, as its important to be able to health check that your numbers are right. 5. Measuring your success against others is tricky as perception isn't necessarily correlated to how well they are doing. What do your opposites do that you don't and how could you do that? 6. I do my SQL in code alone, as I use Microsoft SQL, mysql and oracle SQL. I find the query builder makes the SQL more scruffy and complicated, but that's just my personal experience. I use Excel to initially manipulate my data from the SQL, then use ssrs to analyse and report on it. Any advice or help you want let me know, always eager to help others!
How much documentation do they have and how much have you read? The first few weeks at a new job I'm playing analyst and looking over everything and trying to piece it altogether. The easiest part to figure out most of the time is the technology. The hardest part is the business logic. 1) You want to find ERD / data dictionaries. This should be documentation they have created with tools that shows you how things work together. 2) Create pictures and diagrams as you learn, visually map out the infrastructure and keep cheat sheets. 3) Again, refer to documentation. As you learn, create documentation if it does not exist or append to it. 4) No advice 5) When you ask questions, MAKE THEM GOOD QUESTIONS. I have a 20 year experience IT guy who is absolutely junior and in my position. He asks me questions like, what's my login, what server do we run ssis on, how do I write dynamic sql, how do I use temp tables, etc. I don't mind questions, I encourage them. If it's a question I have to answer a hundred times or it's obvious you did no research or you aren't taking notes or you didn't even try yourself, I'm going to get frustrated. Read this on asking good questions: https://stackoverflow.com/help/how-to-ask 6) I'm not sure what query builder is, I have never used it. What is the SQL platform you are using? I have something that can help you with 1 if you are on SQL Server.
Getting my 70-460 from MS has been on my 'to do list' but I have not made it a priority. I was in the CIS program in college but did not finish. I have back ground in tech support both call center and most recently desktop. I have an A+ and Network + certification. No actual SQL certifications however. 
okay, then... please explain what this means -- &gt; I want the third ~~row~~column to be based on the first two ~~rows~~columns being distinct. 
&gt;There are SO MANY tables, I get so overwhelmed and cant figure out where i can get the data from and how to join them. You will learn, ask for some documentation. When I first got a job as an report analyst I created an Excel document (pick your own poison) and wrote down every table I made use of. The fields I used, why I used them, and if there was any gotchas. After a few years I stopped referring to it, but it was helpful. Also, I would recommend after you get familiar with them, build some template queries for yourself that make use of them so you can remember how they relate. And how they relate via fields/columns. &gt; is impossible to understand each and everything (statuses, locations, history of item, order, packages). I doubt being 1 month in they expect you to know understand everything. &gt;I am not good with numbers (I am willing to learn). I am terrible with math, pretty good with logic flows though. Which makes me laugh that I have 'Analyst' in my title because my math is so shit. &gt;I ask for help but can't ask for too many times. Don't know your circumstances, but I enjoy writing people sample queries at work detailing how things work. What the tables are etc. But if you come to my desk and ask for my help and interrupt my flow, fuck you lol, send me an email and I will be all over it in my downtime. Other than how people ask me, my only other pet peeve is, did you try. Ask me to help and your fucking query is a mess, don't care will help, I see that you tried. Ask me for help and you have a blank screen.... &gt;I tried using query builder but everyone strongly discouraged it. I don't use Query Builder, don't like it, don't like the formatting of the code it builds. So if I have no intention of needing to customize the code, build is fine, otherwise typing it out is my bag. 
what do you want to set the number to? your SET stuff looks like it should be part of the WHERE clause
still confused maybe you could show a couple of existing phone numbers, and then show what you want them changed to
Learning tables is always going to be a struggle. Keep Writing Queries. all out, by hand, don't rely on snippets until you know the database backwards and forwards. If there's a view set up you just need to find a way to learn what the views actually are doing. Do this even if you're doing nothing but re-writing a snippet. It takes months to learn and memorize a large array of the most-used tables. Same goes for status codes, etc. Some of this stuff is documented, often it's not and you have to ask someone or refer to previous saved qork. A lot of BI analysis work amounts to "In a certain time frame" Count A, Count B, divide A by B. A lot of the hard work is making the counts are correct for the question at hand. A step further would be knowing which things to count to answer a given question. The step after that is knowing which questions you should be answering. The actual math is usually not difficult if you understand that how numbers work.
I think it is more of a constraint i have to add than make a change. For example if i add '1234567', it won't be accepted, but if i add '000411234567' it will be accepted, if i add '000427654321 it will be accepted. I am not sure if this is possible or not because i looked everywhere on Google but couldn't find anything that involved adding different prefixes.
ah... what you want is a [check constraint](https://en.wikipedia.org/wiki/Check_constraint)
**Check constraint** A check constraint is a type of integrity constraint in SQL which specifies a requirement that must be met by each row in a database table. The constraint must be a predicate. It can refer to a single column, or multiple columns of the table. The result of the predicate can be either TRUE, FALSE, or UNKNOWN, depending on the presence of NULLs. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I did the check constraint like this, but it now gives me another error ORA-00904: "SUBSTRING": invalid identifier ALTER TABLE JB_CELLULAR ADD CONSTRAINT NR_CON CHECK (UPDATE JB_CELLULAR SET NR = '00041' + SUBSTRING(NR, 5, 11) OR NR = '00042' + SUBSTRING(NR, 5, 11) OR NR = '00043' + SUBSTRING(NR, 5, 11) OR NR = '00044' + SUBSTRING(NR, 5, 11));
What kind of project and what kind of help? If you send me some info i'll see if i can help.
rn = 1 should retrieve the min, but the number of rows can be different per partition. In oracle I don't believe you can do a aggregated function in a where clause
what rn=1 returns depends on how you order it. Could be max or min depending on asc or desc. So you could just put two in there as sub queries, one asc and one desc, but that would probably blow it up with tons of data. And you're right... I even tried counting the partitioned field in the select statement and putting a where rn=1 or rn=p_count clause, but that doesn't seem to work either. 
I would also like to know this. I'm also doing Stanford's DB5 which comes very highly recommended. It's really enjoyable, reminds me of back when I was in college.
&gt; I ask for help but can't ask for too many times. Who says? Just because the other guy is picking it up faster than you (or SEEMS to... for all you know the difference could all be in your head, which is very common among programmers, look up imposter syndrome, we all get it) doesn't mean that you shouldn't be asking. The most frustrating people I've worked with are ones who will not ask questions. They work much too slowly and usually deliver stuff that doesn't work properly. I've been writing code and SQL for 20 years, and it still takes me 6 months to get comfortable at a new job because of having so much new stuff to learn. It's absolutely normal to be in the dark about where to get data in a database you've only used for a month! As far as asking too many questions, that can be frustrating for the people answering them too, but really only if you should know better by now, and a month in, I don't think you should. The advice I've been given by surly people who didn't want to answer questions was to try figuring it out yourself first, so that when you come asking you don't just say "where do I find the widget data?" but you say "I'm looking for the widget data, I've scanned the tables in these 4 databases, tried looking for code that references it, having no luck finding it." This shows you're not just using the other person as a crutch. But also, at this point in your career there don't feel like you have to spend tons of time on these things either. Lean into people on your team who want you to succeed, and this theoretically includes the other new hire. I started my current job on the same day as another developer and we have both helped each other more than anyone else has helped us.
I don't think a check constraint is going to be the best solution, as it would need to check for/validate multiple prefixes, which could mean a big clobbering CASE statement validating the concat of 3 valid prefixes (likely varchar) to the target phone number, also likely a varchar. That could be a performance nightmare, and isn't exactly nimble, should the prefix(es) ever change. 
Select column1,coulmn2,count(1) From t1 Group by column1,column2 Gets you all distinct combinations. If count(1)&gt;1, the combinatiom exists more than once in the table. You can then store the result of that query into a temp table and do whatever you want based on the count, or just add a having clause if youre interested in distinct combinations of column values in rows that exist at a particular frequency in the table. Its not exactly clear to me what youre trying to do but i think that will get you close to what youre after 
&gt; isn't exactly nimble upvote
Yep i managed to fix it. I used regex instead of the logical statements. Thanks for the suggestions above dude
I am an advanced Excel user, building things in VBA and userforms, the whole 9 yards. I recently started learning SQL because the things I know I can get from my data would be so clunky and take too much time to develop in Excel. A couple queries get me reports that can take an hour to compile in Excel using various pivot tables and napkin math.
‚Äú1) You want to find ERD / data dictionaries. This should be documentation they have created with tools that shows you how things work together.‚Äù Wouldn‚Äôt it be nice if this were always the case :) 
first/last touch attribution, marketing funnels, churn, product/customer/transaction data (all anonymized and nonconfidential). nothing with SSRS or SSIS tho. maybe we can work that in as an article item ;)
Yea some more info would be good.
Hey, i basically need to build a basic human resource database that can run some queries on the data. ive been given some specifics on what that has to entail. ive attached the spec and the schema that ive drawn up, however im not sure my schema is correct https://docs.google.com/document/d/1THMO1VfyMyvxIex_-TOu2nhQgn54IvHfqE8C3CALGW8/edit?usp=sharing roie
Question, why don't you have multiple cohorts running simultaneously instead of waiting for one to finish to start another?
I second this. I use vba in access and excel. I started SQL this week and it's already changing the game for me
Couldn't figure out SUBSTRING vs SUBSTR so you switched to REGEX? Username does not check out. 
I mean fake it til you make it and don't drop tables? It seems like his set up is relatively easy to pick up compared to yours and appropriate for entry level on-the-job (self) training/
Try playing with this: Phone LIKE '004[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]' The [0-9] acts like a wild card for any digit. Have a [0-9] for every unknown digit. Our issue was phone numbers were formatted in a mix of international format, national area code format and local format (where area code could be omitted). Fortunately, each format type has a different length and follows predictable patterns so this technique allowed for quite a simple query to match against each variation.
:(
What about setting code up as an identity column (or possibly a sequence)? 
Every time I transfer to a new Agency I am in the same boat as you. I have moved to 5 different Federal Agencies and Consultant work on the side. Each Agency and private employer are incredibly different (oracle, sqlserver, linux, windows, OBIEE, Tableau). Dont pressure yourself! It takes time to learn the data.They only way by learning is by doing. For me, I keep a large note book by my desk, and document anything new. In about 6 months you will be an expert in all things your company does. PM me if you want any more advice. 
I cant change it or add anything to database/tables, and table already has identity column. I want to fully learn unique values, but a lot of information is missing even from microsoft documentation, its all about creating, but nothing on the internet about how to use it after... Safety guarantees for unique constraints when inserting is my primary concern - by the lack of information i guess that you have to do it all manually, so how do i know that the value of unique constraint that i got, increased and tried inserting new data row with hasnt been already used, if there could be a lot of threads that constantly insert data ?
You don't "use it" after, it's simply a constraint on the column that stops you inserting a value that already exist in it. If you want to increment values for the code you're going to have to write a function yourself because you're the one who knows your requirements. Just think about how many diffrent ways there are to increment a 10 character code. 
I liked the schema layout mostly, it needs a little fine tuning but getting the requirements and needs down may help. In my mind, I'm picturing this as a functioning HR database for an enterprise company that needs to scale out well and handle a lot. It sounds like this probably more homework or interview related? Here's some questions / comments I had while looking at your schema. It can vary based on a lot of other factors. If the goal is to make a simple and basic schema / db to accomplish the task, then some of my ideas could take it too far. - You are assuming a department cannot be on more than one floor or at more than one location. - No auditing? Do you care about audit tables for historical? I would highly recommend temporal table setups if you want to cube out the data later for analyzing. At least have createdat, createdby, updatedat, and updatedby columns on tables.
don't use MAX(x)... tis a terrible pattern if it's not inserted immediately within the transaction, otherwise two calls can (and inevitably will) get duplicate values. you can use a custom SEQUENCE, which is scoped at the database instead of a specific table... same benefits of an IDENTITY (single use, etc)... then just figure out how to convert the new value into two columns. but i'd still question the design
I'm actually doing something similar but to sql Express! Trying to uniform all the data and get it one place vs our 6 systems...
If you're using SSMS and you click New Query it will default to whichever database is selected in the object explorer. If you add the server and database names to your query it's going to run it on that specific database, even if you last selected a different one in the object explorer.
In this case i am running both queries from the same place in SSMS which is why i'm confused here. If i include USE DBNAME above the select statement, i do get the same result, but otherwise it's like it's pulling from a different source...but it doesn't appear as such if i look at execution plan; shows the same source, but row count is completely different. Weird. Non-issue though....
Move your mouse over the tab your query is in when you get the wrong data source. It will tell you what the query is running on. I.e. "SQLQuery1.sql - servername.dbname (domain\loginid)" Best practice is to include server and db in the From query to avoid this issue.
I¬¥ve just found www.practity.com A compilation of links to blogs and webs with exercises and projects
When you quantify your query with [SERVERNAME] you are invoking a named linked server connection. So you are selecting from an entirely different table on a different server. 
What RDBMS?
I restored the DB again und couldn't find any information about total tables. https://i.imgur.com/Cuy7g73.png Do you need more information?
&gt;I cant properly format in excel as a date. I assume it's because it is now VARCHAR data? No, because plain text formats have no notion of data types. It's Excel screwing up the date formatting. It's trying to be "helpful" but it's being just plain difficult. Open the text file in a plain text editor like Notepad++. I bet you'll see the format you're expecting. Aside: your dates aren't stored in that format, if you're using a proper date/datetime data type. They're stored in a binary representation. The formatting you see in the SSMS Results Pane is based upon your local machine's regional/cultural settings. Date formatting should always be left until the presentation layer.
I would recommend to copy the output to a google doc and share that google doc, I can help more after that. 
I think T-SQL Fundamentals the book is exactly what you want. There are practice problems at the end of every chapter and it's incredibly informative on all the topics you are requesting.
select convert(varchar(10), getdate(), 112);
https://docs.google.com/spreadsheets/d/12GAoL2ioMgCCJykuHz4QbzWM-bsXk3H_Cwm-vqDrsqk/edit?usp=sharing Not 100% sure but I need latitude longitude data for google maps.
As someone who mainly works with 2008R2 so doesn't have lag one way of doing this would be to join the table onto itself. If you join on t1.pin = t2.pin and t2.SingupDate &lt; t1.SingupDate you'd get a table like the below. PIN SignupDate1 SingupDate2 1234 2017 2015 1234 2018 2015 1234 2018 2017 Once you've got that it's just a matter of grouping on PIN and the first date and getting the max for the second date and for every resignup date you've got the last date they did. It isn't going to be the most efficient way of doing this but it should work. 
Yep or TSQL querying. I‚Äôm really impressed by Itziks ability to explain things. 
Ya, Building on this it would be something like this: select Pin, SignupDate, MAX(PrevSignupDate) as PrevSignupDate, MIN(SignupDate - PrevSignupDate) as YearsToSignup from (select t1.Pin, t1.SignupDate, t2.SignupDate as PrevSignupDate from CUST_TABLE t1 join CUST_TABLE t2 on t1.pin = t2.pin and t2.SignupDate &lt; t1.SignupDate ) as T_Final group by Pin, SignupDate having MIN(SignupDate - PrevSignupDate) &gt; 1 
im sure this is a stupid question, but where in your statement is my DateWritten data field? Is you statement just going to return the current date at the time of the query? I have a bunch of written dates for prescriptions i want to list, along with their Rx numbers. 
select convert(varchar(10), DateWritten, 112);
Thank you for trying to help me but I found the data I need. Rightclick -&gt; select top 1000 rows. Worked with cs, sony vegas, belnder, since 15 years pc user... I fear no software... but SQL... It scares me.
Excellent! The query I gave you was basically saying, give me the metadata about the tables. What I use that for is correlating tables. Typically data in one table matches or has additional data of elements in another table. For example: Person table: ID | NAME 1 | Bob Job Table PersonID | Job Title 1 | sales The 1 references the person table. If you were to join the tables together on Person.ID and Job.PersonID, you would see the rows of data get joined together to form a longer string of data. So the query you have from me says, here are my tables. Here are my columns. I will often use the query to see what kind of relationships exist between the tables when they re-use the same column name and have the same data type. This is not a 1:1 guarantee, but it's a good starting point for investigating. The right click you used to get 1000 rows, is saying SELECT all the top 1000 things FROM TABLE. This gives you a 1000 row sampling of the data inside that table. From there, you could specify things. Maybe instead of SELECT EVERYTHING (which is the asterisk, it means every column '*'), you want to SELECT message_gps_available , message_type, message_ip FROM MSG_kre. The above, you are specifying specific columns to be returned. Let me know if you get stuck again.
Understanding SQL implies you have a general understanding of 2 things. Relational Database structure, and technical SQL knowledge (as a programming language). At the end of the day, I'm going to assume these guys aren't going to ask you to *literally* write functioning SQL queries in the middle of an interview, but if you are actually expecting that, message me and I'll provide you with some technical knowledge resources to get going quick. So with that out of the way, that being any sort of real functional knowledge of SQL, what you should focus on is an understanding of Relational Databases. A high-level grasp on the concept of Relational Databases almost *implies* knowledge of SQL. So go online, go to YouTube and start learning all of the theory behind relational databases, what they are, how they are applied in different ways, their limitations, and what they excel at, possible alternatives, like MongoDB, etc.. Coming in with this kind of knowledge will imply you know SQL in an interview, to a certain extent. Just brush over some beginner classes online if you are worried about a couple technical questions.
Thanks for the reply! Actually it's mutually exclusive, so a count of userkeys with rolekey 1 and rolekey 2 or 3 but not both 2 and 3.
Yes
How?
This is what I would do
I don't know if that was intended for me or not. Hopefully OP will see your response!
DON'T try to format the data in the query, keep it in whatever your local date format is or the rawest date format possible. Make sure it's still a "date" when it lands in Excel. Then in Excel just set the date formatting how you like. Always keep your data types intact and just do formatting at the end, in the "presentation layer".
Do a google search for "sql interview questions" and go through the results. It's that easy. What are indexes, denormalization, 3rd normal form, SQL syntax, etc. No need to wait for someone to answer your question on how to start, you should proactively research it yourself.
I have a test that is Platform agnostic which I use for my candidates and is a mix of code and theory. PM me your email and I‚Äôll send it to you, and will review/help with your answers.
I‚Äôm using Microsoft SQL Server. Both databases are on the server. Not sure if this is a simple issue- I‚Äôm pretty new to SQL
You might also post this to /r/Powershell
Definitely a different table. Maybe in a different database, maybe in another schema (other than ‚Äúdbo‚Äù. Maybe there is a synonym for that table, which actually points to a different table. Brent Ozar said it‚Äôs a fun trick he would play on people who don‚Äôt qualify their queries. Create the table in a different schema, then make the user‚Äôs‚Äô default schema THAT new schema. Then sit back and watch the devs panic. ‚ÄúOh I don‚Äôt know how that happened, could have been anybody, everyone‚Äôs sysadmin!‚Äù One of the funniest episodes of the office hours podcast. I think it was 11/22/2017.
 with Uno as (select userkey from userroles where role = 1), Dos as (select userkey from userroles where role = 2), Tres as (select userkey from userroles where role = 3), DosXorTres as (select userkey from Dos where not exists (select null from Tres where userkey = Dos.userkey) union select userkey from Tres where not exists (select null from Dos where userkey = Tres.userkey) ) select uno.userkey from uno Inner join DosXorTres; Doing this from my phone. Didn't test it. It feels gross. It probably doesn't work. 
So what you need is not an or but an xor, which on my platform (Oracle) is absent. But, saying "rolekey in (2,3)" can be converted to (rolekey=2 or rolekey=3), and what you need is a (rolekey=2 **XOR** rolekey=3), which your platform might support natively. Oracle likes things difficult but they assume everyone knows that XOR can also be built by AND, OR, NOT. For example p XOR q = (p AND NOT q) OR (NOT p AND q) So in that case (rolekey=2 AND rolekey!=3) OR (rolekey!=2 AND rolekey=3) aka 2 and not 3 or not 2 and 3. 
Would you mind sending it my way, too?
No worries, just PM your email.
SELECT Col1, Col2, ... **INTO &lt;table name&gt;** FROM Table1 JOIN Table2 ON ...
Know how to answer questions like "Suppose you have a table with students and their scores on recent tests. How would you find the average score of the students on the second most recent test assuming not all of them took it on the same day?" This is prime territory for row_number() over (partition by student_id order by test date desc) as rn
SELECT COUNT(case when role = 1 then userid end) as role1user, COUNT(case when role in (2,3) then userid end) as role2or3user FROM fancytable
I have one more ugly code but I think it covers all cases of: * Has to have role 1 * If it has role 1, only count it if it has role 2 or 3 but not both 2 AND 3. Note that I changed my dummy table to include the following cases: * Has role 1 with no other roles * Has role 2 and 3 but not role 1 * Has role 1 with 2 * Has role 1 with 3 * Has role 1 with both 2 and 3 with mytable as ( select 100 userkey, 1 rolekey from dual union all select 110, 1 from dual union all select 110, 2 from dual union all select 130, 2 from dual union all select 130, 3 from dual union all select 130, 3 from dual union all select 140, 1 from dual union all select 140, 2 from dual union all select 150, 1 from dual union all select 160, 1 from dual union all select 170, 1 from dual union all select 170, 2 from dual union all select 180, 1 from dual union all select 180, 2 from dual union all select 180, 3 from dual union all select 200, 1 from dual union all select 300, 1 from dual union all select 400, 1 from dual union all select 600, 2 from dual union all select 600, 3 from dual ), mytable2 as ( select userkey from mytable where rolekey=1 ), mytable3 as ( select mytable.userkey, count(*) from mytable2, mytable where mytable2.userkey = mytable.userkey and rolekey in (1,2,3) group by mytable.userkey having count(*) =2 ) select count(*) from mytable3; Result: 3 (110, 140, 170) 
SELECT B.userkey, B.rolekey FROM db AS B, (Select Userkey, rolekey FROM db WHERE rolekey = 1) AS A WHERE A.userkey = B.userkey AND B.rolekey &gt; 1 AND B.rolekey &lt;&gt; ‚Äò‚Äô ; You can remove the last line if there are no nulls. This code will return all userkeys that have 1 and another number. Do you want to count whether the userkey has two or three different roles? If so, I can change. Poor formatting on mobile. 
good information.
Link ?
Even though you didn't post a link, this needs to be removed. You're just attempting to funnel folks to your poor-quality YouTube channel. Protip for noobs: If anyone says you can "learn X in Y minutes", they're full of shit and either out to rid you of your cash or gain undeserved exposure for themselves.
&gt; Since code is a varchar, how do you define the sequence? Is A before or after 1? How do you generate the values? Well, database does it magically and automatically, actually. Think about ordering data - whatever symbols you throw at order, it orders them. Same should go for varchar generation, plus giving a list of symbols (only numbers 0-9) to limit the result to integer value, and its done, easy peasy.
sorrry i forget to add the link:) https://youtu.be/zlU4HwpvPdo
dear how you can say poor quality youtube channel and secondly i have mentioned one can learn the basics of SQL not the whole.
With a numeric code there can't already be something bigger than the max. The problem is that you can't be sure that something hasn't inserted a non numeric code so you'll need to make sure you're filtering anything like that out when working out the max.
 SELECT userkey FROM thetable WHERE rolekey IN (1,2,3) GROUP BY userkey HAVING COUNT(CASE WHEN rolekey = 1 THEN 1 ELSE NULL END) &gt; 0 AND ( ( COUNT(CASE WHEN rolekey = 2 THEN 2 ELSE NULL END) &gt; 0 AND COUNT(CASE WHEN rolekey = 3 THEN 3 ELSE NULL END) = 0 ) OR ( COUNT(CASE WHEN rolekey = 2 THEN 2 ELSE NULL END) = 0 AND COUNT(CASE WHEN rolekey = 3 THEN 3 ELSE NULL END) &gt; 0 ) )
"Learn SQL in 5 minutes"...video is 10 minutes long. Haha
So the unique constraint wont allow duplicate code+value combinations... but the whole use of max thing is a terrible design.
Just going to come out and say it. This is almost the worst way possible to handle this situation. If you're insistent on using a trigger, at least just use a stored procedure and pass the contract\_id over from INSERTED or DELETED.
Given the info OP has provided, what solution would work towards instead? Just curious to hear a different perspective. 
Create a stored procedure that whatever is updating the contract calls a stored procedure, which would do all of this. Any time there is a trigger, there is a very high likelihood that it can be replaced with a stored procedure. It's just better practice and makes diagnosing / debugging significantly more transparent. 
&gt; The problem is that you can't be sure that something hasn't inserted a non numeric code I know this, this is guaranteed, this is not the problem. My only problem is safe generation of next unique "code" value and inserting new row with it without the posibility of the "code" value being stolen by other insert query. Would something like this work ? INSERT INTO table ([code], [house number]) VALUES( (SELECT ISNULL(MAX(CAST(code AS INT)), 1)+1 FROM table WHERE [house number] = 'value_x'), 'value_x') Or maybe i need to wrap into some table lock or something ?
So what would be good/ok design then ? So far i havent seen a single example of what to do, so my terrible design is the best at the moment.
Email sent to all who PMed
I dont think you even read what i wrote : 1) I am not creating or adding anything to the database, i am only trying to understand it and use it, this is very important to understand. 2) The question we are looking answer for is "How to guarantee that INSERT statement is executed successfuly into table that has a CONSTRAINT UNIQUE column of type VARCHAR", not "Who said burp". 3) Leave identity, sequence and other stuff on the other side of the internet, there shall exist only INSERT, VARCHAR, CONSTRAINT UNIQUE, and SELECT.
Heavily seconded on moving all the opertational stuff out to a proc. Most shops would crucify you for putting business logic like that in a trigger. Your charge sheet primary key should be an auto-incrementer in whatever dbms this is - don't be your own key master by saying 'hey table, what's the max pkey' every time you want to insert a new row. As soon as you have something else that writes to that table you'll be getting 'mysterious' occasional failures from concurrency/key collisions. I'm reading logic as anytime I delete or update a contract end, calculate the charge as 10,000 plus $50 for every day over a hundred, and stuff that into a new chart sheet record including all recorded expenses. It doesn't appear to handle edits or deletes. Assuming the contract table is subject to add/edit/delete activity, I'd expect you to be testing for change to the relevant columns to decide whether action is needed like 'if new. enddate &lt;=&gt; old.enddate' then ... (Dont forget null checking as well). Passing contract ID, contract start and contract end to a proc as parameters would be the next logical step.
Thanks 
I'm forced to use a trigger, my teacher is trying to get us used to them for an assignment.
It's for a university assignment as a part of my advanced SQL. I'm finding it difficult following what your saying because of how new I am to SQL. To try and dumb it down, we can ignore the logic and assume that's correct. From that point is their any reason you can see syntactically that my trigger isn't being created.
how does it handle PL/SQL? does it have a debugger?
Well, this looks like PL/SQL, which I'm admittedly haven't look at for at least 6 years. I'd start off with something small, then expand it out. CREATE OR REPLACE TRIGGER create_new_record AFTER UPDATE OF contract_end ON contract BEGIN CASE WHEN UPDATING('contract_end') THEN DBMS_OUTPUT.PUT_LINE('Contract_End Update Trigger Hit'); END CASE; END;
Without knowing their transaction volume and other such things, I'll disagree with your recommendation. First, he's clearly using Oracle - not SQL Server. If he created a proc and only passed the ID value to it, he'd have to query the original table to get the other values. That's not allowed (mutating) unless you declare an autonomous transaction, but then you're dealing with dirty data. There's little benefit to creating a proc and then having to pass multiple values to it. You really haven't gained anything. If triggers are designed property, they aren't the work of the devil. 
\&gt; If triggers are designed property Most Oracle masters, and pretty much every professional for other RDMS platforms will likely disagree with that assessment. Triggers universally \(*with very few exceptions*\) always have preferable alternative solution; doubly so for ones that are performing any kind of DM operation. There may be methods to make them tolerable in certain scenarios, but they are rarely preferable. Even if OP uses a trigger, passing the business logic to a stored proc would still be infinitely preferable to what is presented here; and the already known value could be passed without doing a lookup. Triggers make diagnosing and debugging the thing of nightmares. On top of their ability to cause cascading performance issues on systems where audit compliance is mandatory \(ex HIPAA / Financial\).
Are you not getting an error of some sort trying to execute the trigger create ? - should be some feedback that it's syntactically incorrect if that's the case -- one shot in the dark if it's there and just not working - does your charge_sheet table have any current data ? - if not, the SELECT MAX(charge_id)+1 INTO CONTRACT_ID FROM charge_sheet won't get a value and insert into Charge_Sheet will fail on PK constraint - check https://stackoverflow.com/questions/1688715/select-maxx-is-returning-null-how-can-i-make-it-return-0 for your syntax by dbms - assuming PL/SQL... - if this is an assignment, name your local var CHARGE_ID instead of CONTRACT_ID (which is another primary key name) - I'd take points off for the confusing name mismatch 
It's possible that I need to do something at the end that I'm not aware of. I have just been hitting enter for any of the statements I have done so far. [Here is what it looks like when I hit enter after pasting it into the command window.](https://image.ibb.co/kDkD8c/1d13059c89a48f1c22f0d670307e58fb.png)
Sorry to say it, but it's true. Your videos are extremely poor quality. Please stop spamming this sub and others. 
[removed]
Instead of chaining the server name is there a reason you're not using *USING* to preface the query? 
Step away from the SQL and look at hypothetical business questions. I am completely ruffing here, but what questions would an MBA ask? Before I continue, note that SQL is not really a programming language insofar as you don't program complex branching beyond if null or if no data found etc. Other languages select or update the data using SQL. So, back to the MBA. If you had a database of historical stock prices how would you calculate derived technical aggregates such as Alpha,Earnings per Share or Return on Equity? Or if you had Real Estate data, what is the mean, median and mode price for a single family home? You can snag data [here](https://www.dataquest.io/blog/free-datasets-for-projects/) or [here](https://opendata.cityofnewyork.us/). Shift your thinking from the SQL to the problem domain. There are lots of problems. 
It seems like adding a / on the next line after makes it actually create the trigger
Where I work all of end user activity is done through proprietary legal software. From that software it allows a Sync to Sql... So 80% of my job us using sql directly and interacting with things like powershell, ssms, ssrs and a lil ssis. So, in my limited scope... I really don't have the experience to tell you whats best.. Id say just be open minded and think of each skillset as a new tool. We don't have to be experts but just knowing how to use reddit or Google to ask a question goes along way. Stay vigilant my friend goood things will happen. 
Well what I always tell new people is to run it and see what the error is, then look up the error code.
I HIGHLY recommend learning an ORM. It's a very powerful additional tool. Especially if you're doing backend database management. \* Seconding the knowledge of triggers and cursors. \* Database table partitioning is a useful feature as well and can accelerate database performance.
I didn't think about that, and this probably stems from me being new to relational databases, but wouldn't that dissolve the relationship between names and addresses? I.e., it would become: Names table: rowid|names Addresses table: rowid|addresses
I'm sorry for not explaining throughly. You will then have to create a joining or bridge table that maps a unique name to an unique address. So if Mark_hasareallylongname is ID 1 in users it would map to something like ID 1 and 2 for Addresses because he has two addresses. Here's a good example with your problem: https://code.tutsplus.com/articles/sql-for-beginners-part-3-database-relationships--net-8561.
Got it - thanks for the explanation. I think it is much clearer now, and I can see how this would consolidate the data.
I don't have much as experience, but being a DBA, you do learn to make some pretty creative scripts. Cursors is one of the more arguably advanced level syntax, but understanding when to use them and when not to use them. Creating a script to generate other scripts, or learning to audit databases and trace. How to use a bit of powershell also helps a lot since you can do a lot with it that you would otherwise not be able to on SQL. 
Here's my impression so far: pl/sql\(Oracle\) is VERY flexible: exception handling, all sorts of data sructures and objects, etc Other DB's are often not as flexible \- so you might want to use Python or other scripting languages for doing complex logic. I personally enjoy Python for "parametrizing" my SQL because it has the Jinja framework. I currently work with Teradata and I find its procedural extension confusing and poorly documented, so I just use python
Your resume isn't for "proving" what you know. It's more a list of how you applied what you know - types of projects, accomplishments, buzzwords. It gets you the interview - you demonstrate what you know in the interview itself. A GitHub is fine but what are you going to post there? Not everyone looks for a GitHub (I was never asked about it in my last job search), and it's a little easier to suss out in an interview setting whether or not someone knows their stuff with SQL than other languages/technologies.
What's being described here is *normalization* and it's one of the core concepts of relational databases.
Oracle has run on Linux and Windows for some time. MSSQL [just recently got on the Linux train](https://techcrunch.com/2017/07/17/how-microsoft-brought-sql-server-to-linux/amp/) . T-SQL‚Äôs main advantages over PL/SQL IMO are its popularity and cost. It‚Äôs definitely more popular due to Pl/SQL‚Äôs learning curve. It‚Äôs substantially cheaper as well, so companies tend to prefer it. It also has a more seamless connection to .NET framework apps. I love both platforms. The key is to find which one works best for your particular situation.
Interesting I'm go ahead and try that hopefully it can up my chances thanks swardson
Anything particular that I should know for the interview like maybe the types of questions they might ask so, also if I have no real world experience using sql how could I make that work for me then thanks for the input alinroc 
Make sure the check statuses match the given statuses in the question. I saw in your check table there was a status of 'good' listed. Other than that, looks good so far. 
Thank you! I hadn't spotted that!
Asceding order( lowest to highest) is the default for ORDER BY. You need to tell it you want descending (highest to lowest). ORDER BY count(contract_end) DESC
How would I then make it only show the ones with the highest number
You don't really include enough info to say what is going wrong. What you are doing is a somewhat old school and manual way of pulling data from the DB. You should look into a modern orm to use and get out of writing you sql select statements in line. 
It would be easier to help if you described the difference between "high score list", "rank list" and "score list." Also, you say you're stuck but don't say (except for maybe a couple of code comments) what you're stuck on. The basics are there but I'd recommend building smaller pieces that work before trying to assemble the whole program. Here are some pieces to work on: 1) Get your queries right. It is better not to do "SELECT *" and do "SELECT Name, Score" instead. 2) Do you really need to "delete past 10?" Does that mean you need to delete rows from NameScore if it has more than 10 or do you just need to display the first ten? If the latter, just do "SELECT TOP 10 Name, Score FROM NameScore ORDER BY Score DESC." 3) Your problems with SqlDataAdapter would be helped by finding and working from good code examples online. Here's a snippet I found after searching for "sqldataadapter fill listbox": private void BindData() { DataSet dtSet = new DataSet(); using (connection = new SqlConnection(connectionString)) { command = new SqlCommand(sql, connection); SqlDataAdapter adapter = new SqlDataAdapter(); connection.Open(); adapter.SelectCommand = command; adapter.Fill(dtSet, "Customers"); listBox1.DataContext = dtSet; } } 
You need to find a stored procedure programming book like: https://www.amazon.com/Microsoft-Server-Stored-Procedure-Programming/dp/0072262281 
&gt; actually you can group by any columns you want, not just PKs Not in this case, because consultant_fname and consultant_sname appear in the query results. You either need to explicitly list those two columns in the GROUP BY clause, or use the PK column. See: mydb=# create table foo (id serial primary key, bar text, baz text); CREATE TABLE mydb=# insert into foo (bar, baz) select 'abc', 'xyz'; INSERT 0 1 mydb=# insert into foo (bar, baz) select 'abc', 'xyz'; INSERT 0 1 mydb=# insert into foo (bar, baz) select 'def', 'xyz'; INSERT 0 1 mydb=# select * from foo; id | bar | baz ----+-----+----- 1 | abc | xyz 2 | abc | xyz 3 | def | xyz (3 rows) mydb=# select bar, count(1) from foo group by bar; bar | count -----+------- def | 1 abc | 2 (2 rows) mydb=# select bar, count(1) from foo group by baz; ERROR: column "foo.bar" must appear in the GROUP BY clause or be used in an aggregate function LINE 1: select bar, count(1) from foo group by baz; ^ mydb=# select bar, count(1) from foo group by id; bar | count -----+------- abc | 1 abc | 1 def | 1 (3 rows) 
Most of the time I do things in TSQL. Instead of wrapping things in c# method, I wrap them in stored procedure. It should be done with measure. Couple of SQL statements - yes, some aggregation -yes, moderate transformation of data, but not some complex string parsing.
&gt; You either need to explicitly list those two columns in the GROUP BY clause indeed, i believe that is what OP actually did in the original posting sure, two people can have the same firstname/lastname, but that's a different issue you said "You can group by consultant_job only if it's the PK of the consultants table"... ... and my point was, suppose it isn't? *you could still group on it* because you can group on any columns you want
You doubled up your create table at the beginning of the DDL.
Because I'm nice: http://sqlfiddle.com/#!9/c471759 Some other critiques - I'm not sure of the constraints of the question, but if this were real life some things I would consider: * Why are two of the IDs INT(10) but one of them INT(6)? * What if someone wanted to rent for less than a full hour? * Why would you have an index, muchless a UNIQUE on PRICE? This will cause a failure if two rentals have the same price. * You probably don't need the majority of those indexes on the final table
you do not need a GROUP BY clause if all you want is an overall aggregate in your case, the max actually is the max, but it may not be what you expect because you have a **cross join** try rewriting the FROM clause using **explicit JOIN syntax** including an ON clause
Disclaimer: I've got too many years designing databases, so some of this may be taking things further than you need to for a class assignment. However, in the real world, what happens is that you will get specifications from a client/user similar to what you've been handed, but you need to read between the lines and figure out what they will really need to be able to do. So take all of these comments/suggestions with a grain of salt and figure out what you need to do for this project. That said: * Storing age in an int guarantees that it will be wrong eventually. The only way to accurately store the age is to store the birthdate. * You may want to store the rental length as 2 date/time fields (start and end time). It will make it easier to query for items that are due/overdue. It will also make things easier if there are rentals that stretch over multiple days. (There are also valid arguments for storing the rental length in an int... but you may want to use a decimal or store minutes instead of hours... because in the real world, there are ALWAYS exceptions). * Don't store phone numbers as integers. Interestingly enough, your insert statement provides the perfect reason why, as your leading zero in the phone number is dropped from the value inserted. * Why is price a unique index? Also, if each piece of equipment has a standard hourly rate, you may want to store that with the piece of equipment. * In your equipment table, is make, model, and type enough to differentiate between multiple pieces of the same equipment? You'll probably have 2 boards of the same type/make/model, and even color. * You may want Type, Make and Model split off into separate tables. * Fields to store "notes" are always good to deal with things that have been overlooked (such as specific info about one piece of equipment). * Do you have any way to know what equipment is available to rent? (Just because it's past the return time doesn't mean that the equipment got returned, or that it is functional after being returned, and so on.) And what about equipment that is broken/stolen/otherwise no longer available for rental? You can't delete it out of the system due to the foreign key relationships, so you should record some sort of status. Ok, I think this should give you a bunch of stuff to think about... Good luck.
&gt; you could still group on it because you can group on any columns you want Well, yes, but if it's not a pk then you can't group on it so long as fname and sname are in the where clause. The goal is find the set of consultants who have the most contracts. Grouping by first+last seems like a bad idea. If there are two "John Smith"s they're going to show up as a single row and have the combined contract count of both individuals.
i was sorta hoping OP would discover this 
Doh. Sorry 'bout that.
thanks rewriting worked
Nah, you can just either calculate that in your select, or build a derived view on top of it that calculates it.
I can't say what your interview will ask because I'm not them, but based on interviews I've conducted and been the candidate for: * Know your join types * Know natural vs. surrogate for primary keys * Know index types (clustered, non clustered, filtered, etc.) * Know how to do things with SQL, not just the GUI tooling. * If asked about performance tuning, have a better answer than "I'd create an index" (but do know what index(es) you'd create) * If asked to write SQL on paper/whiteboard in the interview, don't worry about getting the syntax 100% perfect. As long as you don't have an error that makes the query invalid or produce wrong results, you're probably OK. * Don't BS the interviewer. If you don't know something, just own up to it. We can tell if you're trying to fake it
Oh, sorry. I meant the way that OP designed age as an INT. It would need to be recalculated periodically no? It makes sense that you could have a simple calculation in a view somewhere with the actual date to be calculated.
Oh yeah, with the way the OP designed it, it would need a scheduled job or some other methodology to go through and recalculate it periodically - you're completely correct.
Hah, you're right. I never even thought of that. +1 for birthday. The only other use case is if it's just for marketing information, and a new customer id is created each time, but that seems like a stretch.
Defiantly got a lot to review lol, really appreciate the feed man
For reference what kind of SQL are you using? Now I'm not the master of all varieties of SQL and haven't had to use the EXCEPT statement before now but looking at: https://www.tutorialspoint.com/sql/sql-except-clause.htm Because it's comparing rows and, as you have said, you need the same number of columns, any extra columns you want in the output you should add to both SELECT statements. I.e. this ``` SELECT cust_num FROM customers EXCEPT SELECT cust_num FROM customers WHERE order_date &gt; "2017-01-01" ``` Becomes ``` SELECT cust_num, another_field, jobby_whatsit, that_other_thing FROM customers EXCEPT SELECT cust_num, another_field, jobby_whatsit, that_other_thing FROM customers WHERE order_date &gt; "2017-01-01" ``` One last note, the way I'd do it, but I'm not certain if it's more efficient would be a SELECT statement in a WHERE clause in the original query rather than combining two, like so: Becomes ``` SELECT cust_num, another_field, jobby_whatsit, that_other_thing FROM customers WHERE cust_num NOT IN ( SELECT cust_num FROM customers WHERE order_date &gt; "2017-01-01" ) ``` Hope that helped, apologies if I didn't explain it well enough, first time reaching out on Reddit.
Yes, this is the only way to do it. The main use case I've found it for is in some of our ETLs where I'm essentially using them to compare against the source data to determine what rows have changed so they can be dealt with. That and ad-hoc QAing.
Not exactly a solution for OP, but I really like using `EXCEPT` without a `FROM` clause, like this: UPDATE target SET Column1 = staging.Column1 , Column2 = staging.Column2 , Column3 = staging.Column3 , Column4 = staging.Column4 , Column5 = staging.Column5 , Column6 = staging.Column6 , Column7 = staging.Column7 , Column8 = staging.Column8 , Column9 = staging.Column9 FROM target INNER JOIN staging ON target.PK = staging.PK WHERE EXISTS (SELECT staging.Column1 , staging.Column2 , staging.Column3 , staging.Column4 , staging.Column5 , staging.Column6 , staging.Column7 , staging.Column8 , staging.Column9 EXCEPT SELECT target.Column1 , target.Column2 , target.Column3 , target.Column4 , target.Column5 , target.Column6 , target.Column7 , target.Column8 , target.Column9); This example will only update records that have a difference in the columns being updates.
That's what we did originally, but the source data is essentially giant CVS files and it required generating the hash for each row of the source each time. The compare faster, but the whole process including the hash generation ended-up taking far longer than doing it with the except. 
I already did that. I wanted something more high level and practice more difficult questions so that I don‚Äôt fail the interview. 
Since this is a homework problem I'll give you a hint instead of solving it for you. Another way to phrase the problem is to return all pilots that don't have a plane they can't fly that's in the hangar. You'll need to use NOT EXISTS subquery.
select distinct c.name from capability c where c.name not in (SELECT c.Name FROM Capability c LEFT OUTER JOIN Hangar h ON c.Aircraft = h.Aircraft where h.aircraft is null)
This is my first thought on how to accomplish it... Select the names of the pilots from Capability. Inner join Hangar on aircraft = aircraft. Group by name. Having a count greater than or equal to the count of rows in the Hangar table. This assumes that the Capability table has a unique constraint so you can't have duplicate Name/Capability combinations... but takes into account there might be duplicate Aircraft in Hangar SELECT c.Name FROM Capability c INNER JOIN Hangar h ON h.Aircraft = c.Aircraft GROUP BY c.Name HAVING COUNT(c.Name) &gt;= (SELECT COUNT(DISTINCT Aircraft) FROM Hangar) Really not sure on how sound this logic is, but it worked on all the tests I did...
Topic to study: Normalization Denormalization Indexes OLTP vs OLAP, explain with real life examples 1st vs 3rd normal form LEFT vs RIGHT vs FULL vs OUTER JOIN Windowing functions RANK vs DENSE RANK WHERE vs HAVING CTEs Execution Plans Define ACID CAP Theorem For the job you are applying for, find out in advance what platforms are being used. Odds are you can get this from LinkeIn profiles. Research those specfic database systems. An interview for a SQL Server shop is going to be very different than a hadoop shop. Do not expect to answer every question 100%, just go in with what you know. Interviewing is a numbers game any how. Line up enough interviews, learn from any mistakes, and you'll do well.
Wouldn‚Äôt something like this be sufficient? SELECT * FROM Capability WHERE NAME = 'VIPER' AND AIRCRAFT IN ('F16','F22','Mig28');
Fun problem, in that it was trickier than I expected it to be. select Name from Pilot where Name not in (select p.Name from Pilot p, Hangar h where not exists (select * from Capability c where c.Name = p.Name and c.Aircraft = h.Aircraft));
Thanks so much! Very helpful!
Make sure you remember to use NOT. You know. To invert it. https://m.imgur.com/cnkUFvH
I guess you dont have one. I'll continue using W3 schools then.
So I just started learning SQL last week. I downloaded MySQL and HeidiSQL and started throwing a lot of our data that's normally housed in Excel into some simple tables. The things I can do with the data in one hour now would have taken day(s) to implement in Excel. I'm working on revamping some of our daily/weekly/monthly metrics using SQL queries instead of aggregating everything in Excel and presenting them to my boss. I absolutely love creating tables and working hands on with the code.
*cough*-ullshit!! ;)
I'm glad it was tricky for you too, and that I wasn't missing something. Thanks for the reply!
That worked, thanks! I just had to add WITH CTE AS and do a count of userkey from the CTE. I'm on MS SQL server 2008 so XOR isn't an option.
Thanks for everyone's help!
SQL is SQL at that level and the different DBMS' will be negligable in the difference if any at all
I haven't tested it but this \*\*might\*\* work. I can't recall how SQL evaluates string operators. SELECT DISTINCT CONCAT\(CASE WHEN a \&gt; b THEN a ELSE b, CASE WHEN b \&gt; a THEN b ELSE a\) FROM table
Wouldn't that just return either audiaudi or toyotatoyota?
Good point, they're wanted in separate columns. Let me fix that.
Okay, this one took a little thinking, but I think I've cracked it - /u/haribofiend had the right idea, but didn't implement it quite right (completely understandable, since s/he appears to have been unable to test it, in which case I'm sure they would have come up with this too) WITH magictable AS ( SELECT a, b, CASE WHEN CONCAT(a, b) &gt; CONCAT(b, a) then CONCAT(a, b) ELSE CONCAT(b,a) END as magicfield FROM example ) SELECT MIN(a), MAX(b) FROM magictable GROUP BY magicfield http://sqlfiddle.com/#!18/eab0d/1
this is an ugly hack
Yep, I was testing it just now and realized SQL string comparison is a little funky and didn't return correct results. Comparing post concat strings solved it.
Being a decent SQL programmer is all about the ugly\-hack.
select distinct least(a,b) as a, greatest(a,b) as b from mytable
Okay, I wanted to refine wtf that hack was before. A better understanding of the problem would help but the OP is essentially trying to do some set mathematics/transformation. SQL doesn't really do this out\-of\-the\-box, per say, so funky solutions tend to be my first though. Then I realize how much of a fool I was and that the solutions was much simpler than I thought. SELECT DISTINCT CASE WHEN a &gt; b THEN a ELSE b END AS a, CASE WHEN b &gt; a THEN a ELSE b END AS b FROM temp;
This is by far and away the shortest is where I was heading as well. I like that it's succinct without any crazy hack or delving into a CTE or sub query. NICE JOB!
I think you should really expand and add some leafs to the diagram. I try hard not to hard code things like status codes, gender and other variable attributes to data. It's a good idea to have attribute tables describing some sort of code that acts as a foreign key on a fact table.
damn impressive! have an upvote
Wow! Was not expecting such detail! Thank you so much for all of this, I have changed my database to correct for your bullet points! I really appreciate you taking the time to write it up and format it :)
Thank you! I have fixed the IDs, regarding the hours I'm going to assume its minimum 1 hour rental, the PRICE was suppose to be unsigned, not UNIQUE, oops.
I found this to very useful for me just recently actually. https://www.youtube.com/watch?v=Et2khGnrIqc Run through the example with that guy will teach you a lot.
what type of position are you applying for? If it's a front end developer you might get lucky with some super basics. If it's a DBA position, well...
Sololearn has a good app to learn it on the go. Make sure you grab their one that isn‚Äôt language specific-all the languages are rolled into on app now.
You're welcome. I was glad to see you actually put forth an effort instead of just "hey Reddit, here's my assignment, what do I do". Also, you posted pictures of your database design. It seems like most people just post a bunch of create table statements, but it takes too much mental effort to read the code and create a visual model in my head... but I can do pictures! (And in all honesty, database design is fun to me) Let me know how you end up doing on your project. 
The absolute best thing you can learn about SQL are those venn diagrams that explain what the joins mean. Joins are a key advantage of SQL databases and still so many in industry do not realise this. Here ya go https://stackoverflow.com/questions/13997365/sql-joins-as-venn-diagram
The first one is a simple select from the students table. The second requires a join between students and teachers. The third references a table that you didn't describe?
Its a mix of sql and c# sorry if you were looking for only sql, yea that wont be as useful for you then.
Hah, I was about to post the opposite of that. I almost never used SQL as an Oracle DBA beyond the extreme basics. "Select \* from table". Now that I'm a developer, I do like 60&amp;#37; Python and 40&amp;#37; SQL. Probably depends on the organization.
I would split the address into multiple columns: Mail Address 1, Mail Address 2, City, State, Zip. That way you can query for targeted geographic areas for mailing promotions. No email address? Come on, it's 2018! ;-) Also, I wouldn't name your date fields "Date". Try to avoid using reserved words as field names.
SQL query is EZ to learn if all you have to do is joins and aggregate functions. Check out SQL for newbs on Udemy
If you just want to cram some basic essentials, w3schools is your best bet. You won't learn anything too advanced, it's all easy stuff, but it'll tell you what you need for that kind of skill level and it's very easy to absorb. 
&gt; I almost never used SQL as an Oracle DBA beyond the extreme basics. "Select * from table". How long were you a DBA? &gt;Probably depends on the organization. I think any reasonable organization would expect a DBA to know more than "the extreme basics" of SQL.
sorry i just realised ive missed the picture out of the first post [http://i63.tinypic.com/2rdc9zd.png](http://i63.tinypic.com/2rdc9zd.png) thats how id like for the results to be viewed \(disregard the field name headings\)
&gt;You could union the two queries, but they need the same number of columns. Number, order *and* type.
Please use Imgur or Reddit for your images. Tinypic doesn't let RES inline the images.
What have you attempted so far? This sounds like homework, to be honest.
Are there common interactions between SQL and c#? I spend like 95% of my workday researching data migration issues using SQL but never use c#.
Nice quick read, thanks for this!
1. Create an empty table with the same structure, but the column renamed. 2. `insert into newtable (field1, field2, renamedfield) select field1,field2, originalfield from oldtable` (optionally `originalfield as renamedfield`)
What are you doing with it? Knowing the reason you want to learn SQL helps us determine what you should learn.
The better part of 3 years, though one of them was as an intern. Most of the responsibilities fell into one of two areas: 1) Upgrading databases from AIX to Linux (so lots of Bash) --- or figuring out why they weren't upgrading, which took a lot of time since there was a lot of legacy stuff floating around 2) Figuring out why data wasn't getting loaded right with a mixture of pre-compiled COBOL and PL/SQL scripts that we weren't allowed to change for data that we also weren't allowed to change. So basically, find the needle in the haystack and tell the developers about it.
You could remove their permissions on pg_catalog and other similar objects if you don't want them to see the structure of those schemas 
Here is some sample code: https://github.com/dejan-shuki/mssql-stored-procedure-programming/blob/master/asset5-scripts.sql 
SSMS is useless without a SQL Server instance (Express &amp; Developer editions are free for OP's purposes) to connect to.
Will give it a go and report back
Not homework. Just a novice. Not much work with dates in the past. UPDATE [TBL 000_Order Detail_Master] SET [TBL 000_Order Detail_Master].Client_Category = IIf([First_PSO_Fiscal_Conversion]=[Order_Date_Fiscal_Conversion],"New Client","Existing Client") WHERE ((([TBL 000_Order Detail_Master].[Reporting Group])="Professional Services")); 
LEAD/LAG window functions perhaps? 
Welcome to a whole new world of querying :)
Select x.a x.b x.c as foo Into new_table From old_table x
Ensure you don't have the security patch KB4055002 installed.
Thanks for the reply! To give you a bit more context, I believe what I'm thinking is similar to the "Extra Fancy" method you mentioned in that the first `date_range` and subsequent `time_range`s that a user defines is the "Master / default" range. From there, a user could add additional ranges to cover specific periods of time that would override the default. For instance, if a users differing availability for a specific date range, (say 5/2-5/10), they could create a new range with new daily availability; This new range would then take precedence over the "master/default" until the end_date/end_time of the new range has passed, in which case the default goes back into effect. If you happen to know of any resources/tutorials that you think might be helpful in architecting this, I would be really greatfull!
I have resolved this issue to the best of my ability. I need to have my image upgraded to Win10. The blend of security, OS version, and SQL versions is making it a requirement.
I think you would need a `master_date_range_id` to define that "this `date_range` is overrides that `date_range`". These sorts of hierarchical queries can get pretty messy.
Yeah, totally agree. The actual table is a bit more complex and has a "master" boolean column. I had left it out as to not overcomplicate the question. Eventually, I will want it to be possible to add multiple `time_ranges` for each day-of-the week as well, for instance: Monday 9-12, 1-5, etc.
Unsure if this is the answer. Reviewing over the database it sees that pg_catalog is nothing more than another Schema. So I'm really beginning to believe the functionality I am looking for isn't possible.
I think I told you the wrong object (it was off the top of my head). My situation is that I had some users who I did not want to see objects outside of their schema. I "revoke all"d on all schemas in pg_catalog.pg_namespace where the nspace name was not &lt;desired schema&gt;, information_schema, or pg_catalog. After that I "revoke all"d on a few catalog tables: pg_user, pg_roles, pg_group, pg_authid, pg_authmembers, pg_database, pg_tablespace, and pg_settings.
You may like Google BigQuery. https://www.youtube.com/watch?v=qqbYrQGSibQ
I'd highly recommend Postgres. Though, if you want to go the cloud direction and deal with only flat\-files, u/Xesteanov's suggestion for BigQuery may be better. Setting up a server isn't too hard. Postgres and MySQL make it easy, so does SQL Server, I suppose. Either way, you're probably going to be getting CSV dumps from those sources. Are you going to be using an ORM?
To reject &gt;.5 values - look into check constraints. To make them .5 - look into triggers.
I asked about the ORM because I use on a lot. My work stack is usually Python \+ Postgres so I use the SQLAlchemy ORM and it's all kinds of wonderful. This may help in those regards: [https://stackoverflow.com/questions/1279613/what\-is\-an\-orm\-and\-where\-can\-i\-learn\-more\-about\-it](https://stackoverflow.com/questions/1279613/what-is-an-orm-and-where-can-i-learn-more-about-it)
I use SQL Server (Microsoft), and you can download SQL Server Developer edition for free and run it locally. Meaning your computer is the server that you connect to; there is no cost for this. This will work fine for importing data as well. I am not familiar with Tableau Public, I would imagine that it would need to access your data. When you get to that point you may need to upgrade to a database on a publicly available server. You can still do this relatively cheap with things like Azure / AWS. I would personally do the local server stuff for now since it costs nothing, and then cross this bridge when you are ready for the Tableau stuff. 
Shall try again in the morning. It's about that time where I start accidentally dropping tables for no reason whatsoever. 
create table test_table ( percent_amount number(2), constraint check_percent_amount check (percent_amount &lt;= 0.5) );
Just a guess: the phrase "a young boy's grandfather" has an embedded apostrophe which probably throws the parser off. As a quick test, remove the apostrophe so that you have "a young boys father" and see if that works.
Emails sent out again
Crows foot identifies the foreign key columns in a table. The "crow's foot" that connects the tables together shows us how the columns in one table relate to the columns in another table. 
You're welcome. They're super useful so long as you don't fall into the trap of relying on them when you don't have to... like I did when I first discovered them!
A little context and a small example will get better answers.
Yes
Yup, some more context, a small data sample, a small table example, and an expected output would help us give better answers.
Does this work? SELECT * FROM `license` left join `support` on `license`.`License_Number` = `support`.`Lizenznr` UNION SELECT * FROM `license` right join `support` on `license`.`License_Number` = `support`.`Lizenznr` https://stackoverflow.com/questions/4796872/how-to-do-a-full-outer-join-in-mysql
It does, thank you very much! So, there is no actual FULL JOIN in MySql, did I read that correctly?
Does the order of the array matter? I feel like it does, but I want to clarify first. How are you getting these strings? Are you passing the strings from an application to the table?
For completion, this is my Statement with the result I wanted: SELECT * FROM `license` left join `support` on `license`.`License_Number` = `support`.`Lizenznr` UNION SELECT * FROM `license` right join `support` on `license`.`License_Number` = `support`.`Lizenznr` UNION SELECT `license_Number`, `E_Mail`, `Name`, `Address`, NULL AS `NoName`, NULL AS `Lizenznr`, NULL AS `EMail`, NULL AS `Unternehmen`, NULL AS `Strasse` FROM license All matched up and the entries from license again with no matches. It might be weird but it is gonna make sense in my webinterface. Again thank you and as you can see in my Query, you need to define "NULL as `something`" not the other way around.
How long are the comma delimited strings? Is the relationship completely arbitrary? If the strings are short enough, hammer it manually with a txt editor. 
Tbh, I'm not sure.That's an area of PgAdmin I haven't played around with enough. What is it for?
Which handles fixed width file imports better? MySQL or Postgres? I run MySQL on my current machine but my datasets can be enormous so the imports can take time. It's not a big issue, and really doesn't warrant a switch. More of curiosity's sake.
Why not do it in a single insert statement if it is not too many rows? insert into table2 (select col1, col2, 'statictext1', 'statictext2' from dbo.clients); Depending on size of table and platform, you may want to break up this to avoid filling up the active transaction log.