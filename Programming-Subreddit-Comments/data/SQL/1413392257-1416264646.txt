Thank you for the offer -- this subreddit has been incredible. 
Don't have the time to write it up right now but here's what I would do. First find the depth(level) each node is at and the path of it This should help.: http://www.sqlteam.com/article/more-trees-hierarchies-in-sql Do that in a temporary table. Then one by one update the left right values as if you were adding each node to the left right table. I was looking for a link to do that and stumbled upon: http://www.honk.com.au/index.php/2010/04/22/convert-adjacency-list-model-to-a-modified-preorder-tree-traversal-mptt-model-hierarchy/ 
This is really good info. Thanks for sharing
Something like this?: http://mikehillyer.com/articles/managing-hierarchical-data-in-mysql/
How many articles on joins do we really need? I think we need a few more...
the venn diagram which is used for the thumbnail image for this thread is garbage it suggests that there are a lot more rows in the band_members table **for bands or members that don't exist** than for ones that do exist so, yeah, massive fail at illustrating the many-to-many relationship
I think a /r/LearnSQL subreddit would be a good resource to nurture. Especially since /r/java has /r/javahelp to separate the discussion from the learning. My 2c.
That makes me CROSS
Well, that's NORMAL.
Most if not all of that content is trivial and covered by the wiki links on the sidebar. I vote 'ban'.
I'm always annoyed by basic tutorial links. I usually think it's a new user with a very simple question, then boom I'm reading a pointlessly verbose W3Schools article on joins. It's even worse when the tutorial is database or software specific. Nothing that shows you how to use SSMS or SQL Developer belongs here. Those should be in /r/sqlserver and /r/oracle 
Dude, bad FORM.
Same, I'm nowhere close to being a SQL expert, but joins are going to be about the first thing anyone serious about learning is going to pick up. Join tutorials do nothing to promote interesting discussion or real learning.
What have we got INTO?
if it's critical to get them as you want I'd write up a mess of code something like: ORDER BY LEFT(IP,CHARINDEX(IP,'.')), LEFT(IP,Patindex(IP,[0-9].[0-9])+1) It'd be a mess and I don't have the time right now to think it through. Maybe someone else has a better idea. Good Luck
I am going to have to INSERT my opinion here.
Should this area not be open to newcomers? Because I tell you what, I personally do not skim through pages and pages of this subreddit.
r/Oracle
It's certainly open to newcomers- my point is just that there are a gazillion articles on joins out there. Anyone looking to learn SQL is going to google and find them. Yet another one just seems like some guy trying to pad his blog. Which is fine, I guess, just redundant. I'd much rather see articles on things that aren't so well-trodden.
&gt; Do you guys know any other ways of solving this problem ... 1. Switch to PostgreSQL. (I know. I know.) PostgreSQL has native support for "inet" and "cidr" data types. They sort the "right" way, and they have error checking on input, among other advantages. [Docs](http://www.postgresql.org/docs/9.3/static/datatype-net-types.html) 2. Write your own user-defined type to support IP addresses. (Someone has surely *already* written a UDT for this; search the web.) 3. Convert the IP address to an integer. (An IP address is really just an integer.) The formula is ( 1st octet * 256^3 ) + ( 2nd octet * 256^2 ) + ( 3rd octet * 256 ) + (4th octet ) You could build that calculation into a user-defined function, for example. 
I vote for ban. SQL tutorials can be found everywhere on the web. Direct links in the sidebar may provide a bit of extra. 
PARSENAME function will take the octets out. 4 is the first octet and 1 being the last (it just works backward). ORDER BY CAST(PARSENAME([IPAddress], 4) AS INT), CAST(PARSENAME([IPAddress], 3) AS INT), CAST(PARSENAME([IPAddress], 2) AS INT), CAST(PARSENAME([IPAddress], 1) AS INT)
This works if the ips are stored with leading zeros.
Thanks for the feedback. We will look into it. Are you on an Android device?
SOMETHING ABOUT A MERGE JOIN
I would put it into a job step, and then find the code for executing a sql job.
There are a handful of other options, but this is by far the easiest, most robust, and most secure way to do this. Some other options include: Xp_cmdshell Sqlclr Service broker 
I think there are still a few LEFT
When I look at the Venn diagram, I see three tables intersecting. In a perfect, non-orphaned world, this would just be one circle. I'm not sure that one circle would accurately describe joins :)
But it won't sort correctly. 10.01.22.35 and 10.01.2.235 both become = 100102235 and would sort different from 10.1.22.35 (the same first value)
Which platform?! It makes a huge difference as RDBMS vendor SQL and their SQL based programming languages vary in capability and syntax.
We just have to APPLY ourselves...
Take a look at this: http://www.mssqltips.com/sqlservertip/2992/how-to-execute-an-integration-services-ssis-package-from-a-sql-server-stored-procedure/ 
Not sure how to link on my phone bur this has come up before. You might the following useful. http://www.reddit.com/r/SQL/comments/2g0rll?sort=confidence 
Can you have the job itself run the next step?
It's not too bad. You can run sp_start_job, and follow it up with a while loop that checks the status of the job. Honestly the other options for synchronous SSIS package execution aren't any better, and have issues of their own. SSIS isn't really designed for synchronous execution. 
try GROUP BY with MIN(requestdate) and MAX(canceldate)
yeah, you should make one
So far distinct and group by have been mentioned. So I'll give alternate advice, try a row number over partition then select your results from that query where the row number = 1. 
You can ask here
Saat kita sudah membuat sebuah database dan kemudian tabel seperti pada postingan sebelumnya tidak menutup kemungkinan kita mendapati kesalahan pembuatan tabel tersebut atau kita lupa menambahkan satu kolom dalam tabel tersebut. Di artikel SQL ALTER Statement Syntax dan Contoh ini akan kita jelaskan tentang bagaimana cara merubah komposisi tabel tersebut menggunakan SQL ALTER STATEMENT.
That's probably more vague than you intended. What do you by "based off"? Are you making a single database for all cafes or a database for each cafe? 
Wasn't sure how to name the thread, soz. I meant for use in a single internet cafe
something similar to [this](https://github.com/marczinusd/InternetCafe/blob/master/InternetCafe.sql)
So what need is not being met in that sample?
Thanks all. Tutorials shall no longer be permitted on this SubReddit.
I don't graduate until May, but from what I've heard from recruiters, your education isn't supposed to teach you how to code really well. If you can show to them you have the ability to pick up SQL, and you have a regular business degree, and you can speak competently about what you have used in SQL they will be interested. All of the companies I interviewed with had training programs to hone in on specific skills that they need. In short, I think you are ready to start talking to recruiters.
Awesome. Thanks. That's good to hear. What do you think of the MS Certificates?
Right on. Hoping I can go that route. What do you think of MS Certificates?
Microsoft certification is a good idea. But there is no need to pay thousands for the classes. Get the books, self study, do the exams. There is lots of information online to assist. Get a copy of SQL Server running on your local PC, then practice practice practice. 
Thanks for the advice. I actually have done both (getting a copy of SQL Server and studied online myself) but I am just really nervous about paying $250 to take a test if there is the remote possibility I could fail that test
No reimbursement unfortunately, but if it makes me more hireable, I should probably go ahead and do it?
I am not sure why you were downvoted, your concern is perfectly valid, it is not cheap to re-take these exams. I found a lot of the exam questions/answers to be very ambiguous and spent a lot of time studying the answers and questions available on the Internet. That way you have a clear feel for the answer they are after.
This is hysterical
I started as in an App Support role with no SQL knowledge. Mortgage as well. I used it everyday as a powerful tool and have become incredible proficient, and have now moved to higher spots. You can absolutely learn in with any degree, and it will become a powerful asset
I would think you would be able to gather the latest date by utilizing the MAX function. I, however, am not sure what that might be, but it has to exist. I could research for you if you are interested. Right now I'm on a phone, so it makes any further research a bit problematic. 
Here you go... drop database if exists InternetCafe; create database InternetCafe; use InternetCafe; create table Employees ( ID integer PRIMARY KEY AUTO_INCREMENT, username varchar(30) not null, password varchar(10) not null ); create table Clients ( ClientID integer PRIMARY KEY AUTO_INCREMENT, username varchar(30) not null, password varchar(10) not null, address varchar(50) not null, ID_Card_Number integer not null, isLoggedIn boolean not null default 0, loggedInAt integer not null default 0 ); create table Computers ( ID integer primary key AUTO_INCREMENT, hardware_specs varchar(80) not null, operating_system varchar(20) not null, isUsed boolean not null default 0, usedBy integer not null default -1 references Clients(clientID) ); create table ComputerUsage ( SessionID integer PRIMARY KEY AUTO_INCREMENT, ClientID integer not null references Clients(ClientID), ComputerID integer not null references Computers(ComputerID), BeginTime DATETIME not null, EndTime DATETIME not null, Payment integer default 0 ); create table Logins ( ClientID integer not null references Clients(ClientID), loggedInAt integer not null references Computers(ID), loginTime DATETIME not null ); create table Payments ( ClientID integer not null references Clients(ClientID), Payment integer not null ); INSERT INTO Employees(username,password) VALUES('sanyi', '1234'); INSERT INTO Employees(username,password) VALUES('pisti', '4321'); INSERT INTO Employees(username,password) VALUES('teddy', 'alligator3'); INSERT INTO Employees(username,password) VALUES('endre', 'bakancs'); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Adam','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Abby','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Aarom','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Abigale','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Andrew','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Amanda','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Alex','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Alice','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Clients(username,password,address, ID_Card_number) VALUES('Annie','1234','New York, Reinhold Towers, 327 Southside',12345); INSERT INTO Computers(hardware_specs, operating_system) values('Intel Core i3, integrated graphics', 'Windows 7'); INSERT INTO Computers(hardware_specs, operating_system) values('AMD Phenom X4, integrated graphics', 'Debian Linux'); INSERT INTO Computers(hardware_specs, operating_system) values('Intel Core i5, integrated graphics', 'Windows 7'); INSERT INTO Computers(hardware_specs, operating_system) values('AMD Bulldozer, integrated graphics', 'Debian Linux'); INSERT INTO Computers(hardware_specs, operating_system) values('Intel Core i3, integrated graphics', 'Windows 7'); INSERT INTO Computers(hardware_specs, operating_system) values('iMac', 'OS X'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(1,2,'2011-07-04 21:03:10', '2011-07-04 22:03:10'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(2,4,'2011-07-04 21:03:10', '2011-07-04 22:03:10'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(3,1,'2011-07-04 21:03:10', '2011-07-04 22:03:10'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(4,5,'2011-07-04 19:33:10', '2011-07-04 22:25:10'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(5,3,'2011-07-02 21:03:10', '2011-07-02 22:03:10'); INSERT INTO ComputerUsage (ClientID, ComputerID, BeginTime, EndTime) VALUES(6,1,'2011-07-05 21:03:10', '2011-07-05 22:01:10'); INSERT INTO Logins Values(3,2, '2011-07-05 21:03:10'); INSERT INTO Logins Values(2,3, '2011-07-04 21:03:10'); INSERT INTO Payments VALUES(1, 1000); INSERT INTO Payments VALUES(2, 1000); INSERT INTO Payments VALUES(3, 1000); INSERT INTO Payments VALUES(4, 1000); INSERT INTO Payments VALUES(5, 1000); INSERT INTO Payments VALUES(1, 1000); /* update ComputerUsage set Payment = (EndTime - BeginTime) / 500 where EndTime &gt; BeginTime and SessionID &gt; 0; select ClientID, sum(payment) from Payments group by ClientID having ClientID = 3; update Computers set isUsed = 0 where ID &gt; 0; create table ClientsWithBalance as select ClientID, ifNull((Second.SumOfPayments - First.Debt),0) as Balance from ((select ClientID, sum(Payment) as Debt from ComputerUsage group by ClientID order by ClientID) as First natural join (select ClientID, sum(payment) as SumOfPayments from Payments group by ClientID order by ClientID) as Second) order by Balance; /* update Clients set Balance = select Full.Balance from ((select ClientID, ifNull((Second.SumOfPayments - First.Debt),0) as Balance from ((select ClientID, sum(payment) as Debt from ComputerUsage group by ClientID) as First natural join (select ClientID, sum(payment) as SumOfPayments from Payments group by ClientID) as Second)) as Full) where ClientID = Full.ClientID; */ /* select ClientID, sum(payment) as SumOfPayments from Payments group by ClientID order by ClientID; update Clients set Balance = 0 where ClientID &gt; 0; select * from ComputerUsage; select * from ClientsWithBalance; select ClientID, sum(Payment) from ComputerUsage group by ClientID order by ClientID;*/
I think he's referring to the type of associations (OTM, MTM, OTO). (Cardinality also refers to the uniqueness of data in a column: http://www.programmerinterview.com/index.php/database-sql/cardinality-in-sql/ )
I didn't touch SQL until last year when I fell into a BA role. Now I'm writing stored procedures. Your biggest ally is the fact that you already have real-life assignments to complete. Get the basics down (I used w3schools.com). After that, sweet talk DBAs and Devs to review your code if they have time (trust me, the good ones would much rather empower you to do basic/intermediate stuff if they don't have to do it). Take on more and more complex tasks as you improve and challenge yourself to come up with better ways to accomplish existing processes. Additionally, to be able to work with Oracle and MS is great, I've only worked with SQL Server. As an aside, a knowledgeable DBA I spoke with said he did research one time and found that those without MS certs actually made more than those with. Expreience and real-life expertise trumps all certs. 
Based off the syllabus for the Introduction and Intermediate SQL classes on ed2go, they seem to be a good match for the MTA Database certificate. It is a generalized certificate basically stating you have an entry level understanding of working with SQL queries and database creation, nothing SQL Server specific. I can't speak to the MCSA level exams as I have not taken or studied for them. I took the MTA after completing two classes that had a structure similar to these--on campus instead of online--and barely passed. I was expecting something a lot more basic after completing some of the other MTAs. The **detail** of the questions, and the ambiguous wording as u/excelevator mentions, were the hard part. The questions mostly seem to be assessing your knowledge and ability to recognize or choose the most correct answer and pick out syntax errors; very few had a single answer that was clearly correct. That being said, if you have a strong understanding of queries, know the differences between similar data types (bigint/smallint/int, date/time/timestamp) and commands(drop/delete/truncate) and when to use each, basic requirements of normalization, and understand basic db and table creation commands and the differences between data definition and data manipulation, you will have a more solid base than I did and likely have no problem with the MTA. 
You don't need any degree, diploma or certs to learn and actually using in real-life system. I studied SQL + RDBMS by reading several books (from Sybase, Oracle, SQL Server and CJ.Dates). Then I worked as developer for a few years, then I get my CS degree. For development related accreditation , I got MS certs (MCAD/MCSD) for further career development, they are more technical specific than my degree. But actually if you can prove, you don't need those certs too.
Well for one I would advise you to at lest look at the MCSE track. Also you can regularly find cheaper vouchers on ebay. Sometimes you can even get second shot vouchers or buy them directly and those let you retake if you fail. At the end of the day though if you manage it, you'll drop a few hundred and make quite a bit more back the first year. I mean if you are worried about the expense you aren't making even Jr. DBA money. 
I would definitely visit that sub daily!
Edgar Codd didn't get his compsci degree until he was 40...
The best course of action for you is to build a computer and install linux or convert an existing computer you own to linux and install MySQL. This is a free path with tons of online resources to guide you. Once you have a database, you can start loading data sets you find online and running interesting queries and performance test suites. I basically did the same thing nearly 20 years ago. Bought a book on SQL, read half of it, practiced, got a job interview and landed a job writing SQL test queries. It's totally doable without an degree, if you can demonstrate you know something and can learn.
I have been in a couple Financial Analyst positions, but my degree was development related. I worked for a hospital developing and maintaining reports in C# and MSSQL. And then I went to a fortune 50 company to write in both MSSQL and Oracle environments. I have to say that learning the business side of the position is pretty tough. Especially when you have no idea how finance works in a business environment. On top of that, at the hospital position I was also learning medical terminology and regulations. My advice for anyone that is looking to get into a financial analyst position who is coming from a background in development, is to do some courses on finance/business. If you dont have a position yet and you are not sure what sort of course to take, take anything related to finance. You will use this in any position and it will benefit you. You will be one step ahead and a company would love to hear you talk the talk while also knowing how to build some complex scripts.
Getting your NASD Series 6/7 can be a big help in a lot of financial companies, even if you're not on the "business" side of the house.
Very nice information, I have not heard of this.
I used to work for a large multi-national financial (among other things; big conglomerate) firm (by way of originally being employed by a mid-sized US insurance/financial services firm which was acquired) and a lot of the developers, especially the ones who'd been there 10+ years, had taken the NASD exams at some point.
&gt; NASD Series 6/7 NASD? Been a couple years, mate :)
Been out of the business a decade, with no intention of ever going back.
Good on ya - trying to get out of the insurance side of things myself at the moment. The Series 7 test is worth it for just about anyone if only for purposes of working to retain information, and there's a lot of neat info there that's useful for everyone.
 DROP TABLE #Step1 SELECT RTRIM(LTRIM(BrandName)) AS BrandName ,CAST('&lt;M&gt;' + REPLACE((LTRIM(RTRIM([BrandName]))), ',', '&lt;/M&gt;&lt;M&gt;') + '&lt;/M&gt;' AS XML) AS String INTO #Step1 FROM bi.dbo.MenuPlan DROP TABLE #Final SELECT DISTINCT S.BrandName ,RTRIM(LTRIM(Split.a.value('.', 'VARCHAR(100)'))) AS FinalBrandsList INTO #Final FROM #Step1 S CROSS APPLY S.String.nodes('/M') AS Split(a); That's what I did but I couldn't understand why it worked. I didn't get why involving XML helped anything.
Hi, have the same problem with the retail Yosemite but installing Java SE Development Kit 8 did not solve the problem. It does not start and creates a "bash" process that hogs the cpu. Any suggestions?
You're turning your delimited string into a set of &lt;M&gt; tags 'This,is,my,test' =&gt; &lt;M&gt;This&lt;/M&gt;&lt;M&gt;is&lt;/M&gt;&lt;M&gt;my&lt;/M&gt;&lt;M&gt;test&lt;/M&gt; You're then using an XML method for splitting those nodes back into field values. It works, but I'm not a fan of these approaches. 1. If your delimited string contains &lt; or &gt; it will not work. 1. It's a sort of hack 1. There will be an overhead as the XML engine will need to be loaded. 1. It's not obvious to you or won't be to anyone else who may need to look at your code later. My approach does exactly what it says on the tin. 
Generally a Primary Key won't reference another table directly. It looks like the two keys you're referencing are both foreign keys. Which is fine if you can use them in combination to create unique values from the other two tables. 
If I'm reading your problem right, you might be trying to over-normalize. Why do you need a Many to Many relationship? Clients Have Client Info Stuff. Claims have FK_Client, Level_Up_Dates, Current_Levels and Current_Steps. (You could have Current_Step and figure out which level with another relationship) Unless you're trying to keep a history of when claims advance. That would be something slightly different.
Yeah, I was worried that I might be overdoing it, however I do plan to keep a history so we can track the whole life of the case. What would be different in the case of doing that? Thanks for your help!
Note: if you're using the outer join method the max length string you can split is determined by the number of rows in spt_values(1025 on ase), you'll need to create and populate your own table to work with larger strings. Cheers. 
How about [something like this](http://i.imgur.com/0HIzyP4.png)? (it's only a draft for discussion's sake) An option would be to combine Steps and Levels in one table, eliminating the ClaimStepLevel. 
 select t1.ID , t1.code , t1.group , min(t2.requestdate) , max(t2.canceldate) from table1 t1 join table t2 on t2.id = t1.id group by t1.ID , t1.code , t1.group order by 1, 2, 3 
You're storing claim attributes in table clients, which probably helps pulling client information into the client application. But which claimStatus belongs to which claim? Another question, how is the history of steps and levels maintained? 
Functions are modular pieces of code. You write them once, debug them then you can use them many times, or base new functions on them, it's the whole premise of good programming practise. SQL itself has many of them. Yes, you should use them. No question at all about it.
Depending on how many fields, you could Pivot the summary table, then Order it and get the first 3?
Year(release_date) = 1990
It honestly takes 30s to find an example of the year function used in a query... Make yourself a favor and look it up !
This is the correct answer.
Correct yes, but slow as hell since its a non sargable expression, and you really don't want to have that on a join. You will get the same result but much better performance by doing this : release_data &gt;= convert(date, '1990-01-01',120) AND release_data &lt; convert(date, '1991-01-01',120) *that would be tsql, but i trust you guys manage to do a string to date conversion in whatever DBMS you are using /edit : as /u/r3pr0b8 pointed out, it might be a good idea to explain what sargable means. Basically sargable means that it can be efficiently looked up in an index via an index seek. If you apply functions or calculations to the indexed field, as would be the case with YEAR(release_date) the derived value can not be searched for in the index, since the index does not contain the calculated value. So you will end up with a full scan on the index / table, calculating the year for every date present in the index / table, to then do the comparison. There is a pretty well written wikipedia article on the topic for a more thorough explaination 
&gt; ... and the year function is exactly what should not be used here
"sargable" is a little beyond SQL 101 however, in any meaningful application, not including small scale hobby projects and homework assignments, sargability is **important**
Honestly, if he doesn't know how to use YEAR(), there is absolutely no point in discussing the use of functions in where clauses.
&gt; Honestly, if he doesn't know how to use YEAR(), there is absolutely no point in discussing the use of functions in where clauses. By that logic you could show him how to do it with a cursor and temp tables
how is re.release_date stored in the DB? DateTime or Varchar? I'd write it just like you did: where re.release_date between '1990-01-01' and '1990-12-31 If it's varchar try converting it to datetime. WHERE convert(Date, re.release_date, 120) between '1990-01-01' and '1990-12-31 23:59:59'--Not sure which date time format is correct (I can never remember ) but someone else went with 120
I always do it like this: CONVERT(VARCHAR(4), Release_Date, 102) AS Release_Year You can use the number in the varchar to clip the extra numbers off. EDIT: Then you would use your WHERE on this column. Typically when I do create new columns in my queries I put them in a new temp table because WHERE, GROUP BY, AND ORDER BY can get goofy.
Absolutely, the ERP system we use will only allow it to work with sorts and groups on a column in a table. Release_Year would return as an unknown column.
Thanks everyone. I missed the s where it said in the 1990s and not 1990. LOL.
there's a tool called Transpose does the pivot, thanks
I'm in no way an expert, but does something like this help: *Select p.PRODUCT_CODE, p.PRODUCT_NAME, sum(o.NUM_ITEMS) as TOTAL* *from PROD p, ORLI o* *where p.PROD_CODE=o.PROD_CODE* *group by p.PRODUCT_CODE, p.PRODUCT_NAME* *order by 3 desc;* I'm pretty sure the answer here is to get a count of unique products ordered, which will require a grouping of sorts.
select prod.prod_code, prod.prod_name, sum(orli.num_items) from prod join orli on prod.prod_code = orli.prod_code group by prod.prod_code, prod.prod_name
Not sure if this is the actual answer, but +1 for re-writing the join. Edit: To be of some use... select prod.prod_code , prod.prod_name , sum(orli.num_items) as ItemCount from prod join orli on prod.prod_code = orli.prod_code group by prod.prod_code, prod.prod_name
are you just talking about a table that shows you which cardinality notations mean what? http://wc1.smartdraw.com/resources/tutorials/images/erdinfoeng.gif
Late to the party but in the system I use the answer would be: where extract(year from re.release_date) = '1990'
Upgrading to a newer version of SQL Developer worked for me.
well, ok, there is the "shitty 3rd party app" argument of course. I feel for you
Can an answer have more than one user? It seems to me the first thing you should fix is drop the answerbyuser table and add a foreign key of userid to the answer table. 
It looks like you want dynamic columns based on how many questions you have. I think a better approach in this case would be to pull the data-set of name, answerid, questionid, userid, and stranswer. From there, you can then manipulate whatever you are using to display the results. This is coming from a T-SQL perspective. Here is some test code I played with in case anyone wants to play with it: CREATE TABLE User_table ( userid INT IDENTITY (1, 1), NAME VARCHAR(20) ) INSERT INTO User_table VALUES ('Juan'), ('Dante') SELECT * FROM User_table CREATE TABLE Question ( questionid INT IDENTITY (1, 1), strquestion VARCHAR(200), q_type VARCHAR(10) ) INSERT INTO Question VALUES ('"Who is the best player in the NBA?"', 'A') SELECT * FROM Question CREATE TABLE Answer ( answerid INT IDENTITY(1, 1), stranswer VARCHAR(200), questionid INT ) INSERT INTO Answer VALUES ('"Lebron"', 1), ('"Durant"', 1) SELECT * FROM Answer CREATE TABLE answerbyuser ( answerid INT, questionid INT, userid INT ) INSERT INTO answerbyuser VALUES (1, 1, 2), (2, 1, 1) SELECT * FROM answerbyuser SELECT a.userid, a.NAME, b.answerid, b.questionid, c.stranswer FROM [User_table] AS a INNER JOIN answerbyuser AS b ON a.userid = b.userid INNER JOIN answer AS c ON b.answerid = c.answerid 
Where Datepart(yyyy,releasedate) &gt;= 1990 and Datepart(yyyy,releasedate)&lt;=1999
Yeah it can because they're predefined answers by a dropdown list. The DB is Oracle.
Like a charm. New UI feels like a bonus as well. Thanks!
The thing is that I don't have an option. It should be done in PL/SQL.
I hope someone who works with pivoting data comes along, because dynamically generating columns is not recommended. With a while loop and dynamic SQL you could probably try to build a create table statement (for a temp table) based on the max number of questions in the question table at the time of executing the proc... or have the temp table already created and use a while loop to keep adding columns to it. The only other thing I can think of is creating a table designed like you want it to be, and add a new column to it every time you want to add a question. You would then populate it with a slightly altered version of my final query I posted above.
In Oracle 11g you have the option of the PIVOT function to create the type of output you want. If you are using version 10 or earlier, then you can use the DECODE function. However, DECODE requires that you know the values of your dynamic column headings. Here is a brief example of using the PIVOT function: http://www.oracle-base.com/articles/11g/pivot-and-unpivot-operators-11gr1.php I haven't tested it since I don't have an Oracle DB on hand, but I think this should work. SELECT usr.userid , usr.name , p.* FROM ( SELECT * FROM (SELECT u.userid , q.questionid , ans.stranswer FROM user AS u JOIN answerbyuser AS a ON u.userid = a.userid JOIN answer AS ans ON a.answerid = ans.answerid ) AS inner PIVOT (MIN(inner.stranswer) AS user_answer_id FOR (Ans. Questionid = ' ||q.questionid) IN (ANY) ) AS p JOIN user AS usr ON p.userid = usr.userid ; One issue I see, though is that you have no way to enforce one answer per question per user. You could either enforce this programatically, by searching the answerbyuser table for each entry to make sure that a User/Answer combination doesn't replace the answer for an existing question or you could enforce this by adding questionid to the answerbyuser table and enforcing a unique key of userid and questionid. Using the uk, you could also use INSERT/UPDATE syntax in a single DML statement to update the table whenever a user enters the answer to a question.
There are two issues. 1. You can't combine the definition of a primary key with the definition of a foreign key. &gt;PRIMARY KEY(ansattnr, kundenr) REFERENCES ANSATT(ansattnr), 2. The number of columns in the foreign key must match the number of columns in the referenced table. &gt;FOREIGN KEY (kundenr, kontonr) REFERENCES KUNDE(kundenr)); Assuming that ansattnr and kundenr form the primary key for table KONTO, then here is the syntax: CREATE TABLE KONTO ( ansatnr char(6) not null , kundenr char(6) , kontonr varchar(11) , PRIMARY KEY (ansattnr , kundenr) , FOREIGN KEY (ansattnr) REFERENCES ANSATT(ansattnr) , FOREIGN KEY (kundenr) REFERENCES KUNDE(kundenr) ) ;
Jeez, you made that code your bitch. Thank you much!!!!
Dbcc checkdb ?
Why would it be better to store the data in a SQL database? If XML files are working for you, why change it??
This is a very important question. &gt;People are now nagging that it would be better to use a sql database. Make them explain why. XML, for all its faults, is much more portable &amp; manageable for situations like this.
It would generate an error if something is wrong - search for the ms docs about how to use it.
&gt; Does anyone know of an opensource application that uses sqlite or similar as a application file format of sorts? [That would be Firefox](http://aplawrence.com/Web/firefox-sqlite.html)
I was getting errors on 3 of my databases. Ended up just rolling those back to Friday. There was some data loss but I was then able to upgrade my program and my backups started working again.
storing settings/application content, is not the same as using sqlite as a .doc or .xls file. 
You're absolutely correct. I was merely replying to OP's question to come up with OSS applications that use SQLite to use as a reference implementation. 
Gotcha. 
Installing the Java SE Development Kit worked like a charm! More people need to see this.
&gt; Does anyone know of an opensource application that uses sqlite or similar as a application file format of sorts? It would be very helpful to look at areference implementation. Asterisk uses sqlite as internal settings db since ver. 10
I'm assuming you're using SQL Server/TSQL, since you didn't specify, but it can be done quite easily using ROW_NUMBER() and PIVOT. [Voila.](http://www.bimonkey.com/2011/05/ranking-and-numbering-rows-and-subsets-of-rows-in-t-sql/)
Thanks for suggesting this. It made me realize I had been misleading in my description. Although there are up to 20 TicketType values per customer, the set of possible values for TicketType are fixed and number in the hundreds, which rules out the possibility of having a column per TicketType value. I've corrected the description. Sorry! and thanks again.
you've already named them quite adequately
The way we do it at work would be `FK_Users_CreatedByUserID` and `FK_Users_LastUpdatedUserID`. A lot of our tables have a `CreatedBy` and `ModefiedBy` value. We drop the `UserID` from the column name for succinctness and because it's such a common pattern in our databases. I would rather have my FKs be long and descriptive than short and enigmatic. It's not like you have to type them out regularly. I most frequently encounter them in error messages where it's helpful to know just by the name exactly which relationship is causing the problem. Really, those two aren't even long compared to some we have where the PK is a compound key.
excellent answer, naming similar to that crossed my mind but wanted to see what others were using. many thanks
although... one concern, what happens when someone wants to change 'Users' tablename? Do you then go through and have to rename your FK_ column names, thus then program code? I like it, but see that as potential issue. curious how you deal with it
Our user table will never change names. I actually can't think of single time we've ever renamed a table after it's gone into production. We will occasionally drop tables when their data is migrated to a new table but we never change the name of tables in place. Doing so would cause much bigger problems than being left with weird foreign key names, particularly with things that aren't in the database, like whatever application is using the database. It would be a huge undertaking to do that. 
&gt; thousands of (integer,textstring) tuples Makes it sound like sqlite makes sense (and having them split out as tables.) I believe Acorn's file format is just a renamed sqlite db. 
If I understand correctly just add max(ordline.num_items) in your select list. 
Um, add it to your select list? You're allowed to use as many aggregate functions as you'd like. SELECT prod_code, prod_name, sum(num_items), max(any column *NOT* in the group by clause) ... 
I'm interested in Teradata and DB2 answers for this question if anyone has good sources.
I don't like constraint/index names being too verbose, personally. The error message will give you enough details to identify the parent table, and if there are multiple references to the same table, and values for one are determined incorrectly, chances are that the same issue applies to similar references in that table/statement as well. FK&lt;Ordinal&gt;_&lt;ChildTable&gt;.
For Teradata, check out the developer forums: developer.teradata.com. There's blog posts as well as very responsive individuals on that site. 
Thanks man!
Yes that would work. How can you filter results to 1?
Easily the best reference manual for any SQL server, and that's actually fairly impressive. For questions, I go to either stackoverflow.com or dba.stackexchange.com. The former for query questions, the latter for design or server management or best practice questions.
What database application are you using?
agreed, i guess i was living a pipedream for a minute hoping that more of our naming here would get changed to longer and more descriptive :)
You can search for Free Data Sets. [Here's one of them](http://aws.amazon.com/public-data-sets/). There are plenty of publicly available data sets and you should be able to find something on a subject that interests you. If it's too big, explain how and why you selected it down to a usable subset. Then invent an interesting problem to solve and solve it. I think they would give you bonus points if you created a data dictionary. "Data Dictionary: a set of information describing the contents, format, and structure of a database and the relationship between its elements, used to control access to and manipulation of the database." In my job, learning the data model is something I can't google because it's company confidential information. I have to look at the tables, views and procedures. I ask around to find someone who can answer my questions. The ability to learn a data model is at least as important as knowing SQL commands.
Well, I'm using rownumbers and modulo too, but hey, its working... DECLARE @userCount INT; SELECT @userCount = COUNT(*) FROM @User U WHERE IsActive = 1; WITH users AS ( SELECT * , userRN = ROW_NUMBER()OVER(ORDER BY UserId) /*or how ever you want the cycle set up*/ FROM @User U WHERE U.IsActive = 1 ), dates AS ( SELECT userRN = ROW_NUMBER() OVER (PARTITION BY d.seqRepetition ORDER BY d.rn) , * FROM ( SELECT seqRepetition = ROW_NUMBER()OVER(PARTITION BY d.rn % @userCount ORDER BY rn) ,OrderId ,BeginDate ,EndDate ,rn FROM ( SELECT rn = ROW_NUMBER() OVER(ORDER BY BeginDate) ,OrderId ,BeginDate ,EndDate FROM @DateRange DR )d )d ) SELECT * FROM dates d LEFT OUTER JOIN users u ON d.userRN = u.userRN
excelevator has asked the question needed to give you actual SQL, but without knowing what flavor of SQL you're using, here's an outline: select * from ( select prod.prod_code ,prod.prod_name ,sum(ordline.num_items) as total_num_items from prod join ordline on prod.prod_code = ordline.prod_code group by prod.prod_code ,prod.prod_name order by sum(ordline.num_items) desc ) where rownum = 1 /* this is the part that will vary depending on your database, however this syntax works with ANSI 2003+*/ 
I'm sorry I should have clarified I am using oracle sql 
Thank you! Sorry, I guess I left out some information. I have created the database, made queries, and a data model. Is there a way to contain all that virtually and share it?
You could make a PHP/MySQL website. It could showcase your database somehow.
Not exactly a Q&amp;A site, but I personally look to Pinal Dave's blog, [SQL Authority](http://blog.sqlauthority.com/) for a lot of my questions. He's covered so many topics over the years and gives a great explanation of his work.
Awesome! The code all worked but I am using oracle sql so the last line (where rownum=1/*) didnt. What should I swap in to only return the first result? 
Actually I got it! &gt;where rownum = 1 Thanks again for your help!
You may try select '''' + &lt;columname&gt; from table 
This is an excel question, not a sql question. I fucking loath how Excel always thinks it's smarter than you. All the changes to your data are happening on the excel end. You can try formatting as those columns as text before you paste them in but Excel will probably ignore you because it knows what you want more than you do.
/u/lukeatron is correct. Select the column or the whole sheet and format the cells as Text before copying the data in. By default everything is set to format as "General" which will try to format it as a number if it looks like a number.
Yes this is similar to the final code that I'm going to end up using. I added it to the post.
IMO, DB2 is pretty difficult to find good online conversation places. Perhaps the worst of the bunch.
I agree, the only place where I usually find some answers are in the official IBM website, and is not very insightful sometimes.
First, set up a temp table that will hold updated rows' IDs like so: declare @tblTemp table ( UserID int); Now modify your UPDATE statement(s) to include an "OUTPUT..INTO" clause, which will save out the UserIDs of updated records: UPDATE T SET Col1 = Val1, ... ConN = ValN OUTPUT inserted.UserID INTO @tblTemp (UserID) FROM Your_Table T ... WHERE ... The "inserted." in the above query is an alias for the recordset containing the updated rows with new values. It has the same column structure as the table being updated. The other useful alias is "deleted." which is the same recordset, only containing the old values. Very useful for reporting. You can now report on the amount of updated rows using the temp table like so: INSERT Your_Reporting_Table SELECT UserID, COUNT(UserID) as AffectedRows FROM @tblTemp GROUP BY UserID; Hope this helps. Side note: The OUTPUT clause works for inserts, deletes, and merges as well.
I think you're stuck with the row number solution if you want pure SQL. Though if this is just a data dump, it's trivial in python or something.
Unless the stored proc is written very inefficiently with cursors, I don't see how @@rowcount will help him track counts by ID.
That's the problem, without knowing what exactly it does I can only guess. It could be a very simple proc being passed userid as a parameter with a few dml statements, and @@rowcount thrown into a few variables would be fine. If he has problems he can come back and get more suggestions providing us more information.
If you are interested in software development there is a clear path from the reporting world. Most companies struggle to find talented SQL developers since it is not a high priority in most college degree programs. If you learn HTML and the basics of a server side programming language, you would have an advantage at some companies. Other than that, I know that almost any financial job would benefit from understanding how a database works.
Thank you! This is all a bit over my head so it took me a while, but I made progress using ROW_NUMBER. When I went to use PIVOT I realized the database is actually at 2000 (yikes!) compatibility level so this won't work. The database has numerous custom VB front ends that haven't been touched in years, and we don't have anything but the complied code for them so I'm very hesitant to try to convert up. Any thoughts on workarounds? It's only 9000 records or so in case that helps.
My suggestion would be to download the [Adventure Works database](http://msftdbprodsamples.codeplex.com/) and restore it to **your own** test database instance. Go and play. Setups permissions, roles, jobs. Basically sample over a few blogs and try stuff out. The best way to learn it is to do it.
Business consultancy : how to read and order data
Basically I am designing a database for an internet cafe as a project of mine. Just for educational purposes. My question is, in your expert opinion do you think the keys and relations are sufficient?
From the ROW_NUMBER query I imported into Access 2010 and was able to create the PIVOT. Thank you!!!! It's no wonder you guys are paid so well - this stuff is HARD!
Just to clarify, the database is newer than SQL Server 2000 (2005+) but is just running on 2000 compatibility mode and ROW_NUMBER() with a PARTITION still works? SELECT CustomerNumber , max(CASE WHEN OrderedTickets.ArbitraryOrder = 1 THEN OrderedTickets.TicketType END) AS TT1, max(CASE WHEN OrderedTickets.ArbitraryOrder = 2 THEN OrderedTickets.TicketType END) AS TT2, ... FROM ( SELECT ROW_NUMBER() OVER ( PARTITION BY CustomerNumber ORDER BY TicketType ) AS ArbitraryOrder, CustomerNumber, TicketType FROM GreatDB.dbo.TicketCustomerLink ) AS OrderedTickets GROUP BY OrderedTickets.CustomerNumber
Work on the front-end stuff that sits atop whatever database you're developing. In my current position, those guys sit opposite me so it's easy for me to take an interest...
Like others have said, i see two natural routes here - business consulting or middle tier(especially web). I was very SQL heavy for about 6 years. I work primarily in proprietary server-side languages and javascript, and I actually think its pretty damn fun. My sql knowledge is still extremely valuable, but I no longer feel stagnant. Companies have enormous amounts of data nowadays. Many of them have no idea how or why to leverage it in an efficient manner. 
The problem I see pretty quickly is that this limits you to only one payment per customer, so you probably also want some form of Transaction_ID. With that in place, since the users max time is dependent on the amount of time they purchased on that transaction, that might need to be tied to computer usage. You see how all of the tables connect in a circle? That could end up being bad and creating an endlessly looping reference. It is unclear how exactly the employees table links to payments, or why employee is on the users table. If anything, the employee should be linked to the payments table and not the user table. (An employee is responsible for one transaction with a customer, not all transactions with that customer).
http://mongrel2.org/manual/book-finalch4.html Zed Shaw's mongrel2 uses SQLite
Yes, SQL 2008R2 and the ROW_NUMBER and PARTITION worked fine to create the row ID field. Excuse my formatting: SELECT ROW_NUMBER() OVER (PARTITION BY CustomerNo ORDER BY TicketType) AS Subset_ID_Key, [CustomerNo], [TicketType] FROM TicketTable
you can export the column as an excel expression. select '="' || zip || '"' from address
Thanks for taking the time to look at this. To fix the relation to the payments table, the employee table/employee id can reference a composite key in the payments table. A composite key of user id and transaction_Id This composite key in payments can also reference the other composite key in the computer usage table. That's assuming that composite keys can reference other composite keys. Here's the improved [**version**](http://i.imgur.com/rZeRPqs.png?1) 
Been a database administrator for 14 years, I've become a prison officer. I'll accept that it's a career 180 but it's way more rewarding.
Why does the "Users" table have a "session_ID"? Wouldn't that mean that each user can only have one session? It might be better to have a User_ID field in the sessions table so that there is still attribution but each user can have multiple sessions. 
This database is designed for use in an internet cafe. Most internet cafes generally have one session per user.
Why? Wouldn't it be better to track your users logins across multiple visits to the cafe (assuming some form of membership is used to keep track of who is whom, edit: heck this database can do it)? Even if you do want to limit one session per user (and assuming one user per session), wouldn't it be better to enforce that at a programming level rather than at a database level? It keeps your data requirements in a minimum format and allows for the most flexibility should later changes be required. The only situation where I can see a user having a session id as being better is if you were to have a situation where multiple users have a single session (which is nonsense for this use case). 
Business analysis or project mgt 
Wow! how do you feel about the change now that you have been through it? What are the pros and cons compared to being a dba? why a prison officer?
ThanksI can't program anything on a programming level though. This program is part of an actual database project. Have to show as many constraints as possible. So would it be a good idea to; get rid of the session id in the user table. Finally to link the session_id on the session table to user_id in the user table. User table only needs two attributes then, the user_id an password. **EDIT** would it be better to get rid of session id on the users table. Ant then on the session table to have a foreign key for the user_id?
That's what I would do. Each session has a user. So a User_id field in sessions would work just fine. 
You haven't really specified any requirements here, so we have no idea what you're actually trying to accomplish. Why are employees in here at all? Are we paying them commission? How is pricing determined? You're providing services, so you should have a service catalog with pricing, keeping in mind that it changes over time, so maybe there is a Service table that just names the service, and a ServicePricing table that has current and historical pricing information. Let's say you're trying to track when an employee helped a user with a service. So we have a Jobs table, reflecting each service performed. This is the fundamental fact of your business. A naive implementation has a Service id, datetime, quantity, user id and employee id. This implies that no job ever requires more than one employee. It also implies that users never get together and buy a service together, for example no gamer clan comes in. If that's not the case, you'll want join tables instead. You might also want to track computers, printers and scanners as Assets. Then you can have a JobAsset table that shows which Asset was used for a job. You could go a step further and make Employees into Assets. This lets us track which employees were involved with which job. You'll want some definition of AssetType, either as a table or as an enumerated type in the Asset table. Now if we think about payment, a user makes a payment, but they might pay for multiple jobs. So it makes sense to have a Payment table, reflecting the overall payment, and a JobPayment table, reflecting each job covered by that one payment. I suppose Payment could have a single user id. Also, never store password in the database. You're going to have two fields, unique_value which is like a GUID or other moderately unpredictable unique value (different for each user) and password_hash, which is a string containing the bcrypt hash of the unique_value and the user's password. If you have some kind of password login system, there's a lot more you're going to need, like number of incorrect password attempts since last valid one, time of last incorrect attempt, a flag for whether the account is locked, and of course a table to store policy rules for your password system - how many tries to lock out an account, how long to leave it locked out, minimum password complexity rules, etc.
SSMS isn't really well suited for data extraction - it'll work in a lot of scenarios, but there are limitations. This [serverfault post](http://serverfault.com/questions/9210/how-to-extract-data-from-a-sql-server-2008-filestream-field-without-using-net) seems to indicate that you have to go through the .NET APIs, though it is a bit old and may be outdated information - it's worth reading for Paul Randall's answer regardless...
Use the current date to calculate actual age, then use a case statement or a function to generate a nvarchar for age group i.e. '18-25', lastly use a group by so they are grouped together
I'm only a SQL developer so I don't know the details of it. But yeah you'll need a domain name, a hosting web server, and the website itself built. You could start over at /r/learnprogramming to begin with.
I moved from doing SQL and BI into doing Ruby/Rails/JS stack stuff, and I really love it. 
nosql. I'll see myself out...
EOD Explosive Ordnance Disposal. Really shake things up.
I have an associates degree. In music. I've created and maintained a database containing every hardware and software product for sale in the US to use with a multi million dollar project management tool. I've built a database of social services in eleven countries and eight languages complete with user base. I've worked at IBM. And now I'm a data analyst for a major retailer, assessing pricing data for the Merchandising team. You don't need certs or a degree. Just get experience. 
I know my company is in need of some really good SQL developers. What do you say, people? Anyone in the Philly area got the chops and looking for a job?
Sql bi pfe for microsoft. Just had a guy join my team who was one and said he loved it. Ended up transferring since his wife got tired of him traveling. 
Why dont you create a SSIS package that does this? Should be more up your alley.
Thanks /u/eStonez, I'm very shaking when it comes to relations. Your questions helped me in questioning and improving them. I've set up answers for each of your questions, let me know if the answers provide the right solution. **Why Employee ID(FK) in computers ? Do you mean only one Employee allowed to operate that computer ?** ----- I was attempting to setup a relationship so that many employees are responsible for maintaining each pc. **What is the link between Employee and Payments ( without EmpID in Payment table ? )** ----- I've recently fixed [this](http://i.imgur.com/rZeRPqs.png?1) **Users (entity) is having 2 combined PK. Why ?** ----- I've recently fixed [too](http://i.imgur.com/rZeRPqs.png?1) **Why SessionID(FK) is in Users instead of UserID(FK) in Session table ?** ----- I was thinking of deleting session id on the users table. Then on the session table to have a foreign key for the user_id? **The "1:n" relationship between Employee and Users .. how come both side is PK ?** ----- This relationship was definitely a mistake, I've recently fixed this [too](http://i.imgur.com/rZeRPqs.png?1) **Same goes to 2 ComputerID PK in "Computers" and "Computer Usage".** ----- I don't know about this one, I was thinking that the Computer_ID (PK) could directly reference the composite key in the usage table, (if that's possible) It's also a 1:1 relationship, so is it correct in saying that the two primary keys can reference each other? 
Thanks u/phillthechill. Great points. This database project is a project for a database. For a college project, so if it can function on a basic level. I'd be happy. Not good practice I know, but my sql knowledge is limited compared to yours. this is the updated version of the [database](http://i.imgur.com/Y5G9SeV.png?1) i'm hoping that that addresses everything. The payments table isn't suppose to track what each other uses, it's done at the counter to say what you want. Otherwise, I'd implement your idea of an asset table. As for the password, I'm using that as a login code for the users to login into the computers. An app can be responsible for generating the login codes. So at the computer level, they have to enter their user id and passowrd to login. Unless , is there a way to randomly generate, lets say a 5 character string. Would this work? You're correct in saying, that only one employee can do one task at a time as well as saying that the asset usage should be tracked. Separate tables would be ideal, I'm hoping I could avoid this to create a simple system that basically functions. 
Business analyst, data analyst, (financial) controller, process engineer. 
How about this .. ? http://imgur.com/sQVfi3t note : ignore the arrowheads can't find appropriate tool to draw diagram :P
A one to one relationship to me tells me that there is no need for another table (unless there is the possibility of it becoming a one to many later on for some reason). I would have one table Computers with all the computer usage on it and all the users on it also. It seems silly to me that you are only allowing one user on one computer. I think your requirements are off. Each computer has one user and one session id... to me that says that says that there is one computer and the same person uses it exclusively. Edit: If you take anything away from my comment it should be that one to one relationships aren't needed for your diagram. Take them away and put them all into one table then re-work it. Check out /u/eStonez's diagram.
This seems better to me
You can use expressions in row groups if you want to do it on the report side vs the SQL side. One option is to add a row group, and use datediff with a switch in your expression to create buckets of age ranges. The report will then group by those buckets. If you want age groups to read across instead of down, use a matrix control and a column group instead of a row group.
A single quote in front of a number will be treated as text in excel, excel will hide the single quote. eg '007 will display as 007 So concat a ' with the value in the query.
One sessions one payment is not work properly in real life, people change from computer to computer for various reason (noise, lighting, crashed, game not available). In this case, user won't pay for it until they finish the usage, system have to record multiple session (unpaid) and tie single payment to those multiple sessions. Another thing is "group of users" .. such as tour group/CS clans. They have prepaid account, one account to cover all of their sessions. Combining session and payment will make you harder for the future.
You might be right. I was just hoping to have a simple script I could use to retrieve documents from the DB. We have a few BAs that spend their entire day in SSMS running ad-hoc queries monitoring activity and this would make their jobs a bit easier. It just seems crazy to me that this can't be solved using SQLCMD. 
 SELECT CASE WHEN column1 = 'value1' THEN column2 WHEN column1 = 'value2' THEN column2 END AS Dates FROM casseTest WHERE column1 IN ('value1', 'value2') [SQLFIDDLE](http://sqlfiddle.com/#!6/4039a/7/0) Or just go like this SELECT column2 FROM casseTest WHERE column1 IN ('value1', 'value2')
 SELECT column1, datetime FROM table WHERE column1 IN ('value1', 'value2')
Something like this perhaps?: SELECT Column1, Column2, CONVERT(DATETIME, Column1) AS Column2Date--? FROM Table1 WHERE Column1 IN ('value1','value2')
You can try varchar(8000) or something like: exec xp_cmdshell 'bcp "select * from tempdb.dbo.t" queryout "\\server\somewhere\" -c -t, -T -S sqlserverhostname'
Looks fine to me, other than what is the nbrOfCards field for? Edit: Also, it's mostly a preference to many people but you could use natural keys rather than surrogate keys by getting rid of a lot of the auto-incrementing id fields and use the name field as the primary key, for example, the status table, card rarity table, class table, etc. Then turn on "on ... cascade". I wouldn't worry too much about many-to-many table size, just make sure you have indexes.
nbrOfCards is supposed to be for either how often a certain card is within the deck or the amount of that specific card the person owns
Are you refering to me about the primary keys ? As in "you should have never uploaded an image of the db u might use" ?
Seems like you'd be duplicating the field? For example: deckHasCards deckId cardId nbrOfCards 11 32 5 11 32 5 11 32 5 11 32 5 11 32 5 43 54 2 43 54 2 11 21 1 Do I have this right? If so, drop that column and just do something like this: SELECT *, COUNT(*) FROM `deckHasCards` GROUP BY `deckId`, `cardId` Also, what is your primary key here? You can't use deckId and cardId as the primary because they obviously duplicate. You're probably going to want to use an auto-incrementing id field on these two tables. For example: deckHasCards id deckId cardId
Why not? Primary keys are presented to a user all the time. For example, if I have a table of "books" id name 1 Book 1 2 Book 2 3 This is another book *psuedo code* &lt;select&gt; foreach book &lt;option value="{{ id }}"&gt;{{ Name }}&lt;/option&gt; end foreach &lt;/select&gt;
 deckHasCards deckId cardId nbrOfCards 1 32 2 1 54 2 1 21 1 1 22 2 So deck 1 consists at the moment out of 7 cards (2x 32, 2x 54, 1x 21, 2x 22) I do not want duplicates and thats what the nbrOfCards field is for, it basically is the group by however without any duplicates in the table Did that answer your question or is it still unclear ? So tables which are made to overcome m:n relationships need to have a primarykey ? Roger
Well I noticed you do not have a link to the cards table 'Cards' in the boosters. The '10% booster' idea for me was because you linked it to the profile, maybe insinuating it is part of a metagame? (League of Legends runes and masteries for example)
Ah yes I misunderstood, that makes sense.
Well It is not fully planned out but it is supposed a way of an inventory basically, you purchase packs and then you own them however can open them anytime(also might link a history for booster contents as well as a history for purchases/transactions) and since those are accountbound. Did that cleared things up ?
&gt; generally considered best practices It isn't though. That 5 year old thread with very little views isn't the best discussion on this. In addition, most of the people in there were disagreeing it's an issue.
will add more histories for purchases(and add missing timestamps), fix a typo at log messages and at
Honestly, I don't particularly care, and tend to expose keys in my applications. I was just trying to respond for those who do in good faith.
Gotcha, that makes sense also. RNGesus for the cards then.
 SELECT column2 FROM table WHERE column1 IN ('value1', 'value2') I think they wanted to return column2 based on the value in column1?
The only way I can think to add the column that stays updated is to create a trigger. CREATE TRIGGER testTable_UPDATE_CUSTCOUNT ON testTable FOR INSERT as DECLARE @cust INT SELECT @cust = cust FROM inserted UPDATE testTable SET [custcount] = (SELECT COUNT(1) FROM testTable WHERE [cust] = @cust) WHERE [cust] = @cust
Though I would probably just grab this data in a select over adding an aggregate column to the table. SELECT tt.* , tCount.custcount FROM [testTable] tt JOIN ( SELECT [cust] , COUNT(1) AS custcount FROM [testTable] GROUP BY [cust] ) tCount ON tt.[cust] = tCount.[cust]
What's your end goal with this, and why are you looking to pull the data from within SQL Server itself? Store it there, sure. Retrieve it from a remote website via the engine? Kinda feels like the wrong way 'round. Weather Underground has an [API](http://www.wunderground.com/weather/api/) which you can use to retrieve weather data. You can call that API from pretty much any environment that supports HTTP requests, such as C#, PowerShell, JavaScript, Python, etc. You could even call one of these from within an SSIS Script Task. I don't know that this would be the best way to go unless it's part of a larger SSIS package though. But understanding how this fits into the context of the rest of whatever you're trying to build would go a long way toward recommending the best course of action.
I think you're right.
Select s.id, s.category_id, c.name as [category], s.name as [subforum] from subforums s Join category c ON c.id = s.category_id
The reasoning behind it is mainly for forecasting sales. We want to look back to 2010 and forward to get some trending information as to why sales might have been good or have suffered. If the data was in a database, I could look up weather data based on date and region to scrub against our data. We're currently going off of memory on how weather was like. As long as I could get a weekly recap of regional weather, that would make this project really shine. EDIT: to make another thing clear, I didn't know if there was a way to get data from a website based on a region by using SQL to get it....ala SELECT [Weather] FROM WebsiteThatHasWeatherInfo Where ZipCode = XXXXX AND Date = Today
You need to compare the dates that the user is inserting into your sql query. Try something along these lines, SELECT FIRST_NAME, LAST_NAME, TimeIn, TimeOut, TotalHours FROM dbo.fnGetApprovedSalaryUserTimeForDateRange join ACCESS_USER on ACCESS_USER.USER_ID = dbo.fnGetApprovedSalaryUserTimeForDateRange.timeinuserid where Daterange.timeIn between '10/9/2014' and dateadd(dd, 1, '10/15/2014) You will also want to look into using alias' 
SELECT FIRST_NAME, LAST_NAME, TimeIn, TimeOut, TotalHours FROM dbo.fnGetApprovedSalaryUserTimeForDateRange('10/9/2014', '10/15/2014 23:59:59') join ACCESS_USER on ACCESS_USER.USER_ID = dbo.fnGetApprovedSalaryUserTimeForDateRange.timeinuserid where Daterange.timeIn between '10/9/2014' and dateadd(dd, 1, '10/15/2014') I do not see an alias. DD? Oh and see, man... I probably should of bolded some parts of the question, the date range isn't neccessarily important because I'll be making the report so that you can select your own dates. The date range in here is just so that it returns information. I'm moderately accomplished on SQL, so I apologize if I don't convey myself concisely enough.
Awesome, yeah I was a little confused on what that table was referencing, specifically your environment since I don't know it lol. Glad to see you got it working.
Haha no problem, glad I could be of some help. I do the same thing for work (mostly ssrs reporting) 
Given you can find a web service that provides you this info (which I believe NOAA or other sites do for free) you can use their web service to pull this info into SQL. I think there are about four ways to do this, and it depends on the security and setup of your system on which one works best for you. - Most secure is through SSIS. Setup an SSIS job to hit the web service and pull data into your database however often needed. - Write a CLR and deploy this to your SQL Server as a Procedure or Function. This would allow you to pull live weather from any SQL query just as you would from a static table. - Write a command line application and use sp_cmdshell to reference it from TSQL. - Again via TSQL use the XML SQL options to query a SOAP web resource. See this for more info - http://technet.microsoft.com/en-us/library/ms191274(v=sql.105).aspx I personally don't like to open my SQL server up to resources outside of my local server or network, so if I were doing this I'd do it via SSIS. It's probably the simplest and quickest way, and SSIS calls web resources out of the box and with little effort. Take care -- Sam 
Here's one suggestion: CREATE TABLE #TMP ([Key] INT, [Cust#] INT, Warehouse CHAR(2), ErrorMessage VARCHAR(20)) INSERT INTO #TMP ([Key],[Cust#],Warehouse,ErrorMessage) VALUES (1,111,'OH','Carton Error'), (2,222,'KY','Ship Error'), (3,111,'OH','Ship Error'), (4,111,'KY','Ship Error'), (5,333,'KY','Invalid Customer') SELECT *, (SELECT COUNT(1) FROM #TMP A WHERE A.[Cust#] = B.[Cust#]) as CUSTCOUNT FROM #TMP B Another option if you're using MS SQL is this, which this might perform better than the above if TSQL is an option: SELECT *, COUNT([Cust#]) OVER(PARTITION BY [Cust#] ORDER BY [Key]) as CUSTCOUNT FROM #TMP B Hope this helps .. Sam 
 select * from tbl a where active = 'y' and p_type &lt;&gt; 'ABC' and exists ( select * from tbl b where b.id = a.id and b.p_type = 'ABC' and b.p_year &lt; a.p_year ) 
I would prefer a single surrogate key over compound primary keys. 
I'd suggest checking out some of the Microsoft apps that are SQL intensive, like SharePoint or Dynamics CRM. I've been an MS SQL DBA for almost 15 years, and I've started transitioning to a position that works more with Microsoft Dynamics CRM and it's a blast! I get to learn come coding, but for the most part my job is writing processes that load and unload data into CRM which is tricky since the only option is using their web service. Another developer works with the front-end part, and I maintain the backend components which is my forte. I'm using my SQL skills to support the application, like SSIS, SSRS, etc, but given the Web Services part I have to be alittle creative when importing data into CRM. SharePoint is a similar beast. Also data analysis and aggregation is another biggie now'days. I've wanted to personally start collecting databases like weather, political results, economic stats, etc to see if there are any patterns in the numbers. So there are LOTS of things you can do beyond just being a DBA if you want to stay within the database arena. 
Could your game ever include more than two players? If so, you'll need another table "matchHasPlayers". Otherwise, you could technically be fine with two columns, player 1, and player 2, or winner and loser. However, a separate table is best practice.
Something like this maybe? Or a better join I could've used? SELECT A.* FROM TBL A JOIN TBL B ON A.id = B.id WHERE A.ACTIVE = 'Y' AND A.P_TYPE &lt;&gt; 'ABC' AND B.P_TYPE = 'ABC' AND B.P_YEAR &lt; A.P_YEAR 
Use a join to get the counts and reinsert to final table. insert into FINAL_TABLE select A.* ,B.CUSTCOUNT from A join (select cust#,count(*) as CUSTCOUNT from A) as B on A.cust# = B.cust# 
10-4 Thanks
Think you are using one or more, when you should be using 0 or more often. Depends how you are using the database. For example. with this database, you must create a decks, playerHasCards, logHistory. Then you can create a profile, and the primary key in profile must have foregin keys in decks, playerHasCards and logHistory. So it is impossible for a player to exist without cards, this makes it tricky to create a user, since you must give him cards first, then the profile can be created 
Someone should tell you this is a bad design. I will tell you. Using a View to get this information on an as needed basis is the way to go.
Thats a good point, for cards that might be for all or for a bunch of classes
First, how can i use 0 to more or 0 to 1. And are you sure about the creation order ? I think it should work as it doesn't have any foreign keys(except status but it is predefined(Might remove it actually) and therefor no restrictions others than a different username. 
That is something you have to test. I've seen cases where existence test this is less expensive than a join.
I'd use GPS coordinates and compare that both the longitude and latitude are within x, which would give you a box not a circle. 
Do you think I should make a query, stored procedure for that in SQL or implement it in c#?
Your join will multiply output if you have more than one 'ABC' type in previous years. I really think existence testing is the correct solution and not joining. Also, you cannot make a blanket statement that a sub-query is expensive. The query optimizer likely does a similar plan with the two approaches and only testing will tell which is more expensive. 
If you want to do queries like that then you should use geography columns. http://stackoverflow.com/questions/7409051/why-use-the-sql-server-2008-geography-data-type
what table and columns do you have?
Select column1, column2, sum(quantity) as quantityonhand from tablename Where itemclass= 'AP' Group by column1, column2 
Since this is a HW problem you should definitely be figuring this out yourself but since you are seeking help I'll see if I can't shed some light and point you in the right direction. BTW, the answer in this thread is not correctly answering your question so I would refrain from using it. Obviously, you know that this OnHand value column is one that you will need to create. I'm assuming that since you're smart enough to be enrolled in a SQL class that you're smart enough to know which columns need to be used to create it. So, you know how to figure out the value of a part but you need to know how to make it a standalone column. SQL will let you put just about anything in a column in the SELECT statement, you can make every row a hardcoded string or int if you wanted. What you need here is a calculated field, or a field derived from the values of other fields in a given record. Creating a calculated field is very simple. It is not an aggregate, so you do not need a GROUP BY and for this specific problem it appears, with some assumption that you won't need one in this scenario. All it takes, is to take first column [insert expression] second column. It's that simple. You'll want a header for this column so you can just put AS [OnHand Value]. The brackets allow you to put a space in a column header as well as use reserved words such as Version or PI or From as the alias. As far as the item class criteria you should be able to figure that one out. I hope this helped.
AWESOME! this is what I needed I was doing it the wrong way Thank you for the help!
Because the `geography` data type is a CLR type, there will be a bit of a performance hit. **However** * Is that performance hit larger or smaller than the one you take by doing the math in your application? * Will not doing the calculation in the database require you to retrieve a large amount of the data, then filter in the application? * Will you have *other* uses for that geographical data that go beyond your current application (reporting, more involved calculations in SProcs, etc.)?
Filtering done on indeed.com is *probably* done in the database based upon the GPS coordinates of the entered ZIP code and the ZIP code of each job's listed location. Keep in mind that ZIP codes, especially outside major metro areas, vary widely in size - for example, the one I live in is 130 square miles, and the one next door is only 22 square miles. So this isn't a perfect search. It's close enough. But anyway, if you have a table of the published location of each ZIP code (which is usually close to the center of the defined area), you can use the geographic function `STDistance` to query your data. Assuming a 2-column table with ZIP codes in a column named `ZIP` and their locations in a column `latlong` (`geography` data type), this will get you every ZIP code in a 25-mile radius (circular, not a box); the geography functions work in meters. declare @p1 geography SELECT @p1 = latlong from ZIPCodes where ZIP='12345'; SELECT ZIP, @p1.STDistance(latlong) as [Distance from Center] from ZIPCodes where @p1.STDistance(latlong) &lt;= 40233.6 order by [Distance from Center];
Glad to help. 
 SELECT CustomerName, Sum(receivable) as SumReceivable, SUM(amount) as SumAmount FROM Customer C JOIN payment P ON C.[customer #] = p.[customer #] JOIN ship S ON C.[customer #] = s.[customer #] GROUP BY CustomerName
Select columns from the tables, joining the tables on your relationships, adding a where clause to filter. How do the seller tables, warehouse table, and product tables relate? Give it a try, and I would be glad to help make it work once you put an ounce of effort into it.
The products table has the WH_ID as a foreign key, and the seller and product's table share another table "sller Inv, with S_Num and ProductsNum as a the FK1,PK1 / FK2/PK2. So far I have found the tables using select s_name, s_num, s_rating from supplier; select product_num, Product_name, qty_onhand, from products So now I'm trying to connect them: select wh_loc from warehouse where wh_loc = x Any advice?
this is actually kind of a tricky query. This is how I would solve it in oracle. --dummy home table with home(wh_id, loc) as( select 1, 'Nashville' from dual union all select 2, 'Pittsburgh ' from dual union all select 3, 'San Diego' from dual union all select 4, 'Huntsville' from dual -- dummy inv table ), inventory(wh_id, qty_onhand,unit_cost) as( select 1, 1, 1 from dual union all select 1, 1, 2 from dual union all select 1, 1, 1 from dual union all select 2, 1, 1 from dual union all select 2, 1, 3 from dual union all select 2, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual ) select loc, invValue from( -- rank the locations by who has the highest invValue -- if there are multiple values with the same max then both will be returned select loc, invValue, rank() OVER(order by invValue desc) r from( -- get the sum for all locations select h.loc as loc, SUM(i.qty_onhand*i.unit_cost) as invValue from home h join inventory i on i.wh_id=h.wh_id group by h.loc ) ) -- get the record with the highest rank where r=1 
No, you said *You may try* *select '''' + &lt;columname&gt;* *from table* which does not explain the Why!
I would guess you would have to join the warehouse table with products, as explained [here](http://www.w3schools.com/sql/sql_join.asp)
If you just want just the highest one, add a top 1 clause and order by inventory value desc. i.e. Select top 1 home.loc ... Order by sum(qtyonhand*unit cost) desc
What if multiple have the same value? That will most likely answer the problem but will gloss over duplicate values. In this case maybe not as important but still worth pointing out. It also does not work because you are ordering and filtering on rows in the same query. Here is an example showing that it returns the wrong value. with home(wh_id, loc) as( select 1, 'Nashville' from dual union all select 2, 'Pittsburgh ' from dual union all select 3, 'San Diego' from dual union all select 4, 'Huntsville' from dual ), inventory(wh_id, qty_onhand,unit_cost) as( select 1, 1, 1 from dual union all select 1, 1, 2 from dual union all select 1, 1, 1 from dual union all select 2, 1, 1 from dual union all select 2, 1, 3 from dual union all select 2, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual ) select loc, invValue from( select h.loc as loc, SUM(i.qty_onhand*i.unit_cost) as invValue from home h join inventory i on i.wh_id=h.wh_id group by h.loc ) where rownum = 1 order by invValue desc
Thanks Jason. We worked around it in a similar fashion but it's good to have that resolved.
I agree with your argument that there could possibly be two (or more) values with the same maximum value. A sub query to pull out all the records with the max(InventoryValue) would do the trick, but that does not seem to be what the OP wanted. rownum is the easy way out!!
Great thank you!
Well the other part is that orderby desc and where rownum =1 in the same query does not give you the correct answer. You still have to wrap the query. Below is an example of how you would have to write the query using order by and rownum. with home(wh_id, loc) as( select 1, 'Nashville' from dual union all select 2, 'Pittsburgh ' from dual union all select 3, 'San Diego' from dual union all select 4, 'Huntsville' from dual ), inventory(wh_id, qty_onhand,unit_cost) as( select 1, 1, 1 from dual union all select 1, 1, 2 from dual union all select 1, 1, 1 from dual union all select 2, 1, 1 from dual union all select 2, 1, 3 from dual union all select 2, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 3, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual union all select 4, 1, 1 from dual ) select loc, invValue from( select h.loc as loc, SUM(i.qty_onhand*i.unit_cost) as invValue from home h join inventory i on i.wh_id=h.wh_id group by h.loc order by SUM(i.qty_onhand*i.unit_cost) desc ) where rownum = 1
Using the ODBC MySQL driver you can setup a Linked Server within SQL Server connecting to MySQL. Then you can just issue most standard SQL queries to pull data to a fro. We used it to synchronize WordPress users with a crm application and a custom bit we wrote.
If this is a one time thing, also look at ssma (SQL server migration assistant). It connects to both and even creates schema in SQL server and maps data types. Using an export works too, but instead of a MySQL dump do a select into outfile with a CSV format and use import wizard/bcp etc. to load it. Linked server also works but I find that to be much slower depending on your environment setup.
http://sqlschool.modeanalytics.com/
I know people rarely want unsolicited advice, but I really want to try to make this clear because it will help you a lot with SQL. Writing a subquery is not expensive. They are absolutely required for more complex queries. The database engine will not execute your SQL directly, but will come up with a plan that produces the desired results. Itzik Ben-Gan refers to what happens to subqueries as unnesting. So lets compare what a database query engine might do with the two approaches. With a test for existence of data, it can stop checking with the first row it finds that matches. With a join, it might have to keep checking for other possible matches. Now, I'm not saying one will be faster than the other, I am saying you have to test and that my own gut feel is that the existence test will be faster because it doesn't need to find all possible values. I feel like I'm beating a dead horse, but I do hope you will get over your aversion to subqueries. 
schemaverse.com Space battle game played with SQL. A lot more fun than a standard business org schema example. 
I'm just going to steal this [link](http://sqlschool.modeanalytics.com/) from this [post](http://www.reddit.com/r/SQL/comments/2kgvm8/looking_to_get_into_sql/), since it's right below yours :P
For SQL server, check out sqlskills.com. Paul Randall is widely known in the SQL community. They usually have classes in Chicago or west coast. Outside of that, Brent Ozar is good too. Also look at SQL Saturday around you.
I would like to get into PL/SQL development, but it was recommended to me that I go to some flavor of SQL basic training to learn the syntax before I take a PL/SQL class. Do you think I should just dive right into Oracle PL/SQL?
Actually I have a similar question. I am moving from a legacy development role into the BI group at my company. We use MS-SQL and MDM/MDS tools. My new group recommended I find a SQL boot-camp style class, like a week long, but as /u/alinroc mentions, the pools is very deep. It sounds like I need a combination of training geared for developer and reporting. Any suggestions would be appreciated. I'm in Southern California, if that helps.
I don't think you're going to find any generic, non-platform-specific classes that will get you what you need. All of the SQL dialects are fairly close to the standard and the variations are pretty well-documented. If you need to learn PL/SQL, get a PL/SQL class.
Sorry. SQL 2008
Many thanks, /u/sqlswerver.
That second one worked a treat for me. Thank you so much. :D This is such a big learning curve, but I'm really enjoying it. Thank you for the help.
 SELECT o1.ordered_by, o1.product, o1.order_date, o2.product, o2.order_date FROM order AS o1 JOIN order AS o2 ON o1.ordered_by = o2.ordered_by WHERE o1.product IN ('Product1','Product2') AND o2.product IN ('Product1','Product2') AND ABS(DATEDIFF(month, o1.order_date, o2.order_date)) &lt;= 3 AND o1.product &lt;&gt; o2.product That would get you the set of order information you want, pairs of orders by the same person of those two items within three months of each other. (please someone correct me if I'm wrong anywhere, I'm not great at writing code that I can never run or test)
If you want to find out which RMs don't have a Vendor, use a LEFT OUTER JOIN and use IS NULL in your where clause. SELECT r.RM_ID, r.RM_FName, r.RM_LName, r.date_assigned FROM relationshipmanager r LEFT OUTER JOIN vendor v ON r.RM_ID = v.RMID WHERE v.RMID IS NULL ORDER BY r.RM_LName
SQL is easy. I learned it from scratch when I was made redundant. It's not secret, you don't need a certification. Use Google.
Your last line should probably read AND o1.product &lt;&gt; o2.product, yes? But I also agree the one below (by SQLsherwood) is a bit better. edit: clarification
Not really sure what the issue is just by looking at your code, but in my experience, I've always had a transaction tied to insert statements like this. That may be something to look into. Also, with the using() block you have, you won't need to have Conn.Close(); the using() block will do that for you.
Great thank you so much!!
Great thank you so much!
&gt; I also changed the datediff from months to days, because months are less specific. (They probably don't care if the customer ordered in a month with 28 or 31 days.) You make an excellent point with the months. Though its even more extreme than even 28/31 days. DATEDIFF(month,dateX,DateY) simply returns the difference in the month number not the difference measured in whole months. So for example SELECT DATEDIFF(month, '2014-07-01', '2014-10-31') and SELECT DATEDIFF(month, '2014-07-31', '2014-10-01') both return 3 even though there are 122 days difference in the first example and only 62 in the second. Because 10 - 7 =3. I don't think most people would really consider 2 dates that are 122 days apart as being within a 3 month period. Another thing to keep in mind is that this doesn't pair the 2 products up. For example they might have purchased Product1 twice and then product2 a further 3 times in the 3 month period. Each of those 3 purchases of product2 will be listed as being linked to 1 product1 that was sold and then each of the 3 purchases of product2 would then be linked to the second product1 that was sold eg purchase of Product1 on 2014-01-01 and 2014-01-10 and product2 on 2014-01-01, 2014-02-01,2014-03-01 U/MS05 's query would result in the following records being returned Product OrderDate Product OrderDate product1 2014-01-01 product2 2014-01-01 product1 2014-01-01 product2 2014-02-01 product1 2014-01-01 product2 2014-03-01 product1 2014-01-10 product2 2014-01-01 product1 2014-01-10 product2 2014-02-01 product1 2014-01-10 product2 2014-03-01 product2 2014-01-01 product1 2014-01-01 product2 2014-01-01 product1 2014-01-10 product2 2014-02-01 product1 2014-01-01 product2 2014-02-01 product1 2014-01-10 product2 2014-03-01 product1 2014-01-01 product2 2014-03-01 product1 2014-01-10 U/SQLsherwood's query would result in the following records being returned Product OrderDate Product OrderDate product1 2014-01-01 product2 2014-01-01 product1 2014-01-01 product2 2014-02-01 product1 2014-01-01 product2 2014-03-01 product1 2014-01-10 product2 2014-01-01 product1 2014-01-10 product2 2014-02-01 product1 2014-01-10 product2 2014-03-01 My guess at what the OP is after would be something like Product OrderDate Product OrderDate product1 2014-01-01 product2 2014-01-01 product1 2014-01-10 product2 2014-02-01 Just not sure if that is what the OP is after or not.
&gt; , like SharePoint or Dynamics CRM. I've been an MS SQL DBA for almost 15 years, and I've started transitioning to a po I saw the word sharepoint and i almost threw up. talk about handcuff'd.
Are you running oracle on a Mac because I think they stopped doing that in version 10g? 
SQL is pretty straight forward to learn. But, it really depends on what version you're going to start off with. If you're already a pro at VBA you may as well start with Microsoft SQL server. You can download and install the free learning version **[here](http://www.microsoft.com/en-us/download/details.aspx?id=29062)**. You can also connect access as well if you're looking for something lighter weight. It's syntax isn't exactly the same and the query editor isn't any better than note pad but it does the job. To get access to the database within VBA you'll want to familiarize yourself with ADO, a pretty good tutorial can be found **[here](http://www.w3schools.com/asp/ado_intro.asp)** and you can find SQL tutorial ont he same site **[here](http://www.w3schools.com/sql/default.asp)** Basic code to connect to and work from an access database is going to look like this: strConn = "C:\Databases\AwesomeStuff.mdb" Set AC = CreateObject("ADODB.Connection") Set RS = CreateObject("ADODB.Recordset") AC.Open "Provider=Microsoft.ACE.OLEDB.12.0;Data Source=" &amp; strConn &amp; ";" RS.Open "SELECT Name, Amount, Quantity FROM SomeTable", AC intRow = 1 Do Until RS.EOF Cells(intRow, 1).Value = RS("Name").Value Cells(intRow, 2).Value = RS("Amount").Value Cells(intRow, 3).Value = RS("Quantity").Value intRow = intRow + 1 RS.MoveNext Loop RS.Close AC.Close Connecting to an SQL server will be kind of similar but you'll need a different connection string. There should be plenty of examples out there. To begin with get familiar with the following: 1. SELECT 1. FROM 1. WHERE 1. ORDER BY The next thing is to learn the "GROUP BY" syntax and use of sums and averages. Then next I'd learn table joins. But first thing I'd do is get a server with a good syntax editor installed and start learning the standard syntax and the [functions native to your implementation](http://msdn.microsoft.com/en-us/library/ms174318.aspx). EDIT: Also depending on what you're doing with the database you may want to familarize yourself with "Classic ASP" so you can make web pages using syntax that's nearly identical to VBA using the web server built into windows.
The solution has been posted but I'd suggest changing the 'friends' table, which makes queries easier. Still three columns: ID, UserID, FriendID If UserID 1 is a friend with UserID 2, you should get 2 entries: ID, 1, 2 ID, 2, 1 This way you can just use an INNER JOIN and be done with it.
Yes, query the query, I mis-read your last post.
 declare @day datetime set @day = GETDATE() IF DATEPART(DD, @day) &lt;= 27 --14 gives first Saturday, 13 = Friday, 12 = Thurs, etc. Select DateAdd(day, (14-DatePart(weekday, DateAdd(Month, 1+DateDiff(Month, 0, @day), 0)))%7, DateAdd(Month, 1+DateDiff(Month, 0, @day), 0)) +7 -- for second occurrence ELSE -- same for next month (2+DateDiff(month..) Select DateAdd(day, (14-DatePart(weekday, DateAdd(Month, 2+DateDiff(Month, 0, @day), 0)))%7, DateAdd(Month, 2+DateDiff(Month, 0, @day), 0)) +7 -- for second occurrence 
It would seem you are missing the @ for the input variables. comm.Parameters.AddWithValue("@GamerType", gamerType); comm.Parameters.AddWithValue("@Genre", genre); comm.Parameters.AddWithValue("@FavGame", game); comm.Parameters.AddWithValue("@Platforms", platforms);
I don't know if the problem sounds silly or not because I can't figure out what kind of data &amp; schema you're using, nor what your issue really is. There just not enough information here for anyone to help you.
It's possibly erroring out between the column ) and values. Add a space on either side. ") " + " values [...]" Are you able to write the sql statement to the console and execute that on ssms? 
Gets even wore when you try to datediff years. I had a lot of 17 year olds pulling up as 18, heh
That's very useful to know. I don't know if the syntax is the same, but I know cpanel's mysql has an export as sql to be able to execute with all the table creates and inserts. A linked server would be a lot easier if the mysql database has to remain online and changes are being made.
&gt; I want to create primary keys for properties using the city shortcut and a number. Why?
Use the database profiling tools (Profiler for SQL Server) and see if your command is reaching the database and with what parameters. Narrow down where the problem could be. 
Wow it worked like a charm! Thank you so much! I scratched my head for hours on this, what a relief! :-)
On first look, this could be easily resolved with a sub-query to separate the minimum values using MIN and COUNT (alternatively you could use [@row_number](http://www.tech-recipes.com/rx/17470/mysql-how-to-get-row-number-order-5/)/ORDER BY and a WHERE @row_number = 1). That way you are only LEFT joining the values you already want. There are other ways to do it, but this will be the most legible. 
No problem, as always just do some spot checks to make sure you're getting the correct results. As an aside, I think the WHERE clause could be: WHERE KVVertreter IN (0000,0008,0009) AND KundeArchiviert = 'aktiv' 
I added your script to my existing script. When I run it the date is a week and one day off. So instead of January 11th 2014 it's January 3rd 2014. InvDate is the field I'm checking. SELECT *, datepart(d,invdate)as invoiceday,DATEPART(m,InvDate) as invoicemonth, DateAdd(day, (14-DatePart(weekday, DateAdd(Month, 1+DateDiff(Month, 0, InvDate), 0)))%7, DateAdd(Month, 1+DateDiff(Month, 0,InvDate), 0)) as NextMonth FROM Journals WHERE CONVERT(varchar(19),AddingDateTime,110)&gt;= CONVERT(varchar(19),DATEADD(day,-30,getdate()),110)
If you'd like an alternate solution that uses only set operations, I offer the following. Even though this was already solved above, I always like the challenge of trying to accomplish something using set operations. So what you have here is a single query that gets you second Saturday of the next two months. WITH n1 AS (SELECT 1 n UNION ALL SELECT 1), n2 AS (SELECT 1 n FROM n1 a, n1 b), n3 AS (SELECT 1 n FROM n2 a, n2 b), nums AS (SELECT ROW_NUMBER() OVER (ORDER BY n) n FROM n3), dts AS ( SELECT DATEADD(day,nums.n - 1, DATEADD(month,1,DATEADD(MONTH,DATEDIFF(MONTH, 0, GETDATE()),0))) Dt FROM nums UNION ALL SELECT DATEADD(day,nums.n - 1, DATEADD(month,2,DATEADD(MONTH,DATEDIFF(MONTH, 0, GETDATE()),0))) FROM nums ), sats AS ( SELECT ROW_NUMBER() OVER (PARTITION BY DATEPART(MONTH,dt) ORDER BY dt) rn, dt FROM dts WHERE datepart(weekday,dt) = 7 ) SELECT * FROM sats WHERE rn = 2 
What's wrong with SharePoint? If a business lives in the Microsoft ecosystem it's a good system. There are open source CMS's I like much better, but I've seen some amazing things done with SharePoint. Now with the newer versions of Office, Lync, SkyDrive, and the other MS products SharePoint is a great way to tie them all together. I wouldn't however suggest SharePoint just by itself. It's a beast for sure, but given you have a department that can support and customize it and have the added MS apps it's very nice.
PluralSite has some great online training that's just generic SQL and also PL/SQL specific. Subscribing to them might be cheaper than taking an in-class course. Also I've used Global Knowledge many times, they have training sites all over the US, and they do Oracle and SQL training. For free resources I agree I like SQL Server Central's online videos. Also Brent Ozar and Dave Pinal are two of my fave SQL Bloggers. 
If you have SQL Server Standard Edition or higher I'd suggest using SSIS to load the data. Install the MySQL Connector on your local system (for testing) and the SQL Server, then you can move data between MySQL and MS SQL with little effort. You just need to make sure your data types and lengths match, but if not SSIS can do some conversions for you. Here's a resource showing how to get to MySQL in SSIS: -http://blogs.msdn.com/b/mattm/archive/2008/03/04/connecting-to-mysql-from-ssis.aspx Another option once your MySQL Connector is installed on your SQL Server is to setup a DB Link in MS SQL. This way you can query your MySQL Database from MS SQL just as if it were local. 
I've been meaning to try out [dbvisualizer](http://www.dbvis.com/download/). 
I use SSMS &amp; Visual Studio on Parallels. Sorry, but I haven't found a tool for MSSQL development/admin better than SSMS and/or Visual Studio.
Yeah, I have MS Windows running via VirtualBox on both my Mac and Linux to run VS and SSMS. Just would love to see some native tools. I always hoped MS would port SQL to the Unix/Linux environment to compete with Oracle and DB2, but I don't think that'll ever happen. It's just blah given I'm a Linux and OSX fanatic, yet I'm an MS SQL DBA :)
Bite the bullet. Buy a copy of Winblows. The cost for Windblows vs the time you'd spend trying to get the stack to work with a POSIX system is definitely in favor of just getting a copy of the OS.
With Visual studio you get some very basic syntax highlighting when using Expressions.
I haven't personally tried this, as I'm totally Windows, but maybe try http://www.razorsql.com/ Edit: Works on both Mac and Linux by the look of it.
it looks like sql server to me not oracle. the first is for a database in spanish and it getting the maximum from the suma(sum in spanish) column for each clientid the second is using a subquery and the "over" function to get the max salesprice for each pub_id hope that helps 
I am able to pull in data through a CSV currently, using the wizard. What I am trying to do is to do that without having to use the wizard and instead use a query. This would let me automate some table updates that I need to make by having it run the query instead of having to go in and manually use the import wizard. Does that make sense?
How are you using this code? In a function? View? Elsewhere? If you want to set a session variable: set somePrefix.someVariable = 'someValue'; If you want to read a session variable: select current_setting('somePrefix.someVariable'); Depending on your version, you might have to adjust postgresql.conf to allow somePrefix. 
I read this as, "Half-Newbie [who gives] smokin head needs your help Reddit". Disappointed.
Ah, got you now. I'm not aware of a solution that solely involves SQL, but you can set up a VB module or macro that performs the import &gt; process &gt; sweep in Access. If the source files have a static name and layout, you could link them instead of importing them, then have a stored query use the linked table to perform the processing to the destination table. But I really don't know enough about your setup to put odds on the viability of this option.
I have read the contrary, that tempdb is better off on the ssd. I don't have any personal knowledge, as we use fusion io, but there are a lot of articles on the matter showing benefits of putting tempdb on ssd. Example: http://www.sqlservercentral.com/Forums/Topic1324128-146-1.aspx
BCP, and even better through powershell
Why not allow all new users to use the existing dataset that is already in production? I think you're going to have to post more information before you are going to get a good answer.
+1 This is a terrible idea. Particularly since the city field in an address is way less consistent than you would think, and that city names exist in multiple states.
But how can I use that as the primary key?
It does not make sense to me. However, my professor wants me to do it. 
Yeah I agree. But right now that's the instruction I got from my professor. 
Something like: Select wh_id,wh_loc from ( Select max(sum(qty_onhand)),w.wh_id, w.wh_loc from warehouse w, inventory i where w.wh_id = i.wh_id group by w.wh_id, w.wh_loc) That would do it but I have no idea why it would matter whether or not you "see" the max sum in SQL mgmt studio. Cut and paste it into Excel or use the query inside the paren's on a SSRS report and don't display the sum field.
Yes NY to be 1 and LA-3 as the ranking is separate for the cities. 
Does your work have any virtual servers available? We use VMWare vdi clients and there's one for MAC, not sure about linux.
For one Visual Studio can connect to Source Control like TFS or SourceSafe where Report Builder can't. Also Visual Studio, in my opinion, gives more options than Report Builder. The team I used to work with called Report Builder the ~~Fissure Price~~ Fisher Price version of the two. It's honestly great for end users who need to build quick and dirty reports, but for IT I generally suggest Visual Studio. 
So why does this need to be the primary key again? You can make it a key if you really feel the need but I'd suggest making a regular primary key identity value. Like I said create a trigger on insert that searches the table for the last value inserted for the given state and increment it by 1. Google for the syntax.
This is standard SQL. SQL Server DBAs prefer nvarchar() over varchar(). create table properties ( property_id varchar(7) primary key, street_address varchar(64) not null, city varchar(110) not null, state_code char(2) not null, zip5 char(5) not null, unique (street_address, city, state_code, zip5) ); insert into properties values ('NY-1', '1234 Somestreet Rd', 'New York', 'NY', '37131'), ('SF-1', '4321 Yerba Buena', 'San Francisco', 'LA', '99491'), ('NY-2', '2234 Central Street Rd', 'New York', 'NY', '37333'); (I'm a consultant. Often the quickest way to clarify requirements is to produce something that's exactly what's called for, and that's also so obviously wrong that nobody will be shy about screaming about it.)
Why use an Access query to populate SQL Server with data in a CSV file? Use SSIS to directly update SQL Server from the CSV file (BCP or PowerShell as mentioned earlier will also be better than Access). This will be much cleaner and can be scheduled from the SQL Server Agent. 
Alter your stored procedure to *also* update the related tables.
Here is some example code for that returns pairs of purchases, which it works out by finding all the combinations with the first product being ordered before the second product and then removing any pairs where one of the items is also used in another pair that was ordered later. I have generated some example data to try and demonstrate some of the pitfalls that you can fall into with data like this though I am sure I have missed plenty (there is no substitute for real data.) Also keep in mind that while this gets an answer I have no idea how its going to run on 150,000+ rows... CODE below: --Generate example Data IF OBJECT_ID('tempdb..#medical_order') IS NOT NULL DROP TABLE #medical_order CREATE TABLE #medical_order( [Id] [int] IDENTITY(1,1) NOT NULL, [ordered_by] varchar(50) NOT NULL, [product] varchar(50) NOT NULL, [order_date] [datetime] NOT NULL) INSERT INTO #medical_order VALUES ('Jim','Product1','2013-01-01') ,('Jim','Product1','2014-01-01') ,('Jim','Product1','2014-01-10') ,('Jim','Product2','2014-01-01') ,('Jim','Product2','2014-02-01') ,('Jim','Product1','2014-02-10') ,('Jim','Product2','2014-03-01') ,('Jim','Product2','2014-04-30') ,('Jim','Product2','2014-08-10') ,('Jim','Product1','2014-09-01') ,('Bob','Product1','2013-01-01') ,('Bob','Product1','2014-01-01') ,('Bob','Product1','2014-01-10') ,('Bob','Product2','2014-01-01') ,('Bob','Product2','2014-02-01') ,('Bob','Product1','2014-02-10') ,('Bob','Product2','2014-03-01') ,('Bob','Product2','2014-04-30') ,('Bob','Product2','2014-08-10') ,('Bob','Product1','2014-09-01'); --Find all the ordered combinations WITH product_pairs(ordered_by, first_product, first_product_order_date, second_product,second_product_order_date,first_product_order_id,second_product_order_id, pair_id) AS ( SELECT * FROM ( SELECT o1.ordered_by , o1.product first_product , o1.order_date first_product_order_date , o2.product second_product , o2.order_date second_product_order_date , o1.id first_product_order_id , o2.id second_product_order_id , row_number() over (partition by o1.ordered_by,o1.ID order by ABS(DATEDIFF(D, o1.order_date, o2.order_date))) pair_id FROM #medical_order AS o1 JOIN #medical_order AS o2 ON o1.ordered_by = o2.ordered_by WHERE o1.product IN ('Product1','Product2') AND o2.product IN ('Product1','Product2') AND o1.product &lt;&gt; o2.product AND o1.ID&lt; o2.ID AND DATEDIFF(D, o1.order_date, o2.order_date) between 0 and 90 )a WHERE a.pair_id = 1 ) --Use the ordered combinations found to then remove the duplicate pairings SELECT ordered_by , first_product , first_product_order_date , second_product ,second_product_order_date FROM product_pairs p1 WHERE NOT EXISTS ( SELECT 1 FROM product_pairs WHERE p1.first_product_order_id = second_product_order_id ) 
I've created a trigger on my procedure for whenever a new insert is made from the c# form, the value of ProfileID (which is auto incremented) is sent to another table which has the UserID field which is NULL. This is my code so far: CREATE TRIGGER IDs ON dbo.UserProfile FOR INSERT AS SELECT ProfileID FROM dbo.UserProfile INSERT INTO dbo.Users ( UserID ) VALUES ( UserProfile.ProfileID //error here ) GO I am getting the error saying "The multi-part identifier "UserProfile.ProfileID" could not be bound."
Yup, random reads =ssd sequential = spindle. I like the idea of file groups to ssd. 
I would be inclined to use the second option, but only a single table is required. TBL_SUBJECT | ID | BookID | ParentID | OrderId | Descr | |:----|----------:|:---------:|--------:|---------| | 1 | 15 | 0 | 1 | Nuclear reactors | 2 | 15 | 1 | 2 | United States | 3 | 15 | 2| 3 |Safety measures You can use either an OrderId, or just order by ID if they are entered in the correct order in the database. SELECT Descr FROM TBL_SUBJECT WHERE BookID = 15 ORDER BY OrderID
Don't have an answer for you, but it might be easier to break it down with a cte then join in other tables. It looks like you are getting the partitioned/ordered columns from your base table and first two joins. Pull in your other tables once you've partitioned it. Not sure if this helps. Hope it works out for you!
With my limited knowledge of TempDB, it sounds like a good idea to have it on a faster reading/writing drive, but is there a concern with It being on an overall less available space drive? Assuming that it's not the only thing on the drive, that is.
Its a bit hard to tell (the solution to your problem) from afar without sample data, but I can tell you why you get 18 rows there. You are using a distinct, to remove duplicated rows. The rownumber however will increment over each row of the result set. Hence you will get back all rows of the result set, instead of the filtered down distinct 2 rows. Try using dense_rank(). From looking at the order by clause of the rownumber, that should work.
Stored procedures are just a collection of SQL statements grouped together by some logic, and are pre-compiled so that SQL Server can execute them more efficiently. You should avoid using triggers, as they are not very intuitive to debug. Instead, modify your original procedure to populate the Users table right after you populate the UserProfile table: ALTER PROCEDURE YourProcName ... AS BEGIN -- temp table containing the IDs of inserted records DECLARE @tbl TABLE ( UserID INT ); -- Your original insert logic INSERT dbo.UserProfile -- ... the rest of your insert statement here OUTPUT inserted.UserID INTO @tbl (UserID) FROM -- ... the rest of your original logic INSERT dbo.Users SELECT UserID FROM @tbl; END In the above example "inserted" is an alias for a table with the exact same structure as the table being modified, containing the new values. More details here: http://msdn.microsoft.com/en-us/library/ms191300(v=sql.105).aspx
Yea, I wish I could do this, but it's not my DB - the structure of it leaves much to be desired.
http://sqlfiddle.com/#!2/fa01aaf/2/0
wow thanks, is it possible tho, that alice starts to repeat? like alice country1 alice country2 alice country3 instead of having all 3 countries in one cell?
http://sqlfiddle.com/#!2/fa01aaf/5/0
http://sqlfiddle.com/#!2/8b4c8/4/0
If you want it to be in the same way you describe in your results, it's just as easy as a simple cross join. SELECT * FROM `names` CROSS JOIN `countries`
Yes, just confirmed it is because of the rownumber and distinct.
which rdbms?
Something else is Report Builder can open a report directly from the Report Server, and when Saved it goes back to the report server so there's no local file (other than temp while working). Visual Studio requires that you create a Report Solution to contain all your reports, then that solution is deployed to the report server. For Source Control and so forth I like the way Visual Studio does it, but as I mentioned before if you want to give users a quick way to build and deploy reports Report Builder might be the best option. Just be sure your Report Server databases (including Temp database) are being backed-up and that you've exported the encryption key for the Report Database. This way you can restore your report database to another server if ever needed. This is in lieu of keeping your reports in a Solution that's checked into Source Control. If all your reports are in Source Control you can easily redeploy them to another Report Server in a DR type scenario.
I don't know if the optimizer in MySQL works as it does in MS SQL, but often times nested selects perform worse than using a Join. For this I suggest waitaminuteholdup's suggestion of using Join instead of a Nested Select like this. Again this comes from my experience with MS SQL and DB2, so this might not apply with MySQL.
I do have VirtualBox on both my Mac and Linux systems, and that's how I generally get into MS SQL is through Windows running virtually. Just looking for a more native way to work in SQL.
I don't know ow to start the Oracle service =/.
At least a year, I think. It's **very** popular on Stack Exchange.
looks like march - april of '14. see traffic analysis in the link below http://www.7ools.com/www/sqlfiddle.com
Using it elsewhere. Just in a simple query editor for now. Is it different when doing it in a function? What do you mean by somePrefix? Is that the datatype of the variable? I tried typing in at the top of my code set date.startdate = '12/12/2013'; This is what I get [Err] ERROR: unrecognized configuration parameter "date.startdate" I'm using Navicat for PostgreSQL. 
I found it at some point over the past year, so it's been around for a little bit. Only trouble is that it seems to be having reliability problems more frequently as it grows. It was down most of the day yesterday; the site itself was up, but trying to do anything gave XML errors.
I just started hanging out on /r/sql recently, and this is the first place I've seen that uses it. I mostly hang out on SQL Server Central, and I've never seen SQLFiddle.com used there. I do need to frequent Stackexchange more, it's another good resource I tend to overlook all too often. But if they can get SQLFiddle working reliably I see it as a great place to setup code repositories. I think all DBA's have a large collection of scripts they use over and over again, some probably with hours of time invested in them. I haven't seen a good site though that gives developers a place to catalog such scripts, but SQLFiddle might be just such a site -- unless it's intended for only short-term examples for forums and such.
Just did that. I haven't used CTEs before but know about them. Thought it was time I tried them.
Great! They're really useful with these functions to bring them down as values. Inserting that into a temp/variable works the same way.
which version of PG? I'm not sure of the exact rules for somePrefix. It shouldn't already be in use. It should probably be a simple string, like 'my_app_name', or 'my_company_name' . Think of it as a namespace. You may have to set custom_variable_classes = 'somePrefix' in postgresql.conf If you want to declare a variable in a function (PL/PgSQL) you can do this: create function foo() returns void language plpgsql as $$ declare start_date date = '2014-01-01'; begin ... end $$; 
*Nowhere near an expert nor would I say I'm experienced but at about the same level as you. Just googled more* You will need an actual sql database setup on a server. I use a VM with a LAMP stack running on ubuntu 14.04 this is a whole other world of much learning but once you have the knowledge it's amazing what you can do for free. I then connect sql workbench to the database. There are a bunch of steps setting up the LAMP and you need to create a user that will be used to access the database with the proper permissions for the workbench. _then_ I design my database on workbench and reverse engineer it and bam... It's done. Lol. Talking about a month's worth of Google, YouTube, w3, etc.... All self taught. And still learning. 
Rownum is an Oracle construct, so yes, it's Oracle. Max is a keyword so as a column alias it should be in double quotes.
I'm pretty regular on Stack Overflow and I think I had noticed the creator, Jake Feasel mention SQLFiddle in couple of his answers. I had a look at the site and as a SQL dev, that's exactly what I needed and used it for couple of my answers as well. I posted it on Hacker News on 02.02.2012 - https://news.ycombinator.com/item?id=3538965. Jake mentions that was his first traffic spike. 
Congrats on hitting that sweet spot of a simple problem that needed a simple answer at just the right time. Good luck keeping up as it scales. It's a great site and it's only going to get more popular.
well, what you can do is, put the 6000 numbers you want to search into a (temp) table, and do an inner join with the table you are searching trough. That would look something like this : SELECT distinct tbl.* FROM searchedTable tbl INNER JOIN #searchValues tmp on tbl.StringValue LIKE '%' + convert(varchar(25), tmp.number) + '%' That will be very very slow I can ensure you, but it will be just as slow in any way you implement this particular logic. 
I just discovered it and it's really cool. I hope I never have to use it but I'm glad it's there for when I will need to.
jfeasel, Congrats on such a wonderful tool! I just became aware of it through a few links here on /r/sql, and I've started using it quite a bit since then. I'm really curious to know more about how it works technically, but that might be for another thread. Are you emulating the logic of each database engine, or do you actually have the various engines stood-up and querying against them? This is such an amazing project! 
In general this is going to be slow. Were I you, I would create a new field in the table and use one of the methods suggested to populate it with just the numberID value for the future. 
If you're looking for SQL quizzes, you can get weekly Oracle based questions at: http://www.plsqlchallenge.com/ Or are you looking for something more along the lines of discussing designing and building databases, queries, etc.?
Maybe an example would be better?
It's already denormalized and really it doesn't even meet first normal form. Extracting the value from the field would not denormalize it further. If you want to not have duplicate data in the field, maybe doing a computed column with an index on the computed column would greatly simplify the process for the future, although it sounds like that could lead to bugs if the form of the datablob changed in unexpected ways. 
This is my first contribution and a total hack, but it's something. And it works! :) SELECT CONCAT('&lt;img src="http://mydomainname.com/wp-content/uploads/', SUBSTRING(`img_string`, LOCATE('storage/', `img_string`) + 8, LOCATE('.jpg', `img_string`) - (LOCATE('storage/', `img_string`)+8)), '.jpg"&gt;') FROM `reddit` And it continues off to the right. Don't know the right formatting to use here.
&gt; Is the content that you want to eliminate always the content following the question mark? No, I want to remove the following: 1. style="width: 250px;" 2. /storage/ 3. ?__SQUARESPACE_CACHEVERSION=1413512531524" alt="" align="left" hspace="5" vspace="5" that number you see after CACHEVERSION= is different every time **I also want to:** 1. remove any uppercase 'JPG' and replace with lowercase 'jpg'. 2. add '/uploads/' to the beginning of the source string where it is currently '/storage/'
FROM `reddit` ? What does that mean? Actually, I don't understand any of that and it seems like you are not entirely sure, so I wouldn't want to take a chance on using that. I don't understand what the +8 means and I don't really see anything in there about that crazy '?__SQUARESPACE_CACHEVERSION=[random number garbage]' stuff.
[Nested sets](http://mikehillyer.com/articles/managing-hierarchical-data-in-mysql/) are a possibility you may not have considered. Your second option can also be viable depending on what options your sql provider supports - but unfortunately [MySQL does not support them](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression) to the best of my knowledge. I highly recommend picking up a copy of SQL Antipatterns. It has a whole chapter on this subject that makes for an extremely interesting read.
Oh ok, I didn't know what level of sql you were coming from. Here's an explanation of the query: It works using a concatenation function which combines strings together. The first part of your image string is first, then the nonesense I'll explain next, and then the end of your image string. The nonsense is a substring function which takes a part of a string starting at a defined location and continuing for a set number of characters. Below is an explanation of my substring function: * First, I just named the table I was pulling from "reddit" you should replace that with your table name * Second, this function finds the location of "storage/" in your image string as the number of characters from the beginning (eg. storage/ might start at the 55th character) * The +8 gives you the character after "storage/" (i.e. the first character of your image string, or the 63rd character). This is the character where the substring would start. * The next portion takes the difference between the first character of your image string and the first character of ".jpg". This is the length of the substring we want. We get the image portion of the substring and concatenate it into the tags you want. Here's the mysql documentation on substring (mysql because you're using wordpress, which uses mysql) http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substring So basically, I'm taking the file name without the .jpg and putting it into the example string you gave. Make sense? Feel free comment back if it doesn't. 
try this instead SuggestedRetailPrice &gt; 0 AND CASE WHEN AdEnd &lt; GETDATE() THEN AdPrice ELSE BasePrice END / SuggestedRetailPrice &gt; 0.8 
Thank you. I just did that. I mean I still don't get why the primary key has to be in the city number format. 
dear OP -- db2? sybase? mysql? oracle? informix? firebird? give us a friggin chance, man
Not true for three reasons: * The number ID field could be a composite foreign or candidate key * As far as I can tell with the given information, the structure probably doesn't violate 2NF * Adding another column to the same table would cause it to violate 1NF Edit - Not sure how much value a calculated column would be either as it doesn't look like there is a consistent string pattern to build one. 
GROUP BY with HAVING
Oh wow, thank you... so then if I understand you correctly, you are just going in the original link and grabbing what you want and concatenating it into a new thing? And just disregarding all that SQUARESPACE random number generator garbage completely? That's pretty awesome. Does it change the old links to the new format throughout the database, though? I see the keyword is 'SELECT' so I'm not sure if that saves it or changes it anywhere. This would be applied to over 800 posts with 1 offending image each. I'd also have to find what table they're in since `reddit` obviously isn't it ;)
They say there is two discreet tables: * table1 (xxxx_numberid_xxx, etc, etc2) * table2 (numberid, other, info) The first column of table1 is doesn't necessarily contain anything else in table one. It could potentially even be a primary key depending on how its populated. At least that is my take on how I see the problem. 
I've updated the OP to say "MySQL" so I don't piss anyone else off :) Or get the wrong info, too, I suppose!
&gt; I mean I still don't get why the primary key has to be in the city number format. There's no reason for that within the relational model. There's no reason with respect to SQL best practice or SQL common practice, or with respect to database design best practice or common practice. My guess is that somebody expects you to learn something by doing it this way. It *will* create problems you might need to solve at some later time. * By default, SQL Server will create a clustered key on "property_id". On the one hand, that column isn't a good choice for a clustered key. On the other hand, this won't kill performance for whatever you're doing *right now*. * You'll run out of useful city abbreviations or numbers pretty quickly. * You'll find it hard to add the next key manually. * A stored procedure can automatically generate the next key, but that's probably a whole new programming language for you to learn. * Using a stored procedure will raise issues with transaction isolation and privileges. * Other issues that aren't on the tip of my tongue, because I'm sleepy.
Cool, I will check this out. Thanks.
well, yeah... you would have to replace that with the actual name of your column you didn't give any info on what your table/column looks like
What do you mean by running total? Like a sum?
Oooooh... thank you! I figured out they're all in the_excerpt in wp_posts so I changed img to the_excerpt and... something happened! Showing rows 0 - 29 (2503 total, Query took 0.0054 sec) YAY! Thank you! Only problem now is I was so excited I forgot to change the domain. Herpa derpa. Is there an easy fix for changing 'http://mydomainname.com/wp-content/uploads/' to my actual domain name? It's a duplicate, so if you are now annoyed by me, I understand. I will make sure to run the code with the correct domain name next time. 
You know what's odd? It totally changed all the 2,000+ instances according to SQL message, but when I go to the pages, the image source code is still exactly the same. Clear cache and everything, too.
Your solution worked as well: Showing rows 0 - 29 (2503 total, Query took 0.0092 sec) ...however, for some odd reason, even though it changed the SQL, it still looks exactly the same when I view the source code on the website. I cleared the cache, too. 
you don't specify an order for the data which you will need for a running total. supposing you want to order by customer_id and ID the following should work. with q as ( SELECT CUSTOMER_ID, ID, QTY, SELLING_UM, PART_ID, CUSTOMER_PART_ID, CALC_UNIT_PRICE, ctQty, row_number() over (order by customer_id, id) rn FROM dbo.vw_QuoteQty WHERE (ID = 'q00002') ) select *, (select sum(ctQty) from q q2 where q.rn &lt;= q2.rn) running_total from q order by customer_id, id
SELECT only retrieves, never changes anything i believe you may be looking for UPDATE?
It's not a blob though, don't know where you are getting that idea.
I just noticed something: This message: "Current selection does not contain a unique column. Grid edit, checkbox, Edit, Copy and Delete features are not available." Apparently, the table row all these image links are in is impossible to alter. 
I just noticed something: This message: "Current selection does not contain a unique column. Grid edit, checkbox, Edit, Copy and Delete features are not available." Apparently, the table row all these image links are in is impossible to alter. 
Oh, well I'm trying to change the entire text of the links to format like I said in the OP so it spits out properly in the search results template and isn't trying to reference Squarespace's cached image anymore. Would just switching the first term to UPDATE do that?
you're looking for UPDATE, not something in your grids (whatever those are)
So, would this work?: UPDATE CONCAT('&lt;img src="http://mydomainname.com/wp-content/uploads/' ,LOWER(SUBSTRING_INDEX(SUBSTRING_INDEX(post_excerpt,'/',-1),'?',1)) ,'"&gt;') AS new_tag FROM wp_posts 
 UPDATE wp_posts SET post_excerpt = *expression* really no need for a separate thread
no you don't update an expression, you update a table by setting one or more columns to a new value (in your case, you set post_excerpt to a new value based on current value of post_excerpt)
I really dont think that is going to work. Post_excerpt contains a lot more than just those image links. I started the new thread because the old one was now a confusing mess since I realizes I didnt explain it properly.
&gt; Post_excerpt contains a lot more than just those image links looks like you didn't explain things very well in this thread either
Looks like that is true. The excerpt grabs the title, thumbnail image and first 500 characters of a full post. The thumbnail images are the only part of the excerpt I want to change. The excerpts are in wp_post within post_excerpt
&gt; Thanks but I have no idea what that means. it means.... ah, heck, just go to your new thread 
&gt; The excerpt grabs the title, thumbnail image and first 500 characters of a full post. still not explained well as far as i can tell, post_excerpt is a single column, right? pretty difficult to imagine a title plus a thumbnail image plus some text in a single column 
They say a picture is worth a thousand words. Perhaps this will help? http://i.imgur.com/9CkKg43.png
Since you are using SQL 2012 you can use a windowing function to get a running total. As already mentioned by cavadure, you need to specify an order. SELECT CUSTOMER_ID ,ID ,QTY ,SELLING_UM ,PART_ID ,CUSTOMER_PART_ID ,CALC_UNIT_PRICE ,ctQty ,SUM(ctQty) OVER (ORDER BY &lt;column&gt;) AS running_ctQty FROM dbo.vw_QuoteQty WHERE ID = 'q00002' You could also do some other fun stuff like PARTITION BY ID in your OVER clause.
If AdEnd is nullable, add "AdEnd IS NOT NULL AND" in front of the compare to GETDATE()
Thank you! Did not know that LIKE could be used like that in a Join.
I like the involvement guys, thanks. It's just a one time reporting job so don't put too much time in this. :) Because the application was missing an appropriate field for the numberid, the users put it in a comment field with all kinds of different formats. Pretty :)
yes, that helps... helps to show that my solution based on the column consisting only of the img html tag is wrong you need to adapt the other solution in your other thread which used LOCATE to find the position of the img tag not sure how the UPDATE is going to work in that case, however good luck
if AdEnd happens to be null, then the WHEN is false, so the ELSE gets used instead
1 4444 1 4444 2 3333 Checking 4444 &lt;--- is that consistent or changes based on something or the other? desired result: 1 YES 2 NO something like that? 
Well your catalog is looking great!! designing a table as a catalog is a good strategies to define your number list. I am not aware of programming that much but its looking good.
As per the sidebar we no longer accept basic tutorials here.
OK thanks. I appreciate your feedback. I started to Google for "SQL Wordpress changed the_excerpt" last night and I probably should have thought of that sooner. Wordpress seems to have been around long enough that almost every aspect of working with it has been covered by someone somewhere. I found some stuff, but I haven't had the chance to read it. If I figure out the solution, I will post it here.
The standard way to do it is via ACL: http://en.wikipedia.org/wiki/Access_control_list 
add both expressions ( CASE WHEN [AdEnd] &lt; GETDATE() THEN [AdPrice] END) and {(ISNULL( CASE WHEN [AdEnd] &lt; GETDATE() THEN [AdPrice] END, [BasePrice]) )/[SuggestedRetailPrice]} to your select list, remove the 'suspect' condition from your code, add [AdEnd]&gt;= getdate() instead, run and take a look @ the results. This might give you an idea what's going on. 
Can you give more details on what application or process will be reading in or using this data and how the data will be inserted or updated in the database?
Each row in the db is just a path to a file. I want to allow/disallow access to the file on a per-user level. The application just uses the file path to serve the file, if authorized, and files are added to the DB by running a recursive directory scan. I'm starting to think groups are my answer to this problem: Add every new user to the 'public' group, and make whatever is public accessible. As users need access to more files, they get added to the groups with access. This should keep the dataset small and with a bit of planning still allow granular per-file/per-user control. Just might end up with a user being in a group containing only them if they need special access to certain files. 
it would look something like this account# Code# 1234566 345 1234566 675 1234566 784 1234566 450 4567776 789 4567776 677 4567776 233 8900223 450 8900223 345 9000223 980 now i need to produce a query that will group the multiple instances of the same account in one column into 1 row, and a corresponding row that says yes/no for the presence of code 450 edit: this is what i would like the results to look like given the above sample data (formatting) 1234566 YES 4567776 NO 9000223 YES
it would look something like this account# Code# 1234566 345 1234566 675 1234566 784 1234566 450 4567776 789 4567776 677 4567776 233 8900223 450 8900223 345 9000223 980 now i need to produce a query that will group the multiple instances of the same account in one column into 1 row, and a corresponding row that says yes/no for the presence of code 450 edit: this is what i would like the results to look like given the above sample data (formatting) 1234566 YES 4567776 NO 9000223 YES
You can have as many columns in the OUTPUT clause as you need, provided they are in the "inserted" structure, and you set up the staging table accordingly: DECLARE @tbl TABLE ( UserID INT, ProfileID INT ); INSERT dbo.UserProfile ... OUTPUT inserted.UserID, inserted.ProfileID INTO @tbl (UserID, ProfileID) FROM ... INSERT dbo.Users SELECT UserID, ProfileID FROM @tbl; 
Are you talking about converting this to Joins instead of nested Selects? If so there's no automatic way I know of, just start with the center and go out. I just built this which may be close, but I have no way to test it. Maybe it'll get you started: select P.rental_id, sum(P.amount) as totalP, r.inventory_id, sum(p.total) ttl, i.film_id, sum(r.ttl) as totalR, f.title, sum(i.total) AS totalI from payment P INNER JOIN rental r ON p.rental_id = r.rental_id INNER join inventory as i ON r.inventory_id = i.inventory_id INNER JOIN film f ON i.film_id = f.film_id GROUP BY P.rental_id,r.inventory_id,i.film_id,f.title order by sum(i.total) desc 
You can usually rewrite table constructors as [common table expressions](http://www.postgresql.org/docs/current/static/queries-with.html) (CTEs). 
There is no table to drop. That is a perfectly normal error to see when running an install script for the first time. You can run the script without the drop statements if none of these objects exist. Here is a query to see which tables already exist in your schema: select table_name from user_tables where table_name in( 'TELECOMMANDE_FIXE', 'EMPLACEMENT', 'MESURE', 'TELECOMMANDE', 'ETATCAPT', 'TEMP_IDEALE', 'ECLAIRAGE_AUTO', 'PIECE', 'CAPTEUR' )
But then I get this error for the table Telecommande_fixe : create table Telecommande_fixe( code_tele_fixe number(2) ORA-02270: no matching unique or primary key for this column-list 
Did you even try to research that error? There is a list of excellent responses if you just [copy the error text into google](http://lmgtfy.com/?q=ORA-02270%3A+no+matching+unique+or+primary+key+for+this+column-list). wherever you have *REFERENCES table_name(column_name)* you need to make sure that the column is a primary key.
oo I didnt knew that, thank you !
Your example led me to the right answer. Here's the new query: select f.title, sum(p.amount) total from payment p join rental r on r.rental_id = p.rental_id join inventory i on i.inventory_id = r.inventory_id join film f on f.film_id = i.film_id group by f.film_id order by total desc limit 10 ; This is profoundly better than my original query. I think I understand what's happening. It's matching the keys between tables and duplicating matching rows, thus creating a massive temp table. Then the group by tells the sum function how many rows to add together, thus collapsing the table into a smaller one with 1 row per film. Thanks so much for your help! 
FYI, By default SQL will execute this cartesian join where there are no clauses. So this will do what your multi line complex inner joining achieves SELECT t1.names, t2.country FROM table1, table2 order by 1,2
Briefly reading through it, it seems that the join to 'SHIFTS' needs more than a date (Shift_Date = Calls_Date).
Use the replace function. Start by doing a select replace on that column, and tune it so that the string you want updated comes back correctly. For example, replace /storage/ with http://mydomainname.com... Then wrap that function with another replace to change another section, like JPG to jpg. Repeat as needed, then profit!
What is the structure of the shifts table?
Cool, that will work to some extent, but if every one of those SQUARESPACE CACHE numbers is different (which they are), it won't really work effeciently, right? I mean, for 800+ posts, I would have to do that 800+ times for those numbers alone. Also, the *style="width:250px"* is not always 250. Sometimes it's 200 and probably many more dimensions.
Are there consistent characters (or numbers of character) before the NumberID? Is the NumberID a consistent length? I would seek to identify common, programmable characteristics of the nested NumberID data inside your field, and create a temp table with just the NumberID. Then, join on that temp table. This will be much faster than some of the other suggestions.
You should use the ANSI-92 syntax for several of reasons * The use of the JOIN clause separates the relationship logic from the filter logic (the WHERE) and is thus cleaner and easier to understand. * It doesn't matter with this particular query, but there are a few circumstances where the older outer join syntax (using + ) is ambiguous and the query results are hence implementation dependent - or the query cannot be resolved at all. These do not occur with ANSI-92 * It's good practice as most developers and dba's will use ANSI-92 nowadays and you should follow the standard. Certainly all modern query tools will generate ANSI-92. * It does tend to avoid accidental cross joins.
For this you need to think in terms of sets. Get the data into a set that you want to work with and then perform a simple count. -- dummy calls table with calls(call_date, time, status) as( select to_date('01/01/2000','MM/DD/YYYY'), '9:00', 'Closed' from dual union all select to_date('01/01/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/02/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/02/2000','MM/DD/YYYY'), '15:00', 'Closed' from dual union all select to_date('01/02/2000','MM/DD/YYYY'), '9:00', 'Closed' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '15:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Closed' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '9:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '15:00', 'Closed' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '15:00', 'Open' from dual union all select to_date('01/03/2000','MM/DD/YYYY'), '15:00', 'Closed' from dual ) select distinct call_date, time, status, count(*) from( select call_date, -- transformation logic for hour to a shift name case when hour &gt;= 8 and hour &lt; 14 then 'Early' else 'Late' end time, status from( -- normalize our data select trunc(call_date) call_date, -- ensure the day is clean -- convert time to an hour number to_number(to_char(to_date(time,'HH24:MI'),'HH24')) hour, status from calls where -- restrict data to a given time range (inclusive) to_number(to_char(to_date(time,'HH24:MI'),'HH24')) between 8 and 20 ) ) group by call_date, time, status order by call_date, time, status
There are a lot more functions you can use for string manipulation, such as finding the position where a string is, selecting a portion of the string, etc. Combine those together to find out where the squarespace string starts and where it ends, then exclude it from your selection. You will end up with a pretty long query but it is possible and will take some familiarity with your data to know how to get it working.
I'm really sorry but I don't understand this, could you eli5?
**Terminology and Concepts** So first things first. *dual* is special table in Oracle that has 1 column and 1 row. It is used for creating a record to work with. select 1 from dual will return one record with one column with the value of 1. select to_date('01/01/2000','MM/DD/YYYY') my_date from dual will return one record with one column with a date value for january first 2000. Second, I used the *WITH* clause to create an inline table for the duration of the query. This is what I use when I am working with someone elses data and can also provide a workable dataset for others to test and run my query. with my_table(column_one, column_two) as ( select 1, 2 from dual ) select * from my_table **returns** column_one | column_two ---|--- 1 | 2 Here you can see I created an inline table called *my_table* and I am also using the dual trick to create a fake row for that table. To clarify there is no *my_table* object in my schema, i literally just created a temporary object in this query. with my_table(column_one, column_two) as ( select 1, 2 from dual union all select 3, 4 from dual ) select * from my_table **returns** column_one | column_two ---|--- 1 | 2 3 | 4 Here I am adding *UNION ALL*. It will combine the query results for the two query it appears between. Think of it like concatenating SQL statement results together. The SQL statements need to have the same number of columns and data types in order to work, which means you cannot arbitrarily union two very different queries. Since each query returns 1 row essentially I am just defining fake rows to use later on. Again all this is doing is creating a dataset for me to query against. I should also point out that *UNION* will remove duplicate rows and *UNION ALL* will preserve any duplication if there is any. Third I am using nested queries to transform the dataset in steps. select one from( select 1 one from dual ) **Returns** one| ---| 1 | **The Query** So in order to read the query you have to go to the innermost query and work your way out. Think of the inner queries as a function that is doing some work and then returning their result set to an outer query until there are no more outer queries. Below is the innermost query. which is used to do a few things: 1. calls *trunc* function on the date column to make sure any time associated to the date is removed and it is only precise to a day. 2. converts the time text to a number. The string literal '8:00' becomes the number 8. '15:00' =&gt; 15, '20:00' =&gt; 20 ... 3. I also added a where clause to make sure the hours are within a given range. I only added this because it was in your original query. *i have to put something here so the code formatting will work and it isn't bundled into the third list item* select trunc(call_date) call_date, -- ensure the day is clean -- convert time to an hour number to_number(to_char(to_date(time,'HH24:MI'),'HH24')) hour, status from calls where -- restrict data to a given time range (inclusive) to_number(to_char(to_date(time,'HH24:MI'),'HH24')) between 8 and 20 call_date | Time | Status ---------|----|------ 01/01/2000|9| Closed 01/01/2000|9| Open 01/02/2000|9| Open 01/02/2000|15| Closed 01/02/2000|9| Closed 01/03/2000|9| Open 01/03/2000|15| Open 01/03/2000|9| Open 01/03/2000|9| Open 01/03/2000|9| Closed 01/03/2000|9| Open 01/03/2000|9| Open 01/03/2000|15| Closed 01/03/2000|15| Open 01/03/2000|15| Closed The next step modifies the innermost queries returned dataset a little more. The goal of this part query is to transform the hours into a string literal shift name *Early* or *Late*. select call_date, -- transformation logic for hour to a shift name case when hour &gt;= 8 and hour &lt; 14 then 'Early' else 'Late' end time, status from( ... inner query from above ) call_date | Time | Status ---------|----|------ 01/01/2000|Early| Closed 01/01/2000|Early| Open 01/02/2000|Early| Open 01/02/2000|Late| Closed 01/02/2000|Early| Closed 01/03/2000|Early| Open 01/03/2000|Late| Open 01/03/2000|Early| Open 01/03/2000|Early| Open 01/03/2000|Early| Closed 01/03/2000|Early| Open 01/03/2000|Early| Open 01/03/2000|Late| Closed 01/03/2000|Late| Open 01/03/2000|Late| Closed lastly we are just counting the calls assuming that 1 call record means 1 call. Also making sure to group by all of the metrics that you mentioned before. select call_date, time, status, count(*) from( ... inner query from above ) group by call_date, time, status order by call_date, time, status call_date | Time | Status | Count ---------|----|------|------- 01/01/2000|Early|Closed|1 01/01/2000|Early|Open|1 01/02/2000|Early|Closed|1 01/02/2000|Early|Open|1 01/02/2000|Late|Closed|1 01/03/2000|Early|Closed|1 01/03/2000|Early|Open|5 01/03/2000|Late|Closed|2 01/03/2000|Late|Open|2
I could stretch agreement to all but the first statement. I still have great trouble with understanding ANSI-92 syntax. Maybe I just have not put the time required into in, and it is so verbose, very messy. 
Assuming that there are no conflicting Shift_Times, something like this should give your original requested output: select c.call_date, s.shift, c.status, count( *) as "calls" from calls c join shift_times s on c.call_time between s.start and s.end where c.call_date = ... and c.call_time between ... and ... group by c.call_date, s.shift, c.status If there are conflicting Shift_times, you will need to define which of the staff is important to determine the shift - Taken_By or Assigned_To. Using that column, add a join to Staff to the statement above, then join to Shifts using Call_Date, Shift and all 4 of the staff FKs in the Shift table (all to the Staff_Code, OR'd together). 
Select Count(Distinct UserID) from Table Where UserID not in (Select UserID from Table where PageID != 'Page1')
&gt;(Select PageID from Table where PageID != 'Page1) Ok, so this is selecting all PageIDs that don't equal Page1. Make sense. This will include all PageIDs that are anything other than Page1. Got it. &gt; Select UserID from Table Where PageID not in (All PageIDs that are not Page1) Ok, this will select all UserIDs where the PageID excludes anything that is not Page1. That makes perfect sense and I feel dumb that I couldn't figure it out. Thanks! 
I actually messed up the SQL initially, my revised comment should address your requirement. In the sub-query (the query in parenthesis), you want to select all UserIDs that have a PageID other than Page1. Then in your query, you want to exclude any users that are listed in your subquery. 
You sir, are a god amongst mere mortals. Works great, thank you for your time and effort.
In Access, I'm pretty sure you need to create subqueries to have multiple grouping sets. In MSSQL there are actual [grouping sets](http://technet.microsoft.com/en-us/library/bb510427%28v=sql.105%29.aspx). Oh and Access and MSSQL are 2 very very very different things.
Replace your where clause with a comparable having clause. HAVING MAX(CASE WHEN PageId = 'Page1' THEN 1 ELSE 2 END) = 1
So, the problem with your query is that it's been running for 8 hours without a result. No idea why. 
Try SELECT SUM(x + y) as z, SUM(CASE WHEN x+y = Primary_key_field THEN x+y ELSE 0 END) as partial_z FROM MyTable blah blah Try to always post your schema with an sql question, it helps a lot. 
&gt; SUM(x + y) as z, &gt; SUM(CASE WHEN x+y = Primary_key_field THEN x+y ELSE 0 END) as partial_z I am not sure what this is supposed to be doing... I am trying to multiply quantity by price, not add it. My schema (not sure... so I put the code and output) http://i.imgur.com/6Dnoegm.jpg I basically want to add all of the invoice ID 1s together, the invoice ID 2s together, etc. Something is wrong because the invoice ID is a PK and therefore should be unique with no difference in invoice 1 and invoice 1, but you can see that the items are different. What..?
As of just a couple versions ago, SQLite now has [Recursive CTEs](http://www.sqlite.org/lang_with.html). One of the "outlandish query examples" on that page is a query that draws a mandelbrot set. Mostly for giggles, I implemented a tool that mostly uses their query to do all the work, but lets you zoom into the set, resize the window for different resolutions, etc.
That actually works really good. Same results as my other longer method and very quick too
It should definitely be faster than the IN/subquery
There are a couple of ways you can do this. Have sub queries in the select statement, or additional data in the FROM statement where you have statements creating a dataset. Also, it is easier to grab the year value from the data and work on the year value only, rather than start and end dates. This is an example of additional FROM databsets, though I do not have the database to confirm it is all syntactically correct, but it wil give you the example of the HOW TO. SELECT v1.Title , v1.Platform , v1.[Price Bought] , v1.[Date Bought] , v1.[Part of Series] , v1.[Bought in Bundle] , val(year(v1.[Date Bought])) AS yearBought , v2.totalPlatformPurchaseValue , v3.totalGamePurchaseValue FROM videoGames v1 , (SELECT videoGames.Platform , sum(videoGames.[Price Bought]) AS totalPlatformPurchaseValue , val(year(videoGames.[Date Bought])) AS yearBought FROM videoGames WHERE [Price Bought] is not null AND [Date Bought] is not null GROUP BY videoGames.Platform, val(year(videoGames.[Date Bought]))) AS v2 , (SELECT sum(videoGames.[Price Bought]) AS totalGamePurchaseValue , val(year(videoGames.[Date Bought])) AS yearBought FROM videoGames WHERE [Price Bought] is not null AND [Date Bought] is not null GROUP BY val(year(videoGames.[Date Bought]))) AS v3 WHERE v1.platform = v2.platform AND v2.yearBought = v3.yearBought AND year(v1.[Date Bought]) = v2.yearBought AND year(v1.[Date Bought]) = '2014' &lt;== change as required.
As long as you know basic SQL, your r experience and stats degree will carry you the rest of the way.
Gotta have a laugh, right? I've been working through project euler to teach myself recursive ctes, quite enjoying it 
I have hired a few people for SQL-heavy analyst roles, and I must say that I would never hire someone without some sort of technical interview where we walk through some problems together. I have hired people without previous professional SQL experience, and who did show up a fully-baked SQL programmer, but I definitely wanted to see how their mind approached the problems. So if I were you I would get on something like sqlzoo.net and try and get as acquainted with as much SQL as you can. What is your geographic region, btw?
Thanks, this is reassuring. Nonetheless, I am going to continue practicing writing queries.
San Francisco, CA. Yeah I have been using SQL Zoo, reading Head First SQL, and going through a self-paced Stanford Database course.
Ah good, sounds like you are on the right track. I know of an opening or two on the East Coast, but that won't be too much help, I suppose.
&gt;Access and MSSQL are 2 very very very different things. Well, crap. I hate that I can't edit the title now.
You can try aggregating subquery in select. Sth like: select r.sname, (select count(bname) from boat where rating &lt; r.rating) from sailor r;
Seriously, you need to help us out here. Whenever you ask a question regarding data and a desired output you need to give us some sample data and a desired output table. It will help remove ambiguity in your question. Post your current version of the query at a bare minimum so we can try to get some context to what you are working with. look at the table section for making tables in comments: http://reddittext.com/ What version of Oracle? What tables are involved? Is this the desired output? Project | Associated Projects -------|------------------- IR1234 | IR1,IR2,IR3 If so and you are on 11g then have a look at [LISTAGG](http://docs.oracle.com/cd/E11882_01/server.112/e26088/functions089.htm#SQLRF30030)
Oops, yes, this is where not having the database to confirm query a problem My cut and paste gave the wrong column name *o.date_purchased* from another query example I was working from. It should be *[Date Bought]* , have corrected the above SQL. also missed the AS qualifier for name alias, not required by some DBMS. See how you go, and please try and understand what I have done. 
I have an opposite question for OP or anyone knowledgeable: I have fairly good DML skills, good procedural programming skills and basic knowledge of OOP. What I lack is a degree in comp sci, stats, or science. What would someone like me need to get a data analyst position? 
 select distinct account#,'YES' from table where code#=450 union select distinct account#,'NO' from table a where not exists (select 'true' from table b where a.account# = b.account# and b.code=450) probably not the best solution, but should work.
For example. how would you achieve [this query](http://www.reddit.com/r/SQL/comments/2l1l3e/ms_sql_running_total_by_group_then_by_everything/clr8nhl?context=3) with ANSI-92?
It seems like you can use a date dimension here. To read about multi-dimensional concepts: http://www.amazon.com/Data-Warehouse-Toolkit-Definitive-Dimensional/dp/1118530802/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1415023036&amp;sr=1-1&amp;keywords=the+data+warehouse+toolkit 
MS SQL has a tool to suggest indexes for you. Just use SSMS run the query then look at the execution plan. It will show you where the query does all its work and also suggest indexes. If it suggests indexes just right click and it will create a new tab with the prepopulated creation script.
If you understand the basics of the main SQL clauses (SELECT, FROM, LEFT JOIN, WHERE, GROUP BY, ORDER BY) you'll be able to figure out anything else with a minute or two of Googling. 
What are the requirements for an analyst and what does it pay. I'm doing a lot of report writing and SQL is stupid easy for me. I'm wondering if I should pick up some other skills and see if I can make more money elsewhere.
I'm a bit confused by the OP but is this what he wanted? http://sqlfiddle.com/#!2/bc662/2 
Do you mean top ten results for different keys? Example: SELECT pk, value FROM (SELECT rn = row_number() OVER (PARTITION BY pk ORDER BY your_criteria), pk, value FROM data_source) WHERE rn &lt;= 10
What you really wanted isn't to faff with subselects, it's a simple left join, by adding two words to your query: Select sname, count(*) From sailor LEFT JOIN boats Where sailor.rating &gt;= boat.rating Group by sname
*headdesk* Can we just attribute that mistake to a brain fart combined with low confidence in my newbish SQL knowledge. Okay, I've added that revision. Also, I noticed that you had vi.Platform instead of v1.Platform So I changed that to match all of the others in that list. I also capitalized the GROUP BY clauses and the AND um things that are part of the WHERE clause at the end. I'm not sure if that really makes a difference, but it's how I learned them. The only real problem now is that I'm getting &gt; Undefined function 'videoGames.val' in expresion. I think that means that I go from 'videoGames.val' to a different function, but I'm not sure which one. I noticed that when I run it, it's 'videoGames.val' that gives me the error, but not all instances of 'v1.val', so I replaced the former with the latter, but then I got the same error: 'Undefined function 'v1.val' in expression'. I'm sorry to be such a bother, I'm still really new with SQL
In addition to implementing the correct indexes, as previously suggested, I recommend creating a Calendar table. You are calling the DATEPART() function numerous times. You could eliminate most, if not all, of those by joining the Invoices table to a Calendar table. Here is an example Calendar table DDL: DECLARE @startdate date DECLARE @enddate date SET @startdate = '1700-01-01' SET @enddate = '2020-12-31' DROP TABLE dbo.Calendar; CREATE TABLE dbo.Calendar ( [date] date NOT NULL CONSTRAINT PK_Calendar PRIMARY KEY CLUSTERED ,[yearMonthDay] int NOT NULL ,[yearMonth] int NOT NULL ,[year] smallint NOT NULL ,[month] tinyint NOT NULL ); WHILE @startdate &lt;= @enddate BEGIN INSERT INTO dbo.Calendar ( [date] ,[yearMonthDay] ,[yearMonth] ,[year] ,[month] ) SELECT @startdate ,CONVERT(char(8), @startdate, 112) ,CONVERT(char(6), @startdate, 112) ,YEAR(@startdate) ,MONTH(@startdate) SET @startdate = DATEADD(day, 1, @startdate) END
I would recommend that you - Format your code. Single line queries are not readable - Write a SQL script creating the tables you join (output of function) and sample data - give a listing of what result you want to get back based on the sample data Actually, here is your query in human readable form : SELECT timeinuserid , FIRST_NAME , LAST_NAME , TimeIn , TimeOut , Day = datename(dw,timein) , TotalHours , FULLNAME = FIRST_NAME + ' ' + LAST_NAME FROM dbo.fnGetApprovedSalaryUserTimeForDateRange('10/1/14', '10/31/14') join ACCESS_USER on ACCESS_user.USER_ID=fnGetApprovedSalaryUserTimeForDateRange.timeinuserid order by TimeIn desc, FIRST_NAME desc I will stop here, since I'm not going to give myself a headache without sample data.
&gt; This does the job, but it looks like it violates the third normal form by creating duplicate data. surprise, it doesn't
In my case I need to provide a user with data &amp; allow them to alter it. However I also need to display the fact that it is altered, who altered it &amp; allow them the chance to revert data. So I need them right there. But it's a good practice to have for other temporal data - like my inventory table. Thanks!
Perfect! Thanks!
I'm sorry, I'm not sure what you mean. I'm fairly new at this. Sample data? You mean what is returned? Mainly I have all the data I want already. What happens is there will be multiple clock ins on the same day. Now I already have a total for each clock in with the respective name of the day. I need it to have a function that sums up a total for those people who clocked in more than once a day.
It makes it easier for us to help if we know what your data looks like, and what you want it to look like. It's good that you have the data you want, but we don't know what you're working with.
"How to calculate relative dates in MSSQL" Formatting is altering the visual representation. convert(varchar(10), mydate, 111)
I just am not sure what you're wanting me to eloborate, I thought I had said that. You mean you want an example of the output from my query? I can give you a line. Timeuserid 600118 First_name Tom Last_name Jones time in 2014-10-30 16:44:00.0000 time out 2014-10-30 17:18:00.0000 Day Thursday TotalHours 0.56 Fullname This is just have to combine first and last. See some people have multiple times in during the same day, and totalhours only shows the amount of time for that clock in. What I want is to display those people with multiple clock ins with a total for the day, as well as the individual times. 
Disclaimer: German here, working for a Swedish company in Germany, things may be different in an American company, though I doubt it. Here is my view: Right know I'm sitting at the other side of the table, interviewing people and checking their statistics skills/ knowledge of SQL and DWH related topics. Here is something I always appreciate coming from potential future employees: Be honest and say "I know the basics", but also elaborate on what these basics are. Talk about how you know the difference between an inner join and a left join for example. Elobarate on what you know, where you've got your experience from and talk about potential (private) projects in which you may have used SQL before. Show your counterpart that the sentence "I know the basics" is not just a hollow phrase, but is based on actual experience. Btw: Often enough querrying skills are good enough for me, for I know that learning T-SQL and setting up procedures is something, that can be learned quickly from this point on. 
Well, I'm not super familiar with CTEs yet, so I'm just working my way through the puzzles solving as many of them as I can. Fundamentally, CTEs can be treated as a general-purpose recursion tool with just some weird implementation warts. For example, looking at SQLite's page you can modify the sudoku solver trivially [remove the where ind=0 clause from the final SELECT] to see that it's really just working as a traditional backtracking algorithm, same as you'd write using any other language.
&gt;Be honest and say "I know the basics", but also elaborate on what these basics are. Talk about how you know the difference between an inner join and a left join for example. Elobarate on what you know, where you've got your experience from and talk about potential (private) projects in which you may have used SQL before. Show your counterpart that the sentence "I know the basics" is not just a hollow phrase, but is based on actual experience. This is excellent advice, thanks so much for this. I think this is what I have been doing wrong.
Generally, it is a good idea to define an index on the intersect columns, i.e. InvoiceID in your example. I'm quite sure the above-mentioned index suggestion tool will confirm this. However, adding a date dimension and adding clustered indexes on ServiceDate and BillDate may provide higher benefits. Why not try both?
The reason people avoid DB2 (and Informix) is the same reason it's been around forever: It's IBM software. The documentation is perpetually somewhere between unusable, incomprehensible, and uninformative, and the community is so small that there's no reasonable alternative to the IBM documentation. The only system I've used that still uses DB2 is a POS system that was originally written for OS/2. 
&gt; Replace NULL with some far-away date in the DateExpired I don't see a benefit to this. NULL is by definition an *unknown* which is what I have in the case of the expiration date. Also, how would I distinguish a column that is already deprecated by new data VS one that is still live? 
Some expansion on your part would be very nice. But, just taking a stab at what you want, you could look at Order By and Limit sql statements. 
Leaving aside the philosophical/theoretical discussion ('NULL and what it is not'), you get the records that are active at any given time by doing either between or 'equal or greater than but least than' (depending on how you populate the corresponding fields). You can get the 'most recent' ones via (DateExpired = &lt;MyFarAwayDate&gt;) condition 
I just read up on the debate whether NULLs should even exist. It's interesting, but doesn't change much. From what I read, dates are the exception to the rule for some people &amp; specifically pointing to unknowns are the exceptions for others. As someone pointed out, while RDBMs do vary in many ways and there are dozens of good ones; still, all allow NULLs. So people who know more than me seem to unanimously agree on it's utility. If you can point to a functional benefit in my case, I would be grateful to hear it. But if it comes from a *"This is just how I do things"* mentality, it may not add value to the conversation.
I can confirm the community part. Never before has a bug as severe as a [minor parser bug being able to crash the whole server gotten so little attention as in this case with Informix](http://stackoverflow.com/q/25035770/521799)
Oh my dear lord. I'm an idiot. I should change my screen name to be similar to yours except I AM AN and then IDIOT. AMA... haha.. there's an idea. I would like to thank everyone for trying to help me, I had actually figured this stupid thing out throughout the progress of my day. Turns out what I wanted was accomplished through a function on SSRS. I ended up grouping time in by the day and it totaled each unique clock in for the specific day and made my life fucking easy. Thanks everyone. Sorry, I got this job about 5-6 months ago and they offered to train me on everything, I'm advancing at a good rate but there are just some thing I'm still yet familiar with so I'll come off as a dunce every now and then. 
Here's how I'd do it, but due to space I'm just showing the first 2 instead of 10: DECLARE @tbl TABLE (ID INTEGER NOT NULL IDENTITY(1,1) PRIMARY KEY, TypeID INTEGER, TypeName NVARCHAR(50)); INSERT INTO @tbl ( TypeID, TypeName ) VALUES (1,'AAAA'),(1,'BBBB'),(1,'CCCC'), (2,'DDDD'),(2,'EEEE'),(2,'FFFF'), (3,'GGGG'),(3,'HHHH'),(3,'IIII'); WITH tbl (RowID, TypeID, TypeName) AS ( SELECT ROW_NUMBER() OVER (PARTITION BY TypeID ORDER BY TypeID) AS RowID, TypeID, TypeName FROM @tbl) SELECT TypeID, TypeName FROM tbl WHERE RowID &lt;= 2 Using the Ranking features you can rank then partition by and order by whatever field you need, and using a Common Table Expression select out just those with the Row ID you want, in this case just the first two. the results of this are: TypeID TypeName 1 AAAA 1 BBBB 2 DDDD 2 EEEE 3 GGGG 3 HHHH This isn't too different from what Coldchaos posted, but I personally don't like the performance of embedded selects. 
Sorry, I should have been clearer, my question is more about querying from datasets and not tables using ANSI-92, and not about the result required by OP. In the FROM clause, rather than Tables, I have 2 queries as the dataset being queried.. How would you achieve the same in ANSI-92? *edit: FROM clause not WHERE clause* 
Oops no2. Same excuse :) I incorrectly had the table qualifier at the start of the function name, and not at the start of the column name as it should be. Corrected in SQL above. 
Read-up Ralph Kimball, he's the king of data warehousing, and he refers to this as a Slow Changing Dimensional Table. I'd suggest using the Merge command to populate such tables. Below are some examples on how to do this: http://www.made2mentor.com/2013/08/how-to-load-slowly-changing-dimensions-using-t-sql-merge/ http://www.kimballgroup.com/2008/11/design-tip-107-using-the-sql-merge-statement-for-slowly-changing-dimension-processing/ http://www.databasejournal.com/features/mssql/managing-slowly-changing-dimension-with-merge-statement-in-sql-server.html I would suggest instead of using NULL as your Date Expired us something like 2900-01-01 or something like that. This way you can select all rows where DateExpired &gt;= getdate() and still get all the current values. I also generally have an IsCurrent bit field to denote the most current data, which if you do this you can use Null. Lots of ways to spin it. 
This method does break several of Codd's rules, but it's considered a data warehousing method for an OLAP database and not something you'd generally see in an OLTP database. Most DW's have denormalized data for reporting and history.
I usually use the XML method, it's quick and simple: DECLARE @tbl TABLE (ID INTEGER NOT NULL IDENTITY(1,1) PRIMARY KEY, TypeID INTEGER, TypeName NVARCHAR(50)); INSERT INTO @tbl ( TypeID, TypeName ) VALUES (1,'AAAA'),(1,'BBBB'),(1,'CCCC'), (2,'DDDD'),(2,'EEEE'),(2,'FFFF'), (3,'GGGG'),(3,'HHHH'),(3,'IIII'); SELECT q1.TypeID, ( SELECT TypeName + ',' FROM @tbl q2 WHERE q2.TypeID = q1.TypeID ORDER BY TypeName FOR XML PATH('') ) AS tbl FROM @tbl q1 GROUP BY TypeID ; Results: TypeID Products 1 AAAA,BBBB,CCCC, 2 DDDD,EEEE,FFFF, 3 GGGG,HHHH,IIII, You'll just need to do some clean-up to remove the trailing commas, but that can be done easily enough using a LEFT function. 
I'm in a similar situation right now. My new employer is DB2 and majority of my experience has been MS. I had SQL Express installed on my workstation for ad-hoc work, but the 10GB limit on databases is sure to be an annoyance that DB2 Express-C doesn't have and the 4x cap on memory is sure to be nice as well. I've installed it on a home lab VM, but haven't gotten a chance to sit down and get familiar with IBM's Eclipse-based alternative to SSMS/BIDS.
That is actually super useful, but it's not at all about formatting.
This was no "conversation" where you have provided some value and expect the other party to reciprocate. Google (www.google.com) could help you answer your basic questions. 
Wow that Google page is amazing. Great find! Is there *any* context where your advice has a quantifiable benefit? I'm not trying to fight with you, but it's expected that if you provide unsolicited advice that someone might ask you *which problem* your solution aims to fix.
Ah our original title was actually "How to Calculate Dates in MS SQL Server" before it was changed. Thanks for the note.
Ugh... Arguing on the internet with strangers... I'm here to gather information. I keep asking you the same question because I chose to believe that you're not just rambling senselessly. If you don't want to answer, don't. But to continue to goad me into petty quibbling is just annoying. I don't know what Google is and the function of a question mark needs to be explained to me. *You got me!* Now can we move on?
You can have a look at this [SQLFiddle](http://sqlfiddle.com/#!3/7435fa/1/0) I was a little unclear what you are after. I have changed the sample data to show the worse case scenario where you don't have a row for the H1 period. Also you will probably need IIF(ISNULL(H1.[Count]),0,H1.[Count]) rather than the IsNull in the SQL Fiddle for MSAccess version.
Yeah, I know the state system I report data to is z/OS &amp; DB2. They get a lot better support than the little LUW DB2 installations mostly because IBM gets $100,000 or whatever for each installation annually so the shit better work and they better answer the damn phone. They're a totally different codebase still making them essentially totally different RDBMSs, AFAIK. They still move like IBM and state government, though. When the POODLE attack came out that system was the only system blocking us from disabling SSL3. That's right, they don't support TLS. Not 1.2, not 1.1, not 1.0.It's literally impossible to log in to that system over a secure connection currently. Literally the only platform they support state wide is IE 8. Christ, support for that ends in January 2016 when Win 7 support will require IE 11 for security updates. They need to pick up the pace. 
Thank you for the info! My date columns are date-time, would I need to create a Calendar table with time elements for this to work? Or do I use cast or convert or something to trim it down to date only while querying? 
Try postgres - I'm fairly new to it, but I keep finding things that I like, and I mainly work with MS SQL with a tiny bit of MySQL.
I just tested that and that actually doesn't give me the 0s I needed. Not sure why, not too familiar with joins yet.
&gt; Try installing DB2 on Linux... Well, I did and I it was not too difficult at all. I even did it headless which is probably the harder way to go. Sure I got some error messages, but they were very helpful. Really, I just cannot say that it was hard to install. However, I'm using Linux for 20 years now (whowjust realized that I first download Linux in 1994). If you are more used to Windows, could it be that you find it hard to use Linuxnot installing DB2 in particular? I didn't uninstall, but the files are kept in just two directoriesno big deal I guess. 
You are on the right track, forward through ssh but I think you have to have admin rights on the sql box to allow ssh deamon to forward
SQL Server doesn't have LIMIT, it's either TOP or OFFSET-FETCH
MarkusWinand, installing wasn't a huge deal, you're right, but uninstalling is where I found the nightmare. I too have about 20 years of Linux under my belt first using it in 1995 and I've used it as my primary OS since 2000. I had to work all kinds of magic to get DB2 uninstalled from my laptop when I installed it, which honestly was a rookie move on my part for not using a VM to start out with. Granted I'd assume most people wouldn't need to uninstall it on a server, but still the process was horrid to say the least. Honestly I do like DB2, don't get me wrong, but I guess my decade of using MS SQL has spoiled me. The interfaces to interact with DB2 aren't as intuitive or in my opinion anyway as mature as SSMS with MS SQL. I often have mixed results with IBM Data Studio, and many tasks going to db2cmd is about the only reliable way to do things. When DB2 works it's great, but when it doesn't work it can be a nightmare to resolve. I can't tell you how many PMR's I've opened with IBM to resolve things that I wouldn't think should be issues. 
He just copied your statement, but forgot to modify it. Syntax is wrong. Should be **ON** instead of **Where** (to let the join work and get all results) and you have to point out what you want to count. (to get proper numbers) **count(*)** will count all tuples, even if a number of *bname* for *sname* is **null**. **Count(bname)** should work fine. Besides, using **group by** clause is not the best idea. It slows down an execution of a query. You'll get the same result, however it's incomparably slower than the previous one.
Something Microsoft did years ago with SQL 2005 was Project REAL where they teamed-up with Barnes and Noble and published about a year or two of purchases with anonymized data. Then they used the various BI tools in MS SQL to analyze the data to help people learn data analysis with real data. The URL is here - http://technet.microsoft.com/en-us/library/cc966416.aspx - but MS never updated it for newer versions of SQL. If you can find this data on CodePlex or elsewhere I'd suggest grabbing it. The Solutions for SQL 2005 can easily be converted to SQL 2012 or 2014, and the data is wonderful even if you just want to use that. Also I'd suggest reading-up on Ralph Kimball, who's like the pioneer in Data Warehousing and data analysis. His techniques aren't MS SQL specific, but he has books that are related specifically to MS SQL. And if you really want to jump into a variety of data just to analyse just start pulling in data from as many sources as you can. Some sites I've used are the USPS, US Census, NOAA with weather patterns, voting stats, http://aprs.fi/weather/ is a good source for APRS weather from amateur radio stations, really anything. If you can drop all this data into MS SQL you can probably come up with all sorts of ideas on how to search and pattern the data. Then use SQL Server Reporting Services to present the data. From this you can build a VERY impressive data analysis portfolio. 
It's funny, Postgres is like the one RDBMS I've never used. I'll check it out...
See if the university has a remote access program so you can use a virtual desktop located on the campus.
We practice this in our shop. DB port is explicitly blocked on all servers.
Have you tried searching and replacing the quote with two quotes? If you use PowerShell, it might go something like this: PS C:\&gt; $drop_var=$drop_var -replace "'", "''" 
The problem is how you're building your query. Check to see if you can bind parameters instead of simply injecting their value into your SQL string. A quick alternative is to escape the apostrophe. replace ' with \' or '' (two single quotes). 
http://php.net/manual/en/function.mysql-query.php mysql_query() is deprecated in part for this very reason (vulnerable to SQL injection). You should rewrite your code to make use of something new like PDO. http://php.net/manual/en/ref.pdo-mysql.php I know this sucks, but some bot somewhere WILL eventually find your vulnerable code and exploit it, causing even more damage and work for you. Save yourself the trouble and fix it now. Relevant XKCD: http://xkcd.com/327/
I think he could also post it in /r/lolphp tbh
Here's an even better example. If the user posts a value for $drop_var of '; DROP TABLE CoName; -- then the string that you build will be SELECT DISTINCT CoName FROM SystemDemo WHERE Place=''; DROP TABLE CoName; --' AND prov='$prov_selected'" and that will drop the table that you are trying to query from. This is what's known as "SQL Injection" and is one of the most common ways that bad guys use web apps to take over your machine. You need to be using bind variables as others in the thread have said, and should probably switch from mysql_ to mysqli_ (http://php.net/manual/en/book.mysqli.php) or PDO (http://php.net/manual/en/book.pdo.php)
You don't have to declare your own table, I just did it so I'd have a data set to test against. DECLARE @my table (product nvarchar(1) ,ctage nvarchar(5) ,period nvarchar(2) ,[count] int ,spent int) INSERT INTO @my VALUES ('A','&lt;18','H1',10,300) INSERT INTO @my VALUES ('A','18-65 ','H1',20,600) INSERT INTO @my VALUES ('A','65+','H1',30,900) INSERT INTO @my VALUES ('A','&lt;18','YR ',30,900) INSERT INTO @my VALUES ('A','18-65','YR',30,900) INSERT INTO @my VALUES ('A','65+','YR',60,1800) Here is the actual query. SELECT Product, Ctage, --H1 MAX(CASE WHEN Period = 'H1' THEN [COUNT] ELSE null END) as H1_Count, MAX(CASE WHEN Period = 'H1' THEN Spent ELSE null END) as H1_Spent, --H2 need to some substracting MAX(CASE WHEN Period = 'YR' THEN [COUNT] ELSE null END) - MAX(CASE WHEN Period = 'H1' THEN [COUNT] ELSE null END) as H2_Count, MAX(CASE WHEN Period = 'YR' THEN Spent ELSE null END) - MAX(CASE WHEN Period = 'H1' THEN Spent ELSE null END) as H2_Spent, --YR MAX(CASE WHEN Period = 'YR' THEN [COUNT] ELSE null END) as YR_Count, MAX(CASE WHEN Period = 'YR' THEN Spent ELSE null END) as YR_Spent FROM @my mytable GROUP BY Product, Ctage This should give you this result http://i.imgur.com/pb6njQr.png
The above will get you hacked in a heartbeat and is a total PHP question but here is an answer. You should use [PDO and a prepared statement](http://stackoverflow.com/questions/767026/how-can-i-properly-use-a-pdo-object-for-a-select-query) 
I do believe this is the first time I've seen an unnecessary apostrophe used in the word apostrophe.
New in 4.0.3 - we support port forwarding - assuming ssh is open, try this http://www.thatjeffsmith.com/archive/2014/09/30-sql-developer-tips-in-30-days-day-17-using-ssh-tunnels/ 
A null is an unknown value. a &lt;&gt; 'zzz' will not include nulls in [a] because the value of null [a] cannot be determined. Try a&lt;&gt;'zzz' or a is null MS SQL has a coalesce function to handle nulls coalesce(a,'') &lt;&gt; 'zzz'
You need to be explicit about NULLs. Change your example to, WHERE ((Column IS NULL) OR (Column &lt;&gt;'zzz')) This will return NULLs.
Bear in mind that your WHERE clause is, literally, checking to see whether a value is greater than or less than 'zzz', and a NULL value cannot be greater than or less than anything. It is merely null!
&gt; MS SQL has a coalesce function to handle nulls actually, mssql has the isnull function for that. isnull perserves the datatype, you get as a result the datatype of the first argument. Coalesce will return a "everything fits" datatype. Small difference, but can be a ripping ones hair out difference
Null can never be equal or unequal to anything. Something can be null but null cannot be greater than or equal to 'zzz'. Null has no value, it isn't the minimum value or the maximum, it is null. You can do Where ISNULL(column1,'') &lt;&gt; 'zzz' If the column1 is null its assigned a ascii null and then can be evaluted against 'zzz'. 
I've never had an issue with coalesce in the where clause. Where would something like this crop up?
I've seen it being and issue on joins, on collation conflicts, on inserts... pretty much anything where implicit conversions can bite you. In a where clause, you could very well run into issues as well. Honestly, its just one of those little things that can drive you mad, and its easy to avoid. To me its the same as always specifying the code page on string to date conversions, and avoiding implicit conversions, of any type, like the plague
Theres a few differences in how COALESCE works compared to ISNULL, see http://msdn.microsoft.com/en-us/library/ms190349.aspx COALESCE is basically optimised down to a CASE statement, and so whilst it supports multiple arguments, there can be a performance penalty for using it compared to ISNULL In short if you're just wanting to evaluate a potentially NULL variable with a default fallback, its usually better to just use ISNULL 
You mean you don't use sql_variant for every column? /sarcasm (I believe sql_variant still defines a data type for joins and comparisons) This can also be an issue when inserting into a temp/declared table or in use of a cte if you coalesce early on and need to join using that value.
 sql_variant... pfff, i moved to make an identity primary key column, a XML column and a varbinary(max). Throw it at the frontend idiots and sleep in peace 
In [MS SQL], you can use left, right, and charindex to extract the image. Here's how I did it with your example &lt;img&gt;: http://sqlfiddle.com/#!6/629f0/12 
This looks great to me. For what its worth (not very much I suspect :) ) I fall squarely in the ,use NULL in this case, camp. Using null for the DateExpired for the current record has all sorts of advantages for querying too as many RDBMs are able to locate Null records more efficiently and it very effectively distinguishes between current and historical records without having to know some magical future date to query by. Just a little thing to be aware of when finding historical records you won't be able to use "X between DateAdded and DateExpired' you will need to use 'X &gt;=DateAdded and X &lt; DateExpired'. Otherwise if you want to look up a record , (using your example data ) of '2014-10-31 10:00:00' you will return 2 records. I now its minor, but its the kind of thing that you can miss that causes you heartache down the road. 
Wow thanks for this! Currently the only issue I'm having is that the remote shell doesn't have listener on or something along these lines. TNS-12542: TNS:address already in use TNS-12560: TNS:protocol adapter error TNS-00512: Address already in use Linux Error: 98: Address already in use Since I don't have admin rights for the shell in question I'm contacting my teacher if he could assist me with this. He did previously mention that there's no way to remotely connect SQL Developer to our database and I'm eager to prove him wrong heh. 
I think granting them [VIEW ANY DATABASE](http://msdn.microsoft.com/en-us/library/ms186717%28v=sql.110%29.aspx) on the server level would work. CONNECT ANY DATABASE does not exist until 2014 so you may not be able to do that. You could however have some job that would loop through all databases and grant them connect. Have that run every day or whatever interval.
That depends on exactly what you define as 'new', either of the following might do what you want: -- Shows all databases that have not been backed up yet SELECT ssdb.Name AS DatabaseName, MAX(mbus.backup_finish_date) AS LastBackUpTime FROM sys.sysdatabases ssdb LEFT OUTER JOIN msdb.dbo.backupset mbus ON mbus.database_name = ssdb.name GROUP BY ssdb.Name HAVING MAX(mbus.backup_finish_date) IS NULL -- Created today or uncomment for the last 24 hours SELECT * FROM sys.databases AS ssdb WHERE -- Created Today YEAR(ssdb.create_date) = YEAR(GETDATE()) AND MONTH(ssdb.create_date) = MONTH(GETDATE()) AND DAY(ssdb.create_date) = DAY(GETDATE()) -- Created in the last 24 hours -- OR ssdb.create_date &gt; DATEADD(hh, -24, GETDATE()) ORDER BY create_date Use the results to figure out who needs permissions and just assist them from the above results.
I see what you mean, but I guess I didn't explain myself well. I could go on the server daily, or perhaps run a job that gives people access to new databases. However, I am wondering if a User can be granted automagical access to any new DB created via a security setting - so there is no need to run a job or human intervention. The app can be tweaked and I can have them all connect as the same sql user and that would solve my problem, but I was hoping to manage their access differently.
I have updated the OP.
STUFF works amazingly well for cleaning up XML: SELECT q1.TypeID, (STUFF((SELECT ',' + TypeName FROM @tbl q2 WHERE q2.TypeID = q1.TypeID ORDER BY TypeName FOR XML PATH(''), TYPE).value('(./text())[1]', 'varchar(max)') , 1, 1, '') ) AS tbl FROM @tbl q1 GROUP BY TypeID; 
Alternately maybe create a trigger on master..sysdatabases or whatever the list of databases is called in your version of MSSQL. You'd want to check, if the creating user was a member of the AD group, grant the AD group access to the database...
I usually prefer it in a job as it is much easier to audit or modify, but if you would automatically like it to occur you could use a [CREATE TRIGGER](http://technet.microsoft.com/en-us/library/ms186406%28v=sql.105%29.aspx) to assign the permissions.
You would want to do something like my previous, except your data source would be a subquery that looks similar to this: SELECT reason, car, cost, count(pk) FROM your_table GROUP BY reason, car, cost You'd also change the partition to be on *reason, car* and the order to be by *cost*. 
Try: select sum(case when the_column like '%&gt;%' then 1 else 0 end) with_sign, sum(case when the_column not like '%&gt;%' then 1 else 0 end) no_sign from abc 
Off the top of my head, give permissions to the group in the model database?
Are you missing the ELSE part of your CASE? If the COALESCE() function returns null, you set discountfinal to 'C'. But, what do you want to set discountfinal to if COALESCE() returns a non-null?
 put the SELECT which creates the aliases into a subquery, then you can reference the alias names in the outer query... SELECT discount1 , discount2 , discount3 , CASE WHEN COALESCE(discount1, discount2, discount3) IS NULL THEN 'C' END AS discountfinal FROM ( SELECT (CASE WHEN yadda yadda) AS discount1 , (CASE WHEN yadda yadda) AS discount2 , (CASE WHEN yadda yadda) AS discount3 FROM ... ) AS s
I'm not familiar with Vertica, but you can try either :- SELECT TOP 500 ... ... ORDER BY SystemModstamp OR :- SELECT ... ... ORDER BY SystemModstamp LIMIT 0,500
im just worried about so many sub queries in a query hurting performance. let me cut out my entire query here and see what you think. CREATE TABLE #table (discount1 DECIMAL(18,2), discount2 DECIMAL(18,2), discount3 DECIMAL(18,2), discountfinal DECIMAL(18,2)) INSERT INTO table (discount1, discount2, discount3, discountfinal) SELECT discount1, discount2, discount3, discount4, discountfinal FROM ( SELECT discount1 = (SELECT blah), discount2 = (SELECT blah, discount3 = (SELECT blah), discountfinal = CASE WHEN CONVERT(NVARCHAR(50),COALESCE(discount1, discount2, Discount3)) IS NULL THEN 'C' END FROM blah WHERE blah ) AS a
I'd suggest using a Common Table Expression. Here's a quick example. Run with just the first Insert and you'll get 'C', run with just the second Insert and you'll get a Null. DECLARE @tbl TABLE (ID INTEGER, Disc1 INTEGER, Disc2 INTEGER, Disc3 INTEGER); -- returns 'C' for discountfinal INSERT INTO @tbl (Disc1,Disc2,Disc3) VALUES (2,2,3); -- returns null for discountfinal --INSERT INTO @tbl (Disc1,Disc2,Disc3) VALUES (1,2,3); WITH tbl (Discount1,Discount2,Discount3) AS ( SELECT CASE WHEN Disc1=1 THEN 1 ELSE null END AS Discount1, CASE WHEN Disc2=1 THEN 1 ELSE null END AS Discount2, CASE WHEN Disc3=1 THEN 1 ELSE null END AS Discount3 FROM @tbl ) SELECT Discount1,Discount2,Discount3, CASE WHEN COALESCE(discount1, discount2, discount3) IS NULL THEN 'C' END AS discountfinal FROM tbl; This will get it down to one query without having to mess with sub-selects (which are horrid on performance) or creating temp tables. 
Chahk, good post... I need to read-up more on the STUFF function. It's one of those lesser known functions I've not fully wrapped my head around. 
subqueries such as the one i wrote do not hurt performance your use of discount1/2/3 in the calculation of discountfinal will not work
...somehow I missed the OVER function. This looks like it should solve my problems. I'll be able to check tomorrow. I'm only just getting my feet wet in oracle, I've only used Postgre and MySQL in the past. Most of the other pointers you offered won't work for me though, since the actual query is about 50 columns made up of 13 tables with only a single column guaranteed to be distinct. Still, OVER seems to be what I was looking for, so that's fine. Thanks a lot for you help, probably saved me a lot of time =)
Thanks a lot. I was pretty close with my queries, but I didn't think to try SUM, I was using COUNT the whole time. thanks again, much appreciated.
The reason this type of subquery won't hurt performance is that there are no indexes being used. There is one extra temporary table that will be created with the fourth column, but since it's in memory it should be very quick. In any case, get it working and then play with the query optimizer to make sure I'm not lying. 
There already are answers to this problem. I figured I'd give you a reason *why* it's happening. Queries are written out in the form: SELECT FROM WHERE GROUP BY HAVING ORDER BY The way the SQL engine processes it is: FROM WHERE GROUP BY HAVING SELECT ORDER BY Each layer can only see the ones above itself, hence why you can't have composite value in the SELECT. 
Piggybacking off u/coldchaos' answer (obviously tables and columns are made up but it should give the the general gist.) The row_number() function does the work here. --first add a rank field for each model (RankForModelByCost field) WITH Repair_Rank ( [RankForModelByCost ] ,[Model] ,[Cost] ,[Reason] ,[DateDone] ) as ( SELECT row_number() OVER (PARTITION BY Model ORDER BY Cost desc) as [RankForModelByCost ] ,[Model] ,[Cost] ,[Reason] ,[DateDone] FROM Repair ) -- Then you select the top 10 rows for each model SELECT [RankForModelByCost ] ,[Model] ,[Cost] ,[Reason] ,[DateDone] FROM Repair_Rank WHERE [RankForModelByCost ] &lt;= 10 
If the application uses the model system db, just add the AD group to model and configure the perms appropriately. If the app creates new objects based on an empty template, you'll have to do something trickier if you need to assign specific object-oriented perms.
Check this out: http://www.dba-oracle.com/sf_oui_10151_during_oracle_install_tips_bc1.htm 
You could either convert the datetime to date, or add a computed column to the original table that does the same. The computer column would make it easier for future queries. [edit spelling error]
The image's name is extracted, from there you can just tack on the extension. I can't recall, but I thought the path is stored in the database, so you can replace or string manipulate to get the image name as shown. I'm confused if that's not what you were looking for.
Is this mySQL (phpMyAdmin) on cpanel? If so, you may have a easier time exporting the database as sql, opening the file in a text editor (notepad++, etc) and replacing /path/ with the /new path/. Repeat for other image paths, then import the SQL. That won't strip out the extra img tag junk though. My previously mentioned solution can extended to grab the content to the left and right of the image tags and appended to the new image tag. Good luck!
Sometimes null isn't really null, it's a blank text field. Try adding: Variable = '' to your string of OR statements for each variable your testing for null.
Hmmm.... that is a good idea! There might be a simple way I can do that using multiple cursors if that random number at the end of the "SQUARESPACECACHE" all have the same amount of digits. Heck, even if they don't, it might be fairly easy to do that way. Thanks so much for mentioning this! 
Not sure exactly what you mean but you can probably use a window function select cust_id ,sum(total_cost) over (partition by cust_id) as cust_total from orders Window functions are powerful and extremely useful, the day you start using them you'll wish you'd started long ago.
On a side note: you can do your last COALESCE like this: COALESCE(discount1, discount2, discount3, 'C') Makes it more readable imo. 
I am new at studying SQL. What does the COALESCE () function do?
You may want to do ... case when [Doctor_Status] IN (N'Open', N'One Time Only') For debugging, you should return the actual rows as well until you get the logic right.
try this: CASE WHEN [Doctor_Status] IN ('Open', 'One Time Only') AND COALESCE (Date_Assigned, Date_Contact, Attempt_To_Schedule) THEN ...
Is that going to help you..? http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html
So you want to have the hyperlink's parameters for use in the WHERE clause of a query being performed on a new page? The comma delimited should work and then you can use them as: query = "select columns " &amp;_ " from table " &amp;_ " where table_id in (&lt;%request("link")%&gt;)" 
You can install 64 bit SQL Developer if you want, doesn't matter what bit level of the DB it is. Is your requirement to INSTALL the database, or to HAVE a database? If it's to install, just install XE - it's avail in 11gR2 and a very easy setup. If it's to HAVE a database, just get our appliance from OTN. It already has SQL Developer on it as well, and ready to go. Also no need to run a Windows VM on your Mac. It all runs on a Linux VM inside of VirtualBox. You can get the VM here http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html And I talk about how to set it up here http://www.thatjeffsmith.com/archive/2014/02/introducing-the-otn-developer-day-database-12c-virtualbox-image/ 
Don't prove your teacher wrong, just let him know we changed this feature only in September - they will appreciate that help :) 
Already did a complete hardware/software check and already checked every tip he had, sorry.
This is awesome, I will give this a shot this weekend! The purpose is to have a DB, as I need to import a .DMP file which has a DB so we can practice :) can't practice without a DB, right?
Build an SSIS package or some kind of powershell script to do this. I'd favor the former myself.
Then you'll be golden. Takes a bit to download and extract/import - and 5 minutes to startup and be running...You'll need to either put the DMP file on the vm or install an oracle client so you can run the IMP command against the database. 
Thank you, this gives me a starting point. edit: In the end, couldn't do SSIS because of lack of BIDS/SSIS/SSRS on the server and was told I can't install anything. Ended up writing a batch file that calls SQLCMD to do what I needed.
Also, welcome to the club of [Accidental DBAs](https://www.sqlskills.com/help/accidental-dba/) :)
Another option which may be simpler than SSIS is setting-up a Report Services subscription. Build a report using Report Builder or even the web-based builder that comes with SSRS, deploy it, and then add a subscription to export the report to a file on a UNC at a prescheduled time. You can also have it email the report in whatever format you'd like if email is a better option. Here's a video I found showing how to set one up - https://www.youtube.com/watch?v=sO0Y_IxdE8w SSIS will definitely do this as everyone else is suggesting, but if you just need a quick process to export the data Reporting Services can be setup to do this in a fraction of the time it takes to build an SSIS process.
I generally try not to work with nulls if at all possible, so for all values that could be null I'd use isnull(@var,'') then write logic to treat null and empty strings the same. Anything compared to a null will always be false, but this isn't so with an empty string. 
Thank god - when does our support group meet? Can I bring beer?
Thanks for this. I couldn't get it to exactly work but I am a step closer. I think there is a bigger problem in how I generate the asp page that I need to solve first.
If you don't need to make changes to any of the script then you can use sqlcmd. Create a batch file with a line for each script that gets executed. Construct the command as follows: Sqlcmd -S &amp;lt;server&amp;gt; -E -i &amp;lt;input file&amp;gt; -o &amp;lt;output file&amp;gt; Eg Sqlcmd -S serv1 -E - i script1.SQL -o script1results.txt It edited my 1st example, so -S is server, -i is input file and -o is outputfile
You could check out [SQL Saturday](https://www.sqlsaturday.com/), might have to drink in front of your computer though. 
I really wish this thing would show me all of the errors at once instead of one at a time like this. Now it's saying 'Data type mismatch in criteria expression'. I've tried finding where this error is happening so I wouldn't bother you so much, but I'm not having any luck. Would it help if I uploaded the database to Google Drive and gave a link?
Trying this while stuff installs on the VM. Thanks! update: Management said don't install anything or update anything on the server, so I went w/ this solution. It took some time to get around the limitations of SQLCMD (like not being able to strip out just the "----" between column header row and data rows), etc. But after an hour of fucking around it works great. It's clunky as hell but it gets the job done quickly and with a single keystroke versus spending fifteen minutes sorting through .sql and .txt files every morning.
you can do a distinct list using RANK(), then just use the rownumber instead of top 3.
How I would do it: ------------------------------------------------------- SELECT COUNT(t.id) FROM tickets t LEFT JOIN replies r ON t.id = r.ticket_id AND r.created_at = ( SELECT MAX(r2.created_at) FROM replies r2 WHERE r2.ticket_id = r.ticket_id ) WHERE t.status = 'Open' AND ( r.last_reply_by IS NULL OR r.last_reply_by != 'Administrator' ) ----------------------------------------- It's similar, but I've removed one layer of subquery so it should be faster, and I've aliased everything (with different aliases) for readability. That said I'm from a SQL Server background, the theory should be similar but the execution might be slightly off.
* don't store the result of a math formula * be careful when using functions in a WHERE clause * watch for [parameter sniffing](http://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/) * use OPENQUERY on a linked server not select from the linked server
"Don't use column's alias name in WHERE clauses since WHERE is processed before the SELECT clause" I don't know a DBMS where you even can do that.... Besides, that argument is made for avoiding HAVING clauses, even if you could reference a column by alias in the where clause, the DBMS (hopefully) should resolve it and filter as usual
You might want to consider just returning the entire set (if its not too big, or the top 34 in this case) and adding a filter to your set or table with Javascript. 
I generally avoid cursors without a good reason to use one. That being said you can try somthing like isnull(part_id, '') = isnull(@v_part_id,'') Keep in mind this likely won't perform very well. If this code were given to me, I'd be tempted to rewrite it entirely. 
you actually think we are going to sift trough that wall off text ? Format your code ffs and cursor in a trigger, holy crap 
I was trying to polite. You made me laugh :) 
why is "fetch" and "cursors" so bad? If I may ask.
A cursor in MSSQL (bit different in Oracle afaik) will execute row by row. If you do that, it is going to be slow as hell, since all the logic is going to run for every single agonizing row. SQL is ment to be written in a set oriantated way, write the logic, and let the query optimizer execute it in a way so the data gets filtered down as best as it can, and only once run trough all the logic. Putting a cursor in a trigger will make sure it will run a lot, so its bad^2
* Never use "Select *", always specify all columns, especially in Views. * Use Table Aliasing whenever possible * When joining multiple tables always reference the table or table alias with the column (Select A.ID from MyTable A) * Don't use SubSelects within a Select if Joins or CTE's will give the same output * Don't create an Index just because, do so because you need it ... and delete indexes not used * Don't use Distinct just to remove mystery duplicates ... fix your query instead * Use comments in all Stored Procedures, Functions, Views, etc I could go on, but these are a few just off the top of my head :) 
If you're looking for quick and dirty, you can also write a console application in C# in maybe 20-30 lines that would do this too, then this could be scheduled to run from Windows Scheduler. Just another option. You could even use the free compilers from MS and write it in Notepad if you don't have Visual Studio. Not the way I'd do it if SSIS or SSRS are options, but if you have neither this might be the next best thing.
&gt; isnull(part_id, '') = isnull(@v_part_id,'') I just tried this and it wouldn't compile.
So just to make sure I understand what you are looking for: you are trying to count any tickets where the status is "Open" and the last reply is not from an Administrator. So that would include tickets with no replies? Is that correct? If so the below should be fairly fast (compared to the joins method) as well as not too bad at describing the problem. It basically says: "count the tickets that are open _ that don't have a reply from an administrator _ _ that hasn't been replied to by a user". Its worth noting that the r.created_at = MAX(r2.created_at) method of finding the most recent reply may find multiple replies if the timestamp is the same which may inflate your results. (sounds niche but when I ran it on a dataset I had handy it made quite a difference). Your value may vary. SELECT Count(t.id) FROM tickets t WHERE t.status = 'Open' AND NOT EXISTS ( SELECT 1 FROM replies a WHERE t.ticket_id = a.ticket_id AND a.username = 'Administrator' AND NOT EXISTS ( SELECT 1 FROM replies u WHERE u.ticket_id = a.ticket_id AND a.id != u.id AND u.created_at &gt; a.created_at AND u.username != 'Administrator' ) ) As an aside if you can rely on the ids of the replies being in time order then you could even just replace the "a.id != u.id AND u.created_at &gt; a.created_at" with "a.id &gt; u.id". 
big companies use Oracle. You'll find sql server in more small to medium sized companies. 
You did this in the WHERE clauses? I guess I have should have told you that is what I meant. 
I've heard good Oracle DBAs make a lot of money. What I would do is go through the school's program, but maybe set up MSSQL Express on your own (or any other free DB engines you can find) to get some exposure to other platforms. 
Not really a best practice but something that snagged me today... NOT LIKE and not NOT LIKE for NULL values.
*Would it help if I uploaded* the database to Google Drive and gave a link?* Yes However, googling the issue also mentions incorrect data format in the database. So check all your values are correct in the data for the data type that is set.
Why are you wrapping the field name in quotes? And is it giving you an error when you run it?
I'm sorry, what? 
Oracle is primarily used by government and financial institutions and other massive organizations like that. It's also (possibly needlessly) more complicated than other RDBMS and, frankly, has a generally antisocial and unhelpful community. I started as an Oracle DBA and transitioned away from it for those reasons. Even with those down sides, Oracle is still very significant. All that being said, anything you're learning at your level will almost directly translate to other RDBMS. It will be very easy to move from one system to another. 
[Okay, here's a link](https://drive.google.com/file/d/0B_XuLzvJIltdYXBPMmI5cU1TdHc/view?usp=sharing). I went through the data trying to find anything that shouldn't be there, but didn't find anything. All of the dates are in the appropriate places, none of the platform titles are misspelled, etc.
Damn, that's pretty rough. i was pretty sure you were going to end that sentence with "and quite frankly, Oracle programmers are generally assholes." 
Agreed. Three letters that are a major player in the RDBMS world; ERP.
I originally commented thinking, for some reason, that your column names were parameters. I'm an idiot. You shouldn't have to use quotes around 'oppo_biddate'. Are you supposed to be comparing oppo_biddate to today's date? Right now, you're comparing a column to itself plus three. "Where this column is 3 days later than itself" won't return any rows. You might want to do something like WHERE ( SELECT SYSDATETIME() ) &gt; DATEADD(DAY, 3, oppo_biddate) If you're looking for bids past due.
First of all, sounds like you're in an awesome program :) Where are you looking for jobs? Oracle DBAs are in high demand. There aren't very many unemployed Oracle professionals out there. And, they generally make more money that folks on the SQL side. But, what you learn today about relational databases will help you when it comes to working with databases other than Oracle.
http://weblogs.sqlteam.com/markc/archive/2009/06/08/60929.aspx
Sargability. Edit: Since I'm being down voted for a very valid point, I will elaborate. Make your statements [Sargable](http://en.wikipedia.org/wiki/Sargable), otherwise you are doing it wrong.
Yep! Threw my cubemate and I some trouble when reporting off of someone else's database.
For those that aren't aware, /u/thejeffsmith works for the big O. But don't hold that against him, he's a terrific guy really... 
You can use a combination of substring and charindex/patindex functions. There's also a way to do it using the XML type I've seen done. And lastly you can create a custom function to do it for you. To each his own.
Good on you! Still the SSIS packages do not have to be on or ran from the db server. As long as it can open a sql connection you can build it and run it on your own machine using OLE DB connection managers. Research and play around with it a bit when you've taken a breather from fighting fires.
It's cool, I'm actually changing careers from culinary arts to IT, so I miss the trash talking. 
My only advice is to not focus on any one particular RDBMS. They all function relatively similarly and it is more important to dig deep into HOW the relational model works and understanding how different queries work. That knowledge is transferable to any job on any RDBMS. As for the "administration" side of the DB's you will find more differences but there I would suggest focusing on learning SAN, NAS, RAID, file systems (basically storage technologies) as again that knowledge will translate across. The fact is there are lots of great jobs for all of the DB's that are out there but to sustain a career (from job to job) you need fundamentals more than you need knowledge of any one tool (especially proprietary tools).
Yikes.
You need to exclude the Null values in the [Price Bought] and [Date Bought] columns in the two FROM queries. You need to add a Platform clause for tables v1 , v2. some other niggly stuff that I cannot remember after the fact. It is just a matter of playing around with the formats and pulling out the queries seperately and testing them with similar clauses until you find the syntax that work. Experience then usually kicks in after a few times and you do not reproduce the errors in your normal works. SQL above updated.
Hi sorry, I should have made it clearer what I was trying to achieve. tickets id|username|subject|content|priority|status|created_at :--|:--|:--|:--|:--|:--|:-- 1|Jane Smith|Test subject|Test content|Normal|Open|2014-11-06 10:23:59 2|John Doe|Test subject|Test content|High|Open|2014-11-06 12:23:11 replies post_id|ticket_id|username|content|created_at :--|:--|:--|:--|:-- 1|1|Jane Smith|Test reply|2014-11-06 15:09:10 2|2|John Doe|Test reply|2014-11-06 17:58:06 3|1|Administrator|Test reply|2014-11-06 18:55:12 So ultimately, what I would like to achieve is to group the replies by their ticket_id in replies table, get the last reply (MAX(created_at)), and join it to the tickets table with the id. Would also like to count where the status is 'open' and last_reply column is not 'Administrator'.
Large data environments need to consume data you store and provide. Use views and stored procedures to provide the information, rather than tables, when possible. This way you can adjust the design of a table or even an entire data model without disrupting your users (or having to do a bunch of training). All you have to do is edit your view or procedure to make sure it has the same output. Avoid using SQL queries in your front end or middleware. SQL injection queries are risky. Try to force a policy where the app execs a stored procedure and receives data. Semi colons. Semi colons everywhere. This also helps protect against SQL injections. Build a logging table to document how long jobs/processes run. You WILL be asked to provide this information and anytime you can run a simple query rather than run some crazy report against a transaction log or something, its going to be less of a hassle for you (and you look smarter to boot)
I think is it; SELECT * FROM event_log WHERE station_nr in ('50', '53') AND event_time BETWEEN TO_TIMESTAMP('03/Nov/2014 10:00:00', 'DD/MM/YYYY HH:MI:SS') and TO_TIMESTAMP('03/Nov/2014 14:00:00', 'DD/MM/YYYY HH:MI:SS') The date fuctions wrap the values you seek to use as parameters, not the value you are searching for, assuming the event_time is the same format... 
I'd suggest reading [this article](http://sqlperformance.com/2012/07/t-sql-queries/split-strings) on the topic if you need to to be very efficient. Otherwise you can use a recursive function that returns a table to dynamically build it. 
For quick ad-hoc queries yes it may be acceptable, but anywhere else it's bad. I'll create a new post on why this is bad in Views since it's something I think most people don't realize. 
Yes, it works now. Thank you so much, and I"m sorry to have bugged you so much. I need to do a better job of teaching myself this stuff.
And now I have more work to do today. edit- they are all in subqueries on actual tables because of a shitty data model, but I'll do the needful and update the code.
One place I think it's still *sometimes* acceptable is if you're directly querying a CTE or derived table that you've just defined. Something like : WITH Foobar AS ( SELECT b1.ColumnA, b1.ColumnB, b1.ColumnC FROM bazz AS B1 ) SELECT F1.* FROM Foobar AS F1 Obviously in the above case the CTE is pointless but you get the idea. Basically you're not querying the schema directly but your own selection. Even then it's still usually best to repeat the columns, but sometimes I find that actually reduces clarity (depending on how many columns you're talking about). 
Wow, that's crazy. Does MS consider this a bug? I couldn't replicate this in Oracle 11g. Looks like it automatically replaces the * with all fields that are on the table at the time of view creation: create view TEST_VIEW as ( select * from TEST_TABLE); View created. select TEXT from USER_VIEWS where VIEW_NAME = 'TEST_VIEW'; TEXT --------------------------------------------------------------- ( select "COLUMN_1","COLUMN_2","COLUMN_3" from TEST_TABLE) 
I still wouldn't use Select * in a CTE, but yeah it shouldn't be as huge of a deal given you're building the dataset on the fly. 
I've never heard of this being considered a bug by Microsoft, it's just how Views work in MS SQL. I first encountered this in SQL 2000, and I've ran this test in each version up through SQL 2012 SP1 where it's still an issue. When I first ran across it and spent HOURS trying to find out why my Views weren't returning what they should I found some technical notes buried on Technet (which are no longer there) listing how Views worked and why this issue occurs. That was probably 8-10 years ago, but it's still an issue I think many to most MS SQL developers and admins don't know is there. 
This seems like it really should be a bug. I cannot fathom how when the final select declares a column name, it would pull data from a different column. Unable to replicate in PostreSQL and Pivotal Greenplum.
Do you actually need `grading_has_grouptests` or could `group_has_tests` directly reference `grading`? The only reason I could see for not doing that is that was if `group_has_tests` could be associated with multiple `grading`s.
I agree, Select * is bad just from a performance standpoint, not only does the data engine have to work that much harder to pull the list of columns but you also return often way more columns then needed. Example, if a table has 20 columns but you just need two, using Select * would pull back way more data over the wire then is needed. This not only creates latency on the server but also on the client and network. I had one system where the developers wrote all the SQL and used Select * on all the Views. Some of these views were joining 5-10 tables with 10-30 columns each. Needless to say slimming down the Views to just the columns they needed increased performance by a large factor.
I'm not sure I follow what you're asking. Can you post the DDL for the tables or how the tables and primary/foreign keys are setup?
I haven't read through all the posts, so hopefully I'm not being redundant. I'm surprised though that you're not seeing any Oracle jobs, it's one if not the most popular RDBMS out there. According to DZone at least in 2013 it was top with MySQL and MS SQL competing for second: http://java.dzone.com/articles/10-most-popular-db-engines-sql I think it's so easy to spin-up a MS SQL Server and so many applications support MS SQL that you'll often see all sizes of shops using MS SQL which may be why you see more jobs in this area. Also with MS SQL you get the full meal deal with Reporting, Analysis Services, Integration Services (ETL) and more. Oracle and DB2 as well are both more expensive and have fewer bells and whistles beyond the data engine so they're more popular in larger shops like government, insurance, banks, etc. Also from what I've seen it's hard to get into these types of shops without LOTS of experience, so budding DBA's might have more luck with MS SQL or MySQL to start out with. With all this I wouldn't say you're on a bad path with Oracle, but I would suggest you start picking-up some MS SQL experience if you can. 
I posted my Schema here: http://www.reddit.com/r/SQL/comments/2ll6gv/is_there_a_special_name_for_this_type_of/clvwuyg
`grading_group_test` is really a regular old many to many link table. It looks funny because one of the keys is a compound key and as it happens, each part of that compound key is a primary key to a different table. From the perspective of the `grading_group_test` - `group_test` relationship, it doesn't really matter what `group_id` and `test_id` are. What matters is that together they point to a distinct row in `group_test`. If you had an single PK column on `group_test`, `grading_group_test` would link to that. So in answer to your question, no I don't think there's any kind of special name for that. I've always called those link tables but I'm sure there's some textbook name I should probably be using instead.
Great, thank you. I was actually a bit worried if maybe I did something bad in the design phase, but you've seen this before?
Maybe but I don't specifically recall. I just analyzed your schema and broke it down to what the relationships mean. Your description of the business rules driving all of this made it a lot easier to understand. I might have been inclined to put an identity column on `group_test` if the most frequent use case was joining to that table. If on the other hand you mainly just need to know what the `test_id` or `group_id` associated with the `grading_group_test` record was (i.e. you usually want to join straight to those tables and don't need the extra data in `group_test`), the way you have it now is more desirable. Some people really dislike compound keys but I prefer them if they're the most meaningful expression of the business rules. I don't see anything wrong with your schema and it seems to describe your business need well.
http://en.wikipedia.org/wiki/ACID 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**ACID**](https://en.wikipedia.org/wiki/ACID): [](#sfw) --- &gt; &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __ACID__ (*[Atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems\)), [Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems\)), [Isolation](https://en.wikipedia.org/wiki/Isolation_(database_systems\)), [Durability](https://en.wikipedia.org/wiki/Durability_(database_systems\))*) is a set of properties that guarantee that [database transactions](https://en.wikipedia.org/wiki/Database_transaction) are processed reliably. In the context of [databases](https://en.wikipedia.org/wiki/Database), a single logical operation on the data is called a transaction. For example, a transfer of funds from one bank account to another, even involving multiple changes such as debiting one account and crediting another, is a single transaction. &gt;[Jim Gray](https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist\)) defined these properties of a reliable transaction system in the late 1970s and developed technologies to achieve them automatically. &gt;In 1983, Andreas Reuter and Theo Hrder coined the acronym *ACID* to describe them. &gt; --- ^Interesting: [^Acid](https://en.wikipedia.org/wiki/Acid) ^| [^ACiD ^Productions](https://en.wikipedia.org/wiki/ACiD_Productions) ^| [^Lysergic ^acid ^diethylamide](https://en.wikipedia.org/wiki/Lysergic_acid_diethylamide) ^| [^Acid ^house](https://en.wikipedia.org/wiki/Acid_house) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+clw44zt) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+clw44zt)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It's not a _bug_. It's just poorly designed! 
Someone thought, "why would I take up precious space duplicating column names, when I can just use a numbered index? I mean... databases are basically just Excel, right?" 
I agree with using views. [samalex01](http://www.reddit.com/user/samalex01) posted a great post about using select * for views. http://www.reddit.com/r/SQL/comments/2ll2jt/why_select_in_views_should_never_be_used_ms_sql/ also... never trust the data given to you. sanitize all data and fail to a known state.
I think the 'ternary relationship" is the term you are looking for. This kinds of relationships are generally useful and being used if individual binary relationships are meaningless. Judging by the description you gave in another reply, the way you modeled your environment is absolutely fine. 
Thanks!
&gt; Why SELECT * should never be used in ANY PRODUCTION query FTFY
Wonderful post, I learned something tonight. Best of all, the explanation makes sense!
Thanks :)
This isn't an SQL issue. Try an Outlook-related subreddit.
You need to run an `ALTER SEQUENCE` command. Something like `ALTER SEQUENCE my_table_name_id_seq RESTART WITH 1`. See http://www.postgresql.org/docs/9.3/static/sql-altersequence.html
Other than what you've mentioned, lack of index usage would be a damn good reason not to use select * also. You would have to goto the CLUSTERED INDEX, in some way, every time when you use select *, rather it be for index usage or key lookup. 
I think you are actually agreeing with me and we are just missing each others points. Doing a SELECT * when you are just formulating your thoughts or trying things out is fine. But you should NEVER commit a SELECT * to anything that may, some day, end up in production. It's the OO version of breaking encapsulation. So, I was correcting OP in that his comment seemed to be limited to only views and I was saying it should encompass ANY query that may end up in production (including your inclusion of staging, qa, or test). Basically use SELECT * when you are "what iffing" locally but ALWAYS translate it into a list of needed columns when pushing your query forward toward prod.
To extend a bit ACID. SQL Server will take locks on the data when reading or writing. Locks can be on the row level, the page level, or the entire table. Reading will cause a shared lock, multiple transactions can read a row / page / table simultaniously. Writing to that data is blocked however. Writing data will cause exclusive locks, while a transaction is holding an exclusive lock, nobody else can read or write to the rows / pages / tables being locked. That is the basics of how data integrity is done having multiple connections / users accessing the database at the same time.
Excel does have a shared workbook mode where multiple users can edit a workbook at the same time. However, the workbook is updated only when users save the file.
It only happens when [schemabinding](http://msdn.microsoft.com/en-GB/library/ms187956.aspx) is disabled.
Yes, you'd use something like this: Case When C = B then 'duplicate value' Else '' End as D I'd suggest adding something in the Else, but excluding the Else would give you a Null which I always try to avoid. 
You can do as /u/samalex01 suggests to analyze the current row. However if you need to find out if Column C is anywhere in Column B in the whole table you would want to do something like this. SELECT CASE WHEN [Column C] IN (SELECT [Column B] FROM Table1) THEN 'duplicate value' ELSE NULL END AS [Column D] FROM Table1; Without knowing more of your criteria this is the way that will work for most situations. If I knew more I might suggest trying to get that logic into a join instead of a subquery.
ELI5: It's like Google Sheets but on a bigger scale.
This is exactly it! Thanks so much!
I personally think it's far easier to learn in the database that you have to use. If it's possible for you to access a development or test region of the same database at work, that's your best bet
Stupid question: Aren't DB all pretty similar fundamentally? 
In terms of what they CAN do, sure. In terms of how they've been designed, what sort of relationships exist between tables, how jobs are scheduled, etc, there's can be a huge difference from one to the next.
You aren't grouping by anything so the Avg(inv_amount) is failing.
I'm not too familiar with Oracle, but I would think this needs to include the GROUP BY: CREATE VIEW INV_AMOUNTS AS SELECT INV_NUM, INV_AMOUNT, AVG(INV_AMOUNT) AS AVG_INV (INV_AMOUNT - avg(inv_amount)) AS DIFF FROM INVOICE GROUP BY INV_NUM, INV_AMOUNT; 
In the end it turned out I did not needed to use create view All i needed was just: SELECT INV_NUM, INV_AMOUNT, AVG(INV_AMOUNT) OVER () AS AVG_INV, (INV_AMOUNT - AVG(INV_AMOUNT) OVER ()) AS DIFF FROM INVOICE; 
I learned the most in a dev db and working through old requests. Also when I get stuck working with senior analyst/devs is great cause they know a ton and the data architecture (db/schemas/tables etc). 
Your query essentially says "Give me all distinct titles that are both in the table Movie AND the table Rating, exclude all other results. Then remove all records that are in the table Rating"
You are performing an inner join. Then you are excluding all things that actually might result in a returned row by excluding everything that exists in the reviews table. Unsure what your goal is so I am going to guess you are trying to find all movies WITHOUT reviews. one method to achieve that would be to do the following: select distinct(title) from Movie m left outer join Rating r on m.mID = r.mID where r.mID is null This first joins the movie table to the review table and finds any matching reviews. it returns at least one row from table a (movie) if there are no matches in reviews, with the review table columns all being NULL. for all matches in reviews, it returns a row with the matching information from both tables. The where condition then selects only those rows where NO reviews were found. EDIT: here is another method with no joins at all: select distinct(title) from Movie m where m.mID not in (select distinct r.mid from Rating r) EDIT 2: more about joins here: http://www.programmerinterview.com/index.php/database-sql/inner-vs-outer-joins/
Ahh thank you. Very silly mistake. I have no idea why I was joining Movie and Rating. This is what I had pictured in my mind all along: select distinct(title) from Movie m where m.mID not IN (select mID from Rating)
Here are a few resources I like: 1. [SQLZoo](http://sqlzoo.net/wiki/Main_Page). A series of exercises to learn SQL, starting with SELECT and ending with JOIN. This holds your hand. 2. [PostgreSQL Exercises](http://pgexercises.com/). It has the tables set up and asks you to retrieve data. You need to look through the docs to figure out how to do it. Less handholding than SQLZoo, but it will give you the answers if you get stuck. 3. If you like learning from books, Head First SQL is what I started with, and I thought it was great. Hope that helps.
A little belated but thanks! This works too and makes more sense than than with the subquery imo.
The only time select * should be used is for an operand of an exists clause. This is because it is not evaluated.
Depending on how large your dataset is, you may want to move the rating records into a temp table rather than performing a subquery. Try it out and see if the performance is any better. Also, below is a link to an awesome visual diagram explaining different join types. http://www.codeproject.com/KB/database/Visual_SQL_Joins/Visual_SQL_JOINS_orig.jpg
For your learning project it won't make much difference. In the real world, you should however definitly filter the data on the database query. If you filter it down in LINQ you will have a lot of unneccesary network IO for a start, and in addition, you will not be able to make use of indexes, and screw with the SQL servers chaching. You should always filter the data on the database side, unless you plan on caching a lot of data in the application itself, which you also should only do if you actually need the data (caching a million rows, that you need to search trough 3-4 times wouldn't make much sense). 
http://www.postgresql.org/docs/9.1/static/plpgsql-porting.html http://ora2pg.darold.net/
The data set is VERY small  20 tuples. Btw how does CREATING a temporary table improve performance over a subquery? To me it seems that the "creating" part of the creating a new temp table would take longer than simply subquering the dataset.
You should look at Stored Procedures that already exist in the database. Whenever we take on an existing database, this is how I learn the relationships and design. If you are new at SQL, it will also help you to understand joins, nested queries and temp tables. Just remember... No CRUD in a production environment until you know how to recover, and or perform a point in time restore. As you start out, export any table you will be performing CRUD on to CSV. Understand your recovery model and data life cycle. 
You create a View if you want to use that same SQL many times. It's easier to have a view than to copy/paste that as a subselect in every query you use. Another thing you might want is a [CTE](http://msdn.microsoft.com/en-us/library/ms175972.aspx). Not all databases support the WITH clause, but it's tremendously useful for recursion or mutually exclusive sets of aggregates. 
Forgive me if I'm mistaken (I'm by no means a SQL guru), but I believe that subquery would run once for every row in the table. If you were dealing with a large data source, that would be one hell of a lot of subqueries. Creating a temp table runs once and can be indexed so that (for larger datasets) the query logic is only run once. It doesn't matter for your dataset but if you were dealing with millions upon millions of records, the difference in performance would start to become more significant. Regardless, I still think the most elegant solution in your case would be to use the left outer join that someone else mentioned. I personally try to use subqueries sparingly. 
I Googled "Find the landlocked countries". This was the first link. http://stackoverflow.com/a/26825783/562459
Create Read Update Delete. Less so with read, but be sure you understand performance constraints. Some procedures can lock tables for several seconds and beyond - that may not be appropriate on an LOB application during business hours. 
Yeah that's my post.
how come nobody has mentioned the LEFT OUTER JOIN solution SELECT m.title FROM Movie AS m LEFT OUTER JOIN Rating AS r ON r.mID = m.mID WHERE r.mID IS NULL by the way, kingjeet, DISTINCT is ~not~ a function, it applies to all columns mentioned in the SELECT list... so enclosing title in parentheses is not necessary
Our better, use exists instead, then it's a faster subquery without needing temp tables
I've worked through this, and found it overall to be pretty good: http://www.sqlcourse.com/ Make sure to click on 'Advanced Queries" for more material.
Nice, that's not a bad idea either. 
Wow yes! Thanks for telling me what it was called. Now I can Google it and read about it. See I never knew if this type of relation was allowed or if I was setting myself up for failure in the future if the application grows, but now that I can read about the relationship, I'm in a better position. 
Here is an alternate method. Basically what it does is it says look for all the countries that have a neighbour but where no other neighbours to that country exist. So this only works if you are looking for countries with only 1 neighbour. You can see it with some data here: http://sqlfiddle.com/#!12/6db9d/2 it will likely be quicker that the join method you got on StackOverflow but its less flexible. select a.cname , a.height , a.population , b.cname neighboured_Country, n.length from country a inner join neighbour n on n.country = a.cid inner join country b on n.neighbor = b.cid where not exists ( select 1 from neighbour o where o.country = a.cid and o.neighbor != n.neighbor );
Genuine question, why would it be quicker? My experience in using *Not In* and *Not Exists* is very poor performance. I always try and avoid *Not* in my queries where possible.
In this case it may well be about the same. It was really just a gut feeling that directly finding the rows that didn't have any other neighbours would be quicker than grouping all the neighbours to find the rows that only have one neighbour and then joining to it. But I think in hindsight I may have been too hasty with that assumption. They are very different ways of looking at the problem though which I think was the interesting bit I was trying to get across. Its probably worth just checking for this particular case to find out which works best for the dataset. On reflection I don't think I would really know which performs better. As to NOT EXISTS performance in general. It does perform fairly badly but its what you are comparing it to that matters. In many cases where you are trying to achieve a similar result NOT EXISTS performs better than the alternatives and rarely worse. [Here is a related article for MS SQL](http://sqlperformance.com/2012/12/t-sql-queries/left-anti-semi-join) (which is what I am mostly familiar with). 
Probably want to have subqueries for the team and/or player and join them. Something like this. select player.name, (team.H + team.BB) - (team.H - player.H + team.BB - player.BB) as stat from (select team, year, sum(H) as H, sum(BB) as BB from Batting group by team, year) team, (select name, team, year, H, BB from Batting) player where team.team = player.team and team.year = player.year
There is no way I'm downloading a random file off sendspace. Take a good picture and post it on imgur. Or setup your data and queries on http://sqlfiddle.com/ and post the link. Regardless, you probably just want to look at JOIN, LEFT JOIN, and SubSelect. 
Thanks! I didn't know the how else to upload my database to reddit.. The trick here is that for my assignment I have to actually use ALL of those queries at least once. More points for combos. I'll get that up there in about an hour... I'm on the bus now... Thanks a million for your consideration!
'NOT EXISTS' is roughly equivalent to the English 'except when' or 'other than'. So the query gets all all sailors except when there's a boat with a color that has not been reserved by this particular sailor. IMO, an easier query to read would have been 'give me all sailors who reserved as many boat colors as there are': select s.id, s.name from sailor s join reservations r on r.sid = s.id join boat b on b.id = r.bid group by s.id, s.name having count( distinct b.color) = (select count(distinct color) colors from boat) 
Updated in description. Thanks!
&gt;list of queries I don't see any queries here. Do you mean list of tables? &gt;combine them together to make one BIG query Do you mean you need some sort of result set to link these tables together? To increase your chances of help we will need more details and more of your working scripts if any. Maybe more of the requirements for the assignment might help as well. From your current description I don't even know what question(s) you are trying to answer with the data. That being said, here is two result sets based on your relation ships. The first shows the artist and related/linked genre and producer data. Critics can't be related or linked to the other tables because no relationship exists in your given schemas. SELECT a.ArtistID, a.LastName, a.FirstName, a.Gender, a.DateSigned, a.Group, a.Genre, g.GenreDesc, g.ProducerID, p.ProducerName FROM Artist AS a INNER JOIN Genre AS g ON g.Genre = a.Genre INNER JOIN Producer AS p ON p.ProducerID = g.ProducerName; SELECT c.CriticID, c.CriticName FROM Critic AS c;
Are you INPUTting new products, or are you just requesting existing products (where prodname = 'ProdA')? What defines the constraints? Does ProdC set the .5-.75 rule for all other products, or does ProdA set that rule for its specific relation to ProdC?
Basically here is what happens. I input a new product into the input table. I then have a make table query that retrieves pricing information on all related products and puts that into a table at all the available price points for the input product line. So my table would get a result that looked like this 39(possible price for input product)- Product C- Product C price/39 39(possible price for input product)-Product B- Product B PRICE/39 56(possible price for input product)- Product C- Product C price/56 56(possible price for input product)-Product B- Product B PRICE/56 and so on. I have a set of rules that show some possible prices will not work because the price differential will be too much higher than corporate guidelines. I want these filtered out of my results. As for the rules, i have a guideline that looks like this Product A- Must be 1.2-1.5 Product B Must be 2-2.75 Product C Product C- Must be .75-1.1 Product B Must be .3x to .5x product A so if the input product was C, Product C's rules would be used. But if the input product was A product A's rules would be used. 
I absolutely agree that this is easier to read. The only difference for me would be to start by creating a view to resolve the inner joins on the table "reservation". CREATE VIEW reservations as select s.*, b.* from sailor s inner join reservation r on r.sid = s.id inner join boat b on b.id = r.bid; IME, this kind of view is useful far beyond the scope of the question that prompts me to create it. Now the SELECT statement is even more straightforward. select id, name from reservations group by id, name having count(color) = (select count(distinct color) from boat);
It's not an api but we've used a table found on this site http://sqlserverbuilds.blogspot.ca/ for at least the last 4 years to keep track. Maybe you could parse that somehow. You could use a service such as https://www.kimonolabs.com/ to create a custom API that would return the table contents as json possibly. 
A datespan datatype and high performance built in functions to support it. I have to do so much shit with date spans and it becomes completely unmaintainable super fast with any little bit of complexity. I can encapsulate logic away in functions (for instance, testing if two spans intersect) but performance instantly turns to garbage because now indexes are useless. For this kind of thing I generally have to get the data out of the database so I can work with it in a general purpose programming language but this becomes a problem as the amount of data needed for the calculations grows. 
I don't understand your question: you "prefer an alternative and can't seem to find one" How do you know what you want if you haven't found it yet? What exactly are you looking for? By "database host" do you mean a server? A service?
The main problem I have is that the logic to operate on date spans is terribly cumbersome. There are lots of edge cases to deal with for many operations and they make a mess of your query. If you have to do more than one operation in the same query, it quickly becomes very hard to understand, and thereby, maintain. It makes sense to put the operations in functions but that makes the result non-sargable and performance goes down the shitter. An example scenario is 2 tables that each describe something that happens for a period of time. The result I want is a single time line showing the status of what each table represents for any point in time. Say you've got one row that is in effect from 1/1/2014 to 6/1/2014 and another that's in effect from 3/1/2014 to 9/1/2014. From 1/1/2014 to 3/1/2014 you had one state, then from 3/1/2014 to 6/1/2014 another and finally 6/1/2014 to 9/1/2014 you have another. Doing this in SQL is a mess. Add in two more tables and you've got a full blown nightmare.
I'm not sure I understand - could you clarify please? Do you mean a data type that defines a date range, e.g. 1-jan-2014 - 31 dec 2014?
LINQ-style syntax order.
In some scenarios that could work but for the stuff I typically see, that's a lot of transient data to keep track of. There are also a lot of permutations of combinations that I need for various things so keeping a materialized view around for each of those would mean a lot of overhead every time one of the source tables gets updated. In my case, that overhead would definitely be a problem. Too much churn on the source tables and they're too central to permit slow CRUD operations. It would grind the entire system to a standstill. What I usually do is pull the data and do the calculations on demand at a higher software layer. Most of the time I'm only looking at less than 100 rows at a time (for instance, only rows that apply to a specific entity), so this is feasible. Bulk operations are slow as hell though because you have to loop through all that data. As a result I too often have separate code paths to do the same operation on a single entity or all the entities. This sucks for maintainability. Even in a fully expressive language, the logic to work with the spans is pretty messy. The concept of something that exists for a fixed (but not necessarily finite) period in time is something that's come up a lot in my career and I've never really found an elegant way to deal with it. It's already complicated when you're talking about a small number of spans, but when you start talking about handling billions of instances, it's a whole new level of headache.
Apologies for not being able to explain it properly... Basically, for our assignment, we have to use our database to practice writing different queries, just to show that we know how to write queries. For example: SELECT artistid FROM artist Then we have to practice using the many different types of queries that I have listed at least once. The purpose is only to show that we can write the codes themselves, but we get more points if we can combine some/all the queries to make bigger ones. So, ultimately I am not looking for somebody to write my codes for me, but rather show me which queries can be combined together. 
Ahh, I didn't get that the OP wanted to see if the column existed in any row, I assumed just the current row.
LINQ statements are structured like: FROM table WHERE columnA == 'ABC' SELECT columnB, columnC Because the order makes sense it's much better for things like intellisense. In regular sql your program has no way of knowing which columns you can select until it knows the table that you're querying which you don't enter until later in the statement. 
Is AccountNo numeric, or char/varchar? If it's char/varchar, then... and (ContactClass like '% Corp' or ContactClass like '% LLC' or ContactClass like '% Inc' or ContactClass like '% Co') and left(AccountNo, 4) not in ('7604', '7706') If it is numeric, then... and (ContactClass like '% Corp' or ContactClass like '% LLC' or ContactClass like '% Inc' or ContactClass like '% Co') and left(convert(varchar(20), AccountNo), 4) not in ('7604', '7706') 
Look at the data as a whole then start asking questions in plain English. Which bands have the most females by genre. What group has the most members with middle names that start with the letter E. Etc
Try: SELECT B.Product, B.MinPrice, B.MaxPrice, A.Price FROM TableB AS B INNER JOIN TableA AS A ON A.Product = B.Product WHERE A.Price &gt;= B.MinPrice AND A.Price &lt;= B.MaxPrice
Really, no LTRIM(RTRIM( ?
Bleh. SELECT CONTACTS, FullName, CompName, ContactClass, TaxID, Is1099, Status, a.accountno FROM Test.dbo.CONTACTS AS c INNER JOIN Accounts AS a ON c.DefaultAccounts = a.Accounts WHERE ( ContractClass LIKE 'Vendor%' AND ( NOT ( CompName LIKE '%Corp' OR CompName LIKE '%LLC' OR CompName LIKE '%Inc' OR CompName LIKE '%Co' ) OR ( a.accountno LIKE '7604%' OR a.accountno LIKE '7706%' ) ) ) AND ( c.Is1099 = '' OR c.TaxID = '' OR c.TaxID IS NULL );
How did you translate the question into the equivalent? Is there a pattern to the English query? 
Interesting. If it's possible to parallellize the bulk processing you could look into hadoop. It's made for things like that. It takes some getting used to but once you have wrapped your head around it you have a really powerful tool that can be adapted to different queries pretty easily. 
How about a simple, straightforward 'DIVISION' keyword?
&gt; Can I do this in SQL? Yes and no. **No** in a plain SQL, but **yes** in PL/SQL, TSQL or PL/pgSQL. I'm unable to exactly follow your clarification, but it sounds like a some conditional data manipulation so it can be done within procedure.
Hard to give a technical answer to this, since english is my native language and the process is kind of subconscious, but perhaps [this](http://en.wikipedia.org/wiki/De_Morgan%27s_laws) is relevant?
I guess you'll like the features the SQL:2011 brings: temporal and bi-temporal databases. With this you can define a life-span on row-level and the database brings build in functionality to cope with them. E.g. you'll be able to define a constraints that that ensure that the date spans do not overlap for all the rows of a given ID. AFAIK: DB2 has those features for some years now (called "time travel query" I believe) and Oracle 12c got something in this direction too (they had flashback before). PostgreSQL has pretty much everything you need to that (interval types, exclusive constraints) but does not yet follow the syntax described by the SQL:2011 standard (as far as I know). 
 SELECT tableA.product , tableA.price , tableB.minprice , tableB.maxprice FROM tableA INNER JOIN tableB ON tableB.product = tableA.product AND tableA.price BETWEEN tableB.minprice AND tableB.maxprice
Not necessary if ContactClass is a varchar or nvarchar field. If it is a char/nchar, then yes, right trimming is necessary. 
you've never played or watched baseball, have you
No, I know those all correspond to baseball stats. But I also know that those stats have names consisting of actual English **words** and using those would go a long way toward making the schema a lot clearer. Use synonyms or aliases if you prefer the shorthand, but it costs nothing to spell everything out.
When you say *PK*, are you talking about the ID number? Are there any other unique constraints on that table?
This worked perfectly! Thank you very much!
Cool. Gotta love dem crazy business requirements. Thanks for the gild.
http://www.linqpad.net/ is a good tool that lets you interact with various database systems using LINQ, plus it has some other nifty features for the DBA who doubles as an application developer. But I agree it'd be nice if SSMS would incorporate a LINQ interface into itself outside of building a SQL to LINQ class in C#.
There's not much I can think of from a data engine standpoint, but there are lots of features I'd love to see added in SQL Server Management Studio.
To clarify the Auto incrementing index is the primary key on the table. There aren't any other constraints. 
Interesting. Frankly my current work with temporal dbs is not complicated. I audit price &amp; inventory over time to make sure that vendors honor their listed price at the time of offering. Beyond that I do a standard deviation to figure out the variability in inventory (to find fast moving items) and that's about it. Mostly I wanted to make sure that I was building good habits &amp; denormalizing a database is something I approach with caution. 
Thanks for sharing more details. I'd suggest keeping the ID as the PK but just as a constraint, then add a Clustered Index on SKU, DateAdded, and DateExpired. This way when you query based on SKU you can see when a given date is between DateAdded and DateExpired to verify you have the correct price. Going this route your Clustered Index should be used. The PK Index is in there more or less for uniqueness, though I guess you could remove it and use the SKU, DateAdded, and DateExpired as a Clustered Primary Key Index instead. I'm just a huge fan of having a single column as the PK, even if it's just an Identity surrogate key, but that's in the "to each his own" category :)
Very useful answers! Thanks for taking the time. I think I'll keep the PK until I have a large enough issue in performance to justify a change. 
It's our accounting department. They have a lot of requests like these. Most of the time I can figure it out but every once in a while I get stumped. The gold is well deserved because I have been working on this script off and on for two days and just couldn't figure it out. Thanks again!
Now that you say it, it's strange that SQL Server 2014 doesn't seem to add anything from SQL:2011. However, once you know the google term, you can find a lot how people do it with SQL Server. Temporal database: one having validity (from/to) on rows Bi-Temporal database: additionally having a timestamp when this information was added to the system. Example: if one marries and the name changes, the new name is valid from the marriage on. If, however, she doesn't tell her bank until after honeymoon, the bank will learn about the new name with some weeks delay. In that case they still add the new name with validity from the marriage on, but note that this was added some weeks later. Later on, they'll be able to answer the question why they sent her post with the old name the week after she married.
Getting and preparing data is definitely part of "data analysis". But I find the most value is in the data modeling/interpretation rather than summaries. That's just for my work though, where we have a central data center with DBAs that give us the data we want so we can focus on modeling. Summaries of data are nice but I feel like anyone can do that. 
For variable length character data, yes, trailing spaces do not matter. Leading spaces do matter, but not for this specific query, because OP doesn't care about the beginning part of ContactClass. He only cares how they end.
&gt; There aren't any other constraints. &lt;sup&gt;(crickets)&lt;/sup&gt; &gt; it occurs to me that the PK is now almost obsolete. The ID number serves little purpose. But you need a key constraint (either "primary key" or "not null unique") on the columns that identify a row, regardless of whether you use a surrogate ID number.
&gt; There aren't any other constraints. Listen.^(crickets) &gt; it occurs to me that the PK is now almost obsolete. The ID number serves little purpose. But you need a key constraint (either "primary key" or "not null unique") on the columns that identify a row, regardless of whether you use a surrogate ID number.
Didn't know that about variable character fields, great!
Btw, we run SQL 2008 R2, so we don't have data-driven subscriptions... Also, the existing job i'm trying to test with was actually created as SQL Agent job in SSMS...The script i wrote above is just practice for me to learn and understand how to use stored procs, but i'd like to use that example for my first test, then tackle the longer sql code on the existing reports.
We're using Data-Driven Subscriptions with 2008 R2. According to [MSDN](http://msdn.microsoft.com/en-us/library/ms169673\(v=sql.90\).aspx), they go back to 2005.
So have you tabled the Product to Product rules? [Many to Many] Have you tabled the available price points for each product. [One to Many] If so, it should be possible to run a query to find all the valid price points for a particular product, or all products for that matter? 
When you create an email subscription, it actually creates a job in SQL Agent. You can hijack the SQL in that job to create your own statement that will only call that portion if there is data.
Basically the user puts in a date to oppo_biddate and then the system checks that date and if it's the 4th day (minus Sat and Sun) to email the user and let them know they need to forward the opportunity. Most of this stuff is done in CRM, but the SQL needs to return records that are at a certain stage in one field, haven't been checked before (separate field) and those that are 4 days past the biddate.
I understood that part from what i've read since this morning. How would i go about writing the custom code for the example script i posted above? A few different blogs/forums pointed me to these 2 links, but i wasn't able to apply it :S http://stackoverflow.com/questions/6498271/send-report-only-when-the-attachment-has-data-in-ssrs Also, i mentioned we have jobs that are created directly in SSMS and some that are created in SSRS with the MS Report Builder, so not all the jobs in SQL have a subscription in SSRS, it's done using the Agent Job GUI under schedules, the recipients are listed in the query. http://blogs.msdn.com/b/bimusings/archive/2005/07/29/445080.aspx
&gt; &lt;sup&gt;(crickets)&lt;/sup&gt; Should I have more in there? I update the rows exclusively via stored procedure with a commit block within, so it seems safe to me. &gt; The ID number serves little purpose. But you need a key constraint (either "primary key" or "not null unique") on the columns that identify a row, regardless of whether you use a surrogate ID number. Now we're talking! I'm educated through the university of Google, so I often don't know the problems until I reach them. Can you tell me what kinds of issues I would have if I didn't have a primary key?
Yeah, but the thing is the people I send the stats out to are used to/expecting the shorthand, so it just makes more sense to keep it abbreviated in my mind. 
Doesn't it have to be enterprise edition?
Sorry I didnt actually read your post before, just the title. Looks like you already had that part figured out. Maybe I am missing something, but it looks as if you are already doing the check, aren't you? When you say... IF EXISTS ( Select * from .. where date&lt;=2014-11-9) ...isn't that doing exactly what you are asking me how to do?
We're not using Enterprise AFAIK.
Do the translation at presentation time, in your final query or generated reports. You don't design the database for how **your mind** works.
You guys don't get a million "My report didn't come today" emails?
Not if the original requirement was "Don't send it if it is empty." It's only used if that is one of the requirements. For most anything else I use the NoDataMessage property on the tablix. 
if exists under the sql agent, i can show you an example of one of mine.
Can you write dynamic sql? Try this: a stored procedure that uses a cursor to create a view of data with date columns as headers (Jan - Dec) which will only display monthly columns up to the prior month end (so for now, it would only go to November). I have a very picky recipient of a report which only wants data exported up to the prior month end, and this "view" of his data was the most appropriate solution.
So Select, then Group By YEAR(O_ORDERDATE) instead of just O_ORDERDATE ?
Not sure why the two are at odds. I've always created databases with primary keys. Now that I'm doing more data warehousing I'm finding that a degree of denormalization is useful, especially as size increases to the millions. The current price for ABC is whatever is in the row which doesn't have an expiration date. I could also pick the latest PK.
TotalDiscount = max( CurrentPrice) * Sum(SoldBetweenValidDates) - Sum( CurrentPrice * SoldBetweenValidDates) 
If you can't model your data, it doesn't mean shit. If you can model your data, then it can be fit into an SQL db. SQL db's allow interaction with multiple agents at the same time, in an efficient and optimizable format. Learn to fucking use SQL if data is the core of your job. No excuses. Seriously, if you are a data analyst or do that as part of your job and you tell me SQL won't help or is useless, considered yourself fired. This coming from a contract CTO/systems analyst/programmer that HATES working with DBA's- yes; I'm asking for MORE database administration workers/needs. Do you know why? Because they are useful and it is intelligent to use them. Just because I don't like working with data nazi's (yes, that's how I view them in my head) doesn't mean they don't perform a function or fulfill a need. Oh, you don't like SQL? That doesn't mean it don't perform a function or fulfill a need. Learn SQL. /mumbles 'fuckin scrubs'
First of all, #PriceChange should not contain a description, that's not 3NF. I'm sure that's just for the example, though. What version of MS SQL server? Will there ever be a case where the price increases (such as after a holiday sale?) I am just spitballing here, so I may have some syntax errors. On modern (2012+, I believe) systems you can use a lag with partition, assuming that the price never increases: SELECT Product, (LAG(CurrentPrice) OVER (PARTITION BY Product ORDER BY ValidFrom ASC) - CurrentPrice) * SoldBetweenValidDates FROM #PriceChange You may even be able to aggregate then and there, but I don't have my laptop around to test whether that upsets SQL Server: SELECT Product, SUM((LAG(CurrentPrice) OVER (PARTITION BY Product ORDER BY ValidFrom ASC) - CurrentPrice) * SoldBetweenValidDates) FROM #PriceChange GROUP BY Product If you're on a 2005+ system, you can achieve the same effect with row_number(): WITH ShinyPrices AS ( SELECT Product, CurrentPrice, SoldBetweenValidDates, ROW_NUMBER() OVER (PARTITION BY Product ORDER BY ValidFrom ASC) AS SoldOrder FROM #PriceChange) SELECT Newer.Product, SUM((Older.CurrentPrice - Newer.CurrentPrice) * Newer.SoldBetweenValidDates) FROM ShinyPrices as Newer JOIN ShinyPrices as Older on Older.Product = Newer.Product AND Older.SoldOrder = Newer.SoldOrder-1 GROUP BY Product If you have SQL Server 2000 or older you can just open the computer, remove the punch cards and sort them by hand.
Thank you for the reply.. Yep most of the table is just there for example. There may be cases where the price does increase. Also i will be running the query for a specific period, e.g. November. Would this query work, for example, if there was &gt;3 price changes in the month? thank you and im using 2008
According to his example the loss is defined as the difference between the previous price and the current one, not the maximum price and the current one.
Looks like even if there is more than 3 it will still calculate!
At this point we're getting into real pains in the keister due to how your data is defined. If prices can cross month boundaries (Start = 1/1/2000, End = 3/16/2001, Sold = 3,154,062) then we--meaning you, because without a salary I'm out--can split things up but the numbers are really just guesstimates. If they can't, then Totes McGoats: WITH ShinyPrices AS ( SELECT Product ,CurrentPrice ,SoldBetweenValidDates ,ValidFrom ,ValidTo ,ROW_NUMBER() OVER ( PARTITION BY Product ORDER BY ValidFrom ASC ) AS SoldOrder FROM PriceChange ) SELECT Newer.Product ,SUM((Older.CurrentPrice - Newer.CurrentPrice) * Newer.SoldBetweenValidDates) FROM ShinyPrices AS Newer INNER JOIN ShinyPrices AS Older ON Older.Product = Newer.Product AND Older.SoldOrder = Newer.SoldOrder - 1 WHERE Newer.ValidFrom &gt;= CAST(@Month + '/1/' + @Year AS DATETIME) AND isnull(Newer.ValidTo, getdate()) &lt;= DATEADD(d, - 1, DATEADD(m, 1, cast(@Month + '/1/' + @Year AS DATETIME))) GROUP BY Newer.Product EDIT: Yup, the quantity of changes is irrelevant.
Why?
You're right. My formula is for the total successive discount. I stand corrected. (edited the original). On the other hand, the original question does not have anything related to cost, so 'loss' is a bit of a misnomer, IMO. 
My guess would be the cursor, they are procedural and inefficient in most cases. You could easily do this without a cursor, and just a couple lines in the where clause; which is my preferred method for doing something similar. I'm not defending the other guy though. I see a lot of posts like that by people that hire, and pretend they hire, developers - your hiring an individual with a skill set, not a code pick and place machine. Sometimes an elaborate or just plain redonk solution for something simple is more telling of a range of skills (while showing an easily understandable result) as opposed to seeing if they can hit some useless metric.
yes, avoid cursors. if its more complex, they're not looking for slick queries, they're looking for slick performance. how do you determine when to index a table, and what to index? tables that are frequently read from are good candidates for indexing, tables that are written to or updated are bad candidates - be prepared to explain why. research nested queries. research temp tables. be prepared to explain the difference between CTEs, table variables (@table) and true temp tables (#temp .. and ##temp). be prepared to explain which to use in which situation. research primary keys, and foreign keys. when to use them, when to avoid them. research stored procedures, views, and functions. learn how to write one of each from scratch. research some of the fundamentals of version control and source management. if you're interested in dazzling your interviews with "next level" queries, research things like CROSS JOIN, CROSS APPLY, and MERGE. MERGE is actually *crucial* for developers to use, now that 2008R2 is a deprecating standard, being replaced by 2012 and 2014. MERGE wasn't supported by 2005 (or even the original 2008, I think), but its a supported function/command now, and its a massive time-saver when properly used. but its very restrictive. other fun stuff is COALESCE vs ISNULL. HAVING and NOT HAVING are important to know as well, and I find that a lot of developers forget about these. research normalization. be prepared to discuss the five normalized forms. research ETLs. be prepared to discuss job scheduling and ETLs at length.
when I interviewed for IBM, the interviewers gave me about a 20 minute overview of the software platform and data model, with the expectation that I ask good questions during the overview. they were drawing a lot on the whiteboard. then, without much warning, the department lead says, "but forget about all that" and proceeds to erase the board. he says "let's pretend, we're a company called Amazon. we have an idea to sell books, on the internet. well, we'll start out with books. but we're going to branch out to way more. music, movies, and eventually pretty much everything. we plan to offer services all over the world, and support multiple languages. you can log in, look for stuff, buy it - for yourself or someone else - and we'll ship it - either to you, or someone else." he turns to me, holding the dry-erase pen out to me. "draw the data model." I spent the next .. jesus, 60 minutes? .. sweating my balls off, drawing a giant complicated assortment of boxes, with arrows leading all over, process diagrams, and so on. the entire time, one of them playing good cop, the other bad cop. one saying "that's a good idea" the other saying "no, that won't work because bla bla bla .. well wait, why'd you change that, should you have changed that? I might be wrong." part of what this was to make sure I knew what the hell I was talking about. the other part was to make sure I know how to collaborate. that I know how to take criticism and respond to others' input. and that I know how to stand up for myself and say "I think you're wrong, and here's why" to a superior. be prepared to do this. if its next-level data development or architecture, you'll be doing this every day. you'll spend about 2/3 of your days in meetings discussing the work that needs to be done, and 1/3 of your day actually working (or delegating work). try and get yourself into a mental position to do this.
Merge is great, but not always the most efficient way to perform an "upsert" In most cases the performance difference is negligible, but it is something to be aware of... http://www.sqlservercentral.com/articles/MERGE/103127/
Usually people like reasons instead of just straight out "no don't do this". I don't have time for reasons, but hopefully somebody clues in that something might be wrong and to research on their own. Also important to raise concerns of people "wanting things certain ways", once you put a dollar amount to things, people start getting their "Demanding" privileges taken away.
I think this is what you are looking for. http://www.w3schools.com/sql/sql_top.asp
Would be really appreciated, PM me if you think you can help out. I'm trying to add a @@ROWCOUNT= 0 RAISERROR('NO DATA', 16, 1)... I notice now that the SELECT statement in the @Query parameter is acting weird; Can anyone explain why if i put single quotes on a date in the parameter it returns an error, but if i don't i get a bunch more rows than if i just ran the select statement normally :S
Yes, you are. Data-Driven Subscriptions are enterprise edition only.
Not entirely relevant, but I'm preparing for a final exam tomorrow, and I KNOW this is going to be incredibly helpful. I'd upvote twice for your formatting too, if I could. 
Pacos has it, for readability I'd just suggest switching to: WHEN Table.DateField &lt; 30 THEN '1 Month' WHEN Table.DateField BETWEEN 31 AND 60 THEN '1-2 Months' .... ELSE '&gt;12 Months' 
With the 2nd syntax, you can replace 'MyField &gt; 1' with any predicate, like one you would use in a WHERE clause. So basically yes, you can use AND in your syntax.
When you say "stuff" do you mean a single field? IIf(A&gt;B,DLookup("tablename","columnname","where clause"),Dlookup("othertablename","othercolumnname", "other where clause")) If it is not just a single field/column, what are you trying to solve? are your schema's the same for both tables or would you be selecting different column names based on this field?
I think this is just what I need, thank you! I was having a hard time figuring out that syntax. I think I will only be looking at single field.
IF (Select Field1 from Table1) = Something (this has to be a true/false statement) BEGIN Query1 (select stuff from one table. This is executed if the above is true) END ELSE Query2 (select stuff from another table. This is executed if the above is false)
I could be wrong but I think this won't work in Access... Might work with SQL Server?
&gt; Why would I have 3 null DateExpired rows? Because you made a mistake. Good design saves you from the consequences of your mistakes. Related question: Why would you have three rows with the same timestamp? Because you failed to declare a constraint to prevent it. &gt; Presumably entering data through a stored procedure within a commit block is as safe as it gets, no? If that's all you've done, then no, that's not as safe as it gets. &gt; I'm getting the impression that you're not talking about functional day-to-day queries, but rather what to do when the fecal matter hits the oscillating device. No, it's day-to-day stuff. Several years ago I worked on a contract to implement a new inventory system for a chemical company. Their existing system had become unusable. I redesigned at least half the tables. The data migration turned up 200 errors in the existing data every work day for six months. (They only had the manpower to fix 200 errors a day.)
May need some sample data as I'm having a hard time understanding the table schema.
if exists ( your query here ) then ---once that is done and you have a result set exec (your report name)
I got that part working fine now, i'm just stumped with the csv results being all messed up ( Rows and columns all mixed up, i think it has to do with the way the day is formated because there's a bunch of dashes. Can you ahve a look at the imgur link in my reply below and let mek now if you can help with that? I appreciate your input though; I just had to put double quotes around the date in the @Query parameter and it worked fine.
 SELECT name , COUNT(*) FROM ( SELECT Staff.name FROM Staff INNER JOIN Employed ON Employed.spotA = Staff.name UNION ALL SELECT Staff.name FROM Staff INNER JOIN Employed ON Employed.spotB = Staff.name UNION ALL SELECT Staff.name FROM Staff INNER JOIN Employed ON Employed.spotC = Staff.name UNION ALL SELECT Staff.name FROM Staff INNER JOIN Employed ON Employed.spotD = Staff.name ) AS u GROUP BY name
My take on this would be quite apart from the case syntax, let SQL handle the month. Saying 30 days and 30-60 days may not mean what people expect it to when talking about months. If you add a month to the date and then see if its in the future then that will likely be more in line with what people are expecting to see. Also pedantic quibble: one end needs to be inclusive. So its either &lt;1 Month and &gt;=12 Months or its &lt;=1 Month and &gt;12 Months. SELECT CASE WHEN Table.DateField&gt;CURRENT_TIMESTAMP THEN 'Future Date?' WHEN DATEADD(MONTH,1,Table.DateField)&gt;CURRENT_TIMESTAMP THEN '&lt;1 Month' WHEN DATEADD(MONTH,2,Table.DateField)&gt;CURRENT_TIMESTAMP THEN '1-2 Months' WHEN DATEADD(MONTH,3,Table.DateField)&gt;CURRENT_TIMESTAMP THEN '2-3 Months' WHEN DATEADD(MONTH,6,Table.DateField)&gt;CURRENT_TIMESTAMP THEN '3-6 Months' WHEN DATEADD(MONTH,12,Table.DateField)&gt;CURRENT_TIMESTAMP THEN '6-12 Months' WHEN DATEADD(MONTH,12,Table.DateField)&lt;=CURRENT_TIMESTAMP THEN '&gt;=12 Months' ELSE 'Invalid Date' END AS Age
Looking at your sql statement: CASE WHEN g2.GRUID IS NULL THEN URUID ELSE g2.Name END AS [SubElement], I don't believe this will work because g2.Name is a 'varchar' type, and URUID is an integer. When I tried it I got errors stating cannot convert type int to varchar. I also get timeout errors trying to run this... Thanks for helping.
Preciselly, thanks!!!
Or piggybacking off what /u/r3pr0b8 has done for you. Not sure if its better or worse, probably worse... but its a different way to look at it I guess. WITH StaffSpot (Name,SpotA,SpotB,SpotC,SpotD) AS ( SELECT Staff.Name ,CASE WHEN Staff.Name = Employed.SpotA THEN 1 END SpotA ,CASE WHEN Staff.Name = Employed.SpotB THEN 1 END SpotB ,CASE WHEN Staff.Name = Employed.SpotC THEN 1 END SpotC ,CASE WHEN Staff.Name = Employed.SpotD THEN 1 END SpotD FROM Staff INNER JOIN Employed ON Staff.Name IN (Employed.SpotA,Employed.SpotB,Employed.SpotC,Employed.SpotD) ) SELECT StaffSpot.Name ,SUM(StaffSpot.SpotA) CountSpotA ,SUM(StaffSpot.SpotB) CountSpotB ,SUM(StaffSpot.SpotC) CountSpotC ,SUM(StaffSpot.SpotD) CountSpotD FROM StaffSpot GROUP BY StaffSpot.Name; 
Just CAST(URUID AS VARCHAR(128))
Excellent point. I think I just always thought of the CASE statement as non SARGable anyway so it wouldn't mater, but on checking, thats not the case at all. So I think I learnt something today as well :) Yay. Thanks Pacos SELECT CASE WHEN Table.DateField &gt; CURRENT_TIMESTAMP THEN 'Future Date?' WHEN Table.DateField &gt; DATEADD(MONTH,-1,CURRENT_TIMESTAMP) THEN '&lt;1 Month' WHEN Table.DateField &gt; DATEADD(MONTH,-2,CURRENT_TIMESTAMP) THEN '1-2 Months' WHEN Table.DateField &gt; DATEADD(MONTH,-3,CURRENT_TIMESTAMP) THEN '2-3 Months' WHEN Table.DateField &gt; DATEADD(MONTH,-6,CURRENT_TIMESTAMP) THEN '3-6 Months' WHEN Table.DateField &gt; DATEADD(MONTH,-12,CURRENT_TIMESTAMP )THEN '6-12 Months' WHEN Table.DateField &lt;= DATEADD(MONTH,-12,CURRENT_TIMESTAMP) THEN '&gt;=12 Months' ELSE 'Invalid Date' END AS Age
How nested is it? For example Say Group A has UserA in it. Group B has UserB in it. Group C has UserC in it. And then Group AB has GroupA and GroupB in it Can There be a GroupABC that has GroupAB and GroupC in it? ie can a Group of Groups be in a Group? If so then you are into recursion territory. You will need CTEs
http://sqlfiddle.com/#!6/7db34/16 WITH GroupRelations (GRUID, GRUID2,URUID) AS ( SELECT GRUID , GRUID2 , URUID FROM GandURel WHERE GRUID2 IS NULL UNION ALL SELECT child.GRUID , child.GRUID2 , parent.URUID FROM GandURel child INNER JOIN GroupRelations parent ON child.GRUID2 = parent.GRUID ) SELECT Folders.PATH , Permissions.Access , Groups.Name AS [Group] , Users.Name AS [User] FROM Folders INNER JOIN Permissions ON Folders.FUID = Permissions.FUID INNER JOIN Groups ON Permissions.GRUID = Groups.GRUID INNER JOIN GroupRelations ON Groups.GRUID = GroupRelations.GRUID INNER JOIN Users ON GroupRelations.URUID = Users.URUID ORDER BY Folders.PATH, [Group] 
See, that's a great comment, that's what the other guy should have done
Right, no I get it, it's totally dependent on the ask. A 12 row result for the cursor to built the SELECT statement is not any less efficient than writing the statement in the WHERE. 
&gt; I see your point. If I understand correctly, in addition to adding constraints your advice would be to keep the primary key. OLTP system? Drop the ID number. Declare the natural primary key. OLAP system? Keep the ID number. Declare the natural key as "not null unique". But in either case, I think this table needs some work.
Yes, a group can have multiple groups inside of it. Think of it as active directory. Yes, GroupABC can have GroupA, GroupB, and GroupC, and groupAB. CTEs? Thanks
Thanks, ill try this tomorrow!
You have more BEGINs than ENDs. The easiest fix will be to append the following to what you have: RETURN @Result END Might as well remove the other two RETURNs, since this makes them redundant.
As an added note - should WeddingBudget = AverageBudget show up as Underbudget? I would add another piece of logic there for when they equal each other. 
This is a good place to use a "self-join". I remember having this problem myself because it is particularly hard to Google :-) Assuming each timestamp (user) only answers each question once, you could do something like: SELECT what_table.timestamp, what_table.question, what_table.answer, where_table.question, where_table.answer, when_table.question, when_table.answer FROM (SELECT * FROM TABLE WHERE QUESTION = 'WHAT') what_table JOIN (SELECT * FROM TABLE WHERE QUESTION = 'WHERE') where_table ON what_table.timestamp = where_table.timestamp JOIN (SELECT * FROM TABLE WHERE QUESTION = 'WHEN') when_table ON what_table.timestamp = when_table.timestamp
Thank you.
Thanks. What should be the proper/most efficient syntax here? 
you should look into pivots. you want to pivot on question, using answer as the pivot value. http://stackoverflow.com/questions/15931607/convert-rows-to-columns-using-pivot-in-sql-server I haven't tested, by try this: SELECT * FROM Table PIVOT ( MAX(Answer) FOR Question IN ([What?], [Where?], [When?]) ) AS PVT That should give you: 1 | Answer to what | Answer to where | Answer to when 2 | Answer to what | Answer to where | Answer to when 3 | Answer to what | Answer to where | Answer to when
Let's get some indentation happening CREATE FUNCTION dbo.OverAvgBudget (@ClientFirstName VARCHAR(25), @ClientLastName VARCHAR(25)) RETURNS VARCHAR(25) AS BEGIN DECLARE @Result VARCHAR(25) DECLARE @ClientBudget MONEY DECLARE @AverageBudget MONEY SET @ClientBudget = (SELECT WeddingBudget FROM Clients WHERE ClientFirstName=@ClientFirstName AND ClientLastName=@ClientLastName) SET @AverageBudget = (SELECT AVG(WeddingBudget) FROM Clients) IF @ClientBudget&gt;@AverageBudget BEGIN SET @Result = 'Overbudget' RETURN(@Result) END ELSE BEGIN SET @Result = 'Underbudget' RETURN(@Result) END -- END statement missing here Not only is the indented form more readable, but we can see the problem immediately. You're missing the final end. I'd actually rewrite the IF/ELSE statement like so - IF @ClientBudget&gt;@AverageBudget SET @Result = 'Overbudget' ELSE SET @Result = 'Underbudget' END RETURN(@Result) 
You can group over customer and food: SELECT customer, food FROM orders GROUP BY customer, food HAVING COUNT(*) &gt; 1; http://sqlfiddle.com/#!2/fbe26/2
Ah, so you create groups with both attributes rather than groups of each attribute. That's great, thanks! 
A prime example of why formatting matters.
This one also worked really well. Having the columns broken out is very helpful!
if the name "fred" is on 2 rows in spot A and 4 rows in spot B and 6 rows in spot C and 8 rows in spot D, then your joins will return 384 rows for fred and your counts will be ~way~ inflated
You are correct. I ended up with a query similar to what /u/extrajoss posted after looking at it again. --DROP TABLE dbo.Staff; CREATE TABLE dbo.Staff ( Id INT IDENTITY(1,1) NOT NULL PRIMARY KEY, Name VARCHAR(50) NOT NULL ); --DROP TABLE dbo.Employed; CREATE TABLE dbo.Employed ( Id INT IDENTITY(1,1) NOT NULL PRIMARY KEY, SpotA VARCHAR(50) NULL, SpotB VARCHAR(50) NULL, SpotC VARCHAR(50) NULL, SpotD VARCHAR(50) NULL, ); INSERT INTO dbo.Staff (Name) VALUES ('fred'), ('bob'), ('sue'), ('gary'), ('harry'); INSERT INTO dbo.Employed(SpotA,SpotB,SpotC,SpotD) VALUES ('fred','bob','sue',NULL), ('fred','sue','sue',NULL), (NULL,'fred','sue',NULL), ('sue','fred','gary','bob'), (NULL,'fred','bob',NULL), ('bob','fred',NULL,NULL), ('bob','sue','fred','sue'), ('bob','sue','fred','gary'), ('bob',NULL,'fred','gary'), ('bob','sue','fred','gary'), ('harry','sue','fred',NULL), ('bob','sue','fred','gary'), (NULL,'sue','harry','fred'), ('bob',NULL,'harry','fred'), ('sue','sue',NULL,'fred'), ('harry',NULL,'harry','fred'), (NULL,'sue','harry','fred'), ('harry',NULL,'harry','fred'), (NULL,'sue',NULL,'fred'), (NULL,NULL,'harry','fred'); SELECT s.Name, COUNT(CASE WHEN e.SpotA = s.Name THEN e.SpotA ELSE NULL END) TimesInSpotA, COUNT(CASE WHEN e.SpotB = s.Name THEN e.SpotB ELSE NULL END) TimesInSpotB, COUNT(CASE WHEN e.SpotC = s.Name THEN e.SpotC ELSE NULL END) TimesInSpotC, COUNT(CASE WHEN e.SpotD = s.Name THEN e.SpotD ELSE NULL END) TimesInSpotD FROM dbo.Staff AS s INNER JOIN dbo.Employed AS e ON s.Name IN (e.SpotA, e.SpotB, e.SpotC, e.SpotD) GROUP BY s.Name;
Depending on how you want to represent the results - total hours or hours in Decimal - this should do it for you: DECLARE @Date1 DATETIME = '2014-11-13 08:00', @Date2 DATETIME = '2014-11-13 15:33'; SELECT DATEDIFF(HOUR,@Date1,@Date2) AS [Hours_Total], DATEDIFF(MINUTE,@Date1,@Date2)/60.00 AS [Hours_Decimal], ROUND(DATEDIFF(MINUTE,@Date1,@Date2)/60.00,0) AS [Hours_Total_Round]; Results: Hours_Total Hours_Decimal Hours_Total_Round 7 7.550000 8.000000 [Hours_Total] is the total number of hours, but this doesn't account for minutes. [Hours_Decimal] is a Decimal of hours and minutes, and this would be used to easily total time across multiple days. [Hours_Total_Round] is total hours but rounded up or down, so 5 hours and 30 minutes would round up to 6 hours, or 5 hours and 29 minutes would round down to 5 hours. I'm unsure what your goals are, but this hopefully covers all the bases. Also you didn't specify your database server, but this works in MS SQL. I don't know if DateDiff works similarly in other systems or not.
It is MS SQL, so that would work I think. Basically, what I am trying to produce is a 'lost labor hours' report. I am producing their Total_Scheduled hours from scheduled entries, a Actual_Total_Hours from time clock records, then am going to find the difference between.
Not sure if I fully understand your question. Give us some sample data but the query will be something like this. Select symbol, price, type, max(datestamp) from tbl1 where type = 'common' group by symbol, price, type having datestamp in( Select max(datestamp) from tbl1) 
This looks good, thanks! I'll give that a try. This isn't the case, but out of curiosity, what happens if timestamp isn't unique for each user who answered?
Let me make some up: symbol | price | date | type ---|---|----|--- a | 14 | 11/12/2014 1:59 am | foo a | 12 | 11/12/2014 2:59 am | common a | 23 | 11/12/2014 5:59 am | bar a | 12 | 11/12/2014 4:59 am | common b | 4 | 11/12/2014 1:59 am | foo b | 3 | 11/12/2014 2:59 am | foo b | 5 | 11/12/2014 4:59 am | foo b | 23 | 11/12/2014 7:59 am | z c | 1 | 11/10/2014 7:59 am | common c | 2 | 11/12/2014 4:59 am | foo c | 1 | 11/12/2014 5:59 am | foo c | 7 | 11/12/2014 6:59 am | foo So I am using this to join. My idea is that for each symbol, i will return the max date stamp for each symbol then i can join on the table to get the price associated with that datestamp. But, if the type = 'common' then i need the max datestamp of the common types. here's my query: select a.price, a.symbol, b.date from table a inner join ( select symbol, max(datestamp) as datemax from table group by symbol ) as b on a.symbol = b.symbol and b.datemax = a.date What I get is: symbol | price | date | type ---|---|----|--- a | 23 | 11/12/2014 5:59 am | bar b | 23 | 11/12/2014 7:59 am | z c | 7 | 11/12/2014 6:59 am | foo And what I need is: symbol | price | date | type ---|---|----|--- a | 12 | 11/12/2014 4:59 am | common b | 23 | 11/12/2014 7:59 am | z c | 1 | 11/10/2014 7:59 am | common (ignore that i the results arent what i queried for. just left it there so you can see, but i am not expecting to return the type.) 
I'm guessing this solves the duplication of columns returned by the query? Could this also be the reason that the data is not returned? Or is the structure of the SELECT statement also incorrect? Thanks
fyi, please see sidebar, you should always identify your platform in this subreddit
I went ahead and created a test database and put some groups and paths and users and permissions in, and ran the view and it worked nicely. I do wish (don't need it) I could get the Nested Group Name also but that could become an issue when you have: Path = ABC GroupA is a member of GroupB GroupC is a member of GroupA GroupD is a member of GroupC UserD is a member of GroupD So what you have is GroupB -&gt; GroupA -&gt; GroupC -&gt; GroupD -&gt; UserD Ideally I only want to get back UserD is a Member of GroupD for Path ABC but the root group is fine also (GroupA) Also, What happens when: GroupA is a member of GroupB GroupB is a member of GroupA I see a loop in my future, which is probably why the SQL Statement times-out on my Live Data. I have a feeling I have an infinite loop. Thanks for all the help!
This works in SQL SERVER 2005 select price, symbol, [Type],max(date) from [dbo].[test] where [Type] = 'common' group by price, symbol, [Type] Union all select price, symbol, [Type], max(date) from [dbo].[test] A where [Type] &lt;&gt; 'common' and symbol not in(select symbol from [dbo].[test]where [Type] = 'common') and date in(Select max(date) from [dbo].[test] where symbol = A.symbol) group by price, symbol,[Type] order by 2 
I'd actually want to find the events overlap. Is this possible?
The were clause is going to return lots of results because of the OR operator. You'll want to define specific conditions your looking for. Right now there are many potential matches. 
My bad, corrected.
Unfortunately that is as specific as I can be regarding the conditions when I ran the query I was expecting around 200 rows to return but received 0.
Thanks. Let me give it a shot.
Okay, that helped. Here's one suggestion with some table setup: DECLARE @tblEmployee TABLE (EmpID INTEGER, EmpName NVARCHAR(50)); DECLARE @tblScheduled TABLE (EmpID INTEGER, StartTS DATETIME, EndTS DATETIME); DECLARE @tblTimeClock TABLE (EmpID INTEGER, InTS DATETIME, OutTS DATETIME); INSERT INTO @tblEmployee (EmpID, EmpName) VALUES (1,'Stan Lee'); INSERT INTO @tblScheduled (EmpID, StartTS, EndTS) VALUES (1,'2014-11-10 08:00', '2014-11-10 17:00'), (1,'2014-11-11 08:00', '2014-11-11 17:00'), (1,'2014-11-12 08:00', '2014-11-12 17:00'), (1,'2014-11-13 08:00', '2014-11-13 17:00'), (1,'2014-11-14 08:00', '2014-11-14 17:00'); INSERT INTO @tblTimeClock (EmpID, InTS, OutTS) VALUES (1,'2014-11-10 07:55', '2014-11-10 17:10'), (1,'2014-11-11 08:10', '2014-11-11 17:04'), (1,'2014-11-12 08:30', '2014-11-12 16:55'), (1,'2014-11-13 07:22', '2014-11-13 17:00'), (1,'2014-11-14 08:00', '2014-11-14 16:45'); WITH TS (EmpID, EmpName, [Hours_Scheduled],[Hours_Worked]) AS ( SELECT E.EmpID, E.EmpName, SUM(DATEDIFF(MINUTE,StartTS,EndTS)/60.00) AS [Hours_Scheduled], SUM(DATEDIFF(MINUTE,InTS,OutTS)/60.00) AS [Hours_Worked] FROM @tblEmployee E Inner JOIN @tblScheduled S ON S.EmpID = E.EmpID INNER JOIN @tblTimeClock TS ON TS.EmpID = E.EmpID WHERE CONVERT(DATE,S.StartTS) BETWEEN '2014-11-10' AND '2014-11-14' AND CONVERT(DATE,TS.InTS) BETWEEN '2014-11-10' AND '2014-11-14' GROUP BY E.EmpID, E.EmpName) SELECT TS.EmpID, TS.EmpName, TS.Hours_Scheduled, TS.Hours_Worked, TS.Hours_Scheduled - TS.Hours_Worked AS [Lost_Labor_Hours] FROM TS Results: EmpID EmpName Hours_Scheduled Hours_Worked Lost_Labor_Hours 1 Stan Lee 225.000000 224.749995 0.250005 This basically assumes three tables: Employee Table, Hours Scheduled, and Hours Worked. It then compares the Hours Scheduled to Hours Worked and calculates the Lost Labor Hours. This doesn't though take into account Lunch or breaks which could easily be added as another column in the second or third table and subtracted from the calculation. Also I added a Where clause that would allow you to calculate based on a given period. If you had a table with your pay period defined you could join into this and run the report over multiple pay periods. Is this kinda what you're shooting for? 
If you check out this SQLFiddle I have added a way to get the DirectUserGroup http://sqlfiddle.com/#!6/7db34/19 Loops are really going to cause you hell though as you pointed out. The fact that it timed out may not be due to the existence of a loop though. What I have written will work but its not exactly going to be speedy when you get a decent amount of data. It should be ok for asking a specific question though. Like what permissions does Jim have. Or who has permission to write to folder X. But returning the whole hierarchy of access rights for every user could be a bit much. PS: thanks for the gold!! much appreciated! ;)
Pretty easy. Requires a self join. I'll assume there's an unique event ID for each event. SELECT * FROM EventTable E1 INNER JOIN EventTable E2 ON E1.StartTime BEWTEEN E2.StartTime AND E2.EndTime --look for events that start between another event OR E1.EndTime BETWEEN E2.StartTime AND E2.EndTime --look for events that end between another event AND E1.EventID &lt;&gt; E2.EventID --dont join on the same event Something like that. I used MSSQL, so the syntax may be a bit different but the concept should be easily transferrable.
I may be wrong but I'm getting the impression you want a union. SELECT 'Table1.Column1' SourceTable, t.Column1 [abc], t.year [def], FROM Table1 AS t WHERE t.Column1 = '123' AND t.year = '2014' UNION ALL SELECT 'Table1.Column2' SourceTable, t.Column2 [abc], t.year [def], FROM Table1 AS t WHERE t.Column2 = '123' AND t.year = '2014' UNION ALL SELECT 'Table2.Column01' SourceTable, t.Column01 [abc], t.year [def], FROM Table2 AS t WHERE t.Column01 = '123' AND t.year = '2014' 
I had to make a slight edit (needed to look for max date of common type as well because some multiples showed up) but this was fantastically helpfully. Thank you!
Tried your new code, worked well. I took your suggestion and make it more specific using a where clause and it came up in no time so I believe you are correct in saying that the issue was with too much data. Thank You for the help!
The program get's the ACL's (Access Control List) for every single folder in our File Shares. This includes groups and users that are directly given permissions to the folder itself. I am trying to get away from doing direct permissions assignment and replacing them with groups. The issue is Managers can't see the permissions on their folders unless they have ActiveDirectory installed to see inside the groups. That's where my script comes in. I have a powershell script that recursively goes through each folder and grabs the info and stores it in a database. Roughly every two weeks, the data is considered stale and the process is completed all over again (skipping ones that haven't changed). Now before the groups (when they were assigning users directly to folders) they could look at the permissions on the folder (right click, properties, security tab) and see who had access. Now they can't, all they see is a group name. I have a C# ASP.NET web application that the Managers can use to look this information up. The web app queries the database for the information and returns it. I am also scripting in a Add User and Remove User in the application, so managers can add and remove users from the folders (now groups) as needed. Why the nested groups? Usually the basic 3 permissions for file shares are List, Read and Modify (and sometimes Full Control when needed). Also, lets not forget about Inheritance. Inheritance can be broken so your upper level folders (T:\Test1) doesn't have the same permissions as (T:\Test1\Test2). If inheritance isn't broken, Test2 will have the same ACL's (permissions) as T:\Test1. So lets say Jane Doe wants Modify to T:\Test1\Test2. I would have basically 2 groups. Group1: T-Test1_L Permissions: List Group2: T-Test1-Test2_M Permissions: Modify Now if Test2 folder has broken inheritance, if I added Jane to the T-Test1-Test2_M group, she wouldn't be able to get to the folder because she still would need access to the Test1 folder first to see Test2. So, rather than adding Jane to both groups, I would add T-Test1-Test2_M to T-Test1_L. That way anyone in T-Test1-Test2_M will automatically get List permissions (see folder names) to T:\Test1. That's what I am currently going for. I will check out the Hierarchy DataType when I get the chance. Where could I use this. Would this be beneficial for paths? If the root path is T:\Test1, it would have a child path of T:\Test1\Test2. How could this help me in the long run I guess is what I am trying to get at. I guess the easiest way to explain it is, Explain it to me Like I am 5 years old lol. Anyway, thanks again for anything!
The one you may be missing is where E2.StartTime occurs after E1.StartTime and E2.EndTime occurs before E2.EndTime. In other words, E2 event starts later but finishes earlier than E1 Event. SELECT * FROM EventTable E1 INNER JOIN EventTable E2 ON E1.StartTime BEWTEEN E2.StartTime AND E2.EndTime --look for events that start between another event OR E1.EndTime BETWEEN E2.StartTime AND E2.EndTime --look for events that end between another event OR (E1.StartTime &gt; E2.StartTime AND E1.EndTime &lt; E2.EndTime) --added by Dessmond AND E1.EventID &lt;&gt; E2.EventID --dont join on the same event 
Actually its even easier than that. All you need to do is make sure that event 1 starts before event 2 ends and that event 1 ends after event 2 starts and that guarantees an overlap. SELECT * FROM EventTable E1 INNER JOIN EventTable E2 ON E1.StartTime &lt; E2.EndTime AND E1.EndTime &gt; E2.StartTime AND E1.EventID &lt;&gt; E2.EventID --dont join on the same event
&gt;column names were replicated in the query and not combined as I had wished. Your select list has four columns so it won't combine/concatenate them on its own. So do you want the ABC value of table1 and table2 reported in the same data row but combined into one field. OR Are you looking for the ABC value of table1 on one row and then next row would would have the ABC value of table 2. Your answer either makes your solution being a matter of concatenation like one user posted or a union query as another user posted. Your zero returned records is just a function of your criteria. So start by commenting out your WHERE clause stuff until stuff returns then refine and look.
 SELECT t.userid, t.symbol, t.price, CASE WHEN a.date IS NOT NULL THEN a.date ELSE b.date END AS date FROM YourTable t LEFT JOIN ( SELECT userid, MAX(date) FROM YourTable WHERE type = 'common' ) a ON t.userid = a.userid LEFT JOIN ( SELECT userid, MAX(date) FROM YourTable WHERE type &lt;&gt; 'common') b ON t.userid = b.userid That might work. It might be expensive though.
Hey, how about we not do your homework for you? Can we ban people who pull this shit? &gt; http://www.expertsmind.com/library/performance-of-a-distributed-database-5298841.aspx 
You could either make it a varchar and just enter it in that format and make who ever enters them use that format. Or you could have 3 columns , area code, first 3 numbers then last 4 and make a view that uses concat('(',areaCode,')', firstThree, ,-,lastFour) and it would come out looking like this (123)456-7890
I never let SQL handle the formatting. I either store a string of numbers and let the application/report handle displaying space and dashes OR require the spacing and dashes somehow during data entry and pass the now validated and formatted strings to SQL.
Thank you extrajoss, would it be possible to add a where statement so I can use a specific date range?
Thank you, this was what I was looking for
I store them in the db as an int like '1234567890' and parse it out into human readable in my reports/applications. Otherwise you are going to fight every different interation of a phone number. International code, leading 1, dashes, plus signs, paranthesis, periods, etc.
Been using it for years. Just kept at it and somewhere along the line, something just clicked.
My example to the OP used Minutes instead of Hours then divided by 60.0 to get the hour difference as a decimal that could easily be calculated. I agree, using Hours would probably not be best when calculating payroll values since it will chop off any remainder minutes.
I started writing a description of how I'd do this, then realised it'd be easier to just design a schema, so here's how I'd do it: Tables: Person ====== PersonID (PK) Name PositionID (FK) PersonShift =========== PersonID (FK) ShiftStart ShiftEnd TipAmountReceived Position ======== PositionID (PK) Title PositionHourlyRate ================== PositionID (FK) StartDate EndDate HourlyRate Shift Payments Report: SELECT p.PersonID, p.name, po.title as PositionTitle, ps.ShiftStart, ps.ShiftEnd, ps.TipAmountReceived, phr.HourlyRate, (ps.ShiftEnd - ps.ShiftStart) * 24 AS HoursWorked, (ps.ShiftEnd - ps.ShiftStart) * 24 * phr.HourlyRate as ShiftPay FROM Person p, JOIN Position po ON (po.PositionID = p.PositionID) JOIN PersonShift ps ON (ps.PersonID = p.PersonID) JOIN PositionHourlyRate phr ON (phr.StartDate &lt;= ps.ShiftStart AND NVL(phr.EndDate,TO_DATE('01-JAN-9999','DD-MON-YYYY')) &gt;= ps.ShiftEnd) By using a PositionHourlyRate table you can keep track of hourly rates temporally meaning you can use the join shown above (which replaces an empty EndDate with an imaginary far future date for the purpose of calculations) to retrieve the HourlyRate at any given time period. This does rely on the PositionHourlyRate table never having overlapping entries for a given PositionID, and a PositionHourlyRate never changing in the middle of a shift, however.
I'm assuming you have an employee_id or something like that to key time entries to an employee table. If you do, you could create another table called wage, which would be an auto id, employee_id, wage value, and a CURRENT_TIMESTAMP for that row. Then when the wage is modified, insert a row. Always grab the latest row for that employee_id, or the last row between your shift date range. I can provide a better answer when I get to a computer. I'll check back in a bit. 
You do have a couple options. Assuming all of your reports are in the same solution and using the same shared datasource (ds) 1. You can edit the shared ds and re-deploy it (need to delete original ds (if on same site) from the reports site or specify new location first, or else the deploy will fail) 2. You can edit the ds directly on your reports site (http://localhost/Reports) to point to a different location. Option 1 is best, so that your ds in your solution and on your site are the same, but 2 is easy and works in a pinch. Hope that helps
You may have to explicitly cast the substrings as varchar, sometimes SQL gets fussy with concatenation with + when there's numbers involved.
Whoops, I meant this: --this returns 0 records select * from OCT14_PANEL_RCR_HP where HAVE_EMAIL = 1; The in statement ran for about 3 hours before I quit on it. I replaced the FOR CURS_RECT IN ( select ama.menum10 from PBANIEWICZ.AIEMAIL@ORAMRD AI, PBANIEWICZ.AMAPHYS@ORAMRD AMA where AI.PID = AMA.PARTID and AI.EMAILVALID = 'VALID' and AI.UNSUBSCRIBED = 'N' ) clause with this: FOR CURS_RECT IN ( select menum10 from temp_table ) and it worked.
Thanks - going to experiment a bit with these. In case anyone else is migrating reports and hits this thread, I found a solid open-source tool for doing a batch migration: https://code.google.com/p/reportsync/ Will let you move all of the reports and data sources between two SSRS instances, or download everything to a local filesystem in one go. Huge, huge timesaver.
Looking into this. Thanks.
I actually have a task to do this in the near future, so thank you for the tool link!
I'm not sure if this helps, but here's the function I built years ago to format my phone numbers - http://pastie.org/9719336 This basically strips out all non-numeric values then formats the phone numbers to this: 555.322.3333 Ext 444 If there's a leading 1 it gets dropped, and if there's a value beyond 10 numbers it's put after EXT. Examples: SELECT [dbo].[fnPhone] ('444555666633') 444.555.6666 EXT 33 SELECT [dbo].[fnPhone] ('14445556666') 444.555.6666 SELECT [dbo].[fnPhone] ('1(444)555-6666') 444.555.6666 There may be prettier ways of doing this, but this works for all instances where I've needed to clean-up phone numbers. You can change it to use dashes or even to add parenthesis around the area code if you want. It just assumes a 10 digit number exists at the very least, so if you feed it fewer digits it won't return valid data. This may not fit your scenario exact, but many you can use this as a base to build a function that does. 
SSRS 12.0.2000.8
I have a Logitech M510 and like it well enough but I use it as little as possible. Most things are **much** faster with just the keyboard once you learn how.
I apologize if it is just my examples and I'm missing something but I deal with this often in my job and it's really annoying to work around. This applies to MSSQL. Sql Server 2012. If his PhoneNumber column is a numeric datatype, the implicit conversion fails because substring can't have a numeric column as an argument. declare @string1 numeric(9,0) = 123456 select SUBSTRING(@string1,1,3)+'-'+substring(@string1,4,3) Results in error: "Argument data type numeric is invalid for argument 1 of substring function." Also... here is an example of concatenate not knowing what to do with numeric datatypes. declare @string1 numeric(9,0) = 123456 select @string1+'-'+@string1 Results in error: "Arithmetic overflow error converting varchar to data type numeric." A couple of other examples. select 123+'-'+123 Results: 246 select 123+'ABC'+123 Results: "Conversion failed when converting the varchar value 'ABC' to data type int." Edited for formatting.
this was good, thanks, although I had to reverse the logic as such: PositionHourlyRate phr ON (phr.StartDate &gt;= ps.ShiftStart AND ps.ShiftEnd &lt;= IFNULL(phr.EndDate, 'somebigdatehere')) it worked! I think the complexity that i'll have to introduce now into my logic is every time you get a new raise, I have to first check that your hourly wage is different from what's currently in the table, and if it's different, insert a new row. Before that insert i'll have to also update the 'end date' to be today (the current day) so the join will work. Can I just ask - if I didn't do this but instead just put the current hourly wage on the shift table, what problems could I run into later? I know that 'logically' it doesn't relate, but it seems like the above is so much extra work. Have to update queries, joins, and even introduce new tables and classes. What's the advantage you think? Just philosophically better, but would it really future proof me? At any rate I appreciate your help this is awesome :)
Agreed on the keyboard. I use the old black Dell minimal one, no need for anything else. Never tried ergonomic keyboards though.
Too bad Reddit doesn't let me read deleted comments lol
I may be missing something, but isn't the definition of "configuration" the key? I'm not quite sure why you think you need a separate key field. Some more info would help, such as: are these 2 new tables permanent or temporary? (That is, when you say you "want two tables", do you really mean you want queries which produce those "two tables" as output?) 
Aaahhhhhh! This is a pet peeve of mine! Do you ever add phone numbers together? Divide them by two? Perform any other mathematical operations on them? No, you don't! Because they're not a numeric data! Even though they are CALLED "phone numbers", they are not numbers! What if someone needs to enter an extension? What if their extension is #0123? What if the phone number requires a leading zero? Store them as the strings they are! 
hmm, yeah I don't quite get what you're going for either. I'd have to see some sample data. A few rows from each table perhaps. I *think* what you're describing is some kind of identity column (a self incrementing key integer) that describes every instance where table1 joins to table2 on machineID. But I"m not quite sure what question that would answer. so if you did this: create ThirdTable (NewKey Integer Identity, machineId, other stuff) then inserted into it via this insert into thirdtable select a.machineId, a.somecolumns, b.somecolums from Table1 A inner join Table2 b on A.machineId = b.machineID that would give you that new InstanceKey or whatever you want.. But since you're asking about grouping on that key, I dont' think this will give you anything. Because it will be unique and not really groupable. 
Why would you want to use a cursor if you can use a set-based statement? Try this: MERGE INTO OCT14_PANEL_RCR_HP trg USING (select ama.menum10 from PBANIEWICZ.AIEMAIL@ORAMRD AI, PBANIEWICZ.AMAPHYS@ORAMRD AMA where AI.PID = AMA.PARTID and AI.EMAILVALID = 'VALID' and AI.UNSUBSCRIBED = 'N') mrg ON (trg.meaoa = mrg.menum10) WHEN MATCHED THEN UPDATE SET trg.HAVE_EMAIL = 1; Commit; 
Indeed this is how we learned it in school. If it's not going to be manipulated in some way then it should be a string.
I think you're asking how to pivot out columns from a key/value table? with your group by on MachineID, run the following aggregate function for each key you're trying to pivot into a column: max(case keycolumn when 'KeyValue1' then valuecolumn else null end) as KeyValue1, ,max(case keycolumn when 'KeyValue2' then valuecolumn else null end) as KeyValue2, This pivots the single value column into multiple columns, one for each key you wanted to turn into a column. 
Thanks for your reply, here's some sample data. TableMachineIDs MachineID RAM CPUSpeed 1234 2GB 1GHz 5678 4GB 2GHz 9000 2GB 1GHz TableFacts MachineID NumEvents 1234 5 9000 50 5678 100 Note, multiple MachineIDs may have the same configuration if you look at the RAM and CPUSpeed columns. Let's combine them. So what I want out is this: TableMyMachineIDs MachineType RAM CPUSpeed 1 2GB 1GHz 2 4GB 2GHz TableMyAggregateFacts MachineType TotalEvents 1 55 2 100 Does this make sense? 
maybe this: select id, max(case when question = 'who' then answer end) 'who' ,max(case when question = 'when' then answer end) 'when' ,max(case when question = 'who' then answer end) 'where' from tbl group by id
Gosh. Warehouse that shit.
ah okay. It always becomes crystal clear when we see the data. YOu *may* want to build a separate table of MachineConfigurations and use an identity key on that. Though it is not entirely necessary for this to work. But if this is an ongoing system that you are building and adding new machines and events to daily, it might come in useful. I'm not sure what SQL system you are using so your syntax may vary but here's one way to get the data out. (If you don't go the separate table with ident key method): select Ram, CpuSpeed, sum(NumEvents) from tablemachinIDs a inner join TableFacts b on a.machineID = b.Machineid group by Ram, CPUSpeed order by Ram, CPUSpeed 
Now, if you DO want to maintain a separate table of configurations you could create it thusly Create table MachineConfigs as (MachineType Integer Identity, RAm Char(10), cpuSpeed Char(10); insert into MachineConfigs (Ram, CPuSpeed) select Ram, CpuSpeed from TAbleMyMachineIds group by Ram, CpuSpeed (alternatively you can use Select Distinct to return the same rows) with an ident column, as you enter rows, it simply gets incremented automatically by the system. you don't worry about it or enter data into it explicitly. To really normalize this whole system you'd have 3 tables. TableMachineConfigs MachineType RAM CPUSpeed 1 2GB 1GHz 2 4GB 2GHz TableMachineIDs MachineID MachineTYPE 1234 1 5678 2 9000 1 TableFacts MachineID NumEvents 1234 5 9000 50 5678 100 This would mean that you'd have to maintain a MachineConfigs tables separately, and during your data input process your front end might ask the user to PICK one from the configs table when assigning a MachineType. Also be able to edit and add to that table. So finally, with the above 3 table configuration, youre query becomes. select a.MachineType, sum(b.NumEvents) as TotalEvents from TableMachineConfigs a inner join TableMachineIDs b on b.MachineType =a.MachineType inner join TableFacts c on c.machineID = b.Machineid group by machineType order by machineType 
Looking at this on my phone, but could be your where clause on the temp table. If it worked yesterday, maybe try your Where clause as: WHERE T.tdCreateDate = DATEADD( DD, DATEDIFF( DD, 0, GETDATE()), -2) If you run it as a select, does it return the value you want to update? Some background and sample data may help us. 
Beautiful, this helps a lot! Thanks so much.
You're concatenating strings, including user input, to create your SQL statements. This is the fast track to a p0wn3d website. You need to be using [prepared statements](http://php.net/manual/en/pdo.prepared-statements.php) (linked to the PHP docs only because that's what you're running) to interact with your database (BTW, specifying your database is requested in the sidebar). This will eliminate about 99.99% of your SQL injection risk. It will handle your punctuation properly and any other string input as well. Oh, and your database will thank you too (at least if it's SQL Server, can't speak to the others) because it'll be able to better create and cache query plans. If your setup is such that prepared statements aren't possible, fix it so that they are. ###DO NOT CREATE SQL STATEMENTS FROM USER INPUT WITH STRING CONCATENATION. EVER. Take your website offline (or at least this feature of it) until you have this fixed.
You may have misunderstood. I think what /u/Heofz is saying is to create the temp tables and define some indexes on them, then `insert into` them instead of creating them on the fly. For example, do `#templead` this way: create table #templead ( Id int, CONSTRAINT [pk_templead] PRIMARY KEY CLUSTERED ([id] ASC) ); insert into #templead SELECT DISTINCT A.id AS Id FROM [PRDREPORTS\PRD_COGNOS_RPT].[SALESFORCEPRD].[dbo].[LEAD] A LEFT JOIN [MRKT_RO].[BI_REPORTING_RO].[Marketing].[LeadActivity] C ON C.leadid = A.id WHERE A.createddate BETWEEN @StartDate AND @EndDate AND C.leadtype NOT IN ('Partial Lead','Test Lead','Duplicate Lead') AND isnull(C.leadroute,'') &lt;&gt; 'CLUB' But that distinct is going to kill you, no matter what. Is it *really* necessary? **Do not be afraid of temp tables. In a lot of cases, they can make for a significant performance *improvement*.** Do you know yet whether eliminating the temp tables will help? Do you have TempDB contention? Is TempDB on the fastest possible storage? What version of SQL Server are you using? Also: SET TRANSACTION ISOLATION LEVEL Read UNCOMMITTED Is this an OLTP system? If so, do you understand that this **will** get you dirty data and your results can't be trusted? Have you looked at your indexes? Execution plan? Tweaked your queries get the highest selectivity possible, as early as possible? Linked servers are a performance killer, you can't get around that.
Mostly because I had never used a merge statement :p This is great advice though, I'm essentially self taught (with about 2 weeks of actual training) so there are a lot of things I *should* be doing that I'm not currently doing.
Ah, yes I did misunderstand. That seems like it would increase performance but at the moment 4 minutes per month seems acceptable in terms of how it's performing without doing that. I'm not fond of using temp tables at all. The tables themselves are only about 500k, but they link to a few larger tables on BI's side that are in the millions. Not a lot of data, but it's organized very poorly. &gt;But that distinct is going to kill you, no matter what. Is it really necessary? Yes it is :( &gt;Is this an OLTP system? If so, do you understand that this will get you dirty data and your results can't be trusted? I'm not sure but can you please explain? I use that line to function as a NO LOCK because of the tables they touch are constantly in production. &gt;Linked servers are a performance killer, you can't get around that. I hate the way our data is housed. 
2 weeks in and you already know Merge. You're 5 or 6 years ahead of me. =)
You're on the right track, just need to rework the query a bit. I think a temp table would introduce some confusion here; there isn't really any sort of intermediate state where it would make sense. This will do what you're after: UPDATE post p JOIN post_user pu ON pu.post_id = p.post_id SET p.user_id = pu.user_id WHERE p.user_id IS NULL;
So... MySQL isn't my brand, but does this work? update post p join post_user pu on p.post_id=pu.post_id set p.user_id = pu.user_id where p.user_id is null ; 
&gt; I'm not fond of using temp tables at all. Get over it. They're **very** useful and can drastically improve performance in a variety of ways. &gt;I'm not sure but can you please explain? I use that line to function as a NO LOCK because of the tables they touch are constantly in production. OLTP = OnLine Transaction Processing. Basically, is the database actively used for transactional processing? Sounds like the answer is yes. NOLOCK is **extremely dangerous** and ~~can~~will result in bad data read by your queries - incomplete records, phantom records, and records that should exist but aren't returned. It is not a magic turbo button. Make sure your database is using the Read Committed isolation level and don't use NOLOCK or your first line ever again. Google "[randy knight nolock](https://www.google.com/search?client=safari&amp;rls=en&amp;q=randy+knight+nolock&amp;ie=UTF-8&amp;oe=UTF-8)" for a number of blog posts and technical articles (including example queries!) for more. Also have a look at [Read Committed Snapshot Isolation Level](https://www.google.com/search?client=safari&amp;rls=en&amp;q=read+committed+snapshot+isolation+level&amp;ie=UTF-8&amp;oe=UTF-8)
&gt;Get over it. They're very useful and can drastically improve performance in a variety of ways. I feel like they can hide things. If something goes wrong in a temp table I feel like it won't necessarily show up unless the reassembly's are fool proof, which isn't a design element I like at all. On the other hand I feel like if it's contained within a sub-query, or an open query, that if something goes wrong it will show up more prevalently in the report because it's running as a function of the same query/retrieval -- this being especially true when you lock the table to run it. TBH, I'd rather house the data more efficiently then have to rely on temp tables, but I'm somewhat new to SQL and come from a programming background so perhaps it's unfounded. &gt;OLTP = OnLine Transaction Processing. Basically, is the database actively used for transactional processing? Sounds like the answer is yes. No it isn't. It is used for very regular updates but not "real time", although there is an OLTP that exists where I can query to compare the two sets. Our copy is a replication, which annoys me because if something is changed in production then my data set will change immediately... adding new columns, or updating the records with new values in certain areas. This is an area I'm working to improve for obvious reasons. Then on top of that layer we have our BI data, which is very reliable from a financial perspective but not so much from the perspective of the OLTP in terms of marketing data. &gt;NOLOCK is extremely dangerous and canwill result in bad data read by your queries - incomplete records, phantom records, and records that should exist but aren't returned. It is not a magic turbo button. I'm not using it as a turbo button. The tables are constantly being used by other processes which can fail if the table is locked for a query. This particular query started off running at an hour and a half per month (*16 months, for example) and has since been taken down to 4 minutes for 16 months. Still finalizing it and then going to make multiple versions of it that does different groupings... save a SP and have it schedule to run once a month into a table that i can compare against last months run. These iterations will be about 8000 rows each.
Use Google's libphonenumber to validate and format numbers. You should store then in a standardized format such as E.164 If you use Postgres then you can use the javascript version of that library directly in the database with the PL/V8 language
So you want to search for where 2 events overlap and the overlap lies within a certain period? So I might ask, "Do I have any event clashes next week?" In that case you really want to define a period and then make sure that, in addition to event1 overlapping event2, that event1 overlaps the time period and event2 also overlaps the period. SELECT * FROM EventTable E1 INNER JOIN EventTable E2 ON E1.StartTime &lt; @endTime --E1 overlaps desired time period AND E1.EndTime &gt; @startTime AND @startTime &lt; E2.EndTime --E2 overlaps desired time period AND @endTime &gt; E2.StartTime AND E1.StartTime &lt; E2.EndTime --E1 overlaps E2 AND E1.EndTime &gt; E2.StartTime AND E1.EventID &lt;&gt; E2.EventID --dont join on the same event 
You attach it. Not restore. You restore from BAK files.
Select year ,SUM(CASE WHEN SEX = 'm' THEN 1 ELSE 0 END) M ,SUM(CASE WHEN SEX = 'f' THEN 1 ELSE 0 END) F from table Group By year You should also look at PIVOT which should be more efficient, but is a bit harder to wrap your brain around. 
Thanks. That's quite a lot better :-)
&gt;I'd rather house the data more efficiently then have to rely on temp tables, but I'm somewhat new to SQL and come from a programming background so perhaps it's unfounded. Stop looking at temp tables as a crutch and instead look at them as a tool. Even if you have the "most efficient" way of housing the data (spoiler alert: there is **never** one answer to this), you may have a query that will be inefficient in accessing it "raw" but can be improved immensely with a temp table. For example, earlier this year I had a job that timed out at 10 minutes but after decomposing the problem into two steps using a temp table, I got that down to 90 seconds. Since you have a programming background (as do I), you're accustomed to working problems by telling the program *how* to do things, and operating in iterations and sequential steps. SQL is a completely different animal - the language is declarative and the engine works in *sets**. You tell the engine *what* you want and let it figure out *how*. Part of that process is often "ok, grab the records that meet these criteria. Now take that set (saved to a temp table) and apply it to these other sets." If you start trying to "outsmart" it, you will usually lose. &gt; The tables are constantly being used by other processes which can fail if the table is locked for a query. If you database is configured properly, a select-only query shouldn't be locking the table in the overwhelming majority of cases.
how about this: select year, [m], [f] from ( select year, sex from x )s pivot ( count(sex) for sex in ([m], [f]) )p --build table and insert data CREATE TABLE x ( year int , sex varchar(20) ); INSERT INTO x (year,sex) VALUES (2001,'m'), (2000,'m') , (2000,'f') , (2001,'m'), (2002,'m') , (2002,'f') , (2002,'m') , (2001,'f') , (2003,'m') , (2003,'f'), (2003,'f'), (2003,'m') 
Thanks! It worked. Something so simple too :) I'll remember this for next time.
Is the distinct part necessary because the ids in [PRDREPORTS\PRD_COGNOS_RPT].[SALESFORCEPRD].[dbo].[LEAD] are not unique or because of the left join? If its just because of the left join you can get rid of the first temp table like so: SELECT datepart(mm,L.createddate) AS Month ,datepart(yy,L.createddate) AS Year ,isnull(L.lead_source_code__c,'None') AS LSC ,isnull(L.campaign_code__c,'None') AS CC ,isnull(L.responsechannel__c,'None') AS Channel ,count(distinct X.id) AS Leads ,isnull(SUM(Z.setcount),'0') AS Settled ,isnull(SUM(Z.setproduction),'0') AS Production INTO #templeadSet FROM [PRDREPORTS\PRD_COGNOS_RPT].[salesforcePRD].[dbo].[LEAD] L LEFT JOIN [PRDREPORTS\PRD_COGNOS_RPT].[SALESFORCEPRD].[dbo].[ACCOUNT] A ON L.convertedaccountid = A.id LEFT JOIN [PRDREPORTS\PRD_COGNOS_RPT].[SALESFORCEPRD].[dbo].[OPPORTUNITY] O ON A.id = O.accountid LEFT JOIN [MRKT_RO].[BI_REPORTING_RO].[Production].[PremiumProduction] Z ON O.policy_number__c = Z.policyno WHERE L.createddate BETWEEN @StartDate AND @EndDate AND EXISTS ( SELECT 1 FROM [MRKT_RO].[BI_REPORTING_RO].[Marketing].[LeadActivity] LA WHERE LA.leadid = L.id AND LA.leadtype NOT IN ('Partial Lead','Test Lead','Duplicate Lead') AND LA.leadroute &lt;&gt; 'CLUB' AND LA.leadroute IS NOT NULL -- Don't know if you need this as it may have been to do with the left join? ) GROUP BY datepart(mm,L.createddate) ,datepart(yy,L.createddate) ,L.lead_source_code__c ,L.campaign_code__c ,L.responsechannel__c ORDER BY datepart(yy,L.createddate) ,datepart(mm,L.createddate) ,L.lead_source_code__c ,L.campaign_code__c ,L.responsechannel__c 
In stackoverflow example, he seems to swap row titles with column headers. Mine is a bit more dynamic. Thanks anyway, i'll look into it.
Thanks, i will try this.
As i've mentioned, i did search other posts but i couldn't really understand what was happening. Noob here. Thanks for this solution though, i'm gonna try and decipher it now haha.
I'm using MySql 5.5.40. Hope that helps
Hey, so i tried your solution but the 'answer' needs to be dynamic. Thank you anyway.
Hey johnwalkersbeard, i tried your solution in sqlfiddle but am getting an error. Here's what i'm doing: CREATE TABLE myTable1 (id INT, question VARCHAR(255), answer VARCHAR(255)); INSERT INTO myTable1 VALUES (1,'who','me'), (1,'when','yesterday'), (1,'where','dont remember'), (2,'who','him'), (2,'when','tomorrow'), (2,'where','here'); SELECT * FROM myTable1 PIVOT (MAX (answer) FOR question IN ([who], [when], [where] ) ) AS pvt And i'm getting this error: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version ... Using MySQL 5.5.32 Any ideas?
Case can be more wordy but has more flexibility, and having figured out exactly how the pivot should be done the code can be generated to some extent. Most pivot functions I've seen only cover the most common math aggregations so reshaping text generally requires case.
anecdotally, most of the programmers I've worked with are more familiar with CASE and know how to support it. so I tend to follow this in order to pass off the task of managing code I wrote to someone else, with minimal training. also, CASE statements tend to execute quite a bit more quickly. but that said, the smartest thing to do is use a query analyzer to see which is more efficient.
From Access SELECT distinct Year , (select count(sex) from table t2 where sex = 'M' and t1.year = t2.year) as M , (select count(sex) from table t2 where sex = 'F' and t1.year = t2.year) as F FROM table t1
I don't care for pivot because of the disconnect between pivot and unpivot. 
could you spell out the syntax for me?&lt;3
You might end up wanting one of the paid versions, but [QweryBuilder Express](http://www.werysoft.com/EditionComparision.aspx) is decent if you're just looking for a better editor with some intellisence/autocomplete functionality.
It depends on the situation or desired output on which way I use. I generally go with whichever doesn't require sub-selects or the cleanest code if it could be done either way.
Generally you also need the LDF file when attaching an MDF, but if you only have the MDF there are a few ways to attach this and rebuild the Logs. Pinal Dave shows these methods here: http://blog.sqlauthority.com/2010/04/26/sql-server-attach-mdf-file-without-ldf-file-in-database/ 
crappiest article posted here in months q2. "solution" will fail in every dbms because of the period q3. "solution" is not valid outside of microsoft q4. see 3. q5. syntax error on the microsoft "cute" quotes q6. see 5. q9. question is bad, answer is incorrect q10. see 3.
I completely disagree with Q1. Using a Subquery like that is rather inefficient, especially if you're querying a large table which may have horrible indexing. I'd use a CTE instead: WITH Tbl (RowID, ID, Salary) AS (SELECT ROW_NUMBER() OVER (ORDER BY Salary Desc) AS RowID, ID, Salary FROM Employee) SELECT Salary FROM Tbl WHERE RowID = 2; The query in the article does seem simpler, but this I believe will give better performance. If I were an employer I'd rather someone use this because it not only shows the potential employee knows the question but also knows how to properly use a CTE and Ranking functions (or at least this one). And for Q9, wouldn't this only apply if the collation was one that's case sensitive? I've only seen one such database in the wild that did this (PeopleSoft), so unless the employer had databases that were in fact set as case sensitive I don't know that this is even relevant. The other questions seem VERY basic, and I'd almost be insulted if I were asked these during an Interview for a DBA position.
ok, I thought it was me..very easy, but then again, maybe they are seeing how you approach writing queries rather than depth of knowledge.
&gt;Most Frequently I don't think those words mean what you think they mean.
I apologize, I am using Oracle. It looks like I would use the LISTAGG function because I am on 11g but I honestly have no idea how to use it. I tried placing it in my original code and the system says it isn't finding the FROM clause in the right spot.
I think in your instance it would be something like this (remember I don't know Oracle syntax that well). SELECT table2_id ,table2_cat_title ,table2_cat_no ,table1_cat_desc ,table2_att ,table3_id_desc ,LISTAGG(table4_comments,',') WITHIN GROUP (ORDER BY table2_id) "Comments" FROM table1,table2,table3,table4 WHERE table3_id = table2_id AND table2_id = table4_id AND table2_id = table1_id AND table4_id = table1_id AND table4_cat_title = table2_cat_title AND table4_cat_no = table2_cat_no AND table4_cat_title = table1_cat_title AND table2_cat_title = table1_cat_title AND table4_cat_no = table1_cat_no AND table2_cat_no = table1_cat_no AND table2_att = 'ABC' GROUP BY table2_id ,table2_cat_title ,table2_cat_no ,table1_cat_desc ,table2_att ,table3_id_desc
9 is wrong because like is still case sensitive right? How would you write this query in MS SQL?
This should do it. Replace GETDATE() with "date from another table" if you need: update 1stTableTrace set TraceDate= CASE WHEN DATEPART(dw,(GETDATE())) = 6 THEN DATEADD(dd,3,2ndTable.DateOut)--Friday WHEN DATEPART(dw,(GETDATE())) = 7 THEN DATEADD(dd,2,2ndTable.DateOut)--Saturday ELSE DATEADD(dd,1,2ndTable.DateOut) END from 1stTableTrace inner join 2ndTable on 1stTableTrace.Id=2ndTable.Id where 1stTableTrace.TraceDate is null and 2ndTable.Status=ABC and 1stTableTrace.OtherStatusId=17 and 2ndTable.DateIn &gt;'2014-11-17 00:00:00.000'
you must be at wells fargo
Thanks, I didn't know /r/database existed. Actually, you're close; I modeled it after type 2 SCD then added &amp; update &amp; check field. Frankly, I don't think anyone would ask, but there are a lot of popular aggregators out there (IE CamelCamelCamel.com). They're good when they work, but when they fail (and they do fail), it's impossible to know it. I was looking to create something along the lines of a confidence interval. I'll x-post &amp; updates results here if there are any. 
Yeah, sorry about that. I'll add an edit for that.
question 9 is bad because it asks ... &gt; find all Employee *records* *containing* the word "Joe" ... but then checks only one column it is the "record" (implying all columns) that is bad answer is incorrect because it finds only names that ~start~ with joe, fails to pick up names that ~contain~ joe like i said, crappiest article posted here in months 
Not at Wells Fargo
Or use a cursor: declare @cmd varchar(50) declare cmds cursor for select 'drop database [' + Name + ']' from sys.databases where Name like 'ABCDir%' open cmds while 1=1 begin fetch cmds into @cmd if @@fetch_status != 0 break exec(@cmd) end close cmds; deallocate cmds
This one seems to work and I understand it fairly well lol. 
It's been a while for me and SQL, so I imagine newer database technologies have made for better solutions, but this is how I'd do it. Someone else might come along with a more optimized answer, so keep an eye on the thread. select user_time_log.user_id , logCount.entries , case when logCount.entries %2 = 0 then 'Even Count' when logCount.entries %2 = 1 then 'Odd Count' end as ClockInOrOut from user_time_log left join ( select user_time_log.user_id , count(*) entries from user_time_log group by user_id ) as logCount on logCount.user_id = user_time_log.user_id 
 SET SINGLE_USER WITH ROLLBACK IMMEDIATE; A database can be in Single User Mode or Multi User Mode (or a few other modes, but these two are the common ones). The latter is what you generally want, but if you're about to Delete the database you want to put it into Single User Mode so no one else is using it. And the Rollback Immediate basically tells the SQL Engine to not wait until any open transactions are committed, they're just rolled back instantly. MS SQL won't drop a database that has open transactions or processes. NOT IN ('distribution', 'MASTER','tempdb','msdb', 'model') Also Distribution, Master, TempDB, MSDB, and Model are all System databases and should not be dropped. Note, Distribution will only be there if you have Replication going, but the others should be present on all MS SQL installations. Here's more details on the System databases: http://msdn.microsoft.com/en-us/library/ms178028.aspx If you want to script this I updated my query to save the query to a variable then execute the query (execute is commented out). But if you plan on just running this ad hoc I'd suggest removing the Execute option. Dropping databases is a huge deal in MS SQL, so you don't want to accidentally drop the wrong ones. 
I really appreciate this, and I have never seen this done before! Where in sam hill am I suppose to re-inject my Where log_time between 11/17/2014 and 11/18/2018. I thought I was putting it in the right place, but it did not like it.
You can put the where statement in at the end, but remember that will limit your count. This doesn't matter as long as the check-in and check-out are always on the same day, but if someone checks in on 11/16/2014 but then doesn't check out until 11/17/2014, you can see how your odd/even would be off since you wouldn't have the check-in in your count from the day outside of your where clause.
I do this exact thing on our test system. Here's a quick rundown: Backups run in production every night at 9PM. At 1AM, I robocopy over the newest backup(s) to a folder(s) on our test system. I then run this script ([located here](http://www.mssqltips.com/sqlservertip/1584/auto-generate-sql-server-restore-script-from-backup-files-in-a-directory/)) that grabs the newest file and restores it. Set this up as a scheduled job in SQL Server. If you want to do this for another database, just change the database names in the script and the paths the data is located. I think that will get you in the right direction. It's been solid for me and has eliminated a ton of headaches and additional work. Best part is, you never have to guess how old the data is since it should always be as old as your last production backup. The only issue was that I couldn't restore from a file share that I had setup in production, not quite sure why, but that's where robocopy took over. Robocopy also deletes any backups that are X days old so the folder size doesn't grow out of control. EDIT: Here's another link I've found [here](http://www.sqlwebpedia.com/content/automated-restore-database-last-full-backup). I have not used this one but it's worth a look. Another thing. I remember having issues with the script I used, but there is a fix in the comments. It deals with using EXEC instead of PRINT in the TSQL script. If you have any questions, let me know and I can explain further.
Well the plan is to eventually turn this into an SSRS report and I'll probably just end up using =today anyways, this is solely to keep track of who is clocked in, where they are, whether they are or not clocked in. Since people cannot be clocked in during the past, I'm hoping that won't be a problem? haha. Thanks again!
It might help to see the actual BCP command and/or the BCP format file, as well as the table definition (and maybe the 1st couple of rows on a data file).
now all I have to do is understand what you did here so that I may understand it myself, so I don't always have to come around here and, "Cheat."
What database server are you using? I'll assume MS SQL unless you say otherwise. In MS SQL Soundex is really the only 'fuzzy logic' function I know of for comparing strings (someone chime in if there are others). I believe Full Text Search may also have some fuzzy search features, but I can't say I've ever used FTS for this. If this isn't working you might have to resort to writing something custom as a CLR which may use some of the C# text comparison features. If you really want your reports to be accurate a lookup might be your only option.
Wow, are you me? haha This is EXACTLY what I do, with the exception of the script that knows how to grab the newest one - which is exactly what I'm looking for - thanks for that. Since I go over a slow link (20Mbs.. wow since when is that slow?) I have to start my robocopy as soon as possible, so I have tsql fire off the robocopy script as soon as the backup job finishes. As it sits it takes nearly 7 hours to move the data from point a to b. Can't wait to try this out. Again, super thanks 