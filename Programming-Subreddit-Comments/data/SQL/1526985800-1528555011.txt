Did you declare the variable as a table?
I didn't because I haven't learned how to operate on tables like that yet, I don't understand the concept behind it because I had to define an actual table with structure and stuff and what do i need to do next? populate the table with the query and send the variable to the table? I just can't understand the concept.
Honestly it's the 6 month thing, instituted because of data security concerns not involving me otherwise I wouldn't have a job. I enjoyed working there throughout the internship though, just wish I could start out full steam ahead with SQL
I'm assuming this is homework. Google many-to-many relationships.
Sure but I am not linking multiple attributes together into a single table. Let's say I have this rectangle room with doors 1,2,3 on one side and rooms a,b,c on another. Is t here a better way to organize the distances between each door and room than just listing out: A1,A2,A3,B1,B2,B3,C1,C2,C3. Does that make sense?
I ended up going with XML and then concat, seemed like an easier way of doing it then using table, temp table and all that other mess.
^The linked tweet was tweeted by [@WarbyParker](https://twitter.com/WarbyParker) on May 21, 2018 16:19:33 UTC (13 Retweets | 49 Favorites) ------------------------------------------------- Fun fact: our Data Science team has partnered with [@codecademy ](https://twitter.com/codecademy ) to develop a course on SQL! More info here: [https://warby.me/2IvpErz](https://warby.me/2IvpErz) [https://twitter.com/Codecademy/status/995019103793483777](https://twitter.com/Codecademy/status/995019103793483777) ------------------------------------------------- ^^• Beep boop I'm a bot • Find out more about me at /r/tweettranscriberbot/ •
You could maybe assign x,y coordinates for each entry and dock door and perform a calculation to figure out the distance? I know there's a built in function in SQL server that can calculate distances between lat and long values. But it may not be accurate enough for points inside of a warehouse. 
My ETL needs are largely ad-hoc but I'm a fan of PostgreSQL and the psycopg2 library that Python uses. We primarily use SQL Server at my work though, so I end up using pyodbc for that but if it were up to me I'd use the former for most things... but the differences are admittedly pretty mild. You could also consider something like SQLAlchemy if you want something that has the potential to be platform agnostic. Might be worth checking out. You can either use it just as an ODBC or with their ORM.
It's an excellent first step. I highly recommend implementing some Python into your day to day if you can. Now that you have your foot in the door, perform well and you can nudge your way into the position you desire. Just remember to keep learning. Don't stagnate!
Yes, calculating the distances (either on retrieval, or via a computed column on the table) and not storing them as their own value is the way to go. When the rooms and doors get relabeled (with A now meaning B, etc.), you'll save yourself from redoing all those calculations
This is not a gift to anyone who will ever inherit your code. You should always go back and format things properly. I keep simple comments that describe from a high level what a section of code does, why it does it, and what data dependencies are required for it to function as expected. I tend to use a new line for everything and tabs to indicate where it fits in a hierarchy, but I will sometimes break this rule for simplicity. For example: `(Z2.Client = Z1.Client AND Z2.SLALabel = Z1.SLALabel)` that is on one line because it simply makes it easier to read for someone else and lets them see immediately that its one enclosed piece of the nest, but the rest of the nest is one line per string, with an independent line for each closing paran to indicate to a new person how many sections there are and where they live in the hierarchy. Adding white space helps me visually see this, but IMO it doesn't belong in production like this and should be condensed down as shown in my original example. I don't know about others but for me if I start working on some new code that is obviously formatted and has an internal schema (all caps for operators, indented, etc., then I tend to trust it more. When I inherit a wreck of code that has multiple comments from multiple people and there is no obvious formatting schema that the multiple developers have used... then I don't trust it at all and generally will start by rewriting the entire thing with formatting, revising any existing comments and validating that they are true (code functioning as expected) -- then I will add whatever modification I need to it and send it back off to production.
Interesting. I hate when I have to un-condense something just to see what is going on. I much prefer it to be sectioned and indented. 
&gt; Does that seem like a decent plan? Sorry, but no. If I understand it correctly, your enterprise already has a mix of technologies addressing multitude of business goals. Whoever is in charge decided to re-design your architecture in some way with certain goals in mind. Instead of coming up with a departmental solution once again, you should bring/highlight your scope and concerns to the architect/solution engineer and project manager in charge (or accountable for) this redesign. LOE and costs for migrating of your infrastructure and workflows has to be included in the overall project plan and aligned with the rest of the redesign scope. Once that is done, the solution engineer and PM will help you figure out the budget/time constraints and the solution that fits the new architecture.
I see what you're saying yeah the values are correct on my end so that's good. I will have to research the data closer though because even your script returned no results. So I am starting to believe that no student has completed all 4 of these courses.
So you only want the value to show up when it's ONGRADE? If yes then replace sheets with SUM\(CASE WHEN grade='ONGRADE' sheets else 0 end\)
If you are in a linux environment and planning on writing in python, you should consider Apache Airflow for your ETL framework if things are more complex than a few scripts. 
&gt; So I am starting to believe that no student has completed all 4 of these courses. sounds plausible
I've been dabbling with Python the last couple weeks, I just don't do enough high level analysis yet that I would need Python for. I've really been trying to find things to use it for though
Thanks for sharing. I'd like to setup on a Mac...which of the options would you recommend I select via the page you provided? My goal is to take my tracking data and import it into a table to query for results. thanks!
Left a Data Analyst postion \(bad company\) \-\&gt; Business Analyst or Data Analyst
Both options will work on Mac, they run on browsers. Just run it from Chrome. As far as actual DB instances, LiveSQL is the only one that will allow you to keep your DB alive, virtual labs goes away after 2 hours usually. You just have to get used to PL/SQL
I think the problem with that approach is when you have really long chunks of code that are 1000+ lines when condensed... you're now talking about expanding those into maybe 2000+ lines making it significantly harder to navigate through, which is why I tend to condense everything as best as possible (as in my original example), and then as needed if I need to modify that join I can add some whitespace, check the code out, modify it, and then shrink it down and be done with it.
If the cert is by an honorable vendor like Microsoft or Oracle they are nice to have. In my opinion, those certs are the equivalent of 1-2 college courses. You will need to do real effort to learn the material. 
Where?
The Microsoft and Oracle certs are the only worthwhile ones to put on a resume. If you pair that with some solid Excel skills and you interview decently, it should be more than enough to land an entry-level Jr. Analyst or data developer role, then you can start racking up real-world experience. As far as which to choose - database technologies are strangely regional. Oracle dominates certain metropolitan areas, Microsoft dominates others, so look at job postings to see what skillset is more in demand in your area. I don't have any Oracle certs, but I was able to self-study for the Microsoft 70-461 and pass it with just the textbook and some old exams I found online, but it took some discipline - your milage may vary. Hope that helps, and best of luck.
Why? Boston Area.
Bad bot
Are you sure about that? Because I am 99.99996% sure that Murica4Eva is not a bot. --- ^(I am a Neural Network being trained to detect spammers | Summon me with `!isbot &lt;username&gt;` |) [^Optout](https://www.reddit.com/message/compose?to=-spamfighter-&amp;subject=!optout&amp;message=!optout) ^| [^Original ^GitHub](https://github.com/SM-Wistful/BotDetection-Algorithm)
This was extremely helpful, thanks!
I realize that, and thought it might be a nice basis to build off of buy learning and having some physical evidence that some courses were taken as well. Thanks for the help! 
Your mac most likely has [SQLite](http://sqlite.org/index.html) preinstalled, which is a nifty little tool. 1. In Excel, export (save as) the spreadsheet in CSV format. Make sure you delete empty rows and don't have combined cells and stuff like that, it can cause problems... 2. Open Terminal app, and type `sqlite3 test.db` (test.db is the name of the database file which will contain your data) 3. Type `.mode csv`, then `.import Documents/mydata.csv mydata`, assuming you exported to "mydata.csv" in your Documents folder. 4. Type `.schema` to see the structure of the autocreated table, and `SELECT COUNT(*) FROM mydata` to see how many data rows you have, and go from there.. http://www.sqlitetutorial.net/sqlite-import-csv/ 
If it's in report builder, use an iif statement in the sheets column. Something like: =iif(Fields!Grade.Value ="ONGRADE", Fields!Sheets.Value,nothing)
Nothing wrong with comments or the occasional blank newline where things get hairy (I always blank line between major clauses anyways.) Previous response had like 2 and 3 new lines at a time, which starts to defeat the purpose. Whenever I inherit some hard-to-look-at SQL , I use poorsql.com and it is a life-saver!
Probably better to handle that in SQL rather than use SSRS to do it, no? 
Maybe, but if the data source is used for multiple reports, it makes more sense to adjust the formula rather than SQL.
Hi, thanks for your reply. Shared memory were all enabled already. Just enabled named pipes and TCP/IP protocols. Restarting now.
the option above is pretty good, but alternatively you could copy/paste everything into a google sheets document and use =QUERY(). between the parenthesis you can use sql
The datasource is either a view/table, or a custom SQL snippet. Either way you can add a snippet and just feed that individual report a query that does the work so that it can be easily modified without having to take the report apart, etc.
Oh that's fucking beautiful. I have a rather pedantic structure to my format that was taught to me at my first job and it has become somewhat of a preference, but something like this is great. Thanks!
Yeah, everyone's got a style and some are more tolerable than others (yours seems tolerable, btw :) ) I've got some colleagues that make this tool a must-have. It's also really good for de-fucktifying SQL that Microstrategy writes to figure out what's going on under the hood there. 
No bot! No bot! You're the bot!
Ahh, I'm nesting procedures before they get that long. I really appreciate the perspective. I'm in a self-taught lone ranger situation, so I'm always open to breaking potentially bad habits.
Cool, I'll definately check it out. I might be replacing my bookmark to www.dpriver.com. I use that when I need to check out something captured in profiler, just to get it at least vertical, even though I don't love the options.
https://github.com/dinedal/textql can install it with brew: brew install textql Example usage: $ textql -header -sql 'select Product, COUNT(*) as ct from customer_products where Status="Active" group by Product order by ct desc' customer_products.csv
I could see using a tool like this as a team standard across the board.
Get a job as a business analyst where sql is involved. Applying your existing knowledge to solve business problems will teach you how to use it functionally. 
‘Twas a stupid joke, only because you posted the same thing three times and it seemed ad-like. Carry on, don’t mind me. 
another vote for SQLite. Once you figure out how to import the first one, the rest will go through like butter. And it's cross platform so you can easily move that db file to a windows or linux box. 
Hi dan_au, Using your proposed syntax, is there a way I can put specific text like 'Can't find' if some of the string doesn't contain firstName? thanks again. 
You're right, I completely forgot, it's been some time since I worked with Report Builder
 HAVING COUNT(DISTINCT SUBSTRING(crs_cde,1,8)) = 4 In case a student took the same course multiple times to get a better grade.
good point
Anyone who helps you is only hurting you. Most people here are more than happy to help with homework/assignments as long as the questions are specific and some semblence of an attempt was made.
Oh yes of course! I have been on and off this for a week now yet I can't get my head around it. I just really need some help. 
Without knowing the specifics of your setup, perhaps consider lessening the dependency on DWH/ETL as a whole? DWH and SSIS packages can be abused, and can be tough to develop, maintain and debug. Maybe you could replace some of your reporting by simple SQL extracts (automated and executed by batch jobs/scripts), and replace some of the SSIS jobs by regular batch jobs/scripts written in Python. Example: If you have a SSIS package that parses a CSV file on a file share, transforms the data and stores it to a table, you could easily write that in Python instead. You'll be less dependent on SSIS, and more developers on your team might be able to extend and debug the Python script than the SSIS code. I'm just speculating here, but perhaps it might help.
At work I wrote an SQL script to basically generate a concatenate statement cell which converts a row in Excel in into a SELECT... UNION ALL statement. Yes there are better ways of doing this but I just like putting the data in this way (and my datasets are not exceptionally big so Excel wouldn't die). This works in SQL Server not sure about other database environments: DECLARE @char VARCHAR(5) = 'A', --First Column @max_char VARCHAR(5) = 'BO', --Last Column @start_row VARCHAR(5) = '2', --First row of non-header data @text VARCHAR(MAX) = '=CONCATENATE("SELECT N''",' SELECT @text = @text + @char+@start_row+', ,"'' AS [",'+@char+'$1,"], N''",' WHILE @char &lt;&gt; @max_char BEGIN IF RIGHT(@char, 1) &lt;&gt; 'Z' SELECT @char = IIF(LEN(@char) = 1, CHAR(ASCII(@char) + 1), LEFT(@char, 1) + CHAR(ASCII(RIGHT(@char, 1)) + 1)) ELSE BEGIN SELECT @char = IIF(LEN(@char) = 1, 'AA', CHAR(ASCII(LEFT(@char, 1)) + 1) + 'A') END SELECT @text = @text +IIF(@char='AY','CONCATENATE(','')+ @char+@start_row+', ,"'' AS [",'+@char+'$1,"]'+IIF(@char&lt;&gt;@max_char,', N''",','", ') END SELECT @text +'" UNION ALL")'+IIF(@max_char &gt; 'AY',')','') Set the variables as required and always takes the first row as the header row. Put the output in the next column after your last column and then drag it down to every row in the excel sheet and you should be able to copy that into a SQL script to create a table of your data. 
 Login failed for user 'HomeNet\RealServer$' That's your clue. HomeNet\RealServer$ is the computer account, meaning your application is probably running under **NT AUTHORITY\LOCAL SYSTEM** or some other built in account. You're seeing login failed likely because the computer account isn't configured to be able to access the SQL instance (or at least, doesn't have permission to access the specified DB). You can either: 1. Set up a domain account specifically for this application which you then can create a SQL Server login for, then map the login to the DB you need it to have access to with the appropriate permissions. 2. Create a SQL Login for HomeNet\RealServer$ and do the same mapping as described above. Generally not something I'd recommend as this can lead to other things getting access to the DB. 3. If you really don't care about security (if it's a throwaway project or something) just create a SQL login for HomeNet\RealServer$ and assign it the SysAdmin server role. I would **really** not recommend doing this if this is going to be used in any kind of professional environment.
Hi there! Welcome. Someone may be able to help you here, but just a heads up that this sub is about SQL the language, rather than MSSQL administration. If you don't find your answer here, you might want to check out /r/SQLServer as well. 
ANSWER: WHERE a.pidm = :Filter_Menu.FILTER_MENU_EXAMPLE_PIDM Pretty much. Variables in Argos are prepared. You can stick that in whatever WHERE section you want or is best for your query. Argos isn't a great tool, but its functional. It also has help buttons at every portion of the screen everywhere that lead to the relevant document. The ABC button should give you the list of things you can use to filter. 
(Stanford SQL Course [https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/info] Try their courses on: Introduction and Relational Databases SQL Relational Algebra Statistical Learning
https://docs.docker.com/storage/volumes/ You need to mount your data on persistent storage outside of the docker container. 
i have done this, i mounted /var/opt/mssql to a folder i created for it. inside this folder multiple new folders have been created by the server and the data folder contains the MDF and LDF files. the problem is that the SQL server is somehow not loading these files on startup.
https://msbiskills.com/tsql-puzzles-asked-in-interview-over-the-years/ &amp; https://msbiskills.com/sql-puzzles-finding-outputs/ 
That's trivial in Python if you have access to that and want to try that route out. I can assist you with it.
Can you access the SQL server error log? 
yes i do but i dont really see anything that indicates where the problem comes from. 2018-05-23 08:29:39.82 Server Microsoft SQL Server 2017 (RTM-CU6) (KB4101464) - 14.0.3025.34 (X64) Apr 9 2018 18:00:41 Copyright (C) 2017 Microsoft Corporation Developer Edition (64-bit) on Linux (Ubuntu 16.04.4 LTS) 2018-05-23 08:29:39.82 Server UTC adjustment: 0:00 2018-05-23 08:29:39.82 Server (c) Microsoft Corporation. 2018-05-23 08:29:39.82 Server All rights reserved. 2018-05-23 08:29:39.83 Server Server process ID is 4124. 2018-05-23 08:29:39.83 Server Logging SQL Server messages in file '/var/opt/mssql/log/errorlog'. 2018-05-23 08:29:39.83 Server Registry startup parameters: -d /var/opt/mssql/data/master.mdf -l /var/opt/mssql/data/mastlog.ldf -e /var/opt/mssql/log/errorlog 2018-05-23 08:29:39.84 Server SQL Server detected 1 sockets with 4 cores per socket and 4 logical processors per socket, 4 total logical processors; using 4 logical processors based on SQL Server licensing. This is an informational message; no user action is required. 2018-05-23 08:29:39.84 Server SQL Server is starting at normal priority base (=7). This is an informational message only. No user action is required. 2018-05-23 08:29:39.84 Server Detected 9458 MB of RAM. This is an informational message; no user action is required. 2018-05-23 08:29:39.85 Server Using conventional memory in the memory manager. 2018-05-23 08:29:39.86 Server Large Page Allocated: 32MB 2018-05-23 08:29:40.20 Server Buffer pool extension is already disabled. No action is necessary. 2018-05-23 08:29:40.39 Server InitializeExternalUserGroupSid failed. Implied authentication will be disabled. 2018-05-23 08:29:40.39 Server Implied authentication manager initialization failed. Implied authentication will be disabled. 2018-05-23 08:29:40.40 Server Successfully initialized the TLS configuration. Allowed TLS protocol versions are ['1.0 1.1 1.2']. Allowed TLS ciphers are ['ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:!DHE-RSA-AES256-GCM-SHA384:!DHE-RSA-AES128-GCM-SHA256:!DHE-RSA-AES256-SHA:!DHE-RSA-AES128-SHA']. 2018-05-23 08:29:40.45 Server The maximum number of dedicated administrator connections for this instance is '1' 2018-05-23 08:29:40.45 Server Node configuration: node 0: CPU mask: 0x000000000000000f:0 Active CPU mask: 0x000000000000000f:0. This message provides a description of the NUMA configuration for this computer. This is an informational message only. No user action is required. 2018-05-23 08:29:40.48 Server Using dynamic lock allocation. Initial allocation of 2500 Lock blocks and 5000 Lock Owner blocks per node. This is an informational message only. No user action is required. 2018-05-23 08:29:40.50 Server In-Memory OLTP initialized on lowend machine. 2018-05-23 08:29:40.57 Server Database Instant File Initialization: enabled. For security and performance considerations see the topic 'Database Instant File Initialization' in SQL Server Books Online. This is an informational message only. No user action is required. 2018-05-23 08:29:40.59 Server Query Store settings initialized with enabled = 1, 2018-05-23 08:29:40.60 spid4s Starting up database 'master'. 2018-05-23 08:29:40.60 Server Software Usage Metrics is disabled. 2018-05-23 08:29:41.15 spid4s Resource governor reconfiguration succeeded. 2018-05-23 08:29:41.16 spid4s SQL Server Audit is starting the audits. This is an informational message. No user action is required. 2018-05-23 08:29:41.17 spid4s SQL Server Audit has started the audits. This is an informational message. No user action is required. 2018-05-23 08:29:41.33 spid4s SQL Trace ID 1 was started by login "sa". 2018-05-23 08:29:41.35 spid4s Server name is 'microsoft-mssql'. This is an informational message only. No user action is required. 2018-05-23 08:29:41.39 spid4s Starting up database 'msdb'. 2018-05-23 08:29:41.40 spid9s Starting up database 'mssqlsystemresource'. 2018-05-23 08:29:41.41 spid22s Always On: The availability replica manager is starting. This is an informational message only. No user action is required. 2018-05-23 08:29:41.42 spid22s Always On: The availability replica manager is waiting for the instance of SQL Server to allow client connections. This is an informational message only. No user action is required. 2018-05-23 08:29:41.43 spid9s The resource database build version is 14.00.3025. This is an informational message only. No user action is required. 2018-05-23 08:29:41.46 spid9s Starting up database 'model'. 2018-05-23 08:29:41.55 spid19s A self-generated certificate was successfully loaded for encryption. 2018-05-23 08:29:41.56 spid19s Server is listening on [ 'any' &lt;ipv6&gt; 1433]. 2018-05-23 08:29:41.57 spid19s Server is listening on [ 'any' &lt;ipv4&gt; 1433]. 2018-05-23 08:29:41.57 Server Server is listening on [ ::1 &lt;ipv6&gt; 1434]. 2018-05-23 08:29:41.58 Server Server is listening on [ 127.0.0.1 &lt;ipv4&gt; 1434]. 2018-05-23 08:29:41.59 Server Dedicated admin connection support was established for listening locally on port 1434. 2018-05-23 08:29:41.62 spid19s SQL Server is now ready for client connections. This is an informational message; no user action is required. 2018-05-23 08:29:41.85 spid9s Polybase feature disabled. 2018-05-23 08:29:41.85 spid9s Clearing tempdb database. 2018-05-23 08:29:42.65 spid9s Starting up database 'tempdb'. 2018-05-23 08:29:42.71 Logon Error: 18456, Severity: 14, State: 5. 2018-05-23 08:29:42.71 Logon Login failed for user 'mqtt'. Reason: Could not find a login matching the name provided. [CLIENT: 192.168.178.10] 2018-05-23 08:29:43.01 spid9s The tempdb database has 1 data file(s). 2018-05-23 08:29:43.02 spid24s The Service Broker endpoint is in disabled or stopped state. 2018-05-23 08:29:43.02 spid24s The Database Mirroring endpoint is in disabled or stopped state. 2018-05-23 08:29:43.05 spid24s Service Broker manager has started. 2018-05-23 08:29:43.06 spid4s Recovery is complete. This is an informational message only. No user action is required. 2018-05-23 08:29:44.05 spid51 Attempting to load library 'xpsqlbot.dll' into memory. This is an informational message only. No user action is required. 2018-05-23 08:29:44.06 spid51 Using 'xpsqlbot.dll' version '2017.140.3025' to execute extended stored procedure 'xp_qv'. This is an informational message only; no user action is required. 2018-05-23 08:29:44.43 spid51 Attempting to load library 'xpstar.dll' into memory. This is an informational message only. No user action is required. 2018-05-23 08:29:44.52 spid51 Using 'xpstar.dll' version '2017.140.3025' to execute extended stored procedure 'xp_sqlagent_notify'. This is an informational message only; no user action is required. 2018-05-23 08:33:07.24 spid53 Attempting to load library 'xplog70.dll' into memory. This is an informational message only. No user action is required. 2018-05-23 08:33:07.32 spid53 Using 'xplog70.dll' version '2017.140.3025' to execute extended stored procedure 'xp_msver'. This is an informational message only; no user action is required. 2018-05-23 08:34:47.52 spid57 Using 'dbghelp.dll' version '4.0.5' 
You were absolutely soon on by the way. Created a new login and then linked that to the new dB login I created. And voila! Worked! Just saw this message. Only wish I saw it sooner. Ha! Learnt something new about dbs now though. Even when I was thinking of making a shift to NoSql. Nice! Thanks again guys!!! Really appreciate it!
Hi there! Welcome. Heads up that this sub is about SQL the language, rather than MSSQL administration. If you don't find your answer here, you might want to check out /r/SQLServer instead.
Add a healthy fill factor so that tables can go longer without maintenance.
I know, and whoa, I see that now, I don't know how that happened.
I probably am. Gonna need to touch up on my vocab a bit.
Nah don’t worry about vocabulary. You just need to find the solution that works for you: how is that going? Any updates?
Not really, actually. The question came from a brainstorming session between myself and a former professor who I was assisting in re\-structuring a course for next spring. We didn't have anything more than some notes on a chalkboard at the time. He was wondering whether or not storing integer values to represent the datetimes would be worth it, and I took it upon myself to direct the inquiry here. Not sure if he's made any more progress on that since I posted the question, but I'm curious to know. I believe the last I heard from him, he didn't think standardizing would be appropriate here. I guess he was right!
Gotcha. It certainly is a viable option if you’re just storing the data as an archive along with a bunch more data, and never really accessing it regularly. But if the data will be in flight a lot (reporting, ad hoc, app, etc.) then it seems like it will be more trouble than it’s worth. A good resource for theoretical questions is DBA.stackexchange.com . They love this kind of stuff.
Yeah, that's what I realized after reading the original comment. Adding that additional complexity would probably only make things more difficult in the long run. The class this would be designed for is strictly a database management class, not a development\-based one. Pretty certain that the main concern of the class would be accurately collecting and inserting the data, maybe doing some queries as homework assignments as well. Won't be moving around too much, but why make something more complex than it needs to be? Definitely bookmarking that page. Thanks for the reference!
Not a course, but excellent resources: https://modern-sql.com/ https://use-the-index-luke.com/ 
So you want to compare a textbox from Tablix 1 to a textbox in Tablix 2 and display the result of that comparison in Tablix 3? Am I understanding this correctly? I may be wrong, but I don't think you can compare values across different tables like that. You can use Lookup to reference other datasets, but in this case I think your best option is to rework your dataset rather than trying to accomplish this in the report.
Yes, each of the two data I want to compare is in a different dataset. And each tablix is different, with the 3rd tablix being where I want to have the result of the comparison. 
I recently found these: https://www.edx.org/course/querying-data-with-transact-sql https://www.edx.org/course/developing-sql-databases It says they're free and officially endorsed by Microsoft. Anyone have experience with them?
Mssql? Try using online indexes with SQL enterprise yet?
Just up your threshold for REORG vs REBUILD so it reorganizes 100% of the time. Reorgs are an online operation.
But your script is doing reorganize indexes as well right? Are you at the fragmentation threshold to do a rebuild? From what I understood index rebuild with online is a non blocking operation. How much time would it take to not even try reorganize and go straight to rebuild on the specific tables you are having blocking on?
_ but why make something more complex than it needs to be?_ By George, (s)he’s got it!
start by un-noobing yourself a little, maybe? what you are describing is a user interface. https://en.wikipedia.org/wiki/User_interface https://en.wikipedia.org/wiki/Table_(database)
If SQL wasn't loading up your files, it wouldn't have started. You likely don't have the data persisted to the container like you think you do, and are not persisting changes to the system databases. 
SSD's? 
If you're doing online index rebuilds, you shouldn't be getting blocking.
There are dozens if not more relevant results to solve this when doing a simple search. What have you tried, what errors are you getting? What rdbms?
[https://domainwebcenter.com/how\-to\-convert\-julian\-date\-to\-gregorian\-in\-ms\-sql/](https://domainwebcenter.com/how-to-convert-julian-date-to-gregorian-in-ms-sql/) Would this be applicable? 
Have you tried Google? I found tons of relevant answers by googling "SQL JULIAN TO DATETIME". Also, MMDDYYY is not valid. Should be MMDDYYY. 
&gt;SQL JULIAN TO DATETIME what do you mean by this? mmddyyy? a little confused 
&gt;SQL JULIAN TO DATETIME im a programming noob... i'm assuming each rdbms has its own built in functions? is that why you ask?
You're a lot confused 
I meant what have you tried in SQL. yes, the solution will often depend on what flavor of SQL you are writing.
&gt;Also, MMDDYYY is not valid. Should be MMDDYYY. for sure... :\) what do you mean by this? can you please clarify? Also, MMDDYYY is not valid. Should be MMDDYYY.
&gt;[https://www.w3schools.com/sql/trysqlserver.asp?filename=trysql\_func\_sqlserver\_dateadd2](https://www.w3schools.com/sql/trysqlserver.asp?filename=trysql_func_sqlserver_dateadd2) &gt; &gt;Ive been playing around using this... i think i have an answer but still tinkering with it
 CREATE FUNCTION dbo.JulianToDate ( @sJulianDate int ) RETURNS DATE AS BEGIN DECLARE @jdate int, @ReturnDate date SET @jdate = 105031 SET @ReturnDate = CONVERT(varchar(10), dateadd(dd,(@jdate%1000)-1,dateadd(yy,@jdate/1000,0)),101) RETURN @ReturnDate END This will create a function in SQL server in the dbo schema.
thanks bro/sis
why use set instead of select here? (another dumb q, potentially)
The set keyword is used to assign the value of the CONVERT(...) statement to the @ReturnDate variable. If you were to write something like SELECT @ReturnDate after the SET assignment then you would get the value returned by the convert statement since that value is now contained within @ReturnDate. If you replaced the set statement with select then the ReturnDate variable would never be assigned a value and the function would return NULL since the ReturnDate variable would be NULL. (NULL is the default value for a variable that has yet to be assigned value)
From https://github.com/richard512
One has 3 Ys the other has 4 Ys, look again. Years have 4 digits like this YYYY, as in the year 2018.
You can create a variable to evaluate it by doing something like: DECLARE @checkdate DATE; SET @checkdate = (SELECT MAX([DateField]) FROM myTable); IF @checkdate &lt; GETDATE()-1 /* DO SOMETHING */ ELSE /* DO SOMETHING ELSE */ 
The same course you can find on mva. I’ve tried to learn something from these before developing exam, but I really hate the way it works - too much clicking ( next, next, 5min vid, next...) Try mssqltips instead.
From the parameters in OP, reorg at 10&amp;#37; rebuild at 30&amp;#37;, change to: reorg at 30&amp;#37;, rebuild at 100&amp;#37; or just modify proc to comment out rebuild entirely?
SAN
 SELECT MAX(DateField) FROM SomeTable It sounds like you just want to know if the largest date in the table was yesterday.
a couple of hints: 1. use subqueries/cte liberally in your homework - it'll make everything easier 2. Percentile is not percentage and ratio is yet another thing. Neither percentile nor percentage requires excluding anything from the total/sample.
When using **Group By**, everything you want to pull in the select that is not in the grouping has to be a calculation. (Min, Max, Count, etc) So If you want to pull series_id as a non aggregate you either have to add it to the Group By which will modify your grouping calculation or you have to use a Join, a Subquery or a CTE 
Let's introduce a couple of concepts here: 1. "table alias" - in your 'FROM' clause you can give name to a data set and use that alias to indicate which specific data set the column is supposed to come from, like so: select RRR.user_id from reviews RRR 2. You can think of subqueries as of a function/expression of a kind, and they can be correlated - that is, they can use the current dataset row attributes in them, like so: select RRR.user_id , (select max(rating) from reviews WOW where WOW.user_id = RRR.user_id) as WOW_rating from reviews RRR This should help you out, I think.
with lowest_ratings as ( select user_id, min(rating) as rating from reviews group by user_id ) select r.user_id, series_id, rating from reviews r inner join lowest_ratings lr on lr.user_id = r.user_id and lr.rating = r.rating The CTE finds the lowest rating and then you inner join the user_id and rating so that you get only those rows. 
Is it the same job, or are there different stages that run depend? AKA is there a potential that the process is calling itself. Alternatively, is there any schedule tasks on the actual server box that might be trying to kick this off independent of the job agent? Or an auto\-retry on failure that just has the symptom of failing on the second time. 
I'd use a windowing function. Find out if EndDate in row 1 is greater than StartDate in row 2.
&gt; Percentile is not percentage and ratio is yet another thing. Neither percentile nor percentage requires excluding anything from the total/sample. Then how else do I calculate what percentile my NAME is in? Isn't percentile calculated by # of records below X / total # of records * 100? So don't I need a COUNT of # of records below NAMES original count, then divide it by total count?
I'm surprised negative character class is not supported. But there must be a regexp replace function, right? I guess you could run that on the column and see which rows have anything left.
Something like.... (not tested in IDE, freehand written but should get you started ;) ) SELECT Datediff(day,T1.Start,T2.End) from table T1 JOIN table T2 ON T1.PersonID = T2.PersonID and T1.CompanyID != T2.CompanyID WHERE T1.Start BETWEEN T2.Start and T2.End Probably better to split out into a seperate query for working out end date overlap. Careful though, this is what is known as a fuzzy join (where its not VALUE X EQUALS VALUE Y its a range of values) and they can easily cause performance issues. Also a column storing data like this would be subject to change, if the data is static thats fine. However if the data moves at all you will need to maintain it (either by a trigger... eugh... or a job of some sort or even in the code that inserts into the table) If you can tune your query nicely and make sure the indexes are good it may be better to have your columns computed inside a view rather than as static data. Feel free to PM me code if you need a further hand but I would prefer you to have a good crack first (which is why I wrote the column/table names wrong) ;) This may also help: https://stackoverflow.com/questions/4490553/detect-overlapping-date-ranges-from-the-same-table?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa However I try to avoid OR statements in joins as they can confuse the query plan.
Hey, therealcreamCHEESUS, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Counting the counts? Heh, i guess you got me there - expected you to count names or getting rank by counts and mentally substituted 'sum of counts' in your formula. Still, you shouldn't exclude the value itself from the ranking - so, # of records _at or below X_ divided by total number of records. Anywho, this goes into the hint category - aggregate functions aren't functions on their own, you can think of those as going hand-in-hand with the 'group by' clause (or over/partition by). So, you'll need as many 'group by' levels as you have the levels of aggregation.
Yes its possible... an example (not tested) to get you started. UPDATE T1 SET ColumnStr = CASE WHEN LEFT(ColumnStr,5) = 'ABCDE' THEN LEFT(ColumnStr,5) + 'ZYX' CASE WHEN ColumnStr LIKE '%Cheese%' then 'Chips' CASE WHEN ColumnStr = 'Something else' THEN 'A thing' ELSE 'Tronald Dump' END FROM YourTable T1 Another example: https://stackoverflow.com/questions/15766102/i-want-to-use-case-statement-to-update-some-records-in-sql-server-2005?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa If you want it copied down via another field e.g. rownumber then you may find that putting the data into a temp table then processing that and finally updating the proper table joined to the temp would be a better approach. 
Yeah, I was planning on using a temp table. But there is no way to do this in a single CREATE TABLE statement, right?
Pretty much all rows returned. 
I'm SQL server, not mySQL but if you have the regex replace function, just replace any a-zA-Z0-9 with nothing and check the length is greater than 0. You may want to remove whitespace aswell depending on your server config and requirements. 
Not that I know of unless you just select into dbo.newtable but I suggest another approach as it will guess the datatypes and thats not always good. It can be nice and elegant to get some logic all in one statement but a lot of the time its better to break it apart. A view would be another option.
The problem is with the space character \s. Rows with spaces are returned.
I think I found a better solution: '[a-zA-Z0-9[:space:]]' Imagine ^ character between [ and a. What does [:space:] do? 
I'm not doing your homework but to get you started: Create the tables with primary and foreign keys (user, album and request tables). You have statuses, create a status table with an integer ID, store on the request status column the numeric ID that relates to the status table. It will store less data that way and its better design. Create procedures to insert (and delete?) into those tables. Create procedures to search/get data you need. The trick is... break it down into steps. Get your table design done first. Then get your inputs done so you can populate data. Finally get your outputs done (get request history etc). Make sure all your code uses parameters so its secure. 
In SQL server we have LTRIM and RTRIM that would work here as well as replace(col,' ',''). 
its not homework but thanks for explaining cheers\
No problem, sorry for the assumption :) 
You mean you're still getting spaces? If you want to identify an actual period, you would use \. What's your ultimate goal here, if you don't mind me asking?
Maybe MySQL regular expressions identify spaces that way instead of the otherwise universal \\s, which is strange.
Spot out characters in addresses that don't belong, such as !
This doesn't make sense. How does each team have the same opponent score?
So, if you look at the [documentation](https://dev.mysql.com/doc/refman/8.0/en/regexp.html), the MySQL regular expression flavor is bare-bones POSIX ERE, so no perl-style things like `\s` are supported. I found a mention that they switched from Henry Spencer's classic regexp library to libICU's in 8.0.4, which *does* support a lot of newer features. So I'm guessing you're using an older version.
I don't understand it either. What's the point of the "Result" column? Can you post the source tables? Try to right-click the table and script it as a CREATE TABLE statement, or even better post it as a real table.
Ultimately your schema design is poor. You should not be having a separate column for TEAM1, TEAM2 etc, denormalise them into rows instead. But to answer your question, you could probably work around it something like this... (syntax might not be 100%, I havent used MSSQL in a long time) Create View [dbo].[vwBangingMyHeadAgainstWall] as SELECT Date_Time, Result, (SELECT TOP 1 a.* from ( SELECT CASE WHEN tblTournament.Result = 1 THEN TEAM_1_Score-Opponent_Score ELSE Opponent_Score-TEAM_1_Score END as score UNION ALL CASE WHEN tblTournament.Result = 1 THEN TEAM_2_Score-Opponent_Score ELSE Opponent_Score-TEAM_2_Score END UNION ALL CASE WHEN tblTournament.Result = 1 THEN TEAM_3_Score-Opponent_Score ELSE Opponent_Score-TEAM_3_Score END UNION ALL CASE WHEN tblTournament.Result = 1 THEN TEAM_4_Score-Opponent_Score ELSE Opponent_Score-TEAM_4_Score END UNION ALL etc etc ) as a ) as Reigns_Supreme, --OUTCOME from all TEAM 1 Games CASE WHEN tblTournament.Result = 1 THEN TEAM_1_Score-Opponent_Score ELSE Opponent_Score-TEAM_1_Score END AS TEAM_1_OUTCOME, --OUTCOME from all TEAM 2 Games CASE WHEN tblTournament.Result = 1 THEN TEAM_2_Score-Opponent_Score ELSE Opponent_Score-TEAM_2_Score END AS TEAM_2_OUTCOME, --OUTCOME from all TEAM 3 Games CASE WHEN tblTournament.Result = 1 THEN TEAM_3_Score-Opponent_Score ELSE Opponent_Score-TEAM_3_Score END AS TEAM_3_OUTCOME, --OUTCOME from all TEAM 4 Games CASE WHEN tblTournament.Result = 1 THEN TEAM_4_Score-Opponent_Score ELSE Opponent_Score-TEAM_4_Score END AS [TEAM_4 OUTCOME] FROM tblTournament 
since your data structure is designed this way and you're on MS SQL and do not have "LARGEST", either 1. write one large CASE statement to find the largest value 2. create a function that takes 2 parameters, returns largest and nest said function required number of times 3. unpivot the scores and use your regular aggregate function
Hmm-- my apologies. You see, I tried to fake data in lieu of the real data, which is completely abstract. It seems I did a poor job in the substitution, however, and just made things worse. I'll see if I can edit it to make sense.
I tried to use fake data in lieu of the real data, which is completely abstract. It seems I did a poor job in the substitution, however, and just made things worse. I'll see if I can edit it to make sense. Sorry-- first time posting.
It's not amazing, but you can take the max of multiple columns using a correlated subquery with no from clause. SELECT t.ID , (SELECT MAX(x.CaseStatements) AS MaxValue FROM (VALUES (CASE WHEN t.logic1 ...) , (CASE WHEN t.logic2 ...) , (CASE WHEN t.logic3 ...) , (CASE WHEN t.logic4 ...) , (CASE WHEN t.logic5 ...)) AS x(CaseStatements)) AS MaxValue FROM Table AS t;
Can't wait to see if this structure works-- thanks very much for the response.
IANAL... but in a previous role we had an environment where we had tandem servers that were "activated" and "deactivated" based on certain criteria. So for example our production database could be "taken out of production" and its twin could be "turned on" which would always server maintenance to occur, then it would sync itself with the twin, and become "active" again. None of this is anything I am familiar with outside of knowing it exists.
Yes! It's inspiring to see we've got people venturing out to the limit of what's possible in this existence. The work Elon Musk and his teams are doing is exactly the kind of thing that seems to keep humanity pressing forward to the beauty we're capable of.
No probs. Like i say, I typed it without any testing, so inevitably it will have a few syntax problems. Overall the basic approach should be sound though 
The whole "magical" sync itself step is what worries me. We have a cluster (active/passive) where I don't think this is even possible for us to implement without going active/active config.
It isn't. It's not random :)
You can probably replace each case statements with a ABS(Team_1 - Opponent ) which gives you the difference between the scores. Where ABS takes the absolute value. And like others have told you, just unpivot the data and take the max value.
 SELECT 'addr1' AS 'component/@name', '550 Auto Center Dr' as 'component', NULL AS 'Break', 'city' AS 'component/@name', 'Watsonville' as 'component', NULL AS 'Break', 'region' AS 'component/@name', 'CA' as 'component', NULL AS 'Break', 'country' AS 'component/@name', '95076' as 'component', NULL AS 'Break', 'country' AS 'component/@name', 'US' AS 'component' FOR XML PATH ('listing'), ROOT ('listings');
Missing `address` tag can be achieved by changing `PATH` to `PATH ('listing/address')` In order to break down atributes into separate tags you can just use subqueries. e.g. ``` select (select 'addr1' [@name], DeliveryAddressLine2 [@value] for xml path('component')), (... next component tag ...) ...```
Would I be able to "take the max value" of decimal datatype, though?
The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. For free. There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An advanced course is also available.
Definitely not without resorting to dynamic sql which is a terrible idea.
Just glanced at your post....I would consider a "CASE WHEN" or "IF THEN" statement 
So something like this ? SELECT TOP 1000 COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{tablename}' and COLUMN_NAME like '%Points%' Because if I try that, and confirm some columns in that table have "Points" in their name, I get an empty return with 1 column "COLUMN_NAME" that has no records in it.
Are you using MS SQL Server? If so, it should work. Are you querying the right DB? If there are multiple databases on the server you're connected to, you might not be.
I think the only option is to build a dynamic query. You need to first select columns from sys.columns table and embed them in a dynamic query. Then you need to execute that dynamic SQL
This is the script I use. just replace the word "TEMP" below with the name of whatever column you are wanting a LIKE for. SELECT t.name AS table_name, SCHEMA_NAME(schema_id) AS schema_name, c.name AS column_name FROM sys.tables AS t INNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID WHERE c.name LIKE '%TEMP%' ORDER BY schema_name, table_name; 
`SELECT TABLE1.* FROM ...`
Nice, that was easy...
Your username confused me coming from r/Python.
Here's a sneak peek of /r/Python using the [top posts](https://np.reddit.com/r/Python/top/?sort=top&amp;t=year) of the year! \#1: [Automate the boring stuff with python - tinder](https://gfycat.com/PointlessSimplisticAmericanquarterhorse) | [358 comments](https://np.reddit.com/r/Python/comments/7kpme8/automate_the_boring_stuff_with_python_tinder/) \#2: [MS is considering official Python integration with Excel, and is asking for input](https://i.imgur.com/l2f9Zvb.jpg) | [417 comments](https://np.reddit.com/r/Python/comments/7jti46/ms_is_considering_official_python_integration/) \#3: [Python Cheet Sheet for begineers](https://i.redd.it/4iklecheyw601.jpg) | [127 comments](https://np.reddit.com/r/Python/comments/7mwgtw/python_cheet_sheet_for_begineers/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/7o7jnj/blacklist/)
Funny story, that's how I got into Python in the first place. The name was from an old MUD, but someone saw it and asked me if I used Python. I hadn't heard about it at the time, but it piqued my interest, and I taught myself Python after that.
here's a tip -- if the selected option is %, *do not generate that part of the WHERE clause* from your programming language
This is for MS SQL. It's a generic search function that looks at tables and views. You can set the Table_Name, Table_Catalog, or Data_Type filters as desired. Also, I have 3 db's I need to search through, that's why I have the UNIONs. Just add more or take them away as needed. SELECT DISTINCT Table_Catalog ,Table_Name ,Column_Name ,Ordinal_Position ,Table_Catalog + '.dbo.' + Table_Name AS Full_Table_Name ,Data_Type ,Character_Maximum_Length ,Column_Default ,Is_Nullable FROM ( SELECT * FROM db_1_name.Information_Schema.Columns UNION ALL SELECT * FROM db_2_name.Information_Schema.Columns UNION ALL SELECT * FROM db_2_name.Information_Schema.Columns ) subq1 WHERE Column_Name LIKE '%value%' --Table_Name LIKE '%value%' --AND Table_Catalog = 'db_name' --AND Data_Type = 'data_type' AND NOT Table_Name LIKE 'syncobj%' ORDER BY Table_Catalog, Table_Name, Column_Name
It is a lot easier than that. WHERE ('%' = :CODE_BOX or t1.example_column_1 = :Code_BOX) If performance ever becomes a problem and t1.example_column_1 is on the index: SELECT ... WHERE '%' = :CODE_BOX UNION ALL SELECT .... WHERE t1.example_column_1 = :Code_BOX 
While I can offer you ideas to help resolve this, I'd like to first offer ideas of another nature. Why can't the users use SSMS instead? They can use Powershell too I guess, but here's the rub. You should be doing this based on roles and accounts. This is the first level of security. You are X and you can only do Y to A,B, and C. Views and stored procedures are common here to help filter who can do what. In your scenario, I would give them access to stored procedures or views and restrict the views. As a last case effort, triggers can be instantiated to keep users from performing unauthorized DML. See this for [how to have a trigger execute for everyone except yourself.](https://dba.stackexchange.com/a/204341/112253)
In a way, this method would have 2 levels of security. The first is DB access to the particular database and server which is already configured. But in cases where a user has been granted write access to a table, this script should bypass those guidelines and only let them read that information. This wouldn't be run under any service account or situation where the user hasn't been explicitly granted write permissions.
Just FYI, the desire/need to perform this type of operation isn't unique to SQL and if you get into more traditional types of programming, the general term for this type of programming is known as [**reflection**](https://en.wikipedia.org/wiki/Reflection_\(computer_programming\)). Ordinarily all the symbols you write are compiled into a format that a machine can read. Your machine obviously doesn't care that a function is called "callRemoteApi" or whatever--it just needs to know what to instructions to execute and where in memory the instructions are kept. In order to perform reflection, the human-readable metadata must also be kept in memory in order to facilitate the ability to compare, for instance, the function name to a string in your code. There's a non-trivial amount of computational overhead since there is additional meta-data that must be kept at runtime, and not all languages support it...I use C# as my primary language and it's an absolute **staple** in many applications, libraries, frameworks, etc. It makes it significantly easier to debug too since you can investigate all the nitty gritty details of objects when you pause execution.
&gt; In a way, this method would have 2 levels of security. The first is DB access to the particular database and server which is already configured. But in cases where a user has been granted write access to a table, this script should bypass those guidelines and only let them read that information. In every shop I work in, I create two logins for myself. One is a R/W god mode and the other is a R only user. I primarily use the R mode so I don't accidentally cause problems. (No lag and oops accidental click to fail over the Availability Group or drop a table!) Next up, you can make the script connect as a different user, and that user is a read only. What's going to stop that person from using another method to log into the SQL Server though? They already have an account with R/W access, what's to stop them from creating their own powershell script or Python or installing SSMS? You can lock down their computers and try monitoring from there, but this is a bottom up solution and it will cause more headaches. Making a secondary / read only copy of the database with log shipping, replication, AG, ETL, etc is one method. Account / Role access limitations is another, views / stored procedures can also be used here, even schema permission setups too. Triggers will prevent any additional missteps. 
My preferred method is this. SELECT o.OrderDate, o.Price, i.Cost FROM Order AS o CROSS APPLY (SELECT TOP(1) i.Cost FROM Item AS i WHERE i.UpdateDate &lt;= o.OrderDate ORDER BY i.UpdateDate DESC) AS i; You'll want an index on Item. CREATE INDEX IX_Item_UpdateDate ON Item (UpdateDate DESC) INCLUDE (Cost);
are you sure CROSS APPLY and TOP work in Oracle? please see original post 
I think you only got a half-way solution. With the query that checks the information_schema.columns table, you just get the names of the columns you want from the given table. Then you need to piece together a select statement via a cursor (loop) and execute that. That's called dynamic SQL, where you fill a varchar variable with SQL, then execute it, for example using the built-in SP named sp_executesql. Here's a full example I wrote, assuming the table is named TestTable: IF OBJECT_ID('tempdb..#columns') IS NOT NULL DROP TABLE #columns SELECT c.COLUMN_NAME INTO #columns FROM INFORMATION_SCHEMA.COLUMNS c WHERE c.TABLE_NAME = 'TestTable' AND c.COLUMN_NAME LIKE '%points%' DECLARE @sql NVARCHAR(MAX), @columns VARCHAR(MAX) DECLARE @columnName VARCHAR(MAX) = '' DECLARE @index INT = 0, @max INT = (SELECT COUNT(*) FROM #columns c) DECLARE cur CURSOR FOR SELECT COLUMN_NAME FROM #columns OPEN cur FETCH NEXT FROM cur INTO @columnName WHILE @@FETCH_STATUS = 0 BEGIN SET @columns = CONCAT(@columns, @columnName) SET @index = @index + 1 IF @index &lt; @max SET @columns = CONCAT(@columns, ', ') FETCH NEXT FROM cur INTO @columnName END CLOSE cur DEALLOCATE cur SET @sql = 'select ' + @columns + ' from dbo.TestTable' PRINT @sql EXEC sys.sp_executesql @sql Dynamic SQL has its ups and downs, mainly downs - it has no integrity checking when you change an SP that generates dynamic SQL, can be tough to search through since it can programatically generate any form of SQL, hard to write and read (lots of apostrophes), hard to debug, and so on, but it's the only way to generate the type of statement you want. Alternatively, you could write a Python script to do it, but the end result is about the same. 
Pretty sure about CROSS APPLY. Replace TOP with LIMIT or FETCH FIRST or whatever.
A literal dash in a character class needs to be the first or last character in the class. You really should sit down and go through a [tutorial](https://regular-expressions.mobi/tutorial.html) on regular expressions. 
Wall of text crit me for 9000. So, in what way "db_datareader" is failing your solution?
Thank you. I will take a look at your REGEXP tutorial. 
Should be from the source table Lets say the cross reference is to employee to their manager Let's say your source is similar to: ID|FirstName|LastName|Manager|Terminated| :--|:--|:--|:--|:--| 1|Matthew|Perry|NULL|FALSE| 2|Jerry|O'Connell|1|FALSE| 3|Susan|Suranden|1|FALSE| 4|Keanu|Reeves|2|FALSE| 5|Carrot|Top|4|TRUe| The cross reference is more like a self reference but references the sourceID of that employees manager record. If you were to move this to another system and expect to provide the employees manager as their new ID you would have to make sure you move your data from top down so that the manager always exists first and maintain a crosswalk table of the managers old ID and their new ID and change this as you are inserting the data. Having the sourceID's cross reference, everything can be loaded at once and then some other DML process can go through the Source ID's and line up the managers to the Destination ID's.
That would not work. It would return all values in that case, not just the 4 he is interested in.
&gt; The options in the box are %, CB, IH, UFF, UH. did you even read the OP 
I don't know DB2 but try looking for the equivilent of the STUFF function from MSSQL. Only replied because it's been an hour and no-one has replied. 
Db2 should have listagg, I believe
Did you read the description? There are more values in the database than those that show up in the pick list.
 select so.name from sys.objects so where so.name like 'TABLENAME%'
Depends, what version of SQL are you trying to learn?
Maybe first pick a database that you want to learn, and then use the SQL that comes standard with it.
Definitely the second one... The sequel is way better than the original and the prequels just ruined it for me.
What are the specs on the server, particularly RAM? Also database size? Is it possible that the fragmentation is getting higher than it should on some tables due to unnecessary updates? For example, at work we had one query which was updating a table in our database nightly but it was missing something in the where clause so it was updating records it didn't need to. This caused the fragmentation on that table to go up to 90&amp;#37; every single night. Maintenance on that table went down from 10\+ minutes to 30 seconds. Unless this database is absolutely massive and the specs for the server are very bad, 6\+ hours for index maintenance is quite a long time. 
Not only is the ssms free. But the developer edition of the dbms is free as well for learning and development proposes. 
I wrote a blog on listagg in Db2 that links to annother excellent blog on it. http://db2commerce.com/2014/11/25/using-listagg-to-turn-rows-of-data-into-a-comma-separated-list/
What's wrong with dynamic SQL?
As opposed to...? Can you elaborate on your scenario thats led you to this question?
Sure, and apologies for the "vagueness" of my post... | My rudimentary understanding of SQL limits my frame of questioning... My line of thinking is, what are the practical uses of drop table? | What are "typical" uses of the command drop table? (Besides just deleting a table? ... might be a better way to frame the question?)
Honestly, this whole question may be out of context, i'm just trying to understand theory of SQL a little better. 
Well its purpose is to remove a table from a database. People sometimes use a drop followed by a create as an alternative to deleting the contents of a table (e.g. emptying a staging table), as a drop will seem to complete faster than deleting the data, as the deallocation is logged, not the deletes of the individual rows. Of course, thats also what truncate is for. 
Thanks for the reply. I understand the purpose; to delete a table. I'm just exploring the availability of the command and other methods of using drop table in maybe advanced settings? just seeking some level of understanding of the practicality? 
putting the pieces together, thank you!!! [What is a staging table](https://stackoverflow.com/questions/29317162/what-is-a-staging-table)
I was like you a few months ago, and I started with [Firebird SQL](https://firebirdsql.org/). However, I don't know if it's the best for newbies. 
Doesn't matter, just pick one and adapt to the differences. You never know what version a company that eventually offers you a job will use, I'd suggest looking into PL-SQL (Oracle) or T-SQL (MS SQL Server). Ultimately, the version of SQL you choose won't make a difference if you can be adaptable. 
This helps! Thank you very much!
Use BEGIN TRANSACTION in case you mess up. That way once confirmed you can COMMIT or ROLLBACK.
Ever heard of little Bobby Tables? https://www.explainxkcd.com/wiki/index.php/327:_Exploits_of_a_Mom 
Hey, \_Zer0\_Cool\_, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Bad bot
Thank you, Frosty2433, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
And when i say "seem to complete faster", i mean that (in sql server at least), a background task finishes the cleanup after the drop table command has finished executing.
When I started out as an intern for a database team, one of my tasks was to write documentation for installing Oracle. That included going through it myself. The fucking document ended up being over 30 pages long and the uninstall doc I wrote added another 10 or so pages. In contrast, the SQL doc I wrote was only 10-15 pages with uninstall being barely 3 pages.
Awesome, this helps, really, ty.
Ok. Good. That was my line of thinking about MS as well. They've really turned a new leaf under Satya Nadella I think. Lol...About Stored Procedures. That's fairly common developer perspective. I am actually a DB engineer of sorts (there's actually a number different titles for this). So yeah, my perspective is a little different. Truthfully, its a bit like anything else in the industry. There is no hard and fast rule. There's lots of good reasons for and against logic in the DB. Vendor lock in is a big negative when it comes to Oracle, but (for open source DB's) lock-in is only as dangerous as a company having all their codebases in one programming language (although a look at Cobol will tell you that there is a lock-in of sorts there too). In another post [here](https://www.reddit.com/r/Database/comments/8jrlad/so_i_wrote_my_first_stored_procedure_today_not/dz2konv/) I gave a fairly comprehensive PROS/CONS list about this topic. It is really a whole other world behind the curtain of the database. I used to **hate** the idea of programming inside the DB and didn't really understand until I got immersed into that domain (I started out with Python).
Piggybacking here for US, Oracle traps vendors in with contracts so big orgs and smaller ones. All government and small companies are mssql
Inner and left joins will be the most common, but knowing about right and outer join will definitely have useful circumstances. All joins are so similar, it should be easy enough to learn them all at once. 
Not all gov. A lot (if not the majority) of the government DB jobs I've seen are Oracle over MSSQL. I've applied to a bunch of gov jobs recently. 
Oh weird, all the jobs I've seen in govt are mssql
Eh. Probably depends on the area? Honestly though, it might be like half and half. I saw a lot of both in gov, but in general it looks like there are way more SQL Server jobs postings. 
I use OpenEdge at work and it is number 66 on the list.
Left and right joins **are** outer joins. The difference between left and right is just the order you specify the tables. 
My previous job just got off Oracle, just moving to SQL Server saved us almost half. Best decision we made.
Makes perfect sense. What DBMS would you recommend for learning SQL ? I have to work in Teradata, but setting that up on a personal PC is literally insane... 
mysql or postgresql drop dead simple installs also, strongly recommend HeidiSQL to interact with them
So I’m an Oracle DBA with some exposure to SQL Server, hence my opinion is somewhat skewed. Oracle is still, no matter what people say, one of the best performing database systems out there when dealing with large data sets and high complexity. There’s Oracle, there’s SQL Server, there’s things like Informix and DB2, but those last 2 are fairly specialised and limited these days. I’m talking SQL/general data processing here and not AI/ML, ‘big data’ etc. Just in enterprise RDBMS circles you probably won’t see much outside of these two. You’ve got to remember that Oracle has been around for 30yrs+. They know how to make a database system. Oracle is VERY expensive to run. Going from memory, I think list price for enterprise edition is 30k per ‘processor’, and oracle has some funky definitions of ‘processor’ too. However, most people don’t need EE features. EE does some fabulous things but then again Standard Edition is limited in the hardware you can run it on. SQL Server is also bloody expensive, BTW. Don’t think that it’s a cheap alternative to Oracle. Oracle isn’t ‘easy’ to use like SQL Server. I mean you can’t point and click configure. Let’s be fair, you can but it won’t be a particularly performant database. But having said that, you can’t P&amp;C configure a SQL Server database and expect it to perform up to the highest standards either. Oracle’s documentation is thorough and generally excellent. You just have to understand how to read it. And that skill alone can take some time to develop. From what I see in industry, both database systems see growth and stable jobs, but primarily in the various financial areas. Both DB job markets are focussed largely on major financial sectors (eg London, NYC). Though not by any measure are they limited there. This is a bit of a ramble, I apologise. 
Not an expert, but my .02 is that Oracle is more common/more jobs as a whole, but in the world of analytics (my world) it is all about SQL, furthermore it seems that SQL experience is much harder to find. 
This also helps, at least points my in the right direction for study. Thanks for the reply. 
Another example in this vein would be in an environment where you cannot create tables, or add joins, but you want to do something intensive so you create some #tables, fill them with data, index them the way you need, then run your calculations to give you some sort of final output that you might be copying into Excel, or using in SPSS, etc. You may need to modify some parameters and run it several times, but if you haven't dropped any tables you will be unable to do so unless you drop them. Sometimes you just want to get rid of a table that is no longer useful. You created one to store some data for some reason, now the job is done and you've presented your findings, so you drop it. Sometimes you want to drop it so you can recreate it with new column definitions, constraints, etc. You could truncate it and alter, but sometimes it is easier to drop and simply recreate if you have the query saved already.
Yup. I'd love to see PostgreSQL finally get the market share it deserves. Hands down my favorite DB (Oracle is second). Like I said in one of the above replies. Oracle help preserves the DBA role and keep it profitable. So that's something has enticed me to go down the Oracle DBA wrote. I find I like SQL Server as well, but ultimately PostgreSQL is my true DB love.
Oh interesting. Haven't heard of it. Googling now......
I wouldn't call form of database engine nor client "user friendly", since it is by definition not an end-user product. Make sure you don't mix up the database engine and the default "managment tool" or client that comes with it (for example Microsoft's Management Studio). Each live separate lives, and you could choose alternative management clients for each database engine, that might be more or less user-friendly. All end up shooting SQL commands at the database engine in the end.
As far as I've read, you can't really say what will come first - the JOIN or the WHERE. It might not matter much, and the query plan will be optimized for what's best. As for your function call, if you really want to improve performance, extract the code in the function and put it directly in your statement. Alas, you can't segment SQL into small functions that you call everywhere (like you'd do in normal application code), without performing suffering. How complex is the isuseractive() function? If it's just a join or two or perhaps even just a simple WHERE check, definitely write it in your query intead of calling the function.
Yeah - I agree. I would probably store the active status on the user table (if possible) if you are querying it a lot.
Benefits to developing in stored procedures is that you can modify the logic without changing the application and you can tune things from the database side even easier.
Worked with both in my job, use oracle for the main data warehouse and sql server for any new developments. Main reason is sql servers licencing is cheaper in our data center ($40 vs $60 per month), but the visual studio integration is nice too. I find it odd that Microsoft gives the biggest subsidies to schools, but oracle is the only rd they teach.
Fixed the typo. 
In this scenario, active could mean have they logged in more than x times in the past y days or some other metric.
Thanks for the feedback. I'm mostly a C# dev so I tend to compartmentalize code where possible. I found there's always a trade off between [human] readable code and [cpu] efficient code. 
Consider making an SQL job that runs nightly (or perhaps hourly), which performs the check for "has user logged in more than x times in y days", and sets the "active" column on the users table to true or false. Then you'll offload the computation (and it'll be quick since you can make it check all users at once, set-based), and you'll have lightning fast queries on the "active" column.
I would practice on Teradata itself. Knowing Sql is only half the battle. I feel like you really gotta understand the architecture of the DBMS itself to write good code and Teradata is definitely unique in that regard.
I don't understand how to install it at all. It's very much counterintuitive. If you can find me a tutorial that explains it reasonably, sure. But I haven't found one. 
I had a bit of both in my master's courses. For a DB development course I took the textbook was mostly Oracle, but the course projects were with T-SQL. I kinda liked the diversity there. Also, for my program we got free access to Microsoft Imagine. Free license key's for the latest, full SQL Server versions &lt;3. Lovely stuff.
Logical order of execution: https://docs.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql?view=sql-server-2017#logical-processing-order-of-the-select-statement &gt; The actual physical execution of the statement is determined by the query processor and the order may vary from this list. &gt; &gt; 1. FROM &gt; 2. ON &gt; 3. JOIN &gt; 4. WHERE &gt; 5. GROUP BY &gt; 6. WITH CUBE or WITH ROLLUP &gt; 7. HAVING &gt; 8. SELECT &gt; 9. DISTINCT &gt; 10. ORDER BY &gt; 11. TOP There's no guarantee that the function won't be executed on every line if the query planner decides that's wise. Functions *suck* for performance. UDFs doubly so. Non-table-valued UDFs doubly so again. Non-deterministic UDFs (i.e., created without `WITH SCHEMABINDING` or that SQL Server decides aren't deterministic) are doubly so yet again.
This is what I was looking for.
SQLite. No listners or netowork issues to deal with. Just a simple database file that realisticically can be used for lots of projects. Bascially anything where concurrency isn't a problem
Giving some background: I work in a massive company, Fortune 10 type, massive data stores. We have extensive use programs with nearly all DBs. I DBA a MS SQL Server, which I'm likely migrating to PostgreSQL in mid-term future (next year or two). I also have the option of Oracle EE, Cassandra, etc. In general, SQL Server is for smaller to medium projects. Oracle EE is usually an expanse from small to what most companies would consider larger (not for us, but most). We also have massive EDW environments, SAS/Microstrategy/PowerBI, two major Teradata environments, a Vertica environment, and a Hadoop/Aster managed data lake environment. Generally data flow is data lake &gt; structure and organize to RDBMS, migrate to an EDW environment &gt; ETL to smaller SQL Server/Oracle EE as needed. Oracle EE also has a lot of hands in back-end production of systems, mostly related to age of systems and vendor lock-in. To my understanding, this is pretty normative structure for companies operating at our scale, but hopefully that helps. We are not generally hired as "Oracle Dev" or Teradata or whatever, pay is basically the same across the board, you may have specialties but you often work cross-functionally (e.g. I DBA for SQL Server, but do a large amount of ETL work from Teradata/SAS and using an Aster connection to the lake with Python).
I learned T-SQL first, it was a terrible decision! Don't start with a course that is strictly T-SQL because some of the syntaxes won't apply everywhere. SQL can be a very powerful tool that will take you far if you can master it! Cheers! Good Luck!
Syntax is the easiest thing to learn when hopping between SQL variants. The concepts and basic manipulation of data are the hard part.
I'm not sure I'm following your question but the SELECT part is the only required part of the statement.
I agree it's wired. I never understand why select comes before from statement etc
oracle also have a free express version: http://www.oracle.com/technetwork/database/database-technologies/express-edition/overview/index.html along with oracle sql developer (also free) you can get started with oracle db and sql pretty easily.
If you need to wrap logic in a function, if possible try making it an inline table valued function, as these get optimized into the rest of the plan, unlike any other kind of function (thry are essebtially parameterized views). Changing scalar functions to inline TVFs has incresed performance of some queries ive worked on drastically.
The short answer is that SQL has standards and standards dictate this particular command structure. A longer answer is that sql has been originally developed to mimic english phrase composition to some extent and to be a declarative language. So, by design, the user states the goal and the system/interpreter figures out the steps and order of execution. It just so happens that in majority of the cases MS SQL engine will choose that specific order of execution.
My 2c: - SQLite is best for starting off, just because it is absolutely free (public domain) and sever-less (= easy to "install"). - PostgreSQL is best to level up, because it's still free (but not public domain) and has the best SQL support amongst the free SQL databases. There are three products that I recommend **not** using: - MySQL or MariaDB: Bad SQL support, although it's getting better nowadays. However: these products misbehave is many situations so that if you use them to learn, you'll have a very hard time using another DB properly. - SQL Server: The SQL dialect of SQL Server is very special. I'd say no other database has that much non-standard syntax as SQL Server (and Sybase, for that argument). You could also use the free editions of Oracle or Db2 (LUW). There is nothing against them, except that these are basically commercial products (even though they have free editions). Db2 (LUW) is technically a sound product, but it's hardly used in the field. Having Db2 experience doesn't open many doors. Oracle, on the other hand, is VERY popular. However, the free edition of Oracle is pretty old (from 2010). A new one is expected in the near future.
You can also have a look at http://blog.jooq.org/2016/03/17/10-easy-steps-to-a-complete-understanding-of-sql/ Step 2
It is very useful to keep the logical order of execution in mind when writing a query. It makes sense as well, see it as an elimination race. Data which got thrown out in a previous step can't be used anymore later on (disregarding a few subtilities): * Start with a whole database worth of data 1. FROM: narrow the results to one table (or combination of tables if you do a JOIN) 2. WHERE: filter out specific rows 3. In need of a summary/ grouping? =&gt; GROUP BY 4. Need to filter out groups? =&gt; HAVING 5. Horizontal selection is done by now, but which columns do you want to display =&gt; SELECT 6. Doe you have a specific order in mind? =&gt; ORDER BY Keeping this order in mind, explains why you cannot use a column alias from SELECT in your WHERE clause, since the alias simply isn't parsed yet. (If you really want to use aliases as if they were variables, you should check the WITH clause). IMHO it would have been better to have the syntax follow this same order of execution, but I guess it wouldn't flow like an English sentence in that case.. 
Yep - I had to do this at work a few days ago. I set up my load routine to add the current day’s records to my table with an As Of Date that I sue in reports to determine what day that set of data is for. I think I ended up using the as of date as the leading column in my clustered index and performance has been pretty good. The basic concept is copied from a type 2 slowly changing dimension if that helps. I do like the Kimball books on data warehousing too.
Alright man. You’re probably covered learning the basics like you’re doing then.
Either way I'm probably going to try and get my own copy of teradata on my computer. It's a little bit tricky though because I can't figure out the VMware thing. I got VMware Player and now I'm trying to figure out how to install teradata on it...
You’ll need to have your Salesforce Developer add a trigger to the Object that’s being changed, and possibly add an object to capture that historical data. I did DB development using Salesforce Data for almost 3 years. It doesn’t work the same way normal RDBMS does. Your Salesforce data is basically accessible via Linked Server (Or DBLink if you’re using Oracle) from your DW’s RDBMS. You have a limited number of queries to your Salesforce Server per day. Why all the Salesforce work instead of DB Work? Well, let’s say you write some ETL that captures changes on the desired Salesforce object (which is what I used to do). And let’s say a customer’s billing status is one of 3 (Active, Overdue, Delinquent). If your ETL only captures the current state of the Object every HOUR (or any other interval that's not driven by the data change itself but by time), then a customer can probably easily go from Delinquent to Overdue and to active in an Hour (that's hypotheical). So one record will have Status = Delinquent, and the next record will have Status = Active. However, if you write a Salesforce Trigger (not a DB trigger, as you can't put DB triggers on Salesforce Tables), you can capture all changes made to this object into a Salesforce History Object (Customer_History) or something. Then your ETL can simply go to that table to capture it. But yes, as /u/stonedsqlgenius and /u/Pacos say, it's definitely going to be a Slowly Changig Dimension (Type 2 Dimension) on the SQL side. You can choose to do this with ETL SQL, or an ETL Tool like SSIS, DataStage, Informatica, etc. TL;DR: it needs to be a combination of a Salesforce Trigger on the object(s) and Type 2 Dimensions. Otherwise you'll probably lose data. 
Depends on your RDBMS. T-SQL Allows a Select without a FROM, but Oracle doesn't allow it (You have to Select from DUAL if you're just selecting a variable or constant or something).
+1 for SQLite. Another idea is to just use [sqlfiddle](http://sqlfiddle.com/) in a browser while learning. 
Hmm. This is quite an assertion. Can you back up this claim with factual evidence?
I agree --- another example of where MySQL/MariaDB fall short: CTE. That isn't a beginner topic, but certainly is an intermediate topic in my mind and quite useful! I need CTE to do what I do generally speaking.
Although the latest release of MySQL (8.0) and MariaDB (since 10.2) both support CTEs.
My take on an answer using powershell to import into SQL: http://ygx.316.myftpupload.com/powershell-to-upload-excel-to-sql/ You could just loop that function through all the files
Oh that's good news. For the longest time they didn't and it drove me nuts when I used those systems!
Makes sense. 
Your ERD link goes to the same image as your source data set. 
Working in government myself as a MSSQL developer you find that the finance department will use Oracle while everyone else uses MSSQL
I've written C# for 10 years as well, and that's one of the major differences. OO languages such as C# are all about splitting logic into small, re-usable bits. SQL is almost the opposite, you have to write a new, optimized statements every time you want to do something with data. There are exceptions to this of course. One way you can "compartmentalize" SQL is to select data incrementally into temporary tables, refining the data from statement to statement. Then you sort-of feel like you create "objects" (temp tables) that you continue working with until you have the end result. You can also create a stored procedure which either returns a table or sets values on some of its parameters (parameters tagged OUTPUT). Then you can call that SP, get some values, and work with it. In this example, you could create an SP which returns a table with all the users that have a given addressId, insert the output into a temp table, and then make a statement that joins onto that temp table. Untested quick rough xample: create table #users (userid int) insert into #users exec GetActiveUsers 123 -- addressId select * from users u join #users u on u.userid = u.userid - or something like that. Another way is with CTEs (common table expression), which is sort of a select statement you pack into what I like to call a "virtual table", and then you can join onto that in the following statement, for example a SELECT. Try to google it if you're interested.
A lot of this depends on how the data is going to be used. If you wanted to simplify the data down to the absolute least amount of columns and rely on aggregates to derive some of the data. You could: Add a Votes column to Party\_Election\_Cand Table Remove winning\_votes, losing\_votes from Election Table. These are stored in Party\_Election\_Cand now Remove majority\_votes from the Election table. This can be derived from Total Votes / 2 Also, if you store all party votes in Party\_Election\_Cand rather than just the winner and loser, you can remove total\_votes and aggregate all vote counts from Party\_Election\_Cand on year.
sqlite3 is good. Use: &gt;brew install sqlite3 on a mac to install it.
Interesting. I thinks rollbacks are useful during development, you find something doesn't work as expected, now rollback and change/delete the related migration files. But actually database schema development just like many other development\-related things is a matter of preference. Every tool/methodology has it's own cons and pros.
Opening a new database connection in every single function seems horribly inefficient. I assume that error you're talking about is in your item_move function? You should look very carefully at where you're getting the error and think hard about that line.
You want to do is Null instead of is not null The way it's written you're just doing the same thing
On mobile so syntax might be off. Use isnull(Request_Facts.System_Completion_Date, getdate()) instead of just Request_Facts.System_Completion_Date if you want a rolling result based on time passed. Alternately, use a case statement to replace the whole result with your desired results when a case is still open.
Thanks \- This worked. I knew there was a simple way to do it. Really appreciate the help.
You issue is data types. I'm assuming that Lion, Zebra, and Antelope are all of the INTEGER data type. Since everything is an INTEGER, there is no reason to implicitly change the data type to anything besides INTEGER. The same happens for the division of the Quirk values from the two tables since division will give you the [highest precedence data type](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-precedence-transact-sql?view=sql-server-2017). Try explicitly changing your Quirks data type to an actual numeric with some precision, doing the calculation, and then formatting. FORMAT(CAST(a.Quirks AS DECIMAL(16,4)) / b.Quirks, 'p')
Are you trying to get the count of table a? And table a is the one you are filtering? If so use a a left outer join and include the filter in the join. On mobile, apologies for the formatting b.join_column = a.join_column and a.filter_column = filter value 
I’d go with something like: DATEDIFF(d, start_date, ISNULL(end_date, GETDATE()) AS new_date
I changed the SELECT statement to include an outer join between table a and table b, per your suggestion. That makes sense to me, however it did not change the query's behavior, as it still returns a value of 1 no matter what input parameter I use \(and since any number divided by itself equals 1, it suggests that the WHERE clause\-\- what I have called my filter\-\- is still being applied to both the numerator and the denominator. Below is my updated code showing your outer join. Also, I edited my original post to reveal datatypes off to the side. INSERT INTO @MyOutputParameter ([MyPercentage]) SELECT FORMAT(COUNT(a.Quirks),'p')/COUNT(b.Quirks) FROM Quirks_CTE b LEFT OUTER JOIN Quirks_CTE a ON b.Quirks = a.Quirks INNER JOIN Profit_CTE c ON a.AnimalDateTime = c.ColorDateTime WHERE a.Quirks &lt;= @ MyInputParameter AND ([Yellow_Profit] &gt; 0 OR [Purple_Profit] &gt; 0) RETURN END
I edited my original post to show the datatype of Lion, etc. as they now stand. Your advice prompted me to explicitly change them to integer from what they were before \(bit\). Thanks for the link\-\- useful new info for me to study. But from the point of view of syntax how would I combine these FORMAT and CAST functions with the COUNT command as required in my SELECT statement? 
maybe you should state what are you looking for? :)
semi-psuedocode - adjust to your particular flavour of SQL... SELECT TOP 1 * FROM FRANCHISE_APP ORDER BY (decisionDate - appDate) DESC
Can you use a MERGE statement? They can handle more complex INSERT/UPDATE WHEN x or y situations. 
 Select top 1 * from Franchise_app order by (DecisionDate - Appdate) Desc
I'm not sure exactly what you're looking for. Also, it depends somewhat on what DBM you're using. If you want Last_Count to only be one line per agent, you either need to create a sort and select the first row per agent, or you need to use aggregate functions and a GROUP BY. 
Just wrap the Quirks in count like you did before. I skipped over that.
If you're getting duplicates you either need a group by or a distinct. If you're getting multiple agents with unique values you need to remove the column causing the problem from your select
Two errors. line 94: sql = '''insert into Inventory(Items(ItemID)) where Item_Name =?'''(item_name) Line 94 is not valid python. You can do: sql = '''insert into Inventory(Items(ItemID)) where Item_Name =:1'''.format(item_name) That defeats the purpose of using prepared statements, though. Security issues, and I think you're trying to use them, so: sql = '''insert into Inventory(Items(ItemID)) where Item_Name =?''' Line 6: c.execute(sql,data) data must be a tuple or list when you use question marks. For named parameters, a dictionary. There might be other options, but I don't remember off the tuple of my head. So, you need to verify that data is a tuple before line 6 and correct it. Line 6: c.execute(sql, (data,))
&gt; factual evidence? The only organizations using it at large scale are the ones who literally can't afford licensing costs for SQL server due to their size + scale. If it was cheaper, there would be no one using any other DB. For one great example, if you ever had the luxury of working in MSSQL shop, you would immediately fall in love with the tooling that is available at your disposable. That tooling is unmatched in the industry. 
&gt; buy it seems you could modify the enclosing process to feed 'insert' type if records first, then feed the update ones That's what it is currently doing, where it checks if the record exists. If it doesn't, it inserts it with change type 'I'. If it does, changetype 'U'. The problem is if an insert and update xml file come through at the same time, the update updates the insert record before the insert is ingested into the external system, and as such that system see's an update for a record that doesn't exist for them.
I've actually done the Lynda's "Oracle Database 12c: Advanced SQL" for everyday work reasons. It gave me some good new knowledge about partitions for example. But it is not that hands on practical lecture you seem to be looking for. I could suggest the book "Oracle Essentials. Oracle database 12c. 5th edition" which I began reading a while back. That should go into more detail as you wanted to.
&gt; you would immediately fall in love with the tooling that is available at your disposable. (SSMS/SSIS/SSAS etc.) Lol. That may be true. I do love SQL Server from a TSQL newbie's perspective and the MS ecosystem is second to none, but I still think the main reason CIO's don't go for PostgreSQL is that there is no backing corporation they can depend on. Third-party vendor support for PG is still a huge gamble while Microsoft is tried, true, and dependable. So..fair point. I have never seen an open source tool that is as user friendly, integrated, and streamlines as comparable MS stuff. TBQH this is universally one of the biggest downsides of open source. Open source toting enterprises end up doing a lot of work to "stitch" together various open source tools and such. 
Querying Microsoft SQL Server 2012 - Training Kit for EXAM 70-461 It's got a PDF version that's 739 pages and walks you through a ton of examples. Even includes a test database and teaches you how to import and set it up. After that book there's also the book for the next exam 70-462
Based on the CTE/ type of query you wrote, I am assuming that you are looking to have only the latest entry on the "Contra" table for the agentID? My guess is you are looking for something like this: WITH LAST_COUNT AS ( SELECT {Conta}.[AgentId], {Conta}.[Data], {Conta}.[Valor],{Conta}.[Submit], ROW_NUMBER() OVER(PARTITION BY {Conta}.[AgentId] ORDER BY {Conta}.[Submit] DESC) AS Row# FROM {Conta} ) SELECT {Agent}.[Id], {Agent}.[Nome], {Agent}.[Apelido], LAST_COUNT.[Data], LAST_COUNT.[Valor], LAST_COUNT.[Submit], {AgentPicture}.[Id], {AgentPicture}.[Filename], {Agent}.[Telemovel], {Agent}.[UserId] FROM {Agent} LEFT JOIN {AgentPicture} ON {AgentPicture}.[Id] = {Agent}.[Id] INNER JOIN LAST_COUNT ON LAST_COUNT.[AgentId] = {Agent}.[Id] and LAST_COUNT.Row# = 1 WHERE ({Agent}.[Id] LIKE '%' + @SearchFilter + '%') OR ({Agent}.[Nome] LIKE '%' + @SearchFilter + '%') OR ({Agent}.[Apelido] LIKE '%' + @SearchFilter + '%') OR @SearchFilter = '' 
Umm, that's not it - this piece of code seem to work on a single record (or @ClaimReference) at a time and for it to function some other piece of code - an external loop/script/ETL package/stored procedure/etc. - feeds records (@ClaimReference-s) to this. What I'm saying is to modify that other external piece of code to feed first the records to be inserted to the code in OP and then feed all the update ones.
&gt; One way you can "compartmentalize" SQL is to select data incrementally into temporary tables, refining the data from statement to statement. Then you sort-of feel like you create "objects" (temp tables) that you continue working with until you have the end result. Yeah that can work fine but overuse of temp tables will kill your disk. I've worked on a lot SQL code produce by .Net devs and over-use of temp tables is a typical symptom. Its a dangerous mistake to make as it will work fine until enough temp tables are in use for it to spill out of the ram onto the disk and suddenly every query is fighting for disk read/writes and taking 10 times as long to run. In your example code you may find that adding an index to the temp table after it has been populated will make the following query that joins onto it more sargable (and faster). 
The select part is about shaping the output. The from defines the place(s) it gets that output. You need to know you are going to the stationary store before you pick up the pen. 
I think this guys the best \([http://www.learnitfirst.com/Course/AllCourses.aspx](http://www.learnitfirst.com/Course/AllCourses.aspx)\). No affiliations \- Promise! Just love his delivery and simplification of concepts. If you navigate over, search for t\-SQL Programming. Some courses are on YouTube for free. Perhaps they'd throw in some t\-SQL for free if you ask. Also, I know Tim Ferriss mentioned on this podcast \([https://sivers.org/2015\-12\-ferriss](https://sivers.org/2015-12-ferriss)\) about learning SQL rapid fast... but I don't recall the source. 
I like SQL Performance Explained by Marcus Winand. Afterwards, the documentation to the implementation you use.
&gt; Querying Microsoft SQL Server 2012 I'll second this. It's a great book, and I learned quite a bit about CTE's and Pivot Tables that were new concepts to me.
Its an analogy but what if multiple stores had pens but you wanted the specific one stocked by the stationary store. Another way of looking at it is that the table is the parent of the columns so you need to go to the parent first in order to ensure you have the correct children. 
Try this. select dbo.[functionTest](13);
I offer one: https://www.jooq.org/training. Unfortunately not online, yet
Other comments already give you answers that should get you full marks, but I wanted to give you some general advice. Your current answer indicates a misunderstanding of what the Where clause does. You'll want to re-read your textbook and/or some online material to get a better understanding of "where"; otherwise you risk falling behind. 
I would start here: \&gt;download and install VS 2012 Tools for Applications and VS 2012 Shell Isolated, then install Business Intelligence. http://stackoverflow.com/questions/30357538/installing\-sql\-server\-2014\-data\-tools\-bi\-for\-vs\-2013
What version of Oracle are you on?
11i
You only fetch one record. Your query could have 1000 records in it, but if you only fetch one, you will only email one. Maybe use a CURSOR FOR loop to iterate through the records, emailing each one. 
\&gt; There aren't a lot of quality resources for optimizing queries because it depends on too many variables. Any "best practice" is going to be wrong in more examples than it's right, even if the advice is perfectly sound. To add to this, most "Advanced" areas of SQL querying are directly dependent upon how complete shit the original implementation/architecture of the database was to begin with. Querying data in a "best practice" database is a breeze. Trying to query data around horrible physical and logical architectures is where those advanced querying concepts shine and inching out every ms of query time can pay off. Understanding Execution Plans, as /u/jc4hokies mentioned \- and digging into the particulars of that specific database engine is really the only way to begin to work around some of these issues. Thank you the architects in the past who were either limited by technology, application requirements, or knowledge that put into place the database architecture you're now trying to query around. 
same goes for group by tbh :s
Hmm. It looks like Oracle has some restrictions with correlated sub queries (which cross apply is). Something more like this should work. SELECT ooha.creation_date , ooha.price , (SELECT MAX(cch.cost) KEEP (DENSE_RANK FIRST ORDER BY cch.update_date DESC) FROM CST_COST_HISTORY_V cch WHERE cch.inventory_item_id = ooha.inventory_item_id AND cch.update_date &lt;= ooha.creation_date) cost FROM O_O_H_A ooha; http://sqlfiddle.com/#!4/0e0599/6
(O'Reilly SQL Hacks)[http://shop.oreilly.com/product/9780596527990.do] has some good advanced querying techniques to learn.
You put the filter in the `ON` clause of the other join.
Maybe a union of two queries instead of a single big join?
Tableau custom SQL is a pass through statement (almost) so it'll take whatever dialect the connection is to. "Where" clause logically works after the joins are done, so if you need to filter a left joined table prior to the operation, move your filtering condition to the "on" clause of the relevant join
What's the specific problem that you are stuck on?
Thanks a lot but I don’t see the sql oracle certification on that link. Does it fall under oracle database certification? 
SQL exams will be part of the [database certifications](http://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPillarPage?p_pillar_id=2&amp;p_mode=Certification).
By putting 13 in quotes, you're passing it as a string. Just enter the 13, no quotes around it
sql zoo sql anti-patterns sql for smarties
Can you post the query that doesn't work? This one runs just fine.
Guessing MySql SELECT o.CustomerID , COUNT(*) NumberOfOrders , MAX(o.OrderDate) MostRecentOrderDate , CAST(SUBSTRING(MAX(CONCAT(DATE_FORMAT(o.OrderDate,'%Y%m%d'),CAST(ot.TotalSpent AS CHAR))),9) AS SIGNED) TotalSpent FROM Orders o INNER JOIN (SELECT od.OrderID , SUM(p.Price * od.Quantity) TotalSpent FROM OrderDetails od INNER JOIN Products p ON p.ProductID = od.ProductID GROUP BY od.OrderID) ot ON ot.OrderID = o.OrderID GROUP BY o.CustomerID;
This is right
it works like a charm!! thank you so much 
yeah the problem is that I want to use both Conta and Agent table. I could work with just Agent but together it makes the backoffice more productive
I have done something similar. I pulled all the columns I needed to compare from both tables into a query and then just did a query like this: SELECT PreTable.KeyID, CASE WHEN PreTable.Col1 != PostTable.Col1 THEN 'NOK' ELSE 'OK' END AS [Compare Col1] /* Repeat above however many times. */ FROM PreTable JOIN PostTable ON PreTable.KeyID = PostTable.KeyID That would give me a bunch of "OK" &amp; "NOK" values that I could then go back and investigate. To deal with the missing data part, use a LEFT OUTER JOIN: SELECT PreTable.KeyID FROM PreTable LEFT OUTER JOIN PostTable ON PreTable.KeyID = PostTable.KeyID WHERE PostTable.KeyID IS NULL The left join along with the WHERE clause will show you the keys from the pre table that don't have a match in the post table.
Learn the rough basics of sql: select, from, where. Review the applications documentation on what fields are available. Them filter out what you need..
its not that hard for basic use: select &lt;column names sepatrated by commas&lt; from &lt;table name&gt; where &lt;column names with filters&gt; for example select user_id from users where user_name = 'BUKI'
Thanks this is really helpful, im kind of confused on why i would have a bunch of ok and nok? What exactly does this represent?
Note that this will return spent value of the highest order by value should there be multiple orders on the same date and not the spent amount of the most recent order.
I don't think you can change a Primary Key column to Auto Increment once the table has been created. I would load everything into a temp table with an Identity column. pseudo code: Set identity_insert temptable on insert existing values for original table set identity_insert temptable off insert new records into temptable truncate original table and insert everything (or just insert all records with ID &gt; most recent entry). [Based on this](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/04d69ee6-d4f5-4f8f-a115-d89f7bcbc032/how-to-alter-column-to-identity11?forum=transactsql) 
True. CONCAT(Date,OrderID,TotalSpent) would be an improvement.
Obviously the table design is poor and needs fixing but thats not going to help you right now sort the issue you are facing. What you could do is get the MAX ID from your destination table and add that to the ROW_NUMBER in order to generate the correct ID. Something (untested code but to get you on the right path) like so: declare @MAXID int set @MAXID = (SELECT MAX(ID) from someTable) With CTE_DataToInsert AS (SELECT top 50 *,ROW_NUMBER() OVER (ORDER BY ID) +@MAXID from SomeOtherTable ) insert into someTable select * from CTE_DataToInsert
Download TOAD. It will do a compares for you
yup, that worked! thanks
"Indexes are materialized copies of your table" -- what? That's not true
Why arent u selecting the column that you are comparing in the case statement?
Seems like you would want to select what youre comparing to see that rather than just selecting the keyid?
Your results look like a Cartesian join issue (record from table a joins to more than one record in table b etc). Easiest way to test is to take one record you can follow through your code, limit your temps etc to this record and then add your joins one by one. If you know you should only have one row then you suddenly get more you know which join is causing the issue
&gt; In the background my understanding is it is an SQL database. First things first, you need to verify this. Is it a SQL Server or Access or Postgres - figure out what type of SQL database is running on your server. Next, install the appropriate tool to connect and query that database - figure out how to login to the database as well. Then figure out which table stores this data. Then figure out how the columns are laid out in the table, and how you can query it. A basic SQL tutorial such as on w3schools will tell you how to filter data from a table using a where clause.
The CTE is not necessary - I just put it into there to break it up a bit. You can do it without, just need to make sure you get your columns in the right order so selecting * may be a mistake - You want ROW_NUMBER() OVER (ORDER BY [Ticket_Id]) as the value inserted into the 'not auto' incrementing ID column. declare @maxId int set @maxId = (SELECT MAX([request_id]) from [dbo].[i_data]) insert into [dbo].[i_data]( [category_id], [creation_date], [details], [due_date], [file_path], [legacy_assigned_to], [legacy_date_deferred], [legacy_radar_ticket_id], [priority_id], [region_id], [requestor_name], [status_id], [subject], [type_id] ) SELECT *,ROW_NUMBER() OVER (ORDER BY [Ticket_Id]) +@maxId from [dbo].[ai_tickets]
Not sure I'm understanding you here. Let me clarify. Is the column for quarters the QTR column in your image? Where Q116 means Q1 2016? What does the QTR column represent? Is it the quarter the row was created? If so, and you are looking for rows that were added in that quarter, wouldn't it just be WHERE QTR = Q116 for example? Otherwise, how do you know when a row was added?
GROUP BY \[event\_date\_only\], \[DayOfWeek\], csr\_name, unit.\[name\], totMins, TotCall, Pitched, DMs, Accounts That still gives me the 3k\+ responses, if I could get it to return the LAST one, and an addition of all of the hours in the group of dates that would be helpful.
&gt; ♣ Note: 'SQL' statements should only be used by those who are highly experienced with 'SQL' and are familiar with the structure of the 'Comms Suite' database structure and tables! This blatantly isn't you. Talk to your provider and ask for more details on how things work in the background
Do you just need to group by qtr? Btw, those aren't actual client info, are they? 
Pumped an address into google maps, found a hospital in calgary. I think its real data :O
try the query with actual date literals instead of the prompts
This. That error looks like there is a casting error with the between. 
aright cool it works thanks, you've saved me so much time. apreciated
The date prompts work just fine if I remove any attempt to do the inner join.
This is for work. There are efforts underway to update the system, but for the moment this is what it is.
okay, try the query without the WHERE clause i'm just trying to pinpoint what's causing the "too complex to be evaluated" error also, list only the columns you want, don't use the dreaded, evil "select star"
Oh, I don't. Sorry I tried to simplify the query for the sake of this question, ~~since it worked as it existed without the inner join attempt.~~ Okay, so I'm *(an idiot)* wrong and was too focused on the Inner Join as the cause of my problem. Removing the additional 'AND's where I tried to additionally filter out data was what was killing it. So I guess now I need to figure out how to make that part work. Thanks for the troubleshooting support! The exact code is (was) this: SELECT [Operation Entry].[Year Assoc], [events].[Incident No], [events].[RecDate], [events].[Circuit No], [events].[Cause ID] FROM [Operation Entry] INNER JOIN Cause ON [Operation Entry].[Cause ID]=[Cause].[Cause ID] WHERE (([events].[RecDate]) BETWEEN [enter a start date] AND [enter ending date]) AND ([events].[Class ID No]="4") And ([events.[Sequence No]="1") ORDER BY [events].[RecDate];
Yeah, I'm not a fan of the venn diagrams. Since you're using an inner join, there must be a customerid in both tables to be in the results. With a right join there you'd see all customers, even those with no orders. With a left join, you'd see all orders, even those with no customer. With a full outer join, you'd see all orders and all customers. 
I'm going to answer your questions first, afterwards I'll give an explanation of the statement. Some of the answers may not make sense until you read through all of this. &gt; However, I'm not sure what orders is supposed to be. Is that the actual Table or is that the database itself? What we call "schema", "databases", "instances", and "tables" are words that we use to describe objects inside of a relational database management system (RDBMS). Oracle, MySQL, SQL Server, etc all use similar but different terminology. In SQL Server however, I'll illustrate for you what we are referring to. -&gt; This is our command piece SELECT -&gt; Orders is the table we are retrieving the column OrderID from Orders.OrderID, -&gt; Customers is the table we are retrieving the column CustomerName from Customers.CustomerName, -&gt; Orders is the table we are retrieving the column OrderDate from Orders.OrderDate -&gt; A table we are selecting columns to see FROM Orders -&gt; Another table we are selecting columns to see. The inner join says that when the two columns are matched, they must be equal. Only results found that are common between both tables will be returned. INNER JOIN Customers -&gt; You are looking at the column CustomerID in the Orders table and looking at the column CustomerID in the Customers table. When you find a CustomerID common to both, you are asking for the data as illustrated above. ON Orders.CustomerID=Customers.CustomerID; &gt; So it's the orders database and the orders table as well? Confused on this part. I'm also confused on the customers after the inner join statement. In SQL Server, the hierachy goes as : Server -&gt; SQL Server Instance -&gt; Databases -&gt; Schemas -&gt; Tables. So I connect to my server that runs my software. I then have SQL Server installed on the server (it is possible to run multiple installations or instances on a single server, this is not recommended typically), when I sign in, I can see the databases associated to that instance. Each database has a schema which is typically used to help group tables together or limit access for security. Many shops just keep this as DBO but if you are looking at Adventureworks or other data sets, you may see something like Customers.Orders. If you were to look at the whole syntax for that table, it may look like : Adventureworks.Customers.Orders. Adventureworks is my database, Customers is my schema, and Orders is my table. Now here's where I think you may get confused or have been confused. You can have the same named column in multiple tables. How do you know which column belongs to which table? In your example, you specify the CustomerID from the Orders table. Here is another way to write all of this: SELECT Adventureworks.Customers.Orders.OrderID, Adventureworks.Customers.Customers.CustomerName, Adventureworks.Customers.Orders.OrderDate FROM Adventureworks.Customers.Orders INNER JOIN Adventureworks.Customers.Customers ON Adventureworks.Customers.Orders.CustomerID=Adventureworks.Customers.Customers.CustomerID; The last line: ON Adventureworks.Customers.Orders.CustomerID=Adventureworks.Customers.Customers.CustomerID; This can be read as: [Database].[Schema].[Table].[Column] = [Database].[Schema].[Table].[Column] That kind of sucks to read doesn't it? It seems a little redundant too. As long as SQL Server understands the scope of your SQL and where you are running things, a lot of how you have your statement written out is for the benefit of the developer. In your example provided, I would have personally appended the schema to the tables. Appending the schema helps resolve any ambiguity, as does referencing the table before the column name if you are joining tables. Specifying Orders or Customer before your column name allows me to know where it is coming from. If you joined 100 tables and none prefixed the table name, I'm going to have to search through all 100 tables to know which column you are talking about. &gt; It's grabbing similar in trees that are exact matches from both tables but not bringing anything back if there is no exact match found. Is that correct? I would not call the tables trees. Consider them data sets. There is a more advanced level of knowledge here regarding B-Tree index structures when working with clustered and non-clustered indexes, but we could have heaps or even clustered column indexes or OLTP tables that change how the table is physically and logically structured. So in short, forget about this for now until later, just know of it. Inner joins are where two tables are matched on a column(s) to limit the results between tables that are the same. There are more in depth explanations and more complicated explanations, but I would stick to basic for now. SQL is Structured Query Language, it's based mostly off of English. For example: Bring me the red book from the living room that is the same book you had on your bed earlier. SELECT LivingRoom.Book FROM LivingRoom INNER JOIN BedRoom ON BedRoom.BookID = LivingRoom.BookID WHERE LivingRoom.BookColor = 'Red' When you read the top sentence and match it to the code, it makes sense logically and in English. What if I typed the code this way: SELECT Book FROM LivingRoom INNER JOIN BedRoom ON BookID = BookID WHERE BookColor = 'Red' Reading that code as a human, I would say the English version turns into: Bring me the red book from the living room or the bedroom or both where the book is the same book in the bedroom or where the book is the same book in the living room. If the column exists in both tables, you need to specify. The column may exist in both tables but it may not be the same values for the same row. Even if the values are the same for the same row, SQL Server doesn't understand what table you want the column from, it's ambiguous. Likewise, it's not sure if you want to join the BookID onto the same table you are selecting from (there are reasons to do this and you can do this, just know it's possible and move on for now) or if it's supposed to join on another table. Again, it's ambiguous and you need to resolve any clarification for the SQL engine. Now for the last piece, you can alias things. Sometimes table names can be long, so you can rename them for the length of the transaction. Here's an example: SELECT o.OrderID, c.CustomerName, o.OrderDate FROM Orders as [o] INNER JOIN Customers as [c] ON o.CustomerID=c.CustomerID; I aliased Orders as [o] and customers as [c], so when I refer to those tables henceforth, I can refer to them by their alias shortening how much I have to type but maintaining readability. Please let me know if you want me to clarify or go more in depth to anything or if any piece did not make sense.
It's a granularity thing - group by is just a most 'direct' way to set your output granularity. If a csr belongs to a single unit for the purposes of your report, then "group by csr_name" should suffice. If you are struggling with getting the right granularity, limit your 'group by' to minimum required and everything else should be wrapped up in aggregate functions. There are sometimes performance issues with that (Oracle used to not like this approach) but the imperative is to get the output right first, I think.
Did you try by having SName be the first table you create?
glad i could be of assistance
*tl;dr /u/FoCo_SQL did a great job explaining the levels of abstraction in RDBMS. I'm going to explain a different way of thinking about JOINs in general that should hopefully make the syntax easier to understand in the future.* It helps to imagine that every join starts as a cross join, then is filtered based on some rules defined by A) what type of join it is, and B) your ON clause. If you imagine you have two tables: 1 a 2 b 3 c 4 d and 1 z 2 y 3 x Then a full cross join of the two tables would yield every possible pair of the rows 1 a 1 z 1 a 2 y 1 a 3 x 2 b 1 z 2 b 2 y 2 b 3 x 3 c 1 z 3 c 2 y 3 c 3 x 4 d 1 z 4 d 2 y 4 d 3 x So the output of the FROM clause (including both tables) is a single mega-table. From here you can see how your ON clause specifies a slice from this table. A where clause also slices from your single mega table, and older SQL syntax standards actually just put the ON clause conditions inside of the WHERE clause. ON vs Where are just syntactical sugar (but the ON clause MASSIVELY improves readability). Now imagine you joined ON first_table.number = second_table.number. Your output would be the slice of this mega table where those two numbers matched. 1 a 1 z 2 b 2 y 3 c 3 x Everything you can get from a JOIN has to be something that could appear in the original cross join. Lastly: Types of joins determine what counts as "matching" the ON criteria. In our example above, there's a number 4 in the first table, but not in the second table. In the default "JOIN" case, which happens to be synonymous with INNER JOIN, we said the values MUST appear in both tables. If, instead, we did a LEFT JOIN, then we actually made the rule: Include every value of `number` from first_table, match whatever you can, and for any values that don't appear in the right table, leave null. In this case we get: 1 a 1 z 2 b 2 y 3 c 3 x 4 d null null Since there is no (4) in the right table, so it doesn't match anything. Hope this helps! **bonus: walk through what happens if you have two rows with (1) in the right table. e.g. (1,z) and (1,w), and two rows with (1) in the left table. e.g. (1,a) and (1,e). You should end up with 4 rows where 1's match in the cross join and 4 rows in the resulting filtered expressions as well... When there's ambiguity like that, you get permutations. This can be helpful at times, and bite you in the butt at other times. So it's good to be aware of**
&gt;I have full access to the server Sounds like you need to know what the db schema looks like first. If you do have full access, I'd try to log into the SQL server directly, assuming you have the right credentials. Then you can view the tables \(SHOW TABLES\), and view the schema for each table. Log into the server and look at the processes to see if there's a SQL database running. Could be SQL Server, Postgres, MySQL, etc. Then look at the port/socket it is listening to, e.g. 3306 for MySQL. Then connect to it using a SQL client, which should give you a shell and/or a GUI to explore the schema.
Break it down into smaller sections for troubleshooting.
&gt; ON vs Where are just syntactical sugar this is not the case, actually
Can you give an example? From my understanding, once the query makes it through most modern query optimizers, you end up with nearly equivalent performance between using JOIN ON syntax vs comma separated joins. Even comparisons where people move entire where clauses unrelated to the join conditions into the ON clause, they report nearly identical performance. 
Good point. Edited.
Glad to see this. Thinking about cross joins first is by far the most intuitive way to think about joins, at least to me. 
I totally agree. I had a professor who basically said that even though ANSI-92's JOIN ON syntax is more readable for enterprise code, he prefers to teach comma separated joins because it forces new folks to imagine a single massive cross join, then a single list of slicing criteria all together in the WHERE clause. I exclusively use JOIN ON syntax at work, but 100% agree that it's pedagogically better in that way.
Not for nothing, if you wanted to write a book, I'd pay to download it to my e-reader. 
That's good to know. If I can put together the time (and get the access from IT) to try it I'll do so.
&gt; Not for nothing, if you wanted to write a book, I'd pay to download it to my e-reader. Well thanks, that's really encouraging to hear! Hopefully some of my SQL blogging on my site has been paying off.
Try creating the Part table before the Supp_Part table. I can see that the Supp_Part table has an FK to the Part table, but it doesn't exist yet. PS: Can't help noticing that your naming convention needs improvement: * inconsistent column naming (mix of casing conventions) - Status, Sname, QTY, S# (is that even a legal column name?), SALARY, Last (last what? call it Last_Name or something), _ID, etc. * arbitrary name abbreviations - why have a table named "Project", but abbreviate another table to "Proj_Work"? You're excused if the rest of the DB you're working with is named in that manner though. Better to keep it consistent.
Since you are posting in a SQL subreddit, I assume you're copying this data from a SQL query result. When I run a query and right click the results \-\-\&gt; copy with headers, I can paste the results \(which are strings containing colons\) into excel and it copies 1:1 for each row from SQL to excel. A bit more information is needed to help you probably. What does your data look like? Maybe a screenshot of some of the data would help. How are you copying it from sql to excel?
This worked! Thanks!! 
Just so I understand you correctly. This query works and it gives you lots of rows for each client, but you want only 1 row per client based on the max date that comes from this set?
Yes - it's currently giving me multiple - essentially listing every single transaction date per client per matter. I was trying to accomplish this with the max(tt.tran_date), but this appears to do nothing in my query. Looking for the max date only. Should take my data from about 48k rows to about 600!
So it's currently giving you multiple because there are multiple "cases" per client, but you just want the one case for each client and that's it?
the max as you're using it will only give you the max date so long as these conditions are always the same: client_name [Client Name] , m.MATTER_NAME [Matter Name] , c.client_code [Client #] , m.matter_code [Matter #] , replace(txt1+txt2+txt3+txt4+TXT5, char(13) + char(10), ', ') [Comments] , convert (varchar(10), cast (m.OPEN_DATE as date), 101) [Matter Open Date] So if a client has multiple matters, but you only one the one matter with the max date, then it won't work. Or if the comments change, then you will get two rows, etc. What you could do (low brow) is wrap your entire query into a sub-query like this: select * from ( &lt;insert your query here without the order by&gt; ) x Then in that outer query you could select only the max rows by date, per client, but it would only give you one matter#, not all matter#'s, etc. You would have to introduce some kind of logic so that it knows what the max is, you could easily do this with a CTE, or a row_number(), or a join on date.
So it's ok that there are multiple cases (matters). However every time there is a time entry, or time recorded on that case - it is assigned a tran_date. The query is pulling back every single time entry or tran_date, when I really only want the latest one. Does that make sense?
So... do you only want 1 matter per client based on the max date, or all distinct matters per client based on max date? 
Here is your query after SQL has parsed it out: select * from (SELECT * FROM TABLE) a join anothertable b on b.id = a.id For SQL's intents and purposes, we can say it looks more like: select * from Table a join anothertable b on b.id = a.id A view is just the metadata of the query pre-compiled and referenceable as an object. This changes when we talk about indexed views, so I'm not going to talk about them and we're going to forget about them for now. So really, this isn't as much a question about how SQL Server uses indexes with views as it's more of a question of how does SQL Server use indexes? Well, it depends. We need to first discuss, is this a clustered or non-clustered index? The clustered index is essentially the physical ordering of the data set and comprises of the table. If the ID column is a clustered index, the table will be physically sorted by that ID. If the ID of the column is a non-clustered index, it will create reference points based off of the RID or the clustered index values. What does this mean? Let's say I have the ID of 1 and my name is Jon. If I were to do a: SELECT * FROM TABLE WHERE NAME = JON It's going to have to scan the table to find me. If I were to do a: SELECT * FROM TABLE WHERE ID = 1 It's going to use the clustered index to find me. Let's say it's a non-clustered index though. If it were a non-clustered index, it's going to start by asking: "I want to find 1. In my first bucket I have values 1-5, my second bucket I have values 6-10, etc. So I know my guy must be in bucket 1." So it looks into bucket 1, and it finds the value 1. Here's where things get a little wonky. Is my name included in the index? The additional column of data may not reside on the same page as my id column. What happens then is that SQL needs to find a row pointer associated, and it has to look up that row from the pointers and find that name value associated to it. If we had INCLUDED the name in the index, it would have had it instantly. This typically creates what we call, a nested loop in the execution plan. Now that we covered the nested loop, this is where we ... loop back... (ba dum tsh) and talk about it depends. SQL is going to say, is it faster for me to scan the table or scan the index? What do my statistics say? So it's possible that it won't even use the index! It's also possible that it will use the index when it's faster to read the table. It's also possible that it will partially use the index. There is a lot of it depends going on. Now when you join the a.id to the b.secondid and the a.id has an index and the b.secondid has no index, you're going to probably use the index on the a side and do a nested loop on table b. Now if table b is substantially smaller, it may scan that table first and then perform the nested loop on an index scan or maybe it will perform a hash match to the table a. Let's get back to your other example. A has an index and let's assume tables b, c, and d do not have an index. It will probably still use the index for table a and then just scan the entirety or the clustered index of the other three tables. Really, you can just consider your view being a subquery. It's not a subquery, but it's essentially one when we are talking about indexes. It's just saved metadata SQL that compiles. [Here's](https://stackoverflow.com/a/1923828/5149122) a stack overflow by [AdaTheDev](https://stackoverflow.com/users/135531/adathedev) also explaining it.
The origin of this question comes from the fact that our IT/DBA group have recently taken some of our more complicated views and turned them into stored procedures where they dump the data into a table, and then they took the original view name and did a `select * from table` so that existing datasources would not be impacted. The problem is that they did not index these new tables at all, and they (not joking) are telling me that they don't need to index them. We're talking about millions of rows and the views that point to these tables being used in complex joins of their own to create other datasources. So realistically I'm wondering if the simple answer is to look at the existing indices on the parent tables, and simply recreating all of them on the new child table, or, and this would take more work, looking through all of the dependent views/procedures that touch these new tables and creating a new array of indices for only the fields which are relevant.
Lets ignore the view and just talk about it as two tables being joined up \- since it functions the same under the hood. select * from tableA A join tableB b on b.id = a.id Lets say that table B is much smaller than A so it filters down the result set. Lets also pretend that both tables only have a NC index on ID as well. What happens? The query engine **MAY** use the NC index on A to match up against the NC index on B. Great, thats nice and quick right? But your SELECT is for all columns. So now it takes A's ID and does a Lookup on table A to get the values for those columns. And it takes B's ID and goes back to table B to get the columns. That may not sound very efficient \- because its not. If there are too many rows, the query engine ignores the indexes and just does a table scan which may perform better than doing the lookups. Now Lets say that in non clustered indexes were contained \(id, description\) and the query was select a.id, a.description, b.id, b.description from tableA A join tableB b on b.id = a.id In this example the indexes contain all the required information so the engine only uses the indexes saving the overhead of having to go back to the base tables. 
Simple dumb question for you as a follow up: select a.*, b.date from table a inner join ( select id, date from othertable ) b on b.id = a.id That will use an index on `b.id` -- providing it fits the criteria where doing so will be optimal. But if no index exists on `othertable` then it won't?
&gt; So realistically I'm wondering if the simple answer is to look at the existing indices on the parent tables, and simply recreating all of them on the new child table, or, and this would take more work, looking through all of the dependent views/procedures that touch these new tables and creating a new array of indices for only the fields which are relevant. This begins the world of index tuning. I would suggest https://use-the-index-luke.com/ and https://www.brentozar.com/blitzindex/. Luke has good information on how to decide on indexes. Ozar has a free tool that makes it very convenient to figure out your indexes. Indexes should primarily be created out of architectural design and theory, tested, measured, adjusted, monitored, and continually tuned. I try to tune 5-10 indexes per database per month, I sit back and collect the stats afterwards. Index tuning is never complete, data changes as does the statistics and how your users use the data. One of the most important things however is the metadata analysis. How do you know your SQL is better than it was last month? You collect, measure, and test.
&gt; they (not joking) are telling me that they don't need to index them that's pretty sad and (probably) wrong. Do you think your queries will need to full table scan these new tables? Did you have a chance to run explain plan on your query with the newly created temp tables?
Indexes are independent. It'll use any of them that are available \- it doesn't need an index on each table / side of the join. if you had an index on Othertable that contained id and date, then it could use the index on othertable. If the index was just on ID, it'll have to go back to the table to look up the dates. So this isn't 100&amp;#37; correct, but you can sort of think of an index as a mini\-table. Its a copy with less data which makes the server's job easier when matching things up. A 3 column version of the data is much easier to deal with than 15 extra columns that are never used.
No, it isn't wrong. I spent 30 minutes on the phone with them today trying to explain why jobs were failing because there wasn't an index, and was repeatedly told that it wasn't the problem. So I selected the data into a #table, created an index, ran the rest of the job and it was fine. &gt;Did you have a chance to run explain plan on your query with the newly created temp tables? No, lowly peasants like me are not allowed to use the optimizer on production databases.
So basically our DBA needs to get their shit together and stop passing the buck. 
counting REC*01 values between unbounded preceding and current row -1 divided by 4 might do the trick for you
I forced them to add the index and rerun the job and it was fine. The nature of this thread is to confirm my understanding (or partial understand) and give me some insight into how these new base tables should be indexed moving forward. At the moment just reapplying the same ones from their parents seems like a waste, which means someone needs to put on some spelunking gear and wade through a bunch of code to see which fields are predominantly being used. 
To be fair I am a clever fucker. I just removed all the pieces that were locked down, disabled the INSERT INTO component, and spit the final data into a ##table, then messaged our DBA and asked if they wanted to just copy the data form the ##table into its proper home so we could "complete the weekly load" or if they still wanted to go through the emergency change request, add the new index, and rerun the process from scratch. They were surprised I could do that with read access. Which is astonishing considering the query is read only until the final step... 
think about it if you're joining, you have to have every value to see what gets joined this means every shard has to give contents to every other shard
so is this just the reality? there's no way i can do a `WITH` and have be just as fast as hardcoding the dates
use of tools like hadoop comes with consequences
This is very true. I didn't fully understand joins until they were explained based on cartesian products in a book I was reading.
Is this pattern always the same (i.e. 4 REC01 and then 2 REC06, REC09, REC10, REC1)? If it is you can just count every 9 rows.
Not sure what flavor of SQL you're using, but you could try this pseudocode: max(trandate) over (partition by client_name, matter_name order by tran_date asc)
Are there any timestamps associated with the records to determine which 01 is the first one?
Look up that message and error number in the documentation for whatever SQL server you're using?
I think the hardest thing for me to grasp is the complex subqueries. :(
Second this! 
I can't open your file, so purely guessing here. Check the data types of your Keys and FKs. Maybe you have a mismatch in data type (char, varchar, etc.) ? If you wanna post the text in a comment, might get more responses. *shrug* 
Sorry, I'm mobile, so no copy/pasting. Going off memory here. Primary key = #emp Foreign key = #emp\ Maybe its reading these as 2 different things? 
Also, looks like the Project table is the only one with slashes *everywhere*, and I know you can't do that in the varchar(45) so I'm doubling down on syntax error. Maybe it's a hidden character in your query editor, if you didnt do that intentionally. Try an editor like Notepad ++, copy&amp;paste your query, clean it, then paste it back. 
Best explanation I have seen. https://blog.jooq.org/2016/07/05/say-no-to-venn-diagrams-when-explaining-joins/
If you can create a stored procedure, this cursor example will number the way you're looking to do it. CREATE PROCEDURE dbo.renum AS DECLARE @i int = 1 ,@a int ,@cr varchar(10) ,@pr varchar(10) IF OBJECT_ID('tempdb..#tt') IS NOT NULL DROP TABLE #tt --Create temp table CREATE TABLE #tt (a int ,b int ,c varchar(10) ,d int); --Put initial values into #tt INSERT INTO #tt (a,b,c) SELECT a, b, c FROM t1 --Set up cursor. Values are record number (col a) into @a, current value (col c) into @cr --and prior value (lag c,1 over a) into @pr DECLARE rn CURSOR FOR SELECT a, c, LAG(c,1,'REC*01') OVER (ORDER BY a) FROM #tt OPEN rn FETCH NEXT FROM rn INTO @a, @cr, @pr WHILE @@FETCH_STATUS = 0 BEGIN --Checks to see if this is the first record in this data series. If it is, increments the counter @i by 1 IF (@cr &lt;&gt; @pr AND @cr = 'REC*01') SET @i = @i + 1; --Updates the unused col d in #tt with @i UPDATE #tt SET d = @i WHERE a = @a FETCH NEXT FROM rn INTO @a, @cr, @pr END CLOSE rn DEALLOCATE rn SELECT * from #tt; Let me know if this does what you're looking to do. 
it's really simple with GROUP_CONCAT oh, wait... just realized you're using the arithmetic addition operator to do string concatenation... must be a microsoft product, ha ha okay, you need to use "FOR XML PATH" i shit you not google [group_concat in sql server](https://www.google.ca/search?q=group_concat+in+sql+server)
When I did it I had different queries to look at the values for the a given record. However, there is nothing stopping you selecting both values from the tables as well (performance depending).
Maybe it's the subquery? Does Hadoop have an expectation path you can analyze?
Depending on the size of data, or the "busy-ness" of the table you're inserting into, you might want to wrap this in a transaction. You've selected the max value once at the start, and are then inserting data from another table based on that max value.... but if you have something else writing to that table, it won't know that, and you'll end up with duplicates and errors with one or the other.
Thanks for the help with this folks. I had to get this up and running, so i temporary put in a job that finds any errors and cleans them up. So it hasn't fixed the issue, but will cover over any cracks till i get more time to take another crack at it.
It's all Excel. If you export straight to CSV you'll be fine.
It's all Excel. If you export straight to CSV you'll be fine.
"just realized you're using the arithmetic addition operator to do string concatenation... must be a microsoft product, ha ha" 
OP's coworker here. There are no timestamps, but we know which REC\*01 \(in each set of 4\) is the first because of the row numbers.
I remember starting out and learning about SQL. I rehearsed with flash cards the differences on joins and how to do fizzbuzz. The first few months were trial by fire, learning all about the nuances of things and how awful it is to forget a where clause. The internet SQL community was incredibly welcoming, understanding, and helpful to my noob self. I just like helping to pay it forward. 
Oh and to specify, I don’t want to see a column for every weekday. This is currently showing an average number of records for the last month. We essentially need to put a filter on it to only pull between mom-fri and 7-430
If you give us the columns names we can come up with a filter clause.
You can try: rank (transdate) over (partition by client_name, matter_name, order by transdate) Then write an outer query where rank = 1
The file is starting out as Excel. Any work-around?
There are many jobs that require SQL. I consider myself a database developer. Other job titles I see thrown around are ETL developer, report writer. When I was job hunting I looked for keywords "SSIS", "SSRS" or "power bi". I'm Microsoft stack, other tools exist in each stack that do similar things. Then there is the BA/BSA role, or the data analyst. Most DBAs aren't writing complex queries, they're managing the database/server relationship. Some get into performance tuning. Each company handles the roles of developer/dba a bit differently. PS, most companies don't care if you've got a degree note what it's in. They care if you know how to do the job. I went back to school and got an entry level position writing SQL after 2 or 3 semesters part time. 
 select order_number, sum(order_qty), sum(complete_qty) from order_table group by order_number having sum(order_qty) = sum(complete_qty)
 select order_number, sum(order_qty) order_quantity, sum(complete_qty) complete_quantity from order_table group by order_number having sum(order_qty) = sum(complete_qty)
There is also the 'Junior DBA' role. On the SQLCommunity Slack, there is a channel for jobs, some people post there. https://www.brentozar.com/archive/2017/04/chatting-slack-watch-groupby/
You may want to use synonyms instead of views. If the remote table schema changes, it'll break the view. Synonyms, on the other hand, merely reference the source table and will not break if changes are made to the schema.
I work in inventory management and learned SQL on the job. Depending on where you end up, either as a programmer working on other languages or just some bureaucratic schmuck amateur like me, SQL definitely be a tool you will need in your toolkit. But, I see more and more job openings also requiring supplementary data tool knowledge like R and Tableu as well SQL "alternatives" like hadoop and NOsql
Looks like backticks to me though so I'm not sure.
You mean TOP 15. 15 TOP gives a syntax error
Job titles: DBA, Data Analyst, Report Writer, Data Architect, Data Scientist, Database Developer, ETL Developer There's overlap in several of those. Full job description? I'm sure you can find a few on Google, LinkedIn, Dice, etc.
If you want your results to look exactly like the table above, except filter for only complete orders: ``` SELECT * from order_table WHERE order_number IN ( SELECT order_number FROM ( SELECT order_number, sum(order_qty) order_quantity, sum(complete_qty) complete_quantity FROM order_table GROUP BY order_number HAVING sum(order_qty) = sum(complete_qty) ) ) ```
I'm an entry level ETL Dev and make 65k/yr
I also work in inventory management and am trying to get better at SQL. Any specific resources you suggest?
I learned the basics from codeacademy and the rest I picked up on the job. I wish I could suggest something other than that but I really learned everything I know from just getting my hands dirty in our dirty databases. I would suggest getting set up with db access and taking an hour at least a day to mess around building SQL queries from stuff in your companys tables, even if it's useless at the moment. 
Highest in demand at the moment tends to be Data Science, Data Engineering and Business Intelligence. Pay can vary a lot depending on the company but pay is relatively good in general. If you are not sure where you'd like to go, a data analyst is always a good place to start. You can pivot into pretty much any data related role without too much trouble. As a Business Intelligence Engineer, I'm currently at 6 figures with 6 years of experience.
I'm just a basic Prod Support Tech Ops kind of guy(official title is Sr Site Reliability Engineer) and a huge percentage of my job is SQL related. From researching the data for members with issues to writing CR update scripts for fixing members production data, etc. I'm not a DBA and don't handle actual maintenance, but actually writing complex queries and data mining is a pretty big chunk of my job. Currently just a little under 6 figures. 
Same how I did it. I ended up learning to integrate SQL and Excel and develop invoicing programs and such through practice on the job. Went from no SQL knowledge to running analytics/dashboarding the company/developing software that works with our database to do other things like invoice, quote, make order forms for our POS, and more. SQL is amazing, no need to learn anything else than your Database right now. Learn the basics and then learn your database. Learning how your database works is the hardest part imo. 
If just needing the order number, this should do the trick: select order_no from orders group by order_no having sum(order_qty) = sum(complete_qty) ;
Your story gives me hope 😁. I'm doing just fine but I'm definitely at the point where my resume matches nothing I went to school for. I still have so much to learn. 
Hey, you’ll always have so much to learn. That’s the fun of life :) got many years to do many things! 
You guys didn’t have an ERP already doing all that?
Yeah thats what im doing now. Ive gotten decent at it but i look at some of the stuff others have done that use complex subqueries and pull data from a ton of different places. Im trying to become high - intermediate level quickly.
I kind of get what they are used for I've wrote a couple but so far anything that ive pulled doesnt require them.
tried, it's saying 'string_agg' is not a recognized built-in function name.
Microsoft SQL Server 2008 R2 (RTM) - 10.50.1617.0 (X64) Apr 22 2011 19:23:43 Copyright (c) Microsoft Corporation Standard Edition (64-bit) on Windows NT 6.1 &lt;X64&gt; (Build 7601: Service Pack 1) (Hypervisor)
If you're getting into Data (Big Data?), then SQL is one tool you will want to know. I suggest looking into R and or Python. I am not sure what program you're going into. I am going into graduate school to get into financial/business analytics. Good luck! 
Data analyst with 4 years of experience making 6 figures. I went to college for information science and kind of learned about data on the job. Also, keep in mind that SQL is just a tool, and your toolbox should have more than 1 tool.
&gt;I've tried googling, but I can't find anything :-/ Yea, that's like 90% of the job. 
All my google results pull up job requirements, and very generic "maintain databases, use sql and r." I was hoping for more in depth, and experience related findings. Most of these replies are pretty helpful.
There's no *good* answer, but take a look at this for ideas: https://stackoverflow.com/questions/451415/simulating-group-concat-mysql-function-in-microsoft-sql-server-2005
Never heard of the term BI Engineer. I am guessing are involved in Data warehouse and ETL?
You really can’t mess anything up with a select query. Find a real world need (purchase to sales analysis), get data models, and just plain figure it out. Validate your data and repeat. Nothing beats real, practical experience.
Yes. Primarily focused on BI, but we do handle ETL, data modeling and DW. 
I do write queries at work already. Just trying to get to the point where i can write really compex ones
Please describe exactly how you partitioned your table. Also give us an idea of how much data it has in each partition. 
Much appreciated! I already found something I like in there lol. Schema! I'll do more study through there. Cheers
For someone who is self taught, how do you prove you know what you are doing when applying for jobs like this? 
Look at a job description and try to make your resume match the job description. IE\- looking for someone with a bachelors degree and 2 years of experience. Get a bachelors degree and 2 years of experience. If you can't do that, get as close as you can. Get a certificate and 2 years of experience working as a programming volunteer a nonprofit. There are lots of nonprofits run by old farts who can't figure out the email and would love a smart young whippersnapper in school to work the email. Great stepping stone for your first internship or job out of college.
How do I flair this? The only options at the bottom of my post are comment edit share save hide delete nsfw spoiler crosspost edit only seems to let me edit the text.
Hey, without having the tables/data set up in a database to test, i think the issue is in your sub query. In the question details it states: 4) A ship found in the Outcomes table but not in the Ships table is still considered in the database. This is true even if it is sunk.&gt; Which means the count of ships per class needs to be a count of the ships in the outcomes table (as this includes ships in the ships table and ships only in the outcomes table). The below (which changes the sub query count should solve the issue (I also made a small change of adding the ISNULL to your count column and changing the second count clause to &gt; 1): SELECT DISTINCT c.class, ISNULL(count(o.result), 0) AS ships_sunk FROM classes c JOIN ships s ON c.class = s.class left join outcomes o on s.ship = o.name WHERE o.result='sunk' and c.class IN ( SELECT 'x' FROM classes c JOIN ships s ON c.class = s.class left join outcomes o on o.ship = s.name GROUP BY c.class HAVING count(o.ship)&gt;=3 ) group by c.class HAVING count(o.result) &gt; 0 
Does using SELECT DISTINCT help?
What kind of variable is popularity? If it is something like number of views, you should select gallery_id, sum(popularity) as popularity_total Then group by gallery_id And order by 2 desc
Only if popularity is the same for all articles of one gallery.
It was my first hit in google, but unfortunately it does not :\( `SELECT DISTINCT news_galleries.gallery_id FROM news_galleries` `INNER JOIN news on` [`news.id`](https://news.id) `= news_galleries.news_id` `order by news.popularity DESC` gave me `ERROR: for SELECT DISTINCT, ORDER BY expressions must appear in select list` `LINE 5: order by news.popularity DESC` but the thing is that popularity is not always unique \(sometimes we have two news with same points\). But for sake of checking further I've ignored it and change sql to `SELECT DISTINCT news_galleries.gallery_id, news.popularity FROM news_galleries` `INNER JOIN news on` [`news.id`](https://news.id) `= news_galleries.news_id` `order by news.popularity DESC` `limit 4` and failed. On 4 rows I've got only 2 unique gallery ids. What's more I don't want my records ordered by anything else than popularity so `DISTINCT ON` will fail as well.
Is integer \(with views or something like that, legacy project...\) I'll try this approach, thank you! 
Take out your group by and just say select top 4
This is the correct answer. 
As your popularity is based on news, which can have multiple galleries, you have to decide the following: Do you want top 4 news, and all associates galleries Or Is there some priority for the galleries so you know which 4 to take (perhaps you don’t care, in which case you can take any 4) So you can do something like: Select top 4 [yourGalleryColumns] From gallery as g Where g.newsid in (select top 4 Id from news as n Order by popularity desc) If you have a priority of galleryids to take, you can just add an order by at the very bottom. 
I actually misread the fact you can have one gallery pointing to multiple news. A better way to do it would be: SELECT DISTINCT TOP 4 g.gallery_id, n.popularity FROM news_galleries as g INNER JOIN news as n ON n.id = g.news_id ORDER BY n.popularity DESC
In SQL, if you can come up with a clean way to do something by manipulating data contained in tables, rather than with procedural code, you generally should. SQL's power comes from its roots in (multi)set theory. We can answer this problem with a single query. In the code below, first we build a [CTE](https://docs.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql?view=sql-server-2017) containing the integers from 0 to 999 (with help from [here](http://www.sqlservercentral.com/blogs/dwainsql/2014/03/27/tally-tables-in-t-sql/)). Note that by cross joining three tables with 10 entries, we get a result set with 1000 entries, and we then number each row sequentially using `ROW_NUMBER()` to get our set of integers (and subtract 1 from each so we get 0 through 999 instead of 1 through 1000). The `SELECT NULL` is just there because we don't actually need to order that result set by anything in order to get the correct result. Then, sum over the `val` column of the CTE (its only column, containing the integers 0 through 999), only adding the integer when it is divisible by 3 or 5, and adding 0 otherwise. ```SQL WITH tally (val) AS ( SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 FROM (VALUES(0), (0), (0), (0), (0), (0), (0), (0), (0), (0)) a(val) CROSS JOIN (VALUES(0), (0), (0), (0), (0), (0), (0), (0), (0), (0)) b(val) CROSS JOIN (VALUES(0), (0), (0), (0), (0), (0), (0), (0), (0), (0)) c(val) ) SELECT SUM(CASE WHEN val % 3 = 0 OR val % 5 = 0 THEN val ELSE 0 END) FROM tally; ``` 
Thanks very much for this comment. You're absolutely right, I wasn't getting anywhere without verifying this. I have found what I can only assume to be the Comms Suite database referred to in the documentation, in the form of a 300MB file on the Desktop of local Administrator called COMMSUITE.FDB God knows why it's in that location, but I think this is what I'm after. A quick Google tells me this is a Firebird database, which is open source. I should be able to have a play from here. w3schools was a nice suggestion as well. Hopefully once I get my head around the database structure it shouldn't be too hard to put together a string that will select only the recordings I'm interested in. Thanks again 
Hello, thanks for this response! I went to wickhill.com but couldn't find this PDF you're talking about. Sounds like I could defnitely benefit from having a look if you're ok to send it my way? 
I got my first SQL job from a staffing firm. They gave me a proficiency test before they represent me. I scored well in the T-SQL test. At the interview, they asked questions about the join types and grouping. If you understand those concepts you understand the foundations of SQL. They taught me how to do stuff and having access to skipped peers, I was able to read code and ask what things did. As a temp contractor, I went out of my way to learn as much as I could while there. IMO a certificate is a waste if time. Some jobs require it. Those places will generally hire without a cert then to keep your job or get a raise you have to get the certificate.
Very likely by hard coding values you have to regenerate a plan. There are exceptions, but they're usually pretty simple queries. Parameterizing queries with sp_executesql is definitely the right idea.
http://www.wickhill.com/uploads/knowledge_library/network_diagram/oak_brochures.pdf on page 4 there're screenshots of reports which provide clues to which field names you could filter on. I suspect they use CamelCase naming, and I see names like StartTime, CallTo, CallFrom, Duration, Direction
I think you could do even better by only generating multiples of 3 or 5. DECLARE @Top3 int , @Top5 int; SELECT @Top3 = 999/3 , @Top5 = 999/5; WITH cteN3 AS (SELECT TOP(@Top3) 3 * ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS N FROM (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v10(N) CROSS JOIN (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v100(N) CROSS JOIN (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v1000(N)) , cteN5 AS (SELECT TOP(@Top5) 5 * ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS N FROM (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v10(N) CROSS JOIN (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v100(N) CROSS JOIN (VALUES (0),(0),(0),(0),(0),(0),(0),(0),(0),(0)) v1000(N)) SELECT SUM(N) FROM (SELECT * FROM cteN3 UNION ALL SELECT N FROM cteN5 WHERE N % 3 &gt; 0) AS n;
I believe you will need to add them as parameters. But in general dynamic SQL often leads to poor performance, pivots also can be a heavy operation. All you need is a load of triggers, function calls in joins/where clauses and a cursor to hit all the performance killers. Personally I always try to keep data rotations (pivots) in the app code, let the web server or client application handle that. You can test it yourself using: SELECT cp.objtype AS ObjectType, OBJECT_NAME(st.objectid,st.dbid) AS ObjectName, cp.usecounts AS ExecutionCount, st.TEXT AS QueryText, qp.query_plan AS QueryPlan FROM sys.dm_exec_cached_plans AS cp CROSS APPLY sys.dm_exec_query_plan(cp.plan_handle) AS qp CROSS APPLY sys.dm_exec_sql_text(cp.plan_handle) AS st WHERE OBJECT_NAME(st.objectid,st.dbid) = 'up_test'
This looks way better than what I wrote. I haven't come across CTEs before. It will take a lot of re-reading for me to get this. Thanks for the help, it's great having the opportunity to learn something new.
My problem is more that I can have a dynamically executed query strait up fail at returning results for customer A while customers B-Z work just fine. And its not that customer A is consistently the same customer either. Every so often the dynamic sql just stops working all together, but tomorrow it will work just fine.
In what sense are you meaning that this is better? Is it faster? You could get away with smaller cross joins with your tactic too, right?
Slightly less efficient but more readable version that the other answers posted here already: declare @Answer int With _Threes as ( select 3 as num union all select num+3 from _Threes where num+3 &lt; 1000 ), _Fives as ( select 5 as num union all select num+5 from _Fives where num +5 &lt;1000), _AllNumbers as ( select num from _Threes union select num from _fives ) select @Answer = (select sum(num) from _AllNumbers) option (maxrecursion 1000) select @Answer;
What SQL engine?
Sorry, forgot to tag it. It's ORACLE.
WHERE ISNULL(stvcamp_code, 'MA') IN ('CP', 'DL', 'ED2', 'EDD', 'EL', 'EM', 'EMH', 'GD', 'MA', 'OS') That's it! that should be all you need
Awesome, thanks! I'm far more experienced in PHP, CSS, HTML, C++, etc than SQL. Would you mind please explaining the logic?
Thank you. That does exactly what we need \(though I'm still not entirely sure how it works\) :\)
Simply put... You are treating any NULL Value as 'MA' when matching in the where clause. 
Ok thats what I kind of thought but in the results it will return a null in the cell and not MA, correct?
correct. You can SELECT with that line of code as well if you wanted to display MA &gt;SELECT NVL(stvcamp_code,'MA') as stvcamp_code
Okkk, where you went wrong... here's some: 1) please stop with the "distinct" stuff. It might be because you have been burned by getting duplicate results by getting your data granularity wrong or you've been damaged by studying relational algebra. Regardless, throwing 'distinct' everywhere usually means that you are not comfortable controlling data granularity (a major part of understanding data in general, in my view) and you're not going to learn by not doing it. 2) Logically, sql's 'select' command executes "FROM" clause first which gives you the 'baseline' result set and "WHERE" clause works on that result set of the 'FROM' clause. So, when you use a simple filter condition on the field that came from a left join, you are 'negating' the "outer" in your join. 3) "count" is actually 3 different functions: "count(*)" - number of rows in a data set, count( &lt;expression&gt;) - number of records where &lt;expression&gt; is not null, and "count( distinct &lt;expression&gt;)" which gets the list of distinct &lt;expression&gt; values, discards NULL and counts the number of records in the result. So, specifically, count( columnthatnevergoesnull) does, in fact, give exactly the same numerical result as count(1).
I don't think it practically matters, but I like avoiding sorts and aggregates when I can. WITH Series(Element) AS ( SELECT Ones + 10*Tens + 100*Hundreds FROM (VALUES(0), (1), (2), (3), (4), (5), (6), (7), (8), (9)) Digits(Ones) CROSS JOIN (VALUES(0), (1), (2), (3), (4), (5), (6), (7), (8), (9)) Digits2(Tens) CROSS JOIN (VALUES(0), (1), (2), (3), (4), (5), (6), (7), (8), (9)) Digits3(Hundreds) ) SELECT Element FROM Series; 
I do not think brute force is the approach meant by the problem. Let's take our upper bound to be a hundred billion, for example. How's that going to work?
While this approach is valid, running a function (in this case, NVL) in the WHERE clause will cause the engine to ignore any indexes on this column. As far as optimization goes, it’s all ignored by running NVL on that column unless there’s a function based index on it. I don’t see why the following can’t be used instead: WHERE (STVCAMP_CODE IN (‘MA’, ‘CP’, etc) OR STVCAMP_CODE IS NULL) 
This is just for SQL PRACTICE [http://practity.com/591\-2/](http://practity.com/591-2/)
My current logic... WITH today AS ( SELECT datestamp FROM bugs GROUP BY datestamp ORDER BY datestamp ) SELECT today.datestamp, count(id) FROM today, bugs WHERE bugs.datestamp = today.datestamp GROUP BY today.datestamp ORDER BY today.datestamp; The above query properly gets a list of the dates and the count for the total ids for that date. But when I try to add a subquery to get the count of ids that are in yesterday's datestamp... WITH today AS ( SELECT datestamp FROM bugs GROUP BY datestamp ORDER BY datestamp ) SELECT today.datestamp, count(id) FROM today, bugs WHERE bugs.datestamp = today.datestamp AND id IN ( SELECT bugs2.id FROM bugs bugs2 WHERE bugs2.datestamp = yesterday(today.datestamp) ) GROUP BY today.datestamp ORDER BY today.datestamp; It seems to either be stuck or taking a long time (5+ minutes) which probably means I've done something horribly wrong given my dataset is &lt;10k rows... So whats the proper way to run this subquery basically once per (distinct) day? It might be running it for every single instance of each day or something currently...
Fail in what way, are you getting an error, or you just mean it takes so long it never completes? 
I made another version that works in an Oracle database that is the equivalent of the one /u/arfior wrote, but uses different syntax. Maybe it will help you see what is going on to have another example: WITH nums AS (SELECT LEVEL num FROM dual CONNECT BY LEVEL &lt; 1000) SELECT SUM(CASE WHEN MOD(num, 3) = 0 OR MOD(num, 5) = 0 THEN num ELSE 0 END) sum_nums FROM nums;
You might find lag() function useful
Here is a solution: with a as ( select distinct datestamp from bugs), b as ( select datestamp, LEAD(datestamp) OVER (ORDER BY datestamp desc) AS prev_date from a) select b1.datestamp, bug1.id from bugs bug1, b b1 where bug1.datestamp = b1.prev_date and not exists (select null from bugs bug2 where bug2.id = bug1.id and bug2.datestamp = b1.datestamp) order by datestamp, id 
Here is a solution. To get the counts you wanted: with a as ( select distinct datestamp from bugs), b as ( select datestamp, LEAD(datestamp) OVER (ORDER BY datestamp desc) AS prev_date from a), c as ( select b1.datestamp, bug1.id from bugs bug1, b b1 where bug1.datestamp = b1.prev_date and not exists (select null from bugs bug2 where bug2.id = bug1.id and bug2.datestamp = b1.datestamp) ) select datestamp, count(id) from c group by datestamp order by datestamp And if you want to see which ones were closed (by number) day by day: with a as ( select distinct datestamp from bugs), b as ( select datestamp, LEAD(datestamp) OVER (ORDER BY datestamp desc) AS prev_date from a) select b1.datestamp, bug1.id from bugs bug1, b b1 where bug1.datestamp = b1.prev_date and not exists (select null from bugs bug2 where bug2.id = bug1.id and bug2.datestamp = b1.datestamp) order by datestamp, id
it's trickier than that because he is not just comparing the aggregate count of IDs day by day. He has to actually compare the ID values one day to the next.
For some additional fun, I thought you might like this view of the data that shows, by ID, the date the bug was opened, the last date it was open, and the date it was closed: with a as ( select distinct datestamp from bugs), b as ( select datestamp, LEAD(datestamp) OVER (ORDER BY datestamp desc) AS prev_date from a), c as ( select id, min(datestamp) over (order by id asc) min_date, max(datestamp) over (order by id asc) max_date from bugs) select id, min_date opened_date, max_date last_date_open, datestamp closed_date from b right join c on max_date = prev_date order by id
I can get the results I want with a top 10 in the select, but that's probably not what they want lol. 
I would add the [MS SQL] prefix to the title if I could, didn't know that was something I was supposed to do. Oops.
Use GROUP BY to isolate the records with the most sales for each territory, then use those values to get the rest of the info.
Much appreciated, I will look into these later tonight or tomorrow morning (assignment isn't due till next week). Thanks for the help.
If you check out the last statement from /u/skullki2424 comment (it wasn't mentioned in the OP), you'd see that they had 95% of a solution done and needed the lagged value instead of yesterday. Could have been a learning opportunity. Oh well. 
if they person shows up multiple times you are going to get each record with your request. It looks like the instructions specifically ask you to use max. To the below mess you would want to use Group by to make the max work. You are pretty close just need to make a few changes potentially. USE [AdventureWorks2012] SELECT PP.LastName ,PP.FirstName , ST.Name ,max(SP.SalesLastYear) #this will get the max based on the other things in the table that arent part of the max when we use the "group by" command FROM SALES.SALESPERSON SP JOIN SALES.SalesTerritory ST ON SP.TerritoryID = ST.TerritoryID JOIN PERSON.Person PP ON PP.BusinessEntityID = SP.BusinessEntityID order by sp.SalesLastYear desc,st.name #here we want to group by everything that isnt part of a max, sum, count, ... function Group by PP.LastName ,PP.FirstName , ST.Name
Hmm, I think something might be off here. When I try to add the Group by after the Order by, it's incorrect syntax. When I add the Group by Before the Order by, it throws an error because Sales.SalesPerson.SalesLastYear isn't in the group by clause, and if I replace the Order by with the Group by I get the same results that I posted. 
Scratch my previous reply... I didnt read the assignment thoroughly enough and look enough at the table definitions. Also, I normally run queries and make sure they work and such. Doing this via text without validating the query is hell on my simple mind. SELECT PP.LastName,PP.FirstName, ST.Name,SP.SalesLastYear FROM SALES.SALESPERSON SP INNER JOIN SALES.SalesTerritory ST ON SP.TerritoryID = ST.TerritoryID INNER JOIN PERSON.Person PP ON PP.BusinessEntityID = SP.BusinessEntityID #following query finds the max per state and puts it in a query all by itself then innner joining on that would only return those items. If you select it with your mouse and run just it you will see it find those data points when you hit execute. Inner join (SELECT ST.Name ,max(SP.SalesLastYear) FROM SALES.SALESPERSON SP JOIN SALES.SalesTerritory ST ON SP.TerritoryID = ST.TerritoryID Group by ST.Name) as MS on st.name =mx.name and sp.saleslastyear=ms.saleslastyear 
Thanks for the reply! I think this one is very close, I'm getting errors on the last bit. After the last parenthesis I get a red line on MS, =mx.name, and ms.saleslastyear . Below are the errors I see Msg 8155, Level 16, State 2, Line 39 No column name was specified for column 2 of 'MS'. Msg 4104, Level 16, State 1, Line 39 The multi-part identifier "mx.name" could not be bound. Msg 207, Level 16, State 1, Line 40 Invalid column name 'saleslastyear'. 
Should be Ms not mx
Can u do a cte first to get the max date without all the user, employee, client details. Then join those after u get the max date... Sometimes that helps me when I should be filtering my joins real close..
Try putting 'Set isolation level read committed' at the start of the procedure.. If it doesn't hurt performance it should solve your problem. 
Had to do this exact thing this week. I tried using fancy custom code in ssrs to see the difference. Nope never worked right. Put in sql it was done in 20
&gt; didn't know that was something I was supposed to do. RedditProTip: Always read the sidebar and Submit pages before submitting. Tagging your title is noted in both places on this sub.
Nice, getting close, only 2 red lines now! Here's what's left (thank you SO MUCH for continuing to assist so far!) Msg 8155, Level 16, State 2, Line 39 No column name was specified for column 2 of 'MS'. Msg 207, Level 16, State 1, Line 40 Invalid column name 'saleslastyear'.
I'd do the CTE over the temp table and one table each for the various objects.
It 100% depends on the data sets. I think the only "general" rule that applies here is that you can update data in a temp table without modifying the underlying data. Which you can't do with a CTE. Otherwise, ask your DBA to give you rights to look at the Execution Plan imo.
In SQL Server, CTEs are (primarily) syntactic sugar; they are **not** materialized, nor are they indexed beyond the underlying tables. So if you're referring to a CTE multiple times in a single query, you will be executing that query *each time* it's referenced. No different from inlining that whole query. So, if you're going to reference the CTE more than once in a query, make it a temp table instead. Also, if the underlying tables don't have indexes that support your specific use case, you can index a temp table to make up for that. Being restricted to views (instead of having access to the base tables) does not prohibit you from viewing execution plans; that's a separate privilege and IMHO there's no reason your DBA shouldn't be allowing you to view plans if you're writing queries against that database. Word of warning about views: if you nest them too deep, statistics go out the window and you get terrible row estimates, leading to bad execution plans. How deep is too deep? No one's 100% certain, and it seems to depend on the data and complexity of the views. Going more than 2 layers deep is definitely inviting trouble.
Share an actual execution plan, and I can give some specific pointers. But generally speaking, it depends.
Throwaway account? Homework assignment? Ug. Figure it out.
Change max(saleslastyear) To Max (saleslastyear) as saleslastyear
This. I have had times where a CTE was fine and others were I needed to dump they data into a temp table so I could index it for downstream use. One general rule I do have is to put the data in temp dB if it will be used multiple times though. I’d definitely ask for the ability to see execution plans. I’d download SQL sentry too. I found it to be really helpful when analyzing larger queries.
YOU DA MAN!
&gt; I’d download SQL sentry too https://www.sentryone.com/plan-explorer Just looking for "sql sentry" may land people on the old name and the monitoring suite, not the awesome execution plan viewer.
Final results here, thank you so much for all the assistance! [Query Results] (https://imgur.com/YwYEDGZ) 
1. Sorry it took so long. 2. Do you understand what we did and why? 3. Keep learning and posting 
Noticed you’re on SQL developer, so assuming ORACLE (or MySQL, but they’re similar enough) https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_7004.htm Some pseudo-PL/SQL (I’m on mobile) CREATE TRIGGER schema.trigger_name AFTER DELETE ON schema.tableName —Update the other table’s record here where the ID matches the deleted record’s ID. UPDATE SCHEMA.OtherTable SET column = newvalue WHERE OtherTable.Key = DELETED.KEY COMMIT; 
create or replace TRIGGER schema.opravypracuje AFTER DELETE ON schema.zamestnanci UPDATE schema.opravy SET pracuje = 'F' WHERE opravy.zamestnanci_id = zamestnaci.id COMMIT; doesn't work 
What is the SQL platform and version you are using?
oracle sql developer
&gt; WHERE opravy.zamestnanci_id = zamestnaci.id Maybe because zamestnaci.id is GONE, because you just deleted it, and you are trying to refer to it in an AFTER trigger. Maybe try this instead? WHERE opravy.zamestnanci_id = :OLD.id
I think the problem is your select after if not exists. Your not checking this against the inserted data, just saying select userID from users with no where or relation to inserted. 
Not enough information. There is no one size fits all. But since you are looking for general rules, I'll throw a few at you: 1. If there is no performance difference between two equally efficient solutions, choose the one that is easier to read and maintain. 2. Relational databases are good at doing joins. Do not be afraid to have many joins in a query. Usually in an operational system is is good to reduce redundant data in order to enforce data integrity, to store it more efficiently, and to maintain related data easier. 3. Despite my second point, it's not wrong to exchange "space for time" as you will often see in data warehouses. Sometimes schemas are denormalized into fewer tables either for read performance or ease of use. 4. Hybrid approaches are ok as well. Sometimes data is stored one way for the operational/transactional system, and then a process that runs weekly/daily/hourly gathers the data into another table for reporting and analysis. Beware of anyone who says "views are always faster" or "tables are always faster" or whatever. They can be slower, faster, or the same, depending on many factors.
&gt; Being restricted to views (instead of having access to the base tables) does not prohibit you from viewing execution plans; This may be DB-specific. If OP is using Oracle, then select privilege is necessary on the underlying objects of a view in order to generate an Explain Plan. One Oracle workaround I've seen is where the DBAs set up a stored proc to generate a plan under a user with access to the underlying tables. Non-DBA users can generate a plan with the stored proc. This doesn't integrate nicely into GUI tools like SQL Developer, it instead generates a simple text plan, but works fine.
Maybe. Computers are pretty fast. I haven't tried 100 billion, but I tried 1 million and it came back in less than 1 second.
Good catch. As written, this would only work if there are no rows in the User table.
Good eye, I knew it was something trivial that I just missed. It always good to have someone to look at your code and spot the mistake, heh. Thanks, fixed it and it seems to be working fine.
Awesome! 
I see your issue is already resolved, but I must state for my own conscience: avoid the trigger if you can. If possible, put this logic in the stored procedure(s) that insert the data. Triggers can be fickle and increase the scope of your transactions. Avoid them when possible. :)
I cannot say for sure how it works in MySQL, but in SQL Server it would be a different [Update] statement for each table. The row of data in each table is subject to row locks, page locks, and table locks. Each statement would happen in the same query/procedure meaning it would all occur as one transaction.
I understand the first joins, and I believe the second SELECT was having issues because we selected 2 columns for the join so that had to be joined on these two columns (as MS on ST.Name = MS.Name and SP.Saleslastyear = MS.Saleslastyear), but why did we have to alias max(SP.SalesLastYear)? Is it because it's not being seen as a column? 
Currently work as a BA in a healthcare company. Spend about half my time in SQL server and the other half organizing/implementing business rules within and outside of our company. I want to get better at SQL. I can handle joins and have dabbled with subqueries in the where clause mostly. Lately I've been working on perfecting my "case when" usage. My question is what is considered "complex". I've looked online for practicing harder stuff but I'm struggling to find anything. 
Unfortunately I wasn't able to get lag to work with my 95% solution. It changes what the query does and you no longer get the right numbers at all. Also wouldn't lag be the same as leading by -1 or changing dense_rank() to look at datestamp desc? 
Yeah anytime you use count , sum , max or any agregating function it comes back as a column with no name. The as gives it a name. 
SCHEMA is whatever schema you’re deploying to. You also need a semi colon to finish the UPDATE statement before the Commit. As stated above, you also need to ensure you grab the DELETED record using the :OLD syntax
I agree, this should be checked on the front end with a lookup method of some sort. If it fails, don't even try the insert and end the sub or function. I've found myself many a times troubleshooting an issue that turned out to be a trigger and had to use Alter Table Disable Trigger All to run some SQL to fix said issue. Triggers are so easily forgotten about. Though, in defense of OP, its not likely this is an issue with a simple raiseerror and they want to avoid confusion for the user assuming they get an FK error on the front end and will likely not know what that is. 
Well thanks for the insides, unfortunately I had a pretty terrible teacher who teaches us to use triggers like this. This is actually for the assignment I'm currently working on.
Thanks for the insides, but don't worry I'm only doing this for a school assignment.
let's say you do this: WITH today AS ( SELECT datestamp, lag(datestamp,1) over (order by datestamp) as prior_day FROM bugs GROUP BY datestamp ) .... think you can do it then?
Thanks a ton for these. I'll get back with the data on Monday to play around with it. The chaining of the 'with' queries makes it pretty straightforward to understand too rather than the giant messes I had.
Makes perfect sense. Thanks a ton for the help, glad I was able to learn something.
Thanks, it was fun! I hope it works out for you. 
I believe unless you're explicitly locking tables during, multiple reads are okay from the user's. Look into commited and uncommitted. At that point though, you're at the mercy of the hardware. 
I am not a MySQL expert, but your system will most likely be pretty functional. Since you are running "hundreds of thousands" of individual queries, this is different than running one massive, 5-minute query on a single table that will hold a lock and block other queries. Your user queries should be able to get in there, unless you are starting a single transaction at the beginning and committing it after 5 minutes. On the flip-side, you're running just a single SSD. Most of my experience with production systems involve SANs and other disk arrays, so I'm not really sure if that will cause any performance issues.
Is this SQL Server? You can pass a table as a parameter to a proc.
 SELECT SUM(N) AS Sum FROM N WHERE (N % 3 = 0 OR N % 5 = 0) AND N &lt; 1000
Yeah it's SQL Server unfortunately. I'm gonna have a look at this when I get up I'm not quite sure it's gonna work for the exact task I need it but I can give it a try, first I need some sleep.. 5am. Thanks for your input.
You've got a lot of options! Like MaunaLoona mentioned, you can insert into a temp table and pass it as a parameter. You could also have a single variable with a comma delimited list you parse out. If the count is truly only 1-3 names, you can always just make three optional parameters for names and only use them if they're not null. And finally, it's always worth taking time to think - is the logic of which names I need when something the database itself can figure out at runtime?
If you have a problem you could reduce the transaction isolation of your SELECT. By default MySQL is more stringent than you probably need it to be: https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html Also, you should consider moving to a raid array instead of a single SSD. There's very few good reasons to run a DB on a single drive. 
You can also try passing in an XML parameter, that should do what you need
Based on your description, this sounds like a proc with optional parameters (I.e. defined with = NULL or some other default value), and then some logic to check if the parameter was supplied in the body of the proc.
https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-table-valued-parameters-database-engine?view=sql-server-2017 This is how I would do it.
You need to look into table tops that are designed for this sort of work. An example would be RTREE in sqlite3 or mysql.
This looks exactly like the thing I need ! Thanks a lot for your answer !
You're welcome
Do you think it is normal that my SSD only shows 1-8mo/s during the task ?
Alright, thanks
I will check that, also I may find a VPS with Raid 1 but nothing amazing
That's 2 parameters where the second is a list. Use fn_split inside the sproc to break up the list.
If the RDBMS you're using has GIS extensions or built-in functions, you can define your locations as the appropriate data type, then search by distance (range). [Example](https://stackoverflow.com/questions/10192498/search-nearby-points-from-a-geography-column#10194237) Tinder probably uses a variety of techniques to do their searches (haven't used the app myself) to manage performance. They may narrow by ZIP code (there are APIs to get the ZIP code you're located in from coordinates), then do a radius search that only includes the closest N ZIP codes (ZIP codes are assigned roughly geographically).
Not sure why you say "unfortunately". SQL Server is possibly the best RDBMS right now. TVP is the solution.
XML rarely makes things better.
&gt; Not sure why you say "unfortunately" Because some people are in denial about Microsoft as a company having evolved over the bast 10 years (past 5 especially).
Wait, do you have a different table for each product? Something seems off here. You shouldn't have to join a table for each product.
I think there's a ton of "Microsoft sux!" general reddit pandering too. Gotta get those easy karma points in /r/programming 
Boutell.com/zipcodes I would start there and import the CSV as a table. Then just add/remove as you see fit. I used it as a foundation for a location table which powers my PowerBI location features, and it’s been great so far.
Well I have a table for each kind of product. To be more specific we produce blinds for Windows, since of them a are motorized and once in a while we ship then with extra cables if the customer demands needs it. So I would have a table describing the specification of the blinds and another describing cables. Both of so I might have 2 entries in a 'blinds' table and 1 entry in a 'cables' table, belonging to the same order
&gt; Well I have a table for each kind of product. That's a red flag there. Try to have a foreign key to product sku or something. See if you can generalize qualities shared by products (dimensions, weight etc.) and map them as needed. I can see how you'd need different ways of describing products though. Punting, you might find how someone else did this in some open source software. One might map product_type as a parent to product_detailed_type and have product_detailed_type be parent to product_attributes. So there could be product_type = 'book', product_detailed_type = ('dictionary', 'nonfiction') and product_attributes = ('number_entries', 'language', 'grade_level'). I think you see where this is headed.
Well no. I really don't care about karma points whatever that is, considering karma in my language means "dog food", anyway. I only been learning SQL for the past month tbh and I used Postgres, MySQL and SQL Server and for some reason SQL Server just feels so bad comparing to the other two. On top of that I was kind of force to use SQL Management Studio or whatever is called and that is just horrible horrible horrible scary thing. It's slow, it's annoying, it's clunky most of the time it doesn't work properly I have to restart it all the time because it's not referencing objects that have been recently created. I felt so much better using InteliJ with datagrip plugin, adn then you have stuff like Navicat Premium which is just so good in my opinion. I don't know if people actually use management studio for sql server I just hope they don't because that might lead to suicide. I don't want to be rude or offend anyway I know people use SQL Server, and Management Studio for whatever purposes. On top of everything I said I'm not a Windows fan when it comes to productivity and time management I generally think that on Windows you have to waste time on stupid shit constantly.
Order &lt; OrderProduct &gt; Product = ProductCharacterisic1, ProductCharacterisic2, Etc Don't create totally separate tables for different kinds of products. If you must have new tables to describe something, create it as a 1 to 1 relationship with your core Product table.
CTRL + SHIFT + R And you're comparing a free IDE with ones that cost hundreds of dollars per user per year... but yeah SQL Server is an A+ tool. I've used them all, but over the past 12 years, not just one month. 
If you have separate product tables (with more in the future) and your order table is putting the product ID in the same column. You should look into adding a new ETL process and union all of the tables together into one FinalProduct Table. I would not recommend a view of the union unless these are really small tables with few columns. The set up of the union might be a pain but trying to do what you are asking sounds like a nightmare. 
Yeah I think the biggest issue here is that OP has the relationship from orders TO products and not the other way around. SalesHeader (Sales header info, customer info, etc. 1:M with SalesDetail) SalesDetail (Individual product lines with sales &amp; related info, 1:1 with product table) Product (Product header info, product name etc. can have any number of 1:1 or 1:M relationships with other tables such as...) ProductPrice (current and possibly historical pricing) ProductCategory (product categories) &lt;---- THIS is what you can use to solve your problem ProductInventory (stock/on hand info) etc. etc. 
Yes, but it just seems like making the tables overly convoluted by making the genetic enough to incorporate all the characteristics of possible products. For example if one were selling remote controllers and the occasional cable, then there's a huge difference in describing the number of ways a remote control can be put together for a certain use case versus just selling a 5 meter cable. Or am I missing your point?
Then the next reasonable solution is to make another table for characteristics and put a record in the table for each characteristic for each product.
Never thought of that. Will definitely see if that could work. Also yes, as I read the replies a realise that I need to be more liberal about splitting up my tables
It depends on whether you need these attributes to be queriable in an efficient manner. You could always do something as simple as having a metadata field on column and adding the attribute json metadata to that. It would still be queriable ( JSON supported in MySQL nowdays for queries ) though not sure how efficient. Another approach is the following: You can create an attributes table that has roughly two columns: attribute_name | attribute_value This isnt really much different from the original approach and it begs the question which one of the two is better to implement. So end of the day, both approaches will work, not sure which one's better, if somebody knows please illuminate me as well.
lol 
Well I'm using it on a £300 laptop inside a vm so I'm not expecting much but I'm expecting something to work as intended. It's not like I can carry my £2000 PC on my back.
Just repeating the truth for the masses. 
If it is a very simple query and it is safe to do so, sql server will auto-parameterize it for you and then you will get plan caching even though you hard coded parameters in string, otherwise you must parameterize in order to benefit from plan caching. http://www.benjaminnevarez.com/2010/06/auto-parameterization-in-sql-server/ 
It's not the truth, it's your opinion. You'll learn, eventually.
I've read over this comment a few times now and tried in my head to apply it to what I'm doing. I think this is what I will try to apply in the end or something of this model at least. I like your productCategory table but is it really M:M? So that would be a table dedicated to all the parts that make up a product is that correctly understood?
Lolol hilarious. 
Personally I would always try to avoid metadata columns, even if they are supported, and always tend towards more rows than more columns. I'd go with the attributes table design, its flexible enough to deal with any issues, even if attributes were discontinued you could add effectivefrom and effectiveto dates. Querying performance may take a slight hit (and more cumbersome queries), but in most cases you could cache them quite easily
Hey there. Someone might be able to help, but if not, maybe try /r/SQLServer Here SQL refers to the language, not MS SQL 
You probably just need 1433 tcp for a default instance running on the default port. 1434 udp and 1434 tcp are used for SQL browser service and the dedicated admin connection.
If I'm understanding correctly, one way would be to use a self-join with the dates as the criteria for the join. Something like this would be close, but I don't know Presto syntax. ` SELECT a.timestamp::date , a.account_id , count(distinct b.user_id) FROM logins a JOIN logins b ON a.account_id = b.account_id AND b.timestamp::date &lt;= a.timestamp::date AND b.timestamp::date &gt; (a.timestamp::date - 30) GROUP BY 2, 1 `
I haven't actually installed SQL Server in several years. (Cloud, yo.) What is the "master port" you mentioned? 
5\-year\-old? No. 5\-year olds don't know what logarithms are, or what big\-O notation is. But predicated on the idea that you know both of those: First, make sure you have a complete mental model of what an index is. For what it's worth, I find SQLite's documentation on the subject to be the best explanation I've ever seen: [http://sqlite.org/queryplanner.html](http://sqlite.org/queryplanner.html) Having read and internalised that, it's easy; a clustered index is how the on\-disk table is stored \[where SQLite only described being able to do it by the PK\]. It thereby avoids one of the two O\(log\(n\)\) lookups involved in a non\-clustered \[ie, a "normal" index\]
Let's say you've got a lot of books. They're in a big pile, because you recently moved in and it was a pain to shelve them. That's a heap. But that pile isn't helpful. So maybe you compile a list alphabetical by genre. That way, if you want, say, all of the fantasy books, you at least know where in the pile to look for them. Those lists are non-clustered indices. It takes time to write the lists and paper to write them on, but they can be useful. And obviously, you can have as many as you want. If you also want to have a list alphabetic by color of the cover, that just takes more paper and more time, but you can do it. But let's say you finally get sick of the pile, and you put them on the shelf alphabetically by author, like a book store does. This is a clustered index - an index that actually manipulates where the files are on the disk. It has the advantage that searches involving the author will be lightning fast, since you don't even need to cross-reference the list - your shelf is already in order. And in one sense, this also saves "paper", since you don't actually need a list - your bookshelf IS the list of books by author alphabetically. But, it also means that shelving new books and taking out old ones takes more time than back when you had the pile of books. Also, if you do a lot of adding and removing books, there will probably be some annoying gaps in your bookshelf - you packed it efficiently back when it was new, but keeping it alphabetical meant a lot of wiggling stuff around as you got new books. Maybe twice a year you take all of the books off the shelf and re-put them back in alphabetical order just to remove those annoying gaps before you had to buy another bookshelf - that's defragmenting. 
Can some explain ELI5 like u/chunkyks is five?
Clustered Index is an ordered entity holding data. Let's say you have a table storing employees data. By default the data is stored in the same order in which it is inserted. Now let's say you create a clustered Index on it on column employee id. Once you do it, all the data is arranged by employee id. So 1 employee id will be stored and then 2 and and so on.
The eli5 analogy use is a phone book which are physically sorted by last name. In a database there would be pages at the start of the phone book that give hints as to what page to go to to find a given last name. The regular pages of a phone book hold all the information for each record (first name, address, etc.). To take it a step further a non clustered index would be like the index at the back of the book (first name, street number, or street) and tell you which page(s) you can go to get the rest of the data.
KISS, baby. Keep it simple, stoopid.
/r/IAmVerySmart
/r/IAmVerySmart material
What's the extension on this SQL file you speak of? Is it on a server or did you just download it?
I just downloaded it. its SQL file type. I cant open it with Mysql,microsoft sql,notepad++,. I want to easily search some data in this sql. 
If it's a ".SQL" you can open it using notepad. But a query file of 400 gb is kinda odd. Maybe it's a script to build a database with the data in the script as a bunch of insert statements. 
how can i run it?? file is too large for notepad
Not OP, but I have some follow up questions. Where does the distinction of clustered/not clustered come from? Let's take the phonebook/dictionary analogy, the clustered index is like the table of contents. Is the 'cluster' defined by the fact that the name/word list is sorted? Or is it because you can get hints that some range of words/names are stored in some on-disk location? If we consider non-clustered index, this would be like the index, which references the data by other classification means, like area code, residential/commercial, part of speech, etc. Is this correct?
Is there any way to test this sort of thing?
Maybe this link could be helpful https://www.mssqltips.com/sqlservertip/4155/executing-large-scripts-in-sql-server-management-studio-with-insufficient-memory-failures/
Also a small point to add, in SQL Server at least, a clustered index is not the same thing as a primary key, although the two usually go together.
A regular index is a list of all the rows in the table in some order. A clustered index not only lists the rows in order, but also changes the order of the rows in the table to match the order of the list (index).
Thank you. bookmarked.
clustered means that the books are actually in order on the shelf. nonclustered means that you have a picture of the pile of books (the heap) and alongside that picture you've got a list of the book titles sorted A-Z with their coordinates in the picture... you can search the title and find it in the heap, but it's a two step process (find the coordinates / address-on-disk, then go to that location).
A disk drive is at low level a rewritable set of bits in a sequence. When it's not clustered, a separate index is written elsewhere in that sequence but the data remains in the original order. When it \*is\* clustered, the data is rewritten to that sequence of bits in such a way that it is \*stored\* sequentially.
Let's say you've got two tables with orders data, A and B, and you need to combine them to get a full report of all of your orders. Maybe there is some overlap between A and B; so some order_id are present in both tables. Maybe the order_price is sometimes missing in A but present in B and vice-versa. Then you could do: select coalesce(A.order_id,B.order_id) as order_id coalesce(A.order_price,B.order_price) as order_price from A full outer join B on A.order_id = B.order_id The result will be more "dense" than A or B in the sense that now, for every order_id, you have order_price available if it's available in either table.
example: in a LEFT OUTER JOIN, retrieve default options (the left table) along with any override options (the right table) then you can use `COALESCE(override_option, default_option) AS option` so when there's no override option, the outer join returns NULL there, and you end up with the default option
Wouldn't he need to start with a calendar of some sort, as some periods might not have any logins? Maybe he doesn't care about that or i'm missing the requirement.
reformat your list of values (a good text editor makes this trivial) into a subselect, and join that SELECT lst.item , tbl.data FROM ( SELECT 'value1' AS item UNION ALL SELECT 'value2' AS item UNION ALL SELECT 'value3' AS item UNION ALL SELECT 'valueN' AS item ) AS lst INNER JOIN tbl ON tbl.item = lst.item 
I am not going to reinvent the wheel, please see this [article](https://www.mssqltips.com/sqlservertip/2689/deciding-between-coalesce-and-isnull-in-sql-server/) by Aaron Bertrand on MSSQLTips. I will touch on one piece however. &gt; Also, I'd like to know best practices of when it is most useful to use it particularly in combination with joins Typically, you want to join on a value that is easy for the computer. This is typically done with int type values. However, you may need to join on another type of field, date, time, varchar, etc. Sometimes that column you have to join on could be null, or it could be blank. Best practice is to link your tables and data points with int based value types using constraints and foreign key relationships so you will not have those circumstances I described above. Sometimes, this still happens. Now, this next piece is not best practice but it can happen. Normally when this happens, it spurs a discussion about architecture. SELECT * FROM TABLE1 INNER JOIN Table2 ON COALESCE(ID.Table1,'') = COALESCE(ID.Table2,'') What is happening in the above? Let's take one step back, what's a NULL? I have three apples in my hands, I step behind a door and I may have set any number of the apples down. You have 5 apples in your hands currently. How many more apples do you have than me? You probably answered I don't know, that's essentially a NULL. It is an unknown entity. It is not a number, it is not a character, it is unknown. In my example, you had a hard time comparing how many apples we had because you didn't know how many I had. SQL runs into the same issue. So what we are doing is taking everything that is unknown, and we are telling SQL that unknown entity should be classified as a blank value. &lt;''&gt; Now that we have SOMETHING that is tangible, SQL can compare the two fields. There are two issues here. 1. The data is not relational in a lot of circumstances. If I am trying to join on an ID that can be null and I want it to join on the nulls, you're probably going to get a lot of nonsense data. You are basically trying to make SQL join two tables together on a field it knows nothing about and just happens to match. It could cause a Cartesian join that cascades and never fails. Think about holding four apples. How many types of distinct groups can you place them in? It's exponential. So if you had two rows, that's 2^2. If you had 100 rows, that's 100^100. And so on. 2. This is no longer search arguable. (SARGABLE). When you modify a column you are joining or using in the WHERE section of a query, you force SQL to scan the entire column or clustered index and convert that column. This renders any indexes on that field worthless. Now it has to go through and do a hash match on every row. This is a very expensive operation. It is not best practice to perform functions on fields in tables that you are joining on or placing in the where predicate. In a join, the tricks are more difficult. In a where clause, you usually flip it. Example: SELECT * TABLE WHERE CAST (DateVarChar AS Date) = '2018-06-04' You take the above and turn it into this: SELECT * TABLE WHERE DateVarChar = CAST ('2018-06-04' AS Varchar(10)) You are moving the operator over to the non-column as seen. The second example would be able to utilize the index where the first could not. 
Thanks I'll give this a whirl
A clustered index tells you the order of the records on disk. It's like the contents at the start of a book: it can only ever be in that order. A non-clustered index is like the index at the back of the book, and can be in any order: eg like a catalogue may index things by purpose, not name. You can have exactly one clustered index, because it can only relate to the real order the records are in. This makes reads really fast, though, because the order is fixed. It makes writes slow, because you have to update the actual disk You can have as many non-clustered indices as you like, and they're fast to write to, but not as lightning quick to read from.
Think of a process where you get a verbal quote for a product, write a PO for that product, and get invoiced for that product. If you left join your quotes to POs to invoices, you can coalesce in descending order of "realness" of the price: invoice price, PO price, quote price. This gives you the closest to actual price at any time.
* You don't do Big Data / Power BI in Excel, it's generally used to pull aggregate data sets from SSRS / etc * Never use Access, there is basically no viable reason outside maintaining legacy systems; there are a number of open source or free products that make it look like a dumpster fire by comparison. * Learn to use Shiny / RStudio * The #SQLHelp hashtag is your friend, as long as you've put some effort in. Don't just ask every single question as they come up; it's a good way to get ignored. * Get yourself a install of SQL Server Developer/Express and SSMS and download a sample database 
Just a clarification. A heap is unsorted data, a clustered index is sorted data, and a non-clustered index is just an independent sorted reference to the data. In the phone book or dictionary example, the clustered index is not the table of contents - it's the actual pages of phone numbers or dictionary entries in the order they appear in the book. You'll note that in both cases, the data is sorted - that is, you could reasonably flip the book open to page, view what is on the page, and then make a decision about where to find a specific entry by trying to open the book at an earlier or later page. If this data wasn't sorted, you couldn't locate a specific entry without literally checking every page (e.g. scan). The table of contents and appendix/book index in these examples aren't great comparisons to non-clustered indices because they are both summarized references. A non-clustered index literally has every record or entry in order of the index. A table of contents is closer to the concept of statistics - it gives you a rough guide on where to find information.
This is a great point! A primary key is "the set of attributes such that no two records match on all of them." So going back to our book analogy, the primary key would be author name, book title, and maybe cover if you're the kind of person who'd re-buy a favorite book because you liked the alternate cover (I've done this for Night Watch and Embassytown). But wait - huh? I described primary key in terms of the books that go on the shelf, so what does it mean to "specify" a primary key? It turns out that a primary key is actually more of a guarantee (the SQL word for this is "constraint") - in this case "I will not shelve a book if I have a book with that title, author, and cover already." This seems a little weird for PEOPLE to do. However, the SQL query optimizer has tons of little tricks to make it good at answering questions, and that means it can do a lot when you give it a guarantee. For example, if I say "I would like Terry Prachett's 'Night Watch', with the Paul Kidby cover", and it finds one, it knows it can stop looking because my rule from shelving means it will ONLY find one. Of course, again, we have the issue that "What's good for the lookup is bad for the shelving" - it's kind of a pain to look at all of my books and make sure I don't have this book already before shelving, huh? This is why, by default, SQL creates a clustered index on the primary key. For one thing, it's likely to be a useful clustered index. Notice how "make sure I only have one copy of the book" started with author, how I wanted to sort my clustered index already? And having the shelves in that order makes the uniqueness checking a breeze, because if I'm putting my books in alphabetic order for author, title, illustrator of cover, then all I need to do to check uniqueness is shelve it in order then check if it's touching an identical copy of itself. But if you want, you can have the clustered index be one set of attributes and the primary key be another. It requires a bit of extra work, because now you're shelving the book by one rule, THEN updating the non-clustered index list by another rule, THEN checking if the entry you just put on the list is next to another identical entry. But SQL's pretty fast with non-clustered indices too, even if they take a bit more work.
Just wanted to report back that this works well and gets the right data! There is a corner case of if zero bugs where returned. The earliest date in the db will always have 0 closed bugs (and the rare case where the devs are lazy and/or on vacation). So I tweaked it a bit with a left outer join which will have a null row for dates with no ids being closed. I ended up with... with a as ( select distinct datestamp from bugs), b as ( select datestamp, LEAD(datestamp) OVER (ORDER BY datestamp desc) AS prev_date from a), c as ( select b1.datestamp, bug1.id from bugs bug1, b b1 where bug1.datestamp = b1.prev_date and not exists (select null from bugs bug2 where bug2.id = bug1.id and bug2.datestamp = b1.datestamp) ) select a.datestamp, count(c.id) from a left outer join c on a.datestamp = c.datestamp group by datestamp order by datestamp; Thanks again!
Yes, they're functionally equivalent. However, the second is the modern \(as of SQL92 \- a quarter century ago\) syntax and it's vastly preferred over the older "comma join" syntax in your first example.
I see. Thanks alot man, I'll use the second expression! Kinda easier to read too, imo.
Hey, MERI0, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Eyy, I learn more and more from this thread. Thanks *a lot*, bot.
not quite the same -- one returns `NAME`, the other returns `navn` -- might have different results in databases where identifiers (table and column names) are case sensitive in neither of the two ways are those pesky parentheses required or desirable
Wops. I forgot to change that to NAME! This is originally a Norwegian task, will change that.
1. No, they aren't the same (first one uses "ranking" column from the participation table, the second one uses "rank" 2. No (first one syntactically is a cross join); although "inner join on &lt;condition&gt;" is functionally equivalent to "cross join ... where &lt;condition&gt;" 3. With a semi-decent optimizer and once the column used in condition is the same, they should result in the same execution plan. 4. Kind of: explicit JOIN syntax was introduced by later versions of SQL standard, but the 1st version is still fully compatible and supported.
Wops, it's supposed to be RANKING. I'll change that. Thanks for the answers!
The offset in postgres is an integer, so `lead(datestamp, -1)` and `lag(datestamp, 1)` are the same thing. So that explains a bit of that confusion. I was using dense_rank() with the datestamps as a way to assign an index to the datestamps as I wasn't sure how lag/lead would work with non-sequential dates (only having data for Jan 1, 2, 5, 6, etc). You could then compare the indexes instead of the dates themselves, and could change the sorting order to reverse the indexes. So using the indexes from `select datestamp, dense_rank() over (order by datestamp asc) as index` and `select datestamp, dense_rank() over (order by datestamp desc) as index` ended up being pretty similar to using lead/lag. Theres also just switched the sorting order of the over clause with lead/lag. Using lead with desc is the same as lag with asc. Just lots of ways to do the same thing - so I wasn't sure if you were suggesting that lag has some nuance that I didn't know about. Unfortunately, I was starting from a query to get all the new bugs as opposed to closed bugs and the queries didn't end up being similar enough to simply switch lead/lag. I was really missing the potential of multiple WITH queries (do those have a specific name? It feels like a pre-subquery?), which was what helped. 
Try selecting the subqueries into #tables then writing your query such as: select * from table inner join #table You may need to index the #tables.
Sorry but I'm not sure what your saying here.
They will give the same result, but on logical level it makes a difference. The 'FROM' and the 'JOIN' is the first step of in the logical proces. While the where is the second step. So when you join tables with the join operator there will be a lot less records the filter in the where clause.
He’s saying to dump each select into a temp table and then join the temp tables. 
Like this: BEGIN SELECT VV.varValue, VV.id, D.value, D.varId, D.request_id INTO #PRB FROM trs.tbl_task_varval VV INNER JOIN trs.tbl_task_data D ON VV.id = D.value WHERE D.varId = 6 END --BEGIN --CREATE INDEX Blah ON #PRB(ID) --Do you really need an index? Will this make it faster? Test and find out. --END BEGIN SELECT * FROM Table A INNER JOIN #PRB B ON B.ID = A.ID END
Excellent! Thanks for the follow-up. It's not always common to get some closure on things like this, so I'm happy you reported back.
You can start by consolidating your logic into a CTE so you don't have to type the join over and over. with taskvarval as ( SELECT VV.varValue ,VV.id ,D.value ,D.varId ,D.request_id FROM trs.tbl_task_varval VV INNER JOIN trs.tbl_task_data D ON VV.id = D.value ) SELECT R.id AS RequestID ,R.date_created AS DateCreated ,U.display_name AS CreatedBy ,PRB.varValue AS Problem ,SER.varValue AS Seriousness ,REG.varValue AS Regularity FROM trs.tbl_task_requests R JOIN sugarcrm.tbl_users U ON R.createdBy = U.id JOIN taskvarval AS PRB ON R.id = PRB.request_id AND PRB.varId = 6 JOIN taskvarval AS SER ON R.id = SER.request_id AND SER.varId = 7 JOIN taskvarval AS REG ON R.id = REG.request_id AND REG.varId = 8 WHERE R.system_id = 3; Note: I chose to put the JOINs all on one line because it's easier to read together, if there were more criteria or if the criteria were different between the joins, I'd break them out. Beyond that, for performance, I'd look to make sure you have indexes that have columns you're using here, especially those "id" columns. 
What's CTE short for?
Common table expression
&gt; I wasn't sure how lag/lead would work Anything you're not sure about - just try on some test data. IMO, gives you much better understanding if you try to do it with your own hands. Using dense rank to figure out the prior value - yes, this should work. And yes, similar results can be achieved multiple ways in SQL. "WITH queries" are called CTE, common table expressions. 
Sorry! It's a "Common Table Expression". It's used to encapsulate a particular query so you can use it multiple times. (it can also be used recursively, but ignore that for now, it's complicated.) Here's a stack overflow discussion on it. https://stackoverflow.com/questions/4740748/when-to-use-common-table-expression-cte
I had a look at numerous way and I still wasn't able to achieve this. I need to provide the arguments from outside of the SPROC so I can't use TVP or temp table to get it done.
Thank you, this helped. :)
Thanks!
Thank you, this was pretty clear. :)
Seems like this is more or less already solved, but as an alternative you could get your list of values comma separated (CSV format with no header row) and then copy that to a string and run: WITH cte AS ( SELECT unnest(string_to_array('[csv values]', ',')) as joinme ) SELECT * FROM cte JOIN table t ON cte.joinme = t.column
This is close, but what OP is showing is the old syntax of how to do a join comparing to a new syntax of join. What you are referencing is something along the lines of: SELECT * FROM TABLE1 INNER JOIN TABLE2 ON TABLE1.ID = TABLE2.ID AND TABLE1.Name = 'Jack' In my example I am filtering the first table before the join. I am telling it, "Join only records in Table1 where the Column and Row match 'Jack' to Table2." If I change it to: SELECT * FROM TABLE1 INNER JOIN TABLE2 ON TABLE1.ID = TABLE2.ID WHERE TABLE1.Name = 'Jack' I am telling it to join Table1 to Table2, after they have joined results, we are going to filter the results to only those that include Jack. This is the example I believe you were referring to but it is different from what OP is requesting. Kimberly Tripp has a great post on [sqlskills](https://www.sqlskills.com/blogs/kimberly/determining-the-position-of-search-arguments-in-a-join/) taking this topic into better detail and justice than I can bring it.
Giving this a whirl. I'm a super sql noob - and have been forced to get into it out of necessity at my job. I'm a finance guy!
Instead of joining the whole table, if you just want to know Yes or No if those values are in your table, just do something like this: Select 'Yes Where exists ( Select 1 From tbl Where item in ('value1', 'value2', 'valueN') It will output "Yes" if any row matches any of the values, otherwise it will return 0 rows. This will be much faster than the other solution because it will not join anything, nor create a temporary result set, and it uses "short circuit logic" which means the query will stop running as soon as it finds even one matching record.
One of my common uses is when I have a dataset with multiple related date fields but some of them may be null. Lets say I have an orders table, with datetime stamps for ORDER_CREATE, ORDER_ACCEPT, and ORDER_COMPLETE. Lets also say the latter two fields are null on create. COALESCE(ORDER_COMPLETE, ORDER_ACCEPT, ORDER_CREATE) AS LAST_TOUCH_DTM would give me the last time that order was touched out of those three dates (in this very simple example).
Yup - that would be something else that would need to be added.
Might be a little simpler to use this kind of syntax: SELECT item, data FOM tbl WHERE item = ANY(VALUES ('value1'), ('value2'), ('valueN')) But in any case, I still like an EXISTS instead if OP just wants to know if the value is in there.
You can just go to the regular programs menu and find SQL Server [version] and uninstall. It will bring up the SQL installer wizard and you can choose the instance. Make backups first... always. 
Not knowing anything about the tables you’re working with, but one potential fix is, After the ON keyword - Replace &lt;CustomerName&gt; with &lt;CustomerID&gt;.
What’s it doing? And is customer paid the same as customer Name?
When you're writing a query \(particularly with JOINs\), pay close attention to what columns you're choosing and what you expect them to contain \- is a CustomerName \(for example, "Fred"\) the same as a CustomerID \(for example \(12345\)? If they don't match exactly, the JOIN will not return a result.
Thanks for the reply. However, it didn't seem to work. You can actually view the tables by clicking the link I shared. [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_op\_in](https://www.w3schools.com/sql/trysql.asp?filename=trysql_op_in)
https://www.w3schools.com/sql/trysql.asp?filename=trysql\_op\_in I'm getting a syntax error \(nothing shows up\). :\(
Thanks for the tip. If by chance you figure it out [https://www.w3schools.com/sql/trysql.asp?filename=trysql\_op\_in](https://www.w3schools.com/sql/trysql.asp?filename=trysql_op_in) Please let me know :\)
Need to join customers.customerid =orders.customerid Not customerID = customerName
I’m on my phone and don’t think it’s displaying the whole site. I don’t see tables. I think I can take a good shot at it though. SELECT o.*, C.CustomerName FROM Orders o JOIN Cusomters c On o.customerid = c.customerid
You'll need to change *Customers.CustomerName* to *Customers.CustomerID.* Let's take a look at how it works. A JOIN looks for \_exactly equal\_ values in the columns in the ON, so when we write "JOIN Customers ON Customers.CustomerName = Orders.CustomerID" we're asking for all rows where Customers.CustomerName exactly matches Orders.CustomerID. But a quick look at Customers.CustomerName shows that it has values like "Wilman Kala" and "Hanari Carnes", and shows that Orders.CustomerID has values like 90 and 34. So when we JOIN on those two fields, they will never return a result. On the other hand, JOINing on Customers.CustomerID and Orders.CustomerID will return matching rows: for example, OrderID 10248 has CustomerID 90 .
SELECT c.Name FROM Customers AS c INNER JOIN Orders AS o ON o.CustomerID = c.ID
Thanks alot
Hey, kidvibe, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Thanks
Thank you for the detailed reply
Thank you
Just looking at the top of your query, prior to the group by clause, it looks like you are firstly not joining on a particular field and its on the same table anyway. Try removing the join, and just do you date query on [Ellenőrzés dátuma]. Also, why do you need max date, if you need to return all records within a year? (as opposed to all ids who have a record within the last year) 
&gt;I want it to return me the most recent records that are not older than 1 year. Right now it gives me every single record that is not older than 1 year, not just the most recent one. So you just want the most recent record (singular)? If so add: SELECT TOP (1) t.Azonosító,... and ORDER BY [Ellenőrzés Dátuma] DESC
It's because you are returning all ids that have at least a single record within a year, not just the records within a year. You shouldn't need that first join at all. So try removing this part: INNER JOIN (SELECT Azonosító, MAX([Ellenőrzés dátuma]) AS 'Ellenőrzés dátuma' FROM Jegyzőkönyvek WHERE [Ellenőrzés dátuma] &gt; DateAdd("d",-365,Date()) GROUP BY Azonosító ) AS x ON jj.Azonosító = x.Azonosító) and add the where clause in the very last line: WHERE [Ellenőrzés dátuma] &gt; DateAdd("d",-365,Date())
I probably wasn't clear enough, sorry. I made an edit in my post.
I probably wasn't clear enough, sorry. I made an edit in my post.
no problem, are you using microsoft sql? If so, you can make use of cte and row_number with something like this. ;WITH cte AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY Azonosító ORDER BY [Ellenőrzés dátuma] DESC) AS rn FROM Jegyzőkönyvek ) SELECT [all the fields you need] FROM cte as jj INNER JOIN Vizsgálatok AS v ON jj.[Jegyzőkönyv száma] = v.[Jegyzőkönyv száma] INNER JOIN Hegesztőgépek AS t ON v.LSZ = t.LSZ WHERE (((t.Állapot)="OK") AND ((v.[Vizsgálati eredmény])="Megfelelt")) AND jj.rn = 1 and jj.[Ellenőrzés dátuma] &gt; DateAdd("d",-365,Date())
&gt; (((t.Állapot)="OK") AND ((v.[Vizsgálati eredmény])="Megfelelt")) with this method, just be aware that one of those 2 isn't true for the most recent record, it wont return anything. So put the inner joins and those criteria in the cte if you want the most recent record within a year that has that criteria, even if there is a more recent record in the table.
I'm using MS Access and it throws me an error message.
I'm not sure but it looks like you want to retrieve all records which share the most recent date? If so I think you could just use TOP WITH TIES SELECT TOP 1 WITH TIES t.Azonosító, t.LSZ, t.[Gyártási szám], t.Megnevezés, t.[Gyártási év], t.Gyártó, t.[Hegesztési eljárás], t.[Típus neve], t.Áramerősség, t.Hűtés, t.Áramtípus, t.Állapot, t.[Gyártási év1], jj.[Ellenőrzést végző cég], jj.[Ellenőrzést végző személy], jj.[Jegyzőkönyv száma], v.[Vizsgálati eredmény], jj.[Ellenőrzés célja], ['Ellenőrzés dátuma'], jj.[Következő felülvizsgálat] FROM Jegyzőkönyvek AS jj INNER JOIN Vizsgálatok AS v ON jj.[Jegyzőkönyv száma] = v.[Jegyzőkönyv száma]) INNER JOIN Hegesztőgépek AS t ON v.LSZ = t.LSZ WHERE (((t.Állapot)="OK") AND ((v.[Vizsgálati eredmény])="Megfelelt")) ORDER BY [Ellenőrzés Dátuma] DESC
So for each distinct LSZ, you want only the most recent (via Ellenőrzés dátuma) row to be there?
Yes, exactly! Thank you so much for your help so far!
&gt; I thought COALESCE only selects the first non-null value found though It does, but it returns the first non-null parameter.. COALESCE(NULL, NULL, 5) --&gt; 5 Your subselect works because it is returning a single value, not a result set
Exactly. OP: Consider Coalesce as a first come first serve function. The first parameter to return a NON NULL value gets the trophy.
[http://www.itprotoday.com/software\-development/coalesce\-vs\-isnull](http://www.itprotoday.com/software-development/coalesce-vs-isnull) Very good article that might help
http://sqlfiddle.com/#!9/661a6b/3
Thank you, I never knew about SQLFiddle and I'm surprised it didn't turn in up a Google search for me. I wasn't a million miles off in my estimation which is surprising, but not particularly close either. Thanks for the detailed response, it is much appreciated. 
This seems to be a copy paste from https://community.modeanalytics.com/sql/tutorial/sql-performance-tuning/ 
 SELECT Symbol , SUM(Qty*Price) AS [Net MV(allclients)] , Producer , STUFF(MAX(RIGHT('000000000000000'+CONVERT(varchar(15),Qty*Price),15)+Client),1,15,'') AS [Largest Client Purch] FROM Table GROUP BY Symbol , Producer;
If `LEN(@ColumnNames)` = 0 then `LEFT(@ColumnNames,-1)` will give this error.
If LEN\(@ColumnNames\) = 0 then LEFT\(@ColumnNames,\-1\) How can I fix this? Can I use IS NULL here?
smt like this? select symbol , sum(qty*price), producer, (select client from table1 where symbol = t1.symbol and qty*price = (select max(qty*price) from table1 where symbol = t1.symbol) LIMIT 1) from table1 t1 group by symbol, producer; 
`LEFT(@ColumnNames,ISNULL(NULLIF(LEN(@ColumnNames),0)-1,0))`
Thank you! Two questions.... Never utilized STUFF....What does it do exactly? I am receiving an error 'Arithmetic overflow error converting numeric to data type varchar.' How do I get around it? 
STUFF, in this case, deletes the first 15 characters of the MAX string. The error could happen if you have purchase amounts longer than 15 characters. Try: `STUFF(MAX(RIGHT('000000000000000000000000000000'+CONVERT(varchar(30),Qty*Price),15)+Client),1,30,'') AS [Largest Client Purch]`
Msg 102, Level 15, State 1, Line 17 Incorrect syntax near '\)'. Still gives me an error for that line. For some reason.
I've confirmed it works. It's something on your end.
Alright, I will see what I can figure out. Thanks for the help!
What's happening first is your subquery is executing. We are retrieving the MAX value for the ActualCost from the Production.TransactionHistory table. In the event that there is no max, say there is a NULL, then it would substitute 0. Since there are a lot of records, it's very likely there is no NULL as the max value as it would have to only contain NULL for that to come back. Example: select NULL as Col1, 1 as Col2 into #temp; select max(Col1) from #temp; Now if you wanted the MAX ActualCost for each TransactionID / ProductID / Quantity while handling the NULL values, you would need to relate the MAX cost to each row. The example I'll give you is a correlated subquery which is typically not recommended because the subquery will execute for each row on the outside join. Now I want to also note that this is warm memory right now and warm storage. These have been ran a few times, this is important because things are in memory and faster than they would be if it was cold. SELECT b.[TransactionID] ,b.[ProductID] ,b.[Quantity] ,COALESCE(( SELECT MAX(a.ActualCost) FROM AdventureWorks2014.Production.TransactionHistory a WHERE a.TransactionID = b.TransactionID ), 0) AS MaxActualCost FROM AdventureWorks2014.Production.TransactionHistory b; [Take](https://www.brentozar.com/pastetheplan/?id=SkxiqMEgQ) a look at the execution plan I generated from this. (May vary to yours because of the optimizer and version.) I'm scanning the clustered index three times! That's insane! Here's some stats: (113443 row(s) affected) Table 'TransactionHistory'. Scan count 2, logical reads 341923, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. SQL Server Execution Times: CPU time = 390 ms, elapsed time = 820 ms. I had to scan twice and I had to read over 300,000 times, that's a lot. We can make this easier to read and work with by using a windows function. This should perform better on 2016. SELECT [TransactionID] ,[ProductID] ,[Quantity] ,COALESCE(MAX(ActualCost) OVER ( PARTITION BY [TransactionID] ,[ProductID] ,[Quantity] ), 0) AS MaxActualCost FROM AdventureWorks2014.Production.TransactionHistory; [Here's](https://www.brentozar.com/pastetheplan/?id=rkQC2fNe7) the plan. (113443 row(s) affected) Table 'Worktable'. Scan count 3, logical reads 680659, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. Table 'TransactionHistory'. Scan count 1, logical reads 797, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. SQL Server Execution Times: CPU time = 562 ms, elapsed time = 1120 ms. From the stats, we can see this was actually more expensive but it is mmore maintainable. Let's try once more. SELECT [TransactionID] ,[ProductID] ,[Quantity] ,COALESCE(MAX(ActualCost), 0) AS MaxActualCost FROM AdventureWorks2014.Production.TransactionHistory GROUP BY [TransactionID] ,[ProductID] ,[Quantity]; [Here's](https://www.brentozar.com/pastetheplan/?id=SyiETM4x7) the plan. Here's the stats: (113443 row(s) affected) Table 'TransactionHistory'. Scan count 1, logical reads 797, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. SQL Server Execution Times: CPU time = 31 ms, elapsed time = 530 ms. 1 scan with 797 reads and we almost removed the CPU time. So to summarize: 1. The query was written to only return 1 value regardless of the other columns / rows and it would always be the max value. I have re-written it in a few examples to give a corresponding max value pertainable to each row based on the combination of TransactionID, ProductID, and Quantity. There could very well be duplicates within the data and there is no duplicate removal logic in my queries. 2. You can write this several ways. I believe the way they were trying to teach you was the correlated subquery which can be powerful but also devastating. I would say to check your execution plans, construct proper indexes, and apply the correct tool for the job. Everything has a specific reason and use, weigh those with best practices and those edge cases where you need to do something less vanilla in your environment. Please let me know if you have any questions or if I can clarify anything for you. 
That makes sense....one last question (thank you again for taking the time for the response)...If I wanted to add another column for largest client purchase amount data would look like (Row 1:42 Row2:20 Row3:12)....How would I do that? Would I need to use a nested query?
Just `MAX(Qty*Price)`, right?
There is little to no value or content in this post, plus it's just a spam link to a site to (presumably) drive up ad revenue. If someone sees this and *really* wants to know about how to write efficient queries and to understand testing for performance as a whole check out guys like Cary Millsap and Ric Van Dyke. They offer LOADS of info for free and it's far more valuable than what has been posted here. 
Yep.
In my experience, if you're doing select stuff ,(select something from a_table) as something_else from b_table something has gone terribly wrong. It's just a bad pattern.
Most typically, yes. I cannot give you an example off the top of my head where that's the best solution. I see most people write SQL like that when they first start off because the ideas of sets and correlations is a little difficult to grasp at times. You may see that with older SQL too that lacked newer features / syntax / optimizer traits. 
More blogspam. Not only does it seem to be copied and pasted as /u/ToyoMojito found but I find it very hard to take seriously any article on query performance that does not delve into execution plans and include actual results data IE sub tree costs, timings, disk &amp; CPU usage etc etc.
Thats not what i referencing to... This applies pure on logical level and i know SQL will come up with a maintenance plan seek predicates to make the query execute faster. and that in this simple example it will make no differance... But its important to know in what logical order SQL works to get the mindset of SQL So the logical order of SQL is 1. FROM 2. WHERE 3. GROUP 4. HAVING 5. SELECT 6. ORDER 7. FETCH OFFSET So its logical that the more records are excluded for the next step the less work the next step will have Also good to know is that SQL do not guarantee you it will execute AND/OR operators the same ORDER as you write them. So when u use an CROSS/CARTESIAN JOIN \(example 1 of OP\) and lets say you have 10 records in each table, so when you enter the WHERE clause SQL will have an resultset of 100 records \(10 x 10 = 100 :D\). Even when you write *TABLE1.ID = TABLE2.ID,* as said above order of statements has no influence, SQL still need to validate *the TABLE1.NAME = 'J*ACK' 100 times, even if the only 2 ID's matches. When u use an INNER JOIN and SQL will also need to validate *TABLE1.ID = TABLE2.ID* a 100 times \(as SQL starts with an CARTESIAN JOIN before applying the ON operator\) but when it enters the WHERE clause it will have A LOT less records to apply the *TABLE.NAME = 'JACK'* to \(max 10 records\) a very good posts about logical processing from Itzik Ben\-Gan where he explain clause by clause [PART 1](http://www.itprotoday.com/microsoft-sql-server/logical-query-processing-what-it-and-what-it-means-you) [PART 2](http://www.itprotoday.com/microsoft-sql-server/logical-query-processing-clause-and-joins) [PART 3](http://www.itprotoday.com/sql-server/logical-query-processing-clause-and-apply) [PART 4](http://www.itprotoday.com/sql-server/logical-query-processing-clause-and-pivot) [PART 5](http://www.itprotoday.com/sql-server/logical-query-processing-part-5-clause-and-unpivot) [PART 6](http://sqlmag.com/sql-server-2016/logical-query-processing-part-6-where-clause) [PART 7](http://sqlmag.com/sql-server-2016/logical-query-processing-part-6-where-clause) [PART 8](http://www.itprotoday.com/sql-server/logical-query-processing-part-8-select-and-order)
Usually the host is the machine name on which the DB is installed. For example I might have a database whose SID is 'myDB', hosted on a machine called 'testserver.domain.local'. The host is 'testserver'. For the port, you need to figure out what port your listener is "listening" on. Is it the default port? You should be able to see that in the documentation. For Oracle, 1521 is a popular default port number. Before using python, how about you connect using some other method such as your favorite development tool? SQL developer, toad, SSMS, benthic, etc? That way you know that it works before adding the python variable into the mix. How about your DNS? Can you resolve the plain text machine name? What happens when you ping the host from your machine? Depending on your environment, your DNS server must know of the entry for that host, otherwise it won't work. If you know the IP and it's a local deal, you can manually stick it in your hosts file so that it resolves to correct IP of the host. 
 SELECT table 1.file FROM table1 INNER JOIN table2 ON table1.idnum = table2.idnum AND table1.version = table2.version WHERE table2.date &gt; '0001-01-01'
If this doesn't work, its likely a datatype issue on the tables. Please post the table structure with datatypes if this is the case. `SELECT` `t1.File` `FROM` `Table1 t1` `JOIN` `Table2 t2 ON (t1.IdNum = t2.IdNum AND t1.Version = t2.Version)` `WHERE` [`t2.Date`](https://t2.Date) `&gt; '0001-01-01'`
If you want the ansi 89 style joins: select table1.file from table1, table2 where table1.idnum = table2.idnum and table1.version = table2.version and table2.date &gt; '0001-01-01'; Double check the format of that date though. If by chance you are having zero records returned and you think it's a date issue, you can always comment out the date constraint line and see if it runs, then put the date back and figure out the problem. 
This seems to do exactly what I want. Didn't know I could do a compound JOIN, which is why my first attempt at this was yielding extra results since I was joining only on idnum and not idnum and version. That's where my ugly scripting came into play to strip out the extra results that shared idnum but were for a different version. Thanks for the assist. My script now runs in about 5% of the time it was taking.
Yep you can! For your example, you could actually put all of the conditions in the JOIN and remove the WHERE clause completely. Like this: SELECT t1.File FROM Table1 t1 INNER JOIN Table2 t2 ON (t1.IdNum = t2.IdNum AND t1.Version = t2.Version AND [t2.Date](https://t2.Date) \&gt; '0001\-01\-01') The execution plan will be the same for this query as well. In this case, you just need to decide which one is more readable.
/r/doyourownhomework
/r/ainthomework 
37.
 DROP TABLE Transactions
42.
37.what? Or should you have declared an Int?
CAST (CAST (37 AS TINYINT) AS BIGINT);
&gt; **Help posts** &gt; If you are a student or just looking for help on your code please do not just post your questions and expect the community to do all the work for you. We will gladly help where we can as long as you post the work you have already done or show that you have attempted to figure it out on your own.
Use print @SQL to help diagnose these issues. It'll help you pinpoint these problems. 
This plus FYI OP, you're not likely to get far in here unless you show how far you've gotten or are at least willing to engage.
haha I was just thinking this 
The later. `RAND()` generates a single value, so it's not continuously changing. It's akin to: SELECT TOP 8000 * ORDER BY 1 You can use `NEWID()` to achieve this instead: DECLARE @sampleCount1 int = 8000 Select top (@sampleCount1)* From #Table1 Order by NEWID() I seem to recall using CHECKSUM(NEWID()) is more efficient, but I'm not really sure: DECLARE @sampleCount1 int = 8000 Select top (@sampleCount1)* From #Table1 Order by CHECKSUM(NEWID())
Hey, thanks for the reply. I'll check out NEWID(), I haven't seen this before. Also, the CHECKSUM does increase the speed of the query from 6 seconds to 4 over about 2 million records. Sweet discovery!
Whoa whoa whoa. Are we jumping the gun here, maybe? I don't see the question in question. Maybe (just maybe) the question is "does querying data produce more answers or more follow-up questions?" or "is getting a result set or getting a query right a better indicator of experience?" This could be really deep and you all just decrying this as a 'do-my-homework-for-me' post.
Ok, so let's make this readable first. We're going to make the caps consistent and also put in some tab spacing to make it easier on the eyes: SELECT Licensing_Facts.Licensing_Facts_Key , Property.Property_key , Property.Street FROM Property INNER JOIN Licensing_Locations ON Property.Property_key = Licensing_Locations_Key.Property_key AND Licensing_Facts INNER JOIN Licensing_Locations ON Licensing_Locations.Licensing_Facts_Key = Infodbo.Licensing_Facts.Licensing_Facts_Key WHERE Property.Suburb = 'XXX' AND Licensing_Facts.Type_Code = 'XXX' We've got a few problems. First, alias these tables. It makes things much more readable. Second, I think you meant to join in the "Licensing_Facts" table and not the "Licensing_Locations" table twice. Third, I am not sure why you are referencing the 3 part identifier in the second join, but that should be part of the table and alias and not the join predicate. Fourth, I think you confused the table with the column in the second join. Let's fix all these issues: SELECT LF.Licensing_Facts_Key /* note we used the aliases to reference the tables here */ , P.Property_key , P.Street FROM Property P /* alias table as P so we can read it easier elsewhere */ INNER JOIN Licensing_Locations LL /* alias here too */ ON P.Property_key = LL.Property_key /* dropped the extra join predicate from this line */ INNER JOIN Licensing_Facts LF /* this was originally Licensing_Locations */ ON LL.Licensing_Facts_Key = LF.Licensing_Facts_Key /* removed the schema reference from the join */ WHERE P.Suburb = 'XXX' AND LF.Type_Code = 'XXX' I think that should get you to the goal line.
&gt; If you want the ansi 89 style joins https://i.imgur.com/yNlQWRM.jpg
Thanks for the response, the explanation cleared up where I had gone astray. I have cleaned up my script now just chasing down a Incorrect syntax near '.'. error. Appreciate the help! 
Is `Infodbo.`actually a schema or an error? Try with just `Licensing_Facts` instead.
The DB that is being used seems to require Infodbo. to preface each table reference. I removed these to make it easier, it's likely that a mixture of that is causing the problem.
 SELECT Licensing_Facts_Key, Property_key, Street FROM Property NATURAL JOIN Licensing_Locations NATURAL JOIN Licensing_Facts WHERE Suburb = 'XXX' AND Type_Code = 'XXX' * You don't need a '.' operator unless you are trying to differentiate the columns of different tables that match on their name. Aliasing the tables is only really useful if you need to reference them, but this query doesn't actually need it. * `NATURAL JOIN` will join your tables on matching columns names and is significantly more readable. It performs an inner join by default, but you can also do `NATURAL LEFT JOIN | NATURAL RIGHT JOIN`. You can also explicitly say `NATURAL INNER JOIN` if you want.
Thanks, its all sorted now :D Thank you both.
Create an Operator with the email address you want to send the notification to. Now in the job, go to the Alerts section. You can select to notify that operator by email on failure. Hope this helps!
Unfortunately, I'm not using e-mail for this. I'm automatically creating JIRA tickets based on a script. I'd like to automatically fill out information regarding the actions in the job, but I want to make sure it's actually valid before I do so.
Keeping the code along the same lines as what OP was doing. 
The failure info is written immediately to the history table in the made database. You also could try to build the job to go to a new step on failure which could possibly run your script. 
The failure info is written immediately to the history table in the made database. You also could try to build the job to go to a new step on failure which could possibly run your script. 
https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-from-psql.html &gt; In the left navigation pane, click Clusters. Click your cluster to open it. Under Cluster Database Properties, record the values of Endpoint, Port, and Database Name. You should be able to figure out the corresponding value between the psql example and psycopg2.
That's what I'm trying to do, the question is if the step is "on failure" is that for lack of a better term, live? Will a query of those databases reflect the current failure? or would the job have to terminate and then be reflected
It's purely a preference thing.
Multi-decade SQL software developer here. I've never found a use as I see it as unnecessarily complicating a given statement. I'm not a fan of the INNER and OUTER keywords either. It would be interesting if someone can make a valid use case for it.
For those following eagerly along, it's immediate and you can reference that script in a "failure" step.
Inner and outer are unneeded syntax. I don't use them. The only time I've used a right join is when I'm adding a necessary table with a new join to an existing sp and don't want to reorder all the table joins. 
I often use right joins for filtering data via the join instead of doing it in a where clause all the time. It can significantly speed up a query, especially when working with large datasets. Doing this eliminates a full table scan and increases efficiency. 
Here is an example. Let’s say you’re looking at sales data. Your left table contains all your sales. Your right table contains the source of the sale. Let’s call the sources “in store” and “online”. You only want to target online sales. It would be more efficient to right join on left.key = right.key AND right.source = ‘online’ than it would be to do use it in a where clause of right.source = ‘online’. 
I personally prefer a nested query, but power to you.
That might be your personal preference, but left and right joins are completely interchangeable.
Just use `SELECT` with a few sub-selects. Depending on how big the table is it shouldn't be terribly inefficient: DECLARE @Name as nvarchar(255) = 'Nico' SELECT (SELECT count(name) FROM Table1 WHERE name = @Name) as TotalCount ,(SELECT count(name) FROM Table1 WHERE name = @Name and date = '2018-06-03') as JuneThird ,(SELECT count(name) FROM Table1 WHERE name = @Name and date BETWEEN '2018-06-02' and '2018-06-30') as AllJuneExceptFirst http://sqlfiddle.com/#!18/53d47/7
 -- this is postgres syntax SELECT name, COUNT(*) as all_occurrences, SUM(case when date &gt;= CURRENT_DATE - 7 then 1 else 0 end) as last_week, SUM(case when date &gt;= CURRENT_DATE - 31 then 1 else 0 end) as last_month FROM your_table WHERE name = {target_name} GROUP BY 1 ;
Well, about the only time I can find `RIGHT JOIN` useful is when I want to replace the `FROM` table, and only under specific circumstances, mostly for debugging purposes. I've never used it in production code because it confuses the shit out of people 95% of the time. So imagine you're writing a query with a `FROM maintable`, with inner joins. Then you can slap a right join at the end, to the maintable again, to see what you're missing from there and if it's what you expect. Example: SELECT Sales.* FROM Sales JOIN Customer ON Customer.ID = Sales.CustomerID Imagine there are multiple inner joins between all kinds of tables. Now if I add a `RIGHT JOIN Sales AS AllSales ON Sales.ID = AllSales.ID` at the end I can clearly see if all of the needed sales are included, filter for the ones that aren't (`WHERE Sales.ID IS NULL`) and validate that my query actually does what I want it to do. Normally in production code, I would wrap the main query in a CTE and go for something like: WITH ProcessedSales AS ( /* ... */ ) SELECT * FROM Sales LEFT JOIN ProcessedSales ON Sales.ID = ProcessedSales.ID [Which produces the exact same plan as a RIGHT JOIN anyway](https://i.imgur.com/YoePCXv.png). 
Yup its terrible performance. Do it as a join, its faster and cleaner.
In MS SQL, it's very important to use RIGHT OUTER JOIN if the join order is forced (via hint), but I'm not sure it's relevant to Oracle.
I don't think this is true. The execution plans will be identical in any modern database engine.
New ID creates a value of the uniqueidentifier datatype which we can use as a Globally Unique IDentifier (GUID) because when you create one it's basically guaranteed to be one that has never been generated before. This makes the ID unique not only in your table or database but across everything that has, or ever will, exist. Cool hey? But part of the way it does that, simply put, is by having a really really really big range meaning it both takes up a lot of space (16 bytes with a normal INT only being 4) and takes more time to sort. Checksum() creates an INT type hash of the data you input into it. The idea being that you'll always get the same hash out from the data you put in regardless of what, or how much, data that is. This has many uses but it's use here is to basically to shrink that 16 bytes of data into INT and because the GUID's are all unique and random you'll get random and, likely, unique numbers out. With a large number of GUID's you start running the risk of getting two or more GUID's who's hashes happen to be the same number. In other words the checksum is not guaranteed to be unique in this case because it's taking numbers for a unimaginably big range and turning them into numbers from a much much smaller range. But that's largely okay in this case as SQL with order ties as it discovered them which will probably be random enough. Seems likely that it's much faster to hash the GUID's then order by the INT hash then it is to simply order the GUID's themselves. Which is understandable because if you take one thing away from this it should be just how many GUID's you can generate in 16 bytes of data. 
Not true, and the results would be different.
Just for clarity, we're talking about the difference between SELECT * FROM SalesData sd RIGHT JOIN SalesSource ss ON ss.key=sd.key AND ss.source='online' and SELECT * FROM SalesSource ss LEFT JOIN SalesData sd ON sd.key=ss.key WHERE ss.source='online' right? We're returning all of the 'online' SalesSource rows even if there's no SalesData. And these have different Oracle execution plans. Intriguing! From a clarity standpoint, disregarding performance, I definitely prefer the later.
Given your use case, I'd focus on *advanced* Excel skills first; /r/excel is a pretty good resource if you're not subbed there already -- but there's no harm in getting more skilled in SQL, especially when you start to look at more and more complex data sets.. being able to use Excel at an advanced level and use SQL gives you a bigger advantage IMO But.. I've never been a demand planner, so... this is just one guy's opinion
Knowing how to at least write queries to extract in SQL can be very helpful, but it's probably not the key skill you need to become a demand planner. In whatever company you're in, I highly recommend you reach out to some demand planners and invite them to coffee or lunch. Then use that time to ask them about their jobs, what skills they need, and what you can do to become a demand planner. Even better, try to get a short meeting with a demand planning manager in your company so you can ask them what they look for when making hiring decisions. If your company offers "stretch assignments", then see if something like that's an option. Talk to your own manager and let them know you'd like to learn more about demand planning so you can make that your next step. So while SQL might be a good skill to have, maybe forecasting techniques, or learning the forecasting software your company uses could be more valuable. Also - when you talk about your ideas of moving into demand planning, they'll probably ask "why". You should have more of an answer than "to make more money". Try to come up with a "5 year plan" about how moving to demand planning will help you reach some step in a career progression.
It can help if your ERP software that handles your inventory is SQL driven. In the case of the ERP that I work on, it is. As a real-world example of this, I have done quite a bit of development in regard to min-max planning, which tries to predict demand considering attributes about the inventory such as seasonality, and put it into a linear data model. From there it goes through a multi-variable regression analysis to place weight on certain attributes to determine "is P, Q, and R a good predictor of X"? In the real world, it would be more like "is this retail location in this state in February likely to demand 10,000 units of product?" I generate confidence intervals and let that be the guide for the inventory buyers. My entire ERP is a SQL-based Oracle database, so naturally SQL is the ONLY method used for extracting. No matter if you're banging out SQL manually, or if you're using a front-end to build the SQL for you. Either way, it's straight up SQL. For planning, I'd say that knowing HOW to get the data, how the data is organized, how to interpret the data, and how to make the data help you make decisions is far more important than knowing SQL in general. For you, it's more about understanding your data and applying correct statistical/analysis to the data to make decisions. And, SQL is mainly for the extract. The real magic happens in other programs that a geared toward manipulating and presenting data, such as Excel pivot tables and Tableau. 
This should do it: SUM(CASE WHEN increment = 1 OR increment IS NULL THEN 1 ELSE 0 END) OVER (ORDER BY person_id, event_sequence ROWS UNBOUNDED PRECEDING) AS sequence_id If you have an index on (person_id, event_sequence) including increment, it will be reasonably fast too (no sort).
^That should do it, _but_, given where we've met you (trying to RBAR your way through 20mil+ rows) and would-be uniqueness of a use case that would require straight sequencing of events across people, i'm willing to bet that "increment" and "sequence_id" columns are a by-product of some non-dataset oriented thinking. Care to share how these "increment" and "sequence_id" columns came about?
Yer a wizard /u/jc4hokies! Seriously, thank you, that worked like a charm.
Derived in the prior step. sequence_id is just a row_number partitioned by person_id and ordered by event_timestamp. increment is based on business logic that defines when the next event is a new sequence. 
No doubt, no doubt. So, out of curiosity, what's the business case behind the sequence_id (or straight sequencing of events across people identified by id)? 
The point is looking at how customers resolve their issues. The events are across many contact methods and have specific contact reasons and other data elements associated with them. Assigning a unique sequence id to the string of events allows us to analyze, for various specific customer issues, where they first came to us, where they last came to us, the average number of times they hit different contact methods, the cost to service specific customer issues, etc. 
Thanks for the advice. I think my excel is very good. Iam getting VBA badge to Learn more
Getting error: Windowed functions can only appear in the SELECT or ORDER BY clauses. 
For an update, I would write it something like this. UPDATE x SET sequence_id = new_sequence_id FROM (SELECT sequence_id , SUM(CASE WHEN increment = 1 OR increment IS NULL THEN 1 ELSE 0 END) OVER (ORDER BY person_id, event_sequence ROWS UNBOUNDED PRECEDING) AS new_sequence_id FROM myTable) AS x;
It sounds like you are reverse-engineering a session ID of sorts. Session IDs are fine for uniqueness but relatively useless for other purposes. Unless you really want to do this (create a 'session ID') for some specific case (like communicating to 3rd system, auditing, etc.) the analytics listed and more might be better served by identifying session number within a particular customer (i.e. 1st customer session #1, session #2, ..., 2nd customer session #1, ...). You can get that number (and you'll need to use it with person_id to identify a session in the original table) using the similar to /u/jc4hokies expression: SUM(&lt;business logic that produces "increment"&gt;) OVER (partition by person_id ORDER BY event_timestamp ROWS UNBOUNDED PRECEDING) AS person_session_number Just a thought :) 
This is off-topic, but I would also look at the table design and column naming. It is not very clear why you need the increment and sequence_id columns at all. Or at the very least, you should name these two columns so it is clear what their business reason to exist is.
I've intentionally made them as generic as possible for getting help. The actual names are different. 
Wow, that is really fascinating. Thanks for sharing!
I had actually done it this way previously, but it had been a while and slipped my mind. I did not realize this was the ansi 89 way of doing a JOIN. It was just the natural extension of my very limited knowledge of SQL. Introducing the word JOIN to the query has always confused me, but seeing a solution to a specific problem I had has helped it click. I think I get it now, the old way and the new. Now once I run into the need for LEFT, RIGHT, OUTER and whatever other kinds of JOIN, expect me back here. Lol 
This sounds like a basic many-to-many relationship. SELECT * FROM TableA a JOIN TableAB ab ON ab.TableAId = a.Id JOIN TableB b ON b.Id = ab.TableBId Your description is much too vague to understand otherwise. Try posting sample data from tables TableA, TableAB, TableB, (and TableX if you want) as well as your desired output with that data. 
My first thought is select the distinct combinations plus the reverse. SELECT ab1.A AS [Value A], ab1.B AS [Value B1], ab2.B AS [Value B2] FROM AB AS ab1 LEFT OUTER JOIN AB AS ab2 ON ab2.A = ab1.A AND ab2.B &gt; ab1.B UNION ALL SELECT ab1.A AS [Value A], ab1.B AS [Value B1], ab2.B AS [Value B2] FROM AB AS ab1 INNER JOIN AB AS ab2 ON ab2.A = ab1.A AND ab2.B &lt; ab1.B; or SELECT ab1.A AS [Value A], ca.[Value B1], ca.[Value B2] FROM AB AS ab1 LEFT OUTER JOIN AB AS ab2 ON ab2.A = ab1.A AND ab2.B &gt; ab1.B CROSS APPLY (SELECT ab1.B AS [Value B1], ab2.B AS [Value B2] UNION ALL SELECT ab2.B AS [Value B1], ab1.B AS [Value B2] WHERE ab2.B IS NOT NULL) AS ca;
My point is - if we know what the increment and sequence_id fields are for, it might be worth considering restructuring the table differently. For example, you could pivot this and make them columns instead. Sure, it has its own limitations and tradeoffs, but it has fairly significant benefits too.
To get every possible combination of those two tables. Just coming up with a random example, let's say I have a table, Shops, another table, Products and finally Shops\_Products which lists the products for sale in each shop. If I need to populate Shops\_Products I could do something like INSERT Shops\_Products(shop\_id, product\_id) SELECT s.shop\_id, p.product\_id FROM shops s CROSS JOIN products p 
Doesn't seem very practical for something that you would be doing every single day. Seems like something you would need to do once in a very very very blue moon
Everything
We use CROSS JOIN in a number of scenarios. 1. Build date related datasets. We want to populate a table that projects how much of each item will sell every day for the rest of the year. We'll do Items CROSS JOIN Dates to give us our dataset, then apply statistics to calculate the numbers. 2. Joining a single record with a variety of useful columns to a larger dataset. It's sometimes beneficial to construct a record with a number of useful columns, then join it to the rest of a query. Like we might construct a record with LastRefreshDate, ClosedStatusId, and HolidaysYTD. Then we can CROSS JOIN it to a larger query, and use each of those values in logic without needing more expensive joins to a Status table, or Date table. 3. Building "junk" dimensions (tables). These tables are the CROSS JOIN of possible values, and it's only natural to build them is CROSS JOINs. An example might be a table with 10 boolean values, like IsOpen, IsOverSLA, HasIssue, IsAdmitted, IsReferral, ... These are populated with a query FROM cteTrueFalse AS IsOpen CROSS JOIN cteTrueFalse AS IsOverSLA etc. 4. Building scripts with dynamic SQL. We might want to split a big script into a hundred little scripts that run for individual clients. We'll take the query that helps generate the original script, and CROSS JOIN client. There's probably a few more scenarios, but those are the big ones looking at a quick search of our source control. 
It doesn't get used often. Definitely don't write one every single day \- but there are stored procedures in our codebase that use a cross join that would surely get executed every day. 
I use it most often for generating "dense" time series data. Let's say you have a sales table that has (date,product,quantity) which records product sales. If a product has no sales on a date it won't appear in this table for that date. This could be a problem if you want to do some kind of time series analysis. To solve this, you make a "dummy" table that has all possible combinations of date and product, then join on the actual sales: with dummy as ( select d.date, p.product from dates d join products p on 1=1 /*this is the cartesian or cross join. there are other ways to write it.*/ ) select dum.date ,dum.product ,nvl(s.quantity,0) as quantity from dummy dum join sales s on dum.date = s.date and dum.product = s.product The result has a row for every single date and every single product, with quantity = 0 for days where there were no sales.
Right, the "old" way of joining was to put table1.something = table2.something for an inner join. If you wanted a left join, in Oracle you'd do table1.something = table2.something(+), where the (+) style markup is indicating table 1 left joining to table 2. In that example, table 1 would have all records and then it would just fill in nulls where a matching record was not found in table 2. The "newer" ANSI-92 style removes joins from the where clause and instead you integrate it in with the FROM clause, and allows you to specify inner join, left join, right join, full outer join, etc. All spelled out with lots of words. There are pros and cons to each method I suppose, mostly from an editing standpoint, but from my experience, Oracle devs use 89 style and I've never seen code written for Oracle's ERP that uses 92 style joins. In fact, I just grepped for the word join in the thousands of sql files that make up my platform, and I found none. But to appease the 92 supporters, the official stance published by Oracle documentation is: "Oracle recommends that you use the FROM clause OUTER JOIN syntax rather than the Oracle join operator" This is because there are number of restrictions that apply to the (+) operator, but not the "outer join" clause. But I rarely use outer join and if I do, then I simply switch to 92 style. Anytime I provide a sql answer with 89 style joins a lot of people will downvote it even though it's okay to use and the optimizer doesn't care. The newer generation of folks like 92 because that's how they were taught. Older folks stick with 89 because that's what they are used to and it's less typing. For some reason, this divides people as they cannot seem to easily translate between the two join types. If anything, I've found that folks preferring the 92 style joins are not flexible on their opinion that you should never use 89-style joins, yet I've never found any concrete evidence that one way is better than another, with exception to outer join challenges. 
It would probably be best to store as an INT like you suggested. There are times where this is not true, but this is typically the best practice.
Can you elaborate on how that would be more useful? I mean, if I want to know the average number of events in a session: select avg(event_count) from ( select sequence_id, count(1) as event_count from myTable group by sequence_id ) I'm not really seeing where there's a benefit.
The first one would save some space if you have a lot of values, and make it easier to add i18n support if you want, say, a French version of the site at some point in the future.
MS example in the training is to quickly make test data.
Note that a cross join isn't doing a "multiply". It gives you all the pair-wise permutations of two tables. If table1 has 3 rows, A, B, C, and table2 has 2 rows, 1, 2, your result will have 6 rows: A 1 A 2 B 1 B 2 C 1 C 2 We often used them when we have data from different sources and we needed to have all the pieces come together as a unified whole, even if all the elements didn't match perfectly. For example, as an overall key, we'd have product-id and month-num. We might have had data coming from sales, finance, operations, etc. and need to come up with a large rectangular table with all those fields for wherever the data exists. Maybe the sales data doesn't have every product-id that exists in the finance data, etc. So the first thing you do is get a list of all possible product-ids and months and you do a cross-join to populate the key of your table with all the permutations. Now with a comprehensive key set up in your table, you can then just do simple inner-join/updates with each data source, populating the values that exist, leaving the non-existent ones as nulls. You could also just have a table with those keys set up and build a big view that accomplishes the same result. This large rectangular dataset is now a nice source for things like dashboards and visualization tools (e.g. Tableau). But the key reason you do that cross-join is to make sure you get every product-id and month-num that appears in any of your data so that you don't accidentally exclude some data. Some people talk about cross joins like they are the `GOTO` of SQL, and that's probably because they can quickly blow up the amount of data coming out of a query and they're easy to get wrong. But when used appropriately, they're simple and very powerful, and the best way to achieve the result you're interested in.
It's a trade-off. From a strict design perspective, it's usually better to use an immutable value for a key. A text value like "apple" or "banana" might need to be changed or renamed. There's no problem with a numeric code like "7" or "9". If at some later date we want to add "Granny smith apple" and "Honeycrisp apple" to the list we can do that without forcing a cascade update to occur. If we need to internationalize our system, we can change "apple" to "manzana" easily as well. The disadvantage of this method is that you must do a join whenever you need to know the name of the fruit. The majority of systems are designed this way, IMX. Some systems will elect to use a letter code instead of a numeric value, but the result is basically the same, though there are suddenly use cases where you need to update the code. For example, if you have a Sex lookup table with M - Male and F - Female and move the data to Spanish, you need M - Masculino and H - Hembra. Both your code and your label changed. However, this does help with the space issue since codes typically require less space than the label. The alternative would be to use the full value in the table. In addition to potentially costing more disk space, this means that whenever you update the table you'll force a cascade update. That creates a lot of locks on the table, meaning you may run into concurrency issues because a simple rename suddenly becomes a change that touches many, many records potentially across the entire database. The advantage of this method is that you don't need to do a join to figure out what the value represents. This may represent a non-trivial amount of resource usage, but on a modern system a lookup table is typically very small and therefore not going to cause a performance problem, or otherwise good indexing can eliminate or minimize the issue. This is a small amount of denormalization, but it's common enough that you shouldn't be surprised to see it. 
Oh I'm thinking Something like others posted. Or just a cross join... 
Did your MySQL server get updated?
Stored procedure meaning that you just run it and it does it automatically?
If you don't care about a particular session, it wouldn't matter - in your particular example, you do not. Also you don't particularly need the sessionID at all - count(event_sequence)/sum(isnull(increment,1)) would give you your average event count per session :) To give you an example of customer-specific session sequencing let's assume for a moment that "event" is a page hit and you want to build a histogram of page hits on customers' initial visit (i.e. what attracts a new customer on your site) - here the sequential number of the customers sessions will be helpful. Our you want to trend how many clicks a user takes to reach the cart based on number of times they visited the site (for example, to measure if familiarity improves user experience or to identify 'explorers').
&gt; Since this stored procedure is in a chain, what I would like to do is create a sub procedure such as [SLA].[sp_01x_Onboarding] which will look at the parent procedure, and then select it in a similar way as the second example where the code is reduced. It sounds like what you want to do is dump out the source of one stored procedure from within another? If so [sp_helptext will let you do that](https://stackoverflow.com/questions/467482/programmatically-retrieve-sql-server-stored-procedure-source-that-is-identical-t)
Basically. A stored procedure is just a collection of SQL statements that can be called like a function - it takes in zero or more arguments, performs some sort of operation and outputs zero or more record sets. https://docs.microsoft.com/en-us/sql/relational-databases/stored-procedures/stored-procedures-database-engine?view=sql-server-2017
&gt;Also you don't particularly need the sessionID at all \- count(event\_sequence)/sum(isnull(increment,1)) would give you your average event count per session :) Haha, touche. That is true, the simple example doesn't need it at all. &gt;To give you an example of customer\-specific session sequencing let's assume for a moment that "event" is a page hit and you want to build a histogram of page hits on customers' initial visit (i.e. what attracts a new customer on your site) \- here the sequential number of the customers sessions will be helpful. Our you want to trend how many clicks a user takes to reach the cart based on number of times they visited the site (for example, to measure if familiarity improves user experience or to identify 'explorers'). Ah, that makes complete sense. It's not quite the use case here, but I do see the value. 
That isn't going to work because this sproc is a child of a parent that needs to be executed in order for it to inherit the @CalcID, so a second process is going to need to be defined and maybe pass that ID before it prints?
There are system tables and DMVs which can get you the parameter you're after. You may need to do some parsing to find what you're looking for. [This from SQL Authority may help](https://blog.sqlauthority.com/2009/01/07/sql-server-find-currently-running-query-t-sql/) [You can also get information from DBCC](https://stackoverflow.com/questions/27072276/view-parameter-values-on-currently-running-procedure)
You should store it as lookup table, with id and text ID: 1 Value: "Apples" ID: 2 Value: "Oranges" Then in your other table, you store the integer ID There is very rarely a convincing argument to store the text inside the parent record, instead of referencing a denormalised table via ID 
Between those two options, using the the int might be better, keeping in mind that integers do have a hard limit. * If your tables are going to be small enough to fit within the limits of integers ( 2,147,483,647 ) and you don't have to sync your data with both locations, then a number is better. * If you need to merge two databases together on a fairly consistent basis, like two locations get their data merged on regular intervals, then consider using a guid. There is a performance hit with the Indexes, especially with the clustered index, but that could be mitigated to some degree. * If you need to identify the entry by the index, well, reconsider why you want to do that, because that's usually a bad idea.
Time to verify your data. I'm assuming that any field named `ID` is a key field that is not allowed to be NULL. Further, I'm assuming that any other field *is* allowed to be NULL. First, verify that your serial number is a valid serial number. Second, verify that the serial number exists in the `ASSET_DATA_5` table. If it is, note the value of `ASSET_DATA_5.ID`. Pay special attention to leading or trailing whitespace. Use something like: SELECT * FROM ASSET_DATA_5 WHERE FIELD_10002 LIKE '%$SerialNumber%'; We're using a `LIKE` expression to do some fuzzy matching. If you get no results, then your serial number doesn't exist in the table. If you do get a result, check to verify that there is no leading or trailing whitespace in your field. Third, verify that the asset is in your system. Look at the `ASSET` table where `ASSET.ASSET_DATA_ID` is the value of the ID you had in the previous step. SELECT * FROM ASSET WHERE ASSET_DATA_ID = &lt;value from above&gt;; If there's nothing here, then you've got a data consistency error. You've got `ASSET_DATA_5`, but no `ASSET` record for it. That's a problem. If you get a result, take a look at `ASSET.OWNER_ID`. First question: is it NULL? If it isn't NULL, then note it's value. If it's NULL, then you appear to have an unassigned asset. You'll want to use a LEFT JOIN instead of an INNER JOIN (see below). Five, if you got a value for `OWNER_ID`, then verify that it exists in the `USER` table: SELECT * FROM USER WHERE ID = &lt;value from above&gt;; My guess is that either `ASSET.OWNER_ID` is NULL or the user is missing from the `USER` table. To look for that in the future in general, you can do this: SELECT ASSET.OWNER_ID, USER.USER_NAME, ASSET.ID FROM ASSET INNER JOIN ASSET_DATA_5 ON ASSET.ASSET_DATA_ID = ASSET_DATA_5.ID LEFT JOIN USER ON USER.ID = ASSET.OWNER_ID WHERE ASSET_DATA_5.FIELD_10002 = '$SerialNumber'; Now, if `ASSET.OWNER_ID` is NULL, in your output, then there is no owner assigned to the asset at all. If there's an `ASSET.OWNER_ID` but no `USER.USER_NAME`, then either the user doesn't exist with that `USER.ID` or the `USER_NAME` field is NULL for that user. 
If you're doing it often, why not add another parameter something like @DebugFlag with default being False and use the if statement?
If you can alter the original procedure, you can put debug checks so that it knows to execute fully or just return the dynamic sql. There's a couple ways to do this. The simplest is it can check for the presence of a #temp table to direct it's logic. `IF OBJECT_ID('tempdb..#debug') IS NOT NULL THEN {debug} ELSE {normal}` Then you can get trigger the debut output like: `CREATE TABLE #debug (ID); EXEC sp_01_Onboarding` A trickier way, but may be specifically useful is with `SET FMT_ONLY`. This essentially runs through each line in the stored procedure, but with `TOP(0)` on everything and not evaluating logic or doing loops and stuff. The trick is you can know this is happening and escape out of it, by putting debug logic inside unreachable code (`IF 1=0 {debug logic}`). It might be convenient for you, because it will naturally skip parts you might want to skip anyway. Here's a simple example. IF OBJECT_ID('dbo.DynamicSqlTest') IS NOT NULL DROP PROCEDURE dbo.DynamicSqlTest; GO CREATE PROCEDURE dbo.DynamicSqlTest AS DECLARE @sql nvarchar(max) , @debug bit; IF 1=0 BEGIN; SET FMTONLY OFF; SET @debug = 1; END; SELECT @sql = N'SELECT ''This is a test'' AS NormalExecution'; IF @debug = 1 BEGIN; SET FMTONLY ON; SELECT @sql AS DebugMode; END; EXEC sp_executesql @sql; GO SET FMTONLY OFF; EXEC dbo.DynamicSqlTest; GO SET FMTONLY ON; EXEC dbo.DynamicSqlTest; GO
#1 reason to use it is (or used to be) is the 'unpivot' data transformation - converting columns to rows. Depending on the system, there are other ways to express that nowadays.
look into 'exists' condition
How about Select staging_log.* from staging_log inner join (Select distinct PID from staging_log where batch_derivation_2 = 'y') Valid_PID on staging_log.PID = Valid_PID.PID Swap batch_derivation_2 ='y' for BD2 = 'fail' in your example.
This worked beautifully! 
I work in inventory control in SCM currently. Try to find a job that has SAP HANA or good Access databases set up. I use mostly SAP , SQL for SAP HANA, Business Objects (user interface that uses SQL logic kind of), and Excel (I know a bit of VBA)
Use text and use "on update cascade" so you can modify the text keys. 20 years ago, maybe there was a perf advantage of an integer key. No longer.
What databases use case insensitive collation?
By default? SQL Server and MySQL.
https://www.db-fiddle.com/ or download and use sqlite
You could look into Microsoft Certifications, specifically https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-development-certification.aspx and https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-administration-certification.aspx They exams are fairly cheap, but training courses tend to be very expensive. Just study in your own time Doesn't matter if you plan to use other platforms rather than SQL Server, they look good on your resume anyway. Truthfully, the exams are a bit fickle but kinda noddy too. Udemy and ProMonkey courses would never hurt to bolster your knowledge, but I suspect many employers will not pay much attention to them Personally I would rather hire someone with solid experience than a few MS certifications
https://sqlzoo.net 
Get the certification offered by the vendor of your RDBMS of choice (MS cert if you're a SQL Server person, Oracle if you're an Oracle person). A random "SQL certification" isn't going to carry nearly as much weight.
You can grab SQL server developer edition for free and use free SQL server management studio to write code. You can even get a sample database called wide world importers (formerly known as adventure works). There is a ton of documentation on the adventure works and wide world importers database. It will give you a realistic expectations on what to expect walking into somewhere where they expect you to start with what they already have.
Sdo you have to write code om SQL? 
That works great for the one name but I don't know how many names there will be in advance. Is there a way to do this for all the unique names in the table? 
I was able to get this working great with a few modifications. Thank you so much! I spent hours already trying to figure it out haha
Every now and then yeah. I only look at views so im just extracting data not creating tables. I mostly look for queries that help automate my work. Im starting to learn Tableau also. Not sure how common this is in inventory management though.
Take some free classes over at EDX. A lot of those are Microsoft classes and run the range of the SQL Server/Azure world. https://www.edx.org/course?search_query=sql
I love adventure works we got back to Crystal Reports
I want to moving into IT
https://livesql.oracle.com/
One of the best online course. For beginners and for free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Oracle has a new 'pure' SQL certification as well (without dba-topics): http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=5001&amp;get_params=p_exam_id:1Z0-071
it annoys me they write "schema sql", theres a name for that! its called DDL!
&gt;Personally I would rather hire someone with solid experience than a few MS certifications That is the ideal but if someone can prove knowledge to me, typically about basic things like the types of joins and ctes then I might be pleased to give them a chance. For this the OP needs practical exposure to a popular relational database (Oracle, SQL Server etc) at least as part of their training. 
PDO and Idiorm are PHP-specific technologies, so questions about them are more appropriate in, for example, /r/php That said, the answer is "it depends". PDO and most OORMs do nothing to prevent you from righting injection-vulnerable code. What they do do is enable and promote other mechanisms of performing queries that are (or can be) injection-safe. The real key is to write no queries at all (as most OORMs allow) or to write queries as Prepared Statements with Placeholders (as technologies like PDO allow), rather than concatenating or formatting strings including user-input variables.
Compilation of free resources just for PRACTICE www.practity.com
First you need some solid experience writing and dealing with SQL. The certs will be way too tricky for someone who does not have a good foundation. You are looking at spending a lot of money to find a course that takes you from 0 to accredited and it would probably take a few weeks for even the basic certs. 
We do something like this for a number of inventory tables. SELECT d.DateValue , SUM(CASE WHEN t.Created = d.DateValue THEN 1 ELSE 0 END) AS Created , SUM(CASE WHEN t.Resolved = d.DateValue THEN 1 ELSE 0 END) AS Resolved , SUM(CASE WHEN t.Resolved &gt; d.DateValue THEN 1 ELSE 0 END) AS Open FROM Tickets AS t INNER JOIN Dates AS d ON d.DateValue BETWEEN t.Created AND t.Resolved GROUP BY d.DateValue;
I don't think certs generally help much in fields that are more development based, vs admin based. It's a roll of the dice. Are you currently in a different IT role and wanting to transition? It may be worth doing something like a MOOC certification (Coursera or something similar. Also, I know that EdX has at least one PowerBI course) and then building a case to move to the BI department in your company. I think most hiring managers are going to look for someone with BI experience or someone with a Comp Sci or IT degree (IT degree if it includes programming and not just network / security type stuff).
&gt; Personally I would rather hire someone with solid experience than a few MS certifications Experience is king usually in IT, a few paths require education but it's the exception to the rule. Experience + certs is nice though. 
Thank you for the reply but I still don’t understand very well how to pass strings and number from third user into SQL without querying. I saw also that mysql_real_escape_string can help so I’ll add also this after I understood better what you mean what all you mean with prepared statements. I’ll get a better look into this. 
&gt; without querying This is a complex topic. You need to learn a particular OORM to understand it, usually in the context of a [DSL](https://en.wikipedia.org/wiki/Domain-specific_language). The idea then is that you use OOP principles to interact with an object graph, and the ORM creates and runs queries as necessary behind the scenes to retrieve data and update the database. &gt; prepared statements This is the much easier concept to understand. Let's say you have code like $user_input_value = ...; $db-&gt;exec("SELECT * FROM t WHERE f=$user_input_value"); This is injection-vulnerable code; a major security risk. Now, you could do as you said and escape the variable before interpolating it into the string. But an easier way is to do something like this $user_input_value = ...; $q = $db-&gt;prepare("SELECT * FROM t WHERE f=?"); $q-&gt;exec($user_input_value); The "?" here is a placeholder, and $q is a Prepared Statement. When you run exec, the placeholder is replaced by your variable's value, in a guaranteed safe manner (depending on the specifics, the actual placeholder replacement might be done by the client library, or it might be done by the database server) 
**Domain-specific language** A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH softcode. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt;I want to know what the shortest path is, certification wise, to becoming an BI Skip the cert and try land a job in the field is the quickest way. However the perceived shortest path can often be the longest and considering the price of the certificates you don't want to have to re-do them because you thought you knew it already. &gt;My friends in the field tell me to not bother with mcse and mcsa Your friends are idiots. Both certs are highly valued but not cheap and also tricky to pass. &gt; I allready know SQL Doubtful, you might be able to manipulate the language but there is not a single person on this planet who 'knows sql'. Its too big for one person to know everything. If someone stated to me that they 'know sql' in an interview I would bring out the hardest most obscure questions to find out if they really did and judge them more harshly for wrong answers - you don't want an employee that over estimates their knowledge and limits to that great of an extent. I mean can you answer these correctly without google? If the answer is no then you really do not 'know SQL'. Whats the database recovery model when you first create a database? What is the order of execution of SQL statements? IE which bit gets parsed first? SELECT... FROM... JOIN... WHERE... GROUP BY....HAVING.... Can you predict the output of this code without running it? BEGIN TRAN SELECT @@TRANCOUNT BEGIN TRAN SELECT @@TRANCOUNT COMMIT SELECT @@TRANCOUNT ROLLBACK BEGIN TRAN SELECT @@TRANCOUNT BEGIN TRAN SELECT @@TRANCOUNT ROLLBACK SELECT @@TRANCOUNT ROLLBACK 
Thank you so much, and I apologize for my being slow, but could you help me understand what is going on here, logically? I am returning results but not the results I would expect, and I am having trouble diagnosing because I do not fully grasp what is occurring within the query.
Yes. Joining Tickets to Dates expands each ticket into multiple records; one record for each day a ticket was active. It's important to use truncated dates in the logic; time components can change things the way I have it written. Then the case statements perform logic for each record that determines if it was Created, Resolved, or Open. The results are finally aggregated to the day. TicketID | Created | Resolved | DateValue | Created | Resolved | Open :-|:-|:-|:-|:-|:-|:- 123 | 1/1/2018 | 1/1/2018 | 1/1/2018 | 1 | 1 | 0 124 | 1/1/2018 | 1/4/2018 | 1/1/2018 | 1 | 0 | 1 124 | 1/1/2018 | 1/4/2018 | 1/2/2018 | 0 | 0 | 1 124 | 1/1/2018 | 1/4/2018 | 1/3/2018 | 0 | 0 | 1 124 | 1/1/2018 | 1/4/2018 | 1/4/2018 | 0 | 1 | 0 125 | 1/2/2018 | 1/3/2018 | 1/2/2018 | 1 | 0 | 1 125 | 1/2/2018 | 1/3/2018 | 1/3/2018 | 0 | 1 | 0 DateValue | Created | Resolved | Open :-|:-|:-|:- 1/1/2018 | 2 | 1 | 1 1/2/2018 | 1 | 0 | 2 1/3/2018 | 0 | 1 | 1 1/4/2018 | 0 | 1 | 0
Why in the world would a business intelligence analyst need to know that sql query or about database recovery models? I don’t think there’s a clear enough delineation between DBA and BIA at your organization. 
I was making a point about stating that they 'know sql'. It really comes across as a massive over estimation of their skills. Recovery models are more DBA territory however its a good question to find out if someone really knows their stuff. The other two questions are something that anyone who considers themselves proficient in SQL should know. I mean how can you consider yourself capable of dealing with a transactional database system if you do not understand transactions work? And if you do not know the logical order in which different parts of a SQL statement are parsed you will construct poorly performing queries and not understand things like why you cannot join on an aliased column name but can order by an alias. Its being able to answer questions like these that differentiate a novices from skilled SQL professionals.
/u/jc4hokies solution is logically sound under a couple of assumptions that your actual data might not meet. If your Created and Resolved columns are actually datetime, the comparison to a date column may not behave as you expect. The plain old date column will be treated like a datetime with time = midnight. So something like "Created = Date" will only have a chance of being tru for tickets Created at midnight. The next possible issue is that your actual ticket data might be so sparse that some dates don't get returned by the query. Same would happen for recent dates if no tickets had been resolved on/after that date. Try the following, where I'm assuming Dates is a calendar table with one row per calendar day: SELECT d.DateValue ,SUM(CASE WHEN t.Created is null then 0 when t.Created &gt;= d.DateValue and t.Created &lt; dateadd(day,1,d.DateValue) THEN 1 ELSE 0 END) AS Created , SUM(CASE WHEN t.Resolved is null then 0 when t.resolved &gt;= d.DateValue and t.resolved &lt; dateadd(day,1,d.DateValue) THEN 1 ELSE 0 END) AS Resolved , SUM(CASE WHEN t.resolved is null then 1 when t.Resolved &gt;= dateadd(day,1,d.DateValue) THEN 1 ELSE 0 END) AS Open FROM Dates d left join Tickets t on t.Created &lt; dateadd(day,1,d.DateValue) and (t.Resolved is null or t.Resolved &gt;= d.DateValue) GROUP BY d.DateValue; The idea behind this one is that we're makeing sure we get a row for every calendar day by left joining onto the calendar table. This one also handled unresolved tickets and the possiblity that Created and Resolved have a time component. 
Wow, where do I send the check?! &gt;I'm assuming that any field named ID is a key field that is not allowed to be NULL. Further, I'm assuming that any other field is allowed to be NULL. Correct. &gt;First, verify that your serial number is a valid serial number. Verified. &gt;Second, verify that the serial number exists in the ASSET_DATA_5 table. Verified. &gt;Third, verify that the asset is in your system. Verified. &gt; If you get a result, take a look at ASSET.OWNER_ID. First question: is it NULL? Yes. This must be where the change took place when this system was upgraded to a newer version. Before, unassigned assets had the value "unassigned". Assets which had never been assigned have a null value, but if it was ever assigned, there was no through the front end app to reset that assignee value back to null. Setting it to unassigned sets the value to "unassigned". Well, now the front end still says "unassigned" but it's writing null values to that field. &gt;You'll want to use a LEFT JOIN instead of an INNER JOIN (see below). That was it. This was professional grade support, and I don't even know where to begin to express my gratitude. 
Check out APEX, I am pretty sure their free formatting tool will do what you want, albeit manually.
I tend to agree, and not just for these reasons, but because in the majority of settings you're not going to be using SQL so much as a proprietary extension of it. If you know 100% of the SQL standard that's great, but (I'm completely guessing here) only about 80% of any given proprietary extension is standard. The other 20% is implemented by the vendor. It's not even really a sure thing that you'll be using SQL in-house. Maybe your company has a NoSQL DB, or keep all their data in Excel. And half the work of a BIA occurs after the data's already been loaded anyway.
&gt; Doubtful, you might be able to manipulate the language but there is not a single person on this planet who 'knows sql'. You're just being unnecessarily adversarial. Your questions aren't even really that obscure. I know #2 and #3 off the top of my head, #1 is like, who gives a shit? Just specify the recovery model you want. I might be misunderstanding what you're saying though. I've never memorized the dictionary, so I "don't know English". There are plenty of obscure grammar rules that I am ignorant of, so clearly I don't know English. Indeed, the only thing that prevents me from rivaling Mark Twain is that I couldn't tell you when a verb is in the subjunctive mood. I bet you had to Google subjunctive mood. I know I did.
http://poorsql.com/ to the rescue
I am looking for a really good SQL formatter. All the ones I'm finding struggle with this code or are not all encompassing. I spent most my day formatting code to make it readable.
Thank you so much, you have really helped me understand how to approach this problem, I very much appreciate it. Thank you.
i guess i'm fortunate in having learned some killer text-editing skills i re-format SQL by hand, it's second nature, and it's faster than any SQL formatter i've tried, and i've tried a few IF OP had provided actual text instead of a friggin screenshot, i would've showed you how easy it is btw my text editor is Ultraedit 
This worked, thank you so much! The only thing is that the 'Open' at the end of the naming had to be in quotes because SQL thought it was a function. Thank you so much for your help.
I am fairly fast at formatting sql as well in notepad++ but it grows tiresome when you open a stored procedure, there are 30 statements jammed into about 20 paragraphs. Subqueries are just packed into a single line. You end up formatting it and finding it and one statement goes 3 levels deep. Lot's of SQL hokey pokey. Create a temp table, put some data in, take some data out, put some data in, shake it all about. When you have to spend 30-45 minutes going through 600 lines of code that formats to about 1700 lines of code and that spend the next 4 hours rewriting down to about 800. Ultimately, it's all shit because the underlying table structure and data handling is garbage. ugh...
&gt; Can you predict the output of this code without running it? depends on the scenario... running in SSMS, the SELECTs run immediately, whereas a C# type of app would (I believe) abort the data connections with the error message before the rows were obtained... so, in SSMS, 1, 2, 1 (but would then roll back to 0)... then 1, 2, 0 unless you use named transactions. (i'm also damn tempted to test this before submitting, but i'll wait) not saying everyone knows everything offhand... but my employer / team has fairly high expectations, to the point that candidates should have enough general coverage to feel comfortable saying "i know SQL".
This is what I did, but we use sql server at work so made sense. Call your new boss tomorrow and ask what SQL they use, then Google that this weekend - probably a setup like this for it. 
Is `tz` a column, a schema, or both?
&gt;I know #2 and #3 off the top of my head Go ahead, tell me exactly what you would see in ssms when running #3. Its a trick question for a reason, the answer is not as obvious as many assume. Based on the rest of your post I suspect you wouldnt know without running it. &gt; who gives a shit? Just specify the recovery model you want. You are not a DBA or if you are not a very good one. Recovery models are important for many obvious reasons. Even if you select full when creating a new database it will be in simple mode. Its not until you take a backup that full mode kicks in. Thats massively important in regards to recovering a DB. Its knowledge like that which differentiates amateurs who play with SQL a bit from people who actually know what they are doing and can be trusted on a live system. 
#### Sample on how to break up table to store value in lookup table Lookup tables are tables that hold a key-value pair list. Value is normally readable/descriptive for user and the key is used to store it in table. If you have a table with phone numbers. Then you may want to store the type of phone Like this: *Phone table* |Name|Field type| |---|---:| |Phone number|VARCHAR(50)| |Type|VARCHAR(50)| &lt;br&gt; You know that there is only two number types. "Work" and "Private". Storing these types as text in table, what happens if you you do type errors like "Woork" or "privatee"? Breaking out these two values to one Lookup table is better. Sample: &lt;br /&gt; *Phone table* |Name|Field type| |---|---:| |Phone number|VARCHAR(50)| |TypeId|INT| &lt;br&gt; *Phone type table* |Name|Field type| |---|---:| |TypeId|INT| |Description|VARCHAR(50)| &lt;br /&gt; To find the type name for each phone you can write a query like this ```sql SELECT _phone."Phone number", (SELECT _phone_type."Description" FROM "Phone type" _phone_type WHERE _phone_type.TypeId = _phone."TypeId" ) "Type" FROM "Phone" _phone ``` Table "Phone" is now storing a foreign key to value in "Phone type" table
I didn't think I'd see something worse than what MS Access generates but, here it is! At least Access does line breaks on each clause.
Redgate Sql Prompt. Best sql formatter in the market imo
[Poor Man's T-SQL Formatter](http://architectshack.com/PoorMansTSqlFormatter.ashx), which GP linked an online version of above, has a Notepad++ plug-in and an SSMS plug-in. It's intended for T-SQL and is about as configurable as any formatter I've used. It can absolutely handle the capitalization problem. You will *never* find an SQL formatter which outputs SQL exactly how you want. If that's what you're looking for, your answer is to write your own. You'll quickly find that in order to do it you will need an SQL parser that's as complex as the one that's in SQL Server itself because you need a parser that understands SQL as well as SQL Server does. SQL is not a simple language to parse completely. Your goal should be to get a formatter that gets you 90% of the way there. That's just reality. 
Poor Man's T-SQL Formatter is already installed in Notepad++. Sometimes it gets a bit happy with the carriage return.
I bet your coworkers love you
I am new to SQL. What should this look like?
Yes, it does. `IN` expressions make it very excitable.
Really like the one in TOAD but using Db2 which does not play well with Toad so that's out...
#3 isn't a trick question, it's a simple question. Committing reduces the transaction count by 1, rolling back sets the transaction count to 0. There's nothing tricky about it. 1,2,1,1,2,0 As for your response on #1, *that's* a trick question depending on how you want to interpret it. Yeah, it doesn't start logging until you take the first full backup, but the model is still full recovery. That's how the full recovery model functions, it doesn't start logging until after the first full backup. Either way, it shouldn't be shocking that this doesn't affect me, as with any new DB deployment I always check that I am able to both take and restore backups before it's considered production.
There's a number of ways to do this. To clarify: what would be the result if an A value is paired with exactly one B value, say B1? What would be the contents of the pair then? This: Value A | Value B1 | Value B1 Something else? 
In my experience for SQL Server, Apex has the most complete formatting options. I like SQL Prompt and I currently use Poor Man's due to compatibility issues with Apex at the moment. 
It's up to the author but something like [this](http://www.thatjeffsmith.com/wp-content/uploads/2014/03/lucida14.png) is just good manners. 
Alternatively, SQL Complete from Devart. Tons of options, you can get your code to look exactly the way you want: https://i.imgur.com/LMX8zpU.png
SQL Operations Studio had a format on paste function and it's actually pretty good, see how it handles?
DataGrip works pretty well for formatting, if you provide that in text format I'll show you how it comes out 'by default'... it may not act the same when it hasn't been able to introspect the schema objects though. They use the same basic engine as they do for code formatting in all their IDEs, so it's pretty solid. All in all I've found it comparable to SQL Prompt, but much cheaper, supports multiple database engines/syntax, is cross platform, very configurable, and goes far beyond just formatting functionality.
I blame clang-format.
UltraEdit is great. I initially bought myself license maybe 15 years ago for its ability to work with massive files and column based editing, and have never looked back. I'm sure there's plenty of others that now support the same thing but I still think it's one of the best work tools I've ever personally paid for.
I prefer to cheat, so I found an equation instead of using actual recursion to calculate the values. http://www.askamathematician.com/2011/04/q-is-there-a-formula-to-find-the-nth-term-in-the-fibonacci-sequence/ WITH cte10 AS (SELECT NULL AS Blank FROM (VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(0)) AS v(N)) , cte100 AS (SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS N FROM cte10 AS n10 CROSS JOIN cte10 AS n100) SELECT SUM(CONVERT(int,ROUND((1 / SQRT(5)) * (POWER((1 + SQRT(5)) / 2,n.N) - POWER((1 - SQRT(5)) / 2,n.N)),0))) FROM cte100 AS n WHERE n.N &gt;= 2 AND ROUND((1 / SQRT(5)) * (POWER((1 + SQRT(5)) / 2,n.N) - POWER((1 - SQRT(5)) / 2,n.N)),0) &lt; 4000000 AND CONVERT(int,ROUND((1 / SQRT(5)) * (POWER((1 + SQRT(5)) / 2,n.N) - POWER((1 - SQRT(5)) / 2,n.N)),0)) % 2 = 0; EvenSum| :-| 4,613,732|
Looks good. Although i would use f1 instead of f0. 0 is not Fibonacci number. TSQL can go up to 183 fibonacci number. WITH Fib AS ( SELECT CAST(0 AS DECIMAL(38,0))f0 , CAST(1 AS DECIMAL(38,0))f1 UNION ALL SELECT f1 , f0+f1 FROM Fib --12345678901234567890123456789012345678 WHERE f1&lt;CAST(50000000000000000000000000000000000000 AS DECIMAL(38,0)) ) SELECT f1 fn FROM Fib OPTION (MAXRECURSION 200) 
one example -- column mode to indent subqueries i paid too ;o)
Thanks for the response. What are you doing here: (SELECT NULL AS Blank FROM (VALUES(1),(2),(3),(4),(5),(6),(7),(8),(9),(0)) AS v(N)) Is there a name for what you're doing, I haven't really seen v(N) before.
It sounds really nice! I’ll have a look in the morning.
Thanks PinguinSuit! Let me know if you have any questions or if you want a free copy!
Hi, I would be happy to have a flick through. It looks like a book I’ve just bought. I have a few BCS exams in the coming months and I know I can still pick up a thing or two. Best of luck, Ashley
I might be wrong but I think you might just need a batch separator on your drop table.... drop table #TimeOut; &lt;--
Not in SQLServer
Ok, long time since I used SQL Server. I think it was true at one point though (or maybe it was just for create table etc)
SQLServer uses the keyword GO to seperate batch statements.
Hey, Thriven, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
I think I misunderstood, sorry. I now realise you are not using SQL Server. I really thought you were, but were saying the batch separator wasn't necessary
It's ok I forgive you.
Anyone or software that allows this to happen should be shot to the moon
I'll have a look. 
Hey Ashley, thanks for your interest! That’s cool, I’m hoping that this helps you with your exams or at least serves as a good reference. Can you send me your best email address?
Hey Boomer70770, sounds great! Can you send me your best email address, too?
Apex SQL Refactor. Free, plugin for SSMS.
I downloaded this but haven't installed it actually. Since I have been working in VS a lot, I have been using a SQL formatter plugin in Visual Studio. Its ok. I'll install the ApexSQL one tomorrow. I like all things Apex but they generally are costly once I find that "free" is a "trial".
It's creating a 10 record dataset. Then I cross join two 10 record datasets to get a 100 record dataset. The `VALUES` syntax is short hand for: SELECT 1 AS N UNION ALL SELECT 2 AS N ...
I get I just want to learn sql and use it for a bit in my daily job and hope I can be good as it to become sql developers. I like to create think more than making business decisions 
What's your criteria for deciding what parts to upper case?
What you have is a [one-to-many relationship](https://code.tutsplus.com/articles/sql-for-beginners-part-3-database-relationships--net-8561), which SQL works great for. Search for that term and you'll get a lot more information. (Note that when enforcing foreign key constraints with sqlite, it has to be enabled with a pragma each time you open a database).
thank you for the info. I am not good at communication , but I work hard and can be creative. that's why I would like to work on technology sign now. from inventory control the farthest I can go is allocation analyst. and it is not paying well. 
Example using sqlite: https://pastebin.com/p4r11Bkr
In the first one, the only criterion is string ‘nyc’ in the text. Imagine some rows wouldn’t contain the ‘nyc’ string In the second one, the only criterion is string ‘kfc’ in the text. Hope it’s clear enough. Thanks
[replace()](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_replace) or whatever your particular flavor of SQL calls it.
Try UPPER() Function
Yeah I was thinking of using a combination of replace and uppercase functions. Just wanted to see if there are other ways of doing it. Thanks heaps
Only by using it, all letter would end up being capital
I am going to talk to one of our merch planner next week
Excellent idea! No matter where you find yourself in the supply chain process (Demand Planning is typically in "sales", but it's deeply relevant to the supply chain, with "Merch" in Product Creation), it's important to understand how all the pieces fit together - and also struggle with each other. Merch forecasts, sales forecasts, demand planning forecasts, supply planning forecasts... they all have different inputs, time horizons, and constraints - so there's a good reason why there typically isn't just "one forecast to rule them all". 
Thanks for the help.
Yea you are right and I wrote the following on SQL Server, but can easily converted to TeraData i guess. DECLARE @TestString varchar(MAX), @WordToReplace varchar(MAX), @WordToUpperCase varchar(MAX), @SearchIndex int, @CurrentIndex int SELECT @WordToReplace = 'nyc', @TestString = 'I love nyc', @CurrentIndex = 1, \-\-search start from the index @SearchIndex = LEN(@WordToReplace), \-\-last index of the word that is being replaced @TestString = REPLACE(@TestString, @WordToReplace, UPPER(@WordToReplace)) SELECT @TestString You can easily make this a function to replace the words you want by passing the string and the word to be replaced.
 SELECT offer_id , status FROM tab.offers WHERE whatever_your_date_column_is = '2017-12-31'
Does your database track the status changes and when they happen?
there's no date column
yes
Then there should be a date column you could use. 
Awesome, that's exactly the information I was looking for. Thanks very much!
That's super helpful, thanks for the practical example
then you're screwed
If you're on MSSQL, get Redgate tools. (Ctrl\+a) \-\&gt; (Ctrl\+k) \-\&gt; (Ctrl\+y) = Viola my code is formatted pretty.
I will give it a go. Thanks heaps
Thanks! Will do.
SoftTree SQL Assistant is my preferred tool for formatting and much more. 
well sorry the is date column \- status date change, but how can I show statuses on given date when they have been changed now?
 well sorry the is date column \- status date change, but how can I show statuses on given date when they have been changed now? 
If you aren't tracking *each* change and recording a change log, then you can't go back in time to look at that history in a point of time.
You don’t need the upper function if you’re doing a straight replace. replace(“nyc”, “NYC”)
Check database if there is table with history of the changes. If there is no such table, you won't be able to find it. 
 Select id_number, max(date) From table Group by id_number
&gt; Is there a way to take combinations of columns into consideration when doing the Select? You just did, with DISTINCT I think you answered your own question already. Try it and see what you get!
perfect answer to question as stated now watch OP come back with "but there are other columns i want to return as well"
i suggest you abandon this thread, start a new one, and give full particulars
Unless your table is keeping changes historically (read, a slowly changing dimension), then what you're asking for is impossible to obtain.
Does DISTINCT take into account multiple columns though? That's where I'm kinda confused. 
I haven't thought about the max function. Thank asshelmet, now I know. Is there a good resource that has good tips for sql queries?
yes, DISTINCT applies to the combination of all columns in your SELECT clause.
Assuming there is a date field (which is uncertain based on OP's responses) and supposing that the record is only saved upon status change, the query might have to change a little to get the last updated record for a given date. For example: select offer_id, status from tab.offers where offer_id = ? and change_date = select max(change_date) from tab.offers where offer_id = ? and change_date &lt;= ?)
 SELECT id_number, date, colA, colB, colC FROM (SELECT id_number, date, colA, colB, colC DENSE_RANK() OVER (PARTITION BY id_number ORDER BY date DESC) AS myrank FROM table) WHERE myrank = 1; 
I'd use one of [these docker images](https://hub.docker.com/r/aa8y/postgres-dataset/), they are simply PostgreSQL with a pre-loaded sample dataset. You'll be up and running in minutes and can use whatever client you like ([DataGrip](https://www.jetbrains.com/datagrip/), [DBeaver](https://dbeaver.io/), [PGAdmin](https://www.pgadmin.org/)). There are probably similar projects for databases other than PostgreSQL. 
Sorry, I forgot to mention that this is in mysql.
Readable. I should be able to glance at that to see the columns and tables you're pulling from. This I can't read anything at all at first glance. It's cluttered.
If you want a "time machine", you need to catch all updates to the status column, and this must be done yourself. History of row values are not automatically tracked in typical database servers, as this would require huge amounts of storage and eat performance, so it's not a desirable feature to have by default. A good way of catching all changes on a table is with a trigger that reacts to all INSERT, UPDATE and DELETE statements on a specific table. Then you can create either a complete clone of the table (called an "audit" table), or just in a smaller table with only the columns you want to keep track of, and write the changed data to that table. Add a datetime2 column, and it's also nice to log the hostname, username and such that did the change. Info: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql?view=sql-server-2017 An alternative is to do this in your business layer, data layer or service layer, but that won't catch raw SQL updates done by hand. This may or may not be desirable.
Proper casing makes such a huge difference in readability.
You should normalize your data. That will make this simpler and more performant.
What I'm working on is creating a normalized and detailed data structure about myself. I'll insert the data afterwards, push my DB to Azure, and hook up my PowerBI to my database. Afterwards the PowerBI page will be my resume and my database code will be on the github. Shows I know how to index, architect, query, configure, and analyze data in SQL. 
FoCo... Fort Collins by chance?
You got it!
You could head on over to /r/datasets and find something tat interests you. Then get the dataset into a database, and write a bunch of queries to draw some conclusions from it. The point would be to demonstrate you are capable of interrogating and analysing data using SQL, rather than simply demonstrating you are familiar with SQL 
That's a great idea! I might look into doing something similar.
This is another great idea. This is really the type of work that interests me so I'll be sure to add something like this to my repo as well. Thanks for the suggestion!
You're welcome, good luck!
For sure man, I actually work as a data analyst though I’m a pretty firm beginner so this could potentially be a huge help. My email: Ashley.parker@manchester.gov.uk 
Normalize?
 SELECT id_number, specialdate, colA, colB, colC FROM mytable WHERE row(id_number, specialdate) IN (SELECT id_number, max(specialdate) AS specialdate FROM mytable 
 SELECT id_number, specialdate, colA, colB, colC FROM mytable WHERE row(id_number, specialdate) IN (SELECT id_number, max(specialdate) AS specialdate FROM mytable GROUP BY id_number) 
Hell Yeah!! Go Rams! I've been out of college for nearly a decade and haven't been to FoCo in 5+ years. I'm hoping to fly back there this summer to check out the new stadium! That's a great idea for a resume too. And a cool way to have a personal playground environment!
This is the better query. The one in my OP post is missing a few of both duplicate values.
Glad to help. I hope it works for you.
Compare the state's death records to state's voting records. do it carefully though, so you won't get suicided.
Good idea. 
Does Python have an embedded SQL package available like R does? R has sqldf that allows you to write SQL queries against data frames and spawn new data frames. If Python has something similar, you could bundle both into one repo. That would also demonstrate reproducible research which is expected of data scientists. So, given /u/pease_pudding 's suggestion, your script could download a data file, cleanse it and prepare it in SQL syntax, and finally run statistics and visualizations, all in one file that anyone should be able to download and run.
Hold on a second. Big tables are not scary. Databases can handle large tables. Don't be afraid to exceed 100 million rows or 1 billion rows just because they are big numbers. Performance on a 100 billion row table can be great. Also, more shards or partitions is not necessarily better. More database objects can mean increased overhead of administration and maintenance (sometimes not much more however) or sometimes more complex code (not always, as it can be transparent to code). So don't try to break a perfectly good large table into small chunks unless there is a compelling reason to do so. Having said that, I do agree with the idea of minimizing the number of rows if you can. All things being equal, that's a good idea. Why choose some arbitrary partition key like that? Will you ever want to execute a query across partitions? Yes, you will. So to me that is a big red flag that it's the wrong partition key. What if a customer has 5 transactions, all for different lengths of time? That one customer's data will be in 5 partitions. Yuk. You will not benefit from having broken it into pieces, and performance might get worse. What if I don't know how many tables this customer has data in? My single index lookup becomes 30 index lookups. &gt; the big table had all the proper indexes Are you sure? You could have accomplished the same performance gains by introducing a composite index with your INT column being the first column in the composite (multi-column) index. That way you get the benefit of high performance while preserving the simplicity of a single table (and no changes to your application). 
As /u/asshelmet said: yes, DISTINCT is what you want, which works for multiple columns. However, if you're creating a brand-new table, you can use `CREATE TABLE TableY AS (...)`. Depends on if TableY already exists or not.
Let me just add to this a little more. What about a time-based partition. I bet you don't care much about what prices and availability were 3 years ago. And your customers don't care. The data is there for historical reporting and trend analysis, but nobody needs prices from 2015 to book today. So why not split it by year? Operationally it makes sense. And you would rarely ever need to run queries across years. But I can imagine there are reasons why you *would* want to run queries across multiple availability timeframes. Pick business, functionally meaningful data to shard on and think of all the use cases. Post some more specifics and we can help make the performance great. We can help you tune the hell out of your database design and your SQL. I just think this 30 table decision will come back to haunt you later.
Thanks for the feedback, appreciate some input. &gt; Will you ever want to execute a query across partitions? Yes, you will. Actually no. A customer searches for holidays homes in a certain region, at a certain price limit, and a certain length of stay. A search will never span 4-7 days for example, always precisely 7 days, 10 days or whatever. That's why I chose to shard/partition by this value. &gt; Are you sure? I can't say for sure, but everything indicated that it had. No alerts in the execution plan, all hits to the table were index seeks, the column in question was in the index(es), so I couldn't find anything better to index by. &gt; I bet you don't care much about what prices and availability were 3 years ago This is already taken care of actually. The SQL jobs that update the huge table with prices is updated nightly, and only contains the absolute minimum necessary data - from now, and 2 years ahead. &gt; why not split it by year? Queries could span years, if a customer wants to book from 24th of December to 2nd of January. Rare of course, but not impossible. Since we're only dealing with 2 years in my case, perhaps another good split would be by month. 24 tables, each table representing 1 month ahead of GETDATE(). But that's pretty much the same downs and ups as my current solution - multiple schema-identical tables, dynamic SQL to use the correct tables, but also with the downside that it would have to very often look at two tables at once, when a booking spans two months. &gt; Post some more specifics I'd like to, sure. Many people in my previous thread naturally couldn't understand why you'd need a thousand-line ling SP and so much fuzz to look for holiday homes. That's what all developers have said upon joining the team - how hard can be it? A table with houses, a table with bookings, a table with some prices and book, two joins, done. Wrong, hahaha. You wouldn't believe how complex it is. Off the top of my head: * Everything, and I do mean everything is date-driven. A house itself has a start and end date. The rooms in the house have a start and end - because a new room could be built upon the home. A bed in a room of a house has a start and end date, because it could be removed, or replaced by a king size bed. The spa in the house has a start and end date. The owner of the house has start and end, because it could change. The price plan for the house has a start and end. The seasons of renting the house have time periods, because obviously the summer holiday periods cost more to rent than the winter. The discounts available on the house have start and end. The mandatory cleaning fees on the house have a start and end. ARRRGH everything is date-driven! * Discounts. Sound simple, book now and get 20% discount right? No. There are many types of discounts - a discount can be applied depending on the length of stay, the number of people booking, and/or how close to the arrival date you are booking. The search SP needs to calculate the potential prices of all matching houses in the region searched, find the cheapest one, and return it to the customer, with respect to a potential max price filter on our website. And of course discounts can be used across multiple houses, and have a start and end date. * The SP also needs to return the count of houses available for booking on the day before and after the date entered when searching on our website, to lure customers into potentially booking a day ahead or after. As you can see, it's not so easy. In my opinion though, the fundamental concept of this denormalized search database is solid - create SQL jobs to sniff data from the master (not the "master" database, but your main pretty denormalized database), dump it into denormalized tables tailor-made for fast searching, and create an SP which is top-tuned for searching data, quickly.
&gt; A customer searches for holidays homes in a certain region, at a certain price limit, and a certain length of stay. Are customer searches the only way this data is used? You never ever query this data for anything else? Not from the property owner's perspective? Not for management reports?
&gt; everything is date-driven That's what I'm talking about... that sounds like your key field: time. But date/time is not the same as how long a customer wants to rent a property.
Yup, this whole ordeal is purely about searching. Property owner reports are done ad-hoc, perhaps once monthly about the activity on their house, and management reports are either done via Datawarehouse (which has its own environment I don't know much about), or through SQL extracts where it doesn't really matter if it takes 1 second or 1 minute. Keeping customers on the booking website is key, and as you can imagine, slow searches are a big turn-off. That's why I'm working on this project to improve search times.
Wouldn't it be better to make a website that does.... something, indirectly demonstrating the use of a database? Something people could interact with and search through data, rather than just code? Perhaps you could snatch some data and make a website to allow searching the data, based on certain criteria and filters.
Ok. Just trying to give some help from the outside looking in. You know your application better. It just seems to me that this might be one of those cases where other tuning techniques might have solved the issue better. In fact, I'd wager that I could as good or better performance from the big table compared with the query times you get against the 30 tables, given the chance to tune it. Not saying your solution is a bad one. It looks like it's working for you, and it is making sense. It just might have been unnecessary. 
That sounds like a really cool idea! Can you elaborate about the detailed data structure you would use? I'm having trouble imagining any sort of data structure about myself having any meaningful interaction with Power BI. I'm currently interning at a job where I am heavily using database stuff and this sounds like something I can work on during downtime to really help me when the internship ends! 
Is this a common practice for hiring managers to look at Github? Or more so just verifying skills? 
Pyodbc is great for SSMS and postgres and pymysql has great documentation for querying mysql.
Normally I don't have the time, I doubt HR do either. 
I doubt HR would understand the contents of a Github repo.
I'm really jealous. This looks like such a fun problem. Looking at your [queries](https://www.brentozar.com/pastetheplan/?id=Bk1SmuM9M) I wish I got to solve big problems like this at work. 
&gt; I'm curious why you didn't go this route Tabooyah's approach made sense, but doesn't cover the complex price logic we have. A table with just house prices per day isn't enough, because as mentioned, the price of booking the house per day varies depending on how many total days you booked, how many people are staying, special discounts available in the booking period, and even the amount of days until arrival influence the price. So it's just much more complex than it sounds. Also, without having tried it, I think doing the NOT EXISTS on the Bookings table will end up being dog slow when you, like us, have hundreds of thousands of bookings. Right now we have some denormalized tables which, for example, contain every day (date field) from now +2 years, house ID, and whether the house is available on that date or not. Believe me it's more complex than it sounds :)
It’s the new hotness to add your github to your resume if you are job shopping.
If that's your only restriction in the clause, why have you it at all? If it's not, why use string 1 instead of numeric 1? While testing my code, I frequently have something like this: WHERE 1 = 1 AND foo = bar AND fizz &gt; bin This lets me test specific parts of the where clause by commenting out individual lines.
didn't get u i mean what's the ouput
I would say maybe 70% will look at my website. I have not officially advertised it, so it's pretty easy to track the stats. I notice a surge in traffic typically after submitting a resume. As for the git, no idea but I honestly doubt anyone has ever looked at mine. 
Is there any case where that isn't true? No. So it will return all rows.
I cannot take credit, I met Tjay Belt at a SQL Saturday and his presentation was on PowerBI for the SQL Server DBA. Lots of great stuff and I think his slides are on the internet. What I used as a rough template to start was exporting my Linkedin profile as a CSV. Here's a few examples I've seen that impressed me: [Source](https://ideas.powerbi.com/forums/265200-power-bi-ideas/suggestions/15977968-a-live-and-interactive-resume-with-powerbi) [Resume 1](https://app.powerbi.com/view?r=eyJrIjoiYTU4NjU5YWUtMTJlZC00YjNmLWFhYzktZWE4ZDFmYjdkYjAxIiwidCI6ImI4ZGFiMmRlLWMzZWUtNDdmYS04NGIzLWVhNzE5OGUyOGFiZCIsImMiOjZ9) [Resume 2](http://josepropm.com/my%20blog/how-to-create-your-own-dashboard)
What doesn't work with SQL Fiddle?
SQL Server 2012 is a software service that runs a database engine and a gui interface to allow you to interact with it along with many more components. When you have SQL Server installed on a machine, you can typically connect to it as the localhost. When it is installed on another machine, you must input the name of the instance or the IP address of the SQL Server instance that is running. Can you define what test is? Your scenario and information listed is vague. Are you testing whether you can log into the database? Are you taking a test on a database? Are you testing the data within the database? Testing ETL processes inside the database? What was the project you were assigned? Where is the SQL Server instance in relation to you? There are hosting applications like AWS, Azure, and others that will host a SQL Server in the cloud, but this typically requires some money.
It says "sqlfiddle didn't send you any data". I'm searching using my phone, not sure if that's the problem it won't load it for me.
Also, I believe if it's a named instance on a remote machine that needs to be added to the connection details, as opposed to if it was default of MSSQLSERVER. Something like MACHINENAME/SQLINSTANCE 
If using the return command then true returns 0.
Theme of the assignment is on Rent a car, not much info besides that has been given to me. I'm taking a test on database. I've scribbled something on paper to try. I am aware of local connection but that dosnt work as well. Not sure if I accidentally missed a step or something that would cose this problem. I've reinstalled it 3 times now and still have no clue as to why it's not working.
i think it retrurns only when user column called 1 and username is 1
how it's possible to return all usersdata
Hmm you're right, it appears to be down. This is similar : https://www.db-fiddle.com/
Example query: Select cust_id, cust_name From customers Where 1 = 1 This query will only return from the “from” clause table, and only the “select” clause listed fields. The where clause will filter off rows where the statement evaluate to not true. Since there is no case where 1 &lt;&gt; 1 it returns all rows in the table. It is exactly the same as no where clause.