what is a clause
&gt; will only return from the naturally without technically in all cases when the condition is true it extracts all users data
It's working, thank you.
A clause is the basic unit of a SQL query. Select clause: "Select *" or "Select Cust_ID, Cust_Name" From Clause: "From table" Where Clause: "Where 1 = 1" it is the parts of a query. 
You need to do a [tutorial](http://www.sqltutorial.org/). Go use this it should help, you need to learn the correct words to use for your questions and then you will ask better (more answerable) questions and in the process, a lot of these questions will be answered. 
arguments like in other programming languages?
Arguments are passed into a function, and a query is maybe a little like a function. I guess you could view each of the clauses as a function and the details as arguments. Query: fx 1: Select: Args: Cust_id, Cust_Name: Purpose: declares what to return fx 2: From: Args: Customers table: Purpose define the domain to select from fx3: Where: Args: 1=1: Purpose filter the returned data. In this case, the fx3 is always true, so it is the same as no Where(filtering clause). 
So you have existing customers, returning customers and new customers right? And you want the list of existing customers ?
Yes. And because it is web based segments, it is possible for a user to be identified in more than one segment. Like when you first visit the site you are new, then returning and convert later then existing customer 
Is this what you're after? SELECT * FROM ExistingCustomers WHERE NOT EXISTS ( SELECT * FROM NewVisitors INNER JOIN ReturningVisitors)
give this a try, since you got new visitor table and returning visitor table, you can filter out the visitor who do not belong to both of these tables. SELECT date, segment\_id, visitor\_id FROM main\_table AS m LEFT JOIN returnvisitor AS e ON e.visitor\_id = m.visitor\_id LEFT JOIN newvisitor AS r ON r.visitor\_id = m.visitor\_id WHERE r.visitor\_id IS NULL AND e.visitor\_id IS NULL
are you asking reddit to run a sql query for you and tell you the result? why don't you just install sql and find out?
give this a try, since you got new visitor table and returning visitor table, you can filter out the visitor who not in both tables. SELECT date, segment\_id, visitor\_id FROM main\_table AS m LEFT JOIN returnvisitor AS e ON e.visitor\_id = m.visitor\_id LEFT JOIN newvisitor AS r ON r.visitor\_id = m.visitor\_id WHERE r.visitor\_id IS NULL AND e.visitor\_id IS NULL AND e.visitor\_id IS NULL
Thank you! That example looks amazing and gives me something to really strive for! 
Your post history is as confusing as your question. From PHP to C to "blisters on face from masturbating" to SQL. None of which appear to show any clear understanding of the actual subject matter in question. Best of luck to you, but I really think you should stick to a single topic and attempt to better understand that instead of hopping around.
If you want to comment out individual lines then why not just comment out individual lines?
If you have something like WHERE foo = bar AND fizz &gt; bin And you want to comment out foo = bar, I guess the easiest way would be to comment the where and change the and to where. The other way lets you comment each and without worrying about the WHERE 1 = 1 line.
Nope. Sorry. We are really low on reddit fuel today. May need to try tomorrow when we get some more. /s Whats the error?
Bwahaha yeah screenshot or it didn't happen.
Are you able to test the connection on ODBC?
Sorry. I’m a noob. What’s ODBC?
Looking at the query plan, I also optimized another thing. If you have a huge SP that performs dozens of queries, a good strategy is to play whack-a-mole at each query the SP contains. In SQL Server Management Studio, you'll see how many percent each query takes of the total execution time, and this gives you a good method to try to optimize the heaviest queries. Of course some queries need to take up a lot of the execution time, and naturally it'll all add to 100%, but still. For example, I reduced one query's part of the total execution time from 16% to 1%, which reduced total query time by around 100-150ms. I did it by creating a new table which is now nightly populated (by an SQL job) with just the necessary, denormalized data, instead of before where it looked at three normalized tables in order to stitch together some metadata about images. The SQL job uses a simple MERGE statement to just update the necessary data from the source to target. As a general concept, if you want fast searching, it's all about denormalizing data, and then the fine art of balancing how quickly data needs to be updated with respect to master data. Most companies/systems will have lots of data that doesn't need to be updated in real-time, and you can use this to your advantage.
Dude, how are we even supposed to try to help when you give next to ZERO detail on your problem? FFS
Could you provide a screenshot of the error? Or at least more information so we could have some context? This is kind of a blind call for help. 
Usually you set up the connection on ODBC and then map to SQL Server with linked table connection. https://support.office.com/en-us/article/import-or-link-to-data-in-an-sql-server-database-a5a3b4eb-57b9-45a0-b732-77bc6089b84e
What's a computer?
Yep!
Those things are still here. Are you using a mobile view of the site, perhaps?
You are right. Thanks 
http://www.reddit.com/r/SQL/wiki/index
Seems to me that you could have fewer rows by including more columns, one for each price variation on a property for a given day. It is possible that it adds 10 or 20 or even 30 columns for all the discounts and possibilities. But that seems preferable over hundreds of millions of records, and better than breaking the table into 30 pieces. After reading this thread and the other original thread that was linked, it appears like there is something fundamentally wrong with the data modeling for this system. I would look more closely at how the data is organized before attempting to tune the SQL.
First bit of advice is to know that "SQL" is not a database. I can only assume you mean MS SQL or SQL Server. It would be funny if they are looking for "a DBA who knows SQL" and it turns out they need a MySQL database administrator who can code SQL queries.
I'm a full stack web developer and I only have project experiences on my resume. I did put that I know SQL but only the basics. Yeah, they want someone to Administer and maintain SQL Server 2016.
If I were interviewing for an entry level DBA, I'd ask about: 1. Backup / Restore 2. Index design 3. Index maintenance 4. Where they like to eat
&gt; After reading this thread and the other original thread that was linked, it appears like there is something fundamentally wrong with the data modeling for this system. I would look more closely at how the data is organized before attempting to tune the SQL. How data is modelled (in this search database) is really up to me as the main DBA (since it's de-coupled from the DB with master data), but I can't really argue against the business rules. We need discounts of all these crazy types, prices in various currencies, and the goal is to deliver low search times on our website. I wouldn't call myself an expert database developer (but in the advanced stage), but the previous DBA who designed this made some really ingenious table structures that are still good. When you look at some of the tables you go "WTF?", because it's just millions of rows of IDs, bits, dates and some decimal values, absurdly denormalized. Some tables only have 2-3 columns, and some are only used in a single sub-query of the large SP. But that's the whole point, because these tailor-made tables are designed for super quick query times. Having SQL jobs grind nightly to update these tables cost little to nothing in the big picture, since our website is close to dead during the night (single time zone market). For example, having a big "Booking" table with 500,000 rows (in our case) with HouseID, Arrival and Departure date might sound logical, but instead, we have an "availability" table with just two columns: HouseID and Date. Each row represents a date where a specific house is available. If there's no row in the table on a given date, it's booked. If I'm not mistaken, this should be multitudes faster to query, given the proper index. As said in another post, it's possible it can be done better, and I'm not claiming to be an expert, but I think the concept of making these tables that are super-optimized for searching makes sense. It would be pretty awesome if a database developer who has worked with bookings of sorts could contribute. :)
Is first interview consists of behavior and technical portion? or does it just depends on the company?
An on-site interview for an entry level position is the only interview, so it will include technical, culture, and everything else.
and I want it In respect to the total amount in the column
I see lots of comments saying it is very denormalized and lots of comments saying there are several tables with just 2 or 3 columns. Those statements are not consistent with each other. 
So Workbench is a client, which allows you to connect to a server. You need to be running a server as well. You can run that on linux or windows, but you need to run a server/create a database. https://dev.mysql.com/downloads/installer/ Get and install the server from here. Am assuming you're on windows. 
try AVG(the column name)
It's hard to say precisely, and I can't of course my company's database, but yes, it's a mix of tables, some hyper-denormalized, some less, each particularly made for a specific purpose and parts of the whole search database's searching and price calculation functionality. The queries that make up the huge SP are fairly simple, SELECTs with 1-2 joins typically, and just a lot of where clauses for the dates, timespan, prices, etc. You might ask: Why not pre-calculate every possible search query, but that would make the data amount explode, and 99% of it would not be hit. It's a fine middle ground between pre-calculated data (which can be done overnight) and on-the-fly calculations.
Depends on the Type of DBA. Operational (Production) vs DevOps (Development DBA) I am a DevOps DBA since I had an Application and SQL Development background. Sysadmins tend to go for Production/Operational DBA positions. This article helps out a bunch. https://www.brentozar.com/sql/picking-a-dba-career-path/
 SELECT q3 , partial , 100.0 * partial / ( SELECT COUNT(column_1) FROM table1 ) AS percent FROM ( SELECT COUNT(column_1) AS partial , q3 FROM table1 GROUP BY q3 ) AS subkwerry
I downloaded installer and somehow it worked but its not asking me for a password anymore lol
Ok. Sounds like you have everything figured out then. 
Significantly. Even with a low speed SSD eliminating the seek bottleneck is going to make a huge difference. 
Sounds like you want a join with three tables 
That sounds perfect, I'll start looking up syntax and go from there. Sorry only been working with sql for a few months still new to all this. Running queries like this won't permanently join tables this will only display the results from the query right?
Weird: I couldn't see it when using my laptop but now I'm in my phone, it's viewable
&gt; Running queries like this won't permanently join tables this will only display the results from the query right? Correct. You're going to use a SELECT statement with a JOIN in the FROM clause. That just pulls data out of the database and doesn't alter the database or its contents.
Sounds like a simple inner join on entity id
You just need a basic inner join on entity id
The Data Warehouse Toolkit, 3rd edition, by Ralph Kimball. 
Microsoft transact sql the definitive guide by Jeffery Garbus
The lower factors of a number are the numbers up to the square root who produce 0 as a divisor's modulus DECLARE @Start INT, @End INT SET @Value = 600851475143 SET @Start = 2 SET @End = sqrt(@Value) # no smaller factor will ever be above sqrt SELECT DISTINCT n = number FROM master..[spt_values] WHERE n BETWEEN @Start AND @End AND value mod n = 0
sql in a nutshell select * from stores join cities on stores.inCityId = cities.id join realities on cities.inRealityId = realities.id where store.isOnFire = false and cities.isDestroyed = false and cities.controlledByThanos = false and realities.controlledByStewie = false and stores.hasBattleToads = true order by realities.title, cities.name, stores.name; 
Yes just think about the fact that there is no platter spinning back and forth constantly.
I've heard of some people shooting the shit about spinning up RAMdrives for DB's, but one unexpected reboot can ruin a project. If you're not using a specific server hardware, your ACID tests are relaxed and don't need high-reliability RAID arrays, and you can cram the DB onto an SSD in your budget, then an SSD would be preferable.
For the SQL syntax and practice problems: SQL Queries for Mere Mortals For theory about how to create a robust normalized database: Database Design for Mere Mortals
The textbook my database management class is using is murachs sql development 2016. I like it. Good simple problems and basic overviews for tsql. 
https://www.vitalsource.com/products/database-administration-fundamentals-cci-learning-solutions-v9781553323860 my professor suggested this for my database/sql course. It also prepares you for the Microsoft exam.
hello! got the same problem. thanks!
Very important for the speed of importing data into SQL servers is *how* you are importing data. Anyone know what methods workbench uses? If possible, use "LOAD DATA LOCAL INFILE" instead of INSERT statements. Additionally, how large are the CSV files, and what kind of hardware is backing all of this?
I did it a while back. Here was my solution. Runs in 10 seconds, most of which is spent populating the numbers table. DECLARE @Number BIGINT = 600851475143; DECLARE @MaxDivisor INT = FLOOR(SQRT(@Number)); CREATE TABLE #Numbers ( Number BIGINT ); WITH P0(N) AS (SELECT 1 UNION ALL SELECT 1), /* 2^2^0 */ P1(N) AS (SELECT 1 FROM P0 A, P0 B), /* 2^2^1 */ P2(N) AS (SELECT 1 FROM P1 A, P1 B), /* 2^2^2 */ P3(N) AS (SELECT 1 FROM P2 A, P2 B), /* 2^2^3 */ P4(N) AS (SELECT 1 FROM P3 A, P3 B), /* 2^2^4 */ P5(N) AS (SELECT 1 FROM P4 A, P4 B), /* 2^2^5 */ Numbers(N) AS (SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) FROM P5) INSERT INTO #Numbers (Number) SELECT N FROM Numbers WHERE N &lt;= @MaxDivisor AND N &gt; 1; SELECT TOP 1 Number FROM #Numbers N1 WHERE NOT EXISTS ( SELECT * FROM #Numbers N2 WHERE N2.Number * N2.Number &lt;= N1.Number AND N1.Number % N2.Number = 0 AND N2.Number &gt; 1 ) AND @Number % Number = 0 AND Number &gt; 1 AND Number &lt;= @MaxDivisor ORDER BY Number DESC;
Are you using MSSQL, MySQL, Oracle..? I’ve found that SSIS on SQL Server works quite well; I’ve also made some scripts with BULK INSERT that work quite well. Keep in mind, a lot depends on the size of the data! 
https://stackoverflow.com/questions/33296569/mysql-workbench-table-data-import-wizard-extermely-slow
I’m using MySQL, files range from 50mb to 300mb. Would it be possible to share those scripts?
I’ll look into this
Load data local infile might be the way, I’ll look into this tomorrow. The files range from 50mb to 200mb. Using my MacBook to run the sql server. I am currently learning the system. 
You can practice SQL on the following resource. There you can submit exercises get feedback online. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
It was a rather large import for a customer using MS dynamic SQL, but I believe it would work fine for MySQL. I’d have to double check to be sure. They worked quite well for pretty hefty imports. There would likely be slight syntax differences you might have to iron out, mainly what delimiter you’re using and any MSSQL/MySQL differences, but the basics of what you would need are there. It’s wrapped in a stored procedure, so it’s pretty clean and quick. I’ll have to go over it, but I’d be happy to send it your way. 
That would be awesome! Thanks!
Hi This is not working for me :(
Thanks this works actually. Just one question, why is the sum of nv,rv and existing customers not equal to total number from main table? I'm using unique values though 
The below works for SQLServer provided you use a table instead of an array (and include your wildcards in the table). I'm sure there is a better way. CREATE TABLE #tmpHost ( HostName VARCHAR(50) ); INSERT INTO #tmpHost ( HostName ) VALUES ('Something.Host1.Somewhere'), ('Something.Host2.Somewhere'), ('Something.Host3.Somewhere'), ('Something.Host4.Somewhere'); CREATE TABLE #tmpFilter ( FilterName VARCHAR(50) ); INSERT INTO #tmpFilter ( FilterName ) VALUES ('%Host1%'), -- The % are important here ('%Host3%'); SELECT HostName FROM #tmpHost a WHERE EXISTS ( SELECT 1 FROM #tmpFilter b WHERE A.HostName LIKE b.FilterName ); DROP TABLE #tmpFilter; DROP TABLE #tmpHost; 
not sure what your actual error is, but you should change your date condition to this -- WHERE close_date &gt;= '2017-10-01' AND close_date &lt; '2018-02-01'
It's really not as hard as it sounds. Window functions create a set in your result set (the "window") with the `PARTITION BY` clause, similar to the `GROUP BY` clause. Unlike `GROUP BY` though, the result set is not reduced to just these GROU PBY values - instead the entire "source" is preserved. Imagine a table like this: x| y ---|--- A | 2 B | 2 B | 2 B | 1 C | 1 C | 1 If you grouped by x, you'd get 3 resulting rows - for A, B and C. If you `PARTITION BY x`, you'll still get 6 rows, but groupped (partitioned) into get 3 sets: one for A, B and C; if you partition by y, you'll get two sets, one for 1 and one for 2. The simplest function is `ROW_NUMBER()`, which just give you the number of row within the set, given an order. So if you `ROW_NUMBER() OVER (PARTITION BY x ORDER BY y)`, you'll get: x| y| row_number ---|---|--- A | 2 | 1 B | 2 | 2 B | 2 | 3 B | 1 | 1 C | 1 | 1 C | 1 | 2 You get 1 for A because it's the first and only row in the 'A' window. Then you get 2, 3, 1 for the 'B' window, because we're ordering by column y and 1 comes before the two 2s that we have there. So actually the fourth row in our table is the 1st row in our 'B' window when ordered by y. And then again you get 1,2 for x = C. So the `PARTITION BY` sets the boundaries across which your function operates. If we did `ROW_NUMBER() OVER (PARTITION BY y ORDER BY x)` (switched the partitioning and ordering columns): x| y| row_number ---|---|--- A | 2 | 1 B | 2 | 2 B | 2 | 3 B | 1 | 1 C | 1 | 2 C | 1 | 3 We only have two distinct values in column y, so we get two windows and the results in each are ordered by x. A &lt; B = B in window for y = 2 and B &lt; C = C in window for y = 1. Now onto some other function: SUM(y) OVER (PARTITION BY x) is simple enough, for each distinct x you'll get a sum of y, so: x| y| row_number ---|---|--- A | 2 | 2 B | 2 | 5 B | 2 | 5 B | 1 | 5 C | 1 | 3 C | 1 | 3 For A, it's just 2, so the sum is 2. For B, we have 2 + 2 + 1 = 5 and for C we have 1+1 = 2. You can of course use any valid SQL expression in both the `PARTITION BY` and `ORDER BY` clauses. Like for example you can use `ROW_NUMBER() OVER (ORDER BY random())` to order the entire set randomly; notice how I omitted the `PARTITION BY` - in this case the entire set is our window. So now onto your query from SO. Let's break the entire query down to pieces: SELECT country, food_id, COUNT('x') AS freq FROM country_foods GROUP BY 1, 2 That's simple, this counts how many occurrences of food_id were there in each country. Basically: give me a list of how many times each food was consumed in each country. Then you apply `ROW_NUMBER()` to the results, partitioning by each country so that each country gets its own set (window) and in that window, you want to order by how many times given dish was eaten, in descending order. In other words, rank these foods by how often were they eaten (actually `RANK()` is a window function too, as is `DENSE_RANK()`, but they produce different results for items that have the same counts, read up). `WHERE rn = 1` (rn is just the name that was given to the result of that `ROW_NUMBER()`) gives you the first, for each window, dish when ordered by how many times it was eaten. Or in other words: the most often eaten food in each country.
&gt;Using my MacBook to run the sql server. Just so you know, SQL Server is a product from Microsoft. Since you wrote that you're using MySQL in another comment I understand what you're getting at, but in the future using "sql server" to mean "database server" can make it harder for you to get the correct help and/or search results.
Sorry I never got back to you about this. This was so helpful! You were spot on with your example syntax which led me to figure out what i wanted to do in the end Thank you very much 
No problem! I've got my non-ethical hackers on you to get to your personal info so that I can bill you for my precious time! One more thank you message will double my fee, hope you can understand. :-) 
Turns out the error was caused by the semi colon at the end. Cheers for the tip to clean up the syntax a bit. This will later turn into a variable to use current week and going back 3 months 
Thank you for putting in so much effort to write this up. Your explanation really made it easy for me to understand what is happening. Thanks a to!
Managed to get this join working after a litter while. Cheers for the advice, quite shocked by the result
Of course it's not going to work as written. Can you share the modified code isn't working for you?
Any suggestions? I can't quite figure it out. It looks as if the IN() is null when it is trying to run? SELECT user\_id FROM (SELECT \[employee\_id\] ,COUNT(\*)\[TRAN\],CAST(end\_tran\_date AS DATE)\[create\_date\] FROM \[t\_tran\_log\] WHERE wh\_id='0750' AND CAST(end\_tran\_date AS DATE) BETWEEN '02/23/2018' AND '02/28/2018' GROUP BY employee\_id,CAST(end\_tran\_date AS DATE)) AS BaseData PIVOT( SUM(\[TRAN\]) FOR \[create\_date\] IN() )AS PivotTable Msg 102, Level 15, State 1, Line 9 Incorrect syntax near ')'.
sure enough there's a lot more to sql, ***but***, this turns out to be 95% of what you ever do in sql
Out of curiosity is there a difference in syntax between something like phpmyadmin and passing a myquery inside a bash script? I'm getting errors left right and centre automating this 
How are you doing th eimport today? Are you doing it in a [single transaction](https://dev.mysql.com/doc/refman/8.0/en/commit.html) (start transaction ; do the work ; commit)? Are you doing the import from a different machine than the server? Do you have ssh access to the server? scp'ing the files to the server and running the mysql client there to do the input can make a big difference.
I wouldn't bother using a text editor to write code. SSMS is pretty darn good. With the ApexSQL free tools, it's all I need. I use Notepad++ if I ever have to get into ascii editing, but that's pretty rare - only if I'm looking for weird chars in input formatting. If I'm writing anything besides SQL, I use Visual studio or VSCode.
I've never used a text editor for SQL. Always been in an IDE? like SSMS. Not sure why you would want to give up a feature like Intellisense. I have used Sublime Text for other languages like HTML/CSS and I loved it. If you are doing SQL, might as well use a tool designed specifically for SQL. You get the benefit of SQL focused features while also learning industry used tools.
These days I know my ecosystem at work well enough that I know most objects by memory. I typically write all of my SQL in notepad ++. If I am working on something where I don't have stuff quite as memorized, I might copy and paste the table structures into my notepad++ file at the bottom. There are plugins for notepad++ that do auto-complete, so with those plugins, it will pick up object names already in the doc for autocomplete. That said, when I was first learning SQL, i cut my teeth using Toad for Data Analysts / Toad Data Point. It has a visual query builder. I would use that, but once I had reached the limits of that I started using the SQL editor there to further learn how to hand-write SQL. I still use notepad++ as my primary dev environment for writing SQL, but I use DBVis for all my operations, so sometimes I will modify, tweak, etc a SQL in DBVis' built-in IDE. It's can be quite handy at times, and I can see how someone would like to use that as their primary IDE. Regardless of what tool(s) you use, I definitely suggest learning to write SQL without the crutch of any helpers, because it makes you much more flexible and adaptable to the situation you may find yourself in, and gives you a better understanding of what you are actually doing. Many IDE's won't come inbuilt with all of the possible functions and commands for various databases, so you still end up having to write a lot of code by hand anyways. Bottom line, work in whatever tool you are most comfortable using, just be sure not to let the tool become a crutch you cannot live without.
there's no such thing as passing a mysql query in a bash script. i have no idea what tool you're using, but if you're using mysql -e, you probably have quoting issues don't try to learn sql in phpmyadmin or bash just use the console
Thanks. I'll have to deal with a 256GB SSD and 2TB secondary drive for now, but the plan is to upgrade to a better workstation later if the system proves its worth. I can just shift datasets back and forth as necessary.
I use Sublime, pretty much exclusively. I find that the multiline editing, which I use quite a bit, is much better than n++ or SSMS.
SQL For Smarties and the corresponding workbook are pretty solid. 
I wrote all of my sql in either SSMS or VS. The times I tend to use a text editor (always Notepad++) will be when I don't want to open SSMS and want to view something instantly.
i use Ultraedit -- it knows SQL, too 
Hey, do you feel ApexSQL is worth investing time into? I’ve been working on SQL for about a year now and haven’t found it lacking. Does Apex do that much? It’s not the first time I’ve heard it’s useful. 
Occasionally I use Notepad++ to store bits of code, but really it's just while I'm building a more complex query, not as an editor. 
There's no reason not to use a SQL specific IDE these days. If you can't or don't want to use SQL Server Management Studio (SSMS) then go get the new SQL Server Operations Studio (SSOS) which is Microsoft new light weight IDE for MSSQL. It's based on the very good/popular VSCode and you can just go download the [latest preview build](https://docs.microsoft.com/en-us/sql/sql-operations-studio/download?view=sql-server-2017) and get going. I've personally moved to SSOS for pretty much all my query writing.
All my SQL is prepared statements in C or perl programs... So yes.
Yeah, I like the look and feel and intellisense, snippets in SSOS better than SSMS + ApexSQL. There are a few things that miss, though- for example, it's much easier to split the current view (so you can, for instance, look at something at the top of the file while working on something further down) and comment/uncomment multiple lines in SSMS. Other little things bug me, but otherwise SSOS feels so light and snappy I put up with it.
Aye, there's lot of niggley stuff like that that I hope'll be smoothed out. I can't remember if there were even any bindings for un/commenting when I went to set them because even if there were they weren't the same as SSMS. Seems a bit odd to me but on the other hand it's highly customisable and there's already an extension that adds a lot of the more poplar SSMS keybindings so I'm looking forward to seeing where it goes.
Interesting thing about Notepad++. I have historically used it for years, but at my new job it kept being flagged by our virus scanner as having malware in it. I thought that was absurd, but months later I read that they were part of the NSA hack. Found it interesting that our corporate security caught something that my past job(s) had missed. Anyway, now I use Sublime Text which seems to work fairly well.
I like SSMS to a point...I just wish it didn't have the weird chord keyboard shortcuts. I'm not a VS user so it just all seems so alien to me. Honestly, once it gets released, I'll probably move over to SQL Operations Studio which is the best of both worlds for me. Text-editor-like for 95% of the stuff i do in SSMS, with that as a fallback when I need it. 
If I try to run this I get back errors, is this for a version other than MS SQL?
 First, you are doing row by agonizing row. You are making SQL scan the table for a row, order it, match it, and relay the data to the outer query. It does this process ~3600 times. Second, you have no indexes, so no matter what you have to scan the entire table. So let's try scanning the table a single time. SELECT P.ID ,PI.FIELD1 ,PI.FIELD2 FROM [person] [p] INNER JOIN ( SELECT ID ,FIELD1 ,FIELD2 ,ROW_NUMBER() OVER (PARTITION BY FIELD1,FIELD2 ORDER BY DATEUPDATED DESC) AS Rankz FROM [person_info] ) [PI] ON PI.ID = P.ID and PI.Rankz = 1 What is happening is that the subquery is returning a window of data from person_info. That window is framed by the Field1 and Field2 fields, sorted by the date. So say Field1 and Field2 have the values (1,1) twice in the table. It will rank one of those rows as 1 and the next row as 2, based on the DATEUPDATED column. So you are able to retrieve the ID, field information, and rank. You are taking that data set and then joining it on corresponding ID and then where the Rank = 1. Before the WHERE clause is hit, the records not ranked by 1 are already filtered. A similar result and probably a faster one, would be based on the same methodology but using GROUP AGGREGATION and not WINDOWS functions.
Right, all too often there is this assumption that your use case is my use case. I use Emacs, sqlcmd, mysql, and DBeaver. Personally I want and need to be able to script DDL and data population procedures in order to roll out changes in git version controlled scripts.
I think you need an outer join for your scan table, not an inner, and a where xxx Is NULL on the end before your order by
The only time I use a text editor is if I'm dealing with an MS Access query that is anything other than "short and simple". And that is only because the MS Access editor is so, so bad. For MS-SQL, Teradata, and Oracle I just use their editors. 
I think you've gotta fill in the (IN) statement with some values. 
It seems to me that it's because of a business rule. You are filtering out rows you want either by the JOIN conditions or the WHERE clause. Because you are not joining or filtering out itemtypename, this leads me to believe that the rules of the data / query force you to have a documenttype assigned to a scan queue. You can plausibly fix this by changing your joins to LEFT, I would start by that and change them one by one to see where you are filtering your records out. The other option is to remove the where clause and keep your query as is, and see if the values are being filtered out in the where clause. It may be that you need to convert and handle NULL or blank values into a default value of 0. 
Time, no. I use Refactor and Complete. I spent maybe 40mins setting up Refactor and then saved my settings off - never needed to do it again. The free stuff is all I need and it's pretty much plug and play.
Huh. Hadn't heard that - good to know. Thanks!
I'm intrigued about SOS, but haven't played much in it. I suspect it will be better than SSMS once the release bugs are squashed. Powershell is the one thing I use the native editor for over VS or VSCode... Dunno why really. But for now I haven't found much in SSMS I don't like, besides tiny little usability quibbles - like having to go to another window to see job history failure text, instead of a log being right on the job.
&gt; I'm trying to retrieve data from a table that has no indexes. What do you mean? It has no secondary indexes? Or it doesn't even have primary keys? Remember, constraints are indexes. 
&gt; `ROW_NUMBER() OVER (PARTITION BY FIELD1,FIELD2 ORDER BY DATEUPDATED DESC) AS Rankz` Pretty sure this needs to be `PARTITION BY ID, FIELD1, FIELD2 ...`. I'm also not guaranteed that this will be any better, though it's definitely worth a try. I've switched queries from windowing functions to a correlated APPLY and gotten better performance just as often as the other way around. The window function might just run faster because the query engine decides it's expensive so it parallels the whole thing. IMX, a window function will work better than correlated APPLY when the number of partitioned rows is low, but the correlated APPLY will work better than the window when the number of partitioned rows is high. So it *should* work better here, but it *may not*.
oh i just wrote it off of the top of my head. it's probably buggy the point was to communicate the idea "just walk the integer sequence, don't do all that schmancy stuff"
Sorry - it does have a PK, which is the ID field and a different date field... which is strange to me. Otherwise, no indexes. It is a home-grown table created by someone who knew even less than me, and I'm trying to use it as a learning opportunity for how indexes could improve it (as tested through the temp table).
I'm thinking it would be: ROW_NUMBER() OVER (PARTITION BY ID ORDER BY DATEUPDATED DESC) So the first earliest date would have a row number of 1, second earliest would be 2, etc... regardless of field1 and field2.
Thank you. That is very insightful. Using the window function in the derived table does run a lot faster in this case. I'm just confused, because I can use a cross apply to find the correlated top 1 row in *other* tables, similarly ordering by an un-indexed date field, and retrieve 10,000's of records in just a couple seconds. ID is the primary key in [person] and I've created an index on it in the temp [person_info] table. It is doing a ton more logical reads on the original table: SQL Server Execution Times: CPU time = 0 ms, elapsed time = 0 ms. SQL Server parse and compile time: CPU time = 0 ms, elapsed time = 2 ms. (3620 row(s) affected) Table '#temp_person_info_____________________________________________________________________________________________________000000024ABA'. Scan count 13099, logical reads 1034821, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. Table 'person'. Scan count 1, logical reads 454, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. (1 row(s) affected) SQL Server Execution Times: CPU time = 32750 ms, elapsed time = 32743 ms. compared to: SQL Server parse and compile time: CPU time = 0 ms, elapsed time = 0 ms. (12582 row(s) affected) Table 'person_info2'. Scan count 13099, logical reads 71479, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. Table 'person'. Scan count 1, logical reads 454, physical reads 0, read-ahead reads 0, lob logical reads 0, lob physical reads 0, lob read-ahead reads 0. 
Anything over 10 lines in vim or pycharm.
Yes, right. Dang, that's what I thought I said! 
Knowing that the ID is the primary key on the table is a good note. Your example illustrated FIELD1 and FIELD2 and stated no indexes, I assumed the table would be HEAP structure and the combination of FIELD1 and FIELD2 was the record to be partitioned by date, but it is actually B-Tree structure with ID as the record to be partitioned by date. The ID is perfect to use instead because it is the primary key. Using ID, FIELD1, and FIELD2 to partition is different from FIELD1 and FIELD2 and is different from ID. The only reason to know WHY it is slower than the other queries is to set those parameters I gave you and run it. You will want to see what kind of scans and reads are happening and see what the execution plan gives you. That's the only way to compare those apples and oranges. 
What was the execution time on the bottom query? Can you [share](https://www.brentozar.com/pastetheplan/) the execution plans from a table where the cross apply works well and where your current example it doesn't?
Thank you, the JOIN conditions were causing the problems. Time to learn more about joins. 
Look into Navicat. It's all I use. It has some great tools for viewing tables and data, but also text editor with all the features you'd want. Premium version is worth it, IMO. 
 SELECT [p].[ID] , [pi].[FIELD1] , [pi].[FIELD2] FROM [person] [p] INNER JOIN (SELECT [x].[ID] , CONVERT({FIELD1 datatype},STUFF(MAX(CONVERT(char(23),[DATEUPDATED],121)+CONVERT(varchar(100),[FIELD1])),1,23,'')) AS [FIELD1] , CONVERT({FIELD2 datatype},STUFF(MAX(CONVERT(char(23),[DATEUPDATED],121)+CONVERT(varchar(100),[FIELD2])),1,23,'')) AS [FIELD2] FROM [person_info] [x] GROUP BY [x].[ID]) [pi] ON [x].[ID] = [p].[ID]; This doesn't require a sort, so indexes are not required to perform reasonably well. What it does is turns FIELD1, FIELD2 into strings, prefixes them with DATEUPDATED, takes the MAX of that string (sorted alphabetically by DATEUPDATED), strips off the prefix (STUFF), and finally converts it back into the correct datatype.
Sure. [Poorly performing execution plan](https://www.brentozar.com/pastetheplan/?id=BJSCVrngm). If it's not clear there, the query is: IF Object_id('tempdb..#temp_insurance') IS NOT NULL DROP TABLE #temp_insurance SELECT 'P' + [PEOPLE_ID] AS [PEOPLE_CODE_ID] ,[INSURANCECOMPANY] ,[POLICYNUMBER] ,[DATEUPDATED] INTO #temp_insurance FROM [MMA_StudentInsurance] [msi] CREATE CLUSTERED INDEX ix_temp_insurance_id -- experimenting with indexes... ON #temp_insurance ([PEOPLE_CODE_ID]); CREATE NONCLUSTERED INDEX ix_temp_insurance_date ON #temp_insurance ([PEOPLE_CODE_ID], [INSURANCECOMPANY], [POLICYNUMBER], [DATEUPDATED]); SELECT [p].[PEOPLE_ID] ,[p].[INSURANCE_COMPANY] ,[p].[INSURANCE_POLICY] ,[msi].[INSURANCECOMPANY] ,[msi].[POLICYNUMBER] FROM [STUDENT] [p] CROSS APPLY (SELECT TOP 1 [INSURANCECOMPANY] ,[POLICYNUMBER] FROM #temp_insurance [x] WHERE [x].[PEOPLE_CODE_ID] = [p].[PEOPLE_CODE_ID] ORDER BY [DATEUPDATED] DESC) [msi] [Better performing execution plan](https://www.brentozar.com/pastetheplan/?id=HykzIrne7). In my examples here, I had changed the names of the tables, but the structure is the same.
The issue there is the poor performing query is scanning the entire table(the clustered index is essentially the table) and the good performing query is seeking for specific values from the index. The reason for the scan is that the datatypes do not match up. In order to match the datatypes, it converts the datatypes on the table side, rendering any index assistance useless. You want to convert the data you are comparing vs the table you are comparing. The value PEOPLE_CODE_ID from #temp_insurance is not equal to PEOPLE_CODE_ID from STUDENT. Find out what that column type is and instead of doing a SELECT INTO, try creating the temp table as you would any other table in T-SQL and make sure to declare the column types. Once the types match between tables, you'll have a more performant query. (Looks like comparing NVARCHAR to VARCHAR) This is called Search Argument Able or SARGABLE or SARGABILITY. It means that we can cleanly use the index to compare the values we are given. If we cannot, that means the query where clause predicate is not Sargable. 
I really like the creativity of this approach. A little lengthy but it fits the bill perfectly. 
Wow, that definitely made a difference. SELECT INTO was creating a VARCHAR(10) field. When I explicitly created it as NVARCHAR(10) (as it is on STUDENT), it runs instantaneously. Thank you for taking the time to explain what was going on!
That will definitely do it! When I look at performance troubleshooting, this [PDF](http://web.swcdn.net/creative/infographics/1403_Confio_SQL_Server_Tuning_Infographics_8_5x11.pdf) is a nice cheat sheet. To anyone intermediate / senior, yes it is lacking obvious things and is very simplistic. To anyone learning and getting their feet up to speed on tuning however, I think it's a great resource.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://web.swcdn.net/creative/infographics/1403_Confio_SQL_Server_Tuning_Infographics_8_5x11.pdf) - Previous text "PDF" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20e0i00r2) 
The one reason I usually prefer NotePad++ is its matched variable/text highlighting...like how you can double click on an alias or column name in a query and see that text highlighted throughout the entire query. Is there any way to get this in SSMS?
Notepad++ to tidy stuff up. Am a newbie at SQL and colleague recommended. Also Prettifier is good although works on keywords so doesn't always put breaks/commas where expected, but it trains you to look and identify stuff
I’m sorry as I said I am learning the language. I am running MySQL workbench on my MacBook. 
I had this issue when I was working a contract at Microsoft. They dissalowed NP++ but then later allowed it due to a patch update. You should check if updating to the latest version removes the security blocker. Else it could be that that your security team just blocked the app entirely, regardless of version. 
This was before the NSA hack was revealed and a patch was offered, it may now be allowed but I picked up a license for Sublime and I like it. I don't use it very often since I just work in SSMS and have a license for SSMS Tool Pack, but it can be helpful when I need to make large changes to a big piece of code, etc.
See if this [stack overflow post by derobert](https://stackoverflow.com/questions/879176/how-to-recover-mysql-database-from-myd-myi-frm-files) helps at all.
Thank you very much! It took me a while to figure it out, but the solution by Druzion ended up working perfectly! :D You're a lifesaver.
You didn't store your dates using one of your RDBMS's date or datetime data types?
DATEDIFF requires two date expressions, so you have to convert `created` which is in microseconds or whatever this returns 4, which is the correct result for today -- SET @created = 1528415807559000 / 1000000 ; SELECT DATEDIFF(NOW(),FROM_UNIXTIME(@created)) 
i use notepad++ as a... notepad. litterally have hundred of tabs open in it with code and other random stuff, best thing is i can search through it all.
Yeah, that is a really great feature. Off the top of my head, to do something similar would be to Ctrl + F and then that might highlight all occurrences of the search term?
It looks like SPLUNK to me.
SSMS and RedGate is all I use. I think its great! RedGate version control plug-in for SSMS is the best.
Good point. Not quite as convenient, but probably “close enough.”
Codecademy has a great free course that'd be a nice refresher if you have a little experience
I've learnt a lot from Lynda.com and various online site. below are the 4 sites I refer the most https://www.w3schools.com/sql/default.asp https://www.1keydata.com/sql/sql.html http://sqlines.com/home https://docs.microsoft.com/en-us/sql/t-sql/functions/string-functions-transact-sql?view=sql-server-2017 
Hey OP, it may depend on what specifically you're working with, ie, MySWL vs PostgreSQL, but I recently purchased this to make my way through: https://www.udemy.com/the-ultimate-mysql-bootcamp-go-from-sql-beginner-to-expert/learn/v4/ This guy's bootcamps speak for themselves, he is wildly popular in the Web Dev community. I'm about 60% through his web boot camp, and completed his Python one as well. Great instructor.
If you have indexes on those tables. Remove the indexes before the import then recreate the indexes after the import.
So how far can you go with this course? After finishing this course, do you think you will be able to make something meaningful by yourself with SQL? Do you think you will be able to solve some real work problems?
You can practice SQL with the following resource. There you can submit exercises get feedback online. Everything for free. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
What would be the advantage of doing this instead of ROW_NUMBER() OVER(PARTITION BY [x].[ID] ORDER BY [DATEUPDATED] DESC)?
From my experience CROSS/OUTER APPLY (SELECT TOP 1 ... ORDER BY) can have very poor performance. I no longer use it for that reason. Unless you have some insanely complex correlated subquery that would otherwise require a costly CROSS JOIN, you should be able to rewrite it.
The formula for nth Fibonacci number requires real numbers. In other words, it either needs arbitrary large precision. SQL does not have real numbers (well, it does have a data type called this, but that's not what I mean). This means at some point the formula will break down due to lack of precision. In other words, if we want to ensure that the answer is correct we need to prove that we have enough precision for it. I don't know how to do this calculation, so I decided not to use the formula for my answer. Comparing the two methods, yours breaks down on 71st and subsequent Fibonacci numbers.
Looks like both.
The batch separator is needed after the drop table if the schema of the temp table changes. If all the column types stay the same it's not needed.
IMHO codecademy is with the $20 for their courses and curation of knowledge. 
Ranking functions, like ROW_NUMBER, require that data be ordered. So either it gets data from an ideal index, or it has to sort. MAX can more efficiently consume unsorted data with a hash aggregate. Also, MAX can achieve optimal efficiency with a less restrictive index. Compare the two indexes: `CREATE INDEX IX_Max ON person_info (ID) INCLUDE (DateUpdated, Field1, Field2)` `CREATE INDEX IX_RowNumber ON person_info (ID, DateUpdated DESC) INCLUDE (Field1, Field2)` Finally, a GROUP BY has the advantage of guaranteeing uniqueness on ID. This affects the row estimates in the execution plan. The MAX join will be guaranteed to be one-one. Even though ROW_NUMBER will produce unique results, the database engine can't guarantee that resulting in a one-many join. That's not so bad, but in a different scenario the difference between a many-one and many-many join estimate can be substantial.
Thanks I'm picking this up today
&gt;This doesn't require a sort... &gt; ... (sorted alphabetically by DATEUPDATED) I don't understand the distinction of taking MAX sorted by DATEUPDATED as opposed to taking TOP 1 sorted by DATEUPDATED.
"even if" is a misnomer - sql optimizers have terrible time with recursive CTEs (at least every time I've seen these in production) so they should be used only if everything else fails. Anywho, going with the "for education purposes only", the "idea" for recursive CTEs is that you have: 1) a table-valued function (query) that takes a table parameter ("table" in the standards sense, i.e. any result set) - G(Sx) -&gt; Sy that somehow "reduces" the input result set in some sense. 2) you have an initial result set - S0 3) on every step you apply the function G to the result of the prior step, until you get to an empty set: Sn = G(S&lt;n-1&gt;) or Sn = G(G(G(G(....G(S0)...)))) (n times) 4) Your actual result is the union ("all") of all the steps from S0 to the last one (empty set) R = S0 + S1 + .... + SN So, your initial description/thinking is similar - you just need to switch to set-based (it's a table/result set on every step) and you need to come up with a function( aka query) to give you the next "step" in your "Walk the record set from the bottom to the top" process. What your current query is doing is skipping "steps" gets more records that you need for the final result on the first step and then adds even more records that you dont need on following recursion steps. Hopefully this helps.
I don't know much about query optimization but I do know the distinct keyword is probably a big part of the issue, along with having the NOT IN subquery. It can be rewritten using joins rather than the subquery and as for the distinct keyword, I wouldn't know how to get around that without knowing what the data looks like. 
Free courses through Edx, but they assume some basis in SQL and relational databases. https://www.edx.org/course?search_query=sql
I will edit the post to show a little of the data. I added another temporary table from the "select distinct" query. I'm also trying a left outer join. So now this is what I have. create temporary table files (file\_name text); load data infile '/var/lib/mysql\-files/file\_list.txt' into table files; create temporary table files2 (select distinct files.file\_name from files); select f.\* from files2 f left join content c on f.file\_name=c.file\_name where c.file\_name is null; So far it seems to choke just like the other query. I'm going on about 10 minutes of waiting for results.
You're running the statements individually and you're 100% sure it's not choking on the load itself, right? It looks like you are, but I wasn't sure from your post. In the past when I was doing work that took several hours via mysql, I would connect to the server hosting the database and have it run a script locally that dropped my results into a file for me to come back to later. That approach may not be low effort, depending on your setup.
I would say undoubtedly you need to index file_name in both tables: [temp table index](https://stackoverflow.com/questions/14397785/create-a-temporary-table-in-mysql-with-an-index-from-a-select)
There are a lot of approaches that can probably work but might require some understanding of query plans, etc. Will this work as a hack if you have some basic scripting skills? 1) Select all files that are in `file_list.txt` and in the database, simply by joining the table and the temp table. 2) Write a script that takes those results, loops through the text file and removes the file if it's a match. This should leave you with a text file that only has the missing files in it. It still leaves you with what's essentially a double foreach loop, but if you can keep both the results and `file_list.txt` in memory, it might be fast enough for you.
Don't know if this applies to MySQL but NOT EXISTS maybe better than NOT IN here. At worst they are equivalent and you're problem will be elsewhere but at best NOT EXISTS can be significantly faster than NOT IN when dealing with null-able columns. I can't remember exactly why that is off the top of my head but I think it has to do with how the nulls can screw with logic NOT IN uses so SQL will use a more complex plan to check for nulls when nullable columns are used. Even if there are no nulls in the set the planner can't know that ahead of time. Something like should work in MSSQL SELECT content.file_name FROM content WHERE NOT EXISTS ( SELECT null FROM files WHERE content.file_name = files.file_name ); 
In simplest terms, GROUP BY MAX has less restrictive options in how it executes than CROSS APPLY TOP 1 ORDER BY. Specifically, it can use a hash aggregate which does not require a sort. Also, CROSS APPLY tends to use NESTED LOOPS which may not be best in this case. Assume that person has 1000 records and person_info has 10,000. No indexes. In this case I suspect "CROSS APPLY TOP 1 ORDER BY" is doing this: TABLE SCAN &gt; person_info &gt; SORT &gt; INDEX SPOOL (ID, DATEUPDATED) &gt; tempdb TABLE SCAN &gt; person &gt; NESTED LOOPS &gt; INDEX SEEK x1000 &gt; tempdb (ID) &gt; TOP 1 x1000 And "GROUP BY MAX" is doing this: TABLE SCAN &gt; person TABLE SCAN &gt; person_info &gt; HASH AGGREGATE (ID) &gt; HASH JOIN &gt; person 
Another vote here for not exists. It allows you to fine tune the select for what you you are hoping doesn't exist in the subquery. 
&gt;What I'd love to explore is having a bunch of game and player data and stats in a queryable format so when the typical "The Red Sox are the better team" quotes start flying I can whip out some data to prove them wrong. Many people would love this data as well, as it drives statistical analysis and ultimately sports betting. &gt; I was wondering if anyone knows of a good place to download data from the current season? It's been awhile since I've done any sports stats analysis, but you could try [Sportradar](https://sportradar.us/sports-data/) if you're willing to shell out some money, depending on what you're wanting. Unfortunately, hundreds of companies and thousands of people are after good, correct, current sports statistics \- which means you're either going to have to pay for the scrubbed, queriable data or you're going to have to get creative and start making some web\-scrapers to scrape statistics off of sites like Fox Sports or ESPN. 
Did that work?
This is definitely possible to do with a SQL query. What database are you using?
Not current season, but you can get this into a db real quick. http://www.seanlahman.com/baseball-archive/statistics/ And here for other references. https://github.com/baseballhackday/data-and-resources/wiki/Resources-and-ideas 
Damn! That's what I was worried about, this is just for fun so I'd rather not pay for anything, guess I'm stuck playing with historical stuff for now. Thanks for your reply!
Split the cases into multiple records, one for each month it was open. This can be done by joining to a numbers table. Then add the number to the month of the OpenDate and aggregate. WITH cteN13 AS (SELECT v.N FROM (VALUES(0),(1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12)) AS v(N)) SELECT DATEADD(month,DATEDIFF(month,'2000-01-01',c.StartDate) + n.N,'2000-01-01') AS [Month] , COUNT(*) AS OpenCases FROM Cases AS c INNER JOIN cteN13 AS n ON n.N BETWEEN 0 AND DATEDIFF(month,c.OpenDate,c.CloseDate) GROUP BY DATEADD(month,DATEDIFF(month,'2000-01-01',c.StartDate) + n.N,'2000-01-01');
I tried that but I got zero rows as the result. I'm not sure what I have wrong. It is faster though. :\-D
I may have gotten the syntax wrong or it might not be the same for MySQL so I'd suggest you look it up in concept for MySQL and check I've not got you doing something silly!
Forgot to mention the data comes in from a database table, I have no control over the setup.
regret i have only one upvote to give for **numbers table**
You might not need to pay. I remember looking into college football data last year as I was wanting to know whether 'icing the kicker" was really an effective tactic. I didn't have to pay at all. I'll check when I get home later to see if I can find where I got it from. 
https://dingerdb.com -- this was created by a couple friends of mine. They recently opened it up to the public. They pull game by game data from MLB everyday. 
Thanks for explaining. I will have to test it out on some large data to see how well it works. Usually I trust the optimizer to do its magic and don't try to outsmart it. Since the ROW_NUMBER pattern is so common perhaps the optimizer can figure out that we're doing the same thing and avoid a sort. After all we're just getting the max element in both cases, which is theoretically a lenear operation (if there is no index) unlike a sort which is n log n. On the other hand that's a lot of text operations which might be larger than the log n factor. If it's better I might have some ETL operations that could benefit. Just have to be careful that the string representation of the data type sorts as expected -- padding INTs with 0s and so on.
Let SQL Server do that hard work: CONVERT(DATETIME,@Input,103)
I'm don't have much experience with optimizing for MySQL but "where not in (list)" is a bad pattern in a lot of dbms. You have to do (number of rows returned by "from") X (number of rows in "list") comparisons to evaluate it. Looking at it this way, you can see why it this might be ok for small sets, but gets out of hand pretty quickly as the sets grow. First thing I'd do is switch to subquery and join: select content.file_name from content left join ( select distinct file_name from files ) f on f.file_name = content.file_name where f.file_name is null So we're taking content, comparing to what you've got in "files" and then discarding any rows where you found a match (f.file_name is null means the left join missed, i.e., the file_name is in content, but not files.) Writing as a join lest the optimizer use a hash join or indexes (if present) instead of a costly nested-loop-style operation. If you need even more speed, you can put "select distinct file_name from files" in a temp table, and index both it and content. 
This is awesome this is exactly what I was looking for! Thank you!
CSV Kit is kinda nice. Not super fast, but useful. It's a command line tool https://csvkit.readthedocs.io/en/1.0.3/scripts/csvsql.html
Please tell us what database you are using, as it could have an impact on the SQL syntax. But you will probably be interested in using the UNPIVOT operator.
Hero! Thank you! 
 SELECT * FROM Table AS t UNPIVOT (MAX(Value) FOR Key IN (Company,Department,Account,FiscalYear,JanuaryAmt,FebruaryAmt,Etc)) AS u; or SELECT kvp.* FROM Table AS t CROSS APPLY (SELECT * FROM (VALUES('Company',t.Company) , ('Department',t.Department) , ('Account',t.Account) , ('FiscalYear',t.FiscalYear) , ('JanuaryAmt',t.JanuaryAmt) , ('FebruaryAmt',t.FebruaryAmt) , ('Etc',t.Etc)) AS v(Key,Value)) AS kvp;
Allow me to plug the book *SQL Cookbook*, by Anthony Molinaro. It changed my life.
Better yet, if you're on SQL Server 2012+: TRY_CONVERT(DATETIME,@Input,103) TRY_PARSE(@Input AS DATETIME USING 'en-GB') The `TRY_XXXX()` functions will return NULL instead of an error when the data can't be converted. Beware, of course, that there are some possibly unintended effects, especially when converting things that look like integers to datetimes. 
Thanks for the suggestion! I've read through all this and it seems way over my head. I was hoping to find a simple tool for uploading an already formatted csv to an existing table.
It would help to know which flavor of DB you're using in order to know what features are available. I would probably do something like this... DECLARE @Months TABLE (MonthOfYear int) insert into @Months values (1),(2),(3),(4),(5),(6),(7),(8),(9),(10),(11),(12) select df.Company, df.Department, df.Account, df.[Fiscal Year], m.MonthOfYear, case when m.MonthOfYear = 1 then df.[January Amt] when m.MonthOfYear = 2 then df.[February Amt] when m.MonthOfYear = 3 then df.[March Amt] when m.MonthOfYear = 4 then df.[April Amt] when m.MonthOfYear = 5 then df.[May Amt] when m.MonthOfYear = 6 then df.[June Amt] when m.MonthOfYear = 7 then df.[July Amt] when m.MonthOfYear = 8 then df.[August Amt] when m.MonthOfYear = 9 then df.[September Amt] when m.MonthOfYear = 10 then df.[October Amt] when m.MonthOfYear = 11 then df.[November Amt] when m.MonthOfYear = 12 then df.[December Amt] else null end Amount from DepartmentFinancials df cross join @Months m You can replace the numbers with strings if you need to, but the numbers allow for better sorting.
Useful to know for date and time formats that aren't supported by CONVERT!
The data is also available from github at [chadwickbureau](https://github.com/chadwickbureau/baseballdatabank)
Interesting... how does Azure handle the BULK INSERT functionality?
SELECT COUNT(DISTINCT m.visitor\_id) AS ExisingVisitors FROM main\_table AS m LEFT JOIN returnvisitor AS e ON e.visitor\_id = m.visitor\_id LEFT JOIN newvisitor AS r ON r.visitor\_id = m.visitor\_id WHERE r.visitor\_id IS NULL AND e.visitor\_id IS NULL SELECT COUNT(DISTINCT visitor\_id) AS ReturnVisitors FROM returnvisitor SELECT COUNT(DISTINCT visitor\_id) AS NewVisitors FROM newvisitor SELECT COUNT(DISTINCT visitor\_id) AS MainTableTotal FROM main\_table Give this a try, if the vistor\_id is not unique then you have to filter out only one instance per visitor\_id
[Baseball Reference](https://www.baseball-reference.com/) exists, in case you didn't know about it. 
I rarely see a need for doing dynamic pivtot tables, but if he can't control the data and the data is going to keep coming with a new column every month it would seem very appropriate for this situation. I wrote an example a long time ago that OP could use.
I don't think I'd use SQL for this. One file of all filenames, one of tracked file names, use [comm](https://linux.die.net/man/1/comm) to figure out what's in the first but not the second.
You need to generate dynamic SQL, assigned to a variable, to do something like this. Couple other things: * Don't use table variables, the cardinality processor always assumes one row regardless * You're not specifying the TOP or DELETE ordering, meaning the row you SELECT and DELETE could be different * SQL Server has [UNPIVOT](https://www.mssqltips.com/sqlservertip/3000/use-sql-servers-unpivot-operator-to-help-normalize-output/) * This is a terrible solution all round, you should get out of the habit of using loops with SQL when at all possible. 
Thanks! This is where I found retrosheet.org, it seems they pull their data from there, just not sure where they get their current season data.
It would be awesome if we could connect directly to these sports reference sites db’s through ODBC. Of course, someone’s gotta pay for it! I would guess the best bet would be to create a scrape that logs the data in a personal database. 
&gt; Don't use table variables, the cardinality processor always assumes one row regardless @Records is an input to the stored proc. How else am I going to get a list of id's to use in the stored proc? &gt;SQL Server has UNPIVOT From what I have read, CROSS APPLY can be faster than UNPIVOT, which is why I was trying to use it. &gt;You're not specifying the TOP or DELETE ordering, meaning the row you SELECT and DELETE could be different Thanks for noting that, I will change it &gt;Also using SELECT * I suppose so. I was just coping the structure from an example.
This might get you started if you want to work out a dynamic solution. I am too busy today to get you any further over the finish line, but someone else might be able to chime in: DECLARE @DynamicPivotQuery NVARCHAR(MAX) DECLARE @ColumnName NVARCHAR(MAX) SELECT @ColumnName = ISNULL(@ColumnName + ',','') + QUOTENAME([IssueState]) FROM ( SELECT * FROM sys.columns WHERE object_id = OBJECT_ID('dbo.yourTableName') AND [name] LIKE '%AMT%' ) A ORDER BY [column_id] SET @DynamicPivotQuery = ' SELECT ' + @ColumnName + ' FROM ( ) A PIVOT(SUM([Col]) FOR [Col2] IN (' + @ColumnName + ')) AS Z ' EXEC sp_executesql @DynamicPivotQuery
https://www.fangraphs.com/tht/a-short-ish-introduction-to-using-r-for-baseball-research/ There is an r module called Lahman that is easy to use.
&gt; @Records is an input table\-valued parameter. How else would I go about passing a list of id's to use in the stored proc? Might be out of luck unless you're using global temp tables, which I also don't recommend. &gt; From what I have read, CROSS APPLY can be faster than UNPIVOT, which is why I was trying to use it. Doing a loop with a CROSS APPLY is going to be orders of magnitude slower than UNPIVOT in almost every scenario. Getting the results right and the query legible supersedes runtime during development; worry about speed after you have something that works. &gt; Which I have yet to see somebody offer. UNPIVOT is the better solution. Alternatively, and not recommended by me, you can use the sys.tables and sys.columns to dynamically generate a UNION SELECT statement for the list and then execute it. 
Ok, I'm writing this blind so I might have a bug or two to be worked out here, but it sounds like you'd want something like: DECLARE @sch VARCHAR(MAX) , @tbl VARCHAR(MAX) , @cols VARCHAR(MAX) , @castcols VARCHAR(MAX) , @sql VARCHAR(MAX) CREATE TABLE #tmp ( id INT , DataSchema VARCHAR(MAX) , DataTable VARCHAR(MAX) , ColName VARCHAR(MAX) , ColValue VARCHAR(MAX) ) DECLARE tblCrs CURSOR LOCAL FOR SELECT DISTINCT DataSchema, DataTable FROM #Columns OPEN tblCrs FETCH NEXT FROM tblCrs INTO @sch, @tbl WHILE @@FETCH_STATUS = 0 BEGIN SET @cols = (SELECT STRING_AGG(ColName,',') FROM #Columns WHERE DataSchema = @sch AND DataTable = @tbl AND ColName &lt;&gt; 'id') SET @castcols = (SELECT STRING_AGG(ColName + ' = CAST(' + ColName + ' AS VARCHAR(MAX))',',') FROM #Columns WHERE DataSchema = @sch AND DataTable = @tbl AND ColName &lt;&gt; 'id') SET @sql = 'SELECT id, DataSchema = ''' + @sch + ''', DataTable = ''' + @tbl + ''', colName, colValue ' + 'FROM (SELECT id, ' + @castcols + ' FROM ' + @sch + '.' + @tbl + ') t ' + ' UNPIVOT (colValue FOR colName IN (' + @cols + ')) u' INSERT INTO #tmp EXEC(@sql) FETCH NEXT FROM tblCrs INTO @sch, @tbl END CLOSE tblCrs DEALLOCATE tblCrs SELECT t.id, t.ColName, t.colValue, c.ColType, t.DataTable, t.DataSchema FROM #tmp t INNER JOIN #Columns c ON c.DataSchema = t.DataSchema AND c.DataTable = t.DataTable AND c.ColName = t.ColName I'm assuming this is Sql Server. If you're on an older version, look up using the FOR XML method of creating a comma-delimited list. Basically, we're looping through the tables, building a string for the column names and a string for casting the columns as VARCHARs (so that they'll all be the same type) and using them to build an unpivot sql string and then executing it into a temp table.
Thank you everyone for the solutions, I will try them out this week!
Thanks so much. 
Just realised I so much to learn. Though the old google didn’t help much 
I've imported raw xml data directly into SQL tables by using the openrowset (or opendatasource) function. Look at this link below - it goes through each option and you can pick the best method for your task. This was such a game changer for me in my product development. You can create a "linked server" connection to the file and then all you have to do is query the "linked server" to translate the data. For your case, it looks like you can use the bulk insert or openrowset options for CSV. https://docs.microsoft.com/en-us/sql/relational-databases/import-export/import-data-from-excel-to-sql?view=sql-server-2017 
UNPIVOT is a specific kind of CROSS APPLY, so they're equivalent (though obviously not how OP was doing it). Furthermore, if you are going to unpivot 2 sets of columns (JanSales, JanCalls, FebSales, FebCalls), 100% use CROSS APPLY. It's much better, because it can do everything at once.
`DECLARE @test TABLE (id INT);` `INSERT INTO @test VALUES (1);` `INSERT INTO @test VALUES (2);` `DECLARE @sql VARCHAR(MAX) = '';` `SELECT @sql = @sql + 'SELECT ' + CAST(`[`y.id`](https://y.id) `AS VARCHAR(MAX)) + ' AS KeyId, ''' +` [`c.name`](https://c.name) `+ ''' AS colName, ' +` [`c.name`](https://c.name) `+ ',''' +` [`t.name`](https://t.name) `+ ''' AS tabName FROM ' +` [`t.name`](https://t.name) `+ ' WHERE id=' + CAST(`[`y.id`](https://y.id) `AS VARCHAR(MAX)) + ' UNION ALL '` `FROM sys.columns AS c` `INNER JOIN sys.tables AS t ON c.object_id = t.object_id` `CROSS APPLY @test y` `-- Filter specific table list` `--WHERE` [`t.name`](https://t.name) `IN (SELECT tableName FROM #tablelist)` `SELECT LEFT(@sql, LEN(@sql)-LEN('UNION ALL'))`
Yeah, I was trying to avoid the big UNION ALL to be faster. Cursors aren't universally bad, they just get misused a lot.
I was talking about general legibility. Unraveling multiple CROSS APPLY can be exceedingly tedious for those not familiar with them, where UNPIVOT is comparatively easy. From a performance perspective, you're 100&amp;#37; correct. That said, I always work on getting a solution correct over fast, doubly so when working on an unknown or less well defined problem. Finding edge cases in a potentially nested cross applies is sketchy territory. 
 Name, employeeID, dateCreated Fred, 201, 2018-02-01 18:05:35.000 John, 115, 2018-02-01 19:00:18.000 John, 115, 2018-02-01 19:00:19.000 Fred, 126, 2018-02-01 19:05:19.000
Sweet! I don't know much about R but this might be my opportunity to learn, thanks!
You have not mentioned your RDBMS, so here's one MS SQL Server answer: SELECT a.* FROM Employee a JOIN Employee b ON a.employeeID = b.employeeID WHERE DATEDIFF(ms, a.dateCreated, b.dateCreated) &lt;= 1000 AND a.dateCreated &lt;= b.dateCreated 
I think that only works on the server directly? That's the idea I got from looking it up earlier. Thanks for the suggestion though!
Thanks. I am on SQL Server
Sure? https://docs.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-2017 Also, this post from a long time ago: https://stackoverflow.com/questions/6352660/bulk-load-files-into-sql-azure Explains how you can insert into a local instance and the. Bulk copy into Azure. Maybe that would work?
That gave me an overflow error *The datediff function resulted in an overflow. The number of dateparts separating two date/time instances is too large.* But this gives me a good idea where to go. Thanks
Hey \- I have a general baseball theory that revolves around collecting a specific statistic that I've not been able to find anywhere. Wondering if you might know something I don't about where to find it. The best stats site I know of is [fangraphs.com](https://fangraphs.com). I've still not looking for what I'm interested in: foul balls. I would like to know how many pitches a batter has swung at, and out of that total what number or percentage were balls in fair play, how many were fouled off, and how many were swings and misses. I'd like to see if there's a correlation that proves that better hitters might foul off more balls because they're able to get a piece of balls that less skilled hitters whiff on. I'm curious if that data is collected and where to find it.
Assuming from your previous posts you are still using Azure SQL. Not sure of the import capabilities in sql operations studio but you can import the data into a staging table with the date column defined as varchar(20) and then use the convert function SELECT CONVERT(datetime, 'Jun 01, 2018 06:52AM', 100); to perform the insert into the final table with the column defined as datetime. . 
Use a 3rd table. 
seller - seller_id, seller stuff.... inventory - inventory_id, inventory stuff... seller_inventory, s_i_id, seller_Id, inventory_id 
Is this a 1:1 relationship? If so, consider combining the tables. They could also share the same PK (seller_id), and insert seller first. Other options might be make one of the FKs nullable. Or create a dummy record (inventory_id -1) that the FK can reference until updated later with the correct reference.
As /u/rseese said, the bets way to do this is to `CONVERT()` your data to an appropriate date or date time data type. Although [`try_convert()`](https://docs.microsoft.com/en-us/sql/t-sql/functions/try-convert-transact-sql?view=sql-server-2017) may yield better results for you. **Do not** store dates &amp; times as strings; SQL Server (and Azure SQL Database, and pretty much every other RDBMS) has proper binary types for [storing dates and times](https://docs.microsoft.com/en-us/sql/t-sql/data-types/date-and-time-types?view=sql-server-2017) which are more efficient, less error-prone, and will perform much better overall. Bonus points for storing UTC with an offset (`datetimeoffset`)instead of implicit assumptions about time zones, or 
Well, you use PostgreSQL, so you can have a Deferred Constraint - https://www.postgresql.org/docs/9.1/static/sql-set-constraints.html https://stackoverflow.com/questions/16323236/deferrable-check-constraint-in-postgresql
100% this. This third table is called a relationship table and simply has the two IDs of both tables within it. This also allows a seller to have 0 inventory items they currently sell. And also allows inventory items to have no current sellers. Giving you more flexibility
#Availability Id|Enabled|UserId|StartTime|EndTime :-:|:-:|:-:|:-:|:-:|:-: 1|1|55266|6/12/2018 15:00|6/12/2018 17:00 2|1|55266|6/13/2018 15:00|6/13/2018 17:00 3|1|63523|7/1/2018 9:00|7/1/2018 17:00 #Series Id|AvailabilityId :-:|:-: 1|1 1|2 No need to make it too complex. Let the user populate the Availability table from the UI by either creating an individual time range or a series. Creating a series adds a bunch of time ranges (maybe max six month or a year in the future). From the UI the user can select whether it's a daily, weekly, monthly, etc series. Have checkboxes for skipping weekdays, weekends or holidays. Then the user can manipulate the series as a whole or each time range individually. Or is six months or a year not long enough and you want the series to be indefinite?
Can you explain series more? Like how do I differentiate between a "every Monday" series and a "every second Monday + every Tuesday" series? Or are those multiple series? I'm having trouble visualizing how complex a single series is. And I would assume it needs to be indefinite, but it could just reassign itself every x months if the user hasn't adjusted anything? Thanks so much for your reply, it has made things a bit clearer I think.
Try DATEDIFF_BIG if you're on 2016 and up.
Let's say you have a UI where the user can create the series. The user sets the options they want. The user can create series as complex as your UI allows. Then when they click done your application materializes the series (creates a whole bunch of entries in the Availability table according to the rules the user specified) and also adds them all into the Series table under a single Series Id. Perhaps you don't even need a Series table and just have it all in one table: Id|SeriesId|Enabled|UserId|StartTime|EndTime :-:|:-:|:-:|:-:|:-:|:-:|:-: 1|1|1|55266|6/12/2018 15:00|6/12/2018 17:00 2|1|1|55266|6/13/2018 15:00|6/13/2018 17:00 3|1|2|63523|7/1/2018 9:00|7/1/2018 17:00 Grouping them into a Series would let the user manipulate them in bulk, such as deleting them or altering the time. Anyway, that's one (fairly simple) approach with pros and cons. The disadvantage is you can't make the series indefinite (at least not without making way too many entries). Perhaps you can even save the UI settings in the Series table: #Series Id|Settings :-:|:-: 1|&lt;Some XML or binary&gt; 2|&lt;Some XML or binary&gt; And then the user can right-click and edit the series to extend it or to regenerate deleted entries.
s_i_id is not necessary and counter-productive 
&gt; Any reason why its happening this way? the difference between NOT and not NOT 
simplest approach is to agree that availability equals no events planned 
I don't like using `between` for strings because a "range" of strings like that feels unnatural (also, wildcards only work for `LIKE`). `BETWEEN` is **inclusive** so if you only want `A` through `M`, you're going to get invalid results with this query anyway -`N` is being included. Also, as pointed out previously, you're using `NOT` to negate your check. Remove that and it'll work. In SQL Server, you can use a regex: `when city like '[A-M]%`. Other RDBMSs support simple regular expressions as well.
You can get it with what u/waitaminuteholdup linked to. https://dingerdb.com/ You'll have a bunch of joins to get what you want, but the data looks to be there. 
It's not exactly what you're looking for, but there is a concept of plate discipline. https://www.fangraphs.com/library/offense/plate-discipline/
why does the seller table have a FK for inventory_id? is the seller limited to only ever a single product?
technically true, but often for usability it's easier to just include a surrogate IDENTITY, even if it's not the PK... as far as usability goes, it can have a separate nonclustered unique index, which then allows deletions to refer to a single column instead of two. not necessary, but often found (out in the wild) for the ease of use that it adds.
Just to add on, learning regexp is a great way to go from beginner to intermediate 
This is poor design. Seller table(employee) should be owner of record for employee data with no reference to employee. Same for inventory. You would have a sales or sales details table that relates the two. 
If you want to see the differences in two large sets, then do this with a Set Operator: SELECT file_name FROM content MINUS SELECT file_name FROM files; As others have said, putting an index on each of the file_name columns on both tables may help. With a set operator, however, even without indexes it will probably still perform really well if it just full scans both tables once. We want the fastest throughput, which is why doing this all in one go is probably best. Some suggested to use NOT EXISTS. Whereas I agree that NOT EXISTS can outperform a NOT IN under many circumstances, doing it that way might run one by one (slow by slow) for each record in the driving table. You will be asking the database to perform one lookup against the other table for every single record in the driving table. It would be the fasted method for a single row (best response time for the first result), but not the fastest for overall throughput (to finish for all rows).
&gt; what are some of the most common tools you use alongside SQL? To be clear, are you using "SQL" interchangeably with "SQL Server"? Or are you talking about SQL as in the query language?
Oh thanks!
\^ The only reasonable answer right here.
That's funny that someone down-voted you when technically your advice is sound according to OP's funky design. If everything OP said were true, then technically combining tables would solve it. The real problem is that OP's design is bad. He never should have set up the FKs like he did. 
For a small number of individual look-ups, I agree with you and /u/IgneSapien that "not exists" would be fast. But when comparing two large data sets, a set operator (like MINUS) would be much faster.
Hi there! You might also want to ask this over in /r/MSSQL since this sub is more about SQL as a query language. But to be fair, from your link it looks like Microsoft is partly to blame for people using the term "SQL" to mean MS SQL Server. &gt; CIS Professional Certificate in SQL There it is right there! It's not actually a certificate in SQL, from the description it's about database administration in SQL Server.
Awesome. It sounds like you're already working some SQL as a data analyst! That's cool! Thanks for your email. I'm sending it over right now!
While it's not a bad thing to obtain other certificates, I want to point out that if the cert does not come from a respected vendor (such as Oracle or Microsoft), the cert will not actually hold much or any weight. Don't think about this certificate as something to get you the job or improve the job. Think of it as a goal to obtain that signifies the conclusion of a regimented study plan broken down into clear objectives and goals. If you can create a portfolio and outline the notes and how you studied with proper documentation and host that on say a github or blog, that will have infinitely more value than the cert. \- College and degrees will help you get past the HR doorway and into a job. It could help decide between close calls with candidates. \- Certs will help you get past the HR doorway and into a job, albeit less helpful than a degree. If you don't have experience though, this is the next best thing to include. It could also help decide between close calls with candidates. \- Experience is what will get you past HR, get you hired, and keep your job. Experience is king, degrees get you past HR, and certs are icing on the cake.
I tend to move a lot of data around before I analyze or use it. SSIS is a common tool I use. Pluralsight has a good course on into to SSIS. It is a little dated by showing 2008 R2, but I didn't find much trouble following along with my 2015 version.
Yes, each query is running fine. I edited my post to explain a couple tweaks I found to get the query time down significantly. Also, Terminus on my Mac seems to stay connected to the server for longer than SmarTTY on my PC does. I didn't time them with a stopwatch or anything so it might just seem that way. But not getting disconnected from the server before the query completes any more.
I keep getting a syntax error with minus. Maybe our version of MySQL is old. That would not surprise me in the least.
The results were probably zero because I goofed. Please see the edit on my post. I'll try to tweak your query to account for my goof and see what results I get.
Oops, unsupported in MySQL. You can simulate the MINUS capability like this: SELECT file_name FROM content LEFT JOIN files USING (file_name) WHERE files.file_name IS NULL;
Well, there is no way around SQL. The SQL database is a listing of the files in the file system. It tells the app, here are your files and here is some metadata about those files. Therefore, at least in theory, the list of files and the list of file records in the database should be an exact match. In the real world shit happens and database records get deleted but files don't. I'm trying to find those files.
Really? I Googled "mysql minus" and it the MySQL documentation comes up, showing minus in the manual. It just doesn't say in what version the command was added. Anyway, I'll try the query you suggested here.
I know, I did the exact same thing, leading me here: http://www.mysqltutorial.org/mysql-minus/ But it's misleading. If you continue to scroll down, it says "Unfortunately, MySQL does not support MINUS operator". So right after giving an example that looks like it should work, they are like, "just kidding! do it this other way."
&gt; Does the other SP do anything besides generate that list? It's going to be called from an external application, given a list of tables and a list of ids.
What if you want to unpivot y columns from x different tables?
Ha! That is dumb to the point of hilarity. "Hey here's an entire article we wrote on how to use a function... that we don't support. Fooled you!"
SQL Server 2008. I have no experience with Cursors, are they valid in ss2008?
Yep. They got me. I have more of a background in Oracle, so I quickly verified (or thought I verified) that MySQL supports the same syntax. The top of the web page made it seem like it was all good. So silly.
 SELECT ca.SchemaName , ca.TableName , ca.ColumnName , ca.ColumnValue FROM dbo.Table1 AS t1 INNER JOIN dbo.Table2 AS t2 ON t2.ID = t1.ID CROSS APPLY (SELECT * FROM (VALUES('dbo','Table1','Column1',CONVERT(varchar(100),t1.Column1)) , ('dbo','Table1','Column2',CONVERT(varchar(100),t1.Column2)) , ('dbo','Table2','Column1',CONVERT(varchar(100),t2.Column1)) , ('dbo','Table2','Column2',CONVERT(varchar(100),t2.Column2)) ) AS v(ShemaName,TableName,ColumnName,ColumnValue)) AS ca;
most relevant username in /r/sql history
most relevant username in /r/sql history
I would also suggest /r/SqlServer
for recurring events, you should use iCalendar RRules (and ExDates) Use a library (PostgreSQL has several) that run in a materialized view, and creates the real events for you
it's selecting all columns because you are saying select *, meaning select every column. Your where condition of 1=1 has nothing to do with any column selection. 1=1 is always true and it will never not be true, so your statement would return the same data whether or not you have the where clause of 1=1 or if you completely omitted the where clause of 1=1. I think you maybe confusing the 1 with the first column position, but in this case it is just an integer having nothing to do with any column. In the ORDER BY clause, you can say to "order by 1, 2", for example, and it will order by column position 1 ascending followed by column 2 ascending. In the select, you cannot "select 1" to select the first column, because you're literally going to select the number 1. Does that make sense? 
I found an answer on \[Stack Exchange by Twinkles\]([https://dba.stackexchange.com/a/54966/112253](https://dba.stackexchange.com/a/54966/112253)) that explains the 1=1 for you. To help you resolve the question, I think if you change the code to: SELECT \* FROM users WHERE \[column 1\] = 'data1' That may help. The WHERE 1=1 is essentially meaningless in your query at the moment, that is my understanding at least. I welcome discussion here on this topic as I have had mixed interactions using that theory of using the WHERE 1=1.
Yes, that sub is way bigger than the one I listed. Thanks.
&gt; I have had mixed interactions using that theory of using the WHERE 1=1 OT, sure, but do tell, please. 
Thanks but this will be a regular problem as I will need to import a CSV daily with the date formatted as above. So I'm trying to find a less cumbersome solution. I actually moved from sql ops studio to smss on Windows for it's importing tool. So basically my problem is figuring out how to define the incoming data type and the data type of the field in the table so that the tool can successfully convert and keep a date and time.
understanding `WHERE 1=1` can be facilitated by considering why the **leading comma convention** is so popular if you have this -- SELECT client_id , client_name , client_hatsize you can easily add additional columns, one per line if the commas are at the end, then you have to add something like this -- , client_age which is clumsier now consider adding optional filters to a WHERE clause suppose out of several columns shown in dropdowns in the user interface, the user chooses `state='NY'` as well as `size=12` in yout user front end, each dropdown value has to be inspected to see if it needs to be generated into the sql for the query with `WHERE 1=1` all you have to do is generate each line with an AND in front of it -- AND state='NY' AND size=10 however, without `WHERE 1=1`, your front end code has to duplicate the "is this the first condition i'm generating into the WHERE clause, if so i have to use `WHERE condition=value` but if i've already hit one, the rest have to be `AND condition=value` code bloat
Yes, you can use cursors in 2008. Since the STRING_AGG aggregate is not available in 2008, you'd use something like SET @cols = STUFF((SELECT ',' + ColName FROM #Columns WHERE DataSchema = @sch AND DataTable = @tbl AND ColName &lt;&gt; 'id' FOR XML PATH(''), TYPE).value('.','varchar(max)'), 1, 1, '') SET @castcols = STUFF((SELECT ',' + ColName + ' = CAST(' + ColName + ' AS VARCHAR(MAX))' FROM #Columns WHERE DataSchema = @sch AND DataTable = @tbl AND ColName &lt;&gt; 'id' FOR XML PATH(''), TYPE).value('.','varchar(max)'), 1, 1, '') The XML PATH business will give you a comma-delimited list, but with a leading comma- the STUFF([query],1,1,'') part is there to replace that leading comma with an empty string.
It's been a long time since I've used that method, but the issue I had faced was I did not receive back the records I was expecting. When I removed that piece, the rest of my where clause functioned as I expected. I am sure it's a fundamental misunderstanding on my part on how to use this or I may have had something else going on. It's just not something I have cared enough about to focus more time and energy on vs other things.
I apologize, I was unclear. I meant SQL in the query language sense. I am still learning about the correct way to refer to things. I realize this entire post is pretty vague, so it might help to provide more context for how I use SQL. I'd identify as an inexperienced but hopefully improving data user rather than someone who is responsible for any kind of data administration. An example of a common workflow would be to create a query in SQL mgmt studio (SSMS?), and copy the results into an excel spreadsheet to manually iterate through in order to complete tasks in our proprietary software which has fairly limited capability to connect to external data. I was hoping to hear some more about the contexts in which other folks on here use SQL, though I realize now this may be too broad of a topic.
I think you are very confused. HR can care less about degrees these days and prioritizes experience and certification over all else.
I too am using MySQL, but would you mind PMing them to me as well? Thanks!
Thanks for the response and the suggestion. SSIS refers to SQL Server integration services? Could you elaborate on any specific pieces or ways you use SSIS? I had worked through what I think was the data import/export wizard in sssms but not sure how to take a saved "data extraction" (maybe not the correct term) and reuse it without rerunning the wizard. Thanks again. As in sure you can tell I need all help I can get and while I know there's no substitution for reading documentation, I find it helpful to talk to folks with experience and learn from them as well.
I'm guessing this is a cultural or location based subject then. There are many companies I have seen that will throw your application away if there's no degree listed. This is not always the rule, and it can definitely vary by company. The same can be said with experience and certs. I have never been denied an interview or rejected from an application based on certificates however. I have been rejected based on experience or lacking a degree or sometimes both. Obviously this is all anecdotal and personal experience, I don't have any data sets to interpolate this kind of assumption at a grandiose level across states / nations. 
Yup, just read about that too, after asking the question.
Obviously and industries were a degree is the standard such as information technology, unless you have a significant amount of experience, then of course I degrees required. But it's a gatekeeper requirement. Or a checklist requirement. Must have the degree. Doesn't matter what the degrees in. Certificates however matter a lot because it's extremely difficult to get a certificate if you don't actually know what you're talking about or how to do something. Good luck getting a CCNA if you have no knowledge of networking whatsoever. Certificates matter a lot because they test your actual understanding wear as a degree is just an aggregate
&gt; Obviously and industries were a degree is the standard such as information technology, unless you have a significant amount of experience, then of course I degrees required. But it's a gatekeeper requirement. Or a checklist requirement. Must have the degree. Doesn't matter what the degrees in. Yup! That's exactly it and what I have seen. I don't have a degree, but I am at 8 years of experience and continuing certifications + education. Most places will take my experience in lieu of a degree, but there are 10%-20% of jobs that will deny me because of the lack of degree or experience or both. (My local city government is a good example, my application goes straight into their trash file if there is no proof of a degree.) &gt; Certificates however matter a lot because it's extremely difficult to get a certificate if you don't actually know what you're talking about or how to do something. Good luck getting a CCNA if you have no knowledge of networking whatsoever. Certificates matter a lot because they test your actual understanding wear as a degree is just an aggregate I agree they matter and help illustrate knowledge. You definitely should have experience prior to getting a cert as you said, it's extremely difficult without that hands on experience. I have seen three camps of people when regarding certs. 1. People who hate certs. They would say there are dumps, you can memorize things, you can plausibly cheat, and even if you pass this doesn't mean you could implement the knowledge on site. These people either hold certs in disdain or doesn't care that you have them. 2. People who are indifferent. These people may use it as a way to decide between two candidates close in the running but won't remember you said you have a cert otherwise. 3. People who value certs. They understand the journey you went through to learn, they know you bought three books, read 2500+ pages, then watched 100 hours of videos and spent countless hours practicing / implementing solutions you learned into your work place to better the business. Again part of anecdotal experience, I have only seen people from camp 2 where they are glad you have the cert but they don't really care and it doesn't better or worsen your interview ranking. I'm hoping this will change as I am actively getting more certified, but so far I have noticed no changes as I accrue the certs. So fingers crossed that changes in my own personal experiences!
Correct SQL Server Integration Services. It's part of the SQL Server Data Tools package. I use it in two main ways. First is to bring data from one source to another. For example, we have an Enterprise level database with all customer information. This database is read only for me. I have a readwrite database that houses all of our correspondence information. I use SSIS to bring our Enterprise level Customer data into a table on my correspondence database so I can match who we emailed with which customer that email address is associated to. Second is to fire off stored procedures that need to happen in a complex sequence. Other tools could be used for this but I like the visualization of SSIS. Helps me to keep things organized. So for example, sp1 can go at the same time as sp2, but sp2 flows into sp3 and then sp3 into sp4, but sp5 must fire after sp1 and sp4 have finished. I don't always know if sp1 will end before sp4, so I throw them into SSIS and set a constraint on sp5 firing after both sp1 and sp4 have completed. Sorry, I'm on mobile so no images to illustrate. Again, this process could probbaly be done just as well in another tool, but I appreciate the compartmentilization and organization that SSIS allows. The Pluralsight course is worth the $30 a month fee. Should only take a month to get through the when SSIS course. I recommended having SSIS up and following along yourself with the lessons.
I certainly am not discounting your individual experience, and /u/r3prob8 has a good primer on the shortcut where 1=1 saves a few keystrokes when debugging/developing, yet "1=1" is a simple 'true' value thrown into your conditional expression so it's definitely something on your side that threw off your expected results. Definitely no reason to promote it to the title of 'theory' and to mold it into a long-term rule of thumb or preference, imo.
Thanks for this answer. I use the multiple CTE cross join you used here in the next problem.
why 1=1, and not 'this string so hype lit af fam' = 'this string so hype lit af fam', as I put in all of my professional queries that go into client production code? 
&gt; is a simple 'true' value thrown into your conditional expression so it's definitely something on your side that threw off your expected results. That's my assumptions as well. &gt; Definitely no reason to promote it to the title of 'theory' and to mold it into a long-term rule of thumb or preference, imo. I would consider it coding preference and it would be a piece of a style guide that ideally the organization would have to set standards. (I've never worked anywhere that had one in place.) Personally, I prefer leading commas instead.
I've had more of a look at this now and I understand better what you're doing. spt_values doesn't contain the largest factor so the script doesn't really work but I re-wrote yours so it will work as you intended. I think if I had taken the same approach as you I would have a far less clunky answer. DECLARE @Start BIGINT, @End BIGINT, @Value BIGINT SET @Value = 600851475143 SET @Start = 2 SET @End = sqrt(@Value) --# no smaller factor will ever be above sqrt SELECT DISTINCT n = number FROM master..[spt_values] WHERE number BETWEEN @Start AND @End AND @Value%number = 0 --number = 6857 SELECT 600851475143%6857
Can simplify the WHERE clause down to WHERE [PROD] = REVERSE([PROD])
I hope you're joking in more ways than using this particular line. Why are you leaving your debug code (i hope that's debug code) into client production? That is something that you should definitely reconsider, especially if you label yourself a "dba".
looks good from here i just kind of crapped this together `:D` two key perspectives in sql: 1) you can very often do with a scan and aggregation what you thought you needed a bunch of complicated juggling for 2) math tricks like the sqrt thing often mean radically smaller scans
no jokes on /r/sql, got it. Also, you're not invited to my party. 
i will often write something like this -- SELECT COUNT(CASE WHEN something = 1 THEN 'humpty' END) AS ones , COUNT(CASE WHEN something = 2 THEN 'dumpty' END) AS twos and sure enough i will get noobs to ask what humpty and dumpty mean
I go to /r/funny to downvote posts.
I did a quick one using Oracle syntax for fun: with digits as ( select level the_number from dual where level between 100 and 999 connect by level &lt;= 999), products as ( select a.the_number num_A, b.the_number num_B, (a.the_number * b.the_number) product from digits a, digits b where to_char(a.the_number * b.the_number) = reverse(to_char(a.the_number * b.the_number))) select num_A, num_B, product from products where product = (select max(product) from products) and rownum = 1
Is there a particular date in mind? Like you always want to subtract the days between GETDATE() and say, May 1st of the current year? I have a very kludgy and inelegent: SELECT GETDATE()-DATEDIFF(d,'5/1/' + CAST(YEAR(GETDATE()) as VARCHAR(4)),GETDATE()) 
Not sure what you mean. If you subtact 5 from the GETDATE() you'll end up with a different date, tomorrow if you subtract 5. Now you can subtract the difference of GetDate() to the very beginning of time as seen by SQL and get a particular day of the month. For instance: DATEADD( mm, DATEDIFF( mm, 0, GETDATE() ), 0 ) --* Returns the first of the month DATEADD( mm, DATEDIFF( mm, 0, GETDATE() ), 0 ) + 15 --* Returns the fifteenth of the month DATEADD( ww, DATEDIFF( ww, 0, GETDATE() ), 0 ) --Returns the monday of the week. DATEADD( ww, DATEDIFF( ww, 0, GETDATE() ), 0 ) + 2 --Returns the Wednesday of the week. This can be carried on for for the minute or even 15 minutes, as well as day, week, month and year. 
I've never seen that function before. That is way better than what I did. Thanks for that.
&gt;DATEADD( mm, DATEDIFF( mm, 0, GETDATE() ), 0 ) + 15 --* Returns the fifteenth of the month 16th. Also why wouldn't you simply use DATEADD(mm, DATEDIFF(mm, 0, GETDATE()), 15) to achieve that same goal? 
If you always want it to be the same date, why not just specify a literal date? Or would it depend on some other factors?
Thanks cool to see answers in other syntax.
Thanks I'll try to keep that in mind going forward.
Just a curiousity.
Hey, notasqlstar, just a quick heads-up: **curiousity** is actually spelled **curiosity**. You can remember it by **-os- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
So you're saying I could concatenate a solution to always give me a specific date, such as `CAST(DATEADD(x) = 1999 + DATEADD(y) = 12 + DATEADD(y) = 31 AS date)`?
" At times I have seen developers use just about 15\-20 &amp;#37; of this tool. They basically use this just to write their T\-SQL code and that’s just about it. ", I work with SSMS for around 7 years now and I think this will still apply to me.
It's absolutely reasonable. This is more of a pattern for me. Whenever I see *DATEADD( «period», DATEADIFF( «period», 0, GETDATE() ), 0 )* pattern, I know exactly what it is without thinking. Through anything in that pattern, I have to pause and determine what's going on. It's basically laziness on my part. Besides, this whole thing is simple math, which is extremely fast for SQL to handle and takes nearly no performance hit.
`Getdate()` is non deterministic so no, you cannot. Don’t subtract integers from dates. You’re making an assumption about behavior which may change in the future. Use `dateadd()` to do date math like this. 
You should move to VSCode for your powershell. The ISE is not getting any more development and everything is happening in VSCode now. 
I’m all to familiar with the various string functions in tsql. Had a boss once that all but mandated anything we wrote be in tsql even if say C# would be easier and faster since everyone on the team knew tsql but only a few knew C#. Good use of CTE’s for deriving a numbers table. Could potentially eliminate the temp table entirely and just reference the CTE.
To do what you want, you could calc the number of days which have elapsed since a given date (DATEDIFF), and then subtract that from todays date (DATEADD). But then you still need to specify that 'given date', so you may aswell just do.. SELECT ISNULL(MAX(ModStamp), '2010/01/01') 
Well yeah, it just depends upon your position: DBA: use 80+% Database developer: use 25 - 60% Software engineer: use 10 - 40% Some dude or dudette in accounting who queries the production databases and pissed off the DBAs: 5 - 20%
Really love this Udemy course: https://www.udemy.com/share/100eZICEEfdlpRQQ==/
This sub is not for MS SQL. Say it with me now. SQL IS NOT MS SQL. This has nothing to do with structured query language. 
I'm getting flashbacks of work.
 &gt;create a query in SQL mgmt studio (SSMS?), and copy the results into an excel spreadsheet to manually iterate through in order to complete tasks in our proprietary software which has fairly limited capability to connect to external data Do you mean you are unable to create a data connection in excel to the database? If that's not what you meant, I highly recommend looking into it. Excel can store the query, and you click "refresh" when you want updated data. Especially if you consider yourself a beginner, you might find cell calculations easier to implement than calculations in the query itself. Doing it that way also does a better job at handling the data. Nulls, for instance, stay null instead of the word "NULL" when copying and pasting from ssms to excel.
Is there a reason to eliminate the temp table in favour of CTEs? I've only learned about CTEs recently so I try to shove them in everywhere. &gt; Good use of CTE’s for deriving a numbers table. That was something u/MaunaLoona showed me in my last post, very neat. 
I did this one a while back. Here is my solution: WITH P0(N) AS (SELECT 1 UNION ALL SELECT 1), P1(N) AS (SELECT 1 FROM P0 A, P0 B), P2(N) AS (SELECT 1 FROM P1 A, P1 B), P3(N) AS (SELECT 1 FROM P2 A, P2 B), P4(N) AS (SELECT 1 FROM P3 A, P3 B), Numbers(N) AS (SELECT ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) FROM P4) SELECT MAX(N1.N*N2.N) Answer FROM Numbers N1, Numbers N2 WHERE N1.N &gt; 99 AND N1.N &lt; 1000 AND N2.N &gt; 99 AND N2.N &lt; 1000 AND CAST(N1.N*N2.N AS VARCHAR(6)) = REVERSE(CAST(N1.N*N2.N AS VARCHAR(6)));
Afraid I don’t understand the context here. Are you saying you need to automate the query to output somewhere? Can the output be to another table, or does it have to be external to like an email as HTML or CSV?
The client asked me to replace the query in the project (which is a web app which queries the database server) with queries like I posted in the OP. He want's it to be single file in the end so I can't use some sort of sqlite trickery. The only option I see is to convert my normal queries like say: SELECT A,B,C from table to Select 1 as A, 2 as B, 3 as C UNION ALL Select 2, 4, 3 UNION ALL Select 1, 6, 3 with all the results the normal query would return UNIONed together.
Sure, then just create a stored procedure that can be called by the App to return the data. Basically, create a procedure that takes in whatever parameters you need to identify the person/client calling it, and any where clauses you want. Then make the procedure SELECT your query at the end. Bingo bango bongo. (☞ﾟヮﾟ)☞ 
I was subtracting integers from DATETIME in one of my procs. Then I changed the column to DATETIME2(0). That broke the proc. Learned my lesson not to do it, unless it's one-time use code.
There is no SQL Server present in the demo. Just a component to run sql queries. So the data has to be included in the query like in my example. When you run the code you get your result without being connected to any sql server. I just need a way to generate the queries from existing tables so I don’t need to write thousands of rows by hand .
Depends on how much data you have. If you have under 10,000 rows this should work flawlessly: SELECT * FROM ( VALUES (1,2,3), (2,4,3), (1,6,3) ) T(A,B,C) Otherwise dump the data into a csv file and use BULK INSERT. This will work as long as you don't have field or row separators in your data.
What about using GO 1000 or something. I’m sorry, it must be early. I’m wondering why you don’t just insert 1000 rows into a table and then select from that table or something. You say demo, but do you mean proof of concept? A demo would be demonstrating a working component, while a POC would be like a proposal to start working.
What about using GO 1000 or something. I’m sorry, it must be early. I’m wondering why you don’t just insert 1000 rows into a table and then select from that table or something. You say demo, but do you mean proof of concept? A demo would be demonstrating a working component, while a POC would be like a proposal to start working.
If you have control over how the CSV is generated I suggest using ISO 8601 for transferring datetimes as text. In SQL Server that's format 126 and 127. By the way, if you can SELECT you can create temp tables. 
This is downloadable from the company's site, https://www.trivadis.com/en/downloads/plsql-sql-coding-guideline-v-32
&gt; There is no SQL Server present in the demo. Just a component to run sql queries. So the data has to be included in the query like in my example. If you have a component that can run SQL queries, then you have an RDBMS of some description that's capable of executing them, which means you can create a stored procedure in that database.
I was able to create a connection like you described for one project. In this case I was querying to get a calculated sum value of a few different fields (?) and I was tracking the change in those sums every day. I think I had it set up basically as a pivot table but if I remember right the instructions that I was following to set up the connection were fairly convoluted - I don't doubt there might have been an easier way. Would you recommend connecting in a specific way or perhaps be able to point me towards any resources describing this? Thanks!
There are � all over the web\-app output. It wasn't on our site/app/etc before, our architect can make them go away. What is wrong and how long will it take to make the characters render? \&gt; TIP: ask to talk to the person that has it working, say you'll do what they are doing to make it go away ;\-)
Assuming less than a million rows, why not paste it into Excel and build your statement that way? You write the formula once and then drag it down. https://i.imgur.com/KqI72sk.jpg Then as the final step you'd have alias your first line and adjust the last line to take of the union all. 
The sorting and grouping of reddit's comments are two distinct operations. The hierarchy only groups the comments together and establishes the relationships. Within each level of the hierarchy, the comments are sorted by time or the various algorithms. It stands to reason that the hierarchy is determined first, then the sorting done in each level as the second step. And it may not be a purely SQL operation. Don't assume that everything is (or can be, or should be) done in the database.
That makes sense. Thanks for your explanation. 
That's actually a pretty good idea. Thank you
One way to do it is calculate a global sort, then concatenate the ancestors global sorts to make a hierarchical sort. I've made an example that sorts an org chart alphabetically within the hierarchy. http://sqlfiddle.com/#!18/70988/2
As another alternative if you wanted to keep it in the same realm and not deal with Excel, you could also stick your source query into a CTE, then build the custom select/union from that. I'm using Oracle, but aside from the "dual" stuff, it's basically the same idea: https://i.imgur.com/ElJY7RT.jpg 
If you are learning SQL, you should learn a stack like LAMP/LEMP so you are somewhat well rounded. Linux, Apache/Enginx, MySQL/SQL, and PHP (or even Python). That makes you very marketable.
So learning LAMP would help me be in a web dev kind of role? I would be handling the database behind a website?
 with in as ( SELECT [ABC] ,[DEF] FROM [DB] ) select 'SELECT ' + cast(ABC as nvarchar) + ',' + cast (DEF as nvarchar) + ' UNION ALL ' as output from in In MSSQL. We have a winner. This is what I was looking for, mostly. Fully automated would be better but this will work.
Absolutely. LAMP developers know how to set up web pages on Linux servers, optimize the web server engine (apache/enginx), manage the DB and user credentials, and edit web pages in PHP (or python if the server is up and running for data analytics).
IIRC Oracle does not use `AS` when aliasing tables. Just `FROM COMMONS_POST cp, COMMONS_COMMONS cw, COMMONS_COMMONS_POST cwp`.
Take off the "as" in your from clause but leave the alias
That worked! Thanks! 
That worked! Thanks! :D 
My platform, Oracle, is typically installed in some sort of *NIX environment. On the OS side it's helpful to know shell scripting. For a language, pick one that pairs nicely with the OS you are using. For *NIX that's something like Perl, PHP, Python, or Java. If you're working on MSSQL, you might learn VB script and powershell, along with heavy front end user-facing stuff like Excel or Access, and the VBA methodologies that drive it. An understanding of ODBC is also helpful in Windows. 
I thought this was well known already, but it does become more of a problem with non-Latin languages. There's a number of Chinese and Japanese characters that are not supported by the MySQL UTF-8. More disturbing is how insert operations involving offending characters might apparently succeed, but will actually truncate everything in the field after that character. Then again, this is a database that had Swedish collation by default...
Don't forget knowing the nix actual shell language, like Bash.
Actually after thinking on it and looking at the plans ... using a temp table here is actually beneficial. If you use the CTE directly here, the numbers table will be calculated twice, which is unneeded overhead.
This may seem stupid or noob but what are the first few queries actually doing? Is there some resource I can read to try and learn this, I only understand SQL at a high level and this is over my head. 
For what purpose? Are you a DBA? Analyst? Developer?
I would look into: * SQL Server Express 2008 R2 / 2012 / maybe 2014 (Dev requires Windows 10) * Oracle Express (Unsure of the versions compatible with 7) * PostgreSQL * MySQL * MariaDB
Do you mean something like a .net app? If so I’ve built this several times and it’s quite simple.
&gt; Installing SQL I think you are trying to install a RDBMS, and you're probably using the term "SQL" to mean "SQL Server" / "MS SQL". If you can't find the help you need here, you can also check out /r/SQLServer 
I did not know that about the temp tables. Thanks!
EXISTS is used pretty commonly used and it's very useful, as is NOT EXISTS. The power of it comes from doing correlated subqueries. 
this help? http://blog.databasepatterns.com/2014/02/trees-paths-recursive-cte-postgresql.html
You don't even need to reference another table! We use EXISTS to limit UPDATEs to just records that have values that will change. UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn WHERE EXISTS (SELECT tgt.Column1 , tgt.Column2 , tgt.Column3 EXCEPT SELECT src.Column1 , src.Column2 , src.Column3);
I'm not allowed to do updates take me so because I'm not a DBA I'm just an analyst
&gt; My local time is CST Are you sure? Not many places are using CST right now, due to Daylight Saving Time being in effect. However to be fair, some parts of the world do not observe DST, but you are probably in CDT right now. 
No, I meant a tool that let you build everything. Not having to make your own app
SQL is a language. There is a standard for it written by ISO, and the various vendors The vendors support the standard to a degree. Each has their own implementation of "SQL". And they have their own extras for features that aren't standardized yet, and for competitive lock-in You use a semi-colon to allow multiple statements in a file. Generally you don't need one to terminate a single statement Generally, you don't need to quote table names, just literals. If a table name has a space you need to quote it with "table name" or [table name] or `table name`, depending on the vendor
It works for SELECTs also. You have to snapshots of data; find the records that have changed. SELECT tgt.KeyColumn , tgt.Column1 AS Column1_2017 , src.Column1 AS Column1_2016 , tgt.Column2 AS Column2_2017 , src.Column2 AS Column2_2016 , tgt.Column3 AS Column3_2017 , src.Column3 AS Column3_2016 FROM Snapshot2017 AS tgt INNER JOIN Snapshot2016 AS src ON src.KeyColumn = tgt.KeyColumn WHERE EXISTS (SELECT tgt.Column1 , tgt.Column2 , tgt.Column3 EXCEPT SELECT src.Column1 , src.Column2 , src.Column3);
Sorry, that's a habit. I'm in Central Time which observes daylight savings so depending on the time of year we're either \-6 or \-5 from UTC. The date given in the example is \-6.
Okay, actually I think this is an issue with DBeaver. I have no weird conversion issues when I use datetime in MySQL workbench. will report back when I figure out how to fix it.
Data Science roles seem to be opening up to those with decent analytical Python/Scala knowledge. Which means you can probably safely skip learning SAS now. Perhaps others can give a better perspective on this field. I work mainly with regular scienceless data (marketing, mi/bi &amp; product development) which is still mostly SQL based with modeling and reporiting done in SAS and adhoc small scale jobs done in Excel/Access. 
&gt;In MySQL -- Nevermind, just never use MySQL FTFY.
Is there a particular front end you were thinking of? It sounds like you want a GUI of some kind. I could probably build something simple and send it over if you'd like.
&gt; insert operations involving offending characters might apparently succeed, but will actually truncate everything in the field after that character. Sounds like typical MySQL behavior before 5.x, which is when utf8 was added. I'm sure you'll see a warning about it if you run SHOW WARNINGS, too, but otherwise it'll be completely silent. 
Well the problem with having to only use mariadb or postgre is that I'll lose compatability with the forum users, hence me using mysql
I was afraid that would be the case. I've read a lot about this recently and I wanted to use mysql as much as possible, since it's built to order and sort things. Worst case scenario I could use uksort in php or something. I figured if I could get the order down in mysql, and since I'd keep a depth column, templating the resulting array would be trivial comparatively. I had even considered having a second or third nested set for different sorting, but that would bloat quickly if I wanted to add more.
Shots fired
Recursive querying without rcte is not fun 
Came here to say this. MySQL is on the list of things that should not be associated with production environments.
Also, say that I was able to come up with some recursive select and join monster, would that be more or less efficient than if I did it in php? 
It'd be much more efficient
I don't think mysql supports the cte you've made. So you'd make some sort of materialized path then?
[It looks like MySql supports recursive CTEs in 8.0.](https://dev.mysql.com/doc/refman/8.0/en/with.html) Otherwise it's pretty yucky to get the sort values for all the ancestors. http://sqlfiddle.com/#!9/6dd457/7
Maybe a BI suite, like Google Data Studio, Tableau or PowerBI or what have you. Could be an overkill if you just need text results back.
Yes, I'd like a software or something, that can connect to my base, and let me create query, with variables I can enter in fields. And I'd like to ba able to keep the query I made.
This improves on mine in every way. My answer looks a bit silly now selecting the top one DESC instead of using MAX. The N1.N*N2.N product doesn't need to be CAST to use REVERSE so you could shorten your answer slightly.
Sometimes it helps to comment out the conditions and then add them back in one at a time until the data disappears. That might give an indication of where the problem is. And if removing those still leaves you with no data then it may be a bad join.
Yeah but 8 wont be used widely enough for a while. I'm trying to read your results but it would be a lot easier if the Manager ID column was visible, which I edited here: [http://sqlfiddle.com/#!9/6dd457/10](http://sqlfiddle.com/#!9/6dd457/10) Looks like it does indeed sort an adjacency list alphabetically to a pre defined depth. Cool! Problems I see with this approach: you limit the accuracy of the query by only taking the first two letters of the employers names. For example, all posts in the thread might have mostly the same subject line, until the OP changes it midway through or someone changes their own, and subject lines are relatively long. Or, lets say that I wanted to order by the # of votes each one got instead whilst maintaining order. I can't really trim votes, or reliably sort those either. Perhaps if, per each level of depth, you could order the elements of that level and make a materialized path based off of those results per level, that could work as well. But I've no idea how to do that, haha
I guess I'll keep looking for a good answer then!
Another method is to filter on a specific element in the WHERE clause that you know should be in the result set and then remove joins and other WHERE conditions until you find the culprit. Another clue is how fast the query executes. If it's instant it could be due to a logical contradiction. If the optikizer sees something like 0 = 1 in the WHERE clause it doesn't need to evalutate anything else.
&gt; You limit the accuracy of the query by only taking the first two letters of the employers names. I did that because MySql doesn't have ROW_NUMBER(), and I was too lazy to look up a better way. In the MSSQL query I use ROW_NUMBER() to establish the alphabetical order of the entire population, 1-31. Let's call this the global sort. Then I concatenate the binary representation of the global sort from the ancestors in the hierarchy. This is the hierarchical sort. So the CEO (10th) is `0A`. The CFO (11th) is `0A0B`. The Director of Finance (8th) is `0A0B08` And the COO (26th) is `0A1A`. &gt; Or, lets say that I wanted to order by the # of votes each one got instead whilst maintaining order. Then you define your global sort by NumberOfVotes instead of alphabetical, and follow the same concatenation process. You could even concatenate the literal NumberOfVotes, probably easier in binary representation, as long as you keep it fixed width. Here's an example of concatenating literal votes, compared to doing a global sort first. Level|CommentId|Votes|LiteralVoteSort|GlobalSort|HierarchicalSort :-|:-|:-|:-|:-|:- 1|1|100|"100"|1|"1" 2|6|40|"100040"|5|"15" 2|3|5|"100005"|6|"16" 1|2|50|"050"|3|"3" 1|4|10|"010"|5|"5" 2|5|99|"010099"|2|"52" 3|8|10|"010010"|5|"525" 3|7|-1|"0100-1" ^?|7|"527"
Think of EXISTS and NOT EXISTS as a multitool that handles everything you throw at it in an intuitive manner. It either returns true or false based on the existence of rows. - It will never increase the number of rows or add duplicate rows. Useful if you don't know your data or think it might change in the future. - Works intuitively with NULLs. Compare NOT IN to NOT EXISTS if the column in the subquery has a NULL. - I find it more readable. It's usually block of code in your WHERE clause that filters on the existence or not existence of data in another table. On the other hand if you see a join it could be doing a lot of things like adding rows or columns. 
Interesting use of EXISTS. Not sure I would do it this way...
Nice thanks, I'll try that tomorrow.
What would you recommend for an analyst?
You have a 1,000,000 potential updates, but only 10% of them have different values. Which do you choose? --#1 Poor performance, excessive writes, large transaction UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn; --#2 Good performance but invalid logic UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn WHERE CHECKSUM(tgt.Column1,tgt.Column2,tgt.Column3) &lt;&gt; CHECKSUM(src.Column1,src.Column2,src.Column3); --#3 Good performance but messy logic UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn WHERE (tgt.Column1 &lt;&gt; src.Column1 OR (tgt.Column1 IS NULL and src.Column1 IS NOT NULL) OR (tgt.Column1 IS NOT NULL and src.Column1 IS NULL)) OR (tgt.Column2 &lt;&gt; src.Column2 OR (tgt.Column2 IS NULL and src.Column2 IS NOT NULL) OR (tgt.Column2 IS NOT NULL and src.Column2 IS NULL)) OR (tgt.Column3 &lt;&gt; src.Column3 OR (tgt.Column3 IS NULL and src.Column3 IS NOT NULL) OR (tgt.Column3 IS NOT NULL and src.Column3 IS NULL)); --#4 Good performance and slightly cleaner logic UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn WHERE NOT (tgt.Column1 = src.Column1 OR (tgt.Column1 IS NULL AND src.Column1 IS NULL)) OR NOT (tgt.Column2 = src.Column2 OR (tgt.Column2 IS NULL AND src.Column2 IS NULL)) OR NOT (tgt.Column3 = src.Column3 OR (tgt.Column3 IS NULL AND src.Column3 IS NULL)); --#5 Pretty good performance and cleanest logic UPDATE tgt SET Column1 = src.Column1 , Column2 = src.Column2 , Column3 = src.Column3 FROM Table1 AS tgt INNER JOIN Table2 AS src ON src.KeyColumn = tgt.KeyColumn WHERE EXISTS (SELECT tgt.Column1 , tgt.Column2 , tgt.Column3 EXCEPT SELECT src.Column1 , src.Column2 , src.Column3);
That resource was definitely helpful. However, it does very poorly for examples. Her correlated sub-query is, it ends up showing a joint statement and says the associated exists statement is wrong and shouldn't be used because it doesn't heal the proper result but doesn't show and exists query that does result in the proper results so I'm confused now
One thing I like about EXISTS is that it uses short circuit logic, meaning it will stop searching as soon as one matching row is found. A join will combine the entire table even if there are multiple matches.
So it just returns true? Is that it? And the join returns rows?
Nice self promoting blog spam account ;\\
For example, this psuedocode SELECT a bunch of important volume FROM my_table WHERE condition1 AND condition2 AND EXISTS (select 1 From massive_table Where my_table.column = massive_table.other_column And complex condition) The last AND will be true or false FOR EACH ROW in my driving table 
A quick and dirty way is to just say WHERE tgt.col1 || tgt.col2 || tgt.col3 != src.col1 || src.col2 || scr.col3 
You can use Python for both data analysis and technical tasks. Web development requires completely different set of skills from what you have now. It's a huge investment with questionable payoff.
Thank you for explaining. Here is how I would do it: WHERE tgt.Column1 &lt;&gt; src.Column1 OR tgt.Column1 IS NULL OR src.Column1 IS NULL OR tgt.Column2 &lt;&gt; src.Column2 OR tgt.Column2 IS NULL OR src.Column2 IS NULL OR tgt.Column3 &lt;&gt; src.Column3 OR tgt.Column3 IS NULL OR src.Column3 IS NULL; Of course this is a bit messy if columns allow NULLs.
Thanks for explaining. I see that the possibility of NULLs complicates the logic greatly. Your EXISTS method handles NULLs in an intuitive way. In general it's a way to compare two vectors -- something that other RDMS support.
It's much better in this climate to learn Salesforce then it is to learn SAS. 
So from that I made this but with different values: [http://sqlfiddle.com/#!9/ce8b73/3](http://sqlfiddle.com/#!9/ce8b73/3) It orders it, but with lowest votes first. Sort DESC messes it all up, so you'd have to count it differently. There are a few problems here however. This would work fine up until you get more than 999 votes, then it would get confused. I suppose having a delimiter might get around that, but then there's the problem with the negatives. A better way to sort negatives would be like this: [http://sqlfiddle.com/#!9/786d89/2](http://sqlfiddle.com/#!9/786d89/2) I tried to do (l1.TotalVotes - (2 \* l1.Negatives)) AS l1.Votes but it wouldn't let me do that, so that's why I formatted it like that. Now I'm not sure if I mentioned it, but is it possible to create the local minimum per each node level? Then for each comment level under a parent you could add that minimum to the calculated negative for the hierarchy, and you'd get around that problem. 
As others have said, like most things, the answer is it depends. I am a SQL Server / Postgres DBA and I work for a company that does a lot of web development, in a Linux based environment. So naturally bash has been very useful for scripting and automation, but having some exposure to a web framework like Ruby on Rails or Django with Python has been helpful as well. Look at companies that you think you might be interested in and explore their tech stack. Decide what appeals to you and let that drive what you learn. All things being equal Python is super versital and is is ued heavily in data science, scripting, and web development. It is also relatively easy for a novice to pick up.
So when I first came across this, it was an inherited project years ago, so it was using MSQuery. That seems to be maybe on its way out, but it is still available. There's also an option to connect to sql server without MSQuery. Both options are in data &gt; from other sources. Here's a good shot by shot instruction: https://www.mssqltips.com/sqlservertip/4585/using-microsoft-query-in-excel-to-retreive-sql-server-data/ Honestly, once it's asking for tables and columns, I hit cancel to go directly to the query editor. There's a GUI for adding tables,etc, but it doesn't allow for anything complex. So on there, I suggest clicking the sql button at the top- from there just paste your query from ssms. It'll probably give you a warning about not being able to graphically represent your query - fine - hit ok, it'll still work. Here's a link for the SQL connect without MSQuery: https://support.office.com/en-us/article/connect-a-sql-server-database-to-your-workbook-power-query-22c39d8d-5b60-4d7e-9d4b-ce6680d43bad When you exit and send to excel, it'll give you the option of putting it in a pivot or straight table.
For basic database creation you could use Microsoft Access.
It's true though. It's a complete joke except for small business. Even then I can't recommend it over MS SQL express
Learn ONE LANGUAGE. Repeat after me. ONE. LANGUAGE. No employer will ever care that you know 10% of 5 languages. If you are an expert at one however, very useful 
Access - shudders -
Python. R if you're insane / dislike a work life balance. VBA if you're really, really insane. 
You're on a SQL sub so you'll get the bias here that MS Access isn't SQL. I agree with that opinion. It's fine for a data entry layer though. I don't think you can go wrong with learning any of the big database providers and Excel. In the business world, you will be hard pressed to find a company that will not value Excel skills. Being able to pull data from your database with SQL and then display reports in Excel is a valuable skill set. I think it is the most solid foundation one can have to build from. My bias is MS SQL Server (T-SQL). Mostly because the tools I've experienced with SQL Server were much more friendly and prodcuctive that what I've experienced with Oracle.
As a data analyst it could be possible you use none of those programs listed. You could be using a BI software like SAS (I think that's the name) or Power BI. I work at a college and we use a product called Infomaker. Luckily I do get to use MS SQL Server. Try looking more at BI (Business Intelligence) softwares
UWUTM8?
Enclose it in single quotes. `'5/25/2018'`. And consider using `YYYY-MM-DD` format instead for accurate ordering (since it's actually a string, not a special date type). https://sqlite.org/lang_datefunc.html might be of use too. 
This is exactly what I'm worried about. I don't know which Business Intelligence softwares that I need to familiarize myself with. Are there any popular ones that companies use? Currently I've learned the basics using MS SQL server but I heard that only the folks in the IT department use MS SQL Server. I don't have an IT background, I just graduated from an economics program and hence I'm more inclined for an analyst position. I'm trying to fake a bit of work experience because without some decent experience, the chance of landing even an entry level job is basically 0&amp;#37;. I need to sound the part when I'm in an interview so I need to get the details right on what BI software I used.
What does the data in the Timestamp column look like? It should be something like this (if it is a timestamp) 2016-10-15 08:15:45.000 The date format you are using may not be compatible for comparison purposes. Copy one of the dates and modify it to your specification Ex. Where TIMESTAMP &gt; '2018-05-25 00:00:00.000' Note: you may also need the quotes around the modified timestamp as I put above. 
Out of everything you said, learning Excel will be the most immediately useful skill. As far as databases, you won't have a choice with what you use on the job but Access databases will be available to create. In fact, I use Access to store data that I get via csv from a web application. If you want to practice extracting data and inserting data with code (maybe via ADO) then Access will be easy to get started with. That said, you shouldn't settle for Access as you will be very limited. I too am a fan of SQL Server in terms of friendliness along with connectivity to Excel. 
What do you mean by "data entry layer"? I'm not going to be a professional SQL developer working in IT, I just need to use it for whatever a data analyst needs.
I like it. You've solved all the niggles I was thinking about over dinner.
Excel is something you will use in almost any job, so I'd learn that first. Many analyst jobs use Excel heavily, daily. I wouldn't learn Access. That's a category all of its own, versus MS SQL Server, MySQL, PostreSQL, Oracle SQL all being pretty similar. It's better to learn one of that group because it will transfer to the others. For enterprise, learn MS SQL Server or Oracle DB. For web development, learn PostreSQL or MySQL.
Well still, it doesn't like negative values... and it looks quite ugly. Is there a way to do all of the math in the select statement up there so I can cache each math query there? Besides that, there would still have to be a way to store a local minimum/maximum vote value, to have the minimum be added to the path to make the min vote always be at 999 or 9999 or whatever. I guess each post would have to have its own column for their local values, and when a vote is made then it would have to also check the parent id's row and update the values accordingly, but that's more php overhead... I'm not entirely sure if calculating these values every time someone wants to go to viewtopic.php is good either, if I wanted to generate the min/max every query. So I guess it's best if the local min/max is made separately. At least then I would be able to use php to change the LPAD and concat lengths in the query dynamically that way, 
I have been practicing with VBA code in Excel. Is there much difference between coding in MS SQL Server and Access? I've learned SQL basics using SQL Server but I hear that SQL Server is only for the IT folks, not analysts. I just graduated with an Economics degree and I need to fake some work experience. Don't question my morality here. I need to sound the part on the interview so if I say I used SQL Server at an analyst job, would that raise some eyebrows?
Learn SAS or Power BI. Can't go wrong with either. Most BI suites it is learning how to generate the script but the logic of the script can't be taught and is what companies will be looking for more.
For most data analyst roles you're going to need to know both SQL and Excel pretty well. The different flavors of SQL are similar enough that once you know one you can pick up the unique quirks of any of the other ones easily. Try the Mode Analytics tutorials.
I mean data entry for those that need to add or change data. Say someone needs to record the color of shirts for each person on their office floor wears each day. Assume you don't have an app that does it for you. It is much easier to use an application like MS Access to type in what each person is elwearing each day versus typing UPDATE statements directly to the database. You could use MS Access like any other entry form you might see on a webpage. Personally, I find "data analyst" to be a rather broad term. As a data analyst, you may also be tasked with maintaining your own data sometimes. Data entry layers that take near zero maintenace and setup time like MS Access are helpful in those cases. For example, I get a vendor report that is in a human readable format that would be a huge pain to make machine consumeable. I need to update our records annually with 20 values from this vendor report. There is no efficiency to be gained in trying to automate something like this that will take me maybe 5 minutes to link tables in MS Access to my database with 2 minutes of data entry annually. If you happen to never ever work somewhere that you'll need to manage your own data set, then sure, data entry layer is forgettable. To put it in perspective though, I'm talking about a few five minute youtube videos worth of knowledge here. Nothing fancy or time consuming.
Familiarize yourself on the data. Understand your tables. You can be a guru on the platforms, but if you don't know your data then you suck.
Local min and max is just grouping by ManagerID in another subquery, yes? I think you just need to pick a base number that allows enough room on both sides to accommodate negatives. 900 instead of 1000 works in this examples. http://sqlfiddle.com/#!9/786d89/46
What he means is that MS Access is a conduit to access (no pun intended) the data in a database. It's more of a sandbox where individuals can create forms and menus which can insert, modify, and update data in a database. It isn't really geared towards crunching data. Based on your other comments you'll probably want to use MS SSMS (Microsoft SQL Server Management Studio). It has administratively functions but you'll use it mainly to extra/crunch data. I'm a systems analyst but I do some data analysis work for my company. The flow where I work goes something like this: You build a model (or whatever you want to call it) in SSMS, turn it into a view, stored procedure, or some kind of OLAP cube, then point a BI tool (i.e., Excel Power Pivot, Power BI, etc) at the model, making it accessable to decision makers. SSMS is my go to for simple and complex datapulls. SQL has a pretty low learning curve but it can take years to master.
MySQL workbench
I use SAP HANA at work but thats not something you could practice with. When I was first learning the basics I used the Northwind database in Access
Oh wow you're magic. But see, the point of calculating the min in the first place would need to be done before the sort materialized path is made, same with the max variable, because then we could change the length of LPAD and the concats to account for that globally. So is it possible to calculate sort for all of them at the end or get min/max from the beginning without losing too much efficiency? This would be ran every time someone goes to viewtopic.php Another point just to be sure, I just tested this and it seems to work, but changing WHERE l1.ManagerID IS NULL to match the ID of the highest node we want to calculate is the right way to go about it right? It seemed to work: [http://sqlfiddle.com/#!9/786d89/49](http://sqlfiddle.com/#!9/786d89/49) 
Same here. I'm using the Northwind dbase right now. 
I work as a data analyst and use MS SQL Server. It's not super important that you know a specific RDBMS, but that you just know some SQL language (they're all functionally the same, just different syntaxes) and are familiar with one RDBMS. Every data job ever will use Excel, make sure you are solid as fuck on that - vlookups/hlookups, if functions, aggregate functions, pivot tables, etc. basically make sure you can give business persons reports in Excel that they can use/look at and understand. Once you feel okay with an RDBMS look to learn a reporting tool like Tableau or SSRS. You'll be able to make really neat reports that utilize the queries that you make. That would give you enough to get a good job honestly. I recommend you do an online course at Udemy or something similar if you are serious about this and really stick it out. I promise it's worth it. 
SQL Server 2014 or earlier will do.
Whoever told you only IT uses SQL Server is dead wrong. Download SQL Express for free, and start learning the interface using the AdventureWorks database. Next, get familiar with building an SSAS Tabular model, and DAX (which can also be used in Excel PowerPoint and Power BI). You will be valuable to any company which manages a lot of data... and you won't be placed in the IT department, I promise. 
Good to know that I don't have to sweat about the little differences between each of the RDBMS. I'm pretty confident with Excel VBA so hopefully I'm not too far away from placing these keywords on my resume.
Postgres
Yes, WHERE ManagerID = 3 will filter to that part of the tree. I think dynamically figuring out the min/max to get the padding right is too fancy. I would just assume some extreme values. All this is much easier if you can figure out how to turn the sort logic into sequential numbers first. [Maybe take a look at this.](http://www.mysqltutorial.org/mysql-row_number)
If you can nail down joins and simple subqueries its a good starting point
You're right, I guess forget about that idea. Good job figuring it out though! By turning the sort logic into sequential numbers, what do you mean exactly? Because if you mean to sort by negatives, I think what you said and what I said above about having padding length be one more length than the max vote would work.
I'm on mobile, so I haven't really dug into this. My first thought would be to use a case statement or nvl2 and count. Something like: Select shift_date, Shift_type, Count(nvl2(manager, 1, 0) manager_count, Count (nvl2(operator, 1, 0) operator_count, ....and so on for each role. I might be wrong though, haven't really looked at the table structure.
I'm a data analyst (specifically, financial) for a fortune 10. So you understand where I'm coming from, pay range for my title in the co starts around $70k and goes up to $150k annual, + bonus. I worked my way up from analytics jobs in the same co starting from about $35k, so I have a good grasp on what is expected to start and to move up. Entry-level, you should be good at VBA and some basic ANSI SQL. Learn power query/pivot in Excel, as learning how to build reporting layers with VB &amp; SQL is an incredibly effective tool that will get eyes on you if you can get creative with solutions and have an eye for design. I would stay platform-agnostic on the SQL, learning some T-SQL is good too, but generally make sure you know how to write solid, ANSI-standard SQL, as basically every IDE will take that as a start. You can start to focus on different platforms when you find out what the company uses. If you are planning to work for a large co, understand data warehousing and basic terminology. A *lot* of what got me where I am was getting *really* good at extracting data from huge DBs in Teradata, and being able to transform to something useable for the field (Excel, SQL Server, etc.) If you are *really* forward thinking, learning a data lake platform and some secondary scripting (e.g. Python, R) is the way a lot of the front of the industry is heading, and will move you up on interview lists. (Predictive, proactive analytics, vs. reactive analytics). Lastly, and most importantly, don't think of different parts of SQL as "IT jobs". IT jobs are mostly ETL and similar, and if you want to be effective as an analysts, you will need these tools - e.g. you will need to learn how to do things like make tables, load, transform data across resources, etc. If you don't, you won't be effective, and will be little more than an Excel jockey. Many higher-level jobs will also require you to take on some of these roles - for example, beyond my analytics jobs, I am DBA for a small-minded SQL Server, and have to do those "IT jobs" too, along with analytics. Hope this helps!
I just read the question again lol. You'll need to add the roles together instead of splitting them off into separate columns.
If you want to maximize your salary as an analyst start by learning T-SQL and the more advanced Excel features - complex functions, pivot tables, and external data connections. T-SQL (MS SQL) is one of the most feature rich SQL platforms and is a bit easier to learn than other types of SQL (solid error reporting, large analyst community, and more analyst focused). If you have T-SQL and Excel dashboarding expertise most hiring managers will assume you can easily pick up other flavors of SQL, dashboarding platforms, and have no problem with things like MS Access. 
This helped a lot. I am aware that some analysts are required to maintain data warehouses, but in the near future I will steer clear of those jobs. Never knew R was useful. Used it as a "learning" statistical software at my university. 
VBA can *connect* to SQL Server but cannot be used *within* it. While in Access, you can use VBA just like you would in Excel albeit with minor differences. With ADO, you can connect from Excel to Access to SQL Server to Oracle....pretty much everything on this site: connectionstrings.com
Count up the number of unique staff coded assigned to a shift, because one staff code is one person. Don't count by role.
This is where I'm getting stuck, how can I count the number of unique codes in one row(single shift) across multiple columns?
I use SQL Server, SSRS, Report Builder (essentially the same thing as SSRS), Crystal Reports, and Excel. Most of my job responsibilities cross over with a Data Analysts. I work in IT and there’s *a lot* of people in my department who use SQL. I’d say about 25% of my department uses SQL on a weekly basis. Me and my data team use it every day. As for Access, it really depends where you’re working. In my department, Access is sort of looked down upon. No one uses it, but I’ve heard it mentioned before in a joking manner. I guess it’s because it’s more user friendly with a fully featured graphical user interface whereas SQL Server is code based. 
I'm a Data Analyst in IT. I use SQL Server and Teradata daily. I use DB2, MySQL and MongoDB about once a week and haven't used Access in years. Learn about aggregates, INNER JOIN, LEFT JOIN, LEFT OUTER JOIN, WHERE, HAVING... These show that you have basic understanding and are the same (or similar) for most databases. I don't imagine that they would ask beyond these for an entry level position. I have worked with Analysts that exaggerated their SQL abilities before and as long as you make the effort to learn and grow you'll be fine.
SQLPlus is text based because it is a text tool. You can use GUI tools with Oracle, too.
 SELECT 5/25/2018 Result: 0 SELECT CAST(0 AS DATETIME) Result: 1900-01-01 00:00:00.000 You're comparing Timestamp to 1900-01-01 00:00:00.000, which essentially returns all rows minus where Timestamp is NULL. Hope that helps.
One way is to do multiple queries with unions to get all the codes from each column.
I made a simple app i you'd like to try it. Where do I send it?
Sounds like we are mixing concepts of data warehousing (used primarily for reporting) with operational design (used to enforce data integrity and eliminate redundancy). Technically speaking, the most efficient insert would be into a table with no foreign key constraints and no indexes. In fact, oftentimes logging tables are designed exactly this way. However, what you are describing with fact tables and base url and parameters sounds more like optimizing for maximum data analytic capability. That's a different objective from insert efficiency.
A few points of clarification for you. *SQL* as a general term refers to Structured Query Language. It is an ISO standard (current spec is SQL2016). When implemented as standard, SQL can be used to retrieve and manipulate data in: * Microsoft Access * Microsoft SQL Server * MySQL * Oracle Database * IBM DB2 The above are all examples of *relational database management systems*, or RDBMS. Of those listed above, SQL Server is probably the most prevalent for line of business applications in the average corporate world. Oracle Database is the big data of online transaction systems (think financial institutions and the like), IBM DB2 is used on legacy mainframes consistently still. Each one of those systems use their own tools to login and query from their hosted databases. Each also has extensions to the SQL language (Microsoft's is called T-SQL, Oracle is PL/SQL) that are proprietary extensions to the standard specification. In addition to the vendor provided tools (MS, Oracle, etc.), there are specialized analytical tools that can connect to any of those databases. Examples include: * Tableau * Power BI * Excel * IBM Cognos * SAP Business Objects Find some online courses that teach the fundamentals of RDBMS, the SQL language and then go from there. Once you have the concepts down, learning specific tools for the platform you are working on becomes much more trivial
In my **very** lay opinion you would attack something like this thusly: 1. You would simply dump all of the raw data for both your visits and referrers into a raw table, this would be an "ETL" job. 2. You would have a set of stored procedures which would take this data and organize it into somewhat of a "normal form" and into some type of warehouse. This might not be really appropriate for your example if all you're doing is splitting values "organizing" the raw data in a better way. In that case you would probably load all new data into a 'staging tables' in the ETL phase, then perform this phase here, then dump the data from the staging tables into the dump tables and truncate the staging tables. It doesn't sound like you're needing to update, or worry about dupes, so you want to keep everything... but the value of "normalizing" or organizing your data so that you can add indexes, join to other tables, etc., should be obvious. 3. From the table(s) you've established in step 2 you will create new tables which analytically explore your data in meaningful ways and are used for "producing reports" -- but anyone who does steps 1-3 to produce a predictive model is going to hate you for referring to it as a report, and this is not the same as 'being an analyst', but without digressing further you would simply rerun these 'reports' or stored procedures whenever you finish running the jobs involved in step 2, which is dependent on the job in step 1. No need to do any updating or inserting here unless you have something crazy designed, but at that point it has nothing to do with your original question. Establishing PK's or FK's for this type of data may have very limited use, and may not be at all necessary for any reason other than to stroke your ego. In my non-DBA mind if a PK is not being used for anything and never being joined... then it is a totally worthless entity that takes up time and resources to generate.
Ik love sql developer. It's great
Learn Excel first, pick up on some VBA to write simple code for macros to make some repetitive tasks quicker, then learn SQL. I have a Masters in Business Analytics and work for a very large corporation. Excel is used heavily. No one uses Access. Once you know SQL, depending on what BI tools your company uses, you can easily pick up. For instance, with SAS, you can write with SQL code to retrieve and summarize data.. all in SAS application. PowerBI is good but yet another language to use. It's more for dashboards and data visualization anyways. It's like Tableau. If you're going to be an "analyst" and plan to predict or forecast, you may want to learn R or Python. They're easy to learn once you have some data wrangling background with SQL... just my opinion. Good luck!! 
does a SELECT query create a relationship? no does it generate a new table? not exactly, although the results are returned to you in tabular form
Selecting data from tables has no impact on the underlying database objects. 
To be clear, I didnt say "efficient inserts" I said "efficient inserts into a normalized table"
Well, thanks a lot, you can upload it somewhere and give me the link, or send it by mail at shuya\_nanahara@hotmail.fr
MS Access with passthrough queries and SQL Server Management Studio 
True. Just pointing out that there are competing priorities when it comes to database design.
well, nothing worked, and it was too complicated. I'm trying with variables but have some troubles. For exemple, if I want to do : searchTitle = "toto" select \* from books if searchTitle is not null { where title = searchTitle }
(SQL server) If I’m not mistaking, you are trying to compare a column to a table ( column title from table book to table titles). If you want to get all the books that have a title similar to any title in the table title, it would be something like: Select * from books where title in (Select title from titles ) If your table Books has an Id referencing to a title in table Titles it should be more something like: Select t.title , b.column , (any other columns you need) From books b Join title t on t.id = b.title_id Hope I didn’t do any typos or said something wrong 
First query as a stored procedure maybe? 
Could you link the two by making the second query a table values udf and linking it to the first query using an outer apply?
CTE CURSOR with While Loop
HeidiSQL is outstanding and it's free
&gt; I ideally need the second query to use the 1st queries list and run it for each entry. join you're describing a join
Reporting tools are really stupid easy once you have SQL knowledge. You can get decent at those within a day
Salt or freshwater?
SQL Server only introduced temporal tables in 2016 and a lot of ISVs try to maintain compatibility with older versions, so adoption is...slow. I think you'll see them more in home-grown systems where the developers &amp; DBAs have full control over things. Personally, I'm looking forward to the prospect of implementing them when I get my upgrade to 2016 complete in a few months. I've got a few tables which have triggers to write their history to a second table and I'd like to eliminate the (visible) clutter. &gt;MS SQL certification for aquarium transact SQL There's something fishy with your auto-correct...
No that's not what I'm tryin to do. I'm trying to define a variable titles, and get all the results from the table books where books.title in titles, titles containing ('t1','t2')...
Should you just wrap your variable in paranthesis? SELECT * FROM books WHERE title IN (&amp;titles)
It's Google voice typing which I think is Microsoft speech engine...
It's Google voice to text. When I'm running around the office don't have time to type. The errors are worth it
Thanks, but no, doesn't work
Ah, the parentheses are included in the definition, my bad.
Ahh the ol’ reddit [sql-a-roo.](https://www.reddit.com/r/assholedesign/comments/8r2o7b/comment/e0owa8h?st=JIFWPGW9&amp;sh=9136469f) 
DECLARE @MyList TABLE (Value INT) INSERT INTO @MyList VALUES (1) INSERT INTO @MyList VALUES (2) INSERT INTO @MyList VALUES (3) INSERT INTO @MyList VALUES (4) SELECT * FROM MyTable WHERE MyColumn IN (SELECT Value FROM @MyList)
it's not the syntax which is failing as it is correct. what platform are you using? whats the error message?
Comma joins with oracle +/= syntax for best efficiency and readability.
https://youtu.be/ZjsH_V5EkSk This has been my favorite simple video explaining. This article as well: https://www.brentozar.com/archive/2016/05/temporal-tables-partitioning-columnstore-indexes/amp/
Exactly what I needed, cheers!
Ok it worked fine when I removed the first ; after the def thanks
im still curious what you are using. do mind sharing that information?
What? No C#/ASP.NET (MVC, Entity Framework, Linq, etc) or SSDT?! :(
Who said anything about a comma join? What about a subquery or even a temp table?
That guy (☞ﾟヮﾟ)☞ 
My company does quite of bit of analytics and use temporal tables all the time. 
You got it champ.
How would I create a relationship? I use keys?
 def titles := ('t1','t2') select \* from books where title in &amp;titles; instead of def titles := ('t1','t2')**;** select \* from books where title in &amp;titles; For some reasons, the := is underlined in red, but it works without errors or warning, I just have a script result saying ancient... but everything seems to work now.
Just sent it over.
Depending on why you want to learn SQL, it may benefit you in the long run to learn how to get more comfortable with a CLI. I would also suggest staying away from mySQL, not because it's difficult to learn but because there are a number of database settings it allows you to make which encourage very bad habits (zero dates, anyone?).
If MSSQL, go with a table valued function. 
How can table1.col1 be equal to 2 and 4?
the ~only~ way to create a relationship is with FOREIGN KEY syntax
There was a talk on temporal tables, use cases, implementations, benefits and downfalls and current support at this year's SQL bits. I found it quite useful there is a video here https://sqlbits.com/Sessions/Event17/Temporal_Data_in_SQL_Server 
Still not willing to share the database and version you are using? 
Nah, probably no cursor loop needed. You can likely do it all with a simple query with a join or a subquery.
sorry, wrong operator, it is supposed to be OR will edit
So just to clarify, you're looking for output like: 2|hostname1|Count all Col1 "2" records/1024 4|hostname2|Count all Col1 "4" records/1024
My company tried to implement them, but ran across a bug in SQL Server 2016 with cascade delete that Microsoft didn't bother fixing until version 2017. So we gave up and went back to our home-grown poor man's CDC.
Correct the output should look like this |table1.col ( count of 2&amp;4)|table2.col1|count/1024\*100 to get whole number)| |:-|:-|:-| |986|hostname1|96| |762|hostname2|74|
Tried both of those things, no luck :(
I can't get the same figures as you have as you seem to have a different dataset, but how about something like: SELECT table1.Col1, table1.Col2, 100.0 * COUNT(table1.Col1)/1024 FROM table1 JOIN table1 on table1.Col2 = table1.Col2 WHERE table1.col1 IN (2,4) GROUP BY table1.Col1;
SSDT You store the metadata and use that to create the databases from scratch. You can have scripts to pre-populate data, too. You can then store that in the source control of your choice. We use TFS's implementation of Git. Another option is just purge your data. Loop through your tables running Truncate Table.
I found it, thank you!
What 
I think you're using "SQL" to mean MS SQL Server. Because a "SQL database" could be any RDBMS that uses SQL. 
OK, sorry if you got another reply previously - think I had misunderstood what you were looking for, and I'm still not 100% sure I'm there. Looking at your original query, I think you're trying to get a sum of how many occurrences of 2s or 4s are in table1 broken down by hostname in table2 (does that sound about right?): SELECT TABLE2.Col1, 100.0 * COUNT(TABLE1.Col1)/1024 FROM TABLE2 JOIN TABLE1 on TABLE1.Col2 = TABLE2.Col2 WHERE TABLE1.Col1 IN (2,4) GROUP BY TABLE2.Col1; I'm not sure if this agrees with the output in your last table, as the dataset seems to be different, but it might still not be quite what you're looking for. Otherwise, what would the expected result be from the data in the OP?
I don't think it's *quite* as widely useful as you may think. Remember that you're creating a *full* copy of each row each time even a single value is changed. There are times when you need more granularity than that, or you need to economize on space. For that, there's CDC (which then requires another step to process the data into history tables, like a data warehouse ETL). For my use case, where we're tracking the history of a configuration table, it'll actually end up being better than our current tracking as the current tracking *doesn't* cover all the fields and I'd like it to do so.
I am not sure that I completely understood the question. However, assuming all the data you need is within Table1, this should be a good starting point. SELECT [Town], COUNT(DISTINCT [USER ID]) FROM Table1 WHERE [Country ID] = 10 AND ([Rejected conditions]) GROUP BY [Town]
My learning project has a goal of working on MySQL and PostgreSQL. I kept the question SQL agnostic because I thought it might allow more people to comment how they rebuild their databases.
I will check out SSDT. Is it generally better to use an IDE as opposed to a shell script?
A shell script is easy to do. (Sqlite example; adjust as needed) #!/bin/sh sqlite3 foo.db &lt;&lt;EOF DROP TABLE IF EXISTS bar; CREATE TABLE bar(name, date); -- and so on EOF 
you drop _all_ tables and then recreate them? Why not back up the 'empty' state and just restore it instead of dropping/recreating?
It was just what I came up with, perhaps because I am still making changes often to the schema so I thought I should just start from scratch each time. Is that an uncommon way to work?
Oh, I see, I can just put my sql directly into a bash script like that? Awesome. Thanks, this is what I was hoping I could do. In your experience, is this pretty good way to work on projects?
My learning project has a goal of working on MySQL and PostgreSQL. I kept the question SQL agnostic because I thought it might allow more people to comment how they rebuild their databases.
We used SQL Scripts in the past to roll out our SQL DBs. Which was alright but it meant a lot of time rolling out a database. We've also used SSMS to add tables, sprocs, etc but we lose history of changes. Storing everything in SSDT allows us to quickly rollout the database structure to different server at anytime OR delete our database and rollout a new copy. It will also update existing databases. 
I'm going to disagree with most people here and say to go with MySQL. The documentation is extremely thorough and [W3Schools](https://www.w3schools.com/) is a great resource. There's no wrong answer though. If you plan to go down this path, almost any RDMS will work. This is where I got started. The community edition. https://www.mysql.com/products/community/ 
I have a strong preference for the flexibility of working from a command line instead of getting locked into some ide or fancy gui interface, so it works well for my work flow.
You need to use a cursor to loop through \`@DBs\`, construct the query, then \`exec sp\_execute @sqlCmd\` to execute it. The inside of your cursor will look like this: `set @sqlcmd = N'update ' + quotename(@CurrentDB) + N'.dbo.list set flag = 1 where color in (''RED'');` `exec sp_executesql @sqlcmd;` I'll leave the construction of the cursor to wrap around it to you. Note that I'm using \`QUOTENAME()\` around the name of the database to protect against injection attacks if someone puts malicious content into that \`@DBs\` table variable.
Hey Sam... Thanks for your help. It got me close... Here is what I got working; SELECT DISTINCT (table1.col2), table2.col1, SUM (CASE WHEN table1.col1 = '2' or table1.col1 = '4' then 1.00 else 0 end) / 1024\*1 as PCT From table1 RIGHT JOIN table2 on table1.col2 = table2.col2 GROUP BY col1, col2 Hope that helps... It gives me an unwanted column but I can just not call that in the report so no big deal there.
Just started a new job a few months ago. They didn't have any BI infra and we are starting things up, with a 2017 installation. We've decided to go all in on temporal tables for source data historization. (pretty much a straight copy of all tables from source system with a couple of added columns). In order to save time, out first pass is simply creating temporal tables for each data element using the original source system keys and such, which allowed us to divert resources to tackle critical reporting needs. 
Dropping and recreating tables is a common practice, dropping all of them is somewhat rare in the dev environments where I work.
My suggestion is not to try to fake SQL or VBA experience. Learn a little about SQL and VBA, and be upfront and honest with them about how little you know, but also about how quickly you learned what you do know. Successful analysts aren't made from what they already know, but from what they can learn.
I'm a Data Analyst for a major financial institution. We use lots of different DBs. Our main data warehouse was SQL Server but that changed to Teradata. We also use Oracle for a few things and some sort of Hadoop solution is being built. I use Access frequently to join up data from multiple sources or produce reports. There are 2 huge reasons to not learn SQL with Access: * Access SQL is kind of weird and lacks some very powerful functions you'll find in enterprise grade DB's. It also lacks a couple of very basic functions. * The Access SQL editor is pure garbage. Those two factors will combine to make learning SQL a lot harder than it needs to be. There's no way to know which db a future employer will use, so you have to just pick one and start learning. Luckily, the different SQL dialects are very similar so moving one to the other is pretty easy. I'd just download MS SQL Developer Edition and start learning. 
Lock my rows I'm going in!
Store the user credentials in a local access table, preferably with their password hashed. Then in your function pull their username and password out of the table and add it ton the connection string. Make sure you also have a function that re-links your linked tables with that new string so that the users have the appropriate permissions on the table too.
 dim tdf as dao.tabledef with currentdb for each tdf in .TableDefs if Len(tdf.connect)&gt;0 then tdf.Connect = GetConStr() end if next end with
Old Oracle-style join syntax is obsolete and should only be taught to a new SQL-er after they're fluent with ANSI joins just in case they run into it in the wild. Oracle docs themselves recommend ANSI joins. Oracle joins are not more efficient. Readability is subjective so I can't argue with you there. I will say though that the non-standard nature of Oracle joins means that non-Oracle-heads might not be able to read a simple query using that style. 
I was being sarcastic. I hate Oracle Joins.
This is the best answer given OP's description of the problem. OP: please try to solve this with joins. Others have suggested some stuff that might only work on one DB product (Cursor, stored procedures, user defined functions, etc.) Using one of these might solve your "now" problem, but will leave you with a "later" problem, say, when you have to port this code to a different platform or when you have to solve a similar problem on a platform where procedural language isn't available. 
You can't send an already hashed value. Well, you can, but it's not going to work because the receiving system will has it again, and the login will fail. And storing login credentials in an Access DB is the very definition of "extremely unsecure".
Good :D Just making sure OP doesn't turn into [Wimp Lo](https://www.youtube.com/watch?v=d696t3yALAY&amp;feature=youtu.be&amp;t=41) 
Right of course, I misspoke. I meant store a salt, not the password or hashed/salted password itself. Right, you cannot send the hashed value. You need a mechanism to take their password input and a known salt value to produce the hash and check that against the remote server. 
You'll probably get a massive boost in performance if you switch from linked tables to pass-through queries. That way, Azure does the work and just sends you the recordset. Otherwise, you need to wait for Azure to send over the table contents so Access can run queries against the data. So, so, slow. There are times you actually want a linked table, but I only use them when I actually want every value in the table, or I want to push data in. But to address your problem. If you use pass through queries or linked tables with an ODBC connection, the first time your user tries to connect, they will be prompted for their credentials. Access will then use that connections for every other link to that db until the user shuts down Access. So they'll only be prompted once. If you want to use Connection Strings via VBA, which is kind of doing it the hard way, you need to build a Login form that opens when they open the db. Get their credentials there and store them in TempVars, which will hold their value until Access shuts down. Then you can plug the TempVars values into your connection string as needed. If you do that, I would recommend testing those credentials immediately after their entered. Run a super small, super fast query. If it returns a value, they're good. If it errors out, you ask for the credentials again. You could also do the login form thing with linked tables and pass through queries, then your users will get that part out of the way when they open the db. DO NOT do as the other user suggested and store your users credentials in a table or in the code somewhere. That is incredibly unsecure. Never forget that Access security is a complete joke.
Got it! It's so common around here for people to use SQL that other way, I must have assumed wrong. Thanks for the clarification! 
Yes I had a brain fart when I typed it out. I wanted to store a salt, not a password. 
If you're using t-sql, try using ms_foreachdb. It will loop through each database and run whatever code you specify e.g.. 'use [?] GO INSERT INTO schema.table ... ' 
What good is storing a salt?
- ms_foreachdb is unsupported has some quirks IIRC, and may disappear at any time - what if you don’t want to hit every database on the instance?
^(T)
I think you answered your own question. Unless you're finding a senior role in a fortune 100 company, your probably not hitting your salary requirement. 
If you are hashing passwords you need to know the salt to apply to the one way hash check to see if hashing the password plus the salt with the algorithm of choosing matches up for the purpose of authentication. 
That doesn't help you log into SQL Server. You need the password itself.
There is a free tier for Db2 on cloud, but I believe the data limit is only 100 MB. You can use Db2 developer-c on your own linux vm for free, as long as it is less than 2 cores, 16 GB of memory, and 100 GB of compressed data 
Well obviously the user provides that. But yeah I guess the sql password isn’t hashed... I was thinking of ways to help authenticate users without sending to azure over clear text which is probably how ms access does it. I’d have to run a trace. 
 from sqlalchemy import MetaData, create_engine engine = create_engine('{connection_string}') meta = MetaData(bind=engine) meta.drop_all() meta.create_all() 
Mind if I ask what the bug was? Was it the ON DELETE CASCADE issue? I read through the list of considerations and the only ones that caught my eye were the linked server issues and the fact that indexes/stats don’t carry over to the historical versions. I am in the thick of architecting implementing our DW and am about to start implementing Temporal tables, so any info is super useful atm.
Hey, check this out: [http://sqlfiddle.com/#!9/674922/17](http://sqlfiddle.com/#!9/674922/17) Another solution I was able to get using a closure table that generates materialized paths instead. Looks much cleaner :) Two problems: how to limit the number of children per node based on depth, and #2 how to sort by name desc Sorting is done in group\_concat, and 1e6 - totalvotes works for votes asc and normal totalvotes for votes desc. EmployeeName works for name asc... but how in the world would you reverse that to make names desc? 
The "limitation" is that you cannot enable temporal table on a table that has ON CASCADE DELETE on any of its FKs. This is for SQL Server 2016 only, and was resolved for 2017. Seeing how we just finished upgrading everything to 2016, there was no way corporate would go for 2017 any time soon.
I dont know what area you're in but where I'm from thats pretty good money for a SQL coder. I assume you meam PLSQL too? Have you thought of database administration? Not sure if youre aware (if I'm over simplifying I dont mean to) but theres a whole dictionary of system related tables in Oracle for administration, tuning, database backups, restores. Oracle DBAs (imo) arnt has sought after as they used to be but they can contract out at good money in a senior role. I'd make your skillset more valuable by learning SQL server, mysql, Azure Sql etc. Then there's the world of DBA development. You'll run into that in the administrator role as well. Good luck.
Thanks for the info. Luckily for us this is just a Summary/Dimension/Snapshot table so we won’t have too many issues (especially ones with FK). Basically it was using this or setting up a type 2 dimension table for it with the appropriate ETL via SSAS/SSIS. I’d rather try the temporal table out via SSIS and eliminate the overhead. If it doesn’t work out, we fall back to standard practice.
Thanks! Can we use the free threshold without entering payment details? I have difficulty with keeping track of things -- I'm worried that I will accidentally surpass the threshold and end up having to pay for it
Thanks! Can we use the free threshold without entering payment details? I have difficulty with keeping track of things -- I'm worried that I will accidentally surpass the threshold and end up having to pay for it
As long as you are in the US, no payment details are required for the free tier.
Other side of the pond! :L
Unfortunately, last I heard (within the last month) the free tier was not fully available outside of the US.
Thanks! I didn't know about that. Definitely will try that today.
Good to know, I will keep that in mind. I often use it like that for Google searches, probably because it is faster to type. I haven't used the MS SQL, is it common?
What sort of SQL are you talking about when you say you’re gifted? If it’s PL /SQL, then your best bet is to find a company that still uses Oracle and wants to spring for new development or enhancement of current processes. If it’s SQL, then you’d be able to take that knowledge and find a job at a company with any other RDBMS as there are similarities. I am a SQL Server and Oracle DBA who came from a Data Development and Application development background, so my best fit was as a DevOps DBA, where I mostly manage the codebase, do development/tuning, permissions, installation, and some backups. I use T-SQL, SQL and Pl/SQL on a daily basis, and only use Pl/SQL when I’m writing automations or functionality for an App. I really really really don’t mean to be rude or offensive, but I’ve interviewed what must be 30-40 people in the past year or two who have claimed to have substantial PL/SQL knowledge, and said they were 7 out of 10 or more, and they couldn’t tell me what a package was, or explain the concept of a cursor, a reference cursor, how to set variable and parameter data types as references to tables, or even how to set default values in a procedure. I’m not saying this is how you are, I’m just advising that you ensure you’re familiar with all the additional things Pl/SQL is used for. You make a lot of money. If you’re comfortable where you are, I’d suggest seeking opportunities within your company where you can grow as a developer. Here is some reading that helped me on my path. Hopefully it’s useful to you as well. https://www.brentozar.com/sql/picking-a-dba-career-path/
What do you recommend to use as the back end? Microsoft SQL server? I have a bunch of stuff running in Access that I want to back end into a more manageable SQL environment, but am not sure what my best option is. What do you recommend?
Do you have a good link that covers what you discover here? This is basically what I need to do to take the next step in my learning process on my project.
Do you have a recommendation for reference material to build a SQL server that is accessed with by front end Access programs?
I've never heard of that, I'm all for free stuff though... however i've been a bit of a JetBrains fan for over a year, and I know that sounds contradicting. I'll try this Heidi out but I'll need to know one thing before hand ..... does it have a dark mode?
wtf..... not cool on the autonumber. Sigh time to find the time to finally convert my data scraping off AHK and my data scrubbing/table populating off Access and into SQL Server.
Segue to Business Intelligence, as an engineer, and you'll enjoy bigger salaries
what's a dark mode? is that like when your parents come into your room while you're watching porn and you hit a button that shows a speadsheet?
Ego much?
*Multiple copies* of a database? 
This is a fairly simple query... But what I don't understand is are you pulling all your data from just one table? You reference other tables, but only table 1 and your scenario. And what are the rejected conditions? Is the table one only populated with users that have been rejected? Or is there additional criteria?
Yeah, that's Microsoft's flagship database product. So it's a proprietary database (not free). 
I'd like to pursue a business analyst career. What were the steps you took to get where you are today? I have a degree in business administration, but many of my first 2 years were focused on information systems (info systems security AA) and I have a couple years of help desk experience. Currently in a role with light analyst responsibilities. Any info is greatly appreciated!! :)
Creat a CURSOR. While it loops increment/decrement the max children/depth values. 
You can set up your own free SQL server. I set up Postgres running on Ubuntu Server on a virtual machine which I access on my Windows laptop via SSH tunneling. Free cloud hosting is going to be harder to find I imagine.
Do you have a link to your original post with the criteria? I'm thinking that a recusive CTE would be the best way to do a hierarchy like that - and generally faster than the union all type queries. Using the CTE will get you all of the levels, which you can then order/sort as needed. Totally possible that I'm missing something as I'm on mobile.
Look into data science/analytics or data engineering. SQL is important there, and you probably know more SQL than most people in those fields. However, for data science/analytics, you would also need to learn statistics and programming in R or Python.
I'm a developer. I've been under the impression that a skilled BA makes more than a skilled developer. I suppose each shop handles that differently. I'm SQL Server so I can only speak to that. If I became a contractor and was willing to travel a skilled developer can make 200,000 +, but they work a lot of hours with static must reach deadlines. Part of that appeals to me, but part doesn't. I've worked with people that do that before. They have a high burnout rate.
I'm looking to hire people with exceptionally strong data and SQL skills in Portland. Base comp is around 120k however with bonuses you can easily double that. We're in fintech.
What if I’m honest about being a 4/10 and wanting and intro position? I remember telling my current job I was a 5 and then after a week was like “I lied, I’m a 2. Maybe a 3 now”. 
I’ll take an honest person that’s modest and willing to learn over someone who thinks they know more than everyone else any day. I much prefer it. One candidate was like this and he slipped through my fingers. Something that Brent Ozar said in one of the Office Hours Podcast has stuck with me. It was something along the lines of: “Do you REALLY have ‘x’ years of Experience, or have you just been repeating the same year for ‘x’ years? “
You can use a cursor to perform operations on individual rows in a row set. In the past I had used them when I needed to perform an operation that I just couldn’t figure out how to do using set based logic. There are lots of examples on line (I’d give you one but can’t think of a good one right now). 
You posted this twice mate
Isn't a cursor just a loop for non-incremental values?
I’d wager to say you’d probably enjoy a Systems Analyst position a little more, and your talents would be more useful there. That is, if you want to get your hands dirty with some coding/administration. Network/security/database admin are good to look for as well.
Must be using a window function with his cursor.
That’s how I felt when I left my internship. They kept offering me more and more to stay, but my mind kept yelling at me that five years from now I wouldn’t have the experience to actually move up, because it was a stagnant job with minimal skill requirements. I’m in a high demand job right now, and I still wouldn’t rate myself above a 5. In my review I stated I may be at a 5/10 for the next few years. But I do work for a place that’s super noob friendly as well, as my boss would agree with you and rather teach a hungry grad over a vet arguing that VB may be better than C# for the job. 
If you wanted to do something with the rows of a table one row at a time. There's almost always a better way of doing that IME. To answer your question : I don't
But when would you want to use this practically? I don't understand what you need to do one row at a time
So if you can't even think of one, does that mean it's really not that useful in everyday work? I'm trying to figure out if it's going to be something that I'm going to be using constantly every day...
Agreed. I started out developing in C# and realized after about 1.5 years that I wasn’t fantastic at it. Luckily my bosses at the time gave me an opportunity to work on ETL (SSIS, since I could still use C# that I knew) which set me up for my career. Can’t thank those guys enough. 
In most cases you should avoid using a cursor, as performance can be very poor. An example of a query which *might* use a cursor in some cases... Lets say you selected a list of all tables in your database (using INFORMATION_SCHEMA). You now want to loop over them backing each one up to a separate file. You could use a cursor to loop each over row, and then call the backup command with the correct parameters. There's various other ways to achieve this without cursors though, Im just trying to think of an example you can relate to. You don't (shouldn't) use them often, so just be aware that they exist. But also they're pretty trivial to learn when you do need them. 
I'm am definitely interested in a role like that. Thanks for the advice! 
Sure thing friend. System Analyst Systems Analyst Application developer System Administrator And since you’re in /r/SQL Data Analyst Database Developer Data Warehouse Developer Etc 
Agreed. I started out developing in C# and realized after about 1.5 years that I wasn’t fantastic at it. Luckily my bosses at the time gave me an opportunity to work on ETL (SSIS, since I could still use C# that I knew) which set me up for my career. Can’t thank those guys enough. 
So I had an existing SP that accepted one line of a table. A big hairy SP that had been around forever and sent some output somewhere that I was in no mind to rewrite to accept a table/set/multiple rows. My first thought was to load a cursor to execute the SP over and over again. 
a cursor is a person who comes in on your behalf and swears a lot i would use one to bitch about everything i hate, except i wouldn't want my name to be associated with hating them hence, the cursor is actually a form of surrogate key
A cursor is a method of looping over the records in a result set one by one. *Most of the time*, there is a way to operate on a result set in a set-based way which doesn't require the use of a cursor. That said, I use cursors daily because I have tasks that truly can't be done with set operations. Most commonly, I have to execute the same query/queries on each of many databases. I run a query which collects the set of database names that I need to operate on, then iterate over that set to run the query against each.
Honestly, yeah. Typically I've found that cursors are used by developers that picked up SQL after the fact. Even with stuff that requires operations on previous records you can get better performance with a recursive CTE. For a real life example, we had to calculate rent on a property based off the sales of the previous month and already paid rent (which was based off the sales of the month before that and the rent before that which was based on...) This is where you might typically use a cursor.
The only time I have really found it useful is when I want to scroll through a list of databases and run a stored procedure or something on each. There are still other ways of doing it though. 
Check out the top answer to [this](https://stackoverflow.com/questions/4222868/what-is-the-use-of-a-cursor-in-sql-server) question on StackOverflow for a pretty good explanation of what cursors can be used for and why they are generally not the best tool for the job.
master: -fruits1 -fruits2 -fruits3
that was too clever to be downvoted to -1, so just raised you back to zero with my upvote. i mean, cursor = curser, yeah kinda simple, but surrogate key, nice one. 
Sorry I’m going to hijack your question here. Wait... so should I NOT be using a cursor when doing a select statement through pymysql? My code looks like this usually: Cursor = connection.cursor() query = select * from table where Xxxxx cursor.execute(query) Data = cursor.fetchall() cursor.close() Am I just doing a bunch of unnecessary steps? This is all for a flask web app
Hey guys, thanks for guidance below, I will be sure to post the full Q as it may not have made sense
Why do I feel like these are questions for a HW assignment? ;) 
Mysql doesn't support cte's unless I use mysql 8.
Does mysql support that? What does that do? 
It could be but couldn’t you make it set based by using some type of windowing function? I feel like most problems can be solved in a set based manner (maybe with a while loop sometimes).
Yep...who don’t love Lead and Lag?
Cursors are useful. I use them from time to time. Usually they are fallback solutions when a set-based solution is unavailable. Usually the use of a cursor is preceded by "ugh, I have to use a cursor".
Maybe the people saying you don't need cursors only use SQL for cases where "for each matching row, do something with the record data that can't be done in SQL" isn't the normal workflow. Not sure what those cases would be, though.
No, I have used loops in the past and they need to be loops. In fact, just this last week my code was finally approved by the business to be moved to project, it's a massive chain of loops and it needs to be one. As I understand it a cursor is a way to use a loop for non-integer values, but I've never learned about them because whenever I need to loop non-integers I just insert them into a @table and give them an incremental PK, and then loop off that.
I would think the best way to act on the data besides how What is possible in standard sql would be to use your python app to work with it. I’m struggling to understand what the real issue with cursors is though other than those cases. I would hope the data gets stored for a reason, to do SOMETHING with it. 
You rarely wouldn't, but there are some other uses: 1) Calling an SP for each row in a dataset. This might not perform well, but if it's for a seldomly used report or a one-time extract where performance isn't that important, it can be a good way of reusing existing logic. As an example, I recently got a task where they wanted me to run a budget calculation on all the properties in our holiday rental database. The SP for calculating the budget was only rarely called in production when users actively did something, and not designed to be mass-called, but I wrote a script that looped (with a cursor) through all properties in the database and executed the SP. 2) Building dynamic SQL. You loop on a certain dataset to build and execute dynamic SQL. 3) Generating SQL for DBA work, for example looping through all databases on the server and generating BACKUP and RESTORE statments, setting an option/config on all databases, or other fun stuff.
Cursors are the WORST CASE way of executing code based on one result-row at a time. In almost all cases set-at-a-time is better, but there continue to be some exceptions - especially when dealing with external processes. Example: you need to send an email to all the recipients returned by a query, and then set the SENT flag. Set-at-a-time? Melts your email server or makes your ISP flag you for spamming. Row-at-a-time - let's you throttle the process. 
There are two kinds of cursors. Server cursors and client cursors Client cursors are useful for STL style workflows where you want to process one record at a time Server cursors are rarely needed. I cant remember the last time I used one 
Could you not do a lot of the same with a session variable? 
Hello future people! 
We receive data that is not well normalized (duplicate employee ids, overlapping email addresses), rather than allow the entire transaction to fail, the individual records (rows) that fail are skipped and logged. It's very easy to do that with cursors. 
Is the official doc incomplete? https://dev.mysql.com/doc/refman/8.0/en/keywords.html There aren't links for them, but they're typically easy to search for or easy to know if you're familiar with the syntax.
Who cares about MySQL 8.0 I mean what a stupid naming convention. They have been on 5.x for 30 years.. why do they feel the need to use 8.0? Why not 6.0? It's retarded. 
6 and 7 do technically exist, though. &gt; Years ago, before the Sun Microsystems purchase of MySQL AB, there was a version of MySQL with the number 6. Sadly, it was a bit ambitious and the change of ownership left it to wither. The MySQL Cluster product has been using the 7 series for years. With the new changes for MySQL 8, developers feel they have modified it enough to bump the big number. [Source](https://opensource.com/article/17/2/mysql-8-coming)
As a side note, a function already exists for this in SQL Server which is awesome for this kind of thing. &gt; EXECUTE master.sys.sp\_MSforeachdb 'USE \[?\]; EXEC sp\_spaceused' 
I'm on mobile so it's a little bit of a pain in the butt to search them all, I was hoping for the same thing but with links. Damn. 
Query 1 look up INNER JOIN to link the tables, then WHERE to filter it to the id you want 
Thanks this helps a lot and steers me in the correct direction 
Just to add this would be for both Quiries right? I managed to use the Inner Join for Query 2 with success
&gt;It feels weirdly disconnected (metaphorically speaking hehe) from the actual database, and most importantly, no matter how hard you try, you can't be sure that the scripts match what is actually live on the database. `Schema Compare` You're welcome.
While that stored procedure does exist, it is not without faults, it may disappear in a future version, it's undocumented and unsupported (Microsoft has closed at least one reported bug with "won't fix") and rarely do I need to query *every* database on an instance. So it's really not as awesome as it may seem.
So you should really get Git. We use Atlassian's BitBucket+Sourcetree (GUI for GIT) as our version control for SQL scripts. We then developed a tool that aggregates the scripts in different folders by specified databases, then "deploys" (Appveyor) to target environments. Works really well. 
Sure, but that requires actively doing it, and doing it very very often in order to ensure you're working on the latest code, if you're on a team for example.
Yes, I've used it. SSDT has been my main method of dealing with SQL projects since 2008 or so. I've dealt with scores of projects and I've run the comparison tool thousands of times. That said, I've demoed the tool for developers a dozen times. Most of them just stare at me. A month later, they release broken code. My biggest gripe with SSDT is that SSDT does not integrate with Linq. When I feel "disconnected" from a tool or that I am "just not getting it", the problem usually falls into one of these categories: * Trying to force the new thing into working the old way. * Lack of experience or practice * Bugginess (debugging Powershell in VS Code lead to little besides missed break points and crashes until late 2017. Things are much better now.)
Sort of like committing code to GitHub or any other repository? I am not sure what you would expect here to be honest. I am genuinely curious as to what your ideal scenario would be here. When you add a SQL file to VS it automatically restructures your database? When someone makes a GitHub or VSTS commit it auto updates the db schema? This seems to have serious downsides. At least with a Schema Compare you can tell in 2 seconds if you are up to date with the schema or not. Seems pretty good for a free solution. You could always look into APEX SQL solution or another paid software. 
Ah good, Erm query 2 is a bit more complex (fairly new to sql myself). My advice would be to build a similar query first, which would return all results. Then you need GROUP BY to group them by postal and to count the results. 
Q1 SELECT t1.fields, t2.fields, t3.fields FROM Table1 t1 INNER JOIN Table2 t2 ON [t1.id](https://t1.id) = [t2.id](https://t2.id) Q2 SELECT t3.postalname, COUNT(distinct [t2.](https://t1.id)Rejected\_Parents) FROM table2 t2 INNER JOIN Table3 t2 ON [t3.](https://t1.id) Postal\_ID = [t2.](https://t2.id) Postal\_ID GROUP BY t3.postalname
Basically you create a temp table and step through it one row at a time loading the values into variables. I mainly use then when I'm creating stored procedures to do something with data. e.g. I have an import process that loads data into a staging table and then runs the stored procedure. The stored procedure takes the data from the import table and uses it to step through the records one at a time, applies various checks and then either inserts new records or updates existing records. As always with SQL there's often more one way to do what you need doing and the above example could be done without using a cursor. 
I suspect a "cursor" in PHP is different from a cursor in plain SQL.
&gt; either inserts new records or updates existing records. There's always `MERGE` if you're feeling masochistic
Sounds interesting, can you explain further? You make your schema changes in an editor (Visual Studio?), then use the tool you made to figure out the diffs and then "deploy" the change script on the target database? Or do you just make single change scripts by hand (for example a script that adds a column to a table), check them in, and then run them on the target environment? That's how we used to do it on my old team. Downside is that it requires a lot of manual digging to find when and why something was changed, but that's not always so important, depending on the setup. I find the history is mostly important if you have an SP-driven data layer, with lots of (shudder) business logic in the DB. The way my team has it setup, there is literally a complete clone of the database schema, one CREATE script per object, on our TFS source control. The point is that every time you manually change an object in the DB, you script it and check it in, so that we have version history on the DB objects, so we can see how a table or SP (for example) has evolved over time. However, it is far from perfect, as it requires doing the manual scripting every time you change something (and remember to do it in the first place), and naturally the timestamps will be off.
&gt; When you add a SQL file to VS it automatically restructures your database? When someone makes a GitHub or TFS commit it auto updates the db schema? Well, there are two sides to it: 1) Syncing from DB to source control 2) Deploying from source control to DB Neither should be automated due to the huge risks, but the former should really be done every time you want to start working on editing an object (depending on your team size, if you're solo it's no problem). In my experience, the built-in schema compare in VS (2015) was really crappy and gave a poor overview. I don't know if APEX or RedGate can schema compare and sync to a Visual Studio data project under source control?
Never tried it TBH. Always just gone with the certainty of checking conditions (as sometimes it's not always the field that's being updated that needs to be checked) and then updating or inserting. Might give it a whirl though with some test data to see if it might come in handy at some point. 
What is in the repo is the latest code. Trust the process.
Used it all the time in my past life. Does what it says on the tin. Who cares if it's cumbersome, the alternative is much worse.
The Red Gate development toolbelt blows all of the built-in MSFT tools out of the water, IMO. Of course, it's not free. If you can talk your company into paying for a license (or afford it yourself if you're a freelancer), it's WELL worth it.
I've switched over to Ssdt after years of using ssms for dB development. The change is different since you are doing declarative sql and letting the build system determine the migration steps for you. I really like that as I make a change and run build it tells me of any conflicts immediately, additionally leveraging the built in unit testing of VS is much better than tsqlt. If everyone on the team uses Ssdt for dB changes and deployment is handled through you CI server you should never have an ssdt project different than the live db
Ahh maybe so. I don’t know PHP at all
Then why are SQL junkies so anal about it? Some guy said you must know about cursors and how they work to be an expert, but they're never best practice???
Yes, I use it religiously! We have all databases in source control and use a gitflow strategy. From UAT to Production releases are done automatically using dacpacs and some PS code - therefore changes should never be made directly in UAT onwards that are not in source control (permissions are locked down for this reason). It works really well. 
Ahh, didn't think of this. Thanks a lot!
Does php 6 exist? 
I mean, it was only internal but yes, I believe PHP was undergoing development as 6.x for a time. I guess my point is that it's silly to get upset over version releases when version numbering itself is a fairly subjective affair. Hell, plenty of Japanese or Chinese-based shops skip 4.x all the time because of how ominous the number is culturally.
I work with this type of setup, it can be a little confusing sometimes but your stmt..can't be sure that the scripts match what is actually live in the database...is something I've had to deal with. The simple answer is whatever is in source code control wins. End of discussion. Tfs builds our dbs from what is in source control, if something doesn't match its usually because a dev put something directly into the db. If they loose that change when the db(s) are updated then too bad.
the background is dark and the text elements are of a lighter color. It's easier on the eyes.
Looks like only annotations, all, choice and sequence can go inside a group. Like the error says, element is not a valid option.
You may well be developing on a dev env to test stuff (which you then commit), and you may well have some sort of CI onto the dev env, but you should never be syncing from the DB to your repo! It goes the other way.
You use it for developing databases. It can be cumbersome, but I find it far preferable to dealing with a repo full of scripts which don't have any assured integrity, unit testing framework, means of managed updating etc. It doesn't matter whether the repo project matches the dev env, because the repo is canonical. By all means work on stuff in SSMS on dev, but commit it afterwards. It entirely addresses your query regarding SP history. Just view the repo commits made to the SP.
I've used database projects extensively for the past 5 years. If you half-ass it, it's pure garbage. If you 100% commit to it, it works quite well. You must design your methodology to work with database projects. You can not design database projects to work with your methodology. &gt; Any good tips for version control/history with MS SQL? Database projects. ;)
SELECT COUNT(*) AS TotalSales FROM ....
Much appreciated, this worked as intended
In MS SQL, you can also drop the 'as' and get the same result: SELECT SUM(columnName) ColumnName FROM TableName
In Oracle SQL you can also have it as SELECT SUM(columnName) “column name here” FROM table 
Looping across databases with dynamic db name variables is the only good use for cursors imo
In SQL server you can have it as SELECT SUM(columnName) 'column name here'. The downside is you can't reference such columns by name in ORDER BY.
A quick google search brings up a number of sites with a (failed) query injection attempt for that number on SQL based website so if I had to hazard a guess this is an automated initial probe to see whether or not there's any reason to try attacking the site proper.
And also Select columnname = sum(columnname) from table works too 
Interesting, I have plenty of these logged but never noticed the numeric pattern until now. Someone has identified the [responsible malware](https://github.com/bediger4000/php-malware-analysis/tree/63e122348f3721192ea40c9a58affcf33402a11f/188.120.231.151-2018-01-07a), here's a link directly to [the function](https://github.com/bediger4000/php-malware-analysis/blob/63e122348f3721192ea40c9a58affcf33402a11f/188.120.231.151-2018-01-07a/dc1.php#L5754) with the `2121121121212.1` value hardcoded. The script also uses `999999.1` and I see a bunch of those in the logs, too.
You can also order by column indices.. ORDER BY 3, area, 2 It's not a great idea to do this, as it means the query can be more difficult to maintain. But it can be useful in some rare cases
I genuinely mean well.. but the question you asked is extremely straight-forward that a simple google search would have told you. Or if you read the first few pages of any tutorial or documentation. Consider doing a SQL tutorial end to end. [w3schools](https://www.w3schools.com/sql/) has a decent one although there are plenty more that a google search will tell you about.
We do what you've suggested, however, all SP's (and usually all views) are drop/created every time the "build" script is run on the server. This allows for seeing what changed each time, as the version history is checked into Git. 
Sorry, I misread your post. Replace "PHP" with "pymysql" in my post, same holds.
Loop over each row in a data set in the database itself (query, stored proc, etc) - always a code smell, usually bad. Loop over each row in a data set returned from the database to the calling application - normal, common (although LINQ is a thing on some platforms). TL;DR: If you're writing a loop inside your SQL code, you *probably* need to rethink your process.
Speaking as your DBA: Looping over data in your python app? I don't care, it's your app server/desktop, do whatever you need to do. Looping over data inside the RDBMS instead of using a set-based approach? We're going to have a chat.
And followed by "I feel dirty now"
Thank you very much for the detail. Back in the office so that I can address this properly. A few of the reasons I went with linked tables: - Multiple drop downs fed by server side tables (did this so when new values are added I don't need to push out an update to the front end) - Automatic saving of values in the DB. Our sales team is more concerned with lost data than incomplete records. There's a process in place for dealing with incomplete records. - Parameterized execution of stored procedures which I was not able to do with a pass through query. I had to build the SQL string in VBA or have a different PTQ for each stored procedure. The latter is not a big deal, but I was really aiming for efficient code, less duplication of effort and make updates quicker. I'm happy to make changes, and will need to, to improve performance when we switch to Azure. Eliminating the VBA connections is ideal though it will mean spending time to change code. Without a linked table, is it possible to automatically save new records to the DB and how would I accomplish that? Thank you very much, particularly your feedback regarding storing credentials in the Access DB. I don't want to store the passwords anywhere except the grey-matter of my users.
Ah ok. Pymysql cursor just represents the dB cursor to manage a data fetch
Please elaborate... I’m fairly new to all this. I don’t do any loops within my sql, but I do stuff like select, update, insert, and delete, - and then will use python to loop through data if it needs to be mass altered. Is this the right approach? For example: I do this Select data from table The table data looks like this: Name | some large number So then if I want to change the large number to be represented by a smaller one (for example, 1 “million” instead of 1,000,000) I will use a python loop like this For x in data[‘number’] X = x/1,000,000
As long as it acts as a disconnected result set and doesn't hold the connection open until you've finished crunching the data, it's OK.
As long as your database access library fetches the full result set and disconnects before your python code continues, it's all good. Holding the connection open while you're looping over the data may induce blocking or other performance issues. If you're doing a "mass alteration" of data in the database, find a way to do it via a set-based operation and run a single update/delete query - not thousands of individual rows getting updated in rapid succession via a loop in your code.
I close the cursor before crunching data but I might leave the conn open... the reason for that is I have multiple functions that load data for a certain flask web page and I’ve been passing the connection through them so I don’t have to connect and disconnect between function calls. Example: App route homepage Def homepage() Conn= connect() D1 = getdata(conn) D2 = getmoredata(conn) Conn.close() Return render template (home.html, d1= d1,d2=d2) Then in each function I have something like this Def function Cursor=conn.cursor() .... ..... Cursor.close() Return data Is it bad that I keep the same connection open through the function calls? Having the function calls separate is great for readability but if I don’t keep the conn open through them it takes a lot longer to load. Ps, thank you for teaching me all this :)
Does it make a difference that the web user is a read only connection? I thought that would allow for multiple users to be reading data without locking No mass alteration here - I do that manually if I need to in MySQL workshop
Unless it's a common but of code available for download by script kids, or you're being probed by one person using a botnet. Depending on the nature of your business/websites, it may be worth blacklisting certain regions of the world by default. I work for a local service company, so we automatically block any international incoming traffic, which greatly reduces our malicious requests. 
&gt; Does it make a difference that the web user is a read only connection? It depends on how your database and connection are configured.
I have one program that is a write access user to the dB and updates a table every 60 seconds. The web client is read only user on that same table. I figure that way there are less locks. 
Closing the cursor should be sufficient, I misspoke re: closing the connection. &gt;I’ve been passing the connection through them so I don’t have to connect and disconnect between function calls. If you use connection pooling, you can reduce the overhead of opening/closing connections without having to pass the connection object all over the place.
Again, it depends on how your connections and database are configured. Are you using optimistic or pessimistic locking? What transaction isolation level (for example, my biggest systems use `READ COMMITTED` and as a result, readers will block readers)?
Ah ok. Thank you. For connection pooling - would it be sufficient to store the connection like this? Def connect (): Conn= pymysql.connect(*connection info*) Return conn Currently I have that def and I do this at the start of a bunch of sql functions Conn=connect()
Honestly I am not sure exactly. I would have to check the server but it’s pretty much a default innoDB MySQL DB Thank you again for all your help by the way :)
I honestly don't know, I've never used python.
From what I can tell from googling pooling - this is sufficient for my needs. Out of curiosity, where did you learn so much about this stuff? I’m looking for some ways to buff up my expertise 
Link doesn't work.
I mostly deal with pulling data down, and for that pass throughs are the way to go. I usually go with one PTQ and just swap out the SQL as needed. For pushing data in, I can't think of a way other than a linked table. That's a different situation though. The only thing I really don't like about the PTQ thing is when the login fails, or the connection times out, they get a very generic error message, regardless of the reason.
Ok. I was hopeful, but perhaps I can reduce the number of linked tables and use PTQ for the dropdowns. It's going to require a fairly significant rebuild unfortunately. I'll keep digging into options for pushing data and if I come up with a compelling solution I will try to remember to share it back.
Because, as with most tech fields (and, for that matter, most non-tech fields) there's always some gatekeeping neckbeard who likes to feel superior. You'll always find someone delighted to tell you that you can't be an expert because you don't know x, y, or z - despite the fact that you know everything you'd ever need to do your job. People for whom "expert" is an absolute, rather than subjective, term - and, of course, for whom the criteria "expert" always seems to conform to exactly what they know at any given time...
Generally speaking, you want to *limit your data set* in SQL, but not *act on them*. You act on them in your calling application. This is pretty much standard practice, and is exactly what you're doing. The problems come when you try to loop within your SQL code - eg you're acting on the data set within SQL, particularly when you're holding the result set open to call back to the code. This is usually bad, although not always: that's why the user above called it a "code smell" - a code smell being something that should attract your attention (like a smell) for you to decide if it's good, bad, justified or not etc. A code smell is not automatically wrong - it's just something that should catch your attention in order for you to ensure it's justified and sensible. If you're doing anything in SQL that is not set based, you need to think carefully about why you're doing it. In most instances, this will use a loop - so if you see a loop in SQL, stop and think
Why are you allowing schema changes in your database that don't originate in source control? You can manage this fairly easily and the risks of automating it are much lower *if* you only allow changes to "flow" in one direction - source control to database, most likely.
I agree that cursors are almost always awful but your example would be better fixed in the app code, add a throttle (ie records per second) so that when the external thing changes to no longer be slow/limited you can fix your side via a config change rather than re-write.
Reformatted.. SELECT C.Client_Number, Membership_Status, Customer_Duration, Joined_Year, Gender, Points, Joined_Date, C.Client_Type, Civil_State, sum(Total) as SalesTotal, sum(Discount) as Discount FROM Client_Detail C LEFT JOIN Sales S ON (C.Client_Number = S.Client_Number) GROUP BY C.Client_Number - SELECT C.Client_Number, sum(Total) as Sales2016 WHERE Year_of_Sale = 2016 FROM Client_Detail C LEFT JOIN Sales S ON (C.Client_Number = S.Client_Number) GROUP BY C.Client_Number; - SELECT C.Client_Number, sum(Total) as Sales2016 WHERE Year_of_Sale = 2017 FROM Client_Detail C LEFT JOIN Sales S ON (C.Client_Number = S.Client_Number) GROUP BY C.Client_Number
Perhaps some bounds/validation checking, that number would guarantee it would fail on an int32 column and if it was accepted then it would imply the database was using numerics in a string field (or a large decimal) as an ID which would make the code more likely to be injectable through the app code as you can make the app code store ' DROP TABLE USERs;-- in a string type variable but not a int or a decimal or long etc.
Is this what you want? SELECT C.Client_Number, Membership_Status, Customer_Duration, Joined_Year, Gender, Points, Joined_Date, C.Client_Type, Civil_State, S.Year_of_Sale, sum(Total) as SalesTotal, sum(Discount) as Discount FROM Client_Detail C LEFT JOIN Sales S ON (C.Client_Number = S.Client_Number) WHERE Year_of_Sale IN (2016, 2017) GROUP BY S.Year_of_Sale, C.Client_Number
In the first query you have: &gt; sum(Total) as SalesTotal, You can also do 2017 with: &gt; sum(case when Year_of_Sale = 2017 then Total else 0 end) as Sales2017 Similarly with 2016 
Try DateDiff [Oracle](https://docs.oracle.com/goldengate/1212/gg-winux/GWURF/column_conversion_functions011.htm#GWURF780) [SQL Server](https://docs.microsoft.com/en-us/sql/t-sql/functions/datediff-transact-sql?view=sql-server-2017)
Try trunc(date_col_1)
try using datediff instead. 
No it isn't. Cursors are nearly always a bad idea. Find another way before you follow bad practice.
Why not just call the stored procedure directly from Access? If you are concerned it will fail for some reason, you could handle any error and push it into a local Access table instead. Then have a process which runs regularly and calls the stored procedure for each record in the local table, before deleting it
I've seen examples where people take their local access table and XML-ify it, and send the XML to a procedure that parses it back out. That way you send a big XML string to azure rather than a bunch of insert statements which can cause slowdown. You might have to turn tracing on in MS Access and do some testing. Here's a pretty long video on some of this: https://www.youtube.com/watch?v=ztHX7JkIIeU I think the presenter is being translated through the speaker.. which slows things down. But the presenter does go into tracing MS Access at about the 43 minute mark. 
Blogspam trying to create a requirement that does not exist. I sure as hell would not want to run any code generated by some numpty with a data science degree who is trying to solve a problem that does not exist to generate ad revenue. 
&gt; you're being probed by one person using a botnet. That's very unlikely since the probing is repetitive. The pattern looks more like independent actors. If it was a botnet it would be more likely to send each injection attempt from a different IP. &gt; I work for a local service company, so we automatically block any international incoming traffic, which greatly reduces our malicious requests. If it works for you that is OK. I personally consider this the wrong approach. Getting an IP from a specific country is not that hard. I live in Switzerland and my provider occasionally assigns IP addresses that are assigned to the Netherlands. What I ended up doing was implementing a system that blocks a malicious IP for an hour. This is enough to send all other attempts into the void. but doesn't hurts whoever gets the IP next.
If you use office 2010 or earlier.. you can setup an access data project and the import from one access file to the ADP without linked tables. Or you can do the opposite.. from access you can import data from SqL. Access data projects rock. If you want my help with this.. I'll give 30 minutes free and then it starts at sixty per hour. Let me know if you want my contact info. And hell no this cannot be done with bullshit office 2013 or newer.
Yeah it's really a shame they dropped ADP from new versions. 
This was it, thanks!
Have you checked out sqlzoo?
I have, the only problem is I think you need to work on their website and I kind of wanted to try out pgAdmin as that's what I've been using with Udemy course. 
Ah ok. Thanks for the thorough explanation. Starting to make a lot more sense now
Download the Stackoverflow database and have some fun playing with the mass of data!
That's pretty cool, but I think I'll save that for processing more than one record at a time. Thank you very much for sharing it with me.
Ok, so on error it ends up in the temp table rather than adding them by default. That's probably a better approach. However, I guess edits need to be done through a stored procedure as well. I may need to stick to linked tables.
Yeah it’s really for bulk operations but there are plenty of those to be found. 
I try to encapsulate everything into stored procedures, so you are just working with an API, rather than running all sorts of adhoc queries (this is especially important when you have multiple software and versions running against your db). So yeah, another stored proc for editing would be best practice. You could also consider keeping your existing SQL Server, and have Access manipulate that. Then set up replication from your SQL Server to the Office247 one (although I haven't used Office247 so no idea if this is possible) . How does your existing system work if your local SQL Server was unavailable? Presumably you would face the same problem already, in that it would fail to insert?
I started using ADP with 2010, but abandoned it when Microsoft did (we adopted 2010 in 2013). Knowing it was deprecated I didn't want to invest time into something that would just end up leading to another migration later. Thank you for the offer!
MySQL (dead simple install on your desktop computer) along with HeidiSQL, comprehensive front-end app... and it's free
SQLite and PostgreSQL
Linked tables, but the server is local so performance issues are minimal on a gigabit network. We've not had outages, except when the power was out. Honestly, I don't know that the Cloud solution will pose much of a performance hit, but if it does I really need to start development on a new solution before that becomes a problem.
&gt; Why are you allowing schema changes in your database that don't originate in source control? That's just the way I was introduced to the concept by my team. Work in SSMS, and when you're done, check in the script on source control for the sake of history and making the final deployment diff script. Sounds like we're using it incorrectly though, but I don't feel comfortable using VS to edit SQL. A loooooot of my work is debugging SP's, so there's no way around using SSMS as the main development tool.
You *can* work that way. But you can't work both ways at the same time. Otherwise you'll be doing a lot of manual merging. Do one or the other - not both - and be consistent about it. You have to decide what the single source of truth is - source control or the central development database - and then tailor your processes around that. Thing is, source control is really good at managing and merging changes made by multiple developers to a single file. SQL Server, not so much. If you &amp; I are both working on a stored procedure in a shared database, it's only a matter of time before one of us clobbers the other. &gt;A loooooot of my work is debugging SP's, so there's no way around using SSMS as the main development tool. Why can't you write your stored procs as ad-hoc SQL in SSMS, and when it's working properly, copy it into a stored proc in Visual Studio?
Pretty much anything. This is a very strange question.
do you have any examples of problems i would be running into on the job? 
They all have their own little things. If you are looking for a skill to sell on a resume I would go with Oracle or SQL Server because many of the mainstream companies hiring for this will have these installed. MySQL is free so that is another alternative although mostly startups use that.
do you have any examples of problems i may run into?
 &gt; performance ... &gt; I'm considering using a local Access table to store the new record and a Save script which will push the data in using a stored procedure. I'm not sure that is the most effective way to accomplish this. [Pass Through Querys](https://support.microsoft.com/en-us/help/303968/how-to-create-an-sql-pass-through-query-in-access) are much faster than Access tables linked to an external data source. If you have a large table or if your data is growing... I recommend coding Access with a Pass through query. There's a lot of little issues in the differences between how Access treats tabular results from a pass through query but if you know VBA it is completely fixable. 
You kinda need a development environment really, I would not leave it until the day it goes live with no benchmarking, and then suddenly switch over to Office 247. The biggest issue will probably be latency (round trips). It depends how many round trips your app (and the linked tables) does. It will be difficult to test fully though without just testing it against the remote server 
Literally anything involving relational data stored in a relational database.
Could be anything. Debts on accounts, arrears reporting etc
SQL used for pulling data out of relation databases, putting data into relational databases, and analysis of data in relational databases. Your role will probably use it most frequently to pull raw data into analysis or reporting tools (excel, tableau, etc.) As you become more fluent in SQL you'll find that you can actually do a lot of analysis with SQL queries themselves, and it's often (but not always) efficient to do so. At first though, you'll want to use what I would consider the primary operations of SQL queries: filtering data, aggregating data and combining data from multiple tables. These operations are almost always done most efficiently with SQL against your database(s). The good news is that SQL is pretty easy to learn, so no need to be scared. 
Thanks for your response. Point taken, and no offense, on not exaggerating what you can do - I don't like to say I know something unless I really know it. And even with what I know, there are certainly features of SQL I'm not familiar with and I always push myself to learn more about those (for instance, lately I've been digging into the MATCH_RECOGNIZE clause). I was referring to SQL in my post. I've done a fair share of PL/SQL (writing, reading, and debugging) and have written code touching on all the concepts you listed, but admittedly my knowledge in that area is not as strong as my plain SQL skills. That's something I'm currently working to improve. For your job do you work on both SQL Server and Oracle? Have you found that knowledge of both is more appealing to employers than just one or the other? At my company, development teams generally work on one or the other.
Not knowing how to write a query for the information you're needing, whether it be analytic functions, PL/SQL, date/time formatting etc. This is such a broad question, it's really hard to answer. Just start practicing. SQL is a very logical language and will make more and more sense the more you use it.
I have this same problem. Every database class I have taken or books read always start with Ch 1 : Select Statements. How about a "After you download, this is how you set up the database and login information instead of use the built-in admin so you can gain experience doing that" chapter.
I have watched a lot of youtube videos trying to find it for oracle and no luck, or at least nothing that is up-to-date or doesn't require to have a server or some other connection already
It depends on what you're right. Just today I had my year-long project promoted to production and it is a series of 31 stored procedures that are daisy chained together. Some tips that I used for it are as follows: 1. Give them an intuitive naming schema that allows for future growth. For example, my main process is 000a_Global, and the second process is 010a_Description, then the next step is 020a_Description, etc. Around step 090 I have an 090a, 090b, ... 090k. This might look weird but it allows me to insert new procedures in the future while still keeping a vertical hierarchy, so each procedure will always be in the list where it executes in the process. If I need to I can make an 001a, or an 002b, etc. 2. Document the shit out of everything. If you open up my global process it will tell you what it is, and what main procs it triggers. Those procs then trigger other things based on certain conditions. All of this (high level ) is spelled out in the global sproc, as well as who wrote it (me), as well as an end-to-end description of the entire process, the ETL, which email address you send the alert to, which network drive you save the file to import, etc. 3. More sprocs are better than fewer sprocs that are longer and do multiple things. In the very very beginning of this project when it was still a POC I had everything happening in 1 super long (several thousand lines of code) sproc. It worked just as well as it does today, except now it's 31 sprocs. This was decided on after consulting with my IT group and we decided as a group that doing this would be a best practice so each sproc is unique and does one thing, and one thing only. It makes it more cumbersome, and it has expanded the total number of sprocs on the list by 25%, but it also makes debugging easier if something fails. 4. Design error handling and descriptive logging. All along the way my process will insert a row of data into a custom made logProcedure table that I built for it, it describes which step executed, the time it executed at, which "session" it is from, etc. This lets me answer lots of questions like, "how long does it run?," "when was it last run?", "Have my staging tables been updated recently?," etc. 5. Make absolutely certain that your sprocs are re-executable. A simple example here is to INSERT INTO tables rather than INTO tables. If you're dropping something then have a step to check if it exists at the very beginning, and if so to drop it. Imagine if your process is 60% finished and someone accidentally kills it. It should be able to be reran without any inputs. 
It definitely does have it's faults (like not properly recognizing database name modifications). Most of the time I need to do something to a large batch, I'll just use PowerShell / Invoke-Sqlcmd in a loop. Makes for much easier checking/breakpoints if you're looking for issues and reading/writing to a file is quite seamless. Also, significantly simpler to use with PAM.
If you were referring to mostly SQL, then I’d advise that you can take that to most if not all platforms. You shouldn’t feel constrained over which platform you’re most familiar with. Yes, I work with both daily and in equal amounts. It is a bit odd to balance as I used to “think” in SQL server and just translate to ORACLE, but I’ve had to force myself to “think” in each individually depending on which one I’m in, as their engines are a bit different. Knowledge of both has been more appealing to both my current employer and other employers, but I’ve found it’s mostly those that are transitioning from one to the other that are most interested. That or they have some odd 3rd party vendor app that runs on one of the two, and but the vendor no longer wants to support. When I look for people, I prefer those with knowledge of both, but in my search that has been extremely difficult to find. Again, if your current employer has any room for growth in the data &amp; analytics development space, I’d highly recommend staying and moving into that space. Unless you go for a consulting firm (or willing to relocate to an in-demand area) you’ll be hard pressed to find an organization that will match the salary you’re wanting.
&gt; Configure a mysql/maria database using schema.sql Log into mysql using the command-line as an admin user, and create your database... CREATE DATABASE MyDatabase; quit the mysql client. Now import your schema.sql using mysql commandline client.. mysql -u &lt;username&gt; -p MyDatabase &lt; Schema.sql
&gt; Does this work? no... you cannot say `AND WHERE`
You don't want to provide a password on the command line, and if you're redirecting a file into standard input, the client prompting for the password isn't always going to work. Hence, credentials in a file (With appropriate permissions so it's not world-readable).
If you are using this for and assignment,and the assignment is not specifically *about* configuring a RDBMS, then don't waste your time. Use every hour to concentrate in the deliverable for your project, and skip the stuff that your instructor will not see. I say you use [SQLIte](https://www.sqlite.org), which uses no passwords and [requires no DB server.](https://www.sqlite.org/cli.html)
your understanding of what a view is, is correct a view is a saved SELECT statement that has all the joins built in, so that nobody has to know the intricacies of their relationships, they just write queries against what looks like one simple table
&gt; You don't want to provide a password on the command line The -p syntax prompts for a password. To pass it inline would be -p&lt;password&gt; and I agree, this is very poor practice &gt; Hence, credentials in a config file (With appropriate permissions so it's not world-readable). Yeah, that's not so bad, so long as it is has permissions to be readable only by this user I think I even use my.cnf for this in some cases myself. I just wasn't familiar with what mysql_connection.ini actually was, maybe its the same file, except has been renamed now, but it kinda sounded like a random config file just lying around 
How would I connect two or more where clauses? Does this work? Select * FROM Employee WHERE emp_id = '&amp;fired_emp_id' Where dept_id = '&amp;fired_dept_id'
Thanks- it's not a course assignment, just some advice for a research project that I'm working on.
What are the features/benefits of BitBucket and Sourcetree? I'm looking a bit into GUI for git. Do they overlap?
MS SQL Server Express is free and part of the same ecosystem as Access, so it's an obvious choice. I haven't kept up as much with Access, but there's probably some version of the Access Upsizing Wizard still available. There are other free options available, but I think you'll find more and better support for the Access/MSSQL pairing.
WHERE x AND y
I started out in a help desk / light analyst role. Although I'm officially a business analyst, a lot of things I do fall more under the systems analyst role. Here are some things that I think will help: Be curious. Express interest in learning new things and expanding your skills. If you're wondering how certain components of your application fit together - ask! If you want to know more about the business side of things - ask! "Can you give me an overview of such-and-such product? I'd like to know more about it." And ask good questions during and after the overview, etc. Also, spend some of your downtime just pondering what you've learned - maybe you'll get an idea about how to do things better. Take ownership. If others on your team frequently have to troubleshoot/resolve certain issues, ask how they do it and ask if you can assist. If you're always asking someone to provide you something, such as log files or a data extract - ask how they get it and whether you can do it yourself. Volunteer. If your manager asks if someone can help out with a certain project, volunteer. If it's for something you're not familiar with, volunteer and mention you are very interested but you may need some guidance/assistance to get started. Likewise, if you hear about an upcoming project that sounds interesting, ask if you can be involved. Even if it's just, "Can I listen in on the meetings, I want to learn more about such-and-such product" or "I want to know how this project will affect our end users." Look for inefficiencies. If you think one of your job duties takes longer than it should, take some time to figure out how to do it better. Could be something like an Excel macro, or a shell script, or even just keyboard shortcuts or learning how to use the tools you use more efficiently. Something I did early on was use AutoIT to speed up data entry into an internal web application. It took some time to learn how to script it correctly, but once I figured it out, tasks that often took hours were whittled down to minutes. Help. As a business analyst, you will often support a development and testing team. Figure out how to help them do their tasks better. Could be anything from coordinating meetings, to doing simpler tasks (e.g. writing a simple insert or update statement so they don't have to worry about it), or pitching in with the testing.
ah, that makes sense, thank you!
Maybe something like this: select dupe_count, count(*) as "count" from (select pid, count(pid) as "dupe_count" from r_ptest_1 group by pid) as s1 group by dupe_count order by count(*) desc;
Could you elaborate #5? I think I know what you're getting at but do you have an example?
I had been professionally administering Db2 for over a year before I actually had to install it. If you are interested in Db2, there is a few-click install of the free developer community edition using docker: https://www.ibm.com/analytics/us/en/db2/trials/ Or I also blogged about how to set it up manually on an ubuntu VM: https://datageek.blog/2017/01/19/building-a-sandbox-vm-for-db2/
Piggybacking off notasqlstar, I'd like to reiterate... -The importance of having a solid naming convention. -Document, comment, note! I can't tell you how many times I've open procs that's I've written years ago and I just stare at it thinking "what the fuck is this?" You don't have to write paragraphs. One or two sentences near each code block should be sufficient to jog your memory years later. -Generally procs should be broken up for the sake of organization but there are exceptions. For special cases or to build process around archaic programs it's ok to have lengthy procs. -Depending on your environment, it's usually a good idea to just go ahead and fully qualify all objects. Someday, somewhere, you'll need to migrate servers or you may want to move a single DB to another server. Having fully qualified names can help ease the process.
Obviously, you're not a golfer.
"unpivot' is a modifier for the FROM clause. In your case, columnX columns should be replaced by colornames and medianvaluea column 
So the next major question is that can security roles be maintained in a view? If certain columns can only be viewed by certain roles, can I script that in to be maintained in the new view... or am I just adding new security to the new table In fewer words can you add security roles to a view? Or does a view inherit the security of its parent columns/tables?
OK, so at the top of the definition of the stored procedure, you want a comment block. Denote the author, date, and description of what it does. In addition, add an example execution statement within the comments so that someone can debug it without having to determine the appropriate input parameter values.
I would suggest that's because you are buying SQL development books. What you want is not a SQL development book, but a database administration one for the DB you are interested in. Or, you know, you could look at the documentation.
@ichp When you say, "columnX columns," are you referring to (\[ColumnAlpha\], \[ColumnBeta\], \[ColumnCharlie\])? Does the following accurately represent your recommended changes? INSERT INTO @TableSampleMedianValue (MedianValue, ColorName) SELECT ColumnAlpha, ColumnBravo, ColumnCharlie FROM MedianCTE UNPIVOT ( Values FOR ColorNames IN (ColorName, MedianValue) ) AS MedianValues RETURN END
I'm confused by your comment - 'doesn't require to have a server'. How exactly are you expecting to configure a server... without a server? The documentation for Oracle is freely available. 5 sec google: [Connection .ini files in Oracle](http://lmgtfy.com/?q=connection+.ini+files+in+Oracle)
I've been under the same impression, hence the reason for my post to see what others thought. I do enjoy a BA, but I've been gravitating toward the more technical aspects of my job for a while and I just really enjoy working with SQL. I don't think I could do the travelling contractor thing, though.
You should be able to. See [more](https://www.ibm.com/support/knowledgecenter/en/SSEPEK_10.0.0/seca/src/tpc/db2z_rowcolaccesscontrol.html).
Did you have a chance to try it out?
My #1 rule: If you're using cursors your doing it wrong.
 &gt;Looking at the execution plans also shows that this is a good practice. It's dynamic sql. Don't do dynamic sql. 
Thank you! I have been googling and trying to read, but a lot of times I am not sure what I am looking at. ( I am on the IT Governance side, more of an architect than a DBA). Thank you for your help.
I need to rebuild some tables when I convert to the SQL server, plus I have little experience out of a class room operating in SQL. I assume my general approach is 1. Standardize all tables 2. Create tables 3. Rebuild my macro that scrubs the data in Access with PL SQL 4. Run in parallel w/ the old Access system 5. Verify results 6. Reorient front end databases to refer to the new SQL Server tables. 7. ??? 8. Profit.
If you want to drop a temp table for example run this instead of straight dropping it, which could give an error if it didnt exist in the first place (for whatever reason). IF OBJECT_ID('tempdb..#Results') IS NOT NULL DROP TABLE #Results
you should ask what will they be testing you. They also have data scientist positions. You should see which one has the more relevant tests/knowledge for you.
Every tutorial starts with "SELECT * FROM db_name" but no tutorial starts with a database setting episode or something.. 
Your methods are definitely unique. That's not a compliment.
Are you sure some popup window in SSMS isn't preventing it from opening the query? That's when it launches a new instance.
This must be Oracle because SQL Server is not letting me subtract two DATEs. It only does the implicit conversion for DATETIME.
http://www.sqlfiddle.com/. Is a great way to practice syntax for many of the popular DBMSes. It won't give you practice in the MSSQL Management Studio application, however. For that I would look into downloading MSSQL Server Developers Edition. It lets you install a single user server on your desktop.
Ok i will look it up. Thank you!
Interesting because I was literally told that this is exactly how it suppose to be done.
Start with [SQL Server Express](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) it's a free version of SQL Server that (assuming you have a fairly new laptop or desktop) you can install and run locally. You'll also need [SQL Server Management Studio](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-2017) to manage and query the database. Let us know if you have any questions! Good Luck!
I will be sure to let you know if I was able to start it up! Thank you!
Thank you for this lengthy and detailed reply. I rarely work with a technical team and have adopted a few tech projects in my non-tech position because I enjoy it and there's no one else here to do it. I do work closely with the IT dept. on some projects and one member in particular has been very helpful in regards to answering questions and letting me try things on my own. Most of your suggestions are things I already do or try to do, so I'm glad that I'm on the right path. I have focused on your second to last suggestion--look for inefficiencies--and it has allowed me to cut out some repetitive work for my office. I hadn't heard of AutoIT, but it looks like I could use it for at least a few tasks. Thank you again for taking the time to offer your suggestions and guidance. 
There are no popups or messages, it says it's Ready and it still starts a new instance.
Someone else already did and gave the example I would have. I did come up with a sixth point though: * Always try to slip in some cool ASCII art into your sprocs to see if the IT/DBA teams have been paying attention or not. For example, in my latest development I have the company logo in ASCII signed with my name and the date the job was finished being written. 
I was thinking more about this. I recently switched shops. The technical interviews focused on SQL skills. Now that I'm hired though I'm offering suggestions on how to do things and that required knowledge of other tools (SSIS). I think if you want your current pay, you'll have to be a BA. FWIW I'm in MN, I think the pay scales are similar to TX. I'm guessing developer pay caps out around $125,000. Architects make more, but that's a different technical hat.
Some answers for you. Hopefully this helps: Do you need to put single quotes around &amp;my_variable? That depends. Is it a string or a number? I am guessing that your ID fields are numbers, so the answer is NO, you do not need quotation marks around a number. One &amp; or two (&amp;&amp;)? One &amp; means it prompts you every time. Two &amp;s (&amp;&amp;) means it prompts you the first time, and it uses the same value for subsequent variables with the same name. Is the syntax correct for IS NOT NULL? Yes. That's good syntax. However, those lines are completely unnecessary. Because you already say that fired_emp_id must equal &amp;fired_emp_id, it already excludes nulls. The check for the right dept_id is probably redundant if emp_id makes it unique already, but there is no harm in checking for the right dept_id, but you do not need to check if it is null since you are already checking for a specific value. Shortened version: INSERT INTO Prior_employee SELECT * FROM Employee WHERE emp_id = &amp;&amp;fired_emp_id AND dept_id = &amp;&amp;fired_dept_id; DELETE FROM Employee WHERE emp_id = &amp;fired_emp_id AND WHERE dept_id = &amp;fired_dept_id; COMMMIT;
Cursors have their place. I have used a cursor at least once, and possibly even twice, over the preceding decade.
Hey just saw this, I’m new to sql but the error I’m getting is FROM keyword not found where expected. 
Already ahead of you! My personal fav....[ASCII Beavis](http://www.chris.com/ascii/index.php?art=cartoons/beavis%20and%20Butt-head).
I don't disagree, but I find the hard and fast rule makes sense. If you have the mentality that they are useful sometimes, you will likely use them when you shouldn't. If you insist upon yourself that they never be used, you'll only use them as a total last resort when you must get something done and can't figure any other way.
Sourcetree is just a GUI for Git. It works pretty well. BitBucket is Atlassian's interface into Git, cloud hosted solution, and integrates seamlessly with JIRA. 
&gt;I've thought about just making a new database on each server, non-Synchronized, which contains the 4 necessary tables permanently, so I don't have to dig in tempdb. Sounds good? That's a better idea IMHO. However, there seem to be conflicting opinions on the subject: * https://stackoverflow.com/q/9359648/1324345 * https://www.brentozar.com/archive/2016/05/creating-tables-stored-procedures-tempdb-permanently/ * https://social.msdn.microsoft.com/forums/sqlserver/en-US/ab564914-ff87-4473-b1e4-711fa37bfed2/how-to-create-a-table-in-temp-table-in-tempdb * https://www.sqlservercentral.com/Forums/Topic401213-8-1.aspx &gt;there is some black magic on the server that creates 4 tables in tempdb (actual tables, not # temp tables) upon startup. I've found the SP that does it with CREATE TABLE statement, but not found out how and where it's called. This is part of why I'm concerned about the current approach. If it's not visible/documented how and why something is being done, then there's risk inherent in it. Also, since "it's only tempdb", someone may come along, trying to be "helpful" and drop those important tables. And now something's broken in a weird way. Have you looked at Agent jobs that run on startup?
So... As someone else said that is dynamic SQL and it is best to be avoided. For the case you've given there is simply no need for it, HOWEVER, that doesn't mean there is no need for dynamic SQL. My most recent project is almost entirely dynamic SQL, and frankly there is no other elegant way to solve the problem without using it. The reason other analysts are saying it's unique is because most people do not often have any real reason to use dynamic SQL, and therefore are not familiar with it. In my opinion this is a bit of a shortcoming on their part, but the truth is the matter is that most people are right when they say, "don't use dynamic SQL." You can still use variables without using dynamic SQL, though. For example I will often do something like this for a query that is frequently run: /* declare @datevar1 datetime = '2018-01-01' declare @datevar2 datetime = '2018-06-01' declare @client varchar(200) = 'Microsoft'; select blah , * into #table from table where datefield between @datevar1 and @datevar2 and client = @client */ select * from #table The only reason I'll do this is for my own ease of use. These aren't queries that I am writing to put into production but just ones that I may need to run regularly to pull data and QA it, look for outliers, etc. Really no reason to wrap that into another @var and then execute it, but there are reasons where you will need to do this such as a LOOP that is using Dynamic SQL where you want to insert dynamic variables into the code and then insert the output into a table.
You could do a `row_number over(partition by) as rn` and then join to it on the count you have such as `on a.fruit = b.fruit and a.rn &lt;= b.count` 
 Give them an intuitive naming schema that allows for future growth. For example, my main process is 000a_Global This is interesting. Are you using 000a_Global to call the rest of your stored procedures (eg 000a_Global is the one your application executes?) or are these nested in some other way? Are any of the CTE/temp tables created/used, etc accessible from any of the procedures in that scope? (eg if you make a table in 010a_Description, if it still exists in 090a_Whatever is it still available? I am more of a "lets bang this out in one long SPROC" sort of guy so this is intriguing to me. 
Yes, Global is the application executor of all the other processes and it will (through using other procs) decide which ones need to be run given certain variables. There are no temp tables at all used across these procedures, however I have several sprocs (040?) where I take data from our tables and dump them into a permanent staging table, such as: sproc_040a_Orders_Segment_1 insert into stagingtable_orders select *, 1 AS SegmendID from table where [dynamicdate] between [dynamicvar] and [dynamicvar] sproc_040b_Orders_Segment_2 insert into stagingtable_orders select *, 2 AS SegmendID from table where [dynamicdate] between [dynamicvar] and [dynamicvar] sproc_040c_Orders_Segment_3 insert into stagingtable_orders select *, 3 AS SegmendID from table where [dynamicdate] between [dynamicvar] and [dynamicvar] Then later I have other sprocs which will call data from these tables such as: sproc_090a_Description_Calc1 insert into finaltable select blah from stagingtabled where client = [dynamicvar] and segmentid = [dynamicvar] sproc_090b_Description_Calc2 insert into finaltable select blah2 from stagingtabled where client = [dynamicvar] and segmentid = [dynamicvar] &gt;I am more of a "lets bang this out in one long SPROC" sort of guy so this is intriguing to me. So am I, but this process has multiple "legs" and not all of them will fire every time the job is run. There are three distinct paths that can be taken: 1. The full job will run and each stagingtable will be truncated and have fresh data reinserted into it. This will happen only once a month after the most recent month's data has been loaded through to the end of the month. After the stagingtables are refreshed the rest of the calcs will run for all of the clients. 2. The files that are ETL'd in contain the variables per client and per calculation for the queries to be compiled in dynamic SQL. In a perfect world these only need to be loaded once, but if a client were to make a change to their contract, or if we are QAing a new client to make sure that the calculations are correct we may need to load our ETL file 10 times in a single day. Therefore I have designed a column named isTrigger, and the job will only run the calcs for the clients where the trigger is TRUE, but not refresh the staging tables. At the end of the job it will change the TRUE to a FALSE. 3. If all triggers are false, the job will run for all the clients and all the calculationid's once a week after the check is done to see whether the staging tables need to be refreshed. So once the main sproc got up over a few thousand lines of code... it started making sense to break things down into sub-processes. 
You could create a 'compound' value identifier - use a row number partitioned by the distinct value (as )u/notasqlstar suggested) shift by a sufficient number of digits and add a synthetic id (another rownumber, maybe) of the distinct value itself.
I got it working by adding this line after my first from statement in brackets: select isnull(PlanType,'Other')as PlanType, CustomerID and changed the pivot function's "null" to [Other] like this: pivot ( count(GPCustomerID) for ServiceType in ([All-In Plan], [Other], [Rental], [Service]) ) piv; 
dear /u/NiceGuyBrian -- please, do remember to identify which platform you're on (see sidebar)
dear /u/fragmonk3y-- please, do remember to identify which platform you're on (see sidebar)
How about PlanType in XYZ or PlanType is NULL? Probably a little more elegant 
Null, isnull and is null didn't work, that's why I was having so much trouble, I thought the same thing.
I guess you could try RANK() OVER (PARTITION BY Account Number)
Huh interesting. MySQL? 
MS SQL Server
You can set ROW_NUMBER() for a group by using PARTITION BY e.g. ROW_NUMBER() OVER(PARTITION BY [Account Number] ORDER BY [whatever you want] ) Note this would label the first entry 1, not 0 or blank, but you can modify it as you need.
If anyone has any other query. let me know, I try to solve asap
 select top 2 from google where engine like 'mssql'; select from google limit 2 where engine like 'mysql';
&gt;How do I select the "latest" record of the two or three duplicate records? If the specialdate is the same and there are no other columns on your table which can help you determine the latest record... there is no way. You'll just have to go by whatever SQL returns from your query and call that the "latest".
Right now both or 3 duplicate records returns with the same specialdate. I'm looking to choose whatever comes last in the csv file.
At this point you need to turn to the data source or the functional business expert to tell you which record is accurate and up to date.
&gt; this would label the first entry 1, not 0 or blank As it should be!
It's been a week since we contacted the data source and we are still awaiting a response. For now, I am looking to select whichever of the two or 3 records comes last in the csv file.
There is no guarantee that the last record in the CSV is the latest. Can the person who generated the file confirm that the order of the CSV file determines the priority of the records? You have to always be accurate in the way the data is presented.
If you have the data in a CSV file and you're certain the "bottom most record" in the CSV is the latest, open it in Excel and add a new column called `tempid` or something and populate it with an incrementing number. Then modify your query to use the `max(tempid)` instead of `specialdate`. Not sure what DBMS you're using, but in TSQL you can use `ROW_NUMBER()` and `PARTITION BY` to achieve this: CREATE TABLE mytable ([id_number] int, [specialdate] datetime, [tempid] int, [data] varchar(1)) ; INSERT INTO mytable ([id_number], [specialdate], [tempid], [data]) VALUES (11111, '2018-01-01 00:00:00', 1, 'a'), (11111, '2018-01-01 00:00:00', 2, 'b'), (11111, '2018-01-01 00:00:00', 3, 'c'), (22222, '2018-01-02 00:00:00', 4, 'd'), (22222, '2018-01-02 00:00:00', 5, 'e'), (33333, '2018-01-03 00:00:00', 6, 'f') ; SELECT id_number, specialdate, data FROM (SELECT ROW_NUMBER() OVER (PARTITION BY id_number ORDER BY tempid DESC) as RowNum , * FROM mytable) t WHERE RowNum = 1 Output: id_number|specialdate|data |:--|:--|:-- 11111|2018-01-01 00:00:00.000|c 22222|2018-01-02 00:00:00.000|e 33333|2018-01-03 00:00:00.000|f
MySQL
It is a hard fast rule and you are correct... until you learn enough to know that you're 100% wrong, and that it is not a hard fast rule. Typically I agree with you. A completion model (predictive analytics) for example will require a loop of some kind, and unless I'm mistaken a cursor is just a non-integer based loop. I tend to avoid cursors and use loops by just creating a incremental PK and using that to do my loop. Unless I'm not mistaken that pretty much leverages what a cursor does.
I am assuming that your job_id number is a sequence that is continually increasing. Do you really want to delete records that are *greater* than a certain job_id number? It is more common to delete records that are *lower* than a certain number, or even more common, older than a certain date.
I try not to put anything even mildly offensive in because I value not being fired, but one time I was called into a meeting with a senior VP and had to open my code to review it with them in order to justify some decisions that were made, and convince them that my data was more accurate than what had been done in the past. Meeting went really well but after my explanation they just looked at me and said something like, "Only one question... why was there an alien giving the peace sign in the middle of your code?"
You'll probably have to use the ranking function of MySQL--I'm not super familiar with the syntax, though.
Right. For a newbie he should be avoiding cursors (and loops) at all costs. The chances of him being tasked to do something that requires them and can't be done in a better way is unlikely. In 20 years I could probably count on one hand the times I've seen cursors or loops used because they had to be, the vast majority of the time has been because the person defaulted to them because it was the obvious easy solution, but it never performs well.
SQL is probably not the place to do it. Google's libPhoneNumber is the best library You could do it in PostgreSQL with a PLV8 function or a C extension
I agree with /u/boy_named_su . Do that validation in the application. And separate the phone number from the phone number type. Depending on how far down the normalization trail you want to go, you'll be setting up anywhere from 2 to 4 (maybe more) tables.
I agree with /u/boy_named_su that this should be checked as part of the user interface as the data is entered. Typically applications also do some sort of server-side validation (you should be doing this for web apps anyway). However, if you also want to check data on its way into the database, you will want to look into using regular expressions. Don't forget that there is a lot of variation in phone numbers if you include other countries. And in a case like this, I would think about stealing code rather than reinventing the wheel.
No idea about Zapier, but I've seen Talend and Jitterbit used for this with mixed success. As good as the tools are, they only work as well as the guy you pay to hook up the integration. Make sure you've got a guy in-house for it, there'll be ages of tweaks and updates!
Yeesh, up to 4 tables for a phone number? In general I see very little value in normalizing things like area code. Maybe it gets its own column, but not a parent-child table. That's excessive.
Off the top of my head: * Phone number (holds number ID, type ID, actual number) * Phone number type (holds type name and ID) * Link table between phone number and owner Unless you have a hard business rule of only one phone number per "owner" (person, business, etc.) you don't want to put the phone number in the "owner" table because you'll have to keep building out columns every time someone has more numbers than you're currently equipped to handle.
I would run Process Monitor from sysinternals and filter on the hostname of the sql instance. You can get process monitor from here: https://docs.microsoft.com/en-us/sysinternals/downloads/procmon When it starts, the filter dialog should load. Select Path, Contains, &lt;enter the hostname of sql&gt;, choose include and press add. On the bar at the top there are buttons to show registry, file system, network, process/thread, and profiling events. You can probably deselect all but network.
Yep. That looks good to me to have a one-to-many between a person (or organization) and their phone numbers. That's fine. But that's probably as far as I would go. A relationship table between person and phone, probably not. I'd go with one to many here.
So I can tell you in my current role I was hired to do something which was thought to be a very easy 90 day project. I have now been here going on 2 years and just this week have deployed my code to production, and it 100% required a loop. It actually took me longer to convince them that I needed one than it did to write the framework, and then ten times as long to QA it. Actually it's not just a loop, it is a loop-of-loops-of-loops. &gt;In 20 years I could probably count on one hand the times Going on four years in this field and I build one every couple of months. &gt;but it never performs well. It depends on your use. If I am running a loop to create a predictive model then I'm probably only going to need to run it once a month or quarter... and I don't really care if it takes 20 hours to run. But you need a loop to do something like that, or another tool which is more appropriate. You can here argue that you shouldn't do this in SQL, but SQL is fully capable of it and here is where I am going to argue that your server should be capable of it, and that I require a specific environment to work in order to create specific types of business solutions. In some cases it might be literally impossible to integrate new (more appropriate tools) into a existing environment based on your specific role, or the role your group has in the larger organization relative to IT and data management. I agree with you fully that loops are often inappropriately used by newer users because they aren't thinking in sets, but once you start thinking in sets and interpreting / statistically analyzing data... you start to use loops or cursors. For my own sake cursors look too technical and scary, so I just bashed my way into using loops for non-incremental values, e.g. with a table such as: | AlphaValues | | :--- | | Ohio | | Blue | | Turnpike | I will then: DECLARE @List TABLE ( ID int identity(1,1) , AlphaValues nvarchar(255)) INSERT INTO @List SELECT DISTINCT AlphaValues FROM TABLEName DECLARE @Loop int = 1 WHILE @Loop !&gt; (SELECT MAX(ID) FROM @List) BEGIN DECLARE @AlphaValue nvarchar(255) = (SELECT AlphaValue FROM @List WHERE ID = @Loop) INSERT INTO TableNameFinal SELECT * FROM TableName2 WHERE AlphaValue = @AlphaValue SET @Loop = @Loop + 1 END This is generalized because it will often be wrapped up in DynamicSQL, but as I understand cursors that's what they do, so I just stick with loops for all my DBA-infurating needs.
Have a look at SQL Agent, there may be a startup job which creates the temp db tables. What's your main motivation for moving these tables from tempdb into the same database? One advantage of having them in tempdb is that it self-documents they are temporary and discarding/recreating them should not pose too big an issue. It also makes backups of your main database easier... I imagine these tables could get quite big, but I also guess there is little value in backing them up 
It might be IntelliSense caching data, or it can be pretty much anything else. Have they provided you with query text? Have they confirmed that it was ssms process (sql server is able to show the PID of the connection when using native client)? It's kind of impossible to say what's going on without actually tracing it from the db server side. Double check your code for things like sp_msforeachdb and such. Double check there are no addins in ssms that might gather additional data about databases similar to IntelliSense. Also, inform your manager right away, and express your desire to help with investigation even though you're unaware of any queries that might cause it. Not sure what else can be done here.
So unless I'm mistaken, you don't need a loop here at all. This example is just getting a distinct list of values, then inserting all the records from TableName2 that match any of those values. Could just as easily be done like this: INSERT INTO TableNameFinal SELECT T2.* FROM TableName2 T2 JOIN TableName T on T.AlphaValue = T2.AlphaValue (I should note this is how to write it in SQL Server but it will be similar in other SQL dbs). I am assuming that you are providing a simplified example here rather than actual code you run, but your example is the kind of thing that I have seen and fixed many times, where a loop/cursor was written to insert records one at a time when it could be done in a single insert statement, written properly. That's exactly why I said you should avoid cursors (and loops). Now, with doing loops inside of loops, you can still use batch statements, depending on what it is you need. I'm not trying to say you are absolutely doing it wrong, but (with all due respect) if you are of a skill level where cursors are over your head then the chances are you just need to improve your SQL so you can avoid loops in the future. If you have a simple, real world example of a query where you're using a loop, I'd be happy to take a look and see if I can show you a better way.
This was not an applicable example of a legitimate need to use a LOOP/CURSOR, this was an example of how I use LOOPS like CURSORS because I don't know how to use a CURSOR. I don't know much about them except that they are LOOP's for alphanumeric values, but I learned that after I "invented" the little process I just described, and I've been able to get by using it. I'm getting older. I'm scared of change.
Don't be scared of change! I don't know how old you are but I'm 40 and been doing this for 20 years, there's never a good reason to stop trying to learn to do things better if you can. =) Why else would you be here in the SQL sub if not to learn something? I'll tell you this though, cursors are not loops for alphanumeric values, cursors are loops that step through each row in a recordset. So if you wanted to query some data and then do some operation based on the data from each row individually, a cursor would do that. But 99.9% of the time the same thing can be done in a single statement. The offer stands, if you'd like to learn how to write more efficient SQL.
The details that are important is which endpoints you need to connect to, what kind of transformations you will be doing, what licensing and infra cost is involved, who will maintain this in your team and what their skills are, and what support you're going to get. And such. Or to put it another way, why can't you write a simple script that does this??
So I'm 36 and I've been programming since I was 9. When I said that these loops were necessary I wasn't being trivial, if you'd like I can further convince you. &gt;I'll tell you this though, cursors are not loops for alphanumeric values, cursors are loops that step through each row in a recordset. Unless I'm mistaken that would conform to the code example I just provided. &gt;The offer stands, if you'd like to learn how to write more efficient SQL. Thanks :) 
&gt; I'm 36 You're way too young to be scared of change! I barely accept that line from my 72 year old father. =) &gt; Unless I'm mistaken that would conform to the code example I just provided. Yes, you could use a cursor in the example above, but that is the exact type of scenario that I am saying people use cursors for, but shouldn't. Using a cursor or a loop of any kind is bypassing the built-in optimization of the database engine and will always be much slower. And I believe you that the loops aren't trivial, but loops also make things more complex than they need to be. But even if you could take the lowest level of loops and turn them into single statements you'd probably improve the speed significantly with a lot less code. Wish I could see your code, I love refactoring stuff like that!
Thanks for the tip - it's marked as a startup procedure :) &gt; What's your main motivation for moving these tables from tempdb into the same database? Really just to reduce the black magic feeling of this spaceship of a DB setup I've inherited. I only discovered the reliance of these tables recently as I'm in the process of setting up a brand new SQL Server 2017, where I'm migrating all the stuff by hand (except restoring databases from the old server) in order to clean up obsolete stuff an get knowledge of every single thing in the DB. &gt; Splitting activity by Windows software vs website activity may not be optimal, and you might be better off using some sort of non-discriminatory but persistent load balancing. The idea is to utilize the secondary server in the AAG, so it's not just idling 99.9999% of the time. Makes sense to me, but what do you think? Our AAG consists of two servers located on two physically different servers.
I'll give you a concrete example of a need for a loop outside of a predictive model. Say you have 100 or more global clients, and contractually you have between 10 and 15 KPI's per client that you are obligated to meet or face financial penalty. So now you want to create a global dashboard that very simply just says something like: | ClientName | |KPIName | Score | FinancialGoal | Cost | | :---- | :---- | :---- | :---- | :---- | | ABC | Compliance Regulation | 54% | 62% | 10000.00 | | ABC | Regulation Compliance | 92% | 75% | 0.00 | Now this seems perfectly approchable using set logic, right? Wrong. Each KPI, while it might have the same name, may (based on the contract language) be actually looking at two entirely different fields, and the ramification of this might be that the client's are using those fields instead of other fields, to measure something and then calling it "this", while another client is doing an entirely different calculation and calling the same thing, "this." Sometimes one client and another client are doing the exact same thing, but they are excluding different things. For example Client 1 might be based in the US but have an office in Tokyo and be excluding any events in Japan, while Client B is based in Japan and excluding any events in the US. So realistically what you're looking at in order to do this is inserting data into a table where each client, and each calculation has different parameters, or some might share the same parameters. That means if a client has 10 KPI's, you have 10 queries to support. If you have 100 clients, that means you have over 1,000 queries to support. This is solved by a loop and storing variables in a table and dynamically generating the queries based on segments, and it can run new calcs in seconds with an incremental load because it is a loop, of a loop, of a loop, that checks itself to see which part needs to fire when. In analytics you find a lot of need for them too when it comes to calculating probabilities. They're really useful. I use them a lot.
&gt;You're way too young to be scared of change! I barely accept that line from my 72 year old father. =) I pine for the days of RPG and COBOL, son. &gt;Wish I could see your code, I love refactoring stuff like that! Needs some tweaks but I'm planning on anonymizing it and making a post. 
Uh, what? You absolutely have to account for several people sharing a single number. What happens if you have a husband and wife that share an address? A boss and a secretary that share the same business line because the secretary routes all calls? You also have to account for potential duplication - you don't want to block submission just because Joseph wants to submit info with his name listed as Joey.
I did not say that people cannot share a phone number. I said that the database does not need to be modeled in that way. You have to make choice when you do data modeling. I simply would not make this a many-to-many relationship. I think it is unnecessary. 
OH boy because scaling and bursts get hard real quick. And so does a spaghetti of complicated ETL logic. we've gone from Snowplow to a pretty large distributed GAE 'ingestor' that we just basically write new endpoints for when a new data source comes on and customize how we need to etl for each source. Snowplow has better schema management and much more complex failovers etc but it was so hard for a small group to manage. being able to not lose data when you get bursted 10k incoming POSTs per second from 0 (in literally 1 second no warm up) was a big challenge that we just ended up keeping servers live which is cheaper than engineering time
there are a bunch but they usually all get either really complicated DIY or really expensive fast. stitch, blendo, fivetran, segment (kind of), you can pay snowplow to host for you etc there are a bunch but see my warning above
#temptables should be session-level, so whether or not you drop them shouldn't matter, and mutliple copies of the proc should be able to run at the same time without conflicting. Dropping a temp table in a proc has never been necessary in my experience, but I think there may be a setting somewhere to determine whether they are session-level or database-level.
&gt; Really just to reduce the black magic feeling Ok, yeah Ive been there too :) I'm sensing maybe its a small team with you having the main DBA role? If you feel it will help make maintenance easier then there's probably no harm in relocating it into the main DB. There may even be a solid argument to relocate them, in the event you had another identical database for some reason (which would then mean the tempdb tables would conflict) If the tempdb tables are huge, you could just exclude them from your backups. But since they are just cache tables I guess they are being pruned regularly so probably less hassle just to back them up with the rest of it. &gt; The idea is to utilize the secondary server in the AAG, so it's not just idling 99.9999% of the time It's difficult to say, it would depend on how much traffic your primary server is getting, and whether it offers any material performance advantage to load balance it. I personally would only start balancing them when there is a demonstrable advantage in doing so (be it response times, or different usage patterns, ie the caching is not as effective as it could be etc). I know its tempting to think of a failover server as being wasted if its sat there doing nothing. But it could be sat there for 12 months with 0 utilisation, and so long as it takes over if your primary server fails, then it is still money well spent. Thats it's main role in life after all :) 
I cannot understand this whole scenario. Firstly, why are concerned about newly inserted records? their SCN would be greater than the start of your delete statement so they wouldnt be seen by your statement. Secondly, what are you trying to lock and why? If you are trying to prevent reads, you cannot - it's oracle. Inserts you shouldnt worry about (see above). If its updates - why are you deleting records that other processes are still writing to? Same-ish with deletes.
I have not used this software. Could it mean that you correctly left it unchecked?
Thank you for answering. Yes, the job_id is a sequence that is increasing. The reason for ordering the results first and then deleting from the top most is to prevent the table lock escalation from happening in other database. If the query would be changed to purge records older than a period of days, would you still check if there were locked records before executing the delete statement? DELETE FROM jobs WHERE job_date &lt; sysdate - 60 FOR UPDATE NOWAIT 
&gt; would you still check if there were locked records before executing the delete statement? No. Why would you want to do that?
Thank you for answering. I'm concerned about newly inserted records because the query sorts to delete the topmost records so I would like to check first if they are locked records rather than executing the delete statement directly so the delete statement would not wait for the locked records.
Thanks for the plug. If you have any questions about DingerDB just hit me up!
Do you have 'NULL' instead of *NULL*?
Yes that is what it means. Clunky phrasing but that is how it is used consistently in that course. 
Thanks. You got to the heart of my dilemma - data or web? Why do you think web development has a questionable payoff? Is it the industry or more about the amount of time/effort it would take for me to get up to speed?
Well, that's the problem, I suppose. I don't know which way to go. Given my current skillset, data analysis of some kind makes the most sense, but the job availability in my area is low. I live on the east coast. Only one city in my state has much going on with data. 
Thanks for your reply. I was able to answer the questions I had that I didn't know how to ask. 10 google searches later with your keywords and I have a much better grasp of what web dev entails. 
Why do you say that? I'd never heard of salesforce, so I had a hard time interpreting an article on it I found from Tech Crunch - https://techcrunch.com/2018/06/17/after-twenty-years-of-salesforce-what-marc-benioff-got-right-and-wrong-about-the-cloud/
So that means OP has just not figured it out. 
&gt; Only one city in my state has much going on with data. **Every** company is, or should be, a data-driven company now. It's inescapable. If you're not leveraging the data you have, you're missing out on opportunities. You just have to find out who's doing what.
Dynamic SQL is fine, when called for. Lots of scenarios where it's the right thing to do. OP's example is a poor use of dynamic SQL.
I would also recommend you change your password. Both SQL and AD account ( if they are not the same) to see if the behavior continues if you haven't found the culprit.
Its like your favorite wet dream, you gotta do it in the front and the back!! Front end with a solid library. Don't let them type incorrect characters into the input fields to begin with. Disable form submission if js is disabled if you can. Back end with stored procedure parameterization and then usually simple regex check to be extra sure and raise an error if anything doesnt pass the regex and return the error to user.
I will agree with that statement but you have to teach people in absolutes because many businesses lack a senior developer or DBA with the experience to know it's limitations. Saying dynamic sql is like telling mothers they can drink while pregnant. I am a firm believer that a glass of red wine is great for a pregnant woman. Whats good for the mom is usually good for the baby. However, doctors won't tell people that because the uneducated masses will not drink for 6 days and drink 7 glasses on the seventh day. I started a job recently where the entire product is dynamic tables and they used a stored procedure, added criteria to limit results and could include a comma delimited parameter that would run recursively to request a single or more dynamic tables in pivoted form. It was mind boggling. [It was all formatted like this too](https://i.redd.it/x0fq5x8o6m211.png). Why is it that dynamic sql is always formatted soooo poorly? I wrote a procedure that uses a cursor (faints) over the list of dynamic tables and then builds dynamic sql (faints again) that drops/creates a view of the dynamic table pivoted. The views based on dynamic tables we manage are actually checked into source control and updating all these views is included in a post deployment script. It works amazing and I showed my new coworkers how 900 lines of code can fit in less than 25 lines but I am literally cringing. Why? A JS API doesn't care whether data is row or a column. It can read the recordset and turn it into Json class objects either way. I am turning magic out of a database structure but it shouldn't be that way. It took me 2 weeks to reconcile the database in development because of things like the stored procedure pulling two dynamic tables expecting the same set of columns and one has a column added to it and now the union between them fails. There is no unit testing. What I have found through out my experience with SQLserver is everything should return an expected schema (I really learned this with Biztalk and MSSQL), it will bite you in the ass one day.
Cheers! LAMP/LEMP is a very valuable stack. You can branch off from there and continue your journey. 
For certain bits of information like that the overhead of creating the tables and relationships isn't worth it just for the sake of perfect normalization when you're never going to do anything interesting with it. But that's all going to be case specific, maybe your company has a good reason for really caring about phone numbers. 
Yes fair enough. Temp table not a great example here.
I'm assuming you're answering my last line: &gt; why can't you write a simple script that does this?? I wasn't advocating writing a script. I was using it as a KISS baseline and asking OP to work upwards of that (if needed). In your case, you had specific needs that had to be addressed. There are hundreds of ETL and data integration tools. Unless you have a clear and well defined list of requirements, you can't narrow down the list. Plus, there's realtime integration and there's batch based integration. In your case, I'm curious - what kind of data does GAE send you?
Are you positive SSMS is the client that causes it? Dbeaver, for instance, pulls in much more metadata from my teradata server than the native client. Are you sure no other client is running? Also, why anyone would want to sack someone over a literal technicality like this is beyond me. Discuss this with your manager and help the DBAs to resolve this. 
SQL 2016 has native R supporting regular expressions. We use it all over the place. Here's a simple example. https://www.mssqltips.com/sqlservertip/4748/sql-server-2016-regular-expressions-with-the-r-language/ 
Is it just two fruits or do you have a table telling you how many of each you want to grab? Basically the same as /u/notasqlstar's answer, but in more detail. Using MSSQL. DECLARE @Fruits TABLE (Fruit VARCHAR(10), Value INT); INSERT INTO @Fruits VALUES ('Apple',1), ('Apple',2), ('Apple',3), ('Apple',4), ('Orange',5), ('Orange',6), ('Orange',7), ('Orange',8); DECLARE @FruitCounts TABLE (Fruit VARCHAR(10), FruitCount INT); INSERT INTO @FruitCounts VALUES ('Apple',2), ('Orange',3); WITH CTE AS ( SELECT Fruit, Value, ROW_NUMBER() OVER(PARTITION BY Fruit ORDER BY Value) AS Row FROM @Fruits ) SELECT CTE.Fruit, Value FROM CTE INNER JOIN @FruitCounts FC ON FC.Fruit = CTE.Fruit WHERE CTE.Row &lt;= FC.FruitCount;
Make sure to give your procs creative names. CREATE PROCEDURE dbo.[💩] AS BEGIN SELECT 1 END And yes, that works.
It really depends on what you're doing and the kind of SQL database you work with. Use cursors and loops where it makes sense. Sometimes the only feasible alternative to cursors or loops is a recursive CTE, and that comes with its own can of worms. The right tool for right job. The key is knowing what the right tool is.
Is this a homework problem? Then I'll give you a hint. You'll need to use COUNT and ROW_NUMBER window functions. Think about the set of rows that you want these functions to work on.
yea it is kind of hw problem. i thought i must use something from these [https://docs.microsoft.com/en-us/sql/t-sql/functions/analytic-functions-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/analytic-functions-transact-sql?view=sql-server-2017). thank you for your answer.
SELECT SUM(salary), Deptno FROM emp GROUP BY deptno For your first question. Second i only can think of a nasty solution quickly. Sthm like SELECT TOP 2 * FROM emp WHERE deptno = XX ORDER BY salary there are better Solutions but i only had 1 coffee so far..
Those are the right type of functions for the job, but not all of them are listed there. The COUNT function can be used as an aggregate function through a GROUP BY and an analytic function like COUNT(*) OVER().
Don't validate at the database - read [https://github.com/googlei18n/libphonenumber/blob/master/FALSEHOODS.md](https://github.com/googlei18n/libphonenumber/blob/master/FALSEHOODS.md)
The link doesn't work.
Converting int to serial is basically adding a sequence to the int. 1. pick a starting value for the serial, greater than any existing value in the table SELECT MAX(id)+1 FROM mytable 2. Create a sequence for the serial (tablename\_columnname\_seq is a good name) CREATE SEQUENCE test\_id\_seq MINVALUE 3 (assuming you want to start at 3) 3. Alter the default of the column to use the sequence ALTER TABLE test ALTER id SET DEFAULT nextval('test\_id\_seq') 4. Alter the sequence to be owned by the table/column; ALTER SEQUENCE test\_id\_seq OWNED BY test.id
I have to fill out a table so I know exactly how much I need for each "fruit". 
Thanks I will try this out and report back
Maybe I was googling the wrong languages/software, or there's a lot of outsourcing going on around here. I could only find a handful of jobs asking for R and a few more for Python (that were also data focused). These few I could find were not entry level either. Some were literally asking for PhDs. 
I think you are overly worried about locked records. And even if you are concerned about row locks, sorting by the most recently inserted rows just makes that problem worse.
Seriously, don't. Text the number. Call the number. 
What if the number to text is '7K@!bbk%?'. Can't text that. Surely you want to have some kind of input validation.
WELL, then you have your answer. The phone number is not reachable. If you're user wants to enter garbage, there isn't really much you can do. Some other people suggested libPhoneNumber, but I don't have much experience with that. What if the user enters a perfect valid phone number that belongs to someone else? You can't control that.
How do I pass parameters through the PTQ from a Search form, for example? Here is a sample SP I use for querying the database and the VBA function I use to call it: ALTER PROCEDURE [dbo].[sp_SmartSearch] @FName nvarchar(50) = null, @LName nvarchar(50) = null, @OrgName nvarchar(255) = null, @Address nvarchar(255) = null, @PCode nvarchar(7) = null, @Email nvarchar(255) = null, @Phone nvarchar(22) = null AS BEGIN -- SET NOCOUNT ON added to prevent extra result sets from -- interfering with SELECT statements. SET NOCOUNT ON; DECLARE @sSQL NVARCHAR(2000), @Where NVARCHAR(1000) = '' SET @sSQL = 'SELECT TOP 200 dbo.Gifts.g_ID, dbo.Gifts.g_Date, dbo.Gifts.g_Modified_User, CASE WHEN c.c_Organization IS NULL OR c.c_Organization = '''' THEN c.c_FName + '' '' + c.c_LName ELSE c.c_Organization END AS CName, dbo.Gifts.g_C_ID FROM dbo.Constituents c LEFT OUTER JOIN dbo.Gifts ON c.c_ID = dbo.Gifts.g_C_ID' IF @FName is not null SET @Where = @Where + 'AND c.c_FName LIKE @_FName + ''%'' ' IF @LName is not null SET @Where = @Where + 'AND c.c_LName LIKE @_LName + ''%'' ' IF @OrgName IS NOT NULL SET @Where = @Where + 'AND c.c_Organization LIKE @_OrgName + ''%'' ' IF @Address IS NOT NULL SET @Where = @Where + 'AND c.c_Address LIKE @_Address + ''%'' ' IF @PCode IS NOT NULL SET @Where = @Where + 'AND c.c_PCode LIKE @_PCode + ''%'' ' IF @Email IS NOT NULL SET @Where = @Where + 'AND c.c_Email LIKE @_Email + ''%'' ' IF @Phone IS NOT NULL SET @Where = @Where + 'AND ((c.c_PhoneNumber LIKE @_Phone + ''%'' ) OR (c.c_PhoneNumber2 LIKE @_Phone + ''%'' ) OR (c.c_PhoneNumber3 LIKE @_Phone + ''%'' )) ' IF LEN(@Where) &gt; 0 SET @sSQL = @sSQL + 'WHERE ' + RIGHT(@Where, LEN(@Where)-3) EXEC sp_executesql @sSQL, N'@_FName nvarchar(50), @_LName nvarchar(50), @_OrgName nvarchar(255), @_Address nvarchar(255), @_PCode nvarchar(7), @_Email nvarchar(255), @_Phone nvarchar(22), @_Tbl nvarchar(50)', @_FName = @FName, @_LName = @LName, @_OrgName = @OrgName, @_Address = @Address, @_PCode = @PCode, @_Email = @Email, @_Phone = @Phone, @_Tbl = @Tbl END Public Function spExecute(ByVal spName As String, paramCount As Integer, spInputParam As Variant, Optional ReturnRec As Boolean = False, Optional spOutputParam As Variant, Optional OutputType As ADODB.DataTypeEnum = adInteger) As Variant Dim ACon As ADODB.Connection Dim ACmd As ADODB.Command Dim parm As ADODB.Parameter Dim rst As ADODB.Recordset Dim i As Integer ' Connect to SQL Server Set ACon = GetCon Set ACmd = New ADODB.Command Set ACmd.ActiveConnection = ACon ACmd.CommandText = spName ACmd.CommandType = adCmdStoredProc ACmd.Parameters.Refresh ' Loop through parameters and assign values For i = 0 To UBound(spInputParam) ACmd.Parameters(spInputParam(i).paramName).Value = spInputParam(i).paramValue Set parm = ACmd.Parameters(spInputParam(i).paramName) ACmd.Parameters(spInputParam(i).paramName).Size = spInputParam(i).paramSize Next i 'execute query that returns value If Not ReturnRec Then Call ACmd.Execute Else Set rst = ACmd.Execute() End If If Not ReturnRec Then spExecute = ACmd.Parameters(0) Else Set spExecute = rst End If End Function And how I call it (SPParam is a custom class that I pass to the function in an array): Set srchParam(0) = New SPParam Set srchParam(1) = New SPParam Set srchParam(2) = New SPParam Set srchParam(3) = New SPParam Set srchParam(4) = New SPParam Set srchParam(5) = New SPParam Set srchParam(6) = New SPParam srchParam(0).paramName = "@FName" srchParam(0).paramSize = 50 srchParam(0).paramValue = Me.txt_FName srchParam(1).paramName = "@LName" srchParam(1).paramSize = 50 srchParam(1).paramValue = Me.txt_LName srchParam(2).paramName = "@OrgName" srchParam(2).paramSize = 255 srchParam(2).paramValue = Me.txt_Organization srchParam(3).paramName = "@Address" srchParam(3).paramSize = 255 srchParam(3).paramValue = Me.txt_Address srchParam(4).paramName = "@PCode" srchParam(4).paramSize = 7 srchParam(4).paramValue = Me.txt_PCode srchParam(5).paramName = "@Phone" srchParam(5).paramSize = 22 srchParam(5).paramValue = Me.txt_PhoneNumber srchParam(6).paramName = "@Email" srchParam(6).paramSize = 255 srchParam(6).paramValue = Me.txt_Email Set rst = spExecute("dbo.sp_SmartSearch", UBound(srchParam), srchParam, True)
&gt; belongs to someone else? Yeah, but that's a separate issue. Inputting data in the expected format is a different issue from proving that the user is being honest and accurate. Proving ownership of a phone number is not the same as controlling input for a phone number. Input validation versus verification.
I agree, however I'm stuck waiting for the integrator to get me access to the environment. I'm working a little blind on this. The more I can do for efficiencies now, the better later.
I kind of don't see it as a separate issue. If the number isn't important enough to verify, then is it really that important to validate? I'm not the most experienced person, so I can accept if I'm wrong. I just don't believe that a bad phone number is that big of a deal.
for starters: strip all values except numbers. then check length. assuming USA 
&gt; isn't important enough to verify Hang on, I did not say that it is not important to verify. I said that verification is a different issue. To stay on topic, I am differentiating between validation and verification. Let me give a very simple example. Suppose you make a website that accepts an email address, and then attempts to verify it (by sending an email for confirmation). Firstly, by having a maximum length on your email field, you save on storage space. You can limit the input to 100 characters, or if you are feeling generous, maybe 250 characters. But surely you will not accept email addresses that are over 1000 characters. You have to draw the line somewhere. Furthermore, why put the burden on your email verification process, when you can stop invalid email inputs on the client side application? If an email address does not even resemble an email, it's a waste of resources to store it, try to send a message to it, and handle the resulting error. Furthermore, it's a bad user experience. Maybe the user accidentally forgot the @ symbol in the email address. It's better to notify the user now, at the time of input, rather than wait for a failure. There are many reasons why input validation is useful to both the user and the system owner. All of that takes place before any kind of verification process.
So, that is a weird issue.. Sometimes, you could have broken permissions somewhere or it could be trying to access another file that it doesn't have permissions to for whatever reason. Have you tried event viewer? - That may give you more insight into what is going on. So you should be okay reinstalling SSMS, it is just the program to access SQL. They are completely separate instances. You could also just download SSMS by itself and try the new version. I believe its 17.x.x at the moment. I use it on most of my server and it has worked flawlessly.
Cab you connect from the command line? Before repairing this system, I would try to install SSMS on another system and then try to connect to the database from there, and take a db dump.
I kind of feel like we're not having a very productive conversation, and I feel like you're distorting my position. Then, taking offense because you feel I distorted your position. I never said to not place some sort of limit on your the length of your phone number or email. "Why accept phone numbers over 50 digits?" Never said that. Accept invalid characters, sure. Why not? I kind of feel strange since you're arguing about saving the space for a field that contains at most 250 characters for each user on one of your systems? Is the big bottle neck of your application really the signup? Is the big bottleneck really sending 1 verification email, text message, or phone call that doesn't reach the user? If you have access to a library to validate something for you, sure use it. 
Interestingly enough, typing whoami brings up an "Access Denied" message in command prompt. I know our computers do a funny thing with identifying us. We log in to the anti-virus system at startup using our AD username and passwords. Once Windows loads up we don't have to log in after that, we just press Ctrl+Alt+Del and the desktop opens (we have to use pw to unlock though from a lock screen once it's on). I'm logged in as a domain user and installed everything under my domain account on this PC over a year ago, and never had any issues. Interestingly enough, in my team's access db (that the rest of the team use for day to day work) I have a function written that when called on the immediate window, displays the PC Number and User who currently have the database open (it helps when I need to make quick updates to the db I can then call them and tell them to exit). In past places at which I've worked, this function has worked exactly as it should. Here, though, the username ALWAYS appears as "Admin", rather than the domain name which is logged into the particular PC. I suspec that has to do with the aforementioned antivirus login above. Doing a fresh install of SSMS at the moment. Will report back.
it's a local server so wouldn't be able to load it from another PC
If they are in the same network, it should be able to connect. You don't need a server os or a full version of SQL server to be able to connect from another system. I've created several .Net apps which are used by several users to connect to and store data in SQL express server running on my dev machine.
Right I see what you're saying. I've reinstalled SSMS on my PC. I'll give it a go and see what happens, if it still doesn't work I'll try connecting on another machine.,
Reinstalling SSMS will not do anything to your SQL Server instance; they're completely separate. I recommend upgrading to SSMS 17.7 (latest). MS is really pushing the improvements and fixes quickly on their tools.
Yeah I uninstalled SSMS 16 and installed 17 instead. Same error. I'm trying to locate my mdf files and copy them for backup, then completely wipe SQL Server and install fresh. Having trouble finding them though. SQL Server Config Manager finds the server though, and Import/Export Wizard displays the database within the server, so at least it's somewhere
How else would you like it to know how many apples and how many oranges? Maybe if we had a little more info (including what flavor of SQL you're working with) we can help more. Looks like you've gotten the same answer a couple times because with the info we have it's the only one that makes sense. Ultimately, it looks like if you're going to be doing this you'd have to use some other method to populate your "FruitCounts" table, then use that table to get your variable TOPs (really just row\_numbers). Unrelated - but be careful using table variables. They're pretty inefficient, and rarely the best choice. Temp tables *almost* always perform better (more than 5ish rows). 
I haven't distorted anything. Field lengths are part of validation. I just used length as one example of a type of validation. &gt; I kind of feel strange since you're arguing about saving the space for a field that contains at most 250 characters Why does it have a maximum of 250 characters? Because of input validation. Otherwise maybe you might accept 10,000 characters for an email address. See where that starts to become a problem? It's non-trivial. &gt; Is the big bottle neck of your application really the signup? Have you ever run a public facing web app? First, people make mistakes and typos, and people want guidance on what input should look like. If you want free-form text boxes for every field with no validation, you can find that on websites from 1999. Second, web apps are often the target of things like automatic bots that submit garbage. You want to limit the amount of garbage in your database, and having strict input validations can help limit that (among other strategies). &gt; Is the big bottleneck really sending 1 verification email, text message, or phone call that doesn't reach the user? These things are not free. It sometimes costs real money to an outsourced provider of text services or mass email service providers. Even if real money is not being paid, the cost on your system resources and maintenance are real, too. Businesses want to limit the number of unnecessary text messages and phone calls that they send. Are there ways to lower the cost via IP phone calling, etc.? Sure there are, but there is still a business cost associated with processing it.
You reference the data from the form **in Access VBA** ... a lot of your code is in TSQL. You want to execute the code from your stored procedure remotely, You'll need to declare the variables (something like this): &gt; Dim rst As DAO.Recordset &gt; DoCmd.SetWarnings False the following QueryDef type is what you will use to remotely execute the code &gt; Dim qdf As DAO.QueryDef &gt; DoCmd.OpenForm "Data_Qry_Page" &gt; Dim queryAudit As DAO.QueryDef &gt; Set qdf = CurrentDb.CreateQueryDef("Data_Qry") Example: &gt; qdf.Connect = "ODBC;DRIVER=SQL Server Native Client 10.0;Server=YOUR_SERVER_NAME; Database=YOUR_DB_NAME;Trusted_Connection=Yes;" This is the code that pulls the data from the Access UserForm to VBA: &gt; If IsNull(Forms!Data_Qry_Page!Fname) Then Fname = "" Else Fname = Replace(Forms!Data_Qry_Page!Fname, "'", "''") Note that this example takes a firstname or FName from a userform called the Data_Qry_Page; &gt; qdf.SQL = "SELECT * from dbo.MyTable where [FirstName] like '%" &amp; Fname &amp; "%';" This is a very simple example. You can execute your stored procedure in a similar way. (You are just passing the query through to SQL) &gt; qdf.ReturnsRecords = True &gt; Set rst = qdf.OpenRecordset &gt; DoCmd.OpenQuery "Data_Qry", acViewNormal, acReadOnly rst is a record set and you can search it in Access or just print it like a table. Good in principle to close these: &gt; rst.Close &gt; Set rst = Nothing &gt; Set qdf = Nothing 
This computer part of a managed domain?
Sorry if I was being confusing. I already know ahead of time how much I need of each. There's a template table in excel that has the top entries blank for each category. Currently I'm using sql server management studio
I frequently have to look at rolling weekly data based on a join between data in SQL Server and Teradata, so I need to use that dynamic SQL to easily align dates. The biggest drawback to me for using it is the readability. Having keywords formatted blue/gray is way better than a solid block of red.
I also have to maneuver through teradata and oracle.
Couple questions. 1. Have you tried running SSMS as an administrator? 2. Have you checked your Services to see if SQL Server is still running?
What type of database are you using? MySQL, Oracle, MS, something else?
Then I think what everyone has already posted is your best bet to start with. Personally I would just upload that Excel file into SQL and use that table instead of the table variable @FruitCounts in he example /u/MaunaLoona wrote out for you, and plug and play. Good luck.
Oracle I believe 
Consider using SERIAL data type for your product_id column. Then you won't have to muck around with functions and procedures to increment your PK column.
The following should work with Oracle: SELECT s1."dupe_count", count(*) AS "pplcount" FROM (SELECT ppl_id, count(ppl_id) AS "dupe_count" FROM ppl GROUP BY ppl_id) s1 GROUP BY s1."dupe_count" ORDER BY count(*) DESC; 
Yep it does, I’ve actually completed the problem yesterday and it basically looks like that. Thanks a lot for the help!
I’ll use dynamic SQL if it means I can pass the code off to other users to run themselves. For example, if I want somebody to be able to compare the values in to databases, I’ll set the server and database as params so the user doesn’t have to modify anything else. 
If you're able to install it you should be able to access the files afterwards. That's really strange. 17 is not an upgrade to 16. You can run the two side by side. Are you sure you're not continuing to launch 16?
This makes sense as well. My stuff is simply to allow me to run queries locally. It’s a daily/adhoc portion of report that can’t specifically be automated or in most cases are needed sparingly. 
we run the code/logic to accept incoming data and then ETL with a google app engine app, which is pretty good at scaling up but with bursts we have to have a set minimum number of instances running always. Though that's something I never really thought about: actually looking at the GAE logs, it would be pretty easy to get them into our database though except for trying to find errors I'm not sure what else we'd spend time looking at
Ty all for great help, I was able to talk it out, got access back and figured out when it starts. It was ssms causing it. Upgraded to 17.7 issue resolved. Job safe and access returned
I think there's an issue connecting to the server itself which is 2012. I've located the mdf (for some reason it wasn't showing on file search) and backed it up. Going to wipe all terraces of sql server tomorrow and stay over.
Here's an example. create table temptbl1 ( ID int NOT NULL AUTO_INCREMENT, field0 varchar(65), field1 varchar(65), field2 varchar(65), field3 varchar(65), field4 varchar(65), field5 varchar(65), field6 varchar(65), PRIMARY KEY (ID) ); Other option: ALTER TABLE temptbl1 ADD COLUMN `id` INT(10) UNSIGNED PRIMARY KEY AUTO_INCREMENT; 
Other item pertains to combobox source. At the moment, data is stored in a Server side table. Some forms have multiple combo boxes, with index/ID for the table value stored in the record. Should I also link those comboboxes to a PTQ or is it better to link the table? I would expect to have purpose built PTQs for each combobox.
If you're sure Site A can ping Site B, then it knows where it is, and can route to it It would suggest a firewall on Site B blocking the SQL port perhaps? 
I thought the same thing, but by using **hostname.domain.com,1433** in the server name field I was able to get it to connect, so it's not the firewall, but I'm still confused as to why I cannot see the SQL 'broadcast'. This was brought to my attention by a coworker and ties in to whether or not we can run SQL queries across subnets. Anyone have any ideas?
By broadcast, do you mean the client doesn't automatically detect the server in the connection dialog (ie, the drop down list)? 
I think you can google the term "OR" in a WHERE clause. The way your query is set up, you'll never return any rows. You should probably write it like this: WHERE (attribute_id = 1 and value = 'A') OR (attribute_id = 2 and value = 'Z') OR (attribute_id = 3 and value = 'X') Notice I also took the quotation marks off of the numbers around your attribute_id values, assuming they are numeric.
When I say that the 'broadcast' isn't visible, I mean that in SSMS when opening a new connection, under **server name &gt; browse for more &gt; network servers** the only SQL servers that populate are the ones on the Site A subnet. And given that I can manually type **hostname.domain.com,1433** to connect to the server but cannot use **hostname.domain.com** (which should intuitively use 1433, I've never had to specify port before), it seems that port 1433 traffic is possible between the two networks but only if I force it. In other words the SQL traffic on 1433 doesn't try to look at other networks and prefers to only bounce around locally but will go from network to network if told specifically to do so. What I'm trying to figure is why a SQL server on 10.1.x.x can't talk to a SQL server on 10.2.x.x and was wondering if this is some sort of known SQL quirk.
Looks like you are using the term "SQL" as a synonym with Microsoft's SQL Server. You might the help you need here, but if not, you might want to post over in /r/SQLServer
The computer is, but the server is installed on C:
where attribute_id in (1,2,3) and value in ('a','z','x');
Right, as is it wouldn't find any rows, but I'm trying to find MEMBER\_ID records who have all 3 of those properties, that would just give me a records with any one of those properties.
But that could potentially find rows that do not match OP's requirements. For example, that could find a row with Attribute_id of 2 and Value of X. Not a listed combination.
That would select records that have any of the values, I'm trying to find records that have all 3 of the specified value.
Ok then, google the term INTERSECT SELECT member_id FROM my_table 
Ok then, google the term INTERSECT SELECT member_id FROM my_table WHERE attribute_id = 1 and value = 'A' INTERSECT SELECT member_id FROM my_table WHERE attribute_id = 2 and value = 'Z' INTERSECT SELECT member_id FROM my_table WHERE attribute_id = 3 and value = 'X'
couldn't he just do a union and get the same results?
A union would include results that appear in any one of the queries. OP wants to return results that appear in ALL THREE of the queries.
INTERSECT?!......processing. If this works, I might love you, asshelmet.
ahhh. i missed that part. pretty crucial piece. lol
This is known as entity-attribute-value schema (some call it antipattern). I would recommend googling 'pivot' for your platform.
Take this with a big pinch of salt, because Im not 100% sure its correct (I ditched SQL server and Windows quite a few years ago). From what I can remember though, SQL Server advertises itself via NetBIOS (which is the broadcast you refer to), but I don't think NetBIOS broadcasts are designed to traverse subnets, even internal ones. A few articles suggest you could support this by creating a WINS server, but its not something I've ever had to deal with 
Check **SQL Server configuration Manager** to see if *SQL Server Browser Service* is enabled and if the ports are setup right under *SQL Server Network Configuration*
Then that's awful phrasing, as it's entirely ambiguous between "You were correct to leave this un-selected" and "The un-selected (and therefore designated as incorrect) item is, in fact, correct and should have been selected"
Set theory is an interesting read on the wiki if that's your kind of thing: https://en.wikipedia.org/wiki/Union_(set_theory) https://en.wikipedia.org/wiki/Intersection_(set_theory)
This may or may not be the problem, but it might help: https://stackoverflow.com/questions/5231712/sql-not-in-not-working A NOT IN query will not return any rows if any NULLs exists in the list of NOT IN values.
Get rid of the second join and move that part to a NOT EXISTS in the WHERE clause.
Which SSMS version were you using before?
Sorry I am lost, do u mean removing the b join entirely? 
Use a window function / partition: SELECT requests.source, COUNT(requests.id) OVER (PARTITION BY source) as nrequests, COUNT(responses.request_id) OVER (PARTITION BY source) as nresponses FROM requests JOIN responses ON requests.id = responses.request_id I haven't tried it, so it might have errors, but that's the basic concept. Let me know if I can help more.
Use a window function / partition: SELECT requests.source, COUNT(DISTINCT requests.id) OVER (PARTITION BY source) as nrequests, COUNT(responses.request_id) OVER (PARTITION BY source) as nresponses FROM requests JOIN responses ON requests.id = responses.request_id I haven't tried it, so it might have errors, but that's the basic concept. Let me know if I can help more.
You're leaving out a lot of information about what software you are using. But in general broadcast searches can't cross subnets unless you have setup a mechanism to handle this. Usually something on the applicable routers that rebroadcasts queries and sends back replies. This isn't very common though because it's at the application layer and has to understand the protocol.
This is one of the reasons I curse the entire time I have to use MSSQL.
Going to write this in psuedo-sql, but you'll get the idea: select a.stuff from production a left join production b on a.customer_id = b.customer_id and b.date &gt;= 2016-01-01 and b.product_id &lt;&gt; 200 where a.product_id = 200 and b.customer_id is null The result is customers who've purchased product 200 but not anything else since 2016-01-01.
I like this answer, the apparent failure of the SQL Browser Service is definitely to blame. The only thing I would add is checking that port 1434 is also open between subnets, if the browser service is running and 1434 is open then you should be able to enumerate remote instances. If you fight with this for an inordinate amount of time, you can always just specify the port every time going forward. Some enterprises I've worked with standardize on keeping the browser service turned off (Security?) so it's something I've had to work with before.
Yes. I'll explain more later if no one has helped you yet. Also your query is missing a line right above B1.
16 installed along with ssdt SQL local server local instance
I know this as a sql query to de-normalize data. Here is a link to a great example. http://www.webdesignforums.net/forum/coding-articles-tutorials/19512-mysql-tutorial-how-denormalize-normalized-table-using-sql.html
A foreign key will not increase the speed of your queries. They are for integrity. Indexes are for performance. I think your mind is on the right track, though. You generally will want an index on the columns you are using in your WHERE clauses and your JOIN predicates. It's quite a broad topic. I'm not really understanding how your tables or queries are structured though, to be honest. Can you post the create scripts from SSMS for the two tables? 
Hi ickies, I have attempted similar syntax in the past with no luck. A1.Customer_ID NOT IN (select B1.Customer_ID from production where B1.Date&gt;= {ts '2016-05-01 00:00:00'} and A1.Product_ID = '200') 
Thanks again! This worked perfectly, aside from the fact that the SQL access I have doesn't have the balls to run the full query I need(I can run up to two of them, and each one individually, but anything more and it times out.) But that's not your problem, thanks again. I'll never forget you, asshelmet.
I like this style best: where (attribute_id,value) in (VALUES('1','A'), ('2','X'), ('3','Z')) I find this syntax the least confusing. I know it works on Db2, not sure about all platforms. Do please be sure you are properly quoting or not quoting the numbers based on whether they are strings or numeric. It may work either way, depending on the RDBMS, but if it is doing conversions, it is likely to be much slower.
It looks like you have duplicate records in the response table. You can easily check this by seeing if this query returns anything: select id, count(id) from requests group by id having count(id)&gt;1; If it turns out you have duplicates you should fix that! If that's not possible you can use a "group by" to de-dup, as that is generally much faster than a subquery that produces the same results: with requests_deduped as ( --deduping so each id is counted once for each source select source, id from requests group by source, id) select requests.source ,count(requests.id) as requests ,count(responses.request_id) as responses from requests_deduped as requests join responses on requests.id = responses.response_id group by requests.source; Those duplicate requests.id values are a real problem, though! For example, what if an id is shared across sources? That's a common problem, and in those cases the actual unique identifier is the combination of both requests.source and requests.id. In which case you need to join on BOTH columns to avoid a Cartesian product. But it sure doesn't look like you have a source column in your responses table. In which case you cannot identify which responses came from which source/id combo, which means you can't answer the question. Feel free to reply here or PM me, happy to help further. 
You're welcome. Glad I can help! 
Check your indexes if it takes a long time to run. I can help you tune it if you need better performance! 
Hey, I learned SQL from the following sources: [Sololearn](https://www.sololearn.com/) [W3Schools](https://www.w3schools.com/sql/) [SQLbot](https://sqlbolt.com/lesson/select_queries_introduction) I'm also trying to complete the exercises from [W3Resource](https://www.w3resource.com/sql-exercises/). Hope it helps, Cheers.
Thank you! It helps heaps. I need to learn it for work.
Good luck! I'm trying to master them
I learnt it from w3schools. It is very good 
&gt;Also, why anyone would want to sack someone over a literal technicality like this is beyond me This could very easily look like attempted data theft. In a corporate environment (bank, financial software vendor etc) that would certainly ring some alarm bells. If I were involved I would also be pulling access until a reasonable explanation is provided.
I used w3schools plus [this book](sql quickstart guide: The Simplified Beginner's Guide To SQL https://www.amazon.com/dp/1508767483/ref=cm_sw_r_cp_apa_Uu4kBb42EEG82) I then practiced at work to nail down the basics now I am using the Practical SQL Handbook 3rd edition to go in depth on best practices to refine my skills
try edx.org free courses with video and exercise but they are very microsoft focused so T-SQL in this case.
You use AS key word. SELECT ColumnName AS YourColumnName
www.practity.com/591-2/ 
You can retrieve the remote data with a PTQ in VBA then reference that data on opening the page. If the underlying table data is small and you want to create a linked table -you can use the form refresh method to update your page. You can combine both techniques the PTQ and the linked table within Access. The linked table method requires less coding and the only downside is the speed. I don't know if that makes sense. This is technically getting into Access VBA coding rather than SQL. 
Just a heads up that there is another sub dedicated to SQL Server. This one is more about the query language. You might find a helpful person here who can help, but if not then you can also check out /r/SQLServer 