Thanks, much appreciated! I'll post there also
You are absolutely right that FKs are there for integrity purposes. However, sometimes the existence of an valid, enabled and enforced foreign key constraint can actually increase the speed of a query. For example, consider this psuedo code: SELECT child_table.col1, child_table.col2 FROM child_table INNER JOIN parent_table ON child_table.parent_id_fk = parent_table.id Some RDBMS engines will actually skip the join and just return rows from the child_table because the foreign key (if valid and enforced) guaranteed that for every child record there is a corresponding parent record. And since there is no other parent_table column that needs to be used, it can skip that table altogether.
Check out pluralsight if money isn't an issue. Some really smart people from the SQL community make great training videos. As far how long it takes to click, that's really up to you. Getting the basics down should be no problem at all with some commitment. 
I started with Bill Weinman's SQL courses on [Lynda.com](https://Lynda.com). Then moved onto Adam Wilbert's series of courses designed for the MTA, Microsoft's entry-level certification. SQL doesn't take any longer than other languages to really "click" if you ask me--at least not for the Data Manipulation Language (DML) side of it. It might be a little disorienting if you're coming from a procedural language background (like Javascript or Python), since SQL is declarative, but that can be overcome fairly easily. At four hours of studying a day I felt fairly comfortable doing basic DML stuff in SQL after about a month. One of the first big roadblocks of SQL DDL is [logical processing order](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql?view=sql-server-2017#logical-processing-order-of-the-select-statement). (That link is for Microsoft's proprietary T-SQL implementation, but it's generally true for all SQL languages). Realize from the start that your leading keyword (SELECT) is actually going to be executed much later than the ones that follow it. One habit that really helped me out with this is to start every query by typing this: SELECT FROM WHERE ... since these three are probably the three most common keywords. Next, build out your query via logical processing order. That means filling out the FROM (and any JOINs), then moving onto WHERE. If I need to add GROUP BY or HAVING, I throw them on there at this point. Only after I've checked down the first seven items from that processing order list do I start building out the SELECT statement. Everything I've described so far is applicable for DML. Depending on what you want to do though, you could find yourself in the Data Definition Language (DDL) realm, and that's a whole different animal.
Free Coursera “introduction to Structure Query Language (SQL)” class, from the University of Michigan. The professor is Charles Severance. It’s my 3rd week and so far I’m enjoying the lessons and practice exercises. Good luck! 
Like others have said, I learned most of what I know by reading W3Schools. Aside from that, I worked through these exercises https://en.m.wikibooks.org/wiki/SQL_Exercises. Specifically, The Hospital is the exercise where things really started to click for me.
Because you haven't actually named it yet, so for example: select name as clientname, count(*) from table group by name The only place you have renamed it is in your output, however you can do this: select cilentname, count(*) from ( select name as clientname, * from table ) x group by clientname 
Can you give us some data structures to work with so we can answer it better?
Thank you.
Sorry I forgot to update. I think all I needed to do was something simple like: Count (col) &lt; 2 
Elaborate?
They probably don't like your table
This isn't about points this is about how hard you try. You need to be more open minded.
Thanks for taking the time to write this, but the multiple responses are not an issue as it is part of how our application works. Thus producing our challenge, we need to be able to count how many requests and responses we get, filtered by criteria from either table (or possibly both).
Elaborate a little bit more for you. The order of execution for SQL clauses are: * FROM clause * ON clause * OUTER clause * WHERE clause * GROUP BY clause * HAVING clause * SELECT clause * DISTINCT clause * ORDER BY clause * TOP clause So it hasn't "renamed" by your select statement until the WHERE and GROUP BY clauses have already been evaluated. 
Thanks, this is helpful, also should have mentioned I am using postgresql, in which I can use column alias in group by, but not in where
I assume you're talking about ERDs, maybe? There are some free web apps online. I've used lucid charts and visio before. There are paid apps that allow you to draw diagrams and models as well. I'm not sure how they compare.
I've been messing around w/ networkx via Python for doing database design. It "kinda" works but I haven't found a good GUI for it. Lucidcharts was already mentioned and that might be the best internet solution. 
Honestly, some might disagree, but it is bad form to use aliases in clauses outside a final select for column names other than computed names. Maybe I am biased since I often work on highly complex SQL, but over use of alias', and more importantly overuse of poor alias' makes debugging code a chore. Even if postgresql let's you do something it doesn't mean you should. In your example, using the alias would hide the fact that the group by was actually grouping by calculated field. Yes, study would bear this out but it makes reading harder.
actually you already almost did it right the column you called key? that should be a `key`. (lol) generally speaking, the rule is "try not to repeat yourself." repetition is slow, and the opportunity for mistakes because most say `pittsburgh` but one says `pittsburg` . you're overdoing it. a table should be a concept, including all its stuff. if a thing changes over time, then the thing that's changing might want to be pulled out. if a thing is getting repeated a lot, it's probably its own concept and may want to be pulled out. it's a judgment call. so, from this, i see at least four tables. 1. the stuff for the company 1. the stuff to describe a city 1. the stuff to describe one person 1. the stuff to describe the changing name of a company and its relevant date range do all your joins on integer ids; string comparisons are slow and can collide. your intuition about the gm being a number into the people table is correct.
not normalized enough, but it depends on how deep down the rabbit hole you want to go For example, companies can have multiple names (GE, General Electric), can be located at multiple addresses, have multiple managers, and multiple phone numbers Addresses are also fucked, if you want to go global. You're not tracking State, and some states have cities w the same name. I believe there are like 5 Springfields in Wisconsin, for example 
22 columns isn’t a lot or even very taxing by itself. It sounds like you’re trying to normalize your data but realizing that most of it is 1 to 1. If you were storing large blobs of text in all those columns, it might make sense to split into other tables, but that doesn’t look like the case. You could create indexes on fields you commonly query if you notice slowdowns in your searches I would suggest not using “name” in numeric columns, as it gets confusing since a name is usually alpha or alpha-numeric. I’ve used auto-identity columns (unique and auto-increment) as primary keys and suffixed those column names with “-ID”. You focus on everything being an attribute of the order, but the order is your set of relationships between your objects. You could create a table of stores and/or managers since either of these can be on multiple orders. Same with customers. Do you have an inventory of what you can order? This should also be a separate table. In the end, your order table can be: -Order # -Address -City -Zip -Lattitude -Longitude -Store # -General Manager # -Store Manager # The # fields are foreign keys to other tables. When you define relationships, what you are doing is creating rules and constraints to prevent violating the keys in the relationship. I don’t know if that will help you understand why we programmatically define them instead of just document them and hope the tables don’t corrupt. Before you worry about BI stuff, work on this stuff so you don’t get overwhelmed. 
&gt; Out of curiosity, where did you learn so much about this stuff? Lots of trial and error, reading code &amp; articles, and surrounding myself with people who are way smarter and more experienced than I. I haven't really been on the app development side of things for a long time.
It should be noted, in case it wasn't clear, that quickbooks is running on a full SQL server.
DROP DATABASE [Quickbooks]; GO Will get rid of it pretty quickly.
Yeah, you are going to need to explain the problem better if you want to get any help.
frankly first I would ask the execs what ERP they want to switch to because from there you can insert directly into the database if its an on prem ERP(that can be a monumental task) or the ERP probably accepts flat file import some vendors offer quickbooks database "upsize" wizards but they only work on the actual quickbooks database if they just want to report on it you might be better off finding a data warehousing tool 
&gt;Right, i'm looking to create ERD's, Conceptual data model, Logical data model and Physical data model. 
suicide
Yes...and don’t forget to follow the advice of Hulk Hogan. Better say your prayers and take your vitamins, brother!
Read the execution plan and look at your dreads and writes. Find where the bottleneck is and fix ot
Wow thank you for all the info! I’ll be reading through those articles tonight The tables are not related - not sure why I mentioned the 2nd :). 
I'm not entirely sure what you're asking. This is not a database question. This is a systems analysis question. It's time to start doing research. Quickbooks is so ubiquitous that any ERP vendor worth it's salt will be familiar with migrating away from Quickbooks. If your ERP vendor doesn't know how to do this, then you probably don't want to buy that software regardless of how good it looks. This is not a simple or easy process. Heck, there's a [Wikipedia article on the pitfalls of ERP planning](https://en.wikipedia.org/wiki/ERP_system_selection_methodology). You (or a third party objective consultant you hire) needs to figure out: * What Quickbooks does well. Yes, people hate it. But it *works.* The fact that everybody gets a paycheck every pay period is proof of that. Other packages will suck at something Quickbooks is good at. Even things like "We can assume that everybody we hire is familiar with Quickbooks," is included here. This one is hard because the things Quickbooks does well your users won't think about. It's often easiest to have each person describe what they do -- including monthly or annual tasks -- and just write all of it down. * What Quickbooks does poorly. In other words, what your users complain about. This is where you'll want to find the gains. * How Quickbooks is integrated with your other systems. You need to understand how Quickbooks works with the other data systems in your business. * What features your company *requires* in an ERP. Like the fact that it has support for federal and state tax systems with vendor updates to handle law changes, electronic check handling, SOX/HIPAA/FERPA/whatever compliance, using an RDBMS, third party reporting, integration with Square or other external systems, etc. * What features your company would *like* in an ERP. Your users that have worked with other systems at other jobs will be useful here. * What features Quickbooks supports but that you're not using. Often, you'll find that the system actually supports some features that your users want, but for some reason your company isn't using them. To do a fair analysis, you'll also have to examine if those features can be made available. * Examine your business processes. You will need to understand what your users need to do, what process they're actually using to accomplish their goals, and what Quickbooks *intends* for your users to use to accomplish those goals. You need to understand if the business process itself is broken or flawed. There will be a lot of, "We've always done it this way," and, "I don't know who thought doing it like this was a good idea, but this is how we do it." This is one of the areas that is politically more dangerous. Your users will feel like you're telling them they're doing their job wrong or planning to blame them. The truth is, you might end up doing that. Basically, you need to look at and account for everything that Quickbooks does, could do, and doesn't do, as well as understand what the complete business process is for all users of the system, understand how every other system in your business uses Quickbooks, etc. This is not an easy or quick undertaking. Information gathering alone can easily take weeks -- think of how many meetings the above will take just to do interviews -- or longer if you've got a large company or complex systems. It may sound like you're building a case for *not* switching away from Quickbooks and instead changing your business process. You kind of are. Changing your ERP software is expensive both in terms of money and in hidden costs. It costs money for the software, money and employee time for training, money and employee time for systems modifications, it's throwing out all the existing knowledge and experience of your users, etc. And in the worst case scenario after all of that you may end up with a system that is *worse* than Quickbooks. And the thing is, you almost certainly will *lose* some features migrating to any new system, so you'll have to pick and choose. Some of your users will hate the new system as much as the old system. Some will hate it more. You're not going to find software that works exactly the same only directly better. That's Magical Christmasland. You need to prove first and foremost that changing is worth the cost. That means you need to fully evaluate your existing system to understand if it's Quickbooks that sucks or if it's your process that sucks or if it's your users that suck (by refusing to use the software or refusing to follow the process). *Then* when you've exhausted what you can do, you can start looking for alternatives. You may choose an ERP package that is specific to your industry. That may give you features that you won't find in Quickbooks or any other ERP, but it absolutely will be less user friendly. If you want alternatives, I would talk to your own employees and see what they have used in the past. Consider talking to friends or acquaintances at other businesses you know to see what they use. The ERPs I know about that compete with Quickbooks are NetSuite (Oracle), Dynamics (Microsoft), Sage, Greentree, and I'm sure SAP has a medium sized ERP offering.
Here is my best effort based on OP's bad description: SELECT year, year - 1 AS prev_year, id AS key_column FROM T1 :)
^exaxctly big big big emphasis on ERPs being way less user friendly vs quickbooks; there will be a huge learning curve SAP's midsize onprem ERP (Business One) isn't all that great ,has a redic data model and a bit expensive to deploy because it uses HANA and their midsize cloud ERP(ByDesign) requires Silverlight to use the web client. for the midsize market I have soft spots for Sage Intacct ,Acumatica and Xero
What you want is called an Outer Join.
Look into a NOT EXISTS clause
From my experience a common practice is to use a left/outer join and search where some value is null in the outer joined table. That would mean it exists in the main table, but does not exist in the joined table. You could reverse it and say where some value in the main table is null and that would return values in the joined table that do not exist in the main table. Example: Select employeeid from EmployeeTable Left Join TimeOffTable on EmployeeTable.employeeID = TimeOffTable.Employeeid where TimeOffTable.employeeID is null This will tell you all the employeeids that do NOT EXIST in the TimeOffTable. Hope this helps.
anomaly
Ok good to hear. I was somewhat aware of this, but wanted confirmation and understanding as to why. Especially why you should join on integers and the value of converting.
In this case companyName is more and internal name for a location (and actually has a different name in my tables but it's awkward for examples). Likewise since every record on these tables will tie to one location there can't be multiple addresses. This should literally be a table of every physical location our company has, and all basic data associated with it. That said I do actually have everything from latlong up to state, just didn't list it all.
the join on integers thing is easy to understand intuitively in three steps 1) on paper, by character the way a computer would, compare `abcde` and `abcdFUCK` until you discover whether or not the two strings are the same 2) do the same for `6` and `14` 3) figure out how many comparisons you need to make on average to sort 10k rows, which is nothing to sql
The way you've described this is unclear. You say *"I'm trying to return values that do not match the criteria"*, but what criteria are you talking about? This? ON tblA.B = tblGenericData.Column001 OR tblA.C = tblGenericData.Column001 Or this? WHERE (tblGenericData.WorksheetID = 1) Or both? Assuming you mean the former, i.e. you want to find rows in `tblGenericData` that **do not** exist in Table A, you would use a `LEFT JOIN`: SELECT colval, etc FROM tblGeneric LEFT JOIN tblA ON tblA.col = tblGeneric.col OR tblA.etc = tblGeneric.col WHERE tblGeneric.ID = 1 AND tblA.col IS NULL; A `LEFT JOIN` returns everything in the left side table, i.e. `tblGeneric`. For rows in `tblGeneric` that do not have a corresponding row in `tblA`, all columns in `tblA` will have a value of `NULL`. The final line in the `WHERE` condition -- `tblA.col IS NULL` -- selects for these null values. 
Understanding the process is just as important, if not more important, than knowing how to code. Keep pushing.
&gt; midsize cloud ERP(ByDesign) requires Silverlight to use the web client. Which makes it pointless to buy today, since Silverlight is only supported by IE 8, 9 &amp; 11 andMicrosoft has been [telling people to move away from it for a long time](https://blogs.windows.com/msedgedev/2015/07/02/moving-to-html5-premium-media/)
Use a subquery to get the minimum date with a value gte 1, then use a CASE to set the flag: select BUSINESS_DATE, COLUMNA, case when BUSINESS_DATE &gt;= MIN_DATE then 'YES' else 'NO' end FLAG from TBL cross join (select min(BUSINESS_DATE) MIN_DATE from TBL where COLUMNA &gt;=1) ;
yeah i don't understand why SAP even bothers with the midmarket considering they pretty much own the Fortune 500 market, both their offerings were acquisitions to begin with and bydesign is slower than molasses to boot (the actual API servers are slow) 
Another idea is to use a subquery, to avoid the potentially expensive cross join select Business_date, ColumnA, CASE WHEN EXISTS (select b.ColumnA from Table where b.Business_date &lt;= a.Business_date and b.ColumnA &gt; 0) THEN 'Yes' ELSE 'No' END as Flag From Table a
Or just ditch the WHERE clause and use a RIGHT JOIN
You just suggested an even slower method by suggesting that the EXISTS query run one time for every row in the table, against a potentially large table. The cross join is not that bad, and will perform loads better than the slow-by-slow method you've got there. If performance of the cross join is a concern, and you want to run a scalar subquery in the SELECT clause, then obtain the date first in a WITH clause or CTE like this: WITH a AS ( SELECT MIN(BUSINESS_DATE) AS MIN_DATE FROM TBL ) SELECT Business_date, ColumnA, CASE WHEN BUSINESS_DATE &gt;= (SELECT MIN_DATE FROM a) THEN 'YES' ELSE 'NO' END AS FLAGEXISTS FROM TBL
Storing like-data across multiple tables is not a good idea. I would not create a new table for each sample. Instead, write all results to a single table and devote a column to identify the sample. If I were building this I would create three tables: events, samples, and sample\_items. 1. events: Each testing event is represented by a single record in this table. Data describing the event (e.g. event name, event date) can live in this table. PK: event\_id 2. samples: Each event sample is represented by a single record in this table. Data describing the sample (e.g. sample name, sample type) can live in this table. PK: sample\_id and FK: event\_id 3. sample\_items: The individual log records belonging to each sample live in this table. Data describing the individual item can live in this table (e.g. test\_result). PK: sample\_item\_id and FK: sample\_id Example SQL: select e.event_name, e.event_date, s.sample_name, s.sample_type, si.test_result from events e join samples s on s.event_id = e.event_id join sample_items si on si.sample_id = s.sample_id where e.event_id = 9356
Ahh! This is an OLAP like question, then. Since your tables have a many-to-many relationship, you cannot join them together directly. Instead, first you need to aggregate both tables up to the dimension(s) you want to report at. Only join them after aggregating, and only join then on the dimension(s). So in your example, you need the column "source" to be on both tables in order to count responses and requests. You cannot join responses to requests to get their source, because they are a many to many relationship and each id can have many sources. You can write the sql like this: with agg_responses as ( SELECT **source**, COUNT(*) as responses FROM DATASET.responses WHERE id IN (SELECT id FROM DATASET.requests WHERE source in ("source1", "source2") GROUP BY **source** ), agg_requests as ( SELECT **source**, COUNT(*) as requests FROM PUBDATA.requests WHERE id IN (SELECT id FROM DATASET.requests WHERE source in ("source1", "source2") GROUP BY **source** ) select **agg_responses.source** ,agg_responses.responses ,agg_requests.requests from agg_responses join agg_requests on **agg_responses.source = agg_results.source**; Now, in this query you're analyzing counts by the dimension "source". If you wanted to add an additional column to be analyzed (say, the imaginary "foo" column), you should add it to the group by on BOTH aggregations, AND also add it to your join columns. Like so with agg_responses as ( SELECT **source, foo**, COUNT(*) as responses FROM DATASET.responses WHERE id IN (SELECT id FROM DATASET.requests WHERE source in ("source1", "source2") GROUP BY **source, foo** ), agg_requests as ( SELECT **source, foo**, COUNT(*) as requests FROM PUBDATA.requests WHERE id IN (SELECT id FROM DATASET.requests WHERE source in ("source1", "source2") GROUP BY **source, foo** ) SELECT **agg_responses.source** ,agg_responses.responses ,agg_requests.requests FROM agg_responses JOIN agg_requests ON **agg_responses.source = agg_results.source AND agg_responses.foo = agg_results.foo**; 
That makes much more sense. I forgot to mention that the data can be of varying types with vary different structures. In this case would I create a table for each type then reference the sample_type field in samples to determine which table to query?
Can you provide an example of what you're asking? I not quite following. I was thinking that each sample would be a record in the samples table. Type was a theoretical attribute you might store in there.
Take a look [at this page](https://stackoverflow.com/questions/17112852/get-the-new-record-primary-key-id-from-mysql-insert-query) for a discussion on using LAST_INSERT_ID(). Insert into the parent table first, then grab the new auto-incremented primary key, then use it to insert the record of the child table.
So, each event produces samples (physical objects) which are then tested in different ways using different machines. So if an event has 4 samples, 2 events could have one type of test done on them, and 2 could have another. The data produced all falls under the same event, with each sample having it's own test data, which can be of the same format, or different formats.
Try
This is a good opportunity to learn about stored procedures. https://dev.mysql.com/doc/connector-net/en/connector-net-tutorials-stored-procedures.html You can also inefficiently handle this in PHP. I don't recommend it, but it can help you get something up and running quickly since stored procedures can be a little tricky to learn.
This is a one off, for fun site for myself essentially. Just some practice with web development since I've barely done any. I'll look this over, but quick and dirty php is good enough for what I'm doing.
Thanks, I found [this](https://www.w3schools.com/php/php_mysql_insert_lastid.asp) site and it worked perfectly for me. 
Look up udemy classes “ search for sql “ and u’ll fund many rated courses that u can pick one of them. I wouldn’t recommend books as for me vidoes r more intertaining.
I learned using Itzik Ben-Gan's T-SQL Fundamentals. It's a great book that covers a broad range of topics. It's focused on (MS) SQL Server but it's convers a lot of ANSI stuff. I'm using Postgres now and I can use a lot of what I learned.
Is a test type 1 to 1 with a sample? If so then just add a column called test_type to your samples table.
I think my nomenclature has been causing some confusing. The actual process where data and everything is coming from is like this: A trial is run with the idea of running some test on the specimens produces, e.g. trial 20 produces 4 specimens to run some type of test on (such as temperature testing). Each of these specimens is put through a machine that generates a csv file for the test, so we have 4 csv files for each specimen from trial 20. There has to be a log detailing the trial that made the specimens, and a way of storing the test data. The structure of the csv files is very dependent on the test, but there are a finite number of tests being done. This was the main rational of using a schema per trial with tables per specimen as there is one type of test per trial, but the data structure will be different for different test. I believe it would also be possible to simply use a table per trial with a specimen column to select data by specimen, but in doing that there are still several tables with the same structure.
Start with code academy or sql zoo. There’s a bunch of other resources online as well. Once you have the basics down try to find some data to play with. It’s much easier to learn if your are writing meaningful queries. Remember when you get stuck, google and stack exchange are you friends
Good! Cheers, man.
Build a mesaage forum! Easy way: 1 table that logs the message, user, id, if its stickied Harder but better: 3 tables. 1 table for usernames and userids, another table with messages, message id, and thread id, and another for thread titles and thread ids. 
Most video courses are variations of the same basics. Books by far any away give you a greater insight and understanding of a wider range of topics and principals. My feeling on it anyway.
Self plug: I'm running a website/blog that focuses on advanced SQL features and concepts: https://modern-sql.com/ Also, if you'd also like to learn more about indexing and performance, I run another site that focuses on that: https://use-the-index-luke.com/ Unlike modern SQL, Use The Index, Luke! is already "done" and has also been published as book under the name "SQL Performance Explained".
And what if we I use Python with it. Can you tell me in that case? 
Are you looking to build an accompanying web front end as well? Learning a server side scripting language can be fun. There’s also lots of publicly available data sets that are fun to mess with. You could take one of these datasets and transform it into a cleaner relational model. 
Actually I am going to work on a project where I'll be needing them both. 
Also can you tell me the name of that website
Okay, that is much more informative. My previous design suggestion did not consider that the data format would be different for each test log. Scrap the sample_items suggestion. Create a table called sample_tests that FKs to samples. Then create a table for each test log format. In sample_tests, create a field called test_type that will tell you which type of test was performed so you know which test_log table to join to. Note: If it's only possible to test a sample once then replace the sample_tests table with a column called test_type in the samples table.
Not really sure, if you figure something out i would be interested to know! I use a program called Ignition thats by a company called inductive automation, but its for induatrial automation and probably wouldn't help you very much to learn unless thats what you're trying to do. It does have an infinite resetting free trial though, and is pretty user friendly 
https://www.w3resource.com/sql-exercises/sql-retrieve-from-table.php#SQLEDITOR 
I think that will work much better, thanks for the help.
i can help you rewrite the query, but unfortunately i cannot see which table the various columns belong to please do a SHOW CREATE TABLE for both tables, thanks
This is a compilation of exercises, projects, tests and so on. Only Practice http://practity.com/591-2/
Good luck!
I updated the post with the creat table statements
okay, try this -- SELECT COUNT(CASE WHEN booking_data.is_canceled = 0 AND booking_data.is_partial_cancel = 0 THEN 1 ELSE NULL END) AS total_bookings , SUM( CASE WHEN booking_data.is_canceled = 0 AND booking_data.is_partial_cancel = 0 THEN Payment ELSE NULL END) AS total_payments , COUNT(booking_data.booking_id) AS raw_bookings FROM tutoring_booking INNER JOIN booking_data ON booking_data.booking_id = tutoring_booking.booking_id WHERE tutoring_booking.booker_id = 1 the fact that ~both~ `booking_data` and `tutoring_booking` have ~independent~ **auto_increment** columns called `booking_id` leaves you wide open for them to fall out of sync 
I was converting code from oracle to MySQL and so when I was recreating the tables I made some mistakes.
No, almost never unless you are writing queries for DBA reporting (ie, report showing index coverage etc). Just pick a convention and go with it. I usually prefix with idx_ for normal indices, cidx_ for clustered and uidx_ for non clustered unique indices Doesn't really matter though
so did you run the query?
Yes I did it runs perfectly. I didnt know yu could case statements like this. Why do you use an outer join?
&gt; I usually prefix with idx_ for normal indices, cidx_ for clustered and uidx_ for non clustered unique indices upvote for good naming convention
&gt; Why do you use an outer join? in case the booker has no bookings, you'll get a result with zeros instead of no results at all
Data.gov is a good resource. Also your local city may offer locally related data sets. Data.world also has some good stuff. As for the server side scripting, I learned with Php. Definitely not the sexiest language in the world, but I had a blast building my first LAMP stack project. 
What about Python?
Python is great way to go. It is my day to day Swiss Army knife. It could be used for light weight web development as well as data science, etl, and just about anything else you can think of. 
Na, if I wanted to use it with MySQL.
For me it's ix, ux, c(u\/i)x, and ixcs, cxcs for SQL Server's columnstore indices. Also the obvious pk and the less obvious puk for nonclustered PKs. Don't get me started on constraint naming.
So where r the books u talking about
In SQL Server, you can also force a query to use a certain index. Not sure if you can do it in Postgre. https://www.brentozar.com/archive/2013/10/index-hints-helpful-or-harmful/
Ohh ok thanks for your help.
You don't have to, it is optional. 
Have you looked at the actual database to verify that 1) the data is actually being written, or 2) that the table isn't full of rows written multiple times? I don't see anything wrong with that snippet - but you might try wrapping it in a try/catch block to capture any run time errors that might be occurring. 
Yeah, I got it! My Project Solution has 2 mdf file, the first is at the code, when doing some editor. The other one, is at the 'bin' folder when I run the code. So I have to set 'Copy if newer' the database. Thanks thanks! Solved!!!
Why aren’t you closing your connection?
Either you're connecting to a different database, or the transaction is being rolled back somehow. Is there a reason you're using `ExecuteScalar()` instead of `ExecuteNonQuery()`? AFAIK, it should not matter, but unless you're going to include something like `SELECT SCOPE_IDENTITY()` it doesn't make sense.
How can I open an mdf file? MS Access?
I would also suggest you using stored procedures, as it will be much easier to pass value parameters, since at some point your values will definitely have some characters that will require additional attention (',",/ etc.).
When 'using' in C# automatically close your connection after.
What is the difference between ExecuteScalar and NonQuery?
https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand.executescalar(v=vs.110).aspx https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand.executenonquery(v=vs.110).aspx ExecuteScalar returns the first column of the first row of the result set or returns null. [Scalar](https://en.m.wikipedia.org/wiki/Variable_\(computer_science\))means "single value". It's useful if you need a single result item, like a row count. ExecuteNonQuery returns only the number of rows affected, or -1. Output parameters are still populated in the case of a stored procedure. ExecuteNonQuery is primarily used with INSERT, UPDATE and DELETE commands as well as calling stored procedures. It should not matter which you use since both execute the query, but they signal a different intent.
Microsoft SQL Server Management Studio (SSMS) - there are free-to-use downloadable Express &amp; Developer editions.
Cool. Does the Dispose() called by using commit or rollback the implicit transaction?
No you should not have many to one relationship from facts to dimensions, why would you even want that?
Thanks!
Hehehe I don't know what is rollback or implicit transaction.
Right, a every item on a sale would be on a fact table once. If you wanted to lookup something about an item you'd join to the dim table. If you have 2 records in the dim table that join to that fact table you would double sales totals for that item. This would throw off all reports. Many items on the fact table would join to the same record on the dimension table though. EG a widget would be on the fact table each time it was sold, but it'd only be on the dimension table once.
Manually call the connections close method. See if that helps. Every dB action causes a transaction. If you don’t create one, one is created implicitly. Depending on how the dB driver interprets the settings, it should commit the transaction after every SQL statement. However, some databases interpret a dropped connection as a forced rollback. This causes any changes you performed to get “rollback” as if you never did anything. It’s possible for a dispose to be called that releases the resources but doesn’t correctly close the connection, for any number of reasons. This could cause the connection to be seen as “orphaned” and it gets killed off and everything rollback. I understand USING will call Dispose but that carries a lot of assumptions. I tell my developers that if they are confident everything will be fine, it’s ok to do it. But if it errors even once in production, they have to fix their own issues.
Well databases are two things, the engine and the data. Sounds like you need to install or have access to the Vertica Database Engine, and from there you can work on adding data, etc. I assume if the link here: [https://www.vertica.com/try/](https://www.vertica.com/try/) is not helpful, then you might need to work with someone who can give you that environment. However, it is not uncommon for companies to provide training environments for their team members. I'm assuming this is training for your own improvement, so you'll have to do your own IT to figure out how to install. If you want to learn SQL you might have to try the other database engines like MySQL, Postgres SQL, even MS SQL Developer Express all have installations or trials you can use. It just takes a bit of IT knowledge to get it all running. You might want to try the WAMP installer, it has more than what you need; but it does install MySQL. [http://www.wampserver.com/en/](http://www.wampserver.com/en/)
Is there a particular feature you need from Vertica to learn, or are you just trying to learn SQL basics? If it's the latter, you have online options (Mode Analytics and W3Schools have good tutorials and interactive "shells"), or if you want to familiarize yourself with DBeaver, you can install SQLite, create/set up your database server, and connect to it.
Postgres intentionally decided not to provide such a feature: https://stackoverflow.com/questions/309786/how-do-i-force-postgres-to-use-a-particular-index
&gt; However, as far as I understand in relational databases, you should only use UUIDs or SERIALs as primary keys. nope... you can use whatever you like, as long as it's unique find an algorithm to generate a short alpha string on the client side (e.g. anw9dk, like you suggested), and use that when inserting new rows it doesn't have to come from a serial integer 
For SQL practice try here: [http://practity.com/591-2/](http://practity.com/591-2/)
I always recommend [sqlzoo.net](https://sqlzoo.net) for beginning practice, the site is really well laid out. This takes you pretty far into the basics at least.
1. You don't need to do this. You want access control on the web app or in the db 2. Pg supports row security. Use `set role` to change the user after login: https://stackoverflow.com/questions/2998597/switch-role-after-connecting-to-database/19602050#19602050 3. In the java world you would use spring security + spring data to enforce access control
My problem is that I'm not sure how to set up a database server and all of that. Googling SQL tutorials just gets you information on setting up queries, not Set Up SQL For Dummies. :(
I know what your feeling. You simply want to get to the point you can enter something and see an output, even if its an error, but an understandable one. If you use Microsoft's sql server (the developer edition is what you need) it has an option to install the "adventureworks" database along with the install. This is the database used for most sql server tutorials. So, you need a server to host the database. This can be simply installed as a localhost on your personal computer. It's a server because you need to start and stop the service before you can do anything with the database and is what exposes the db across a network (with the localhost you dont really worry about that but explains why). So install the database server but you still need to build a database and add tables. Once the server is installed you use that application/service to create a database, designate space and resources for it. Then you are ready for create table statements. Just stick with it. I know how your feeling and your on the right track. Break up your thinking and searches into 1. Setting up a database server 2. Creating your first database 3. Create tables and start using sql! I don't know the engines or software your referring to and that might be your problem. Microsoft sql server has by far the most free help, followed by mysql and last Oracle (they literly pull free help sites people put up so you have to call them. Their software is over 300k per year so it's somewhat justifiable but also limits their customer base and need for developers to learn it. 
Is there is a reason you've chosen Vertica as your platform? From what I can see its intended for big data (and if its your first venture into SQL I can almost guarantee you wont be using what's classed as 'big data'). I'm not saying its definitely the wrong choice, just if you simply want to experiment and learn SQL, it probably is. 
Your SQL transaction isn't getting committed by the C# code. It's been a while since I've done that in C# but look up the sqlconnection or sqlcommand commit method and call it before you let garbage collection close the connection for you to fix this issue. 
Well you kinda said the opposite of what OP asked. Technically you do want one-to-many from a dimension (parent) to the fact (child). Many fact records relating to one dimension record is ok. But not the other way around. 
Would I put this under the if statement? if (sh\_car = 'FedEx') then DELETE FROM ORDERS WHERE CURRENT OF CSR; DELETE FROM ORDER\_DETAILS WHERE CURRENT OF CSR; DBMS\_OUTPUT.put\_line (cnt ||' CNAME = ' ||c\_nm ||',' ||' CITY = ' ||c\_cit ||',' ||' CARRIER = ' ||sh\_car ); END IF; 
Won't a string PK hurt performance though?
nope 
I chose Vertica because it's what I'll be using for work. I'll have opportunities for on-the-job training but I want to get it set up to practice on my own.
Wait... what?
A quick look at Youtube yielded [a pretty good tutorial video on the process](https://www.youtube.com/watch?v=fmq6-wvbxyA). I agree with the poster below that trying to install and set up Vertica might be more hassle than it's worth for you, but since you're going to be working in DBeaver, it's probably worth your time to install it and SQLite/mySQL/SQL Server so that you can get used to the environment. Again, SQLite looks like the easiest solution in your case.
You want Vertica Community Edition server on your laptop. I’ve never heard of Vertica before, but I’m pretty sure that’s what you want. It sounds like the name ‘server’ is throwing you off. ‘Server’ doesn’t necessarily it’s in the cloud or it’s on a different computer. It’s just any program that provides a service to other computer programs. Often, database servers need tons of storage and reliable network connectivity, so they usually run on dedicated server hardware. Other databases, like sqlite, are designed to run locally. You want a local Vertica database server &amp; it looks like the free option is their ‘community edition’. Then you use DBeaver, also running on your laptop, as your client. Vertica sounds pretty unique, so I do think it’s worth getting Vertica specifically as your training tool and not some other sql db. But it also sounds like a complicated stack - if you’re having trouble, that doesn’t necessarily mean you’re dumb or you’re doing badly. Many developers don’t have this problem because they’re not that curious &amp; never try to run a local server. Keep pushing.
Your commit is inside your exception handling block. 
If you just want a local SQL (no Server), try SQLite: sudo apt-get install sqlite [SQLite.org](https://sqlite.org/index.html) has a good language reference and there are lots of tutorials, as SQLite is used a lot for mobile devices.
Don't forget users may want more than one view per type of table. Keep it simple what is wrong with something like this? You then map the columns back to your hardcoded counterparts. column_view = id, name, table?, user_id, date_created? column_view_columns = column_view_id, column_id, order 
Go for a practice oriented tutorial. The following resource, for free, may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. 
Go for a practice oriented tutorial. The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An advanced course is also available.
The order does not matter at all. SQL is a declarative language: the system will use internal meta data and data properties (statistics, selectivities, histograms, availability of indexes, sort orders, ...) to decide the exact evaluation strategy. The order in which you write down the terms in the above equality predicate does not influence that decision. 
As a general rule no it doesn’t. You can see this by checking the explain plan where they are typically identical. That said the optimizer only follows so many paths and there are ties. Thus it is possible to have a performance impact by the order and I have seen this before. Generally this is remote enough you don’t bother to worry about it. 
I read somewhere that when you have a bunch of nested queries the SQL engine's attempts to determine the most efficient execution would take too long and hinder performance. And so order matters when you need to force the path. In that case, does it ever matter if foreign key goes first or primary key?
In general, you *cannot* use SQL syntax to (reliably, deterministically) influence the system's choice of evaluation plan. Any particular system will have quirks/drawbacks in which details of the SQL surface syntax will lead the system to favor a particular plan, but that's no basis on which you could tune query performance in any useful way. There is no concept of "forcing a path." (That said, check you particular SQL implementation's documentation — it may feature so-called "SQL plan hints".) The efficient processing of a complex nested query needs way more complex strategies to improve performance (e.g., query decorrelation and unnesting — those entail query transformations that go far beyond the simple exchange of the sides of an equality comparison).
"... I'm pretty much guessing..." Well, moving on... 
It really depends on where it’s getting stuck and what you are doing. Is the bottleneck CPU or IO? Is is a poorly written app locking rows while other sessions are trying to read? It could also be bi/reporting users doing heavy reads on the DB causing IO delays. Most DBs aren’t CPU bound. If you can, sit on the server and watch it during a peak time and see where it is getting flogged. A memory upgrade could be a cheap first option as more of the DB can be cached if your IO is slow, but moving to SSDs is defiantly insane.
I'd really love to see the text on the slides. Is there a less jpeg version of this somewhere?
Doesn't seem that insane to me. The size on disk of the database isn't that relevant, it's the size and scope of the workloads. The vendor spec seems reasonable for a small MS SQL instance.
But SSDs? That’s the part that throws me! The rest of it seems like a good plan to handle a decent amount of growth.
If you pick the right SSD, like the Samsung PM863a, they can be a viable option these days. But I'd be more concerned that the spindles be configured for the load type - write, read, mixed, etc.
I think your current server is under-spec'd, but we also don't know your current workload. I think we'll all agree that 13GB is too little RAM to run a production SQL instance though. It sounds like you're running your "server" on a mid-spec desktop workstation, really. The vendor is spec'ing out a server that should have *no* performance issues with their software and your scale so that you stop complaining. Which part of their requirements do you think are "insane"? The SSD may be overkill (though having *some* of your storage on SSD will be very beneficial) and 8 cores may or may not be too much, we don't can't say (see above). Everything else seems reasonable to me. Is the satisfaction and efficiency of your users and reliability of your system work the relatively small amount of money this will cost?
Lmao I though the same thing 
He said that he needed each year in all 6 tables. So I'd add a distinct to your select and join all 6 tables on the year column. Might need to use datepart if you'r working with a datetime/date column and not just an int of the year.
Piggy backing that question. Say I have an index on three columns, such as: `create index IDX_Blah on dbo.Table([Col1], [Col2], [Col3])` When joining to that table from another table with the same index... is the order at which you join at all relevant: on a.col1 = b.col1 and a.col2 = b.col2 and a.col3 = b.col3 vs. on a.col2 = b.col2 and a.col3 = b.col3 and a.col1 = b.col1 I always try to keep things in the same order whenever possible, and I assume it doesn't matter.
Everyone is talking about CPU/RAM and workloads, which is all true, but the real thing that sticks out to me is that they are suggesting you go from 2012 Web to 2016 Enterprise. That's going to be a massive licensing increase, probably much more expensive than the server itself. I can't imagine why they would suggest this, there isn't anything in Enterprise that's going to magically speed up your workload over just using Standard or sticking to Web. It's definitely worth running some performance tools against your instance, even something free like the suite from Brent Ozar but SolarWinds or Idera might be better. See where your actual performance bottlenecks are and work to correct those. Your edition of SQL won't be one of them though.
This doesn't look to far off to me. All ERPs are different and they are hungry for resoures... for example my ERP has a full enterprise RDBMS as a back end, a handful of middleware/technology stack components such as a webserver application, forms server, reports server, etc, and an HTML and java-based front end, all running on the same hardware. The hardware is 32gb ram, raid 10 SSD SAN shared with other servers, and 64 bit 6-core CPU. Our database is about 200gb. And TBH I think it runs like shit. If I could make one change, I'd double the ram. After that I think our SAN is the bottleneck, even though it's state-of-the-art SSD. The thing is, with all ERPs, there's several components, so it's never as simple as spinning up a single-application server.
You are right: the order of comparisons in the conjunctive join predicate does not matter. Things are different if you are trying to support an `ORDER BY` operation with that index. In that case the order of the sorting criteria must match the column order in the index: ... ORDER BY col1, col2, col3 will be supported by the index (as will be `ORDER BY col1` and `ORDER BY col1, col2`), while .... ORDER BY col2, col1, col3 will not enjoy index support. 
"... I'm pretty much guessing ..." Uh. Me too, thanks.
Thanks, I rarely use ORDER BY but that's handy to know. I generally try to always keep the order in line with the index just as a best practice but it was always one of those little nagging questions that were never important enough to ask without a reason.
&gt; that they are suggesting you go from 2012 Web to 2016 Enterprise They're suggesting &gt;SQL server 2016 Enterprise Standard Which isn't even a thing. You're right to point out that this is weird.
Which pixel is the powerpoint, please?
&gt; Im a sql noob. experience is the best teacher... try a query or two, and if you run into problems, post the query here along with the results you got
Well, without any further clarification from OP, I just threw that query out there as a joke. Ha!
You can do it in a single pass with analytic functions and a rolling window. What RDBMS are you using?
 Are you really currently running on a 100Mbps NIC? That alone would be a major bottleneck... gigabit NIC, more RAM, and make sure you're doing regular maintenance on the indexes.
What info? actual baseline info, and testing data and sql configuration. etc
This will work for you: WITH RAW_DATA AS ( SELECT A.PRODUCT , A.WEEKNUMBER AS 'INTERVAL' , A.SALES AS 'INTERVAL_SALES' , B.WEEKNUMBER , B.SALES FROM [WORKSPC].[dbo].[EX_import] A INNER JOIN [WORKSPC].[dbo].[EX_import] B ON B.PRODUCT = A.PRODUCT WHERE (A.WEEKNUMBER = B.WEEKNUMBER) OR (B.WEEKNUMBER !&gt; A.WEEKNUMBER+3 AND A.WEEKNUMBER !&gt; B.WEEKNUMBER) ), INTERVAL_SALES AS ( SELECT PRODUCT , INTERVAL , TOTAL_INTERVAL_SALES , ROW_NUMBER() OVER(PARTITION BY PRODUCT ORDER BY TOTAL_INTERVAL_SALES DESC) AS 'RN' FROM ( SELECT PRODUCT , INTERVAL , SUM(SALES) AS TOTAL_INTERVAL_SALES FROM RAW_DATA A GROUP BY PRODUCT , INTERVAL ) X ) SELECT PRODUCT , CAST(INTERVAL AS varchar) + '-' + CAST(INTERVAL+3 AS varchar) AS 'BEST_CONSEC_4WEEKS' , TOTAL_INTERVAL_SALES AS 'BEST_CONSEC_4WEEKS_SLS' FROM INTERVAL_SALES WHERE RN = 1 To answer a very often repeated question: How do I get better at SQL? Well, this is how right here. I might have done someones homework, or I might have just helped someone do their job... dunno, fairly interesting question that took me a bit of time. It's a hacky solution and I don't like how it from an analytical or statistical approach, but it really wasn't difficult to import a [data file](https://drive.google.com/open?id=1oab3ZvL091g83Y9k6Dyhrwy0MyjB69vj) and start playing around on my local database to come up with an answer. Who cares if you do someone else's work for them if it means you learn and advance your career? And that's how you get better at SQL.
How can you do it in a single pass? I posted some working code in the thread.
This is the correct answer. It sounds like his current configuration is about as fast as my main desktop, or actually a bit slower. For dedicated and highly complex analytic models my specs are OK for running some queries in a few hours, but the idea of using it as a 'production server,' or 'shared environment' is a bit of a stretch compared to the architecture we have at work.
Why not create a subquery which only takes the rows with a 4 (as per your example) and then INNER JOIN that table back to this table with FULLTABLE.COLUMN A = MySubQuery.COLUMN A. That would work. 
Your query selects two values of cst_recno. If you look at your query, it does a SELECT on: individual.cst_recno and co_customer.cst_recno The "individual" comes from a subquery which is in the RIGHT JOIN(.......), where it does some further joints and date checks in the WHERE clause. The "co_customer" table is in the RIGHT JOIN co_customer By prefixing the column name with a table name or identifier of a subquery - such as the "RIGHT JOIN(.....) AS individual" - the query knows which table or subquery to take the column from. If you can want, you can theoretically do a join on the same table multiple times, and select the same column multiple times, as long as you give each JOIN a unique alias. It is used sometimes if you want to JOIN on a table two times where you do different logic in the ON clause on each JOIN.
Ok, valid reason then. I was just checking you hadn't read some tutorial and opted for Vertigo thinking it's a popular default choice 
Are you sure it’s sql web edition and not express? Web is only available to SPLA partners. 
This sounds like hw
I found it much easier to hold shift with my left pinky than to switch between caps lock. I think you will get used to it, just need to work on the muscle memory for your pinky by practicing code a lot.
Came here to say "left pinky." The One True Answer.
Ditto
Use an IDE with code completion or a text editor with a plugin for that language to recognize the reserved words. And "beautifiers" in those editors will format you code for you, complete with letter case and indentation.
Because case isn't important, syntax wise, you're just being a bro and keeping it easy to read. Pinky is the best trick, agreed. Maybe could try Notepad ++ Find "select" and tellace with "SELECT" for all your SQL words... but careful what it'd do to your data :P Truthfully, you get used to it, and fast as time goes on.
You get used to it. Just keep that pinky working, and it will prove as effective and crucial as each other finger. Maybe get tiny weights, and dedicate some gym-time to your less fortunate fingers.
i mean i just don't do it in the first place SELECT column FROM table WHERE column = 10 vs select column from table where column = 10 truth be told i find the second easier on the eyes ymmv
If you use vim (or an editor / IDE with vim-like bindings), you can select words in visual mode (`v`) and then `~` to toggle case, `u` to make the selected words lowercase, or `U` to make them uppercase (I use the latter two, personally)
Thanks for the suggestion. Yea, I'll definitely want to implement multiple views. Was worried that I'd be walking into some pitfalls by managing table metadata as part of actual row data, stuff that I might not be aware of. But it looks like this structure works, so I'll just keep it simple and store the metadata.
yeah I've been using the left pinky like /u/186282_4 says as well. I guess it's just a matter of getting used to it.
I type in all lowercase, copy and paste to a sql formatter (Google search it), generate it, then save the sql scripts. Vola!
Couldn't agree more. Been doing it that way for decades. lowercase directives, camelCase functions. Works like a charm. Clean and modern-looking, too!
oh my god that is beautiful. I love that something like that exists.
I use Datagrip, but other sql could be the same, I just write my code then CTRL+ALT+L to format in the way I like it
I usually type it with my inky on shift if it's going to be a saved view or whatever, but specifically not when doing something temp. Often though I'll decide to keep something so I need to format it. I use auto hotkey and I found a script online which will convert case. So you highlight a word, hit a shortcut, and it will convert the case. 
I ignore case while writing cause it doesn't affect readability for me. When I'm done, I'll put my work through a formatter like http://poorsql.com/ Ive also used plugins in an editor to capitalize for me. 
Sometimes, when I know I'm going to have to post my code where others can see it, I'll use the SQL Beautifier in SSMS. Other times, I'll just hold down the shift key to type out the commands, and to hell with the tabulation.
I prefer lower case + a consistent indentation pattern &gt; select &gt; column &gt; from &gt; table &gt; where &gt; column = 10
I finally installed their plug in the other week for SSMS. No idea why it took me so long to do it but loving it when I have to paste massive queries that have no line breaks from our applications when they break. Not perfect to my preferred style but it's close enough and have it on a simple hot key to reformat.
Never heard of Crate but it looks like it's designed for large number of concurrent inserts, like for sensor data. To get a high degree of parallelism it does away with ACID, at least in some form. You wouldn't want to run your banking database on it but for a large amount of 'continuous' data, such as sensor data, the tradeoff might be worth it. Other than high volume of sensor data what else can it be used for?
Caps lock? That's what I use
Just join us on the dark side and type all lower case. All caps are from prehistoric times when you didn't have other means to detect keywords in the code (i.e. Syntaxt highlighting). 
If you're doing it often why not wire it to a hotkey in your editor?
In SQL Server: WITH CTE AS ( SELECT *, SUM(SALES) OVER(PARTITION BY PRODUCT ORDER BY WEEKNUMBER ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) Sum, MAX(WEEKNUMBER) OVER(PARTITION BY PRODUCT) Weeks FROM Products P ), CTE2 AS ( SELECT *, ROW_NUMBER() OVER(PARTITION BY PRODUCT ORDER BY Sum DESC) MaxSum FROM CTE WHERE WEEKNUMBER &gt;= 4 ) SELECT PRODUCT, CONVERT(VARCHAR(10),WEEKNUMBER-3) + '-' + CONVERT(VARCHAR(10),WEEKNUMBER) BEST_CONSEC_4WEEKS, Sum AS BEST_CONSEC_4WEEKS_SLS FROM CTE2 WHERE MaxSum = 1; Query assumes each product is sold for at least four weeks. 
Can't quite do it without a subquery. Check out my answer on how to do it with window functions.
&gt;Storage: Raid 10 8x512 Gb SSD Yeah sounds like a scam. Ask them to justify it and back it up with data. I'm really curious how they will justify this one.
I feel that good indentation and ANSI joins makes for far more readable code than uppercase. The uppercase thing is quite archaic and unnecessary, since variables and keywords in most modern languages are in lower case.
The database is hosted i.e. we rent the Express license.
What we've seen is that it sometimes happen to have the locked rows problem. Also, Any RAM we assign to the VM, it gets used up immediately. This is why I asked the question, because I expect apps to work with minimal resources, but this one has SQL Server as the backend and the app is rather old in terms of tech (Delphi era).
I still cannot understand why SQL Server would need such resources. We are talking about around 100 users accessing a 64 GB database. What kind of servers do huge businesses use, if we need this monster for this tiny case? I believe that the app is the problem due to using the SQL Server as a backend and having a bad architecture. I've seen modern apps use 512kb RAM for the same scope app.
&gt; Did the vendor collect any baselines/metrics before making this recommendation? They did some analysis, but I think they are exaggerating. &gt; It sounds like you're running your "server" on a mid-spec desktop workstation. It's hosted on a cloud VM, so we can assign resources easily. That is my confusion... I thought our specs are more than enough. More and more I suspect a bad software architecture. The SQL Server is the backend for a Delphi app. Old tech... &gt; I think we'll all agree that 13GB is too little RAM to run a production SQL instance though. They are suggesting 64GB. Our whole database is that size. Why do we need so much? I've seen a same sized app use half of our current server... &gt; Is the satisfaction and efficiency of your users and reliability of your system work the relatively small amount of money this will cost? The additional resources would cost us the total ERP cost on a yearly basis. That is insane... 
&gt; The vendor spec seems reasonable for a small MS SQL instance. What do the huge corporations use if we need this monster server? I am very confused by the resources needed.
They are probably stating wants, not needs. They are not paying for it, so why not throw in the kitchen sink. Still, I am very confused by the required resources. I've seen apps of the same scope use half our server... Why does SQL Server require so much?
Because they are stating wishes i.e. they are not paying for it. We are renting the web edition from our cloud host, which is 10 times less expensive than the standard one. Buying it would be crazy.
They are not paying for it, so why not throw everything in.
SQL as a process will use as much ram as it can for caching. I worked for a big luggage company and they had issues with locking due to developers using a datareader to only process one row at a time (acting like a cursor) instead of bringing back the set all at once. (This was in Delphi XE7, so maybe a bit newer) If you can run SQL Profiler over the DB and capture some running queries you might be able see what it’s doing and possibly push back to the vendor to rewrite some of their code. If at any time you see DECLARE CURSOR, just close your laptop and cry.
They're going to use clusters of monster servers.
As someone without a left pinky, the caps lock key has become my best friend
&gt;They did some analysis, but I think they are exaggerating. Unless you're going to do your own analysis or they share with you the data that was collected, this is only a guess. What have *you* done? Have you collected any benchmarks or performance stats? Even a few looks at `sp_blitzfirst` when things are running slowly to see what the pain points are? &gt; It's hosted on a cloud VM, so we can assign resources easily. That is my confusion... I thought our specs are more than enough. Being "in the cloud" doesn't mean you can cheap out on specs. Clearly your specs *aren't* "more than enough" if you're having performance issues. Where's your app server located in relation to the database server? &gt;More and more I suspect a bad software architecture. The SQL Server is the backend for a Delphi app. Old tech "Old" doesn't necessarily mean "bad." Until you can prove that the database *isn't* contributing to the performance issues, you have to assume that it is. Especially with that criminally under-spec'd network interface. &gt;They are suggesting 64GB. Our whole database is that size. Why do we need so much? If your database is 63GB, you can cache almost all of it in RAM which will significantly reduce the impact that your storage will have on performance (at least once stuff is loaded from disk into memory). Maybe you can get away with 32GB - I ran a 150GB database on a 32GB instance for several years (much of the data was cold, didn't get accessed frequently). The 8 cores of CPU is probably overkill unless you're CPU-bound. You also need to get clarification on what edition of SQL Server they want you to upgrade to. Enterprise is probably overkill. They spec'd "Enterprise Standard" according to your post, but that doesn't exist. &gt;The additional resources would cost us the total ERP cost on a yearly basis. That is insane... The cloud never guaranteed lower costs. Running servers/services that provide the required level of performance is a cost of doing business, just like keeping the lights on, the water bill paid, etc. At one job, we had a C-level executive who wanted everything moved out of the datacenter we ran and put into "the cloud." Some investigation was done, people ran the numbers, and discovered that hosting in "the cloud" (which is really just someone else's datacenter) would cost several times more *per year* than we had invested in our own infrastructure.
Your "monster server" is smaller than my new test/dev server. My production instance is 16 cores (used to be 24) plus hyperthreading, and .75TB of RAM. And I have friends whose production boxes *dwarf* that.
If you're exceeding the capabilities of Web Edition, then you need Standard. It's that simple. Maybe there are features available in Standard which would make a significant improvement, but you can't use them because they're not in Web.
How do your users access the erp application? Is there a 3rd application server too? It sounds to me like the vendor is throwing the kitchen sink at the problem. The machine you are running in currently sounds like its under spec though. 
Are you on Windows? [Use this(https://www.groovypost.com/howto/easily-remap-keys-windows-10-sharpkeys/)]
Yes, I also prefer my code not yell at me. 
Yeah, I knew you were just giving a jumping board. Just wanted to add some warnings for him because he said he was inexperienced. 
I use a SQL [Formatter](https://codebeautify.org/sqlformatter). For production code or something holding client PII, I generally type it by hand or write some Excel formulas to help with what can be thousands of lines of SQL.
Wanted to throw in my approval of the pinky shuffle. If you spend a lot of time writing code you should look at an add in or IDE/ISE that handles formatting for you. I am sure there are many. I use Red Gate SQL Prompt add in for SSMS. It auto completes, formats the code automatically, and autocomplete table names across schema’s. So if I know the table name is storms I don’t need to know the schema it is under, I just start typing storm. Once I am complete enough to find the table I select it and it fills in schema and table name. We have a very well defined schema so this has been a huge benefit.
See if this [post helps](https://www.sqlservercentral.com/Forums/Topic728003-145-1.aspx) you.
I stopped using capitals as well. Saves a lot of strain on my wrists. 
I think this might work! As a follow-up, I have an existing data file that is already the appropriate fixed-length and everything. Is there any way I can use that existing file to create a format file?
I've been looking for an answer to this, and I thought I've done this before but I can't find that. Everything I see is creating it from the database / table.
Omg this is amazing
/ot ohh, much to like here: right joins, conditions on left-joined table columns, (apparently) foreign key from an individual to a customer record, and I wonder what else... please re-factor this beauty.
you're right about ACID. CouchDB trades distributed query execution for eventual consistency. it's a bit of a side point but, it's interesting you bring up banking data. in practice, banking software does not use strong consistency. pretty much the opposite in fact! :) for example, it is possible to pay for something with your credit card at a terminal even if that terminal cannot get a connection to the issuing bank. when the transaction is eventually processed into the system (i.e. when the consistency is achieved) all that happens is that your balance goes down. don't have enough money? welcome to your unauthorised overdraft :( this is a trivial example. but this sort of eventual consistency is everywhere in banking here's a post going into more details about how banks are BASE (Basically Available, Soft State, Eventually Consistent) [http://highscalability.com/blog/2013/5/1/myth-eric-brewer-on-why-banks-are-base-not-acid-availability.html](http://highscalability.com/blog/2013/5/1/myth-eric-brewer-on-why-banks-are-base-not-acid-availability.html) 
You have a 64gb database, MS SQL tries to cache as much of that database in memory as possible. By default a SQL Instance will use all the available ram on a server, unless you limit it. Do you run anything else on this sql server? is the ERP application server also hosted on this server, or is it on another server? Also what ERP is it? 
I mean... I can put that server to work all by myself, so if you had 100 me's running around imagine what your server would need to look like. &gt;I believe that the app is the problem due to using the SQL Server as a backend and having a bad architecture. Possibly. It depends what you're using it for. 64GB isn't a lot when you're crunching big numbers, with or without an app, but yeh, if you have an app that is poorly designed it isn't going to help. 
You can tell a OVER() to only go back 3 preceding (or proceding?) rows? That's wild shit right there.
This is the right answer. I think the uppercase keyword thing is just (another) weird artifact of the language having its origin in the 70s, along with some (IMO) rather misguided thinking that the SQL keywords are the most important part of the statement.
I use DataGrip. I type everything in lowercase and then autoformat. 
Step 1) Throw money at the problem Step 2) Observe the problem. If it isn't fixed, go to 1, else go to 3. Step 3) Profit
Very interesting point on banking. The tradeoff there is increased complexity through auditing and double entry bookkeeping to ensure eventual resolution of transactions. 
It seems like they missed an optimization they could have done with OR queries as well. Run the query n times with each one coming as one of the OR clauses, then run a final dedup merge to get the data. Given that their current infrastructure can't join 500K row tables, this would have gotten them those tables and while it would be slower than the hash join, it still would have gotten then results. I also find it hard to believe that they didn't think to implement the n log n merge join to test large scale as opposed to micro. There are also certainly times where it would be beneficial to run the merge versus the hash, e.g. when they are memory constrained.
Not super proficient myself yet, but could you not use CHARINDEX and SUBSTRING ? Then use it to find the second number, then again for the third? So basically nesting it: SUBSTRING ( CHARINDEX(':_/' , expressionToSearch, 1)) + ',' + SUBSTRING( CHARINDEX(':_/' , expressionToSearch, CHARINDEX(':_/' , expressionToSearch, 1) ) ) + ',' + SUBSTRING(CHARINDEX(':_/' , expressionToSearch, CHARINDEX(':_/' , expressionToSearch, CHARINDEX(':_/' , expressionToSearch, 1) ) ) ) Think that's right, however on a phone and not SSMS!!
So I kind of been working on this idea, here's what I have http://www.sqlfiddle.com/#!9/e942bf/5
I think you're better off doing it in your program itself and not in a query, or restructuring your data to split it up into fields of a table instead of one big string.
Sql 
SQL.
What SQL database?
Why?
Why?
Think of data mining like a form of communication. SQL is a foreign language like french, you can get fluent and be awesome, but you can also accomplish a lot with a basic understanding and vocabulary. Access is like learning how to use an english to french translator. It prevents you from needing to know the initial basics but hamstrings your scalability long term. 
Access is like learning how to use an english to french translator. It prevents you from needing to know the initial basics but hamstrings your scalability long term and instead of getting good at french you get good at using the translator. Really good explanation thank you
Cause this is /r/sql
That looks dreadful!
SQL since it is way more marketable than Access. 
&gt; (...) these join conditions belong to equi-joins: `t1.a - 5 = t2.b + 10` But: &gt; `t1.a + t2.b = 10` means that the join is not an equi-join, because the left-hand side refers to two columns. Hey, let me rewrite this for you: `t1.a = 10 - t2.b`. 
This rewrite is, of course, valid but it's not trivial to do it in a generic way. How about: `sqrt(t1.a + t1.b) = 10`? The work described in the blog post was the 1st step towards performance optimization of joins, and there are lots of additional optimizations to follow. CrateDB is an opensource project and any contributions are more than welcomed.
I was just pointing out the fallacy, an equi-join is just a join using `=` and does not depend on what's on either side generally. I'm not a user, so I don't currently plan on becoming a contributor, you're right of course though. It's fine to make assumptions, and yours are fine too. If interested, [SQL Server doesn't do hash either in a case like that](https://i.imgur.com/eSOtWaF.png). 
Access uses a ms variant of SQL called JET which isn’t all that useful anymore. However— Messing around with jet will help you with writing expressions in SSRS if you decide to get into reporting/business intelligence. I didn’t know how to write a nested IIF statement until I was called upon to convert a bunch of old access reports into our data mart using TSQL from the old jet queries. Once I picked up that knowledge, writing fancier reports in SSRS was easier because I had a better understanding of how sql/jet queries in access are written. So for example, I might write a CASE statement to handle a conditional event in TSQL. But in Access, you would have to write an IIF statement for the same purpose. In SSRS, which is used with SQL server (tsql), I feed my reports with TSQL, but if I want to use any conditional formatting within the report UI, the conditional expression is an IIF statement. Meaning the concepts of TSQL and JET blend together in real world usage. All of this is to say that SQL should be your bread and butter but knowing a bit of Access is helpful. It’s not vital, mind you, but it helps. 
Just `GROUP BY Year, Club`.
you need to group by and select the same non aggregated things. In SQL Server you'd be getting error messages. GROUP BY Year, Club WITH ROLLUP;" 
Hi mate, appreciate the help. I've given this a go, but now I face the issue of having it add a rollup after every row. 
Hey mate, thanks for the reply. Like I said above to pooerh, I have just given this a go but face the issue now that it adds a rollup after every row now. 
IIF is more of a VBA function than it is part of JET dialect, though it is nice SSRS has some 'familiar territory' regarding that. 
If you want another row just for the total, naybe use union with a separate query: SELECT Year AS Year, Club AS Team, SUM(PTS) AS Points FROM result WHERE Player='$id' GROUP BY Year, Club Union SELECT 'Total' AS Year, 'Total' AS Team, SUM(PTS) AS Points FROM result WHERE Player='$id' 
My personal experience: I taught myself Access and database design principles to assist with a side project at work. Later took some courses on SQL. I enjoyed it enough to start thinking about working in this area full-time and am looking for jobs. If I were you I would: 1. Focus on learning database design principles and terminology like (primary key, foreign key, parent-child relationships and referential integrity, tables, query) 2. Try to learn SQL 3. Feel too overwhelmed by SQL? Too impatient to just try things out and build a database? 4. Try Access 5. Return to SQL Bottom line: With a knowledge of database design and SQL - you could learn to work in Access very quickly. Knowing database design and Access - makes it easier to learn SQL but not faster. Learning SQL first is way more efficient. 
Sounds like course names to me.
Fair point. I think the comment still stands but that’s info I’m glad to have!
I think you are referring to https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about Those are really good to get you started but to be honest I could not find one withthose exact titles you mention. I recommend to go through all of these and check out r/learnSQL for all your questions.
To me, SSIS is a great software that structures processes and flows, combining the ability to incorporate tools or features to achieve your tasks. While you can definitely break the file up with SSIS, I would recommend something more flexible. I would look to Python or Powershell to create a loop that can split out the file and incorporate that script into a SSIS package if it's required to chunk up the file. Ideally a stable SFTP would be best, along with a client that allows CMD and can pick up / restart where it left off. 
This is going to be a one time thing so I'm not sure how much I want to push it. I can't use the import wizard for something this big because it's inevitable that the VPN is going to drop. I could send the file to IT and have them do it but the turn around would be aewek or two. There are no security/policy concerns with this data. You know you just gave me an idea... I was starting off by importing it to my local DB but I can probably chunk it out in SQL and then just send the files in stages. This data doesn't exist in prod, and isn't going to prod. We're trying to load it to a DEV database for an intern that is working on his MBA. This seems like a good way to approach it for a one time quick and dirty.
&gt; There are no security/policy concerns with this data. You know you just gave me an idea... I was starting off by importing it to my local DB but I can probably chunk it out in SQL and then just send the files in stages. This data doesn't exist in prod, and isn't going to prod. We're trying to load it to a DEV database for an intern that is working on his MBA. This seems like a good way to approach it for a one time quick and dirty. Then I would absolutely do a local load and batch it from there instead. You'll have better and more control once the data is in a table vs a file when you are dealing at a SSIS / T-SQL level without the use of a scripting or programming language that can better handle files. Not that it can't be done or done well to become an automated process, but just like you said; this is a one time thing that you can quick and dirty. 
That does the job! Just one question, it includes 'Total' in both the Year and Club column in the last row. I'd like just for it to be in the Year column. I tried removing 'Total' AS Team but it just breaks everything for me. I can get away with just keep the '', but I would imagine there is something better? Appreciate the help. Thank you all very much!
You can also use a script block in SSIS and manipulate the file with C# if you know how to code or want to attempt to learn.
I feel you don't really know what you're doing and instead just trying to get the expected result somehow. Learning a bit of SQL would really help you. So with the proposal by /u/Ahuevotl you're running two queries and just concatenating their results together. First, I would advise using a `UNION ALL` because `UNION` will try to filter out results that are duplicate between the two results set, and that's an extra step for the database engine that you don't need as the second query cannot return the same result as the first one (you know that, database doesn't). Second, obviously the result sets need to return the same number of columns to be concatenated into one set, so you can't just remove the `'Total' AS Team` because the first query has 3 columns (Year, Team, Points). You could use NULL (no value) for the Team if you want, but something needs to be there. That's also what `WITH ROLLUP` does: *The super-aggregated column is represented by a NULL value* ([MariaDB KB](https://mariadb.com/kb/en/library/select-with-rollup/)). 
Can you rdp to the box? If so, I would suggest getting the file over to the box and running the import there.
While the responses saying just 'SQL' are not terribly helpful, they are technically correct. My company does a lot of SQL with some old reports/processes still kicking around in Access. If I were to look at two hypothetical resumes and one said 'access expert' and the other said 'SQL expert' the SQL one would immediately be more interesting to me, and I think a majority of data heavy companies would agree. That's just my assessment of the market, and you're certainly going to find places that would value Access knowledge over SQL, but probably not super serious data work that you seem interested it. Also 'SQL' comes in lot of falvors so starting there can set you up to work in a lot of data positions where as I think if you focus on Access your probably going to be just working in Access your whole career. That being said, in my mind learning any type of computer language revolves around two thing: availability and a motivated use case. What I mean is that I think most people will have more success and learn more about programming in general if they have the correct tools and a good reason to use them. I have had enough experiences where I've tried to learn some form of coding on my own and I can get to 'hello world' but then have no reason to do anything more complex. So much of coding is having a burning question or problem to solve and figuring it out. It's all well and good to lean SQL from a text book and to use some dummy database to train on and get the basics down, but the motivation has to be completely external. On the flip side, if you have Access on your computer and an overworked boss who would be super grateful for some report auto generated every Monday...now there's motivation. And in my experience having motivation and nowing how to solve real life problems is way more important to data analysis than just knowing functions. If you want to be a DBA it's probably a slightly different story and there's no way around memorizing every detail about one version of SQL. If you really just love data analysis and working with spreadsheets just start doing more of it and pushing your limits with real life examples. Back when I was still in school some of the most advanced data work I did that really trained me for understanding SQL later was via XML parsing and stupidly complex spreadsheets in Google docs. In both of those areas I had an end goal I was motivated by and the tools on hand to start manipulating data. Those two things allwed me to stay up late working for fun, reading up on websites, and talking with other people about these silly side projects I was so excited about. Starting to solve real data problems now is unquestionably more valuable as a beginner data analyst that having a certificate in one language. Going back to my hypothetical two resumes: the person who can tell me with passion about data projects they've worked on, how they learned new languages/processes to overcome obstacles, and has a great personality is going to be wayyyyy more interesting regardless of what language they supposedly are an expert in. I have seen people start an entry-level analyst position with really good Excel skills and zero SQL, and become proficient in SQL in a couple of months because the work required it. All that said: learn SQL if you can, but being motivated and able to solve problems is the end goal, not just some programming language.
Haha, no. IT will need to do that and take 2 weeks.
5GB is relatively small. While it\`s almost certain the VPN will die during that time, I\`d recommend sending the file to a location at least in the same datacenter and load from network from there. If you want to spice things up a bit, I used to have a situation were I had to import a few large files and to bypass the per file speed limit (tm), I used gnuwin32's split command as a preprocessor to split each file into 8 shards and load them in parallel into a temp tables before pushing them all to the same table.
based on your responses, you are living in some bofh environment
My favorite is the psychology course " relational databases and why they make people cry 201"
You need to send the file to the server first so that it loads from a local location.
The implementation of the Hash Join was our 1st step towards performance improvement of joins. Of course, sorted merge join would help to significantly improve the performance of other types of joins (non equi-joins) and is in our backlog to implement. Sorted merge has a tricky part though, if you have tables with many duplicate values on the joined columns, you need to keep in memory all those duplicate rows to properly emit the joined rows. As we'll explain in upcoming blogposts, the memory limitation of Hash Join "one of the tables should fit in memory" is overruled with the implementation of the Block Hash Join, and the distributed execution on top of that, which means that the total memory consumption is spread across many or all the nodes of the cluster. 
The Access dialect of SQL isn't bastardized, it's just lacking a lot of the functionality of every other version of SQL.
Access is a fine product. You can do a lot with it when you know how. But... whether or not you'll ever need to depends on the company you end up with. Also, if you really want to build cool stuff with it, you'll need to know VBA. If you just want to do simple things, you'll be able to pick that up pretty quickly. As someone who is pretty darn good at all of these things (Access, SQL, VBA) and has used MS SQL and Teradata extensively, plus Oracle and Hive a little bit: Learn SQL. But DO NOT use Access to do so. Its dialect is quirky and lacking a lot of functionality, and the SQL editor is absolute crap. It will make learning SQL a lot harder than it needs to be. Download one of the enterprise grade DB's, almost all of them are free or have free versions. I'd choose MS SQL Server Developer Edition, probably because I'm so familiar with MS SQL Server. But it's also a great product and widely used. Use that to learn SQL.
Yep, but I keep pushing and soon they are going to have to give me my own environment. They keep warning me that it won't be supported by IT and am I really sure I want that... like it's a negative.
Give this a whirl: SELECT t1.ProjectId , t1.ReportID , t1.SubmitDate , SUM(t2.TotalPayment) AS PaidBySubmitDate FROM t1 LEFT OUTER JOIN ( SELECT ProjectID , PaymentDate , SUM(Payment) AS TotalPayment FROM t2 GROUP BY ProductID ) t2 ON t1.ProjectId = t2.ProjectId WHERE t2.PaymentDate &lt;= t1.SubmitDate GROUP BY t1.ProjectId , t1.ReportID , t1.SubmitDate
I'll one up my previous statement. So I have my project that just deployed and which I built on a 'test' server where I have more or less full rights. I built it using SSIS to load my mapping data, but IT decided to build their own process using a SAP product. So now they are telling me that this product only looks at the top 10 rows of a data file to determine what the maximum length of the import is, and if the 11th row is longer... it will truncate. The long term fix that we have decided to come up with? Insert a dummy row with a length of 1000 characters to always ensure the rest of the file loads. Seriously.
I'm pretty sure there's no way to use a column expression in a PIVOT
Do you have any thoughts on what Ben-Gan may have been referring to when he stated that he use the table expression to "avoid repeating the expression"? I'm basically just curious as to how you can use PIVOT in this scenario without using a table expression.
Thanks for the help. I had a feeling I was doing the sum in the wrong place. Owe you a beer if you're ever in DC.
No idea. It could just be a mistake or poorly worded
You can easily do this if you pivot the 'regular' way (with group by/case), without using the PIVOT syntax.
Ok, when you have a column(expression) coming from the 'outer' part of the join, it could be NULL (kind of the purpose for it in the first place). Meaning the WHERE t2.PaymentDate &lt;= t1.SubmitDate will fail on those outer joined records and that 'outer' join becomes functionally equivalent to an 'inner'. Please always always remember to do one of isnull/(or X is null)/move condition to the join's 'on' clause.
Load the file to a local database (no VPN). Then load the server with a sorted SQL query. If the VPN drops, you can figure out the highest record that loaded, and start again from where you left off.
Ahhh you got me. Is that going to have a performance loss by the way if it is written as a LEFT but functions as an INNER? I know a lot of those conditions are best put in the ON condition and not in the WHERE. I tend to work with INNER's in these cases but I wanted to rewrite his FULL OUTER LEFT because it looks so obtuse.
Look at the [HAVING](https://www.w3schools.com/sql/sql_having.asp) clause. select zip, count(*) as number_of_providers from providers group by zip having count(*) &gt; 100 order by count(*) desc
Been doing a bit of traveling around the country and working remotely out of hotels. Might take you up on that.
Theoretically it could but I've never seen one that would scream 'fix me' where there wasn't something else going on.
FILESTREAM is a `varbinary(max)` data type, so you'll need to store it as binary regardless... since you're storing files above 1MB, FILESTREAM is the best choice for read performance when using SQL Server.
Touche.
fair enough - if that's how you want to describe it ;-). As I said it's been ages since I last used it, I just remember that it was a great sigh of relief when I finally migrated the homegrown (and overgrown) database from access to MS SQL + web front end at my last job :). Then again, it's possible that the "bastardization" I seem to remember had it's roots in (un)experience of the original author rather than in Access itself. 
If you create a backup you can zip the backup file and split it into chunks with something like win rar / hj split / 7zip
I always type SQL functions like SELECT, FROM, WHERE in caps, and I always (try to) type column names in the same format as they are stored in the table. So if a column is named with all caps, that's how I type it... generally I use auto-complete so this is automatic. I tend to type datatypes in lowercase, e.g., `CAST([ColUmNnumbeR_one] AS int) AS 'MyName'` I tend to name columns based on what they contain. I like plain English such as MyName, or MyName_VMS, using the underscore to delineate that the value is what is coming out of a VMS, whereas something like MyName_MP might indicate that it is the mapped value for that column. I always use all caps for my aliasing, such as `A.`, `B.`, `MAXTABLE`, etc. I do the same for name segments of CTE's. The important thing is to find a "system" that you like and which you find readable, and that you can use easily. I type really fast and hitting shift while keeping a steady rhythm is just not an issue for me, but I can see how it might be much more difficult for someone who is slower. Sometimes I turn caps lock on and use shift when I want to write in lower case, because most of what I'm going to be typing is going to be in uppercase. It just doesn't matter to me because I can type well. If someone were a poor or slow typist and looking for a really consistent way to write SQL... I would probably ask them to write everything in lowercase. I actually write a ton of code in lowercase and call it, "angry SQL." My boss knows when I send him something in lowercase that it was done quickly and hasn't been validated to the nth degree. He is fair to question me on it and make me explain sections, or defend it. Then if it turns into something interesting that we are going to use long term it will be reformatted in the format I prefer. He knows when he sees that kind of code that I'm telling him I've looked at every angle, I'm happy with it, it's clean... I spent time to make it look pretty so don't fuck with me on what it says, you had your chance.
This was actually the first thought I had, then just email the backup to the user and then they can access it on the localdb.
Thank you! I followed the MSSQL docs and created a database, as well as a table, with the following commands: CREATE DATABASE Archive ON PRIMARY ( NAME = Arch1, FILENAME = 'c:\data\archdat1.mdf'), FILEGROUP FileStreamGroup1 CONTAINS FILESTREAM( NAME = Arch3, FILENAME = 'c:\data\filestream1') LOG ON ( NAME = Archlog1, FILENAME = 'c:\data\archlog1.ldf') GO Created the table: CREATE TABLE Archive.dbo.Records ( [Id] [uniqueidentifier] ROWGUIDCOL NOT NULL UNIQUE, [SerialNumber] INTEGER UNIQUE, [Chart] VARBINARY(MAX) FILESTREAM NULL ) GO I stored some data by using: INSERT INTO Archive.dbo.Records(id, SerialNumber, Chart) SELECT NEWID(), 1, BulkColumn FROM OPENROWSET(BULK 'C:\Users\Fasyx\Desktop\test.txt', SINGLE_BLOB) as f; ...I have some questions: 1. The data presented in column Chart is not binary, it's hexadecimal! [See here](https://i.gyazo.com/e7ea53785b5f7ef63290f70a9cec1d00.png). Why is that? 2. Given the first question, I assumed that the file itself would be stored as binary. I checked the files and they are stored as plain text(decoded). Shouldn't they be stored as binary? 3. Where is the binary? I can't seem to find any binary data... Thanks for helping out a noob! :)
If you can use the GUI is very easy (very similar on versions from 2008 through to 2016). - Here's a video https://www.youtube.com/watch?v=VvaKmujEE7E Microsoft also have documents on other ways (e.g. T-SQL / PowerShell). https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/create-a-full-database-backup-sql-server?view=sql-server-2017
#### [How to backup databases in SQL Server | lynda.com tutorial](https://www.youtube.com/watch?v=VvaKmujEE7E) ##### 43,487 views &amp;nbsp;👍70 👎4 *** Description: This SQL Server tutorial shows how to create database backups using the Management Studio feature. Watch more at http://www.lynda.com/SQL-Server-2008-... *LinkedIn Learning Solutions, Published on Dec 20, 2010* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
You need to clean up the grammar and style in the questions. And something like this isn't likely to be asked on an interview: How do you create table using a designer in Microsoft SQL Server Management Studio (SSMS)?
Thank you. He covers that method in the solution as well, but I'm curious as to how he'd do it this way. Ultimately it probably doesn't matter since I know of a way that I can do it, I'd just like to figure this way out as well to give myself a better overall understanding. 
Hex and binary are essentially the same thing--hex is just a more efficient and easier to use method of representing 0-255, `0xFF` in hex is `11111111` in binary, but you're saving a bunch of bytes from having to be written. &gt; SELECT CAST('HI' as VARBINARY(MAX)) returns `0x4849` which is 72 and 73 in decimal, which are the ASCII key codes for H and I, respectively. This is a lot easier to look at and analyze versus the binary representation of `‭01001000‬‭01001001‬`. With FILESTREAM, the files are actually stored transparently on the filesystem and not in the database, so it's streaming the bytes directly to the filesystem without being stored in-memory. If you were just using a plain `VARBINARY(MAX)` column, the binary (as hex) data would be stored in the actual column.
This is the best solution
Hey, just finishing work for the day. This needs to be a LEFT and not an INNER, I think. An INNER would remove any cases where there is an ID where no payments have been received. You'd want to do an ISNULL() in the select to put a $0 for those cases to show they are unpaid.
Thank you. I just read about it :) It makes perfect sense now! When I store binary data, in form of 11111111 for example, would SQL automatically store it as hex?
One more question though: I deleted the entire entries from the table. I thought this would also delete the files on my filesystem, which it don't. Why is that? Isn't the data stored in the table columns the data from the files on my filesystem? Is it a copy or a reference?
https://stackoverflow.com/questions/13461424/coalesce-with-hive-sql The accepted answer shows: COALESCE(column, CAST(0 AS BIGINT))
Thanks! I'll give that a try. 
You could use recursion, something like this: with TransactionDates(start_date, end_date, year, month, day) as ( select start_date, end_date, datepart(year, start_date), datepart(month, start_date), datepart(day, start_date) from ( select convert(date, '3/1/2018') as start_date, convert(date, '5/31/2018') as end_date ) as t union all select dateadd(day, 1, start_date), end_date, datepart(year, dateadd(day, 1, start_date)), datepart(month, dateadd(day, 1, start_date)), datepart(day, dateadd(day, 1, start_date)) from TransactionDates where dateadd(day, 1, start_date) &lt;= end_date ) select td.year, td.month, td.day, coalesce(count(t.orderid), 0) as order_count from TransactionDates td left join transaction t on t.client = 'clientname' AND t.year = td.year AND t.month = td.month AND t.day = td.day option(maxrecursion 1000) Note: I was only able to test the correctness of the TransactionDates CTE since I don't have a working Transactions table. Also ideally Transaction would have a compound index on (year, month, day) for best performance.
that's not enough... you have to have an actual row with a count of 0 foir the COALESCE to kick in what you're looking for is a way to generate all the dates, which then become your left table in a LEFT OUTER JOIN to the transcations, so that if there are no transactions for a certain date, you get that sweet 0 goodness one way is with a dedicated dates table another way is to generate the dates from a **numbers table** using e.g. `DATE_ADD('2018-01-01',n)` where `n` is an integer coming from the numbers table
Oooh yep that’s a great point. I didn’t notice that the row may not even exist. Thanks for catching this.
You are right. The grammar is tough for me considering the fact I'm not native English native. Hopefully once the community grows, I will be able to find someone who will help me with this aspect (pay a technical writer). Until then I will have to use Grammarly to at least slightly improve the language issues. Regarding the SSMS - again, valid point, this wob't be asked. Except the job interview questions I started to work on a pivot - automated training system. The core is similar - pool of questions and answers. But in order to teach newbies something, the questions need to be asked in a more straightforward way. So in the future I plan to do separate sets of questions for training and covering certain topics and separate sets of questions for job interviews (in other words a) training, b) progress testing). For that I have to rework the question browser which is now like an example of how not to do it from the UX standpoint. I am still more focused on developing the system rather than the content but the people start to be loud about the content issues which is a great motivation and will speed up the process of improving the content. So thanks a lot for your feedback!
Maybe use a window function that will be partitioned by day?? COUNT(t.orderid) OVER(PARTITION BY t.day) AS order\_count
They're linked via references and will get cleaned up when a checkpoint is ran against the database. You can force it by calling a stored procedure: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/filestream-and-filetable-sp-filestream-force-garbage-collection?view=sql-server-2017
You would have to go out of your way to get data in actual binary, everything to read files is probably going to be a byte array either of hex or decimal which SQL will store/convert to hex. I don't think tsql even has native support for actual binary formats (0b00000000)
Date dimension ftw
 With clients AS ( SELECT DISTINCT client FROM dbname.transaction ) SELECT t.client AS instance, t.year AS year, t.month AS month, t.day AS day, COUNT(t.orderid) AS order_count FROM clients LEFT JOIN dbname.transaction AS t ON t.client = clients.client WHERE client = 'clientname' AND year = 2018 AND month IN(3,4,5) GROUP BY t.client, t.year, t.month, t.day ORDER BY t.year ASC, t.month ASC, t.day ASC LIMIT 500 I think that should work. Although who would be nicer to have a client listing the pole from rather than pulling from all the transactions to stink client IDs. If there's a table called clients I would pull that instead of grabbing from transaction.
I can't really argue with your first point about not really knowing what I am doing. I tend to find myself grasping to understand, what are, perhaps, simple concepts in reality. I don't intend to come across as if I want others to just do the work for me. My project is really chopped together with bits and pieces. I get a glimpse that something may work and jump on it without truly understanding it. Something I am working on. I appreciate the advice you have given. The last link is very neat and seemingly (I am out right now) explains things in a way I had not seen previous. Thank you for the help. 
Are you responding to me or to the OP? Anywho, yeah, this needs a left join and also I can imagine there could be multiple reports per project, so that changes how you would join too.
I had to do something similar. I split the file into million row chunks, then used [bcp](https://docs.microsoft.com/en-us/sql/tools/bcp-utility?view=sql-server-2017) to insert the data in. Really, if you can create a staging table to populate first and then load your production table from staging, that would be better since if something messes up you can always drop the staging table and rebuild it.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://docs.microsoft.com/en-us/sql/tools/bcp-utility?view=sql-server-2017) - Previous text "bcp" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
Prepare yourself for [Exam 98-364: Database Fundamentals](https://www.microsoft.com/en-us/learning/exam-98-364.aspx) at Center for Continuing Education and Workforce Development at BMCC/CUNY. Classes run from 07/09/2018 to 08/08/2018 — Mondays and Wednesdays from 6:00 to 9:00 pm.
So I'm not sure I understand your respond. The WHERE in my code would still include dates on the FROM that don't exist in the LEFT, correct?
Say you have a ProjectID 8000 in the t1 table. Your table expression (from ... left join) will bring that record in, but the columns from the subquery t2 will be NULL (there's no ProjectID 8000 in Payments). Your "where" clause then will test "t2.PaymentDate &lt;= t1.SubmitDate" and it will fail, since t2.PaymentDate is null and ProjectID 8000 will be excluded from the final result set. This will happen to every record where t2 is null, discarding the "outer" part of your table expression result set, thus making your statement functionally equivalent to an "inner" join (i.e. it'll work the same as if you'd written "inner" instead).
Oh, I see what you're saying. I stand corrected. My man.
&gt; Although who would be nicer to have a client listing the pole from rather than pulling from all the transactions to stink client IDs. who would be nicer, indeed listing the pole? stink client ids? this has gotta be the most fucked-up sentence in this sub all year 
Voice to text sucks
Do you have a date table (listning all days?) If so, you could do something like: SELECT d.year, d.month, d.day, c.client, , COALESCE(t.order_count, 0) AS order_count FROM date d, client c LEFT OUTER JOIN (/* your query without WHERE and LIMIT */) t ON t.client = c.client AND t.year = d.year AND t.month = d.month AND t.day = d.year WHERE c.client = 'clientname' AND d.year = 2018 AND d.month IN (3, 4, 5) If you don't have a `client` table, you can cross join dates to `SELECT DISTINCT client FROM transaction` but that will obviously slow down the query significantly. Or if you only ever care about a single client, just make it a constant in the `LEFT JOIN` condition.
Maybe this? (CASE WHEN COUNT(T.order_id) &gt; 0 THEN t.order_id ELSE 0 END) as order_count
The common SQL solution is to outer join to a table containing all possible dates. The list of possible dates is typically generated dynamically using a WITH RECURSIVE clause (which isn't supported by Hive, apparently). However, MySQL was suffering the same problem until recently and there are some workarounds. One is presented here: https://use-the-index-luke.com/blog/2011-07-30/mysql-row-generator I guess it's not working 1:1, but it might give you some ideas how to work it out.
You'll get to another solution by thinking about what views on your data you want to have; sounds like `select ... count(*) over ( partition by zip ) ...` is definitely among them. Make that a named view in your data model, then you can easily build more specific stuff on top of that, like `where provider_count &gt; 100 and zip ~ '^94'` for example. The key is to take small steps and naming those steps.
why would you ever have sql with names this oblique? this is unreadable
You are close select A, 'B' AS variable, B as val from tbl ...
yesss! that did it! so simple, apparently. thanks a 1e6 (work-related, by the way. not homework)
it should be "OR sm.schoolID IN" 
You can’t do OR in a case. Go from WHEN cond1 OR cond2 THEN out1 —&gt; WHEN cond1 THEN out1 WHEN cond2 THEN out1
&gt;se. Go from WHEN cond1 OR cond2 THEN out1 —&gt; WHEN cond1 Thanks that worked I knew it was something dumb
You can use OR in a CASE, on most database platforms I've used. Which platform are you using?
I think I was confusing the CASE syntaxes. I was thinking about the CASE &lt;expresssion&gt; syntax (like C switch-case) rather than the one here. Oops! Good catch, thanks!
TL;DR: About 4-7 times slower than PostgreSQL with cstore_fdw (a columnar data store) As someone who doesn't really have time to go through the specifics of each experiment, I feel like this doesn't really tell me much other than that you had fun playing with an unusual combination of tools, but perhaps I'm not the target audience here. I'd find it a lot more interesting if your summary included a SQL Server or MySQL/PostgreSQL etc "standard" setup for reference, although I guess with 1.1 billion rides in the dataset they may just grind to a halt
I've benchmarked PG w/ cstore_fdw already. MySQL doesn't support columnar stores out of the box so I haven't run anything there yet. I'm not sure if MSSQL's licence agreement allows for publishing benchmarks.
I believe they don't allow it - but whether that's enforceable is an entirely different question :p
&gt; SQL is one of the extensively used relational database management systems No, it's a database query language. Stop using "SQL" as the name of your RDBMS. Maybe MSSQL or even SQL Server, but not SQL. 
I'd rather ask for permission than beg for forgiveness in court.
The names are clearly placeholders. I do this all the time when posting code samples. It serves a few purposes 1. Avoids people getting bogged down in my schema context: They don't care what the context is or what my application does. If someone is doing me a favour by helping me debug my code for free, I'd rather make life easy for them. A, B, C are a lot easier to understand, and let people quickly understand the essence of my problem 2. Avoids backseat drivers jumping in and trying to tell me my naming conventions are wrong blah blah 3. Avoids any potential of accidentally giving away confidential information, or doxxing myself etc. Less likely, but very important
&gt; &gt; This is unreadable &gt; &gt; No it fucking isn't, that's one of the most readable code snippets I've ever seen take it down about seven notches . &gt; I'm not an academic, it's not my job to enforce academic standards. that's nice
/u/kaufeinhafen - if you would like some help, just relabel these. i don't know what your goal is and i'm not able to help. looks like the angry guy can't either
&gt; &gt; This is unreadable &gt; &gt; No it fucking isn't, that's one of the most readable code snippets I've ever seen take it down about seven notches . &gt; I'm not an academic, it's not my job to enforce academic standards. that's nice i don't know that anyone asked you to. . &gt; The following would have been a lot harder to read, and slower to understand, for no gain select NumMealsForPilot, LitresFuel as val from tbl union select NumMealsForPilot, NoseWheelTireAge as val from tbl union select NumMealsForPilot, TailFinLogoRGBBlueValue as val from tbl new to sql? try using operator `as` i notice you haven't helped him, only yelled at me 🤷
Most people are self taught, even those that took courses. There are plenty of courses available, but those don't offer much in real-world scenarios. Textbook examples are "perfect", and that's the problem. But it is necessary for concepts. Real world is imperfect. If you're just starting, I've found sqlzoo.net to be helpful to many folks. It's real time interactive learning that you can do right now, for free, without any logins/accounts, etc. Beyond that you might want to try a udemy course, and beyond that you might want to pick up an official book on the platform you choose to work with. But that's super boring stuff. Like anything, the only way to get good at something is to do it often. Once you learn some building blocks, you use those building blocks to create more complex solutions. A lot of times I don't have any specific question, but I read everything posted here to see what others are struggling with. Even though I might not ever do what they are trying to do, I might learn something that I didn't know before. And I've posted my own requests for help as well. Many minds are greater than one person with google. I'd also recommend installing a free VM host like VirtualBox, and then download a free VM that has the database of your choice. You can DIY from the ground up, but for example Oracle publishes VMs with databases that you can use for free for non-commercial purposes. It's a great turnkey solution to get started quickly. Oracle also offers online platforms such as https://livesql.oracle.com/ which requires a free account but you get to write sql on a database without having to install anything. Often overlooked but equally important, you should brush up on set theory. SQL is its basic traditional form is not a procedural language (although PL/sql and T-SQL exists for this), but SQL does operations on sets. You need to understand operations on sets in order to write SQL that will give you what you want. 
Don't email 5gb of data...
I meant back up the database and email them a link to where they can download and restore it /eyeroll
I had a few very specific problems to solve for. SQL, SSIS and SSRS seemed like a very viable solution. My 1st versions were embarrassing and I still don't know how the hell some of them worked. After much trial and error with an ultimate goal/problem to solve for I picked up basic SQL skills. With this new tool in the toolbox I was able to solve more and more problems...to the point where I used ETL to solve for problems i should have solved for using PERL/python or some other scripting language...but that's another story
He’d already had his problem solved by a previous poster. I was just calling you out for being an asshole and backseat moderating (especially when it’s not even against the rules)
I was the first commenter here
I got started with practice and application Later I read Karwin and Celko
The rewrite leads to overflow when t2.b is MIN_INT and the data type is unsigned. You'd need to scan the table or maintain statistics to ensure the rewrite is valid.
I can really empathize with what you are saying here. Gosh, the gap between the imperfect and the perfect world though. 
I recently had a rendezvous with a terrible text column that represented a proportion of "full time employed". I was asked to gather some historical stats regarding how many hours people work throughout their employment and how often they change their hours per week. Example: A full time employee is 40 hours a week, a part time employee is 20 hours a week, etc. If you were .6 or 60% then that means you work 3 days instead of 5 per week, etc. Some of the values in the column where between 0 and 1. Some of the values where something like "50%", or "50". Other values where a bit more spelled out, like "50 Percent", while others spelled it out like "FULL TIME", "Full Time", "Half Time", "1/2 time", etc. And of course a bunch of nulls thrown in there as well. But that's the thing... in the real world, sometimes this is what you have to work with. The data is all there, but you can't do anything with it without heavy modifications, often resorting to case-when and regex to come up with clever ways to translate it into a consistent number that you can do math operations on. It's the kind of stuff that eats up your work day that you won't find in textbooks. 
You just make yourself look like a troll calling someone “the angry guy” when he was right.
No better way to learn than necessity and desire to do things that people say cannot be done ;) - That's what propelled me to learn. I don't think you can really learn any kind of coding without doing for a purpose. Lessons and books are helpful for understanding the concepts behind how things work, but ultimately unless you have real world work to take on, you won't ever become skilled. 
I'm self taught outside of a college course where we learned in Microsoft Access (barf!) The best advice I can give is just practice. I learned the way I did because I had to beat how to do things into my head by repitition otherwise I don't learn anything. Buy a book or use online guides and try all of the examples they provide. Microsoft offers a free version of SQL Server called SQL Server Express and a database called AdventureWorks where you can practice to your hearts content. AdventureWorks tends to be the database a lot of professionals use to show how things work in SQL Server/T-SQL.
I used multiple resources to help me advance, Sites: w3cschools.com techonthenet.com Pluralsight.com (has a trial period, but after that a monthly fee is charged) Book: Mastering Oracle SQL, 2nd ed (O'Rielly) Oracle PL/SQL Programming (O'Rielly) I found a combination of these resources and gaining an understanding how the data warehouse is setup (Metadata resources) at my company helped a lot. 
check out SQL Prompt...
What there needs to be is some kind of website where people can upload their dumpster fires of mangled data, have it stripped of anything resembling proprietary or meaningful value, and then let people play with it like taking apart old clunkers in highschool shop class.
Google what you want to do, then when you find a keyword like SELECT, JOIN, etc, etc Google that keyword and syntax. That has got me to a comfortable point where I'm confident handling queries 
i'm not that worried about whether you think the guy insulting me was right `:)` have a nice day
I assume this is Microsoft SQL Server based on the `ROW_NUMBER() OVER` part... &gt;udplog.user_actions is 100 millions row, but with the where condition it drop at around 5 millions But is there an index on the table to drop it down? Or is the query scanning through the entire table of 100 million records to find those 5 million? Basically put your query into SQL Server Management Studio and click "Display Estimated Execution Plan" (`CTRL+L`). It'll tell you what's taking the most time to run and what you need to look at optimizing... it'll also recommend a missing index... You probably just need to create an index on `udplog.user_actions` containing `action`, `is_student`, `user`. And an index on `exp.f_student_activity_current_week` containing `app_username`, `status`.
Not sure if this is your issue but did you mean to have parens around your OR conditon? AND (sacw.status = 'fg1' OR sacw.status = 'fg2')
run an explain plan and see what's up.
Just wanna say, you can do that in Oracle as well.
Unless the query simply doesn't complete even waiting for hours, you can right-click in the query window, click "Include Actual Execution Plan", run the query, and after the query has run, it will show a new tab in the bottom window with the actual execution plan. Scroll through it and watch for Missing Index warnings (in green text at the top), and/or see which table lookup/scan/seek is taking the longest. Try to fold out the table in SSMS and tell us which indexes are on the user_actions table? Which indexes are there, and which columns do the indexes cover? A quick glance tells me there should be an index on the columns action, is_student and user in order for this to run quickly.
I'd call myself self-taught through work. For almost a decade I did relatively simple CRUD stuff, a little bit of DBA work and such, and lots of ad-hoc queries. A year ago I switched to a team where the database (MSSQL) is **heavily** used for manipulating data, reporting, lots of business logic, and an entire database of denormalized data and stored procedures used for enabling faceted searching of data. Thousands and thousands of lines of SP's and SQL jobs. On top of this, I'm the only DBA / database dev on the team. That forced me learn, and I've turbocharged my SQL knowledge in just a year, learning at least as much as I've learned in the past year. In short, if you are able to join a workplace with a spaceship-level advanced database, you will be forced to learn. Although in your case, you might want to start out at a lower level. :) I think you really need to be forced tasks on you in order to truly learn. That's the whole problem with self-learning (outside a workplace) - there are no deadlines, no production environment, no live data, no sudden problems, no consequence. But I guess it's better than nothing.
Try this and tell me if it gets any faster: BEGIN SELECT ua.user, ua.date, ua.real_doc_id as activity, ROW_NUMBER() OVER ( PARTITION BY ua.user , ua.real_doc_id ORDER BY ua.date DESC ) AS row_num, sacw.type, sacw.status INTO #temp FROM udplog.user_actions as ua LEFT JOIN exp.f_student_activity_current_week as sacw ON ua.user = sacw.app_username WHERE ua.action = 'submit_answer' AND ua.is_student = True AND ua.user LIKE 'blabla%' AND sacw.status = 'fg1' or sacw.status = 'fg2' END BEGIN SELECT * FROM #temp WHERE row_num = 1 END 
I’m self taught, and I fortunately had my company’s database to toy with and learn, but one thing I wish I did early on was simply read the sql docs. We use MySQL, and I didn’t know about simple things like HAVING until pretty far in which I use constantly now. So, yeah, read the docs and find a large set of data to work with and practice on and you should be solid. 
I came into a job 11 years ago knowing only the basics of SQL. As I rose through the ranks, I eventually found myself in a "technical" role (read: problem solver) which made me use SQL even more. Now, I'm in a BI/Reporting role which has pushed what was several lines of SQL to upwards of 100+ lines at times. It's experience and real world scenarios that will teach best IMO. I've taken SQL classes, paid by my employer, and they're good but you have to see the data and make connections, know what your indexes are, etc. Since I'm a nerd anyway, I have found data sources online (NFL/baseball stats) and biohacking stats that I keep and throw those into SQL Server on my machine just to toy around with and make connections with Tableau. That might be something you could try just to become more familiar.
Yes, that should have parenthesis. Highly doubt they're wanted to drop the previous where clauses for that ending one in after the or. This is also likely what's causing the issue with efficiency. 
Wow amazing! thank you so much for that information I'll definitely look into it.
Thank you! this is all great info and right on for the point outs! 
You're trying to get the latest date per user and doc_id, correct? As well as using the good suggestions above you could substitute ROW_NUMBER for MAX(date) into a temp table and join to your main select on date = temp.date
I looked online and made myself a little project to constantly be working on. It reflects NBA players’ career stats so it’s constantly changing which means I have to keep updating it. (Except for the offseason which is right now)
Brent ozar always talks about paste the plan dot Com. It's for query execution plans but they also help with alot of other things. 
Hey, ramborocks, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Thank you so much for these! I'll take a note of that. 
That's amazing man, base on the data your working on - do you know where Lebron is going? haha 
Try looking here : [w3 school](https://www.w3schools.com/sql/sql_and_or.asp)
Sqlzoo.net has a whole slew of exercises to practice on real live databases, only select-based stuff, no data definition, but that’s how I got started.
You want to use an OR, ie WHERE (LocationType = 'Practice' AND PrimaryLocation = 'Y') OR ('LocationType = 'Credentialing')
Is this all in one table? Or is your data table above the result of a join? I'm going to assume you have more than one person in the data set. Try something like this: http://sqlfiddle.com/#!17/e9ecf/4 select cred."Name", cred."LocationName" as "CredentialingLocationName", prim."LocationCounty" as "PrimaryPracticeCounty" from TableName as cred join TableName as prim on prim."Name" = cred."Name" and prim."PrimaryLocation" = 'Y' where cred."LocationType" = 'Credentialing' group by cred."Name", cred."LocationName", prim."LocationCounty" Please let me know what database engine you're using; the languages differ a little. For MySQL, switch the double quotes to backticks, for example.
Jamie. King. guy is great for getting started then moved on to Wiseowl for programmability stuff
Your example and output doesn't make sense. You want rows where Primary Location is Y *and* rows where the LocationType is Credentialing? If that is true then the desired output based on your sample would be two rows, Dr. Joe's Office 2, and Office 1... or do you only want to see the LocationName where *one* of the rows is Y for PrimaryLocation, and *one* of the rows is Credentialing for LocationType?
Copy your view definition, add a count(\*) to the end of the select list. `Group by` those 5 columns with `having count(*) &gt; 1`; that'll let you identify the rows that differ, and you can look at them individually (`select * from table where` those 5 columns = those 5 values) to eyeball the differences. There are ways to find the exact columns without iterating over each one but they involve enumerating the column names with INFORMATION_SCHEMA or similar, and are very lengthy...
And Teradata.
The first thing I would check for is the cardinality of your tables - if you're duplicating user data during the join and then ordering the data with ROW_NUMBER() that could easily result in your database running low on memory or temporary storage space, resulting in long running queries. 
I'm surprised this hasn't been mentioned yet, but you need to learn how to "normalize" data: [https://en.wikipedia.org/wiki/Database\_normalization](https://en.wikipedia.org/wiki/Database_normalization) People can get all pedantic about what level "normal form" so don't get distracted by that, just normalize your data to a sufficient degree that you don't have too much duplication of data between tables. For instance, if you have an online store, you might want to have a product table separate from a category table, and then you can indicate which category a product belongs through a "foreign key relationship": [https://en.wikipedia.org/wiki/Foreign\_key](https://en.wikipedia.org/wiki/Foreign_key) Bonus exercise, if you have DBA rights, i.e. you can create new tables, how would you make it so that a product can belong to more than one category? And what is this relationship called? HTH. Kind of hard to tell because you didn't tell us what level you're at right now.
ROW_NUMBER() is pretty standard for any SQL engine that supports OLAP functions. 
I bought a little book called Teach yourself SQL in 10 minutes (which is comprised of a bunch of 10 min lessons) probably 15 years ago or so. I worked through those lessons and then deconstructed the Northwind database in MS Access to develop an understanding of table relationships and data normalization. That got me started and I’ve just built on that over the years. I’m now more in management than production and the one thing I see that I did that others often miss is taking the time to understand the data you’re working with and the purpose for keeping records in the first place. That part is really key to interpreting what the end user is really asking for - versus what they say they want.
The data table I listed is simplified from what I actually want and is the result of multiple joins. There are also many different names. I'm using MS SSMS, so I shouldn't need double quotes or backticks. I'm self-teaching SQL and never thought to join a table on itself, and din't know that you could put ANDs in a join, but with a little work to account for the other joins that seems like it should do the trick. I'll give it a shot tomorrow and report back. Thanks!
He talk good for a bot
The thing that helped me the most for queries was always knowing what my end goal was. No matter the data structure or what I needed to get, I would write out what I needed the result to be out on paper. Then I would write a simple query to get me one piece. Then just keep chipping away until it gets closer and closer to what I needed. Also, learn what aggregate functions are and how to use sub-queries. Its a little more advanced but you will get there. 
Also, the IDE you use plays a major factor so choose a good one for whatever flavor of SQL you are using. If you are unsure which to use and just want to start playing around, here is a good playground environment. http://sqlfiddle.com
Probably been mentioned here, but just in case, you can download MSSQL Server Express and the Adventure Works database to test and "mess around" with for free. Test out joins, different ways to parse data sets... 
There isn't any weirdness with OUTER JOINs. If your WHERE clause requires the records to exist on the secondary table, it works as an INNER JOIN instead.
I am and I work in the field. My recommendation is learn one aspect at a time and apply it. Success with it, fail with it, use it. Do it over and over until you can truly apply the concept
I wouldn't be a customer at the moment and I think that detecting errors is something that's relatively well covered by a number of different applications out there. Now I know for a fact from my prior engagements, that building a reporting/alerting going off _non-events_ is a much harder proposition. So, if you can build a solution that somehow allows in a user-friendly way to define 'future expected events' and creates (or gives clear instructions/templates how to custom build) detectors/agents for when these expectations do not materialize while giving some user-friendly escalation options, I would think you could find significant number of interested people.
Thank you!
Just a heads up; you know about SELECT TOP right? Might want to look into that if I'm understating you're query correctly
It is fairly easy to pull data out of a database, even if multiple joins are involved. Good designs are another matter. To access data you can use copy and paste coding from existing queries. Build small and then increase the sophistication of the query. If you have "baby" versions of database which mirror the structure of the main database, that can be very useful as even simple queries in 40TB databases can take a looong time. When you have questions, Stackexchange can be your friend but it will not be the solution as you can't easily simplify questions about complex databases. Creating proper databases is a lot more difficult as you first need to understand the cost of operations. Sure, you can store things just as json or xml but the queries can cost so maybe it is more efficient to convert some of those attributes to columns when you load data. If you are loading a lot of data, and every day, then think about identifying it in easily located chunks
Maybe consider writing a web scraper or similar to collect the stats automatically. 
abot
I learned to love Common Table Expressions (CTEs). If your SQL dialect supports them, they make reading your query much easier because you can meaningfully name multiple chained selects and joins up front, treating them as disposable views. Secondly, be sure to read *recent* technical notes. Developers upgrade their products often, and what was a good idea five years ago is not now. Use bind variables. They (should) speed up re-used queries and offer an important defense against SQL injection. Lastly I still encounter databases that are not set up to default to Unicode. Embrace Unicode/UTF-8 and check your RDBMS using it unless you are 100% sure the data is and always will be in some other encoding.
If wrong/inconsistent data has made it into the database, we've already lost. It should be getting trapped well before that point. There's a market for products/services that can validate and standardize things like addresses &amp; phone numbers (and there are a number out there already), but the business rules should be happening before the transaction gets committed to the database, not a check some interval later.
BINGO
Seconded! Google is the modern equivalent of consulting the Users Manual. Half the time I find the answer after being routed directly to the RDBMS's web documentation. Once you master the art of phrasing a Google search then you are set.
And MySQL 8+. Window functions are supported in almost every major DB now.
O I agree. But that's not the reality at my company and I would be surprised if we are alone in that.
&gt; I guess with 1.1 billion rides in the dataset they may just grind to a halt People seriously overestimate "big data". I can assure you SQL Server can handle 1 billion rows perfectly fine. And I mean quite wide rows, not just `(bigint, smallint, smallint, tinyint, int)`.
I am self taught. My suggestion is find a real word application you can use it against. Do you have an application at work that you can use why learning? Obviously you need to exercise caution when inserting or updating. If not set SQL Express at home and create a database to use, load it with weather data or load it with your movie collection.
That's so maximalist and idealist though. You probably have never needed to deal with healthcare data. In your business area, did you truly never encounter a situation of data that's not 100% valid but still valuable for business and research somehow?
Lebron is going around 2000 points per season and if he plays for, let’s say, 5 more years with a similar output, he’ll at least be top 2 in points if not top. And he’s gonna make it into the top ten assists. And I believe the only non-guard to do so. 
I wouldn’t know how to get started. Is there somewhere you could direct me that could help me out? Thanks in advance!
i ran into this problem a few years ago when trying to pass variables into SSIS. what about making your entire query in SSMS a string that you execute later through openquery? in SSIS when i was moving data from teradata to sql, i would make my statements into string variables that could be used later in dataflows or execute sql statements. You can also use volatile tables in teradata. for instance i create a volatile table with dates that i can change on the fly, it creates the volatile table and uses the record for a start and an end date in a later query. Do you have permissions to create tables or views? if you could create a view then maybe you would have an object to point to in powerquery? i.e. declare @tbl varchar(20) set @tbl = 'tbl1' declare @sql varchar(4000) set @sql = 'select \* from ' + @tbl print (@sql) -- use this to print out and see that your syntax is right and proper exec (@sql) -- use this to execute the string as a select statement
It's not weirdness. Parenthesis matter for logical grouping of conditional logic. The way OP's query is written right now evaluates as: WHERE ( ua.action = 'submit\_answer' AND ua.is\_student = True AND ua.user LIKE 'blabla&amp;#37;' AND sacw.status = 'fg1' ) OR (sacw.status = 'fg2') But he almost certainly wants this to evaluate as: WHERE ua.action = 'submit\_answer' AND ua.is\_student = True AND ua.user LIKE 'blabla&amp;#37;' AND ( sacw.status = 'fg1' OR sacw.status = 'fg2' ) Which is simplified by using IN() WHERE ua.action = 'submit\_answer' AND ua.is\_student = True AND ua.user LIKE 'blabla&amp;#37;' AND ( sacw.status in( 'fg1', 'fg2') ) Considering there are 100 millions rows in the table and OP's condition asks for any record with a status = 'fg2', conditional grouping could very well be the reason for the poor performance.
Basically, duplicate everything, maybe? Create a new set of tables, load all the inbound file second time to the new set of tables without truncating (you'd need to figure out why the target tables were truncated - is there data duplication or change - and add logic accordingly) and duplicate and deploy the reports changing all the queries to point to your new tables.
Do your reports look at transactional level data or are they just looking for an overall total? If it is a total then maybe just add a table that keeps a running total for however granular they need it. So for example if they look at it monthly maybe something like: Store | Month | Year | Amount ---|---|----|---- 1 | 1 | 2018 | 1234.23 1 | 2 | 2018 | 2345.32 1 | 3 | 2018 | 3456.78 This way you have the data you need, but you're not horribly duplicating the data. It also has the added bonus of running a bit quicker.
The virtual academy is a good source...I recommend you to read this book https://blogs.msdn.microsoft.com/microsoft_press/2017/04/07/new-book-exam-ref-70-761-querying-data-with-transact-sql/?utm_source=facebook.com&amp;utm_medium=referral I read it when i was preparing for exam. Its really good for understand how all this things works👌
The target tables are truncated by design, every hour when the SSIS job runs in the last step it truncates then reloads with the data from the stores.
The "design" is there for a reason (hopefully). What I'm saying is that you'll need to discover/reverse engineer that reason.
If there is a reason for this design, I'm not sure what it is it was implemented before I joined the company.
It would help to know a little bit more about your schema. Are the ticket prices stored in their own table or is the "sum ticket prices" a column in your passenger table?
They are not transaction level. They are more per day, the reason it runs every hour is because the day at the store does not close until the manager at the store closes their day. This is supposed to happen early in the morning but with over 50 stores the EOD can come in at any time.
The ticket prices are all in a Manifest table that is INNER JOINed with the Passenger Table. This is my current code to just get the top 11: SELECT TOP 11 Passenger.Name, SUM(Manifest.Ticketprice) AS [Total Amount Spent on Tickets] FROM Passenger INNER JOIN Manifest ON Passenger.PassengerNumber = Manifest.PassengerNumber GROUP BY Passenger.Name ORDER BY SUM(Manifest.Ticketprice) DESC;
Can't you just use OFFSET? SELECT passengers FROM table ORDER BY passengers OFFSET 10 ROWS LIMIT 1 n being 10, then you'll get the 11th then limit 1 means it'll only get one record. This kinda thing?
Not sure if this will translate to Access or not but using MS SQL, you can use Row_Number and sub-query: SELECT ABC.Passenger , ABC.SumOfTicketPrice FROM ( SELECT TOP 11 Passenger , SUM(ticketprice) as SumOfTicketPrice , ROW_NUMBER() OVER(ORDER BY SUM(ticketprice) DESC) as RowNumber FROM Passenger GROUP BY Passenger) as ABC WHERE ABC.RowNumber = 11
&gt; SELECT ABC.Passenger &gt; , ABC.SumOfTicketPrice &gt; FROM ( &gt; SELECT TOP 11 Passenger &gt; , SUM(ticketprice) as SumOfTicketPrice &gt; , ROW_NUMBER() OVER(ORDER BY SUM(ticketprice) DESC) as RowNumber &gt; FROM Passenger &gt; GROUP BY Passenger) as ABC &gt; WHERE ABC.RowNumber = 11 Sorry if this is a stupid question, but what does the ABC do? My professor never taught us this
copying my old comment: Pretty much all tasks can be done in variety of ways in sql. Subquery is just a result set different from your main - use it whenever you feel it's easier achieve your results in a step-wise approach; Correlated subquery is a result set dependent on the current row context - think of it as a fancy function. CTEs are a more modern way to write subqueries and also the only way to get to recursion in sql. If you need data from two result sets A and B and you need the columns side-by-side - it's a join of some kind. If you need them stacked (records from A, then records from B) it is a union. Watch for/track your row granularity (hopefully that's a concept that your school course went over) - joins can increase granularity, 'group by' decreases granularity. Pivot (rows to columns) decreases granularity, so it's a 'group by' type of operation; 'Un-pivot' (columns to rows) increases granularity, so it is a (cross) join-type of operation. Union could also increase granularity. 
The ABC simply gives the subquery an alias. You can then reference any column inside that subquery using the ABC. For example, if you wanted to join your main query to another table and have results from that recordset. SELECT ABC.Passenger , ABC.SumOfTicketPrice , D.QualifiesForDiscount FROM ( SELECT TOP 11 Passenger , SUM(ticketprice) as SumOfTicketPrice , ROW_NUMBER() OVER(ORDER BY COUNT(ticketprice) DESC) as RowNumber FROM tblPassenger GROUP BY Passenger) as ABC JOIN tblDiscounts D ON ABC.Passenger = D.Passenger WHERE ABC.RowNumber = 11 In this modified example, we also want to see if this passenger qualifies for a discount. That information lives in a seperate table. We can use his passenger name from ABC to look up that value in tblDiscounts (represented by the letter D). You can name ABC anything you want. I just chose that for simplicity. Of course, this joining would be better served using IDs but this just shows the power of Subqueries. You can essentially wrap it in a subquery and then have SQL see it as a virtual table to be used for filtering, summing, etc. Also, feel free to give this thread credit when you hand in your homework :) (I can see of no practical reason why someone would want to look at the 11th ranked passenger which leads me to think this is a homework assignment.)
Hey, Elfman72, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Thanks! That's exactly what I needed. So apparently, MS Access doesn't have the ranking functions that SQL Server has, but a work-around you could use is this: SELECT p.Name AS [PassengerName], m.[TotalAmt] AS [TotalAmt] FROM Passenger p INNER JOIN ( SELECT PassengerNumber, SUM(Ticketprice) AS [TotalAmt], ( SELECT COUNT(*) + 1 FROM ( SELECT PassengerNumber, SUM(TicketPrice) AS [TotalAmt] FROM Manifest m3 GROUP BY PassengerNumber ) m2 WHERE m2.[TotalAmt] &gt; SUM(m1.[TicketPrice]) ) AS [Rank] FROM Manifest m1 GROUP BY PassengerNumber ) m ON p.[PassengerNumber] = m.[PassengerNumber] WHERE m.[Rank] = 11 Please keep in mind that this is an expensive query. The sub-query will be executed for \*\*every\*\* row in the result set from \*m1\*
This worked great. Thank you so much for your help.
Unfortunately, I don't think MS Access has ranking functions like ROW\_NUMBER 
Sorry to let you down, but I don't know. I've always quit before the tutorials got to that point. :-) But I do know there are plenty of tutorials out there. Many recommend [Automate the Boring Stuff](https://automatetheboringstuff.com) (with Python) and the companion book, and web scraping is in chapter 11. 
How can you tell when a day is over for all stores, or any one store? What you need to do is create a stored procedure that runs on a schedule and which sucks all your data up from the daily table, aggregates it, and then pushes it into a table with a date. Do that every day and you will have reports going back as long as you want.
My typical openquery use is just to pull content TD into SSMS, but for some reason it never occurred to me to use the volatile table code that I use in Teradata and include it with the open query string. Funny how a fresh perspective can get you over a stumbling block. Thanks!
You should be able to pass variables to Teradata with a ? In your query, It may be :? .
Just grab the TOP 1 from your result in reverse order: SELECT TOP 1 Name FROM ( SELECT TOP 11 Passenger.Name, SUM(Manifest.Ticketprice) AS [Total Amount Spent on Tickets] FROM Passenger INNER JOIN Manifest ON Passenger.PassengerNumber = Manifest.PassengerNumber GROUP BY Passenger.Name ORDER BY SUM(Manifest.Ticketprice) DESC ) AS PassengerSum ORDER BY [Total Amount Spent on Tickets] DESC; Make sure that the result is what you want in cases where two or more passengers have the same sum.
SQL Server supports [Dynamic Data Masking](https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-2017) which may or may not be what you want.
SQL Server 2017 has dynamic data masking. I don't believe I have seen this on other DB engines. https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-2017 
I am going to try this in a little bit and I'll be sure to let you know how it goes. Thank you so much!
And starting with 2016 SQL Server supports compressed columnstore. Though while it can handle it perhaps that's not the best tool for the job. Querying such a data set (or anything bigger) in BigQuery or Data Lake Analytics would be more appropriate.
I think this was included in 2016
&gt; SELECT &gt; p.Name AS [PassengerName], &gt; m.[TotalAmt] AS [TotalAmt] &gt; FROM Passenger p &gt; INNER JOIN ( &gt; SELECT &gt; PassengerNumber, &gt; SUM(Ticketprice) AS [TotalAmt], &gt; ( &gt; SELECT &gt; COUNT(*) + 1 &gt; FROM ( &gt; SELECT &gt; PassengerNumber, &gt; SUM(TicketPrice) AS [TotalAmt] &gt; FROM Manifest m3 &gt; GROUP BY PassengerNumber &gt; ) m2 &gt; WHERE m2.[TotalAmt] &gt; SUM(m1.[TicketPrice]) &gt; ) AS [Rank] &gt; FROM Manifest m1 &gt; GROUP BY PassengerNumber &gt; ) m &gt; ON p.[PassengerNumber] = m.[PassengerNumber] &gt; WHERE m.[Rank] = 11 An error is popping up saying "Cannot have aggregate function in WHERE clause (m2.[TotalAmt]&gt;SUM(m1.[TicketPrice])). 
I don't think Access has OFFSET.
Oracle Advanced Security Data Redaction also has Dynamic Data Masking
Data Masking &lt;&gt; Data Scrubbing. Which do you really need? What about your "manual scripted process" makes it unworkable? Is that fixable?
you betcha, hope it works.
Spaceship-level database I love that.
Data Masking, specifically creating databases based on production DBs that contain gobbledygook instead of customer data. Our DBA has to run a series of scripts against the database. It's long and tedious (12+ hours of processing) and he's having to build the automation up by himself with the help of some devs. Really I'm hoping to just find a canned tool we can just buy, configure, and use instead.
This looks like it applies to the production database directly as a policy? We are looking to produce independent masked databases that can be mounted on other servers for testing/QA/reporting purposes.
The fuel data from the store is not populated in the stores database until the manager ends the day.
Hrm, I'll have to play with that. My understanding was that would only provide a user prompt to enter a variable from the Teradata client, I'll have to see if that works when embedded in PowerQuery.
Looks like you want to do data substitution rather than masking. Masking is more like showing XXX-XX-XXXX for a Social Security Number. Sometimes the data is stored unmasked in the database and is masked only in the application. Or databases can mask it on the fly for any query using built-in tools (as mentioned in other comments). But you sound like you want data substitution. You want to take production data and replace it with fabricated data to make a test data set. 
 SELECT n.NAME + ', ' + n.firstname AS 'Paralegal' , COUNT( CASE WHEN s.externaldesc = 'Registered' THEN 'Dumpty' ELSE NULL END ) AS 'Registered Cases' , COUNT( CASE WHEN s.externaldesc = 'Pending' THEN 'Humpty' ELSE NULL END ) AS 'Pending Cases' FROM cases C INNER JOIN status s ON s.statuscode = c.statuscode AND s.externaldesc = IN ( 'Pending' , 'Registered' ) INNER JOIN casename cn ON cn.caseid = c.caseid AND cn.nametype = 'PR' INNER JOIN NAME n ON n.nameno = cn.nameno GROUP BY n.NAME , n.firstname 
Thanks! The results I'm getting are different than the results from running the queries separately. I think it's because I screwed up and gave you a modified version of my queries for simplicity. When I try to make the appropriate changes to yours, it doesn't give me the results I expected. select n.name+', '+n.FIRSTNAME as 'Paralegal', count([n.name](https://n.name)) as 'Pending Cases' from cases C join status s on s.STATUSCODE = c.STATUSCODE and s.EXTERNALDESC in ('Designated - Awaiting Notification', 'Monitoring/Application Allowed', 'Monitoring/Pending', 'Monitoring/Published', 'No Action - Pending', 'Not yet filed', 'Pending', 'Published') join casename cn on cn.CASEID = c.CASEID and nametype = 'PR' join name n on n.nameno = cn.NAMENO group by n.name+', '+n.FIRSTNAME select n.name+', '+n.FIRSTNAME as 'Paralegal', count([n.name](https://n.name)) as 'Registered Cases' from cases C join status s on s.STATUSCODE = c.STATUSCODE and s.EXTERNALDESC like ('&amp;#37;registered&amp;#37;') join casename cn on cn.CASEID = c.CASEID and cn.nametype = 'PR' join name n on n.nameno = cn.NAMENO group by n.name+', '+n.FIRSTNAME
SELECT * FROM myTable WHERE ID IN ( SELECT ID FROM myTable GROUP BY id HAVING count(*) &gt;1 )
I've seen it and written it both ways. I kind of like commas before a bit better. No idea about best practices. IMO, the SQL works or it doesn't. If you can improve the readability, all the better.
Yeah, I'm leaning towards switching to commas before columns. I definitely like the all lowercase look for readability. I know many devs like to have all the reserved words as uppercase. 
I'm an uppercase reserved word guy. Maybe out of habit. Haven't really tried all lowercase yet.
I would suggest: Attaching the database to your SQLExpress instance (if you haven’t already) and ensure it is online. Creating a user for the application within that Database. Using connectionstrings.com to generate a valid connection string. Using that connectionString in your app.
&gt; When I try to make the appropriate changes to yours, can we see this version, please
Ive been working as a dba for the past 3 years roughly. I'm actually switching to putting the commas before the column names as well. My thought is that it makes it a little bit easier when you want to go back and remove or add a new column. I also am just starting to like the look of it more as well. 
i think it's usually called **leading comma convention** and sorry, but i'm in the upper case for reserved words group
I much prefer commas at the end of a line myself. You'll almost never see leading commas in normal code (such as C-style languages, C# for example), so why do it in SQL? I don't think the style your posted in your screenshot is super bad, but if it were me, I would compact it greatly so it takes up fewer lines. I frequently work with stored procedures that are between 500 and 1000 lines long, and it is absolutely essential to keep the line count down in order to have an overview of them. Even on a monster 27" screen with high resolution, it's so important to have lots of code visible on the screen at a time. I'm the kind of guy who writes a "SELECT x, y FROM z WHERE a = b AND 1 = 2" on one line. In your example, I would start by removing the empty line, I'd put all the columns in the SELECT statement on one line, put the ON clause on the JOINs on the same line as the JOIN (but with one line per AND....), and shrink the final OR clause to one line, just to name some examples.
Since you say you usually do this once per week, it indicates that it's not essential that this happens instantly. Have you considered creating an SQL job to do it? If you fold out the "SQL Server Agent" in the Object Explorer, then the Jobs folder, you can create a new job. You create a "Step" (just name it whatever), and in the step, you can either write some SQL in the textbox that moves the data from column email to OtherEmail, or even better, just perform an EXEC on a stored procedure you create for the purpose, which contains the SQL to move the data. I always prefer to make a job just call an SP, for a few reasons: 1) The textbox for writing SQL on a job is ridiculous since it's just a plain text box, no intellisense 2) Writing an SP makes it easier to search for, edit and debug the contents of the SP I usually create a schema named "jobs" on a database where I make these job-SP's, so I'd make an SP called "jobs.MoveEmailData" or similar. In that SP, you write a SQL statement which performs stuff like: UPDATE table SET OtherEmail = email WHERE OtherEmail &lt;&gt; email and Domain &lt;&gt; 'undesirable_domain' On the job you create, you also setup a schedule, and you could for example set this to run every 5 minutes, or just nightly if that's enough in your case. The benefit of a trigger is that the data will be moved instantly, and if something is wrong with the logic in the trigger, or either the source or destination is missing (for example a data), you'll get a hard error, which will also show in the programs/data layer/service which are saving data to that table - the whole UPDATE or INSERT will completely fail. However, this is also the downside - you risk messing other stuff up. Furthermore, implementing a trigger also drags down performance of your insert/updates a tiny bit, although in this case it'll be insignificant. With an SQL job, you can run the data transfer nightly, in a batch, and it's also easier to monitor (job history) and debug. Basically: Use triggers if you MUST MUST MUST have data updated in another destination *immediately* and can accept the fact that the trigger failing results in a hard error. Use an SQL job if data does not have to be moved immediately.
What problem are you having creating the trigger? https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql?view=sql-server-2017
&amp; instead of +?
CONCAT(field,'%') should work. 
I'm suspecting that the conversion is happening because of the values you are using. I find it is best to be direct and also snuff out any accidental implicit conversions you may have. Try this: SELECT CAST(1 AS CHAR(1)) + '%' /u/chardIII has a suggestion that should also work.
I use leading commas because statistically I'm going to add or remove an item from the end of the list. I don't have to remember to add a comma to the line above the line I'm adding or delete the comma from the line above the one I deleted. So when I'm altering my returned information I'm not touching the lines that don't need changing
This function doesn't work in SQL Server 2014 though
Didn't see this when I searched google! Unfortunately, I get this: "The data types float and varchar are incompatible in the '&amp;' operator."
A Date column stores just that -- a date. If you want datetime, then use a Datetime datatype. 
Microsoft Docs says that CONCAT applies starting with SQL Server 2012. https://docs.microsoft.com/en-us/sql/t-sql/functions/concat-transact-sql?view=sql-server-2017
Just read this post: https://stackoverflow.com/questions/43747799/concat-in-sql-server-2014 And it turns out that it's 2008 R2. Why it reports it as SQL Server 2014 in the About tab I really don't know.
I use it all the time in a 2012 instance. 
I generally drop ugly code, which is unfortunately all too common where I used to work, into http://poorsql.com or their Notepad++ extension and have gotten really used to seeing it their way and now generally format my code how they do. Drop this code in there and you'll see a few differences. 
1) you’re using SSMS 2014 to connect to a 2008R2 server. That’s why your about page says 2014. Or 2) you’re running Server 2014 but the Database is in 2008 Compatibility level. SELECT @@VERSION usually solves that mystery.
Yeah, that function returns 2008. The above explanations make sense.
Because the about tab is probably taking about ssms (the client you use to access the server). To check your server version, look in object explorer after connecting to the server. In parenthesis the will be "SQL server" followed by a version number. 2008r2 versions will start with 10.5
Yep, that makes a good amount of sense. I got the query to "work", but I'm puzzled as to why SQL thinks a String + Integer = Integer output.
There were a LOT of multi syllable words in that article. My brain started melting. 
Sorry to be a let down.. I’ve been working with cognos for the last 4 years. You can’t join across packages. You’ll have to use Framework Manager and add the measures/fields/relationships into the package you want to use/create a new package. Or.... embed your own SQL in the report
Based on data type precedence, a character data type will implicitly convert to a numeric data type. You can control this behavior with CAST or CONVERT functions to guarantee a character data type. 
Perhaps use varchar(max) instead of 255, i think it is slightly more optimized. 
so, i can write sql in cognos and join between packages?
Where is being displayed? Better to do this on the front end.
Yep I’m drunk but you should check out the row number function. Order it by created date descending and partition by user Id and you should be good. You can just grab the top two per user
Packages are what you make for end users so they don’t have to know the underlying database and relationships. You can just drag in anything from the same set in a package and it will work out what you need. Say you’re using a GL package, and want to include sales by SKU (which only comes from the Sales package). You’re going to have to write that as SQL for the report. (Or copy the generated SQL from the sales report)
So I would have the top 2 per user then but how would I go from there to get users with a CANCELLED status as the most recent and ACTIVE as the second most recent?
ALl_CAPS ,COMMAS_IN_FRONT = NO_AS_STATEMENTS , INDENT_CASE_STATEMENTS = MAX( CASE WHEN X THEN Y ELSE Z END ) let's see if this works in mobile 
Ah - what if you got the top two rows per user and then dumped it to a temporary location and then just looked at the status on those rows? You could make sure row 1 had a status of cancelled and two a status of active.
To add onto what /u/apikey said [+ (String Concatenation)](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/string-concatenation-transact-sql?view=sql-server-2017) &gt; Result Types &gt; &gt;Returns the data type of the argument with the highest precedence. For more information, see [Data Type Precedence \(Transact-SQL\).](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-precedence-transact-sql?view=sql-server-2017)
that sounds horrible and very rigid and limiting. it kind of assumes everything in your data fits into one nice neat little box which we know is not true for most companies, especially large ones with complex systems. 
If you are looking for accounts that should be active but have later accounts set to something else this may give you what you are looking for? SELECT * FROM Account a WHERE a.Status = 'Active' AND EXISTS ( SELECT * FROM Account asub WHERE asub.UserID = a.UserID AND asub.Status &lt;&gt; 'Active' AND asub.Created_at &gt; a.Created_at ) Keep in mind that you could flip all the filters if you wanted to select the accounts that were not active instead. (&gt; to &lt;, &lt;&gt; to =, = to &lt;&gt;) I don't know how big your dataset is so I must admit I always just take the lazy route of splitting a query into simpler chunks if there is not going to be a big performance impact. If your dataset is small enough / this is not an operation you need to do all the time you could just stack subqueries or put the dataset into a temp table and run similar queries to the one below to narrow the dataset down further.
But for companies where they don’t have someone writing SQL, they can spit out heaps of list reports. Also, its 100% down to who ever wrote the packages to what you see. Is this being implemented by your ERP vendor with their standard setup?
Yes, I will get that when I'm back at my PC. I really appreciate the help.
 select * from (select userID, status, row_number() over (partition by userID order by created_at desc) rn) foo where status = 'CANCELLED' and rn = 1 This will work in MS SQL. Not sure about Postgres.
I prefer a line per column, it is far more readable. Vertical space in stored procs send like an odd thing to try to limit, especially with collapsing in modern IDEs.
K, there are many great choices on the analytics an bi market now - Tableau, Microstrategy, Qlik and the right answer is, of course, PowerBI. Yet you end up with Cognos. How come?
&gt;I am not sure why without it, it tries converting the '%' to an integer. Shouldn't a string + integer return a string by default? I did a few quick tests. When concatenating a string and an integer, it converts the string to an integer and then does addition. If it can't convert the string to an integer it throws an error. From my experience this is normal. String -&gt; data type implicit conversion is common, such as string to date or datetime. Data type -&gt; string implicit conversion I have never heard of.
By putting commas in front you're shifting where the problem happens. Consider: SELECT a ,b ,c With commas in front you can comment out b and/or c columns. However, you can't comment out column a. SELECT a, b, c You can comment out a and/or b but not c. There are slight advantages to the comma in front style but it looks so ugly I went back to putting commas at the end.
Everyone develops their own style as their skills mature. It's a compromise between aesthetics, readability, functionality and the time and effort that goes into formatting the query by hand. Everyone has their own threshold for what they consider the best compromise. All keywords are capitalized Join condition on the same line Always use CTEs for subqueries except for correlated subqueries (where CTE can't be used) Always put parentheses around OR'd conditions, especially if it's the only thing in the WHERE clause (avoids mishaps) Everything aligns to the left edge; no indentation with very few exceptions
What are people's opinions on [alias] vs 'alias'? I feel that brackets look better because it doesn't add all the red but I'd like to hear what others think
With SQL Server you can't reference 'alias' in ORDER BY, so I have stopped using it.
Best tip ever! Thanks elboto
You're right. I don't think you can really go wrong either way as long as your consistent. 
I wrote SQL for about a year before realizing that keywords didn't have to be lowercase...
Bind variables should handle this (after you prepare your statement handle the subsequent execute should never parse code), but if you must, just create the narrowest filter you can. Use lots of surrogate keys and only accept simple integers, or strip strings of quotes and then pack then inside quotes for evaluation.
You're wasting your time trying to individually check for strings that you think are risky. Just use bind variables and you're done. You can sanitize inputs to an extent on the UI (based on data type, field length, lists of values, not allowing certain characters, etc.). But beyond that, binds are your best bet and the safest. Trying to check for every thing you think is dangerous will be a waste.
if a pattern like this existed, everyone would be using it switch to parameter binding instead
The first query really only looks at one column, cab_type, which would explain why a columnar data store would outperform a full table scan against a row-oriented table. But wouldn't it be more fair to compare execution times after placing an index on that column for the regular table? In that case, the table full scan would be replaced by an index fast scan, which may perform as well or better than parquet. Is there a reason this was not done in the benchmarking? I anticipate the argument that both comparisons were made without any additional tuning or index creation. Ok, but in reality it would be rare to run an OLTP or OLAP database without indexes on these key fields.
I guess I should make my point more clearly. I'm not afraid of the attack, but I need to know wheather there are attempt or not.
they spent a year looking for the best BI product and settled on cognos. To say, I was pissed when I heard this is an understatement. I was really hoping for microsoft or microstrategy. 
Best bet would be to implement logging in the application and as a best practice sanitize your inputs. **https://xkcd.com/327/** 
Sanitation and validation: check. So should I log there--especially, maybe, before even sanitizing? If so, backbto my question, any pattern I should check beside the usual suspects? Some I can think of: - stray semi colon - stray # and -- (comment) - start and end with ' - regex /(\S+)=\1/ - drop/truncate table (never from the app) - exec/declare - hex input - anything else?
My bad for digressing. But I want to know some of them. My point is I'm not for the attack, but I want to know if somebody is doing something. See also https://www.reddit.com/r/SQL/comments/8v1oad/comment/e1k712a
I'm not too worry about the query going into the database. I need to know the query itself. See https://www.reddit.com/r/SQL/comments/8v1oad/comment/e1k712a.
I'm not too worry about the query going into the database. What I want to know is the query itself.Any suggestion on the filters? Compare mine https://www.reddit.com/r/SQL/comments/8v1oad/comment/e1k712a.
&gt; I also don't really understand PARTITION so maybe that's why That's why. It creates an "order" of like rows (*partioned by* certain columns)... then he is saying `FROM() WHERE ROW_NUMBER = 1` meaning he is only taking the 1st record and getting rid of any duplicates.
That sounds like it could be solved by "TOP 1" and an "ORDER BY" clause. Maybe that would speed things up
Nope, it wouldn't. Look into ROW_NUMBER().
You mean it wouldn't speed things up or it wouldn't work? TOP 1 would also mean you wouldn't need a subquery
I would not work using a top 1. Top 1 will only give you 1 record, he wants to get all the "top" records based on the conditions of his ROW_NUMBER().
For MySQL you'd also want to look for stuff like `GRANT ALL PRIVILEGES ON *.* TO 'USERNAME'@'1.2.3.4' IDENTIFIED BY 'PASSWORD' WITH GRANT OPTION;`. Both GRANT and DENY shouldn't be executed from your application. Ideally, the application account should not have access to GRANT, DENY, DROP, or TRUNCATE.
Say you have 10 million rows with the following type of structure in a table: | SalesID | Date | Amount | | :--- | :--- | :--- | | 1234 | 2017-07-01 | 250.00 | | 1234 | 2015-02-01 | 400.00 | | 1234 | 2016-04-01 | 200.00 | | 1235 | 2012-11-01 | 12.00 | | 1235 | 2016-09-01 | 2610.00 | | 1235 | 2018-02-01 | 861.00 | | 1235 | 2013-05-01 | 991.00 | Now you have another table full of SalesID's which join to a customer table and I tell you that I want you to give me the name of each customer, as well as the amount of their largest, or smallest sale. How would you do that? 
He could just do ORDER BY us.date DESC
No, that will not work in a sub-query or a join. That won't work. Show me a code example.
I am not certain what you are trying to do, however I can tell you that if both columns are NULL in both sets then you aren't joining correctly. Need something like `OR ((A.Col1 IS NULL AND B.Col1 IS NULL AND A.Col2 IS NULL AND B.Col2 IS NULL) AND Col3 = Col3)` If you have duplicates you could just use a row_number() at the end to remove them.
best way would be to log anything not containing the expected allowed characters.
Is id_a supposed to be uniqie in the output?
What should happen for the 9th item in your example (destination Hong Kong, special = '')?
easily done with `case` or `coalesce()`
so, like the others, i'm not entirely sure what you want it seems like you want "join on col1, or if col1 is absent, then col2, but not both" which is select * from coalesce(sourcetbl.col1, sourcetbl.col2) as id join tgttbl on id = tgttbl.id;
If there is a fixed number of match types and each of them is guaranteed to match no more than once, then of course you can use a join for each one and coalesce to pick the preferred column. If not, then there is always the chance of multiple matches and so you have to pick the preferred row, via a ranking.
&gt; I did a few quick tests. When concatenating a string and an integer, it converts the string to an integer and then does addition. If it can't convert the string to an integer it throws an error. Yep, that's what the issue was in the end. Nothing to do with the percentage sign (which was treated as a string the whole time). I could be wrong, but Java seems to do implicit conversion just fine, but it's possible that this is because the resulting output is created as a string in the first place ("string foo = ..."). I don't personally see the harm in implicit conversion - in fact, by bringing the concatenate function to sql, its developers seem to agree?
Do you mind explaining why it was designed this way? Almost every data type can be expressed as a string, but very few can be expressed as a number (and those that can are usually defined as such to begin with). The concat functions in later versions seem to ignore this principle. 
What are the questions asked in the hw? 
1. Which zip code(s) have more than 100 HHAs? &lt;SQL code&gt; &lt;Results&gt; 2. Among the positive indicators, which question do HHAs reflect the best performance? the worst? &lt;SQL code&gt; &lt;Results&gt; 3. How many HHAs are there per state? &lt;SQL code&gt; &lt;Results&gt; 4. What state has the highest number of ‘State/County’ HHAs? &lt;SQL code&gt; &lt;Results&gt; 5. What is the average length of certification among HHAs in each state?(Hint: do not ignore # signs in DateDiff function and use 1/1/2013 as endpoint for certification) &lt;SQL code&gt; &lt;Results&gt; This is the database: [HHA Database](https://uwli.courses.wisconsin.edu/content/himt/400/su18/sec01/summer/Data-handling%20projects/1-2/HHA_database.accdb?_&amp;d2lSessionVal=oT2uN7auAogB7pqLPzsnggL9z&amp;ou=4121501) (I hope the link works)
I'll do it for $100.
Something probably turned quotes into Unicode smart quotes and whatever you're using to look at it doesn't understand utf-8.
Not so fast, I’ll do it for $150
$200, final offer. 
there is a fixed number of match types (2) and coalesce takes varargs besides the preferred row is the first one listed in the coalesce
Thanks for the help. Any advice on how to fix this? A quick Google search has returned this https://pypi.org/project/Unidecode/ but I'm not entirely sure how I'd use it. Is there something I can just throw the SQL file at?
Please read this wonderful article about outer joins. I reference it in all beginner and intermediate classes. Truly one of the great articles in the SQL space. http://www.sqlservercentral.com/articles/T-SQL/93039/ 
Make sure whatever you're using to copy files over does just that and only that and that whatever you use to edit files doesn't try to format them. Two common culprits, word processors and copy &amp; paste are not appropriate tools for these tasks.
Are you familiar with the following statements?: \- SELECT \- ORDER BY \- GROUP BY \- HAVING
maybe this site will help. I used it when I first began learning sql. [http://sqlzoo.net/](http://sqlzoo.net/)
 it is better to use right method for migration If you find one issue then it will be possible there and many other issues which you did not noticed.
I wouldn't add it as a field to the policy table, I'd rather write a qeury/view that calculates the count of accidents and make that a report, or other output to the end user. This can then recalculate each time run. By adding it to the table, you are duplicating and then you need to worry about keeping it up to date. I'd start with an INNER JOIN a cross the two tables. Get them linked correctly first (so you can check you are libking tables correctly. Then you need a GROUP BY to count them for the year. I think that should work, fairly new to SQL myself. Tldr Build your query up slowly, start joining the tables, check your output then add complexity one step at a time, checking after each itteration
Without seeing the actual table/column names, it's hard for come up with a good query. I need to know which columns exist and are common between the two tables. Does the first table have boat and year columns? If so, you don't even need to use the second table. You can just use count(*) and group by to get the total accidents for each boat by year. select boat_name, year, count(*) as 'total_claims' from Claims group by boat_name, year order by boat_name, year
It depends; if (a, 1) can only match (a, 1) or (a, empty) then ok. But if - as a last resort - it can match (a, anything at all) then the join will have to permit an arbitrary number of matching rows, eg (a, 2), (a, 3), and so on. If so, choosing the preferred match is a ranking problem.
Just to let you know...You are not receive help because there is no clear indication you have tried. 
Thanks. The problem I've got is I've only noticed these issues weeks after migrating (because as far as I can tell they only affect one small part of the site), and all the recent backups have these issues. So if I need to roll back to when we migrated to get a clean copy I'm going to loose a serious amount of work. I know trying to fix it isn't preferable but, if you had to, what would you do?
this seems like an extreme departure from OP's question
Since nothing is found it shouldn't find a reference. I guess I shoulda explained better, but basically its a reference table and 2 of the columns from my first table tell me how to find the reference. If there isn't a exact match for it (a=a, b=b) then it falls into just checking for one when the other is blank (a=a, b=&lt;blank&gt; or a=&lt;blank&gt;, b=b). But then needs to fall into a final try to see if it can match ignoring the column (a=a, ignore b or ignore a, b=b). I guess the problem seems to be I thought in the ON join, that the OR would find the first most exact match and stop in a join. What seems to happen is the join is finding multiple results and presenting all of those. I wrote it with a case statement in another test and that also fails.
Added a bit more explanation. But basically the lookup table has a reference and I need to get that reference from the most specific I can but fall back to less specific matches.
It should be really, but a distinct here can filter the correct results sometimes based on the order of matching from the reference table.
I actually tried to change from left join to others (right, inner) and tried with and without the outer. Oddly enough, from my test data it was providing me the same results in all cases. Figure I was doing something wrong as I know I should get different results.
Imagine you had lookups for (x, 1) and (x, 2) and then (x, 3) showed up in your data. The condition (a = a, ignore b) is going to give you two matching rows. 
Adventureworks db + MS SQL Server community edition? sqlbolt.com w3schools.com sqlzoo.net
That is what I discovered. I thought initially it was because OR was allowing it to find any matches. I tried to switch to a CASE setup and that also failed. http://sqlfiddle.com/#!18/9460d/27/0 Never wrote a CASE in a ON join before, so was interesting to see how simple that is.
Thank you!
If your data contains destination = China, special = foo, and you have lookups for (China, bar) and (China, blah), which would you want to use, if any? 
practity.com
This a great resource, thank you!
If the destination is match-able but the special isn't, the special falls back to the blank value ""). This is is in the 4th and 5th OR statements where I told it destination matches AND lookup special was "blank". Is there a better way to accomplish this then. I realized that now with the join.
Using a sub select inside of the CASE in the ON does limit the results a bit better http://sqlfiddle.com/#!18/9460d/31/0 The main items table has a WHERE that should always limit the results initially to about 200 results, but not ideal for performance reasons. Getting ugly, I think I need to re approach this somehow.
Ok but would you ever want to fall back on a row in the lookup table where destination is the same, and special is different *and not blank*? If not, it considerably simplifies things and you can do everything with join and coalesce. Instead of having one join with a complex condition, try having several joins (on the same lookup table) with different conditions. And don't put subqueries in join conditions, that's just gross :)
Dynamic or Static data masking needed? Here are the differences explained: [https://www.datasunrise.com/data-masking-made-simple/](https://www.datasunrise.com/data-masking-made-simple/) Datasunrise has both actually. What database you have where data has to be masked? Also sometimes could be important to have a role based masking, when users without required permissions cannot see the real data. Hope this helps
I need some revision material and questions/answers please? 
sololearn, sqlbolt, w3resources 
Never would want to do that. Order of what is expected: 1. Find exact match for destination and special 2. Find exact match for destination and special is blank 3. Find exact match for special and destination is blank 4. Find exact match for destination and ignore special ensuring we use the special is blank match from lookup 5. Find exact match for special and ignore destination ensuring we use the destination is blank match from the lookup.
My favorite MaxDop post: https://www.brentozar.com/archive/2013/08/what-is-the-cxpacket-wait-type-and-how-do-you-reduce-it/amp/
In case 2 and 3, you need to be more specific: special / destination is blank in the data, but what about in the lookup table? Can't you simplify it to: 1 - either the item (destination, special) = the lookup (destination, special) 2a - or the item destination = the lookup destination and the lookup special is null 2b- or the item special = the lookup special and the lookup destination is null. I suggested converting empty strings to nulls to simplify testing for blanks.
That is exactly what I was wanting to get. More joins than I wanted to accomplish as my simplified example doesn't include the 7 other tables I already join. But that gets me the exact results I expect.
Ok, good. You don't have to add the other joins here, you can always keep them in a separate CTE. That way you can keep the logic clear. 
You should give that man some sweet Reddit gold.
With DDM, you could apply those policies to the database, then you pull those tables into QA/testing etc using a service account that doesn't have "unmask" permissions, and thus your underlying data is now on the QA database, completely masked.
My workplace has an environment with a similarly sized database, 128GB of RAM with 16 cores. Though it's a major backbone system, so it's a requirement. 13GB of RAM strikes me as low for any production database system, so the server likely has to pull from it's hard disk drives, which are just not slow. Strictly speaking, though, it could be a network latency issue as well, so you'd want to investigate more about the slowness. Could be an application issue, too. Are queries running long? Is your cpu pegged at 100%? Is your page life expectancy always below 300? Jumping up the memory could be a first step if you don't have a better set of troubleshooting info. RAM is pretty cheap and being at 64GB means that almost all of your database gets held in memory, alleviating a disk issue a lot, too. In database land, RAM is the quick fix, which is indeed why every vendor says their application needs 64-128gb of RAM, it covers up all their mistakes. If you increase RAM and see the issue still, you'd want to look at network or application issues.
Will look into that. Awesome help here. I knew what I wanted, but not how to get it. Not a DBA here, just a programmer wanting the program to work how it should and dealing with denied requests to change the UI that fixes this.
Thank you for your input! This has given me a great starting point to figure out where to look, or even what terms to search for. I'm going to study how to use the join function and the other terms you've mentioned and do some thinking. I've uploaded some sample data if you want to take a look! https://ufile.io/n7s1d
Thank you for giving me a starting point for functions to study! I've uploaded some sample data if you'd like to take a look: https://ufile.io/n7s1d
MaxDOP is the maximum number of parallel threads a single query will be allowed to create. SQL Server will (license permitting, and assuming you haven't told it not to) use all your cores to run multiple queries/processes concurrently. So if MaxDOP is 4, you may have a dozen queries *each* running up to four threads in parallel, and those queries will be spread across all your cores. 
Stack Overflow's database is available for free, and there is a smaller 10GB version available as well. https://www.google.com/amp/s/www.brentozar.com/archive/2015/10/how-to-download-the-stack-overflow-database-via-bittorrent/amp/
Update: Through a lot of tinkering and youtube I've figured out how to type a query for just counting one vessel policy's annual claims, row, which is a start: SELECT COUNT(*) AS 2008_CLAIMS FROM CLAIM_TABLE WHERE (CLM_DATE BETWEEN #02/20/2008# AND #02/19/2009#) AND CLAIM_VESSEL_CODE ='BOAT1' 
2nd Update: I've figured out how to return a list of vessel codes with # of claims for the 2009-2009 policy year: SELECT CLAIM_VESSEL_CODE, COUNT(*) AS 2008_CLAIMS FROM CLAIM_TABLE WHERE (CLM_DATE BETWEEN #02/20/2008# AND #02/19/2009#) GROUP BY CLAIM_VESSEL_CODE ORDER BY CLAIM_VESSEL_CODE 
Try [Strata Scratch](https://stratascratch.com). It's a SQL IDE that you can log into and do a bunch of exercises that they have on their website. They also allow you to upload your own datasets.
I like Mode Analytics tutorial. They don't really have a sandbox for you to play around in though.
I uploaded your data into SQL Server and was able to run a couple of scripts that udpates that empty column. This will probably work in Access also, because tehey are both MS, but not sure becuase I'm not familiar with Access. Probably not the most elagant way to do it, but I was in a rush. I used the first query to get a data set and inserted it into a temp table. Then used the temp table to update the policies. If you want to see the select data before inserting into the temp table just comment it out like in [this screen shot](https://i.imgur.com/cBasojL.png). select policy_no, p.vessel_code, v.vessel_name, pol_inception, count(clm_date) as annual_claims into #temp from policies p join vessel v on v.vessel_code = p.vessel_code join claim c on c.vessel_code_ = v.vessel_code where cast(clm_date as date) between cast(pol_inception as date) and dateadd(yy, 1, cast(pol_inception as date)) group by p.policy_no, p.vessel_code, v.vessel_name, pol_inception order by policy_no, vessel_name, pol_inception update policies set policies.annual_claims = cast(t.annual_claims as varchar(10)) from policies join #temp t on t.policy_no = policies.policy_no Couple of other notes. I didnt take time to set the data types when I imported the data, so I had to use cast() on some of the dates to actually make them date data types. You may not need to do that. Also, I assumed all policies are extend to one year after the date in policies.pol_inception column, since that was the only date in that table. To get the date of 1 year after, I used the dateadd() function. Hope this helps, and good luck.
Your link is broken. &gt;ERROR, PASTE ID IS INVALID, OR PASTE HAS BEEN REMOVED!
What do you mean by forward engineer? Did you make the ID columns NOT NULL and provide a default value?
Yeah I did. I managed to figure out how to do it without having to use an EER diagram
Have you properly explored every option? Are you absolutely certain that there is no way of doing this within requirements except as a trigger? 
Okay - if you're still having an issue then can you provide a screen print of the table create statement? 
LEFT JOIN would work. Not saying it will be faster but it would work. So does a sub query with top 1 order by. Very bad practice IMO doing a subquery in this manner but it works. WITH CTE_TEST AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST WHERE Num+1 &lt; 20), CTE_TEST2 AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST WHERE Num+1 &lt; 20) SELECT *,(select top 1 Num from CTE_TEST2 C2 WHERE C.num &gt; C2.num ORDER BY 1) FROM CTE_TEST C 
rdp is really bad for big copying things if you are using the connection itself to copy (IE Copy from your local desktop into the RDP window onto the remote machines desktop). FTP - Filezilla is a better option.
Sorry for the wait. Here is the version that I ran. I noted where I made the change: SELECT n.NAME + ', ' + n.firstname AS 'Paralegal' , COUNT( CASE WHEN s.externaldesc = 'Registered' THEN 'Dumpty' ELSE NULL END ) AS 'Registered Cases' , COUNT( CASE WHEN s.externaldesc = 'Pending' THEN 'Humpty' ELSE NULL END ) AS 'Pending Cases' FROM cases C INNER JOIN status s ON s.statuscode = c.statuscode AND s.externaldesc in ('Not yet filed', 'Pending', 'Published') --this is the area that I changed INNER JOIN casename cn ON cn.caseid = c.caseid AND cn.nametype = 'PR' INNER JOIN NAME n ON n.nameno = cn.nameno GROUP BY n.NAME , n.firstname 
What DBMS are you using? Your reference material could be outdated, but it's also possible that you're attempting to use a proprietary extension of SQL for a system that it wasn't intended for (eg, using PL/SQL in Microsoft SQL Server).
&gt;I was hoping to have my spreadsheet uploaded to SQL and search from there, That will work fine.... so long as you write all the searching code, extend the app code to support searching, query strings etc etc. &gt;I'm well versed in web design, but not advanced coding for making jQuery calls. jQuery is up in the browser layer... databases are below the app layer. Web design has no value here except in deciding in UI layout. Being well versed in web design in regards to this is like being knowledgeable about car window tinting when you are facing an engine issue. You options are: 1) Hire a developer 2) Spend next 3-6 months learning. 3) quickly hack together a solution with many layers of the digital equivalent of cellotape and watch it crash and burn a short while later when some bolted on technology fails or you get hacked.
What flavor of sql are you using? This should be doable with charindex and left, as you mentioned, but everything can vary from platform to platform. 
I think its mariadb but i run it through xampp, i have another post about the content that should come out, if you scroll down you'll see it if not I'll just retype the content here.
I'm kind of getting from the replies that a trigger isn't the neatest, cleanest way to do this. I think I'm going to look at making it a Job instead. 
&gt; Anyone have any idea whats happening? yeah. you're using weird generator tools instead of writing sql, and they're doing awful things
That won't work in a subquery such as: FROM Table A LEFT JOIN ( SELECT * FROM Table2 ORDER BY ) B The only reason it's working in the CTE is because you're basically doing a ROW_NUMBER, yes?
 AND s.externaldesc in ('Not yet filed', 'Pending', 'Published') --this is the area that I changed since it's no longer in this list, you are now filtering out 'Registered' rows therefore it's no surprise that the CASE expression up in the SELECT clause counts 0 of them
I'll upload a screenshot tomorrow, I ragequit SQL for the day
At least I'm learning a whole lot of how *not* to use SQL
Thank you, and thanks for your patience. I feel like I'm being pretty bone-headed here. I will adjust and report back.
https://basitaalishan.com/2014/02/23/removing-part-of-string-before-and-after-specific-character-using-transact-sql-string-functions/
Charindex to find the index of the hyphen. Left to select characters from 1 to the index of the hyphen. select LEFT(':'. charindex(myLabel)) ... something like that. 
Which tool are you using to do ERDs? See if allows to generate deltas (changes only) or has an option to drop/recreate tables.
This worked nicely - thank you again. I made a change to the "status" inner join where noted: SELECT n.NAME + ', ' + n.firstname AS 'Paralegal', COUNT(CASE WHEN s.externaldesc like ('%registered%') THEN 'humpty' ELSE NULL END) AS 'Registered Cases', COUNT(CASE WHEN s.externaldesc in ('Pending', 'Designated - Awaiting Notification', 'Monitoring/Application Allowed', 'Monitoring/Pending', 'Monitoring/Published', 'No Action - Pending', 'Not yet filed', 'Pending', 'Published') THEN 'dumpty' ELSE NULL END) AS 'Pending Cases' FROM cases C INNER JOIN status s ON s.statuscode = c.statuscode AND liveflag= 1 -- attribute encompasses all of the statuses I need INNER JOIN casename cn ON cn.caseid = c.caseid AND cn.nametype = 'PR' INNER JOIN NAME n ON n.nameno = cn.nameno GROUP BY n.NAME, n.firstname order by Paralegal 
&gt;That won't work in a subquery such as: Really??? I'd suggest you try such things before categorically denying they can be done. WITH CTE_TEST AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST WHERE Num+1 &lt; 20), CTE_TEST2 AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST WHERE Num+1 &lt; 20) SELECT * FROM CTE_TEST C LEFT JOIN (select top 1 Num from CTE_TEST2 ORDER BY 1) C2 ON C.num &gt; C2.num &gt;The only reason it's working in the CTE is because you're basically doing a ROW_NUMBER, yes? Point to where ROW_NUMBER is being called in my code. 
Again, try to structure what you wrote into the following format: select * from table join ( your subquery here) Not using a CTE, tell me how you can get it to work.
General rule of thumb - explore every other option before resorting to a trigger. If you cannot think of another option, do some more research. They can add significant performance overhead to any insert/update/delete and there is nearly always a better way. If there is ever a guaranteed sign of a bad database design its one with multiple triggers on each table. 
This is the 3rd time you have been wrong without making any attempt to check what you are saying is correct. /u/notasqlstar Next time please test this stuff yourself as its really basic and getting it wrong 3 times in a row is embarrassing. Also before you come back and say its behaving differently cause its a temp table - go test it yourself this time but I will give you a hint - it will return the same output. WITH CTE_TEST AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST WHERE Num+1 &lt; 20) SELECT * INTO #Test FROM CTE_TEST; WITH CTE_TEST2 AS (SELECT 1 as num UNION ALL SELECT num+1 from CTE_TEST2 WHERE Num+1 &lt; 20) SELECT * INTO #Test2 FROM CTE_TEST2; SELECT * FROM #Test C LEFT JOIN (select top 1 Num from #Test2 ORDER BY 1) C2 ON C.num &gt; C2.num DROP TABLE #Test,#Test2 
A CTE is not a subquery. Using #tables is not a subquery. You cannot use TOP in a subquery to get these results. You simply cannot. You are talking about a *recursive* sub-query (CTE) and sub-queries by definition are not recursive. You *CANNOT* use a subquery with TOP 1 to achieve the result you want. You *can* use a CTE if you want to mimick the functionality of a row_number() but that is ridiculous and not at all a best practice, nor something you should tell someone to do in lieu of a subquery. Keep grandstanding all you want, but you are using a CTE, not a sub-query.
You are putting words into my mouth and still not explaining clearly what you think is impossible to do in a join or sub query. &gt;Keep grandstanding all you want, but you are using a CTE, not a sub-query. CTE existed only to generate the table data. LEFT JOIN (select top 1 Num from #Test2 ORDER BY 1) C2 ON C.num &gt; C2.num That above code is a sub query join on a temp table. It is not from a CTE. It is a sub query off a temp table in a left join. The CTE is out of context at the point the final select is run. It does not exist at that point. You cannot reference it. There is not any question on this. The LEFT JOIN ONLY points at a temp table. It is a select encapsulated in () brackets inside a left join. It is not a CTE. I give up. 
I'm not putting words into your mouth. I have repeatedly asked you to show me an example in the format I provided which is using a **subquery**. You have repeatedly **failed** to do this and keep providing an example that uses a **CTE**. Using **temp tables** is not the same thing, either. &gt;That above code is a sub query join on a temp table. It is not from a CTE. You clearly have no idea what a recursive sub-query is, or a CTE. Your example **FAILS** to meet the criteria that I gave, and does not at all mitigate the **FACT** that you cannot achieve the desired result using only a **sub-query**. You're wrong. Have a nice a day. Go ahead and ask some other people who are active in this subreddit to comment, or go ahead and make a unique post asking them to weigh in if you want to learn. 
You are a champion. Thank you for your help. I have a dumb question: Why are you tying vessel_code and vessel_name as p.vessel_code and v.vessel_name? After the Join function, why are you typing v and c after vessel and claim? I'm looking up the functions thatI don't understand, but I'm not sure how to google what you're doing other than 'entering single letters with table names SQL'
Did you rename the vessel code fields in each table with a v.vessel_code for the vessel table and p.vessel_code for the policy table so you could tell them apart while writing your query?
&gt;A subquery is a query that is nested inside a SELECT, INSERT, UPDATE, or DELETE statement, or inside another subquery. Source: https://docs.microsoft.com/en-us/sql/relational-databases/performance/subqueries?view=sql-server-2017 Now explain to me exactly why this is not a sub query: LEFT JOIN (select top 1 Num from #Test2 ORDER BY 1) C2 ON C.num &gt; C2.num Let me highlight the relevent bit more: **(select top 1 Num from #Test2 ORDER BY 1)** Now please... tell me how this is not a query nested inside another query? Seriously.... how is that not a sub query? Or are you working with a different definition of sub query to the folks over at microsoft? Can you please provide an example of a sub query? IE an actual lump of code that you would call a sub query? Could you explain how yours is a sub query but mine is not? Because the select inside the brackets () is inside another select. This is pretty much the textbox definition of a sub query. 
I have already explained it to you repeatedly. You are using what is known as a "recursive subquery" AKA a CTE. A subquery **cannot be recursive** by definition. Go ahead and ask people about this. You are seriously wrong. You are not understanding what I said to the other person and just talking. This operation **cannot** be done by a subquery alone. The way you are using a CTE is in fact doing exactly what the row_number() function is designed to do. When the original person said, "maybe I don't understand row_number()" he was hitting the nail on the head. He doesn't understand it, because you can't use top 1 in a subquery to achieve this result... because you need to use a recursive or window function. &gt;Because the select inside the brackets () is inside another select. This is pretty much the textbox definition of a sub query. Google "Difference between a CTE and a subquery" and look at StackOverflow, or ask other people in this sub. You are categorically wrong and putting out some really bad information that is going to confuse someone who doesn't (by their own admission) understand row_number(). In fact, if you ever wrote code the way you are suggesting instead of using row_number() I would laugh at you and make you go back and rewrite it. You're using a 'hacky' approach to force a top 1 into a CTE --&gt; which isn't even the example I gave, or even relevant to what I said in the first place. So good job.
I'll repeat myself. Please provide an example in the following format to achieve the desired results: SELECT * FROM Table JOIN (&lt;yoursubqueryhere&gt;) If you can't do that, then you are admitting that it can't be done. Putting shit into a #table, or using a CTE isn't what I said, and isn't relevant to what I am talking about vis-a-viv row_number().
I give up. You are truely an idiot if you think that bold part is not a sub query. Good day sir.
Hey, therealcreamCHEESUS, just a quick heads-up: **truely** is actually spelled **truly**. You can remember it by **no e**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
A CTE &lt;&gt; a sub-query. /u/ichp can you please chime in here? 
Stop replying. I am bored of you and have no wish to continue discussing this.
This notification freaked me out :) Let me read this thread.
Hey look, Subqueries aren't recursive. https://stackoverflow.com/questions/706972/difference-between-cte-and-subquery
I am no postgresql expert. I had no idea what stack builder is (a quick google and still don't have any idea). Anyway. What operating system are you using? How do you install PostgreSQL? That quick googling returned this article https://www.enterprisedb.com/docs/en/9.3/pginstguide/Table%20of%20Contents.htm#TopOfPage maybe it helps. P.S.: If you search for advice "how to install PostgreSQL" google for exactly that. Not "how to set up sql DB" because it is very generic and it won't return many helpful results. P.P.S: I hope I don't sound smart@ss. I really try to be helpful.
Good. You've been summoned. 
Ha no worries, those are called Aliases. https://www.w3schools.com/sql/sql_alias.asp Bascially I am renaming the table so I don't have to type out the full table name every time I reference a specific column in the table. By typing "policies p" that means "from now on policies is p" So instead of having to type policies.policy_no , I can simply type p.policy_no instead
Oh dear lord, your understanding of SQL is as bad as your ability to read. Now please re-read my last message except much more slowly this time (quoted here encase you have difficulties...): &gt;Stop replying. I am bored of you and have no wish to continue discussing this.
No.
Can the rest of the string contain a hyphen? If it can't, then you can use CHARINDEX. If it can, then your solution is more complicated. Here's how I would do it in SQL Server: LEFT(label,LEN(label) - CHARINDEX('-',REVERSE(label)) + 1)
Not at a computer but you probably want to look up Postgresqls guide to installation and first time startup. Are you doing everything from CLI? Or are you able to use the GUI? There’s no shame in using the GUI. 
I am on Windows 10. I did search up "how to install PostgreSQL" but the articles I read were explaining how to do it on another operating system or didn't cover the issue I ran into. I felt like it would be useful to get in touch with someone who can provide step-by-step instruction, like a tutor.
nah, I used the Aliases if the column existed in two different tables. For example, vessel_code exists in multiple tables. So I need to tell SQL Server not only the column name, but also which table. Otherwise it doesn't know if I mean claims.vessel_code or vessel.vessel_code. With aliases, that would be c.vessel_code ot v.cessel_code. I need to clarify the table, or else it will say the column name is ambiguous 
How'd you guys plummeted into the abyss of "arguing semantics"? Anywho, "(subquery)" in the select list/expression is officially known as scalar subquery, meaning that it will return at most 1 row and 1 column. Scalar subqueries are somewhat different that subqueries but the definitions somewhat overlap, meaning that there are scalar subqueries you could use in your table expression ("from" clause) CTE is common table expression, a somewhat better way of writing subqueries, it allows recursion, but ironically CTEs cannot replace scalar subqueries. Going back to OP, it does seem to have been caused by missing parentheses in the WHERE clause, so the above is purely for entertainment value.
Update: I was able to open the "pgAdmin4" application and it gave me a much more intuitive GUI. Actually had tools for creating a DB and such. 
I have no idea. I was trying to help someone understand what a row_number() is and why it's needed/useful for taking out duplicates. Yes, I agree with the entertainment value but relative to the other person who first asked about using TOP 1 and why would you even want to use row_number() --&gt; the answer is that you can't use a subquery (scalar) to achieve the desired results of deduping something. Moving into CTE's and #tables, etc. 
Hello, I know this is not the answer you are looking for, but why don’t you just make it one table that contains employees data and add a column state that references a table that contains State information. And since a state is part of a country in your table state you could have a column country. That way if you want the total number of employees “worldwide” you can simply count the rows in employees table. And if you want only employees for a specific state/ country, you simply make a count ( select count) over a join statement between employees and state tables. Ps: I answered this from my phone so sorry if I didn’t go into full details. And if you can’t refactor your database I am not sure how you’d manage that but I’m interested in the answer
Thank you so much! As I was tooling around with this, I realized why you used single letter, as it was getting to be a pain in the ass to type out all the table names, and also realized why you specified each table the identically named fields needed to be pulled from. I didn't know about the alias function, so I just renamed my tables p, v, and c. I'll try again with aliases. I didn't include the expiration date column in my sample data, which made this a little easier, but I got everything to work with the below code! The structure of the JOIN functions works a little differntly in MSACCESS I think, I had to use INNER JOIN or else I'd get syntax errors. Anyways, this is what it looked like: SELECT P.POLICY_NO, P.PVESSEL_CODE, V.VESSEL_NAME, P.POL_INCEPTION, P.POL_EXPIRATION, COUNT(CLM_DATE) AS ANNUAL_CLAIMS FROM (C INNER JOIN P ON C.CVESSEL_CODE = P.PVESSEL_CODE) INNER JOIN V ON P.PVESSEL_CODE = V.VVESSEL_CODE WHERE CLM_DATE BETWEEN POL_INCEPTION AND POL_EXPIRATION GROUP BY P.POLICY_NO, P.PVESSEL_CODE, V.VESSEL_NAME, P.POL_INCEPTION, P.POL_EXPIRATION ORDER BY P.POLICY_NO, PVESSEL_CODE, V.VESSEL_NAME, P.POL_INCEPTION
row_number() over (order by ID) is a (select count(*) from mytable T2 where T2.ID &lt;= T1.ID) from mytable T1 though :) subqueries can be powerful too :)
Yes, which is why I've repeatedly said that if you use a CTE, or #tables you are simply recreating the functionality of row_number() and using the recursiveness to perform a window function --&gt; which you cannot do in a sub-query alone such as: select * from table join (&lt;insertsubquery&gt;) I'm not trying to be a dick, but I spend a fair amount of time reading and learning. If I'm wrong I want to know.
hmmm.... When I change the table names back to policies, vessels and claims and try to use aliases I get a syntax error on the FROM function SELECT policies.POLICY_NO, policies.PVESSEL_CODE, vessels.VESSEL_NAME, policies.POL_INCEPTION, policies.POL_EXPIRATION, COUNT(CLM_DATE) AS ANNUAL_CLAIMS FROM (claims AS C INNER JOIN policies AS P ON C.CVESSEL_CODE = P.PVESSEL_CODE) INNER JOIN vessels AS V V.PVESSEL_CODE = P.VVESSEL_CODE WHERE CLM_DATE BETWEEN POL_INCEPTION AND POL_EXPIRATION GROUP BY P.POLICY_NO, P.PVESSEL_CODE, V.VESSEL_NAME, P.POL_INCEPTION, P.POL_EXPIRATION ORDER BY P.POLICY_NO, PVESSEL_CODE, V.VESSEL_NAME, P.POL_INCEPTION 
Awesome, glad you got it. yeah in sql server "join" is short-hand for inner join and means the same thing. Best of luck on your future sql journies 
Sorry, I dont get the counterpoint - sql is not a kind of language where you'll have only one 'right' solution. Analytical/window functions are what they are, CTEs and subqueries are their own thing, temp tables you can use to capture intermediate result sets and you can sometimes get to the same functionality in different ways, and you'll have different performance considerations in every case. Can you give a specific example for clarity, please? (I have no clue what OP's statement was supposed to do in the first place, so a new one might be better).
it's because in your select statement, ou are still using the full names. Policies is now P, so it should be p.policy_no. Not policies.policy_no
Thanks again!
If the computer ID itself isn't unique, what combination of columns is? When you are joining, are you using just computer id, or a combination of multiple columns in joins? Need to have a better idea of what kind of queries you're running against this to answer the question.
The counterpoint is that you can't do it in a scalar subquery using a TOP, or more simply a subquery. Example [here](https://www.reddit.com/r/SQL/comments/8unfl6/query_taking_forever/e1khp4e/). The reason for this explanation is that subqueries are not recursive. That's *why* there is such a thing as a row_number() function (or one of the reasons,) relative to [this](https://www.reddit.com/r/SQL/comments/8unfl6/query_taking_forever/e1kfr4k/) question about not understanding row_number().
We use various columns for our joins but it's mostly the date + comouterid or comouterid +brand
for your example I'd write a cross apply (i just love that we can use functional approach in queries now), like so: select sc.CustomerID, sn.customerName, min( s2.Amount) from Select_Customers sc join Customer_Names cn on cn.customerID = sc.customerID cross apply (select top 2 s.Amount from Sales s where s.customer_id = sc.customer_id)s2 group by sc.CustomerID, sn.customerName This doesnt change the fact that analytical/windowing functions are convenient, useful and usually perform well and I could have done the above with row_number as well :)
I realized this as soon as I posted it :)
Fair enough, I stand corrected. You can do it with a CROSS APPLY. 
What is "computer ID" if it's not a unique identifier for the computer? Sounds like a poorly-named column at best. What is the purpose of this table? What are you attempting to track in the first place, and why would it not be better-served with a pre-existing analytics platform?
I think here there are some exams: [http://practity.com/591-2/](http://practity.com/591-2/)
Sorry mysql i got confused.
You should put an index that speeds up your queries. No one here can say what the index should be because we don't know your queries. If you frequently filter the table on computer ID, or computer ID in conjunction with some other fields, those would be good candidates.
Instead of using a subquery in your UPDATE, use two left joins: UPDATE dst SET GrandTotState = COALESCE(tot1.TotCountryEmp, 0) + COALESCE(tot2.TotStateEmp, 0) FROM DEALDATA as dst JOIN inserted AS i ON dst.QDEALDATA = i.QDEALDATA LEFT JOIN GrandTotCountry AS tot1 ON tot1.QDEALDATA = dst.QDEALDATA LEFT JOIN GrantTotState AS tot2 ON tot2.QDEALDATA = dst.QDEALDATA
I'll message you an example I did the other week for work should explain it better
Might as well reply here, This should hopefully point you in the right direction. Bare in mind i'm a newbie and there is a easier way to do this i just haven't found it yet SELECT CONCAT (clawback_date, ' ', retension_status, ' ', opportunity.name) AS comments, retension_status, stage, address_city AS city, address_state, address_country, address_postal_code AS postcode, TRIM(LEADING '0' FROM phone_number.name) AS main_phone, close_Date FROM opportunity JOIN contact_opportunity ON contact_opportunity.opportunity_id=opportunity.id JOIN entity_phone_number ON entity_phone_number.entity_id=contact_opportunity.contact_id JOIN phone_number ON phone_number.id=entity_phone_number.phone_number_id JOIN entity_email_address ON entity_email_address.entity_id=contact_opportunity.contact_id JOIN email_address ON email_address.id=entity_email_address.email_address_id WHERE clawback_date between CURDATE() - 7 AND CURDATE() +1 AND opportunity.deleted = 0 AND stage = "Clawed Back" AND retension_status &lt;&gt; "New Policy Written - Do Not Contact" AND retension_status &lt;&gt; "Complaint Do not Contact" AND retension_status &lt;&gt; "Client Deceased - Do Not Contact" AND retension_status &lt;&gt; "Personal Client - Do Not Contact" 
I don't know of a way that SQL Server will do this for you, but auto-detecting relationships is common in many BI platforms. PowerBI does it. It may not be perfect, but it could make things easier. https://docs.microsoft.com/en-us/power-bi/desktop-create-and-manage-relationships
Basically the table gives a computerid to a person's computer if they go onto our website. We get thousands of users that visit our website and it just tracks it. Say if you log into our website, it will give you an ID and remember your computer . We use this table to analyze how often a user will revisit our website and the count of users that visit our website by date. We use the comouterid and date to perform our joins to other tables 
&gt;Perfect! That worked. Thank you very much!!
Unfortunately with this application I add fields and cascades to an empty form and when I save it will create all the tables automatically. It's really only for capturing data but for this project I needed to have fields updated automatically with triggers.
Here you are a resource for beginners [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. Everything for free.
I’m sorry but I don’t see how that’s an issue. Can you explain what your application does and maybe send the relational schema? From what I am currently understanding right now it looks to me more like a bad database schema.
You need to check the statistics on the table, see what's used the most, monitor for the queries that are hitting it, (or just check the built in reports for starters. what's the use of the table? You could always replicate it to a different table for reporting. Personally, i'd look at what creates the row, and see if there is something to make it unique. ComputerID and country isn't specific enough, but i'm certain you could add a Office level detail as well. State_officeID_floor etc.
I don't think you can use column types as column names without some trickery.
A better option would be an indexed view. No triggers at all. Managed completely by the engine. Or, just a view and index the base table to support the query.
The professor of the online course gave me that name to use. Would it be easier to ask him if I can change the name, or try this trickery? Thanks.
I've set up Postgres a few times. I recently set it up again on my own Digital Ocean server. I'm willing to give some pointers as needed.
I have not used oracle in a few years but I'm guessing you can't use reservered names as column names. What if you put brackets around date? Will it work? I'm guessing not but worth a shot I suppose.
Brackets around colum name and not column type to be clear.
I'll try it, thanks!
Here are two ways to fix this...given I'm not entirely clear on what it is you're doing. Create a new table using composite keys. Ex, combine two or more columns to create a new PK. Create a new mapping table that prevents duplicate keys. 
How about SELECT 1 Location,* FROM DB1..Students UNION ALL SELECT 2,* FROM DB2..Students
A report database shouldn't rely on source keys, if you can help it. Sooner or later you'll need to do something which produces duplicates, but already you have this problem. Another example would be a table with multiple copies of the same data from different time periods, or multiplicative joins for "fact" tables. Create an identity / autonumber column (table_name_id) for the report table to relate to other data, then a "composite key" to maintain integrity. The composite key being a unique index on (source_db_name, source_table_id), as well as a not-null constraint on these columns. The trade off is that for inserting new data or creating relationships, you'd need to use the composite key to identify or fetch the correct corresponding table_name_id. Also, clear naming conventions is important such that analysts don't confuse table_name_id with source_table_id when writing their own queries. It'll probably go beyond the scope of your project in parts, but have a skim over "Building the data warehouse" by Bill Inmon. It covers a lot of this sort of thing.
For one thing I wouldn't even use the *term* primary key if it's not unique. If system 1 has a record with ID of 26, and system 2 has a record for the same person with ID of 26, then the *composite* unique key for the individual on the first system is 1,26 and the *composite* unique key on the second system is 2,26. An this is why internally we use *meaningless surrogate* keys, which we control, in addition to any/all externally imposed identifiers (which are not under our control).
Use CONCAT to create a new key that joins the ID to some other identifier, like location. Or on each select, add the name of the database to the concat. SELECT CONCAT('DATABASE1', PERSON\_ID) AS NEW\_ID FROM DATABASE1 UNION SELECT CONCAT('DATABASE2',PERSON\_ID) AS NEW\_ID FROM DATABASE2 You will have to join in whatever other fields you want using the regular ID to each additional table both times, so stay organized on each section before and after the UNION. This is assuming there isn't some shared table that relates to both databases, somehow? 
Sorry - let me provide more context... For example, say I have the following databases: database_A (8 campuses) - staffTable - staffID, name, email, favoriteFruit - studentTable - studentNumber, name, email database_B (2 campuses) - staffTable - staffID, name, email, favoriteFruit - studentTable - studentNumber, name, email These tables have an insane amount of information, and really for these analytics all I need is a few fields so I don't want to bring in all the data. So what I have done is created views called custom_students_view and custom_staff_view that does something like the following: CREATE VIEW custom_staff_view AS SELECT * FROM database_A.staffTable UNION SELECT * FROM database_B.staffTable I should have provided more context, in that I am bringing these views into Microsoft Power BI to visualize, and using the custom_students_view and custom_staff_view tables to relate across other visualizations that I have built. For this purpose I need the ID numbers to be unique, and this is where I am running into errors, because I have multiple users with the same ID number. So the union works and gets me columns with the duplicate IDs, but obviously I can't have that since they aren't unique. So to get around it I was thinking of doing something like the following, so that I don't have duplicate ID numbers: CREATE VIEW database_A.custom_staff_view_A AS SELECT staffID, name, email, favoriteFruit CREATE VIEW database_B.custom_staff_view_B AS SELECT staffID = (staffID + 100000), name, email, favoriteFruit or instead of adding 100,000 something like: SELECT staffID = (staffID + '-' + campus), name, email, favoriteFruit I guess my question is, **what is the best practice to make these unique?** Would you offset the number by a certain amount, e.g. staffID + 100000? And if it helps to further clarify, the reason I need this table in Power BI is because most of the other reports and views only have a staffID number, and I want to make it more human readable so I'm joining in the names and other user information. The problem here is that now if a built-in report from database_B has user 42, that's now user 100042, so I'd have to then make another copy of that report and then modify the userID there as well. I don't really see a good way around any of this, either way it's going to result in modifying all of the reports on database_B.
&gt; Create a new table using composite keys. Ex, combine two or more columns to create a new PK. That's what I'm leaning towards. I posted some more details in a comment. I'll probably just do something like staffID + 100000 or staffID + '-' + campus Thanks! 
I just provided some more context. The UNION works with that, but I end up with duplicate values in the ID column.
Put Date in double quotes so "Date" date. Date is a reserved word (and Oracle is not case sensitive) so you have to explicitly tell Oracle that it is a column name with the double quotes.
Lovely, thank you! I will try to adapt this today and use it.
Thank you. I will check if they have a PowerBI license and try to figure out how that works. In the meantime I'll try to adapt sudo's query.
This combined with /u/bigfig 's answer really helped to solidify things for me. Thanks a bunch! I just ordered a copy of that book for $4 on Amazon, definitely going to read it. Thank you for the recommendation. I'm pushing our organization to merge the databases together and get this sorted, for now I'll have to continue to work around it. 
Your unique key is going to be Location and ID (segmented key). The IDs are location specific and there's no way around that. Should work fine for views. When you join these multiple unions it will be ON s.ID=c.StudentID AND s.Location=c.Location
&gt; For one thing I wouldn't even use the term primary key if it's not unique. I guess I meant it's locally the primary key (source key). It's unique within that system, but now that system is part of another system, and no longer unique. What happened is that the organization purchased other campuses that were running their own system, and they have continued to run these systems side by side instead of merging them together. I was tasked with building dashboards that span both systems, hence the unions.
&gt; Your unique key is going to be Location and ID (segmented key). So best bet would be something like this? DET-123 (Detroit Campus staff ID 123) and COM-123 (Compton Campus staff ID 123) Would you normally delimit with a hyphen, or smash em together with no delimiter? I'm also trying to figure out what is best and common practice here. :) Thanks a bunch.
&gt; So just assign each source DB an identifier, and keep the id, descr pairings in a source DB lookup table. Ahh I just reread and caught this. That makes sense, thanks. 
You don't have to combine them into a single column, but that works. You're on the right track. Hyphen or no seems to be personal preference to me. 
For each table you are bringing in, add a source field ie a char(1) set to 'A' or 'B' depending which database it is from. This source field now becomes part of the Primary Key in your new table, along with the original ID. Whenever you are querying other tables, you now include the source field also. 
For what op is suggesting the free version of PowerBI would be fine.
Just post a link to the actual article, not a forum post that links to an article.
Using SQL in a PeopleSoft Query platform---I think I figured it out with a series of replace statements!
If you had a two column Primary Key of StaffID and Database would that suffice or not? If not, would a Staff or Student ID that's in the format ##-###### (two numbers dash and then the old ID) work?
The more I think about this I believe it's double quotes rather than brackets. So for example "Date" Date. But I think reserve word will still not work. Usually I try to call my column names a more descriptive name such as birth_dt for birth date.
CREATE VIEW custom_staff_view AS SELECT 1 SourceID, * FROM database_A.staffTable UNION SELECT 2 SourceID, * FROM database_B.staffTable Now you have a column by which to differentiate. 
Maybe something like.. IF(DATE_FORMAT(real_start, '%m/%d/%y%H:%i:%s'),'I','TRUE','FALSE')
Three fully functional, free offerings of the latest version of Db2 (currently v11.1.3.3) are available: Db2 Developer-C, Db2 Community Edition (which is Db2 Developer-C in a Docker container), and Db2 on Cloud free tier. You can download Db2 Developer-C and Db2 Community Edition from this web site: [https://www.ibm.com/us-en/marketplace/ibm-db2-direct-and-developer-editions](https://www.ibm.com/us-en/marketplace/ibm-db2-direct-and-developer-editions). And, you can take advantage of the free cloud tier offering by clicking the "Start now" button on this web site: [https://www.ibm.com/cloud/db2-on-cloud/details](https://www.ibm.com/cloud/db2-on-cloud/details)
You can use up to 4 cores with Db2 Developer-C, and you can use it in a production environment if desired. You just don't get service and support.
This is precisely it. "This sumbitch needs a data warehouse and ETL to cubes."
Out of curiosity, what would the "pre-existing analytics platform" be, exactly? I can easily see how a table like this (one that tracks visits to a website) would get out of hand fast for a large website, but I really don't have any idea what the alternative is. 
[Pick one](https://en.wikipedia.org/wiki/List_of_web_analytics_software)
yet another young mind unaware that ORMs are leaky abstraction, and can cause terrible query performance, vendor lock-in (is the query using ISO SQL?), etc. ORMs are useful... but pretending that the data layer doesn't exist is a recipe for disaster.
 Check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/), I have some great material there for learning SQL. It's very practical, and very hands-on. Problems range from beginner to advanced, and I just added a new set of 40 practice problems. Email me (my email is in the FAQ) for a student discount.
I know SQL can get complicated but I just don't see trying to 'shield programmers' from it as being a positive thing. The more applications trend to easy drag and drop functionality, the more I find myself with fewer options on actually satisfying the business goal. It's like Android and Apple. Apple wants things easy and shields users from the complexity of the situation. The users then have no recourse of basic information to analyze a situation and remedy it. Android will seem more complex but it is just hiding less (in some cases). Maybe empowering users isn't a feasible option but I wan to dream that it is. If a new product comes out without the option to just code a solution, I'm not interested. 
Static data masking is what we are looking for. I went through their documentation (what little of it there is) and they only give a 10,000 ft marketing overview of what the product can do. Have you used the app before? I'm curious to know if it always has to start from zero when making static masked databases or if it can just copying over the changes to an existing database?
Looks like you've figured out a lot on your own, congrats. If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out SQLPracticeProblems.com. I developed it after teaching a SQL course where the material I was required to teach from was very dry and academic. The students didn't get real-world practice, and thus didn't get a good foundation in the SQL that's used most commonly. Afterwards, they emailed me, saying they needed practice problems, so I developed the course!
Brave to post this on r/sql lol. The SQL is the FUN part, why would I want to abstract it away?
I will check this out!
I learned with a job doing technical support for a database product. A constant barrage of customer questions (along with great second level support) developed my SQL ability very quickly! If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course, where the material I was required to teach from was very dry and academic. The students didn't get real-world practice, and thus didn't get a good foundation in the SQL that's used most commonly. Afterwards, the students emailed me, saying they needed practice problems, so I developed the course! 
Or just somebody who copied her colleague's (actually intern's) article, posted it under her own account, and posted it on reddit for easy clicks.
Terrible article. Why would you take the advice of an intern who lacks practical experience?
[They should watch my talk](https://www.youtube.com/watch?v=wTPGW1PNy_Y)
Can we just ban blog only accounts and their urls? Rarely is their content meaningful outside of trying to promote their brand or farm ad revenue off their personal site. Hell even reddit covers this in their guidelines for spam: &gt;If your contributions to Reddit consist primarily of links to a business that you run, own, or otherwise benefit from, tread carefully, or consider advertising opportunities using our [self-serve platform](https://about.reddit.com/advertise/). I'm all about people promoting blogs and whatnot as long as you have some other interaction with the community... this clearly isn't the case.
You know that ORMs stem from the fact that people are unaware of proper database modeling, or are just never taught that anymore. If you complain that your queries are overly complex perhaps it was because your designed your database to be that way. Don't buy into the ORM band-aid, fix the model!
Quick glance - wouldn't your delete statement need to use the CTE, instead of table_main_hold?
&gt; Further, the queries not only become longer in size and complicated but also difficult to comprehend. Well, formatting them helps...
There are a lot of data modeling tools available. I've used a few, and there are a lot of recommendations online for various tools. However I couldn't find a page that lists all of them and was up to date. So I created one. If you're looking for a data modeling tool to use, this list might be helpful.
I wasn't sure if CTE was structural or not. I thought it just named the table
I've figured out pretty much everything I need thanks to you pointing me in the right direction. I have another question though, is there a more elegant way to write the following: WHERE (((C.CLM_DATE) Between [POL_INC] And [POL_EXP])) AND A.VESSEL_CODE ="GLA 500" AND POL_INC BETWEEN #2/19/2007# AND #2/19/2017# OR A.VESSEL_CODE ="GLA 200" AND POL_INC BETWEEN #2/19/2007# AND #2/19/2017# OR A.VESSEL_CODE ="PAC 500" AND POL_INC BETWEEN #2/19/2007# AND #2/19/2017# OR A.VESSEL_CODE ="STR 450" AND POL_INC BETWEEN #2/19/2007# AND #2/19/2017# It just seems like there should be a way to not have to type out the between and the OR functions for every individual vessel. Happy 4th!
RowNumber doesn't exist in table_main_hold from what I can tell so you aren't going to be able to use that to delete anything. You need to join your duplicate query back to the table that you are trying to delete records from. 
I was thinking about that... These duplicate records are exact copies (nothing different). Without the rownumber column created it seems unlikely to connect. Wait!!! Could I just conctonate (spelling is no fun lol) id||date||count and then say delete from hold where hold conctonate is in (query above)?
Why not extend the wikipedia page on the subject?
because no upvotes over there
Yea I need to step my game up here. I'm going to read that Inmon book and do some courses on data warehouses. Thanks!
Ahh I see, so just have a field for source. Thanks!
Yea, after reading through the replies here I have a clearer sense of how I need to set things up. I think the two column approach is cleaner. Thanks!
Ahhh I see! I just tried this out, that makes total sense. Thanks!
Thank you, Sylvia, I have in fact bought your book, and added it to the recommended list of books on my class syllabus. I will email you shortly.
FYI. They want you to hive them your email address so that they can send you a spreadsheet of the details. When I attempted to scroll down to see the details, they did a modal pop up that sent me all the way back to the top. 
Because there's a limit to what you can do on Wikipedia, and I thought I could add more value to the SQL/database developer world if I made my own page.
That's right. I know a lot of people when looking for a tool to use have different criteria (e.g. it must be free and work on Mac, for example). The spreadsheet is for people who want to filter the long list of tools to meet that criteria. It's not meant to scroll back to the top. Thanks for letting me know, I'll fix that.
I'd do this Select distinct * into #temp from table Delete from table Insert into table Select * from #temp
To help out, I accessed it on a iPhone X with latest patches using Safari.
Learning analysis services and multidimensional data and how to commute your transactional database to a cube farm people could report off of maybe doubled my pay. I assure you, data warehousing ten fold worth your time investment. Even if payment comes in the form of less work for you. All the best.
I really appreciate that! I kind of fell into this project because I'm trying to make reporting easier for everyone across the company. I just automated 3 reports this week for our department, which used to be run manually every day, emailed to a thread of people, blah blah. Now it's just a dashboard that refreshes like 10 times a day. :) And I'm really excited to dive into data warehouses, it's something I haven't had much of a reason to get into until now, but I've always been very curious. Thanks for the help.
Worked, thanks!
Worked, thanks! It is "Date"
Assuming a table like this `Create Table Claim (PersonID int, ClaimDate date, ClaimAmount Money)` ... a query like this would do it: `Select` `PersonID` `From Claim` `Where` `PersonID in (` `Select PersonID From Claim` `Group By PersonID ,Year(ClaimDate)` `Having count(*) &gt;= 5)` `Or` `ClaimAmount &gt;=20000` `Group by` `PersonID` If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). 
I use a CTE WITH cteName AS ( SELECT *, ROW_NUMBER() OVER( Partition By(Columns to check for uniqueness)) AS Alias ) DELETE FROM cteName WHERE Alias &gt; 1 Here's a video from YouTube: https://www.youtube.com/watch?v=4npMmdTE2Qw 
Much appreciated, I will have a look at the site. the syntax works like a charm!
If you reuse the prepared statement object, it saves having to reparse the query every time. Handy for frequently called things.
Thanks! I've removed the modal popup so it shouldn't interfere with the browsing experience on desktop or on mobile!
Based on this I can suggest two nonclustered indexes: ComputerId, Date and another one just on date. A clustered index might be something to consider as well.
The SQL injection methods I'm familiar all use a quote character ('), comment (--), or multi line comment (\* and *\). So having one of those four strings would be a necessary condition. Beyond that it could have any conceivable SQL, including RDMS-specific commands. The best way to do this would depend on the nature of the data, how tolerant you are of false positives, how tolerate you are of false negatives, and so on.
TLDR: &gt; Our solution? To implement the block hash join algorithm. ... &gt; Please note: join algorithms and their performance characteristics are common knowledge for RDBMS developers. 
Great! Glad that worked. I would suggest never using reserved names as column names. Also whenever you call that column you have to put the column name in double quotes which sucks I think.
Every SQL statement is prepared by the RDBMS. One of the powers of SQL is that you do not have to specify where the data is or how to get it, only what data you want, logically. This means the RDBMS needs to figure the rest out. Preparing or compiling statements is the process of figuring out where the data is and how to get it and bring it together. If you are using host variables or parameter markers, some RDBMSes (like Db2) can also cache the prepared access plan for future efficencies. This caching is excellent for performance in databases where SQL is repeated.
Then what is the need for prepared statements in PHP scripts for querying SQL database?
I am not familiar with how PHP handles it. Many languages offer the option to either do a separate prepare and execute, or do them both as one step. The separate prepare and execute is easier for the use of host variables or parameter markers, which should be used for repetitive SQL in databases where the data distribution is not crazily skewed.
Prepared statements prevent SQL injection. There's *loads* of resources out there which should help you with that. Here are two: https://www.w3schools.com/sql/sql_injection.asp https://stackoverflow.com/questions/1582161/how-does-a-preparedstatement-avoid-or-prevent-sql-injection 
I would swap the asterisk for 1. As you are not counting snything specific.
If you don't know much about SQL in general then do that first. There are "learn SQL in 24 hours" books. Do that. Do it twice and three times. Bore me to death with joins and indexing. If you have 2-3 years then this should be your first week and forget certs and DBA stuff.
to be honest, I don't actually have any certifications (even tho I have like 8 years of experience IT, don't ask lol). I was hoping to get some info on database certifications so I can focus on other, more general certs that would help build up my overall knowledge going into database stuff. thanks for the book info, I'll pick one of those up this week.
Most of "us" started that way. It's not hard at all but my god is learning this stuff is boring. Can you tell me about left joins? If not, pick up that "Learn to SQL in 24 hours" book. If so, can you tell me about indexing? If you want to get into certs, there are SQL developers and SQL admins (DBAs). Nothing is difficult but is sure overwhelming at first and forever. Here are you certs: https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-administration-certification.aspx https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-development-certification.aspx
If you can use analytic functions like LEAD then maybe you don't need self joins. I'm assuming each call is a single row. First you number the calls sequentially for each operator, using ROW_NUMBER, partitioned by the operator id and ordered by the call start date time. Then you use LEAD to add a column with the start of the next call (again, partitioned by operator, same ordering). Can it next_start. With that you can, for each call, calculate whether next_start is less than 10 minutes of the call's own end. Based on this, add an offset column: 1 if the next call's within 10 minutes, 2 otherwise. Now, add a running sum of the offset column - within the same partition etc. If you then subtract the ROW_NUMBER from the running sum, you will get an id to group consecutive calls, per operator. Once you have that it's easy to get the length of the sequences.
 SELECT taken_by, first_call, last_call, calls FROM ( SELECT MIN(prev_call) first_call, MIN(call_ref) call_ref, MIN(taken_by) taken_by , MAX(call_ref) last_call_ref , MAX(call_date) last_call , MIN(call_ref)-1 As start_call_ref, (MAX(call_ref) - (MIN(call_ref)-1) +1) As calls #we subtract 1 because the first call_ref is the one before #we add 1 because the endpoint needs to be counted too FROM ( SELECT base2.*, call_ref - ROW_NUMBER() OVER (PARTITION BY Flag ORDER BY Call_ref) As grp FROM (SELECT base.*, CASE WHEN Diff &lt; 10 THEN 1 ELSE 0 END AS Flag FROM (SELECT i.call_date as prev_call, i2.call_date, i2.call_ref, i2.taken_by ,MINUTE(TIMEDIFF(i2.call_date, i.call_date)) as Diff FROM Issue i JOIN Issue i2 ON i.call_ref = i2.call_ref-1 #consecutive calls AND DATE_FORMAT(i.call_date, '%Y-%m-%d') = DATE_FORMAT( i2.call_date, '%Y-%m-%d') # same day AND i.taken_by = i2.taken_by # same operator ) base #gets calldate, the directly previous call_date, call_ref and what the time diff was ) base2 WHERE Flag = 1 #creates a similar grp number for all consecutive calls. non-consecutive calls are filtered out ) base3 GROUP BY grp ) base4 WHERE calls = 24
Glad you found a solution. Have you thought of using WITH? It lets you add dependant fields step by step without nested subqueries. Much easier to read and debug.
If you're looking for some very hands-on "learn-by-doing" practice problems, that teach basic to advanced SQL, check out [SQLPracticeProblems.com](http://sqlpracticeproblems.com/). I developed it after teaching a SQL course where the material I was required to teach from was very dry and academic. The students didn't get real-world practice, and thus didn't get a good foundation in the SQL that's used most commonly. Afterwards, they emailed me, saying they needed practice problems, so I developed the course! 
Totally gonna look this up :)~
Yes you are right there is separate sequences for the flags 1 &amp; 0, which can group together and cause errors. So I put the filter in WHERE Flag = 1 to try and work with just one sequence.
Cool! Most companies will reimburse this kind of educational expense.
Was it micros xstore with micros relate? I may be able to offer a few pointers from my lessons with those products. 
I don't understand CTE... What are the advantages if using CTE?
The only SQL certifications that are worth anything are the ones from the vendors themselves. As of 3 years ago, SAP didn't have a certification program for SQL Anywhere. But getting a certification only demonstrates that you can pass the test; it doesn't say anything about your competency with the product. Focus instead on practical knowledge and understanding, not passing a test.
-Can be used to create a recursive query. -Can be substituted for a view -Allow grouping by a column which might be derived from a scalar subset -Can reference itself multiple times
Happy 4th! How are you? What does all this mean? Thanks KS JAMEs
Basically without the CTE you can't use a where clause to filter out the second/third/fourth/etc instances of a row. With the CTE, you are pretty much running a virtual temp table that will delete your extra records without having to create table in memory.
A CTE is a common table expression, its kind of like a view or temp table. But it only runs in memory. So there is no overhead once it's complete.
Try changing the delete from table_main_hold where rownumber &lt;&gt;1 to delete from cte_duplicate where rownumber &lt;&gt;1 
There's probably a more elegant solution but using recursive tables can help. The code below should work. WITH RECURSIVE c (t,sd,cd,pc,cc,rn) AS ( SELECT taken_by , call_date , call_date , call_date , 1 , rn FROM ( SELECT taken_by , call_date , MIN(call_date) OVER ( PARTITION BY taken_by ORDER BY call_date DESC ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING) prev_call , CASE WHEN TIMEDIFF(call_date, MIN(call_date) OVER ( PARTITION BY taken_by ORDER BY call_date DESC ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING)) &lt;= '00:10:00' THEN 0 ELSE 1 END consec_call_ind , ROW_NUMBER() OVER ( PARTITION BY taken_by ORDER BY call_date DESC) rn FROM Issue ) x WHERE consec_call_ind = 1 UNION SELECT c.t , c.sd , x.call_date , x.prev_call , c.cc + 1 , x.rn FROM ( SELECT taken_by , call_date , MIN(call_date) OVER (PARTITION BY taken_by ORDER BY call_date DESC ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING) prev_call , CASE WHEN TIMEDIFF(call_date, MIN(call_date) OVER ( PARTITION BY taken_by ORDER BY call_date DESC ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING)) &lt;= '00:10:00' THEN 0 ELSE 1 END consec_call_ind , ROW_NUMBER() OVER ( PARTITION BY taken_by ORDER BY call_date DESC) rn FROM Issue ) x INNER JOIN c ON c.t = x.taken_by AND c.cd = x.prev_call AND x.consec_call_ind &lt;&gt; 1 AND x.rn + 1 = c.rn ) SELECT t taken_by , sd first_call , cd last_call , cc calls FROM c where cc = (select max(cc) FROM c);
It's not necessary to know the first call in a consecutive series in order to group them. If the calls are ordered, all you need to know is the number of previous calls that weren't continuous. This can be done with a running sum, there's no need for any joins or recursion. 
There's a ton of documentation on the sqlite website.
You can use a `LEFT JOIN` along these lines: SELECT [columns] FROM questions q LEFT JOIN user_answers a ON a.question_id = q.id WHERE a.id IS NULL
&gt; In one table, questions_list, I have my list of questions, with primary key being question_id &gt; In the other table, user_answers, I have all user answers, with primary key again being question_id Why would question_id be the PK of user_answers?
https://blog.sqlauthority.com/2015/11/06/sql-server-how-to-change-sql-server-product-key-or-license-key/
&gt; do I have to do one massive statement with a load of left joins? That is generally the correct way to extract the data. Other platforms that try to detect the relationships may rely on Foreign Keys constraints, which may not exist in your database.
Use it lots. Does the job. We have several legacy systems that are a nightmare to connect to, we have regular data dumps (email to box, but other options are there) that allow us to mix the data sources easily. Can't say it's the best, but it does the job.
Yeah it does. I was just hoping I could get documentation in the form of eBook etc. And yes. You should do that and post the link!
Whoops sorry, meant it's a composite key between question_id and username. That way any user can do any question, but it doesn't allow any duplicates. Do you have any better suggestions?
That sounds better. I was just really confused!
So this kind of works for me, but now I have duplicate columns, and I have quite a large number. I want to avoid having to manually specify a.explanation etc in the SELECT bit. Is there any way to do this? I've tried adapting the syntax for natural join, but the &gt;&gt;&gt;&gt;WHERE a.question_id IS NULL; Doesn't work with that
Just trying to understand...in Add_flag if the time is over 10 minutes, as in a non consecutive call, it gets a value of 1. Then in Add_grp you sum(flag). Wont this just give the total sum of non consecutive calls for one operator, over all his days working? I'm not sure I follow. Also I like WITH, its a lot cleaner, I will use that going forward instead of nested subqueries.
What flavor of SQL? In MySQL you can use `SUBSTRING_INDEX()`: SUBSTRING_INDEX(SUBSTRING_INDEX(`Column1`, '|', 1), '^', -1)
Is this one time? Generate the column list out of the information schema, then copy/paste to your query. If you are sure columns that are named the same are not null and contain the same data, you can also use the natural join.
Thank you for the suggestion. I use Microsoft SSMS. I found plenty on SUBSTRING, but not on the SUBSTRING\_INDEX. 
I've never heard of a natural join before, but looking it up it's exactly what I'd need. Thanks!
Lookup charindex. Use that with substring. SELECT SUBSTRING ( Field1, CHARINDEX(Field1, '&amp;'), Charindex (Field1, '!')) On mobile and I can't find the pipe or carrot symbols. Not do I guarantee I have the fields in the right order for those functions.
Here is my code with the WITH you suggested. Much more readable. /*Second Try*/ WITH AddDiff AS( SELECT i.call_date as prev_call, i2.call_date, i2.call_ref, i2.taken_by ,MINUTE(TIMEDIFF(i2.call_date, i.call_date)) as diff FROM Issue i JOIN Issue i2 ON i.call_ref = i2.call_ref-1 #consecutive calls AND DATE_FORMAT(i.call_date, '%Y-%m-%d') = DATE_FORMAT( i2.call_date, '%Y-%m-%d') # same day AND i.taken_by = i2.taken_by # same operator ) , AddFlag AS( SELECT AddDiff.*, CASE WHEN diff &lt; 10 THEN 1 ELSE 0 END AS flag FROM AddDiff ) , AddFilter AS( SELECT AddFlag.*, call_ref - ROW_NUMBER() OVER (PARTITION BY Flag ORDER BY Call_ref) As grp FROM AddFlag WHERE flag = 1 ) ,AddGroup AS( SELECT MIN(prev_call) first_call, MIN(call_ref) call_ref, MIN(taken_by) taken_by ,MAX(call_ref) last_call_ref , MAX(call_date) last_call ,MIN(call_ref)-1 As start_call_ref, (MAX(call_ref) - (MIN(call_ref)-1) +1) As calls #we subtract 1 because the first call_ref is the one before #we add 1 because the endpoint needs to be counted too FROM AddFilter GROUP BY grp ) SELECT taken_by, first_call, last_call, calls FROM AddGroup WHERE calls = (SELECT MAX(calls) FROM AddGroup) 
The problem is in the where clause. Where company_name in ('company1','company2')
ah fancy, ill give that a try. 
Thanks for writing this code. Love seeing other people's takes. Your code is cleaner than mine because of WITH. I have rewritten using WITH under toms-w's post. I will use that moving forward. I don't know how I feel about the recursive CTE. It's interesting if you have a use case for it, but I don't see an obvious advantage to doing it here. Also interesting is that we both have intermediate steps that create 'wrong' data, in order to arrive at the right answer. My way has two different row_numbers going for flags 1 &amp; 0, then flag = 0 is filtered out. Your way also creates wrong data, because recursively joining means you will get every step in a consecutive series. I.e for consecutive calls 1-4, you will get a record for 1-2, 1-3, 1-4. But you select for MAX in the end so it works. In your opinion, whats the advantage of recursive approach? It's longer to write, read, and to grok since you have to remember what the recursive member is and how its joining back to the base case. Jc, thanks
Oh man thank you so much! Is there an easy explanation why my statement doesn't work in a continuous chain? Such as, WHERE x = 'a' or 'b' or 'c' or 'd';
There isn't much explanation, it just isn't how the where statement works, the in statement does that job. :)
Oooh ok, I sorta get that. So if I wanted to attempt the above statement it would be IN as opposed to WHERE? IN x = 'a' or 'b' or 'c' or 'd'; ?
Patindex is also a fun function to play with. You may have to add or subtract 1 from the results of charindex. Charindex returns an int with the numeric position of the first time it sees that value in the text.
If whatever you're using has regex support, this should be simple. Try ''(?:\^).*(?!=|)' or something like that.
It's not the total sum, because of the ORDER BY it's a running sum. This value will increase every time there's a discontinuous call, so it is constant and unique for groups of continuous calls. Also, you don't need a join and you can't assume that call_ref will be continuous. 
For TSQL you have to get a little more complicated: SELECT CASE WHEN CHARINDEX('|', Column1) &gt; 0 THEN SUBSTRING(Column1, CHARINDEX('^', Column1)+1, (CHARINDEX('|',Column1) - CHARINDEX('^', Column1) - 1)) ELSE SUBSTRING(Column1, CHARINDEX('^', Column1)+1, (LEN(Column1) - CHARINDEX('^', Column1))) END FROM Table1 Basically select the substring of the first `^` position and add 1 (to exclude the `^`), until the first `|` (subtract 1 to exclude `|`) and subtract the position of the first `^` to trim off the characters before the first `^`. The CASE statement handles your second example, where there is no `|` to look against. In this case just grab the text after the first `^` and subtract out the text before it (length of string - position of first `^`).
OR is a secondary clause. I.E. a second WHERE. So each OR becomes like a new clause. For example, your where clause is actually stating WHERE x = a -- (column x has a data row containing a) OR WHERE b -- (column b) OR WHERE c -- (column c) OR WHERE d -- (column d) So the 2nd to Nth WHERE isn't actually stating anything. The engine doesn't know what to do with these WHERE's, hence an error. Does that make sense?
That totally makes sense, thank you for the info!!
Try it. Create a temp table of the select from the employee table with the where clause, and then join that to the collector table. My guess is that the where clauses will be the problem. I can't make sense of what they're trying to accomplish. Could just be me though.
This is what im using. Its not 100&amp;#37; clean but for the data I have it will work 100&amp;#37; of the time. The '|' section isnt working, but throwing the -4 accounts for the 2nd carrot and fixes it. I just need to clean it up a bit. Thank you again. ,SUBSTRING(CI.Chunk , CHARINDEX('\^', CI.Chunk)+1,(CASE WHEN CHARINDEX('|', CI.Chunk) = 0 THEN LEN(CI.Chunk) ELSE CHARINDEX('|', CI.Chunk)-4 END)) As CONTRACT\_NUMBER
&gt;Wouldn't it be better (faster, more efficient, etc) to SELECT the search data from each table and join the results? As with most database-related things, it depends. What are the indexes on the tables? You can use one table as a filter for the other via a join (maybe less successfully on an `outer join`, but still viable). &gt; I don't know quite how to form that as a single query, but I really think it would be less expensive. It wouldn't be a single query, it'd be three - one for each table to pull into a temp table, then a third to perform that `JOIN`. That might be better, it might be worse. It may be better for *some* input parameters but not others. Welcome to the wonderful world of query tuning! BUT a single `select` query shouldn't be locking everyone out in the first place if you have things configured properly. What's the transaction isolation level for the database? Does the system have enough resources? That `WHERE` clause is a big of a mess IMHO, especially the first 2 lines of it (the second obviates the first) and the whole idea of using greater than/less than comparisons for strings (which may produce unexpected results once you're into non-Latin collations and/or unicode strings).
That first line of the `WHERE` clause excludes any `SEARCHNAME` that does not start with `A-Z` and comes after `Zzzzzzzzzzzzz` alphabetically... so yeah, it makes no sense, as `Zzzzzzzzzz*(!&amp;@398213709!#*&amp;U(_~HJOIJMKSL` is valid.
I'm thinking it was intended to be a 1=1 type clause so that additional clauses could be added with an "and" without having to worry about whether they were the first where clause. Very messy.
/u/toms-w has a nicer solution. I couldn't think of an easy way to make a grouping I'd. I would typically wouldn't use recursion for something like this since I would use [RESET WHEN](https://www.info.teradata.com/HTMLPubs/DB_TTU_16_00/index.html) in my OLAP functions. I'm not sure what databases all support it though.
Normally I try to create very detailed replies, but I don't have time at the moment to create a very detailed run through for you immediately. I recommend to look at two websites: [https://sqlworkbooks.com/dear-sql-dba/training-resources-for-sql-noobs/](https://sqlworkbooks.com/dear-sql-dba/training-resources-for-sql-noobs/) [https://www.brentozar.com/archive/category/professional-development/](https://www.brentozar.com/archive/category/professional-development/) Kendra has a podcast with a few episodes related to what you are asking about and Ozar has a ton of blog posts directly related to career growth. As my last recommendation, I would suggest to buy and run through this book: [https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X) T-SQL Fundamentals by Itzik is a great book, I would highly recommend for you to run through it completely while reading those blogs or listening to podcasts regarding career direction. &gt; so I'm a little scared to be looking into a career I feel so unqualified for. Like I said, I normally try to provide more in depth responses but I am unable to at the moment. There are a few key things to remember. 1. The SQL community is amazing. You will find so many selfless individuals eager to help and teach you. 2. If you have a passion to learn and can apply yourself, you will have nothing to fear. It may take a little time, it may be difficult, but I can guarantee that you can do it. 3. Most IT people feel as though they are unqualified. It's called the "imposter syndrome" and it is very real. Some days I feel like a super expert, then other days an incredibly humbling moment may occur bringing me down and forcing me to question whether or not I'm worthy of a senior title. In the end, don't worry about the things you can't control. Stay proactive and focus in what you can control and the rest will fall into place for you.
Thank you!
Ah perfect! Thanks!!! 
WHERE company_name IN ('Company 1', 'company 2', 'company 3', 'etc.')
Do a full outer join on all of the columns they have in common.
`like [A-Z]%' would accomplish the same and be a lot easier on the eyes.
Awesome thank you so much!
if the columns in table names are not unique, then you would need to specify field names. here's a way to return the unique list of columns if you're sure the identically named fields in the two different tables return the same data. its never a bad idea to name the fields as an added column could break the dataset with select \*. with a as ( select distinct table\_name, column\_name from information\_schema.columns where table\_name = \[Table A\] ), b as ( select distinct table\_name, column\_name from information\_schema.columns where table\_name = \[Table B\] ) select \*, case when a.column\_name is null then 'b.'+b.column\_name+',' else 'a.'+a.COLUMN\_NAME+',' end select\_list from a full outer join b on a.column\_name = b.column\_name
This is exactly what I found after I posted this and you reassured me, thank you!
First thing I would do is get rid of ( A.SEARCHNAME &gt;= 'A' AND A.SEARCHNAME &lt;= 'Zzzzzzzzzzzzz' ). Second make sure you have indexes on the Employee.SEARCHNAME and Employee.EMPLOYEE fields. Third make sure the Employee.COLLECTOR and Collector.COLLECTOR have indexes. 
I would recommend looking at what it would take to get a computer science degree. I'm sure you have the math, science and general ed. completed. So all you would need is a bunch of programming classes.
And very inefficient. The way the query is written, the db has to check every single record every single time. Ouch.
cron a 'SHOW PROCESSLIST' once a minute, and log it to disk (use a filename format of HH_ii.log and it will limit the amount of files) Next time you get a block, inspect the recent dump files, and no doubt it will show you a long running query - this may not always be the query you suspect
Yeah I've seen queries written like that by people that think regular expressions are too cpu expensive to use in databases. I say that odds are that at any given moment there is more processor idle than IO except on the most lightly used servers.
That's the standard way to write a query and I don't recall ever seeing optimization tips that suggest what you are thinking. Which leads me to believe that it doesn't matter. The SQL Server optimizer is pretty darn good. First, get rid of: ( A.SEARCHNAME &gt;= 'A' AND A.SEARCHNAME &lt;= 'Zzzzzzzzzzzzz' ) it serves no purpose. I'd also rethink was was going on with @P1 and @P2. There's no reason to make both of those comparisons. Then, add OPTION (MAXDOP 1) to the end of the query. (this may actually slow the query down, there are a lot of variables to consider, you'll need to test. If it does, increase the 1 to a 2, 3, or 4). If this query is causing the server to hang up, limiting the number of cores it's running on should help.
Thanks, I've seen this although it didn't say if reboot is required. 
Tacking on here for the community aspect: Go over to https://pass.org and join PASS. Sign up for a couple virtual groups and find your local one (if there is one). Look for a SQL Saturday near you (https://sqlsaturday.com/ - if I'm guessing correctly, Albany might be convenient for you and it's in a couple weeks). Doesn't matter that you're new, you're welcome to join, talk to people and learn. Feel free to hop on the SQL Community Slack (https://dbatools.io/slack/) just to see what people are talking about too. Many of the people I know in the community don't have CS degrees. It's not a requirement. A lot of us stumbled into being DBAs &amp; database developers by accident and got hooked.
Healthcare is a great field that's going to continue to expand. Epic reporting is an expanding part of the industry which requires SQL. Not sure where you live, but you can look for Epic reporting analysis positions at larger hospital systems. Feel free to PM me if you have any questions. 
Thanks for the fun problem! The problem is surprisingly complex given how simple it is to state. Here is my solution. It works on an open interval, so calls that are exactly ten minutes apart won't count. If you want closed intervals it's easy to rewrite the ABS..DATEDIFF into DATEADD statements. I did not optimize for performance, so this will be slow on larger tables. First CTE labels each interval as Consecutive or not with an EXISTS subquery. Second CTE self-joins the first CTE discards all and non-consecutive intervals with a NOT EXISTS subquery. The ROW_NUMBER() from the first CTE lets us count the number of calls between any two calls taken by the same operator. If two or more operators tie for their longest consecutive streak, one is chosen abritrarily. WITH C AS ( SELECT *, CASE WHEN EXISTS ( SELECT * FROM Issue I2 WHERE I1.Taken_by = I2.Taken_by AND ABS(DATEDIFF(MINUTE,I1.Call_date,I2.Call_date)) &lt; 10 AND I1.Call_ref &lt;&gt; I2.Call_ref /* Exclude itself, otherwise it will always return true */ ) THEN 1 ELSE 0 END AS IsConsecutive, ROW_NUMBER() OVER(PARTITION BY Taken_by ORDER BY Call_date) Row FROM Issue I1 ), Intervals AS ( SELECT C1.Taken_by, C1.Call_date AS first_call, C2.Call_date AS last_call, C2.Row - C1.Row + 1 AS calls, ROW_NUMBER() OVER(PARTITION BY C1.Taken_by ORDER BY C2.Row - C1.Row DESC) Row FROM C AS C1 INNER JOIN C AS C2 ON C1.Taken_by = C2.Taken_by WHERE C1.IsConsecutive = 1 AND C2.IsConsecutive = 1 AND NOT EXISTS ( SELECT * FROM C AS C3 WHERE C3.Taken_by = C1.Taken_by AND C3.IsConsecutive = 0 AND C3.Call_date &lt; C2.Call_date AND C3.Call_date &gt; C1.Call_date ) AND C1.Call_date &lt; C2.Call_date ) SELECT TOP 1 Taken_by, first_call, last_call, calls FROM Intervals WHERE Row = 1 ORDER BY calls DESC;
Thanks for the fun problem! The problem is surprisingly complex given how simple it is to state. This is actually my second attempt. The first one was wrong. The first CTE determines the start and end of a consecutive call streak. The wording of the problem suggests that if two calls are exactly ten minutes apart they should still be counted as consecutive, hence the &gt;= and &lt;= signs. The ROW_NUMBER() here will help with counting the number of calls between any two calls by the same operator. Second CTE self-joins the first CTE discards all and non-consecutive intervals with a NOT EXISTS subquery. The ROW_NUMBER() here lets us pick the operator with the highest number of consecutive calls. If two or more operators tie for their longest consecutive streak, one is chosen abritrarily. If we want to display both we'd need another CTE. WITH C AS ( SELECT *, CASE WHEN DATEADD(MINUTE,10,LAG(Call_date,1) OVER(PARTITION BY Taken_by ORDER BY Call_date)) &gt;= Call_date THEN 0 ELSE 1 END AS StreakStart, CASE WHEN DATEADD(MINUTE,-10,LEAD(Call_date,1) OVER(PARTITION BY Taken_by ORDER BY Call_date)) &lt;= Call_date THEN 0 ELSE 1 END AS StreakEnd, ROW_NUMBER() OVER(PARTITION BY Taken_by ORDER BY Call_date) AS Row FROM Issue I ), Interval AS ( SELECT C1.Taken_by, C1.Call_date first_call, C2.Call_date last_call, C2.Row - C1.Row + 1 AS calls, ROW_NUMBER() OVER(PARTITION BY C1.Taken_by ORDER BY C2.Row - C1.Row DESC) AS Row FROM C AS C1 INNER JOIN C AS C2 ON C1.Taken_by = C2.Taken_by WHERE C1.StreakStart = 1 AND C2.StreakEnd = 1 AND C2.Call_date &gt;= C1.Call_date AND NOT EXISTS ( SELECT * FROM C AS C3 WHERE C3.Taken_by = C1.Taken_by AND C3.Call_date &gt; C1.Call_date AND C3.Call_date &lt; C2.Call_date AND (C3.StreakStart = 1 OR C3.StreakEnd = 1) ) ) SELECT TOP 1 Taken_by, first_call, last_call, calls FROM Interval WHERE Row = 1 ORDER BY calls DESC; 
Inverting the flag and using SUM is really clever. I'll remember this trick when I need to find consecutive groups. I wish someone would compile tricks like this into one big library where we can look them up...
Thanks! But there is a simpler / more efficient way if your consecutive groups are grouped on the value of a field, and you don't have to write an expression to compare adjacent rows, as is necessary here. -- Found this method in https://stackoverflow.com/questions/1269158/oracle-sql-for-continuous-grouping with Grp_Id as ( -- Get an id to distinguish continuous groups of G /* | G | | grp | key for the continous group --------------------|----------------------------- | . | 1 | 0 | (. 0) | . | 2 | 0 | (. 0) | O | 1 | 2 | (O 2) | O | 2 | 2 | (O 2) | . | 3 | 2 | (. 2) | . | 4 | 2 | (. 2) | O | 3 | 4 | (O 4) | O | 4 | 4 | (O 4) */ select t.* , row_number() over (partition by X order by O) - row_number() over (partition by X , G order by O) as grp from T ) ----- select X , G , min(some_f) as start_f , max(some_f) as end_f from Grp_Id group by X , G , grp 
You sure none of the columns are part of the primary key, such as the [Part No] column? Try running sp_help on the table and see what it says...
You are using functions, not stored procedures, to "cleanse" the data. You are also ALWAYS updating ALL data to be "re set", whether it needs to be or not. As a real quick dirty fix - I would build up a temporary table which only inserts records to be updated where the "clean" value did not match the "un clean" value, and then update the physical table by joining on to that. Hope that helps.
Just to add that the functions are likely to be using loops, which are massively inefficient in MS SQL. As above, for a quick fix, create a temp tables for each formatting type and insert the rows that match. A longer term solution would be to write CLR functions and utilise them in the SSIS job.
Positive, the primary key is called [Ticket ID] and the indexed column outside that is [Tool ID]. I ran SP_help as suggested and nothing came back that matches the columns in the query.
Some additional information: This query runs as part of an SSIS package. The functions (well point out there /u/somethingintelligent ) are very simple, this is the Cleanse Comments one: SELECT @Result = REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(CAST(@Comment AS VARCHAR(MAX)), ',',''),CHAR(13),''),CHAR(10),''),CHAR(9),''),'"','') RETURN @Result It seems to be more short hand than anything. I will look into the CLR function stuff (don't know if I have written them before). Also: &gt; As above, for a quick fix, create a temp tables for each formatting type and insert the rows that match. I will try this on Monday but that sounds like a whole lot more work for the engine than just "for all rows do these 4 things". Why would that be faster?
Not wanting to be a dick, *just* knowing SQL isn't likely to help all that much as a data analyst - most analyst roles require an understanding of stats, an ability to explain data analysis (visually and/or otherwise) to stakeholders, and quite possibly a popular data analysis language such as R or Python. That said, it's likely you're able to do most or all of that given your current job. With regard to DB architecture and performance, the material for Microsoft exam 70-462 (or whatever has superceded it) should provide a good grounding. That's getting into more of a Database Administrator (DBA) skillset. Ultimately, just start applying for stuff if feasible. It sounds like you'd need to learn more for DBA roles. I'm guessing you already have the appreciation of stats and stakeholder communication required for analyst roles.
&gt; I will try this on Monday but that sounds like a whole lot more work for the engine than just "for all rows do these 4 things". Why would that be faster? * Don't think you're smarter than the optimizer. You probably aren't. * What may *look like* more work can actually turn out to be much less because SQL Server now has additional metadata it can use to make a better decision about the query plan. * You're telling SQL Server to update **every** row. Chances are, 95% of those rows got updated the last time this package ran. So why are you doing the same work over and over and over again? * If you track the last modified date of each record (put a `DateModified` column on the table and a trigger that updates that field `AFTER UPDATE`), you can add a `WHERE` clause to your query so that you only update records that have been touched since the last time you ran the package. * Ideally, you wouldn't even have to run this. This should only be a stop-gap solution. Better to cleanse this data *before* it's permanently written to your table. Whether it's an extra step in your ETL, an enhancement to the stored procedures used in inserting/updating the data, or fixing the application(s) doing the inserts in the first place.
Bit drunk right now, but might be easier to use SUBSTRING with PATINDEX if you're comfortable playing around with RegEx. Regexr.com is a nice place to test regular expressions.
no worries, you're not being a dick. that's why i'm here! in fact, that was probably next on the agenda, python or R. thoughts on which to tackle first? which will look better to folks looking to fill entry level positions?
I'd personally go for Python as it's more versatile, pandas and numpy libraries etc., but I can't say I'm really qualified to advise as I'm primarily a database architect. If you're looking at working with Microsoft SQL (and possibly others, I wouldn't really know), R might be more appealing as it's integrated with both SQL server and Power BI. However, it looks like Python is next in line for such integration, and a lot of analytics work seems to use more open source software. Ultimately, I imagine either would serve you well, but hopefully someone more familiar with data analysis and your geography will be able to answer.
The columns might also be in an index or other constraint. This would also slow things down as the stats on indexes might be being created at the same time as the update is occuring. This will depend on the settings of the pre update or the defaults on the DB. [Index Stats Info](https://www.mssqltips.com/sqlservertip/2766/sql-server-auto-update-and-auto-create-statistics-options/) 
Hey, bungle\_bogs, just a quick heads-up: **occuring** is actually spelled **occurring**. You can remember it by **two cs, two rs**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
delete
it seems like there's very good resources for getting a basic understanding of python across the web, so i'll probably go there next as it's the path of least resistance. my goal here is just to get my foot in the door, i assume that once i have an entry level position the real learning will start, and that's what i'm really interested in. from what i understand, entry-level jobs in these fields pay at least as well as my current job (fucking grumble grumble, 10 years experience and a master's degree) so i won't be in a bind financially.
Maybe I'm missing something, but wouldn't it make sense to drop the index, make your changes, then add the index? If 90% of the time is in updating the index, then you make it so it does not have to update the index. I'd love someone with more experience to comment on this. 
Since you want the ID of the row with the max date, I think you will need a subquery. Try a subquery to find the max date of each sales date (GROUP BY sales_date), and then join to the main query on the MAX update_date = update_date and sales_date = sales_date. Sorry, on mobile else I could provide an example.
you could use a window function such as RANK() OVER (PARTITION BY s_date ORDER BY u_date) in your select clause - then subquery the whole thing and restrict to where that ranking = 1
Awesome will try it.
Great idea will look into this.
This should work I think. SELECT [s_date], [updated_date], [sale] FROM table t0 WHERE [updated_date] = (SELECT MAX(t1.[updated_date]) from table t1 WHERE t1.[s_Date] = t0.[S_date])
Not that it should happen given the sample data, but I'd use a function that won't return duplicate numbers for rank ties at position 1 instead of RANK, i.e. ROW_NUMBER. 
 SELECT DepartureTime AS [Time of Departure], COUNT(*) AS [Passenger Count] FROM Flight WHERE FlightNumber IN (SELECT FlightNumber FROM Manifest WHERE (FlightDate BETWEEN #07/01/2016# AND #12/31/2016#) AND PassengerNumber IN (SELECT PassengerNumber FROM Passenger WHERE FFNumber IN (SELECT FFNumber FROM FrequentFlier WHERE PremierStatus=3))) GROUP BY DepartureTime; 
Scalar functions are always pretty slow, even when they're simple. Try to do the replace in-line and see how it performs. It'll be dramatically faster. But if you really want to encapsulate that functionality, I'd recommend converting it to a inline table valued function. That type of function can often perform almost as well as straight inline code, because the optimizer will execute it as a set instead of RBAR. 
&gt;Matthew Wilcox, a Linux kernel hacker working for Microsoft (!) Things have changed a lot since Steve Ballmer "resigned". Linux is everywhere now, and Microsoft is no exception. 
MS Access isn't MS SQL. They're two different dialects of the language. Those nested subselects are a mess. Use `JOIN`s instead, this is what they're there for. &gt;Hint: you can identify AM or PM data from the DepartureTime variable using RIGHT(DepartureTime,2) since the departure times always have "am" pr "pm" as the rightmost two characters in that field So much fail here. Your times are being stored as strings instead of a proper date/time data type. Your instructor is doing you a disservice here, showing you poor database design/modeling. If these tables were set up properly, with the departure date/time stored as a `datetime` type, and the data was in SQL Server, I'd do something like this: select count(*) as [Passenger Count], DepartureTime from ( select case when datepart(hour,F.DepartureTime) &lt; 12 then 'AM Departure' else 'PM Departure' end as [DepartureTime] from Flight F join Manifest M on F.FlightNumber = M.FlightNumber join Passenger P on P.PassengerNumber = M.PassengerNumber Join FrequentFlier FF on FF.FFNumber = P.FFNumber where FF.PremierStatus = 3 and M.FlightDate &gt;= '2016-07-01' and M.FlightDate &lt; '2017-01-01') as FlightTimes group by FlightTimes.DepartureTime;
If you don't get the answer you need here, you might want to post in /r/SQLServer also. This sub is more about SQL the query language.
&gt; don't get the answer you need here, you might want to post in /r/SQLServer also. This sub is more abo Oh thank you! I appreciate it. 
The optimizer is a strange and mysterious beast. On your last point, I agree. However, this is a stop gap on top of a bodge-job (the original data is processed using Access!). I am working on a much better implementation that is 100% SQL based using SSIS and minimal update queries.
There's an update() function that you can use inside a trigger that returns a boolean value indicating whether a specific columns was updated. So at the beginning of the trigger on the Customer table, you could have an if statement something like this: `if update(CustomerAddress) = 0 or update(CustomerPostcode) = 0 return` Then proceed with your code to insert into the audit table. That way, if both of those columns have not been updated, the insert will not fire. 
Thank you, but this is a bit too fancy for me, I dont work in the field, this is a ''Advanced Trigger task'' from my DBA assignment... Whats the easiest, entry-level way of doing this? Also, as much as grateful I am, your solution doesnt solve the ''print error message'' mechanism. 
You can use the [`UPDATE()` function](https://docs.microsoft.com/en-us/sql/t-sql/functions/update-trigger-functions-transact-sql?view=sql-server-2017) to test that each required field has been updated. If your criteria aren't all met, rollback the transaction. Example: https://dba.stackexchange.com/questions/172159/trigger-to-prevent-update-when-flag-value-is-1
Great thinking. The problem I see with this theory though is it will only return values from the max updated_date. So the last three values in the example above would not be in the output.
What index? He claims there are none. 
"I have had a look at the projected execution plan on the query and the most costly item by a country mile is a clustered index update (90% cost)." This sure seems like an Index is being updated. Maybe the column isn't indexed, but it is probably included. 
Try putting in a WHERE condition, such as WHERE [Tech Comments] LIKE '%[' + CHAR(13) + CHAR(10) + CHAR(9) + '"]%' OR [Cust Comments] LIKE '%[' + CHAR(13) + CHAR(10) + CHAR(9) + '"]%' OR [Part No] LIKE '%[a-z]%' COLLATE SQL_Latin1_General_CP850_BIN OR [Part Desc] LIKE '% %' You might need to tweak it a bit to make sure it catches everything you want to update -- I'm not 100% sure about the collate.
Worked perfectly.
Two people who do work in the field independently gave you the same solution. What do *you* consider a "not too fancy" solution?
/u/alinroc is right - you are shooting yourself in the foot with those nested select statements. Using `IN` in place of a join in a query like this is not a very good idea most of the time. Definitely read up on what joins are and how you use them - they are a fundamental operation in most SQL dialects. 
Oh I see. Do you have a change date or anything on the data rows themselves? If not you could do a RowCount of the table and then look for the max RowCount for each s_date. As long as the data is inserted at the bottom of your table it should do the trick.
You might try a constraint. You can use a function in a constraint in SQL Server, which would allow you to sum the bookings for a particular id before allowing the insert. 
build something even if it's only on your own desktop computer (and not, for example, online on a web site) 
&gt;can you please give me advice on what steps to take to become junior dba by learning sql? Hey OP, we can definitely help you with this question. There's a lot of good historical results if you search around this sub too. &gt;I don't know if it's likely but I would like to study until the end of the year and start applying for a position if possible. That could definitely be a possibility! It seems to me like you are hopping around quite a bit. From wanting to do sys admin to programming to web development to security to help desk and to database. IT is a HUGE market. It is incredibly immense, so much so, that you could pick a flavor of SQL, then a version of that flavor, then a specific task oriented to that version and flavor, and you would have a job made out of that. Likewise, you could go the opposite of being niche, you could have a wide variety of skills and do a little of everything. I would ask yourself, WHY do you want to get into IT? I would also recommend to possibly seek professional help if you feel as though you have mental health problems. Your health and happiness should always be the first priority in your life. If you can't take care of yourself, how can you advance yourself? It's all building blocks, and that's block one. So I would strongly urge you to look into counseling, and not just emotional counseling. Your college likely has resources in the career department that can help you narrow down what you want to do and how to go about getting started. If you are looking for another alternative, I think 7 habits for highly effective people and How to win friends and influence people would be great books to help you on a self-discovery journey. So back to your questions, we can definitely help you get started in Database and I know there is no limit to people willing to help you on this journey, but based on your posts, I honestly feel your best avenue is to take a step back and evaluate everything else first. If you want to progress into the world of data, I would start by learning general basics of the language and what databases are. I think it is also important to pick a flavor and stick with it for awhile. If you are interested in T-SQL as a flavor, you can see a similar post and my answer: [https://www.reddit.com/r/SQL/comments/8wcsbe/career\_pivot\_guidance\_would\_be\_really\_appreciated/e1uhkir](https://www.reddit.com/r/SQL/comments/8wcsbe/career_pivot_guidance_would_be_really_appreciated/e1uhkir)
Get virtual box. Create three VMs. Download and install Oracle 12 on 2 of the three VMs. Download and install Postgresql 10 on the same VMs. I'd recommend Red Hat as the operating system for your VMs. Configure Oracle in a Data Guard configuration. Configure Postgresql with a hot standby. Use the third VM as a NFS server and have it mounted on both of the other VMs. This is where your backups will go for both Oracle and Postgresql. Hardly any SQL knowledge required, but you will have learned HA solutions for both Oracle and Postgresql. Look at Barman and repmgr for Postgresql. Then, when you feel comfortable, look at ora2pg. Good luck.
Ended up ranking the rows partitioning by sale date then ordering by updated end. Then using a CTE to return only the top ranked fields.
The way your prof has the hint he's probably looking for you to use something like this in your select: SUM(CASE WHEN RIGHT(DEPARTURETIME,2) = "PM" THEN 1 ELSE 0 END AS "PM" SUM(CASE WHEN RIGHT(DEPARTURETIME,2) = "AM" THEN 1 ELSE 0 END AS "AM"
https://www.brentozar.com/sql/picking-a-dba-career-path/ Maybe use this to get a feel for what type of path you want to go on, and explore that a little more.
Sorry, I missed the requirement about "appropriate error messages." In that case, rather than simply returning if the necessary fields are not being updated, you could simply raise an error (using the raiserror function). The link posted in the comment from alinroc shows you how to do that. I really don't think that using an if statement along with the update() function is terribly "fancy" or advanced. It's pretty basic. 
nesting does help at large scale (OP likely isn't)
I’m definitely no Access expert, but I think that you cannot join more than 2 tables at a time together in that dialect. 
I use this image to help explain join properties. https://qph.fs.quoracdn.net/main-qimg-4148705df34e771ff7d1c6cb630aff3d
Left Joins and Right Joins are the same, it's just the order of the tables that differs. I've never actually seen a right join "in the wild". It just seems to be convention that people write them as left joins.
Because they're identical, and people will often stick to one convention to cut down on debugging. I suppose you could also write a more complex query which demands a right join, but I would find it quicker and easier to refactor.
I've used them a few times, mostly having to do with changing the first table in a chain of joins with both inner and outer components without rewriting the whole query. Specifically I wrote a query to look for one thing, and then right outer joined it to another table to add a layer of summarization, it saved me a few minutes of typing at the expense of making people scratch their head at my code. Mostly though right joins are highfalutin (ha, finally got to use that word), or people not really being clear on right vs. left. 
There is no functional difference. It just entirely depends on the table join order. FROM TableA LEFT JOIN TableB Is the same as: FROM TableB RIGHT JOIN TableA In my 20 years of DB development I've never had to use a RIGHT JOIN for technical reasons.
The clustered index is the table itself. I suppose you *could* drop the index, then re-create it, but...ugh.
Standardized coding is important in ALL languages. The point of coding styles is to be consistent. It doesn't matter at the end of the day if commas come before or after (I prefer after), if tab stops are 4 or 5, etc. What matters is you and the rest of your team are consistent. Readability is a huge factor as well, as it makes code easier to maintain, debug, and helps prevent mistakes. My coding style for SQL is pretty much set in stone and I try to get all our developers to follow the same format (with varying levels of success, lol). I also advocate for vertical alignment, as it makes scrolling through pages of code easier to find what you are looking for. 
I completely agree with you on the value of standardized consistent style (particularly readability). But as you said yourself, you have your own SQL style set in stone, so do others like myself. I was wondering if there was some well-accepted source that SQL devs refer to and/or if there is SQL-specific tooling that is commonplace for professionals. For an example of what I mean: in Ruby, the [Rubocop linting library seems to be ubiquitous for open-source Ruby development](https://blog.percy.io/share-rubocop-rules-across-all-of-your-repos-f3281fbd71f8). I can expect repo-maintainers to automatically reject my pull request if Rubocop finds code smells, even trivial cosmetic ones. I think Flake8 and ESLint are similarly popular, respectively, for Python and JS. So when developing in these languages, it doesn't matter whether you have a personal coding style or not, for the most part, styles are automatically enforced by widely-accepted and used community libraries. I'm asking if SQL development has anything similar since I haven't found it myself (nor have tried to contribute to an open-source project that is heavily written in SQL). 
... you're just stringing things together at random here there was, ten years ago, a standard called websql. in order for a standard to get through the w3, it has to be independently implemented by two separate groups, to help smoke out bad choices in the spec (inherited from the c++ standards body.) sqlite is the only product like it. nobody wanted to implement a second sql engine, so the websql standard failed. this is probably for the best: whereas sql in the browser would be great, the websql standard was double terrible. later, people released a somewhat less (not enough) terrible standard called indexeddb. since it isn't a real database, any old jerk could implement it. so, they did. so, there were competing implementations. so, a standard became real. websql is not important. it has nothing to do with sqlite's future. browsers use sqlite because they need a database, not to implement websql. they're not getting rid of sqlite any time soon, and it wouldn't matter if they did.
Not that I've seen out there. My style evolved over the years and was mostly influenced by examples found online as I was teaching myself SQL prior to going back to college. As you said, it's going to depend on the project or organization, hence why a "best practice" so to speak hasn't been developed. I'd be more apt to follow a third party "standard" for processes around SQL development, rather than a style guide. 
SQL doesn't have anything like that that I know of, which is why the questions come up so often. Leading commas serve a common purpose: they allow for efficient adding and commenting out of individual columns, which we do all the time. It's seconds when your query throws an error to commas, but one could argue that the savings in mental overhead is more. If you're on board with leading commas, the next logical step is vertical alignment. It just makes everything easier to read when you're troubleshooting a query. Sure, these things are mere preferences, but they go a long way to increasing efficiency and reducing mental overhead imo.
I am unaware of any tools like Rubocop for SQL, but some DB UI tools (e.g. SQL Server Studio, DataGrip) do have a formatting functionality that can be used to perform some of the auto-correcting that linting tools do.
I use a right join when I'm to lazy to rewrite my query and have everything LEFT JOIN off the last table I've listed in a many table query. I have used them a handful of times.
The goat
Yes I agree the things I posted were kind of random, but I was posting what was on the SQLite Wikipedia page, because it's the first thing that comes up (understandably) when searching for browser support of SQLite. And to reiterate what I quoted before, this is what it has under the [Web browsers](https://en.wikipedia.org/wiki/SQLite#Web_browsers) heading (I didn't previously include both bullet points): - The browsers Google Chrome, Opera, Safari and the Android Browser all allow for storing information in, and retrieving it from, a SQLite database within the browser, using the Web SQL Database technology, although this is rapidly becoming deprecated (namely superseded by IndexedDB). - Mozilla Firefox and Mozilla Thunderbird store a variety of configuration data (bookmarks, cookies, contacts etc.) in internally managed SQLite databases. There is a third-party add-on that uses the code supporting this functionality to provide a user interface for managing arbitrary SQLite databases.[33] So what you're saying is that I could go in and edit the Wikipedia entry to say this: - Google Chrome, Firefox, Safari, iOS Safari, the Android Browser, and Opera use SQLite to store a variety of user data and configuration, including bookmarks, cookies, and web history. - SQLite is also used by browsers as the backend for client-side data storage API. The W3C attempted to define a specification for a [Web SQL Database API](https://dev.w3.org/html5/webdatabase/), which included the requirement that the "the SQL dialect supported by Sqlite 3.6.19.". But the Working Group ultimately gave up, citing an "impasse" in which "all interested implementors have used the same SQL backend (Sqlite), but we need multiple independent implementations to proceed along a standardisation path." I haven't done much with browser client-side data storage, so I didn't know if browsers were looking to unify the data backends they used for web database/storage and for application data. Looks like the answer is "no", nor does it matter for my particular use case -- writing about how to use SQLite queries to explore your own browser user data -- since it looks like most browser makers are trying to follow SQLite syntax anyway, with the exception of Microsoft which (stupidly IMO) is using their own propriety datastore for Edge and IE. 
I like the leading commas when testing out some SQL. I can easily comment out a column trailing another column without having to delete the trailing comma from the previous line. That’s my only argument for it. 
Excellent advice!
I like leading comma style. I don't often do it, but I see the benefit. To me readability is paramount, so named CTEs are nested so they indicate the chained relational logic. It's not worth getting too obsessed over minor styling points when you have code to get out the door.
The problem is likely in LEFT JOIN `product_pricing` ON `products_u_ids`.`u_id` = `product_pricing`.`u_id` The simplest way forward from where you are would be to add AND `product_pricing`.`timestamp_valid` = temp.MaxDate to the where clause.
You might be interested in what's going on in the DBT community. They share a similar view point to what you've shared here, [check out their post on this.](https://docs.getdbt.com/docs/viewpoint)
Sorry if I missed you pointing out, but one other thing to highlight: A lot of these languages allow a trailing comma with no issue. So you can have a trailing comma, no following item, and the code still compiles just fine. SQL definitely needs the list of fields to end without a comma. So this often comes down to diffs in source control. With these other languages, you can delete a field or add one, and the only change is on their respective lines. With SQL, if you add a need to add a new field, you're going to change at least 2 lines if you do trailing commas. That might seem like a no biggie for the simple case, but if you have a complex field with some nested case statements, and then you add a new field after it, you're now needing to review a diff where the complicated "line" has changed, as well as the line you inserted. And all you did was add a new field! Imagine a more complicated PR.
One nit: check out Elm or Haskell for some ideomatically leading-comma languages.
This is both the most interesting and obscure post I have seen in quite a while here. Excellent work!
http://www.reddit.com/r/SQL/wiki/index Note /r/SQL does not allow links to basic tutorials to be posted here. Please see [this discussion](https://www.reddit.com/r/SQL/comments/2jcw2y/what_do_we_think_about_tutorials_especially_basic/). You should post these to /r/learnsql instead.
[This is where I learn online](www.google.com). Yes, I am a smartass, but in this case, I am not joking.
I agree that SQL development is definitely more fragmented - I'd guess that something like 50&amp;#37; of the SQL I look at isn't a SQL script in my standard IDE/text editor- it's a query in a reporting tool query window or a code fragment embedded within another programming language or in a Linux terminal program where I'm writing &amp; running individual statements interactively. Or it's something else entirely. Even when I'm working in my "standard" environment I'm often writing adhoc queries that to do a piece of work once. SQL is much more like a shell scripting language in those contexts. Should you lay out your Bash/Powershell scripts properly? Of course you should - but as those sometimes evolve from little one liners it's natural they end up less structured than something like C or Java. SQL style guides are also hampered by differences between SQL variants - in a normal week I write queries against four different platforms and while there aren't huge distinctions between them there's enough to make a rigid approach impractical. Joe Celko's SQL Style Guide attempts to be platform neutral and gives some arguments for doing things a certain way and I've worked in environments where we tried to follow at least some of his suggestions (simply to save time/arguments). Personally if I join a team or write code for an existing project/system I will strive to be consistent with whatever their conventions are. But I've seldom found it to be a useful use of time (or energy) to enforce strict rules though - especially where preferences are basically subjective and your SQL writers are working across the organisation (and not just all in one development team). But guidance is definitely useful. Although even if you did achieve internal standardisation you'll still find yourself working with third party queries so badly written that whitespace is the least of your worries. In terms of leading commas, I definitely see the logic of writing code that way (to allow modifications) but it's really hard to get past how ugly and "unnatural" it looks. I've tried for 15 years now and I still find it an alien way of writing lists and therefore I still use a trailing comma for almost all of the code I write (that's new/not an extension of something else). I wish there was a cross platform free SQL formatter which worked well. It just seems to be a hard thing to get right across the board. I've got a personal licence for SQLInform which does the job and has almost enough output options to get code how I want, but it requires Java to be installed locally and it isn't something I can just have available on any server I'm working on. Red Gate's SQL tools also seemed to work quite well (not used in a few years now) but they are more expensive and are limited to MS SQL Server which makes them only partially useful for me. There are various online efforts but I've never found a single tool which worked as well as I'd like.
i don't think many people have heard of this little search engine thing..
https://db-fiddle.com is a good place to practice Here's a real simple SQL query for ya: https://www.db-fiddle.com/f/vwnVuszmAoX6ncQpyhyyjq/0
After having worked with sql for 15 years, my habit is to write inner joins first and then left joins. If I’m handed a query that uses right join (This is usually from someone who has been playing around in a visual query builder), I rewrite it to a left join.
Completely understand. Since there is so many programs, I wanted to get some feedback first in order to start my search. Honestly, I do agree on your point though
Buy three copies of *Joe Celko’s SQL Programming Style*. Keep one safe for yourself. Provide one to co-workers to reference. Use the third copy to smack developers and DBAs in the head with when they don’t follow the rules. It’s not a really heavy book so there shouldn’t be any permanent damage.
Left-handed people usually write right joins, and right-handed people usually write left joins.
&gt;SELECT &gt; &gt; TAB\_1.FIELD\_1 AS DIMENSION, &gt; &gt; SUM(TAB\_2.FIELD\_2) AS METRIC &gt; &gt;FROM &gt; &gt; TABLE\_1 TAB\_1 &gt; &gt; JOIN TABLE\_2 TAB\_2 &gt; &gt; ON TAB\_1.FIELD\_ABC = TAB\_2.FIELD\_ABC &gt; &gt;WHERE &gt; &gt; TAB\_1.FIELD\_1 = 'SOME VALUE' &gt; &gt;GROUP BY &gt; &gt; TAB\_1.FIELD\_1 &gt; &gt;HAVING &gt; &gt; SUM(TAB\_2.FIELD\_2) &gt;= SOME\_NUMBER &gt; &gt;ORDER BY &gt; &gt; DIMENSION Start by understanding this query
I found myself adding a right join to a query last month for this reason. I think the first time in years.
I wonder if any SQL code guidelines have "No right joins!" as one of the rules.
SQL is rarely procedural, other programing languages deal more with if a then b, and because of that the order of operation needs to be made clear in formatting. In SQL the order of operations is always the same, joins -&gt; where clause -&gt; select clause -&gt; group clause -&gt; order by clause, so there isn't an imperative on formatting to make the OOO clear. 
Welcome to the best community on the internet: The SQL community.
Normalization isn't required for reporting. If you plan on using it as a transactional database for a web front end, you'll probably want it normalized for ease of processing. At 300 records you could simply use a flat file frankly.
The whole point is not to exclude records without `price2`. But have them as NULL. Any ideas how I can do that? 
Having a hard time figuring out what you're trying to do. Is this what you want? LEFT OUTER JOIN ( SELECT `product_pricing`.`u_id`, `timestamp_valid` AS MaxDate FROM `product_pricing` WHERE `product_pricing`.`timestamp_valid` &lt;=UTC_TIMESTAMP ORDER BY `timestamp_valid` DESC LIMIT 1 ) AS temp ON temp.u_id = `product_pricing`.`u_id` AND temp.MaxDate = `product_pricing`.`timestamp_valid` Not familiar with MySQL so the syntax might be a bit off.
Add AND (`product_pricing`.`timestamp_valid` = temp.MaxDate OR temp.MaxDate IS NULL) instead
Add it to the `WHERE` inside the `LEFT INNER JOIN`? Here: `WHERE product_pricing.timestamp_valid &lt;= UTC_TIMESTAMP`?
When I add it to the `LEFT INNER JOIN`, `WHERE` I get this error: `Unknown column 'temp.MaxDate' in 'where clause'`. If I add to to the main `WHERE` I get all the entries per ID inside product_pricing.
That suggests that the same product ID has multiple price records with the same effective date. If that's the case, which record is the query supposed to return?
There are multiple price records per ID. And their effective dates are not the same. As you can see from the screen shots above. In any case. It always has to return the effective date that is smaller or equal to the current time.
I get it now: the problem is your subquery is grouped on u_id instead of the product ID. In ludo whatever join you need to get the product ID in the subquery, then group on thar Id instead of Uid. Once the subquery is returning the max effective date per product ID (and joins and where clauses updated to use that field ) it should work.
Keep the change to the where clause and remove the
I'm not exactly sure what you mean. Do you mind making the changes to the actual query?
I by no means am an SQL wizard, I’m just taking a database class to fulfill a minor. But I’m positive that this is how my professor wants it set up. I’m not saying it’s the best way because I’m sure it’s not, but what I have is definitely right so far. I just figure out how to change everything to PM and AM
hackerrank.com or datacamp
I use datagrip (really pycharm because I'd rather not open both) religiously. I preferred leading commas, but the automation is so handy and now I don't see an advantage to leading commas over trailing commas when select etc. is on its own line.
But databases grow. Every small database short of ERP systems started small and now they are unruly globs of data because no one put in the half an hour of work on the front end. 
I'm seeing a lot of answers explaining what a right join does, but none really defending the NEED for one. My answer would be that there is no real reason for a right join. I have seen them in wild, but generally hate them with a passion and alter that person's code to get rid of them as they are nothing but confusing to read in the middle of 7 left joins
I guess my gut was right, thanks for posting
Fwiw, not everyone ascribes to leading commas. I loathe them. I might let them exist mid development, but finalized scripts have trailing commas. It's only logical.
But flat file would be sort of disorganized correct? There would be 10-15 columns also. 
I have heard some DBMS only support right joins
Not really, you'd make it delimited. The big drawback is 2 users trying to read or write to the file at the same time.
For what you're talking about normalization isn't a big deal. Let's say you have address info in there. If the same address ends up in there 2x you'd have to update both records (sometimes). 
What can happen if several users try to read or write to it at the same time?
I often wondered if people who's mother tongue is written Right-To-Left use right join more.
Same thing that happens when you try to open a file that another person has open. The second person should get an error saying the file is locked or missing or unavailable, they may not get any error and any changes they make will be lost. Also how will you handle if a user has the file open then their session disconnects? Will you force close the file? After how long? I'd there is any possibility of 2 or more users altering the file choose another method.
Exactly this. I never understood why someone would have to use a right join until one day I didn't want to rewrite everything.
I don't think he was questioning what a right or left join is.
Do you have some sample data? I just don't follow what the inputs and desired outputs are.
Use either Coalesce or Isnull() to substitute an identifiable value for null, when case matches identifiable value then not shipped. Value MUST be unique and identifiable. 
I made an edit with some data
THIS WORKS: &gt;SELECT o.ShippedDate, o.OrderID, o.OrderDate, CASE WHEN o.ShippedDate IS NULL THEN 'Not Shipped' ELSE 'Shipped On' END FROM Orders o ORDER BY o.OrderDate DESC
 CASE WHEN o.ShippedDate IS NULL THEN 'Not Shipped' WHEN o.ShippedDate IS NOT NULL 'Shipped On' END
works as well, thank you
&gt; SELECT &gt; `products`.`wo_id`, &gt; `products`.`fty_id`, &gt; `products`.`price` AS price1, &gt; &gt; `product_attributes`.`fty_id`, &gt; `product_attributes`.`cat_id`, &gt; `product_attributes`.`design_id`, &gt; `product_attributes`.`season_id`, &gt; &gt; `products_u_ids`.`u_id`, &gt; `products_u_ids`.`link_id`, &gt; &gt; `product_designs`.`design_id`, &gt; `product_designs`.`brand_id`, &gt; &gt; COALESCE(`product_pricing`.`u_id`, NULL) AS price2_u_id, &gt; COALESCE(`product_pricing`.`currency`, NULL) AS price2_currency, &gt; COALESCE(`product_pricing`.`price`, NULL) AS price2, &gt; COALESCE(`product_pricing`.`formula_id`, NULL) price2_formula_id, &gt; COALESCE(`product_pricing`.`vat_calculated`) AS price2_vat_calculated, &gt; COALESCE(`product_pricing`.`vat_id`, NULL) AS price2_vat_id, &gt; COALESCE(`product_pricing`.`timestamp_valid`, NULL) price2_timestamp_valid, &gt; &gt; COALESCE(`product_price_formulas`.`formula_id`, NULL) AS price2_formula_id, &gt; COALESCE(`product_price_formulas`.`formula`, NULL) AS price2_formula, &gt; &gt; COALESCE(`global_vat_tariffs`.`vat_id`, NULL) AS price2_vat_id, &gt; COALESCE(`global_vat_tariffs`.`percentage`, NULL) AS price2_vat_tariff &gt; &gt; FROM `products` &gt; &gt; LEFT JOIN `product_attributes` &gt; ON `products`.`fty_id` = `product_attributes`.`fty_id` &gt; &gt; LEFT JOIN `products_u_ids` &gt; ON `product_attributes`.`fty_id` = `products_u_ids`.`link_id` &gt; &gt; LEFT JOIN `product_designs` &gt; ON `product_attributes`.`design_id` = `product_designs`.`design_id` &gt; &gt; LEFT JOIN `product_pricing` &gt; ON `products_u_ids`.`u_id` = `product_pricing`.`u_id` &gt; &gt; LEFT JOIN `product_price_formulas` &gt; ON `product_pricing`.`formula_id` = `product_price_formulas`.`formula_id` &gt; &gt; LEFT JOIN `global_vat_tariffs` &gt; ON `product_pricing`.`vat_id` = `global_vat_tariffs`.`vat_id` &gt; &gt; LEFT OUTER JOIN ( &gt; SELECT `product_pricing`.`u_id`, MAX(`timestamp_valid`) AS MaxDate &gt; FROM `product_pricing` &gt; WHERE `product_pricing`.`timestamp_valid` &lt;= UTC_TIMESTAMP &gt; GROUP BY `product_pricing`.`u_id` &gt; ) AS temp ON temp.u_id = `product_pricing`.`u_id` &gt; &gt; WHERE `products`.`wo_id` IN ('028284', '018305', '031536') &gt; AND (temp.MaxDate = `product_pricing`.`timestamp_valid` OR temp.MaxDate is null) 
To clarify: you want to return only the records where the result of that particular test type (e.g, T3) have changed since they ran previously? I'd be looking at something like SELECT x.Test, x.Pass, X.Time FROM ( SELECT Test, Pass, Time, LAG(Test, 1) OVER (PARTITION BY Test ORDER BY Time ASC) as PrevResult FROM YourTable ) x WHERE x.Pass &lt;&gt; x.PrevResult I always get my directions confused with these though so just try ORDER BY Time DESC as well and make sure I've got that right.
No, I meant if any of the test results change for a timestamp. For e.g. in the graph, the percent of tests passing will be shown and timestamps for which any test's result changed will be highlighted and on hovering on those points, users will know which tests where failing and which were passing at that timestamp. Currently, I just use multiple queries to do it in the prototype. The database structure is almost identical to the one shown, with minor name changes.
Not sure I follow: if any one of the test results are different from the previous timestamp, you want to return all the results from that timestamp, otherwise return none?
I see. I think I will go with database, specifically MySql. Because there are not as many records, I think there is no need to pay for databases. Do you think MySql will be sufficient for this?
Go with the 2-3 tables Is it critical? No But if you're going to use a database, you might as well get it right from the start.
I'd look into sqlite first, especially if you're dealing with that little data. It's still overkill, but less of it since you don't have to configure and maintain a database server.
I'm not trying to be a pain but I still don't follow. What SQL or algorithm would you apply to that example now to process it?
You've shown the input data set, but not the output data set. What data should the SQL query return? Literally the columns and rows; otherwise we don't understand what you want. 
The thing is that I am actually hosting the website on LAMP. So MySql makes more sense here, and it is the one I am at least familiar with. Never used SQLite, but I will consider it, thanks.
Yeah, if you already have a sql server up and running nevermind.
If it's going to be 300 rows forever, and doesn't need to be queried in a high frequency or complicated manner, then it doesn't really matter. Ultimately comes down to a judgement of whether the initial time investment is worth it, given the likelihood of scope creep.
Locking issues. Databases generally handle locking much more atomistically and elegantly than single files.
Why not MS SQL? 
I think that a big part of the reason for style fragmentation is that there's relatively little SQL in OS development, so there's less chance for styles to become widely adopted. Comma placement ultimately doesn't matter. Given that, I use leading to ease the alteration of lists a bit.
Pls post some sample output. Guessing that what you want is a single row for any batch where test output differs from prior, with the batch datetime and a success percentage? I'd try by creating a CTE which generates a batch ROW_NUMBER partitioned and ordered by the datetime, SELECT from that where a self join on a LAG of the ROW_NUMBER and the test ID gives a differing result, then use those datetimes to SELECT from (probably using WHERE EXISTS) the original data your pass percentage for each of the batches that have been identified.
Right, commas-first is in the minority. I don't think they're less logical though.
I have added some sample output
have you looked into SSRS reports and Power BI? They're both basically ways of taking data from the database and presenting it in a more meaningful way. A job title with those skills would be like database developer, bi developer, report developer etc
Oh, that's easy man! You just have to do a group by on time, then count the ratio of passes to the total count of records. This'll do the trick: select time, sum(case pass when True then 1 else 0 end)/ (count(pass) as real) as pass_percentage from yourtable group by time You didn't say what database you're running this on, some represent boolean values oddly. But this will work on most. 
Depending on what you want to be doing and where I think those skills your describing could be part of a really interesting analyst position. Maybe not a straight Business or Product Analyst but you never know. Are you thinking that you'd like to do SQL programming/developing only or are you more interested in a job that lets you consistently use SQl in interesting ways?
More opportunity with Oracle, and lots of opportunity to learn about moving from Oracle to Postgresql. Like it or not, there are more installations (and generally better pay) of Oracle than SQL Server. The majority of those Oracle dbs are on a Unix/Linux operating system. So besides learning the two databases, one will also learn a very popularly used operating system.
There are a few jobs for just straight SQL, but if you want to progress career wise and make more money there are really only 3 paths- 1) DBA, 2) Business intelligence/ data mining, 3) programmer. DBA work is boring for the most part IMO. BI pays well but is close to accounting kind of work from what I've seen. Programmers need to know SQL or some other rdbms too be well rounded. 
Makes sense. Worked at a MS shop and forgot to think of the selection bias involved with that! 
I agree. Your current experience might be enough to get an entry-level job now. The market is hot. Add Power BI to the mix and you'd be in a good place. SQL + Power BI is a great entry point for a business intelligence developer or data analyst. Both of those jobs are broadly-defined. There is tons of possible movement into new technologies or business roles from these two. Just do anything to get your foot in the door. Many jobs will be out of reach if you do not have a related degree, but there are some without that requirement. If you can get into an experienced team, you will have mentors to help you. If not, check Craigslist. They often have small businesses looking for some subset of skills. They probably pay less, but you get your foot in the door. 
You're AWESOME! Works like a charm! Never thought this was going to work like so... Thanks a lot!
Of course knowing SQL Server, Oracle, Postgresql, Windows and Unix/Linux will open up MANY opportunities. 
I do Database Marketing and spend 50% of my time using sql. Can be stressful with deadlines or errors, but can mix sql, analysis, automation, statistics, and art.
Business Intelligence Analyst type jobs would probably be a good fit, that is what I do and part of my job is working with the requestors to better understand their problem and how the data we eventually provide them will help them solve their problems. /r/BusinessIntelligence 
I work at a nonprofit as a data analyst/manager, creating custom datasets for researchers, and while our job is SQL-heavy, we also have to know the data sources really well (health exchange/electronic medical records) and be able to advise study teams. I think it’s a good combination of SQL skills, problem solving skills, knowledge of medical terminology, and soft skills to deal with investigators, etc. Is there another interest you have (for me it’s anything related to health IT, and I worked at a doctors office for many years) that you could intersect with your SQL skills? 
You're a business/data analyst Harry!
I find W3 Schools to have some useful info, I'm a beginner myself. Youtube videos (just look up SQL for dummies) were a big help.
Try SALES_YEAR = 2015 OR SALES_YEAR = 2016. Or SALES_YEAR IN (2015, 2016)
thanks, but the sale error shows up https://imgur.com/a/gKXN6rh
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/nebHI9L.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e2121vj) 
Thanks for the response! I think I'm interested in constantly using SQL (and other programming and Technology) as a problem-solving tool. Business intelligence and analyst type jobs sound interesting, I've always found analytics interesting!