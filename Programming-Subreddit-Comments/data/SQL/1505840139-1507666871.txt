Thank you for the response! What does the 'ok' represent in your formula? 
Can I format this as follows: SELECT * FROM teachers INNER JOIN science_assignments ON teachers.ID = science_assignments.teacher_ID OR teachers.ID = reading_assignments.teacher_ID Do we not need two Inner Joins here? One for the Science and one for the Reading?
The 'ok' is meaningless. You could also just put * there.
Here's the Actual Code: SELECT * FROM teachers WHERE EXISTS ( SELECT * FROM science_reading_assignments WHERE teacher_ID = teachers.id ) OR ( SELECT * FROM science_writing_assignments WHERE teacher_ID = teachers.id ) The error I am getting is on the OR (SELECT *). It is saying subquery must return only one column.
What is the problem in the first place? You can't print because of what error? Something doesn't make sense. If the program was working before--but you need to compile as a 32 bit application--how was it working previously? Well if you're really interested in how it all works you would need to find the solution file and/or project files and open the project in Visual Studio (I'm pretty sure that's what the brother used). Once you have all of the source files you can open the program in the decompiled form and you can make modifications. Once you have the project open you can see all of the items included in the project. Those DLL files, any code the other person wrote, any icons, pictures, or anything else that the application needs to run itself. The MySQL dll is the library that the application uses to make calls to the database. The DLL files made it so this person didn't have to write 5 million lines of code instead of 2,000. They get included with the application when it is compiled so that they are accessible in the local folder. If you want to see *how* it's done I would go into Visual Studio and make a new C# or VB.NET project and start playing around with the designer and code windows and try to emulate the program that you have here. You will probably have a lot of questions and may not make much progress. To fix the printing error though, *if* it definitely requires re-compiling, you will first need the source code and project so that you can open it in VS and then build the program.
correct... but a join isn't necessary, you can also group results from a single table
whoops, forget the second EXISTS 
Does it go after the OR? Like follows: OR EXISTS ( SELECT *
[removed]
yeah, you would use two inner joins... ***if you were going to do it that way*** there's a wee problem with this approach say each teacher on average has 4 science assignments and 7 reading assignments thos inner joins will produce 48 rows for each teacher!!! are you sure you want to "select star" to get all columns from all combinations?? YIKES
what happened when you tested it?
Are you writing this to prepare students for the Database Fundamentals certification?
If there are two inner joins, the teacher ID will have to be in BOTH the science and reading tables. SELECT * FROM teachers INNER JOIN science_assignments ON teachers.ID = science_assignments.teacher_ID UNION ALL SELECT * FROM teachers INNER JOIN reading_assignments ON teachers.ID = reading_assignments.teacher_ID 
He has two separate programs. One opens in 64-bit and the other 32-bit. The 32-bit is the only one that allows printing because on the 64-bit there is an issue with crruntime files, something like that, having to do with Crystal Reports. Upon my quick research, I believe he just has to recompile the first program into 32-bit, or from what I read, the option of "Any CPU" since CR is really picky about that stuff. All speculation but apparently it's a been a long issue for him, so even he would like this help. I'll add a photo on the imgur link on the error if you want to see. There's also other "exception handler type errors". I'll see which I can recreate with the little files I have. Since I'm positive he compiles all of this on his own laptop, then transfers it to each computer, that he can just send me the files and I can try my hand at modifying it from there, instead of having to start from scratch. 
Thank you! You're awesome! Yes indeed you can. You make some great points as well. This has made me more aware of how important these things are and how they should function correctly. The way I see it, just go basic to understand the concept and then build up from there.
Well I did pass the exam, so the book could be used for studying. Though I don't remember all the material for the exam. I'll see if I can dig around and tailor the content around the exam. This book would be much more affordable than that of the study guide.
Yes if he has all the source files on your machine he can send them to you and you should be able to open them in VS. Changing the build to compile on any CPU is pretty straightforward. It's under the project properties&gt;Compile&gt;Target CPU. 
The print issue. https://imgur.com/a/3iCw0 I have, of course, downloaded and redownloaded those necessary files, and I'm sure he has too. The exception error I'm sure has to do with something in the code itself, or maybe not that one, but sometimes there is also a box that pops up like [ "" isn't a Double ]. I can't seem to recreate that error, but it had those words in it :3 But apart from the print issue, the program works decent enough.
Bingo. Thanks for your help!
"" isn't a double probably means that the variable being used is typed as a double (long integer) and it's being passed an empty string (""). That's pretty easy to fix in a number of ways, such as wrapping the value being presented in the UI in a Cstr function, or checking the data type of the value being passed and prompting the user or replacing it, or replacing non-conforming characters with a 0 instead of "", or many other approaches. It really depends on what the variable/type is being used for otherwise. You can't do mathematics with a string, so the data type of the variable is relevant to the context in which it's being used. As for the compile issue, it does sound like you will need to rebuild the application since the report files appear to be embedded in the program. They are effectively "installed" with your program when it's installed, instead of installing CR separately.
Yup, Jenkins can check out code from git (or others) on commit, and compile it and run tests for you You might want to look at a tool like Liquibase to deploy database schema changes, incl updating sprocs . It used to be all XML but isn't anymore
There has been discussion on whether they might as well pay to create this program more elegantly, how much would you say someone would charge to get this program created? Literally the only integration right now is with the printer, but like I mentioned to him, instead of having to recompile the program every time they want to add something new (a new service, item, etc), they might try to look around in having someone create a fully fleshed out program with all of that integrated. At most, just the print feature, but with a more elegant UI, since the only ones who are really "techy" would be myself and his brother. We want the program to be as easy as possible to use for any new employee. Just a topic of discussion but he doesn't know that kind of pricing range, and I suppose it doesn't hurt to ask. 
Sounds like the kind of book I need. 
Excellent! Anything you're seeking to learn in particular? I'm trying to hit some pain points for readers and resolve them. As mentioned in the description, I'll be posting a link back to this sub when the book is published and on its free promotion!
Let me know when you finish it. I teach the course and I'd like to recommend a more affordable book than the MS Press book.
Most definitely! I'd be happy to share it as soon as I'm finished. May I ask where you teach? Not trying to pry or anything. I'd be happy to hear any suggestions you may have as well.
I teach at a college in the Continuing Education department (second job). I also work with SQL Server (day job).
That's awesome! That's now another goal with this book, which would be to help others obtain that certification. Thank you! Good on you for teaching as well. I'm sure you have much knowledge to pass to others.
Thanks, this is really useful!
There's a few ways, the term you're looking for is "flattening". 1. Pivot 2. STUFF + XML 3. Dynamic SQL 4. Correlated subquery maybe 5. Subquery with tricky joins maybe https://dba.stackexchange.com/questions/40647/how-to-flatten-results-of-a-table-with-two-related-many-tables 
Yes, some keys suck to join on. What are the data types / index fragmentation / table stats like? Do the SQL plans / wait types / extended events provide any hints?
I can provide those things for you, but you are going to a place that I don't fully understand. The datatype for the key is an incremental integer. Basically I took the distinct values for Field1, Field2, and Field3 and made it a PK on Table2, then on Table1 I made it a clustered index. I was expecting this to increase the speed but it functioning the opposite way. Not a big deal just wasn't what I expected. This is occurring on staging tables that are being created by a parent process if that matters.
My questions were more to spur thinking for you to look at things. I think the primary answer though is in the SQL plans and wait types when running the query with and without the new index. 
I mean I'll be up front and honest with you. I don't understand how to look at an execution plan and take any meaningful information away from it. What I do understand is how to try a lot of different methods and find the one that works the fastest. In this model Table A has a clustered index and is about 13M rows. Table B has a primary key and is about 500K rows. To be honest I still can't really fathom why joining on this key would ever be less efficient than creating a multi-column non-clustered index and joining on 3 fields when the key itself is derived from those three columns. I am willing to accept that one approach might be better in one environment versus another environment, and honestly compared to previous environments that I've worked in... I am not at all happy with the environment I'm in now.
Also what confuses me is this. When I create the table I do so by SELECT * INTO Table, and then afterwards I create the index. Creating the index as non-clustered vs. creating the index as clustered takes approximately the same time for Table A. For Table B creating the PK vs creating the non-clustered index also takes approximately the same time. I've always been brought up to believe that a clustered index is better than a non-clustered index between a direct join. There is no third join going on here. 
I would recommend Brent Ozars index proc to help analyze things, also SQL sentry has a free plan analyzer tool. 
Well usually, because a PK Identity field is going to be an integer (small amount of data) and since it's clustered it's also going to be the actual table itself, ordered by the PK. But there are definitely cases where a non-clustered index on specific fields could perform faster. It all depends on how many rows are in the table, how many of those rows are being retrieved by the query, how many of those rows being retrieved by the query are part of the indexes being used and the size of those indexes, and a number of other factors. Just think of an index as another set of data... sort of. If your query has to scan an entire table to return a significant amount of data via a clustered PK integer vs having a non-clustered index column where the index only has the columns and rows returned by the query, then the latter could be *much* faster. What's the execution plan on the query? You can see whether the engine is doing seeks (like individual row lookups) vs scans (entire table lookups) and all other kinds of good information like how many rows are being returned before filtering, etc. It's pretty difficult to give you any specific advice without the plan.
FROM Table A (clustered) is left joining to TableB (PK) to pick up whatever data is there whenever the join is valid.
I'll Google this. At the moment I have taken a process that takes more than an hour to run and have it down to about 10 minutes, so I'm really happy with performance. I was just shocked when I decided to try using the key and things went south. It wasn't as expected based on what I know (which is minimal.) I'm more likely to hand this off to my DBAs now and tell them to take it from here and show them the key approach and let them figure it out.
Dude, watch this video: https://www.brentozar.com/training/think-like-sql-server-engine/1-clustered-index-21-minutes/ Just the first one, but I think after you watch the first one you will want to watch the others. Indexes made SO much more sense to me after watching this. And I knew what they did beforehand.
Well I'd be curious to know what it is if you figure it out! 
This is exactly the approach I took. My parent data sources look like this FROM CompositeA (~15M) LEFT JOIN Compositbe B (~500K) CompositeA and CompositeB are being created during a "materialization process" that join lots of sub-tables. They are being created with a `SELECT * INTO` approach. The join conditions between them looks like this: ON B.SRC_SYS_ID = A.SRC_SYS_ID AND B.CUST_ID = A.CUST_ID AND B.ORD_ID = A.ORD_ID All three of these fields are stored as `CHAR(10)` and the index between them looks like this: CREATE INDEX ORD_CUST_ORD ON CompositeA(SRC_SYS_ID, CUST_ID, ORD_ID) CREATE INDEX ORD_CUST_ORD ON CompositeB(SRC_SYS_ID, CUST_ID, ORD_ID) So while I was working I thought I had a very clever thought. So I did something like this: CREATE TABLE dbo.Key ( ID INT NOT NULL AUTO_INCREMENT , SRC_SYS_ID CHAR(10) NOT NULL , CUST_ID CHAR(10) NOT NULL , ORD_ID CHAR(10) NOT NULL ) INSERT INTO dbo.Key SELECT DISTINCT SRC_SYS_ID , CUST_ID , ORD_ID FROM CompositeB Then during the composite process I join to this key table, and for CompositeA I create a clustered index on this key (because there can be duplicates), and for CompositeB it is the primary key. This then changes the join between tables to: FROM CompositeA --(Many) LEFT JOIN CompositeB --(ToOne) ON B.ID = A.ID Going to dive more into these videos. Thanks. To be clear I am surrounding each part of this part with BEGIN/ENDs and expecting them to process in serial. edit: In a few instances during the composite processes I am using a `SELECT *` as opposed to specifically stating the columns. I'm only doing this because of laziness. Not sure if this would impact performance. In this example I want all of the columns from the parent, I'm just not specifically stating them. There is no `ORDER BY` in the join being discussed here.
https://www.reddit.com/r/SQL/comments/7167yi/ms_sql_have_you_ever_heard_of_a_clustered_index/dn8lo9w/
 1. Count the occurrences of and the average count per document for each of the words "blue" and "green" by day in the year 2017? SELECT count (), avg(count(*)) FROM document JOIN document_keyword ON document.document_id = document_keyword JOIN keyword ON keyword.keyword_id = document_keyword.keyword_id WHERE (word IS LIKE ‚Äòblue‚Äô OR ‚Äògreen‚Äô) , created &gt; '2017-01-01 0:00:00' GROUP BY document_id, date_trunc(‚Äòday‚Äô, ‚Äúdocument‚Äù.‚Äùcreated‚Äù) I think I have this one, but any feedback would be great 2. Order words by their relative growth of occurrences between 2017 and 2016 SELECT ???? FROM document JOIN document_keyword ON document.document_id = document_keyword JOIN keyword ON keyword.keyword_id = document_keyword.keyword_id GROUP BY word ORDER BY ? This second questions is really giving me a hard time, other than joining the tables, I am pretty clueless on the rest of the required calculations. 
You probably want to create a CTE to hold a value for each day within your date range and then join it with your dataset using something like ON tbl.date BETWEEN cte.date AND DATEADD(DAY,1,cte.date). Then wrap that up as a subquery. In the outer query you'd just use a COUNT and AVG aggregate in the SELECT clause and GROUP by the cte.date field.
It's an array problem. look at converting to arrays then comparing. 
Any way to do this without CTE, anybody?
Thanks for looking. Could you be a little more specific? Are you saying split the Comment into words in say, a temp table? Then do a join with the exclusion table and look for NULLS in either column, indicating a non-match? Then put it back into a long string for analysis? How would this handle punctuation and/or words that end with punctuation? "word?" wont match on "word" in the exclusion table. And at that point I would think i'm back to doing REPLACEs for individual characters, and i might as well just While-Loop through the exclusion table - just take what we wrote in the sproc, and put it in a function taking the Comment as an input. I don't mean to sound contradictory - just not sure what I'm missing from your idea. 
&gt; 1. Count the occurrences of and the average count per document for each of the words &gt; "blue" and "green" by day in the year 2017? SELECT COUNT(keyword.id) as keyword_cnt, document.document_id, trunc (document.created_at) as date_ FROM document join (select * from keyword join keyword_document on keyword.name in ('blue', 'green') and keyword.id = keyword_document.keyword_id) as needless_subquery on document.id = needless_subquery.document_id and datepart ('year', document.created) = 2017 group by 2, 3 You can easily get the avg count if each keyword and total occurrences by doc per day from this. The question doesn't specify that you need to return both results in one set and it doesn't make sense to do so anyway. &gt; 2. Order words by their relative growth of occurrences between 2017 and 2016 WITH sub as (SELECT keyword_document.keyword_id, DATEPART('year', document.created) as yr, COUNT(keyword_document.keyword_id) as num_occ FROM document join keyword_document ON keyword_document.document_id = document.document_id Group by 1, 2) SELECT b.num_occ - a.num_occ as growth, C.word FROM sub a join sub b on a.yr = 2017 and b.yr = 2016 and a.keyword_id = b.keyword_id join keyword c on a.keyword_id = c.keyword_id and b.keyword_id = c. keyword_id order by 1 desc 
Actually, your idea is better. Split the string to a table and do a OUTER join with a LIKE clause to your excluding table. Keep the nulls, rejoin the strings. Databases are always better at doing that stuff. 
Something like this: SELECT Group, Names FROM (SELECT DISTINCT Group FROM Table) T1 OUTER APPLY ( SELECT STUFF((SELECT ' ,' + T2.Name FROM Table T2 WHERE T2.Group = T1.Group FOR XML PATH,TYPE).value('.','nvarchar(max)'),1,2,'') ) Names(Names)
I beg to differ. I've used ORMs extensively, and I've written a lot of plain SQL. The conclusion is that I'm not going to use ORMs again. ORMs only work for simple stuff, and become a hindrance for any non-trivial scenarios (e.g. using PostGIS immediately creates problems with ORMs and migrations). Furthermore, they suffer from the least common denominator problem, forcing you to use a subset of database features. They're just not worth it. There's a section on my reasoning about ORMs in this post: http://korban.net/posts/2015-12-16-misapplied-abstractions/
It isn't about optimisation at all. Performance is a relatively minor issue with ORMs compared to the rest. I co-founded a company so I'm acutely aware of the need to deliver a product. This is precisely the reason why I *didn't* use an ORM, and it absolutely paid off. We leveraged Postgres to the max, and we ended up with *less* code. An ORM would have crippled the product. The majority of what *I* have seen in my 15 year career was anything but CRUD. It's a big world out there. For CRUD, ORMs are ok, but you still have a problem in that you're locking yourself into a suboptimal technology choice from the start. 
Check that all of the columns you're adding as foreign keys have primary or unique constraints within their tables, and that they're not just one part of a composite key. I particularly wonder about roomno in the room table: it's likely that a roomno is only unique within a building (i.e., the key on that table is buildingno AND roomno).
what is wrong my one,I can't seem to find anything 
please show the CREATE TABLE statements for both BUILDING and ROOM tables 
Please add the table structures referenced by your constraint, then perhaps we'd be able help more effectively
disagree with lower case for SQL keywords (not everyone uses a colour-coded SQL editor) your other suggestions are fine ‚ú©‚ú©‚ú© bonus points for **leading comma convention** 
Actually, I don't use a colour-coded editor either (WebStorm, sadly, doesn't colour SQL queries in strings, unlike some other flavours of JetBrains IDEs). I started out with upper case but shifted towards lower case over time. I found that as queries got more complex (e.g. with window functions and case/when statements), there was a lot of noise with a mix of upper and lower case. Another advantage is that because I have a lot of SQL interspersed with application code, there is no dissonance because everything is lower case. Finally, lower case is just easier to type - no need to press Shift :) 
your argument for lower case is detailed, well-reasoned, clearly stated, and wrong ‚ò∫
I don't see anything wrong with your syntax, it's just that you can't make a foreign key constraint if the field you're referencing doesn't already have a unique or primary key constraint on it.
Alright, I'll see if I can make that work tomorrow. Still open to other options if anybody comes up with a better way!
disregarding the obvious typo in the second SELECT, this UNION query will fail if `science_assignments` does not have the exact same layout as `reading_assignments` plus, UNION ALL will return countless unnecessary rows fixed here -- SELECT teachers.* FROM teachers INNER JOIN science_assignments ON science_assignments.teacher_ID = teachers.ID UNION SELECT teachers.* FROM teachers INNER JOIN reading_assignments ON reading_assignments.teacher_ID = teachers.ID this is still largely inefficient, though here's a better approach using UNION -- SELECT teachers.* FROM ( SELECT teacher_ID FROM science_assignments UNION SELECT teacher_ID FROM reading_assignments ) AS these INNER JOIN teachers ON teachers.ID = these.teacher_id
CREATE TABLE BUILDING( BUILDINGNO CHAR(2), BUILDINGWING VARCHAR2(15), BUILDINGLANE VARCHAR2(15), CONSTRAINT BUILDING PRIMARY KEY(BUILDINGNO)); CREATE TABLE ROOM( BUILDINGNO CHAR(2), ROOMNO CHAR(2), ROOMCAPACITY NUMBER(3), CONSTRAINT ROOM_PK PRIMARY KEY(BUILDINGNO,ROOMNO), CONSTRAINT ROOM_FK1 FOREIGN KEY(BUILDINGNO) REFERENCES BUILDING(BUILDINGNO)); CREATE TABLE CONFERENCESESSION( SESSIONID CHAR(4), BUILDINGNO CHAR(2), ROOMNO CHAR(2), SPEAKERID CHAR(2), SESSIONDATE DATE, SESSIONPRICE NUMBER(4,2), CONSTRAINT CONFERENCESESSION_PK PRIMARY KEY(SESSIONID), CONSTRAINT CONFERENCESESSION_FK1 FOREIGN KEY(BUILDINGNO) REFERENCES BUILDING(BUILDINGNO), CONSTRAINT CONFERENCESESSION_FK2 FOREIGN KEY(ROOMNO) REFERENCES ROOM(ROOMNO));
CREATE TABLE BUILDING( BUILDINGNO CHAR(2), BUILDINGWING VARCHAR2(15), BUILDINGLANE VARCHAR2(15), CONSTRAINT BUILDING PRIMARY KEY(BUILDINGNO)); CREATE TABLE ROOM( BUILDINGNO CHAR(2), ROOMNO CHAR(2), ROOMCAPACITY NUMBER(3), CONSTRAINT ROOM_PK PRIMARY KEY(BUILDINGNO,ROOMNO), CONSTRAINT ROOM_FK1 FOREIGN KEY(BUILDINGNO) REFERENCES BUILDING(BUILDINGNO)); CREATE TABLE CONFERENCESESSION( SESSIONID CHAR(4), BUILDINGNO CHAR(2), ROOMNO CHAR(2), SPEAKERID CHAR(2), SESSIONDATE DATE, SESSIONPRICE NUMBER(4,2), CONSTRAINT CONFERENCESESSION_PK PRIMARY KEY(SESSIONID), CONSTRAINT CONFERENCESESSION_FK1 FOREIGN KEY(BUILDINGNO) REFERENCES BUILDING(BUILDINGNO), CONSTRAINT CONFERENCESESSION_FK2 FOREIGN KEY(ROOMNO) REFERENCES ROOM(ROOMNO));
No offence, but if you've worked with sql for 15 years and still think that orm is the way to go, or is suitable for 80% of the sql you've seen, then that 15 years has been incredibly shaded. 
I agree with all your suggestions. Had to look up what CTEs are - short lived temporary tables. Great way to split big single query monsters and improve performance. And LOWERCASE! Finally, there is hope. 
CTEs - eh... at least in MSSQL (where I spend most of my time, so I can't speak to PG), can have worse performance... which really brings up a bigger topic : readability / maintainability vs performance... sometimes, you really do need to focus on the speed more... most of these times a join or subselect (specifically when it's the output field) yields better performance. ... also, alias in FRONT instead of at the END.. Example: SELECT 1 AS [A] -- bad vs SELECT [A] = 1 -- good The latter format it enables each line to be quickly scanned for the field name, whereas the former requires more time/effort finding the end of each line, checking whether it's aliased or not, and whether the name is of interest
well, there's your problem CONFERENCESESSION_FK2 should reference the entire PK of ROOM, i.e. two columns 
The application code mixed with SQL also be an issue depending how it was done. Could the app call stored procs and views.
too bad the alias in front isn't standard sql
ah... bummer indeed
Hi, would love to test it as I have very recently started learning SQL.
If you're referring to sql injection attacks. Yes that could be an issue. But there is nothing wrong with an application calling stored procs and views. 
&gt; For CRUD, ORMs are ok You still have the problem of learning both the ORM and SQL. The ORM will only get you so far so you need to know SQL in the end anyway. I've tried using Entity Framework a few times and inevitably end up in a situation where I have no clue what it's doing. There's too much magic going on and that's not a good thing when you're handling sensitive information. I take good old SQL any day over some ORM that tries to be overly "helpful". 
Inline comments is a great one--in most flavors of SQL it's a double dash: ``` -- comment here ```
Which version of SQL Server? 2016 has a nice function for this very purpose called STRING_AGG https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql
That would be fantastic, but I'm afraid that I'm on 2008 R2.
Excellent! Please pm me and I'll send you the book! Would love to hear any feedback you have.
Unless they snuck this in one of the service packs, STRING_AGG is only for 2017 and up.
Dang, you‚Äôre right, i misread the introduction version :-(
No, I meant the app should be calling stored procedure and views!
What's the number after the department name indicate?
Sorry my bad, edited the question, it indicates the number of employees in the department. 
This looks like homework to me... so good luck.
Not helping üôÉ 
Hint: read sections on "join" and "count"
I came up with this Select d.dept_name, Count (e.dept_id) AS no_of_employees_in_dept From employees e Join departments d Using (dept_id ) Group by d.dept_name; But my friend said there's an easier solution to it. That's why i posted it here. 
I think I'm the only person in the world who struggles with reading queries that use CTEs, or I've just been unlucky enough to have to work with people who are really bad at writing them. I've rarely said "Man, this CTE is way easier to read". I generally find sub queries easier to understand, and if it is that monster of a query having actual temp tables or views means that you can run the process in steps to verify your composition in a way I've never been able to do with CTEs without changing the actual code structure. The CTEs in your example don't even make that much sense to me. The first two CTEs really look like they should be a single query with a join (why use IN? Isn't this a foreign key, and likely indexed?) and as I read further down I'm not seeing any composition that makes me think "splitting this apart is making the query more readable." This looks like a query that could have been handled almost entirely with JOINs in half the vertical space without making the maintenance programmer have to keep track of your CTE names and what they mean. The only part that seems useful is the days/user_days interaction that gets you a start/end date, but I'm not sold on why the example is easier to understand. If you want to sell CTEs as being more readible, why not look at recursive CTEs? A situation with a query that is self-referential in such a way that being able to say something like this: ;with parents as ( select item_id, item_name, 'Master item' as parent, 0 as level from items where parent_id is null union select i.item_id, i.item_name, p.item_name as parent, p. i.level+1 as level from items i join parents p on p.item_id = i.parent_id ) select * from parents order by item_name, level is a much nicer example, and a better showing of where a CTE can be helpful. Doing that with traditional SQL would take more code and be more complicated to follow. 
When I'm just digging, I'll usually start with something like Select job, employee, count(*) c from table_name group by job, employee having c &gt; 1; That usually tells me how big of an issue it is and where to start looking. Hope that helps! 
You're real close. Select those two fields, group by them, then use `having` to filter out any singletons. select job,employee from mytable group by job,employee having count(*) &gt; 1
I don't know what your friend has in mind, but you query is nearly perfect. I'd re-write with "join ... on ..." instead of "join ... using ..." Join-on is going to work in just about every flavor of SQL; join-using isn't supported everywhere. *edit: also, i'm not sure your query will run as written given the table columns in your question. For "using" syntax to work the column "dept_id" has to exist in both table, but in your question you say the table "department" has a column called "id" (and not "dept_id", like in the employees table.)
CTEs aren't always a performance improvement. You can easily make indexes and other performance tools less usable. You also can end up with a faster query, though. It always depends on the query and the data.
Idk my friend is maybe just fucking around with me. So i was second guessing myself, i know my query wasn't perfect. But i still thought I'll ask for help here and correct myself per se. And yes you're right about JOIN syntax. Both have to be existing in the same table. So I corrected and this is what i came up with. Select d.dept_name, Count (e.dept_id) AS no_of_employees_in_dept From employees e Join departments d On(d.id=e.dept_id) Group by d.dept_name; Will this do? And also btw, thank you so much for your detailed response. üëç 
My system is struggling to process the 'c' in that statement. Do I need to define it?
[removed]
Unfortunately this is returning just about all results. I think it is seeing the 'job' as a duplicate because it has multiple 'employees' attached. 
try `count(*) as c` instead
Try this: select job + employee as test from mytable group by job + employee having count(job + employee) &gt; 1 You really shouldn't have to though. Are you sure there isn't more to your query that you are omitting?
I think this worked for me - unfortunately it didn't find any duplicates (Which is possible) but the formatting looked correct when I opened up to the whole table. Thanks!
Exactly like I guessed, your FK constraint to the room table needs to be on BUILDINGNO, ROOMNO instead of just ROOMNO.
Thanks for catching that. MySQL accepts implicit aliasing, so I leave it out when I'm just digging into data by hand. 
GROUP BY, COUNT, and INNER JOIN are the keywords you want.
Try this: With cte as( Select Employee , Job , rn = Row_Number() over(partition by Employee order by Job asc) ) Select * from cte where rn &gt; 1 Doing this on my phone, so pardon the formatting If you have a unique identifier for each column, that would be better to partition by. 
UNION ALL is what you are looking for. Write each query and combine them with a UNION. The catch is that you have to have the same number/name of columns so you will need to alias them. Another trick you can use is to declare a variable and then use the variable in the where clause for all of the queries in the union. That way you can pass one value to the variable instead of updating all of your where clauses individually.
Do you mean something like this: SELECT * FROM science_assignments WHERE teacher_ID = 385229 UNION ALL SELECT * FROM math_assignments WHERE teacher_ID = 385229 UNION ALL SELECT * FROM history_assignments WHERE teacher_ID = 385229
Yes just like that, but also like this: DECLARE @teacher INT SET @teacher = 385229 SELECT * FROM science_assignments WHERE teacher_ID = @teacher UNION ALL SELECT * FROM math_assignments WHERE teacher_ID = @teacher UNION ALL SELECT * FROM history_assignments WHERE teacher_ID = @teacher Now you only have to set 1 value instead of a bunch. Remember that the columns need to match, so if you get errors that's probably the problem.
Thanks. I'll definitely give this a try. When you say the columns need to match, are you saying the names of all of the columns have to be the same? Or are you saying the number of rows in each database has to be the same? Sorry. Total noob. 
SQLFiddle link demonstrating this: http://sqlfiddle.com/#!6/45bfe/2
The column counts have to match... so: SELECT 1,2 UNION ALL SELECT 3,4 UNION ALL SELECT 5 Will not work because in the 3rd one, you're only selecting a '5'. If you change it to: SELECT 1,2 UNION ALL SELECT 3,4 UNION ALL SELECT 5,6 It will work. So you should avoid doing `SELECT *` and instead do: SELECT teacher_ID, assignment_ID FROM science_assignments WHERE teacher_ID = @teacher UNION ALL SELECT teacher_ID, assignment_ID FROM math_assignments WHERE teacher_ID = @teacher UNION ALL SELECT teacher_ID, assignment_ID FROM history_assignments WHERE teacher_ID = @teacher so if one of your tables somehow gets a new column, your query won't break randomly.
The column names and number of columns. Like if you have science_assignments with teacher_id, science_assignment as columns. And you have math_assignments with math_assignment, teacher_id as columns, you would need to do something like this: DECLARE @teacher INT SET @teacher = 385229 SELECT science_assignment as assignment teacher_id FROM science_assignments WHERE teacher_ID = @teacher UNION ALL SELECT math_assignment as assignment teacher_id FROM math_assignments WHERE teacher_ID = @teacher UNION ALL SELECT history_assignment as assignment teacher_id FROM history_assignments WHERE teacher_ID = @teacher I aliased the columns so they match. The output would be like: Assignment|Teacher_ID :--|:-- Science Homework|385229 Math Assignment 2|385229 etc.
If it's returning a ton of results, then you have a ton of duplicates on job,employee. Are there other columns in your table that are different that would allow multiple records with the same job &amp; employee? Like EffectiveDate or something?
I know it'll take some time to win everybody over :) 
Yes, it's important to keep in mind that they can be an optimisation barrier! Like with most coding problems though, I usually start with a straightforward readable solution using CTEs, and then optimise if there's a performance issue. 
Exactly! That's what I meant when I said you're locking yourself into a suboptimal technology choice. ORMs just add a lot of extra costs with dubious benefits. 
Yes, recursive CTEs are a great example - some things are impossible without them. I simplified the example in the post for better illustration. Realistic queries are more complex, e.g. I have an example in this post: http://korban.net/posts/postgres/2017-09-18-debugging-complex-postgres-queries-with-pgdebug/ If I did that sort of stuff with joins, I think it would be a nightmare! I find that the advantage of CTEs is also that I can break the query into a sequence of steps: first get the days, then extract the relevant positions for those days, then constrain them in some way etc. It reads sort of like a series of function calls then. 
Yep, this does what you need :)
I would rather prefer SELECT [CurrentPenalty] = CASE WHEN ( (@BillCycle = 1 AND SOURCES.Code IN ('PENPMT1', 'PEN1')) OR (@BillCycle = 2 AND SOURCES.Code IN ('PENPMT2', 'PEN2')) ) THEN 1 ELSE 0 END which makes it easy to track the column alias, the conditions and parenthesis are aligned and thus easy to compare, which also makes it easier to identify mistakes.
the thing you're asking for is a bad idea tell your teacher that in the real world, database schemas change over time, and attempting to use a query to non-join merge table results, instead of just teaching the students to differentiate with a column, is going to seriously hinder their ability to understand what they do people remember the first strategy you teach them. if you start with the wrong strategy, that's where they'll go for the rest of time
Can you give some sample data and the query you have so far? 
I assume you have some method of grouping your orders - by load it seems? On mobile, but could you do a MAX(Case when sbol is NULL then 1 else 0 end) as a 'LoadOpenFlag' additional column while grouping by whatever you were doing previously?
Updated post.
You were right thank you http://sqlfiddle.com/#!6/46f9c/1 
Hopefully this helps you out and is commented so you can understand what it is doing. The case you that isn't clear is how you want to handle if two bars carry the same number of beverages. The code below will randomly select one of the bars. There are ways to modify the code to apply a method to selecting which bar you get but how that is done is dependent on more than what is given. If you want to pull all the bars in a city that carry the highest number of beverages, replace the *ROW_NUMBER* with *RANK*. SELECT id , nombre , ciudad , cantidad FROM ( -- Generate a derived table of the beverage aggregate and row number SELECT b.id , b.nombre , b.ciudad -- Count beverages by bar in a city , COUNT(s.id_bebida) AS cantidad -- Generate a row number of the bars in each city based on highest number of beverages , ROW_NUMBER() OVER (PARTITION BY b.ciudad ORDER BY COUNT(s.id_bebida) DESC) AS rnum FROM PARRANDEROS.BARES AS b INNER JOIN PARRANDEROS.SIRVEN AS s ON b.id = s.id_bar WHERE -- Only get bars with low/medium budgets. b.presupuesto IN ('Bajo', 'Medio') GROUP BY b.id , b.nombre , b.ciudad ) x WHERE -- Filter our derived table for only the highest number of beverages in a city. rnum = 1 ORDER BY ciudad , nombre;
&gt;1.)Let's say you had a column in the Student table named 'Gender'. Datatype is char(1). Values for all rows for Gender are 'X'. &gt; &gt;You want to add a new check constraint to restrict the values to be 'M' or 'F'. If you tried to add that check constraint, it would fail because you have data that already violated the constraint (all those 'X' values). &gt; &gt;Here's the SQL to add the check constraint: &gt; &gt;ALTER TABLE STUDENT &gt;ADD CONSTRAINT STUDENT_CHK1 CHECK &gt;(gender in ('M','F') ) &gt;ENABLE ; &gt; &gt;What option could you add on this constraint that would ignore the existing values? In other words... the 'X' values are ignored, but any new insert/update will check the constraint...? &gt; Update student Set gender =null; This will satisfy the check constraint as long as it doesn't have a not null check. &gt;2.)For triggers... if you had an update trigger... and you wanted to abort the update (stop the update) what is the strategy? &gt; &gt;What is the predicate for the trigger? &gt;What action can you do to stop the update? Your assignment is probably to learn triggers but most dev groups will look at you silly if you try to do this stuff. Triggers for transactions are usually frowned upon. But there are two important keywords for transactions in Oracle. *Commit* = all good keep the transaction *Rollback* = we dun goofed. Get rid of the transaction. &gt;3.)For triggers... if you wanted capture information every time someone created or dropped an object... what is the trigger signature. https://docs.oracle.com/database/121/LNPLS/create_trigger.htm#GUID-AF9E33F1-64D1-4382-A6A4-EC33C36F237B__BABBJHHG &gt;4.)How do you write a trigger that would stop an update any time COST in COURSE is increased by more than 10%. &gt; &gt;I only want this trigger to fire if COST is updated. Add a when clause to the trigger. &gt;5.)For triggers... give an example of using the 'when' clause in the trigger signature. When should 'when' be used? Did you try googling? First result: https://docs.oracle.com/database/121/LNPLS/create_trigger.htm#GUID-AF9E33F1-64D1-4382-A6A4-EC33C36F237B__BABBJHHG *edit* I also like these examples http://psoug.org/reference/table_trigger.html 
If you want to abort an update inside a trigger you can make the trigger divide by zero. That will cause an error.
I don't know if my answer will be helpful, but if you looking for any integration tools, you should check Jenkins, Octopus and Bamboo. I personally familiar with Bamboo and seriously nice tool to work with. 
Are you trying to insert the IDs? What's the query and table structure?
I'm trying to merge two tables from 2 different databases,in means populate one table from another. When i'm trying to do that i'm getting above error, however i found a work around which is SET IDENTITY_INSERT [PROMO].Import_Table OFF SET IDENTITY_INSERT [PROMO].Export_Table ON but i don't think this is good practice. I tried clearing the local cache, but didn't work either. 
You didn't really answer my questions... IDENTITY fields are there to prevent you from doing that, intentionally. If you need to merge tables with identical or similar structure, *AND* you are sure there is no collision for the ID field, then it's fine to do IDENTITY inserts. But usually that would be a one time operation. If you are doing it more than once, I would say something is wrong. Like you should be using 2 tables and a foreign or surrogate key. It sounds like you should just be letting SQL generate the IDENTITY values and not including them in your INSERT statements. Is this just a migration or an ongoing process? You need to give us more than "I'm doing an insert and getting an error because there's an IDENTITY field". Well duh... that's good database design and I would hope you get that message when trying to insert ID values.
You can use the BETWEEN function. This finds the IDS of overlapping dates achieved by joining onto the same CTE twice, an EXISTS would also work. You need the AND D.ID != D2.ID to ensure the JOIN does not compare the same record against itself which would make every row appear to have an overlap. with data as (select 1 as ID, getdate() - 10 as StartDate,getdate() - 5 AS EndDate UNION select 2 as ID, getdate() - 6 as StartDate,getdate() AS EndDate UNION select 3 as ID, getdate() as StartDate,getdate() + 5 AS EndDate UNION select 4 as ID, getdate() + 10 as StartDate,getdate() + 20 AS EndDate ) SELECT D.ID,D2.ID FROM DATA D JOIN DATA D2 ON D.StartDate BETWEEN D2.StartDate AND D2.EndDate AND D.ID != D2.ID
It's SQL **Server** AlwaysOn actually. 
Please don't do that. raise_ application_ error(-20001,' there was a problem') or rollback like I mentioned. obfuscating your intent by dividing by zero is a bad idea IMHO.
That seems like a better idea. :) I don't know where I read the divide p√• 0 thing or what db it was referring to. 
Identity_Insert is session specific and can only be applied to one table at a time. If you are repeatedly encountering this error, my money would be on a mistake somewhere in your code (edit: such as turning it on for another table and not turning it off after your insert, or turning it on earlier in code and repeating the command later). If you need to use Identity_Insert, you should follow a process of 1) turn it on, 2) do your insert, and 3) turn it off. 
Can you post the query plans? https://www.brentozar.com/pastetheplan/
I don't have access to them right now unfortunately.
Looks pretty solid to me. If you are on SQL Server 2016 you can use DROP TABLE ... IF EXISTS syntax instead of checking for object. I wouldn't say that's necessarily a performance improvement but something to keep in mind when dropping tables in the newer versions of SQL Server.
Wow, I didn't know that. We have a mixture of servers all the way down to 2008, but the primary reporting server that I use is 2016 so this will work great for me.
thanks, now that i have read about the clause ver and about partitions this is mucho more easy to think about a solution for this kind of problems! i tried running it but i get this parentheses error ORA-00923: palabra clave FROM no encontrada donde se esperaba 00923. 00000 - "FROM keyword not found where expected" *Cause: *Action: Error en la line: 25, column: 21 i do not get why! i mean i see every parentheses it-s correct.
Not sure what is causing that error. Line 25 doesn't have a column 21. Did you change the query any? Also, what happens if you run just the derived table on its own? SELECT b.id , b.nombre , b.ciudad -- Count beverages by bar in a city , COUNT(s.id_bebida) AS cantidad -- Generate a row number of the bars in each city based on highest number of beverages , ROW_NUMBER() OVER (PARTITION BY b.ciudad ORDER BY COUNT(s.id_bebida) DESC) AS rnum FROM PARRANDEROS.BARES AS b INNER JOIN PARRANDEROS.SIRVEN AS s ON b.id = s.id_bar WHERE -- Only get bars with low/medium budgets. b.presupuesto IN ('Bajo', 'Medio') GROUP BY b.id , b.nombre , b.ciudad
Does it have to be a text editor? I think visual studio code can connect to your RDBM and would work for your purposes. May I suggest that you use APEX or lookup APEX? It's free prettifying auto-completing software that plugs into your SSMS. There's also poor man's sql formatter. It's not as good, but it works well. 
Check out VS code from Microsoft. https://code.visualstudio.com
So ... Testing reveals that the use of the varchar(max) type for the comments (which is actually necessary) exponentially slows down any CLR or standard string split function one can google. Looks like I'm back to sequential REPLACES using a while loop to remove exclusion words ... and a While loop to do that to each of the comments. Doh. Thanks for the idea!
This is your best a newest friend.
The idea is to look for a start date that's between start date and end date of another range.
select srac, carton, weight, StoreCount, Lane, (case when stuatus &lt; storecount then 'Open' else 'Close' end) FStatus from ( select srac, sum(numofcartons) cartons, sum(weight) Weight, count(*) Storecount, Lane, count(Case when Bol_number is null then null else 1 end) status, from table1 group by srac, lane )
Have you tried turning it off and on again? 
What you are describing in your query is a window function (wrong syntax) and it is not available in ms access; therefore, you need to write the query differently. Remember: Count(1) = Count(*) There mapbe a preformance drop from the nested loops below depending on the size of dataset. Select ageGroup ,Sum((select count(1) from table t1 where SavingsRate &gt;= [Enter threshold] and t1.ageGroup = t0.ageGroup)) Maxing ,Sum((select count(1) from table t2 where SavingsRate &lt; [Enter threshold] and t2.ageGroup = t0.ageGroup)) NotMaxing From Table t0 Group by ageGroup *EDIT:* Removed part about Count([column name]) = Count(null) being equal, this depends if you are following ASNI nulls or not.
Take my upvote, comments are underrated in the place I work.
not sure if this would be more performant than sub queries, but to truly get segregated counts in 1 query is to use case statements. count and sum can be interchangeable here. select sum(case when [col1] &gt;=x then 1 else 0 end) as Maxing , sum (case when [col1] &lt; x then 1 else 0 end) as NotMaxing from PAExtract 
Loops are bad. Think in sets of data. Use something like select top 5. Order by date. Then loop over the result in your application.
I'm talking about requesting a list of user posts AND a list of comments associated with those posts, and array of arrays. How does select top help here?
Sorry, I misunderstood a bit. I thought you were asking how to do the loop in sql, which is rarely needed. There is not really a concept of array in SQL. There sort of is, but it's really just like declaring a table object and defining the fields. So effectively the same thing as a table. Data sets are manipulated to create other data sets. In your case, you can use JOIN. select p.id , p.body as 'Post_body' , c.id as 'c_id' , c.body as 'Comment_body' from posts p join comments c on p.id = c.post_id This is a single query to sql. But if you don't include a `where` clause it could also be a huge set of data. So you probably want to filter on user_id with each query. I don't know the rest of your table structure or how your front end is supposed to work, though.
Yeah, I'm aware of this and mentioned it in my post. The issue with this is if I need to group by the post ID for whatever reason, then I lose all the comments I wanted to request. In addition, the result also becomes harder to parse, you'd end up with something like let post map = hash map of post IDs to post objects for row in results: if row.p.id is in post map: post map.get(p.id).comments.insert(parse_comment(row)) else: post map.insert(parse_row(row)) post map.get(p.id).comments.insert(parse_comment(row)) Not to mention in my actual example I'd need to join it with author info, so I'd need a double nested join - SELECT ... FROM posts LEFT JOIN comments ON comments.post_id = posts.id INNER JOIN users AS u ON posts.author_id = u.id -- Post author LEFT JOIN users AS u2 ON comments.author_id = u2.id -- Comment author GROUP BY comments.id -- Eliminate the duplicates....?????? 
Group by is used for aggregate functions. So if you're not including an aggregate (like COUNT), then what are you grouping for? And you could do that as a separate operation if you need to display the count of posts by a user. If it's a web application you probably want to balance the number of requests vs the amount of data being passed around. I'm sorry I don't know the language (Python?) very well so I can't really help with the for loop there. What are you actually filtering on when you query? Is it just random set of posts? Posts by user? Posts by date/time? How many results do you need to be returned? Every post in the table?
why do you need to group anything in this scenario? posts and comments imply details... maybe if you wanted posts plus a count of comments, then yeah, some grouping is required but grouping to eliminate the duplicates? what duplicates?
this -- Count(1) = Count(*) = Count([column name]) = Count(null) is **wrong** the first two are always equal the third may be equal the first two, or less the fourth will always equal zero
 SELECT [Age Group] , SUM(IIF([Total Savings Rate %] &gt;= [x],1,0)) AS Maxing , SUM(IIF([Total Savings Rate %] &lt; [x],1,0)) AS NotMaxing FROM PAExtract GROUP BY [Age Group] 
Oh yeah, I got confused with the joins, there wouldn't be any dupes!
It might be helpful to know what your final output / goal is here... Depending on the version of SQL you're using, you can get something like this fairly easily: post_id|post_body|comment_body_joined :--|:--|:-- 1|post number 1|comment 1 on first post;comment 2 on first post;comment 3 on first post 2|post number 2|comment on second post 3|post number 3|(null) --MySQL SELECT p.id post_id ,p.body post_body ,GROUP_CONCAT(c.body ORDER BY c.id ASC SEPARATOR ';') as comment_body_joined FROM posts p LEFT JOIN comments c on p.id = c.post_id GROUP BY p.id, p.body SQLFiddle: http://sqlfiddle.com/#!9/d9faa/18
Ohhhh I didn't know about group_concat, that's super cool!
judging from the name, InvoiceTotal would appear to be a numeric column of some type, not a string, so using a string function on it is problematic what datatype is it?
fixed
Hello, no i did not change it. and if i just run the derived table y get this error. ORA-00933: comando SQL no terminado correctamente 00933. 00000 - "SQL command not properly ended" *Cause: *Action: Error en la line: 8, column: 24 
It's datatype is set as money
it wasn't really clear from your original post, but what's the problem? you just don't want to see the dollar sign? SELECT CAST(InvoiceTotal AS DECIMAL(11,2)) AS no_sign FROM ... note that the MONEY datatype doesn't actually store a currency sign, that's provided by whatever utility you're using, e.g. SSMS
I need to see the currency sign in the output. I could enter it in manually with '$' but that doesn't seem to be what the question is asking for. I think it's asking to use the trim function to get the output that includes the currency sign because the column alias is called 'InvoiceTotalAsCharNo Trim'. This is my first use of the Trim function and I'm very lost.
If I am reading it correctly, you want to format it to look like this: $ ________ 3813.33 (with the underscores replaced by spaces) You could use this: SELECT '$' + REPLICATE(' ', 10 - LEN(InvoiceTotal)) + CAST(InvoiceTotal AS VARCHAR) from &lt;TableName&gt; Just replace "10" with the number of characters you want in the entire string. [Documentation for the REPLICATE function is here.](https://docs.microsoft.com/en-us/sql/t-sql/functions/replicate-transact-sql)
You can use Management Studio (SSMS) and, if you write your queries a certain way, it has pretty good auto-complete. The trick is to write your FROM argument first, then the DB knows the tables and fields available to it when you go back to your SELECT. 
I played around with it. I didn't realize Oracle doesn't allow the use of AS to rename tables. Remove that keyword from your table names and it should work appropriately. I tested it [here](http://sqlfiddle.com/#!4/7469e/1).
THANK YOU! it worked!! edit: btw thanks for the sqlfiddle!
It says NoTrim, so it sounds like they don't want you to use the Trim function. Perhaps the next question has you trim it off?
I used to and still use Management Studio pretty regularly, but in the past year I've really fallen in love with [datagrip](https://www.jetbrains.com/datagrip/) by JetBrains. Auto format, auto complete, schema synchronization, smart enough to know if you add a column you can use it later in the script, calls out any errors very clearly, highlighting of what column names when writing inserts and analyzes foreign keys to auto complete join statements.
Hi, sorry for the late reply. Yes you are right, i'm trying to insert the IDs, and also at the same time i'm importing from X table to Y, where Y table is empty, so there can't be in conflicts between IDs. And both tables are identical and have the same structure. However after i stop and restart the SQL service it worked just fine, but i don't understand why clearing local cache doesn't work. You have to manually stop the service and restart it again. Any thought about this ?
&gt; Hey bro, yes i tried that as well. but i also did another thing, i completely stop the SQL services and restart it again then ONLY THEN it got executed. What i don't understand is that why clearing local cache doesn't work.
Not 100% on what you want the output to be here to be honest, but if you're wanting to get rid of the empty spaces and keep the dollar sign surely a replace will do it? Replace(invoicetotal,' ','')
I'm pretty sure /u/Kiterios answered that question here: https://www.reddit.com/r/SQL/comments/71i78r/identity_insert_is_already_on_for_table/dnb8c6k/ You need to explicitly turn identity insert on or off for the session so you should do it in the same query you do your inserts. Turn on&gt;Insert records&gt;Turn off.
SQL doesn't display $_________________3813.33 by default. Are you sure that's not a view? The data in the underlying table will be a data type format. If it's a string the string could certainly be stored like $________________3813.33 but TRIM won't do anything. TRIM functions remove blanks. And even if those were blanks *after* the $, they wouldn't be removed because TRIM would only remove the *leading* blanks, not the ones in the middle of the string. Not only that, if those are actually the underscore character _ then TRIM functions would do absolutely nothing at all no matter what. If they are blanks like `"$ 3813.33"`... TRIM still does nothing. Again, those spaces are in the middle of a string. TRIM removes leading and trailing spaces. LTRIM removes leading spaces. RTRIM removes trailing spaces. I think you have quite a bit of confusion going on as to a) which tables and views you are actually querying and b) what the TRIM function actually does and c) what data types are in SQL. If this is a money column the data is a decimal and if you are getting a leading $ with zeroes then the query (view or whatever) is modifying the data somehow. What is the *actual* question you are being asked here? Simply to add a $ to the beginning of the query with some spaces? SELECT '$ ' + CAST(InvoiceTotal as varchar(50)) Result: `$ 3813.33`
Yeah this is way advanced for an introductory class. Surely they are not explaining the requirements well enough and they need something simple and they are confused, especially if they are just learning TRIM functions. And if I'm reading the OP correctly, they seem to think LTRIM *adds* blanks or something. 
Don't think so. Technically, the result set is no longer part of the original tables. Also, what if you had a column like `SELECT 1 as ColName`? That doesn't come from a table. How would it be displayed? The result is like a dataset so more like a spreadsheet with no link to the previous data other than those relations that exist between the data itself. I went ahead and looked through the options but there is really only the option to include the query in the result set--not the original tables. You might be stuck aliasing here.
Thank you! I'll take a look into some of these projects and see what I can do!
CTRL + SHIFT + R This keystroke has saved me many irl strokes.
They want you to use the STR() function. It will convert a numeric type, which money is, to a front padded string. TechNet page about STR(): https://technet.microsoft.com/en-us/library/ms189527%28v=sql.110%29.aspx?f=255&amp;MSPPError=-2147217396
1) Use Enable Novalidate (no need to update all the values to null as suggested by /u/nepobot if you don't want to) but in the long run I agree with setting them to null if they are unknown (or assign meaning to 'X' as unknown and allow it in the check constraint). It's better to have validated constraints. 2) Raise an exception 3) I think you want to know about schema level triggers (they can track ddl) 4) Use a when clause in the trigger to limit it to when cost is being updated. Then write a some conditional code so if the update is more than 10% then raise an exception 5) check out links posted by others here
As far as I know the is no option to do that. I would join on information_schema.columns &amp; information_schema.tables if you need that data as part of a result set. If you don't mind me asking, why do you need this?
Hint: What does `SELECT AVG(grade) FROM grades` return? Hint2: You already have a query that returns the 2 students you want to use as a filter criteria.
 SELECT students.first_name, students.last_name, students.email_addresses, AVG(grades.grade) GPA FROM students JOIN grades on (students.id = grades.student_id) GROUP BY students.first_name, students.last_name, students.email_addresses HAVING AVG(grades.grade) &lt; 50;
I'm realizing now that the above query is not very pleasing to the eye. Here's the basic set up. SELECT Farmer, City FROM Apples UNION ALL SELECT Farmer, Planet FROM Oranges For some reason, only Farmer and City are displaying. What happened to Planet? Do all of the columns have to have the same names?
That query will return all of the data from planet but it will be in the city column. 
Union do not add columns, it will simply append 2 datasets together, as long as each field data type match. In your example, they will appear as 2 rows and be indistinguishable. (Assuming 1 row per union sub query. Joins are used to add columns, but depending on relationship and cardinality it could duplicate information across multiple rows which may be correct or not depending on use-case. You can use group by as well, which may be suitable in certain situation, using this form: it will only return a single row per teacher_id, which may or may not be what you want: Select teacher_id , max(math_assignment_date) as Math_assignment_date , max(math_special_assignment_date) as Math_Special_assignment_date From ( Select teacher_id , math_assignment_date , cast(null as date) as Math_Special_assignment_date From.... Union Select teacher_id , cast(null as date) as Math_assignment_date , Math_Special_assignment_date From.... )d Group by Teacher_id 
Is that by design? What if I added a third UNION ALL, like follows: SELECT Farmer, City FROM Apples UNION ALL SELECT Farmer, Planet FROM Oranges UNION ALL SELECT Farmer, Galaxies FROM Bananas In this case, will Planet and Galaxies land underneath the City column? Also, Farmer happens to have the same name and format between each of the columns, but it is coming from two different DBs...Is that an issue?
Thanks for the intel. I'm only two weeks into learning this language, so it's a bit complex. But I'll give it a try! Do you mind if I ask you an Order By question?
That is how union queries work. They are ordinal and the columns take the title of the first query in your union. Every query following just puts the results under column 1, 2, etc. regardless of names. Stop thinking about the names that is where you are getting hung up... SELECT Farmer, City FROM Apples UNION SELECT Farmer, Planet FROM Oranges This will return all farmers, cities, and planets where all farmers are under column 1 and all cities + planets are in column 2. Note that if you have a farmer + city combination from the first query that is the same as a farmer + planet from the second query it will only return one. (IE: it won't duplicate any values in the union). By adding ALL (UNION ALL) it just means that it will return ALL combinations in the 2nd query even if that combination exists in the first (ie: you could have duplicates). It sounds like you are trying to get 3 columns.. farmer, planet, and city.. in that case you would do SELECT Farmer, Planet, City FROM Apples INNER JOIN Oranges ON Apples.Farmer = Oranges.Farmer
Yes planet and galaxies will be in the City column, unless they are incompatible types with City, in which case you'd get an error. Farmer being on a different DB is OK. A union is multiple unrelated queries placed on top of each other (into columns defined by the first query) Perhaps you want to do a full outer join instead. SELECT COALESCE(a.farmer, o.farmer, b.farmer) as farmer, a.City, o.Planet, b.Galaxies FROM Apples a FULL OUTER JOIN Oranges o on a.farmer = o.farmer FULL OUTER JOIN Bananas b on COALESCE(a.farmer, o.farmer) = b.farmer Or, follow this guy's advice: http://weblogs.sqlteam.com/jeffs/archive/2007/04/19/Full-Outer-Joins.aspx I never actually use full outer joins but thought it might apply here. 
Do you know how I can do the following in PostGreSQL. The Declare / Set set up is not supported in PostGreSQL... DECLARE @Dog INT SET @Dog = 555 SELECT * FROM Database WHERE Column = @Dog
Another question. I want to avoid continuing to create new Threads. Does anyone know how to do the following in PostGre? Declaring and Setting Variables appears to be a mySQL thing instead of PostGre... DECLARE @Dog INT SET @Dog = 555 SELECT * FROM Database WHERE Column = @Dog
Go nuts :-)
I am working with a data warehouse application, to over simplify it, you drag and drop tables, joins, filters, etc, define your select criteria ; then when you run it splices all the code together for you. The downside is the error reporting is usually not the greatest so I will usually copy and past the code into SSMS and then debug, ie: I am missing ) etc. When I am getting duplicate records one of the steps I like to perform is to replace all the system generated select conditions with an * so I can see all the data joined. It helps me look for where the my duplicate record is getting generated (usually from a join). The table I was working with this morning had over 20 tables joined together so when you change the select to select *, trying to figure out what columns belong to which table is difficult. This is would be where having SSMS put table name above the column names would be nice. But it looks like that is not an option so I will just have to make the best of it.
*The result is like a dataset so more like a spreadsheet with no link to the previous data other than those relations that exist between the data itself.* That makes sense. *I went ahead and looked through the options but there is really only the option to include the query in the result set--not the original tables. You might be stuck aliasing here.* Thanks for looking, if nothing else I confirmed I am not missing something.
SQL won't really do charts by itself. Have you tried using power query or Power BI directly from Excel? You can keep the data in SQL and use Excel for the charts and Pivots. Power BI does lovely charts. SSRS is a decent reporting tool as well but those other 2 will be easier for you to learn since you know Excel. SQL can do pivots but it's not very intuitive for a newbie. It's a bit more advanced query so we would need to see your columns or an example of what you are looking to do before giving you directions on the query. To do a calculated column you can either add a new column to the table that uses a formula as the value: https://docs.microsoft.com/en-us/sql/relational-databases/tables/specify-computed-columns-in-a-table You can add these columns to your existing table too. Or you can do the calculations yourself in a query as a calculated column (probably a `CASE` statement), and save the query (or create a view using the query), then use the view/query as the source for whatever you choose for the charts.
Thanks but I am still not there. Perhaps it is because our professor's cryptic hints say we should use x, y, and z. Are you saying SELECT AVG(grade) FROM grades returns a single value? I want the average for each student.
Thanks so much! That was it. My query makes sense to me but I see why yours is better (lol, and is the one that actually works...).
You don't have enough information. In order to get time spent, even for one row, you need a start time and an end time. 
Thanks I gave Power BI a go just connecting to the spreadsheets. I get a new spreadsheet from the client every week with new rows but the columns are the same (same name and order). I understand the DAX calculated fields in power bi so its probably not too hard to do the calculations (and the new columns these calculations will reside in). However, when i receive a new spreadsheet from my client the next week, would I have to connect to the new spreadsheet and redo the calculations and columns?? I would think there is a way to save the calculations and easily move them to the new spreadsheet connection. Cheers for your answer 
One question is if you are loading data from other tables, do you need the ID column to be an identity column at all? Are you doing inserts into table Y outside of the merge? if yes, then you need to turn identity insert off before the merge. 
The only additional process would be for you to insert the data into SQL when you get it from the client. That can be done a few different ways, like a bulk import, openquery command or using the import/export wizard. If you use the import/export wizard and save the package you can reuse it weekly too. Then connect Excel/Power BI and any other software you are using to your SQL database and query the data. Now you don't have to store all the data in Excel but you can use all the other tools (including Excel). You'll still learn a bit of SQL to start but it should be pretty easy, especially if your table structure doesn't change.
I am not very familiar with SQL server so I can't give the exact syntax, but I would solve this issue with the following approach: Assuming the price is valid until the next row in timeline, you have to find the time difference between each row and the next. And then summing it up. SELECT -- Time difference between current and next row, summed up and grouped by price SUM((SELECT min(utc) FROM pricetable WHERE utc &gt; t.utc) - t.utc), t.price FROM pricetable t GROUP BY t.price 
Keep in mind that your sub query doesn't tie back to your main query. It will return the average of every grade in the grade table. You could add an alias to your query and make your first grade g and your sub query grade g2. Then add Where g. stridentid = g2. studentid to the sub query. That will make it a correlated sub query. 
Thanks to everyone‚Äôs comment here. I‚Äôve looked at the problem carefully and based on the info, I technically don‚Äôt have enough info to figure this out. Looking at time diff would be the obvious decision. However, the prices and timeframes as entered are technically independent from another. Meaning in that, if the price of $48.00 is entered in the morning at say 10am and then fluctuates to other values only to retrace back at $48.00 again around 5pm, you can‚Äôt therefore say time spent on $48.00 took 7 hours. So looking at it carefully and going back to my original question which was, how can we determine how much time was spent on a specific price? I think the best answer is to assign a value of 1 second per entry. Then SUM all seconds grouped by price. At most, it really just takes a second to actually enter the record at that price. So naturally that would be the time spent. Boom! :)
Wait I think it did. The GROUP BY is what the sub query average was calculated on. I got the right result w/ the above query.
What about taking a shortcut and giving the file a generic name like data.csv? Then just swap the actual file as needed, open PowerBI and refresh the data.
This should help you out with your Anonymous Block: http://nixmash.com/post/using-postgresql-anonymous-code-blocks
&gt;What I want to do is to create calculated fields from the dataset. E.g time difference between two columns as a new column 2) if the time between the two columns is less than X minutes, then tag it as "1" otherwise "0" 3) vlookup like functions. SELECT start_time, end_time, CASE WHEN (end_time - start_time) &lt; 10 THEN '1' ELSE '0' END AS calculated_column FROM table_a; &gt;Next I want to be able to do analysis similar to pivot table in excel. Like slice and dice data to create different views. Microsoft TechNet - Using PIVOT and UNPIVOT: https://technet.microsoft.com/en-us/library/ms177410(v=sql.105).aspx &gt;Lastly I want to be able to create time series charts etc for the pivot table like views. [SQL Server Reporting Services (SSRS)](https://docs.microsoft.com/en-us/sql/reporting-services/create-deploy-and-manage-mobile-and-paginated-reports) or [Microsoft PowerBI](https://powerbi.microsoft.com/en-us/)
 There is nothing wrong with your group by. It's your having clause that's the issue. Select avg (grade) from grade is a compete query all its own. Unless you correlate it to the outer query (the one with the group by), it is getting the average of every grade in the table. Make sense? 
And the above query is correct. I'm pointing out what your original query is doing.
I think context is important here. are each of these entries a "pricepoint" ie bob paid 5 bucks for an Item at 3 pm , marry paid 6 bucks at 3:07 pm? with a sequential order? 
What's the error message? 
 #1064 - You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'GO CREATE PROCEDURE CourseList() AS BEGIN SELECT * FROM courses ' at line 2
Is your GO on a separate line? What happens if you put it on a separate line?
[removed]
Yes as I said they are independent and the UTC field is basically the time stamp.
Good bot. 
It is just reddit formatting is annoying so didn't bother with it for such a short command 
*It is just reddit formatting* *is annoying so didn't bother with it* *for such a short command* ______________________________________________________________________________ ^^^-english_haiku_bot
Not a big mysql user, but in procedures there I don't think you use AS Try removing that from your statement?
If it‚Äôs anything like ORACLE (which I think MySQL is) you can‚Äôt simply run a select statement as a procedure. You have to either: SELECT column INTO variable or outbound parameter FROM table; **this INTO is not like sql server where it creates a new table. INTO here is assigning values. Or Declare a cursor and set it to be the output of a select statement, then return the cursor as an outbound parameter. In other words, you need to have the result of the select statement either populate variables/parameters, or populate a cursor. Procedures with no outputs (outbound parameters) are usually used for DML or to run other procedures. https://dev.mysql.com/doc/refman/5.7/en/create-procedure.html
Good bot
Thank you Require\_More\_Mineral for voting on I\_am\_a\_haiku\_bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
I've hardly ever done interval math, so I'll do some handwaving there, but the overall approach should be fairly straightforward. Roughly, something like this should get you started select price , sum(TimeStampDiff(minutes, end_time, start_time)) mins_at_price from ( select price , utc as start_time , coalesce(min(utc) over (order by utc rows between 1 following and 1 following) , current_datetime) as end_time from MyTable) group by price For each row, it gets the next UTC (or the current time for the last row; you can put in any timestamp you want there), then gets the difference between that and the current row's UTC, then sums that over the set. There's no need to see if the price changes since all we're doing is summing the time for each price. If you need to do that, you can use this same technique to flag rows where the next row has a different price, then throw out all the unflagged rows. You'll have to play around with the TimeStampDiff part for whatever your platform needs.
~~In SQL server, you can only create one stored procedure per script. No clue on MySQL, but it's a simple enough check.~~
That's not true. You can only create one procedure per batch. You can separate batches using GO and therefore create as many objects as you want in one script. 
I would talk to an accountant and find out what the tax implications are so you have an understanding of your costs. You'll want to also look into insurance. As far as negotiating your rates. I would have an hourly rate for onsite and an hourly rate for remote work. As far as travel I bill by the kilometre, if I'm flying then we usually bill at a travel/standby rate. Accommodations are charged back at cost and we usually bill a per diem ($50 per day) for meals and incidentals. 
quick clarification please -- which rows do you want to delete? Loan or Customer or both? also, where do you find OverdueFee?
Sorry Loan and Customer are two separate tables. Loan contains LoanID, CustomerID, InventoryItemID etc.... and the Customer Table contains CustomerID as well as others that dont really matter in this case. The OverdueFee is a column within the Loan Table with the REAL data type
I am wanting to remove all records when the Customer has only ONE overduefee in the Loan table so im guessing I have to somehow JOIN both the two tables by the CustomerID?
My apologies, I believe you're correct. I don't write many stored procs.
I took some initiative when I was working as an IT tech doing help desk stuff, and built some reports management were hoping our ticketing system had built in, but didn't. After that, I was asked from time to time to help out with small tasks our DBA didn't have time for. After doing that for a while, I was moved permanently into a jr DBA role and started learning more.
Delete l From loan l Join customer c on l. customerid =c.customerid Where latefee &gt; 0 Group by c.customerid Having (count(*) = 1) 
I never chose SQL as a career, but I did decide after landing a job as a report writer / data analyst that I wanted to pursue a career in data management (DBA, DB Developer, BI, Data Analysis, etc.). To me, SQL is just a tool in my tool chest along with ETL, Python, and other related tools. In other words, SQL is a means to an end, not the end for me.
So the career focus isn't so much SQL but rather data management? Very awesome.
You got it. More than anything, companies just want to manage their data and understand it better. They want to use it to help them make decisions, or alert them to issues or opportunities. SQL is the main way we interact with databases to retrieve or analyse their data, but there's also file-based data, online APIs that deliver data when called, streaming data from sensors, and more. Being able to help companies manage that data and orchestrate analysis that yields usable results is really the core competency that I strive for, no matter how I have to interact with the data.
How would one work towards a data management career coming from a financial background and SQL skills?
Hey there, I tried that but i get the error near "JOIN: syntax error: Im gathering thats because SQLite doesnt like using the JOIN in the DELETE function if im correct there? Should i go about trying to put it within a subquery?
 can you use CTEs?
Yes
 change the delete above into ; With d as ( Select customerid,loanid From loans l Where latefee &gt; 0 Group by customerid Having (count(*) =1) ) Delete from d
Pro tip: get what you want into a select statement first. Wrap it wth WITH alias as () ave update or delete. The only caveat is that you can only affect one underlying table at a time. 
even if this works, it doesn't delete customer rows as per [OP's semi-clarification](https://www.reddit.com/r/SQL/comments/725gp3/needing_some_help_removing_sql_records_within_a/dnfv2gk/)
all rows of both tables? I THINK NOT all rows of customer table? I THINK NOT only those customer rows of customers that have only 1 overduefee? I THINK NOT (because that would orphan the loan rows) all loan rows of customers that have only 1 overduefee? I THINK NOT (because that might remove other loans that don't have an overdue fee) so, OP, care to give your specs a little more careful thought?
IMHO there's no such thing as "career in SQL" SQL is merely a piece in the big picture. 
I wouldn't think you ever wanted to delete the customer, and really the loan either. Thinking like a business user, i would want to update the late fee to 0 of they've only been late once as a one time courtesy. You almost never delete data in a production environment.
Which one of the onsite or remote work costs more? If so, how much difference?
We usually charge a little premium for onsite.
Yep, no need to apologize. dunno why you got down voted though.
Like others already said.. it's not the career in SQL that you choose.. It's just that SQL is a part of the career you choose.. For me, the career decision was going into business intelligence, what makes SQL an important skill.
There are lots of different places to learn SQL: 1. You could take a SQL course on codeacademy 2. You could take a SQL course online (e.g. EdX) 3. You could read a SQL book (e.g. TSQL fundamentals), install a local instance of SQL on your computer (or use something like Azure) and use it to practice. I've tried all three and personally preferred (and generally prefer) the third approach. I've also heard good things about SQLZoo, but I've never personally used it.
I got a two year degree in computer networking and I'd been doing tech support stuff for about 8 years. I was happy at my job, but I just started meeting with a couple other departments in the company to ask them what they did and how they served the needs of the business. One was QA and the other was Business Intelligence. The BI group sounded like the kind of job I was trying to make all my previous jobs. Finding trends, being more efficient, helping to make data driven decisions rather than guesswork or instinct. Loved it. The manager loved beer, so I mentioned I wanted to start getting into beer more and he offered me the job, knowing full well I had never done SQL or anything like it before in my life. A buddy helped me with a practice homework assignment they gave me and I was in. Then, I did online tutorials, looked at queries that were already written, and tried to pretend I knew what I was doing. After awhile, I got pretty good, I really really enjoyed the work, and I got promoted to a mid level analyst and then to a senior analyst. I like what I'm doing better and there are the same kinds of stresses I had before. Leadership won't listen, salesmen ask the impossible, people don't understand what you're talking about, and you feel overlooked and under utilized. That's just the same wherever you go, in my opinion. I'd love to stay an analyst for a long time. Maybe something higher like a director or manager, but still helping to make the decisions based on sound analytics. Anything that kept me from that would make me sad, I think. 
I have heard great things about SQLzoo!
.Hey there, check these two links: https://franchise.cloud/ https://www.dataquest.io/blog/sql-basics/ All the best!
Thank you for your answer! Will def check that!
I think online learning works best for me. Ive heard a lot about SQLzoo too. Thank you for your answer!
I used Treehouse. 
Uhh BEX is an all GUI system. But the answer is of course not everyone hates SAP and Export to Excel is still the most popular button. You can also run ODBC SQL against SAP. Are you doing BW too? SAP HANA? Do you know anything about data warehousing? In memory analytics? Column oriented databases? Good opportunity to up your skills. 
How can I run odbc? We've been told it's not possible and we'll have to get used to Bex. Is there an easy way to get bex to output to Access like a pass through query? 
it sounds like you and I are in similar spots and moving toward similar things. I'm a cost accountant, and I've talked my company into sponsoring a master's in info systems for me. I'm about 1 year into it, and I have found a couple of ways to implement what I've learned into day-to-day stuff. The best example is putting our E&amp;O listing into an Access file as opposed to Excel, because (imo) Excel was not the right tool for it. It was a struggle. I really didn't know anything about Access except a couple of very basic fundamentals. It took about 3 months for me to get it to a place where I was happy with it. So, I recommend looking for stuff you can port into Access, and then practice writing queries in SQL (as opposed to the Query Wizard). One step at a time.
I started with SQL and report writing. After that I started learning ETL, data warehousing, OLAP, and big data. I'm also trying to brush up on Python and Tableau. That track seems to have worked out so far, so that's an option for you. If you work in a SQL Server shop, then you can take some Pluralsight courses on SSIS, SSRS, and SSAS. I would take up Power BI too. Outside of a Microsoft environment, the tools change a little, but the techniques don't. Talend and PostgreSQL are my preferred choices for an ETL tool and database there. There's a whole myriad of options available to you though. I would recommend hanging around /r/BusinessIntelligence for some more ideas.
**Here's a sneak peek of /r/BusinessIntelligence using the [top posts](https://np.reddit.com/r/BusinessIntelligence/top/?sort=top&amp;t=year) of the year!** \#1: [Do you spend too much time building reports with Google Analytics data?](http://www.infocaptor.com/dashboard/do-you-spend-too-much-time-building-reports-with-google-analytics-data) | [1 comment](https://np.reddit.com/r/BusinessIntelligence/comments/5xnc2d/do_you_spend_too_much_time_building_reports_with/) \#2: [How a SQL Query made us $2,838 in 15 minutes](https://blog.usefomo.com/how-a-sql-query-made-thousands-15-minutes/) | [1 comment](https://np.reddit.com/r/BusinessIntelligence/comments/60b7zg/how_a_sql_query_made_us_2838_in_15_minutes/) \#3: [Would you be interested in a detailed post on building Automated analytics for Salesforce.com?](https://np.reddit.com/r/BusinessIntelligence/comments/6rbio6/would_you_be_interested_in_a_detailed_post_on/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
Sure? There are drivers and a full .net sdk for NetWeaver so you can connect in, ask for the results of a BEX query (with parameters if you want) and populate a table with them.
I will investigate tomorrow. Hopefully become a hero. Can you help me understand the benefits of SAP from a reporting and analysis perspective?
No worries. Just one of the quirks of Reddit.
Umm well in theory it provides a strong data integration platform for you with a virtualization layer on top. So you get a "single version of the truth" where everyone is using the same system, and your sales and operations and finance databases are all conformed together. And you get business friendly entities and attributes, hierarchies, logic etc again with reuse throughout the org. And you get a ton of financial and ERP style reporting out of the box. SAP also has a lot of BPM and financial planning baked into it so you get approval workflows, invoice generation, regulatory compliance, etc. It's not the worst tool ever but it's pretty clunky and you end up doing a lot of bikeshedding and yak shaving with it. Plus it was probably a pitched political battle to get it in there so you've got pressure to make it work and potential fallout if it doesn't. 
Not the answer you want, probably, but the best place to learn SQL is on the job. I love taking classes and reading books, but having a project to work on is the best bet. If you can't be on the job yet, I recommend picking up an individual project that interests you, and learn on that project.
I agree with you, but I think this works best if you have some initial knowledge. Right know I would just stare at the screen and do nothing. Ty for your answer!
You are not the rightful heir to the SQL throne. 
Maybe I was a little missleafing on the tldr thing hahaha
I like the way you think. 
I am about halfway through on SQL Zoo after having done codecademy SQL. I wish I would have started with SQL Zoo, it pushes me to think more critically and learn/chain functions. I would most def recommend it.
I want to throw in my 2 cents by combining a couple of suggestions I already saw. Start with sqlzoo and codeacademy, these will give you a great foundation and teach you the syntax. If possible try to time it so you learn near the time you can do some real sql at work. This is what I did. I had no real world sql experience but by the time I had to do it at work it was really easy to learn, and build upon what you know. Last thing, when you're facing real problems, Google them (specifically look for stack exchange answers). Challenging yourself, attempting to figure out on your own, then googling solutions is the best way to learn. Best of luck!
Go for a **practice** oriented tutorial. The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An advance course is also available. 
Great answer tight there, ty mate!
If I understand you correctly, you want to have the last "Create_Assignment" for each Teacher_ID, which would be done using GROUP BY: SELECT Teacher_ID, MAX(Create_Assignment) FROM Database GROUP BY Teacher_ID To export the content to csv, please check out the COPY command.
tl;dr: &gt; The main topic for the blog post was related to scanning a large table with 100 000 000 records using offset with a primary key (keyset pagination). Overall, 3 different approaches were reviewed and tested on the corresponding dataset. I recommend only one of them if you need to scan a mutable large table.
Here is kind of an oddball answer. One of the best places to learn SQL in on real sources and some of the more interesting ones are in data science and that means Kaggle! For example, on mobile but two that come to mind are the Wal-Mart and Bimbo Bakery competitions. Pull those files down, normailze them (there for DS so many times they wont ne, write some SQL to clean‚Äôem up, and the write report queries on them. Thats pretty real. Extra credit: slap those report queries into analytics tool like Tableau, Qlik, or your fav online reporting tool to further work on your skills.
About to board a plane so will he brief but if you are using MSSQL then you are looking for coalesce. https://www.mssqltips.com/sqlservertip/1521/the-many-uses-of-coalesce-in-sql-server/
Ty for your answer. You basically summerized my nexts steps. Once I get a grip of SQL, I'll put everything together (R programming, Data Analysis Knowledge, and SQL) to do analysis of "real" data. I have been thinking about getting into tableau. That would complete my path by giving me a nice data presentation tool. Havent really made my mind tho, I'm not sure if this would be a skill super required by recruiters. Obviously its always a choice: should I go deeper into SQL or go for data visualization? Today I tend to go for the SQL.
Not sure what the point of this post is...
Started as a kid out of high school in a call center. Promoted off the floor to daily analyst and micro management work after a year. (Keeping people in the right queues, routing calls, seeing if we were making money or losing money on staffing, etc.) Then after another year, I got put into a Telecom / Sys Admin / Database / Programmer role. Year after that, put into Sys Administration. Decided I hate administration, but got an interview at a company that makes IT tools, specific in Database. After some time, moved into a DBA position for a large contractor and moved into a Data Engineer role with a new one after that. I got really good at Excel and macros in the first position when I was a daily analyst. I learned from there that SQL and other tools can do it better and a higher level, so I worked on trying to move towards that. Overall, I just like making automated solutions. We have X,Y,Z problems or questions we want to ask, and I find ways to resolve them and ways to get the questions answered with data. I'd say though that it was largely accident and chance I got to where I am today.
Not exactly an answer for you because I don't know what the answer is. I am interested and want to find out though! See if this helps clear the mud. &gt; Even if an expression is deterministic, if it contains float expressions, the exact result may depend on the processor architecture or version of microcode. To ensure data integrity, such expressions can participate only as non-key columns of indexed views. Deterministic expressions that do not contain float expressions are called precise. Only precise deterministic expressions can participate in key columns and in WHERE or GROUP BY clauses of indexed views. https://stackoverflow.com/questions/8276444/sql-server-indexed-view-column-precision This would probably be a good stack overflow question though.
You have your own LLC and insurance right? So if you mess something up (not that you would, but hey, that's why it's insurance) they can't turn around and sue you to death. Your LLC may end, but that's a small loss compared to your house, car, retirement fund, etc. I have a lawyer who works with IT consultants individually who helps me write up everything / file it, etc, and is on retainer. It's about $250-$500 a year, vs the $50 if I wanted to do it myself. The peace of mind is worth it though. Not really a question you asked, but this was the first thing I tried to find out before diving in, the rest of your questions are good ones though. 
Random user trying to increase post karma
you can do this with grouping on a UNION subquery to create the UNION, you have to write a separate SELECT for each table, and here's the important part -- you have to "line up" all the similar columns here's an example where the 2nd and 3rd table are missing columns SELECT mykey, column1, column2, column3 FROM table1 UNION ALL SELECT mykey, NULL , column2, column3 FROM table2 UNION ALL SELECT mykey, column1, NULL , column3 FROM table3 now you treat this as a subquery and take the MAX of all non-key columns in a GROUP BY -- SELECT mykey , MAX(column1) AS column1 , MAX(column2) AS column2 , MAX(column3) AS column3 FROM ( SELECT mykey, column1, column2, column3 FROM table1 UNION ALL SELECT mykey, NULL , column2, column3 FROM table2 UNION ALL SELECT mykey, column1, NULL , column3 FROM table3 ) AS x GROUP BY mykey 
:) Still learning...
Wow! Thank you for teaching me about the MAX Function. I learned that in PostgreSQL, I can use a DISTINCT ON / ORDER BY clause to simplify the process even further...
select Teacher_ID, max(Create_Assignment) assignemnt1 from Database group by Teacher_ID;
Probably a few gotchas in here that you could be missing. Source: https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views edit: this one is probably going to throw you: *If the view definition contains a GROUP BY clause, the key of the unique clustered index can reference only the columns specified in the GROUP BY clause.* edit2: You also need to use the `WITH SCHEMABINDING` clause in the CREATE/ALTER VIEW statement. Which means you also need to reference the tables by schema in the view. edit3: You also cannot use CAST on the result of the aggregate SUM function and then use that as part of the indexed view. You will have to remove the CAST and probably ISNULL also. Looks like you are simply trying to ROUND so that may suffice. Basically I'm pretty sure it has nothing to do with your column types, but you have other issues... the below query will allow a clustered index: CREATE VIEW vwTEST WITH SCHEMABINDING AS SELECT date, hour, location_number, SUM(energy) AS kW, SUM(energy / 1000.0) AS MW, COUNT_BIG(*) AS count_big FROM dbo.TEST_TABLE GROUP BY date, hour, location_number
If a decimal datatype is involved in a SUM function, SQL defaults to the max precision of 38, as a sort of "worst case scenario" when building the execution plan. https://docs.microsoft.com/en-us/sql/t-sql/functions/sum-transact-sql While we know that your column is only half the max length, and storing it as a decimal(38,x) takes up twice as much space, the query optimizer doesn't have time to account for that when it's trying to get your plan out the door. Rather than taking the extra time to hyper-optimize your query for the decimal(18,2) data type, it simply uses the baked-in logic that all sums on decimal columns are going to be output as decimal(38,x). This prevents it from under- or overestimating the memory needed for your plan. Both are equally bad. The only way to accomplish what you need is to convert the result to decimal(18,x) when selecting from the indexed view. 
I took an intro course on edx.org. But like other people said you will learn best on the job. I was working around a lot of sql before I took the course and as I was taking it, the code just started to make more and more sense.
Does MySQL have a COALESCE() function? If so, then yes, this is what you are looking for.
I learned on vertabelo :) it was challenging and nearly perfectly prepared me for my job. I would suggest paying extra attention to date and time functions since they can be very tricky if you are working with data sets with different time zones.
Thanks I had to create another view and perform the casting on it.
*Thanks I had to* *create another view and perform the* *casting on it.* ______________________________________________________________________________ ^^^-english_haiku_bot
Thank you. Edit3 seems to be correct. I am unable to use CAST or CONVERT. I do need the ISNULL as apparently the query isn't deterministic without it. If I round the sum function it still keeps the decimal(38,x) precision. I can't round the energy data as I would lose too much precision. I had to create a separate view and perform the cast there. No all in one solution is available.
graduated with degree in MIS from four year school at age 22(bullshit degree) I realized very quickly this was not for me because I am not a people person at all. Most people with this degree go into business analyst positions. I worked for nearly 7 years doing customer service until I switched departments at my company and ironically worked in MIS. In that department, I started off learning access to pull reports, just basic stuff. I immediately saw the value in providing data to myself, my team and my boss so I kept learning. The more I learned about the data, I began to explore as many of our databases as possible and pulling data to solve problems. I then started automating processes. This led to diving into SQL where the transition was smooth, but there was a learning curve. Fast forward 5 years later and i'm working as a BI analyst doing a heavy amount of sql reporting with SSRS, tableau, cognos and other tools as well as data warehousing and data modeling. 
This seemed pretty simple in logic so I must be missing an important step. I have my HC_Drug table and a drug, ‚ÄúMagnesium Sulfate‚Äù, with a current DrugID of 11291 that I want to change to 13802. What am I missing?
&gt; &gt;SET IDENTITY INSERT hc_drug ON; Then try it. Though I question why you're trying to update a primary key. 
You should google search for ‚Äúsql identity column‚Äù and read up. What are you trying to do here anyways? TLDR: you can‚Äôt update identity columns, the only way to ‚Äúupdate‚Äù them is to reinsert them with identity_insert off.
Totally get it. I guess I could try it. But your query could probably be rewritten: Same thing, different way to say it? SELECT students.first_name, students.last_name, students.email_addresses, AVG(grades.grade) GPA FROM students JOIN grades on (students.id = grades.student_id) GROUP BY students.id HAVING AVG(grades.grade) &lt; 50;
Thanks for all the help. I‚Äôm new to SQL and trying to learn some of the easy stuff and work my way up. Here‚Äôs the scenario so it makes a bit more sense to you guys: I have a medical database and the HC_Drug table has all the medications for the database. I setup the HC_Drug table back in April with some drugs. I found out I needed the Middle East formulary this month and now I‚Äôm replacing the drugs with my new drugs. Some of the drugs have the same names as the old ones I put in back in April, but they have a different DrugID. I wanted to just replace the DrugID on those drugs. I know this may seem super rookie, but do you guys mind explaining the logic behind what you believe the entire query should look like to accomplish what I‚Äôm trying to do?
Ahhhh...ok the logic is starting to make sense to me a bit. So I can‚Äôt use the same column name in my ‚Äòwhere‚Äô clause as the one I‚Äôm trying to modify. I guess I need to find another common link in the HC_Drug table besides DrugID. I just figured I could use the old DrugID to search so I‚Äôm sure I‚Äôm replacing the right drug. I‚Äôve got about 300 drugs I have to replace the DrugID‚Äôs on. I figured it would take forever to do all of those using the NameX column (the brand name of the drug). I‚Äôll do some digging to see if I can find another common link that‚Äôs possibly integer based.
Sounds like you need to brush up on primary keys. What makes a unique record in HC_Drug? Is it DrugID? Is it a combination of drug name and dosage? I don‚Äôt think you need to be updating existing records, but instead inserting new records and possibly removing the old records. Does it matter if you have inactive drugs in HC_Drug? Do you have any other tables that reference specific DrugIDs in HC_Drug? 
&gt; What you want to do is use another column for your where clause. Don‚Äôt update the column its searching for. It‚Äôs a recursion issue. This isn't the problem at all - OP's query would work just fine if DrugID weren't an identity column.
Don't listen to that dude, there's no problem at all with updating the same column you're using in WHERE.
How do you propose setting up the query with the scenario I mentioned in mind?
The type of that column is IDENTITY, which is a field that is auto populated (normally incrementing by 1 each time a record is added). You can't normally write your own values in there. There's lots of reasons you wouldn't do this in a real world situation, but if this is just your own testing/learning playpen the easy way to get around this would be to go into the design view of the table, select the DrugID field, and set the Identity Specification &gt; (Is Identity) property to No. Change it back when you've finished your query.
Your query is fine, it's the properties of the DrugID field that is the problem.
I'd give simple aliases like a, b, c then do a fins and replace for a.Withallmyrows, b.rowrowroetowboat Ect... But this probably isn't a real good solution... But your question is kinda random. Hope it helped 
Omg this was so freaking helpful!!! üôèüèæ
Thanks for the reply. That doesn't really help much as I was hoping to easily get it to show me table/column names. I was trying to trouble shoot code that wasn't working so I wasn't going to leave it place. BUT, I did want so say thanks for chiming in and offering your two cents. That is what makes this place great. For the record, I did end up finding the issue eventually. I just broke down the joins one by one until I found it.
Why is the data changing? Would it be worthwhile to keep the old records? If so add a flag column like "isCurrent" to toggle old names off. Usually if you're changing the key you should be adding new records and archiving old ones, either moving them to an archive table or by "deactivating" with a flag column 
I'm glad to hear you solved it. I figured you would without the job wish I listed. Good for you and I'm i feel the same way about the community... I love it
I have all the prior imported drugs inactive in the database because I couldn‚Äôt delete them; they were already attached to patient records. The old imported drugs do not have a GCNSeqNo, so they don‚Äôt enable interaction checks to be done. So that drug - allergy interaction checks could be done, I needed to map the old drugs with their new DrugID so that GCNSeqNo would be present. Without this link to the already attached drugs from the earlier import in April, a patient could accidentally be given a medication they‚Äôre allergic to. Will the integrity of the table be at risk if I change the design of the table to run my originally posted query (mentioned by an above listed user)?
Not sure if the default value is the right tool for this. Have a look at an INSERT TRIGGER, this might be a better option.
Yes the integrity of the table will be at risk if you turn identity_insert off and update or delete a primary key. The reason is because if another table references the primary key (as a foreign key) in its table it will no longer exist. Normally there is foreign key constraints but if you are importing these tables individually into the database they will likely not have been created. If this is production data you should seriously consider taking a break and learning more by going through sqlzoo.net and importing this data into a non-production database to test and modify the data before importing into the production database. 
Could create an Instead of Insert Trigger and use datepart(dd, @date ) and datepart(mm, @date) to to figure out what date range the value being inserted is in. https://technet.microsoft.com/en-us/library/ms175089(v=sql.105).aspx https://docs.microsoft.com/en-us/sql/t-sql/functions/datepart-transact-sql
Nvm got it, just broke it down. CONCAT('&lt;a href="files_and_documents.jsp?userid=?',CAST(STD.studentId AS CHAR),'&amp;usertype=1', '"&gt;
Two questions: How many taxes are defined in your table to begin with? Is your report going to have the headers (SaleId, Date, Sub Total, Some Tax 1, Other Tax 2, Grand Total) for every single sale entry like a receipt or is it just going to be a dump of the data like a table result?
[HN discussion](https://news.ycombinator.com/item?id=15335717)
Users are allowed to define taxes. So I can't be certain how many there will be. In practise the most I have seen is 3 on a sale, but in a report I have seen up to 7, especially when government changes tax rates, as new taxes have to be made in the system, so the report ends up with things like Some Tax 2016, and Some Tax 2017. I am not showing sale entries on this report, but am showing every sale. To just sum up the taxes is easy enough as I can select from the tax def table, and do an inner select on the sales. I ended up temporarily solving my problem with a cursor and dynamic sql. But it is ugly to maintain and look at.
Main reason I asked those and specifically question 2 is to know whether is it important not that the taxes are different from entry to entry.
This was exactly what I needed, I never thought about using MAX( with non-integer values! Nice one :)
How? If this was in SSMS, don't close it.
tldr; ITT: Some guy that doesn't want to do his homework role-playing his dad
It was in "oracle SQL developer" and the computer was slow so I googled how to reinstall windows. Any help would be appreciated i am willing to photocopy the assignment to send to someone
That is what I thought when i was posting this it sounds suspicious haha
Didn't the most recent semester start only a few weeks ago? Even if you are lying I was going to see if there was any way to recover what was lost, but I'm not going to just redo it.
I am in Australia. If it makes any difference going by this notifcation the sql is to be typed in a word doc. Theres like questions then it asks you to answer them.
But kudos for creativity I guess :)
I think you should read chapter 1 in your text book again.
Couldn't you just do this: select .... case when Tax1Name = "Some Tax" then Tax1Total when Tax2Name = "Some Tax" then Tax2Total when Tax3Name = "Some Tax" then Tax3Total when Tax4Name = "Some Tax" then Tax4Total else 0 end as Some_tax_total, case when Tax1Name = "Other Tax" then Tax1Total when Tax2Name = "Other Tax" then Tax2Total when Tax3Name = "Other Tax" then Tax3Total when Tax4Name = "Other Tax" then Tax4Total else 0 end as Other_tax_total, .... 
To help clarify, is your current job considering moving your data to Hadoop (etc) or are you personally thinking about getting a different job involving Hadoop? That may help frame the answers.
http://lmgtfy.com/?q=Does+MySQL+have+Coalesce%3F
Concatenating html strings containing user ids in your SQL is NotLikeThis
If you're using MySQL, don't use "go", just use your semi-colons at the end of your statements. "Go" works great for MSSQL but not MySQL. 
We use Octopus at work. Its great, the only issue I ever have is if a script errors out sometimes re-deployment can be an issue. This is solved just writing good code, i prefer things like "if not exist create" as a default otherwise every statement is an alter. 
Who let him in?
You can ignore the above. I think I was giving you an easier way to get the results you aren't looking for.
‚ÄúBig data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it‚Ä¶‚Äù‚ÄîDan Ariely
I do "big data" work for a 14k employee healthcare provider. We have at least one table that I know of that contains 1.5 trillion records. Pro tip, it's still just Microsoft SQL Server, SSIS, RedGate, SSMS, etc. Only difference I've really seen was the Pure SAN (bunches of flash hard drives). PS - If your data is already relational, Hadoop is a step in the wrong direction.
That pretty much sums up big data for most companies. 
this... we aren't in the trillions... but SQL Server and SSIS (plus some custom C# based BCP stuff) and we too, have the big databases pinned to the SSD's in our SAN (other lesser systems use the SSDs for caching alongside HDDs for cheaper storage). Our warehouse server also has a Fusion card dedicated to tempdb, a butt-ton of memory, and a surprisingly few number of cores (we chose to spend on higher frequencies rather than higher core/socket count and associated SQL licensing)
This is actually exactly what I did using dynamic SQL. 
Feel free to PM me the assignment, I'll take a look at it. I'll tell you how many hours I'd spend per question, rounded to 15 min intervals with a 2 hr minimum total at $75 a hour. I can send you my paypal after we come to an agreement on pricing. If you or your kid want help, post the questions and what you tried and where you're stuck and I'll give hints for free.
Like this? SELECT name, RIGHT(name, LEN(name ) - 19) , (left (name, len (name) - 2)) SELECT name, left(name, len (name) - 2) + RIGHT(name, LEN(name ) - 19) Also, mixing upper and lower case is pretty much the worst :|
No, that didn't, the second column matched the first. I try will keep my case more uniform. I'm new to this stuff...
https://imgur.com/pU8fbHg Here's what it looks like, and I want it basically merged so i'm dropping Name (Last, First): and drop , at the end. 
That's how I'd do it too. The problem with having more cores is... well... compiling all of these separate threads back together at the end of a process is really expensive. Here's a video about it from my favorite SQL guru. https://www.brentozar.com/archive/2013/08/what-is-the-cxpacket-wait-type-and-how-do-you-reduce-it/
I am personally thinking about getting a different job involving Big data. Hadoop is the only large data tool I hear about on a frequent basis.
That is what I was assuming. It's newly large and 10 years ago not that common I believe. I find it hard to see that there are all "these" experts running around with YEARS of experience in big data.
What is kind of structuring is done for big data? I assume for example, Hadoop is the ETL tool, and the relationship create in the destination would be? A data lake? I am only familiar with traditional relational data warehousing.
Solved: SELECT name, right (left (name, len (name) -2), len (name)-21)
We just have a big SQL Server and do not use Hadoop. ETL is performed by SSIS or BCP. There are no *twue* star schemas in our environment. Everything is relational. Even the SSAS is tabular. There's been talk about implementing a "data operating system" but insofar as I am aware that's just Hadoop marketing speak that doesn't *mean* anything really other than typical alerting and maintenance stuff.
It always depend how you define "big data". Technically, you're supposed to comply to the 3 Vs: Volume, Velocity, Variety. They all are fairly easy to explain, but the threshold are entirely arbitrary. As for me, I think I'm doing medium data:I have a process that collects roughly 65 million rows on a daily basis via 250GB of XML files (48 of them), does some funky stuff via batch/SSIS/SQL and I output the results to another system, archiving both my input and outputs (tables roughly 90 billions rows right now and a compressed table size of roughly 15TB), all the while have a dozen or so OLAP users poking the thing (had to teach them about partition alignment, but nothing fancy on that end.) I wrote the ETL in a relatively simple manner and extensible fashion: simply put, a parametrized child package called by the master will preprocess the file via batch files, load it, and eventually sends it to the "common" area via BCP, always using it's own sandbox, thus there are no interlocks between the thread/files and I can throw as many cores as I want at the problem. Sure, it creates about 2000 temp staging tables as well as roughly 400 views for the different datatypes (every xml entity has it`s own tables that has to be replicated for every file). But in the end, when we jumped from 2 to 4 datacenter for some time (a the datacenters each provide me with an hourly file, so 24 per day), it took me about an hour to call my child package more often from the parent and I was done. At the end of the day, sure, with big data we are talking about larger datasets. Remember however HADOOP is often an overkill solution, due to the overheads required. If you want to prepare and learn a valuable skill, I'd recommend you start by seeing ways of breaking down huge monolithic processes into smaller chunks that can be parallellized, as it's something can can also be of value in the OLAP world.
get that shit microsoft plus sign outta here!!! 
 SELECT customerID FROM yourtable GROUP BY customerID HAVING COUNT(DISTINCT creditcard) &gt; 1 
Oops, didn't see MySQL...
Thanks, I'll give this a try.
OK, good. So if you aren't thinking about migrating existing data (and be able to explain why you'd need to), then it's a simpler question. Big Data tends to be about unstructured or semi-structured data which is moving through a pipeline (the Velocity and Variety piece). (Often) data is being added continuously, not in batches like an ETL, so the query output are changing dynamically, but in a non-ACID way. Because it's continuously injesting data, it does tend to be a lot larger (Volume piece). So you have to think about the process differently, it's not traditional "ETL then query, only bigger". That's the biggest mistake most companies make IMHO. I'd start by getting an understanding of the tools that make up the Hadoop "ecosystem". There are a ton, it's not a simple as "install hadoop" and being able to run queries. Once you see all these pieces (that was the eyeopener for me at least), it became clearer why "Big Data" was fundamentally a different thought process than traditional RDMS. I hope this helps. 
Yea... we set maxdop to force qieries to behave well in the presence of other users... but similarly effective to your point
You cannot write its not like this then not give an example or a reason why.
Helped me, thanks! 
Agreed, you can only go so far on your own and with classes. On the job, you have real problems to solve.
What are you doing with this data? Querying it often? Updating it often?
New data is inserted every second (no row updates) and the latest data point is read every 5 seconds. Occasionally a trend will be shown that would retrieve the last several thousand data points.
a single table seems fine for this, what issue are you having, is the insert taking too long? are the selects taking too long? 
By aggregate do you mean just add together? Something like SELECT CASE Business WHEN 'Company 1' then 'Company 1 and 3' WHEN 'Company 3' then 'Company 1 and 3' else Business END as Business , LocationID , Date , sum(percentage) FROM yourtable GROUP BY SELECT CASE Business WHEN 'Company 1' then 'Company 1 and 3' WHEN 'Company 3' then 'Company 1 and 3' else Business END , LocationID , Date This counts company 1 and 3 together. There are many other ways of tackling this depending on your exact requirements though.
Keep the sourceID datatype as skinny as possible, same with the datetime value (datetime vs datetime2). Also consider row/page compression depending on your storage limitations, given the limited query scenarios you are talking about.
Can you explain the table structure you're working with?
&gt;PS - If your data is already relational, Hadoop is a step in the wrong direction. This is something I can't get people to understand - especially MongoDB fans. There are proper applications for it, and then there are improper (or completely wrong) applications for it; and yet there are some people seem to want to always be on the "latest-and-greatest", even if it doesn't actually fit their business model.
In the process of transitioning from SSIS/SSRS/SSAS to BigQuery and Tableau. BQ scales up and out to petabytes, not sure of upper limits, but it's huge. Big data doesn't just mean Hadoop. BQ is a structured SQL-based big data db, similar to Redshift. For our team, Hadoop doesn't really make sense as we're reporting on large amounts of OLTP data. If you don't have a use case for Hadoop or don't know if you need it, then you probably don't. There is an interest in ML, so we might utilize Spark for that, but otherwise no Hadoop for us. The big hurdle for our team is training. I'm familiar with the tools (Java, Python, open source tools, Linux, data pipelines, ML, etc), but to the rest of our team it's alien. It's also a huge shift in architecture and infrastructure as we move from internally managed systems to cloud. However the infrastructure cost savings are significant, so we're excited about that.
Based on what I've seen of systems that work with large tables, you should be fine with billions of rows. You probably want a surrogate key for that table -- make sure it's BIGINT.
I've never heard of trillions of records in SQL Server. Is there anywhere on the internet that explains how to work with tables that large? I guess lookups and inserts are log n, so they aren't that much slower than for tables with billions of rows.
[removed]
 SELECT SUM(percentage) AS percentage , date , locationid , CASE WHEN business = 'Company3' THEN 'Company1' ELSE business END AS agg_business FROM table GROUP BY agg_business 
I would suggest a scalar valued function like ufn_getBaseDate(currentDate as Date) which will be called with NOW() in default. In this function you can encapsulate something like @currentMonth = datepart(@currentDate,'MM') IF @currentMonth between 5 and 9 BEGIN @setMonth = 5 END ELSE BEGIN @setMonth = 10 END @setDate = DATEFROMPARTS(YEAR(@currentDate), @setMonth, DAY(@currentDate)) return @setDate A trigger does basically the same but en masse they tend to complicate testing in my point of view. 
Sorry, I am extremely new to SQL. How would I use this in my insert or create table statement?
You should at least be looking into partitioning too to improve read performance. Is SQL Server a hard requirement? A nice key value time series DB would get you sharding, query and formula engines designed for time series (better windowing and slicing for example), easier schema extensibility ... 
I mean, the MapReduce paper is 14 years old. The technology's been around a long time. And anyone working in social media or mainstream web analytics has dealt with Big Data. But it's not hugely common in enterprise yet, a little bit more now with SCADA and the omnipresence of web style analytics in every application ever.
declare @s VARCHAR(100) = 'this is an address ls-1234' select right(@s,4) More information here: https://social.technet.microsoft.com/wiki/contents/articles/17948.t-sql-right-left-substring-and-charindex-functions.aspx Edit: Of course, not sure which SQL you are using, above is for MSSQL.
The three concepts to master with larger data sets even if they're not technically Big Data ... Distributed compute ie running things in parallel. You have to engineer your data storage, movement and structure to support this. It's not rocket science but it's different. So for example using Spark to do distributed ETL. Exact same paradigm as SSIS - cache lookup to memory, shred fact table - just done in small batches highly paralleluzed. Getting comfortable moving away from ACID and into a CAP theorem world. For truly Big Data it's usually acceptable to have a process fail on a small batch here or there. You just re queue it or if the analysis is time sensitive you just yeah it and move on. Addressing these sorts of conditions on the fly for event processing or log streaming is just fundamentally different than error handling on a traditional DWH load. And finally just getting your hands dirty with non-SQL code. I've done way more .NET, PowerShell, R, and Scala/Spark than I ever did as just a database guy. Plus a good chunk of my datasets come as JSON, etc. So a broader development skill set in general. 
I'm using developer 
Thank you figured it out using SUBSTR(speakeraddress, -4)
Oracle? SELECT SUBSTR(SPEAKERADDRESS, -4) FROM SPEAKER; I think... fuck. I hope! üòñ
What I meant is your RDBMS, which seems to be Oracle since you're using 'SQL Developer' (which is merely just a tool to connect to an instance of a RDBMS). 
As kthejoker points out partitioning is probably your best long term solution. 
Is that Star Wars characters in Game of Thrones ‚Äúlands‚Äù? üòÇ
Think his data is no good :)
It has clearly been polluted! #WinterIsComing
Did you take a picture of your computer screen? You are new to more than just SQL...
This kind of blatant self-promotion (clearly the same person is asking the question as is answering) will ensure I never buy this product (or even click on the link).
Maybe or maybe that workstation just isn't connected to the Internet.
No, it's not necessarily faster, and not only that it's 25 years outdated. I would ask your Oracle consultant to show you the statistics for the queries that prove the old style is faster. And I mean the specific queries you are working on *now*. Not other queries, or someone's blog post, and definitely not some random statistics that were cobbled together 25 years ago. The actual queries in front of you at this moment, with both plans side by side. If they aren't willing to do that they are probably an [Expert Beginner](https://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/)
`SELECT SPEAKERNAME, RIGHT(SPEAKERADDRESS, 4) AS SPEAKERADDRESS FROM SPEAKER;`
Lol, a colleague asked me to look at the Cost column in the Explain Plan, and mine is less costly. Plus, in average they have pretty much the same performance, with execution times being very close and alternating. I love the Expert Beginner definition. Definitely using that.
Yea that article was a great read. I've met a few different people who fit the description. Have some idea in their head about the "best" way to do something (usually some overarching approach to simple things), but never will change their mind even when presented with empirical evidence to the contrary. Just keep writing queries based on how good they perform and you will do fine. Don't let some "expert" try to convince you otherwise, especially without data to back it up. And please don't use comma joins they are the literal worst. :)
Here you can find an example implementation for SQL Server 2005: https://stackoverflow.com/questions/442503/bind-a-column-default-value-to-a-function-in-sql-2005
I'm trying to avoid comma joins, good to know that's kind of the way to go. I could definitely think of some people too, when reading about the Expert Beginner. These people don't revisit what they've accepted as true, and thus never learn anything new. I've been programming for over 20 years, and I am still trying to learn new stuff. Thanks for replying!
Spot on champ cheers 
Sorry I'm new to this but yes Oracle 
Uni assignment, obviously whoever made it loves there GoT and Star Wars 
Seemed quicker to do it through reddit on my phone then to screenshot and cut all the other queries out 
select SUBSTR(regexp_replace(SpeakerAddress,'[a-zA-Z, -]', ''),3,4) FROM SPEAKER
On top of what is present here (partitioning and all that), you should also consider SQL page compression for your table. From my understanding, You are doing write once-read many, thus it's the perfect use-case for SQL compression. It actually hasten disk reads since more data can fit a single page, albeit at a small CPU cost. Also validate if a columnstore index apply to what you're doing, as it could help with performance. Depending on exact use-case you could also reduce the total rowcount by using a pigeon-hole like system to compress your table vertically as well. Assuming you only read your table by 5 seconds-slices, you could pre-aggregate the data and store that instead: instead of: SrcID Timestamp value 1 12:00:00 1 1 12:00:01 1.5 1 12:00:02 2 1 12:00:03 1 1 12:00:04 1 You would get a simple entry with a couple of aggregated values instead: SrcID Timestamp ValueCount ValueSum 1 12:00:00 5 6.5 Be sure you fully understand your use-case before implementing this solution however, as you WILL loose your raw data (or you could keep raw data for a month and they use this model for archival purposes). For instance, ValueMin and ValueMax could be of value or not, but once you've settled on the required fields, there is no going back.
I'm not quite sure the question is specific enough. "How to work with" is unclear as to the task needing to be performed. Inserts to that table are done via ssis where it is determined which rows to insert/update based on tablePKID, createDate, lastModDate.
Hi fellow coders, I‚Äôm new to Atom as a code editor and I‚Äôm getting use to all the shortcut features it offers. I have an excel spreadsheet I created of 650 unique Lab Findings. I have the unique ID for all those Lab Findings. I am working on an INSERT SQL query to get these added into a HC_LabFR table in my database. I need to list all of those drugs names as string on individual lines like the first one I have shown here. Is there a quick way to do this instead of manually moving the cursor to each line and pasting each line one at a time? Any ideas welcome üí°!
Yeah it's much quicker to use the SQL Server Import/Export wizard to populate a table. It shouldn't give you too much hassle since you have 2 columns. When you set the properties of the source, make sure the first is DT_I8 (integer) and the second is DT_STR (varchar). First save your Excel file as Tab Delimited (it's one of the options in Save As) * Go into SSMS * Connect Object Explorer to server * Find the database you want to import to * Right click the database&gt;Tasks&gt;Import Data * Data Source&gt;Flat File Source * Browse&gt;Choose file location of tab delim file from above * Data Source: Columns&gt;Confirm tab is the delimiter (it should automatically determine this from a tab delim file) * Data Source: Advanced&gt;Highlight Column 1 (ID) * Data Type&gt;DT_I8 * Highlight Column 2 (LAB ORDER) * Data Type&gt;DT_STR * OutputColumnWidth&gt; Something bigger than your string lengths (4000 is pretty safe) * Click Next * Choose Microsoft OLE DB provider for SQL Server * Change login information as needed (Make sure the Server name: and Database: are correct for your target) * Next&gt;Choose table (or click in the Destination field and change the name Ex: [dbo].[WhateverYouWant] if the table doesn't exist it will be created) * Preview the data if you wish * Next&gt;Run Immediately&gt;"Finish&gt;&gt;|"&gt;Finish Now you have data in your SQL Server table.
Create separate insert statements for each row using strings, column values, punctuation as needed in an empty column using concatenate function or the &amp; method in xls. Then just copy and paste that column minus the header into your editor 
"without using joins" is an absolutely ludicrous requirement you ~have~ to use a join to get the name of the person, unless you run separate queries where you offload the results of one query into some kind of temporary external array structure and then use those values to construct the sql for the next query absolutely ludicrous assignment
&gt; Yea that article was a great read. I've met a few different people who fit the description. Have some idea in their head about the "best" way to do something (usually some overarching approach to simple things), but never will change their mind even when presented with empirical evidence to the contrary. i find this is the exact attitude of people who vehemently insist that every table has to have a numeric auto increment primary key &gt; And please don't use comma joins they are the literal worst. :) co-sign
Like... what's this class trying to teach, at this point? I'm struggling to come up with a real-world use-case where these restrictions would exist.
&gt; I'm not able to use Subqueries, **Joins**, Max, Min, Limit, Exists, etc. This: &gt; &gt; SELECT People.name, A.gradeA, B.gradeB FROM People, Grades as A, Grades as B WHERE A.cid='c4' AND B.cid='c4' AND A.pid = People.id AND B.pid = People.id AND A.grade &lt; B.grade; Is a join though: https://stackoverflow.com/questions/20138355/whats-the-difference-between-comma-separated-joins-and-join-on-syntax-in-mysql
I agree. What if I only wanted the grade though, and didn't need the name. Would that make it any easier. I've been struggling with this one for a few days now but if I figure it out tonight I'll post my solution.
&gt; Expert Beginner great article i noticed as soon as i opened it that it is dated "Sep 30" -- whoa, i thought, did i fall asleep for a couple days or something? i'll give an upvote to everyone who posts what they think the actual date of publication is...
Someone posted that in the discussion and the professors response was "the motivation is one of learning the concepts rather than just using some keywords that others have implemented. Right now, it looks like we can just use MAX() and go home. But when you are faced with an even more complicated query, you will be stumped because you won't have a readymade operator for it and the foundations are weak. So this exercise hones your foundations." Not sure I agree with him.
I think you're right but in some of the examples we've done in class we did joins this way. Here's what our professor said in terms of what we can use: "1. The basic SQL construct: SELECT FROM .. AS WHERE 2. Set operators: UNION INTERSECT EXCEPT 3. Creating temporary relations CREATE VIEW... AS ... 4. Arithmetic operators like &lt;, &gt;, &lt;=, == etc."
back when mysql did not support subqueries, it was a challenge to write queries which today you would just simply use a subquery for but these solutions usually required a **self-join** with an inequality join condition which is exactly what you have here -- A.grade &lt; B.grade so good luck, and i'll bet one of two things happens -- 1) you find out that you misunderstood the requirements, or 2) it's a humoungous practical joke by your teacher
Cool, I get it now. Thanks.
I'm so fucking glad I learned SQL on the job. I'm convinced SQL "teachers" have never actually used SQL in their entire lives. This isn't even like some sort of boring exercise like "how to learn a math proof starting with 1=1". This is like "grill your steak without any heat source". Your teacher is an asshole and not teaching you any useful real life skills. Tell your teacher the internet said "go fuck yourself". And you can quote me. edit: PS the answer probably involves ORDER BY Grade but I'm too annoyed with your teacher to figure out the full query.
Hahaha even though I'm not really getting any help here I'm very glad to know everyone feels the same as I do.
&gt; we did joins this way why do all the classes and assignments we hear about on reddit feature dumbass teachers stuck about 20 years in the past?
can you use ORDER BY or WHERE clauses?
Yes
Por que no los dos? juejuejue
bad bot
Thank you DoggoFish90 for voting on or-yes-bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Yes, use both, you can use the WHERE clause to link the tables and the ORDER BY to sort on a column and take the TOP(1) to get the highest grade.
Right but the answer needs to be a query that returns only the person with the highest grade unfortunately. That's the part that is tripping me up.
&gt; you can use the WHERE clause to link the tables that's a join, not allowed also, if LIMIT is disallowed, then TOP would be disallowed too
Joins using the Where clause are allowed, sorry I didn't clarify that earlier. For whatever reason we just can't use Joins involving any Join syntax. 
If you need highest grade without using max then use inline query. Select * from (select * from grade_stable order by grade Desc) where rownum=1; I use Oracle syntax.
If you need highest grade without using max then use inline query. Select * from (select * from grade_stable order by grade Desc) where rownum=1; I use Oracle syntax.
This might work but I don't think we can use rownum. I think he wants us to use some sort of relational algebra based query involving except.
Well that should work but the entire month will be inclusive since you are only comparing months. So June 1st will be returned for any day in September. As long as you are OK with that it should be fine. Otherwise you could use 90 days or something a bit closer to exactly 3 months. edit: Actually you are checking for 2 months since you are doing &lt;3 P.S. &lt;3
hah, so using deprecated syntax is ok...
I got it.... Where ((name.JOIN_DATE &lt;= DATEADD(M, 3, '@Initdate'))and (name.JOIN_DATE &gt;= DATEADD(M ,-3 , '@Initdate')))
Just when you thought the assignment couldn't be more absurd. "Don't use the proper syntax only syntax that is 25 years deprecated is acceptable" Teacher wha ???? 
Oh good catch - I appreciate you pointing that out. I guess the entire month would work for this purpose though.
Don't have time to answer the full question, but you could use this form: select g1.* from grades g1, grades g2 where g1.grade&gt;g2.grade EXCEPT select g1.* from grades g1, grades g2 where g1.grade&lt;g2.grade
In column C3 just type `="VALUES('LabFRID','" &amp; B3 &amp; "')"`, then copy the formula down to the bottom of the sheet, then copy all of the cells in that range for the C column and paste it in. Done. I don't do this for inserting values very much but it makes formatting a select list, or other lists relatively easy.
Thank you, I'll try this tonight.
There are several ways to do it, but from your example you look like you just want the time and day of week so a simple cartesian join will work. WITH DayList AS( SELECT 1 as DID, datename(dw, getdate()) as DOW UNION ALL SELECT DID + 1, datename(dw, DATEADD(DAY,DID,getdate())) FROM DayList WHERE DID &lt;7 ), HourList AS( SELECT 1 AS HID, CONVERT(TIME,'0:00') AS HOW UNION ALL SELECT HID + 1, DATEADD(HOUR,1,HOW) FROM HourList WHERE HID &lt; 24) SELECT DOW, HOW FROM HourList, DayList
I like the other answer, but I think this is little easier to modify as needed: ;WITH cte AS ( SELECT 1 x UNION ALL SELECT x + 1 FROM cte WHERE x &lt; 168 ) SELECT DATENAME(weekday, Dateadd(hour,x,'2017-9-24')), DATENAME(hour, Dateadd(hour,x,'2017-9-24')) FROM cte option (maxrecursion 168)
So if you were to create a view where you had the data ordered in descending order by grade then selecting the top 1 from that view where cid = c4, right? So that will give you the pid... so make that a 2nd view... then maybe do an intersect with the names table? edit: If you wanted to get fancy you could create another view to insterct where the name = 'Web Development' and intersects with the DESC ordered grades, then intersect the PID to return the single value of the students name. If you really wanted to get fancy you could declare a variable at the top where you type in the name of the course and then it returns the name of the highest graded student based on the logic above. Probably overkill but when a teacher gives you a bullshit assignment the only appropriate response is to blow it up.
&gt; Tell your teacher the internet said "go fuck yourself". And you can quote me. Well played. &gt;PS the answer probably involves ORDER BY Grade but I'm too annoyed with your teacher to figure out the full query. Create a VIEW with grades ordered in DESC, then create a second VIEW to select top 1 where cid = c4, then intersect with the student table for the name?
Yep, that would definitely do it! Thanks for the gold too by the way. :)
I kind of disagree with most of what other people here have said about the professor. I agree that it's an absurd request which frustrated me, and lived in the back of my mind for a few hours, but there is a really simple solution (if that works) which is only really something you'd think of if you actually understand joins, sub-queries, row_number() and other utilities that would make this a trivially simple request. Good exercise that makes someone think outside of the box, and it didn't become apparent until I had exhausted every possibility using a sub-query, join, or other similar utility that was banned here. Just my .02. Should tell the professor exactly who knows what, and I imagine he will grade them based on their attempt/approach, not necessarily their results. I mean he's asking them to retrieve the highest grade for a course he is supposedly teaching to them... or in other words, "pick your own grade... but you can't use these things we've been learning for the last few weeks. Go back to the basics and figure it out." -- which is an all too frequent business need when you're working in a locked down environment that eliminates certain options because of ridiculous business rules that shouldn't exist, but which do exist and you must overcome to be successful. edit: The irony is that creating multiple views like that and selecting top 1 are basically the same as using a sub-query or a row_number() function. So the professor may be trying to get the students to visualize exactly how these things work using very simple basic concepts.
&gt; Joins using the Where clause are allowed, sorry I didn't clarify that earlier THIS IS EVEN FUCKING WORSE
No wukkas!
Interesting theory. I have seen plenty of comma-style join questions from professors posted on this forum though. I'm a bit skeptical of SQL teachers now. I do agree that it could possibly be a good thought exercise. But I would have introduced the question before teaching about JOINS. It would make sense to build on an INTERSECT and then introduce JOINS as opposed to the other way around. Plus, I'm not even 100% sure of the requirements here because it seems like the OP can use a comma join but not explicit JOIN syntax or something? Which is completely convoluted and makes no sense at all other than having some arbitrary limitations. I guess I'm getting a bit jaded and not giving he professor the benefit of the doubt. I don't know the person and they could be doing exactly as you say. Or they could just be a crotchety old person with SQL skills from 1992 that thinks things should be done the hard way. ;)
I read it as no joins is no joins. If he can use comma joins it feels cheap but yes, I'm with you in general, just I used to teach English in ROK and this is the type of shit I'd make my kids do because it forced them to demonstrate a relative mastery over other subjects and to sort of 'abuse' the rules. 20 years ago when I was in programming school we would learn simplified code on paper/diagramming on the board, but when we actually got to work we had to write it like it would be written in a professional environment (this wasn't SQL, but the analogy would be comma joins on the board, but writing the full conditions in our code.)
I think if you supply the same seed each time you won't repeat rows. https://stackoverflow.com/questions/7966430/how-to-sample-rows-in-mysql-using-randseed
Thank you, but I get an error telling me time is not a valid data type.
This works thank you so much!
I believe that will make it deterministic but it doesn't mean you won't get repeat rows. What are you trying to achieve because order by RAND is pretty bad for performance.
Michael Alexander (Microsoft Excel MVP) came up with a tool to convert Excel tables into SQL. You can see the tool [here](http://datapigtechnologies.com/blog/index.php/create-sql-server-tables-from-excel-data-free-tool/). If you're using SQL Server, you can also use the Bulk Insert to create the table with the Excel list.
I'd say spend at least a couple weeks on it. Go through sqlzoo and do a couple tutorials on codeacademy or code school. That should give you most of the basics.
This sounds about right
A foreign key constraint stops you from putting in data that has a key that references another table if that key doesn't exist in that table. In this case, you're getting an error because there is no SpeakerID S2 in the speaker table. You need to add a record to Speaker with a SpeakerID of 'S2' before you can use 'S2' in the speakerID field of the conferencesession table. Or you could remove the constraint, but they're usually there for good reasons.
I think your problem is that MySQL isn't web scale.
If you already have a subscription, I'd suggest pluralsight's path on SQL as well. Took me about 2 weeks to plow through and it gave me a much better understanding of most things. Pluralsight, to me at least, has been worth every penny. 
Are you sure it's the same error? ie it's not now referring to the other FK? 
What do you mean by this?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/CwXE2c0.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dnm9frl) 
There doesn't appear to be a First Name/ Last Name column, only 'fullname'. If that's the case then you can query the table where fullname is not null, but that wouldn't really answer your question. Do you have another table or other columns that contain first name and last name separately? The only other option...at a stretch...and I wouldn't ever recommend using it, would be to search for contacts that contain a space. This would assume that a space indicates there is a first name and second name. I would advise removing any leading and trailing spaces before running that type of query though.
Structured Query Language Commands and Operations, What is SQL?, Purpose of SQL, Who Should Learn SQL?, Subsets of SQL, Data Definition Language, Data Manipulation Language and Data Control Language. SQL Introduction for Database Developers, Database Administrators and Database Testers.
Are you in the same class as [this dude?](https://www.reddit.com/r/SQL/comments/719lkp/whats_wrong_with_my_sql/)
Well, that would make me a little queasy if this method went anywhere near a production environment but I would advise you research the LIKE operator. It's a simple enough query, but I want you to look it up as I'm assuming you're learning. You want to select rows from a table where a column is LIKE a space. This will essentially find any rows that contain a space in your specified column. I'll let you look up the syntax, but that should do the trick.
foreign key must have [bofadem](https://www.reddit.com/r/SQL/comments/719lkp/whats_wrong_with_my_sql/dn9aot8/)
Would it be correct if I wrote it as: CONSTRAINT SESSION_FK2 FOREIGN KEY(ROOMNO,BUILDINGNO) REFERENCES ROOM(ROOMNO,BUILDINGNO)
Yeah I'd say so ahahah
&gt; its meant to be based on the assumption that there is a space Whoever gave this assignment to you [has some mandatory reading to do](http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/). And since there's an address component on that table too, follow up [with this](https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/)
If that's the only criteria, then see if a space exists in the name. https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_instr 
I'd also trim the column before using that.
+1 Agreed. As mentioned in my first reply, trim the leading and trailing spaces to reduce the chance of anomalies.
This is a classic problem that is pretty much unsolvable, and why most systems are designed with 2 or more fields for names. What if you have "John Smith Jr."? Or "Dr. John Smith"? Or "J. Jack Smith"? Now your formulas to parse names are hosed. Good luck and hopefully you are able to redesign the database with this in mind!
You could look for a space as others have mentioned and then measure the length before and after the space. Only count it if both are greater than 3. That would weed out D Angelo or names like that. 
The only question I've ever been asked to "whiteboard" at an interview was something like "Say you have a Customer table and Order table and you want to return the customer name and any order from the last 3 months". To which I wrote " SELECT CustName FROM Customers C JOIN Orders O ON C.CustomerID = O.CustomerID WHERE DATEDIFF(M, OrderDate..." and when I got to about that point in the query the guy interviewing me said "that's good I just wanted to see where your head is at on this". I didn't get the job, but not because of the SQL part of the interview (they wanted a developer with a different language skillset). But if you can do something like a FROM, JOIN, WHERE and know some of the functions you should be able to pass a SQL "interview". If you don't know a specific function I would just say something like "Hey I usually look up the syntax on this one I don't know it by heart" and I doubt it would be a problem. It's basically the truth, I look up syntax stuff regularly even after like 7 years using SQL. Hope this helps.
You will want a success and failure precedent constraint. [This](https://www.red-gate.com/simple-talk/sql/ssis/working-with-precedence-constraints-in-sql-server-integration-services/) article covers some really good information on that. 
Woah! That was super helpful Michael! Thanks a ton for all the help.
what happened when you tested it? ‚Ñ¢
The SQL standard does provide an operator for that, which is not supported by SQL Server: a IS NOT DISTINCT FROM b A solution that is often good enough is this: (a = b) or (a IS NULL AND b IS NULL) Due to SQL's three-valued logic, it returns `unknown` in case only one of the two values (`a` or `b`) is `null`. This is often no problem (especially in the `where` clause, because the `where` clause treats `unknown` like `false`). For the rare cases where you really need either `true` or `false` but never `unknown` you can use this expression: CASE WHEN (a = b) or (a IS NULL AND b IS NULL) THEN 0 ELSE 1 END = 0
good bot
Thank you zanestone for voting on imguralbumbot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
^(thank you) ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
It did not give me any errors
I think the answer you're looking for is: SELECT * FROM SPEAKER WHERE SPEAKERNAME LIKE '% %';
If you can't use a TOP 1 (because you can't use a LIMIT) then simply create a table with an incremental primary key that is based on the class and the descending order of the grade, then select where key = 1.
Yes, ORDER BY is a killer of performance. I only have a basic knowledge of WP. Does it allow you to sort after the data is returned? You can filter by a date field like `WHERE [YourPostDateField] &gt; '2017-09-01'` or something like that to return only recent records without the ordering. Can you archive some posts to a different table so the main one you are querying doesn't have so much data to sort? The bottom line is if you need to sort in the database it's possibly the most expensive operation you can do (probably 10x or worse performance at minimum, if not hundreds of times worse). An index isn't going to solve this problem for you.
read a book on tsql. Honestly the biggest thing with knowing sql is being comfortable with the database schema. once you understand the basic concepts.
Yep, it will show a table of posts with columns like author and date and stuff. Moving posts to another table is an idea that I have been considering. I'll have to look into that some more, but ideally it would be nice to have the database work with this. I know it can be an expensive query but I feel like there has to be some issue if it is taking almost 2 minutes to execute. It's just one or two million rows on a text column. That shouldn't be so hard for a database, right?
So not real familiar with MySQL. Cal1gula has a good point order by sucks. The only suggestion I can make is try adding a non-clustered index on that column and then run a Flush Tables to clear all the query cache to try and force the optimizer to use the new index. 
&gt; That shouldn't be so hard for a database, right? Incorrect. https://www.brentozar.com/training/think-like-sql-server-engine/1-clustered-index-21-minutes/ Here's a good explanation (or skip to ~17 minutes for the answer to your question). Sorting by a text column is *especially* slow. You can try returning less columns in your queries so that there is less data for the engine to sort. That might work for you. 
What are your thoughts on converting the post_title column from text to VARCHAR(255) so that MySQL uses the modified sorting algorithm instead of the original sorting algorithm? From what I understand that would make it take the WHERE clauses into account rather than reading every row in the table. I'm just not sure if that is a bad thing to do for some reason.
This should do it, in general you would look at what you are joining on in question, in this case it is the groupid and the courseid, and that is what you want to partition. Then you would order by the timeadd (assuming that is the field) and then just filter on the row_num being 1 so you only get the most recent: LEFT OUTER JOIN( SELECT DISTINCT groups1.id as groupid, c1.id as courseid, usr1.id as coachid, usr1.firstname as coachfirstname, usr1.lastname as coachlastname, r1.id as coachrole, members1.timeadded as timeadded, ROW_NUMBER() OVER(PARTITION BY groups1.id, c1.id ORDER BY members1.timeadded DESC) AS RowNum /*men1.menid, men1.menfirstname, men1.menlastname, men1.menroleid*/ FROM mdl_course AS c1 INNER JOIN mdl_context AS cx1 ON c1.id = cx1.instanceid AND cx1.contextlevel = '50' INNER JOIN mdl_role_assignments AS ra1 ON cx1.id = ra1.contextid INNER JOIN mdl_role AS r1 ON ra1.roleid = r1.id and r1.id =4 INNER JOIN mdl_user AS usr1 ON ra1.userid = usr1.id INNER JOIN mdl_groups AS groups1 ON c1.id = groups1.courseid INNER JOIN mdl_groups_members AS members1 ON groups1.id = members1.groupid and members1.userid = usr1.id) as t1 on t1.groupid = groups.id and t1.courseid = c.id and t1.RowNum = 1
I have done a lot more work in MS SQL than MySQL so I am not sure the extent of the performance gain. I don't think there would be a negative effect though. THis might help you: https://stackoverflow.com/questions/25300821/difference-between-varchar-and-text-in-mysql Sounds like it can help since you would be able to index the column and get at least some benefit. Just make sure you won't have a truncation problem with the data although I would also assume MySQL wouldn't let you change the type if there will be truncation (at least MS SQL will throw errors sorry to keep comparing but that's really more of my specialty).
Okay thank you!
I sir , am an idiot. It is mysql not mssql......... This is exactly what I imagined however the functions aren't supported by mysql I appreciate all your help though.
Which version of MS SQL server are you using? If you're on 2017 you've got the new [String Agg](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql), otherwise you're stuck doing something like [this](https://social.msdn.microsoft.com/Forums/sqlserver/en-US/f09d4166-2030-41fe-b86e-392fbc94db53/tsql-equivalent-for-groupconcat-function?forum=transactsql). What is your screenshot of? If you're doing this with Access's SQL rather than MSSQL you're even more stuffed.
To be blunt, you can't with how your data is now formatted. Either you can remove all but one record per ID, or you could add another column and put a counter per ID and make your ID and counter the PK. The second suggestion is not recommended for most applications. OR you could make a new table with that huge long definition you have on the right. -IMO best solution.
&gt; To be blunt, you can't with how your data is now formatted. [String_Agg would like to have a word with you](https://docs.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql)
Right, you're advocating he reformat the data in table. Which I suppose is what OP asked for... Never mind, you're being more helpful than I was... OP's screenshot looked like Excel to me.
So we are on 2016, however, I will need to check the status of that. I believe our company is upgrading to 2017 in the very near future (within a couple of weeks). Thank you for the links! I'll research those options when I return to my desk. I just made an edit to the post, and I'm not sure if it changes your answer. It would also be acceptable to have the multiple types in their own columns.
Yeah, sorry. It was meant to be sample data. I couldn't link the full table and field names for confidentiality.
That was a fun talk. I constantly go back and forth with what I want to do in the database and what I want to do in app code. In the end, and as the talk concludes, it depends.
To make it multiple types in their own columns you could look at PIVOT: SELECT RecordID, [Paper] AS Paper, [Waste - not recycled - Mixed Waste] AS WasteNotRecycledMixed, [Waste - recycled - cans/bottles] AS WasteRecycledCansBottles etc etc for all your waste types ... FROM (SELECT recordID, type) AS base PIVOT ( max(type) FOR [type] IN ( [Paper] AS Paper, [Waste - not recycled - Mixed Waste] AS WasteNotRecycledMixed, [Waste - recycled - cans/bottles] AS WasteRecycledCansBottles etc etc for all your waste types ... ) AS Pivot Or you could do the simpler but uglier: SELECT base.recordid , a.type as Paper , b.type as [Waste - not recycled - Mixed Waste] , c.type as [Waste - recycled - cans/bottles] FROM (SELECT DISTINCT recordid from table) base LEFT JOIN (SELECT recordID, type FROM table where type = 'Paper') a on a.recordid = base.recordid LEFT JOIN (SELECT recordID, type FROM table where type = 'Waste - not recycled - Mixed Waste') b on a.recordid = base.recordid LEFT JOIN (SELECT recordID, type FROM table where type = 'Waste - not recycled - Mixed Waste') b on b.recordid = base.recordid LEFT JOIN (SELECT recordID, type FROM table where type = 'Waste - recycled - cans/bottles') c on c.recordid = base.recordid and add a subquery for each of your types. It's clunky and ugly but if it's a once-off thing and the list of types is short and known it's probably the simplest way to attack it.
https://docs.microsoft.com/en-us/sql/integration-services/packages/restart-packages-by-using-checkpoints https://docs.microsoft.com/en-us/sql/integration-services/control-flow/sequence-container Well, you could do some combination of sequence and checkpoint-ing. Once you move past the first "phase" of the package, you could hit a checkpoint. The package could fail and still continue on from the checkpoint. Just a thought.
You can use universal temp tables in lieu of CTEs (avoiding the need for pesky sub queries) ##temp You just have to manually create the table once during development/testing. Once you save and deploy the SSIS it handles itself.
Understood. One of those is probably the path I will be taking. Thank you.
_Understood. This is_ _Probably the path I will_ _Be taking. Thank you._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^vba_question ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
[removed]
Love this blog!
I take it from your GROUP BY clause this is MySQL? The proper way to fix this is to look at your joins and work out where the one to many or many to many relationships are causing orders_product records to be duplicated in the dataset before grouping is applied. The lazy way to try to fix this is just to take your current query and turn it into an inline view where you make the recordset distinct and sum from there: SELECT x.c5_dataset, x.orders_id, x.products_id, x.products_name, x.products_quantity, x.product_certain_amount, x.product_stock_amount, x.product_group_number, x.products_status, x.competitor_prices_price, x.products_competitor_watch, SUM(x.products_quantity) as COUNT FROM (SELECT DISTINCT orders.c5_dataset, orders.orders_id, orders_products.products_id, orders_products.products_name, orders_products.products_quantity, products.product_certain_amount, products.product_stock_amount, products.product_group_number, products.products_status, competitor_prices.competitor_prices_price, products.products_competitor_watch FROM orders LEFT JOIN orders_total ON orders_total.orders_id = orders.orders_id LEFT JOIN orders_products ON orders_products.orders_id = orders.orders_id LEFT JOIN products ON orders_products.products_id = products.products_id LEFT JOIN competitor_prices ON competitor_prices.products_id = products.products_id LEFT JOIN products_to_categories ON orders_products.products_id = products_to_categories.products_id WHERE orders_total.class = 'ot_total' AND orders.orders_status != 0 AND orders.todaystarted &gt;= $sales_stats_from AND orders.todaystarted &lt;= $sales_stats_to AND orders_products.products_id != '' AND orders.c5_dataset = '$c5_dataset' AND products_to_categories.categories_id = '$id') x GROUP BY x.products_id ORDER BY count DESC If whatever is causing the duplication isn't an item already in your select list this might fix it. If not, running the query aliased x on its own will give you a clue as to where duplication is happening.
Concur. If you run a non distinct version of the x query and join a few of the tables at a time you should find the duplication. Look for something funky on the tables you're joining in columns that you aren't selecting: e.g. date columns in competiters_prices. 
A little more assistant with this, it looks like you have two master type records this is working with, orders and products. What you have found is that for a single product there is a weird count difference, like 2x + ~40 or so. This says to me the duplication isn't on (just) the product but has something to do with the order because if there was a duplicate on the product you should see an exact x2. My best case is to look at the orders. My biggest suspect is the order_total table, maybe with your filter there is 2-3 order totals per order with a class of 'ot_total' so it is getting 2-3x duplication. One thing you might find is there is actually 2 issues, so if there is an issue on the total it might be only causing like 40 or so of the duplicates so then after that you are seeing an exact 2x for the count. If that becomes the case I could see either the product categories or the competitor price joins having duplicates that would cause an exact 2x for rows. 
Slight addendum... This view is used for a table on a website that selects TOP 20 and then selects COUNT(*)... No WHERE clause - Top 20 - apprx 1 second COUNT(*) - apprx 5 seconds with WHERE clause - TOP 20 - apprx 3 seconds COUNT(*) - apprx 9 seconds So the cost is almost doubled with the where clause
Add index to the un-indexed fields? &lt;3
if only I could! not my server though. And for arguments sake let's say in the world of "remote server" requesting such a change would go through 10 layers of bureaucracy only to be rejected! lol. 
Not 100% sure if I understand the question, but are these the results you want? create table #test2 (receipt_id int, buyer_id int) insert into #test2 values(123,1),(123,2),(123,7),(123,8),(125,2),(126,3),(126,7),(127,3) select distinct b.buyer_id as input_id, a.* from #test2 a join #test2 b on a.receipt_id = b.receipt_id where b.buyer_id IN (1,2...) order by b.buyer_id, a.receipt_id, a.buyer_id
If i understand your SQL correctly, that not quite what I'm looking for. It looks like you're mapping all buyer_ids to receipt_ids. However, I don't want all buyer_ids mapped, I only have a specific subset which are declared here: and buyer_id in (1, 2, ...)
I assume you can add a where clause if you need to... lol. I can help if you aren't sure how though.
Thanks! I was able to get the package to continue on failure, but it still sends the global fail mail, which is what I was trying to avoid 
Aha! Your suggestion works! Thank you so much!
Use `OPENQUERY()` to go to the remote server, pull all the data using the where clause, and then join to it. 
Can you post as much information as you can about the global fail mail and it's configuration in ssis? I'm curious now, but I'd have to replicate it. 
interesting, I will try this
Under Event Handlers, if I click the drop down under Executables, then the root that everything is under, and with the EventHandler being OnError, there is a normal email task. It has no expressions, just a boring name and nothing important on the Mail settings. 
IIRC when you join to a remote server it will pull everything to the local server *before* starting to whittle the data down. Becomes quite the nightmare. A simplified way to do this would be to select * the data into a #table, then create an index on the #table and join it to your query, but what openquery does is go to the remote server and execute the query locally, so here you're going there, removing any data that is unnecessary, then returning that smaller set before joining. 
Do you need to return every column? You will almost definitely increase performance for each column you remove from your select * list. 
Can provide explain analyze for this query? Can you modify structure of those tables (like add indexes, create partitions)?
Thanks for the suggestion. I've altered the query to select only necessary columns, and performance is improved. (Same query returns 6 records in 1900 ms vs 2990 ms) However, the time grows exponentially the larger the results. (Similar query to return 8 records takes 4900 ms, 50 records takes 10+ minutes.)
Can't remember how indexes work with openquery() on the join condition, but you might have best luck trying this: select * into #table from openquery([servername],' select * from table where x = ''1'' ') a create index name on #table([fieldname]) select * from table a left join #table b on a.field = b.field
I have no direct control over the db, so I'm stuck reading what it provides for now.
ok you actually led me to a pretty good solution. I was able to use OUTER APPLY to skinny down the results before bringing the data over. It has cut query time in half!
I'm afraid I don't understand what you're trying to accomplish, or what you're trying to do. Can you rephrase? Anyway, picking random: how about ramming the deck into a table variable including a guid column and select them out ordered by guid? Or sha hash or md5?
it si hard to give good advise without having execution plans and see where the costs are but here is what I would look at 1. do you ever ‚Äònot‚Äô pass the the project id orthe agent. if a project and an agent is always provided I would remove the ? is null part 2. no matter what you write. campaign, last_call and sale_authorized are inner joins. this is becouse of the conditions you have in the where part. check what indexes you have. cam_id on compaign most probably should be primary key and. so should other ids on other tables. check if columns you are using for filtering are indexed. and the columns on callback used for joins are indexed specially the ones used to join with the tables with the filters
Thanks for the response. Regarding your comments: 1. Both may be null. The idea is if the user selects one or the other (or both) their results will vary accordingly. i.e. choosing a project but leaving agent null will return all callbacks for that project. 2. last_call, sale_authorised, and contact all share the same primary key, which is a combination of 3 columns. (cli_id, con_id, cmp_id). I will investigate to see what the index situation is. 
There is a difference between a link to an Access database and migrating all your data to SQL from Access and then using SQL. Both are possible. Which do you need?
Atta boy.
I need to update an access database, which updates SQL database, including files. So in other words, I will use microsoft access to make all the modifications and create a link that carries everything over automatically to sql server. I am not sure if this is possible. 
is it that one or the other is always provided? give it a try to make 2 queries instead and check on the java code. I would expect to help. but still if you have the right indexes it should handle the query as you have it. Correct Indexing does magic on query performance :) 
That's essentially what Solution 0 and 1 did. 0 treated the state so that every card knows it's zone and is selected randomly by it's identity column. In 1, the state reduced the same cards in a single zone down to a count, but then reexpanded to pick randomly the same way as before. In Solution 2, I dispensed with the expanded state altogether and made the algorithm decide if the card picked fits into each type of card, one after the other. *Let me put it this way: Instead of randomly picking a ROW, I'd like rows to be weighted according to how many cards of that type are in that row. AND make the draw without iterating excessively or at all. I'm asking the class if they can come up with a more elegant solution than #2.*
What you're looking for is a subquery to pull max assignment times for your roles 4 and 5. Join back to assignments, get max user id ( just in case 2 users were assigned at exactly same time). Join to users twice to get detailed user info.
Typically one or both, but still need the capacity to search without either. (Returning the full table, which is a couple 1000 records.) The tables are indexed, so I'll try and use that to speed up results, though I'm not sure how that works so I'll have to do some reading. For example last_call has 5 indexes, covering most of the data I'm looking for.
Does this help? https://support.office.com/en-us/article/Link-to-or-import-from-an-SQL-Server-database-A5A3B4EB-57B9-45A0-B732-77BC6089B84E?ui=en-US&amp;rs=en-US&amp;ad=US&amp;fromAR=1#ID0EABAAA=2010
This is the output of a (very similar) query run with Explain Analyze in pgadmin. (Off of a testing db built from dumped data from the production -- performance is similar) 'Nested Loop (cost=1694.40..10111.63 rows=1 width=131) (actual time=203.015..360572.819 rows=2045 loops=1)' ' -&gt; Nested Loop Left Join (cost=1693.98..6721.20 rows=1 width=99) (actual time=113.165..242379.547 rows=4658 loops=1)' ' -&gt; Hash Anti Join (cost=1693.55..2432.93 rows=1 width=72) (actual time=84.036..258.349 rows=4462 loops=1)' ' Hash Cond: (((callback.con_id)::text = (sale_authorised.con_id)::text) AND ((callback.cmp_id)::text = (sale_authorised.cmp_id)::text))' ' -&gt; Hash Join (cost=8.30..746.45 rows=70 width=56) (actual time=0.374..144.073 rows=4918 loops=1)' ' Hash Cond: ((callback.cmp_id)::text = (campaign.cmp_id)::text)' ' -&gt; Seq Scan on callback (cost=0.00..618.69 rows=31669 width=40) (actual time=0.059..56.029 rows=31669 loops=1)' ' -&gt; Hash (cost=8.29..8.29 rows=1 width=16) (actual time=0.045..0.045 rows=1 loops=1)' ' Buckets: 1024 Batches: 1 Memory Usage: 9kB' ' -&gt; Index Scan using campaign_prj_id_idx on campaign (cost=0.27..8.29 rows=1 width=16) (actual time=0.034..0.036 rows=1 loops=1)' ' Index Cond: ((prj_id)::text = '156600'::text)' ' -&gt; Hash (cost=1134.90..1134.90 rows=36690 width=16) (actual time=83.225..83.225 rows=36690 loops=1)' ' Buckets: 65536 Batches: 1 Memory Usage: 2274kB' ' -&gt; Seq Scan on sale_authorised (cost=0.00..1134.90 rows=36690 width=16) (actual time=0.028..39.063 rows=36690 loops=1)' ' -&gt; Index Scan using contact_pkey on contact (cost=0.42..4288.26 rows=1 width=27) (actual time=32.971..54.242 rows=1 loops=4462)' ' Index Cond: ((callback.con_id)::text = (con_id)::text)' ' -&gt; Index Scan using last_call_pkey on last_call (cost=0.42..3390.42 rows=1 width=32) (actual time=21.378..25.355 rows=0 loops=4658)' ' Index Cond: (((cmp_id)::text = (callback.cmp_id)::text) AND ((con_id)::text = (callback.con_id)::text))' ' Filter: ((con_set = 'callback'::contact_set) AND (lc_status = ANY ('{"Answering Machine",Callback,"Dial Retry","Left Message","No Answer"}'::call_status[])))' ' Rows Removed by Filter: 0' 'Planning time: 6.011 ms' 'Execution time: 360576.882 ms'
The fundamental issue with a predicate on an unindexed field is no different on a remote server than it is a local server. Using OPENQUERY to do the work on the remote server won't solve anything in this case. Without an index on your filter column, the query engine has to return all matching rows based on whatever index it did hit and then filter out the rows that don't match the filter afterwards. As others have said, it's usually best to get the data from the remote server into a local temp table first and use the temp table for the rest of your operation. 
So if I understand correctly you have chosen a particular zone some how and you now want to pick a random card from that zone. So in you example you have picked zone 21 which has (5+4+2)= 11 cards in it of 3 different types. And you want to randomly pick 1 card from the 11 and return a cardtypeId. In this case you can do this via SQL if you want but it's the kind of thing that you can probably do more efficiently in what ever language you are using to do the coding. You aren't going to get any real efficiencies by doing it in SQL for the kinds of numbers. If you had a million cards and a hundred thousand different types.. maybe. In that case you could get a sum of the cardcount for the selected zone then generate a random number between 1 and the sum, you could then pick a row by using sum (cardcount) over (order by cardstateid rows preceding unbounded) to give you a running total and then picking the top row with a running total greater than or equal to your random number. But when you have 3 rows. (Never more than 5) you are just better off doing it outside SQL I think.
Not interested in taking this outside SQL right now. The AI training is intended to live all on SQL. I‚Äôm not familiar with Over and am just becoming aware of partitioning in general. I‚Äôll need to research that. Edit: Read up on the Preceding Unbounded bit. I think that‚Äôs it. Nice. Thank you!
one thing that catches my attention is that columns on conditions are being converted to text. like here `Hash Cond: (((callback.con_id)::text = (sale_authorised.con_id)::text)`. That sounds weird to me. 
not sure if it will help but another thing you can try is to change the join conditions for `sale_authorised` and `last_call` to be joined via `contact` and `campaign` instead of directly with `callback` 
Thanks, these are good to note. Currently the query is in a view and at least for now I'm trying to keep it that way so temp tables are out. But the idea seems straight forward, try shrink the remote data set.
If you don't have access to the remote server, there is nothing you can do to shrink the remote data set. So, look instead to limit your interaction with the remote data set as much as possible. 
What happens when you cast the parameter to VARCHAR first?
Please post a quick result set mock-up, and I'll try to come back when I get home. 
[removed]
Have you tried using ISNULL and does that work? Are the versions of SSMS the same in both the DEV and PROD environment? Also, are there any NULL values for the parameter in the prod database?
I'll give that a try, much appreciated.
Make what repeat? You're just going to change = to &gt;= on the year and get rid of the crspmonth where clause
A small set of sample data would be very helpful. From your description you might be able to use ROW_NUMBER() OVER function or RANK(). 
Use like "% %", as there is a space in between the names 
You've got a stray comma in your SalesOrder and pizza create table statements, at the end of each.
That took care of at line 7. Now i have at line 6 You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FOREIGN KEY (CustomerID) REFERENCES Customer(CustomerID) )' at line 6 
Now you're missing a comma between your foreign key statements. Maybe slow down and make sure your syntax is correct here...
I appreciate the help. this is all new to me and I'm not finding ucertify to be a good teacher.
in both the Orders table and the SalesOrder table, you are missing a comma would've been a ~lot~ easier to spot if you had been using the **leading comma convention** CREATE TABLE Orders ( OrderID INT NOT NULL AUTO_INCREMENT , CustomerID INT NOT NULL , notes VARCHAR(150) , PRIMARY KEY (OrderID) FOREIGN KEY (CustomerID) REFERENCES Customer(CustomerID) ); Create Table SalesOrder ( OrderID INT NOT NULL , PizzaID INT NOT NULL , Quantity SMALLINT , FOREIGN KEY (OrderID) REFERENCES orders(orderID) FOREIGN KEY (PizzaID) REFERENCES Pizza(PizzaID) );
Sorry for the delayed response every, I'm trying to get the largest 500 returns for each month and year since 1990. Months are set in crspmonth and years in crsp year, but there is also a date column with the same information just combined. 
Sorry for the delayed response every, I'm trying to get the largest 500 returns for each month and year since 1990. Months are set in crspmonth and years in crsp year, but there is also a date column with the same information just combined. 
Sorry for the delayed response every, I'm trying to get the largest 500 returns for each month and year since 1990. Months are set in crspmonth and years in crsp year, but there is also a date column with the same information just combined. 
I will look up both of those functions right now, thanks so much
If you don't use leading commas i think way less of you
Aha, you need rownumber
 ;WITH CapsCTE AS ( SELECT dfp.[PERMNO], dfp.[crspmonth], dfp.[crspyear], ROW_NUMBER() OVER(PARTITION BY dfp.[crspmonth],dfp.[crspyear] ORDER BY (dfp.[Shrout]*ABS(dfp.[PRC Pricing Closing])) DESC) AS rn (dfp.[Shrout]*ABS(dfp.[PRC Pricing Closing])) as MarketCAP From [dbo].['Data for project$'] dfp --Where [crspmonth] = 1 and [crspyear] = 1990 GROUP BY dfp.[PERMNO], dfp.[crspmonth], dfp.[crspyear] --Order by [crspmonth] DESC, ([Shrout]*ABS([PRC Pricing Closing])) DESC ) select * FROM CapsCTE WHERE rn&lt;=500 
What I did here is use a windowed function. Row_Number() can't be filtered in the same level that it's used, so that's why I shoved the query into a CTE. Rank() would represent ties more correctly, if we wanted that. If I had to do this long term, I'd probably throw a table that persisted the previous month and year values and added the next one each time.
Why not order by age?
That's the natural behavior of order by, first character then second character, you're SOL on that column afaik. You can copy and paste that same statement and create a new 'index' column. Order by that index. Not pretty but it will do the trick. 
An alternative is to make a table: AgeMin|AgeMax|SortField -|-|- 0|2|1 3|5|2 6|8|3 And join on it. Or as the other poster suggested, just sort by age.
Would it be acceptable to just pad single digits with zeros (e.g. 00-02, 03-05, 09-11)? Otherwise, if you can change the ORDER BY clause, I'd just order by age (as suggested by MayorBee).
I had initially tried that and recieved this error: column "age" must appear in the GROUP BY clause or be used in an aggregate function and wasn't sure what was going on. I just jumped back in and realized that the CASE WHEN statement requires me to explicitly say "group by age" and I couldn't use "group by 1" 
Ah, I see. Dunno how efficient this is, but I would stage the data with the CASE statement in a CTE, then do the aggregation in the SELECT. Then order by lpad(histcol,5,'0') where histcol is the column you've made in your CASE statement. 
I would check triggers. That error has usually been raised to me when using xml in an ORDER BY clause. 
Have you tried something like ABS(CHECKSUM(NEWID())) % COUNT(*) OVER (PARTITION BY CardType)? That might get the weight you are asking for. I may not be understanding the post clearly. 
Thank you. The solution with over unbounded preceding is working great. One of the goals was to learn something new, and you definitely helped me nail that. 
Thanks for the gold! Really glad to have helped. It can be fun mucking about in SQL and the aggregates have a bunch of tricks to them.
I would just create a view with the required unions, gives you one location to query without adding the overhead of storing everything in one LDF/MDF
I don't think strictly speaking that is true. A few years ago I dug into this issue quite a bit, so I'm going from memory and might be wrong, but, consider: select * from table a left join table b left join table c left join remoteserver.database.table d where d.x = 1 IIRC, SQL will first pull all of the data from D to the local server, and then before slimming the data down it will create a total possible data set representing the maximum combinations between the tables, then it will collapse down and pare the data to your request. With OPENQUERY() you are first going to the remove server, using its resources (not the local servers) to reduce the dataset, then only bring over relevant data where x = 1, then joining to the other sets. This can have huge performance implications because while you're correct it will still scan every row, doing so before the join could be a much smaller data set.
We don't know why you want to either create a new database or combine all the databases so we have no idea which would be correct for you in your situation. If you only need to query the databases a couple times you could always try to use something like sp_msforeachdb https://www.mssqltips.com/sqlservertip/2201/making-a-more-reliable-and-flexible-spmsforeachdb/ note it has some issues like accidentally skipping databases that you'll want to watch out for or at least be aware of
I'm working on consolidated reporting. You have me curious though, in what situation would you recommend one solution over another? 
First of all, if you use the CHAR type, you signal that the values are fixed-length. For example, it makes sense to use CHAR(2) for 2-letter country codes (US, GB etc.). The second point, I believe, depends on implementation. You should check with SQLite, but the value actually stored in your example should be not `'123'`, but `'123 '`, with additional 13 space characters at the end. So the value is padded with spaces to be 16 characters long.
Thanks, I was thinking the same, but wasn't sure if a 70 database union would be an optimization nightmare.
Combined means you use more storage, but cut down on complexity. You don't need 70+ unions in a view. You don't need to edit all of those when you want to make a change. You'll likely have faster return times on your queries since things like uniqueness will be handled at ETL time rather than report time, and data can be all indexed together.
Easy way would be to make your query an inline view. Select top 50 * from ( --your entire query goes here ) x Order by [otherfield]
I would nest one query in the other. The inner query is the one you've got above. The outer query does the different TOP limit and ORDER BY that you want. Something like this, skipping lots of things because I'm lazy and you're not paying me. SELECT TOP 50 * FROM (SELECT TOP 500 PermNo, ([Shrout]*ABS([PRC])) as "Market CAP", (CF/P) as "Cash Flow to Price" ORDER BY Shrout*ABS(PRC) DESC) AS X ORDER BY [Cash Flow to Price] 
I would be inclined to start looking at pulling together data from your 70 DBs into a single consolidated datamart - maybe a nightly scheduled SSIS package to just aggregate the tables together. It would likely improve reporting performance and ensure that you're not putting more burden on the production systems than strictly necessary, but whether it's worth it really depends a lot on the environment - is performance an important factor? Are you working in a cloud environment where there's a cost associated with disk and CPU time? Does reporting need to be realtime? Those sorts of things are considerations when deciding whether to create a separate reporting DB.
I do remember encountering space padding in Vertica, so there's that. I guess we can say that in most cases the difference is in semantics and storage.
To clarify what other commenters have said: a varchar(10) will use enough space to store between 0 and 10 characters, depending on how many characters you insert. A char(10) field will use enough space to store exactly 10 characters, even if you insert fewer. There's nothing stopping you from storing two characters in a char(10) field, it just uses more space than storing it in varchar(10).
So I am not an expert so you may wanna find something verifying this but... It is my understanding that char(10) will always take up 10 chars which can speed up lookups of data but comes at the expense of wasting space. Since the sql implementation knows that a string is 40th in a series of elements starts at the 400th element it is able to use those assumptions to speed up the location of data. This comes at the expense of taking up space in cases where you do not use the full length. Varchar, on the other hand, avoids wasting space but it requires the database implementation perform additional work when locating a piece of data which will slow your lookups somewhat. I kinda think there might be some implementations of SQL that avoided properly implementing VARCHAR and just use standard CHAR no matter what the key word is. But the idea when the standard was designed was to offer database designers the option between having a performant lookup and one that saved space where possible. I hope that some of what I wrote was correct. Hopefully someone else will be able confirm or refute some of this. I can probably find a source to back this up in a couple of hours when I have more free time. 
What is b. ? Should be o. 
Oh yes my bad, changed it. Forgot to change it, translated it from my native language to English. 
Dacht ik wel ;) Try this: select c.custid, c.name, count(o.id) as OrderAmount from customer c left join order o on o.custid = c.custid and year(o.besteldatum) = 2014 and month(o.besteldatum) = 12 group by c.custid, c.name And why this works; https://stackoverflow.com/questions/354070/sql-join-where-clause-vs-on-clause
change `COUNT(*)` to `COUNT(o.custid)`, and change the word `WHERE` to `AND` SELECT c.custid , c.name , COUNT(o.custid) as OrderAmount FROM customer c LEFT JOIN order o ON o.custid = c.custid AND YEAR(o.besteldatum) = 2014 AND MONTH(o.besteldatum) = 12 GROUP BY c.custid , c.name
Thanks, query works!
hii nice dude 
I‚Äôve always gone with the rule of keeping filtering in the where clause. With that, you‚Äôve got to make it so you look at dates if there are any orders, or nulls if there aren‚Äôt. Can anybody speak to performance hits by doing it this way? Are NVL or IsNull bad in the where clause?
You inserted '123' and the engine padded it with the remaining 13 spaces. That is fine. Functionally, the fact that it's a char(16) will have no effect with queries. There are 2 things that will change, and neither will be noticeable except in some high-performance / high-volume applications: 1. fixed-length char values take up more space on their own *and* make bigger indexes, since every index element contains the 'maximum' column length. 2. in some special circumstances, fixed-length columns will make updates perform better. Specifically, if a raft of other conditions are met, such as no columns allowing NULLs and no triggers, then when you update an existing row by changing a column value not involved in the primary key, the engine can do an *in-place update*, writing only the changed data back to the table. Obviously, this is a lot faster than the usual replacement of the entire row that happens on update.
That's a great question I've been curious about myself. I know in MS SQL you can generate SQL plans based off of queries so that would be worth seeing if MS SQL compiles them any differently or if they're optimized the same way. 
Sounds like you are dealing with a linked database in acess so you are not really making much changes in access, its all done to SQL and you see a copy. 
The OP is asking about speeding up a where clause on an unindexed field and the point I'm trying to make is that it doesn't matter where the table is (remote or local), if the column you're trying to filter on is not in an index key, there is nothing you can do to speed it up or slim it down. In your code example above, if you're working locally on remoteserver, the lack of an index on d.x is the same as if you were working on (localserver) and trying to filter where a.x = 1 or b.x = 1. If you only need to use the data from the remoteserver once, then I agree that OPENQUERY can be a great solution, but if you need to use the data several times, like in a stored procedure, then it's almost always best to get everything over from the remote server into a local temp table, and use the temp table from that point forward. I have a "product" table on my local DB with about 5M rows in it. Running a query to select the PK with a filter on an unindexed column resulted in 72000 logical reads and a runtime of about 400ms. I created an index on the filter column with the PK in the include, and reads dropped to 516 and runtime to about 60ms. Again, I agree that every effort should be made to slim the data set down as early as possible, but there is only so much you can do when the column you need to slim down isn't indexed. 
[HN discussion](https://news.ycombinator.com/item?id=15377339)
The reason it didn't work before and now it does is actually pretty simply: By putting the left join condition in the where clause instead of part of the left join clause, you are actually forcing all rows conform to the clause. As customers without an order don`t conform to the clause in question, it implicitly changes the left join into an inner join. With that being said, there's a good practice thing you should do here. It's a more advanced notion, but there's nothing wrong with starting early :-). You are not evaluating the o.besteldatum in your clause, but the result of functions on said field. This is a bad idea since SQL cannot evaluate the result of a function by any other mean than testing the value itself. This means that if you had larger tables with indexes to help with performance, said indexes would be bypassed in order to test every row. When writing a clause that includes both a constant and a field, you should always leave the "field" part as naked as possible, which will execute orders of magnitude faster. It's entirely optional while training with sample data sizes, but whenever you reach the job market, it's a really good habit to have. Your left join clause should read: (you can make it easier to maintain by using a variable as well, so you only have to change it once when running against another month.) left join order o on o.custid = c.custid AND o.besteldatum &gt;= '2014-12-01' AND o.besteldatum &lt;= EOMONTH('2014-12-01') 
&gt;The OP is asking about speeding up a where clause on an unindexed field and the point I'm trying to make is that it doesn't matter where the table is (remote or local), if the column you're trying to filter on is not in an index key, there is nothing you can do to speed it up or slim it down. That isn't true. You are correct it won't speed it up because of the index, but I believe you are incorrect as it relates to being on the local or remote server. If OP is asking about speeding up the where clause, then put it inside an openquery. &gt;If you only need to use the data from the remoteserver once, then I agree that OPENQUERY can be a great solution, but if you need to use the data several times, like in a stored procedure, then it's almost always best to get everything over from the remote server into a local temp table, and use the temp table from that point forward. Totally agree, you throw it in a #table and index it for your join. &gt;Again, I agree that every effort should be made to slim the data set down as early as possible, but there is only so much you can do when the column you need to slim down isn't indexed. I agree, I'm just pointing out that selecting it from the remote and putting it on the local (i.e. using OPENQUERY) will slim the total execution time down, and slim the data for the join. It won't improve the read because of there being no index. 
Just to add to the discussion, some people (including myself) try to avoid char as a datatype, opting for varchar all the time. I'm coming from an Oracle background, so SQLite may be a little different. But char is not faster than varchar, and it does not save space either. But it can be problematic at times due to the way the RDBMS pads char values out to the number of defined characters. So if char has no advantages but does have some disadvantages, why use it ever? (Except for maybe char(1) because it will never be padded). The Oracle expert Tom Kyte famously avoids the char datatype in almost every situation. He has explained his reasoning several times online. http://www.google.com/search?q=ask+tom+char+varchar
it looks like your case statement should work, but i would create a mapping table with those 2 columns and join to it. then you have this mapping table you can reference with all possible variations instead of a hardcoded select. its also expandable in that you can add more metadata to the groupings in the future if you ever need to do so.
It might work fine depending on your data... but if you have a row where `[EMPLOYMENT FIELD 2]` is not in either of those lists and is not `NULL`, you'll end up with `NULL` as your output... maybe add an `ELSE` at the end to handle those cases? Also your 3rd `WHEN` statement should probably be ([BENEFIT CLASS] = 'FULLTIME') AND (I.[EMPLOYMENT FIELD 2] IS NULL OR (I.[EMPLOYMENT FIELD 2 IN (...)) Not that it really matters in this case, but it does has a different meaning, and if you add an additional `WHEN` statement afterwards, it might cause issues :) I like to hard-code values in a simple select while trying to work out the details of a complex `CASE` statement: DECLARE @bclass nvarchar(16); DECLARE @field2 nvarchar(16); SET @bclass = 'FULLTIME' SET @field2 = '999' SELECT CASE WHEN @bclass = 'NBE' THEN 'NBE' WHEN @bclass = 'FULLTIME' AND @field2 in ('024') THEN 'NBE' WHEN @bclass = 'FULLTIME' AND @field2 IS NULL OR @field2 IN ('044') THEN 'FT' ELSE @bclass END
https://asktom.oracle.com/pls/asktom/f?p=100:11:0::::P11_QUESTION_ID:2668391900346844476 &gt;Anytime you see anyone say "it is up to 50% faster", and that is it - no example, no science, no facts, no story to back it up - just laugh out loud at them and keep on moving along. " - Tom I like this guy
use order by on age CASE WHEN age between 0 and 2 THEN '0-2' WHEN age between 3 and 5 THEN '3-5' from table order by age ;
I wouldn't waste your time. Excel just won't support that kind of data. You're probably going to have to find another medium for them to review the data and use SSRS to just generate the CSV file for them to use. 
What's the question? How to stop Excel from crashing? I'm going with, "don't populate 1800 sheets at once". You're going to be limited by hardware. The client should be informed of this. I doubt it would be worth the effort to make it work. It may never work. Even if it does work, chances are it won't work for all users and they won't be happy. Sounds like a no-win situation for you.
Do teams write queries together at the same time? It would be difficult to track the data in your head when multiple people are changing the query. It's a neat idea in theory but does not really solve any problems that I have encountered.
mmddyyyy exactly like that without any other formatting? You could CONVERT to style 110 then use REPLACE on the - dashes like: REPLACE(CONVERT(nvarchar(30), value_date, 110), '-', '') this should also work: FORMAT(value_date, 'MMddyyyy', 'en-US')
Actually the issue is SSRS won't even generate it . . . crashes or times out after hours. I agree it's totally ridiculous, even if it could work, I don't see how it would be actually useful to have all of this in a mega Excel workbook. Hell I doubt the average office PC would even be able to open it. But, client is complaining they can't do it, shit rolls downhill and they're our biggest customer. I'm going to try and use a macro or something to just mock up a fake Excel workbook with 1800 sheets and see if it's even possible with fake data.
Maybe an SSIS job or sproc to export it? That won't solve the issue of Excel not being able to handle the size though...
Yeah, I'm going to try and work that angle and see if there's a way we can pre-summarize some of the data so SSRS's job is a little easier. The sproc it runs now returns about 20 million records when they run it for all locations, then of course this is summarized/aggregated on the SSRS side.
It's probable when there's a database lock, in which case data can only be "pulled" or composed from a fixed input. Overall though, didn't seem like a problem most people would encounter.
Even if it is possible, it's not going to be anywhere near optimal. Break it up by region, state, etc, and provide a separate report to summarize the entirety.
I would need to know your definition of what a "DBA user" is to do that. "DBA User" would have many different interpretations to different people. Some would equate that to system admin, others have it defined with an AD group. Others could have it defined on a custom database role. Have you looked in the security tab in SSMS Object Explorer? It will be under the server itself and under each database. I would poke around there and see if you can find your answer to what the definition for "DBA User" is in your environment. Once you have that defined you can start asking the, "Now how do I do that with a query?" question.
You know what? You're right, and I'm gonna give you this one. Using the same "product table" scenario I laid out earlier, over an average of 10 runs each, all from a cold cache, the OPENQUERY query was about 100ms faster than querying the remote server directly. Well done brah, srs. 
Thanks. If the remote server was over a shitty archaic network that's part of a cluster fuck of other servers and databases and you're talking about tables that go into the millions it becomes a massive difference.
Maybe they are really looking for some sort of roll-up on the data? Might be worth asking instead of going on a quixotic quest.
Can a person reasonably sift through **nearly two thousand** sheets in a workbook? And to what end? Ignoring **all** of the technical issues here, it's ridiculous to think that this is going to be useful to the end-user at all. Your clients have not considered in the slightest how they'll be using this output. Sounds very much like a "well, it should be possible, so that's what I want even if it's unreasonable" coming from your users. They've already made their mind up that this is what has to be done and to change that now would mean having to admit they were wrong. There's no winning with people who think this way unless you can get someone higher on the food chain than they are to tell them it's not going to happen and they have to accept that fact &amp; move on.
You're preaching to the choir. Financial &amp; operational reporting is my life, I know all these things. From a business perspective, I know this would be completely useless. This request is coming from the top of the food chain. I'd rather not say who, but this is coming from the CFO of a very popular fast food chain in the southern/eastern US, if you're south of St. Louis and east of Dallas there's probably one in your town. I'm about 75% sure they're trying to fire us and not being able to create this report is simply the excuse they'll use to say that we were "unable to meet their needs" and get out of their contract.
250MB of data in Excel is not a ton, I've successfully worked with multi-GB workbooks. I would change it, if possible, so that all data is in one sheet, separate from whatever the customer is actually looking at. Add a column to indicate what the site is. Then, on the sheet that the customer uses, provide a dropdown or other filter for them to choose the subset of data they want. This will generate way faster (not having to automate creation of many sheets) and you are probably dealing with some sort of challenges around having 1800 sheets with unique names and length limitations. How to they get to the sheet they want? Anyway, something along those lines might be a much better solution all around. Or find a non-Excel solution if possible. Does it have to be published from SSRS? What about an Access Database (I'm sorry, I usually wouldn't suggest this, I hate Access lol) with reports? Without more info, I think my first suggestion will likely be at least a lead on your best solution.
SSRS is what I have to work with. It's a pretty big and complex report, around 35 columns, probably 900 rows of data for each location. So, that times 1800 locations. It's a standard financial metric report we use for all our customers, they love it, just love it so much they want to see ALL THE THINGS on it. It's not just plain data from a database it has tons of calculations on it - year over year comparisons, trend comparisons this year vs last, tons of aggregates and groupings, etc. Downvote for mentioning MS Access (no not really) 
https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/principals-database-engine Look into joining master.sys.server_principals to master.sys.server_roles And joining dbname.sys.database_principals to dbname.sys.database_roles 
Why it is wrong: Product_size is not in tableA. How to fix: look into "in" condition instead of =.
Welcome to technology. We were all new once. Step one is that you have to actually try. What have you tried? No, it's not easy, but we can't learn for you.
I thought that it's unreasonable as well, so I manually created a workbook with multiple spreadsheets via duplication in Excel 2016 (64bit). I didn't know how large ops sheets were, so I created a sheet with 51*26 entries without additional formatting. Duplicating 1600 sheets to a final of 3200 sheets took about 11 minutes on my i7-2600k while using only a single core and &lt;300MB ram. The resulting workbook is about 20MB in size. Loading the workbook only takes a few seconds and making changes in it don't cause any stutter. As far as I can tell, there is no reason to believe that Excel itself can't handle the requested workbook. Update: Tried again with bigger sheets (900*35) and not removing formulas used to populate cells this time. Duplicating 900 to go up to 1800 took 5 minutes. The resulting file size is 700MB and loading it takes about 3min 30sec from an ssd. I have only 8GB ram installed so my system seems to have had to shuffle around a bit. Max ram usage shown while loading was 4300MB. In idle or when editing the file, ram usage stays around 100MB. Navigation through the workbook could be done via an index sheet. If I didn't miss anything, I can actually say that it's pretty reasonable to create such a report for someone who does not want to leave his Excel environment in his daily routine.
I've been using SSIS for basic ETL for about a year now (mostly self taught) but my new position requires a lot more ETL so the boss recommended a training to help round out my experience. I also do the data visualization and analysis after ETL is done. Im about two to three hours into a Pluralsight.com course on SSIS and recommend it. The lessons are on SSIS 2008 R2 but most everything is the same as 2016. The author takes a methodical, step by step approach to each package type and a demo for each. They also have a few courses on BI, which look good. Ive been happy with PluralSight and recommend you check them out. $30 a month seems pretty reasonable for you/your company for what you get out of it. Integration Services Fundamentals course from https://app.pluralsight.com/courses/ssis-basic
Sometimes a divorce is best for both parties. Source: Used to do financial consulting and reporting.
/u/camelrow has some great ideas and insight. I think learning BI and architecture / ETL best practices by searching for great reading materials is your best bet. With a solid understanding under foot, you can come up with some test scenarios to practice in SSIS along with the resources camelrow listed. What I'm thinking of starting which you may consider after a few months, is looking at BIML. Which is like if you wanted to script the creation and generation of SSIS packages and make it re-usable and quick. (That's the idea anyway. I've seen some presentations and it looks cool, just a VERY steep entry curve is what I'm told.)
Agreed. I've found one should always prepare for the worst when dealing with remote servers. Also, bear in mind that the further you get from the data (cache &gt; local database on SSD &gt; remote database (on same instance) on SSD &gt; remote server in same datacenter &gt; remote server who the fuck knows where) the access time and latency typically increases by several orders of magnitude at each step. I'm fortunate enough to have access to Fusion-io drives and even they are about as fast as a carrier pigeon when compared to reading from cache. 
We had an audit trigger on the table that once turned off fixed the problem. Thanks!
Yeah I tried something similar as proof of concept, ran the report for one location, then used a macro to copy it 1800 times to 1800 new worksheets. I ended up with a ~200mb file that takes about two minutes to open on my machine (OC'd i7 3770k, 32GB ram, 4x SSD in RAID10). I mean, it works, but definitely not the way I'd want to view this data.
[Relevant](https://signalvnoise.com/archives/001053.php)
One of my clients was company that was a sports team that has red socks for a logo. They were *the best* client. Their accounting person was probably the ideal client. One time we had a bit of an issue with a process during a phase of their deployment. We both stayed on the phone until like 9PM one night until we got the problem solved. It could have been nightmarish. It could have been the worst workday of my life. It wasn't though because the client and I had a great relationship. The next day we talked to them and they were so happy we helped them work through the problem and she was a *dream* to work with. Seriously. Every time she emailed or called she got the best and immediate service because it was such a pleasure to work with them. Then we had this other client. Banking company that did issues health insurance cards. Complete disaster. Their CFO did nothing except complain about everything. Missed an arbitrary deadline that didn't affect the project at all? Well he's calling and emailing every person on the team and demanding a refund. We tell him that something isn't possible due to a technical limitation? He's threatening to sue. Has a problem with a report that he's looking at on a Sunday? He wants an immediate response or he's not paying his bill. Guess which project I walked out on? The consulting firm I worked for was desperate for clientele because they were constantly buying out smaller companies (like the one I worked for) and then running their LOB into the ground through firing all of the new consultants and putting the work into the hands of the tenured people. Since the clients didn't like any of the new people that they got "stuck" with, they left in a hurry. So the consulting firm took any business they could get. That article is spot on about the two-way relationship and how you are hiring your clients as much as they are hiring you.
Better yet, take a look at EXISTS
&gt; I think learning BI and architecture / ETL best practices by searching for great reading materials is your best bet. He's literally here asking for material, and your answer is "You should search for material"? Wtf?
UPDATE tableA SET product_price = product_price*2 WHERE product_color = "B" AND product_num in (SELECT product_num FROM tableB WHERE product_size = "S");
Yup, pretty much.
Could you use a data driven subscription to generate each sheet separately, then use a macro to compile all of them together?
Define a string and a table var. Use a while loop to load the arguments from the string into your table var. Basically, you want to turn your list into a table that looks roughly as follows: argument a;_;_;_;_;_;_;_ _;b;_;_;_;_;_;_ _;_;c;_;_;_;_;_ _;_;_;d;_;_;_;_ _;_;_;_;e;_;_;_ _;_;_;_;_;f;_;_ _;_;_;_;_;_;g;_ _;_;_;_;_;_;_;h Then SELECT DISTINCT table.* FROM table INNER JOIN @tableVar ON table.list LIKE @tableVar.argument + '%' Pardon syntax / formatting errors, I typically use MSSQL but this should get you on the right track. If the parameters don't have to match exactly (the value of parameter a can match any position in the list), then you can get rid of the wildcards / semicolons in your table var and join on table.list LIKE '%' + @tableVar.argument + '%' FWIW, you can avoid the clunky while loop if you can use stored procedures.
update tableA Set tableA.product_price = 2*TableA.Product_price from taleA inner join tableB on (tableA.Product_num=tableB.product_num) where tableB.Product_color ='B' and tableB.product_size = 'S';
The only way to really acquire these skills is through constant engagement with the field. A course is a good jumping off point, but good ETL practices aren't a set it and forget it type skill. Very few companies needs will *force* you to stay current so you gotta keep up with the rest of the world.
You *may* be able to [attach the MDF and LDF files](https://dba.stackexchange.com/q/30440/35474) to a reinstalled instance of SQL Server, but there's a possibility of corruption in those files depending on the system state when the crash happened. If you can get the database attached, make sure you run DBCC to thoroughly check for issues. Once you have things back together, install [Ola Hallengren's Maintenance Solution](https://ola.hallengren.com/) or [Minion Backup](http://www.minionware.net/products/backup/), get the jobs scheduled, and have them create real backups to a location that's backed up properly on a regular basis. Both of these solutions are about as close to "fire and forget" as you can get. Set them up properly and they'll run your backups for years. You might want to consider something in the next contract with them to the effect of "if you don't implement certain things per our recommendations, we can't support your implementation without significant additional cost" to head off the issue of them running a critical service on trash hardware with no backup plan.
Also, if they used TDE and don't have the key... well that's that.
&gt; how to find banking transaction amounts that are duplicate, but have different memos and could have different trade dates Possibly this? SELECT RIGHT(AccountNumber,4) AS 'Account Number (last 4)', VoucherNumber, Amount FROM BankingTransactions a LEFT JOIN joinVoucherBankingTransaction b ON (a.BtID = b.BtID) WHERE SuID = 12345 GROUP BY RIGHT(AccountNumber,4) AS 'Account Number (last 4)', VoucherNumber, Amount HAVING COUNT(*) &gt; 1 https://stackoverflow.com/questions/2112618/finding-duplicate-rows-in-sql-server At the very least, the logic is there.
Good point. Though I'd hope that if they were using Enterprise Edition and *hadn't* found their way into doing backups, they also wouldn't have found TDE.
And good lord please make backups of your backups of your MDF and LDF files before you do anything lest you end up overwriting them and ending up with nothing at all. And if/when you get the DB online make a real backup and then another offsite backup!
Thanks for your reply, we told them before we started this adventure that we wouldn't be able to install any 3rd party applications, but they want us to try, so here we are, we've attached the database files in the studio manager, but when we try to launch and login to their third party app, it says that the server doesn't exist or access is denied.
Are you able to log in through SSMS using the same username/login?
Agree completely. Plus they could easily google SSIS practice and find any resources and references. If they provided us with what they have read and what it was missing, we could better help them with other suggestions and filling in the gaps. Learning how to learn and learning how to ask good questions are a more valuable skill set than proper ETL and SSIS performance. Build your foundation before you hang the glass chandelier. 
Is there a specific database engine you are working with? MS SQL I can answer the first two for you... 1. The database is made up of two files, the MDF (database) and the LDF (transaction log). 2. Yes.
1) varies a lot depending on db server. Where and how are the actual data stored should be a problem for DBA not for SQL programmer. 2) yes, files with .sql extension usually contain sql code. However this is SQL that lives outside of the DB server. Most database engines have some kind of internal storage for things like views - so if you'll find a random .sql file somewhere and edit it, nothing will happen until the server execute the queries contained within file. 3) it doesn't have much to do with SQL imho, but yes, .py is a common extension for python source codes. A .py file can contain a meaningful script (i.e. If you call python interpreter on it, it would execute some action) or it can be just a part of larger solution. I don't know what leads you to these questions, but they show that you have generally a relatively low understanding of some basic principles, maybe you should start with some essentials (learning about OS, programming languages and databases in general) and then move on to study some DB system in detail.
No engine specific atm, but judging from your comment this plays a role in the extension of the database file? 
I know python has nothing to do with sql in general, the only reason i asked was to help make a connection to a language i do understand, if any. I basically do have a low understanding, definitely not a software engineer or computer science major, ive only used python to analyze data, but from .data files recorded and stored on my own pc. By the way, appreciate the quick response.
&gt; No engine specific atm, but judging from your comment this plays a role in the extension of the database file? Yep! And in some instances, there may not be files at all - I know that in the past at least, you could configure Oracle such that it just took over a disk on which to store the data. The term for it is "raw" and there is no filesystem, and thus no files per se. Instead, just a big 'ol pile of disk that the database itself would manage.
Thats really interesting!
Re:3, you'll see a lot of python scripts floating around because there are more python coders than DBA's who share their code. Like, a python script will initialize a database, read data into it, do some process to achieve an output, then shut down. Handy for sharing to others. Scripts used by DBA's are specific to their production environments and tend to be made to minimize repetitive tasks, so we can spend more time drinking coffee and listening to silly user requests.
&gt; Msg 8155, Level 16, State 2, Line 6 No column name was specified for column 2 of 'A2'. So, you have a subquery you've aliased to A2. That subquery has two columns -- VendorId and Count(Distinct PaymentType). VendorId is...well, VendorId. The second column has no name as it contains the result of an aggregate. Change your subquery to something like this: SELECT vendorid, COUNT(DISTINCT paymenttype) PaymentCount FROM ZpCustomers_Kim.dbo.VendorListPaymentTypeChangeAudit group by vendorid That will solve your error about column 2. &gt; Line 8 Invalid column name 'paymenttype'. You've aliased your subquery to A2. Your last condition in your where clause has "2." instead of "A2.". That said, your condition should be "A2.PaymentCount &gt; 1" if you aliased the aggregate in the subquery to PaymentCount.
:) I consider this popping my /r/SQL cherry. A few years ago I came here and didn't know how to `select * from table`. But at one point I looked into openquery() rather extensively to optimize a process and agree that you should always prepare for the worst when dealing with any SQL process. 
Thank you! I will have to wait until tomorrow to try it out (I lack remote access). So, I have to wait until tomorrow to see if I can grasp what you're saying (obviously I'm pretty novice at SQL...) 
I don't have a lot of experience outside of MS SQL, but yes, I believe it does, a quick Google shows MySQL uses .frm, .myi &amp; .myd files. Each DB engine is quite different, including T-SQL syntax can be engine specific. To make it more confusing, MS SQL doesn't actually care what the files are called, so you can rename the MDF and LDF to have any extension, it is the file structure it cares about. 
&gt; Turns out for some reason Access 2016 uses * instead of %? also, Access is not MySQL
Really? My tutor fucked up then. What does it run? 
 ;with y1 (createddatetime, itemid, custaccount, dataareaid) as (select max(createddatetime), itemid, custaccount, dataareaid from salesLine group by itemid, custaccount, dataareaid) select t1.orderaccount as CUSTNO, t3.NAME as "COMPANY", t1.createdby as "SALESMN", FORMAT(t1.invoicedate, 'dd-MM-yyyy') as "INVDATE", t1.invoiceid as "INVNO", t4.salesid, -- ter controle t4.itemid as "ITEM", t4.name as "DESCRIP", CAST(t5.QTYORDERED as INT) as "QTYORD", CAST(t4.qty as INT) as "TQTYINVOICED", CAST(t5.salesprice as money) as "PRICE", CAST(t5.lineamount as money) as "EXTPRICE", CAST(t5.costprice as money) as "COSTPRICE", t6.ITEMGROUPID as "ITMCLSS", FORMAT(y1.createddatetime, 'dd-MM-yyyy') as ILORDR from CustInvoiceJour as t1 inner join CustTable t2 on t1.orderaccount = t2.accountnum and t1.DATAAREAID = t2.DATAAREAID inner join DirPartyTable t3 on t2.party = t3.RECID inner join CustInvoiceTrans t4 on t4.invoiceid = t1.invoiceid inner join SalesLine t5 on t5.salesid = t4.salesid AND t5.itemid = t4.itemid inner join InventItemGroupItem t6 on t4.itemid = t6.itemid inner join y1 on t4.itemid = y1.itemid and t1.orderaccount = y1.custaccount and t1.dataareaid = y1.dataareaid where 1=1 and t1.DATAAREAID = '101' and t1.invoiceid = 'INV1703725' order by t1.invoicedate DESC 
I agree with both people who have answered so far. From the example given, you don't need the full outer join, but mostly it's the DISTINCT. Can that and test the performance, then ask yourself if you actually need it. I'm going to upvote you because you did an excellent job crafting your SO question.
Access.....
&gt; backups of your MDF and LDF files before you do anything Listen to Cal1gula. We don't wear our scars on the outside, but we bear them all the same.
Thanks, I put in some effort into constructing my question and I'm glad it turned out okay. You're right, I did need to refactor my query. I followed the answers and I've noticed a few changes for the better. It certainly seems like I'm headed in the right direction!
Thanks, its still slow. I've tried the CTE approach, also a direct join on a subquery table I think what im asking is just too much, and there's no decent index on the table. That's the reason it's slow.
 ;with tmm (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select id, ContentID, ChronoOrder, UpdateFlag, row_number() over (order by ContentID, ChronoOrder) from TheMajesticMoose), PriorRec (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select * from tmm where UpdateFlag = 0) --/* comment toggle update upd set ChronoOrder = PriorRec.ChronoOrder --*/ select upd.*, PriorRec.ChronoOrder from TheMajesticMoose upd inner join PriorRec on upd.rownum = prev.rownum+1 where upd.UpdateFlag = 1;
The CTE is syntactic sugar in SQL Server (unlike Oracle); using a CTE in most cases will not change performance one way or the other.
&gt;select top 1 y1.createddatetime Try with `max(y1.createddatetime)`. May or may not help, but worth a shot. What indexes do you have on your tables? Can you capture the execution plan and post it to http://pastetheplan.com/ (and give us the link)?
Rather than use a CTE or subselect, put your max order date by item/customer/area into a temp table, then `join` to that. If performance still lags, you can put an index on the temp table to help out. Or you may find that you need an index on `salesLine` in the first place. If you can filter the data that goes into the temp table (like by customer account, or a date range, or a data area id), even better. A temp table may *seem* like more work, but it can actually result in *less* because the query optimizer has more/different/better information with which to produce an optimal plan. select max(createddatetime) as lastorderdate, itemid, custaccount, dataareaid into #lastorder from salesLine group by itemid, custaccount, dataareaid select t1.orderaccount as CUSTNO, t3.NAME as "COMPANY", t1.createdby as "SALESMN", FORMAT(t1.invoicedate, 'dd-MM-yyyy') as "INVDATE", t1.invoiceid as "INVNO", t4.salesid, -- ter controle t4.itemid as "ITEM", t4.name as "DESCRIP", CAST(t5.QTYORDERED as INT) as "QTYORD", CAST(t4.qty as INT) as "TQTYINVOICED", CAST(t5.salesprice as money) as "PRICE", CAST(t5.lineamount as money) as "EXTPRICE", CAST(t5.costprice as money) as "COSTPRICE", t6.ITEMGROUPID as "ITMCLSS", FORMAT(y1.lastorderdate), 'dd-MM-yyyy') as ILORDR from CustInvoiceJour as t1 inner join CustTable t2 on t1.orderaccount = t2.accountnum and t1.DATAAREAID = t2.DATAAREAID inner join DirPartyTable t3 on t2.party = t3.RECID inner join CustInvoiceTrans t4 on t4.invoiceid = t1.invoiceid inner join SalesLine t5 on t5.salesid = t4.salesid AND t5.itemid = t4.itemid inner join InventItemGroupItem t6 on t4.itemid = t6.itemid inner join #lastorder y1 on on t4.itemid = y1.itemid and t1.orderaccount = y1.custaccount and t1.dataareaid = y1.dataareaid where t1.DATAAREAID = '101' and t1.invoiceid = 'INV1703725' order by t1.invoicedate DESC
from https://www.connectionstrings.com/sql-server/ : Trusted Connection Server=myServerAddress;Database=myDataBase;Trusted_Connection=True; Standard Security Server=myServerAddress;Database=myDataBase;User Id=myUsername;Password=myPassword; edit: if database doesn't work, try Initial Catalog instead. sometimes you need to specify that you want to use SSPI (Integrated Security=SSPI) when using built in security. If you need encryption but don't have a signed cert, use TrustServerCertificate=true. edit2: I usually use powershell when debugging conenctions to sql server, it uses the default .net types that c# can use aswell so it should be translatable for anyone using c#: #connects to localhost, instance sql2017, database demo, using integrated security and selects the name of a table at random to prove connectivity from the computer where the script is run $conn = New-Object System.Data.SqlClient.SqlConnection $conn.ConnectionString = "Data Source=.\sql2017;Initial Catalog=demo;Integrated Security=SSPI;"; $conn.Open() $cmd = New-Object System.Data.SqlClient.SqlCommand $cmd.CommandText = "SELECT top 1 table_name FROM information_schema.tables;" $cmd.Connection = $conn $results = $cmd.ExecuteScalar() $results
&gt; ;with tmm (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select id, ContentID, ChronoOrder, UpdateFlag, row_number() over (order by ContentID, ChronoOrder) from TheMajesticMoose), &gt; PriorRec (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select * from tmm where UpdateFlag = 0) &gt; --/* comment toggle &gt; update upd set ChronoOrder = PriorRec.ChronoOrder &gt; --*/ select upd.*, PriorRec.ChronoOrder &gt; from TheMajesticMoose upd &gt; inner join PriorRec on upd.rownum = prev.rownum+1 &gt; where upd.UpdateFlag = 1; I'm not sure this works, prev isn't specified anywhere as an alias. 
12 seconds! I don't understand yet why it's faster than a CTE... but thanks :D
I tried Trusted_Connection = True; and TrustServerCertificate=true; Tried running the program but I got Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); Application.Run(new Form1());
if you run into errors, check thestack trace in the error and try and isolate the problem. Make sure it is actually a problem with your sql connection and not anything else. If you are using visual studio, try debugging with a stop near the part of the code where you get the exception.
The critical part is the Integrated Security setting as that is the difference between your school and home computer. At school you are logging in with a SQL login. At home, you appear to be logging in with a Windows login (integrated).
Sorry, I renamed it to PriorRec but failed to do it everywhere. 
It's all good. in addition, did you forget a partition by for the rownum section? to partition by content id? 
CTE or no, I took it from being a correlated subquery sorting and returning top 1 createddate to a function in the select clause and moved it to a straight join, which should have made a difference. 
Examine the execution plan, it'll tell you what the query is doing. CTEs are nothing more than syntactic sugar in SQL Server. To over-simplify a bit, it's just a macro/substitution. By the time the engine is finished parsing, analyzing and optimizing the query, the CTE version is identical to your original. When you create the temp table, SQL Server builds statistics for that table which help it optimize the query better *and* is better able to utilize indexes and statistics about that `SalesLine` table in doing the filtering &amp; aggregation for the latest order date. If you can get that temp table down to *just* `dataareaid = 101`, I think you'll find that things run even faster. Even more so if you can create indexes to support your query. Solar Winds has a good [infographic on optimizing query performance](http://www.solarwinds.com/-/media/solarwinds/swresources/infographic/1403_confio_sql_server_tuning_infographics_8_5x11.ashx) and the first step is to make sure you know your rowcounts (cardinality). The earlier you can reduce the number of rows you need to examine, the better off you'll be. The temp table in this example helps achieve that.
You can add that if you want and it would be ‚Äúsafer‚Äù in case the real table is different than the example. 
I think this is close, but not there yet. The last update statement says from MajesticMoose (great DB name btw) but that doesn't work, is that supposed to be tmm?
Without seeing the actual execution plans for both versions, one can only speculate as to what differences there are between them.
No it should be TheMajesticMoose but if you added the partition by ContentID then you need to add that to your on clause as well because rownum won‚Äôt be unique by itself. If I sound vague, I‚Äôm on mobile and need bifocals so can‚Äôt read the code well at the moment. 
When I try to do the last join in the update statement, I get rownum as an invalid column name, for the "upd.rownum=Priorec.rownum+1
Thanks a lot for your effort. When I'm writing queries I always wonder if it's the fastest / most efficient way. I've learned something today again :)
**In this case**, the execution plans are identical so it won't make a difference. (BTW, if you want to share execution plans check out https://pastetheplan.com/ from /u/brentozar &amp; Co.)
&gt; ;with tmm (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select id, ContentID, ChronoOrder, UpdateFlag, row_number() over (partition by ContentID order by Id, ChronoOrder) from TheMajesticMoose), &gt; PriorRec (id, ContentID, ChronoOrder, UpdateFlag, rownum) as (select * from tmm where UpdateFlag = 0) &gt; --/* comment toggle &gt; update upd set ChronoOrder = PriorRec.ChronoOrder &gt; --*/ select upd.*, PriorRec.ChronoOrder &gt; from TheMajesticMoose upd &gt; inner join tmm on upd.id = tmm.id &gt; inner join PriorRec on tmm.rownum = PriorRec.rownum+1 and tmm.ContentID = PriorRec.ContentID &gt; where upd.UpdateFlag = 1; I do vaguely remember removing a join that no longer seemed necessary. Doh. Here‚Äôs an attempt at an update. Still mobile so it probably has something horribly wrong. 
You're a very nice person for helping me like this. This problem is very frustrating and I thought I had it licked before, but double maintenance flags screwed me up. 
Oh, two update flags within a contentid! Maybe if you also unset the flag upon update, the update could be re-run until there are no more update flags? That sounded gross just to even suggest it. 
Yeah. So your code does the same thing mine did before. It works, but with consecutive flags it fails. I've been trying to find a way to pull the value of ChronoOrder where the first flag before a maintenance flag in sequence, is but I can't seem to get it. I had originally written this as a lag function. So if you had content id 1 for 10 rows, and then 0011001011 how would you update the chrono orders? It's a bear of a function and any help you can provide would be great!
Even tho that may have seemed pretty obvious i was still a little uncertain, thanks for clearing it up for me :) I edited the post to include the plan form Brentozar, nice to know about that tool.
Two updates in a row is one thing (maybe dense_rank would work), but two in a row with different chronorders is trickier. 
How would you handle 3 in a row? Or X number in a row? I can think of no other way than setting that maint flag to 0 where it was updated, then run process again. 
Use a #table and index it if necessary.
u/coldflame563 I have 2 questions that will help me, and may help others: 1. why should ID=11 be updated to 1 and not stay 2? 2. should ID=13 stay at 3, as the row above it (with or without any partitioning) is 3? Also, it doesn't look like ID=16 should be updated in any case, as its UpdateFlag=0. Can you clarify exactly what you mean by "the prior row (by ContentID and ChronoOrder)?" (by ContentID and ChronoOrder) kinda implies a ranking function possibly with a partition, but partitioning by both ContentID and ChronoOrder would yield the same rownum for ID=3 and ID=4, which would run afoul of your first requirement that ID=4 gets updated to ChronoOrder=1. 
Id 11 should go to 1 and not stay as two, because the business logic states that the lowest chrono order where maintenance is not required is what the record should be. I say prior row by content ID and Chrono order so that it's not just an explicit prior row,but rather the prior Chrono Order row where content id is the same. The table has a date field (not shown) where things are further ordered, but it's not necessary to this question. As for a possible solution, I think I might've come up with a rather interesting one that requires no loops. By creating a table with ChronoOrder lag and lead where update is 0, to provide upper and lower bounds, I can "join" to that table and then update to the lowest bound if the ChronoOrder falls between them and the update is one. 
with System.Data.SqlClient.SqlConnectionStringBuilder it's literally got the SOLE purpose of doing EXACTLY what you're asking about.
&gt; retype and reformat As both SO and reddit use markdown, I don't see much reformatting involved. But, perhaps I'm missing something?
Perfect that works. So essentially my issue was not having an alias for the calculated column in the subquery. Thanks again! 
If you are going to looking at execution plans a lot then definitely think about using SQL Sentry plan explorer pro, they made it free recently and its the tits balls. I don't really know what you are aiming to learn but it seems like you are trying to learn how MS SQL Server is creating the execution plans and how it is finding the data, brent ozar has a set of videos to learn how sql thinks (but you have to pay for access to the videos) but there are some abridged videos on youtube that are pretty nice and quick, definitely worth checking out, they arent going to help you pass a mcsa/mcse test but they will absolutely help in real life situations and problems you are going to encounter: https://www.youtube.com/watch?v=ShAtKVTsq7Y https://www.youtube.com/watch?v=uwGCPtga06U
Heartily endorse Plan Explorer. But for sharing online (like here), pastetheplan.com is my go-to.
I would recommend MS Access as the front end (never the back end!), as it's inexpensive and productive for making data GUIs. And PostgreSQL as the database server, as it's free and open and well made
for the love of sql standards, would you please identify your platform
Microsoft Access my apologies
And, at least for SQL Server, the files don't really need to have a particular extension, so you don't want to make assumptions.
 DateSerial(Left(numDate,4),Mid(numDate,5,2),Right(numDate,2)) Where `numDate` is your column (string or number) formatted as YYYYMMDD.
Why delete the post though? 
What do you mean?
You deleted your post after you got an answer. Just seems odd.
Once I get the answer I dont care. I dont want 20 answers for one issue. I just want to know the issue and move on. 
Ah, well that's nice and selfish of you...
Doesn't it just boil down to: SELECT fa.id AS id , fb.d_id AS d_id , fa.name AS name , fa.disabled as disabled , fa.deleted AS deleted FROM test_r fa INNER JOIN test_r fb on fa.name = fb.name WHERE fa.name IS NOT NULL AND fb.name IS NOT NULL AND fa.d_id IS NULL AND fb.d_id = 1 AND fa.deleted = false AND fb.deleted = false ORDER BY name asc You should already have unique results due to d_id and name having a unique index, and also specifying d_id in the where clause. You shouldn't need to coalesce anything due to: * id is the pk * fa.d_id should always be null, and fb.d_did should never be null * already specifying name is not null * disabled sounds like it should be a bit / bool - shouldn't be any nulls * deleted field sounds like it should be a bit / bool - shouldn't be any nulls 
You were on the right track. You can't use `GROUP BY` with `WHERE`, but `HAVING` is the equivalent for use with aggregate functions. edit: Should clarify that you can use WHERE, just not in *combination* with an aggregate function.
Check out the example here and I think that'll be enough to get you to your final product... if not let us know. You were on track with the group by, just not in the where clause :) http://docs.oracle.com/javadb/10.8.3.0/ref/rrefsqlj14854.html
Why not simply export the data to (1) sheet in Excel, and then create something like a pivot table with a filter for location that will display the "raw data" on sheet (2)?
One of my clients once, while we were on a huge conference call, wanted to talk about what the definition of a, "phone call" was. I wanted to blurt out, "How can you not know? We're on one right now!," but no, I kept my mouth shut and listened to execs talk for an hour about what they consider a phone call to be, and not to be without anyone bothering to ask what the phone system defines it as, and how its configured to report on them. Then with much delight I was able to inform them that their definition was not compatible with the system they spend a million dollars installing.
&gt;SELECT customers.cust_name AS "Customers w/ 2+ orders" FROM customers INNER JOIN orders ON orders.cust_id = customers.cust_id GROUP BY customers.cust_name HAVING COUNT(orders.pid) &gt;= 2 This seemed to do the trick, thanks for the tip on HAVING! Not sure if this is the 'best practice' way of doing it, but it works on the console, so I'll roll with it :)
&gt;SELECT customers.cust_name AS "Customers w/ 2+ orders" FROM customers INNER JOIN orders ON orders.cust_id = customers.cust_id GROUP BY customers.cust_name HAVING COUNT(orders.pid) &gt;= 2 This seemed to do the trick, thanks for the tip on HAVING! Not sure if this is the 'best practice' way of doing it, but it works on the console, so I'll roll with it :)
Hopefully you were getting paid by the hour the whole time! It never ceases to amaze me what some people are willing to call a conference for...
edit: /u/Beefourthree makes a good point actually... I didn't look at your table structure first. You could group on customerid which would avoid any issues with duplicate names. Or what they suggested and use a subquery.
Cool, Sentry looks good. I've already seen the video on optimizing queries, but haven't seen the one about how SQL server thinks, i will check it out when i have the time for it :p In all honesty it was just something that struck my mind while practicing for the mcsa. I got the correct results but really just wondered if not using the WHERE clause would do anything to the execution plan and outright performance of it. It should have been obvious for me, but i guess i needed help seeing it :) Thanks for the reply.
The only other way I could see being able to get this is to use subqueries... which took me way longer than I would like to admit to get working: SELECT cust_name FROM customers c WHERE c.cust_id IN ( SELECT DISTINCT cust_id FROM ( SELECT o.cust_id, count(*) as count FROM orders o GROUP BY o.cust_id ) d WHERE count &gt;= 2 ) So, yeah, `HAVING` is the way to go.
I don‚Äôt want to discourage learning. But is there any reason it needs to be built from scratch? There are highly customizable solutions out there already that could be purchased. A clinic sounds like a medical database to me. It‚Äôs a very common SQL 101 project. But involves much more than a webpage with a few forms. The scope of this will grow exponentially once you start building. There are lots of usability, edge cases, workflow and important security factors to consider.
So what happens if John Doe at 123 Main St makes an order, and John Doe at 456 Second St makes an order? Grouping by a NAME column is rarely a good idea. I would create a subquery that groups ORDERS by CUST_ID and filters down to CUST_IDs with 2 or more orders. Then join that subquery back to the CUSTOMER table.
I've once had similar silly requirements and the solution isn't that complicated. But you'll have to ditch SSRS as it's simply not made to handle such a edge case scenario. The only way I know of making this work is to actually go from the push model to the pull model to populate the workbook. This means that you'll have to learn rudimentary office VB to execute the query against the DB and instanciate the worksheet from there. In a nutshell, put all your data in a SQL table, field an extra field called sheetName. From the workbook, populate 2 recordset. One that does a select distinct on the sheetName, and another that is basically a "select *" query. From there, you loop row by row in the worksheetname recordset, and on each iteration, you filter the AllRecorset with the proper key, perform the sort you want, copy the worksheet from a template, rename it and finally paste the recordset. Rinse and repeat for as many sheets as you need to create. save the workbook when you finish the loop and you're done. I've once done it with 800 sheets or so and it worked well.
 &gt; This isnt a real database so I can't test it out. In other words you're asking for help with your homework... also http://sqlfiddle.com/ for all your non real database, database needs.
Try grouping on `(MONTH(datejoined)-MONTH(viewdate))` as well as branchno and use the month function rather than the number of days elapsed between dates unless you really want the specific 30 day range for all months.
It's actually a take home interview test (shh)
Yeah, since you're dealing with medical information, you might not want to build it yourself. I would look into HIPAA regulations and probably a whole slew of other potential issues before just diving in.
SQL for Newbs through Udemy. It‚Äôs a really great course and I got it for 12 dollars! 
https://www.w3schools.com/sql/default.asp is a very basic overview of SQL as a language that transfers to pretty much any flavor of SQL... but it will NOT teach you important things like database normalization, data integrity, and constraints which are important when creating a production database from scratch. I would seriously consider reading up on at least database normalization or lookup existing schemas for the type of data you want to store. You can use www.SQLFiddle.com to get familiar with some more advanced queries / building the tables, or install something like SQL Server Development 2016 which is a full featured enterprise server that you can use to learn SQL... or of course MySQL, PostgreSQL, etc... depending on what platform you want to go with. Also check out the Wiki: https://www.reddit.com/r/SQL/wiki/index and /r/learnSQL/ for additional resources.
PHP, MySQL and Javascript is what i'm currently learning/doing. You can install XAMPP to have a local webserver on your PC. Create your database where all your content will be stored. Then basically create your webpages as php files, where inside you're using HTML/CSS and JS/jQuery for the look and feel, and php to connect to your database and run queries.
Yeah I figured I was missing something with the dates. Other than that you think this should work?
Don't do this from scratch. Get a free CMS and a free ecommerce plugin/add-on. I love SAL but this project will never get done. If you don't know a server side scripting language like php, python or even client side... You are years from a working product. We all started somewhere. Implement an existing product, you will learn a lot.
One approach would be to not really learn SQL at first (both SQL and websites is a lot to take on at once). Start by building a website using an ORM framework like some flavor of MVC, which will do the initial work of starting the database for you. Then you can learn to connect to the database backend and start examining the code the ORM built, and start writing your own queries as well. Another option is to take a well-known example database (like Microsoft's AdventureWorks, StackOverflow, etc) as a starting point, and start building your own app on top of it. It is often much easier to start with something rather than from scratch, and this can expose you to different code styles, methodologies, etc. There are tons of resources, examples, and tutorials out there. Postgres is very popular and can be adopted without licensing fees and has a lot of technical depth and great syntax features. SQL Server has better tooling IMO and surprisingly excellent documentation, but if you ever want to go Production with it the licensing costs can be prohibitive. Happy Cakeday!
 SELECT [ID] , [Control Type] , MAX([Date]) AS latest FROM Control GROUP BY [ID] , [Control Type] 
Actually, I didn't think it through enough: GROUP BY branchno, DATEDIFF(MONTH, viewdate, joindate) My original suggestion fails for anything that crosses years.
Are you a DBA or just writing queries against the database? Syntax changes will be the big one to get used to. CTEs are convenient but not mandatory. If you're concerned about patterns you used in MySQL causing performance issues with MSSQL, talk to your DBA about how to adjust. Enjoy using SQL Sever Management Studio, it's excellent. ETA: Oh yeah, track down your local [PASS](http://pass.org) chapter and, if possible, drop in on a [SQL Saturday](http://sqlsaturday.com/)
I think the other answer here is very straightforward...maybe too straightforward to actually be the answer to what you're asking. Could you provide an example of your expected output from the sample data?
I completely agree, but this is for a university assignment, and the professor is a huge chode about anyone who does anything *more* than the requirements. So if you do anything that isn't explicitly in the described problem, you lose points. Even if there's a much better way that you'd definitely use in a PROD environment.
both of them are timeless and will serve you well today
The SQL language doesn't move terribly fast, and things aren't removed (certain implementations may deprecate features, but MS SQL Server has had some things marked "deprecated" for a decade and yet they live on). It's generally additive. What's in those books will be fine, especially if you're just starting out.
Sweet, thanks! 
Thanks, I'll use them to start off then and update myself afterwards.
I would say, in this project SQL is one of the simplest part you would have to deal with. 1. You need a database to keep all the data you need, but it's not to complicated 2. scrap the web or find open API to pick the data not simple and prone of failure as sites might change the pages you are scraping 3. some sql queries to prepare the data for a machine learning algorithm it can vary on difficulty based on data quality you have collected 3. build a machine learning model to predict. fun but not simple you can get some results without machine learning algorithms but they will be simple and in my opinion will not have as good results as a machine learning algorithm so not sure if this is the best sub to ask for help There are some github repositories which do at least part of what you are looking for but than for the real football aka soccer :) 
&gt;Then to predict winners BWAHAHAHAHAHA. Dude, if it were this easy, Vegas wouldn't be taking bets anymore because too many people would be winning money.
Nah man, he knows basic SQL, he's got it all figured out.
Dude. I know I've been at it a long time. If they played it on paper the favorite would always win. I'm not bad at sports betting already, I'm not your average public bettor who is betting on their favorite team because their good. I'm also not a sharp. I've paid for home improvements with some of my winnings. I'd just like to take my knowledge, and try to see if I can't increase my winnings.
I said in the post, I want to take the opportunity to learn more. I'm not under the impression that I can skate by with what little bit of knowledge I have, that's a decade old.
Thank you. github was very helpful. I've put together a Visio document of how I plan to put together the database...I don't want to do the work twice. The trickiest part is going to be getting the premium content scraped. Stuff behind passwords. I'll get MySQL up and working on a PC at home. I can put static information like team names, etc. in there as a start, and work on the dynamic information as I get it figured out. Eventually I'll get the prediction model set up. I've got it in my head, but I need to get phase 1 completed, getting the data from the internet into a DB first.
Case when month(ddate) &gt;= 7 then year(ddate) +1 else year(ddate) end
 YEAR(ddate) + CASE WHEN month(ddate) &gt;= 7 THEN 1 ELSE 0 END as FiscalYear 
thank you az
Going to need some more info about the table here methinks. Unless me suggesting that you try `HAVING SUM(Transactions) &lt;&gt; 0` is enough to get you there. Which is me assuming that the debits and credits are all in the same table and that the + and - are in the same column.
Another answer works well, but I always build a date dimension table in my databases. Fiscal year is just one of around a hundred columns in that database and would also work for this.
Assuming that the only way to identify your transaction is based on amount and doesn't contain a customer id for example. Treat it as a logical answer, you can figure out the functions... Sum(If(transaction &lt; 0, 1, If(transaction &gt; 0, -1, 0))) Group by absolute(transaction) Where funky logic is not 1 Assign negatives as 1's, positive vals as -1's, sum these up and an overall transaction total should be 1, meaning that there is no unaccounted for dupes. Let me know how it goes, post back your query and i can help verify if needed
Just a question...what do you do if it's the first 6 days of July? Curious...
Want to know this too
That was a typo and should be the first. I believe the FY comment below might be the answer
This is my FY. How would the system FY dimension work in with this?
The key in your date table would be each and every date for whatever period of time you need coverage for. Your columns would include things such as... * Day of the week * ISO 8601 Week Number * Manufacturing Week Number * Fiscal Year * Calendar Year * Company Holiday * Federal Holiday Continue this on for a hundred other things. Many of these can be calculated in code, but it's often easier to just look up whether or not a day is in a fiscal year than it is to repeat that calculation in many different views.
simple if you rework the problem select sourceTable.*, dateData.result from sourceTable JOIN ( VALUES ( '07-JUL-2009' , '30-JUN-2010' , '2010' ) , etc ) dateData(start, end, result) ON sourceTable.ddate BETWEEN dateData.start AND dateData.END
Look at www.sqlsaturday.com and watch for events nearby. SQL Saturday is free day-long mini-conferences
The basics never change. SELECT...FROM...WHERE, boom, you get data back
Don't even need case: YEAR(DATEADD(m, 6,ddate))
pluralsight.com It's a pay for site, but offer free trials. Offers beginner, intermediate, and advanced material. 
Check out littlekendra.com she used to work with Brent and she knows a TON.
Little Kendra is good, her posters are a little random ^ ^ . Also the free "How to think like the SQL server engine" and "T-SQL Level up (1)" on Brent's site are a must watch, both are really good and I've pushed most of our C# developers to watch them.
Not knowing anything about your structure I would recommend: Two CTEs (or temp tables ideally but if it's a small or well indexed source CTEs are fine) One CTE contains duplicates. One CTE contains duplicates that have been paid back. Delete from your table where ID is in CTE 1 and not in CTE 2. Is that enough for you to work with? Do you need to only delete one duplicate and keep the other, or delete both duplicates?
Nice work allinroc. Couple additional suggestions and an explanation for "don't understand yet why it's faster" When you do SELECT * INTO you're telling the SQL server "hey I wanna make a table" and the server is like "cool what kind" and then you're like "we'll never mind about that now ... I'll surprise you, just be ready.." Then you write a query against tables whatever it doesn't matter. SQL server interrogates the source table (s) and checks out what kind of data type it is. Sometimes by interrogating sys objects sometimes by just looking at the result set and seeing whatcha got and making decisions (which might be different from the actual data type in the source table). Then data types are declared, there's a sort of half re-query of the source data and then your temp table is made and populated. Or. You could create your table first, declare the definitely-right data types, and INSERT INTO. Its more development but it saves SQL the job of doing research and development for you again and again. (Also, this way you can create an index on the JOIN field.) These tips could cut the 12 second query down to 5. *** To OP, if this is feedback for good code. I just broke down what SQL is thinking/doing with good code. Think of what it's doing with bad code. Sub-queries frequently perform table scans, sometimes twice. Pretty much anytime you see a sub query pipe it into a CTE or #temp table. That's what allinroc did. Great catch, easy solution. Lighten the the load in a backpack before you get to the top of a mountain, rather than after.
Thanks for this!
Forgot the commas?
Need commas between your variables in the declaration
Since 2008 there have been [2 new "versions" of SQL](https://en.wikipedia.org/wiki/SQL#Interoperability_and_standardization): * [SQL:2011](https://en.wikipedia.org/wiki/SQL:2011) * [SQL:2016](https://en.wikipedia.org/wiki/SQL:2016) All the new changes are intermediate/expert level stuff, you'll be fine learning the basics from those books.
Sign up dor sqlpass.org. once there goto their Virtual Chapters, they have on going webinars based on topic you can signup for. Best part is, their old videos are all linked and searchable. You could of course also signup for your local in person chapter for monthly meetings with other professionals to further boost your learning All free
Yes, and I also put my code down here that I have done so far: SELECT heg.Azonos√≠t√≥, heg.LSZ, jegy.Azonos√≠t√≥, jegy.[Ellen≈ërz√©s d√°tuma], jegy.[Ellen≈ërz√©s c√©lja], jegy.[Jegyz≈ëk√∂nyv sz√°ma], vizs.[Jegyz≈ëk√∂nyv sz√°ma], vizs.LSZ FROM (( Vizsg√°latok vizs INNER JOIN ( SELECT LSZ, [Jegyz≈ëk√∂nyv sz√°ma], MAX([Ellen≈ërz√©s d√°tuma]) as maxdate FROM Jegyz≈ëk√∂nyvek GROUP BY LSZ ) AS x ON x.LSZ = vizs.LSZ ) INNER JOIN Jegyz≈ëk√∂nyvek jegy ON vizs.[Jegyz≈ëk√∂nyv sz√°ma] = x.[Jegyz≈ëk√∂nyv sz√°ma] AND jegy.[Ellen≈ërz√©s d√°tuma] = x.maxdate ) INNER JOIN Hegeszt≈ëg√©pek heg ON heg.LSZ = vizs.LSZ; Unfortunately no matter how I format it, I always get an error, usually a syntax or an empty query. I'm expecting to get something like this: https://imgur.com/vwWpVzv
What is the exact error message you're getting?
You don't need CASE per lazerath's comment. If you don't have any luck with the YEAR function (I've run into compatibility issues with it in the past), then you might find that DATEPART is more suitable. You might find that DATEPART or YEAR give you some grief due to how you formatted your dates, in which case you might want to SET DATEFORMAT.
I did a screenshot of it: https://imgur.com/a/H3iyW
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Fe7qfvO.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dnzkobv) 
A guess, but are all of your LSZ columns fully qualified? Sometimes Access does that to try and resolve ambiguity in column names. Like in one of the GROUP clauses? I.e. you can‚Äôt just use LSZ, Access doesn‚Äôt know which one, and the SELECT needs to match the GROUP BY columns exactly. 
I think you are missing a closed parentheses.
I find it really odd. If that's the case I have no idea what to do. I'm fairly new at SQL and a complete beginner at Ms Access.
Where am I missing then? I tried everything I could, but couldn't find it. Unfortunately I'm fairly new at SQL and a complete beginner at Ms Access.
Doesn't look like you have no FROM [table] in your inner query. So Access is prompting you for those values for the fields that have no data/columns behind them. You can probably remove the two excess columns but you will still need to add a FROM statement to get your maxdate from an actual table. SELECT heg.Azonos√≠t√≥, heg.LSZ , jegy.Azonos√≠t√≥ , jegy.[Ellen≈ërz√©s d√°tuma] , jegy.[Ellen≈ërz√©s c√©lja] , jegy.[Jegyz≈ëk√∂nyv sz√°ma] , vizs.[Jegyz≈ëk√∂nyv sz√°ma] , vizs.LSZ FROM (( Vizsg√°latok AS vizs INNER JOIN ( SELECT Jegyz≈ëk√∂nyvek.LSZ , [Jegyz≈ëk√∂nyv sz√°ma] as sz√°ma -- Added an alias here just in case , MAX(Jegyz≈ëk√∂nyvek.[Ellen≈ërz√©s d√°tuma] AS MaxJegyz≈ëk√∂nyvek -- Also one here -- you are missing a FROM here?? otherwise these are just columns with no data -- if you need the max date you probably do not need LSZ or Jegyz≈ëk√∂nyv sz√°ma -- as those columns are not helpful to obtaining your maxdate in this subquery ) as maxdate FROM Jegyz≈ëk√∂nyvek GROUP BY LSZ , [Jegyz≈ëk√∂nyv sz√°ma] ) AS x ON (vizs.LSZ = x.LSZ) AND (vizs.[Jegyz≈ëk√∂nyv sz√°ma] = x.[Jegyz≈ëk√∂nyv sz√°ma]) ) INNER JOIN Jegyz≈ëk√∂nyvek AS jegy ON x.maxdate = jegy.[Ellen≈ërz√©s d√°tuma]) INNER JOIN Hegeszt≈ëg√©pek AS heg ON vizs.LSZ = heg.LSZ;
Thank you for the help so far! Yes, the problem is apparently, that in the subquery LSZ is empty, I just confirmed it. But then deleting it would cause troubles with the joining because I would have no means to write anything after the "ON".
Instead of INNER JOIN ( SELECT Jegyz≈ëk√∂nyvek.LSZ, ... FROM Jegyz≈ëk√∂nyvek GROUP BY LSZ.. try INNER JOIN ( SELECT Jegyz≈ëk√∂nyvek.LSZ, ... FROM Jegyz≈ëk√∂nyvek GROUP BY Jegyz≈ëk√∂nyvek.LSZ... Note that both the SELECT and the GROUP match now.
You did not read my post very well! As you said, your query returns nothing. So you are joining on nothing. So those columns are useless other than to causing other problems, as you noticed. You either need a FROM clause to select data from a table so your subquery returns rows (and then add more columns to join on whatever you need to join on), or you need to remove those columns altogether and just SELECT MAX([Ellen≈ërz√©s d√°tuma]) as your column with no join. Access is prompting you for parameters *because there is no data behind this subquery, thus no values to join on, hence the prompt for parameters*
I woke up thinking...it's a comma....and sure enough...it was the commas. Feel so stupid hahaha. Thank you for ther reply though
haha yeah, i woke up with that thought and sure enough that's exactly what it was. Thanks! 
[removed]
I watched the whole series for the MTA DB on Microsoft Visual Academy. Not bad at all. 
Lol. We've all been there one time or another. Next time go for a walk or do something else for a few. So many times I've been stuck for an hour or many then walk away and think of the answer.
If you know SQL, there is such a small difference between DB that it isn't something a quick Google search can't fix.
I don't know of any resources to teach PG specifically, but if you want to learn the nuances between these different RDBMS, then I recommend taking your code in MySQL and seeing what works and doesn't work in PostgreSQL. For example, unless you're running MariaDB 10.x/MySQL 8.x (pretty much the past year or so), you won't have access to window functions which are available in PostgreSQL. So if your server is running an older version of MySQL you may not have some specific functions. 
How does dateadd work, mainly within this example?
What does it mean when I say "You can have ice cream and cake or tortilla chips and salsa?" Does it mean that your choices are (ice cream and cake) or (tortilla chips and salsa)? Or that it's (ice cream and cake and salsa) or (tortilla chips)? It's the former; [the answer is because AND has a higher operator precedence than OR.](https://docs.oracle.com/cd/E40518_01/server.761/es_eql/src/ceql_expr_precedence_rules.html)
nice try on the analogy, but this -- &gt; Or that it's (ice cream and cake and salsa) or (tortilla chips)? is misleading -- SQL would never evaluate that way
Postgres is a little stricter / more correct. It has fewer forehead-slappers. I would skim (read the headings at least) of the main PG docs, and read this as well: https://wiki.postgresql.org/wiki/Converting_from_other_Databases_to_PostgreSQL#MySQL
That's exactly the point they're trying to make. You're saying that their example of incorrect associativity is incorrect.
Thanks! Your answer makes sense!
That will add 6 months (m for months) to ddate. This would turn June 30th into December 30th, but it would still be the same year. Whereas if you add 6 months to the date July 1st or later, it would end up being the following year.
I think it sounds like a fun project. It wouldn't be about making money necessarily. It would be about seeing if you can do it and the satisfaction of getting somewhere with it would be worth the effort.
Exactly! The ultimate goal is that I learn some programming skills. The system I create is only as good as the data I put in, so making money is only a possible benefit.
T SQL fundamentals by Itzak Ben-Gan. Its a book but it helped me a lot more than videos. 
Pass.org contains a lot of top quality webinars for purchase, many with sql sample scripts for download. They're associated with sql saturday.
Good luck!
[removed]
I've done this in SQL server by making my own function. It was a case statement. Case If colA &gt; colB then colA Else colB End Unfortunately with all your columns you'll have to do many compares.
I think pivot is what you are looking for. This looks like a decent intro http://www.artfulsoftware.com/infotree/qrytip.php?id=78
not true if you know MySQL
not true if you know MySQL
&gt; Anyone have any ideas how I can get the minimum across the row? use the MySQL LEAST() function SELECT `Car Park Name` , LEAST( 8am , 9am , 10am , 11am , 12pm , 1pm , 2pm , 3pm , 4pm , 5pm , 6pm , 7pm , 8pm , 9pm ) AS smallest FROM ( SELECT `Car Park Name` , AVG(`8`) AS 8am , AVG(`9`) AS 9am , AVG(`10`) AS 10am , AVG(`11`) AS 11am , AVG(`12`) AS 12pm , AVG(`13`) AS 1pm , AVG(`14`) AS 2pm , AVG(`15`) AS 3pm , AVG(`16`) AS 4pm , AVG(`17`) AS 5pm , AVG(`18`) AS 6pm , AVG(`19`) AS 7pm , AVG(`20`) AS 8pm , AVG(`21`) AS 9pm FROM `TABLE 1` WHERE `Car Park Name` LIKE '%Invent%' GROUP BY `Car Park Name` ) AS subquery p.s. your aggregate query needs a GROUP BY 
Ok, I admit I'm not very familiar with mysql specifics.
Ask away! In the meantime, please read the rules for posting in this subreddit.
Don't ask to ask, just ask.
Haha!
&gt;Creating Calculated Fields from existing fields (e.g. time differences) &gt;Create mapping tables and "Vlookup" to create new fields Able to create various cuts of data similar to a pivot table to conduct analysis &gt;Create Charts/ tables for consumption e.g. bar charts, Line charts, Statistic Process Controls and be able to put these onto powerpoint Sounds like Periscope Data (runs on Amazon Redshift) is the silver bullet for you, if your employer is willing to purchase the software for you. You can perform all your necessary calculation and mapping within the SQL editor itself, create materialized views, create tables, cohort table, pivot charts, etc, and then have charts and tables ready with the click of a button. 
In-memory database might be a better option for live chat. While saving logs to disk if you need.
Im thinking of this more as a PM system for a forum, but with the ability to have multiple participants. Then the plan was to add socket.io to read the chat. I guess I'm now wondering if lots of chats will slow down the database. 
SQL is not a good option for chat. Look in to NOSQL as what you really want is just a flat store of messages
I think you're over thinking it. What you said about joining the tables is correct. So now you have each transaction assigned to a genre, forget the movie part of it.
 select g.id, g.name, count(*) from genre g join movie_genre mg on mg.genre_id = g.id join transactions t on t.movie_id = mg.movie_id group by g.id, g.name having count(*) &gt; 20 order by count(*) desc Don't count customer_id. It could work above if you wrote it well, but would lead you to thinking about it the wrong way, because customers can appear on multiple transactions, presumably.
I'm a sql guy through and through and but if you are already going nodejs, create a session based char system in memory and store in a nosql solution. If you are talking even remotely high transaction in a centralized system, you'll have a ceiling in which everything falls apart. Your chat should be stored in memory and then periodically distributed to a node in a cluster of a nosql based solution. They have cornered the market on distributed data solutions. They have their faults but availability and distribution is the key.
&gt; What is your preferred method of storing table templates for table you create often? when i create a table, i create it only once if i were to consider a template, i would use a text file containing the SQL
This is not related, but what tools did you use to draw this diagram?
[removed]
What DB are you on? If MSSQL server, then check out cascading deletes.
1) Your joins should come before the WHERE clause. 2) DELETE statements will only delete data from the FROM table. You may want to consider Dynamic SQL, Cursors, or SSIS to delete from multiple tables is succession if you‚Äôre going to do it constantly.
Are you using 2 BAK files and a TRN? Can I ask what that second differential BAK has that the first BAK doesn‚Äôt?
I think you just need multiple delete statements. If you want them all to happen together (and who doesn't, really) you can wrap them in begin transaction and commit transaction statements. Like Begin transaction Delete from table1 where col1 = :param1: and col2 = :param2: Delete from table2 where col1 = :param1: and col2 = :param2: Commit transaction Alternately to make it easy on your calling source, you can put the above in a stored procedure, then just call the stored procedure. Another option is to use referential integrity constraint declarations to have all the rows in dependent tables delete themselves when you delete the row from the primary table. Check out how referential integrity constraints are declared on foreign keys in your database.
after my full bu i added a few rows with enough memory to create a few extra pages and then did my differential
Adding onto this. You should also wrap the queries in if exists() or if not exists().
I had to create a proc to update multiple times and the output clause in sql server extremely helpful and fast! You should check it out. Pretty easy to do to. Wish I leaned about it years ago. Output clause will put all the rows effected in a insert, update or delete into a temp table for you. Which you can later use for a inner join for another update or delete.
&gt; If MSSQL server, then check out cascading deletes. dude, every database does this now
platform, please
Thanks I wasn't sure about that. So, if I moved the where clause to the bottom, would the delete still occur on only the first table?
I have been reading about cascading deletes as I've been trying to solve this problem. Does it require some ... configuration ... or, something like that to be performed on the existing database to establish the relations? I wonder why my boss would not have mentioned that if this was clearly the way forward. 
Thanks for your reply. I originally came up with a solution that just generates a delete statement for each table, and than I started to think about performance (as this will be running thousands of times, regularly). I have read, in the past few days, that one large query is always better than many small queries from a performance perspective.
Thanks everyone for your help and replys. Just wanted to mention that the DB is PostgreSQL. I'm about to sleep, but I will revisit this thread tomorrow morning. 
You are correct. Apparently people keep downvoting you on this, maybe because they do not understand why you are calling this out. The example given was not a valid analogy because it does not illustrate the problem. The way SQL will parse something like this: where a and b and c or d and e and f Is like this: (a and b and c) or (d and e and f). NOT like this: (a and b) and (c or d) and (e and f) To do achieve the second result, the query would need to use parentheses like this: where a and b and (c or d) and e and f Always be careful of ORs! They can split your predicate in half in ways you were not expecting. Using parentheses is the best approach.
Correct, it would only occur on the first table. Your joins in this case are simply acting as where clauses.
It would be nice if this was always true, but it is not, even for select statements. In your case, I may be out of date but I do not know of any databases that allow you to delete from multiple tables in a single statement. Referential integrity and cascading deletes can get you close. Perhaps the best performance would entail using multiple connections and issuing one delete on each connection, either asynchronously or with each delete in it's own thread, but then you lose transactional protection, one of them could fail while the others succeeded and where would your database integrity be then.
Simply put, the queries are different in this way: Where (item.item_ser='COS' and sorditem.status!='X' and sorditem.status!='P' ) OR (item.item_parnt in ('RC17134','RC17135','RC17136','RC17137') and sorder.order_date between '01-Jul-2017' and '01-Oct-2017' ) Versus this: Where ( item.item_ser='COS' and sorditem.status!='X' and sorditem.status!='P' and sorder.order_date between '01-Jul-2017' and '01-Oct-2017' ) OR (item.item_parnt in ('RC17134','RC17135','RC17136','RC17137') ) I think it's plain to see from the above reformatting (and my addition of parentheses) how these queries are very different. Always use parentheses when using ORs to prevent this type of problem. What did you mean to do with your OR? 
Be careful when you enable this! One delete statement will delete records from many tables.
I thought as much, but wasn't sure. You learn something new every day :) 
What specifically are you having problems with? Don't know what to do in general or is there something else? 
what have you tried so far? ‚Ñ¢
Thanks, I will definitely look into it.
It's asking you to find records with the accepted action and compare them to the other types, then display them as a percentage. It's a simple query, which you can find the answer to with &lt; 10 mins of googling. 
I am new to SQL. Here is my code: SELECT Count (*) AS total_accepted From table WHERE Date=xxx AND Action = ‚Äúaccepted" SELECT Count (*) AS total_rejected From table WHERE Date=xxx AND Action = ‚Äúrejected" And use the two numbers in the output to do the calculation. Any comments? 
if you need DATE to be a specific value, it won't be null that third line is redundant
create temp table t_test (boo boolean, value0 int, value1 int) insert into t_test values (false, 1, 2); insert into t_test values (false, 1, 1); update t_test set boo = (value0 = value1); select * from t_test; result: f 1 2 t 1 1 If I understand your problem correctly, then you should be ok.
Im trying to filter out where the RISK_SCORE are 0, not the date. Am I not going about this properly?
select count(date) as total, sum(case when action = 'accepted' then 1 end) as accepted, count(date) / sum(case when action = 'accepted' then 1 end) as acceptance_rate from table where date = '2017-10-04'
You'd need an AND Risk_Score &lt;&gt; 0 if they actually hold the value of zero. You can do Risk_Score Not Null if they are null. Basically, your AND is missing the field to be evaluated after it. 
SQL does not allow side effecting assignment, you're fine.
Ah I see now. Is there a more efficient way of doing this instead of stringing a bunch of and statements together?
I don't see that as inefficient, but then again I've been dealing with SQL for years so it's just normal to me. Basically in SQL each item you need to evaluate will be separated by an AND or an OR, or any combination thereof. What you have is as basic as it gets. SQL doesn't have shortcuts like, say, Java does. 
Thanks for your help. 
Will the Risk scores always be 0 or will they sometimes be null? You might want to go with coalesce(risk_score, 0) !=0
Im not sure I follow, I thought if it was 0 that means it is null? 
The application is slow for the client, but fast when you run it locally.
Select an.answer from bdo.reddit.sql an where an.reason like "%i don't want to do my own work%'
Oh, thanks so much for the suggestion. That sounds like there would be lots of possible problems/solutions. If you don't mind, what kind of company would have a problem like this? Sorry, I just need to create an imaginary business to put this problem in. Thanks. 
It's a very generic issue for any company that uses a database. Healthcare, Logistics, Law, you name it. It's common enough that you should be able to find answers, but obscure enough that there's many ways to solve it, and not all of them are taught. And thanks for not straight asking us to write a query for you. Most people asking for help here just want us to do their assignments for them. I don't mind bouncing ideas around.
Awesome. Thank you so much! I've been stressing so hard about this assignment and you've given me a great place to start. 
Here‚Äôs a great write up: https://www.brentozar.com/archive/2016/11/query-sometimes-fast-sometimes-slow/amp/ And the famous ‚Äúslow in the app, fast in SSMS‚Äù article: http://www.sommarskog.se/query-plan-mysteries.html
They weren‚Äôt asking for a Syntax or logic answer. It was just a ‚Äúday in the life‚Äù type question. I don‚Äôt like to hand out answers either but this doesn‚Äôt really fall under that.
Funny how this this is the exact article I was thinking of. ‚ù§Ô∏èBrent
You could also cover how this can happen because of lack of indexes. It's easy to miss an index when the dataset is small but on a live machine when the dataset is huge, it could be horribly slow!
Usually those are completely different. For example, my exam score on a test I haven't taken yet might be null. But the score on the exam that I failed everything on might be a score of 0. Null does not mean the same as 0. Null is nothing, no score, or unknown. However, your business could have assigned special meaning to the value of 0 to mean something else. You would have to know that. Finally, do not worry about nulls in aggregate functions. Nulls are automatically excluded from things like AVG and SUM. They do not could for zero. For example, the average of 100 and null is 100. (not 50).
Ah I see. Makes sense. So this SQL database they come up as 0s, which means the risk scores haven't been received yet. I was told to exclude them when taking the avg. So would this be sufficient to not avg the ones with a 0 risk score?: SELECT AVG(RISK_SCORE) FROM MEMBERS WHERE GROUPER_ID ='578' AND DATE ='201709' AND RISK_SCORE &lt;&gt;'0';
Yes, that would work. But if we assume that the risk_score is a number data type, then take the quotation marks off. It would be AND RISK_SCORE &lt;&gt; 0; or written another way: AND RISK_SCORE != 0; You might also want to take the quotes off of '578' if grouper_id is number. Also is the DATE column a number, a character, or a date data type? Use a TO_DATE function if it is a date.
I'll have to double check those columns when I have access to the database again on Monday at work. Thanks for the tips, im sure there will be many more questions in the future. 
SYNTAX ERROR!
Thank you so much 
Thank you! 
&gt; I have read, in the past few days, that one large query is always better than many small queries from a performance perspective. It *greatly* depends on what you're doing. A single set-based operation *usually* works better than RBAR, **but** it can be helpful to batch up queries and do them in smaller transactions due to lock escalation, for example.
I am not sure that is the best idea, phones aren't the best place to host databases, can you try and get a raspberry pi and host a database there? You are trying to something that is meant to be centralized on a device meant to be decentralized; nevertheless, if you are insistant on going this route, one way is to setup the iphone as a web server and having it execute SQL commands via REST calls. This would require some coding on your part.
Spin up a cheap server on Vultr or DigitalOcean and install MySQL on it. Then use one of the MySQL IDE‚Äôs in the App Store to connect to it. 
I mean MySQL is free. But for personal use on the computer‚Ä¶ I don‚Äôt know how you would connect to your server or database with the free version?
You could host a free web server via heroku with mysql
No. MySQL runs a bazillion websites. You spin up a $2.5 a month server on Vultr and install MySQL. You would then use the IP address of the server to connect. 
Or try a free plan on heroku like the user below me said. Same concept. 
Hi, this is the Execution plan: Aggregate (cost=24210.6..125708.5 rows=249566 width=4316 conf=0) l: Group (cost=24210.6..125707.5 rows=249566 width=103 conf=0) l: Hash Join (cost=24210.6..125676.3 rows=2495664 width=103 conf=37) l: Sequential Scan table "V2WBTF2.FCT_PAGE_VW" (cost=0.0..53949.4 rows=2511099108 width=24 conf=90) (FACT) r: Hash (cost=21187.5..21187.5 rows=2164798 width=111 conf=0) l: Hash Join (cost=4412.7..21187.5 rows=2164798 width=111 conf=58) l: Sequential Scan table "V2WBTF2.FCT_VISIT" (cost=0.0..1968.6 rows=102998143 width=20 conf=90) (FACT) [BT: MaxPages=1538 TotalPages=367649] (JIT-Stats) r: Hash (cost=508.2..508.2 rows=2144686 width=91 conf=0) l: Hash Join(right exists) (cost=0.0..508.2 rows=2144686 width=91 conf=72) l: Sequential Scan table "V2WBTF2.FCT_MMC_CLCK" (cost=0.0..508.2 rows=14165566 width=91 conf=90) [BT: MaxPages=397 TotalPages=94805] (JIT-Stats) r: Hash (cost=0.0..0.0 rows=37 width=0 conf=0) l: Sub-query Scan table "_vt_inlist_rel0" (cost=0.0..0.0 rows=37 width=0 conf=100) l: Result (cost=0.0..0.0 rows=37 width=0 conf=0) 
This is the result: Aggregate (cost=24210.6..125708.5 rows=249566 width=4316 conf=0) l: Group (cost=24210.6..125707.5 rows=249566 width=103 conf=0) l: Hash Join (cost=24210.6..125676.3 rows=2495664 width=103 conf=37) l: Sequential Scan table "V2WBTF2.FCT_PAGE_VW" (cost=0.0..53949.4 rows=2511099108 width=24 conf=90) (FACT) r: Hash (cost=21187.5..21187.5 rows=2164798 width=111 conf=0) l: Hash Join (cost=4412.7..21187.5 rows=2164798 width=111 conf=58) l: Sequential Scan table "V2WBTF2.FCT_VISIT" (cost=0.0..1968.6 rows=102998143 width=20 conf=90) (FACT) [BT: MaxPages=1538 TotalPages=367649] (JIT-Stats) r: Hash (cost=508.2..508.2 rows=2144686 width=91 conf=0) l: Hash Join(right exists) (cost=0.0..508.2 rows=2144686 width=91 conf=72) l: Sequential Scan table "V2WBTF2.FCT_MMC_CLCK" (cost=0.0..508.2 rows=14165566 width=91 conf=90) [BT: MaxPages=397 TotalPages=94805] (JIT-Stats) r: Hash (cost=0.0..0.0 rows=37 width=0 conf=0) l: Sub-query Scan table "_vt_inlist_rel0" (cost=0.0..0.0 rows=37 width=0 conf=100) l: Result (cost=0.0..0.0 rows=37 width=0 conf=0) 
So you want your report to allow the user to modify that True/False column with a single click per record?
yes!
I'd be really careful with this. Using SSRS to allow users to modify data, even if it's the one column, is something you want to properly restrict and lock down. That said, I haven't done data modification inside of reporting in a long time. What I would try to do is give each record a check box as the first (leftmost) column. The user can go through the records and check all the ones that need to be toggled. There would be a button at the top of the page that says "Update" or similar, and when clicked it would execute a stored procedure to update each checked box. Here is a good example: https://www.experts-exchange.com/articles/8737/SSRS-can-be-also-used-for-user-inputs.html
I believe this is EXACTLY what I'm looking for. Thank you!! &gt;I'd be really careful with this. Using SSRS to allow users to modify data, even if it's the one column, is something you want to properly restrict and lock down. I appreciate the sentiment. Until I'm much better at what I do, I will be extracting the data myself from our production database, and making a tiny separate database for the end user to use. That way I can easily make backups and restore anything on my tiny separate database if the user screws anything up - or if I do. :) Thank you very much again, I will give this my best shot. 
Your first example should work, if I've understood the question. You can test as few or as many fields as you need to, and they will be evaluated sequentially until one is evaluated as true, or the else condition will be applied if none are true.
Basic rule of a CASE statement at least in T-SQL: The procedure will break out of the CASE at the first TRUE condition it finds, in order, from the top. You don't need to do all possible combinations. Look up 'nesting' to break it into manageable chunks Here's some examples of nesting CASE statements in T-SQL: https://stackoverflow.com/questions/505747/best-way-to-do-nested-case-statement-logic-in-sql-server 
I'm on mobile and having a hard time finding it. The video is part of a series called group by, there's a video around a hour long on ssrs tips and tricks. It is a series with Brent Ozar. Around the 45 ish min mark they do some demos for what you are looking for. 
My query is supposed to be testing and reporting a list of all possible iterations derived by a very complicated software engine that evaluates something like 90 identifying factors, in various sets of relevance. Also, my sql skills are just barely the other side of beginner. I will have to study up to find a better way. Due to deadlines, step 1 is make it work then step 2 is make it work better. Thanks for your help! I appreciate you getting to me so quickly. :) 
Thank you!
I think my key here is going to be proper sequencing. I know the "S" stands for "sequential" but for some reason I haven't gotten a straight answer from my sr team, teachers, trainers, etc - does it actually perform each command in a query in order as it executes (like an object based performance language)? The answer I get is always "well it all just happens at the same time if it's in the same query", which while it may *seem* to all happen instantly, can't possibly be true, right? I'm definitely going to look into nesting. I'm still pretty new to all this and Google is only helpful when you know what you're looking for. I never would have thought to look up nesting in this scenario. Thanks for the quick response!
multiple conditions should work fine, but you are repeating the CASE keyword which will give you a syntax error
Yeah, that's because I was copy/pasting mobile. I can work out those sorts of formatting issues. :) Thanks for the heads up!
&gt; does it actually perform each command in a query in order as it executes (like an object based performance language) Yes and no. In writing your query there are some things that are evaluated in a way where the sequence you've entered them matters - CASE statements and table joins for example. But the execution happens all at once, e.g., UPDATE table set column1 = column2, column2 = column1 In an sequential object oriented mindset you might think that column 1 gets set to the same as column 2, then column 2 gets set to the new value of column 1 - i.e., they end up the same. In actuality though the whole lot is executed at the same time, the columns both get set to the original value of the other, so the values swap places. When thinking about the actual operations that are happening to the data rather than the syntax structure, think of it as all-at-once.
Restating some of the other points - Order of the case statement matters CASE WHEN 1 = 1 THEN 'This always returns' WHEN condition_1 = 'X' THEN 'I never get run' WHEN condition_1 &lt;&gt; 'X' THEN 'I never get run either' END So different items take priority over others. If a check for condition_6 is more important than the value of condition_2, put condition_6's check first. But the order of the comparison's do not matter CASE WHEN condition_1 = 'X' AND condition_2 = 'Y' THEN 'We are the same' WHEN condition_2 = 'Y' AND condition_1 = 'X' THEN 'We are the same' END If one bit of logic gets repeated a lot, like 90% of your lines include condition_1 = 'X' AND Condition_2 = 'Y' you can nest that CASE WHEN condition_1 = 'X' AND Condition_2 = 'Y' THEN CASE WHEN Condition_3 &gt; 7 THEN 'Something' WHEN Condition_3 = 7 THEN 'Something else' ELSE 'Fallback' END WHEN Condition_3 &gt; 7 THEN 'Condition 1 wasnt X, was it?' END Just don't over use nesting - its always good to make sure the person after you can figure out what you were doing. 
That is an awesome explanation, and the first one that's made ANY sense to me, and way better than my trainers answer of "how the hell am I supposed to know! Why don't you write a letter to the guys that invented it and ask them why it is this way and how all that works" while he walked away shaking his head at me for asking. I dunno but from my background, sequence of events is important. That totally explains why I can't evaluate the results of a case statement while I'm evaluating the cases themselves. Major lightbulb moment. I am eternally grateful. 
I comment the living hell out of everything I do, because I hate coming behind somebody and having to research why they did this thing that looks stupid but is actually genius. I also tend to leave my working files to people even less experienced/trained than I am, and I still have co-workers from 3 projects ago asking me how to fix it when they break it. So no problems there. Are nested statements more or less efficient from an execution perspective? (I.e. Is the database manager gonna come hunt me down and shoot his stapler into the back of my head because I'm gonna hang the server? Do I need to wear a helmet?)
I'm nearly positive nesting the statements is no more or less efficient than typing it all out. I'm most familiar with MS SQL Server and the way that works is the server takes the query you write and generates a Query Plan - it figures out how its going to implement what you wrote against the actual tables and indexes. So if you nest things 7 levels deep the server has to expand out all the logic but it would have to convert your query regardless so no real penalty there. 
Data analyst isn't necessarily the first step on the path to DBA. If DBA is your end goal finding some kind of devops role might be a better way forward. But for data analyst roles learn some R, maybe some Python, and have a play with some BI visualisation tools (e.g., Power BI or Tableau). Excel is likely to also be unavoidable.
There are a bunch of performance issues with this query you should be aware of and some techniques that are just bad practice. If you have any performance problems with this and want it faster, let me know. There is also the question of how the archiving is done, as if you know the methodology you may be able to choose a better query. In the interest of brevity, the simplest thing you can do is repeat the pattern you have with the derived table you have labeled 'rec', but with the tables and columns replaced: LEFT JOIN ( SELECT r.wh_id ,r.po_number ,r.hu_id ,pom.STATUS FROM t_receipt r(NOLOCK) ,t_po_master pom(NOLOCK) WHERE r.wh_id = pom.wh_id AND r.po_number = pom.po_number UNION SELECT r.wh_id ,r.po_number ,r.hu_id ,pom.STATUS FROM t_aht_receipt r(NOLOCK) ,t_aht_po_master pom(NOLOCK) WHERE r.wh_id = pom.wh_id AND r.po_number = pom.po_number ) AS rec You want to replace this part: INNER JOIN t_po_billed pb ON pb.wh_id = rec.wh_id AND pb.po_number = rec.po_number With something like this: INNER JOIN ( SELECT b.wh_id ,b.po_number ,b.closed_date FROM dbo.T_PO_BILLED as b WITH(NOLOCK) UNION ALL -- Assuming no dups SELECT ab.wh_id ,ab.po_number ,ab.closed_date FROM dbo.T_aht_PO_BILLED as ab WITH(NOLOCK) ) AS PB You may be better off using a cross apply here, I can't remember if SQL will expand the derived table and use indices properly on the tables contained in the derived table, but I think it does and it's a moot point. If you cared, you could test both. 
Another thing I'd do if you absolutely *have* to have a large number of case conditions is put them all into one or more functions so your case statement becomes much more readable. 
 SELECT DISTINCT e.Fname, e.Lname, e.Job_Rank FROM employee e INNER JOIN student s ON s.advisor_ID = e.ID WHERE s.gender = 'M' and e.gender = 'F' Please consult your educational institution for information on how to reference this comment in the bibliography of your assignment in accordance with the academic honesty policy. 
Appreciate the tips. I am interested in how to improve the query in all ways, please enlighten!
What do you mean by "one or more functions" ? 
Thank you very much! I don't know the SQL format we are using in class right now but it looks like this œÄ ID,Job-Rank (Employee) Where œÄ (pi) = Project which shows only the columns you specified. If you are familiar with this format I would really appreciate seeing the answer that way. If not, still thank you very much!
If you‚Äôre thinking of becoming a data analyst, I‚Äôd suggest taking stats courses. 
I am on my phone and have limited time, but the biggest thing you should do is look up "SARGable" and understand it completely. There are a few instances in this query where the join criteria or where clause is NON-SARGable. The basic premise is don't wrap functions around columns used in join criteria or filtering expressions because SQL Server treats it as a black box and can't take advantage of indices. A good example is that expression with GETDATE: Start by rewriting it so the functions are applied to the side with the GETDATE and leave the column bare. The scalar udf is another matter - I know it seems like good encapsulation, but scalar udfs just don't perform well and end up killing performance when used as filtering criteria like this. Try to do the calculation in a set based manner. In some cases, perhaps like the expression using substring with Charindex or the scalar udf, consider saving data at write time via your insert/update sprocs. Given the expression with the substring and the contortions you are doing to join it with another column, the base column is likely not conforming to normalization standards to begin with, so I highly recommend giving yourself a refresher on database normalization. That's all I have for now. Hit me up if you need more help. I have lots of experience optimizing sql. 
Generally you see a few routes taken to be a DBA. Those of us that started as Devs and figured the code part of it and ended up doing the administration. On the other side, I regularly see guys who were Systems Administrators, jacks of all trade, who end up taking care of the DB and end up specializing in it. There's pros and cons to each path of course but both are valid ways. If you want to be a DBA, get in where you can. Find an entry level analyst position that uses SQL and get it. Foot in the door is usually the hardest for a DBA position. Because you are starting your career, you might investigate SQL Certifications. (MCSE: Data Platform for MSSQL, Oralce Certified Professional for Oracle for two of the big RDBMS ones) Usually the ONLY time I look at certs when hiring, is if you are brand spanking new, from my chair, no experience but certs tells me you know you don't know and are attempting to shore up your foundation, which show potential. 
If I am reading your question right, it sounds like the "Key" (job-number + item-number) isn't unique (and as such isn't really a key as far as sql is concerned), in which case if you wanted to know the sum total of quantity ordered per key you would simply use a group by with the sum function: Select SUM(Quanity Ordered), Key From table Group by key
That format isn't SQL, it looks more like some proprietary middle ware language maybe?
This sounds very accurate. I'm away from my PC for the night but will try this out and get back to you tonorrow. 
Try adding this as a column to the end: SUM([Quantity Ordered]) OVER(PARTITION BY Key) AS [OrderSum]
&gt; Preferably in the same table. What do you mean by this? If you have non-unique keys and want to get key -&gt; sum pairs: ```sql select Key, sum([Quantity Ordered]) as [Quantity Ordered Total] from Table group by Key; ``` For table Key|Quantity Ordered --:|--: A|4 B|7 A|1 B|3 C|8 The result will be: Key|Quantity Ordered Total --:|--: A|5 B|10 C|8 If by "the same table" you mean that the shape (columns, rows) of your result set should be the same as the source table, try ```sql select Key, [Quantity Ordered] , sum([Quantity Ordered]) over (partition by Key) as [Quantity Ordered Total] from Table; ``` This will get you Key|Quantity Ordered|Quantity Ordered Total --:|--: A|4|5 B|7|10 A|1|5 B|3|10 C|8|8 Since you came from Excel world and by your wording I can guess that you haven't yet got a feeling of basic concepts in SQL. In Excel, when you work with data and reshape it, you get another table (on the same or another Excel sheet). In SQL, you have tables as objects in the database. When you write a SELECT query, what you get is called a result set. It's ephemeral, so if you don't do anything in addition to SELECT, it's not saved into the database and it doesn't change the source table. So you can't really get anything "in the same table", because what you get is a result set (that you can then save to another table).
you lean sqlserver or oracle and do some certifications of DBA. That will help u
Thanks! Yeah, we're talking about a database that has basically had 10+ years of band-aid fix after band-aid fix, non e of which was done with optimal performance in mind. And I'm a new hire right out of college. So, in short, it's a mess.
IMO /u/CrossWired nailed it. The 2 paths I've seen is developer or sys admin. Just be aware of what your goal actually is. When you intern see what hours the DBA works and how often they get paged. I'm a developer and I've got no interest in becoming a DBA. The extra 15% raise is not worth working many weekends and nights for me. On call sucks. you can try it, but when you do, be aware that many shops don't require developers to be on call, but almost everywhere requires a DBA to be on call.
Hours for a DBA are going to vary by shop. I've worked at Fortune 100s that had 6 MSSQL on the production DBA team, we rotated on-call every week on a 5 week schedule (1 floater) Most all weekend work was briefed to the on-call DBA who took care of it. Of course there were occasional exceptions, but in those cases the backup On-Call (the next person in rotation) was the one to called in for help. I've had other jobs where, because they knew i had a dev and sysadmin background, I was also treated like the default troubleshooter, so I was there for nearly every rollout, issue, or whim of management. Sometimes, as a DBA, you may be the final say on SQL code going into production by providing a code review others you are a one man SQL army, responsible for all SQL, SSIS, reportins and administration. Just like dev jobs, DBA jobs come in all sizes.
Here it is: https://www.youtube.com/watch?v=9WecPsBnBec The last 15 min are specific to what you'd like, but honestly, the whole thing is awesome. 
Thanks for breaking this down. I'm at work and can't pull this up to test right now, but I'll let you know when I give it a whirl. Like I mentioned in my OP, I come from an Excel background and am really picking up as I go. So that being the case, where would you point someone for the more conceptual understanding of SQL Server and the SQL language in general? I assume Coursera, EdX, Lynda are the go to answers?
Please don't get your users accustomed to having the ability to modify data via reports. The old saying "give an inch, they'll take a mile" applies here. If users need the ability to modify data, you give them a proper application (client/server, web, whatever) to do it with. Do **not** attempt to do this via SSRS. IMO reports should never alter system state - they should not be used as a front-end to business processes!
No, books. Itzik Ben-Gan's "T-SQL Fundamentals" was especially useful for me. Someone in your company should have it if you work with SQL Server.
Very true. To op, ask questions about expectations (nights and weekends and on call) when you're interviewing for a real job. I left my last position because there was a huge gap between what I was told about being on call and what actually happened when I was on call. I was told the phone hardly ever goes off. When I was on call the phone went off at least 3 time a day outside of business hours. The co workers all considered it normal that all you did when on call was work on your nights and weekends. I was not willing to do that when I could get the same money at a different shop and only work 9 to 5.
Most databases present a slightly different flavor of SQL. Access is it's own flavor. It's way different than all the others. That said, don't waste your time on Access. Go straight to SQL Server or any other database engine. Access is too limited.
Worth noting that SQL Server Developer Edition is free.
Personally I would avoid Access and go straight to SQL desktop. The skills you'll gain are more relevant and will scale better. MS Access is on its way out in the coming years IMO. I experienced what MS did with Foxpro a decade ago and see the writing on the wall for Access.
+1 This! Developer edition has ALL the features of Enterprise so its not crippled in any way. Its not time limited either (no reinstalls after 180 days). Skip Access and learn on the tool you'll actually be using.
Imho, go with PostgreSQL, either that or SQL Server.
from an SQL perspective, you want a UNION ALL query, where you select opened tickets in one SELECT and closed tickets in another
[My understanding](https://www.w3schools.com/sql/sql_union.asp) of UNION ALL says that I'll get records from one table and records from another table. Can I do a UNION ALL for records on the same table? Will something a la: SELECT opened_by FROM ticket UNION ALL SELECT closed_by FROM ticket produce the following results: Name|Ticket :--|:-- John|001 John|001 John|002 Sally|002 if John opened both Tickets 001 and 002, John closed 001, and Sally closed 002?
you've got it!
Thanks a bunch, I'll work out how my tool support union and be on my way :)
Another vote for SQL Server. 
I started as an SSRS Report Writer and that was a great way to start out on the path to SQL development. 
A good book for picking up the basics is 'SQL Queries for Mere Mortals'. MySQL has MySQL Workbench and Postgres has PGAdmin for GUI interfaces for both Windows and Mac. SQL Server has SQL Server Management Studio (SSMS), but I'm not sure if that's available for free. While Access makes it feel like they are a single thing, the actual database program and the program that you write your queries in (i.e. the database client) are usually separate programs. Just something to keep in mind when picking your technologies.
and crippled...
Use MS Access as the front end, to develop a nice form GUI, and PostgreSQL as the back end. Because MS Access is an inferior database engine. And PostgreSQL is the most standards-compliant, so your cross-db learnings will be maximized
Turns out it also requires windows 10. There was no mention of that until after I downloaded the entire thing. Thanks, Microsoft. I'm on Windows 7. can I get a legacy version of SQL server / Management Studio or am I better served going with MySQL or Postgre or something?
I don't know but you better get that shit in writing. I've had numerous employers tell me I'd "get X something if I do Y something by next review" and then I got nothing. Actually that's how I got my first real SQL job is because my employer promised an internal training position and never delivered (even 6 months *after* the positive review) so I lined up another job and had a final meeting with my boss. Put in my two weeks on the spot after I got the runaround again. Sorry for the tangent. Hope your results turn out better than mine.
I did mine 70-461 last year, for the same reason actually. If memory serves it is mainly multiple choice with some adding to correct scripts, so you‚Äôll have the basic of the script and then drag and Drop commands in the right order. If you can I would recommend a practice exam to give you a better idea of what is expected.