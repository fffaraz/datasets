I am building something similar and this I am using this pattern as well. I have staging tables in roughly the same structure as the source files. It is a close 1 file to 1 table ratio. Then a procedure per file source to make the transformation from the staging table to a normalized structure that can hold claims data from any source. Usually with claims the core tables are diagnosis,procedure,orders,results,patient and provider.
Could you elaborate on the staging table part?
ok thanks I will try this tomorrow and let you know if it works.
Would dropping the temp table if exists first, and then turning the SQL statement into a SELECT * INTO #temp FROM source work? I know wildcards are frowned upon, but this may be a use case. 
MySQL doesn't supported nested JSON, however, unless that's changed recently.
Just to make sure that is analgous to &gt;= vs &gt;, where you consider the input value as well as greater/less than?
Dammit, now I have to try it tomorrow and see. Thanks a bunch!
It works for me. create table #test (id int, testvarbinary VARBINARY(16) NOT NULL CONSTRAINT DF_MyTable_NewColumn1 DEFAULT 0x00000000000000000000000000000000); insert into #test (id) values (1), (2); select * from #test; select @@VERSION; -- Microsoft SQL Server 2017 (RTM-CU5) (KB4092643) - 14.0.3023.8 (X64) Mar 2 2018 18:24:44 Copyright (C) 2017 Microsoft Corporation Web Edition (64-bit) on Windows Server 2016 Datacenter 10.0 &lt;X64&gt; (Build 14393: ) (Hypervisor) 
Sorry yes I should've tested that myself, a new table is fine. I am adding a column to an existing table where the problem still occurs. Microsoft SQL Server 2017 (RTM-CU11) (KB4462262) - 14.0.3038.14 (X64) Sep 14 2018 13:53:44 Copyright (C) 2017 Microsoft Corporation Developer Edition (64-bit) on Windows 10 Enterprise 10.0 &lt;X64&gt; (Build 17134: ) &amp;#x200B;
No... I used to hate them. Then I learned how to use them. People who use JOINs like to see the world burn =). The Sql engine is a giant cursor. I dont like to make it think about N sets of rows to check. Just cursor through what it finds and handle accordingly. 
This is the correct method.
Please never write a query like the one that you wrote. Either stick with the USING syntax or use the old ANSI SQL style, but don't mix both together. You are literally violating the biggest SQL rule and that's to make you stuff readable and consistent. Your teacher, I'm guessing that's who wrote that, wants CITY and STATE in the join key. You placed CITY in the predicate. While you'll get the same results, the database might not be kind to you seeing only STATE in the join key.
Try LAG and LEAD window functions.
The data type is relevant too. If your field is a DATE field, then the whole end day will be included. If it's a DATETIME / DATETIME2, then any timestamp given without a time defaults to "00:00:00.000" of that day, excluding it from the results.
Do you need to transform and store it, or just analyze it? Power BI actually makes it very easy to parse JSON data from a column, file, or multiple files. https://www.mssqltips.com/sqlservertip/4621/using-power-bi-with-json-data-sources-and-files/ https://youtu.be/ipI6mrWLQKA Even if you end up needing to ingest and parse in a database, Power BI is useful for exploring it first. SQL Server 2016, 2017, and Azure SQL [can parse JSON] (https://docs.microsoft.com/en-us/sql/relational-databases/json/json-data-sql-server?view=sql-server-2017) , but you need to know what elements you are trying to pull out.
Sorry just had another look at what you posted, can you try this? create table #test (id int); insert into #test (id) values (1), (2), (3); alter table #test add testvarbinary VARBINARY(16) NOT NULL CONSTRAINT DF_MyTable_NewColumn DEFAULT 0x00000000000000000000000000000000; select * from #test;
I meant the calculations behind are fairly simple, not the implementation detail. Although I don't particularly see anything really difficult with recursive CTEs.
It's inclusive, but make sure your dates don't include time or it will ignore datetimes after midnight.
The general form, which always works right regardless of timestamps and microseconds, uses inequality operators, not BETWEEN. WHERE SOLD_DATE &gt;= '2017-01-01' AND SOLD_DATE &lt; '2018-01-01'
If those columns are of type 'timestamp', you might be surprised.
Are you a troll or just a misguided vandal? 
The net result the same with this method - running multiple queries in succession with very slight differences. You're only doing it to not use a cursor for the sake of avoiding cursors. OP's usage of a cursor *is* appropriate here.
Do you find indexes to be a waste of space, and avoid clustered indexes because they're too slow on inserts?
what does having (count(in-zone)) do here? when using having you are filtering based on an evaluation of groups based on an aggregation. this just has an aggregation but without any criteria.
I disagree that this is an appropriate use case for a cursor. A better solution IMO would be to set up a staging step and shape, validate and control the data properly. At a potential 2gb per nvarchar column per record could put each row at 40gb or more. In most situations thats just wasteful and indicative of bad design.
Maybe we can model the scenario with an example? What would complete the dbfiddle? [https://dbfiddle.uk/?rdbms=mysql\_8.0&amp;fiddle=60ad0d243e9326ac52d9a6e83dfc55f1](https://dbfiddle.uk/?rdbms=mysql_8.0&amp;fiddle=60ad0d243e9326ac52d9a6e83dfc55f1)
Interesting, I'll have to try this!
No I store my clusters and indexes in separate files. 
Generally speaking you can rewrite multiple UNPIVOTs into a single CROSS APPLY. I'm not sure if it's the case here that you want to combine records with the same suffix though. CROSS APPLY (SELECT pays_fiscal_1 AS pays_fiscal, nif_1 AS nif, departement_1 AS department, 't1' AS Suffix UNION ALL SELECT pays_fiscal_2 AS pays_fiscal, nif_2 AS nif, departement_2 AS department, 't2' AS Suffix UNION ALL SELECT pays_fiscal_3 AS pays_fiscal, nif_3 AS nif, departement_3 AS department, 't3' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_4 AS department, 't4' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_5 AS department, 't5' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_6 AS department, 't6' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_7 AS department, 't7' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_8 AS department, 't8' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_9 AS department, 't9' AS Suffix UNION ALL SELECT NULL AS pays_fiscal, NULL AS nif, departement_10 AS department, 't10' AS Suffix) AS up' Or a bit prettier. CROSS APPLY (SELECT pays_fiscal, nif, department, Suffix FROM (VALUES(pays_fiscal_1,nif_1,departement_1,'t1') , (pays_fiscal_2,nif_2,departement_2,'t2') , (pays_fiscal_3,nif_3,departement_3,'t3') , (NULL,NULL,departement_4,'t4') , (NULL,NULL,departement_5,'t5') , (NULL,NULL,departement_6,'t6') , (NULL,NULL,departement_7,'t7') , (NULL,NULL,departement_8,'t8') , (NULL,NULL,departement_9,'t9') , (NULL,NULL,departement_10,'t10')) AS v(pays_fiscal, nif, department, Suffix)) AS up' 
I'm an end game raid boss. https://www.youtube.com/playlist?list=PLPI9hmrj2Vd8m_w3By7pI7xlkXMRzNYzS
It works as expected on my version. I'll ask around to see if some people with other CU levels on 2017 have a different result.
It seems your issue is with the application itself rather than the SQL query logic. I'd imagine you won't have much luck with your issue in this sub then. Is there any documentation about the application you could refer to?
If I am not mistaken you move the asterisk after select and remove comma
Thanks. Unfortunately I'm just an analyst in some datamarts and the DBAs blocked execution plans, can only view estimated.
I always use aliases and always prefix every column. It makes it much easier to read a query for the first time, when you might not know which table a column belongs too.
OK, I got someone else with CU11 to verify he gets the same expected results I get. Here's what I had him run: CREATE TABLE #TestDefault (id INT); ALTER TABLE #TestDefault ADD TestValue VARBINARY(16) NOT NULL CONSTRAINT DF_TestDefault_TestValue DEFAULT 0x00000000000000000000000000000000; INSERT INTO #TestDefault (id) VALUES (1); SELECT * FROM #TestDefault; He got back 0x00000000000000000000000000000000 for TestValue. What value do you get when you run that test?
I usually use aliases to help someone reading my code (usually myself at a later date). This is so I don't need to try to work out why have I joined this table multiple times or what a subquery is meant to do. If I see something like previousyear.field or previousquarter.field it's immediately obvious what the code is trying to do. &amp;#x200B; Also for the love of all that is holy use single letters or single letter number combinations for aliases. A.field, B.field, T1.field, T4.field are as clear as mud.
 &gt;You're only doing it to not use a cursor for the sake of avoiding cursors. Gold. 
 UPDATE yourtable SET filelocation = '/newpath/' + SUBSTRING(filelocation, 10, 99) WHERE filelocation LIKE '/oldpath/%'
If a table is named, I use an alias. If a column is referenced, I prefix it with the alias. Compare these queries. SELECT IsMember , StatusDate , DisplayName , Quantity , Price , DefaultPrice FROM Customer INNER JOIN [Order] ON [Order].CustomerID = Customer.CustomerID INNER JOIN LineItem ON LineItem.OrderID = [Order].OrderID INNER JOIN Item ON Item.ItemID = LineItem.ItemID WHERE OrderStatusID = (SELECT StatusID FROM Status WHERE StatusName = 'Back Ordered') AND ItemID IN (SELECT ItemID FROM Inventory WHERE Quantity &lt;= 0) ORDER BY IsMember , StatusDate; SELECT c.IsMember , o.StatusDate , i.DisplayName , li.Quantity , li.Price , i.DefaultPrice FROM Customer AS c INNER JOIN [Order] AS o ON o.CustomerID = c.CustomerID INNER JOIN Status AS s ON s.StatusID = o.OrderStatusID INNER JOIN LineItem AS li ON li.OrderID = o.OrderID INNER JOIN Item AS i ON i.ItemID = li.ItemID INNER JOIN Inventory AS iv ON iv.ItemID = i.ItemID WHERE s.StatusName = 'Back Ordered' AND iv.Quantity &lt;= 0 ORDER BY c.IsMember , o.StatusDate;
&gt; Column 1 is every service Column 2 is quantity sold over the past 7 days Column 3 is quantity sold over the previous 7 days Column 4 is the difference &gt; &gt;123pets. We have a prompt called getuserdate(&lt;'type'&gt;,&lt;'tbl1'&gt;, &lt;'message for user'&gt; that asks me a range of days before showing me the report. That's when I pick the 7 days for column 2, but can't get another prompt to pop up for column 3, so I'm not even close to column 4. &gt; &gt;Is there a way to avoid 'getuserdate' and report the sum of 7 days before a chosen date, and a sum of previous 7 days, and a way to show the difference? &gt; &gt;Thanks for anyone who takes the time, I was told sql experts are hard to find and probably expensive, but the instruction books make it sound so easy! tblTicketsRow... do you have a date in that table?
Aliases are required when joining tables that have columns with the same names, self-joins and subqueries. I've seen a lot of people use them as a matter of course for readability.
Check out SQLServerCentral
&gt; DBAs blocked execution plans And that's when you tell those DBAs that not being able to optimize your query hurts them when your queries slow the system to a crawl. Seriously, DBAs that prevent using an explain of the execution plan are hurting themselves. And if they don't want you touching a PROD for explain, they need to make a DEV or QA with reasonable data in it so that you can test. There's a minimum for what needs to exist for there to be a "productive" shop.
It looks so obtuse it may actually work.
Oh man.. symbols. I stuck at this. S1 |X S2 C = C I think that's it. A semi join relationship between student 1 and student 2 where class = class. S1⋉S2 C=C
Also, since everybody on your team is using the same tables, try to establish standard aliases for the various tables and views. It makes it a lot easier to read and understand what everybody is doing and also to be able to copy/paste bits from one query to another.
this will depends of the client you use to connect to your db. A .sql file is just a script containing commands, the client you use to connect to your database is the one that will use it, and depending the db, the client will be different. If you use MS Sql Server, you probably use SSMS (Sql Server Management Studio) and there is just file -&gt; open -&gt; your file. If you use postgresql, with the command line client you give the file as a parameter to the client at start "psql -f C:\\Users\\bob\\OneDrive\\Documents\\script.sql" &amp;#x200B;
&gt;This is too complicated, or too cumbersome to actually work. As a Senior Engineer, this is what I strive to teach my junior techs. I want if I get hit by a bus tomorrow, my code will sustain itself till someone can come in and pick up my work. I rarely get hit by a bus but I do switch jobs. [I share Dwight Schrutes sense of loyalty](http://www.elitecolumn.com/wp-content/uploads/2018/03/Dwight-Schrute-Quotes-3.jpg). If you want good references from your previous employers you want people who will say ,"yeah it's sad we couldn't keep them but we would definitely rehire them." What you are doing is interesting. Couchbase they say is really just hacked nosql over sqllite so I'm told. However, I would not recommend what you are doing to anyone. Your setup is really unique and can accommodate your code. Cursors and avoiding joins like this is Caché in the 1980's is really dangerous. Most people will implement code that will cripple their systems.
Thanks u/jc4hokies and u/deny_conformity ! This is really awesome. For information this is my final query : SELECT DISTINCT reference, libelle, date\_naissance, pays\_naissance, nationalite, pays\_fiscal, nif, us\_tin, us\_person, type\_activite, etat\_civil, rac, pep, departement, etat\_individu, relation FROM S2i\_individu LEFT JOIN relation\_individu ON S2i\_individu.reference = relation\_individu.individu CROSS APPLY ( VALUES (pays\_fiscal\_1,nif\_1,departement\_1,'t1'), (pays\_fiscal\_2,nif\_2,departement\_2,'t2'), (pays\_fiscal\_3,nif\_3,departement\_3,'t3'), ('','',departement\_4,'t4'), ('','',departement\_5,'t5'), ('','',departement\_6,'t6'), ('','',departement\_7,'t7'), ('','',departement\_8,'t8'), ('','',departement\_9,'t9'), ('','',departement\_10,'t10') ) AS v(pays\_fiscal, nif, departement, suffix) WHERE import\_id = (SELECT TOP(1) id FROM import ORDER BY import\_date DESC) &amp;#x200B; &amp;#x200B; Again thank you, I have my deadline on Monday ...
I realize 200ms may not seem slow, but basically I want to load up all the data into my C# app into a Dictionary&lt;String, List&lt;Data&gt;&gt; to keep it properly indexed inside C# for instant accesability. So when I do this query for all 150 symbols, it's 150 * 200ms = 30 seconds.. 30 seconds is a long time. Whereas, if I stored this data into 150 flat files which are "preindexed" because each file is the symbol.. then it would basically instantaneous to load this data instead of 30 seconds. You mentioned partitioning, I can look into that some.. can anyone speak as to if this is what I'm looking for? I'm just trying to figure out a standard method for doing this- as I want to attempt to keep all my data/queries in a standard db instead of having to revert to custom file-system solutions for what *seems* like a simple problem.
&gt; Test both queries and check their execution plans. Can you provide more details/guidance on this? 
Good idea, thanks. However I went with the other solution. Thanks for the quick answer !
Old me would agree with you 100% Old me thought cursors were the devil. Then I was hired (full) to train under a PhD (contracted) who has implemented this system in many fortune 500 companies. Once the system is built, its data driven. And yes, if you dont understand how to do it, you're gonna have a bad time.
Way above my pay grade, bud. Hopefully someone else can help you.
I also want to bring up that this model is a more readable version of ms MDS. And MDS uses the same pattern. I just have more control, thus the business has more control. This is MDM. And as the series develops, it will make more sense. I respect your opinion but ask that you wait for this series to develop and see where it goes and how it works.
I intend to watch that series. I am truly curious in that setup even though I'm 100% against what you preach here. SQL Server and RDMBS's struggle with a lot of things. They aren't going anywhere as they do certain things really well. What you are proposing I feel is a misuse of the sql engine. I feel like you should be taking what you are working on an forking an open source rdbms and offering it as it's own independent product. Let's say you make a product with a market and people like it, they are then forced to pay microsoft's per core licensing fees on top of paying a specialty support and development group. Usually businesses either pay for a supported product or specialized talent and not both.
I converted the join to one single where statement without any brackets and it worked . Thanks for your help!
No reason you couldn't do both, servers can be in multiple folders. I'd guess it comes down to: what server might you want to run a multi-server query on? Maybe a security check script on all Prod servers at once? Or all levels (dev-prod) of a particular application? Do it all. Nothing is super convenient when you have hundreds of servers. But I guess you could over think it by having all servers in a managed app (MS SCM for example) and script out all of the servers once a day automatically, so your registered servers are always updated and organized how you like it. 
If you're using Windows learn to use the Snipping Tool, it's standard in Win7/8/10. It lets you take screenshots of specific parts of the screen. You can then post the images to imgur and add the URL links to your post.
That worked! Thanks for your help.
I have some legacy databases that I maintain which use multiple-key joins (country, type, batch) and I don't notice much of a difference whether I do it in WHERE or JOIN ON. I usually go with JOIN ON for clarity's sake as it helps to segment out which conditions are relevant for each join, but YMMV.
Prefixing isn't manually appending columns so instead of 'orders.id' you have 'orders_id'? 
This is getting me close, but not quite there. Instead of replacing, this is adding the new path in front of the existing one and/or smooshing them together. 
Select convert(varchar(20), gatdate(), 101) Replace 101 witg other values. Microsoft has a webpage that will explain this in much detail. On Mobile so I can't easily find it now.
Get imgur dang it, always something 
Do you not have an index on the `Symbol` field already?
Why are you formatting dates for display to the user in SQL in the first place?
Execution plan is graphical (and textual) representation of how SQL Server processed your query. If you are not familiar with execution plans, I recommend [this article](https://www.red-gate.com/simple-talk/sql/performance/execution-plan-basics/) and this [training video](https://littlekendra.com/2017/09/22/how-do-i-analyze-a-sql-server-execution-plan/). These are a little dated, but the basic concepts are solid.
Wouldn't you pivot with a count on the IN_ZONE field then divide the yeses column by the sum of the yes and no columns?
Can you change the order of the statements, insert the value first, then add the new column with the default constraint? The issue is what value it applies to the existing records when you add the new column, subsequent inserts are fine.
IIRC if a date is stored as a datetime, and you say between '2018-01-01' and '2018-02-01' you will get all of Jan, but if you say between '2018-01-01' and '2018-01-31' then you will not get the last day of the month. so you can add +1, or cast the datetime as date.
What do you mean pivot? 
Yes there is an index on the Symbol field.
There's a lot going on here that needs a little bit to be unpacked. First, what is your compsci education background? It's going to shape the detail of the answer. My gut reaction here is that you're over complicating this, and running way too much code in really inefficient manners which is killing performance. First, did you already slap a basic clustered index on the symbol column? MSSQL uses a b-tree to store data, a clustered index will store each row with identical symbols together on the disk. MSSQL will find and return that data just as fast as, or even faster than, opening a file through the filesystem. The underlying data storage methods are (basically) the same. Without it, you need to look at all 3 million rows to find all of `symbol1`. Another thing that I need more info on, is how much data are we talking about here? If it's 'sizeable' (sizeable means different things to different applications) like 1GB, well, it just physically takes time to read that data. Again, reading from a file doesn't help. Something that makes me scratch my head here, is how you mention when you query for all 150 symbols, it takes 30 seconds. How are you doing this in C#? Running 150 different queries? Because if you're trying to get every single row in the database, you should probably just query every single row in the database at once. Also, how long does it take for the following queries to run? `Select top 1 * from mytable`, and `Select top 20000 * from mytable`? The first one is just checking general overhead, the second one is a bit more of checking indexing/general disk read speed. I'm also really curious as to what kind of operations you're doing on `Dictionary&lt;String, List&lt;Data&gt;&gt;`, and how you could possibly be doing this async. I know it's nice to just have all the data, but I can't imagine you're loading the data, then immediately hitting a random data point from all 150 symbols .
In Oracle: - DESCR &lt;table&gt; will give you fields. - all_tab_columns will contain fields table_name and column_name.
OK, show us the execution plan (upload to https://www.brentozar.com/pastetheplan/). I'm guessing you're getting an Index Seek and a Key Lookup, with the bulk of the execution cost being the Key Lookup. Do you have a clustered index on the table? How are you getting your 200ms time measurement? Is that the CPU time used by the query, or the time reported by your application? How many reads &amp; scans does the query need (`set statistics time, io on`)? Have you tried adding the `Data` to the index as an included column, and changed the query to `select Symbol, Data from mytable where symbol='mysymbol''? Are you querying with a parameterized query or a plain string (created via concatenation/replacement at the client end)? Partitioning the table is not going to do anything for you here.
Thanks, will definitely check out power bi. I think my biggest roadblock with using SQL is that the data and the way I'm storing it would violate the relational database. Essentially I'd have to have multiple records with identical information. Not really what I'm after...
&gt; So when I do this query for all 150 symbols, it's 150 * 200ms = 30 seconds.. 30 seconds is a long time. If you want to load all the data at once, why are you querying once per symbol? You're killing yourself with all those round-trips. Eliminate the `where` clause, or at least the filter on `Symbol` - you'll do *far* better pulling the whole table at once vs. pulling 150 subsets and gluing them all together.
What flavor of SQL are you using? MSSQL it really *shouldn't* matter. The optimizer should be handling that stuff for you. As always there are exceptions...like be careful with outer joins with logic in the where clause. 
&gt; this is adding the new path in front of the existing one no, it's adding the new path to the substring of the old location after the old path could you show your actual attempt? my sql used fake locations and paths
in MySQL: - SHOW CREATE TABLE
Do you have a clustered index on the `Symbol` column and a non-clustered index on the `Data` column? You don't need to add the `Symbol` column to the NC index as it's the clustered column and is always included in NC indexes. If performance is a concern, maybe run the loading of your data in parallel or query the entire table at once then load into your lists. &gt;One idea that seems purely messy is to create a new table for each symbol, so it's already pre-indexed... but that seems a bit ridiculous and will just get very messy.. especially if I had a lot of other types of data I'm looking to do. I don't understand why this option is considered "messy" in SQL, but not messy as flat files? The data really isn't normalized, so using SQL isn't really going to be the greatest way to go... maybe consider something like redis?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ktCEfqA.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
How many tables are on the Primary file / filegroup, and where are your clustered indexes.
SQL Server - for one table: SELECT TABLE_SCHEMA , TABLE_NAME , COLUMN_NAME , ORDINAL_POSITION , COLUMN_DEFAULT , DATA_TYPE , CHARACTER_MAXIMUM_LENGTH , NUMERIC_PRECISION , NUMERIC_PRECISION_RADIX , NUMERIC_SCALE , DATETIME_PRECISION FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = 'dbo' --schema AND TABLE_NAME = 'Test'; --your table name
That's what I first did- but it takes about 5 seconds for me to pull the 3 million rows into a list in my C# app, which is not too bad.. but then I still need to index it, so I need to basically do 150 .FindAll() operations on the large list which takes a while, and I end up with my ~30 second timeframe still. So it's not any faster, or at least nothing significant. So I don't agree with your example; my query overhead from my C# app &lt;&gt; SQL is extremely low... if the data was properly indexed on the SQL side, doing all the queries should be way faster.
Without seeing an example, I would suggest REPLACE would work: UPDATE yourtable SET filelocaiton = REPLACE(fieldname,'/oldpath/','/newpath/') WHERE fieldname like '%/oldpath/%' https://docs.microsoft.com/en-us/sql/t-sql/functions/replace-transact-sql?view=sql-server-2017
It's a pretty essential skill to have if you're doing data analytics. Google 'how to pivot &lt;sql flavor&gt;.' You could also use sum with a case statement as well for a similar result, which avoids a pivot. If you have a time constraint dump it into excel and use the drag drop pivot table functionality. 
I've gotten into the habit of using aliases for everything, including single-table queries. Maybe you realize that you can use an ad-hoc query you wrote last week as part of a larger query today, or your end user asks for a change to your report and the query needs to start joinng tables together. You could go back and add in your alias and prefixes, but that's a pain in the ass. If you get into the habit early on, or your tooling supports it, adding them up front doesn't take a noticeable increase in time versus figuring out the query in the first place. It helps if the project/team also agree on aliasing rules. I'm a fan of abbreviations: CustomOrders aliased as "co", that kind of thing, but some people want better descriptions. I do get irritable when people just start using the alphabet in order, with aliases having no resemblance to the table names, but that's just because it adds more mental gymnastics to reading the query later.
&gt; Does this tell you even if it's being indexed great this top 20k, taking 240ms the maximum speed my query could ever be? That seems very slow.. That is exactly what it's telling me, and it does seem very slow. Is the database server different from the machine you're running the query from? I'm guessing yes, but I have to ask it. But you also said Dapper could load it all in 5 seconds, and I double checked, Dapper doesn't do lazy loading. &gt; and then I used a findall on the list to split it into my dictionary object For a quick answer, I think this might be your killer here, if you do 150 different `findall(symbol)`'s. I don't think `findall()` cares if a `list` is sorted, because there's no way to guarantee a `list` is sorted, so you are looking at the entire 3 million items 150 times. I am really curious why the raw query is performing so slowly, though.
I just alias everything all the time. Keeps things readable and makes it easier to call the columns you need. 
thank you! I will give it a try!
thank you! I will give it a try!
The test machine just calls it SQL. So I don't really know.
I can make it date specific, but then a range of days and a separate range is where the words get all jumbled
What makes you think the purpose is "for display to the user"? That's very presumptuous of a database administrator or developer. Some libraries/APIs don't take date/time types, only strings or numbers. For example, maybe it needs to be stringified directly to JSON, which doesn't have a date type, and there is no ability to alter the middleware to support a translation from a native date. Maybe there's using no library at all, and the developer is just running a query in SSMS and saving the results directly to CSV. Ask about business needs rather than making assumptions under the pretense of being "principled." In addition to being a detriment to your productivity, it makes you look very unprofessional.
By no way am I saying this is the best way. It is not. But.. given you are using reportbuilder? Select fId , sum(iif(dateField between dateadd(day,-14,@date) and dateadd(day,-7,@date),quantity,0) lastLastweek ,sum(iif(dateField between dateadd(day,-7,@date) and @date,quantity,0) lastweek From table Where date between dateadd(day,-14,@date) and date Group by fid Should work if your incoming @date has no time. 
 select count(distinct manager) from employee Or more simply: select distinct manager from empoyee 
SELECT * FROM tableName ORDER BY symbol Then you read through your recordset once, loading each element into your data structure as you go, and just have to watch for when "symbol" changes. If that particular ORDER BY doesn't work for you, and the indexes provided by SQL Server don't either, you could add a LoadOrder column and sort on that. You'd have to maintain it, of course, but you be sure to get the data in your preferred order.
Thanks for that, is this running SQL Express? I've just narrowed it down, and it appears to be a result of the column add being an online operation in Developer/Enterprise editions. It works fine for me in SQL 2017 Express. &amp;#x200B;
on top of notasqlstar answer, if you want to know how many employees each manager has you can do a group by SELECT manager, COUNT(*) FROM employees GROUP BY manager
join the tables on an ID or other key (join t2 on t1.id=t2.id), then do a query where t1.dateofbirth &lt;&gt; t2.dateofbirth
Perhaps you could query this in advance at certain fixed intervals? How often are these updated? Lets say you knew this table was only updated once every hour, for example. You could have the query run every 60 minutes and output the result to a table that you pull from instead of the primary table. Another type of index... well, sort of index, is an indexed view which is sort of what the above is. https://stackoverflow.com/questions/3986366/how-to-create-materialized-views-in-sql-server The basic idea is that you can have it do its work in the background and then fetch the results since they're already calculated when you need them. 
Mine is Web Edition. 
It’s really giving you every record with a manager populated, so there will be a lot of rows returned with duplicated information. You’ll most likely want to use count(distinct manger) or group by manger as other commenters have suggested. 
Right, didn't think about that version, its just Developer and Enterprise that support online schema change. Basically it appears that due to the change being online (where the default value isn't actually written to each existing record), when you query those records and get it gets the default value from the table metadata, it is returning the trimmed default rather than the full value. I also queried on StackOverflow, and it does appear to be a bug.
I can replace row\_number with sum though?
Nope. MAX() doesn't use Over/Partition. You'd want something like this I think, if I'm understanding your question... &amp;#x200B; select t1.espsd\_id, case when sum(t1.cob\_amount=0 then 0 else 1 end as cob\_flag from mylib.cobexport t1 GROUP BY t1.espsd\_id
espd cob\_amount flag 1 0 0 1 0 0 1 0 0 2 0 1 2 12 1 3 0 1 3 0 1 3 6 1 3 0 1
espd_id cob_amount flag 1 0 0 1 0 0 1 0 0 2 0 1 2 12 1 3 0 1 3 0 1 3 6 1 3 0 1
 espd_id cob_amount flag 1 0 0 1 0 0 1 0 0 2 0 1 2 12 1 3 0 1 3 0 1 3 6 1 3 0 1 &amp;#x200B; |||| |:-|:-|:-| |||| &amp;#x200B;
So you only want the flag when your espd\_id is 1? Based on those results you don't need any group by. &amp;#x200B; &amp;#x200B; select t1.espsd\_id, t1.cob\_amount, case when t1.espsd=0 then 0 else 1 end as cob\_flag from mylib.cobexport t1
Try this? select *, case when exists (select sm.cob_amount from mylib.cobexport cob where sm.epsd_id = t1.epsd_id and sm.cob_amount &gt; 0) then 1 else 0 end as cbo_flag from mylib.cobexport t1; or same idea with a subselect and a join. 
No, I want the flag when there is a positive amount for anything with the same episode\_id
Thanks for updating your question with the answer :)
Ahh that makes sense. Take a look below. &amp;#x200B; &amp;#x200B; select t1.espsd\_id, t1.cob\_amount, (select case when count(\*)&gt;0 then 1 else 0 end from mylib.cobexport sub where sub.espsd\_id=t1.espsd\_id) as cob\_flag from mylib.cobexport t1
Yep - unpivot is really the ideal answer, although in a pinch you can also union the results if it's only 2 to 3 results you're looking to combine in a single field (just remember the data type needs to be the same - i.e. no mixing integer and decimals)
Or just add to the join clause. Join t2 on t1.id = t2.id and t1.dob != t2.dob. 
https://www.mssqltips.com/sqlservertip/4749/sql-server-2016-online-alter-column-operation/ WITH (ONLINE = OFF) I guess you can try adding that to the ALTER statement. 
is this a rant or a question? i'm not sure what you're working with or what tables/dbs you're attempting to join. &amp;#x200B; Can you provide more details?
It takes time to get to know your tables... &amp;#x200B; my advice is to find foreign keys and map them out... eventually you will get to know your tables and work efficiently. You go this!
How do you get a job as a DB admin without knowing about joining across databases? Anyway, the first question is, what kind of databases are they? 
It definitely helps if you asked for some documentation of the schema, if they have one. If not, well, now would be a good time to start one. Not only will this be useful and evolving document, but it will give you some opportunities to learn about their environment as well as position yourself _securely_ in future projects. 
It’s basically a data admin where I am part of the team that creates queries for SAS platform for salespeople. I know how to join tables, groupby, orderby, aggregate functions, etc. I took a course on PostgreSQL so I know how to do enough sql to make queries. I’m just frustrated because I was just thrown 20+ columns to make, from 3-4 different databases, with 10-15 tables in each databases and 10-20 columns for each tables. In addition, I just learned about 10 of the tables today as we went over and I started this week. So I was sitting there trying to figure out from which table and which column I have to pull the data from. I asked for an example of the type of the query done and basically I was given a quick lecture about using With to create several temp table to join. I have not learned about creating temp table with join so I sat there looking pretty stupid I guess. I’ve also informed them prior that I had no professional experience and was told I would be given couple of months to get used to database and learning the system. Now first week I’m basically thrown in the ocean. 
I know how to join tables. I just didn’t know about creating temp tables and joining several tables that way. Also, I’ve been given several databases, with 10-20 tables each and 10-20 columns for each table. I told them I didn’t have professional experience and this is my first week. 
Ask for a summary of tables if you can get them.
Taking your tables out for dinner at a nice restaurant, maybe a movie afterwards, works wonders for getting to know them.
The only way to learn your data is to be given queries and explore the data. But you should ask for a data dictionary if they haven't provided you with one. It sounds like its a fairly complicated database. I started a roe last year and that's how I felt initially--- now I know it like the backside of my hand. 
Same, started a job a few months ago where I was basically given a bunch of SSRS reports to maintain, create new reports etc. I just had to work my way through the tables, work out what the keys are and from there work on my joins. OP, I would suggest any time someone asks you to do something, ask them if there are any existing queries that do similar or pull in any of the columns that you've been asked for. You can often steal some of the joins from there. Worst case scenario you'll figure out what you're joining on at least. It gets way easier. Just ask lots of questions and tough it out. Before you know it you'll know where everything is and won't even have to think about it.
@TopWizard , what version of sql are you running? Also, why isn't the agent logging "good enough?"
Even if you have experience with SQL... data can be really messy. It would really help if people who built databases used explicit foreign key relationships and normalized data but I've never worked at a place where that was practiced until now and its because I'm building the databases. I insist on normalizing everything properly and documenting it and fortunately really supportive supervisors. 
First off, everything is fine, no one just knows a database they have never seen. Next, sit with the bloke; watch them write code and ask questions, they are feeling you out, pushing your skill set to find its limits. If this is MSSQL or access, highlight the query in the editor, then go into the design query in editor...i think thats what its called. Anywho, it will show the joins in a graph-ish/diagram look. Much easier to understand starting out.
There is always more than one way of doing something. Since this is your first job (assuming) and you have limited knowledge, then do things the way you know how. If you are using SQL Server a temporary table could be anything. You mentioned using 'with' which sounds like a common table expression (Google it). There are plenty of examples online. Accept that you won't have the answer to everything, and that you will spend the majority of your time on Google learning what to do. 
Yea seems like a pain. Are you talking about different databases or different schemas within a single database? Do you have anything working at all right now (like how you are getting data from a different 'database'?) that you could share (instead of trying to restate the problem)? 
Not a dba, but a sysadmin who works nights in a noc (also not advise so much as encouragement): the first tech job is always daunting, you sell yourself short and underestimate your skills. These jobs are like drinking from a fire hydrant. 
Map the d/b tables. Print them out. Draw lines. The # of columns doesn’t really matter at this point. You’ll need to learn where they all are at some point. Keep things simple. Breath. Get the easiest stuff done first so you’re making progress. 
I have a physics background so given time I know I can get it down. I just feel overwhelmed because I’m expected to create a 20+ column query joining several different tables 
Select p.purchaseprice, s.salesprice From purchases p Join sales s On p.id = s.id Where p.purchaseprice &gt; s.salesprice 
B is partially correct. Think about what the result would be if you did an inner join (HINT: that is similar to B). What then is different about the left join?
I’m using PostgreSQL, asked for a sample of how he does his query and basically showed me how to do a query using With, from a table which wasn’t even in the list of database / table he told me to check out, and when I tried it on my comp I didn’t even have access to it. I could tell inside he was thinking I’m an idiot for not knowing With and being able to do a query like him. I believe he has had several years of experience 
If you are in Seattle, I can come sit with you for a couple hours. 
What DB platform are you on? On SQL Server I'd use a cte: ;With lotconcat as( SELECT id, CONCAT(lotnumber,'/', sectionnum, '/', planlabel)AS lot_name_concat FROM cadastre.nsw\_cad\_3008) Update ca Set lot_name = lot_name_concat From cadastre.nsw\_cad\_3008 ca Inner join lotconcat co on (ca.id = co.id) Is there an ID you can join on?
There’s two databases, with bunch of tables each and with 10-20 columns per table 
Does Visio still map databases? It used to pull tables/columns, and any defined relationships. Even if the relationships aren't defined, you'd have a picture of each table. You can do the same thing with pure T-SQL, but it's ugly and text based. 
Using PostgreSQL 
You're probably better using a sub query / CTE &amp; RANK() to get your answer. That way it ranks by the jobID not all the People SELECT \* FROM ( SELECT p.\*, RANK() OVER (PARTITION BY p.JobID, ORDER BY p.salary DESC) AS SalaryRank FROM People p ) x WHERE x.SalaryRank &lt;= 3;
I use the CROSS APPLY unpivot almost daily in my current job because nearly every table is wide and getting the data out in long format makes writing SSRS reports significantly quicker, easier and with much less room for error. Because of the poor table structure I inherited a large number of reports with what can only be described as gratuitous abuse of the lookup function, I've fixed a few to consolidate data sets and properly use row and column groupings to pivot the data. At the same time the reports are now substantially easier to maintain, have less places to hide errors and run quicker for the end users (and aren't a mess of &lt;&lt;expr&gt;&gt;.
Hi can we get a table for 2?
Don't panic. Learn the structure and work your way up, one column at a time.
It sounds like they want a master record of runs. I just designed a similar logging process to this for similar reasons. It's fairly simple enough. You just have a process that increments a @parameter for each @session and logs the attempt and /or completion of an attempt. Coding it to log where it fails becomes harder and is still a problem I'm working on. Any kind of system or pregenerated logging is totally useless for the "meta" process that this logging is being designed for.
I always found having a few drinks in a quiet bar much better for getting to know tables, people get very upset when you try to talk to your tables during a movie.
You can pass in variables into a stored procedure. Example 1, I have a stored procedure that has a dynamic query. I can pass in a variable to indicate my action - executing the stored procedure to update tables, or simple output the query in text so I can review the query. Example 2, use the variable to pass in filters.
You're pretty much correct. Temp tables (local, global and table variables) are used to store subsets of data for lots of different purposes. I don't use them very often but in the past I've used them to store sets of dates to be used in multiple queries. Stored procedures allow you to reuse code, you use them for the same reason you use them in regular coding. If you have a set of commands that need to be done regularly it's nice to store them in the database they are being used in. This means you can do things like schedule the code to be run automatically on a schedule (hooray for automating laborious tasks). I use them all the time when writing reports so that the same code can be reused with minimal effort - commonly things like list of customers who ordered in a period, dates etc. It is also the preferred way of writing SSRS reports because it means you can tweak logic across multiple reports much easier. I've even had cause to use both in the same code - to store the results of stored procedure in a temp table and then do further processing on the result. In the past I've seen stored procedures that used temp tables and ran other stored procedures on the temp tables to chain together lots of complex business logic. 
From the first query if a tenant with the same name was in properties A and B then both would have a count of 1. You'd need to use a subquery to solve this. 
&gt; several databases, with 10-20 tables each and 10-20 columns for each table Be thankful, that's a tiny tiny setup in the grand scale of things. I work mainly with two databases (plus a few minor ones), that have a combined 400+ tables, 800+ stored procedures, dozens of triggers, 50+ functions, 30 SQL jobs, 70+ views, an AAG, and so on - all replicated in 4 environments (4 separate SQL Servers). I don't even consider this setup "big". It sounds to me like they were too easy on you when they interviewed you. How could it be easy, when you are this taken back by the work? That's a bit of a shame, but don't worry, press on and you will learn fast by doing.
How about: update table set name = concat(...); On mobile so this is the best I could do but hope you get my meaning. 
Open up your favorite text editor like Notepad++ so that you can have a bunch of tabs open. Start making small select statements and experiment. Once you have a simple one working, move on to the next simple one. Then lookup a temp table example and add that. Save often. Keep building and making it a little more complex bit by bit. Share real examples if you get stuck. If you really want directly applicable help, find out how to make a get a Create Table script based off the one that troubles you and include it with your question so people can see exactly what the tables look like. Look for primary and foreign keys. You can also use a sql server feature to generate a diagram of the database. It should visually show you relationships between tables, that helps with the learning how it all works together. Look at existing queries that are being used. Read them. Look at the inner most queries and walk backwards through them. Look at stored procedures if there are any. Use the visual query builder and experiment. Small first, then grow.
gid is the primary key
Just for correctness sake: I'd call that an analytic function not a classic aggregate one.
&gt; IIRC if a date is stored as a datetime, and you say between '2018-01-01' and '2018-02-01' you will get all of Jan... No, you get all of January, *and* you get Feb 1, too. 
&gt; I’m just frustrated because I was just thrown 20+ columns to make, from 3-4 different databases, with 10-15 tables in each databases and 10-20 columns for each tables You poor summer child! Honestly thats the type of task that I would be expected to get done in like half an hour. It gets easier with experience but your options here are: Kick hard and swim Sink 
FWIW, it's relatively unusual in PostgreSQL to join tables from different databases. It's much more common to join tables from different *schemas* in the same database. &gt; basically I was given a quick lecture about using With to create several temp table to join Temporary tables are not the same thing as Common Table Expressions. Be more careful with technical terms than whoever taught you that.
Mine has 40,000 tables. Luckily I don't need to know about them all though!
Yea this was the only thing that worked
No it was a normal instalation. I also checked the rights. Everything was fine. The only thing that helped was this Revo tool which is searching every part of your harddrive for postgress files and deletes them this worked. Then I installed it again and got the same bug xD Maybe its a windows bug I dont know.
Didn't know about the replace function, I'll look into that. Thank you.
What type of RDBMS are you using? SQL Server? MySQL? Postgresql? 
I think you may be talking about CTE (common table expression) when you say he uses WITH. CTE are not physical tables. They are derived through a query a d can be used within the same transaction. I can walk you through it if you want to post a sample of that. 
Weird
I was basically thrown in to a similar situation, my boss described it as learning to swim by throwing me into the deep end.. he wasnt really wrong. My first project on the job was adding 7-8 columns to a 1400 line stored proc with temp tables trunc and load from storage tables, etc.
It sounds like you are fine with the syntax, you just need to get familiar with the tables. I agree with others that you should ask for a data dictionary or at least a summary of the tables. Also, I recommend taking the list of columns and marking down what type of info it looks like- ie customer, sales, account manager. Then, I'd start looking for tables that look like they may contain that info one table at a time. That's less overwhelming than looking for 20+columns individually. I also recommend creating a meta data query to list out all the available columns and tables. Either way, you might find as you look for the info, it could exist on multiple tables. Account manager, for example may be different on the sales transaction than on the customer. Don't be afraid to clarify the requirements when you are unsure - that is totally normal. Break it down one piece at a time, and you'll get there.
Just read through all the comments. Looks like you are using Postgresql (I would mention this in your original post so its easier for others to help). So, I agree with many others, you are in a perfectly normal situation as all DBAs start off with not understanding the schema and how all the tables work with each other. The first step is to start looking at PK and FK relations. This will help you see what tables (within a single database) are linked to each other. Now the problem you have is figuring out how the tables relate to each other within MULTIPLE databases. This is where it gets slightly harder to identify relationships. &amp;#x200B; In this case I would suggest the following: Step 1: Read through queries that were already created to identify joins between tables in the two (or more) databases. Step 2: Sketch a little diagram to help you keep track of the tables and joins within the two (or more) databases Step 3: Review what those joins are for and how the data relates to each other for each related tables. Step 4: Write useful queries based on what you've learned about the relationships. &amp;#x200B; Just to give you a short overview, there are multiple ways of writing queries to get the same results. The most commonly used forms are: 1. Single Query Statements 2. Subqueries (Query within a query) 3. Using derived tables (creating a temporary table which has the result set from a query that can be used again later) 4. Common table expressions (In memory derived tabled that can be used within the transaction) Depending on the RDBMS, there are others as well. I would say these are some of the top 4 most commonly used. I hope this info helps you get started in your quest. I can go into more detail with anything if you would like. 
What the hell, 10-20 columns in 10-20 Tables? That is literally kindergarten databases, put your big boy pants on and get on stack exchange and work it out. Also if you cant find stuff you can always try to snipe a column heading with something like this. It really helped me working with unfamiliar large databases. &amp;#x200B; SELECT DISTINCT TABLE\_NAME, COLUMN\_NAME FROM INFORMATION\_SCHEMA.COLUMNS WHERE COLUMN\_NAME LIKE ('%Impostor%') OR COLUMN\_NAME LIKE ('%Syndrome%') AND TABLE\_SCHEMA='YourDatabase'; &amp;#x200B; If it is your first job, just hold on the point of being lost in the sauce passes quickly and it does not sound like you have an incredibly crazy start. 
Thank you for the suggestions. Ill look into the optimisation. I think once its working well it will be run through Alteryx so i should be able to define the event\_ts from there. &amp;#x200B; Sadly those queries took me some time to make with a fair bit of googling. I thought the best option would be to rewrite the sql as a single query but think that will be way beyond my ability. Once again thank you for the time you have put in to explain those parts to me
fake it till You make it
You have the job. Relax. There's a lot of advice saying you can learn SQL at home at a proficient level, but it doesn't compare to the job when you're working with other people data and anomalies that have become accepted practice. My first piece of advice is to read your documentation a little a day until you understand the features of your database. https://www.postgresql.org/docs/ Check out chapter 45 System Catalogs. Check out pg_tables and pg_constraint. Second, ask questions. Get to know the business users. You're in a supporting role. Third, get a gui tool if they have one available or research one that you can use. Learn to use it. A lot of tools have easier ways to navigate the data model than purely textual/scripting. This isn't exact advice since do not work with postgres daily, but this is how I would start. You'll be fine. Keep reading the postgres documentation and asking your business users questions. 
Not sure if mentioned but APEX SQL Search is your friend. I was on the help desk of a place at age 30 starting again and after six months was going to leave due to a toxic department. In my time there I worked with the information systems team and saw what they did. I didn’t know how to write sql code but I could logically follow it. I interviewed for a software developer title that was basically a combination of DBA/report analyst/business analyst/sys admin/application admin. Knew nothing about sql and was always unsure of myself. Three years in everyone left the place due to bad managers and I was forced to find a new job. I was terrified and looked all around and got lucky finding a Systems DBA job. Went into the new job and they were in the middle of an erp deployment. While learning about the ERP, they deployed a report writing tool and I had to go through training to use it. I felt like an idiot because I was struggling with this reporting tool (because it’s designed for end users with no sql skills) As everyone else was flying through the training I was not. It was making me self doubt that I could even do the job even though everybody was really kind about it. It wasn’t until a week later when I was trying to write a report to show me when updates for last applied to the system then I realize why I was struggling with the training. It wasn’t that I didn’t know how to go and write in SQL joins, temp tables, cross database anything, it was that I didn’t know the data and I didn’t know where any of the data lived. The next few weeks after that even though I’m not a business analyst they asked me to start writing reports to help out the business analyst. Are used a pack SQL search to figure out where a lot of the tables were a lot of the data was that I needed. Building reports slowly in my off time has help me to gain more confidence and where everything lives to better assist the business analyst in to do my job better as well. No the awful thing is that I have to use this report writing tool instead of SQL management studio, and it doesn’t like temp tables which is awful because I do. Apex sql search is your friiiiiiend. I hope this story helps a bit and I am by no stretch the best DBA out there but I am learning daily. 
Oracle SQL here. Any guidance you can provide on pulling starts of most used tables and columns?
Couldn't you put the two different ways to join the tables for BO1 and BO2 into temp tables and then join the temp tables together where temp1 profile &lt;&gt; temp2 profile where customers match?
sounds like the guy has been solo for a while, so he just assumes everyone knows what he knows. he might be annoyed because the explanation takes longer than he expected. 
Using a varchar(100) with values duplicated 20000 times each isn't helping. 
Ah I see what you mean. I am assuming c is the one I am looking for. Correct me if I am wrong, but the reason is because an Outer Left Join basically mean to include ALL records of the table in LIST\_MAIL and to include information of the table in CUSTOMER if there's a match of IDs in both tables? and an INNER JOIN is basically to include records of IDs that exist in both tables LIST\_MAIL &amp; CUSTOMER? Something like that?
mySQL doesn't support over
So let me make sure I understand this correctly: The left join finds every instance of an employee being paid less than someone else in their department(jobID). So the top paid employee in their department would have 1 record returned by this left join, the 2nd highest paid would have 2 records, etc. So the GROUP BY P1.ID HAVING COUNT(*) &lt;=3 is limiting to records where there are less than or equal to 3 instances of being paid less than someone else in their department. I was not even aware that you could restrict a group by to a count of the item it is grouped by. Is that universal across most dbs? Oracle, DB2, SQL server, etc?
1. Start documenting tables if they aren't documented already, draw a diagram of relationships to get a feel for its structure. 2. Build the query 1 piece at a time.
oohh I reread the question. Your statements are correct but C and D are vague. I could honestly see it going either way but you understand the concept. I'd say what you did and ask the professor if you could elucidate the distinction between those two. Technically, I think C is more correct. But because of the way they are definining the customer... I can't tell if it's a red herring or what just from the description. Like I said though, you get the concept now so that's... Sort of ... The more important part though it would be nice to get credit too! In a professional setting for which this is a microcosm you'd definitely ask. Or try if possible. But you don't get that opportunity in a aterile worksheet !
Im from sqlserver land, but also deal with MySQL for our website. http://www.mysqltutorial.org/mysql-window-functions/mysql-rank-function/
This is my first week. I just started. 
Read this link and post any questions if needed. https://docs.microsoft.com/en-us/sql/sql-server/install/column-aliases-in-order-by-clause-cannot-be-prefixed-by-table-alias?view=sql-server-2014&amp;WT.mc_id=email
To be honest, no, I can't think of anything to give meaningful practice. You should learn quickly once you get started and get tasks to solve, with a little frustration along the way. Do furious Google searching at every issue you're stuck at with regards to syntax, but as always, the "pic picture" can't be found online. 
SQL Express is completely free to use, but there are limitations so do your research. MySQL Community Edition is also completely free to use. I'm not sure if there are limitations? I'm not in the loop, mainly a MsSQL guy
The place I'm starting has no documentation and many column/table names are unclear according to what they told me when I interviewed there ;_; at least the DBA will be there to help train me.
Do they really want a different system that would require someone with more training to maintain? What happens when you leave the company? Did you run past the idea with them? 
I think its best to just use them every time, easier to keep track of what table is being referenced when doing subqueries
Little Bobby Tables...
So I'd start out by taking just the query you use to fill the temp table: SELECT convert (numeric,DATEDIFF(minute, DET.DateTime, LEAD (DET.LoginDateTime,1) over (partition by DET.EmployeeID order by DET.datetime)))/60 as iexHours ,DET.recoveryKey FROM dbACD.Detail.vwICMChildAgentEventDetail DET JOIN dbEmployee.Summary.vwEmployeeSnapshot as EI on DET.EmployeeID=EI.EmployeeID Where EI.IEXManagementUnitID in ('2270','2272','2273','2283','2284','8870','8873','8883','8884','8885','8886','8889','8891','8892','8899','6570','6591','6583','6584','6579','6597','4469','4470','4473','4475','4484','4490','5669','5671','5672','5675','5681','5682','5684','5690') AND DET.DateTime &gt; @startdate AND DET.Event = '2' Run this in SSMS and Include Actual Execution Plan (CTRL+M) to see the diagram of where potential pain points may be. If I were to guess, not seeing the underlying tables - the names of them start with vw so I assume those are views, and once you're exposing data in views you add a layer and if those views reference other views, your queries can get exponentially slower. I'd guess it is NOT the fact that you are inserting this into a temp table, but the insert query itself isn't performing well because of possibly multiple factors. For instance, you're partitioning dbACD.Detail.vwICMChildAgentEventDetail by EmployeeId - again using a view with, most likely, the EmployeeId not being a key, or being a key of an underlying table that feeds the view. There could be many other things similar to this. In summary, it's not the TEMP table causing you issues, it's the insert statement so optimize that, if possible, and you should see improvement. I'd guess it was the complex lead/partition statement so if I were starting out I'd just take that select statement, and run it without doing the lead/partition statement, see if the records were coming back fast from that. If the rest is good, then I'd try to optimize the lead/partition statement if possible.
Are you just trying to calculate the last day of the current month? Try the EOMONTH function, or if you're in a version that doesn't support it, you can use: Dateadd(month, datediff(month, '31 Jan 1900', Getdate())+1, '31 jan 1900')
Using our date table I am just trying to get a list of 24 dates, where each date is the last day of the month going back 2 years (24 months).
modify accordingly. SELECT DATEADD(DAY, -1, DateDate) FROM dbo.X WHERE DATEPART(YEAR, DateDate) &gt;= DATEPART(YEAR,GETDATE()) - 2 AND DATEPART(YEAR, DateDate) &lt;= DATEPART(YEAR,GETDATE()) AND DATEPART(DAY, DateDate) = 1
Ah, ok, try: Select Top 24 ... Where d.date = EOMONTH(d.date) And d.date &lt;= Getdate() Order by d.date desc OR Where d.date = dateadd(month, datediff(month, '31 Jan 1900', d.date), '31 Jan 1900') 
My friend has Enterprise and replicated the problem when he inserted records first, then added the column with the default. 
if you are getting 0 results. then something is breaking the duplication. like.. maybe there is AAA , 2324, Bob Bobson , 1 AAA , 6464, Bob Bobson , 0 try to get the minimum you need. I usually do these like.. Select name ,count(1) From X WHERE X GROUP By name having count(1)&gt;1
Oh that's pretty clever. I'm not sure I am going to do that because I don't know if its intuitive enough for another developer to understand, but that's pretty fucking clever.
You give us 0 context about your situation. A DBA is a **very** position, having someone mess up in that position can lead to lots of repercussions, so it is less likely someone with "little experience" will be offered such a position, unless the company is okay with the risk involved. 
Are you applying for Junior DBA positions?
This is an important clarification - the OP reads as if they believe a DBA is a junior-level position.
DBA is not an entry level position. Look at junior DBA and possibly helpdesk/desktop support if you have no job experience.
&gt;I tried to implement similar logic to that: &gt; &gt; &gt; &gt; `SELECT p.scode,` `t.scode,` `t.istatus,` `Concat(t.sfirstname, ' ', t.slastname) NAME,` `Count(4) AS 'num'` `FROM tenant t` `INNER JOIN property p` `ON ( t.hproperty = p.hmy )` `GROUP BY p.scode,` `t.scode,` `t.istatus,` `Concat(t.sfirstname, ' ', t.slastname)` `HAVING Count(4) &gt; 1` &amp;#x200B; It's still returning 0 rows. It sounds like I need to use a subquery but I guess I'm having trouble understanding why. This seems to make logical sense to me.
I couldn't seem to make a valid statement when trying to use that, I think it may not be valid when adding a column, I didn't delve into it further as I had to leave work.
I don't think they are too worried about if I leave the company no matter what solution they choose to go with. I would just be using SQL to pull data to use as reports. Another HRIS system wouldn't require that much training it's not too difficult to learn I just really don't want to do double the amount of data entry. 
Interesting. I like building my excel expertise too. My main thing that I am think about at this point is that yes, I can house all of the data in excel but rather than messing around with so many filters can you write a query with power query and tell excel what data you want and it spits it out? I want to get rid of doing do much manual manipulation to a giant spreadsheet itself.
Search YouTube or Udemy. &amp;#x200B; Also check out [essentialsql.com](https://essentialsql.com) 
&gt; I would just be using SQL to pull data to use as reports But when they replace you, will they have the budget to hire a DBA or does the solution need to be something that the average office worker? Excel is a lot more widely known than SQL. Why do you have to do double the amount of data entry? If you're entering the same data twice in 2 locations, look into creating a macro.
Great thank you this worked. Here is my implementation in case anyone finds it useful: substr(date, instr(date, ' ') + 1) || '-' || (CASE substr(date, 1, instr(date, ' ') - 1) WHEN 'January' THEN '01' WHEN 'February' THEN '02' WHEN 'March' THEN '03' WHEN 'April' THEN '04' WHEN 'May' THEN '05' WHEN 'June' THEN '06' WHEN 'July' THEN '07' WHEN 'August' THEN '08' WHEN 'September' THEN '09' WHEN 'October' THEN '10' WHEN 'November' THEN '11' WHEN 'December' THEN '12' END) || '-01' AS fulldate
&gt; the names of them start with vw so I assume those are views You are 100% correct. They are views. &gt; For instance, you're partitioning dbACD.Detail.vwICMChildAgentEventDetail by EmployeeId I'm partioning by EmployeeID because I think I have to for a couple of reasons, but I may be wrong. My reasoning is twofold. I don't want it to calculate a number based on the login time of a different rep, especially in the case of the last logoff on record, that would throw a really big negative number. Perhaps there is another way around this? &gt; the EmployeeId not being a key, As I stated, I'm pretty new to this SQL thing. The two day course I took at work was using Teradata, where it was easy to see keys, primary and foreign, but now I work in MS SQL Server and it doesn't show me what the keys are, at least not that I can see, is there a way to determine which columns are primary keys or indexes that I'm missing? &gt; If the rest is good, then I'd try to optimize the lead/partition statement if possible. It's almost certainly that, because when I was first building it I couldn't get the LEAD syntax correct, so I had a placeholder, and it was much much faster. Thanks for the pointers, I'll see what I can do with them first thing tomorrow.
Yes. With power query you basically do SQL queries but on other sources, including web pages, Excel files, and according to MS website, Facebook. 
Depending on how much/little experience you have a role in Business Intelligence should exercise your skills and education. Wiki has some good info to start and salary. com has salary ranges starting around $25 depending where you live. Like others have mentioned jumping into being a DBA right away seems unlikely, setting your sights a bit lower while building a solid resume in data sciences may yield better long term results.
http://www.snippetmanager.net/ Allows configuring FTP for import/export
I’m with you, I always thought I could be a firefighter, so first thing tonight, because I like the night shift, I’m going to walk right in and report for work. 
Thanks!
Sometimes a developer or report writer is forced to use views instead of the underlying tables for security or other things. I'm assuming your using the EmployeeID is correct - but was pointing out that if EmployeeId isn't a primary key or indexed in any way, the query, no matter what you do, will be slower without that index, etc. I know SQL Server seems like it's VERY overwhelming at first, but it really isn't difficult. In SSMS, to find out information on an object/table/view, you should only have to expand the object and see folders underneath with things like Columns, Keys, Constraints, Triggers, Indexes, Statistics - expand those objects to get more information. So with the views you have listed, if you have the right permission, you could right click the name of the view in SSMS and [Script View As] -&gt; [CREATE to] -&gt; [New Query Editor Window] and now you can see the query behind your view. Probably SELECT blah FROM tables JOIN other tables, etc. But again, I imagine the slow down of your query is the partition/lead bit, but i'm just guessing. But if you comment out that one "field" and just write your query without it like below, and see if the results are returned super fast - you know the pain point is the partition: SELECT NULL as iexHours ,DET.recoveryKey FROM dbACD.Detail.vwICMChildAgentEventDetail DET JOIN dbEmployee.Summary.vwEmployeeSnapshot as EI on DET.EmployeeID=EI.EmployeeID Where EI.IEXManagementUnitID in ('2270','2272','2273','2283','2284','8870','8873','8883','8884','8885','8886','8889','8891','8892','8899','6570','6591','6583','6584','6579','6597','4469','4470','4473','4475','4484','4490','5669','5671','5672','5675','5681','5682','5684','5690') AND DET.DateTime &gt; @startdate AND DET.Event = '2' Good luck with your new project! I applaud you for trying to move an ETL step from code to SQL. TSQL will ALWAYS outperform data manipulation over code. I know, dems fighting words but every programmer I work with has agreed after dueling with me. :)
You can use CTEs to get around all of the alias restrictions. 
Little Bobby Tables
DBA is usually a lateral move. Try looking for business analyst or developer positions where you might be given a dev environment. 
The only certs with weight are the ones offered by the platform vendors themselves. Go right to Microsoft, Oracle, and whatever official cert there is for Postgres (if any). 
&gt; so I need to basically do 150 .FindAll() operations on the large list Why? Of course that's slow. Loop through the whole list once and build up your Dictionary as you go. It'll be faster. Like this: private class Data { public string Symbol { get; set; } public int Number { get; set; } public float PL { get; set; } } static private IDictionary&lt;string, List&lt;(int Number, float PL)&gt;&gt; Load() { var datum = conn.Query&lt;Data&gt;(sql); var dict = new Dictionary&lt;string, List&lt;(int Number, float PL)&gt;&gt;(150); foreach (var d in datum) { if (!dict.TryGetValue(d.Symbol, out var list)) dict.Add(d.Symbol, list = new List&lt;(int Number, float PL)&gt;()); list.Add((d.Number, d.PL)); } return dict; } 
FIGURED IT OUT LAUGH WITH ME AT HOW DUMB I AM
Seledium/CodedUI if you have to take data from a website, Visual Basic to make macros for putting it into multiple excel sheets
You’re probably applying to the wrong jobs. If you understand information systems and data, then apply to entry level analyst positions. Nobody starts as a DBA, it’s like applying to a management position when you’ve never worked a job. They’re probably tossing your resumé to the side with nothing more than a glance. 
The best way to figure this out is to remove the group by from the query and select all the columns (p1.* and p2.*). Examine the rows returned. HAVING is the equivalent of a where clause for data that’s been grouped, as a WHERE is processed before the group by, 
This is the worst kind of response on any thread. Really rampant back in the day when you are searching for an answer to an obscure problem and the person asking the questions responds with "I found the answer." and leaves it at that. 
No no I totally get it! I was just being selfish and was looking for a quick fix 
Use that exact query you used to get the results as your subquery - the having part needs to be within the subquery. It's counting the rows in the group. 
Can you elaborate on the issues you are running into?
Technical they are required if anything stored in the column is not an integer, not just the value you are filtering.
Which question are you having problems with and what is the problem?
Oh that’s probably what I did wrong but it makes sense. I’ll try it when I get home. Thanks! 
And what do you have written so far?
I'm on my phone so this will need some readjusting but basically: SELECT FirstName, LastName, CASE WHEN ISNULL(Grade, '') IN('C', 'B', 'A') THEN 'yes' ELSE 'no' END FROM Students LEFT JOIN Grades ON Students.TNumber = Grades.TNumber AND Grades.CourseID = 'CSC2310' Note this should print out 'no' if a student has not even registered for this course. This might not be desirable depending on your use case.
So far all I've been able to do is query to get the students that have gotten a C or higher &amp;#x200B; select FirstName, LastName from students natural join grades where CourseID = 'CSC1310' and Grade &lt;= 'C'; &amp;#x200B; but I don't know where to begin with using this information to put "yes" or "no" next to all the student's names &amp;#x200B;
Ay Dee bee at is not ay entree levole poleshisun.
That gave me all of the names with "no" next to it. So close &amp;#x200B;
Well, some people start but it does seem to be rare. I started as a DBA intern and then they transitioned me full time but that's not technically the same thing admittedly. Actually, one of the people who started around the time I left pulled me aside and indicated, 'Are you sure you want to go down to a business analyst position? They don't pay as much here for sure but its a lot easier to get an analyst role than in administration." I don't regret the move but yeah most of my co workers at the time came from some other analyst role after having a few years experience.
CSC2310 isn't in the table 
Look up ISNULL(), COALESCE(), and SUM(CASE WHEN ... END) AS thing.
So you're saying you want to insert into B, and if the entry doesn't match up with A you want to insert that entry into A as well?
Sorry this was just my test data, I was not paying attention on data types and the allocations. &amp;#x200B; I am facing issue with all 6 questions - basically I was not able to understand it properly. I tried my own and searched everywhere but found no similar questions.
Thanks, I am getting output as you can see below: DATE TOP_PAYER 1/3 A 1/4 C 1/7 A 1/1 1/2 1/5 1/6 Is this correct.? &amp;#x200B; As per your suggestion I tired adding p.memberID in group by clause but the results were similar as above. Thanks again for directing me to row\_number function, i will try to tweak this one. &amp;#x200B;
This looks really interesting. Thanks for letting us know.
Where is the extra data coming from in those distinct rows? Are you somehow splitting the distinct rows to get to 10 rows? If you're in Oracle, a PL SQL procedure would achieve this. Not sure on a single query without a lot of CTEs and subqueries.
Yeah, thinking it through I'm not making the best representation above. **Table 1 (1X2)** `String A` `String B` **Table 2 (1X5)** `String a` `String b` `String c` `String d` `String e` **New Table (2 X10)** `String A String a` `String A String b` `String A String c` `String A String d` `String A String e` `String B String a` `String B String b` `String B String c` `String B String d` `String B String e`
 SELECT table1.str , table2.str FROM table1 CROSS JOIN table2 
If you need to correlate the data, use a cross or outer supply. Select a.field, b.field2 From tablea a Cross apply (select top 10 field as field2 From tableb Where b.id = a.id) as b
It's glorious. I can even use a calculated field from the SELECT clause in the WHERE clause. Then they also have a QUALIFY clause, which lets you using window functions as a HAVING statement. At the end of the day all the optimizer is doing it wrapping these things in subqueries just like we would have to do with any other RDBMS, but it's certainly a nice time saver.
How are you with PL/SQL? If you're going to be using Oracle longer term, its a very useful addition to your SQL to learn. In this instance you could get the distinct rows from your two tables into a cursor and loop the insert. This won't be that efficient if you're doing a lot, though. A hacky and simple way of doing this would be to use a CTE (With data as) and then use UNION ALL in the main body multiple times to select from the CTE. Put that into an insert and that's a hacky but simple way of doing it. 
Alation
Thanks! It's really good for schema design. I'm hoping to expand its reach in new versions over time.
It’s called a case statement, and they are pretty simple to get the hang of. Search for some examples 
The web portal I couldn't tell you. but the RDBMS is just SQL 2008R2. I haven't dealt too much with XML but it just weird that the current bak does work but my previous does.
Trying to figure out how to get the send\_first &amp; send\_last columns to be different from the receive\_first &amp; receive\_last. 
If I'm understanding the above correctly, you'll need to join the the person (AS person2) table again on the message.receiver\_id = person2.person. Then instead of person.first\_name as receive\_first, it should be person2.first\_name as receive\_first.
You can download adventure works and Ms sql. Iirc there's a free version of SQL server ms purrs out for people to play in. Get a used SQL server text book complete the assignments.
Great work, i m looking fwd to yr vids, enjoyed last one n will watch this one tomorrow. Are you covering scd1/2 n late arriving dimensions in this vid or planning to cover them in future?
Fantastic question. So. I have to get my indexes up, then views, then generate the procedures. When I have my procedures up, that is how we will be insert update and deleting data. That DUI process will be hooked up to the history and archive databases. So we can start seeing how the data changes, but for recovery purposes. We need to actually model something like adventure works (in core) so we can flatten the data in atomic and send data to our warehouse (strategy). That is when slowly changing dimensions really take shape. Absolutely I will be covering them, but I could not guess if the video numbers would be in the 40s or higher before I got there. 
I cannot put a difficulty or target on who this is for. You can follow along and it may or may not make sense. These are very advanced concepts and this series will take a year to complete. But I do start with no database objects so it could help.
It looks really interesting! Thanks for the uploads :D
I am in communication with some people who are following along, and the suggested that i separate these videos into Days. To have a giant playlist and to also have additional playlists of what i am calling days. Day 1: [https://www.youtube.com/watch?v=pf5gWPLfWNo&amp;list=PLPI9hmrj2Vd9cqm4Ueb1\_wNMqpCMBin6b](https://www.youtube.com/watch?v=pf5gWPLfWNo&amp;list=PLPI9hmrj2Vd9cqm4Ueb1_wNMqpCMBin6b) &amp;#x200B; Day 2: [https://www.youtube.com/watch?v=sZiK81u0vJc&amp;list=PLPI9hmrj2Vd8qrC\_VBGEobbyt19VXD6kZ](https://www.youtube.com/watch?v=sZiK81u0vJc&amp;list=PLPI9hmrj2Vd8qrC_VBGEobbyt19VXD6kZ) &amp;#x200B; For future posts, i will only be posting the "Day" playlists and their descriptions. &amp;#x200B; Thanks again all for the feedback.
Table b has complete history of repairs. so same id can be in multiple times, but table a is just a table of inventory only adding to it if we have a new id. So when I am inputting it into the history I want it to look through inventory and check if there is a matching id. and if not then to take the information used for the history and create a record in a since B has the same columns just different names for them with a few extra columns i dont need
Thank you. I started breaking down tables and writing each columns in a spread sheet so I can see what columns are in each table instead of looking at the whole table view and getting confused 
Thanks. I started writing the columns for each table into a spread sheet so I can see what columns are In each table as well as writing columns for views that are also in the database. Then I started writing query for each column I have to do, to join them all later 
It won't teach you SQL, but after you learn SQL, these videos will teach you things you didn't even know could exist. I've been a DBA for 18 years, and the first video blew my mind. I'm following along, but I'd be lying if I said I understood it completely. Yet. It's starting to gel. 
Probably then it is not the right thing for me then, but something I will bookmark and refer in the future. I know only basic querying in SQL and haven't worked professionally with SQL ever 
Backing up an active, in-use sqlite database from a script is easy: sqlite3 original.db ".backup backup.db" If nothing has it open and it doesn't have a lingering journal or wal file from the last connection not gracefully closing it, you can just cp the database file.
I can see the shape of where this is going. I hope it won't take me 2 years, but if it does, it does. This should be the basis of all database education. I'm going to be an evangilist for your videos. It's like getting to peek inside the gearbox of the universe of information. You said you don't think the same. That's happened to me before, and I look forward to it happening again.
Hey OP, this is way cool. I teach Data Warehousing at a local university and have found making videos to be a real useful teaching tool for MS SQL. I really appreciate you sharing your knowledge as it seems a lot of experienced people really like to hoard it. Way to be, dude. Also nice music :) I'll be chilling with these videos for a while
Thanks man. Make them beats myself. &gt;as it seems a lot of experienced people really like to hoard it. Silvered. &amp;#x200B;
Would be interested to hear otherwise because I'm 99% the answer is no and its a real pain! Even SQLAlchemy relies on having the drivers and accompanying modules installed to connect to the respective database. Alternatively, if you can get a dump of the database and put it in SQLite that mght be useful depending on your context.
&gt;if you can get a dump of the database and put it in SQLite Say I did, then what? :)
There are some pure python database drivers available for mysql and postgres. Would those help? 
It might. Tell me more (:
Jdbc or odbc?
not even clear on the difference. I'm using odbc right now. I just need a way to connect from python to sql without needing an external installation
Sqllite doesnt have rownumber... Maybe... this? SELECT IFNULL(price_inven.inventory,0) FROM price_inventory JOIN items On items.itemID = price_inventory.itemID Where key in (SELECT key from ( select MAX(key) key, itemsid FROM price_inventory ) )
Isn't creating temp table impossible due to the size of data? As said, I can't use a query to get all the data from BO1 or BO2 because it's too big, isn't gonna be the same with creating temp tables? I'm not that familiar with temp tables, sorry if it's a dumb question 
Sympathize with you. My org requires 12 page form and months of waiting to install drivers.
[pytds](https://github.com/denisenkom/pytds) may work for you.
Do you currently have *any* database drivers installed? Python should be able to use any database drivers you have installed. Then you'd just have to provide the parameters to connect to the database. If you don't have any database drivers installed, then the answer is no. A database driver is what you use to connect to a database. And you can't connect to it without a driver installed. If you're using an interface like SSRS, you can try connecting to something like that using requests in Python. I'm a bit surprised that an organization with such stringent database driver requirements is on SQL Server Express. If I wanted to go the free database route, I'd use Postgres.
ok, how do I check what IS installed in terms of database drivers? I can change what drivers python is using.
I'm not an expert, but you should have separate drivers for every type of database you use. For example, I use SQL Server Express, Postgres, and SQLite. So I have drivers for all three. Strictly speaking, you don't need one for SQLite unless you want to connect to it from another program like Access I think. But yeah, if you have software on your computer that you use to connect to SQL Server, you should have the drivers installed for that and should be able to use them
I have a measurement machine. The machine's Win7 PC writes to a SQL 2005 database. Does that mean there are SQL drivers installed? :)
Oh if you had all the data already, SQLite3 is native Python so you wouldn't need to install anything. No drivers, just works. Don't even need to pip install, its all ready to go. There s a browser that I like called DB Browser (https://sqlitebrowser.org/) but it isn't required, its just for convenience. It doesn't need to be installed, it is a portable application. It *can* be but doesn't need to be at east.
How would one go about doing this? Can you please list a step by step summary? Thanks! 
ahhhh I see. Sadly that seems to not be an option. the measurement system outputs data to the local sql db automatically then I have to retrieve it and do stuff with it
but I *don't* have the python drivers installed I only have [this](https://www.microsoft.com/en-us/download/details.aspx?id=56567) and the python script is working on my laptop, using the pydobc *library*.
Could you use sqlcmd?
Possibly. I've never looked into it before :) 
Its a great command line tool for querying. Only problem is that your results will be shell output so you may need to serialize it.
I can deal with that potentially.
If you are using Windows and have Pyodbc installed your are (and have SSMS) you are good. Just open up your start menu and search for ODBC. The click drivers. In drivers you should see a SQL Server driver use that in your pyobdc connection string
sweet I see it. I'll have to check what the measurement machines are running. I bet you they have drivers already installed right? Typing "pip install pyodbc" doesn't install drivers right?? 99% sure. People are telling me if a SQL database is running on the PC then there are sql drivers I can use. 
 You could use "for xml" to get your result in xml if that would help make it easier to parse. 
Pip installing does not install the drivers, just a method to use them. The drivers are a separate. You could also try pymssql it on Linux or Mac 
If you need a simple client, sqlcmd should already be installed. If you need something a bit more robust, I'd look into PowerShell. That's a .Net environment, and that ships with a built in SQL Server driver. Then you can take .Net code and write it as PowerShell. A function like [this one](https://blog.jourdant.me/post/simple-sql-in-powershell) will server pretty well for basic queries. 
Hm, is it easier to ask them to give it to you in another format? If they're outputting it they can probably give it to you with CSV files too pretty handily.
Can you run a Linux vm so you can admin pip install an appropriate package? Maybe try local installing the package(s) with pipenv?
You should be using `INTEGER`, not `INT` in sqlite3, btw: https://www.sqlite.org/datatype3.html *ESPECIALLY* for integer primary keys if you're looking for as fast as possible: https://www.sqlite.org/lang_createtable.html#rowid
Actually, it does have window functions including `row_number()` these days.
Anyways... maybe SELECT ifnull(p.inventory, 0) FROM items AS i JOIN price_inventory AS p ON i.itemID = p.itemID GROUP BY i.itemID HAVING max(p.key) = p.key;
I do believe that the tables are are listed as INTEGER, I am typing out of memory. I didn't create the tables, I'm just trying to get the data out as quickly as possible.
I will plead my ignorance, what do you mean "have indexes on them."?
... oh. You *really* need to read https://use-the-index-luke.com/
Couldn't you run sthell commands against sqlcmd? I'm guessing that anything with Sql express would have sqlcmd.. there used to be other clients apps isql is the one I am thinking if right? 
Not put it all in sqlite and keep it there.. but how about pushing Sql command for your real database into sqlite and then Email it to somewhere with drivers I'm guessing that is an option. Kinda brings Sql injection to a whole new level. 
What about using MSSQL JDBC driver and JayDeBeApi? You don't "install" a JDBC driver as such (if I recall correctly), you just call the downloaded .jar file. You'd need a JRE (possibly JDK?) to run it.
Why not? (\S*[^, ]) &amp;#x200B;
I’ll see what I can put together or find. 
Thanks bud, I know it takes work so I will repay with coffee or beer if you like. Just PM me. Thanks again!
They are completely two different path and curriculum will not overlap. You can probably find answer to this by reading program requirements. If you find yourself comfortable writing codes, and working with data, there are opportunities for data analyst, and further advance your education and career into data scientist. You can also focus on database administration that is more IT oriented than data themselves to ensure things are running in the most optimal way for users. If you prefer learning about infrastructure, security, and encryption etc. the. choose Networking. You will do well salary wise in all of these areas but the main thing is finding your interest. You rarely excel in a career that you are not passionate about. 
Thanks that works perfect! Can you explain why exactly it works? It extracts all non whitespaces and then removes , ?
You should check out [https://regex101.com/](https://regex101.com/). Type in the expression and it'll explain it way better. Also, you'll have a handy tool for playing around Regex.
Can you paste your code? 
its in the pictures &amp;#x200B; &amp;#x200B;
I got that, can you paste it? Its a lot easier to work with
create or replace procedure average\_dog\_fee(dogname in varchar, the\_fee out number) as begin select round(avg(e.fee), 2) into the\_fee from examine e where e.did in (select did from dogs where dogname = dogname); end average\_dog\_fee;
This code doesn't match anything in your picture
just realized this. mistake on my part. please check picture again
I’m very sorry for this hassle. That select statement gives me the correct average fee, which is 22 (rounded 22.67). The other statements give me 30.5
This is a homework question right? All I can see if that your variables are different in the queries potentially. You haven't given the all the table data so its hard to tell. Is 'Fido' in the d alias'd table?
Yes Fido is the name. I’m just so confused on how I can pull my select statement out of the stored procedure and have it give the correct average. But when I execute the stored procedure I do not get the correct average 
Yes Fido is the name. I’m just so confused on how I can pull my select statement out of the stored procedure and have it give the correct average. But when I execute the stored procedure I do not get the correct average 
Are you putting 'Fido' into one query and then 'Spot' into another? 
No I’m not
I switched it back to being null and ran the exec with Fido and it’s still the same
Installing the python modules isn't the problem. Most of the drivers that talk to the database is, but a VM *might* be able to do that but given how locked down the org sounds, they'd probably have 4 weeks of paperwork to install VirtualBox on the machine!
I did some quick napkin math on the numbers you're getting. You're selecting the wrong ID. ID 324 has average 21.67, 731 has average 30.83, 582 has average 41.25 approximately. Which one is Spot and which one is Fido? I would guess from the numbers you've given that dID for Fido is 324 and the dID for Spot is 731?
Yes that is correct and that is how I have it
Pm me please 
I'm surprised no one has suggested either of these:. 1). Use C# or C++ .NET 2). Use Docker on Windows? 3). Tell them that tieing your hands makes it pretty hard to do your job. 
&gt; We need to actually model something like adventure works (in core) so we can flatten the data in atomic and send data to our warehouse (strategy). That is when slowly changing dimensions really take shape. Just accepted a job offer where my first big project will be something similar - except portions of the source data resides on 80,000+ hardware devices. I've been tasked with setting up their DW architecture and flattening all of the data so it can be used for client reporting and financial purposes. After watching the first video, I can definitely see the awesome implications that this type of work could have - I may be seeing if I can put it into practice. Thanks for the insight and knowledge sharing, and I'll definitely be watching your other videos. 
This line near the error message is definitely using a MS SQL function and partition syntax. row_number() over(partition by id,userid order by comment_timestamp) it might not be compatible with MariaDB or MySQL. You should look up the syntax and find a compatible way to get this number if its not. Break the subqueries out and try to narrow down exactly where the error is.
No [Check out here working on MariaDB 10.3](https://dbfiddle.uk/?rdbms=mariadb_10.3&amp;fiddle=7a3847b43e4486836285bc4e0cc83c1b) But not in MySQL
Window functions (ROW_NUMBER() OVER(...)) where introduced to MariaDB with version 10.2 and to MySQL with version 8.0. They were simply not available in earlier versions.
try this: CREATE INDEX IDX\_SEARCH\_LOCATION ON FILM (INSTR(DESCRIPTION,'Boat'));
Select text from table group by text;
The result you say you want still has repeat values in it. Could you be a bit clearer? If you want distinct values from the one table, use DISTINCT.
The result has repeated values, but not one after another. Let's say you have the following array: `['A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'C', 'B', 'B']` The result I want is: `['A', 'B', 'A', 'C', 'B']`
This. The index is on the function(column).... not on the WHERE condition (where something &gt; 0). The index will be created on that 'something', and when your query is run, it will find all the records where your 'something' is &gt; 0, but the index itself just stores the 'something' value for each record.
I can't give an honest assessment on tools for Postgres. I do not use it enough. I have used pgadmin before, and that wasn't a horrible experience: https://www.pgadmin.org/ Either way, the postgres community maintains a guide: https://wiki.postgresql.org/wiki/Community_Guide_to_PostgreSQL_GUI_Tools
Window functions were added in MariaDB and MySQL 8. 
When you select only one date, you're getting the `Close_price` for that one date because there are no preceding rows to average over. The database doesn't have any other data to work with. [Remember the execution order for SQL](https://docs.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql?view=sql-server-2017#logical-processing-order-of-the-select-statement) Brute force method: select * from ( SELECT MARKETDATE ,STOCKID ,CLOSE_PRICE , AVG(CLOSE_PRICE) OVER (PARTITION BY STOCKID ORDER BY MARKETDATE ASC ROWS 13 PRECEDING) AS SMA14 FROM dt WHERE MARKETDATE between '2017-01-01' and '2017-12-31') as FullYear where FullYear.MarketDate = '2017-08-31' Slightly less brute force: SELECT top 1 MARKETDATE ,STOCKID ,CLOSE_PRICE , AVG(CLOSE_PRICE) OVER (PARTITION BY STOCKID ORDER BY MARKETDATE ASC ROWS 13 PRECEDING) AS SMA14 FROM dt WHERE MARKETDATE between '2017-08-01' and '2017-08-31' order by marketdate desc There are probably better ways to do it but those are the first that come to mind.
So each time the text value changes, you want the first row of that group until it changes again? If the Ids are always going to be sequential (or you have a date column or something you can use for ordering instead), you can use the LAG function to get the text value of the previous row and compare this to the current row using a CASE statement. If it is different, set a flag to return the row. If not, don't flag it. Put that inside a CTE and you can select only the rows that you need. &amp;#x200B; SELECT Id ,Text FROM ( SELECT Id ,Text ,CASE WHEN Text &lt;&gt; LAG(Text, 1) OVER (ORDER BY Id) OR LAG(Text, 1) OVER (ORDER BY Id) IS NULL THEN 1 ELSE 0 END AS 'Display' FROM Table ) res WHERE res.Display = 1 &amp;#x200B; (The OR LAG(Text, 1) OVER (ORDER BY Id) IS NULL part is to make sure the first row is returned)
 So each time the text value changes, you want the first row of that group until it changes again? If the Ids are always going to be sequential (or you have a date column or something you can use for ordering instead), you can use the LAG function to get the text value of the previous row and compare this to the current row using a CASE statement. If it is different, set a flag to return the row. If not, don't flag it. Put that inside a CTE and you can select only the rows that you need. &amp;#x200B; SELECT Id ,Text FROM ( SELECT Id ,Text ,CASE WHEN Text &lt;&gt; LAG(Text, 1) OVER (ORDER BY Id) OR LAG(Text, 1) OVER (ORDER BY Id) IS NULL THEN 1 ELSE 0 END AS 'Display' FROM Table ) res WHERE res.Display = 1 &amp;#x200B; (The OR LAG(Text, 1) OVER (ORDER BY Id) IS NULL part is to make sure the first row is returned)
Wow, I didn't know the LAG function. Your example fits perfectly to what I need. Thank you.
Happy to help :) Window functions are great for stuff like this, running totals etc. Just make sure to have an index on the ordering column or they can perform poorly with big tables/complex queries.
You are welcome, and thank you for watching.
If the goal is to not have duplicate insert values, couldn't you just use INSERT IGNORE?
Good to know. I work with TSQL all day every day and haven’t kept up with the others.
I use the datapointids and datasetids to create a source and destination from dbcore to dbatomic that helps me take vertical data and store it horizontally using a function that creates the update statement for any domain / shared attribute/rcft-r/cft-b shape. The tables in atomic are automatically created because each table has repeating column groups and depending on our record size, it could contain 30-40 of these repeating column groups. There is a header table that will be FKd to the N number of these flattened structures for physical modeling. That will be our type 1 dimension, always current. The same relationship table that moves data from everywhere to atomic will be responsible for doing the same thing from atomic to report (for reports) and atomic to strategy. Strategy is tables are similar except, there is a previousId column that is similar to the cleansable field. That is the type 2/3. 
I feel your pain. I have 5 different DB systems at my job and have to pause each time I write a query to figure out what is/isn't supported for each one!
I believe you have 2 errors here 1. You want to move **as C** to the end of your subquery 2. you are selecting sum(c.line_total) inside of the subquery that you alised as 'c'. The alias c does not have scope inside of itself like that. I assume you just need to remove the c. from c.line_total So try this select c.line_total from ( select email_combined ,return_year ,TimeFrame ,sum(line_total) as line_total from bi_sandbox.[reports] group by email_combined, return_year, TimeFrame ) as C 
`The multi-part identifier "c.line_total" could not be bound.`
&gt; ,sum(line_total) as line_total are you sure you fixed the line above?
Wow Dave that did the trick. Thanks so much Man...
no prob, sometimes you just need a fresh set of eyes
Glad someone helped you. To add on: Using `c as ()` works if you're using a cte, such as: with c as ( stuff ) select * from c If you have multiple parts you need to make sure to use a `,` after the `)` for all sections above the last section before you do your final select, such as: with c as ( --with select blah from table ), --comma c1 as ( --no with select blah from c ), --comma c2 as ( --no with select blah from table ), --comma c3 as ( --no with select blah from c2 join c1 ) -- no comma select * from c3 join c join c2 I think you can also do `c = (query)` but there are other syntactic things you'd need to do to get it to run. Otherwise as mentioned you would `select * from (query) as c` and as you probably already know you need to give an alias to that query or else it won't work, but you do not generally to alias a table unless you are using joins. So `select * from table` will work just as well as `select * from table as c` or `select * from table c`. Little things like this drove me nuts for the few few months.
 SELECT MIN(ID) AS ID, Text FROM Table GROUP BY Text or: select id, text from ( select id, text, row_number() over(partition by text, order by id asc) as rn ) x where rn = 1
 select , case when vintage_status = 'New' then 'New/Reactivated' when vintage_status = 'Reactivated' then 'New/Reactivated' else 'Returning' end as 'Vintage Status'
 SELECT CASE WHEN vintage_status = 'New' THEN 'New/Reactivated' WHEN vintage_status = 'Reactivated' THEN 'New/Reactivated' ELSE 'Returning' END AS 'Vintage Status'
Thanks for the help.
You need to calculate the moving averages per day, and then you can select your day. I did it in MSSQL using a CTE. Something like this... WITH cte AS ( SELECT Ticker, RecDateTime, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 1 PRECEDING) AS RollAvg1, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 2 PRECEDING) AS RollAvg2, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 5 PRECEDING) AS RollAvg5, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 10 PRECEDING) AS RollAvg10, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 15 PRECEDING) AS RollAvg15 FROM StockPrices ) SELECT Ticker, CAST((DATEADD(week, DATEDIFF(WK, 0, CASE WHEN DATEPART(DW, RecDateTime) = 1 THEN RecDateTime - 1 ELSE RecDateTime END), 6)) AS DATE) AS RecDate_WkEnd, AVG(cte.Price) AS Price_Avg, MAX(cte.Price) AS Price_Max, MAX(cte.RollAvg1) AS Price_Max_1MinRA, MAX(cte.RollAvg2) AS Price_Max_2MinRA, MAX(cte.RollAvg5) AS Price_Max_5MinRA, MAX(cte.RollAvg10) AS Price_Max_10MinRA, MAX(cte.RollAvg15) AS Price_Max_15MinRA FROM cte GROUP BY Ticker, CAST((DATEADD(week, DATEDIFF(WK, 0, CASE WHEN DATEPART(DW, RecDateTime) = 1 THEN RecDateTime - 1 ELSE RecDateTime END), 6)) AS DATE) ORDER BY RecDate_WkEnd
unfortunate inadvertent example of **leading comma convention**
It would work most definitely but I needed to emulate a process that is being implemented at the Tableau script and wanted to verify if it works in SQL server.
que?
Could you post the requirements and what you have?
thank you!
You might make Route be a standard route, and then Flight is an instance of a Route (on a specific date/time). Also, your Customer, Passenger, Reservation tables are somewhat confusing... I think Passenger and Reservation are redundant, unless your requirements specify they should be separate, you could just make a single "Reservation" table which will tie in a specific Customer, Flight/Route, Fare, and Seat. And because a single Reservation could have multiple pieces of luggage, I would put the ReservationID as a foreign key on Luggage instead of having LuggageID stored on a Reservation.
We were told we needed to separate customer and passenger as you could book a flight butnot actually fly. Thanks for those hints, how does the country code table work. Do I need separate tables for country and city as one country can have many cities?
Here you go: SELECT Id, Text FROM (SELECT Id, Text , ROW_NUMBER() OVER (PARTITION BY Text ORDER BY Id) AS RowPriority ) SubQuery WHERE SubQuery.RowPriority = 1
Also can you have multiple PK and FK in one table?
The PK can be concatenated, which means the combination of all the PK column values is what makes a record unique. And you can have as many FKs as you want (they just relate back to another table's PK to create a relationship). 
I wouldn't even worry about including Cities. You have Airports and Countries. Flights will only care about the Airport (and maybe the country), and not really care about the City.
I'm on my phone but something like this? SELECT to_char(value, 'HH24:MI:SS') FROM ( SELECT TO_TIMESTAMP('10/7/2018 1:09:51 PM', 'MM/DD/YYYY HH:MI:SS PM') value FROM dual );
SQL deals with sets, not ordered lists. There is no concept of the "record before" or the "record after".
 select TO_DATE(TO_CHAR(SYSDATE,'hh24:mi:ss'),'hh24:mi:ss') from dual or SELECT EXTRACT(HOUR FROM CURRENT_TIMESTAMP) || ':' ||EXTRACT(MINUTE FROM CURRENT_TIMESTAMP)|| ':' ||EXTRACT(SECOND FROM CURRENT_TIMESTAMP) FROM DUAL;
You don't have to use a window function for a single date's running average: select stockid ,avg(close_price) as sma14 from dt where marketdate between to_date('2017-08-31','YYYY-MM-DD') - 13 and to_date('2017-08-31','YYYY-MM-DD') 
 select , case 
This is a Common Table Expression. At a basic level, it works like a subquery or temp table. WITH cte AS ( SELECT Ticker, RecDateTime, AVG([Price]) OVER (ORDER BY RecDateTime ROWS 1 PRECEDING) AS RollAvg1, FROM StockPrices ) The meat of it is: AVG(Price) OVER (ORDER BY DateTime ROWS 15 PRECEDING) I have a couple of versions of this in my query, because I was using it to get various intervals. It says "Order all the rows in StockPrices by date time, get the average price going up (preceding) 15 rows. Store that in a CTE named "CTE" (I'm very creative)". Now I have a table of every ticker, date, and averages. I need to select what I want from the table. SELECT Ticker, cte.RollAvg15 AS Price_Max_15MinRA FROM cte There's some funky date code in there because I'm looking for the max average over the span of a week. My original code is not for stock prices, but could easily have been. 
Normally this is done on whatever reporting program you're using to display the data. Can't you just paste into Excel and format there?
Meh the sma is kinda useless, how to get the ema?
lol savage
This should be done in your reporting tool of choice, which may be the query exported to Excel and applying the correct number formatting to the column. You wouldn't modify the database.
Thank you!
Just a guess but maybe it's hh:mm:ss
I'm not super sure on the date it's supposed to be but it's either the number of days since 1/1/0000 or 1/1/0001. This https://www.timeanddate.com/date/dateadded.html?m1=1&amp;d1=1&amp;y1=0001&amp;type=add&amp;ay=&amp;am=&amp;aw=&amp;ad=736941 should answer it
Each system does things differently. I've seen systems that store dates as an INT. Some start at 1900-01-01 (like excel) and others start when the Gregorian calendar started. I've seen one that started on something odd like 1980-07-01 (the date that product went live). Without more knowledge there's no way to answer that. It's likely 736941 days after something.
that's it for sure since i was expecting a date in September. thanks dude
https://www.sequelpro.com/
I’ve never heard of an employer looking for a sample portfolio for a SQL job, but I’d only consider myself having an intermediate level of SQL skill. That said, I just got a job writing reports and they tested me with live query questions. So maybe just keep applying and bone up on your queries. I guess it really depends on what you’re looking for. 
I'm not sure I follow, I was assuming OP knew enough to include any other relevant logic. I always lead with commas... is that something you don't like?
What is the significance of ROWS 15? How does it now what is 15 preceding? Or is it very simply doing something like a "rownumber" and ordering all of the datetime rows, and then only taking an average of the last 15 preceding?
There's the Oracle OCP certification. There are two exams to get the cert in Oracle Database, the first is SQL Fundamentals, the second is Administration. You don't get a cert for just the SQL exam, but I guess you could always put it on your resume.
The ORDEr By clause tells the system how to order rows for PRECEDING, LAG. Etc.
Thanks man, I'll file this in the back of my brain.
My .02 is that you're 100% wrong. Never heard of someone looking at a portfolio SQL job? Cool, so you're saying 1% of the applicants have a portfolio that shows their ambition? /u/ipjck if we were hiring right now for an entry position I'd ask for your resume and put you at the head of the pack just because you did this to learn. Keep doing it. Show me how hungry you are son, and we'll make sure you stay fed. Feel me?
I think it was like $125 or so. Can't remember, I took it about 10 years ago.
Thank you. Is it mostly syntax and queries?
You could draft a few reports in SSRS (if that's your thing) or create an ERD. Also perhaps write some creative queries and put them in a word doc if you want to, as part of your portfolio. TBH, I never was asked to show DB stuff I worked on, but after entry level jobs normally you have technical interviews. The higher level you get to the longer the technical part goes. My latest was 4 separate technical interviews with 4 separate groups for a total of like 8 hours, no joke. They get pretty grueling. My advice when interviewing, is - if you don't know the answer to something admit it. If they ask you to guess, then try to vocalize some logic on why you're guessing. Never, never try to BS. Actually landed one of my earlier mid-level jobs that way - they told me they chose me because I admitted when I didn't know something whereas the other people would try to BS their way thru. Good luck.
I don’t know what kind of job you have, but yes, I’ve never been asked for any kind of portfolio. I write reports, I’m not a DBA.
Joins, unions, sub queries, maybe a pivot. Again, I write intermediate level select queries to make reports, so you need to hav a good grasp of what you’re applying for.
I had a similar, but less intense interview process; 4 people and live questions for a mid level job. I told them I didn’t know the answer but I could find it given the time. Admit to the all knowing Google, it’s always going to know more than you. 
I didn't say you were asked for it. But if I have one, and you don't, and we both apply for the same job and interview the same? Who wins? Get a portfolio.
Got a junk email from Oracle the other day that they are overhalling their training and adding new certifications.
The brute force method seems to be working but it’s taking really long, my code’s still running haha 
We have a bunch of questions that range from what I consider easy to really hard. Some of the hard ones are like "Show me 4 different ways to start with this table and end with this result set". Sometimes they'll require recursive cte's, ugly row_number's, and other fun stuff. We also have a query where I show the execution plan and want them to tell me where I should be looking for efficiencies. I also have a couple different examples (a building security system, a library, a cargo shipping company) where I will ask you to build me a data model, naming all the tables and key columns. When you're done, I have a couple different queries that I ask you to write off your new data model. This line of question really weeds a lot out. I usually have them write the data models and the queries on the whiteboard or in a notepad window (if interview is remote). I don't really care about the exact syntax/parameter orders/even function name if they comprehend. I am a firm believer in looking up simple stuff on google and using your brain for the big picture stuff. 
Did you try a subquery using a case statement to only grab Cash for one column and CashB for the other? Subqueries would definitely solve the overall problem your specific situation seems to have.
I'm very confused.
How similar or different is oracle and PostgreSQL? Would knowing oracle well help in Postgres or are they different by a lot? If you know please let me know 
I'd be wary of a company that asked for a 'SQL portfolio' because it would show a lack of understanding on their part. It's not really possible to do a SQL portfolio because SQL is used along with other code on the back end. You can't just say this SQL script or procedure works without an understanding of the database or the code that uses it.
The first approach is calculating a whole year's worth of averages before returning the result for the date you're interested in so it's normal for it to take a while (assuming you have a lot of data). If I understand the question correctly, you should be able to change the date range inside 'FullYear' to cover just the 14 days you're interested in and get the same result (similar to the second approach, which should be much quicker as it's only dealing with a month's worth of data from the outset).
You could always use a database instead of MySQL.
Couldn't you just subquery in a select avg where date&lt;selected date and date&gt; selected date - !4 days? 
I have to agree here (ish) . Unless you are a DBA, you don't really have a general "Sql Portfolio" with all of your DBA queries. DBA's (atleast most) should have a bunch of queries prepared which can be used against multiple databases which can help identify exactly what is going on in the servers. For example: Index fragmentation, open connections, long Running Queries, etc. However, if you are trying to prove to an existing employer that you know the existing database and how to work your way around it... It doesn't hurt to have some ready written queries (which can be used for reports as well) specifically for the business logic (not DBA tasks). At my existing company I do have several queries written that have nothing to do with DBA tasks, but have a lot to do with day to day business needs. If I were to join a new company, these queries I wrote are completely useless, because they are specifically written for current company's business needs. PS: I do have a repository of DBA queries as well organized by SQL Server Version. 
For all those saying they’ve never brought a portfolio to an interview, I’ve broug examples of what I have done to my last two interviews. The first time I did it was for an interview for a career change. I got offered the job, accepted, and one of the first things my new boss told me on my first day was how impressed she was at me bringing examples of work I had done and how no one had ever done that. Be creative, bring a few examples. Maybe even load some data into an Access database, create some queries, and make a nice little menu to enter prompt values and run the queries from. 
Could you give an example of your intended end result of the data model, and perhaps the key points you're hoping to be made? I'll likely get asked model questions in an interview scheduled within 2 weeks, would really help for your interpretation of what you're looking for.
TIL about ROWS PRECEDING in a window function. Thanks!
There is the ANSI SQL standard, which I believe both systems comply with... the standard stuff like SELECT FROM WHERE INNER JOIN max min avg GROUP BY, etc. But then for non-standard functions, each vendor will have their own syntax.... like if you want to pull out the year from a DATE object... Oracle uses EXTRACT while other vendors might use DATE_PART. The ANSI SQL stays the same, but the extra little bits are platform specific.
I've had four database jobs and never been asked for a portfolio for any of them. 
I'm a SQL developer. I have been writing SQL full time for 5 years. Once you get the interview you'll have a technical aspect to it. You'll be asked questions like what's the difference between an INNER JOIN and a LEFT JOIN. How do you get the distinct count of [something] from a table. What's the difference between these 2 queries (what will they return?). SELECT * FROM Customer c LEFT JOIN Orders o ON c.CustNum = o.CustNum AND o.OrderDate &gt; '2018-01-01' SELECT * FROM Customer c LEFT JOIN Orders o ON c.CustNum = o.CustNum WHERE o.OrderDate &gt; '2018-01-01' The hard part is breaking in. Once you have a few years experience, future employers will know you have some skills. To break into the SQL world, check out a staffing firm. You'll be an hourly contractor with no benefits and no job security. It should pay decent (&gt;$30/hour in the USA) assuming you're in a metro area. Once you've worked for a year or two you'll have enough experience to move on to a different (more stable) job.
Happened to me the same. Looking for a career change, brought an example of work done (a quite complex access DB), impressed everyone.
The reason why you back up the database and transaction log is for recoverability. To restore the database you need to first start with a full backup, then roll up every transaction log backup up until the point in time you want to recover to. If you lost data yesterday or the db got corrupted, and you last did a full backup a month ago, you have to restore the month old backup file and pray that you have each and every daily transaction log file backup for the past month. If you better understood how to restore a database, you might make different choices in how you backed it up. Honestly it is way too easy to automate this with a simple job. Have it do a daily full backup, log backups every 15-30 min and delete old backups outside of whatever retention you set. Now you have set yourself up to be the hero if anything ever went wrong with your database. 
I would not feel confidant in with that solution. I would check out using [OLA's Backup scripts](https://ola.hallengren.com/sql-server-backup.html), if you haven't already. This all depends on how much space you have to work with and your retention policy. Maybe going with a weekly full backup, daily differentials, hourly logs? Then hold those for two weeks and then purge anything you no longer need. If something happens between your full backup and your last log, you're up the creek. 
Why not just put the database in simple mode if you don't need backups? Then you don't have to worry about the tlog.
How do you bring it though? I have a DB on MySQL
simple recovery mode. how are you able to not need ANY backups of the DB? or.. leave the db in full or bulk recovery, and set a scheduled job to backup the log every few minutes and then delete the file. 
It's used for testing and they want it to mirror our pre-production/ production environments as closely as possible.
Thanks for replying - will that keep the log file in check even without DB backups?
Am I taking crazy pills?
the log file will only grow during transactions and auto purge when complete. keep in mind, the growth is identical per transaction regardless of recovery mode. aka: you run some massive INSERT/Update, it needs that space until complete.. 
It orders by date, then takes the preceding 15 rows. You can change it to 30 or 60, whatever to get however many days. For stocks, you’d probably want to partition it be ticker in this part too.
&gt; Couldn’t you just say well you could, and it would run, but you'd get **all** customers as long as the entire orders table is not empty this -- `EXISTS (SELECT * FROM orders)` -- will always return TRUE if there is at least one row in the orders table you need the **correlation** in the subquery (yes, it's like a join) in order to match each order to the customer it belongs to if a particular customer has no orders, then the WHERE condition evaluates to FALSE 
`NATURAL JOIN`
It depends. One is not strictly better than the other. The best way to tell would be to do some performance analysis for yourself
I tried to document how I do things but I don't have a clean way to post pictures here on Reddit. So, I'll give you a few links that are good starters. [Design Database Diagrams](https://docs.microsoft.com/en-us/sql/ssms/visual-db-tools/design-database-diagrams-visual-database-tools?view=sql-server-2017) [Visual Database Tools](https://docs.microsoft.com/en-us/sql/ssms/visual-db-tools/visual-database-tools?view=sql-server-2017) Those should be some good starting locations. I usually start with some key tables and then expand out from there in defined branches. If the d/b is really large they can get messy real quick so I tend to keep my diagrams small so that they print out on 1 page and keep them task specific to what I'm working on. &amp;#x200B;
So mine modelling questions are usually around data warehousing. I am a huge fan of a well designed Kimball model. I have seen them perform well in the many billions of rows. I want to see facts with very clear/deliberate granularity, no attributes/junk data, absolutely no snowflaking. I will usually ask about natural vs. surrogate keys. I will usually single out datedim and really like to see the DateID in the "112" format (20181022). I will ask if you intend to put a surrogate key on the fact, and if so why (either answer could be right, depending on how you back it up). I don't necessary like everything to be done "my" way. All of my good architects do things a little different, but I trust their designs and they all doing things deliberately and clearly. A Security System will have something like this: factEvent| ---| factEventID| DateID| TimeID| EventTypeID| BuildingID| ScannerID| BadgeID| PersonID| CompanyID| DateTime (as a datetime)| And obviously all associated dimensionalality. I'll then ask the candidate questions like, "how do I know who is in the building at the moment, assuming no piggy backing?. "How can I find out if a user piggybacked in or piggybacked out?" 
I have not. I'll give that a shot, thanks. Seems obvious now but i've been fighting this particular issue for so long i'm all turned around.
RemindMe! 7 days
I will be messaging you on [**2018-10-30 16:08:51 UTC**](http://www.wolframalpha.com/input/?i=2018-10-30 16:08:51 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/SQL/comments/9qodps/can_i_perform_only_transaction_log_backups/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/SQL/comments/9qodps/can_i_perform_only_transaction_log_backups/]%0A%0ARemindMe! 7 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Thank you, makes me feel a lot better with regards to potential questions and that I could comfortably answer these! Obviously indexing is pretty mandatory (depending on database structure/size) - I'm more worried about DBA aspects, although I'm aware of query execution plans and building efficient, scalable queries, things like security and "paramaterization" is still new to me, and I've not implemented a warehouse from scratch, I've just worked and developed existing ones. Going from "understanding, improving, and developing from" an existing DW - how easy do you suggest it is to transfer to "implement a DW"?
Depends on the database platform - run both queries with an explain and see how the execution plans differ. Generally for IN vs EXISTS, they should be the same. For NOT IN and NOT EXISTS, however - they tend to be quite different.
Your join statements need to be together. from table1 Join table2 on col1=col2 join table3 on col1=col2 where col1 = val1
So while NATURAL JOIN is a thing, I would suggest against using it, personally. It makes debugging incredibly difficult and anyone that has to touch your code down the line is going to curse you if something breaks. Especially if your DW looks anything like the one I inherited where column names are arbitrary values that were decided on a whim. The same column name in two different tables is sometimes the same thing, sometimes not - it's a crapshoot.
Tasty.
Change Join Enroll on Class.classNumber = Enroll.classNumber to Join Class on Class.classNumber = Enroll.classNumber The way it was previously written, you were trying to join Enroll back on to itself. This is possible, but you need to create aliases for the tables in this case. This is something you'll encounter later in your class, I would presume.
I'm not even sure how this is working (but I'm a SQL Server guy)... where's your GROUP BY Customer.name? SQL Server throws an error... perhaps MySQL just puts the total sum with with the first customer? TL;DR - Add: GROUP BY Customer.name to the end of the query and tell us what you get.
MySQL for god knows what reason allows aggregate functions without a group by.
Being asked for something is different than having something and offering it, innit?
Oh, I did that deliberately to be honest but I understand your point now.
SWAG: Bonds reads like a master table that stores the latest value (and other info) about a security. So I think OP might be off the hook there. I'm still cautious about order of operations (parens and MDAS) when I'm multiplying aggregates, so good call, /u/sHORTYWZ.
I think you're missing the point that everyone us telling you: it isn't a useful type of portfolio and giving someone random queries isn't going to do anything to assist you getting these types of jobs. 
What is your reluctance to creating a back up of your dev enviroment? Storage is so cheap these days, it's laughable. That's why your getting so much push back, the idea that this isn't affordable is a joke. If it saves your developers 15 minutes of time once a year you've saved money over the long run. 
Hey, reubendevries, just a quick heads-up: **enviroment** is actually spelled **environment**. You can remember it by **n before the m**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
`NTILE(4) OVER (ORDER BY SomeDate) AS Quartile` will give you values 1-4 representing which quartile they are in.
Laptop. Bring it with and have it on and ready to go. If you need internet access, don’t assume you’ll be able to connect your computer to their network, set up a hotspot using your phone ahead of time. You’ll have all your bases covered if you have your own laptop and internet access. 
Do I just show them some queries I made then? The database I made is sort of childish but still very in depth. It’s not your average customer-order-product database.
`SELECT` [`c.name`](https://Customer.name) `as 'CustomersName',` `SUM( i.amount * b.latest ) + c.assets as 'TotalValue'` `FROM Customer c` `INNER JOIN Inventory i on i.ssn = c.ssn` `INNER JOIN Bonds b on b.ticker = i.ticker` `GROUP BY` [`c.name`](https://c.name) I like to specify my joins, though... makes things a little more self-documenting. I think /u/sHORTYWZ might have a good point - your SUM line might add the assets in again for each security the customer has. When I want to troubleshoot an aggregate (like SUM), I'll temporarily remove it (and the GROUP BY, of course) to get individual rows. In this case, if a customer has ten securities, you'll see ten rows, each with that one customer's name, and a value that represents the number of shares of security 1 times the latest price for that security, plus the value of her assets. Because these three numbers will be used to calculate the value for each of the ten rows, I think that when you put the AGGREGATE back in, the assets would be added back in ten times. In other words, I don't think there's any difference between `SUM( i.amount * b.latest ) + c.assets` and `SUM((i.amount * b.latest ) + c.assets)` Since I'm not super familiar with MySQL, however, I'm ready to be wrong, and I'm interested in hearing your results. Cheers!
And that goes in my select?
Spot on with the troubleshooting tips - I intended to write that out but got called in to a meeting. Always take the query down to the lowest level possible and troubleshoot upwards. 
Method|Pros|Cons :-|:-|:- `LEFT OUTER JOIN`|More query hints are available to the from clause|Can add rows to an intermediate dataset on a one to many join `NOT EXISTS`|Sometimes get's stuck at illogical places in the query plan|Doesn't add to the intermediate dataset `NOT IN`|Same as `NOT EXISTS` with some NULL edge cases|Same as `NOT EXISTS` with some NULL edge cases `EXCEPT`|Compares the equivalence of EVERY column|Compares the equivalence of EVERY column
It does. It will show a column with values 1-4, depending on the value of SomeDate.
Yes. [https://docs.microsoft.com/en-us/sql/t-sql/functions/ntile-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/ntile-transact-sql?view=sql-server-2017)
Well, there are a lot of jobs so maybe my sample is off. I just know if you wouldn't play ball with SQL competency portions of an interview, none of my previous employers would have even given you a call back because they'd assume that your portfolio was made by somebody else and you refuse to answer questions proving even minimal levels of competency.
I totally shut those portions down. I do not answer technical questions in specifics, I point to my portfolio and explain how I have done what they are talking about in specific ways. I have found it to be far more effectively and convincing someone that I am capable of doing the job, and 99% of the time I get responses like, "oh we've never seen one of these before." Good. That makes me the only candidate with one. I might not get the job, but I almost always get to the final round of interviews. Because it's on Dropbox I can email the link to people before I even meet them. And generally I ask them straight, "did you look at it?" You need to learn how to set the tone in an interview. You are not there to impress them. They are there to impress you. &gt;, none of my previous employers would have even given you a call back because they'd assume that your portfolio was made by somebody else and you refuse to answer questions proving even minimal levels of competency. I would not work for them.
good human
Very similar. If you have strong knowledge of one implementation of SQL, you will be able to function in another version. Obviously, system tables changes, functions change, but the core is still set based thinking. Some things are easier in different implementations are harder. Some are easier. Here's something cool that's possible with Postgres that isn't easily possible with Oracle: https://www.citusdata.com/blog/2018/03/14/fun-with-sql-generate-sql/
We work with PHI here too. Our code and DB are all up in Azure. Each organization must do what is best for them so check with whoever you need to check with before deciding to move forward. &amp;#x200B; Here is some stuff from Microsoft's site. [https://www.microsoft.com/en-us/trustcenter/compliance/hipaa](https://www.microsoft.com/en-us/trustcenter/compliance/hipaa)
Its really just an idea to house all our SQL code in one central location. Right now it's the wild west so everyone dev is saving their code in different places. Once someone leaves, it's impossible to find where their work is saved. &amp;#x200B; No real resistance to gitlab, just that SQL Developer itself doesn't connect to it for some reason. So I was trying to find an alternative git-style system that it can connect, and maybe set it to commit work at X time of the day. Basically, less interaction the actual team member has to do, the better, lol.
I 100% agree with you about it being bad practice - but yeah you can lead a horse to water, you can't force them to drink...
Haha agreed!
That's only going to work if there are an equal number of records per day, as well as an equal number of records per quarter. Would be better off with a case and datepart. CASE WHEN DATEPART(MM,SomeDate) BETWEEN 1 AND 4 THEN Q1, etc...
Though i would alias your table names. select s.stuID, s.lastname, e.classNumber, c.sched from Enroll e Join Student s on e.stuId = s.stuID join Class c on c.classNumber = e.classNumber Where s.lastname = 'Smith'; 
Lol that sub is borderline satire.
I don't follow. NTILE always groups records into even segments (+-1 record).
yup - I once got downvoted to hell for chiding a guy who worked for a public school was gripping that someone asked him to support some android tablets that were donated to a classroom, I guess his wifi network he created would for some reason not connect to android tablets so to fix that problem they just created a no android tablet policy... FFS...
I was asked to model astronomical objects. maybe offer to solve something for them, on a white board? &amp;#x200B; I ask people if they know the 4 types of joins (MSSQL). -1 point if they say left or right.
Well the feeling is mutual. I wouldn't want to work for someone who doesn't require people they hire to prove even minimal levels of proficiency live.
yo dawg i herd you like window functions
SQL injection is still probably the most prevalent vulnerability.
There’s no right answer here, whatever works for the programmers and gets the job done. If you need to ’sell’ the program to management and users, you could start off by drawing wireframes and showing them to people to get feedback as early as possible. This way you can delay the tech choices until you have a clear view of the requirements. 
Yeah i am throwing the results into a table and selecting on the ntile column.
it worked for me. 1 and 2 had 545 rows 2 and 4 had 544 records. Close enough for what i need it to do.
What is the platform we’re talking about? Web UI or desktop application or mobile app?
Do you think it would be a crapshoot, if I only use it mash the data I need and pull a extract?
Thank you
Thank you!
That's a very egocentric approach and assumes that a company can only make a Type II error (where they fail to recognize your genius through a PowerPoint slide deck). Your portfolio only demonstrates that you (or someone) knows things that would sound good to a manager and your references may well be someone you paid to say nice things about you. 
It doesn't really depend on the use case, it depends on the proper formatting of the tables you're using it on. Without understanding the relationships and the grain of data in each table, you run the risk of duplicating data or joining completely unrelated (and unfortunately named) columns. Again, you're probably fine, but it's just not something I would rely upon as a rule.
11:59 pm PDT happens when this comment is 10 hours and 47 minutes old. You can find the live countdown here: https://countle.com/rNw_Ny6hH --- I'm a bot, if you want to send feedback, please comment below or send a PM.
or parameterized queries
I like comments in line with SQL. If I'm doing something odd or out of the ordinary, i'll put as much into plain English as possible. Most of SQL is self-evident...joins, selects, wheres are all self-evident. Using an NTILE function or DENSE\_RANK() or something out of the ordinary are things I like to comment and explain what I'm accomplishing. At the end of the day, the best test here is to pull a coworker or someone who understands SQL aside and ask them if they have any questions or points of clarification on your code. 
We use Git, Jenkins and DbMaintain to centralize and historize all db modifications. It works pretty well.
By 'SQL Code', do you mean just random snippets and queries, which are used regularly Or SQL which is embedded in applications, or your actual stored procedures (which are already stored in the database, and so in backups too)?
**Always** use prepared statements to bind values to placeholders in a query; don't try to build a sql statement string at runtime with those values in it. http://bobby-tables.com/ for examples.
Use liquibase or flyway for database migrations. Keep your changesets in SQL files, with comments. Use Git for source control Some database, like PostgreSQL, support comments on items like tables, columns You could also use [Pentaho Metadata Editor](https://help.pentaho.com/Documentation/8.0/Products/Metadata_Editor) 
Depends, `join on true` would match join all the rows
Sublime with GIT for all of our code snippets.
You can't tell what requirement the self-explanatory SQL is supposed to implement just by reading the SQL. I like to include ticket numbers, too. 
After reading the comments, I see that this continues to be a thing. Please, please, please comment your code. And when you do, here's the secret: You're not just documenting the programming logic. You're documenting the business logic. Yes, any junior dev should know \*what\* is returned when I join table A to table B. But what needs to be documented is \*why\* I'm joining table A to table B (to table C, and SELECTING distinct, and using DENSE\_RANK() and blah blah blah) in terms of the business problem I'm solving. Hell, \*I'll\* probably be glad I made some notes when I come back to this code next week/month/year. Good SMEs are hard to find - when you get one, write down what they tell you. In your code.
Just have to include the table aliases in your ON clause: FROM data as d JOIN extra as e ON d.tag = e.tag WHERE a.data= 'type';
 FROM data as d JOIN extra as e ON d.tag = e.tag WHERE a.data= 'type'; a.data='type'??? You have two tables: d and e; no table with an alias of `a`. Also, data is a table name, not a column name. Same with 'type': type is a column name, not a literal value. The result you say you want: returning one row from the first table and two rows from the second table where the columns are different is not a valid SQL query.
What im trying to illustrate is that " 1 hard lorem url green" is appearing twice. I only want it once.
We do exactly what you describe with GitLab. Separate SQL files are housed in a few different repos separated based on function/business area with a standardized naming structure. We don't use the git functionality from SQL Developer at all... I think I looked at it one time and hated it. I do commits from GIT Bash or GitExtensions (my favorite GIT GUI) and view diffs in GitLab/GitExtensions or vscode.
I would guess that order detail probably also has a unique primary key on it which is getting rid of your uniqueness instead of selecting star only choose the columns you actually need to convey the information
I see what you are saying now. I was speaking of statistical quartile, not calendar quarter. I believe quartiles are what OP wanted.
It's the "why" that's not always self-explanatory. 
Set up the same in your dev environment but set you retention to only a day or so. Otherwise consider running your development dbs in SIMPLE recovery model if they aren’t at part of an AAG. That way you don’t have to really worry about log growth. 
Don't select * since it means you are selecting a.OrderID and also b.OrderID. Instead select he exact fields you want... SELECT a.OrderID, a.Field2, a.Field3, b.Field2, b.Field7 FROM Orders a, [Order Details] b WHERE a.OrderID = b.OrderID Also you are basically doing an INNER JOIN here, so you may aswell do it properly... SELECT a.OrderID, a.Field2, a.Field3, b.Field2, b.Field7 FROM Orders a INNER JOIN [Order Details] b ON (a.OrderID = b.OrderID) Finally don't put spaces in your table names, or field names. Nobody wants this ever. 
Here's what I would do: At the beginning of the interview I would just let them know that I brought my laptop with a few examples of x,y,z if they are interested. When you get to those topics, they may ask to see examples, but don't expect this, expect that like any part of the interview you are going to actively work in your experience. At some point they'll probably ask about your experience with one of the topics you've dealt with in your database. At this point, I would say, "as a matter of fact, I do have such and such experience. I did that in a MySQL database; would you be interested in seeing an example?" They will probably say yes (if they say no, don't push it, but they probably won't do this), at this point open your laptop that already has the application pulled up, is connected to your hotspot if need be, etc (the key here is to not waste time but have a smooth, quick transition), and be able to speak more to your example. You may touch on some aspects of how prior to the example you didn't have any experience doing x and how you researched the problem and taught yourself, or maybe how when you first worked on the query you ran into such and such a problem and how you overcame the problem. The key here is to have your examples 1) showcase your abilities/experience, 2) show that you take initiative and figure stuff like this out for yourself (very important in this line of work) and 3) open up the conversation a bit more. You want to avoid just showing the queries, and instead use them to talk about what you learned, how you learned it, challenges you overcame, etc. Have a general idea of talking points for each query/example. Who cares if the database is childish. I bet it is better than you are giving yourself credit for. Maybe I'm wrong, but it sounds like this would be an entry level position and/or your first position using SQL; the interviewer isn't expecting a complex and revolutionary database. The examples I showed a few years back are a drop in the bucket of what I can do now, but you've got to start somewhere. Employers love engaged employees who take initiative, use your portfolio to demonstrate this about you. 
Oh lord, that's definitely my bad. I mostly work on financial analysis so I jumped the gun on this one. 
At minimum, documentation should meet the needs of: Industry requirements (e.g. HIPAA) Company policy and requirements of management or the "shop-style" of your teammates. SQL itself can be straightforward, but as u/[DevelInTheDetails](https://www.reddit.com/user/DevelInTheDetails) and u/level 1[Indigo9Emerald](https://www.reddit.com/user/Indigo9Emerald) said,documentation and documenting the business logic helps others understand how and why choices were made. For myself, I use documentation as another way to plan and engage the project. I like to make daily journals with searchable keywords, notebooks evaluating performance tests and results supporting a choice, and keep test sets and unit tests. Others may have no use for such granular details, but I think its worthwhile to attempt to find ways to make large amounts of information presentable and digestible.
Thanks, fixed. I'm just tired, my attempt to illustrate the problem failed. I've linked a clip above that exactly explains my problem. Unfortunately, they do not talk in the clip how to solve this.
It's still not making sense because you have two type columns in the data table and the formatting is off, but this is what your query is giving you, right? id | type | text | link | title | category | price | icon ---|---|----|----|----|----|----|---- 1 | hard | lorem | url | h1 | x | 5 | smile 1 | hard | lorem | url | h2 | x | 10 | cry And you want 1 row instead of these two? What your video describes is not a problem, it's how joins work. You are joining on the type column, so every row in data that matches the type in the update table will be shown. If there are two rows in update that have the same type as one row in data, then that row will get duplicated to display the data for each of the rows in update. 
&gt;If a developer can't decipher what code is doing, I question why they are modifying code at all. Comments exist to illustrate *why* code does what it does, and may be the only clue to the designer (or legacy maintenance coder)'s purpose in what they did. "Comments are waste of space" is almost as precious as that other chestnut of the profession, "I didn't understand this code so I am announcing it was poorly written, and will write it from scratch in my own style, which is understandable and logically superior."
I have just started learning SQL. I've been using the w3schools website, but it seems the book is much more thorough. I've just been learning about SQL logical processes. 
It isn't egocentric at all. It's how to interview well. There are tons of jobs in this field. Tons of them. There aren't enough people to fill them. Hell, my first SQL job paid 60k/year and I got it without having ever used SQL, and was 100% up front with them about it. I would have literally failed a `select * from table` and yet somehow was not only able to do that job well, get promoted, find other SQL jobs, You don't have to listen to my advice. You can say it's egocentric, or arrogant, but modern studies in how to interview have consistently shown that this kind of behavior will get you more job offers. &gt;k). Your portfolio only demonstrates that you (or someone) knows things that would sound good to a manager A basic test won't demonstrate anything, because some people are bad at taking tests, and other people are great at taking tests but can't do a god damn thing in the real world. You won't know until you hire someone, and that's the bottom line. However, a portfolio is far more convincing. An interviewer has never, and I mean never asked me to explain something in SQL, or asked me about a SQL concept after I've directed them to my portfolio. I point to an example, talk about the SQL that the specific example leverages, and then move on to my next question. An interview is **not** your chance to impress them. You've already done that with your resume. It is their chance to impress you, and your chance to see if it is a place you want to work, and a job that you can do. For example: Q: Can you explain what a stored procedure is? A: Have you see my portfolio? No? Here's a copy. So here is an example of a project that I led that used 30 stored procedures and this is exactly what it does, why it was created, and what benefit it brought to the company (e.g. it doubled sales.) Does this position involve writing a lot of stored procedures? Q: Can you explain how you validate and test data? A: So here is another example in my portfolio that required doing a ton of that because it was working with data from over 100 different clients that used multiple VMS systems all of which had their own custom ETL process. One of my favorite things is how you can use SQL to "ask questions" of a dataset to determine if there are duplicates, what the distributions look like, etc. Can you tell me what the quality of your data is like, and how rigorous your BI architecture is? You see how I answered their questions without actually answering the question in technical terms? You see how I then turned that question around to find out more about the job itself and what the duties are? I'm answering it with concrete examples and then asking them specific questions which indicate I know the technical answer, but don't have the time or inclination to get into the weeds during a job interview. What I want to really know is whether or not your database is a clown show, or if you have a proper BI team that has already done that job. Does the employer even know what those things are? Do they laugh at the question and say, "oh man, our data is fucked?" The interview is about me. I don't give a fuck if you think I'm egocentric, arrogant, or whether I know what an INNER JOIN as opposed to a LEFT JOIN. I want to know if this is a good place to work, and duh, of course I know those things.
I can't recall code comments every really being helpful, but maybe I'm missing something. If you code review this function, do the comments help you? CREATE FUNCTION [Audit].[udf_GetExecutionLogRoot] ( @AuditKey INT ) RETURNS TABLE AS RETURN ( -- derive results using a CTE WITH RootNode AS ( -- select the anchor node first SELECT AuditKey, ParentAuditKey FROM [Audit].PackageExecutionLog WHERE AuditKey = @AuditKey UNION ALL -- select the parent node SELECT node.AuditKey, node.ParentAuditKey FROM [Audit].PackageExecutionLog node INNER JOIN RootNode leaf ON node.AuditKey = leaf.ParentAuditKey ), CurrentAuditKey AS ( SELECT @AuditKey AS AuditKey ) -- get the parent node or the current node if there is no parent node SELECT ISNULL(rn.AuditKey, cak.AuditKey) AS AuditKey FROM RootNode rn FULL JOIN CurrentAuditKey cak ON rn.AuditKey = cak.AuditKey WHERE ParentAuditKey = -1 );
You need to load the data into a dataset using a language like c#. From there you can build a simple page. Here is how to he the data from SQL into a dataset. https://stackoverflow.com/questions/6584817/direct-method-from-sql-command-text-to-dataset 
W3Schools is good in my opinion. It covers wide ranges like SQL Sever, MySQL and Oracle. It is a great resource. Though, with this book I tried to be more focused and specific to SQL Server, since the syntax and some aspects of it differ from other databases platforms. I’m glad you found it to be more thorough though! Regarding SQL logical processes, are you referring to the logical querying process?
I don't really understand your question. SQL is just a language - there are things you can do (and it sounds like you already know more about them than I do) to protect your databases and infrastructure, but how do you put a firewall around a language?
Well that depends, how many rows were in the table in between those dates? 
3 months in that date range, assuming there is a value for at least each month. 
Gotta use distinct 
I still think the MS walk through is Good depending on what your trying to accomplish: https://msdn.microsoft.com/en-us/library/tw738475.aspx
You can dynamically add partitions just by inserting into the table. As long as the table is defined with the partitions. Check out the Dynamic Partition Inserts from their Confluence page: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML#LanguageManualDML-DynamicPartitionInserts.
er, yah for executing queries. jut got to the part with LIKE and IN &amp;#x200B; I am studying for a potential job and they said they are starting to use SQL. I probably should ask what specific one they use. But thankfully so far they seem similar; W3Schools lists all the differences if there is any.
Brother we can neither use aggregate function in group by clause nor the alias name . So this query is seems to be going nowhere.
Looks like 3 to me as well
&gt; SQL injections have to be handled by the application rather than by the database Disagree. Denying direct table access to the user and creating stored procedures with correctly typed parameters will force the app code guys to adhere to secure methods. In addition dynamic sql can be vulnerable. I've seen dynamic sql injections in ETL code, a would be hacker would never see any error as any syntax failure was logged to the ssis log a day after rather than the UI so it would be blind. Got to love those Irish surnames... O'connell etc. 
Generally you would have your db server inaccessible from the Internet and have a IP whitelist for app servers to connect. You do not want some random IP be able to even try login. 
What you're describing sounds a whole lot like sub-queries, most of the time they're in parenthesis. There's a wealth of information about them out there.
Try this subquery approach: SELECT d.id, d.type, d.text, e.title, e.category, e.price, e.icon FROM (SELECT id, type, text, link FROM data WHERE text = 'lorem') as d JOIN update as e ON d.type = e.type;
Do you know your ABCs? If you do, you can do something like: select user , max(case when grp = 'A' then role else null end) as A , max(case when grp = 'B' then role else null end) as B , max(case when grp = 'C' then role else null end) as C from reddit_9qwrmj group by user [Working example](https://www.db-fiddle.com/f/68WZNwwhqYF2RLxskFDhLN/0). 
Is there a semi-colon (;) inbetween them? This starts a new query. Otherwise it will be a sub query where LEFT JOIN table is the same thing as LEFT JOIN (SELECT * FROM table) this is really useful if you want to join to just a subset or an aggregate of a larger table, like this: LEFT JOIN (SELECT [AccountID], SUM([Amount]) FROM table GROUP BY [AccountID])
Sort of. [Here's a working example supporting up to 5 different groups](https://www.db-fiddle.com/f/qcNTbqhTaQ4ampb8nCJUjY/1). In our case groups 3 and 4 will always be empty because there are only 3 distinct values of grp - A, B &amp; C. Furthermore we can't name these columns A, B, C because we don't know their names. It's impossible to name these columns appropriately, and have the same number of columns as you have distinct values in grp field without using dynamic SQL.
SQL is an ansi standard language, but each vendor will have minor variations. For example different flavors might use different string delimiters, have slightly different functions available, different transaction and flow control syntax, etc. Some vendors have things not part of the ANSI standard like XML or JSON parsing, temporal tables, etc. About 99% of what you need to learn to get started will be almost identical across all of the variations of SQL so don't worry too much about which to start on. If you're working use whatever your company uses. Otherwise, postgresql is open source and highly ANSI compliant, MS SQL has free developer options, relatively simple setup, good tools and a good ecosystem of cloud and BI toys, MySQL gets a lot of use in Web app development (but has lagged behind implementing the full ANSI standard). 
Oracle is not at all the same as MySQL.
For learning, it's barely going to matter: the syntax for retrieving data is going to be near identical in every one. 
Bad comments are like bad code. Actually that strengthens the argument, since if a comment is just parroting what the code is doing, you can be sure the author wasn't thinking holistically about program logic.
Only an error, you've missed a comma in your select list. 
Some variants do allow you to reference aliases in the group by clause.
Your query is using some form of implicit join, by the looks of it. There's no clue to us from the query how the two tables are linked. To be standards compliant, readable, and maintainable, you should explicitly join the two tables: SELECT Ansvarlig.Fornavn, Ansvarlig.Etternavn, Utgivelse.ISBN FROM Utgivelse u join Ansvarlig a on a.id = u.author_id /* or whatever column */ To get the answer you want, take the above with the join from authors to books, and group by author's ID - if ID doesn't exist then group by firstname, lastname (or lastname, firstname). Because I'm lazy, I'm just going to use table names author and book. It'll look something like this: ``` select a.firstname, a.surname, count(*) from author a join book b on a.id = b.author_id group by a.id ```
I honestly haven't tested, but even with delimiters I'm not sure these dates would be successfully converted. 
What languages are involved (besides SQL of course)? There are many tutorials but they are going to be specific to each technology.
Note that SQL is used by several data science frameworks to abstract data access and extraction. For example you could open a Csv file using python 's pandas library and use a SQL query to get some rows. Try to do this * study SQL fundamentals * play a bit with sqlite to see how a relational database works * get some python pandas online course, there you should see some Sql applied on data science 
cheers! yeah i forgot to mention that both tables have AnsvarligID relating authors in one and books in the other
I'm offering you the opportunity to convince me that comments can be good. What does the code do, and what would good comments be?
You are correct - those comments are useless, and demonstrate a misunderstanding of their purpose. Start at the top, right under the CREATE FUNCTION line. Tell me what @AuditKey is (in business language), describe its domain a little bit (again, in business language), and where it might be passed in from. Tell me what the rows of the returned table represent (in business language). Give me a short description (in business language) of the function that's occurring. I have no idea what the code does. To these comments, it returns a table when handed an integer. To my comments, it (perhaps) helps authenticate an ERP user to review audit logs. I really don't know. Perhaps some comments would help.
My only comment is the author clearly hates declaring table variables.
/u/fauxmosexual gave a great answer. Here's a recommendation. Start here: https://www.w3schools.com/sql/. This will teach the basics of SQL in an easy to follow tutorial. It will also teach you want SQL is and a little about various databases. From there, you can expand your understanding and tool set depending on your needs. 
The DB I'm trying to connect to is not named DBNAME... That was just a placeholder
The first step in troubleshooting an aggregate query is to take out the aggregates and see if the individual data rows are what you actually want to aggregate. The second (and third, in your case) step is to add each aggregate in separately and see if they each return what you want/expect. I suspect that your sum aggregate and your count (distinct) aggregate aren't playing nicely together. I also see that line\_total is both being SUMmed and GROUPed BY. I'm not sure of your skill level, your DB platform, or what you're trying to accomplish, but this isn't typically the way it's done.
Cool, just making sure a detail didn't get missed in a copy/paste.
/u/gopalrocks you can definitely do this in postgres as fauxmo says
It looks to me like you are trying to aggregate two numbers based on two different group bys. Based on your examples, it appears you want to see the line\_count based on return\_year + data source while you want to see count\_distinct\_emails based solely on return\_year. In your result set, you are then only interested in displaying the return\_year = 2017 and datasource = 'efile'. Aggregate each number in two separate queries and then join those two queries together. I'm using temp tables below but you could use sub selects or CTEs. select return_year ,datasource ,sum(line_total) as line_total into #temp_a from reports group by return_year,datasource select return_year ,count(distinct email_combined) as count_distinct_emails into #temp_b from reports group by return_year select a.return_year ,a.datasource ,a.line_total ,b.count_distinct_emails from #temp_a a INNER JOIN #temp_b b on a.return_year = b.return_year where a.return_year = 2017 and a.datasource = 'eFile' &amp;#x200B;
You could maybe left join yesterday's values to today's and the use a coalesce or isnull function referencing today's table and the dates. Would become quite a chore as more dates were added though. 
Ah okay great! Good luck to you in your potential job offer! I would definitely use this subreddit if you have any interview questions, as people on here are really knowledgeable. I think by asking them which database product they use, that will really help you hone in on where you can really invest your time. That’s good that W3Schools lists the differences. I hadn’t actually seen that but that’s good to know! Keep us posted with your job offer!
I wouldn't try to do it in a single query or within a single table. Whatever process you use to populate the day's data, have that data inserted into a separate table, then compare today's data to the previous day's data and find the records which exist in the previous data but not today, and add those to your data set. Then finally, insert your complete set of today's data into your reporting table.
`line_total` being grouped was a mistake. It should not have been there. Thanks Bud!!!
I suggest Microsoft SQL Server (T-SQL). The interface SQL Server is called SSMS (SQL Server Management Studio) and the Developer/Testing license is free. I've not used Oracle before but everyone I know that has says they don't like it. The amount of free resources online for learning and troubleshooting is huge. Which is the measurement I think you should use to make this decision. I just searched t-sql Machine Learning and this was the first result. https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/machine-learning-services-tutorials?view=sql-server-2017
This applies to regular tables, where partitions are dynamically created as data is inserted. However in the case of external tables, no data is \_inserted\_ per se, so dynamic partitioning wouldn't work. Thanks though, we ended up scripting things :/
Awesome man, this is fabulous stuff. Thanks.
I am actually very new to SQL as well and currently learning it so please excuse my ignorance in this topic. I am learning the basic fundamentals (Aggregate functions, creating tables and databases) using the postgresql database with pgadmin as the graphical interface. I find this to be very user-friendly and easy to understand especially from someone with little to no programming experience. I believe this would be a great first place to start and also learning some of the underlying concepts along the way that can help expand your knowledge of databases and how they work and for what purposes. Keep things very simple. You can then use kaggle or something to find datasets to build your own databases in PostgreSQL and come up with some challenging business questions where you’ll query data and then use whatever program (excel, python, R, SPSS, on and on) to do most of the advance analyses. I’m not a data scientist, but from what I am learning, it is more important to ask and (know how to) answer the relevant questions more than whatever methods or fancy software you use. 
Is this all you're looking to do? select studenttests.name ,studenttests.testdate ,dateadd(month,3,studenttests.testdate) as retestdate from studenttests ;
Group By Week and a comma in select
yes and insert the retestdate back into the table in the testdate column for each student. the students have different testdates
unless I'm missing something: update studenttests set studenttests.retestdate = dateadd(month,3,studenttests.testdate) from studenttests ;
Are you sure you installed the 64-bit version of the ODBC driver (I'm assuming you are using 64-bit Windows)? If you haven't already done so, I would focus on taking full backups of the SQL Server databases (including system ones), before worrying about getting it into Postgres
Not sure how well this will perform...... SELECT e1.firstname, e1.lastname, m.manager, e2.firstname, e2.lastname, m.manager2, e3.firstname, e3.lastname, m.manager3 FROM managers m JOIN employees e1 ON m.manager = e1.emplpyeenum JOIN employees e2 ON m.manager2 = e2.emplpyeenum JOIN employees e3 ON m.manager3 = e3.emplpyeenum
Not from a DBA...
Not from a DBA...
Not from a DBA...
is e1, e2 and e3 and employees e1 supposed to be table 1?
Do you know that rows should be returned? Try: `SELECT` `Start,` `sum(count_ID)` `FROM reports` `GROUP BY Start` `ORDER BY 2 desc` and see if any of the rows **have** a sum(Count\_ID) of 8000.
Do you know that rows should be returned? Try: `SELECT` `Start,` `sum(count_ID)` `FROM reports` `GROUP BY Start` `ORDER BY 2 desc` and see if any of the rows **have** a sum(Count\_ID) of 8000.
Oracle is an absolutely top notch database. Very complex to support and extremely expensive to use commercially. And Oracle are dicks. But there is a free Oracle Express install and the Oracle SQL Developer free tool is reasonably good.
yes, sorry :)
It's a source control application, similar to Subversion or Team Foundation Version Control Has a command line interface, but there are library and GUI tools too It is very efficient (fast) *GitHub* is a website that lets you host and share Git repositories
It's a source control application, similar to Subversion or Team Foundation Version Control Has a command line interface, but there are library and GUI tools too It is very efficient (fast) *GitHub* is a website that lets you host and share Git repositories
It's a source control application, similar to Subversion or Team Foundation Version Control Has a command line interface, but there are library and GUI tools too It is very efficient (fast) *GitHub* is a website that lets you host and share Git repositories
That should work as written, so I'd confirm there is a condition where the `sum(count_id)` is 8000? select Start, sum(count_id) from reports group by Start having sum(count_ID) &gt;= 7900 and sum(count_id) &lt;= 8100
It returns a blank `Start` and `sum(count_ID)`. Yes 8000.
I would run this instead and see what your highest sum is: SELECT Start, SUM(count_ID) FROM reports GROUP BY Start ORDER BY SUM(count_ID) DESC It sounds like you don't have any between 7900 and 8100.
No matter what you choose, you are a loser. There is no way to win in this game. Your best off just studying SQL, Structured Query Language. You will always have to learn some variation. 
Oracle is an absolutely top notch database. Very complex to support and extremely expensive to use commercially. And Oracle are dicks. But there is a free Oracle Express install and the Oracle SQL Developer free tool is reasonably good.
SELECT any women FROM dating field INNER JOIN.... You get the picture. 
Make it a left join because I don't wanna miss anything.
SELECT dat_ass FROM singles WHERE gender = 'female' OR @beers &gt; 20
Try this: Select isnull(q2.account, q1.account) account, Isnull(q2.name, q1.name) name, Isnull(q2.address, q1.address) address From query1 q1 full outer join query2 q2 on q1.account = q2.account This will force to use q2 first if there is a matching row, otherwise it will default to q1
lol
Got an email about this today. Looks pretty promising to me! I assume there are some other DigitalOcean users on here that might be interested in a managed scalable DB solution. PostgreSQL support coming first, looks like they're planning MySQL as well later on. Seems like it'll include some nice functionality like clustering, daily backups, and horizontal scaling. I'll probably try this out soon. I don't know about you guys, but I'm a dev, not an ops/sysadmin. I don't particularly enjoy managing my own database installs. It'll be nice having another managed DB option without having to go outside DO.
wait.. so after 20 beers you're into dudes? lol
ERDs suck.
The only reason I became a DBA is because I was told there would be a TON of ladies. 
logic checks out!
Did you try putting Where (Max(Price)-Min(Price)) = 0 ? 
Yes I just did and received this error: https://gyazo.com/a6aefe3857a4a4c397e9bb14879bed9d 
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/a6aefe3857a4a4c397e9bb14879bed9d.png Imgur mirror: https://i.imgur.com/tRWf2rW.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
Just use this. Should be easier and will accomplish the same thing. WHERE MIN(Price) = MAX(Price)
Where do the matching_date and previous_date come from?
Ack. I meant to use HAVING. So, keep your group by and below it type: HAVING MIN (Price) = MAX (Price)
How do I make it take the previous value which isn’t null, which maybe be a few days ago
bitches love the way I coalesce
Do you have a particular Data Analytics project that you're passionate about? My suggestion would be to start a portfolio project using some publicly available data and begin putting your Data Analytics principles to good use. Find some data sets that relate to the questions you're trying to answer, or if you don't know of any particular projects you might check out and try [Kaggle](https://www.kaggle.com) for some ideas or challenges. Practice obtaining the data in a programmatic way (via an API, or pulling from flat files/CSVs) and importing the data into a database while scrubbing it (your ETL development phase). Learn a language like R or python to then work with the data using analytical/statistical modeling/concepts and come to a conclusion as to what the data represents based on the questions you've asked. Then work with a data visualization tool like D3.js, Tableau, Power BI, etc. to show graphical models/reports of your data. 
Unfortunately, this is out of my domain when it comes to data analytics (not my area of expertise). You may want to hit up /r/datascience for specific use cases of python in the data analytics world - I just know it's used heavily in that sector. I, however, would only stick to resources that are written for python 3 - a nice foundational course on python is [Learn Python the Hard Way](https://learnpythonthehardway.org)
Here's a sneak peek of /r/datascience using the [top posts](https://np.reddit.com/r/datascience/top/?sort=top&amp;t=year) of the year! \#1: [perfect answer 😎](https://i.redd.it/2yc30ije9ol11.jpg) | [188 comments](https://np.reddit.com/r/datascience/comments/9f18t6/perfect_answer/) \#2: [Data science recruiters](https://i.redd.it/xp9lqug9o8111.jpg) | [49 comments](https://np.reddit.com/r/datascience/comments/8nl2ps/data_science_recruiters/) \#3: [Data Scientist: \'dādə ˈsīən(t)əst\ (n.)](https://i.redd.it/ov8fwgw4hnw01.png) | [46 comments](https://np.reddit.com/r/datascience/comments/8hxnk9/data_scientist_dādə_ˈsīəntəst_n/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
You're going to destroy somebody's database. 
There is. You'll frequently get calls, such as "I accidentally deleted the entire customer registry. Can you pull up a backup from *just* before I did that? I can't lose the changes I made before".
"No locks or locks? I'll leave it up to you Daddy ;-)" -My gf, if she knew SQL ( probably )
One of the most important things in SQL is grammar. Another is punctuation. 
"Well it passed the unit tests, so in a way this is your fault."
Build a portfolio. Also learn some mainstream SQL practices. PostGres is good to know but not used a lot in the corporate world. Learning T-SQL wouldn't be a bad thing.
I should add, the common name for this type of query is 'Top N per group' and there are multiple approaches - it's worth googling to see the advantages/disadvantages of each.
Well ladies tend to be into the whole being employed thing.
Wow, I just found his thread in /r/sysadmin asking the same question and it's even more cringeworthy.
Here's a sneak peek of /r/sysadmin using the [top posts](https://np.reddit.com/r/sysadmin/top/?sort=top&amp;t=year) of the year! \#1: [Net Neutrality, let's do our part. Who knows this shit better than us?](https://np.reddit.com/r/sysadmin/comments/7etxx7/net_neutrality_lets_do_our_part_who_knows_this/) \#2: [This is why you should always lock your computer before you leave your desk.](https://np.reddit.com/r/sysadmin/comments/8idk0m/this_is_why_you_should_always_lock_your_computer/) \#3: [After 6 months of warning users, we finally did it. Tonight, I denied 2,400 Windows 7 computers from log on.](https://np.reddit.com/r/sysadmin/comments/7uf1yl/after_6_months_of_warning_users_we_finally_did_it/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
What's the use case? Why do you need the results in this form? Will you ever have more than (or less than) 3 employees? 
What's your query?
shows who is reporting to who like a hierarchy. there are more coloumns so it goes from level 1 to level 11 but unfortunately the table doesn't show employee names just their numbers 
SELECT b.DateHour ,c.SourceDwId ,d.UserHierarchiesDwId ,e.RelationshipsDwId ,a.TableName ,a.TableId ,a.RowsInTable ,a.TableKey FROM a JOIN b ON a.EventDateHour = b.DateHour JOIN c ON a.ServerName = c.ServerName AND a.DatabaseName = c.DatabaseName JOIN d ON a.TableKey = d.TableKey JOIN e ON a.TableKey = e.TableKey; Where TableKey and ColumnKeys are unique and TablesDwId is a surrogate key
`SELECT b.DateHour` `,c.SourceDwId` `,d.UserHierarchiesDwId` `,e.RelationshipsDwId` `,a.TableName` `,a.TableId` `,a.RowsInTable` `,a.TableKey` `FROM a` `JOIN b` `ON a.EventDateHour = b.DateHour` `JOIN c` `ON a.ServerName = c.ServerName` `AND a.DatabaseName = c.DatabaseName` `JOIN d` `ON a.TableKey = d.TableKey` `JOIN e` `ON a.TableKey = e.TableKey;` &amp;#x200B; &amp;#x200B; Where TableKey and ColumnKeys are unique and TablesDwId is a surrogate key
I would suggest that most entry level IT jobs require at least \~1 year of technical prowess in that area. Many people start out in a help desk type role that teaches you basic skills to advance from there. The IT jobs that are entry and require a little experience is because they want you to be able to learn and have some sort of skills you can use to maneuver and learn their environment with. This does not need to be job experience, it can be personal or schooling. &amp;#x200B; All that said, most SQL jobs are not considered entry level and the ones that are entry level are usually an analyst or SQL developer role. The developer role will require some solid education to hold your own and the analyst will require additional skills and experience. &amp;#x200B; That said, I'd recommend to use this list to gauge what sort of SQL skills you should learn next. I'd recommend to get through most of intermediate since you will be self taught with no experience. &amp;#x200B; [https://softwareengineering.stackexchange.com/a/181657/256008](https://softwareengineering.stackexchange.com/a/181657/256008)
Your data in the screenshots is pre-aggregated data stored in the table?
Are you not?
SELECT col FROM tbl GROUP BY agr HAVING CUNT(\*) &gt;= 1
Yes. The data is gained from already pre-aggregated source.
Two words: Cartesian Join
there's a joke with an inner join around that
there's a better joke with a CROSS JOIN around it
As everyone else has stated, it sounds a lot like sub-queries. As I'm very much a beginner myself, I've used sub-queries quite a bit in my work. I mainly use them in dynamic SQL or when I'm needing results of something simple that I don't need to create a table for. [This](https://stackoverflow.com/questions/4799820/when-to-use-sql-sub-queries-versus-a-standard-join#4799847) should be a good starting point to beginning to understand sub-queries and how/why to use them!
Insert into table where "Not exists". Search Not Exists functionality
This should roughly do it. First you have to figure out all the combinations that need to be filled in (the cross join of dates and stocks), then use a subquery to find the most recent date for what's left. If you have things indexed well should perform decently. Note that it's not going to fill in for dates where *nothing* was traded (a weekend). So if you need full date coverage, you'll need to generate a different table with all the possible dates and use that for the cross join instead. CREATE TABLE #Stock_Data ( date date, stock_id int, Close_Price money ) SELECT t1.date, t2.stock_id, (SELECT top 1 Close_Price FROM #stock_Data last where last.stock_id = t2.stock_id and last.date &lt;= t1.date ORDER BY last.date DESC)) as Close_Price FROM (SELECT DISTINCT date FROM #Stock_Data) t1 CROSS JOIN (SELECT DISTINCT Stock_id FROM #Stock_Data) t2
No. It's already computing the COUNT(*) for each row, the HAVING will just filter the result using what it just computed.
For OP, and to expand on this: You can check out [this article](https://www.itprotoday.com/microsoft-sql-server/logical-query-processing-part-7-group-and-having-0) that explains Logical Query Processing for the GROUP BY and HAVING phases. Essentially, your HAVING phase is processed from the resulting set that's the output of the GROUP BY phase - so it's not recalculating the COUNT function in your HAVING, merely using the COUNT that's returned from the GROUP BY phase. 
Just learned a better way to do this. After a few weeks more of study, I finally got it. Just had to learn about dense\_rank, over() and partition. Now I'm able to select any rank of salary, like the 2nd highest in each dept. I'm just updating this in the event someone gets to the post looking for the answer I was. &amp;#x200B; `select e.*, d.DEPARTMENT_NAME` `from` `(select EMPLOYEE_ID, FIRST_NAME, LAST_NAME, DENSE_RANK () over (PARTITION BY` `DEPARTMENT_ID ORDER BY SALARY, HIRE_DATE DESC nulls last) Salary_Rank, SALARY,` `DEPARTMENT_ID` `from EMPLOYEES` `) e, DEPARTMENTS d` `where e.DEPARTMENT_ID = d.DEPARTMENT_ID` `and e.Salary_Rank = 2;`
and now in SQL server too
I'll take the one on the left with curly hair. You guys can fight over the rest.
You have a number of options. For starters, you could remove the filter from the view, and add the where clause outside the view. The query optimizer is smart enough that this should produce the same execution plan. You can create a new table that has the 'start_date' and 'end_date', and use that as a subquery. I don't suggest doing this in a real, production system. You can also create a [table valued function](https://www.codeproject.com/Articles/1201868/Table-Valued-Functions-in-SQL-Server), which requires you to pass in the 2 dates as parameters.
Yep. HAVING is what is needed when you want to filter aggregates.
Yes
FYI, when you are grouping results you don't need the DISTINCT as its being taken care of by the grouping.
Good to know, thank you very much!
Are you aware of COALESCE? If you go COALESCE(expr1, expr2, expr3, expr4) the function returns the first non-null expression. Your case statement is off, there's no way to reach the second clause. Anything that meets the criteria of the second clause has already been captured by the first clause. Start by replacing it with a coalesce. BUT if everything is hitting the third clause anyway then that's not actually the problem. Have you tested that SP.NewDep and SSP.NewDep are actually null where the value is missing, rather than something like an empty string or placeholder value?
I know this sounds simple but since you using NULL in your logic and only your else is getting picked up, have you verified that the column you are checking is NULL or is it blank? You may have to change your IS NULL to = '' if that is the case.
u/titletoimagebot
[Image with added title](https://i.imgur.com/SgucJPr.png) --- summon me with /u/titletoimagebot | [feedback](https://reddit.com/message/compose/?to=TitleToImageBot&amp;subject=feedback%209r4zy9) | [source](https://github.com/gerenook/titletoimagebot)
Each Value being checked for NULL is from a Left Outer Join therefore it will always be NULL if the data is not found that's why I'm checking for those. 
Not everything is hitting the 3rd clause, but everything not hitting the 3rd clause is coming back as NULL Should have clarified that, sorry. I'll have a look at coalesce though, sounds like it might be exactly what I want.
Count your parentheses.
ISNULL THIS PLEASE
Why? Well, that's just how it works. What can you do instead? Common table expressions if supported by your DBMS, or SELECT total1 - total2 FROM ( SELECT blah+blah2+blah3 AS total1, blah4+blah5 AS total2 FROM whatever ) a
Should be pretty straight forward if you are comfortable with data frames. However this really depends on what you are aiming for. 
Im thinking your second option uses a sub query in the from statement? That worked perfect. I will also look up common table expressions and try that approach as well. Thank you!
More specifically, this is an inline subquery. It's very useful for querying your query.
my_penis FROM here UNION your_vagina FROM there
I know a little python and some R. I will say SQL was pretty easy to learn after knowing some programming. The other way around it might be a little more difficult but you'd still have a leg up over someone who knew nothing. 
Thanks for the insight. I thought it was an interesting question, they seem to go hand in hand. I've learnt scala as it was built into Apache Spark, that came quite quickly. I think R might be a decent addition to learning SQL for analytics based roles. 
Not trying to be rude, but, that's like asking "how hard is it to learn chinese if you're proficient at german?" They're sufficiently unrelated that the only thing they share is what any artificial languages will share - a knack for extreme literalism
Excellent ! Thankyou, I find the hardest part of learning is 'how to ask' - what may make sense in my head means literally babble to someone else.
I think it's really good to be interested in R, and to start learning it. I came from a heavy SQL background before I picked up R. Learning R and all of the libraries that are suddenly available to you is like a Pandora's box. It really makes you rethink a lot. If you do a lot of complicated analytics in SQL, learn R. You'll start to kick yourself with how easy it is to do certain things in R, whereas in SQL it can be *very* verbose. But it really is like learning a new language. To start, you need to a task to complete. Just today (literally) I was tasked to report on sales projections for a new product we launched. I used existing order data from our SQL server to pull order history on a related product, and got that into R using the RODBC library, and put all of the order data into a data.table (using the data.table library). Given the parameters business gave me, I created a model using the existing order data, and plugged in the conversions from the sales pipeline, and spat out a 6 month projection. That took me about 3 hours, and it was *maybe* 20 lines of code in R. Whereas doing that in SQL could quickly become hundreds of lines. It's all about the task at hand and the best tool to tackle it.
This is really insightful, thank you very much!
Don't filter the start date inside the view, filter the startdate when you select from the view. If this is too slow, then maybe you want a stored proc or load this into a table and index it well
Ah right - well a good part of your problem might be fixed simply by reversing the first and second clause in your CASE statement, but it would be easier just to let COALESCE take care of the priority.
Also look at putting the calculations in a cross join lateral. I had the same requirement recently and it worked well.
\+1, This solution was perfect-o and helped a bunch! Thankyou! 
I've done it in this direction. If you know SQL, you already think in sets, which is great for R, because it works with sets. Programmers who don't have a lot of SQL experience can get tripped up trying to force R to work row-by-row. And, it can be done, but ruins the usefulness of R. 
R would be interesting, but if you are going to further your analysis language, I believe Python os the way to go. Its much more versatile 
As others are saying depends on what you're looking for. When I'm doing data analysis I prefer just to use SQL to pull the data out. I might use an aggregation to save some R/pandas code, but as far as data manipulation and analysis go SQL is nowhere near the level of convienence as either of those. You can never go wrong with learning it though. It's an ansi standard language and relatively unchanged the last 30 years. It's not going anywhere unlike other languages that fade in and out. 
Every time you perform a log backup, it will truncate the log file to the last checkpoint. That’s what would of been missing. A normal backup won’t truncate the log file.
`EXTRACT(year FROM column_name)`
This was very insightful, double thanks.
Okay. So should I shrink the file then itself and bring that space back? Would it be safe to do that?
Yes, look up nested case statements. 
The file itself will be empty, just taking up X amount of gb on the disk. You can shrink it, but it will still grow out to the size required before the next log backup. Leave it for now after you do the log backup, and see how much space is used up till your next backup. Then give it some breathing space and set it to shrink down to that.
Triple thanks :)
I would definitely suggest creating a new table in a production environment. You wouldn't use a sub query though. Populate new variables at the start of the view.
Seattle? PM me. I can come sit with you for a couple hours, next week. Or we can meet up with your laptop and discuss. 
Fam you can't create variables in views in mssql. You can do a fair number of things to sort of emulate variables, like using a CTE, but why? Also, what process controls updating the start date? When does it update? If this is for reporting, which I have a wild hunch that it is, how do you differentiate user A populating the start date ranges, and user B populating the start date ranges at roughly the same time? Why not have the view return all results, and use a separate where clause on the results of the view to get the data you want? Again, the query engine will produce the same execution plan for the where clause being inside or outside of the view.
OP, yes try using the ALTER command it should work fine. If you're really using SELECT * instead of field names, don't forget to run sp_refreshviews
As a rule of thumb, try to avoid using COUNT(*) and instead use COUNT(1). Limit your counting to a single field. You'll get the same result but it's a negligible efficiency save. That said, if your query is underperforming I'd be more inclined to point my finger at the [name] filter. String interrogation can be costly Try using the actorid or actor.id instead.
Thanks for the tip!
It does matter what you put into your COUNT() function - for example, you may want to eliminate any duplicates that could arise when doing a COUNT - so you could do COUNT(DISTINCT &lt;column&gt;). Additionally, please know that you're not calling COUNT() twice in your query. You're calling it once, and then grouping by the count. The count in the query, however, isn't called back to the SQL engine twice. 
 SELECT * FROM OldTable EXCEPT SELECT * FROM NewTable SELECT * FROM NewTable EXCEPT SELECT * FROM OldTable 
Or: (SELECT 'OldTable' "TableName", * FROM OldTable EXCEPT SELECT 'OldTable', * FROM NewTable) UNION ALL (SELECT 'NewTable', * FROM NewTable EXCEPT SELECT 'NewTable', * FROM OldTable) 
&gt; Do you know of documentation on this? Not that I don't believe you, but I'd like to read up on that. &gt; When an SQL statement references a nonindexed view, the parser and Query Optimizer analyze the source of both the SQL statement and the view and then resolve them into a single execution plan. There is not one plan for the SQL statement and a separate plan for the view. https://docs.microsoft.com/en-us/sql/relational-databases/query-processing-architecture-guide?view=sql-server-2017 For a stored procedure, for sure I would be passing in the start and end date at the same time, too. The reason why I suggested a table valued function originally, is they are more flexible in use cases. Like, you can't use the result set of the stored procedure in other queries, or slap on another where clause onto the bottom of it. 
Here is the solution, for Teradata at least Sample When Field = A Then 100 When Field = B Then 200 End
Whoever coined the `Alter view` syntax was a visionary.
There are some case options in R, in case you hadn’t dabbled in them yet. 😁
That threw me a for loop
No, it doesn't. CREATE TABLE #TableA ( id INT PRIMARY KEY NOT NULL ); CREATE TABLE #TableB ( id INT PRIMARY KEY NOT NULL ); INSERT INTO #TableA (id) VALUES (1),(2),(3),(4),(5),(6),(7),(8),(9),(10); INSERT INTO #TableB (id) VALUES (1),(2),(3),(4),(5),(6),(7),(8),(19),(20); (SELECT '#TableA' "TableName", * FROM #TableA EXCEPT SELECT '#TableA', * FROM #TableB) UNION ALL (SELECT '#TableB', * FROM #TableB EXCEPT SELECT '#TableB', * FROM #TableA) Outputs: TableName id ------------ ----- #TableA 9 #TableA 10 #TableB 19 #TableB 20 
I misread. You are correct. For some reason I thought you had: SELECT 'OldTable' "TableName", * FROM OldTable EXCEPT SELECT 'NewTable', * FROM NewTable
ISO/IEC 9075-2:2016(E) ## 8.8 &amp;lt;null predicate&amp;gt; ### Function Specify a test for a null value.
I wrote this in response to another question about aliasing, but applicable here if you want to understand why you can't alias when in a TSQL Statement (applies to SQL Server): For SQL Server, the best way to wrap your head around this is to understand how SQL executes a select statement. We write SQL the way we write English, but that's not how the engine executes it. SQL Query Order of Operations: 1. FROM clause 2. WHERE clause 3. GROUP BY clause 4. HAVING clause 5. SELECT clause 6. ORDER BY clause So knowing that ORDER BY is the final execution step, this explains why you can use an ALIAS in the ORDER BY - because the SELECT step has created the ALIAS. It also explains why you cannot use an alias in the GROUP BY because the ALIAS hasn't been created yet. Make sense? Also, why you can use a table alias everywhere, because the FROM clause is executed first, so the alias exists for all subsequent clauses. https://www.bennadel.com/blog/70-sql-query-order-of-operations.htm
This is great info. Very logical thank you
What numeric value does null represent then? Null is a state, not a value. Irregardless of what the technical writers and non-practioners of the craft at ISO think. If null is a value, why can't I do mathematical operations on it? Because it's not a value. It's the absence of value (as was stated in the very article linked). 
Any chance you'll be willing to pass a pdf version of the book?
1) I didn't write that article. 2) It is a value. The unknown value. You don't know it. I don't know it. We don't know who knows it. But there is a value.
&gt; 2) It is a value. The unknown value. You don't know it. I don't know it. We don't know who knows it. But there is a value. And you can use any operation on it. The result will simply be... unknown. Deal with it! This is assuming that the NULL marker is representative of some value that exists - that's not always the case. The NULL marker can be representative of something that is completely invalid and would never have any value assigned to it for that particular element. This was due to ISO's implementation and merging of NULL's representation of applicable and inapplicable unknowns - veering away from Codd's representation. So no, we cannot assume that there is an unknown value either because based on SQL's implementation of NULL there could just be no application of it, period. Therefore, NULL has to be representative of a state - and not a value. Which is why you cannot perform mathematical operations on NULL but you can use it to return a boolean expression in a predicate. 
Unfortunately it's not supported for Mexico, thanks though 
Yeah, it would be phenomenal if we could get a pdf or a downloadable version of it.
I do have a PDF version. It’s a bit outdated, but not too much. I can pass it along if you would like to check it out. Just PM me a good email. 
Ah that’s no good! What are you seeing when you go to the Amazon page?
So your question boils down to this... If I use getdate() twice in a query, do they get evaluated separately (and so could be different). The answer may depend on what database platform you are using, but in my experience its quite possible they will differ, even if it does not happen frequently. Do not rely on them being identical If you want them to be identical, then evaluate it once into a variable, and then use that variable instead of calling getdate() multiple times 
You can always convert the first getdate() to a date. The time stamp will sort you out unless you’ve the ability to run the query at midnight.
That is an excellent question! I had to do some digging to find out, turns out, this would never be false! `getdate()` is non-deterministic, because 2 calls with exactly the same input can have different outputs. However, [SQL Server treats these as a **runtime constant**](https://blogs.msdn.microsoft.com/conor_cunningham_msft/2010/04/23/conor-vs-runtime-constant-functions/) and only evaluates `getdate()` once per query, and then fills in that result everywhere.
Here https://drive.google.com/file/d/1crDlIGOrJbOmCGu-kxa-W35ssPDjaWLU/view?usp=drivesdk
Not supported for UK :(
I can't get it in Germany also. :/ Also is too much to ask for Kobo reader format?
Try this please. I think this is the link for Mexico: https://www.amazon.com.mx/Learn-SQL-Practical-Database-Fundamentals-ebook/dp/B07D5S2W4Y
Hmm. Try searching for learn SQL Jacob. You should then see the same book, but free. :)
Awesome, that worked really great :3 Thank you very much, not to devour that info \*u\*
I have a 15-year background in functional (C, C++) and OO-programming (Java &amp; C#), and when I dabbled a little in R, it was super simple to learn. Just really flat and simple programming for babies really, variables, function calls, you know. It's not like SQL at all, but if you've ever programmed in C, C#, Java, Python or whatever, I'm sure it'll come natural to you.
Lol! I’m glad it worked out for you. Thank you! :)
Hey thank you for pointing that out. I think I’ll have to go over it again but Amazon’s kindle layout is not like traditional formats. When you read it, the words flow together smoothly though, yeah?
I think it points to the screen's first displayed sentence line instead of the reference page from total pages in the format
Hmm. Do you mind sending a screenshot of what you’re seeing? Also what device are you viewing the book on? Mobile phone or laptop/desktop maybe?
I don't mind, it's mutual help, I'm on Android kindle https://drive.google.com/file/d/1AhiER7BkoD1Gd4okcX07YdR8AgFaWA4V/view?usp=drivesdk
Thank you for the link. It is working now. [https://www.kobo.com/](https://www.kobo.com/) I get books from here. Or the creator offers them on their websites directly. &amp;#x200B;
Close. I swapped it for NZ as NVL doesn't exist in Access. Changed [output](https://i.imgur.com/AkhFpY8.png)
That fixes everything except the running total starting at the 2nd row, thanks.
hi. its good you went back to OLA scripts. to determine how big you should leave your LOG files, look at their size of their transaction backups. make sure you turned off the backups of the database and trans logs in the Maint Scripts. since you know you have 60gig available space wise, you're safe to shrink that log file down to the same size of the database. this is just to ball park it. set the autogrowth to the log and database to 128 or 256Meg open a explorer window and observe the LDF file size shrink the log file down by a small shrink first. try 59G and see how long. if it shrinks quickly, then try a bigger chuck. 50g. then 40, 30 10. (whatever size is your goal) if you just smash in a 50gig shrink, and your disk is shit and the dB is busy, it can take a long while. I'm going to assume your also doing corruption checks and indexing via OLA? if your system is critical, you can reduce the interval of the trans log to 30 or even 15 or even 5 minutes. depending on your confidence of restoration and testing. to address your other concern: Shrink is utility. try it on a different server first to get familiar. Shrink isn't something you would use regularly. A log file blows up in size, or you're moving to a new disk and can't go offline etc. 
Seems clever, but if the actual expiration date he's concerned about is earlier today with a time attached, it'll return the wrong result for the rest of the day. Granted, it's not ideal to store time sensitive values in a column with a name that just says date, but it wouldn't be the first time I've seen it. 
No idea why this is voted down It's similar to asking how much your CSS skills will help you in C++
I have OLA pretty much fully engaged, though I need to take personal time to really dig deep in and look. It will help me to fully understand what his scripts are doing and possibly how to modify them comfortably. Our transaction logs are being performed hourly. Since then the log file hasn't even been updated it looks. That shows me that since Im doing hourly backups of the transaction logs that the log file should be shrank and the size isn't needed. I'd prolly shrink it down considerably and the fact that it hasn't grown at all in the past three days.
you can also monitor the use of the log file. the file itself will never grow if the use of the file is smaller than your transactions per hour. make sure you set the autogrow size (for ALL databases) as the default type of percent growth is bad policy. you can also read about VLFs for you version of sql, and it'll explain how to set up your log file sizes. based on the stuff you said, you should check sizing, number of files, growith on your tempdb and other user and system dbs
To shrink a database In Object Explorer, connect to an instance of the SQL Server Database Engine, and then expand that instance. Expand Databases, and then right-click the database that you want to shrink. Point to Tasks, point to Shrink, and then click Database. auto-growth is for when it needs more disk space when use goes up. 
Awesome, thank you!!
Management yes. Tech? No
Does an Associates make that big of a difference? I could finish but would you recommend I aim for a second BA or maybe even grad school? 
I had a AA degree for 2 years in IT. All my computer stuff was self taught or training classes. I did continue my BS in IT part time. 
Gotcha, thank you for the feedback :)
The hardest part is always the HR gatekeepers. They tend to use things like degrees as easy filters. Hiring managers tend to care less. All things equal a degree makes it easier to get interviews. 
You could take lessons, read books, etc., but honestly you won't learn nearly as much until you get your hands dirty. So create a database about something you know a lot about. I give the example of baseball statistics because there is a huge wealth of knowledge out there, but you could do it for fantasy football, or census data. There are a lot of large datasets you can download, especially if you scrape your own. What I would do is design a database, design your tables, and then fill them up. Once you've done this start asking it questions. Which left handed pitcher had most strikeouts on days where it rained? How about on Thursday's but only if it rained on a Wednesday? Now go get some weather data and figure out how to join that shit together. Once you feel competent start using your C/Java skills to add on. Maybe you create a webform that submits new statistics to your database. Connect to an API. Can you automate your web scraping tools? Now start using tools like Tableau to visualize your data and make it accessible. Start looking at statistical distributions, trying to model the data, etc. Each one of those niches is a career unto itself. You wanna be a DBA? A web guy? Analytics/Modeling? Then there are web guys who work on / develop for things like Azure clouds which host sites that leverage Tableau/reporting. Security guys. It all starts by making your first database, rolling up your sleeves, and getting dirty. You will have questions and won't know the answer, so you will Google it, and teach yourself. This will be far more relevant than taking an online course, but I also recommend you explore those to some degree, and become active in communities such as this trying to answer other peoples questions. Read a question here. Generate some sample data. And answer their question. You might be doing their homework, but you're the one learning.
Eh, I have an Econ undergrad, which is an Arts degree at my school Several others in our Business Intelligence dept had Econ degrees as well Learn the shit out of the right tools, and you'll be okay
Comon back here when you get stuck and have a more specific question. You can download MS SQL Server and learn how to install it pretty quickly, then just "create database" and get started making tables. Read about concepts like normalization, but don't get too caught up, in fact you don't even need to really care about them at all for now. You'll learn more about them when you realize how stupid your database is... probably nuke it and start over. This will probably happen a few times. Don't sacrifice progress for perfection because you don't know anything, and regardless of how much you try to learn about concepts like master, "data management," you will still fuck up. These are concepts that after four years I am *really* starting to now look at and try to understand on a deeper level. I understand them primitively, and have a desire to learn more about them, but frankly they are moving in a different direction than the direction I am presently choosing to take my career. And understand that there are "limits" to many of these advanced concepts. For example, I work in the statistics/analytics side of SQL development. Normalization is an important concept for anyone, but I break it *all the time* and *by conscious choice* because I need to in order to have my work properly consumed by Tableau. A DBA isn't wrong to yell at me for this, but they can yell until they're blue in the face and it isn't going to make a world of difference. The value of my work is in the statistical accuracy side of things, e.g., a pitcher has a 4% more likely chance to win a game on a Thursday if it rained on a Wednesday. The fact that a statistical model works, and can execute/rerun quickly as new data is automatically consumed is what matters in my world. Any "space" saved by normalizing my data to remove some "labels" is minimal, a few megabytes at best. On the flip side if your job is to maintain a HUGE database of medical information, normalization is not trivial at all, and you need to really understand what you're doing. Some people spend their careers in this space and prepare data for some people who spend their careers in other spaces. Some rules and concepts are more applicable to some spaces. Techniques and specializations change. Building your own databases will give you a lot to talk about in interviews for your first job using SQL if that's a direction you want to take. I would hire you over an applicant with intermediate SQL skills, but not solely because of this. Your C/Java skills are far more appealing and a demonstrated competency in those languages should translate into you learning SQL very quickly. You might want to check out Python as well.
If you live in a larger town or even smallish city, one good approach is to get on as a contractor - find a tech recruiter and they'll work pretty hard to get you placed. Once you're "inside" at a company you like, you then have opportunities to "show your stuff" and do networking. This makes it much easier when you find a job there you really want to apply for. Remember, all too often, the best way to get a job is to know people, not stuff.
Install SQL Developer.
Thanks for your input /u/So_average I will install it right now.
Still I don't have it, I only got the console, and not the GUI.
How did you get into BI, if you don't mind me asking? 
I have a liberal arts degree in Math and Philosophy. I’ve been a DBA for about a year after spending a year in a Math-related research position. I don’t think that not having an IT/CS degree hurt me, but often times getting the experience is the difficult part. I’m looking for a new DBA job now, and most of the job postings don’t call for any specific degrees. Usually they ask for 2-5 years experience and knowledge of SQL.
Started playing with RapidMiner. Made lotsa videos. A guy at a company noticed and hired me
No. It won't hurt to have a degree but you can achieve high career and financial success without it both in tech AND management. My boss is a great example. He doesn't have a degree, only a seven month computer programming tech school certificate. He came from a lower-middle class area in Philadelphia and his family didn't have any business contacts so he had to make his own way. He's now responsible for large scale efforts with budgets nearing 50 million dollars and he's had teams of dozens of direct reports. He's had senior management roles at GM, JnJ and was offered a CIO position at a medium-size company. He mentors us and explained that he he used the certificate to land a helpdesk role and networked through his company and elsewhere. Within a year he landed a sys admin / DBA position. Eventually he was made a supervisor and racked up a successful track record. He considered getting a degree but instead was advised to focus on networking with industry leaders at business events (like SIM), trade shows, etc. He says it's just as important to meet people and build a reputation as a problem solver as it is to have actual problem-solving skills. It obviously worked for him. I happen to know how much he earns too, and it's about 10% more than his "degreed" colleagues at the same company. I wouldn't consider him a genius. He's done things that are in reach of all of us --&gt; business networking, self study, earnestness and integrity. Also, he never let himself get comfortable and complacent in his work.
Sql developer or sql workbench
For a lazier alternative to get you started, look up yhe Northwind or Chinook databases. There are scripts associated with each that will set up DBs and tables and populate them with data for you to play with. Might be an easier way to get your feet wet. Once you're a bit further along, review the setup scripts to get a sense of how everything was created.
Ok i will try with sql workbench
Dozs it work with oearcle 11g ?
I have no degree, and a six figure salary as a Sr. Data Analyst. You'll be fine.
I have used both. Workbench (not MySQL workbench ) but workbench works with pretty much every database engine. 
It used to be. Pricey now. I wasn't saying use that particular tool. Pick a toolset that you like, become and expert. Let people know 
Sql plus is a product developed by oracle. It’s a command line interactive tool to communicate with oracle. Your post says you want a GUI. Sqlplus is simply not a GUI 
I want the white screen of SQL plus, we woek with on our lab. How can I have with, with the commend line I cant even copy ans paste :(
How could any of us possibly know what that means. 
Ok i will link a picture
I don't have [this](https://imgur.com/a/XYn7OZE) SQl PLUS 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/MtH2G0C.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e8kg4c2) 
Even if you change ANSI_PADDING, it will not affect existing data, only data that is inserted once it has been changed. You could maybe consider creative a view for the tables you use, and TRIM them all, and alias them back to the original field names. Then query the view rather than the table?
You're probably talking about the windows executable of SQL*Plus. Instant client has this available. https://www.oracle.com/technetwork/topics/winx64soft-089540.html You'll want to install the base plus any additional tools. Get the version for your database. Also it needs a prereq of Microsoft Visual Studio 2013 or 2010 depending on the version. In my windows environment I have TNS and ODBC set up. But honestly if you're in windows, SQL developer is the way to go. SQL*plus is good for executing scripts and doing one-liner stuff. 
Does work on windows 32 ? And thak you so much !!!! By the way, I only need to type a one liner.
This is definitely true, and unfortunate. As someone who hires, it's a double edged sword because I don't want those kinds of dumb filters in place but I absolutely need someone to help filter through truly unqualified applicants, and the people who do that almost never really understand the role they're filtering for. 
I see. I think you need the oracle binaries to get that. There is literally no difference. I would recommend using sql developer or workbench and running your queries there. You will get the same outcome and a better user experience. 
It should not matter, especially after you get some experience. One of the best developers I ever knew was a Music major.
Thanks !
What is the smallest amount of data you can loose and be 'okay'. If you are in a production environment this is something to be highly considered. You would possibly need up to the hour or minute recovery and thus you need the log backed up as well in event of complete loss. (you have a complete loss. you restore the latest full pg\_dump backup and then restore the log to the point in time you want to recover to) If you are dealing with a non prod environment you can probably be fine with database backups taken weekly. &amp;#x200B; &amp;#x200B;
Possibly consider changing char columns to varchar. &amp;#x200B;
Especially if you are a non-technical major, data analyst is a good place to start. You will typically learn the data retrieval and data visualization skills that can help you transition over to Business Intelligence. There are a lot of entry level positions available.
Thanks for the book Jacob.
It's exactly as you stated.
Both sql*plus for windows and SQL developer work for windows 32-bit environments. 
Advanced knowledge of SQL probably is understanding joins, unions, except, sprocs.... Strong programming might be 3 years experience in a dev role, understanding data structures, how to implement multiple design patterns, good problem solving skills.
Sounds to me like they want you to have a solid knowledge of PL/SQL at the media company
I'm out of luck if that's the case.
No problem! That’s good to hear! Glad it worked! Thank you for the link by the way. I’ll have to look into Kobo and see what my options are. I’ve also considered creating an audiobook to go along with this book. What do you think?
You bet! Thank you for the support! I hope you find the book enjoyable and useful!
May I ask why you've chosen to use a trigger?
My HW asks me to use one
Ah okay. You should be able to use a query as part of the insert instead of "values". INSERT INTO Tablename SELECT rid, avg(stars).....etc
Thanks!
Could you not just use a view?
&gt; I'm not fully comfortable with this stuff yet and still often need to refer to stack overflow or the documentation to understand things Welcome to the real world. This happens at all levels, daily. The main thing is you're intuitive enough to know what's capable, and what to search for. Heck, I Google simple stuff sometimes because it defeats me exactly how I should go about it. Advanced to me would mean loops, stored procedures, being able to pull data through SSIS and an ETL process, understanding query performance through execution plans and the impact of indexing.
Audiobook is good, especially as we are all spending too much time in traffic. But content needs to be adjusted. Especially examples with code and tables as results.
Yeah, that's how my own role was, mostly since my VP didn't know SQL and half the team didn't. Which is why, at least for the 40,000 person company, I'm a little less worried about the wording of advanced. It's not a non-technical role that requires "advanced programming". The other role I feel might be more technical and hands on, which is why strong programming is a bit more intimidating to me. My assumption is that your role is not a technical one in nature? I've been largely going under the assumption that any data analyst, scientist, engineer type of roles, "advanced" is actually advanced. This is not one of those positions. I could be wrong though
Just apply and be honest if you don’t feel comfortable learning on the job. I assume they will test your knowledge in the interview process so they will relatively know where you stand. It’s ok to take a job where you don’t know everything yet and figure it out as you work. 
Thanks for the suggestion! It's a very large vendor database. I don't have rights to make any changes to it.
Thanks! That's what I'd read about ANSI\_PADDING, it's just whenever I try to Google for some way to accomplish my goal, ANSI\_PADDING always shows up. &amp;#x200B; I can see about creating Views. That's a very good idea. It's a vendor database, and I'm not sure if I can even create views in our Prod environment, but I'll give it a shot! That should cover most of my frequently-used tables, but there are thousands in this database.
&gt; The post specifies strong programming in SQL. I imagine the expectation to be something like you can write moderately complex queries with reasonable fluency. You have almost no need to google anything relating to join, group by, having, subqueries, window functions, etc. Most of your time acquiring information will be specific to the business domain rather than understanding the technology or how to write a particular query. It might also mean that you know how to use different techniques to retrieve the same data and how to choose between them for reasons like performance, and perhaps that you are able to create additional data structures or indices in order to achieve acceptable performance on large data sets.
did you tried to enter "prenom", without the accent ?I have no access to an oracle db, but maybe it's not capable to work with them.Other than that, maybe "email" is a reserved keyword, or datatype?Do Oracle have user defined data types, like [MS sql server](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-type-transact-sql?view=sql-server-2017) or [postgresql](https://www.postgresql.org/docs/9.5/static/xtypes.html) ? I could see something like this also causing a conflict. Otherwise, I would try to create / drop the table commenting one of the fields, until you locate which one causes the issue.
Ok i will try
Nope I only tried this : create table Client( Numcli Number(7) primary key, ); ORA-00904: : identificateur non valide
Still not working here's the results: create table Client( Numcli Number(7) primary key, ); ORA-00904: : identificateur non valide
You're an unsung hero to some of us. Thank you, kind person.
Try implementing the same query a couple of different ways and then do some benchmarking. Learning to solve the same problem different ways is useful in itself, but by benchmarking it you can start to get a feel for what is "expensive" in whatever SQL DB you're using.
Learn how the database engine works (in a general sense). It'll make it easier to write optimized queries. 
What do you mean with database engine? Are you perhaps talking about the optimizer?
1. You can't tell me what to do. 2. Why would you want to do that? 3. In SQL Fiddle use the text to DDL link and paste the contents of your CSV in there.
I mean the actual software that your database system uses to create, update, read, and delete from a database. https://en.wikipedia.org/wiki/Database_engine Becoming familiar with the query optimizer is good too. 
Try your hand at reading query plans. Also, work through the examples in chapter 6 of The Guru's Guide to Transact-SQL, if you can find a copy. This chapter is like a Skyrim skill book. 
Amazon actually has it available. I’m wondering, do you recommend this book over Microsoft’s “T-SQL Fundamentals”? I ask because the guru book was published in 2000. I know there probably hasn’t been too many changes to T-SQL since then, but I wanted your opinion before shelling out the ~$40. 
The funny thing with SQL, at least T-SQL (SQL Server), is that the most performant code is quite often not the most concise code. Sometimes, you'll have code that looks simple - a single query within your batch with some joins, maybe a call to a user defined function inline. But then you rewrite it, move the function calls into #temp tables, and break your query out into parts, and it's much faster. The code looks more complex, but you'll come up with design patterns that make it easy to follow in the end. I guess what I am trying to say is that books aren't going to help you with what approach to take to SQL query syntax selection. They'll teach you about how specific syntax works, and that's valuable. But you've really just got to practice, and find what works for you.
&gt;It says "Order all the rows in StockPrices by date time, get the average price going up (preceding) 15 rows. I'm a bit confused... does the AVG() OVER operation use the full stockprices table or does it use the result set that comes after the WHERE condition (if there where any)?
What version of MySQL?
Any resources you could point me at? I’m new to SQL server coming from teradata.
[https://epdf.tips/the-gurus-guide-to-transact-sql.html](https://epdf.tips/the-gurus-guide-to-transact-sql.html)
When I was starting out I spent a lot of time on [https://www.sqlservercentral.com/Forums/](https://www.sqlservercentral.com/Forums/), when people post problems they usually post the ddl allowing you to replicate their problem. Paste their code into a database (maybe in tempdb where you can do no harm) and try to solve their problem. The cool thing is that this site has loads of MVPs who post and if you can't solve the problem, you can bet that someone else will and they will post their solution. I learnt loads by doing this. Also on SQL Server Central they have a stairways series, this is around 10 or so chapters on loads of different topics [http://www.sqlservercentral.com/stairway/](http://www.sqlservercentral.com/stairway/), the ones I have read have been excellent quality.
From experience I can tell you that biggest problem you'll have with self directed learning is not knowing what you don't know and so being unable to ask the right questions. As such you should find the book for the Microsoft "Querying Microsoft SQL Server" certification for the version of SQL server your using. Going over this exam prep, even if you don't plan on taking the cret, will give you the basic grounding from which everything else follows. For example reading query plans is a useful skill but completely pointless if you have no idea how indexing works and how how you write the query can effect their usage. 
Even if the book is solid and still relevant, T-SQL Fundamentals is another expertly written book that is packed with great content. It also contains many enhancements and considerations since 2000, so unless you deal with sql server 2000, I'd start there. 
Sorry , yeah
People always knock me for getting certifications claiming it’s a waste of time but this is my #1 biggest reason for doing it. Simply knowing what’s out there even if I’m not using it is hugely beneficial sometimes because I may eventually use it and often times will end up using it later.
This is very true and this only comes with experience. I’ve rewritten many queries so they look very unappealing but their impact on server resources and execution time is phenomenally better.
Honestly, a lot of the performance tips on the original t-sql are still very relevant and I find that a lot of the new syntax comes with too many hidden performance hits and caveats to bother with them. I’ve had to rewrite quite a few procedures where a dev decided to use a merge statement because it was more concise but the underlying locking / performance was terrible
Good catch. Yes. It uses the entire dataset. In my original code I was looking for rolling cpu averages. You can add a “PARTITION BY Ticker” qualifier to group by ticker symbol. 
&gt; but I want the smallest number group to come first In other words what you want to do is order your rows by the minimum of each group first then by the numbers themselves. So find a way to get the minimum for each letter group in a way that will let you join it back to the main data so you can order by it. 
The problem with your query is that your outer query is *non-deterministic*. There is no rule in your outer query defining which `Player` to return, so it just returns the most convenient one. Most RDBMSs wouldn't even run that query because it's non-deterministic. My guess is that you're not using the ONLY_FULL_GROUP_BY option, which is usually frowned upon. This problem is called the "Greatest N per Group" problem. You can find similar solutions Googling that. If you're on MySQL 8, this is pretty easy: SELECT Year, Player, Score FROM ( SELECT Year, Player, Score, ROW_NUMBER() OVER (PARTITION BY Year ORDER BY Score DESC) rn FROM ( SELECT Year, Player, SUM(S) AS Score FROM y_scores GROUP BY Year, Player ) q1 ) q2 WHERE q2.rn = 1; If you're not on MySQL 8, then you've got to use variables. I'm *very* rusty, but something like this should work: SET @num := 0, @year := 0; SELECT Year, Player, Score FROM ( SELECT Year, Player, Score, @num := if(@year = year, @num + 1, 1) as rn, @year := year as dummy FROM ( SELECT Year, Player, SUM(S) AS Score FROM y_scores GROUP BY Year, Player ) q1 ) q2 WHERE q2.rn = 1; 
Instead of nested queries, when you aggregate the sum, order by sum(s) desc limit 1
This dirty hack works, but it's not displaying the sort value correctly, and I wouldn't be confident using it in a production report. SELECT TestData.Alph , TestData.Num , MIN(NUM) OVER (PARTITION BY 1 ORDER BY Alph ASC) SortCol FROM ( SELECT 'A' AS Alph, 4 AS Num UNION SELECT 'C', 0 UNION SELECT 'C', 666 UNION SELECT 'C', 4453 UNION SELECT 'D', 3 UNION SELECT 'D', 2 UNION SELECT 'X', 454 UNION SELECT 'X', 21345 UNION SELECT 'A', 44 UNION SELECT 'B', 22 UNION SELECT 'A', 24 UNION SELECT 'A', 5 UNION SELECT 'A', 7 ) AS TestData ORDER BY MIN(NUM) OVER (PARTITION BY 1 ORDER BY Alph ASC) ASC Alph|Num|SortCol ---|---|--- C | 0 |0 C | 666 |0 C | 4453 |0 D | 2 |0 D | 3 |0 X | 454 |0 X | 21345 |0 A | 4 |4 A | 5 |4 A | 7 |4 A | 24 |4 A | 44 |4 B | 22 |4 
Your innermost query already provides the scores, so I just added one further outer query to add the newest date: select T1.id, T1.score, max(SF.time) as time from scores_fifteen SF inner join ( select id, min(score) as score from scores_fifteen group by id ) T1 on SF.id = T1.id group by T1.id, T1.score 
I think you need to add player to your outer group
I recommend grabbing a dev version for your home (its free), and build something extravagant. "How to think" is more important than "how to code". You need to understand the physical model (storage, clustered non clustered indexes, files) and focus on code reuse. Grab adventure works and/or wide world importers and remodel them into something more edficient. I wont link-jack your thread. Message me and I'll send you a playlist of live MSSQL development. 
Wow... How can something this polished be free? Getting the opportunity to watch over the shoulder of a senior DBA while he explains what he's doing, in a native english accent, a pleasant voice and perfect atmospheric background track... You have a very lucrative career path ahead of yourself as an instructor. I would be surprised if no one has gotten in touch with you with offers by now. Putting this on my Must-Watch list! Before it gets placed behind an expensive pay-wall :)
Woah woah woah. My grammar sucks. Pleasant voice? I'll take it. Pay-wall? You want my scripts that I spent my time on... pay. But knowledge should be free. Polished... maybe I'm too critical of myself. Live development cam go south real fast and I have to undo as much as possible to get continuity. Thank you for the kind words and stay tuned. I haven't even scratched the tip of the iceberg.
&gt; I find that a lot of the new syntax comes with too many hidden performance hits and caveats to bother with them. Window functions though.
Feel free to link jack! I'm likely not the only one who's curious about this.
 WITH cteData AS ( SELECT * FROM (VALUES ('A - 13') , ('A - 223') , ('B - 543') , ('C - 4') , ('A - 26') , ('B - 123') , ('C - 374')) AS v(GroupBy)) SELECT x.GroupBy FROM (SELECT d.GroupBy , SUBSTRING(d.GroupBy,1,1) AS C , CONVERT(int,SUBSTRING(d.GroupBy,5,4)) AS N , MIN(CONVERT(int,SUBSTRING(d.GroupBy,5,4))) OVER (PARTITION BY SUBSTRING(d.GroupBy,1,1)) AS Sort1 FROM cteData AS d) AS x ORDER BY x.Sort1 , x.C , x.N;
 SELECT * FROM contacts AS c JOIN association AS a ON c.id = a.contact_id JOIN deals AS d ON a.deal_id = d.id
Just playing devils advocate but what happens if they get to AA, or AAA?
https://www.youtube.com/playlist?list=PLPI9hmrj2Vd8m_w3By7pI7xlkXMRzNYzS Every weekend I upload a "day" and post to reddit. This playlist is from video 001 (there are 19 up). This is end game raid boss data management system design. Try to make this at home. Try to follow along. There are some serious patterns here and I'm just getting started. I think day 6 or day 7 I'll be grabbing adventure works and wide world importers, and create a parallel series to dissect what ms did, into what I did.
A valid point. Edited.
Thanks!
&gt; subtotal of all orders amounts placed partitioned by custid use a simple `GROUP BY` if it's more complicated than that, you didn't explain it well
Something like this should only show the subtotal on the last row. SELECT O.Customer, O.OrderId, O.OrderTotal, CASE WHEN ROW_NUMBER() OVER (PARTITION BY O.Customer ORDER BY O.OrderId DESC) = 1 THEN SUM(O.OrderTotal) OVER (PARTITION BY O.Customer) ELSE NULL END AS SubTotal FROM Orders O
ROLLUP, CUBE, and GROUPING SETS if you are in MSSQL - most RDBMS have similar functionality. https://www.databasejournal.com/features/mssql/using-the-rollup-cube-and-grouping-sets-operators.html
Thank you!
To add to it, your inner select needs to be correlated with the outer select. If you want to dive deeper into this, look up correlated queries and anti-joins.
Probably not what you want to hear but I've been taught to do many of these higher level functions of grouping or subtotaling in your reporting tool rather than in the raw queries. SSRS or even Excel are valid tools that we use all the time.
Have you tried replacing the "where" with and "and" so that the join evaluates the matching userid AND the timestamp criteria?
Do you have an idea while I'm trying to join two columns which same value I get an error: No operator matches the given name and argument types. You might need to add explicit type casts. http://prntscr.com/lbx8jg
Somebody already provided an answer, but I figured I'd provide an alternative: SELECT * FROM Students WHERE StudentID NOT IN ( SELECT StudentID FROM Enroll WHERE ClassNumber = 'CSC201A' )
Pretty much. Everybody googles, no matter how fluent they are. For example I have to look up cursors every time I use them (to the point where I try to avoid them altogether if I can) simply because my brain never seems to be able to retain the exact syntax.
Aww yiss. This is the shit.
One little thing that bothers me... the figures / result tables should match up to the queries... :) This query: SELECT * FROM (SELECT SUM (# of friends) FROM facebook GROUP BY state) Does not produce this table in any DBMS I'm familiar with: |State|SUM(# of friends)| |:-|:-| |CA|1400| |FL|400| &amp;#x200B; as you're not selecting the State anywhere... it'll just return the SUM() column.
Excited to check this out. Thanks OP!
I forgot to mention, it's SQL Server 2017. Sample code: DECLARE @case\_ident\_id INT DECLARE @identObj NVARCHAR(MAX) = N'\[ { "identName": "apple", "importId": 17755, "invs": \[{"userId": 10022, "invTypeCd": "LI", "importId": 17755},{"userId": 10010, "invTypeCd": "SI", "importId": 17755}\], "services": \[{"serviceId": 1280, "importId": 17755},{"serviceId": 1281, "importId": 17755}\] }, { "identName": "orange", "importId": 1545, "invs": \[{"userId": 10022, "invTypeCd": "SI", "importId": 1545},{"userId": 10010, "invTypeCd": "RV", "importId": 1545}\], "services": \[{"serviceId": 1280, "importId": 1545},{"serviceId": 1281, "importId": 1545}\] } \]'; \*/ &amp;#x200B; AS &amp;#x200B; BEGIN &amp;#x200B; SELECT B.\* INTO #tmpIds FROM OPENJSON(@identObj) A CROSS APPLY OPENJSON(A.value) WITH ( identName NVARCHAR(500), importId INT ) B; &amp;#x200B; INSERT INTO case\_idents( ident\_name, import\_id ) SELECT identName, importId FROM #tmpIds; &amp;#x200B; SET @case\_ident\_id=SCOPE\_IDENTITY(); &amp;#x200B; select D.\* INTO #tmpInvs FROM OPENJSON(@identObj) WITH ( invs NVARCHAR(MAX) AS JSON ) AS caseIdentInvs CROSS APPLY OPENJSON (caseIdentInvs.invs) WITH ( userId INT, invTypeCd CHAR(5), importId INT ) D; &amp;#x200B; WITH cte AS ( SELECT c.\*,ci.case\_ident\_id AS case\_ident\_id, ki.inv\_type\_name AS inv\_type\_name FROM #tmpInvs c INNER JOIN case\_idents ci ON c.importId=ci.import\_id INNER JOIN kdd\_inv\_type ki ON c.invTypeCd=ki.inv\_type\_cd ) &amp;#x200B; INSERT INTO case\_ident\_to\_inv(inv\_id, case\_ident\_id, inv\_type\_cd, inv\_type\_name) SELECT userId, case\_ident\_id, invTypeCd, inv\_type\_name FROM cte &amp;#x200B; \---------- ADD INTO case\_ident\_to\_service -------------------- &amp;#x200B; select E.\* INTO #tmpServs FROM OPENJSON(@identObj) WITH ( \[services\] NVARCHAR(MAX) AS JSON ) AS caseIdentServs CROSS APPLY OPENJSON (caseIdentServs.\[services\]) WITH ( serviceId INT, importId INT ) E; &amp;#x200B; WITH Scte AS ( SELECT s.\*,ci.case\_ident\_id AS case\_ident\_id FROM #tmpServs s INNER JOIN case\_idents ci ON s.importId=ci.import\_id ) &amp;#x200B; INSERT INTO case\_ident\_to\_service(service\_id, case\_ident\_id) SELECT serviceId, case\_ident\_id FROM Scte &amp;#x200B; \------------------------ END ADD INTO case\_ident\_to\_service ---------------------------------------------- &amp;#x200B; SELECT ci.case\_ident\_id AS case\_ident\_id, ci.ident\_status AS ident\_status FROM #tmpIds c INNER JOIN case\_idents ci ON c.importId = ci.import\_id &amp;#x200B; UPDATE case\_idents SET import\_id = null; &amp;#x200B; END
I'm using SQL Server 2017. I'm trying to post my sample code but it's a bit long, and not formatting correctly.
If I'm understanding the question correctly, I think it would look like this within SQL Server. SELECT Table2.* FROM Table2 LEFT JOIN Table1 ON Table1.user_id = #T2.user_id AND Table2.createtimestamp &gt; Table1.createtimestamp
Ah I see. Thank you for the suggestions. Do you have anything specific you can point out to me that you feel should be modified?
Good catch, I think it is all fixed now. Let me know if you spot any other mistakes.
Is there any particular difference between a sub-query and temp tables? Are there specific times when you should use one or the other or do both accomplish the same thing, just in a different way?
Thanks for pointing this out!
I want to know this too. When I started I used to create a lot of temp tables but now I do a lot of sub queries unless it becomes too slow.
Exactly why I want to know too. I just "discovered" temp tables recently and have been using the hell out of them. However, I'd like to know if one is more efficient and/or less resource intensive than the other.
These aren't temp tables and some aren't even "subqueries". The first ones are derived tables that are processed for each query that calls them. When they get down to the comparison, that actually is more inline with subqueries. Also, you can have multiple results from a subquery but you have to use an appropriate operator for the comparison. When they use the WITH clause, this is essentially being used as a derived table but you can use the WITH part to process recursion. This is called a common table expression (CTE). Lastly, temp tables are actually tables that get created and can be manipulated basically like any other table. The difference between these and a normal table is that they only exist and persist for the session.
5.7.24
If you want to compare how writing a query affects performance, review the execution plan. When you submit a query, it'll eventually be handled by the optimizer which can generate the same execution plan for queries that are written differently.
Good call on labelling as a Common Table Expression instead of temporary table, the article has been updated. Can you give an example where you can have multiple results from a subquery and make use of it in a WHERE statement?
Row_number would probably work, if understand the question right. Create a derived table (or CTE) and only display where rownumber = 1, with a partition on name, date, time in.
Just like the error says, you're comparing values of different types. Don't do that?
It has now been updated to reflect CTE instead of temp table, thanks for pointing it out!
What if you create the table first, then run the statement as an insert?
 SELECT * FROM employees WHERE state IN ( SELECT state FROM offices WHERE region = 'NE' GROUP BY state ); 
If you don't have to use the NOT EXISTS: Select s.StudentName From Students s Inner Jioin Enroll e on s.StudentId = e.StudentId Where e.ClaswsNumber &lt;&gt; 'CSC201A' 
Ah! that is so obvious. An IN example is now in the tutorial as well with animation.
Thanks for the suggestion, but it turns out that I am using a pre-2008 version of SQL, before the ROW_NUMBER() function was added. Is there a way to count duplicate tuples some other way?
Very nice! I love the visuals. I do see a small typo in the Subqueries Placement section. First paragraph, last sentence starts with “WHERE of HAVING” instead of “WHERE or HAVING”. 
I can answer your first part. Few days back I have also encountered same type of error. As a newbie I am not so much used to SQL. So i choose to try various demo versions software for SQL recovery. I found this [SQL Database Recovery Tool](http://www.databasefilerecovery.com/mssql-database-recovery.html) quite impressive. As it smoothly runs on SQL versions.
For SSMS 2008 that information is stored in a binary format here: C:\Documents and Settings\&lt;USER&gt;\Application Data\Microsoft\Microsoft SQL Server\100\Tools\Shell\SqlStudio.bin For obvious reasons, unless you write your own program to read/write the **BINARY** file, you have two options. 1. Delete the bin file and lose all your settings for SSMS. 2. Ignore the entry and move on with your life. If you are half interested in writing your own .NET program to read in the file and perhaps maybe be able to write a bin file minus the entry, [you can check here](https://stackoverflow.com/questions/6230159/how-to-delete-server-entries-in-sql-server-management-studios-connect-to-serve/6374534#6374534). IMHO, you should stop using SSMS 2008, it's crap unless you've got a specific plug-in for it that will work nowhere else. Lots of luck, because you'll need every ounce of it.
Hi D_C, I do believe I'm on an older version... slow learner, I guess. I really appreciate the help for starters, so thank you. I'm going to give Greatest N per Group a search tonight and try to understand better. As well as use of variables. The bottom worked after I added another ORDER BY Season DESC after the last line. Just one more issue, I didn't really think to mention. Some Years will have multiple people tied. Any ideas on how to solve that would be appreciated too. Thanks a lot! 
I'm also a beginner / intermediate but something like this? select m.acreage as "Maize", p.acreage as "Paddy", m.date, m.region from Maize m left join Paddy p on m.date = p.date and m.region = p.region where m.region='A' union select m.acreage as "Maize", p.acreage as "Paddy", p.date, p.region from Paddy p left join Maize m on m.date = p.date and m.region = p.region where p.region='A' order by date desc
I think you might want to look up grouping sets, rollup and cube. This is for PostgreSQL so I’m not sure if that’s what you’re using, but it gives you a combination. Also maybe cross join? With condition as well 
I’m sure someone will chime in soon with a much better and more efficient solution 
Immediately after your PARTITION BY statement you just need to add ORDER BY. Something like... SELECT O.Customer, O.OrderId, O.OrderTotal, CASE WHEN ROW_NUMBER() OVER (PARTITION BY O.Customer ORDER BY O.OrderId DESC) = 1 THEN SUM(O.OrderTotal) OVER (PARTITION BY O.Customer ORDER BY [ColumnName]) ELSE NULL END AS SubTotal FROM Orders O &amp;#x200B;
 SELECT one.name , two.name , one.points + two.points AS point_total FROM yourtable AS one CROSS JOIN yourtable AS two WHERE one.salary + two.salary &lt; 12000 
[Yes there are.](https://docs.microsoft.com/en-us/previous-versions/office/developer/office-2007/bb208890\(v%3doffice.12)) Dates can be qualified with `#` in Access, `'` in T-SQL. Strings are `"` in Access, `'` in T-SQL Access has no `CASE` statement (I think?) Access requires parens when you go beyond 2 tables for `joins`. [Data types vary](https://docs.microsoft.com/en-us/previous-versions/office/developer/office-2007/bb177899\(v=office.12\))
 select SUM(CASE WHEN type = 'm' THEN acreage ELSE 0 END) as "Maize", SUM(CASE WHEN type = 'p' THEN acreage ELSE 0 END) as "Paddy". date, region from ( select 'm' as "type", m.acreage, m.date, m.region from maize m UNION ALL select 'p' as "type", p.acreage, p.date, p.region from paddy p ) group by date, region;
I understand that using previous versions of a language is not best practice, but I don't get to decide what we are using. I wish we would, but updating roughly 16 years of software would inevitably cause problems. 
I am actually using access but someone from stackover flow helped me. Sorry for not clarifying that. Is there a way I can have a table constantly update from a website? There’s two websites I use and right now. I’m currently using a data mining tool to find tables because the website import wizard on access isn’t finding the table . This is a lengthy process that needs to be done often so I was wondering if my access database can automatically update with a website. I’m using nba.com mostly.
Thanks for your help, but my biggest hurdle now is updating database constantly. Please refer to my comment I posted.
My suggestion is it download an open source database first and foremost. There are numerous ones out there such as MySQL, PostgreSQL but would differently recommend MySQL due to the profuse resources available for it. I think the open source version already comes with dumpy data to play and practice with. And remember the rule of thumb: Try, Fail, Retry, Fail, Learn...Success.
 WHERE sum_amount in (SELECT MAX(sum_amount) FROM TEMP_TABLE); You're telling the Database here, "There exists another table independent of *anything* written above this statement called "Temp_Table", please get me the MAX(sum_amount) from that table", to which, it does not exist. You'd want something like: WHERE sum_amount in MAX(TEMP_TABLE.sum_amount);
Thanks! Yep, typo now fixed
Depending on your SQL env, you can use a sub query. \`\`\`WITH temp\_table AS ( SELECT invoice\_id, SUM(amount) AS sum\_amount FROM invoice GROUP BY invoice\_id) SELECT invoice\_id, sum\_amount FROM temp\_table WHERE sum\_amount IN (SELECT MAX(sum\_amount) FROM temp\_table); \`\`\`
&gt;"There exists another table independent of anything written above this statement called "Temp_Table", please get me the MAX(sum_amount) from that table", to which, it does not exist. . &gt; you can't put aggregate functions in a where clause. Where can one learn things like that? I followed many tutorials and MOOCs but none of them really talk about these peculiarities.
Are you just trying to grab the row with the MAX(sum_amount)? Why not just pull the top 1 SELECT TOP(1) invoice_id, SUM(amount) AS sum_amount FROM invoice GROUP BY invoice_id ORDER BY sum_amount DESC
What if there are two invoices with the same max value
SELECT TOP (1) WITH TIES
Not falling for this HMRC
You are creating TEMP_TABLE in your FROM clause and trying to use it in your WHERE clause. I don't know why that isn't allowed, but it isn't. You could create a view first with your definition of TEMP_TABLE, and then use the view in both places. Also, in your WHERE clause, your looking for the maximum value of sum_amount. That will be a single value. Your query might run faster with "WHERE sum_amount = ...". Finally, you might try this (although I'm not sure of it): SELECT invoice_id, SUM(amount) FROM invoice GROUP BY invoice_id ORDER BY sum(amount) DESC (and add the syntax, which is different in each version of SQL, to return only the first row)
Sr. Systems/Data Analyst Bank Enough or Not Enough depending on who you are
TIL HMRC = Her Majesty's Revenue and Customs
Business Systems Analyst. 70K... I've heard from other coworkers that they think our company underpays for the role but this is my first real job out of college so idk.. been here 3 years so far and my salary has gone from 55K to 70K in that time. 
Reporting Specialist, 75k
That’s about the track I was on right after college until I got experience. I would say learning the skills and getting experience under your belt is worth it. If you’re not in a HCOL either it makes sense for entry level. 
- Senior IT Data Analyst — $84,000 — work for a Fortune 500 but not a tech company. Also relevant is your locations/COL &amp; how long you’ve been in the field — places like FB/Google etc. will be much higher salaries due to HCOL. - 6 years out of school. Medium COL 
What does that mean?
I want your gig. I'm a "Data Engineer" as of now. Mind sharing a small bit on how you got there?
New career doing data analytics and database management in California. 55K plus 5k bonus. Had no professional experience though. I feel like I’m getting under paid but perks are pretty good, relax work environment, big international company, and close to home. I’ll take it as is for now considering I need the experience and the company name is good for my resume 
Your best bet is to look into GROUP BY GROUPING SETS, GROUP BY ROLLUP, and GROUP BY CUBE. These things are made to work best for this, all the other options where you have a CASE WHEN ROW\_NUMBER() OVER(... is hacking it together and should be used as a fall back if your DB doesn't support GROUPING SETS. But you will get the best performance with GROUPING SETS, ROLLUP, and CUBE. 
You are not using table alias but a subquery with a name. You can use another subquery with another name in where condition. 
This is a Common Table Expression and yes, this would be the ideal way to do it that a vast majority of DBs will support.
https://www.brentozar.com/archive/2018/01/2018-data-professionals-salary-survey-results/amp/
Yup yup!
My first job as a data analyst I was in the same boat and thats about what I made (in TX though). Got 8 months of experience and showed up at work everyday and kicked ass, got a new job for more money. 
Great question! I really don't have a good answer unfortunately, I just learned these things through brute-forcing attempts, research, and on-the-job situations that taught me so. =\
Database Administrator. I work for a contractor for the Defense Department. I have 10 years of IT experience but only one year in database administration. I make $100k in an area with a medium cost of living. We NEED database administrators please come apply.
IT Swiss Army Knife Canada Not Enough.
Just moved from DBA to DB Architect, also in US, my pay has almost doubled in the last 6 years.
You can [specify the ANSI wildcard set](https://support.office.com/en-us/article/Access-wildcard-character-reference-AF00C501-7972-40EE-8889-E18ABAAD12D1#bm2) to use in an Access database and make it more harmonious to SQL Server.
Although, building up a portfolio seems rational but most positions out there do not ask for a portfolio but rather ask technical questions during the interview process. Then again, contingent upon what positions interest you.
Data quality coordinator... 34k I've been using smattering of sql for over a year, but use ms access a lot and they're training me on sql and sas I'll move up in salary soon I hope
ERP Data Analyst, 65k, Midwest region on US. 
remove the "
Jr. Data Engineer, Western US, 75k
What is this .xlsx evil I perceive? I DEMAND THIS BE IN TAB-DELIMITED FILES. WITH A SEPARATE FOR THE HEADER.
No remote jobs?
You are underpaid, but mainly because of what the Californian economy is like. I'm in Oklahoma, where the dollar is worth more, and my $55k is the equivalent here of what almost $70k would be in California.
Median salary in Uganda for 2018 is \~149k? Sign me up!
Hillary Morbo Rodham Clinton.
I'm QA and Automation Engineer and I'm the same.
* Data Analyst * A well-known convenience store chain * $55k This is my first year as an analyst.
Quality Assurance Associate in New Mexico. 42k yearly.
Huh, salaries in the US seem wayyy higher than the UK. I’m a data analyst for a water supply company just outside London and am on 30k, with a years experience in analysis. Feeling far more underpaid than I did 5 minutes ago haha. 
Won't go into detail but learning SQL and keeping an eye out for who to impress with it has helped me do quite well advancing diagonally (mostly lateral moves that all came with raises upon hire) in the big corp I work for. 
All of them
Really? Since which version? I can't find it in the doc.
BI Developer. Hospitality and entertainment industry, worked at a large global manufacturer for most of my data career. Tipped the 6 figure mark last year with 5 years progressive experience. 
Administrators or Developers?
Automation specialist. I work with SQL and an RPA software Make 58k 45 minutes from NYC.
Oh yeah that’s a good point. Although I have a friend from Germany who commented on how low our salaries are compared to Germany, whilst our cost of living was also way more expensive than back home 
Senior Analyst, Detroit, 1y out of college. $70k + 10% annual bonus. Other perks: 5% on 6% 401k March and near free health insurance (HSA plan “costs” $540/y, I get $600 put in my HSA AND I get $350 in credits towards my premiums, $1.5k OOP max)
UK Tax office. Think IRS.
UK, Oracle DBA, 55k. For the area I live, my expenses are around 21% of my wage, so decent enough wage! PL/SQL is fun. 
Software engineering but with a focus on dba and application data pipeline development. So I touch databases and application code regularly. Kind of an intermediary between pure software engineers and data scientists as well.
ok ! thank you very much :)
55k data analyst in upper Midwest. Looking to move up or move out here soon. I’ve recently begun moving into a SQL developer roll with no guarantee of title or salary change. They’re getting a ton of work from me at a very reduced cost, but I’m getting vital experience. 
Code janitor, US West, enough.
Are you me? Similar situation
Is it safe to assume you have certs? And if so what did you do to prep for them if you don't mind me asking.
Database Dev, 65k, I work for a health suppliment company. 4 and half years experience out of college. I think I am underpaid but the market is good here and I been starting to look around. Unfortunately, it seems you have to find a new gig to get a decent raise. 
Oh man. That's my dream job. I'm getting a small taste of it where I'm at now. Have about 9 years development experience. Doing some light admin tasks, now.
CTO, but got here gradually over the course of 15 years with this company, starting as Senior DBA. 52 years old, 30 years total experience with databases and more general systems architecture. Mid sized company (300-ish employees), Bay Area. I started here a little after the .com crash when it was a 10 person startup as the primary development DBA making $80K. Currently making $275K with up to 50% bonus depending on our earnings and a little under 3% of the company in stock. &amp;#x200B; I mention none of this to brag, but to highlight that database knowledge can be very versatile and open doors you didn't expect. Understanding data modeling and how to decompose business ideas into entities is applicable to all kinds of things beyond the literal database itself and if you're good at explaining those concepts, you are more valuable than you may realize. Also to point out that although it goes against the conventional wisdom, it's not \*always\* a bad thing to stick with one company if you can see a path to advancement within it. &amp;#x200B;
Data architect and 130k..you should be around 160-180 for Seattle.. Just a fyi
Business Analyst Associate, Small-Mid size Insurance Company, and I'm at $47-50k/yr. I got hired on without any SQL experience, entry level. I'm 22, just starting out. Is this pretty fair? I'm extremely thankful and lucky, but just interested if that's pretty standard?
ETL Developer at a medium sized insurance company making 65K
Some of my previous jobs/titles: * Database Administrator/Analyst - $52k in Texas * Junior DBA - $55k, then bumped to $59k after a year or two in Louisiana. Yearly bonus of 1-10%, so the $59k would become roughly $65k on a good year.
30.0 miles ≈ 48.3 kilometres ^(1 mile ≈ 1.6km) ^(I'm a bot. Downvote to remove.) _____ ^| ^[Info](https://www.reddit.com/user/Bot_Metric/comments/8lt7af/i_am_a_bot/) ^| ^[PM](https://www.reddit.com/message/compose?to=Ttime5) ^| ^[Stats](http://botmetric.pythonanywhere.com) ^| ^[Opt-out](https://www.reddit.com/message/compose?to=Bot_Metric&amp;subject=Don't%20reply%20to%20me&amp;message=If%20you%20send%20this%20message,%20I%20will%20no%20longer%20reply%20to%20your%20comments%20and%20posts.) ^| ^[v.4.4.6](https://www.reddit.com/user/Bot_Metric/comments/8o9vgz/updates/) ^|
Senior Business Intelligence Developer, in Salt Lake City Utah. $123,000 / year. &amp;#x200B; &amp;#x200B;
Probably can be achieved via pl/sql, because you have to dynamically set tables to search your values in. Btw, why are you doing this?
What is a COL
Apply where?
Same! 50k to 72k in just under 3 years
Wow I’m starting to realize how underpaid I am at my current company. I’m a data analyst making $40K first year out of college. 
You’re making 10K more than I am and I was in the same boat as you when I started out last year.
Database Administrator Columbia, SC 31k I feel like I'm underpaid I have a Bachelor of Computer Engineering and have been in the position since February 2018. I run the ERP for a college
Look up count and group by.
Cost of living
SQL DBA, 50k GBP (about 63k USD). Local government, so reasonable amount of holiday compared to some.
That actually sounds really cool. What skillsets do you need for that besides writing queries. 
Would it help if I upload table schema?
I use group by
However, there is another question that I like to touch
We're not going to do your homework for you. What I gave you should give you enough to work out what to do, especially if you Google the parts that you haven't heard of before.
If I understand correctly, a full update replaces all rows each time the sproc is executed. An incremental update only adds new rows since the last execution.
I use this in a job. So once a week my job runs and I want it to run a full load. Then during the week I might make some changes to mapping values and want to rerun the job, but only for the data where I made changes, which greatly improves how long it takes to run the job. I don't need to run the whole thing since no other sets of data are changing. I achieve this by having an IF statement based on the SUM() of a column named isTrigger. If I update a mapped value I set the isTrigger = 1 and then when the sproc runs it determines whether to do an incremental run, or a full run based on whether the sum = 0 or not. Then at the end it changes the 1 to a 0 so if I run it a second time it will run a full load.
Derivatives analyst in an organization related to financial services. 95k.
Holy hell, where at? I'm at 81k in Minneapolis. 3 years experience as a BI Dev. Anything I should learn to get to that level of pay?
Nice - not only is the pay good but the clearance you probably need means you can't be replaced by an H1B.
I am in Utah too! Gives me hope that there are actually tech positions that pay over 6 figures here. 
Great benefits? I'm going to guess you work somewhere in the UC system.
 * Data Analyst * Pacific Northwest * $70,000 I have just over 1 year official experience, maybe 1.5 years doing 'data analyst' work. I work in SQL, Excel, &amp; Power BI. University educated, but not for this. Self-taught, started doing the work...and with a bit of a lag, got the title. With a bit more of a lag, the salary. 
&gt;feel like I'm underpaid Yes. You are with those credentials. 
does your source table have a ChangeDate, ModifiedOn, or similar field that has a update trigger that sets getdate() on any update? If so you need a where ModifiedOn &gt;= LastRun and your stored proc should update LastRun in some controlling or logging table
What is COL ?
Whatever it was, it just went up £730 as of yesterday.
In London, yes. Just outside of London that $500k will get you a box room, a toilet, AND half a parking space.
$40k first year out of college in any field is still just fine. College means you passed a bunch of tests and gets your foot in the door. It's real-world experience that gets valued, so re-evaluate in a year or so to see where you see yourself going (either within the current company or possible elsewhere).
PM. Just north of 200K near Seattle.
that's awesome
By getting the opportunity to not only design a couple or large scale OLTP databases (around 3TB when I left that gig) as well as designing three multiple PB scale Data Warehouses. For the DW I also scoped and wrote all of the ELT code (SSIS). I also got involved in a large lift and shift of all of this company's SQL Server DBs to AWS. That included a 4.7 PB Enterprise Data Warehouse we moved to RedShift. Get your cloud certs - road to instant money :)
That's low, but if youre in the South it's not outrageous. Just pick up more experience and bounce to a higher paid job 
?
I discovered it entirely by accident quite recently. I built a Db on SQL Server several years ago and used Access for the GUI. Issued a new computer to one of the users, installed Acccess 2016, and for some reason their installation defaulted to ANSI-92 and broke a couple of my queries. Fun. 
I miss the weather monster. 
or alternatively you post your problem and we can review it and comment on it 
Names of members who have gone to every event held in 2018. All the upcoming events (title, date, location) in a given postcode (e.g. 6150) for the next month (i.e. the whole of the next calendar month). Names of any members who have hosted more events than they have attended. The number of places available in total for the “Know your Grevilleas” event series. MEMBER_SERVICE (MemberID, ServiceNo, ServiceDetails, WhenAvailable); SERVICE (ServiceNo, ServiceName, CategoryID); CATEGORY (CategoryID, Category, Definition); MEMBER (MemberID, MemFirstName, MemLastName, StreetAddress, Email, ContactNo, Biography); EVENT (EventID, Title, Description, EventDate, EventType, Location, PostCode, NumberOfPlaces, NumberOfHosts, HostsFirstName, HostsLastName, MemberID); EVENT_PARTICIPATION (EventID, MemberID, BookingID, BookingType); INDIVIDUAL_EVENT (EventID, IndividualName, IndCoordinatorName, IndCoordinatorContact); SERIES_EVENT (EventID, SeriesName, NumberOfSeries, CoordinatorName, CoordinatorContact); INDIVIDUAL_BOOKING (BookingID, IndSpecialRequirements); GROUP_BOOKING (BookingID, NameOfGroup, NumberOfGroup, SpecialRequirements); GROUP_MEMBER (TempID, BookingID, GroupFirstName, GroupLastName, GroupEmail, GroupContactNo);
It’s automotive, but most of the companies are now on a hiring freeze— also, my degree is in Economics. So I do SQL all day and I needed SQL on my resume as a qualifier, but economics is what actually got me the job.
Look up HAVING
Best answer is to ask him. In SQL server, (which I doubt this is) you can declare a datetime column with the number of bytes it uses, which will be different from the input length.
Hey I was a history major too! 
"How are your children, Morbo?" "Belligerent and numerous".
Date/Time is usually stored internally as a number and is not actually in that nice format as raw data. For timestamps in oracle the number represents how precise the fraction of a second will be.
Data Analyst Manager, Connecticut, $100k. We're hiring if anyone is in the NY/CT area and looking for a job. 
Thanks for your input. I have asked our partner start this week, but they're swamped at the moment and the person who was handling this intially has been on sick leave a couple of weeks, hence me trying to get some other input as well ;)