A-splain. LIKE is considered bad form? I hadn't run across that opinion before. The devs certainly don't think so.
It will most likely require a table scan. I'm no expert in DB's but it will have to search for whatever string through the entire field you are looking at, which could take a lot of time over a million rows our so. Better table design would allow you to query the attributes you care about more directly (e.g. Type fields, boolean). Of course if the tables are setup as fee form text fields then LIKE is your only choice. 
LIKE should be used to pull rows matching search patterns, and that is about it. If you are using it to select discrete data, you should just use: " WHERE `xyz` = 'FooBar' " or the IN clause. LIKE is not intended to be used to filter data; it is meant to be used to search for patterns in text. Example: SELECT `suggestion`, `solution` FROM `Ideas` WHERE `Problem` LIKE '%developer%' AND `Authority` IN ('respectable', 'certified', 'best practice') ORDER BY `Effectiveness` DESC ...would pull 'developer','program developer', 'photo developer', etc... Anything with the word developer in it. Probably okay in 95% of your situations, but of course, it's the 5% that takes up 90% of your time. I would also suggest only one syntax clause per line, capitalizing your syntax lines to make them appear differently, enclosing every field with it's proper brackets or backticks ( [] in MSSQL, ` in MySQL, etc ). But most of all, enclosing those fields. Try to SELECT from a column or table with the name of 'group' without using brackets/backticks 
Yeah... In my imaginary Ideas table, the Problem column is likely a text or blob data type, containing descriptions of the problem(s). Since I want to only return ideas for problems which contain the sub strings "developers" and "production access", I'm pretty sure LIKE would be appropriate. The table isn't going to have "DeveloperRelated" or "ProductionAccess" boolean columns.
Ahhh, that does make it more complicated.. Then perhaps you'd have to get the developers to create the troubleshooting and reporting views for you, which you simply pass the parameters when you need them? Still way far from perfect.. sry can't offer more!
VBA, makes me shudder. Programming in office, should be done as a very last resort IMO. Even then, just don't do it.
I am not an Oracle expert but on other platforms Updating a table with a subselect in the same query does not work. Usually you need to select into a var and then do your update.
Instead of wrapping the select statements in a trim(), can you try just trimming the column in the select? &gt; SELECT &gt; trim(d.ATTRIBUTE_VALUE) &gt; FROM &gt; SA_SPECIFICATION_DETAILS d, &gt; SA_ASSET a &gt; WHERE &gt; d.SPECIFICATION_NO = a.SPECIFICATION_NO &gt; AND d.ATTRIBUTE_DESC = 'MANUFACTURER'
I did. Bad guy typo makes me look dumb. Also, you CAN convince them. Start tracking customer feedback via surveys and record call open to closure time. PROVIDE METRICS! "I CRUSH your stupid management decisions with FACTS muthafucker!"
Thanks, changed it but still didn't work. Enclosed the entire select in parenthesis and that worked but I think it's running into the problem alienzx described.
&gt; What kind of harebrained implementation would actually name its tables the same as reserved keywords? People like the people you work for
Just to check, you want to concat all four of the attribute value columns into one asset_desc, correct? 
Correct
Which version of Oracle? Makes a difference since the pivot command might be useful. 
That ORA-01427 error would be caused by one of your subselects returning more than one row. Like duncan6894 mentioned, the pivot command could be useful. I'm not overly familiar with that one, myself, as I haven't had to use it yet. Another option would be to look at your individual subselects, figure out which one returns more than one result, and if possible, adjust the query so only one result comes back.
Thanks, I'm doing that now - it's selecting 35 records on all the selects; I'm sure it's because the table holds 14 attributes for each specification number. I'll try to get the sub-selects to where it only returns one row. But I'm intrigued about selecting them into variables and might re-write it to attempt that.
11.1.0.7.0
I tried it as is and errors out with: ORA-00928: missing SELECT keyword 00928. 00000 - "missing SELECT keyword" I'm trying to rewrite it with the selects into variables; but if that doesn't pan out I'll read up on pivot. I appreciate the help!
That's not very clear. Can you try explaining it again. It looks like that if you're 2012ing it, you go for lag and lead. Of you're 2008, do a good old row_number style simulation first. 
Yes, it's actually a datetime but I didn't want to clutter it by typing out the full dates. 
I'm in 2008 (I added a note). 
I believe this gets where you are going: declare @t table (m datetime, v int) insert @t (m, v) values ('20120101', 0) , ('20120201', 2) , ('20120301', 1) , ('20120401', 2) , ('20120501', 0) , ('20120601', 1) , ('20120701', 0) , ('20120801', 2) , ('20120901', 0) , ('20121001', 1) , ('20121101', 2) , ('20121201', 0) select t.m, t.v from @t t select datename(month, t.m) [Month] , substring(datename(month, t.m), 1, 3) [Mon] , t.m , t.v from @t t where not exists ( select 1 from @t tx where t.v = 0 and tx.v in (0, 2) and dateadd(month, 1, t.m) = tx.m ) 
Close. Make Aug a 0 and Sep a 2 and you get Jul, when it should skip Jan, Jul &amp; Aug.
Slight modification then. I'll update my previous post then.
A couple of things: If you want any sort of true static date ranges, you need a date table that defines your ranges. This would typically be like this, but adjust it for your needs. Note the CHECK constraint to give some basic validity to each value: CREATE TABLE [dbo].[Calendar] ( [Date] [datetime] NOT NULL, [Year] [int] NOT NULL, [Quarter] [int] NOT NULL, [Month] [int] NOT NULL, [Week] [int] NOT NULL, [Day] [int] NOT NULL, [DayOfYear] [int] NOT NULL, [Weekday] [int] NOT NULL ) ON [PRIMARY] GO ALTER TABLE [dbo].[Calendar] ADD CONSTRAINT [CK_Calendar] CHECK (([Year]&gt;(1900) AND ([Quarter]&gt;=(1) AND [Quarter]&lt;=(4)) AND ([Month]&gt;=(1) AND [Month]&lt;=(12)) AND ([Week]&gt;=(1) AND [Week]&lt;=(53)) AND ([Day]&gt;=(1) AND [Day]&lt;=(31)) AND ([DayOfYear]&gt;=(1) AND [DayOfYear]&lt;=(366)) AND ([Weekday]&gt;=(1) AND [Weekday]&lt;=(7)))) GO ALTER TABLE [dbo].[Calendar] ADD CONSTRAINT [PK_Calendar] PRIMARY KEY CLUSTERED ([Date]) WITH (FILLFACTOR=100) ON [PRIMARY] GO and populated like: declare @FirstDate datetime, @LastDate datetime select @FirstDate = '20120101', @LastDate = '20150101' begin with CTE_DatesTable as ( select @FirstDate AS [Date] union all select dateadd(dd, 1, [Date]) from CTE_DatesTable where dateadd(dd, 1, [Date]) &lt;= @LastDate ) insert [dbo].[Calendar] ( [Date], [Year], [Quarter], [Month], [Week], [Day], [DayOfYear], [Weekday] ) select [Date] , datepart(year, [Date]) [Year] , datepart(q, [Date]) [Quarter] , datepart(month, [Date]) [Month] , datepart(wk, [Date]) [Week] , datepart(day, [Date]) [Day] , datepart(dayofyear, [Date]) [DayOfYear] , datepart(weekday, [Date]) [Weekday] from CTE_DatesTable option (maxrecursion 0) end Next, even when you have a date table with ranges, if you need to have a query similar to the one you are using to calculate fortnight, I'd suggest something simpler: select min(c.[Date]) [MinDate] from [dbo].[Calendar] c where c.[Date] between '20130101' and '20131231' group by floor(datepart(week, c.[Date]) * .49) Pro-tip: use ISO8601 datetime when possible, or better use the simple date version. '1-Jan-2013' is '20120101' because there is less ambiguity.
This is incorrect. Consider this: declare @d datetime select @d = getdate() select @d, @d - 0.1, @d - 1 
In SSMS, the common shortcuts I use are: * CTRL-K, CTRL-U is to uncomment a section * CTRL-K, CTRL-C is to comment a section * CTRL-SHIFT-U to UPPERCASE * CTRL-SHIFT-L to lowercase * CTRL-R to toggle the results window * F6 jumps from the query window -&gt; results -&gt; messages -&gt; exection plan -&gt; query window * Tools...Options...Keyboard. Fill out your shortcuts. I use [these](http://pentonizer.com/sql-server/ssms-custom-keyboard-shortcuts/) from my blog. * CTRL-SHIFT-SPACE in a function will give quicktips.
Yes, every second fortnight set should start at the 1st of the month. The example posted is my results, please see my edit for desired results :) Always grouping the first 15 days of the month, then the remaining days is a solution I'm after.
This should do what you want: dateadd(day,case when day(table.[date]) &gt; 14 then 15 else 1 end-day(table.[date]),table.[date])
Oooooh, great idea, thanks.
So row 8, 18, 29, 59, 71, &amp; 72 should be removed from this set. I know there are some cases that start out OnHold so I also need to meet the case where there are any number of 0 at the beginning followed by a 2.
Spot on, NJTraffic - It's a practice called normalization, here is a decent article on the topic http://databases.about.com/od/specificproducts/a/normalization.htm 
Since you want to base this off of the 1^st of the month, you can use this - which I believe is rather elegant in the sense that you can adjust the range by changing a single value, and the meaning is extremely clear. Two examples, one using only a date column and the other using my [previously proposed calendar](http://www.reddit.com/r/SQL/comments/14pplc/mssql_grouping_by_fortnight/c7fazjt) table. **EDIT HERE TO ADD 'initial 15 PLUS' query** **For the 'initial 15 PLUS rest of month' proposition:** select min(c.[Date]) [MinDate] , count(*) [Days] from [dbo].[defCalendar] c where c.[Date] between '20130101' and '20131231' group by convert(char(6), c.[Date], 112) , cast((case day(c.[Date]) when 31 then 30 else day(c.[Date]) end - 1) / 15 as int) order by [MinDate] **Using only the date** select min(c.[Date]) [MinDate] , count(*) [Days] from [dbo].[Calendar] c where c.[Date] between '20130101' and '20131231' group by convert(char(6), c.[Date], 112) , cast((day(c.[Date]) - 1) / 14 as int) order by [MinDate] **With my proposed date table** select min(c.[Date]) [MinDate] , count(*) [Days] from [dbo].[Calendar] c where c.[Date] between '20130101' and '20131231' group by c.[Year], c.[Month], cast((c.[Day] - 1) / 14 as int) order by [MinDate] **Range of 10 days** select min(c.[Date]) [MinDate] , count(*) [Days] from [dbo].[defCalendar] c where c.[Date] between '20130101' and '20131231' group by c.[Year], c.[Month], cast((c.[Day] - 1) / 10 as int) order by [MinDate] 
Thanks for the tip!
I was thinking if there was going to be multiple of both food and intros that you should probably use 3 tables. One for userid, foods, intros. I think I read the question a little different though.
That's where I would have gone too, and I don't think there's anything wrong with it. Some times you can over-normalize though; the joins can get complex. I spend quite a bit of time looking at raw data within SQL, and though I do appreciate normalized data, doing extra joins, just to see what kind of food they like seemed like over kill. The same reasoning goes for the intro. The only downside to not putting intro into it's own table is that if specs/requirements change.
JOIN The thing you are looking for is JOIN.
agree, but the foods table should not have an auto_increment pk_id instead, the primary key should be ( person_id , food ) bonus: you'll never accidentally add the same food for the same person twice 
 Now the obvious problem. If you have "bread" and "bread sticks" and "cheese sticks" all in the same column please find only people with "bread" You must also write the query in such a way that you can deal with other "* sticks" without knowing about them a head of time that might be added later. Good luck :) 
I can appreciate this, but what would you make your clustered index look like, assuming you have one.
This worked... thanks mate
thanks, protip taken 
&gt; please find only people with "bread" WHERE food = 'bread'
You have no join between the SA_ASSET in the update and that in the concatenation records, which may be your problem. Surely there is a relationship? I'm assuming you are referring to the same record? Oracle won't allow this syntax because each sub select could return multiple rows, your approach is best described as a bit bizarre to say the least. I suspect you are trying to pivot specification details for the asset and build a description up. It's unclear where some of your fields come from because they aren't aliased, but try something like this (I haven't checked this as I'm not at work). The pivoting techniques should work on all versions of Oracle 9i and above and this uses a technique called a correlated update statement, there is no where clause on the update, as the correlation occurs in the subselect (via the call b.ASSET_ID = c.ASSET_ID). update SA_ASSET b set b.ASSET_DESC = ( select TRIM(c.ATTRIBUTE1)|| ' - Service Type: PW, Manf: '||TRIM(c.MANFAC)|| ', Model: '||TRIM(c.MODEL)|| ', Main Valve Size: '||TRIM(c.VALVE_SIZE)|| ', Year Manf: '||TRIM(c.YEAR_MANUFACTURED)|| ', Location: '||TRIM(c.STREET_NUMBER_CHAR)|| ' '||TRIM(c.STREET_NAME) from ( select ASSET_ID, ATTRIBUTE1, STREET_NUMBER_CHAR, STREET_NAME, min(case when d.ATTRIBUTE_DESC = 'MANUFACTURER' then d.ATTRIBUTE_VALUE else null end) MANFAC, min(case when d.ATTRIBUTE_DESC = 'MODEL' then d.ATTRIBUTE_VALUE else null end) MODEL, min(case when d.ATTRIBUTE_DESC = 'MAIN VALVE SIZE' then d.ATTRIBUTE_VALUE else null end) VALVE_SIZE, min(case when d.ATTRIBUTE_DESC = 'YEAR MANUFACTURED' then d.ATTRIBUTE_VALUE else null end) YEAR_MANUFACTURED, from SA_ASSET a join SA_SPECIFICATION_DETAILS d on d.SPECIFICATION_NO = a.SPECIFICATION_NO where ASSET_TYPE = 'HYDRANT' and ASSET_STATUS = 'ACTIVE' and UPPER(ASSET_DESC) NOT LIKE '%LOCATION:%' and UPPER(ASSET_DESC) NOT LIKE '%HYDRANT%' and a.ASSET_ID = '006130' group by ASSET_ID, ATTRIBUTE1, STREET_NUMBER_CHAR, STREET_NAME) c where b.ASSET_ID = c.ASSET_ID) 
Expand the "skills measured" and "preparation materials" sections at http://www.microsoft.com/learning/en/us/exam.aspx?ID=70-461 You'll find both a comprehensive list of the tested topics, and the title of a relevant book from Microsoft Press. You could start a free Safari Books online trial and read the book that way. http://my.safaribooksonline.com/9780735667259
 that will not work when using a single column for multiple values ... 'bread,bread sticks' 
7 yrs ago, I got my MCAD/MCSD with self study + those exam preps. This website ( http://www.examcollection.com/ ) would be useful for you.
if there's anything else in the column besides bread, then it won't be what you asked for -- "only people with bread", which i interpreted as "people with only bread" i understand what you were trying to do, though... to show the problems with multiple values in a single column thankfully, in mysql you can also use the FIND_IN_SET function on these types of columns
http://www.amazon.com/Training-Kit-Exam-70-461-Microsoft/dp/0735666059/ You are welcome.
I love the one review from the dude who claims to have bought the book, rammed through all 700 pages in a day, then took the exam and failed it because the book contained irrelevant information. Seems legit. The 70-462 guide was awesome, so I assume the 70-461 training kit is just as useful. 
Have you completed any of this? I will throw a hint or two your way, but I will not do the whole project for you. Start by drawing up an ERD. On paper would be fine, but if you have Visio use that. If this is a college class, your school should have somewhere for you to download the free student version. Once you have an ERD completed, most of this should fall in to place pretty quickly.
1.) Two things just some simple joins, and a curveball with account number since it has a space in it: select a.customername from customer join depositor b on b.customerid = a.customerid join account c on c.[account number] = b.[account number] where c.balance &gt;= 6000 The only one that doesn't follow this pattern is the seocnd one, which is a group on the depositor table like so: Select [account number], count(distinct(customerid)) as customers from depositor group by [account number] having count(distinct(customerid)) &gt; 1
Here is a free program you can make ERDs with. It is designed to create mySql databases, so there are a few minor differences, but it is close enough for what you are doing. http://www.mysql.com/products/workbench/ 
Download something (Access 2003 Portable, LibreOffice Base, ETC) &gt;http://sourceforge.net/projects/sqleo/ &gt;http://easyquerybuilder.com/ &gt;https://www.nubuilder.net/nubuilder.php and do it in there; if you are new to SQL and databases and set theory, you need a visual query builder that will allow you to visualize the relationships between tables. 
The company i work for uses sharepoint to host all of the reports we build in SSRS. 
i am gettting a data type mismatch. here is my code SELECT dbo_employee.fname, dbo_employee.lname INTO DriversImport FROM dbo_employee WHERE dbo_employee.empno &lt; 700 OR dbo_employee.empno &gt; 2500 AND dbo_employee.inact=False 
 SELECT dbo_employee.fname, dbo_employee.lname INTO DriversImport FROM dbo_employee E WHERE CAST(E.empno AS INT) &lt; 700 OR CAST(E.empno AS INT) &gt; 2500 AND E.inact=False make sure empno is infact all numbers
I have known dozens of marketing professionals that use SQL.
error using pass-through query: [microsoft][odbc SQL Server Driver][SQL Server]Invalid object name 'dbo_employee'. (#208)
 SELECT dbo_employee.fname, dbo_employee.lname INTO DriversImport FROM dbo_employee WHERE (CAST(dbo_employee.empno AS INT) &lt; 700 OR CAST(dbo_employee.empno AS INT) &gt; 2500) AND dbo_employee.inact=False
Approximately how long did it take you to prepare? 
This might make it a little easier in the future: Create View EmployeesNumeric As ( Select EmployeeNumber , EmployeeName From dbo.Employees Where isNumeric(EmployeeNumber)=1 ) Go Select * From EmployeesNumeric Where EmployeeNumber Between 700 And 2500
can you confirm that all the ID's are in fact numeric? Does this return anything? SELECT EmployeeID * FROM dbo_employee WHERE isnumeric(EmployeeID) = False If it does, converting or casting ain't gonna work. If it does NOT return any records. Try this ------------------- SELECT dbo_employee.fname, dbo_employee.lname INTO DriversImport FROM dbo_employee WHERE Cint(dbo_employee.empno) &lt; 700 OR Cint(dbo_employee.empno) &gt; 2500 AND dbo_employee.inact=False ------------------------ Cint = Should be MS Access 'Cast/Convert' equivalent to change string to integer. It's been awhile though. 
You just did "Block Comment" and then "Copy". Here are the sequences for a default install of Notepad++ (v6.1.4) * Toggle Block Comment: **CTRL+Q** * Block Comment: **CTRL+K** * Block Uncomment: **CTRL+SHIFT+K** * Stream Comment: **CTRL+SHIFT+Q** You likely want to use *Stream Comment* instead. That gets you: /* this was a stream comment */ at least in my version of Notepad++ EDIT: Note that you might have some keys assigned to different actions. Additionally, the Visual Studio IDE has the concept of a command sequence (multiple shortcut keys, like CTRL+K, CTRL+C) and Notepad++ doesn't have that (by default, at least).
i've seen one such question posted a couple years ago (guy was asking for help with a specific homework problem using sakila) but i'll be darned if i can find any sample questions using search engines
What?
Are you sure you're not talking about this? Tools -&gt; Options -&gt; Query results -&gt; Results to Text -&gt; Maximum number of characters displayed in each column SQL Server will let you have columns with thousands of characters in them. It's probably just getting cut off when you query with SSMS.
Are you sure you used the correct data type? 128 seems oddly specific to me. SYSNAME comes to mind as only allowing 128 chars. The other way to look at this is, are you sure it's SQL server and not whatever language you're coding in?
tried it. Same damn problem :(
show the definition of the table
I agree. It sounds less like a SQL Server problem and more like a language or ODBC driver limitation.
How are you defining "empty date"? You mean NULL date, yes? update t set InDate = convert(char(8), getdate(), 112) from R0_J t inner join inserted i on t.NAME = i.name where t.InDate is null 1. Format your queries for readability up-front. 1. Use the complete **inner join** rather than just **join** 1. Consider using **getutcdate()** rather than **getdate()** as SQL Server doesn't have to apply the timezone offset to the output. 1. Use aliases for your queries. Makes future updates MUCH easier. 1. Consider an **INSTEAD OF** trigger if you don't want to do multiple write operations.
Thanks for the heads up on the formatting, I just did a quick edit for that. I will give your solution a shot, thanks.
The **i** and **t** are aliases, used to help simplify SQL statements. Here is a larger, simplified example for you to check. Don't forget to drop the [test] table after you are done: create table test ( colA varchar(10) , colB datetime , colC int ) go create trigger TR_test_u on test for update as begin update t set colB = convert(char(8), getdate(), 112) from inserted as i inner join test as t on i.colA = t.colA where i.colB is null end go insert into test (colA, colC) values ('t', 1) insert into test (colA, colC) select 'q', 1 union all select '1', 1 select * from test update t set colC = 3 from test as t where t.colA = 't' select * from test 
Ended up with 56% first time, then retook it two weeks later and got 90%! Practice exams that matched the format of the real thing were incredibly useful. The only problem is getting hold of ACTUAL practice exams, because "hey-ho we're Oracle and we want your money. All of it."
Congrats! i'm currently about a 10 year sql server DBA, and a year of support with oracle, the biggest killer with me is the SQL and terminology adapting. but after running through the practice quizzes, seeing what i don't and then re-reading, im getting better. a month or so and i'll be taking them myself.
Dunno FilemakerSQL, but one method is to make use of a function like ISNULL or equivalent. Change UPPER(surname) to ISNULL(UPPER(surname), ''). Then you'd be comparing against empty strings instead of NULLs, where there is no data.
Can we get a sample base schema (what you will build the view from) and desired output? Just remember this: Under the hood, a view is essentially a SELECT query. So (aside from trying to update a view), you need to build that select query and get the desired results before you can create the view.
&gt; I am trying to write a trigger ***cringe***
Besides creating the view, you should also learn what the pros/cons of them are.
What are they?
I'm working on my sql server admin cert right now. I got the first book, and took the test and got a 70 on it. 
I've done the first two Oracle DBA cert tests and am about to take the OCP. They are the hardest certification tests I've taken, by a good bit.
Thank you for this. I have been tinkering around with the thought of OCP (not a DBA) but don't know anyone who has taken it. 
Feel free to shoot me some questions if you want. I'm an OCA currently and should have my OCP in the next month or so.
I've always wondered how someone can get into the oracle scene. Does it start with a love of linux and just falling into the DB scene? Do you have a rich friend how likes giving away oracle licenses?
While I do like working in *nix environments, my experience is actually mostly in Oracle on a Windows platform. I got into it because I have a bit of a background in SQL Server and the contracting company I worked for put me on a project for a client that also used Oracle. In order to use me on both platforms (and increase my bill rate) they sent me to several Oracle University trainings. While Oracle trainings are extremely expensive (I've done about $10k worth of trainings, which cost about 25k with travel and what not), you can download, and practice on, the platorm for free.
Yeah, it is really not an easy test. None of them are. I've been told, by Oracle instructors, that they make the tests excessively hard to keep the certification exclusive and valuable. I know a DBA with ten years experience on Oracle that failed both of the first two OCA tests the first time she took them and had serious trouble with the OCP. If you've already passed the SQL fundamentals test, it would be in your interest to buckle down and finish the OCA. It's a great cert to have and, like you said, life long. I also prefer life long certs, I have yet to bother with any that request renewal. I will say that the Admin I test is harder than the SQL Fundamentals test, as it requires both a knowledge of SQL and a fairly decent understanding of the basic internal workings of Oracle. I personally used a Sybex study guide, an Oracle University class, a Kaplan Self-test engine and a lot of time on my Oracle VMs to prepare. The OCP is a whole different monster beyond that, as it focuses on the advanced internal workings of the RDMBS and all of the ancillary systems. Like I said to the other poster, feel free to PM me about it if you want to. 
I agree with you on that, except the statement that certs don't mean anything to employers. I see that stated a lot and, in my experience, it's just not true. A lot of job listings I see require certain certifications. Certifications levels can also determine your salary in a lot of places. Short term employment contracts often focus heavily on certification requirements. Experience is more important when it comes down to getting the job done, no argument there, but employers definitely care about certification.
I find that it tens to be HR departments that care about the certs. Hiring managers tend to be more interested in what you can do. I haven't done sort term contract work, so you could well be correct there, although when I look to bring people in I always focus on what they have done (contact or fte). 
"You don't join your tables" that's taken care of in the WHERE clause (look at [this example question](http://programmers.stackexchange.com/questions/78225/using-join-keyword-or-not). These are equivalent statements: SELECT ... FROM Times AS t, Lender AS l WHERE l.Lender_ID= t.Lender_ID SELECT ... FROM Times AS t JOIN Lender AS l ON L.Lender_ID = T.Lender_ID "Don't need to use datediff" - DateDiff is a valid option, since we have a simple 'X days after lending". Either of these would generate the same results: Times.[Date of Return])&lt;=[Date of Lending]+15 DateDiff(d, T.[Date of Lending], T.[Date of Return]) &lt;= 15 "understand the Distinct keyword" - Distinct followed with a Group By is redundant. Distinct does a "Group By" all selected fields. Use one, or the other... group by allows aggregate functions (count, average, sum, etc) but distinct is "easier" without the need to match select/group by fields. 
**You have to ask yourself, why was it returning what it did?** You said "The current query gives me separate results, for example, one lender has returned 3 books in due time, but the query gives me one row with 2 books and another row with 1 book counted." The select you had was "grouped by" the lenders name... and "days borrowed" aka ReturnNo IN THE GROUP BY (Ignore the Count in the Select): DateDiff ("d",t.[Date of Lending],t.[Date of Return]) This shows what? How long the book was borrowed (assuming it's been returned - no return date will result in a NULL). Say Jack has borrowed 4 books. He returned 2 books after 3 days. Returned a book after 5 days... returned a book after 20 days... and still has a 5th book (Not paying attention to when he borrowed it). We would have something like: 3, Jack 3, Jack 5, Jack 20, Jack null, Jack You then add your "Where DateDiff is not null". This excludes books not returned yet. So your results are: 3, Jack 3, Jack 5, Jack 20, Jack Your exact group by was basically "(DateDiff() &lt;= 15), Name"... so "Group By True/False/null, Name". Above would have basically shown up as (if we included count(*)): T, Jack, 3 F, Jack, 1 Why? Because there was 3 books lent on time, 1 book late and 1 book still out (which was excluded in the WHERE). There are two groups (since we excluded NULL in the WHERE). ... Not sure if that made any sense. You query was thrown off with the "Group By (DateDiff() &lt;= 15)" statement. **For your original query**... You probably are looking for something closer to this: SELECT t.Lender_ID, l.[First Name], l.[Last Name], l.[Contact Number], l.[Email] ,(DateDiff("d",t.[Date of Lending],t.[Date of Return]) &lt;= 15) as ReturnedOnTime ,Count(*) Counts FROM Times AS t, Lender AS l WHERE l.Lender_ID=[t].[Lender_ID] AND DateDiff ("d",t.[Date of Lending],t.[Date of Return]) IS NOT NULL GROUP BY t.Lender_ID, l.[First Name], l.[Last Name], l.[Contact Number], l.[Email] ,(DateDiff("d",t.[Date of Lending],t.[Date of Return]) &lt;= 15) ORDER BY t.[Lender_ID]; Simplified, and in logical order: FROM tables WHERE book as been returned GROUP BY Lender, ReturnedOnTime Select Lender, Name, ReturnedOnTime, Counts OrderBy Lender Hope that makes sense.... it adds the "returned on time" to the SELECT to make it more apparent, as well as adds a count. If you want to only select people with a book returned on time? WHERE l.Lender_ID=[t].[Lender_ID] AND DateDiff ("d",t.[Date of Lending],t.[Date of Return]) &lt;= 15 Only people with a late book? WHERE l.Lender_ID=[t].[Lender_ID] AND DateDiff ("d",t.[Date of Lending],t.[Date of Return]) &gt; 15 people with a book still out? WHERE l.Lender_ID=[t].[Lender_ID] AND DateDiff ("d",t.[Date of Lending],t.[Date of Return]) IS NULL 
Hi, I think we generally agree on most things, but some of the things you are saying… I haven’t said. I would like to clarify. I understand that JOINS and WHERE can be equivalent in many cases, and are in the supplied example. In my experience, JOINS are used to illustrate the relationship between tables, and WHERE is use to limit the data you want back. These websites says it better than I am. JOIN is the convention, basically, for readability, and to avoid subtle bugs from using WHERE with NULL. &gt;http://programmers.stackexchange.com/a/78229 &gt;http://stackoverflow.com/a/894659 Datediff is a valid option. I did not say it was invalid, I said it wasn’t needed. If you are making a VBA call inside an SQL query, you are making it harder to debug, reducing performance, and making it more difficult to maintain. Let’s say you want to hand this off to a rookie analyst. Instead of just SQL, you need to go into the Code Editor to use IntelliSense with Datediff. About DISTINCT… I’m not exactly sure what you are saying here. Could you please clarify? My interpretation of the question is that OP wants all LenderIDs who are compliant with the 15 day limit. The point I’m making is that if he wants perfect compliance, he probably needs to exclude all the unique LenderIDs that are not-compliant, instead of listing all the LenderIDs that are compliant. (Distinct does a Group By all selected fields and returns a unique record. If there are multiple books out by the same LenderID, without DISTINCT you could create a situation where the query returns multiple LenderIDs because multiple books were returned on time. ) If you could explain your point with regards to DISTINCT, I admit I might be missing something and I’d like to better understand. S
Yeah... I do think I'm talking semantics and generally going all nerdish... Nothing like some time off to make me crazy :) Joins~ The INNER/OUTER/LEFT/etc JOIN syntax is much more readable, and what I was taught, so I always get thrown off with how much I see the "other" way of doing it. DateDiff~ Not sure this is a "VBA Call". If it is, it's a common one (DateDiff apparently works in Access... I use it all the time in SQL Server. MySQL, DB2, Oracle all have similar options - if different). I'm fairly certain these are built in functions. Personally, I also find the datediff syntax easier to read than the other syntax, although it may be a matter of preference. DateDiff(d, Start, end) &lt;= 15 vs End &lt;= Start + 15 Distinct vs Group By~ DISTINCT selects distinct variations within the selected fields. GROUP BY does the exact same thing. They both result in [the same query plan](http://stackoverflow.com/a/427064). I've always approached DISTINCT as a "short cut" for a GROUP BY that doesn't have aggregate functions used. [This Example](http://blog.sqlauthority.com/2007/03/29/sql-server-difference-between-distinct-and-group-by-distinct-vs-group-by/) shows it well: Example of DISTINCT: SELECT DISTINCT Employee, Rank FROM Employees Example of GROUP BY: SELECT Employee, Rank FROM Employees GROUP BY Employee, Rank Example of GROUP BY with aggregate function: SELECT Employee, Rank, COUNT(*) EmployeeCount FROM Employees GROUP BY Employee, Rank The first two produce the exact same result. The third adds a count (which is not possible with a "Distinct", thus the reason to use a GROUP BY). Using DISTINCT and a GROUP BY is redundant. I use DISTINCT to get rid of the extra fluff in the GROUP BY.
Which version of MSSQL is it? You may want to look into creating a Change Data Capture job. http://msdn.microsoft.com/en-us/library/bb522489(v=sql.105).aspx
I did not know that about DISTINCT vs GROUP BY. Fun fact: Access adds GROUP BY statements to all select queries. I think you're an SQL Server guy and I'm an Access guy. We should all get along, mainly because I want to be you when I grow up ;) Happy Holidays!
I am wonder if you are confusing (localdb) this with "(LOCAL)". When you connect to the server in SSMS, exactly what are you entering into the "Server Name" field? Try putting in (local) and see if that helps. Do you actually have an instance named "LOCALDB"? Check your named instances .... Open the SQL Server Configuration Manager from the Start menu, browse to SQL Server Network Configuration on the left pane and see what named instances show up. Feel free to link to screen shots of what you see.
I believe it's SQL server 2005, but I would have to check with the admins for that server. This might be a possible solution though. Thanks
I've looked as SSIS as a possibility as well as it would implement the first option I listed. I'll probably end up going this route. 
oh my god this is a world of pain. So there's 14 different syntaxes for this? There go my dreams of abstracting SQL away from my code :$ I'm trying to catch up with programming (mainly as a hobby) and... ehm... I'm a bit dazed by all this. Thanks anyway!! I hope to become more active here soon
At a higher level, whats the best way to implement this over a session? For example browsing a fourm, if you used LeTroniz's method, each time you hit next page the database would have to sort the entire table and then give you back the ten rows in that range. Is this the most effective method?
Are you sure? I scored full marks on sql 2008 dba exam and scored 1000 points
Pagination is best done at the driver layer. ODBC / JDBC have methods that let you set the result size and/or row offset that you want to fetch. I've used this with great multi-platform success. No need to change queries around, and the systems I've used show much better performance when the database knows how many rows you are expecting to use.
Well, that's part of the fetchness. At least with Sql Server. anyway... Sql server knows that you only want the certain number of rows from the fetch statement. So it takes your offset number, and adds the fetch number and pulls a top statement for you. Its great from an optimizer standpoint, since you are working at cutting the result set at every step... even though you are basically pulling the entire table... kinda, on the last 'page' Where as the rownumber has to sort everything, order it up, then apply the filter to get the page. Usually 3 more steps and more scanning compared to fetch. It still boils down to how you have the table laid out and if you are indexing correctly. I've also simplified a lot of things i bet in this post, but these are my observations when using fetch.
ya, you just happened to have a test with 1000 points :D It happens. Congrats on the pass btw. http://www.microsoft.com/learning/en/us/certification-exam-policies.aspx#item-ID0EPAAAAEAAAAABA That's a definite source of the scoring thing. bleh, microsoft changed the cert site again... i'm not sure for the better yet. its hard to find study points when looking up certain certs.
MSSQL certified here. I used MSSQL for many years before boss suggested I get certified (purely to tick some company compliance boxes, yawn) I dont think the exams are particularly difficult BUT I would recommend you view some past papers. Often the questions are rather ambiguous, so there is a certain knack to identifying what the question is trying to test you on. 
jobs in marketing or consulting are becoming more demanding because they are getting easier. Its not about just knowing your field anymore, is about knowing how to use the tools that let you work more efficiently. All of our market research people know how to work with data, and so should anyone considering a career in any sort of marketing or research consulting
sharepoint licensing for internet accessible pages is VERY EXPENSIVE. If you aren't paying or it microsoft will audit you soon. If you are paying for it, you are getting ripped off. :)
Aha. But then you're a BA you're not working in the Marketing Department. I think there's some confusion going on surrounding job roles and their definitions here. My assumption was and is (with 10+ years in a corporate environment) that those in Marketing do not do any sort of Analysis. I have never seen a BA in Marketing, just like you don't see Developers in Marketing.
Do you know about [this?](https://class.stanford.edu/db/Winter2013/preview/)
This is great and it stars in 14 days. I looked about half of the course just a few weeks back on coursera where it's self study, and it's really good. I joined now because I want to interact with others and finish the full course. I definitely would recommend it.
Some code might help, along with your table schema
Right, sorry. I added to the original question. Hope this helps with figuring out what I want to achieve.
remove the "null" insert into Artists values ('Jack'); insert into Artists values ('Fred'); insert into Artists values ('George'); You don't put anything where auto-implement or other "calculated" fields would be. Could also do: insert into Artists (ArtistsName) values (@Name[@Counter]);
Ok so the null doesn't need to be there, but having to make x amount of insert statements its not going to work, as I have a variable amount of artists to insert, could be anything from 5 to 1000. Thought I suppose I could make a non-sql script that would generate the insert statements, I was rather hoping to learn some more SQL though.
Unfortunately SQL Server does not handle arrays or lists with any kind of internal structure. You can pass in a list of names but will then have to use a split string function to parse the data and write it to a table variable. You can then iterate through the records in that variable and perform the insert. I highly recommend checking out the [Splitting Strings post on SQLPerformance](http://www.sqlperformance.com/2012/08/t-sql-queries/splitting-strings-now-with-less-t-sql) for examples on how to do this. One other thing to note, for your insert statement it is a best practice to specify the columns you want to insert to. Right now you would get an error as you are not going to insert anything into the ArtistID column as it is an identity column (you can specify values to insert here, but would need to set identity insert on first). So it would be INSERT INTO Artists (ArtistName) values (@name); Hopefully this will get you most of the way there, let me know if you need more assistance.
If the data is currently in a comma separated list you could use a table function to split the result into a set. The one I use relies on a tally table but the below is a similar sort of idea which should work: CREATE FUNCTION [dbo].[fn_split](@arr AS NVARCHAR(2000), @sep AS NVARCHAR(100)) RETURNS TABLE RETURN( WITH Numbers AS ( SELECT TOP 2000 ROW_NUMBER() OVER(ORDER BY sc1.NAME) AS N FROM Master.dbo.SysColumns sc1 ) SELECT SUBSTRING(@arr, n, CHARINDEX(@sep, @arr + @sep, n) - n) AS element, ROW_NUMBER() OVER (ORDER BY (SELECT null)) as pos FROM Numbers WHERE n &lt;= LEN(@arr) + 1 AND SUBSTRING(@sep + @arr, n, len(@sep)) = @sep ) Source : [here](http://social.msdn.microsoft.com/Forums/en-US/transactsql/thread/e9a031da-9578-4c30-8b6e-a2de29c9534a) Once that's been created, you can do something simple like this: DECLARE @name NVARCHAR(2000); SET @name = 'The Beatles,Björk,Nirvana,Johnny Cash'; INSERT INTO [dbo].[Artists] (ArtistName) SELECT S1.element FROM dbo.fn_split(@name,',') AS S1; The above is limited to a string of 2000 characters though, you could alter the function to accept longer strings if necessary (the function uses the syscolumns table to get a list of numbers which is a bit messy but you should still be alter the TOP 2000 to TOP 4000 without issues). Obviously if your names have commas in them then you'll come unstuck and have to use a different separator, e.g. pipe or something. 
As an alternative to my above function, you can also (ab)use the xml datatype a bit. DECLARE @x XML DECLARE @sep NVARCHAR(10)=','; DECLARE @str NVARCHAR(max) SET @str = 'Guns and Roses,Kate Bush' SET @x = '&lt;i&gt;' + REPLACE(@str, @sep, '&lt;/i&gt;&lt;i&gt;') + '&lt;/i&gt;'; INSERT INTO dbo.Artists (ArtistName) SELECT LTRIM(x.i.value('.', 'VARCHAR(255)')) AS Item FROM @x.nodes('//i') x(i); 
[This](http://www.sommarskog.se/arrays-in-sql-2005.html) is the ultimate guide to string parsing for arrays in SQL Server.
Your post title indicates [MSSQL] which is SQL Server. I think you can understand the confusion.
Well, honestly this IS learning SQL :) Just learning the limitations and the proper places for programming (like PHP or C#) and for sql. Using the language of choice, you would create [CRUD Operations] (https://en.wikipedia.org/wiki/Create,_read,_update_and_delete) that meet your particular programs needs. Make sure to use [paramaterized queries](http://stackoverflow.com/a/60496) when you use programmatic sql inserts. Wouldn't want to create [SQL Injection Vulnerabilities](https://en.wikipedia.org/wiki/SQL_injection) - aka [Bobby Tables](http://xkcd.com/327/)
Oh crap your right... I miss read it lol
That is true. I guess I was mostly hoping to learn how to make stored procedures that would do this kind of operations on strings. Coming from C#, I was something bewildered as to why I couldn't find anything on how to do this. Only to find out it is because it does not support this. Live and learn, thank you :) Edit: This paramaterized queries, (I realize I should look for this myself) do you have any good sources for use in C#/PHP and MSSQL?
I don't understand the confusion. My post indicates MSSQL which as you point out is SQL Server, which is what I am using. My limited amount of code, should be SQL used in SQL Server, T-SQL if I understand correctly.
Ahhh... I misread the post (thought it was MySQL not MSSQL). [Comma Delimited to Table function](http://www.sql-server-helper.com/functions/comma-delimited-to-table.aspx) + Insert Into Select From Try something like this... (First run, CREATE FUNCTION instead of ALTER FUNCTION): ALTER FUNCTION [dbo].[ufn_CSVToTable] ( @StringInput VARCHAR(8000) ) RETURNS @OutputTable TABLE ( [String] VARCHAR(50) ) AS BEGIN DECLARE @String VARCHAR(10) WHILE LEN(@StringInput) &gt; 0 BEGIN SET @String = LEFT(@StringInput, ISNULL(NULLIF(CHARINDEX(',', @StringInput) - 1, -1), LEN(@StringInput))) SET @StringInput = SUBSTRING(@StringInput, ISNULL(NULLIF(CHARINDEX(',', @StringInput), 0), LEN(@StringInput)) + 1, LEN(@StringInput)) INSERT INTO @OutputTable ( [String] ) VALUES ( @String ) END RETURN END GO IF OBJECT_ID('dbo.Artists') IS NOT NULL drop table dbo.Artists CREATE TABLE [dbo].[Artists]( [ArtistID] [int] IDENTITY(1,1) NOT NULL, [ArtistName] [nvarchar](50) NULL, CONSTRAINT [PK_Artists] PRIMARY KEY CLUSTERED ([ArtistID] ASC) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] GO declare @String nvarchar(max); SET @string = 'John,Jason,Jacob'; INSERT INTO dbo.Artists SELECT * FROM dbo.[ufn_CSVToTable] ( @string ); GO SELECT * FROM dbo.Artists
There are no arrays in SQL Server. However there are temporary tables and table valued variables. So the cleanest would be to put your list in a table with a number and value, then pull the value you want using a SELECT where the key is equal to your loop counter. You could also use a cursor but that way lies madness... EDIT: Where is the source data coming from? You can potentially just use OPENROWSET or BULK INSERT to extract it from the remote source or file directly.
I ended up doin the same, this time. I only had 59 names, but I could have an arbitrary amount of names, and I was looking at automating the task.
The source data comes from a textfile, which is tab separated, or comma delimited. I realize I can just import this into the database and work on the data as a table. But I was looking at learning some text manipulation. I guess I should take the path of least resistance and import the file to a temp table, and perform the required text manipulation and then insert the remaining into the main artist table. Edit: Spelling, clarification.
You could just insert it in to the final destination table if it exists, or use a script that creates the table and then does a BULK INSERT to it.
I should clarify. I insert the data into a temp table and then perform a few operations on it, and then use a script to insert the remaining data into the main artist table, for those where the NAME does not already exist in the artist table. 
My company uses them in the data warehouse, but in fairly minor ways. For example, traversing the org structure tree to find out how many employees are in each department, traversing the category structure to find out how many products are in each category, building a common table expression of all the dates in a range to join sales data to. 60% of your grade sounds ludicrous to me though. They are a very small piece of what SQL can do, and not every flavor of SQL even implements recursion. 
We use them for org charts (who works for who?), product catalog structure and engineering bill of materials (assemblies of assemblies of parts). None of it occurs in a data warehouse, though. I've found examples that can process comma separated values within SQL, but I can't wrap my head around them. They're slick to use where applicable (ie, hierarchies) but are trickier to apply to other problems of arbitrary depth.
The product I support uses them extensively to manage the embedded devices for the system. Whether the actual database design is all that great or not is open for argument; but, they do get used.
Due to preformance issues I tend to avoid recursive logic whenever possible, but there are some areas where it's unavoidable. If I were writing the course, query optimization would be the 60% of the final grade. An emphasis on cursors and the like seems to be moving in the opposite direction. 
I do the data warehousing for an MLM company and I use recursive queries on a daily basis. They are a primary function of my job. But to be honest I couldn't see myself needing to use them often if I worked for a regular company.
I have a Powershell/SQLCMD script that does what you want to do for the most part. Private message me if you would like the code. 
I just used it for doing cumulative sums (SQL Server 2008R2). Had to use a common table expression and a table variable to squeeze out the proper amount of performance. Usually it comes about from a trade off during data modeling. It's been very uncommon for me though. I've probably only had to write them 3 times in the last 4 years but it has solved those issues in a nice manner.
Yes, they're used all the time. What other things that you're learning in college do you consider to be "academical wastes of time"?
Other than uses listed so far. For MS SQL server I also use them to generate a quick date table. I've also used them for parsing text fields into words on individual rows so we could query the most common occurring words from a notes field. This isn't fast, but it works when needed. I've also done just the opposite - take several rows of data and combine them into a single field. If you want to play around with them you can also implement common recursion algorithms such as Fibonacci or the 3n+1 problem.
usually it requires multiple *index* scans of a table. I use recursive queries quite a bit (org charts, menu structures for applications, projects... ) sometimes on potentially large tables. The key though is to make sure you know what your expected result size is, that indexes can be used, and what depth is involved. Even on a large table, a small result set to a small depth (say no more than 5) shouldn't be that bad if appropriate indexes are in place. Millions of records to a depth of 1000.... no.....
I had to laugh at this: &gt; Sometime earlier this year, a decision was made at the upper levels of the company to remove all Support team access to all production databases, and grant production access exclusively to the developers. Ok, I am a developer and I think this is backwards. The developers should have access to staging dbs and the like and give the support personnel access to the production db's.....
I use a combination of windowing functions, recursive CTE and a table variable. I can skip the last 2 only if I'm using SQL Server 2012 where window functions were updated to allow for cumulative sums natively.
What you're asking has nothing to do with Linux / UNIX and frankly, has nothing to do with databases either, and certainly not SQL. Any application should cleanse data when picking it up from one place and moving it somewhere else. You can do it with .NET scripts (which are supported on almost all platforms via mono) or with any programming language / framework of your choice. Regarding your question about cleansing a CSV file, you can do it with any programming language of your choice. I suggest x86 assembler or obfuscated-C for the best results. 
The best answer you should get is "It depends". Conceptually, it's all the same. For instance, Oracle has it's Data Integrator, then there's open source stuff like the data integration piece of Pentaho. You go the old school route of some grep/sed/awk (tradtional linux command line apps) combination to meet your needs. Unfortunately, your question is way too broad to get a really good answer..
I was recently on a SQL Server course a few weeks ago and I lost count of how many times "It Depends" was mentioned :) Anyway, I know this is a very open ended question which isn't very helpful, I just wanted some examples of how data cleansing was performed in the linux/unix environment. I'm aware that these environments have a plethora of tools that can be used to perform these but being new to it a lot of them i've never heard of. Thanks for mentioning sed and awk though, I was aware of grep but not these two.
Yeah, the thing is, if you have an organization of people who know what they're doing, then recursive sql isn't that much of a threat. However, if you have even one novice coder who is likely to imitate code without understanding what is going on in the deeper engine, you have a performance issue waiting to happen. Which is why you want to have code reviews as a first line of defense, and a strong cultural discouragement of doing things like recursive queries as a second line. *Encouraging* recursive queries is right out. 
The IN clause is just a short hand way of using OR So when the statement compiles it turns it into 99 OR statements. I would ensure there is a clustered index on ID. Additionally you may want to change your WHERE clause to WHERE id &gt; 0 and id &lt; 100
Not entirely sure if this is what you are looking for but could you do a NOT EXISTS with a query that finds all machines with play in 1999?
Sweet! That worked. Thank you very much.
No problem!
Well, the ids are not realy sequential. Bad example by me.
The first query takes about 2 seconds, while creating the table, inserting, selecting and dropping takes about 0.08 seconds. The example was slightly wrong, it is about 30 ids in the IN part, and the matches table holds about 1,500,000 rows. The id is the primary key.
i'm not buying your statement that the id is primary key primary keys by definition are unique, so when you have GROUP BY id, you will get a count of exactly 1 for each id do an EXPLAIN on the original query and you'll probably see a table scan, which will indicate that there's no index on id
 What index's do you have and number of rows in the table? The really advanced method for doing this is to use an index'ed view. Not Support by mysql / postgres. you create a view with the query then put an index on the view. Thus the values are pre-calculated during table updates / inserts (Performance hit here) but the read performance will be really quick. Note: You can simulate the above with triggers as well with an extra table but the admin overhead sucks. 
Yes, I will get a count for exactly 1 if the id exists. Thats all I am interested in, whether the id exists or not. I've never really used explain before. I just assumed that MySQL did the job with indexing Primary keys and Foreign keys. http://pastebin.ca/2299444 for the result of Explain. soccer_matches = matches in my example. Also adding the Create statement for the soccer_matches table. http://pastebin.ca/2299445 I'n not sure how interested you are in getting to the bottom of this. If you want to, I can share a file with all the data in the table and a real example query you can try out yourself.
Thank you for the input, however, I am not confident I actually understood what you meant here.
&gt; whether the id exists or not wow, that wasn't at all obvious from your initial post, nor was the fact that id is varchar(16), but never mind... and i don't understand why the EXPLAIN said that the key_len was 18... it does appear that it's using the index, but try the query again without the COUNT() and the GROUP BY, and then compare the rows returned to the list of ids submitted (which is what you have to do anyway, because you'd never have gotten a count=0, eh)
&gt; Select distinct(m.serialnum), ... **please** don't do this DISTINCT is ~not~ a function, and putting parentheses around the first column that comes after it has no effect on how it actually works, and the downside is that other people might be misled 
good point well made. I just copied and pasted the orignal query. Should have spotted that. My bad
select * INTO #myNewTempTable From TableI'mGettingDataFrom
Oops. My formatting got eaten. C'est la vie. Anyway, the important part of this is the INTO portion of the query. Now, to my knowledge this only works when you're populating a table from an extant table, but that might not be true. Do your homework, but if you have source data, select...into works wonderfully, and quite quickly.
Sorry, if I was unclear in my initial post. I really did not expect this to be an issue with the indexes, but I assumed rather that using IN was a slow lost cause. I've tried to change the request and use explain as you said. http://pastebin.ca/2299453 You're right that I don't get count=0, and I don't need it either. Sorry for not providing this information in the initial post. Thank you for taking the time to look into this. This is by no means crucial for me, this is used in a batch script that will run rarely. I just thought it would be fun if it could run in 5 minutes rather than 30.
Thx for the input, this was sort of what I was looking for. However, my data does not come from another table. So what I am trying to do will be something like this: SELECT * INTO tmptable FROM ("A", "C", "G", "J", "Y", "M") But I can't seem to get it working. It might not be possible because this way there is no structure information, its just data. However, when reading from another table one can read type information etc. from that table.
So there's that, answered. I imagine that if you're importing lots of data, though, it might be more time-efficient to do a massive import into a temp table first, and then a select...into, so you don't have to spend a bunch of time doing that by hand.
your new EXPLAIN now says it's using the league_id index... wtf? in any case, the most important thing you've said is "run rarely"
no prob... that's just one of my pet sql peeves 
I took notice of that as well. I might just rebuild the entire database to make sure everything is correct. Thx for all the input.
it has everything to do with linux and databases. Thats the whole point of his question
 SELECT * FROM TABLE1 JOIN TABLE2 ON (TABLE1.value1=TABLE2.value1) WHERE TABLE1.date &lt; 'mm-dd-yyyy' AND TABLE1.date &gt; 'mm-dd-yyyy' AND TABLE1.status = 'a' You get the gist. 
But this is true with all SQL is it not? A novice coder who copies and pastes SQL without understanding what the db is going to do with it may make stupid performance-killing mistakes with anything. I was once brought in by a customer to track down a performance problem on a Perl/CGI/PostgreSQL application (one I did not write). Performance was fine for small sets but for their case of millions of invoices, it just was horrible. I discovered in my troubleshooting that the original programmer was retrieving all invoices from the db and checking in the middleware to see which ones were paid. On a more subtle note, I have been hit with cache miss issues where something worked fine in testing a small set but failed when managing thousands of invoice payments at once. Had I been less experienced or not had access to the gurus I had access to, this would have been very difficult for me to solve..... Recursive SQL poses some dangers but not more than stored procedures and those dangers are even present in the most basic SQL.....
I can't imagine why changing the SuppName would cause this to not work unless Name 2 doesn't exist in the stock table. Or there's a missing stock code. I'm sure you would've caught both since you've been checking the data. Could there being trailing spaces behind Name 2? WHERE TRIM(st.SuppName) = 'Name 2' I've ran into this in certain tables.
Is the field a CHAR() datatype? CHARs get padded with spaces so they match the length defined. There are some server options which also affect how CHAR fields are compared too, although I think with default settings comparing 'a ' to 'a' would match for a CHAR field. Either way, you might prefer to use RTRIM so its only truncating trailing spaces, instead of both leading and trailing.
Ah....its VARCHAR...would that do the same thing? That would explain it as Name 2 was shorter than the defined length and Name 1 wasn't.
VARCHAR would not automatically pad with spaces. My guess is that some bad data snuck in there somehow. Not sure what the data source is, but you should be sure you're trimming data before inserting.
Hm strange. I'm the data source, just typing it into the insert page on phpmyadmin. I'm only using test data atm. Thanks I will bear in mind when I get round to coding the inserting data part of the site.
Next time it fails, try pasting it into a hex editor, so you can see exactly what bytes you're pasting. Its possible you were pasting some duff bytes which weren't printable, but were affecting field length.
Take a look at (or give us a copy) of the output from the following: SELECT HEX(st.SuppName) FROM sales sa INNER JOIN stock st ON st.StockCode = sa.StockCode WHERE TRIM(st.SuppName) = 'Name 2' ; That should give some insight as to what you have stored there.
check out pentaho data integration, open source, java.
Very interesting. HEX(st.SuppName) 20424F5353 This was the result. Name 2 was "BOSS" in the database btw.
So you've got a space before BOSS, the string stored in the field is actually " BOSS". Must have happened when you inserted the data. If you were hand typing the queries it was a manual typo, if you are loading from a source be sure to TRIM() your data on the insert in the future.
Yo, I too increased my skill set by doing everything this website had to offer: http://sqlzoo.net/ Major contribution to landing my current role.
Also does this already exist anywhere? If it does great and I'll just subscribe, but I thought this would be a worthwhile subreddit since optimization in itself can be a hell of an endeavor, and IMO deserves its own subreddit. 
Given that /r/sql has a post from 25 days ago still on the front page, I doubt there is the traffic to support a whole extra subreddit for a specific sub topic. 
Like Stuart up there said, probably interest, but not enough content. At the very least I'd suggest you crosspost anything on that sub to this one, simply because this sub doesn't really have a lot of traffic in the first place.
Alright ... I'll kick something off then. I think WITH (NOLOCK) (MSSQLSERVER) is an excellent way of querying popular (in the sense that locks are obtained upon them often) tables without causing table contention, providing that the data returned is not required to be precisely accurate. Discuss.
Suggestion for the sidebar: Here's the text that I have for cut'n'pasting in StackOverflow SQL optimization questions. &gt; **You need to show us the table and index definitions.** Diagnosing slow queries requires full table and index definitions, not just a description or paraphrase. Maybe your tables are defined poorly. Maybe the indexes aren't created correctly. Maybe you don't have an index on that column you thought you did. Without seeing the table and index definitions, we can't tell. 
Fair point. I was thinking more for ordinary run-of-the-mill data in structured tables with defined non-LOB datatypes. I've not seen the issue you describe but it sounds interesting. I raised this topic since the SQL Server 'literati' are often disparaging of NOLOCK and this view seems to be propagated through other SQL practitioners, often without a full understanding (or genuine original opinion, in fact) of the arguments for and against. I'd quite like to hear some original opinions backed by fact - yours is the first in a long time.
While I agree with many of the other posters that there may not be sufficient content to make it a big community, I've gone ahead and added a link in the sidebar for you anyway. I hope you'll continue to post to /r/SQL as well, of course.
Cool. I'll be working on the sidebar/etc after work today. Thanks!
 Select c.* From Customers c Join ( Select s.custId, count(s.custId) cnt From Sales s Join DealProducts p -- Table of products in the promotion On s.productId = p.productId Group By s.custId Having count(s.custId) &gt; 1 ) q On c.custId = q.custId
 -- create &amp; fill temp table create table #sales ( id int identity, ticket_id int, item_id int, quantity int ) insert into #sales (ticket_id, item_id, quantity) select 1, 1, 1 union all --ticket with one item select 2, 1, 1 union all --ticket with two items from the list select 2, 2, 2 union all select 3, 1, 3 union all -- ticket with two items, only one from the list select 3, 12, 1 -- do the search select ticket_id from #sales where item_id in (1,2,3,4,5,6,7,8) group by ticket_id having COUNT(*)&gt;=2 -- cleanup drop table #sales Results: (5 row(s) affected) ticket_id ----------- 2 Ticket 2 is the only one which contains 2 or more items from the list.
upvote for doing the work to create a repeatable test case
Any luck?
Not in front of a computer w/ SQL, so this is winging it: SELECT CustomerIdentifier FROM MyTable GROUP BY CustomerIdentifier HAVING SUM( CASE WHEN Item IN (1, 2, 3, 4, 5, 6, 7, 8) 1 ELSE 0 END ) &gt;= 2 Edit: Formatting 
Note that this only works if multiple sales of the same item on a ticket are represented by 1 entry as it is in his sample. If there can be more than 1 entry on a single sale ticket for a single item this query will include it. SQLDave's solution will handle this case. 
i don't see why people would force their database to do all this overhead work that can be managed by 3rd party programming language.
yeah, sure, just dump the entire table out... downvote
Turns out I don't work there anymore so it has fallen by the wayside. I did find some articles on how to do it but never got a chance to try it out as it wasn't the highest of priorities. Sorry....
Not sure what you're trying to do, but Oracle does not support identity columns. Personally, I wouldn't bother with adding precision to an ID column, and would replicate identity column functionality by creating a sequence and have an insert trigger on the table to use the sequence value for the ID and advance the sequence. Then if you wanted to add bounding to the ID's or whatnot, you can do that by configuring the sequence. I know it's not what you're asking, but perhaps it's where you're going... 
Hey, thanks for the input! I am basically just trying to figure out if it is better to declare the primary key id column as number or number(p,0). I want to fix some of the terribleness of an old system and I have been tasked with creating a new table and I wanted to do it the best way possible this time around and then alter the wrongness on the older tables. I will use a sequence.
I've also tried: UPDATE JM_TEMP_010713 A SET A.NPI = ( SELECT B.NPI FROM JM_TEMP_010713_NPINA B WHERE A.UNIQUEID = B.UNIQUEID AND A.UNIQUEID IN ( SELECT B.NPI, A.UNIQUEID FROM JM_TEMP_010713_NPINA B JOIN JM_TEMP_010713 A on A.UNIQUEID = B.UNIQUEID AND A.NPI IS NULL ) );
&gt; the update statement I'm trying to run keeps over-writing the values I had before What do you mean? That's what an update does: Update values. Can you give us a very simplified example (with data) of what you're seeing and what precisely your issue is (as well as what you're trying to accomplish)?
I'm trying to take NPI values from two tables; JM_TEMP_010713_NPINA and JM_TEMP_010713_NPIPH; and add them to a master table JM_TEMP_010713. The problem is arising when I run the second update statement, it erases the NPI values from the first update statement. UPDATE JM_TEMP_010713 A SET NPI = ( SELECT B.NPI FROM JM_TEMP_010713_NPIPH B WHERE B.UNIQUEID = A.UNIQUEID ); --a bunch of code UPDATE JM_TEMP_010713 A SET A.NPI = ( SELECT B.NPI FROM JM_TEMP_010713_NPINA B WHERE A.UNIQUEID = B.UNIQUEID AND A.NPI IS NULL ); The end result should be 62 NPI values. Instead, I'm only getting the 12 from the second statement.
Is this running in a transaction? Does "a bunch of code" include anything that would roll back the update (implicitly or explicitly)?
the two tables with new NPI values are the results of some fuzzy match processes. The code in between I use the newly found NPIs to pull data from other tables. 
Can't really help you too much until you can provide a simplified reproducible case that we can look at. Can you provide a small series of statements that demonstrate the problem?
 UPDATE MASTER_TABLE A SET NPI = ( SELECT B.NPI FROM FUZZY_MATCH_1 B WHERE B.UNIQUEID = A.UNIQUEID ); --run these statements while second fuzzy match is running UPDATE MASTER_TABLE A SET MENUM = ( SELECT B.MENUM10 FROM PBANIEWICZ.NPI_ME_BRIDGE B WHERE A.NPI = B.NPI ); UPDATE MASTER_TABLE A SET PID = ( SELECT B.PARTID FROM PBANIEWICZ.AMAPHYS B WHERE A.MENUM = B.MENUM10 ); UPDATE MASTER_TABLE A SET EMAIL = ( SELECT B.EMAIL FROM PBANIEWICZ.AIEMAIL B WHERE A.PID = B.PID AND B.EMAILVALID = 'VALID' AND B.UNSUBSCRIBED = 'N' ); UPDATE MASTER_TABLE A SET MPA_PHONE = ( SELECT B.PHONENUMBER FROM pbaniewicz.mpaphone B WHERE A.PID = B.PID AND B.PHYSINFOID = '1' ); UPDATE MASTER_TABLE A SET AMA_PHONE = ( SELECT B.PHNNUM FROM pbaniewicz.AMAPHYS B WHERE A.PID = B.PARTID ); --second fuzzy match is ready UPDATE MASTER_TABLE A SET A.NPI = ( SELECT B.NPI FROM FUZZY_MATCH_2 B WHERE A.UNIQUEID = B.UNIQUEID AND A.NPI IS NULL ); after that last statement the NPI values from FUZZY_MATCH_1 are lost. Thank you for the patience I've been staring at my screen mumbling in confusion for about 2 hours so I not english good right now.
 UPDATE JM_TEMP_010713 A SET A.NPI = ( SELECT B.NPI FROM JM_TEMP_010713_NPINA B WHERE A.UNIQUEID = B.UNIQUEID ) WHERE A.NPI IS NULL I don't know much about Oracle, but in sql server, you would actually need to say Update [table] FROM [table] to do the where if i recall correctly
Is the "second fuzzy match" running in another session/transaction? If so your first set of statements here will see the values in fuzzy_match_2 as they existed at the start of the beginning of this transaction when you first update Master_Table to set npi. Try putting a transaction around everything but the last sql (second fuzzy match is ready) then when second fuzzy match is committed, run the last update. Another thought, are you updating NPI in the first query to NULL or Empty String ''? Do some testing just before you run your last update, turn that update into a select statement and select master_table.npi, master_table.uniqueid where master_table.npi is null. How many rows come back. Then do the same select but join it to fuzzy_match2 with your join conditions and see how many rows come back and if it matches what you think should happen. Short of this, you need to give sample data of what master, fuzzy_match1 and fuzzy_match2 have in their tables so we can look at what is going on.
FUZZY_MATCH tables only have UNIQUEID and NPI fields. We use [pervasive software](http://integration.pervasive.com/products/data-integration-platform.aspx) to generate a CSV and another pervasive program to create the tables. The processes themselves take about 3-4 hours so there is some pressure to get some footwork done in between. Basically, the problem is I'm totally unqualified for a good chunk of what I'm doing now and my predecessor who trained me did a lot of hack work arounds. I know this makes it kind of tough to help me since I'm not really speaking the right technical language
HUZZAH. Thank you, this worked perfectly.
What you have right now is good, but I would do it a different way. If your query works TIL you can use common table expressions without a from clause. Anyway, I think what you really want is something that looks like this: YR MTH DAY MAX_RATIO 2012 1 4 .78 2012 2 23 .88 2012 3 12 .53 2012 3 29 .53 ... Is that right? Note that if multiple days have the same ratio you'll get multiple rows. If this is right you'll do this: WITH RATIOS AS ( SELECT X.DATE, xNUMBER/yNUMBER Ratio FROM ( SELECT DATE, SUM(NUMBER) AS xNUMBER FROM X GROUP BY DATE) X INNER JOIN ( SELECT DATE, SUM(NUMBER) AS yNUMBER FROM Y GROUP BY DATE) Y ON X.DATE = Y.DATE ) So now you have each date with the ratio. Now do this: SELECT * FROM RATIOS R INNER JOIN ( SELECT YEAR(DATE) [YEAR], MONTH(DATE) [MONTH], MAX(Ratio) MaxRatio FROM RATIOS GROUP BY YEAR(DATE), MONTH(DATE)) R2 ON YEAR(R.DATE) = R2.YEAR AND MONTH(R.DATE) = R2.MONTH AND R.Ratio = R2.MaxRatio Pull all ratios then inner join on only the max ones for the given month/year. Hopefully this is what you want!
Did my query not work?
Maybe that person does not desire performance and likes to fill the cache with data that is not actually needed?
with rank() windowing function (beware b returning 0, or multiple days sharing a max ratio): ;with a as ( select date, sum(number) as number from x group by date ) , b as ( select date, sum(number) as number from y group by date ) , ranked_ratio_by_day as ( select a.date , a.number/b.number as ratio , rank() over (partition by year(a.date), month(a.date) order by a.number/b.number desc ) as rank from a inner join b on a.date = b.date ) select year(date) as year , month(date) as month , ratio as max_ratio , date as date_attained from ranked_ratio_by_day where rank = 1 
A couple initial comments first: 1) It would help people read what you're doing if you wrote SQL that was formatted in an easier-to-digest manner, for example (this took me all of about 20 seconds to format and it's *worlds* easier to read): SELECT CAST(C.custid AS CHAR(5)) AS "Customer ID", CAST(C.cname AS CHAR(20)) AS "Customer Name", STR(I.qty * V.price,6,2) AS "Total Value" FROM Customers AS C INNER JOIN Orders AS O ON C.custid = O.custid INNER JOIN OrderItems AS I ON O.orderid = I.orderid INNER JOIN Inventory AS V ON I.partid = v.partid GROUP BY C.custid, C.cname, I.qty, V.price ; 2) When you ask for SQL help it is extremely helpful to provide a sample data set (including create table statements for your contrived example), the query you are running, as well as the output you *want* to see. Stating what you want in poorly written sentences leaves interpretation up to the reader and may not result in the solution you are looking for. Having said that, here's my suggestion (without testing this because I don't feel like writing up a set of example data and you haven't provided me with any): You say you want this "grouped in single totals per customer", I'm going to assume this means you want to aggregate "Total Value" by custid/cname. If that's the case you should try the following: STR(SUM(I.qty * V.price),6,2) Diclaimer: I'm not sure if this works, I didn't test. Of course to achieve this you need to remove your GROUP BY on I.qty and V.price because, if I'm reading your question correctly, you don't want to group these columns and instead want to aggregate them. If I'm misreading your question or if my solution isn't correct I apologize. Please provide an example output (and preferably a set of create and insert statements to demonstrate a contrived example) and I'd be more than happy to help more.
No that's fine, I was under the belief that I could not stack aggregates, IE : COUNT(MAX(something)) but that might appeal to the aggregates themselves. I'll give it a try
P.S. your formatting is way sexier. thanks for the info. 
MS SQL isn't my thing, so take my answer with a grain of salt (also, it's 4am and I've been drinking), but my statement that simple steps to create a reproducible example will yield massively better support from the community stays the same. If my blind suggestion doesn't work I strongly suggest you put together even a simple set of statements to reproduce what you're doing as it would help anyone trying to troubleshoot be exactly on the same page with you.
You might find this helpful, instant SQL formatter http://www.dpriver.com/pp/sqlformat.htm Eg your code now looks like: SELECT Cast(C.custid AS CHAR(5)) AS "Customer ID", Cast(C.cname AS CHAR(20)) AS "Customer Name", Str(I.qty * V.price, 6, 2) AS "Total Value" FROM customers AS C JOIN orders AS O ON C.custid = O.custid JOIN orderitems AS I ON O.orderid = I.orderid JOIN inventory AS V ON I.partid = v.partid GROUP BY C.custid, C.cname, I.qty, V.price 
I don't remember a time where I've specified both parameters while creating a number in a table. What's more common for me is either specifying just the precision and not the scale or not specifying either. I think the "best" way would boil down to the data itself. If you know that the data for the column will always be a certain format (length, scale, etc), then specify it - it will help the data remain "clean" in the future by enforcing the intended data format. My general approach, though, is to think about the parameters on the number type as a check constraint, and if at some point in the future the requirements grow, it's easy enough to modify the column and increase either precision or scale. I'm not entirely sure if that's what you were asking for, but I hope it helps!
If I am understanding correctly you need to use a HAVING clause after your group by. Are you wanting the customer with the highest grouped total or with the highest individual purchase amount?
This is my experience too, and it's why I let my certs lapse and am just getting back into renewing them, more out of personal interest and accomplishment than anything else. I was hired for my current position over a guy who the hiring manager told me "had every cert you can think of", but I had a bit broader experience, had better writing and speaking skills, and had a few references that sang my praises in epic bardsong. I **do** wonder if my rate would increase based on certs, though. Anyone have specific experience with their rate going up after getting a certification? 
Glad to hear it worked. Like I said before, I don't really use MS SQL so I have no idea about any restrictions on MAX and COUNT being used together, but this is certainly supported in other databases. COUNT aside, you should be able to use multiple similar aggregate functions in the same query without issue.
this is correct, after Kthanid helped me with the first part i got to here. but I'm getting 0 rows returned.... SELECT Cast(C.custid AS CHAR(5)) AS "Customer ID", Cast(C.cname AS CHAR(20)) AS "Customer Name", Str(Sum(I.qty * V.price), 6, 2) AS "Total Value" FROM customers AS C JOIN orders AS O ON C.custid = O.custid JOIN orderitems AS I ON O.orderid = I.orderid JOIN inventory AS V ON I.partid = v.partid GROUP BY C.custid, C.cname HAVING Str(Sum(I.qty * V.price)) = (SELECT Max(A.cnt) FROM (SELECT Str(Sum(I.qty * V.price)) AS CNT FROM orderitems AS I JOIN inventory AS V ON I.partid = V.partid GROUP BY ( I.qty * V.price )) AS A) EDIT: all the things. I need the totals grouped and from that the highest total purchased amount. before I added the HAVING clause to this I got each company's grouped total purchases. Now I just need to single out the highest one. 
It's pretty simple. You want something that is commonly called a cross-reference, or xref table. In the example below, I whipped up 2 base tables (colors and animals) and then created an animals_colors_xref There's some stub data that I'm inserting into each, (names of some animals, and names of some colors) We only want to store the color 'Brown' once (obviously, you're trying to do this with something for more important than the sample data) but rabbits, fish, dogs and cats can all be brown. Similarly, we only want to store 'Cat' once, but cats can be brown and black and white and tabby. The xref table has contraints define that require only values from the base tables are allowed, so you can't put id 30 for color, unless the colors table actually has a 30 id in it. CREATE TABLE [dbo].[colors]( [color_id] [int] IDENTITY NOT NULL, [color_name] [nvarchar](128) NULL, CONSTRAINT [PK_colors] PRIMARY KEY CLUSTERED ( [color_id] ASC ) ) ON [PRIMARY] CREATE TABLE [dbo].[animals]( [animal_id] [int] IDENTITY NOT NULL, [animal_name] [nvarchar](128) NULL, CONSTRAINT [PK_animals] PRIMARY KEY CLUSTERED ( [animal_id] ASC ) ) ON [PRIMARY] CREATE TABLE [dbo].[animal_color_xref]( [animal_id] [int] NULL, [color_id] [int] NULL ) ON [PRIMARY] ALTER TABLE [dbo].[animal_color_xref] WITH CHECK ADD CONSTRAINT [FK_animal_color_xref_animals] FOREIGN KEY([animal_id]) REFERENCES [dbo].[animals] ([animal_id]) ALTER TABLE [dbo].[animal_color_xref] CHECK CONSTRAINT [FK_animal_color_xref_animals] ALTER TABLE [dbo].[animal_color_xref] WITH CHECK ADD CONSTRAINT [FK_animal_color_xref_colors] FOREIGN KEY([color_id]) REFERENCES [dbo].[colors] ([color_id]) ALTER TABLE [dbo].[animal_color_xref] CHECK CONSTRAINT [FK_animal_color_xref_colors] INSERT animals (animal_name) VALUES ('dog'),('cat'),('fish'),('rabbit') INSERT colors (color_name) VALUES ('brown'),('black'),('tabby'),('grey'),('blue')
this doesn't have any errors but it's not pulling the highest value from the data. merely the top one listed. 
So here's what I'm looking at now both you and i wrote a code that returns the same results, but both are wrong. here's a grouped total before any change was made (I.E. when kathnid halped.) results: Customer ID Customer Name Total Value ----------- -------------------- ----------- 26 Monterey University 990.30 3 Wellesley Inc. 946.65 16 Cardiff Industries 836.60 25 Mission Hills Inc. 824.50 12 Alexandria Liquor Co 688.90 11 Agorist Distributors 400.35 1 Stanwood Consulting 395.00 21 Kalakaua Corporation 264.50 18 DRT Marine Lab 2140.0 2 Vallecito Industries 172.50 15 Blumenfeld Education 146.35 22 Ladera Enterprises 1454.3 (12 row(s) affected) both our codes pull Monterey University at 990.30, where clearly DRT lab has the highest value. I hope that makes it a little clearer? edit: for the life of me reddit will now not format it, however i dropped the subquerey and went with a top 1 clause at the beginning of the statement. which only like your statement picks literally the first entry. 
 SELECT Top 1 with ties Cast(C.custid AS CHAR(5)) AS "Customer ID", Cast(C.cname AS CHAR(20)) AS "Customer Name", Str(Sum(I.qty * V.price), 6, 2) AS "Total Value" FROM customers AS C JOIN orders AS O ON C.custid = O.custid JOIN orderitems AS I ON O.orderid = I.orderid JOIN inventory AS V ON I.partid = v.partid GROUP BY C.custid, C.Cname ORDER BY "Total Value" DESC there we go. 
never-mind, My order by statement was wrong. changed it to Str(Sum(I.qty * V.price) and bam, done, but thanks for your help. Cheers!
It seems to me the key is the e_add_date field which held the timestamp (via getdate()) when the row was inserted. Inspect the data and determine the min and max values you want to remove. Then you can do SELECT * FROM email_opt WHERE e_add_date BETWEEN ... to verify, and DELETE FROM email_opt WHERE e_add_date BETWEEN ... to remove.
Im not sure I understand the question. Also, keep in mind you can put four spaces in from of text to make a code block: insert into email_opt (eml_key, e_email, e_cst_key, e_xtp_key, e_add_user, e_add_date, e_delete_flag) select distinct cust_eml_key , cust_eml_address , cust_key , (Select xtp_key from x_type where x_name = 'List #1') [xtp_key] , 'username' [i_add_user] , getdate() [i_add_date] , 0[delete_flag] from cust_indivduals (NOLOCK) where cust_num in ('18188', '10091', etc, etc)
Thanks! I'm going to try this in a few hours and update everyone.
I'll remember that, thank you. To clarify, what this code did was pull data from 2 separate tables (cust_individuals, and x_type), and insert their values into the first table (email_opt). The " where cust_num in" specified 2000 unique customer identification IDs (I did not list them all of course). Ideally I would be able to delete these inserted rows using the same select statements. I am not experienced with delete functions yet, however. Would replacing "insert into" with "delete from" work ? On mobile, I apologize about formatting. insert into email_opt (eml_key, e_email, e_cst_key, e_xtp_key, e_add_user, e_add_date, e_delete_flag) select distinct cust_eml_key , cust_eml_address , cust_key , (Select xtp_key from x_type where x_name = 'List #1') [xtp_key] , 'username' [i_add_user] , getdate() [i_add_date] , 0[delete_flag] from cust_indivduals (NOLOCK) where cust_num in ('18188', '10091', etc, etc) EDIT: Thank You Grammar
Thanks for your reply to my other thread. Your query worked well... this thread is attacking a new hurdle I've encountered. 
Great explanation. Thanks! That is pretty much exactly how I feel without being able to form those feelings into coherent words.
DELETE A FROM TABLE1 A JOIN TABLE2 B ON A.KEY = B.KEY 
 BTW. SELECT * FROM blah JOIN blah2 etc.... You can change the from to a insert * / delete / update directly and it should work.
Works for me: use northwind; GO if object_id('dbo.tbl_itinerary_item') is not null DROP TABLE [dbo].[tbl_itinerary_item]; CREATE TABLE [dbo].[tbl_itinerary_item] ( [activity_id] [int] NOT NULL, ) GO INSERT INTO [dbo].[tbl_itinerary_item] values (0); INSERT INTO [dbo].[tbl_itinerary_item] values (1); -- INSERT INTO [dbo].[tbl_itinerary_item] values (1); -- generates "duplicate" error on alter INSERT INTO [dbo].[tbl_itinerary_item] values (2); INSERT INTO [dbo].[tbl_itinerary_item] values (3); GO ALTER TABLE [dbo].[tbl_itinerary_item] ADD CONSTRAINT [PK_tbl_itinerary_item] PRIMARY KEY CLUSTERED ( [activity_id] ASC ) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, SORT_IN_TEMPDB = OFF, IGNORE_DUP_KEY = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] GO Maybe try adding a space between )WITH? You are coming from SQL2000... what database are you going into? MSSQL? 2005? 2008? 2012? MySQL?
The ALLOW_ROW_LOCKS, SORT_IN_TEMPDB, etc "with" options were not available in SQL Server 2000 or were a slightly different syntax. Since you are using SMSS (Which didn't come around until 2005) it is trying to use the new conventions on the generate table script (since that is a function of SMSS and not SQL Server). If you are using 2000 and dont want to have to edit all of your generation scripts and whatnot, you could try to dig up a copy of Query Analyzer and the Enterprise Manager MMC snap-in (Ahhhh nostalgia). Edit: added info
Oh and thanks for helping
haha no we will eventually *phase out* the sql2000 server... not looking for Query Analyzer! **god i just cringed typing that** Thanks for the reply.
What do you have to show? I could knock those out in 10 minutes, but what do you learn by me doing that? 1. Group by PID, and use the Avg(), Min(), Max functions 2. Use the "book_title like 'D%'" function 3. use Group By/Having functions 4. Select from where city = (select city where aname = 'Name') Huge hints...
[Found this on google](http://support.microsoft.com/kb/976982#method1)... maybe the fix-it will work?
Have you looked through the event logs to see what information might be in there? How about just [downloading the service pack](http://www.microsoft.com/en-us/download/details.aspx?id=27594) and attempting to install it rather than going through Windows Update? 
I'm more of a Sql Server guy but I think regardless of what kind of engine, I would take into consideration how active your users are/might be, how big is the table, how expensive would it be to filter by inactive versus assuming all data is active, and what (if any) indexes do you have on the table that could be affected by updating/deleting data..and a bunch of other variables. Long story short, it may be the most simple approach to just delete upon request. If you're working with some awesome, highly tuned, system then maybe it makes sense to let some backend process go perform a delete in a set based way on some interval like you said. In the end there's no straight answer and the easier approach is probably best until proven otherwise.
Ok, i've installed sql server 2012, ran the win update again and it said there was nothing to update. Apparently it was able to update after installation of sql server 2012, not sure why? You might want to know why did I have this problem to begin with - when I installed visual studio pro, i also chose to install sql server that came with it, I think express 2008 is the one that comes with it.
Did you upgrade to SQL 2012 or run a side by side installation?
Check out this stack post, it has some useful advice: http://stackoverflow.com/questions/6153330/can-a-sql-trigger-call-a-web-service There's a few different ways you can do it asynchronously, such as the queue/Agent job advocated in the post, or a service broker.
I know this will sound pedantic; but, which database engine are you using. I can probably come up with something for MSSQL Server; but, I'm lost on MySQL or PostrgreSQL. While all of them (I think) will have some close variation on 'CREATE TRIGGER MyTrigger ON MyTable FOR INSERT AS' (Or AFTER INSERT, I would think in this case), how you go about moving a file around the OS (another useful bit of info to have) will vary a bit. For example, on MSSQL Server 2008 and later, you might consider a C# based function which does all of the filesystem level work and just call it from the trigger. In SQL 2005 and earlier, you'll probably be stuck with something like xp_cmdshell.
Ms SQL 
I can think of about 3 ways to go about this. The easiest way is this CREATE TRIGGER dbo.Blah ON dbo.Table AFTER insert AS BEGIN exec xp_cmdshell 'cURL and some stuff' END Now this may be a problem because if the cURL command fails then the entire transaction will fail. You may want this behavior IDK. This also means that any update or insert statements in the table will have to wait until the cURL command completes before they actually apply. This could cause a problem as it effectively locks the table while waiting for the web operations to complete. The other problem is that xp_cmdshell may not work if you have not set up a proxy account on the SQL Server.
The second way to do this would be to create a SQL Server job that does the appropriate cURL commands through the "Operating System (Cmdexec)" job type. Then using the trigger to start the job. EXEC msdb.dbo.sp_start_job N'My cURL Web Update Job'; This will solve the performance issues of the first one as the trigger and job will operate asynchronously.
Two problems with your analysis. You are only considering INSERTS to the index. What if someone happens to UPDATE a row in the middle of the index with more data than was there before? You are also only considering PRIMARY KEY indexes. What if the index doesn't happen to include the primary key? Indexes are sorted on the fields that are present in the index in the order you specify them. They contain pointers back to the rows in the original table. They are not sorted on the primary key unless it happens to be the first field in the index.
Also, would this lock the DB or the record it is updating?
Awesome, sorry, I missed the other messages here. Thanks. 
No.
Cool, thanks for answering! I can understand why it splits the data and leaves both pages with a half of a page of free space. Page splitting occurs the same way every time, no matter how the index is built, right? So if the index is not built on a column that has been filled with sequential data, then you want to leave plenty of space for new data on both new pages. But if the index is built on the primary key, and the primary key has data which doesn't typically change (for example, an Order Number), then updates wouldn't matter to the index, since the index only has the primary key and the row pointers. If anything does get updated, it's not the Order Number. Now if the index includes other columns, then updates are definitely relevant and I can understand the need for the extra space on the page. But in the very specific hypothetical situation I've created, that extra space in one of those pages would go to waste, right?
I think you might be looking at this the wrong way When SQL Server does a page split it is mostly concerned with accomodating new data, but also to maximise uptime. Page splitting is I/O intensive, so when it splits the page, it wants to make sure it wont be doing it again in a few seconds time. There probably been PhD papers written on which algorithm is most efficient So yes, splitting a page in half does result in some wasted space *at the time*, but you have to remember that its important to schedule and run maintenance routines on your database periodically. Few databases will remain at top efficiency without being maintained. One such maintenance task would be to rebuild the indices, which would reconsolidate the index pages. It would still leave some space in the pages, to allow for future growth. How much space it leaves would be decided by the fill factor you specify, when creating (or rebuilding) the index. 
While this would work, if kingmikolaj needs more control i'd go with something like: IF @isACMJob = 1 AND @currentjoblength &gt; @minutelimit BEGIN SET @minutelimit = CAST( @minutelimit as varchar(max)); SET @subj = 'ERROR: ACM Job (AutoSearch) ran for too long'; SET @imp = 'HIGH'; SET @msg = '&lt;strong&gt;&lt;span style="color: red"&gt;&lt;strong&gt;Client Auto Search Job ran for ' + @minutelimit + '. Client Auto Search Job ran for too long.&lt;/span&gt;&lt;/strong&gt;'; END ELSE BEGIN **SOMETHING ELSE** END What dpenton said is preferred if you don't need the flow control.
Ah, okay. That makes a lot of sense. I'm not trying to suggest that they change the way it works or anything, because I'm sure it does what it does for a reason. I'm just trying to understand why it does what it does -- and this has helped me! Thanks again.
Thanks so much! I was getting stuck on the syntax near the if statements and this works like a charm!
SET @date = CAST(GETDATE() as DATE) is an easier way to do it.
If I read your post correctly I believe the only thing wrong here is that there should be a NOT before the EXISTS.
May not have the DATE type if it's pre-2008 SQL Server.
As long as we're talking 2008+ please also move the set into the declaration. DECLARE @curDate DATE = GetDate()
What is all this talk about 2008+? DATE formats have existed in SQL Server for ages, and I know for a fact CAST(GETDATE() AS DATE) works in SQL 2000.
Strictly speaking no. According to Kimball the only time it is acceptable to use anything other than an incremental unique number as a FK/PK pair is in the date dimension using the date.
When you say a fact table you must also have dimensions. The foreign keys that link rows in a fact table and the primary keys in the dimensions should be arbitary, meaningless and ideally as narrow as possible, so composite keys are probably not the correct choice. 
How many files? Is this suppose to be on-line? Have you looked at [Lucene](http://wiki.apache.org/lucene-java/LuceneFAQ#How_can_I_index_PDF_documents.3F)?
I am under the assumption that my 3rd condition is wrong. AS it pulls all the orders that contain the first 2 items. 
You can get away with removing the subqueries and just setting your where clause to look for either 'Gizmo', 'Gadget', or Not Equal 'Gizmo' and 'Gadget'. Ex: WHERE V.description = ('Gizmo' OR 'Gadget') OR V.description &lt;&gt; ('Gizmo' and 'Gadget') This is based off the subqueries you provided, but I believe that your gonna end up with everything anyways.
The 'Order' table contains the OrderID, which will be the time im trying to display, The Orderitems table contains the 'partid', which is the reference im using for the table Inventory, which contains the description of the item. the results IM looking for is: Order ID 6128 1 row(s) returned where order 6128 based on simply looking at the tables is the only order where it contains partids with the description of gadget, and gizmo, and conatin no other partid within that order. (I.E. The didnt order anything else but those two items.)
 SELECT CAST(O.orderid AS CHAR(5)) AS "Order ID" FROM Orders AS O INNER JOIN Orderitems AS I ON I.orderid = O.orderid INNER JOIN Inventory AS V ON V.partid = I.partid GROUP BY O.orderid HAVING COUNT(CASE WHEN V.description IN ('Gizmo','Gadget') THEN 'ok' ELSE NULL END) = 2 AND COUNT(CASE WHEN V.description IN ('Gizmo','Gadget') THEN NULL ELSE 'uh oh' END) = 0
Sounds like you need to add another table. One which contains an ID, Profile#, and a reference to the attribute's ID. If you have profiles and attributes tables, they're in a many-to-many relationship (One profile can have many attributes, and one attribute can belong to many profiles). Whenever you have a many-to-many relationship, you have to create another table to resolve the problem. Then you could search that table for the specific attribute you're looking for and find all profiles which contain that attribute.
 I think you want pivot? http://msdn.microsoft.com/en-gb/library/ms177410(v=sql.105).aspx If you also require it WITH can do a recursive query 
Yup, pivot.
That's kind of the approach I'm attempting to take now. I'm trying to generate a temp table with the appropriately named columns, then use that table for the query. The current speed bump is finding a way to return the unique attributes as a string... hmm, maybe it's better if I just write this out in a little notation: declare @our_max int, @our_min int, @our_sql varchar(max) set @our_max = (SELECT count(distinct attr) from profiles set @our_min = '0' set @our_sql 'SELECT bla bla bla FROM profiles' WHILE @our_min &lt; @our_max BEGIN SET @our_sql = @our_sql + 'left join [table to itself, aliased as profiles.attribute value] SET @our_min = @our_min + 1 END Then use what that generates for the WHERE ResX=1080/etc. The current problem is getting/using the distinct attributes to use as references (because the subquery to get them returns more than 1 value).
If I understand what you are asking for correctly, you have a table like |Id |Attribute |Value| |:-:|:--------:|:---:| | 1 | ResX | 1080| | 1 | ResY | 1920| | 2 | ColorSpace | RGB | | etc... | With an unknown number of attributes and you want to pivot it out into a table with a column for each distinct attribute. In that case, I believe what you want to do is something like this: DECLARE @qry VARCHAR(MAX) SET @qry = (SELECT STUFF((SELECT ', ' + CAST(Attribute AS VARCHAR) FROM (SELECT DISTINCT Attribute FROM Profiles) p FOR XML PATH('')),1,2,'')) SET @qry = 'SELECT id, ' + @qry + ' INTO PivotedProfiles FROM (SELECT id, attribute, value FROM profiles) p PIVOT (MAX(Value) FOR Attribute IN (' + @qry + ')) up' EXEC(@qry) We're dynamically constructing our query- With the first statement, I'm building a list of the distinct attributes, and then with the second I'm turning it into a [PIVOT](http://blog.sqlauthority.com/2008/06/07/sql-server-pivot-and-unpivot-table-examples/) statement. Then we execute it. PivotedProfiles should look something like |id|ColorSpace|ResX|ResY| |--|----------|----|----| |1||1080|1920| |2|RGB||| |etc| Piece of cake! :)
Did you ever know that you're my hero? utexaspunk, you're everything I wish I could be. Thank you so much!
Aw, shucks! And reddit gold!? This is the best day ever!! 
If you are planning to stick with the DBA stuff I would recommend starting down the [Microsoft certification](http://www.microsoft.com/learning/en/us/mcsa-sql-certification.aspx) road. You can get some good certs there and a lot of learning that will help you in the job now and your career going forward. You will be able to find MS approved courses to help you get training (or you can self start).
The elegant way to do it 
Thanks for the reply. Having worked through service centres, helpdesks and infrastructure support it's high time I got qualified in something and I am hoping to stick with the DBA stuff. I can see the MCSA is split into three sections, are the sections worth anything on their own? Obviously the full MCSA would be best but I don't know if they'll pay for it all. I did one or two sections, would a future employer see them as worthy qualifications? I would look at completing the rest of them on my own back but wouldn't be able to afford to where I am now.
Glad to see someone shares my opinion of the official book...! If you can spring for the official practice exams (transcender etc ... ) these are definitely worth it, if only to get used to the style of questions.
Upboat for Transcender they helped me get through mine.
The exams are cheap (about £70 if I recall correctly). Training on the other hand, costs a lot. But its mostly a waste of money, and all they are teaching you is how to pass the exam. If you're willingly going down the DBA route, its better to actually learn it all inside out. My employer wanted me to become certified too, but didnt want to spend megabucks. I asked if I could take a day a week working from home, which I used as training. Grab a few books for £100 and the rest is on the net. Worth a try, if your boss is reasonably open minded. 
Definitely Access is a major part of the problem. Access is a local DB for single users. You should upgrade your .MDBs into SQL Server if you want multiple remote users to have efficient access. 
If you're using SQL Server 2008 or newer, you don't have to listen to these Kimball purists. Unless you're just OCD and like the consistency of treating all the dimensions similarly (Many of us, including myself, fall into this category), with the compression in SQL Server 2008 onward there's little advantage to replacing fields that have a small quantity of values and no attributes (like a status field) with keys and dimension tables.
The challenge with BI practicioners is balancing theory with practical solutions. What I see in the responses below are most likely folks that do not understand their host business very well. And I say that because BI works best when the theory conforms to the business...not the other way around. So the star schema is great on paper but it may work best in my company (utilities and theft prevention) when I can tweek the schema...and at the end it may not look like an n-star shape that we normally associate, but its fast and used by the leadership. It delivers value. For the purist, I say this, understand your companies business and process better. The CEO won't give a flip that his/her new datawarehouse is a snowflacked blahblahblah... 
Exactly :)
Whereas Access is really not very good, it's almost certainly good enough for someone who's been learning for nine months. Please remember your skepticism. If people are telling you "definitely this is the problem" without even seeing your queries, they're shooting cluelessly from the hip. Start by finding some trivial query that shouldn't take any time at all. In MySQL it'd be something like `select 2+2;` but that's not ANSI SQL and I don't know whether Access wants that phrased slightly differently, or whatever. Alternately, something like `select concat("ab","cd");` might do quite nicely. Just something that ought to take near-zero effort. The germane purpose here is, if it takes it a lag to do a simple addition, then the problem is outside the SQL layer (in the connection probably, or possibly the server's misconfigured or under ridiculous load.) On the other hand, if that goes through instantly, then your problem is about your queries, or something similar (like your tables aren't properly indexed or something.) At that point, you'll want to show us your queries, and possibly the relevant schemas.
Probably gonna butcher your query (sorry!), but we partition by our ID and select the newest one (which we call BEST). we do this as a dedupe method but using something similar may allow you to sort and partition by date so you get the records with the latest date per ID. select provider_entry_id, provider_entry_type_id, provider_entry, provider_entry_visit_dt from (select provider_entry_id, provider_entry_type_id, provider_entry, provider_entry_visit_dt, ROW_NUMBER() over (partition by provider_entry_type_id order by provider_entry_visit_dt desc) as best from tbl_claimant_provider_entry) x where best=1
&gt;'ROW_NUMBER' is not a recognized function name. Is row number a 2005+ function? This is on SQL Server 2000
First, join the tables Select * from Details inner join Docks on Details.DockId = Docks.DockId Then get the specifics you want Select (List of columns needed from details table) from Details inner join Docks on Details.DockId = Docks.DockId where Details.DockId = @provided Hope this is correct and helps
It's not pretty, but... SELECT * FROM Details WHERE DockID IN (SELECT DockID FROM Docks WHERE WarehouseID = (SELECT WarehouseID FROM Docks WHERE DockID = @provided))
Yeah I'm used to using the rank over partition method... I did find an answer, and updated my post.
updated my post with an answer if you're curious. Thanks for your help.
updated my post with an answer if you're curious. Thanks for your help.
Thanks for this. My follow up question is this: All of my employees will be working from home. One of their jobs is to investigate websites, while hiding their IP address. However, the websites they need to investigate are provided by my database. It is my understanding that the only way for these remote employees to access the database is by VPN to the database computer, since they are remote. Is there a way to serve up the database without having remoute users VPN in, like make it available via web or something else?
where did you find the answer?
SELECT NOW(); is a valid Access query
Does IFERROR() work in SSRS? Not on my usual computer to check.
it's a classic technique, joining to a "max subquery" thanks for the update, and glad you didn't go the route of creating a temp table
Im not an Oracle user, so Im just winging it... This shows how to add a date based constraint http://stackoverflow.com/questions/203484/oracle-10gr2-enforce-dates-entered-are-between-9am-and-5pm You should be able to eliminate the IF by changing the constraint to something like... CHECK((entered_m_date &lt;&gt; col_m_date) OR (entered_e_time &lt; col_s_time) OR (entered_s_time &gt; col_e_time)) 
As you've already determined there is no such thing as cross row constraints in Oracle. The easiest solution would be to fix your client code and introduce serialization before making the INSERT or UPDATE call. If you can't modify the client side code the only other way to meet this requirement would be to introduce serialization via database triggers. That should not be taken lightly because it is very difficult to code triggers correctly in a multiuser scenario. Here's an example using triggers: [AskTom: Overlapping values](http://asktom.oracle.com/pls/apex/f?p=100:11:0::::P11_QUESTION_ID:474221407101)
So we're getting excited over something you could do in Access since about 1997 ? Whilst I appreciate the effort, this is borderline blogspam.
To be honest this is a pretty ugly query, bad database design - you really want those values broken up into a separate table: &gt; SELECT Account FROM TABLE WHERE Key LIKE '%,2,%' OR Key LIKE '2,%' OR Key LIKE '%,2'
if this is your table, redesign it immediately, it violates first normal form WHERE ','+Key+',' LIKE '%,2,%' this query will do what you want, but it won't scale because it requires a table scan -- the more rows in the table, the slower it gets
Thanks for the suggestion! As this is a live system with new data coming in constantly, any change to the table format or making a backup or working in temporary tables is ruled out (I think?). Any kind of backup or temp table would be fine for processing the data, but then there would be a third set of data that came in between the time the backup was made, and it would bring me back to my original problem.
thank you, that is not the design of the table, it was just to illustrate what I wanted.
thank you, that is not the design of the table, it was just to illustrate what I wanted.
Its also possible to pivot that shit first so its broken out into individual rows.
Just tested it out in SQL Server 2005 and it works fine. Thanks for sharing this tip.
At least it is real content. Most posts are just cries for help. 
PAIR1 is cleaner than PAIR2, and only does 1 select vs 2, and no cursor. 
PAIR2 and PAIR1 are flawed right from the start. In the time between when you execute the query with the count or the query to check for the pair_id and the INSERT a second session could insert a row into the table and an exception would be raised anyways. Here is an example that is better in my opinion. CREATE OR REPLACE PROCEDURE i_pair3 ( name_ IN VARCHAR2 , value_ IN VARCHAR2 , result_ OUT NUMBER ) IS v_pair_id NUMBER; BEGIN INSERT INTO pair ( pair_id , name , value ) VALUES ( seq_pair_id.NEXTVAL , name_ , value_ ) RETURNING pair_id INTO result_; EXCEPTION WHEN DUP_VAL_ON_INDEX THEN -- Any special handling here, like a customized exception if necessary. END; This will fail immediately and not be subject to the problems of multiple queries being executed at different times giving the false impression of a state that may or may not be true. Additionally, there is less code which means it will be easier to maintain over time. I've been both an Oracle DBA and Oracle developer. I don't understand why your DBA has a preference to either one. It's not about style, it's about correctness and I don't think either is correct in the context of an Oracle application. 
Its a really poor way to do it. Read up on joins, you cannot write SQL if you dont understand how to join
All query and DDL result in cursors even if they aren't explicitly defined with the CURSOR keyword. Both PAIR1 and PAIR2 are flawed for the reasons identified in my comment below.
I agree that you will avoid the race condition. Your way would work well if there are few unique failures vs successes, (and probably both ways, I would need to test). My concern would be if there were more failures than successes, as the insert/rollback takes quite a bit more than a query. I had a developer decide to just reinsert all the data into a unique index even though the data existed was there, and "It worked in test just fine". :) Either way, I always learn something from you (primarily Oracle DBA, I dabble in development when needed). 
I disagree that the simplified approach should not be followed due to the resources required to rollback a transaction. I would argue that if there was a high failure rate that implies high concurrency. If there is high concurrency the two approaches presented by the OP would fail as frequently as my approach due to other inserts happening between the check (SELECT) and insert. In addition to needing to rollback the transaction, the OP's approaches have additional overhead of additional consistent reads due to the unnecessary queries executed prior to the insert. 
Due to the minor differences in each vendor's dialect of SQL, I'd recommend that you install a test environment of the same type - it'll save you getting confused by the nuances. You haven't mentioned which SQL product you're using at work, but you should have options on all platforms. The only product likely to cause you problems is Microsoft's SQL Server (as it's Windows-only), but you can create a Virtual Machine, and download and install evaluation copies (180 day) of Windows and SQL Server 2012.
http://www.sqlite.org/index.html
Install [Postgres.app](http://postgresapp.com). Then open a terminal and type `psql`. You're now in a console. If you want some synthetic data to play with, you can lift and modify some of the queries in my [Learn to love your database](http://love-your-database.herokuapp.com) slides. EDIT: Oh yeah, if you want to be able to share this data with people, you could give our [Postgres service](http://postgres.heroku.com) a try. It's a great way to learn because you can collaborate with dataclips and give other people access to your database very easily. Databases &lt;10,000 rows are free, so have fun. :) 
[Sorry here ya go](http://i.imgur.com/lfOkuwr.jpg)
I think you need to put a comma just after "BONUS DECIMAL(6,2),"
I think Pacos is right, yea But wow. Is this book really teaching you to use a varchar() for employee ID? 
i seriously spent like half and hour trying to figure that out. I'm gonna go cry now. Thank you.
...yes? are you thinking integer or something? maybe it's assuming employee ID's have both letters and numbers
It's pretty bad style to be using varchars in a primary key, and for 'id' type fields. Sometimes its unavoidable of course, but for a book which is supposed to be teaching you good sql practices, it doesn't inspire confidence!
do you mind elaborating as to why varchars arent good for primary key's?
Generally speaking, its good to use integers for an id / primary key because.. varchars have some subtle effects with spaces (default mysql will trim spaces from the end, and so comparing varchar('myfieldvalue') to 'myfieldvalue ' will return true. They could also behave differently depending on the collation (case sensitive etc) Since the primary key is used to define the clustered index, all non-clustered indices need to store the primary key as part of the index record. So its best to keep this short as possible (32 bit integer is 4 bytes, whereas varchar will usually use more space - even ignoring the field length, it needs an extra one or sometimes two bytes to store the length). For joins, its quicker for mysql to compare 4 byte integers than to compare varible length strings Likewise the clustered index dictates how the records are actually laid on out disk. if you are using an integer auto_increment field, then all your inserts get appended nicely onto the end of the table. Using a varchar, inserting a new record will usually mean its having to insert somewhere in the middle of the clustered index, which can hurt performance. As usual though, there is no hard and fast rules which apply to every single situation 
If you're brand-new to SQL and most any kind of scripting or coding in general you're in for a lot of crying. I've spent so much more time than I care to admit hunting down where the extra or missing single/double quote or comma is than any human being should ever have to deal with and I'm pretty sure I'm nowhere near done.
learning that i didn't know how to spell 'integer' was fun... also, is there any way to move back up a line of code to edit it? or do i have to the whole thing over?
This looks really useful, i can't seem to get it to work... Edit : can't seem to get it to work the way they explain it, but i can copy paste no problem!
This may do what you want: http://www.sqltreeo.com/wp/ Used to be free, but apparently not anymore. 
if you have a windows computer, have a look at [heidisql](http://heidisql.com) (free) you'll never have to retype a query again, and it automates a substantial number of routine tasks, including backups, which are a breeze, running saved queries, defining new tables, etc.
Sorry I took so long getting back, this proved very helpful, thank you.
So, if there is a dup_val_on_index I would do the select into and return that pair_id (I have to return the pair id for a cross reference entry associating the pair to the new record). Can you explain why inserting and then selecting if needed is better than selecting and inserting if needed? Does the distribution of the data have any effect on it? And, thanks for the information.
&gt; Can you explain why inserting and then selecting if needed is better than selecting and inserting if needed? I would say the main benefit would be that you would not run into the situation where you run the SELECT first, *think* that a value doesn't exist, and then get a DUP_VAL_ON_INDEX exception anyways (where you'd need to duplicate the SELECT logic again) when you attempt the INSERT. With my approach it would fail immediately and then you can return the value by executing another SELECT. I haven't done any performance comparisons between the two methods. I would think on the average my approach would perform better due to the fact that the SELECT statement would optionally run, unlike the prior solutions where it would execute every single time.
I see that you've found your answer (awesome). I still have some comments. I applaud your efforts, but you need to know there is no way you can learn SQL in 24 hours. I do not even think you can effectively learn how to use it in that time. Most 'learn XYZ in XYZ hours/days' courses are bullshit that do no more than teach you the syntax. Long story short, you are lying through your teeth if you put that on your resume; not to say that you shouldn't, but don't try to 'keep up' with somebody talking about databases if they obviously know their shit. They will find you to be as ignorant as you are, and at that point, it will be much better if you have said that you are new to SQL and barely know how to use it. I've been using MySQL, PSQL, MS-SQL, and many more for my entire professional history (about 8 years commercially), and I still would not consider myself to have a complete knowledge. I'm certainly a far degree farther than most programmers and sysadmins, but there are still things to learn. If you want to do yourself a favor, please give a study to set theory and discrete math. It will make relational databases a lot more understandable. Indexes and foreign keys and stored procedures and all that cruft can wait; if you do not understand the base relational model, the db queries and systems you create are going to be daily WTF submissions. It's super easy to make a usable simple database, no matter how many bad design choices are made. It's super easy to expand or build yourself into a corner from that point, where everything is unmanageable and undecipherable. The worst part is that you won't even know the point at which you went wrong; it isn't like procedural logic where you can point out 'this algorithm isn't ideal' or 'this function is obviously returning null to a pointer'; the flaw/bug will likely be tied to something inherent in the design. All that said; once you 'get' it, SQL is super easy in structure and execution. Joins can get hairy, but that's more about layers of simplicity creating a complex abstraction. SQL is a language that you will spend hours writing one line. This is normal. Don't give up or allow the frustration to get to you. If you find you enjoy SQL, try LISP or Scheme on for size. Many of the same thought models are used, but to build programs and scripts instead of data storage.
DB navigation can be handled with a decent desktop application or web portal to the DB, but it is good (preferable, actually) that you are learning queries. For anything but the most basic stuff, I find I work faster at a command line or via scripts than I do in something like SQL Management Studio (M$) or MySQL workbench. 
honestly, i'm already familiar with joins, relations, and manipulating data in tables using ArcGIS, but normally the data we use needs to refined. alot. so it stands to reason that they would prefer us to know how to do that refining with something like SQL, which is designed specifically for that, instead of ArcGIS. you can use ArcGIS for that, but it's really not designed for it.
**EDIT** Misread this as MySQL... posted MS SQL variant below reply pointing my mistake out :) I'd hate to think of how this would run with a million rows in it... I'm assuming that "Source" is an auto-increment number... and that ID something like "A1", "B1".... use Test; drop table if exists Test; create table Test ( Source int not null AUTO_INCREMENT, id char(2) not null, Prop1 char(3) null, Prop2 char(3) null, Prop3 char(3) null, Prop4 char(3) null, Primary Key (Source) ); INSERT INTO `Test` (id, Prop1, Prop2, Prop3, Prop4) values ('A1', null, 'ABC', null, null), ('A1', 'XYZ', null, 'AB1', null), ('A1', null, null, 'OPQ', null); insert into Test (id, Prop1, Prop2, Prop3, Prop4) SELECT Distinct ID ,(SELECT Prop1 from `Test` where Prop1 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop1 ,(SELECT Prop2 from `Test` where Prop2 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop2 ,(SELECT Prop3 from `Test` where Prop3 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop3 ,(SELECT Prop4 from `Test` where Prop4 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop4 from Test T; INSERT INTO `Test` (id, Prop1, Prop2, Prop3, Prop4) values ('B1', '123', null, null, 'FOO'), ('A1', 'Bar', null, 'OPQ', null); insert into Test (id, Prop1, Prop2, Prop3, Prop4) SELECT Distinct ID ,(SELECT Prop1 from `Test` where Prop1 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop1 ,(SELECT Prop2 from `Test` where Prop2 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop2 ,(SELECT Prop3 from `Test` where Prop3 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop3 ,(SELECT Prop4 from `Test` where Prop4 is not null and ID = t.ID Order by Source Desc Limit 0,1) Prop4 from Test T; SELECT * FROM Test
If I am understanding this correctly... It sounds like you could use a view to abstract the data from the base tables. Use a recursive CTE to build your hierarchy with the OVER clause and PARTITION BY. There are a lot of examples online for how to do this. You could do it in a procedural way like wernercds example but like he said it will get slower as the tables grow. 
Seems obvious, but... INSERT INTO table SELECT ID, MAX(Property1), MAX(Property2), MAX(Property3), MAX(Property4) FROM table GROUP BY ID; It avoids sub-selects and the hidden selects of recursive CTEs and is going to be relatively quick. By "relatively quick" I mean it's going to be about as fast as your disks or CPUs, whichever is slower. This will fall down if there's a possibility that multiple values of a property can exist for a single ID. E.g. Sources 1 and 3 have a value for Property3. 
1. Set up things the way you want. 1. Go to Tools, Import and Export Settings 1. Under General Settings –&gt; Object Browser Options. 1. Export your settings 1. Under Tools, Options, Environment, Import and Export Settings, you can set the default file. If that doesn't work you can manually import the file each time... Or maybe there is a startup parameter that loads it up.
You're looking for SELECT COALESCE(Column1, Column2, Column3) AS Source_ID FROM Table1 Searched CASE would work too: SELECT CASE WHEN (Column1 IS NOT Null) THEN Column1 WHEN (Column2 IS NOT Null) THEN Column2 ELSE Column3 END Edit: we're talking about MS SQL Server, right?
 SELECT CASE WHEN COLUMN1 Is Not Null THEN COLUMN1 WHEN COLUMN2 Is Not Null THEN COLUMN2 ELSE COLUMN3 END AS 'Source_ID' FROM Table1 alternatively... SELECT COALESCE(COLUMN1 ,COLUMN2 ,COLUMN3) AS 'Source_ID' FROM Table1
Use CASE WHEN...THEN And then, seriously consider getting out of Access. Regardless of if this is a corporate gig, a personal gig, etc, seriously consider getting out of Access. If you have questions as to why or want a point by point, respond to this, and I'll find time to give you an explanation why. Your replacement for Access should be a .NET (C# or VBasic, most likely) and T-SQL if you are tied to M$. If you want to get off the M$ train, give MySQL or PostgreSQL a shot with C or PHP or Python. If you need GUI work (you probably do if it is Access), PHP+Apache can make a localhost webportal, C has a million libraries (though I suggest GTK). Well; PHP sucks for many of the same reasons as Access, but it doesn't bash you into doing it only one (broken) way at least. Really, just anything to get off of Access. Your time is too valuable to fight with Access's BS. Of course, this is all opinion; but I started out running support and development for a large-scale db driven Access application, and have from there worked in all manner of DB driven commercial applications. It took me about a year to recover from using Access every day.
I personally think using CASE is better but I learned at PASS that IIF is [available in SQL Server 2012](http://msdn.microsoft.com/en-us/library/hh213574.aspx).
That does make sense and might be a good way to answer the question. It is an important distinction about having multiple properties for a single ID. Also, I have not used MAX() in this way before, I understand it will get the highest value in the collating sequence but if you have two values 'ABC9' and 'EFG1' ... Which one will it return and which one would they expect it to return to meet the criteria?
Max(Property) will select the "Max" - alphanumerically... not "Max" as last in list. use Pubs; if Object_ID('dbo.Test') is not null drop table Test; create table Test ( Source int not null Identity, id char(2) not null, Prop1 char(3) null, Prop2 char(3) null, Prop3 char(3) null, Prop4 char(3) null, Primary Key (Source) ); INSERT INTO Test (id, Prop1, Prop2, Prop3, Prop4) values ('A1', 'ZZZ', 'ZZZ', 'ZZZ', 'ZZZ'), ('A1', null, 'ABC', null, null), ('A1', 'XYZ', null, 'AB1', null), ('A1', null, null, 'OPQ', null); SELECT ID, MAX(Prop1), MAX(Prop2), MAX(Prop3), MAX(Prop4) FROM Test GROUP BY ID; Result is ZZZ, ZZZ, ZZZ, ZZZ... not XYZ, ABC, OPQ, ZZZ as desired.
I've actually been trying to think of how to make it faster... I'm not sure CTE's would do any better or worse than my suggestion. It would still basically involve going through n rows for each property to get the "Most Recent"... I don't think that this setup would be realistically optimizable due to the inability to add any kind of Index to it. I was actually thinking that given enough rows, that a Trigger and a secondary table, might be a better option... use Pubs; if Object_ID('dbo.Test') is not null drop table Test; create table Test ( Source int not null Identity, id char(2) not null, Prop1 char(3) null, Prop2 char(3) null, Prop3 char(3) null, Prop4 char(3) null, Primary Key (Source) ); if Object_ID('dbo.Test_Scratch') is not null drop table Test_Scratch; create table Test_Scratch ( id char(2) not null Primary Key, Prop1 char(3) null, Prop2 char(3) null, Prop3 char(3) null, Prop4 char(3) null, ); GO Create Trigger Test_Insert on dbo.Test AFTER INSERT AS Begin MERGE dbo.Test_Scratch as T USING Inserted as S on T.ID = S.ID WHEN NOT MATCHED by TARGET THEN INSERT VALUES (S.ID, S.PROP1, S.PROP2, S.PROP3, S.PROP4) WHEN MATCHED THEN UPDATE SET T.Prop1 = Coalesce(S.Prop1, T.Prop1) ,T.Prop2 = Coalesce(S.Prop2, T.Prop2) ,T.Prop3 = Coalesce(S.Prop3, T.Prop3) ,T.Prop4 = Coalesce(S.Prop4, T.Prop4) -- output $action, inserted.*, deleted.* ; END GO INSERT INTO Test values ('A1', null, null, 'ZZZ', null); INSERT INTO Test values ('A1', null, 'ABC', null, null); INSERT INTO Test values ('A1', 'XYZ', null, 'AB1', null); INSERT INTO Test values ('A1', null, null, 'OPQ', null); INSERT INTO Test SELECT * FROM Test_Scratch; SELECT * FROM Test; INSERT INTO Test values ('B1', '123', null, null, 'FOO'); INSERT INTO Test values ('A1', 'Bar', null, 'OPQ', null); INSERT INTO Test values ('B1', null, 'HIJ', null, 'BAR'); INSERT INTO Test SELECT * FROM Test_Scratch; SELECT * FROM Test Would bypass the extra row lookups, at the cost of a trigger and a secondary table... Could even just use the secondary table for "the most recent" data, without having to insert it back into the first table. Depends on the actual needs of the program.
Good God....I am not mad. I use SQL. I dislike Access. The reporting tools which were set-up before me were written in Access. Reports take forever to run, the database crashes and I have to reopen it sometimes when I am running it, queries and macros all over the place. I'm in the process of updating the previous Access reports to SQL.
You could also nest ISNULL(,) for a similar result. SELECT ISNULL(Column1,ISNULL(Column2,Column3)) AS Source_ID 
 SELECT COUNT(CASE WHEN metric &gt;= 10000000 THEN 'ok' ELSE NULL END) AS "first" , COUNT(CASE WHEN metric &lt; 10000000 and metric &gt;= 5000000 THEN 'ok' ELSE NULL END) AS "second" , COUNT(CASE WHEN metric &lt; 5000000 and metric &gt;= 1000000 THEN 'ok' ELSE NULL END) AS "third" , COUNT(CASE WHEN metric &lt; 1000000 and metric &gt;= 500000 THEN 'ok' ELSE NULL END) AS "four" FROM daTable WHERE date = '2013-01-04' 
Thank you so much!
Is there a way to make this work when I replace count with sum? Because right now it returns 0 when I do this and I know that not to be the case.
The sum of all the metrics in the specified range, ie, what would be returned if I ran select sum(metric) from table where date='2013-01-03' and metric&lt;10000 and metric&gt;=5000;
You are trying to write the procedures? I'd tackle the first one in little parts like thus... Take your input, lets call it @INPUT and count to see if you only have two comma's, if you have more then you have a comma in the title and that's a problem case that needs dealt with. If you have less than 2 then something is wrong with the input so throw an error. Split it up into @TITLE, @FORMAT, @PUBLISHER using the comma positions to mark the start and end of the substrings. Then using @TITLE, @FORMAT, @PUBLISHER get all your exact matches. If you then want to get a fuzzier match use a LIKE '%'+@TITLE+'%' and @FORMAT, @PUBLISHER to get titles containing the @TITLE but exact matches on @FORMAT and @PUBLISHER 
This stood out as a red flag to me: &gt; loop through each tag Try to stay away from looping constructs when using SQL. But, rather take advantage of the set based capabilities of SQL. The query in the SP should probably look something like this: select col1,col2,... coln from WhateverTable where tags in(@tag1,@tag2,... @tagn) or if you were using literals instead of variables, the last line might look like where tags in('Gaming','PC','Unisoft') 
Try the split Function: [SQL Server Forums](http://www.sqlteam.com/forums/topic.asp?TOPIC_ID=50648) Pass in a CSV variable @tags = 'Ubisoft,EA' Join the table function: dbo.Split(@tags) to your products
Actually, I spend more time defending PHP in most cases. That doesn't remove the fact that it has flaws, and those flaws are in common with many of the programming languages built on spoken language or made to be easy for non programmers to learn (VBasic 6.0 and prev, Access, COBOL, etc). I don't ever want to give the impression that you'd be able to get away from those flaws by moving to something else with those flaws, so I spent half a sentence noting that. I did not spend time bashing PHP. I could have spent two pages explaining why PHP sucks in a thread about Access and SQL... but I didn't, because as you've noted, it is a thing (that we probably are all aware of), and it was not relevant. I noted that it has flaws much of the same class as Access, because that is relevant. I'm just surprised that you think I'm bashing it when all of half a sentence was spent on it. Are you compensating for something, projecting, or just have a berserk button with 'PHP sucks' rants?
I will have to add that one to the ole toolkit for use in the future. Thanks.
The last estimate of yours is the most exact. I generaly try to keep away from rants and the whole "PHP sucks" thing is getting to me. I probably need to stay out of reading those too. Anyhow, please accept my apologies, I too believe it was a moment for me to see some mention like that about "my beloved" PHP ;)
Try the following: select count(metric), sum(metric) from TABLE group by CASE WHEN metric &gt;= 10000000 THEN 'A' WHEN metric between 5000000 and 9999999 THEN 'B' WHEN metric between 5000000 and 9999999 THEN 'C' WHEN metric between 1000000 and 4999999 THEN 'D' WHEN metric between 500000 and 999999 THEN 'E' ELSE 'F' END This will give you two columns, one for count and one for sum
What I've done in the past is two things: * Using my web server running PhpMyAdmin to create a simple database, flat file, to query against. * Mac SQL Studio to query against it. But actually, if you are able to figure out the config for PhpMyAdmin on your own localhost apache server, you can run the queries right there. Another option...if you're already using Excel for analysis, you might want to look into Microsoft Access. The syntax of the queries is slightly different from MySQL or Oracle, but it's close enough to at least give you an idea of how it all works. My personal recommendation is to go with whatever is open-source since the documentation for it is plentiful. PhpMyAdmin on an Apache local server. It's barebones, but enough to get you learning more.
You can have as many inner joins as you want on a query. And it looks to me like the query is trying to join [Order Details] twice when it has already occurred once with the needed columns. It could also be the way that Reddit is showing the code. I like to break my queries down to a line-by-line basis, which makes them easier to read and analyze. SELECT whatever FROM something.whatever INNER JOIN something.whatever ON You get the point. Hope this helps.
Cleaned up a bit and you get: SELECT C.CompanyName ,O.ShippedDate ,OD.UnitPrice ,P.ProductName FROM Products as P INNER JOIN Customers as C INNER JOIN Orders as O ON C.CustomerID = O.CustomerID INNER JOIN [Order Details] as OD ON O.OrderID = OD.OrderID ON OD.ProductID = OD.ProductID Where 1=1 -- AND O.ShippedDate Between '4/1/2008' And '4/30/2008' And P.productname = 'TOFU' Order By C.CompanyName Not sure why, but my NW stops at 98... so I don't get results on that... but... if I'm not mistaken... this translates to: FROM Products as P INNER JOIN ( Customers as C INNER JOIN Orders as O ON C.CustomerID = O.CustomerID INNER JOIN [Order Details] as OD ON O.OrderID = OD.OrderID ) ON OD.ProductID = OD.ProductID And then into... FROM Customers as C INNER JOIN Orders as O ON C.CustomerID = O.CustomerID INNER JOIN [Order Details] as OD ON O.OrderID = OD.OrderID INNER JOIN Products as P ON OD.ProductID = OD.ProductID It SHOULD be OD.ProductID = P.ProductID (or... [Order Detail].ProductID = Products.ProductID) Run the report with this instead: SELECT C.CompanyName ,O.ShippedDate ,OD.UnitPrice ,P.ProductName FROM Customers as C INNER JOIN Orders as O ON C.CustomerID = O.CustomerID INNER JOIN [Order Details] as OD ON O.OrderID = OD.OrderID INNER JOIN Products as P ON P.ProductID = OD.ProductID Where 1=1 -- AND O.ShippedDate Between '4/1/2008' And '4/30/2008' And P.productname = 'TOFU' Order By C.CompanyName Took me from 2k results... down to 22. "X inner join Y on 1=1" will basically return all possible combinations of X and Y. Also... the layout of the query is ugly as sin, and hard to read. Make it easier to read with shortcuts (as C, as OD) and better spacing (easier to match stuff together and read)
There doesn't seem to be a statement that specifies exactly *where* you JOIN on Products...
Can you explain the 1=1 concept to me? The 1=1 is in a WHERE clause, not ON, which in my mind means it's completely useless. edit: I am a novice, not saying you're wrong :)
Well "1=1" is used in two places on my post **1)** "X inner join Y on 1=1" - in the original query "[Order Detail].ProductID = [Order Detail].ProductID" - it's selecting all records, because ProductID will always equal ProductionID... aka "1=1" **2)** On MY query, in MY WHERE statement, I use it as a filler. It'll never exclude any records BUT it allows me to use AND at the start of the next line. Notice how I have: Where 1=1 -- AND O.ShippedDate Between '4/1/2008' And '4/30/2008' And P.productname = 'TOFU' Basically I've gotten into the habit of using "WHERE 1=1" so that I can easily comment out the lines below it... both AND statements if I want... easily... without having to edit stuff. Otherwise, you have to move lines around and pay attention to ANDs and it's generally easier, once you get used to it.
look at where I moved stuff around... the query has to be doing some weird joins. Notice the parenthesis on my second set. Unless I'm mistaken, it translates into: FROM Products as P INNER JOIN ( Customers as C INNER JOIN Orders as O ON C.CustomerID = O.CustomerID INNER JOIN [Order Details] as OD ON O.OrderID = OD.OrderID ) ON OD.ProductID = OD.ProductID
Number 2 is what I was referring to. And I must say... That is brilliant and I am going to adopt that habit. Thanks!
So, I was right in my thought that the join is joining [Order Details] twice. Thank you for confirming. I tried to keep the formatting identical to what was in the book just so the "wrongness" of it would not get lost in translation. So, the weird way it displays is probably intentional.
Ability to comment stuff out easily is also why I have my lines STARTING with ,'s (except for, obviously, the first one). I never comment out the first one (Almost always some kind of ID). I'll forget comma's at end of lines ALL the time when removing/commenting out last few lines.
It was intentionally hard to read, I tried to keep the formatting the same as the book has it. I don't yet know enough about SQL to say "oh this is wrong, but moving the text down a line won't effect the outcome." My apologies for it being hard to read, but I blame the text. Anyway, seeing the query in a more optimized fashion does make that easier to see what should be done with it. Thank you for your insight. 
Well... like anything, you get better at "reading" a language the more you use it. I wouldn't have been able to spot this, or move the stuff around easily, without years of practice. You might be able to read parts of Dune and Shakespeare in the second grade, but it's only after a lot MORE practice that you can really dig into books that deep. (Hell... only made it through Dune because of Audio books lol... to many big words... I digress...) And I'm also pointing out that I assume that's what is happening, because it's the only thing that makes sense to me. The two ON's back to back don't error out like I'd expect, so their has to be some grouping going on to let that query run without errors. Move, run, move, run... kept getting same results.
I don't use Access or MySQL, but I think you may have an extra ) just before your last ON. More important, I think you're going to run into problems joining tables from different sources. One approach would be to make local temp copies of the data from the disparate sources you want to join together.
Oh god if it's that simple I am quite embarrassed. I know the nested query needs to be encased in () and I assumed this included the AS clause which designates the alias. Maybe this isn't needed? I will test it out tomorrow. I have made the local temp copies and I'm using them to test, but this won't be possible with the end user of this query since the data in the source tables will be changing regularly.
Nothing much to add, just wanted to say that I completely agree with the advice you're giving, it's supremely helpful for fast debugging and playing with a query. I've been doing this for a long time myself (for the same reason, ease of quick commenting) but rarely see anyone mention it. TL;DR - OP, pay attention to what wernercd is telling you here, it's extremely sound advice.
upvote for "leading comma" convention... i've used this for many years and it's awesome, but you won't believe the hostility i've encountered from others who think it's wrong
1)I don't get why the same JOIN has to ONs... you should have ON and then AND... 2)One of the clauses is redundant... "[Order Details].ProductID = [Order Details].ProductID" why is that there? 3)Is there no proper clause to join Products and Customers? I imagine in the redundant clause, you actually meant "[Order Details].ProductID = Products.ProductID... that would make my all my points invalid and the whole query would make more sense... in which case I'd suggest JOINing Products AFTER [Order Details] for readability sake. I don't know of a good reason NOT to do that, and readability is very important... although I personally have no problems with your actual formatting of the code. edit: Example of what I think you were going for, and how I would format it. SELECT Customers.CompanyName , Orders.ShippedDate , [Order Details].UnitPrice , Products.ProductName FROM Customers INNER JOIN Orders ON Customers.CustomerID = Orders.CustomerID INNER JOIN [Order Details] ON Orders.OrderID = [Order Details].OrderID INNER JOIN Products ON [Order Details].ProductID = Products.ProductID Where Orders.ShippedDate Between '4/1/2008' And '4/30/2008' And products.productname = 'TOFU' Order By CompanyName Note that this will actually provide different results. You had a sort of cross join with Products which would have created a much larger (and probably less useful) result set in most databases.
Since I didn't write the query, I can't really answer the whys in your post. (I think you missed the part where I said this was an exercise for us to look at a query and tell what is wrong with it. But I'll forgive you, because you're awesome for answering.) I will answer with what I was thinking as I looked at it myself. 1. This was the original thing that jumped out as wrong to me. But I felt like I was missing something else hence my post. 2. It's there for astute people like you to catch. I originally though this particular line of the query was a recursive clause that called itself. I was mistaken. 3. No there is not. This was the part that I sat here thinking, "I'm missing something. I don't know what it is yet, but I know it's there." Thanks for going above and beyond what I originally asked for. While rewriting the query was not part of the assignment, doing so does help me better understand how it *should* function in a "best case" example. As for readability, I agree that it's important. But we aren't being taught that. It just isn't important to our instructor. I couldn't tell you why.
Never understood the hostility towards this, but I've faced that as well. It works well, is easy to read, and is still syntactically correct. What's the issue, right?
Not sure if it was mentioned but you should be using date functions when searching on date ranges. &gt; Where Orders.ShippedDate Between '4/1/2008' And '4/30/2008' Won't show orders after 00:00 on the 4/30/2008. You can either floor the date from the table to remove the hour and minute, add '23:59' to the end date or do something like &gt; Where YEAR(Orders.ShippedDate) = 2008 AND MONTH(Orders.ShippedDate) = 4 
tl;dr: Your application can't connect to the database, and the application's author didn't handle that failure very well. Make sure MySQL is running and is listening for network connections (not just sockets), and make sure your application is connecting to the right settings.
Nice tip, looks to work great!
nice thanks for this.
I installed the SQL Server Management Studio plugin for [poor sql](http://poorsql.com/). It turned a 1500 line query into 3000 lines and made it so much easier to fix.
Unfortunately this is not a good approach, as Year(date) is not Sargable and thus can't utilize an index on ShippedDate. The even show the year function as the first example here: http://en.wikipedia.org/wiki/Sargable
Cool, yep, I knew there had to be good alternatives. Same great concept :)
Getting all the data under one umbrella makes life loads easier here. Is this for a scheduled report/data extract? If so, scripting a mini-ETL in python or whatever you're comfortable with to get everything into one DB is pretty easy. If this is for a web UI or something, I'm told PHP will glue data together from different data sources...I have no direct experience there though. 
Glad to see I'm not the only one to capitalize all keywords. I go into a disagreement with a co-worker who says the standard is to leave the datatypes in lower case. So, we made that our standard...yet this formatter doesn't follow it. The other thing is that I'm so used to formatting as I go, this really didn't change much. 
Thanks for the Gold :) not sure what to do with it though lol Practice makes perfect, and it's crazy that I enjoy these small puzzles that help perfect the basics.
http://www.reddit.com/gold/about reddit gold = money for making reddit keep on going; so double bonus ;) personnaly i follow /r/SQL, for analysing how a beginner understand SQL and how as an expert i can teach them a fast &amp; easy method for mastering it :)
Also... SELECT DISTINCT x1.PK, tblPM.fieldMDPK, tblPM.fieldODPK, tblC.fieldC, x1.fieldCC, x1.fieldFNK, FROM Notice the "x1.fieldFNK, FROM" And... I think you actually have TWO extra )'s at the end. I'd suggest you learn how to use "Common Table Expressions" (IF Access lets you?) to help split your code up... Not sure if this will work in access or not: WITH x1 AS ( SELECT tbl2.fieldPK ,tbl2.fieldCC ,tbl2.fieldPT ,tbl2.fieldFNK ,tbl2.fieldM FROM tbl2 AS mirror INNER JOIN tbl2 ON (tbl2.fieldM = mirror.fieldM) AND (tbl2.fieldFNK = mirror.fieldFNK) AND (tbl2.fieldPT = mirror.fieldPT) AND (tbl2.fieldCC = mirror.fieldCC) WHERE ((tbl2.fieldPK) &lt;&gt; (mirror.fieldPK)) AND ((tbl2.fieldCC) = 102) ) SELECT DISTINCT x1.PK ,tblPM.fieldMDPK ,tblPM.fieldODPK ,tblC.fieldC ,x1.fieldCC ,x1.fieldFNK FROM tblPM INNER JOIN tblC ON tblPM.fieldC = tblC.fieldC INNER JOIN x1 ON tblPM.fieldPK = x1.fieldPK 
I'd say the formatter is badly formatting half the code then. There's [no right and wrong](http://stackoverflow.com/questions/292026/is-there-a-good-reason-to-use-upper-case-for-sql-keywords) on this. Myself I mostly use lower case since it contrasts with my uppercase DB object names (typing them in lower case would prevent me from using intellisense).
&gt; a co-worker who says the standard is to leave the datatypes in lower case he's blowing smoke out of... well, you know have him try to find an actual reference which says that sql keywords (including datatypes) are *not* case-insensitive in the standard (hint: he can't)
Has nothing to do with case-sensitivity. It's more of readability. Considering I started using SQL pre-colors, it was easier to distinguish keywords from our objects by using case. I capitalized ALL keywords. 
#2 is a terrible practice. It can lead to generating bad/less performant execution plans. It's also lazy.
I'm a big fan of Red Gate's SQL Prompt. Includes key combos for casing, shortcuts and auto completes for statements(i.e. inserts with the full column list to insert into), configurable formatting, and more. Whole team of db developers use it with the same set of options ensuring similar looking and easy to read code. The only thing that sucks is that typically a proc only gets formatted when someone goes in to fix something, causing the compare function to show that essentially the whole script has been modified. I'm guilty of not doing separate check-ins in the scenario more than I care to admit and cursing myself later.
[premature optimization is the root of all evil](http://www.codinghorror.com/blog/2009/01/the-sad-tragedy-of-micro-optimization-theater.html) &gt; you should **be more worried about the maintainability and readability of your code than its performance**. And that is perhaps the most tragic thing about letting yourself get sucked into micro-optimization theater -- it distracts you from **your real goal: writing better code.** [Good Programmers are Lazy and Dumb](http://blogoscoped.com/archive/2005-08-24-n14.html). &gt; Lazy, because only lazy programmers will want to write the kind of tools that might replace them in the end. Lazy, because only a lazy programmer will avoid writing monotonous, repetitive code – thus avoiding redundancy, the **enemy of software maintenance and flexible refactoring**. Mostly, the **tools and processes that come out of this endeavor fired by laziness will speed up the production.** The vast majority of the queries I write run near instantly... Occasionally, I'll have a report where I have to worry about going over 10 seconds - and I can optimize THAT query as needed. I don't really agree that THIS (1=1) makes me lazy though. This makes it easier to edit, and follow, queries while I'm working on them... and 6 months later when I have to edit or reuse a query. It ALSO makes it easier for my COWORKERS to read and follow my code. Everything is nice, lined up, flowing smoothly... Next: To some extent it's MORE work to keep the code I write lined up... I've meant to check out some of these "Auto formatting" tools, but never have because I can just do it by hand. Often, in retrospect, it takes as much time - or less - to write the code than it does to "prettify" it. I'm not going to stress about making a query run a micro second longer if it makes my job harder on a day to day basis. I don't work at facebook or google speeds and levels... I'll worry about THAT level of needed speed when I need to.
If you're not using this one you're stupid. jk
I kind of agree and disagree. The premature optimization link really speaks to me as there's always more than one way to solve a problem, and in a lot of cases, one's no better than the other. The other link you provide is dead on, but not for the reason you think. SQL formatting tools are examples of that concept applied well. But take this for instance: I don't like your formatting style. It's nothing personal, nor is there anything wrong with it. It's pretty consistent (to be fair I would wonder how you deal with other types of things like sub-queries etc but irrelevant to my point). But it's not my style, and be forced to stick with that, maybe I'd get used to it, maybe I wouldn't. It doesn't matter though, because the first thing I'd do is format it to my preferences. I've done no harm, and it's all the same code. What you are doing with the 1=1 is introducing variability into execution plans. On the small scale, sure it's probably no big deal. But looking down the road, you may not know how long that code lives or where it may be moved. Sure today is fine, but in the future, say some consolidation happens. Let's say your 1=1 check causes a bad plan to gen on a contentious server, not only are you impacting your own code, but others as well. For the record, I'm not saying this happens all the time, but it's out there. If it wasn't, people wouldn't discuss it. I don't think code has to be formatted "pretty" for it to be maintainable, nor ease of flexibility. That's all about proper design, not readability. You are right though, it's easier to comment a 1 line, rather than manipulate the "and" a little. Someone should write a utility for that :P tl;dr There's always more than one way to solve a problem with some ways being equal, readability is in the eye of the beholder, and proper design trumps all, and I still say that 1=1 is still a bad practice. 
Is this a one time task or will you need to run this report regularly? If it is a one time task, SQL Server Management studio has a built in "Export Data" tool (right click on the database, choose "Tasks" and "Export Data" and a wizard will pop up to walk you through it). You will have the choice of exporting data from one particular table or view, or to specify the query you want to use to export. If it is something you will do regularly, you have the option in that wizrd to "Save SSIS Package" - SSIS = Sql Server Integration Services, a tool that comes with SQL server that lets you create/save packages of tasks to run - such as data imports, exports or transformations. I don't really ever use it, so I don't know much about what you will need to do beyond saving the SSIS package in the wizard to get this to run on any sort of schedule. Your other option would require you to know another programmling language besides SQL - such as a .NET language (VB.NET or C#) or Java. All you would need to do is use any one of the number of ways to connect to a database via those programs, execute your query, and use the built in File I/O to write the results to a comma separate flat file. EDIT: I just saw that you are looking to output a .csv for each date out of the 365. Since I assume you dont want to do the "export" wizard 365 times, I would either investigate SSIS - I bet there is a way to save the package with a run-time variable that lets you specify which date to export. Or you can do the .net path and that would let you look your "SELECT/OUTPUT" over each day in a year. EDIT 2: Googled "using ssis to export csv" and found some links: http://social.msdn.microsoft.com/Forums/en-US/sqlintegrationservices/thread/41293231-1d96-4315-8884-d3a89abc2f66 http://www.codeproject.com/Articles/302599/Dynamically-Export-Data-to-Flat-File-Using-SSIS http://decipherinfosys.wordpress.com/2008/07/23/ssis-exporting-data-to-a-text-file-using-a-package/ http://www.mssqltips.com/sqlservertip/1775/different-ways-to-execute-a-sql-server-ssis-package/
Actually, this is what stops me. I don't know what's clientside, or server side. But then, I'm lazy and use Redgate's SQLPrompt, so I don't research it either.
My main beef with all of the intellisense-type formatters is that the little subsecond that the system takes to look up table names, column names etc irritates me excessively. I like the formating function but that's it. 
You've stoked my curiosity... [Good, Bad or Indifferent: WHERE 1=1](http://dba.stackexchange.com/questions/33937/good-bad-or-indifferent-where-1-1)
Goddamn developers and their crappy code formatting... SSMS Tools, SSMS Boost, and SQL Pretty Printer are all pretty decent.
I'd be more apt to use this when answering questions on places like this... "Help me solve this query? *insert badly formatted query here*"... No need to fire up SSMS... copy/new tab/paste.
I agree... somewhat. I've lost countless hours as well, but a lot of that also leads to memorizing stuff that never would have stuck in my head otherwise. So it's not *completely* lost.
I feel your pain. happens every time when i find some really feature in sql, like : merge, cross apply, table valued functions, aliases,coalsce and the compare of two records based on xml, for xml and the rest which just went for a short walk not to forget :)
hit that site about 5 times today
I actually haven't run into this with SQL prompt unless I've just connected to a database. It was like this with SQL Complete if I recall.
Same here. As long as intellisense is off, which it is by default after the installation, it's quite responsive. I just hate working on a customer system, etc. and expecting to do shortcuts like scf or st100 and being half way through the query before realizing I'm not in my dev environment.
It's fairly steep but in enough time it pays for itself. They also have a 2 week "pro" version fully functional trial that can be extended for 2 weeks by typing "i need more time" into the serial # box once you run out. Luckily, I was able to convince management to eventually opt for the full SQL Developer licenses to cover the entire team. At the discounted rate for the full software set it's well worth it but Prompt and SQL Compare(schema) get the most utilization(outside of the free SQL search that you can get for free - which I'd recommend if you don't already have it)
If you're using Oracle SQL Developer, Ctrl + F7 kinda works.
I think this guy is the winner. You could also use Excel's data functionality to query SQL Server, or a [reporting tool with a CSV emitter](http://www.eclipse.org/birt/phoenix/). But I don't see the need when unsigned_dg made it so simple.
Surpised it's not mentioned: [Coalesce](http://msdn.microsoft.com/en-us/library/ms190349.aspx) - picks first null in values. Coalesce - given a list of values, go through the list and pick the first one that isn't null. "SELECT COALESCE (null, null, 1) as result" returns "Result: 1" USE Temp; Go IF OBJECT_ID('dbo.Test') is not null DROP TABLE dbo.Test; Go Create Table dbo.Test ( ID int identity, Column1 varchar(10), Column2 varchar(10), Column3 varchar(10) ) GO INSERT INTO dbo.Test VALUES ('a', null, null); INSERT INTO dbo.Test VALUES ('a', 'b', null); INSERT INTO dbo.Test VALUES ('a', 'b', 'c'); INSERT INTO dbo.Test VALUES ('a', null, 'c'); INSERT INTO dbo.Test VALUES (null, 'b', 'c'); INSERT INTO dbo.Test VALUES (null, null, 'c'); INSERT INTO dbo.Test VALUES (null, 'b', null); INSERT INTO dbo.Test VALUES (null, null, null); SELECT ID, Coalesce(T.Column1 ,T.Column2 ,T.Column3 ,'OMG THERE IZN''T A RESULT!!!' ) AS 'Source_ID' FROM dbo.Test T GO Results: ID Source_ID 1 a 2 a 3 a 4 a 5 b 6 c 7 b 8 OMG THERE IZN'T A RESULT!!!
Well, in this example, coalesce is the easiest route - by far. Select first item that isn't null? Coalesce. Secondly, access has its places... just like GOTO's and premature optimization... Just like PHP. I'm willing to bet that Access isn't HIS decision to make. You can say "don't use X" all day, but when teacher, boss, customer says "Fix my Access issue" - you fix the access issue. (Edit: Or... in this case... migrating AWAY from access, hence the "translate this Access garbage to SQL")
Yhar, COALESCE would be better looking at it; I just suggest CASE WHEN THEN as the replacement for Access Iff in general. Whenever I see or receive Access questions, I always suggest moving off of the platform completely. I know it's redundant in many cases, but on the off chance that it's something like a person who is merely stuck with fixing an internal tool and hasn't thought about replacing it, I honestly believe that in most cases they can make a better product/program/system by moving off Access, and that they will spend less time total doing that then trying to fix and manage change requests in Access. I know most people understand this, but for the cases that don't, I really don't want to think of them wasting their time on Access. &gt; You can say "don't use X" all day, but when teacher, boss, customer says "Fix my Access issue" - you fix the access issue. ...and then you move the system off Access so you don't get stupid requests again. My first promotion was a result of migrating a large portion of our system off Access and into C# / T-SQL. In retrospect, I would have preferred to use Python and MySQL, but I did not know Python yet. The mode of 'make the broken thing work, but ignore the root issue (Access)' is a poor method that shouldn't be encouraged, and only barely tolerated.
MSSQLSERVER is directly accessible via (local) or MACHINE-NAME Anything else needs the (local)\ or MACHINE-NAME\ start (local)\SQLEXPRESS .\SQLEXPRESS MACHINE-NAME\SQLEXPRESS Your choice.
This is your answer, but to be a bit more specific, so you understand what's wrong here: You need to give SSMS more information about what it's connecting to. It always needs to know the server (as in machine, or VM) hosting the SQL instance. In ramse's example, this is (local), . (a dot is just a built in alias SQL has for local machine, it can ONLY be used when you are running MSMM on the machine also running the instance, this is also true for local) or [MACHINE-NAME]. You can also use the IP address, or a loop back address locally. Following that, you are giving it information to find the instance. In every example ramse gave, this is \SQLEXPRESS, the \ being a delimiter between the machine info and the instance name. This is the way you'll normally do it, but you can also use a comma and port, for example ,1433 (1433 being the default listening port). Hope this helps. Feel free to let us know if you have other questions.
Have a look at [this link](http://www.microsoft.com/learning/en/us/mcsa-sql-certification.aspx). Under "Upgradeable certifications" it states that if you have any MCTS on SQL Server 2008 (e.g., 70-433), you can take two upgrade exams to convert it to a MCSA (2012) certification (note that the whole MCSA scheme separates the certifications from specific versions a bit more, but at this stage, it's the 2012 set). The upgrade exams you need are: 70-457 Transition Your MCTS on SQL Server 2008 to MCSA: SQL Server 2012, Part 1 70-458 Transition Your MCTS on SQL Server 2008 to MCSA: SQL Server 2012, Part 2 
Your question isn't really clear... and your formatting doesn't make it any easier to read. Add 4 spaces to start of lines you want to format. IF Object_ID('tempdb..#TP') is not null drop table #TP; create table #tp (id int, job varchar(255)); insert into #tp VALUES (10, 'SR'), (10, 'JR'), (14, 'KR'), (10, 'PR'), (10, 'KR'), (11, 'SR'), (12, 'JR'), (12, 'PR'), (13, 'PR'), (14, 'SR'), (12, 'SR'), (11, 'PR'), (11, 'PR'), (13, 'SR'); GO Or maybe you want DISTINCT? You want the duplicate values (11 PR) removed? SELECT DISTINCT ID, JOB FROM #tp Order by ID, JOB GO Do you want "MAX" as in "2 is greater than 1" max? Or "Max" as in "Most recent"? or Oldest? -- Group by ID -- ...Generates Counts, Max, Min SELECT ID ,Count(*) Count ,Max(job) Max ,Min(Job) Min From #TP Group By ID ORDER BY ID 
Cheers mate, thanks for that. I totally missed that part of the site. What's worrying though, is this part of both upgrade exams: &gt;Preparation Materials Preparation Tools and Resources To help you prepare for this exam, Microsoft Learning recommends that you have hands-on experience with the product and that you use the following training resources. These training resources do not necessarily cover all of the topics listed in the "Skills Measured" tab. &gt;Learning Plans and Classroom Training There is no classroom training currently available. &gt;Microsoft E-Learning There is no Microsoft E-Learning training currently available. &gt;Microsoft Press Books There are no Microsoft Press books currently available. &gt;Practice Tests There are no practice tests currently available 
It does look foreboding! A quick look at the contents looks like it contains content from all three of the exams [70-461](http://www.amazon.com/Training-Kit-Exam-70-461-Microsoft/dp/0735666059), [70-462](http://www.amazon.com/Training-Kit-Exam-70-462-Administering/dp/0735666075), and [70-463](http://www.amazon.com/Training-Kit-Exam-70-463-Implementing/dp/0735666091). I haven't started looking at 2012 certification yet, but it is on my list for this year, so I'll be making the same decision soon - go three books, and three exams, or three books and two exams? At least with 3 exams, you know exactly which material will be in each. 
I believe SQLite will silently ignore varchar and other unsupported column types (there are only a few valid types and they're basically all aliases for text with weak type checks). The big issue is that sqlite doesn't support that syntax for inserting multiple rows that this database is using. If you can code your own it wouldn't be too hard to import the data into MySQL, SELECT it all and then reinsert it as SQLite. You could do something like this DROP TABLE IF EXISTS `new_sms_download`; CREATE TABLE `new_sms_download` ( `sender` TEXT NOT NULL DEFAULT 'unknown', `receiver` TEXT DEFAULT 'unknown', `send_time` TEXT DEFAULT 'unknown', `collect_time` TEXT NOT NULL DEFAULT 'unknown', `collect_method` TEXT DEFAULT 'unknown', `content` text NOT NULL, `native` TEXT DEFAULT 'unknown' `country` TEXT DEFAULT 'unknown' `age` TEXT DEFAULT 'unknown', `input_method` TEXT DEFAULT 'unknown', `experience` TEXT DEFAULT 'unknown' `frequency` TEXT DEFAULT 'unknown' `phone_model` TEXT DEFAULT 'unknown', `collector` TEXT DEFAULT 'unknown', `gender` TEXT DEFAULT 'unknown', `smartphone` TEXT DEFAULT 'unknown', `lang` TEXT NOT NULL DEFAULT 'en' `city` TEXT DEFAULT 'unknown', `id` INTEGER PRIMARY KEY ); BEGIN TRANSACTION; INSERT INTO `new_sms_download` VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?); ... COMMIT; edit: I got it to insert. will make a reply explaining how in a minute.
I'm going to try what you said as soon as I get back to my desk, but do you think you could post the file anyway, in case I can't get it exactly? This way I can try the file you've got working and if it doesn't work, I'll know it's an issue at my end. I'm not normally one to get someone to do something for me, but I've been at this for days and it'd be really great to have it working. Also for some reason you've been downvoted once (you're at 0 at the moment) but that wasn't me, so I'm not sure why that's been done. I'll upvote you.
sure here you go: http://cl.ly/Mcg3 edit: i dont know why i didnt zip that first. changed the link.
Sorry to be a pest, but this link comes up as broken. Is the file still there?
i just edited the link because i uploaded 16mb text file, i zipped it.
Thanks, got it! How do I go about inserting this file into SQLite?
try sqlitespy - http://www.yunqa.de/delphi/doku.php/products/sqlitespy/index you can just paste it in and press execute. Usually I use http://code.google.com/p/phpliteadmin/ &lt;--- that, to manage databases, if you have a web server.
I'm using the latter as I'm familiar with phpMyAdmin, but I'm getting syntax errors when I paste the following into the SQL function: DROP TABLE IF EXISTS `new_sms_download`; CREATE TABLE `new_sms_download` ( sender TEXT NOT NULL DEFAULT 'unknown', receiver TEXT DEFAULT 'unknown', send_time TEXT DEFAULT 'unknown', collect_time TEXT NOT NULL DEFAULT 'unknown', collect_method TEXT DEFAULT 'unknown', content text NOT NULL, native TEXT DEFAULT 'unknown', country TEXT DEFAULT 'unknown', age TEXT DEFAULT 'unknown', input_method TEXT DEFAULT 'unknown', experience TEXT DEFAULT 'unknown', frequency TEXT DEFAULT 'unknown', phone_model TEXT DEFAULT 'unknown', collector TEXT DEFAULT 'unknown', gender TEXT DEFAULT 'unknown', smartphone TEXT DEFAULT 'unknown', lang TEXT NOT NULL DEFAULT 'en', city TEXT DEFAULT 'unknown', id INTEGER PRIMARY KEY ); This is SQLite syntax, right?
I can't get it to give me an error, [it works fine](http://i.imgur.com/hZpZvCl.jpg)...
Thanks! I got it working just before I got this from you, so I saved and compared both files. I forgot to reply here so here I am. Thanks for all of your help!
Bad. It generally means there was bad design
 Triggers to enforce data integrity are usually a bad idea (for reasons given by one of the posters in that thread - they don't necessarily work and generally aren't the right tool for the job). Triggers which contain large chunks of business logic are usally rather troublesome from a performance and/or management perspective. I've worked with systems where some tables had 15+ triggers on them, many of which contained 100+ lines of code and/or contained very expensive queries. They also made multiple changes to other tables which in turn also had multiple triggers. Perhaps unsurprisingly, as our database grew performance got worse and was often dreadful. Worse than this, it made a lot of updates slightly unpredictable - it wasn't clear (to users or even those of us administering the database) what the exact impact would be of updating anything or the order updates were applied in. Triggers however (for me) still serve a valuable role for certain tasks. Auditing is the obvious example, where we might want to update a column to show the last time a row was updated, or the user who made the amendment. Alternatively we might want to write to some sort of history table in the event of certain changes. Yes, these things could be handled in the stored procedures responsible for updates, but what happens when someone makes a change in the database directly? In these cases though the query being executed is not complex or expensive. There is a grey area of certain types of business rules which can't be enforced by basic database functionality. It's straight forward to have a constraint which says "Don't allow items to have a unit cost above 5000 GBP" but I've tended to find that the business rule ends up being "Don't allow items to have a unit cost above 5000 GBP *unless* this project is of type X and the user is of type Y and we're at a particular progres stage and....". Sometimes you're forced to do some of the validation in the client/application side but I've seen that approach fail multiple times where (for one reason or another) some data bypassed the client while making it's way into the system. I'd tend to avoid triggers for these purposes too but I can at least see the argument for using them.
I know basically nothing in SQLite; but, this should be possible. As a PowerShell junkie, I'll go ahead and recommend it. It's going to be a matter of getting the results from the database into an array and then using a ForEach loop, ala: $myArray = ('c:\users','c:\temp','c:\windows') ForEach($path in $myArray) { &amp; C:\Windows\explorer.exe $path } The trick (which I am not sure offhand how to do) is going to be getting the data from the database into PowerShell. In MSSQL, I usually use something like: # Create a connection string $sConnectionString = "Data Source=myServer;Initial Catalog=myDatabase;Integrated Security=SSPI" # Create the connection object $oConn = New-Object System.Data.SQLClient.SQLConnection #Assign the connection string to the connection object $oConn.ConnectionString = $sConnectionString # Create a SQL Command object $oCom = New-Object System.Data.SQLClient.SQLCommand # Assign the connection object to the command object $oCom.Connection = $oConn # Create the SQL Query string (this should be SQL syntax $sQuery = "Select [myColumn] from [myTable] where [myIdentifier] = 'use this row'" # Assign the query to the command object $oCom.CommandText = $sQuery # open the connection $oConn.Open() # execute the command $oReader = $oCom.ExecuteReader() #Collect the results into an array $myArray = @() if($oReader.HasRows) { # Gets each row of the result set iteratively While($oReader.Read()) { #gets the first column of the row as a string $myArray += $oReader.GetString(0) } } # Close database connection $oConn.Close() #and then to open, we use the code from above ForEach($path in $myArray) { &amp; C:\Windows\explorer.exe $path } This should work for MSSQL, and I would think it would work for other SQL RDMS in a windows environment; but, *caveat emptor*. 
Do you control the output process? Or is it a file you have no control over? Are you saying that your file is comma delmited, with `\,` to mark a comma that shouldn't be a delimiter? You have an example line, or two? of good data...
Unfortunately I do not have control of the output process and I cant share a sample of data as it is healthcare/patient protected. I can only say that it can look like this: 1,,"text" 2,"test",'text"
Yeah, meant to use a single quote, the second column can be null or text... 1,,"text" 2,"test","text" My file format has 1 SQLCHAR "," 2 SQLCHAR ",\"" 3 SQLCHAR "\"\r\n"
Are YOU escaping stuff to post HERE? or does your text file actually have \'s in them? \r\n is an end line, but you won't actually SEE \r\n in the code. The only thing that would confuse me, personally, is if the actual TEXT had quotes or comma's in it: ,,"Testing one, two, three..." Assuming the actual text doesn't have those... things become incredibly easy. Create a [schema.ini file](http://msdn.microsoft.com/en-us/library/windows/desktop/ms709353(v=vs.85).aspx)... Say C:\Data\schema.ini [YourCSVFileName.csv] ColNameHeader=True Format=CSVDelimited Col1=A Text Width 100 Col2=B Text Width 100 Col3=C Text Width 100 Sample Test data (includes header row): C:\Data\YourCSVFileName.csv A,"B","C" a,b,c ,,"C" ,"C", Create an [ODBC Text Driver](http://www.c-sharpcorner.com/UploadFile/mgold/ConnectODBCText11262005070206AM/ConnectODBCText.aspx) (ignore C# programming part... just create an ODBC Driver. F1.01 through F1.05, although you can ignore the File definition, since we already made one with the schema.ini... another way to make it). Connect to your ODBC connection, and SELECT `A`, `B`, `C` FROM `YourCSVFileName.csv` A B C a b c C C IF your data has comma's and quote's... ACTUAL comma's and quotes... this won't work, and you'll have to come up with something else... (actually... this answer only gets f'd by ,'s. IF your text doesn't have commas, and you can guarentee that, then this is easy) But schema.ini\odbc text driver... would be an easy answer otherwise.
I'm escaping stuff with the \ because that's what the bulk insert file format tells me to do in regards to double quotes and line terminations. So when you see me say "\",\"" I'm really saying the field terminates in "," Because the bulk insert does not recognize double quotes as a text qualifier, you have to define each field and use the double quotes in combination with the comma as a field delimiter. Given that I've defined the delimiters in such a way, commas in the field should be no issue. It's when there is a null in the field in 1 row, and text in the next row. Like so.... 1,NULL,"Text 2" 2,"Text3","Text4" On line 1, the end of field 2 on row 1 is ","....just a comma On line 2, the end of field 2 on row 2 is "\",\""....double quote, comma, double quote 
I am not familiar with SQL Express or Bulk Insert however that output format is the standard format for a CSV file. Does bulk insert expect a file? Or do you parse the file in a separate program and generate the bulk insert statement?
It's useful in that context if you're planning on creating your own rdbms or working on an open source one with hopes of building/improving the optimizer used to generate query plans. It's also useful if you're actually analyzing the business/organizational data you're collecting. If you plan on moving into a business intelligence/data warehousing role then learning statistics is extremely valuable.
I was looking for info in regards to the optimizer as you were saying. So, knowing statistics won't necessarily improve skills with the optimizer? I don't have any plans to home grow a RDBMS. On the other hand, I already work with data warehousing to an an extent. So, I might look into Statistics for that purpose.
Statistics form a part of the optimizer. But unless you're diving really deep into the internals, the only thing you need to do is make sure that your database updates statistics for certain tables on a regular basis so that the optimizer can make the most intelligent decision possible.
In the cases I've seen there's been differing causes: 1. Client validation had been explicitly turned off (temporarily) and left disabled for longer than originally intended (i.e. someone forget to switch it back on). Worryingly, this was on a financial system and led to overpayments to suppliers of £200k (~$315k). Theoretically recoverable but a major pain and obviously hugely embarrassing for all involved. 2. In an order processing system for a retailer different rules were applied to transactions manually entered vs. those imported from their website. As it happened the imported orders constituted about 70%-80% of all business activity and for these, a couple of the rules weren't applied at all (until we noticed a while later). 3. In other cases, data has been manually imported (via processes outside the client) - e.g. using SSIS, BCP or manual SQL statements. Some systems don't have very good bulk processing functionality so doing it direct in the DB is the only feasible way to update 8,000 separate orders/accounts/etc. There isn't necessarily anything wrong with this but it puts the onus on the people doing the imports/updates - they *really* need to be familiar with the logic applied by the client (and the data generally) since the database isn't going to catch any violations. 
Looking closer... when you use this script, what errors (Bad data import, or similar) do you get? BULK INSERT CSVTest FROM 'c:\csvtest.txt' WITH ( FIELDTERMINATOR = ',', ROWTERMINATOR = '\n' ) GO --Check the content of the table. SELECT * FROM CSVTest GO 
Go to sqlskills.com, find the places Kimberly Tripp is speaking, and go listen to her talk about indexes and statistics,. If you have a specific problem, ask her during a break. (I'm not affiliated with them in any way, but I do look like a hero at work because of her and Paul) 
Programming languages like R or Python have SQL integration as well as statistics. Well worth looking into it.
Date doesn't exist in 2005, afaik... Datetime? yes... Date? No... Unless we are talking about something totally different than [date](http://msdn.microsoft.com/en-us/library/bb630352.aspx) - notice, the drop down for "Different Versions" - only available in 2008 and 20012. but... the most important thing: He's talking DECLARE &amp; SET in one command. in 2005 you have to do two statements: DECLARE @VAR int; SET @VAR = 42; In 2008+ you can declare and set in the same line: DECLARE @Var int = 42;
Thankyou. If I go to SQL Server Configuration Manager and click on SQL Server Services it says there are no items to show in this view. I'm not sure if this is correct or how I can get these services running.
Are you trying to connect to the same machine? Or from a different machine?
I have checked my services list and I have no SQL related services, I have checked a colleague and he has 4, any idea what this could mean? 
Don't take this the wrong way, but is it actually installed? When you installed SQL Server, did you select "Database Engine" and not just the client tools (i.e. Management Studio)?
You could try running the installer again and make sure the engine itself is being installed. I haven't installed Express in a while but it should be one of the default options. But when you say you have no SQL related services - you have checked in services.msc, correct? Finally, what rights do you have over the machine presently? Are you a local admin?
I will try un installing and running the installer again to make sure I'm not missing anything. Yes I am checking in services.msc and I have no SQL related services and I am local admin.
Try doing a Telnet test to the database. Make sure that you have Microsoft telnet client installed. open CMD, use the comand Telnet SERVERNAME 1433 Replace SERVER name with the name of the server running SQL, Port 1433 is the default port used by SQL. 
you may also wana check that you are on the same domain as the server you are attempting to connect to. 
This will only work if something is actually listening on 1433 (SQL server is running properly). Telnet only works when there is a service listening on the destination port.
Hi guys thanks for the help, this is now resolved, I managed to fix it by also installing Microsoft SQL Server 2008 R2.
first bug: Lines starting with four spaces are treated like code: if 1 * 2 &lt; 3: print "hello, world!" Help us help you.
Take a video of yourself learning it from a book / playing with Northwinds DB. We'll probably find it funny. 
Better?
I don't think INT is a valid data type. Try changing it to NUMBER. You don't use the SET command to assign values to variables ( you're doing it correctly elsewhere). And where you're trying to assign a value into v_Count, you're telling it to give a column alias to the count(*) called "v_Count". You want to do this: select count(*) into v_count from holidays where &lt;....&gt; You ported this from T-SQL or something?
Thank you! I made the corrections that you suggested. I wasn't getting an error with the INT statement but I changed it just in case. The errors that I am getting now: 1. at most one declaration for v_total_days is permitted. 2. identifier 'DATEPART' must be declared -----I think this should be: v_DayOfWeek :=to_char(pi_startDate,'D') 3. expression 'PI_STARTDATE' cannot be used as an assignment target For #1 ... how do I set the v_total_days = 0 then +1 later when counting holidays or weekends? Again, first time attempting this - I appreciate your help. Oh and [here](http://www.tek-tips.com/viewthread.cfm?qid=937309) is where I saw the orig function Edit: Format, links, and answering my own question in part 2 maybe 
I like http://www.w3schools.com/sql/default.asp. Pretty much gives you all the basics. I think you can also write and run practice queries with the SQL Demo.
Yes. I also suggest a firm understanding of set theory (critical) and discrete math.
**Thank you very much** for your reply. I changed the code as you suggested, using a temp var for the pi_startDate. It works now - yippee!
No. That's a horrible test. Why would I look at a mugshot at any point in my education on programming and code? Why would I know what any programmer looks like? I'm not trying to fuck them, for fuck's sake! Seriously, I don't even know what Linus looks like and Linux has eaten half my adult life's free time. Past all that, the guy has not really provided anything historically significant to computational theory or computer science. Yeah, he knows what he's talking about, and yeah, he's a popular character in a big position, but *makes masturbatory motion here*. Seriously. Fuckin' stupid. I hope you are trolling, in which case; good game, sir.
Normalized relational tables. Create one table for users (id, user_name) and one table for posts (id, user_id (foreign key to users table), rest of post data) This way if you want to look up all posts for one user you just pass the user id in the query. The foreign key enforces an index (at least in mysql) so the query will be fast. 
So would the table have multiple instances of the user_id, with unique IDs? So the unique ID could be 1, 2 and 3, but the user_id would always be the same because all of the data is related to the user?
[Example of normalized tables](http://sourceforge.net/apps/trac/tenor/attachment/wiki/GettingStarted/database.jpg) You have a CategoryID (with associated table that has the name)... Information about that Category. Then you have a Post, which is on CategoryID... Then you have Comments... which are by PostID... Adjust this to add a Person table, and make the post by CategoryID, PersonID...
Users table * id, name * 1, dartalley * 2, tehhnubz post table * id, user_id, title, post, ... * 1, 1, my first post, blah blah * 2, 1, my second post, blah blah * 3, 1, my third post, blah blah * 4, 2, your first post, blah blah * 5, 2, your second post, blah blah * 6, 1, my fourth post, blah blah
do you mean, you have to import DATA into sql server from files, or actually import files into SQL Server via a linked file or a BLOB? if just holding files for others to get, it may be simplier to have them go to a specific folder on a SERVER (typically best to NOT be the sql server) and the web app is given a link to that server in order to download. or its IMPORT data from files, and then web users download that data? can you clarify?
Would you happen to know a good link on how to insert files into SQL?
I need to both import data from some files (.txt), and other files I need to be able to provide (zipped files). The preference is that the only connection between the web server and the sql server is the ODBC connection.
not being snarky, but why aren't you just using sharepoint then? or was that just a comparison for comparison sake? filestreams may be an option too: http://beyondrelational.com/modules/2/blogs/28/posts/10369/sql-server-how-to-load-data-into-a-filestream-enabled-column-using-tsql.aspx though loading a picture file into the sql database isn't the best method. and as for the ZIP, do you mean to import the data from within the zip, or just host the zip for the end user? 
I was just using sharepoint as an example for comparison. Unfortunately, this project is a custom application and SharePoint was not a suitable product for this task. I am not actually loading images, but used that as an example as well. The zip files are to be provided to the end user for download.
one method: http://www.sqlservercentral.com/Forums/Topic1147081-392-1.aspx or http://stackoverflow.com/questions/7134032/store-binary-files-with-sql-server-management-studio and personally, i love powershell http://philergia.wordpress.com/2011/03/18/inserting-binary-objects-into-a-sql-server-table-using-a-stored-procedure-and-powershell-2/ 
cluster index on runid? (+ columns to make it a key)
Thank you.
WOOT
This is how it should look like. where tags in(@tag1,@tag2,... @tagn) I found this [function](http://blog.logiclabz.com/sql-server/split-function-in-sql-server-to-break-comma-separated-strings-into-table.aspx) but i can't make it work propriety. This is what it loosk like now. @tags NVARCHAR(MAX) --(Gaming, PC, Ubisoft) SELECT DISTINCT Products.Name FROM Products INNER JOIN Tags_Products ON Products.ProductID = Tags_Products.ProductID INNER JOIN Tags ON Tags_Products.TagID = Tags.TagID WHERE Tags.Name IN (SELECT * FROM [dbo].[Split] (@tags, ',')) 
Both Queries are the same (and execute in the same time): | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+---------+-------+---------------+---------+---------+------+-----------+--------------------------+ | 1 | SIMPLE | runData | index | NULL | PRIMARY | 12 | NULL | 124855802 | Using where; Using index | 
already is a clustered primary key
what is your primary index? can you run show create table EDIT: Also how many run ids are there?
Correct. It sounds like you installed the SQL management tools but not the database service itself.
I have written all the SQL at my school district. I wrote a script to combine all the text of all the "saved" queries I've written in the past 3 year and it was just above 251k rows. I'm notorious of word wrapping the columns being selected unless they require a lot of manipulation. I can't imagine if used a carriage return after each item. Usually if I have to go back and reformat, its because I didn't have the time to write it correctly the first time.
I use the poor sql command line formatter in conjunction with database NET. So great. I use SSMS now and then too but prefer database NET, lightweight and self contained. (fishcodelib.com)
&gt; ... a front-end application to search SQL data not to be pedantic, but SQL is a language, not a database system presumably you meant *SQL Server* (the microsoft database system), fairly obvious from the context, but it would help if you actually said that 
 SELECT customerid FROM tickets WHERE ticketdate &gt; CURRENT_DATE - INTERVAL '30 DAYS' GROUP BY customerid HAVING COUNT(*) &gt; 1 hope i got the date arithmetic right, i'm not a regular postgresql user
It's a good thing you posted here, because Google will definitely not have any results on this topic. Also, the sidebar to the right is useless and does not have anything remotely related to what you're asking. Please be vigilante in these trying times. There is a lack of information SQL on the internet these days. It's tough but we'll make it through it!
Something like this possibly?: select ta.customerid from tickets as ta inner join tickets as tb on ta.customerid = tb.customerid and ta.id != tb.id and ta.ticketdate between tb.ticketdate - interval '30 days' and tb.ticketdate group by ta.customerid 
I've really liked JasperSoft/iReports for developing any JDBC based SQL reports. They also have JasperServer which can allow for ad-hoc reporting.
Thanks, but I need to essentially bring tableB.text back as two separate columns. 
Pentaho . Free, open source
I got it with the following SELECT b1.text as 'Num1' ,b2.text as 'Num2' FROM tableB b1 join tableA a on b1.value = a.Num1 join tableB b2 on b2.value = a.Num2 WHERE b1.id = "1" AND b1.status = "A" and a.acct = 'abcdefg' Thanks again for your help!
The above Self Join works great to detect the presence of a customer who had more than 1 ticket in the period. I would go with this one unless you need to see the tickets purchased within 30 days of another ticket.
[sqlzoo](http://sqlzoo.net) disclaimer: don't know if it's like codecademy, i've never seen codecademy
w3schools
Not exactly like codeacademy but below are few very good tools/sites 1. [GalaXQL](http://sol.gfxile.net/galaxql.html) - Downloadable app that lets you learn progressively, uses stars and galaxies as sample data (its more fun that way) 2. [SQLRU](http://www.sql-tutorial.ru/) - Interactive SQL textbook. Good explanations, lets you run queries directly on different RDBMS from the site. 3. [SQL Exercises](http://www.sql-ex.ru/) 4. [Learn SQL The hardway](http://sql.learncodethehardway.org/) - I like this text book a lot. Uses sqlite for exercises. 
There's a giant market popping up for this kind of thing. I don't know of any SQL classes like Codeacademy, but In the mean time I'd recommend W3schools since SQL isn't so difficult that interactive courses as opposed to just 'try it out' tutorials make a large difference. In the next year or so I would expect a codeacademy/codeschool/other online interactive programming school to have a course in it.
i've glanced at their sql stuff and it's way too simple, and if it's anything like their html stuff, it'll have errors i wouldn't trust that site as far as i could spit, since they're all about the ad revenue and not about quality or accuracy see http://w3fools.com 
Absolute luck. I didn't see that post yesterday but I did see this one today. I never pass up an opportunity to pimp out The Schemaverse and would have responded if I saw it :) 
&gt; what's the secret for getting help? use something other than oracle /s 
Will have to give this a go.
Some tips: -Don't get discouraged (it's hard to get started admittedly) -Feel free to ask for help! (#schemaverse on irc.freenode.net) -Don't be afraid to create a new account if you home base is lost for the round
Thanks! I always love to hear of any feedback you can offer too. I am working on a new version at the moment so now is a great time for suggestions. 
that's not the right site sqlzoo.com is a garbage site, you meant [sqlzoo.net](http://sqlzoo.net) 
This should get you started on the XML. SELECT * FROM &lt;table name&gt; FOR XML RAW Quite frankly there are a LOT of ways to format and manipulate XML from SQL server. I'd recommend looking up "FOR XML" in SQL Books Online and going from there.
Datatypes also need to be compatible. Not sure if that applies when using UNION ALL.
it applies to both, whether using UNION DISTINCT or UNION ALL
You should exclusively use IDs in every table that is not the ID definition table. For instance, in the Drug table, you have a DrugID and a DrugName and the Interacts table has DrugID1 and DrugID2, not DrugName1 and DrugName2. I believe this is a rule of the [3rd normal form for database normalization](http://www.guru99.com/database-normalization.html). I'm kind of new to this so someone please correct me if I'm wrong.
It is just good practice.. Names change, ids don't
From a storage optimization standpoint, if I have a list of medical symptoms that go back to conditions, and each condition has symptoms listed for it, then I will have to store a lot more information than an ID. If I assign a 3 digit ID number to a condition and a 4-5 digit ID number to a symptom with a 1:n relationship for condition to symptoms. A disease with 30 symptoms would have its name repeated 30 times with each symptom listed once. In the case of a condition like "Pneumonoultramicroscopicsilicovolcanoconiosis"; the storage space requirements would be much larger than the number "1252". 
1) first of three SELECT COUNT(*) apples_and_oranges FROM ( SELECT a.id FROM a INNER JOIN b ON b.a_id = a.id AND b.x IN ('apple','orange') GROUP BY a.id HAVING COUNT(DISTINCT b.x) = 2 ) d lather, rinse, repeat
This is in TSql... but... USE Test; GO if object_id('dbo.TabA') is not null drop table dbo.TabA; create table TabA (ID Int); GO if object_id('dbo.TabB') is not null drop table dbo.TabB; create table TabB (ID Int, X varchar(20)); GO insert into TabA Values (1),(2),(3),(4),(5),(6),(7),(8),(9),(10); GO insert into TabB values (1, 'Orange'), (1, 'Apple'), (1, 'Peach'), (2, 'Apple'), (2, 'Orange'), (3, 'Apple'), (3, 'Peach'), (4, 'Orange'), (4, 'Peach'), (5, 'Apple'), (6, 'Orange'), (7, 'Peach'); GO SELECT App.X as "Apple" ,Ora.X as "Orange" ,Pea.X as "Peach" ,Count(*) FROM TabA as A Left JOIN TabB as App on A.ID = App.ID and App.X = 'Apple' Left JOIN TabB as Ora on A.ID = Ora.ID and Ora.X = 'Orange' Left JOIN TabB as Pea on A.ID = Pea.ID and Pea.X = 'Peach' Group By App.X ,Ora.X ,Pea.X Order By Apple Desc ,Orange Desc ,Peach Desc Apple Orange Peach Cnt Apple Orange Peach 1 Apple Orange NULL 1 Apple NULL Peach 1 Apple NULL NULL 1 NULL Orange Peach 1 NULL Orange NULL 1 NULL NULL Peach 1 NULL NULL NULL 3
I am not sure what you are going for with "Prescribes", "Prescription", and "Takes", i would do something like Patient 1:n Prescription/Takes PatientPrecriptions PatientID PrescriptionID PhysicianID PrescriptionDate 001 001 021 02/18/2013 002 002 074 02/19/2013 PrescriptionPharma Prescription Drug Dosage Frequency StartDate EndDate 001 001 44mg 4 ppd 02/18/2013 NULL 001 002 12mg 1 qad 02/18/2013 NULL 001 003 8mg 1 qqh 02/18/2013 NULL And this is just a cursory look. Im sure someone else has a different idea in mind. edit: Formatting
Not totally sure what you're going for here but there are some performance tools that you can use with SQL Server that can help you test the IO. *[SQLIOSIM](http://support.microsoft.com/kb/231619) - used to perform reliability and integrity tests on disk subsystems. These tests simulate read, write, checkpoint, backup, sort, and read-ahead activities. *[SQLIO](http://www.microsoft.com/en-us/download/details.aspx?id=20163) - used to perform benchmark tests and to determine I/O capacity of the storage system
I have heard a couple people having this issue on the tutorial. What browser(s) have you tried? Do you use any script or cookie blocking add-ons? I will ask one of the other folks who had this issue and see how they resolved it. Thanks for your patience :) Edit: Another thought, sometimes it's just that the user isn't created right (I don't confirm pw's with a re-type). Try creating a new user as well and see if that works. 
You are on the right lines of what I need, I have had a little look in to SQLIO but not SQLIOSIM. What I was going for was to gather hardware performance data using a utility whilst actually running data intense tasks on an SQL database. I will be doing this in multiple environments (VM's via. Hyper-V and ESXi, standalone etc..) so I can reflect how each performs. I'll go and have a read about SQLIOSIM now though, thanks! 
Glad you like it! Since you have tried a bunch of other resources I would love to hear your feedback on where I can improve it :)
Keep in mind that SQLIO runs a "SQL Server like load" against your IO sub-systems. It doesn't actually touch your DB and is a completely separate application from SQL Server. It will have no indications as to the performance of your DB architecture. 
This is the correct answer. You should run these and get a baseline prior to deployment and then you have a comparison later on if you suffer io issues. 
SELECT column1[, column2] FROM tablename [WHERE column1 = somevalue AND column2 = someothervalue]; 
&gt; Is this the right place for a question? Yes.
**Warning: Assholish NSFW language coming** Dude... Dude. Come'on. Sure, I can help. I'm good at moving furniture. Need a moving buddy? Maybe you have some math homework you need to do. I used to tutor math classes. I'm also good at cooking, if'n you need to cater an event or something. Oh? You want SQL help? That's why you're here? Well then... What do you need help with? No, I'm not going to go read sqlzoo.net for you. No, I am not psychic, and I don't know where you are on that site. Neither am I willing to write a help case for every possible solution. So, instead, I'm going to teach you how to ask tech questions. Perhaps with this knowledge, you will run into condescending assholes like myself less often. You'll also probably leverage better help from people like myself too. First, we start with the domain of the issue. Fortunately, you're almost already good-to-go expressing the domain. The 'domain' of a question is the area to which the question pertains, namely, the context. Example: "I'm starting to learn SQL writing..." - that's the domain of the question. However, we need you to be more specific. Are you using MySQL? T-SQL (MSSQL)? PostgreSQL? What type of OS are you on? Windows? Mac? Linux? What specific version of the database engine are you using? MS-SQL 2008 R2? MySQL 5.1? What tools are you using to connect to the database and run queries? MS-SQL Management Studio? SQLYog? Also, is your mother hot? Like, how hot? Fuckable at least? Finally, are you following any specific course work or tutorial (like sqlzoo.net)? I mean, we're not going to read it, but it's good to know. Sometimes, certain tutorials (such as sqlzoo.net) are known to be steaming piles of monkey poo, and finding out that you might be using those might get us to suggest a better place to learn SQL (http://sql.learncodethehardway.org/book/). Once we have domain covered, we need to move onto the question. So, at this point, we know what we're talking about, but we don't know why we're talking about. I give you an F- on the question domain, because, **(pet peeve/irrational reaction coming up)** YOU DON'T MENTION A GOD DAMN WORD ABOUT WHERE YOU ARE STUCK, WHAT THE PROBLEM IS, ANY ERROR MESSAGES YOU MIGHT BE GETTING, A LINK TO YOUR CODE (Pastebin mother fucker; have you heard of it?!!!), AND ALL IN ALL, HOW THE FUCK ARE WE SUPPOSED TO HELP YOU? HERE, LET ME GET MY KNOWLEDGE RAY AND I'LL JUST ZAP EVERYTHING I KNOW ABOUT SQL INTO YOUR FUCKING HEAD ALONG WITH SOME GOD DAMN COMMON SENSE. Whoooo... breathe.... in.... out.... Okay. You see, we can't help you if you don't give us the tools to help you. We literally don't why you're having problem, we just here this random unknown voice in the wilderness yelling 'HELP!' and we really want to, I mean, we're not bad people or anything, and we'd love to see you succeed, but fucking damnit man, you aren't telling us why you need help or what the problem is, and ain't nobody got time for that. An example question that would help us is: &gt; I'm trying to write a query that groups all rows with same 'subject' columns into one column and has the latest 'last_touched' date of those grouped columns. Here is my SQL statement: "SELECT *, MAX([last_touched]) FROM [some_table] GROUP BY [subject]", but it's giving me this weird error message: &gt; Column 'some_table.id' is invalid in the select list because it is not contained in either an aggregate function or the GROUP BY clause. See, this let's us know what you're trying to do specifically, so that we might be able to tell you your specific problem, and it tells us what the specific error message is so that we specifically might be able to target our efforts to specifically help you. Specifically, this greatly reduces the effort we have to exert to help you, therefor lowering the barrier that exists to get our knowledge into your head, making it more likely that we'll even attempt to help you at all. By the way, the issue with that was that there are many columns pulled by "*" ([id] being one of them), and the system doesn't know how to group all of them- does it SUM the id's, take the MAX value for the id's, or maybe it should ignore the id's all together. At any rate, all of these necessitate not using an asterisk. You shouldn't use asterisks in production code anyways, for debugging and performance reasons. But, getting back to the original issue. We now know what the person is doing, why they are trying to do it, how they are trying to do it, what went wrong, etc. Is there any more that they could give us to help us help them, since **OUR TIME ISN'T FUCKING FREE AND NOBODY IS GOING TO HELP YOU IF IT IS DIFFICULT TO EVEN GET TO UNDERSTAND WHAT YOU NEED HELP WITH?** You bet there is Mykee. You can let us know what you've tried doing, so that perhaps we won't suggest something that won't work. We want you to succeed, we want you to understand and fix your problem. It gives us warm fuzzies, and it helps the community by ensuring that you have a chance to learn how to become a quality coder. This is nice, because at some point, somebody is going to have to read the bullshit that you code. When this happens, it would be nice if it was sane, used proper abstraction, didn't do anything crazy or weird, and was as straight forward as possible. We can help you get there, Mykee, but you gotta help us help you, and right now, I just want to smack you because you're acting like a little child that is crying but won't tell an adult why they're crying. It's time to grow up and let us know what your problem is like the adult that I think you are. **TL:DR; WHAT IS YOUR FUCKING PROBLEM (Literally)?**
Woah.....
Not a day goes by where I am asked for help by somebody who refuses to tell me anything other than they need help. It has grown into a rage-inducing pet-peeve. "I need help" can only be answered with "Yes, yes, you do." anymore. Do not ask for help. Ask the question and we will help you.
Um, read the damned sidebar then: &gt; When requesting help or asking questions please prefix your title with the SQL variant/platform you are using within square brackets like so: [MySQL] Obviously, we are okay with questions.
It may be a bit cliched, but this is one of those times I wish I had more upvotes to give. If you want an answer to a question, you have to actually ask a question first. Additionally, how helpful/relevant/efficient the help you are to receive on an issue will be is generally directly proportional to how good of a question you're asking. You'll get a lot further in life if you learn how to ask *good* questions. Your teachers were lying to you when they said there's no such thing as a bad question.
Thanks a lot! Could you please tell me the reason for inner join?
you're right, i guess you could do without it
and you can do all of these with HAVING
btw I did change COUNT(DISTINCT b.x) = 2 ) to COUNT(DISTINCT b.x) = 3 ) for number (2)
We can use distance formula for this. 1. Create a Function for computing distance formula e.g. compute_distance(long1,lat1,long2,lat2) 2. We can create a simple query given the user's position is in Longitude 22, Latitude 11; SELECT longitude, latitude, compute_distance(longitude,latitude,22,11) AS Distance FROM ShopsBranches ORDER BY Distance;
Thanks a lot .
I'll have to search for a function to calculate the distance..but so far, it seems pretty easy.
OMG, thanks a million that helped a lot :)
there's no way this would be asked during an interview i would not have enough data memorized to be able to write that code, i could only write that code after having access to the reference manual for whichever database system this is, and also google so i can see how others have done the same thing as einstein famously said, "i never commit to memory anything that can easily be looked up in a book" and of course online manuals and google searches now fall into that category 
It was sent to me as a test case before the actual interview.
do you knowthe name of the formula?
Two things: First... You have to consider the [order of operations in SQL](http://www.bennadel.com/blog/70-SQL-Query-Order-of-Operations.htm). FROM clause WHERE clause GROUP BY clause HAVING clause SELECT clause ORDER BY clause You define PRICERANK in your SELECT part... which is "logically" AFTER the WHERE part. You can't use PriceRank anywhere except for in the OrderBy clause, beause it is AFTER the select. This would error out, because `Example` isn't logically defined yet: SELECT ProductID ,ProductName ,case when UnitPrice &gt;= 10.00 then 'Expensive' else 'Cheap' end as Example FROM Products WHERE Example = 'Expensive' Normally you'd simply paste function in the WHERE like: SELECT ProductID ,ProductName ,case when UnitPrice &gt;= 10.00 then 'Expensive' else 'Cheap' end as Example FROM Products WHERE case when UnitPrice &gt;= 10.00 then 'Expensive' else 'Cheap' end = 'Expensive' Secondly, and more importantly in this case, you can't use Windowed Functions in WHERE. You'll error out on: SELECT ProductID ,ProductName FROM Products WHERE RANK() OVER (ORDER BY Products.UnitPrice) &gt; 10.00 Try this (Thirdly, put 4 spaces before code - otherwise you lose formatting): USE Northwind; GO with Data as ( SELECT Products.ProductID ,Products.ProductName ,Products.UnitPrice ,RANK() OVER (ORDER BY Products.UnitPrice) AS 'PriceRank' FROM Products ) SELECT ProductID ,ProductName ,UnitPrice ,PriceRank FROM Data WHERE PriceRank = 15;
Soooo, by replying to this with an answer you're saying that you knew how to do it. But you don't. You're asking Reddit. If you say you know something and you don't, that's called lying, and it's possible you're stealing a job from someone who DOES know without lying. Ethics, much?
Man this shit was fun. This approach is for SQLServer 2008R2 and up. You can right click on google maps and choose "what's here" to get coordinates that you can just paste at the end of my sp called xsp_CreateOffices. Speeds things up. --Table to store offices with name and location CREATE TABLE [dbo].[OfficeLocation]( [Id] [int] IDENTITY(1,1) primary key clustered, [Name] [nvarchar](64) not null unique, [Position] [geography] not null ); GO --To be able to add offices with lat/long CREATE PROCEDURE [dbo].[xsp_CreateOffice] @OfficeName nvarchar(64), @Latitude float, @Longitude float AS BEGIN declare @geo geography set @geo = geography::Point(@Latitude , @Longitude , 4326) insert into dbo.OfficeLocation(Name,Position) Values(@OfficeName,@geo) END GO --Making some offices in Sweden as testdata exec xsp_CreateOffice 'Malmö',55.609481,13.000817; exec xsp_CreateOffice 'Trelleborg',55.38011,13.16058; exec xsp_CreateOffice 'Ystad',55.427324,13.824508; exec xsp_CreateOffice 'Kalmar',56.661557,16.360027; exec xsp_CreateOffice 'Stockholm',59.330316,18.059667; exec xsp_CreateOffice 'Gävle',60.675919,17.150463; GO --Sorts all offices in the database by how close they are CREATE PROCEDURE [dbo].[FindNearbyOffices] @Latitude float = 0, @Longitude float = 0 AS BEGIN declare @geo geography set @geo = geography::Point(@Latitude , @Longitude , 4326) SELECT l.Name ,l.Position ,Position.STDistance(@geo) as DistanceMeters From OfficeLocation as l ORDER by DistanceMeters asc END GO --Measuring distance to offices from my old hometown exec [dbo].[FindNearbyOffices] 60.629165,16.783633
Thank you very much. I appreciate it greatly. That explained a lot and made sense to me.
Upvoted, even though others are downvoting you, because you have a point. He's going to have real trouble at a job where he couldn't do the interview questions.
Why discrete mathematics?
The interviewer told me that it will require research..so I researched and I didn't lie about it. This question is just a test case before the actual interview.
 Sometimes it can be used to judge somebody's attempts at bullshit :). My current boss interviewed me like that. He would ask questions then just wait for you to blabber on. He would keep making it harder and harder until you got out of your depth basically expecting the answer "I don't know the answer to that" but I could find out by doing xyz etc.. Its actually really good. Since almost everybody in our office when out of their depth just says so. When they are out of their deph instead of giving some half arsed time wasting help with something 
Depends... There isn't a day that goes by where I don't hit Google to find a resource or an answer - and I'm beginning to consider myself better than average on something I've been doing for years now. One one hand, an employee is simply there to solve issues. Using whatever tool, or resource, necessary. Why re-invent the wheel when I can find 10 procedures to meet my need, within 10 minutes of searching? On the other hand, you need to understand what you are doing so you don't make a mess of things. I don't think using online resources is "bad" per-se... but you do need to learn while you use those resources.
I use Google enthusiastically for these same basic types of questions (How do I do X in a procedure?)... I have no trouble doing my job. I have no qualms about doing it... at the end of the day it's my job to solve an issue. It's also my job to learn and get better... As long as I'm doing both, what he's doing is no different than what I've been doing, successfully, at my job for years now.
What? All joins are inner joins by default. Omitting the word doesn't change the query. 
i believe he was asking why you would need to join b to a when the problem only calls for a count and that can be obtained from b alone and omitting the join does change the query, it makes it simpler ;o)
I agree 100%.
Oh, true. 
 SELECT ID ,MachineTypeID ,case when MachineTypeID='5010' then 'Siberian Storm' when MachineTypeID='42' then 'Answer to Life' else 'MachineTypeID' end as MachineDescription From MyTable
I get the error Conversion failed when converting the varchar value 'siberian storm' to data type int. any ideas??
One of the values in your 'then's or your else is an int. They should all return the same type and I believe the type for the column is based off the first value returned
So I just tried it with one machine SELECT MachineTypeID, CASE WHEN MachineTypeID='5010' THEN 'Siberian Storm' ELSE MachineTypeID END AS description FROM view_machine WHERE MachineTypeID='5010' AND status='a' Is the issue that machinetypeid and description are different datatypes? EDIT: Figured it out SELECT MachineTypeID, CASE WHEN MachineTypeID='5010' THEN 'Siberian Storm' ELSE Convert(varchar(20),MachineTypeID,102) END AS description FROM view_machine WHERE MachineTypeID='5010' AND status='a' Thank you much! 
What is the type of 'MachineTypeID'? I'm assuming an int? Don't put single quotes around int values look at this and see if it works SELECT MachineTypeID, CASE WHEN MachineTypeID=5010 THEN 'Siberian Storm' ELSE CAST(MachineTypeID AS VARCHAR) END AS description FROM view_machine WHERE MachineTypeID=5010 AND status='a' I removed the single quotes from around your int values and converted the ELSE into a string to make it match the return value 'Siberian Storm' of the THEN Edit: your edit is correct, but its because 'Siberian Storm' and MachineTypeID where different types. 
Try this: SELECT AgentName ,Sum(case when ovs &gt;= 8 then 1 else 0 end) as Good ,Sum(case when ovs &lt; 8 then 1 else 0 end) as Bad ,Count(*) from tbl_qos group by AgentName 
I would place this at lower-intermediate? not quite basic... but far from the tough stuff I've had to deal with :) Are you using MS Sql Server? if so, I highly suggest [TSQL Fundatmentals](http://www.amazon.com/Microsoft-Server-2012-T-SQL-Fundamentals/dp/0735658145). I have the 2008 version, but this is one of a handful of books I will upgrade when I get around to it.
yeah, something like that. no offense taken. i know where i currently stand and very thankful for helpful people like you. in the topic of tutorials, do you have a link handy for me to learn SQL? I've been reading a lot from doesn't sources and get confused sometimes. 
Bulk insert into a temp table and handle it from there. I don't think bulk insert handles updates/append scenarios.
there are many decent sql tutorials out there... try a reddit search in this sub first the only one i've ever looked at in detail (because i started learning sql straight from the ibm manual in 1987, before there were any internet tutorials, heck, before there even was an internet) is [sqlzoo](http://sqlzoo.net)
sure enough. in mssql server you can write this to only get books that have no spaces. Other dialects might use something other than % for wildcards. select * from books where b.title not like '% %'
thank you very much!
Does SQL Server assume `b` refers to `books`? Or did you mean `from books b`?
It handles them by failing the entire operation, if I recall correctly. 
I've always known to add a secondary table for "0 on missing column" questions... this one had me stumped for a couple minutes tho... For each Year/Month and Company, Sum(): USE Northwind; GO Set NoCount on IF OBJECT_ID('dbo.MyTable') is not null drop table dbo.Mytable; GO Create Table dbo.Mytable ( RowID int identity ,VendorID int ,DateOf datetime ,Sales int ) ALTER TABLE Mytable ADD Period AS CONVERT(VARCHAR(4), DateOf, 12) GO INSERT INTO Mytable VALUES (9,'12-10-2009',10) INSERT INTO Mytable VALUES (3,'01-18-2010',23) INSERT INTO Mytable VALUES (1,'01-01-2010',7) INSERT INTO Mytable VALUES (1,'01-01-2010',7) INSERT INTO Mytable VALUES (1,'02-01-2010',7) INSERT INTO Mytable VALUES (1,'03-01-2010',7) INSERT INTO Mytable VALUES (1,'05-01-2010',7) INSERT INTO Mytable VALUES (1,'05-01-2010',7) INSERT INTO Mytable VALUES (1,'07-01-2010',7) INSERT INTO Mytable VALUES (1,'07-01-2010',7) INSERT INTO Mytable VALUES (1,'12-01-2010',7) INSERT INTO Mytable VALUES (1,'12-01-2010',7) GO Set NoCount Off GO declare @StartDate datetime = (select min(DateOf) from MyTable); declare @EndDate datetime = (select max(DateOf) from MyTable); ;with AllPeriods as ( SELECT Distinct CONVERT(VARCHAR(4), DATEADD(MONTH, x.number, @StartDate), 12) as Period, VendorID FROM master.dbo.spt_values x, MyTable WHERE x.type = 'P' AND x.number &lt;= DATEDIFF(MONTH, @StartDate, @EndDate) ) SELECT AP.Period ,AP.VendorID ,SUM(Coalesce(Sales, 0)) AS TotalAmount FROM AllPeriods AP LEFT JOIN MyTable MT on AP.Period = MT.Period and AP.VendorID = MT.VendorID GROUP BY AP.Period ,AP.VendorID Order By AP.Period ,AP.VendorID
Hell... I didn't now about SPT_VALUES table until 15 minutes ago. I think this is probably the most graceful answer to the "Missing values" question. I'll definitely have to bookmark this myself so I don't forget it lol
LIKE is probably not the right condition to use if your searching for a string with only one word. Why not try SELECT * FROM books WHERE CHARINDEX(' ', title) = 0 (have not tested this at all) Edit: Add a LTRIM &amp; RTRIM to cut out any leading or trailing spaces..
and that's why I am a major supporter of using AS when defining an alias SELECT * FROM books AS b WHERE b.title NOT LIKE '% %' ; Notes: * Never do SELECT *. It's a waste of network traffic and can lead to all sorts of problems if the column order or column names get changed. * if the LIKE predicate begins with a %, no indexes will be used for that part of the query.
this is equivalent to the LIKE as given above neither will perform well, as the are not **sargable**
It fell out. =)
They wont perform well but they get what you want. What could we use to optimize this query in the future?
store the emails in two pieces, the name and the domain, then you could do an equijoin, very efficient
What does that have to do with books with spaces in their titles?
shit... my apologies... i answered on the messages inbox page without looking at the context... answer was for a totally different question i am deeply sorry for the confusion
I'd be interested to see if somebody can find a solution that can use an index
Should be fine. My cert is for schemaverse.com so if you access www.schemaverse.com it will see the hostname as a problem with the cert. Either way, The Schemaverse welcomes hacking attempts to the game so I would use one of your less important username/password combinations :)
Solution adjusted for the actual schema and it works! On a side note I can't use spt_values as my periods table is prepopulated earlier with the 5 values I needs. select vp.VendorNumber, vp.VendorName, vp.ReceivingPeriodNumber, SUM(POAmount) FROM ( select v.VendorNumber, v.VendorName, pd.PeriodNumber as ReceivingPeriodNumber FROM ( select VendorNumber, VendorName FROM @tempresults group by VendorNumber, VendorName ) v cross join @Periods pd ) vp left join @tempresults m on vp.VendorNumber = m.VendorNumber and vp.VendorName=m.VendorName and vp.ReceivingPeriodNumber = m.ReceivingPeriodNumber group by vp.VendorNumber, vp.VendorName, vp.ReceivingPeriodNumber
Here's how you can write a query to find which studens are enrolled in math and then which ones are in computer science. Maybe that can give you a start (I've already figured out the rest but you didn't want it yet?) --Returns all enrollments for math SELECT e.sid ,e.dname FROM dept as mathDep INNER JOIN enroll as e ON mathDep.dname = e.dname WHERE mathDep.dname = 'Mathematics' --Returns all enrollments for computer science SELECT e.sid ,e.dname FROM dept as compsciDep INNER JOIN enroll as e ON compsciDep.dname = e.dname WHERE compsciDep.dname = 'Computer Science'
What do you have so far? Find the most efficient way to join tables such that you can have student AND department information available to you. Are we lucky enough to have a table with both student AND department information? (hint: yes) 
okay, post what you get trying to put it all together. I got what I think is the perfect answer for your assignment once you are done. Then we can compare results :)
You don't need a join, but your query there shows you understand the concept of the join! Also, I think your query would work if you did [MESSAGE REDACTED DUE TO PURE FAILURE] edit: Do you need distinct?
Using OR will return any student who is enrolled in either math or comp science, not both.
Good point, misread his post.
You could make an intersection as long as you don't want to return any fields that doensn't represent the student entity. I'll post my query if you ask for it once you are done. :)
In your Where statement, only look for dname in ('mathematics','computer science'). Then all the ones you'd want have a count(distinct dname)=2. 
i'm not sure is the EXISTS operator is an option for you as i only used it in oracle content. How i would approach this problem without giving you the code: either by - inner join - a scalar subquery in the where clausule which states something like WHERE student in ( select student from ... where department=MATH ) but other comments clearly have sorted it out for you :)
--now combine no? SELECT e1.sid FROM dept as mathDep INNER JOIN enroll as e1 ON mathDep.dname = e.dname AND mathDep.dname = 'Mathematics' INNER JOIN enroll as e2 ON e1.sid = e2.sid INNER JOIN dept as compscidep ON compscidep.dname = e2.dname AND compscidep.dname ='Computer Science'; yea i dont even know how to do a /r/n in reddit :P 
FROM Students is wrong based on the information presented. Perhaps consider a nested query.
I can help you with some math and basic problem solving. You first need to find the SUM of each students credits. Then you need to COUNT the number of students. You then need to do the math (SUM/COUNT) to get the AVG credits for each student. You with me? Then you need to compare each students SUM of credits to the AVG credits you worked out above so that you can find those with &lt; AVG. I'll let you research the SQL.
I appreciate the feedback!
Your current query is finding the average for each student. If I'm reading the problem correctly, you want to know the average number of credits for all students. Assuming each student is listed only once in the table, you can just use a subquery in the where clause like below (formatting might not be great): select stuid, stuname, credits from student where credits &lt; (select avg(credits) from student) 
and if the student is listed more than once: select stuid, stuname, Avg(credits) AvgCredits from student where credits &lt; (select avg(credits) from student) group by stuid, stuname
I think I missed something earlier. This query should work, except that you'd need to *SUM(credits) AvgCredits* in the outer SELECT. 
Post the sql you were using to get those results... hard to know what "magic incantation" was doing otherwise.
select min(timestamp) start, max(timestamp) end, max(timestamp) - min(timestamp) start_to_end, unix_timestamp(min(timestamp)) startUTC, unix_timestamp(max(timestamp)) endUTC, unix_timestamp(max(timestamp)) - unix_timestamp(min(timestamp)) start_to_end_UTC from post_snapshots group by permalink;
I doubt it's always 1900 but if it is you could hack to_date('19'||some_column, 'YYYYDDD') .. It screams of not best practice
Yes, the table contains data from current all the way back to the mid 90's. What I want is the data only from 01 OCT 2012 to current. Problem is it returns everything because 2096 is greater than 2012.
This is my first time actually offering help since I'm still kinda new, but try this: select to_date( case when substr(some_column,1,2) &gt; substr(to_char(sysdate,'YYYY'),3,2) then '19'||some_column else '20'||some_column end , 'YYYYDDD') from some_table
Is this DB2? Try 'RRDDD' for the date type. edit: [source for DB2 for i v7.1](http://publib.boulder.ibm.com/infocenter/iseries/v7r1m0/index.jsp?topic=%2Fdb2%2Frbafzscatsformat.htm) doesn't include julian dates though.
Sorry, it's Oracle.
reply just looked up oracle and the correct format string is 'RRRRDDD' http://www.techonthenet.com/oracle/functions/to_date.php
Perfect. Thanks
The diagram at the top of the [wiki article on data warehousing](http://en.m.wikipedia.org/wiki/Data_warehouse#) illustrates a typical data warehouse. You'll notice most of the diagram is about handling/processing data from operational systems to make it easier for data users to access it. These processes for loading raw data, merging data streams from multiple sources, meta data about data, error detection and correction/reporting, etc are the real meat of a data warehouse. Once you have a data warehouse, data marts are sets of tables with commonly requested results/calculations/summaries sourced from the DW. 
awesome, that's much better than the monstrosity that I built
All good questions. Buy and read this [Kimball](https://www.google.co.uk/shopping/product/9843613540147928069?q=kimball%20data%20warehouse%20book&amp;hl=en&amp;bav=on.2,or.r_gc.r_pw.r_cp.r_qf.&amp;bvm=bv.42965579,d.d2k&amp;biw=1440&amp;bih=775&amp;sa=X&amp;ei=slYuUb2ULcr64QTWsIGIAg&amp;ved=0CFsQ8wIwAg) I build and maintain DW's by trade and that's what go me started. 
Not really. Very high level A Datawarehouse is very similar to an Excel spread sheet. Totally de-normalised build for speed, but lots of duplication. A conventional OLTP database is uber normalised and build to be compact, but querying can be slow
You're talking about a data mart: tables and views of convenience sourced from the normalized DW detail tables. The nut of the idea is to schedule the "expensive" queries and store results for easy retrieval later by users. OLAP is the classic example, but any sort of pre-digestion of normalized DW data for downstream reports or processes could be deemed a data mart. 
Often the DW is normalized like the OLTP system (operational data stores often look like OLTP, except updated in batch instead of twinkling with transactions). Data marts are where you often see things de-normalized for convenience/speed. I would argue your xls example is a DM, not a DW. 
**"merging data streams from multiple sources"** from learningsql is pretty much the general idea in a sentence. For example for a PSI you have the following: * Student Information System * Online LMS System * HR System * Maybe transcript or Distance Learning systems. A warehouse for this PSI would probably have the following information condensed into easily searchable tables. * Staff containing high level staff information * Students containing information required by PSI staff to do their job on a daily basis but might/should not have access to the primary SIS * Courses / Term / Enrollment information You can think of data warehouses as buffer zones between mission critical primary systems and less tech-savvy end users, while keeping it in a more organized and easy to handle format. 
If a DW is fully normalized (3rd normal form for example) its not a DW anymore it's OLTP. More than one nested dimension (Star vs Snowflake) is discouraged by Kimball. 
So an operational database (like a CRM, ecommerce store) is highly normalized, but that makes it slow to query (though fast to upsert/delete), and difficult for business analysts/your boss to join so many tables. To fix this, a data warehouse uses a different, simpler schema. Consider all of the columns that might be pertinent to an analysis. Put them in one table. This is called the fact table. You would have one fact table per major type of analytic need. To save space/increase validity, do one level of normalization, to put the non-numerical measures into their own tables. These are dimension tables. An example would be to split out a date table and a customer table, but leave say price and quantity in the fact table. A data warehouse might also consolidate data from many sources, such as email, accounting, customer service. Stuff to read: http://www.kimballgroup.com/data-warehouse-and-business-intelligence-resources/kimball-core-concepts/ FOSS software to check out: * PostgreSQL or MonetDB * Mondrian OLAP server * Pentaho Schema Workbench * Pentaho Data Integration (ETL) * Saiku Pivot Tables 
This was really helpful for me for understanding the concept of a datawarehouse. Also made me realize that I've been mixing the terms Data Warehouse and Data Mart. There are still some terms I'll have to look further into, but I've got the idea of what they mean.
Absolutely clear, here. Thank you. When I hear the expression "data streams from multiple sources" or when I see your example of different 'systems', I want to make sure I'm not misunderstanding or complicating the concept. If my HR department used it's own DB and my finance department used their own, and both were normalized, I could merge those into a third database which would be considered the Data Warehouse, correct? A follow up question here: If that IS true, does it make sense to normalize that data warehouse or denormalize it and run queries against it? Would it be beneficial to use the DW just to merge the source DBs and then build a fourth denormalized DM in addition to the DW?
Awesome! Thanks! A semi-quick follow up: Does it make sense to use the DW as a normalized merge point with DMs on top of that? When does it make more sense to use DWs in addition to DMs instead of an 'all-in-one' DW?
I tried making a DW with a single fact table utilize a great snowflake schema. Had to drop that in place of a star schema to have any chance of getting SCD2 to work. Snowflake, not even once.
Generally a Warehouse normalizes the data, but in the process of doing so will also verify redundant information for records that match each other. Using your example of an HR and Finance system you could do the following when pulling from both: * Check to see if the Finance payout address matches that of the HR system;some people will update their address with Finance, but not HR or vise-versa which can cause all kinds of havoc around tax time. (Which can be easily averted by doing this) * Find redundant records for people; example with someone getting married and changing their name A warehouse is meant to be a general overview of your organizations data but does not store all the extra information of specific systems. Overall, what is does and does not store is entirely up to the designer, it's more of a concept then a refined set of rules.
Database : A collection of data Relational Database : Sets of related data linked via common property Normalized Relational Database (OLTP) : Sets of related data linked via common property, cleaned and any duplicated data pushed into multiple layers of look-up tables to reduce overall disk size Data Warehouse : A large (normally one) fact table that is complimented by normally one level of dimension tables that describe the fact. A DW is NOT normalized, in fact duplication is encouraged. 
Not 100% sure this will work on MySQL (I'm an MSSQL two-bit hack) This is also off the cuff without a SQL server nearby to test so, caveat emptor: Select a.[application_id], b.[application_id] a.[date], b.[date], a.[rank], b.[rank] From [MyTable] a inner join [MyTable] b on a.[date] = dateadd(dd,1,b.[date]) and a.[rank] != b.[rank] There is an assumption here that the [date] column is always just the date with either 00:00:00.000 as the time; or, has not time at all. If it does, you'll want to strip that. Usually, I use: cast(floor(cast([date] as float)) as datetime) This returns the date with the time set to 00:00:00.000
A quick and dirty way to get off the ground on reporting is to create an ODS (Operational Data Store) database. This would be a straight copy of your OLTP environment(s). People here agree that they are typically inefficient to query. For users to query it efficiently you will need to model your data appropriately. But your hardware may be good enough to get by on just an ODS for now. You should already know that there are two main schools of thought on datawarehousing. One is Kimble and one is Immon. You should study these schools of thought and adopt one of these strategies. Or at least a hybrid approach of one. That is the general layout of your DW/Data marts. Know that DW is a huge topic and contains many sub-disiplines. To run a LARGE scale enterprise data warehouse you need some or all of the following: production DBAs, ETL developers, data modelers, dedicated business analysts, SME contacts you are friendly with, DW team leads, project managers, technical/resource managers to name a few. Data warehousing requires more than just db design. It requires a lot of thought be put into integration design as well. You don't write and forget a data warehouse. You will need to support it forever. And especially at 2AM when there are PK violations coming in from the fucking ERP. The real question you should be asking is what business needs will be solved by creating a data warehouse? Talk to your logistics guys, talk to finance, talk to HR. You will get different answers. The answers you get will drive your data models. Use an industry recommended ETL tool. Don't re-invent the wheel! 
I haven't found dimensional DBs to be faster than relations DBs, well without an OLAP cube anyway. In a relational model in 3NF you probably have a handful of foreign keys for any table, and every table save for lookups has some relevant information in it. So getting the information you want is only a join or two away, and with 3NFs focus on eliminating duplicate data, the overall table size in a relational DB tends to be smaller. In a star schema the fact table only has cumulative capable information at the most granular level possible, everything else is stored in a myriad of Dimension tables. Which usually means to get anything of relevance You start at a dimension table, join to the largest table in your DB the fact table, and join it against one or more additional dimension, worse Dimensions can be a significant portion of the size of the fact table. If the Kimball group has told you that dimensional models are universally faster and simpler than equivalent relational DBs they have performed an act of marketing against you. Also during my read through of the data warehouse toolkit, I was skeptical of the idea of n-dimensional data space being easier for management to understand and navigate than a relational database. They obviously have more savy users than I've dealt with. I just imagined myself explaining "It's like a 12 dimensional space containing data, and you're using tuples, which are sets of coordinates, to isolate regions containing data, and then measuring the results", perfectly intuitive right.
Much more concise that what I wrote. Kudos
The Kimball group (whom you linked to) says snowflaking (EG Normalizing) should be avoided in a datawarehouse. Per them, space is cheap, and it's more important that dimensions be complete in order to avoid additional joins. The closest they get to snowflaking is a factless fact table, which is basically a lookup between two (or more) dimensions. A dimension is far more than a column, for example a date dimension should include the date, week of year, day of month, month, quarter, year, fiscal year, day of week, and a weekday flag. You should never make an assumption on how the users will use the data. If the analyst wants to ask if widget 1 sells better in the first or second week of any given month he shouldn't have to know a lick of code to answer that question.
A star schema has one level of normalization, a snowflake has 2+. I never said anything about snowflakes or more than one level of normalization. Space is cheap for rinky-dink applications. Space still ain't cheap in the petabyte range, which is where some companies are at. Mathematically, a dimension is an attribute/column. If you happen to put a few dimensions in another table and link to it, then you are in effect linking to many others. We are saying the same thing, just not in the same way.
Just told my boss to order this book for me :)
Some advice for you: Data Dictionary This is essentially fields and metadata information about them. Typically you might have 'label' 'field name' 'description' 'source' . This is a great first step to understand the database(s), as well as how information is retrieved. Datawarehouse (before you begin) Now take that data dictionary, and start grouping and looking for similar things. Ok your HR and Finance db's are linked by staff number, thats good, make a note of that. (foreign keys and so on). This will assist you when building your data warehouse and creating the various structures. Datawarehouse (beginning) Try and import some of those tables into your datawarehouse. Essentially a datawarehouse is a repository of data in your organisation. As stated elsewhere here, it just basically a nice one shop stop for all your information requirements Reports Now that you have your data in your datawarehouse, what do you want to do with it? Reports? OLAP cubes? Dashboards? etc. (I hope you had some things in mind before you even started with a datawarehouse!). 
The most efficient way is to use analytic functions - this results in a single table scan. All other methods result in an table join back to itself, two table scans and multiple nested loops which have a much larger overhead. So, something like this (untested) : select APPLICATION_ID, ADATE, ARANK from ( select c.*, lag(ARANK) over (order by ADATE, ARANK) PRECEDING_RANK from ATABLE) where PRECEDING_RANK is null or ARANK &lt;&gt; PRECEDING_RANK 
Let me clarify, I'm trying to do multiple fields at one time too. Update table Set Field1 = IsNull(Field1,0) Field2 = IsNull(Field2,0) Etc.... 
use COALESCE -- it's a standard function and works in all database systems, whereas ISNULL might not also, when posting in this (non-dbms-specific) subreddit, please prefix your title with the SQL variant/platform you are using -- see sidebar at right
So is an ODS different from a DW? If I simply denormalize my primary OLTP data into a new database, what is that second database considered? An ODS or a DW? Or is it simply a piece of the DW? Also, I have a high demand for reporting on data as it's been happening in the last half hour, hour, and day. Live data seems to be a kicker when it comes to DWs. Is there a DW-friendly solution for providing nearly-live data?
I just picked up a copy myself! First three chapters alone have been worth the 35 bucks.
I'd try a "case when" - looking it up in sql help should tell you what you need/let you tweak, but something like: Select Branch, '2010 stats' = case when year= '2010' then avg(score) end, '2011 stats' = case when year= '2011' then avg(score) end, '2012 stats' = case when year= '2012' then avg(score) end, From allScores Group By Branch 
ISNULL is a function which returns one of two values supplied. IS NULL is a comparison operator. So: WHERE ColumnA IS NULL Is a bit like saying: WHERE ColumnA = NULL Except the latter doesn't work in SQL.
 SELECT Branch ,AVG(CASE WHEN [year] = 2010 THEN score END) AS [2010 stats] ,AVG(CASE WHEN [year] = 2011 THEN score END) AS [2011 stats] ,AVG(CASE WHEN [year] = 2012 THEN score END) AS [2012 stats] FROM allScores GROUP BY Branch
Couple ways... maybe three rows? SELECT Branch ,Year ,Avg(score) Stats FROM allScores Group by Branch ,Year Or: SELECT Branch ,'2010' = avg(case when year='2010' then score else 0 end) ,'2011' = avg(case when year='2011' then score else 0 end) ,'2012' = avg(case when year='2012' then score else 0 end) FROM allScores Group by Branch
So as redking666 says in another post, case statements are what you're looking for here to put branching conditional logic within a select clause. However, it becomes a little tricky when dealing with AVG() - to explain why, let's look at what we would do if we simply wanted a sum of the scores instead of the average: SELECT branch, SUM(CASE WHEN year = '2010' THEN score ELSE 0 END) as score_2010, SUM(CASE WHEN year = '2011' THEN score ELSE 0 END) as score_2011, SUM(CASE WHEN year = '2012' THEN score ELSE 0 END) as score_2012 FROM allScores GROUP BY branch Now looking at how that code works, within each SUM() we check which year the record is for - if it is the year we want to add, we use the value of the score - if not, we use 0. Simply replacing SUM() with AVG() in the above example won't work, though - since we don't want to include 0's for the records which are not for the year we are looking at or it will drag our averages way down. Fortunately, aggregate functions like SUM() and AVG() ignore NULL, so you can use this code instead: SELECT branch, AVG(CASE WHEN year = '2010' THEN score ELSE NULL END) as score_2010, AVG(CASE WHEN year = '2011' THEN score ELSE NULL END) as score_2011, AVG(CASE WHEN year = '2012' THEN score ELSE NULL END) as score_2012 FROM allScores GROUP BY branch Now the NULLs won't drag down the average and only records for the correct year will be considered. EDIT: Whoops - accidentally left the aggregate function as SUM() in the second query when it should be AVG(). Fixed now.
See my comment on this question - You can't use 0 in the else clause here or it will drag your averages way down (since the total number of score records will be the same, but all of the records that are not for the year in question will be counted as 0). You need to use NULL instead.
It looks like the functionality you're looking for is to "pivot." Pivot's not the easiest to work with (what version of SQL are you using?), and with your data column changing week to week, you would need to build the select statement dynamically if you wanted a completely automatic solution (since the pivot keyword requires the column names explicitly declared). Starting out, adding the week to the pivot manually would be easier. http://www.mssqltips.com/sqlservertip/1019/crosstab-queries-using-pivot-in-sql-server/ 
Yes - I saw that and upvoted it, as it is absolutely a correct answer. I was in the midst of typing my response when you must have posted yours.
Yeah... I actually noticed that after... Try a pivot? USE Northwind GO DROP TABLE [AllScores]; GO -- Creating Test Table CREATE TABLE [AllScores] ([Branch] VARCHAR(25), [Year] int, [Qty] INT) GO INSERT INTO [AllScores] VALUES('A',2010,2) INSERT INTO [AllScores] VALUES('A',2010,6) INSERT INTO [AllScores] VALUES('A',2010,1) INSERT INTO [AllScores] VALUES('A',2011,12) INSERT INTO [AllScores] VALUES('A',2012,12) INSERT INTO [AllScores] VALUES('A',2012,12) INSERT INTO [AllScores] VALUES('A',2012,12) INSERT INTO [AllScores] VALUES('A',2012,1) INSERT INTO [AllScores] VALUES('A',2012,1) INSERT INTO [AllScores] VALUES('A',2012,1) INSERT INTO [AllScores] VALUES('A',2012,1) INSERT INTO [AllScores] VALUES('B',2010,3) INSERT INTO [AllScores] VALUES('B',2012,24) INSERT INTO [AllScores] VALUES('C',2012,3) INSERT INTO [AllScores] VALUES('C',2009,1) INSERT INTO [AllScores] VALUES('C',2013,1) GO SELECT * FROM [AllScores] PIVOT( AVG([Qty]) FOR [Year] IN ([2010],[2011],[2012]) ) AS pvt GO 
are there other plugins you use/recommend ?
This is very good. I wouldn't have noticed that having ELSE 0, ELSE NULL or ending the case without ELSE would make any difference. Thank you for this explanation. I came here to ask one question, you guys answered some more!
Are you sure you need it averaged and pivoted? SELECT week, average(score) FROM table GROUP BY week This will result in two columns of data. Then you can pivot it if you really need to. Many reporting programs out their have pivoting built in to handle this.
hmm, this looks to advanced for my level as of the moment but something I feel I would need in the future. Is pivot widely used? Do you think it's best if I stick to the simpler case for now or is it better if I learn and use Pivot as early as now because it is more efficient etc?
ELSE NULL and ending the case without ELSE will be the same. ELSE 0 will be different (for AVG(), that is. SUM() I believe all 3 would be the same which is why I wanted to go into more detail in my example)
Pivot is an advanced concept that is wildly useful when you need it - but I find is not needed all that often. It is way better for you to stick to the simpler case for now.
SPT_VALUES seems very under-documented, so I'd refrain from using it because it may get removed/modified in future releases. Personally, I've always had a "calendar" type table with all of the dates I need, with the PK on an ID field, and reference everything by DateID. 
I'd also like to add that ordering is the most expensive thing you can do in SQL Server, so if you can, avoid doing so.
Alright, I better save these comments for future use then. Once again, I'm grateful to you guys!
No other plugins that I use. I do use the completel separate and free Atlantis SQL tools. http://www.atlantis-interactive.co.uk/
Sometimes we overthink things. I've done it a bunch myself. Anyway, glad it works. Ironically, I just noticed the name of the Redditor who posted the fix. Ha!
For efficiency, you should do: update ATABLE set AFIELD = 0 where AFIELD is null The where clause predicate should be limiting the dataset to be updated. This prevents any before or after update / instead of triggers from firing, lessens any auditing entries and reduces the transaction size and thus the rollback segment. All other solutions are poorly considered.
I think pivot is very limited in the use cases where it can be of great value but when you do encounter that use case, I enjoy being able to implement it. Many people including me do find the syntax to be a little annoying.
Thank you so much for the reply! You'll have to forgive me, I'm not very SQL experienced - this isn't working for me. Once I plug in my table names and such, I'm getting this error: ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '(order by date, rank) preceding_rank from theTable' at line 1
group by .... to_char(compl_dt, 'mm' ), to_char(compl_dt, 'yyyy' ) Your where clause isn't right. Your date column can't be in those months and be null.
Rank is a reserved word in SQL, as it's also an analytic function. Check your version of MYSQL supports analytic functions such as LEAD and LAG. I know the current version does. 
Ahh... I see. Your query is still grouping by the date time because... drum roll... you have the date time in your group by clause. remove the `,compl_dt` from your group by, order by clauses. And it would probably be easier to read if you switched the 'where' to actual datetime instead, and removed the 'IS NULL' And, as Zip said, how can your date be last month AND null? at the same time? SELECT support_uic ,proj_cd ,TO_CHAR(compl_dt, 'MON-YYYY') date_completed ,COUNT(won) close_count FROM mgt0.V_WON_R_VIEW_ALL WHERE support_uic IN ('big_list') AND compl_dt between to_date('01-OCT-12') and to_date('28-FEB-13') + (86399/86400) GROUP BY support_uic ,proj_cd ,TO_CHAR(compl_dt, 'MON-YYYY') ORDER BY support_uic ,proj_cd ,TO_CHAR(compl_dt, 'MON-YYYY')
It took me a minute to understand what you were talking about. In this case, the compl_dt (complete_date) field could be null if the task isn't complete yet. Your answer looks workable though.
That's the one. For some reason I thought you had to put every column in the WHERE also in the GROUP/ORDER BY unless it was the aggregate. This makes life so much easier.
I don't think that's the error, since I'm not actually using rank, I'm using something along the lines of k_rank. I don't want to use the actual expressions since it would basically give away what company I work for. My MySQL version is 5.1.62, which as far as I can tell supports LEAD and LAG. EDIT: Thank you for taking to time to help me with this, by the way!
You put in the group clause what you want to group by. Even if it's not in the select cause. It's bitten me a few times as well, so don't feel bad. I hate having to group by when I have a dozen or more columns to select. Thankfully TSQL has `OVER (Parition By...)` Well shit... looks like [oracle has them as well](http://www.orafaq.com/node/55) Try this: SELECT DISTINCT support_uic ,proj_cd ,TO_CHAR(compl_dt, 'MON-YYYY') date_completed ,COUNT(won) OVER ( PARTITION BY support_uic ,proj_cd ,TO_CHAR(compl_dt, 'MON-YYYY') ) close_count FROM mgt0.V_WON_R_VIEW_ALL WHERE support_uic IN ('big_list') AND compl_dt BETWEEN '10/1/2012' AND '2/31/2013' ORDER BY support_uic ,proj_cd ,compl_dt ,TO_CHAR(compl_dt, 'MON-YYYY') Removes the `group by` part. You could add other info to the SELECT part without affecting the totals.
Well the issue is then, if the complete_dt can be null (case open, not yet closed) you need to do: WHERE support_uic IN ('big_list') AND (compl_dt between to_date('01-OCT-12') and to_date('28-FEB-13') + (86399/86400) OR compl_dt is null) What if you run the report TODAY. It's March. Null stuff wasn't completed between October and Feb?
http://www.techonthenet.com/oracle/functions/to_date.php To_Date(12274, 'J') I think... I don't mess with Julian Dates.
Yeah, these workorders some times take many months. The reason for dt_compl IS NULL is because I want to see only those still open. In the other case I'd use dt_compl IS NOT NULL.
Post your query
part 2 2. Write a trigger that will log the following values when an update is done on the [Order Details] table : ChangeID OrderID ProductID DateChanged OldQuantity NewQuantity 
Unless I misunderstand something with Oracle... SELECT * FROM TABLE WHERE SomeFlag = True and X = 1 and X is null Should return... nothing. No matter what, a column X can't be 1 **AND** null. I've only had to use Oracle sparingly, but to my knowledge their logic should work basically the same as everything else: SELECT * FROM TABLE WHERE SomeFlag = True and (X = 1 or X is null) 
In some records, that field is NULL and in other records it contains a julian date. I wouldn't use both ... IS NULL and ...IS NOT NULL at the same time. That would and does return no records. AND TO_CHAR(compl_dt, 'MON-YYYY') IN ('OCT-2012', 'NOV-2012', 'DEC-2012', 'JAN-2013', 'FEB-2013') AND compl_dt IS NULL Should really be AND TO_CHAR(**accept_dt**, 'MON-YYYY') IN ('OCT-2012', 'NOV-2012', 'DEC-2012', 'JAN-2013', 'FEB-2013') AND **compl_dt** IS NULL
To give a proper answer we'll need to see a fuller schema information. But something like the below might help for the first question: CREATE PROCEDURE dbo.ChangeOrderQuantity (@OrderID INTEGER, @ProductID INTEGER, @NewQuantity INTEGER) AS BEGIN BEGIN TRANSACTION [OrderUpdate] BEGIN TRY UPDATE [Order Details] SET Quantity = @NewQuantity WHERE OrderID = @OrderID AND ProductID = @ProductID AND @NewQuantity&gt;0; END TRY BEGIN CATCH Select 'Error Updating Order' AS Result If @@trancount &gt; 0 ROLLBACK TRANSACTION [OrderUpdate] END CATCH If @@trancount &gt; 0 COMMIT TRANSACTION [OrderUpdate] END; Is there a log table in existence already?
I'm really looking for the current FY which starts 01 OCT. This works though.
Starting on April 1st Coursera is offering a Introduction to Data Science Class which you may find beneficial. https://www.coursera.org/course/datasci
I believe I actually did not need to have a distinct selection for this particular case. The following query produced my desired results. It would still be cool to see what people have to say about joining to distinct selections. SELECT Step.ST_TEST_ID as 'Test ID', Step.ST_RUN_ID as 'Run Id', COUNT(Step.ST_RUN_ID) as 'Step Count' FROM STEP GROUP BY Step.ST_TEST_ID, Step.ST_RUN_ID 
&gt; It would still be cool to see what people have to say about joining to distinct selections. wish i knew what you meant by this also, there's no join here, unless you meant to produce aome kind of subquery?
Sorry for the poor description. An explanation of what I was trying to accomplish would be... A column in a DB with 100 records in it called NUMBERS and each of those records can be a number between 1-20 so you will only have 20 unique values out of a total of 100. If I were to query SELECT DISTINCT NUMBERS as 'Unique Numbers' FROM TABLE It would return a list of numbers between 1-20. My objective was to have a count of each occurrence of the unique numbers 1-20 out of the 100 records as a new column to the right. The query I have above doesn't exactly reflect this though. Thanks for the responses guys.
Select x as numbers, count(*) as countNumbers From table Group By x Replace x with your column of 1-20 numbers On my phone but should give you an idea 
&gt; The query I have above doesn't exactly reflect this though yes, it does... GROUP BY automatically produces distinct values however, suppose that number 17 was missing from the 100 numbers, and you wanted the results to include 17 with a count of 0 -- in that case, you would use a LEFT OUTER JOIN where the left table is a (usually generated on-the-fly) table of all the desired numbers, and the right table is your data table
Without any additionnal information, it's going to be difficult to give you a clear answer. But, here are a few tips : * Try to limit the quantity of rows to update, and run your query through SQL Query Analyzer * Look at the explain plan * Create a temp table, and insert into it the lines you got from the SCHEDULE sub req. Then you can join on this table in your update * Indexes, indexes, indexes :)
Thanks for the response, I'll work with that suggestion.
Do you have transactional users connected when trying to run this? What kind of a failure are you getting?
**You need to show us the table and index definitions.** Diagnosing slow queries requires full table and index definitions, not just a description or paraphrase. Maybe your tables are defined poorly. Maybe the indexes aren't created correctly. Maybe you don't have an index on that column you thought you did. Without seeing the table and index definitions, we can't tell. If you know how to do an `EXPLAIN`, put the results in the question as well.
&gt;EXPLAIN Or an execution plan, considering it's SQL Server.
Another option, might be to use a cursor? Loosely based [on this](http://www.manjuke.com/2011/07/how-to-use-update-cursors-in-sql-server.html): USE Northwind; GO SET NOCOUNT ON DECLARE @STNO AS INT ,@Year AS INT ,@Balance AS VARCHAR(50) ,@TEMP AS VARCHAR(50) DECLARE BALANCE_CURSOR CURSOR FOR SELECT DISTINCT STNO ,[Year] ,Balance FROM Balance FOR UPDATE OF Balance OPEN BALANCE_CURSOR FETCH NEXT FROM BALANCE_CURSOR INTO @STNO ,@Year ,@BALANCE WHILE (@@FETCH_STATUS = 0) BEGIN SELECT @TEMP = ( SELECT schedule.STNO ,YEAR.year ,sum(AMOUNT) FROM dbo.Schedule ,dbo.Year ,dbo.Balance WHERE SCHEDULE.Year &lt;= YEAR.Year AND BALANCE.STNO = @STNO AND BALANCE.Year = @YEAR ) UPDATE dbo.Balance SET Balance = @TEMP WHERE CURRENT OF BALANCE_CURSOR FETCH NEXT FROM BALANCE_CURSOR INTO @STNO ,@Year ,@Balance END CLOSE BALANCE_CURSOR DEALLOCATE BALANCE_CURSOR SET NOCOUNT OFF As others say... without knowing more about the layout, it's hard to guess if this might be a better option or not. I don't use cursors much, but they do have their places.
Since you are doing aggregations... this is probably why you are filling the crap out of the tempdb with that cross join. For all we know, its summing the table EVERY time it joins on whatever is on balance. And you are using lovely syntax that doesn't work anymore in 2012. Stop using commas to join tables. If you don't know, you are doing a cross join. Look up the syntax to see if this is what you want to happen. How many years are in the schedule? Why are you summing everything under the sun? How much is in the schedule table that you need to join EVERY row in the year table, unless its less than the schedule year for every row? What are you trying to accomplish with this query? As others, the index information would help in making this easier to suggest things for you guys... Im making a guess, but here's how i would make this query (with a little psuedo since I'm guessing what you need): Create table #temp (stno varchar(255), Year smallint, constraint pk_temppk primary key( stno, year) ) insert #temp Select distinct STNO from dbo.balance where balanceYear &gt;= YearsNeedingToUpdate ;With cte as ( Select sum(Amount) as sum_amount, t.Stno, t.Year From dbo.tblSchedule s join #temp t on s.Stno = t.stno and s.Year = t.Year group by t.Stno, t.Year ) update Bal set Bal.cb = cte.sum_amount from dbo.balance bal join cte on bal.Stno = cte.stno and bal.Year = cte.stno The biggest thing you can keep in mind when querying, besides understanding how the data is going to be accessed, is to try to use as little data as possible. Is there a reason it has to hit the entire table? is something i've said way to often to my reporting guys :(
upvote for sharing your excellent cut/paste snippets to help noobs get the best response to their problems
You're probably getting a shit query plan, that causes way too many reads to create the work table in tempdb, for a good reason. As it runs only once a year last years the good one you had last year is no longer in cache. Make sure table definitions haven't changed, especially indexes. If the statistics on all tables are up to date you "should" get a good enough query plan. If not you can try and change the syntax to tsql style joins and start dictating what sort of joins are being used. Take a look at the query plan the current execution estimates using, it's probably a bad choice for joins. And I'm guessing one of these tables is huge and contains a lot of records timestamped from many years ago. If only a few years there are really relevant you might consider dumping the non-relevant ones into a different table along with this calculation job. Especially if this calculation job is what makes those records relevant. MSSQLS has a limitation of how many steps can be in the statistics, so the larger the table the worse decisions for a good enough query plan can be made. And no amount of updating your stats will fix that. VLTs suck.
The Finance guys want to see balances per year as well. I might just do it year per year as opposed to everything at once and see if that works
Thanks, does SQL injection matter if it is not online?
Thanks, I'll have a look and see.
Thanks. Using indexes would help with the query? Doubt there is even indexes actually.
I think that the "temp table then join within the update" should be more performant than the cursor. But both are worth looking at if you've never done this.
Have done the temp table before update before, but thought that an all in one query would be better. But I was mistaken :) 
Yes, never trust your input. Maybe someone will use your code for something else and not know it is unsafe.
Last years closing balanced shouldn't have changed, should they?
No, but unfortunately they do backdate and 'correct' certain transactions.
haha! yes data mart thing :)
Your Population column is a character when it should be a number. A MIN, MAX or any sorting is going to read left to right as if it's a word, so '250' is less than '30' or '40' because 2 is less than 3 and 4. 
^^^ This guy.
Yes, it is almost guaranteed that using indexes will improve your query. If you don't have indexes, then the query will have to search through every row in the table to find matches. Start here: http://use-the-index-luke.com/
Do potential misusers of your code only exist on the web?
This is almost certainly the correct answer. Example using Sqlite3: CREATE TABLE foobar ( country TEXT, population_char TEXT, population_num INTEGER ); INSERT INTO foobar VALUES ('Brazil','250',250); INSERT INTO foobar VALUES ('Chile', '30',30); SELECT MIN(population_char), MIN(population_num) FROM foobar; Gives: MIN(population_char)|MIN(population_num) 250 |30 Incidentally, is this a population forecast? Google suggests that Chile's population is 17m and Brazil's population is 193m. 
It seems like a really bad idea to recalculate past years after they've ended, if for no other reason than potentially changing the data your company used for taxes. If you ever get audited and someone changed something that caused the math to change for past years, that could result in a penalty. Once you calculate taxes the data becomes reality even if it doesn't reflect what truly happened. That's kind of why most companies keep fiscal years completely separate. Accounts get rolled over into new years; in some systems the last year's account actually get closed a new ones open.
"Stealing a job" thats laughable. Its not anyones job. Thats like saying you deserve a job over someone else, the world does not work like that and the sooner you realize that the better off you will be.
You will definitely need cursors and string manipulation to do this, which means programming a solution in whatever language your database uses. What dbms are you using?
You might want to check for white space on your status column you might have values under Emerging that aren't being included in the min statement. You might have... 'Emerging' as well as 'Emerging '
The joins are needed because you need to look for Art Garfunkel, and you need to look for what movies he's in. So in the actor table you can find his name, and therefor his ID. But that ID doesn't do you any good without being able to link the ID with a movie. Now you can't go directly from the actor ID to the movie table, because they have no common keys. So you join the actor table to the casting table. Then you join the casting table to the movie table. Because they are all joined, you can reference information across all 3 of them. From then on, it's just finding anybody else who was in a movie ID that Art was, (minus Art himself of course) I ended up with something a bit different, but it does the trick. SELECT a.name FROM actor a JOIN casting c ON a.id = c.actorid JOIN movie m ON c.movieid = m.id WHERE m.title IN (SELECT m.title FROM actor a JOIN casting c ON a.id = c.actorid JOIN movie m ON c.movieid = m.id WHERE a.name = 'Art Garfunkel') AND a.name != 'Art Garfunkel' ORDER BY name The joins (in both the initial query and the nested query) are just bringing all the tables together so we can piece together all of this information. Basically I'm looking for actors who have movie titles that are IN the group of movie titles that Art was in AND who's name isn't Art Garfunkel. *EDIT: [bmay's response](http://www.reddit.com/r/SQL/comments/19sklx/mysql_walk_me_through_this_sqlzoo_problem/c8qyivh) is a lot cleaner. Though you still may need to order it by name in order for SQLZoo to call it "correct".*
Someone needs to double check this, but here's my take on this. First lets take this piece by piece. SELECT movieid FROM actor,casting Is returning all actors and all castings. There is no join between these two tables so there is a Cartesian join where for every actor, every casting will be returned. (SELECT movieid FROM actor,casting WHERE name='Art Garfunkel' AND actor.id=actorid) With the previous statement, now you are filtering all of the actors to 'Art Garfunkel' and now are joining the casting with the actor. Returning only the movies that 'Art Garfunkel' has appeared in. This type of syntax is generally frowned upon. The Cartesian product generates lots more rows than you need to. It would be the equivalent of getting all of the ingredients for a sandwich, but instead of getting all of the stuff you need. You go to the fridge and take out everything, then putting back the stuff you don't want, instead of just taking the things you need. Its a loose analogy, but the point is you are wasting time. In this case the time is memory and having to produce a giant list of things you don't need is not really needed. You also run the risk of getting bad data. If you did not have the actorid join, then the result set would say all actors worked in all movies. So now lets move out of the sub-select. You have the same cartesian join between actor and casting. SELECT name FROM actor,casting This is returning a list of all actors and all castings with no similarities inside. After it gets this huge list it needs to get an identical huge list inside of the sub-select and only after it has both of those two will it start to filter it out. The query is likely timing out when you submit it. Will what you have work? Sure if you throw it enough resources. Think of it this way. How many actors have ever existed, multiply that by the number of movies. That is what you are working with when you use the cartesian join. Instead lets just work with a listing of the actors and only the movies they have worked with, which is why they have the inner join. A lot of times when you write queries they will logically make sense, but the performance won't be as well as other things. As you progress you'll learn about index's and performance. 
You only need one join: SELECT name FROM actor --every actor JOIN casting on casting.actorid = actor.id WHERE movieid IN --casted in a movie with Art ( --All the movies Art is in: SELECT movieid FROM casting WHERE actorid = ( --Art's actor id: SELECT id FROM actor WHERE name='Art Garfunkel' ) ) AND actor.name &lt;&gt; 'Art Garfunkel' --don't count Art
SQL noob here. How can you SELECT a.name when the alias 'a' isnt defined until the FROM statement?
It is (2nd line): &gt; FROM actor a and &gt; WHERE m.title IN (SELECT m.title FROM actor a
You don't need to JOIN on the movie table at all.
When you're telling a query to look for a.name, it just trusts you that it will find one. Then it searches for it where you tell it to in your FROM statement. 
So does it run twice, like: 1.) Syntax looks good 2.) lets do it. or does it look for the FROM first to see what tables its going to find and then join everything up and return just whats SELECTed.
That makes more sense. Thanks!
I didn't know how else to get the tables in a form so I put it in a picture. Thanks in advance for the help!
Thank you, yes this problem got fixed once I changed the column to 'number'. I just made up everything in the table...I am pretty much brand new to all of this stuff and I wanted info that is easy for me to wrap my head around while I test various types of queries. If ANYONE has good websites where I can learn complex queries and that sort of stuff let me know. I am just beginning. I am a data analyst at an IT company and I often dump tables into excel and do vlookups, countifs, pivots, etc on them. Lately I realized that it would save me alot of time if I could do 50-75% of the hard work in sql much more quickly. Also helps me analyze developers queries and understand what might need to be double checked or tell them how to build something in the first place. good websites as in not w3bschools or whatever its called. I enjoy real examples that go beyond basic select * from ___ where delete = 0 but instead combine 3,4,5,6+ things going on at once. 
Ah, very interesting. I should have specified, my query was failing by giving a time out error. I assumed it was just b/c I had a query that was malformed in some way, but what you say here makes perfect sense to me. Thanks so much for your help.
I see, thank you. It looks like this is almost identical to my failed attempt shown above, just with a different style of join. archerv addressed the probable cause of the problem with mine (the style of join was causing a time out) here: http://www.reddit.com/r/SQL/comments/19sklx/mysql_walk_me_through_this_sqlzoo_problem/c8qy907 Thanks for such a clearly-explained example. Even knowing the individual bits it's easy to lose track of what I'm doing with each step.
Thanks very much
I do agree. 
ah the fun that is batching. It does work but as always 'it depends'. I've had to batch by hour since batching by a certain number of rows wasn't cutting it when there was millions upon millions of rows to be updated. Even then... billions of rows makes this lovely difficult. Partitioning helped, but... only so much. Batching at least lets other things work while you do the crazy. I would stay away from rowcount though. That's old hat sir. Also, try to find the primary key columns that you need to work with then batch through that. That worked the best for me. 
well, the syntax above calls for distinct stno, when i had the table definition for a little more... so i should modify it to include those columns too. I would assume you indexed things properly, and things work now again? Then again, i fear a brute force method is being applied. 
Heh, went with a temp table and then used it to update, instead of having the sum function in the insert. We are still looking at the indexing and how best to implement. 
I believe using SET ROWCOUNT is either deprecated, or generally inadvisable. As LeTroniz says, use the relevant primary key or if you're going to use the update flag approach use TOP in your update statement. e.g. UPDATE TOP (10) myTable SET myColumn = 'Blah', updateFlag=1 WHERE updateFlag=0; edit: Microsoft's page on SET ROWCOUNT in the online help for SQL is here : http://msdn.microsoft.com/en-us/library/ms188774.aspx It says: &gt;Using SET ROWCOUNT will not affect DELETE, INSERT, and UPDATE statements in the next release of SQL Server. Avoid using SET ROWCOUNT with DELETE, INSERT, and UPDATE statements in new development work, and plan to modify applications that currently use it. For a similar behavior, use the TOP syntax. For more information, see TOP (Transact-SQL). So definitely avoid using SET ROWCOUNT. Although I would note that it's says that SET ROWCOUNT will stop working in the "next version" of SQL Server in both the 2005, 2008 2008 R2 and now 2012 help pages. 
I've not used it personally but people say good things about [SqlZoo](http://sqlzoo.net/). Personally though, I find I mainly learn through trying to solve problems, getting stuck and then looking for help when I get stuck. Not the best way to learn (you pick up bad habits) but you've got to find what works for you. I'm not a huge fan of computer books but I learnt a lot from a coupe of Joe Celko's titles. You're right about how effective SQL can be compared to Excel for a lot of tasks. One of the reasons I love Sqlite is how useful SQL can be as a general tool, and I can have it as a portable utility which takes less than 500kb. And if I need a GUI, I can use Firefox. 
If you have any MS SQL Servers, I'd suggest [T-SQL Fundamentals](http://www.amazon.com/Microsoft-Server-2012-T-SQL-Fundamentals/dp/0735658145). I try to reread it when I have the time to go over fundamentals. Numbers vs chars has gotten me a few times as well.
I've seen clients use batching, but not serialize it, so they end up with multiple such queries running in parallel with batches of 100000 rows or more. Brings a system to its knees, it does.
Yes that was it! Thank you. A NOT IN is what I was missing. I also used a full outer join. 
As already mentioned *ROWCOUNT* is being redacted and *TOP* should be used. The only difference is that *ROWCOUNT* last through a connection(global) while *TOP* is query specific(scoped) thus it's better practice to use *TOP* anyway because you don't have cascading problems because you forgot to get *ROWCOUNT* back correctly after using it.
makes sense, thanks
I don't know if its canon but I convert dates to yyyymmdd format. Then you can use plain ole math on them. Select Rate from table where Effective &lt;= 20110119 and State='AK' ORDER BY Effective Desc Limit 0,1 mysql user, please be kind about syntax variations.
Does you other table have some sort of primary key so that you can isolate row duplications. If so: Select * into #temp1 from tableA left join tableB on tablea.state = tableb.state and tableB.Effective &lt;= tableA.month WITH dups AS ( SELECT *, ROW_NUMBER() OVER (PARTITION BY tableA.PrimaryKey ORDER BY tableB.Effective desc) AS RowNumber FROM #Temp1 ) Delete from dups where WHERE RowNumber &lt;&gt; 1; Kinda hard to test my solution or give a great query without both tables. Also, insurance industry? 
If you add an end date you could just use between in your join on statement
Also the PK can be more than one column, like maybe subscriber ID and month for an example, as long as you are sure that one one row is supposed to exist for that PK
Yes, this is right. In T-SQL syntax: SELECT TOP 1 Rate FROM Table WHERE Effective &lt;= '1/19/2011' AND State = 'AK' ORDER BY Effective DESC Don't forget to check if no records are returned and perhaps if Rate is null.
 SELECT rates.state , rates.effective , rates.rate FROM rates WHERE rates.state = 'AK' AND rates.effective = ( SELECT MAX(foo.effective) FROM rates AS foo WHERE foo.state = rates.state AND foo.effective &lt;= '1/19/2011' ) 
Its better to always use universal date format yyyy/mm/dd is never ambiguous to sql, regardless of locale settings. Whereas '03/06/2011' can be
I work as a systems analyst on a student information system for a school district, so I deal with date ranges all the time. The entry/withdrawal screen's table essentially looks like this (CAPITALS indicate primary key): DISTRICT -- state's code for the school district; in every table to allow multiple districts in one DB. SCHOOL_YEAR -- the school year BUILDING -- the code for the building ENTRY_DATE -- the date a student begins school entry_code --method the student is enrolling (in district, in state, out of state) &lt;various data values here, such as FTE values for determining our funding from the state&gt; withdrawal_date (nullable) -- date student leaves the district withdrawal_code (nullable) -- reason student leaves the district (graduated, expelled, etc) So each student gets one record for every school year and every building they attend for that school year. If they leave and come back the data is there. Then when you want to know which vector applies on a certain date: SELECT * FROM entry_with WHERE @MyDate BETWEEN entry_date AND COALESCE(withdrawal_date,'9/9/9999') The application logic prevents entering overlapping dates, so that is a weakness. You could write a trigger to deal with this, but it would be complicated. The same pattern is used for adding and dropping classes, or entering and leaving school programs such as free/reduced lunch or advanced and accelerated programs. The basic pattern is to have a START DATE that is required and an END DATE that is not, and to use application logic to prevent overlaps.
OK, not a problem with that. Was matching the format used to store the date in the table. My favorite is dd-mmm-yyyy (01-JAN-2012).
 SELECT referlist.referid , COUNT(transaction.referid) AS refercount FROM referlist LEFT OUTER JOIN transaction ON transaction.referid = referlist.referid GROUP BY referlist.referid 
Without seeing the data on either table, I'd probably do a: JOIN cubing..WCA_export_RanksAverage AS RanksAverage ON RanksAverage.personID = RanksSingle.personID AND RanksAverage.eventID = RanksSingle.eventID
If PersonID is supposed to be a PK in both tables, it should be good the way dude's got it now. EventID might mess up the join. So what's the matter with the current results?
Thank you! That did it.
I wouldn't say that yyyy/mm/dd is *never* ambiguous - check out the following. SET LANGUAGE 'us_english' SELECT 'us_english', CAST('2012/08/12' as datetime), CAST('20120812' as datetime) --us_english 2012-08-12 00:00:00.000 2012-08-12 00:00:00.000 -- Both 12 August SET LANGUAGE 'french' SELECT 'french', CAST('2012/08/12' as datetime), CAST('20120812' as datetime) --french 2012-12-08 00:00:00.000 2012-08-12 00:00:00.000 -- December 8 and 12 August? Granted, this won't be a problem if your client settings are always set to us_english. I usually still use yyyy-mm-dd out of habit. For more information, check out [Jamie Thompson's post on the subject](http://sqlblog.com/blogs/jamie_thomson/archive/2009/12/08/unambiguous-date-formats-t-sql-tuesday-001.aspx).
I believe [this article](http://dataqueen.unlimitedviz.com/2011/06/how-to-default-to-all-in-an-ssrs-multi-select-parameter/) describes what you want to do. You have to define 'all' to show the results.
I'm not sure where you should post and I don't use SSRS. I do use BIRT for reporting, and there are tons of ways to solve this with simple scripting or in the query. You can do 'ALL' in the SQL with: Where t.value = CASE When ? = 'ALL' Then t.value Else ? End As far as the letter, you can add the % to the end of all values and use LIKE, but it may have undesirable results. Where t.value LIKE ? + '%' This would get you Adam and Alex. However if they entered 'Alex', you would also Alexandra and Alexei. In this case, I use BIRT's autosuggest parameter textbox. It uses a simple distinct query to get all the values and filters as the User types.
If you wanted to include all them whether they attended or not: SELECT CASE WHEN allRanks.eventID IS NULL THEN 0 ELSE 1 END AS attendedEvent, allRanks.SingleBest allRanks.SingleWorld allRanks.AverageBest allRanks.AverageBest FROM (SELECT DISTINCT eventID FROM (SELECT eventID FROM [cubing].[dbo].[WCA_export_RanksSingle] UNION ALL SELECT eventID FROM [cubing].[dbo].[WCA_export_RanksAverage])) AS eid LEFT JOIN (SELECT a.PersonID, a.EventId, s.best as SingleBest, s.worldRank as SingleWorld, a.best as AverageBest, a.worldRank as AverageWorld FROM [cubing].[dbo].[WCA_export_RanksAverage] AS a INNER JOIN [cubing].[dbo].[WCA_export_RanksSingle] AS s ON a.PersonID = s.PersonID AND s.eventID = a.eventID) AS allRanks ON eid.eventID = allRanks.eventID If you wanted after that you can just add a where to check if attendedEvent = 1 to remove any unattended events.
Some background: ------------------ there is no table; we're basically working with an imaginary table. The steps are exactly how he says in the picture, I just don't understand what I'm doing wrong. I've gotten everything else correct but this problem.
Unless they are expecting the Table in full but with it ordered in price. So Select * from Products Order by Price Desc
I think what you may want to do is a multiple value parameter and allow users to type in their own options. Typically what I do though is use query based multi-value parameters. so Select unique Name from Table order by Name ASC and use that as a query to populate the "name" parameter. Automatically the user is given an option to "Select All" or only some of the names they want to see in the report 
this "full projection" means all columns, so... SELECT *
That was it, thank you. I don't know why I was having trouble with that. If you don't mind, could you help with [this](http://i.imgur.com/nq8HoFn.png) one too? Once again, no clue what I'm doing wrong, but it's probably a 1 character mistake...
Actually, /u/RegularJohn1986 helped me with the answer a little ways up. Thanks though!
That's what I thought too, but that's not right either :/ their answer may have a typo, which would make my correct answer incorrect.
Where year(date) &gt; 1991 Another way to achieve the same. 
You should avoid using this as it slower in performance than dtseilers way.
Man I learned SQL stuff at work a few weeks ago and was excited to actually help someone. Glad you got your answers though.
Good point. If it is Oracle then the default date format would have you use 31-DEC-91, unless you cast it like to_date('1991/12/31','YYYY/MM/DD')
Of course, but he had already tried the right way :)
And not sure if that's MySQL-specific
MySQL is YYYY-mm-dd
Come on, now, you should be able to get this one. You're asking for records after the first day of 1991, whereas you should be asking for records after the last day of 1991.
I would add something about SELECT * FROM ... is that if someone add a column you will have superfluous fields
I'm not with you on that one. SELECT * not only selects *all* fields, but must do extra work in order to find out what they are. Therefore, specifically requesting the columns you need is a better practice and will generally result in a better benchmark -- it will certainly do no worse.
Anything is possible. You use UPDATE statements to "Edit" existing data rows.
I'm actually just starting to be trained in SQL but we do bring it up in the MySQL query window. I just wanted to see if it was possible and if my idea for an efficiency on our end was actually viable because often we use SQL to bring up the data on the invoices and then we note those invoices separately so I was figuring there had to be a way that we could just update the information in MySQL after it's queried and then edit it within the window without having to go back into our separate programs to edit the accounts.
You may want to work on a better solution, [this link will help you on your way](http://www.jqwidgets.com/jquery-grid-cells-editing-with-php-and-mysql/) to making an editable grid view webpage that connects / updates to the MySQL database. If you want to get fancy you can also do input validation before updating.
awesome, I definitely appreciate it! thank you!
thanks! :)
For some reason I can not pull the referName from my referList table. I have tried to include it in the Select but I get errors. Got any ideas?
Sorry about that. SELECT referList.referName, referList.referID, COUNT(transaction.ReferID) AS TotalCount FROM (referList LEFT OUTER JOIN transaction ON transaction.referID = referList.referID) GROUP BY referList.referID HAVING COUNT(transaction.ReferID) &gt; 0 error: You tried to execute a query that does not include the specified expression 'referName' as part of an aggregate function Do i have to add another JOIN?
&gt; Do i have to add another JOIN? no :) you added referList.referName to the SELECT clause -- you have to also add it to the GROUP BY clause also, remove the parentheses from the FROM clause
And like /u/Coldchaos said, BE CAREFUL. SQL is powerful, and UPDATE statements are done in a flash. If you make a mistake, no one can save you. (Usually.) If there is a "lab" environment you can play in, I'd recommend you test there, and then make your "official" changes.
In production? not even once. Seriously. I don't even want the privelages to run it on production, but some applications are too small for that sort of deal.
You can't do that for just a plan itself. However, you can configure an SQL Agent job to send an email to an operator on success/failure/completion of the entire job. You'll need to a) set up DBMail if you haven't, and b) add operator email addresses.
I did do that and the format of the emailed reports are like [this](http://pastebin.com/fi9tEKLt). I was looking for something a bit cleaner and easier to see at a glance if the job was successful.
Only thing I'd caution is - as usual with SQL Server - backwards compatibility needs some thought if you've got other boxes running different versions of SQL Server. Didn't notice any problems with older databases but (again, as usual) you won't be able to backup databases from SQL Server 2012 in a way that can be restored on older editions (regardless of what compatibility mode they are set to). Similarly, once you've opened / worked on SSIS packages or SSRS reports under SQL Server 2012 you won't be able to open them from earlier editions. Oh, I did have an issue installing the client tools on an XP machine - that may have been fixed by an update though (I can't recall).
Great info. None of your caveats affect us so going to speak to the boss about upgrading! Can i use a 2008 backup to my new 2012 server?
Yes, forward restoring (for want of a better term) is always fine.
 DECLARE @mydate DATETIME ; SELECT @mydate = '2012-10-10' ; -- Start of Month SELECT CONVERT(VARCHAR(25),DATEADD(dd,-(DAY(@mydate)-1),@mydate),101) ; -- End of Month SELECT CONVERT(VARCHAR(25),DATEADD(dd,-(DAY(DATEADD(mm,1,@mydate))),DATEADD(mm,1,@mydate)),101) ; 
nevermind - looks like it was because GO was not on its own line...
If you are using SQL Server 2012 there is the new EOMONTH() function DECLARE @date DATETIME; SET @date = GETDATE(); SELECT EOMONTH ( @date ) as ThisMonth; SELECT EOMONTH ( @date, 1 ) as NextMonth; SELECT EOMONTH ( @date, -1 ) as LastMonth; But there is no BOMONTH(), so use something like DECLARE @date DATETIME = GETDATE(); SELECT DATEADD(day,1,EOMONTH ( @date, -1 )) as BOMonth; 
Not using 2012 :(
Is this what you're after? &gt; INSERT INTO newTable (col1, col2, col3) &gt; SELECT (col1, col2, 'my own string here!') &gt; FROM oldTable &gt; WHERE 1 =2
Does this do what you mean by changing venzann's code? &gt;SELECT @mydate = '2012-10-10' ; to; &gt; SELET @mydate = DATEADD(YEAR, -1, GETDATE()) 
The syntax that you are using should work. Are you getting an error when you try to run that? Maybe try it with explicit coumn names instead of using *. http://www.blackwasp.co.uk/SQLSelectInsert.aspx http://msdn.microsoft.com/en-us/library/ms188029.aspx
that gives me the last of the current month of the prior year ('3-31-2012'). So getting closer!
Oh right, I think I understand your initial question now.. try this &gt; SELECT @mydate = DATEADD(YEAR, -1, DATEADD(MONTH, -1, GETDATE()))
GO is not, technically, T-SQL [http://msdn.microsoft.com/en-us/library/ms188037(v=sql.105).aspx]. It's a special utility command that MS SQL clients recognize like the backslash for long literal character strings.
gotcha, thanks. the site I copied it from had it all on one line, and I didnt think it mattered.
SSME support that UI method to generate a script and above easy code will do the same thing you wanted ... what else do you want ? ( Copy table structure function ??? Or some kinda extensive schema copying function ? )
Batch completion and semicolons get me from time to time, usually with the semicolon preceding a CTE definition. Glad you figured it out on your own, though! That's the best way to learn.
Right, if you have oldTable(foo INT, bar NVARCHAR(64)) and all of the values in the 'bar' column happen to be integers, the SELECT INTO query will create newTable with both 'foo' and 'bar' as INT columns. I don't think it's klunky/kludgy so much as an attempt at type sniffing.
LOL, maybe we should count ourselves lucky that the SQL interpreter doesn't realize the query returns zero rows and optimizes the shit out of it, so it doesn't do anything at all.
yes I know .. this /r/sql isn't for feature request forum for certain RDBMS product. Maybe Microsoft don't want this feature included in their language lib .. who knows.
Hmmmm, I've never been too bothered by this although the MySql CREATE TABLE ... LIKE feature does sound quite nice (mainly because it sounds like it deals with primary keys which the SELECT * INTO approach doesn't). But I don't think you can chalk this down to being a Microsoft mess up. As far as I know, that feature isn't available in Sqlite, (Sybase) SQL Anywhere, Informix or Ingres. Not sure about Oracle. Some of those support the CREATE TABLE AS SELECT ... syntax though, but that's not radically different from SELECT * INTO. Incidentally, you can use : SELECT TOP 0 * INTO foobar_copy FROM foobar; Instead of 1=2 but that's obviously just as kludgy.
I guess the trickiness is that if you wanted to clone a table with primary keys etc correctly, you'd need to give them unique names. For whatever daft reason, the primary key names (and probably all other constraints) must be unique within the database, which is why you shouldn't explicitly name your temp tables' primary keys. With that in mind, what would you call the new constraint(s)? What about foreign keys referencing other tables? Suddenly the table isn't as cloned as you wanted it to be. Sample code to show constraint isssues: USE tempdb CREATE TABLE #test (a int, b varchar(100), c char(50), d bigint, e nvarchar(64), CONSTRAINT PK_Test PRIMARY KEY(a) ); CREATE TABLE #test2 (a int, b varchar(100), c char(50), d bigint, e nvarchar(64), CONSTRAINT PK_Test PRIMARY KEY(a) ); DROP TABLE #test I'm not justifying why the feature doesn't exist, but just an idea as to why it might be slightly tricky to implement. It would be handy, but it's got caveats that mean the end result with either be random names, or no constraints (which is exactly what we get now with the SELECT ... INTO WHERE 1=2 syntax). Lots of development effort for Microsoft for not much gain, given how easily you can script a definition and insert it into your code (although that doesn't help with a table structure that changes dynamically, although that's a scary practice!)
Hi mate, You might already got this as I've used your code to get the first day of the last month; select DATEADD(mm,DATEDIFF(MM,0,DATEADD(mm,-1,DATEADD(YY,-1,GETDATE()))),0) as firstDay, dateadd(day,-1,dateadd(month,1,DATEADD(mm,DATEDIFF(MM,0,DATEADD(mm,-1,DATEADD(YY,-1,GETDATE()))),0))) as lastDay It's effectively just adding 1 month on to that then subtracting a single day. Hope it helps. I'm sure there is a much more elegant solution as well but I'll leave that to you.
Is the following DB2 only? CREATE TABLE NEWTABLE AS (SELECT * FROM OLDTABLE) with no data I use this all the time! 
Create a dates table. It will make this work easier, will improve the readability of your queries, and will be useful in many other scenarios.
Upvote for still having a working SQL 7 instance. You must go way back. :D
I think that is supported by Oracle but not MSSQL.
Hah, truth be told, I inherited it when I started this job a few months back. God, it's primitive, doesn't have Books Online installed, and you cannot find SQL 7 reference material online.
Ok I think the problem is that both tables have the same column name. So it doesn't know where to pull it from. So the first change i did was to clarify where to get the data from. The second change was a full outer join to a left outer join ( i'm assuming this is for T-SQL i'm not sure about the syntax for MSSQL) This will return a list of all people, and if they have any cars. SELECT ap.Person, pc.Car FROM AllPeople AS ap LEFT OUTER JOIN PeopleCars AS pc ON ap.Person = pc.Person
This. Replace FULL OUTER with LEFT. 
Hmm, I'll give it a try, I did not specify. It is for MS SQL....
Yup, FULL OUTER will only exclude the matching rows.
Just as an FYI; LEFT OUTER JOIN is the same as LEFT JOIN. In MS SQL anyway.
...you originally proposed a FULL OUTER JOIN... you said that wasn't what you wanted??
The strange thing is that the query should have failed if he didn't explicitly state which table he wants the "Person" column from. 
Left will do that
when he names the tables ap and pc and refrences the columns as such the database engine can tell the difference just fine. 
Cool, perhaps they've "fixed" the behavior in 2012. I run mostly 2005 and 2000 still... :X
We use an Oracle SQL server where I work. We don't log directly in, but use the Oracle SQL Developer program. Microsoft has a similar one called, SQL Server Manager (the express version is free to download.) From there, you can write/save queries, or export to CSV. Microsoft's SQL Server Reporting Services has a tool called Report Builder that's good for building online reports (you just drop the SQL in, then design the output), or with Power Pivot -- it's an Excel add-in -- you can drop in a query and pull the data directly into Excel.
Usually Management Studio for MS SQL server for writing tsql and viweing the results. The results can be exported to csv, excel, etc...
How different is this from business objects? Running and testing the queries/tables and can export into CSV etc. 
Thanks! This is exactly what i needed
Agreed. I was assuming that the query used and the query he wrote in reddit were not exactly the same. Looking at the edit that he made, I made the correct assumption.
Not when the column he is selecting appear in both tables with the same name.
Think of this as two parts. An Engine and a Client. Examples of Engines might be: MSSQL, MySQL, Postgres, Oracle, etc. Examples of Clients might be a programming language, a command line tool, or a standard piece of software like a spreadsheet, word processor, or even Call Of Duty. (yes, a game, bear with me here). The client constructs the query. In the example of a command line tool, the query is constructed by a human being typing on the keyboard (select name from...). In the example of a game like Call of Duty, the player isn't likely to be writing much SQL, but the game itself might need to look up a high score so it will construct a query (select high_score from....) The client sends the query to the engine. The engine does the work and sends the response back to the client. How the client handles it is up to the client. In the command line tool, the response is simply displayed on screen. In the case of the game, the programming sifts through the response, picks out the score(s) and draws some graphics to show the high scores. Every client uses the data in whatever way it was programmed to use the data.
So Business Objects is essentially running to the SQL Server (Engine) with a query to download the data.? Hence BO = Client and whatever our company had the SQL on was the Engine? Thanks!
I've never used BO so I can't give you any specifics but yes, it has two parts, a client and an engine. It may even feel like the two are built into a single program... that doesn't matter, there's still two parts. Of course, for flexibility, you want to be able to switch clients and engines whenever you want. Let's say your client is the PHP programming language and the engine is Oracle. After a few years, you decide that PHP doesn't have the features you want... you should be able to switch to a new programming language (like Perl) and leave the engine where it is. (So you don't lose your data). Similarly, maybe PHP is working fine for you but Oracle is too expensive or the database engine you're using is too slow or doesn't have certain features you want. You should be able to continue to use PHP and simply connect to a different engine. I don't know about BO specifically but if the client and engine are built into a single program, you might be locked in.
Thanks, I've watched about ten Youtube clips on SQL; read through a lot of walk throughs and only know how to use it and not really the core client/engine relationship. You've explained it very well - thank you so much again! 
if he tried to do: SELECT Person, Car FROM AllPeople LEFT OUTER JOIN PeopleCars ON Person = Person he would get an ambiguous column reference error. But when he uses the alias pc and ap it works fine. You can take a look at this StackOverflow thread. http://stackoverflow.com/questions/431391/php-mysql-how-to-resolve-ambiguous-column-names-in-join-operation
I spent about 6-7 years working with BusObj. The "Universe" is a layer that separates the objects from the actual SQL tables and columns. So if you grab 3 objects from a universe, the BusObj engine will use it's "preprocessor" to generate a SELECT obj1, obj2, obj3 from table1 [join table2 if necessary] statement. 
There is no way in SQL Server to copy the structure of a table including indicies and constraints. What you suggested is the only way to get close, and that only creates the table and column datatypes not the indiceis and constraints. 
BO? I knew I smelled something. Ahem. But seriously, BO is not really an SQL client from the end user's perspective. It's a Business Intelligence tool that abstracts database information to a higher level for users to manipulate and query as they wish. So yes, you're correct in stating that BO is client to whatever database (or databases) you're pulling data from. I believe you can even write SQL queries directly in a BO Universe with the Designer tool, or just use the pretty pictures and symbols to aggregate or drill down data as you need it. 
At my company we have a proc in the DB for scripting objects on the fly, If I need to copy a table I run EXEC uScript 'MyTable' and it spits out a script, I suppose it wouldn't be difficult to add a renaming chunk to the proc too if I wanted to return a script for a copy of it. I'd share it here but it's not mine to give away. Either way if I were to strike out on my own I think that similar tools would be high on my to do list. If there isn't a free alternative. Either way learning how to query object definitions from the database is a skill that affords a lot of other useful tools like searching object definitions for keywords and fun stuff like that.
Fyi you can write sql in Excel but first you need a working odbc connection string (or established dsn) for your database. 
no, it works whether rates go up or down
you're welcome.
First, "Email Report" option from [here](https://support.quest.com/KBArticleImages/SL4092/98201016.jpg). There is no easy way to edit the created maint plan to remove that option, considering you are new to this. I'd recommend deleting the existing maintenance plan and creating new. [Here](https://dl.dropbox.com/u/2928961/maint1.jpg) is how you delete it. Next, create the maintenance plan again, this time, don't choose email report. Based on the tasks and schedules you select, the maintenance plan creates a bunch of jobs in SQL Server. You can find them by expanding SQL Agent node. Select a job, right click ==&gt; properties ==&gt; Notifications ==&gt; Enable email notification [when job completes](https://dl.dropbox.com/u/2928961/maint2.jpg). Repeat this for each job. Now these instructions assume that you have database mail configured and an operator created on SQL Server. If not, follow [these instructions](http://www.orcsweb.com/blog/desiree/how-to-set-up-database-mail-for-sql-server-job-failures/) to configure database mail and then go ahead with maintenance plan. 
I'm thinking that my best option at this point is to restructure the query so that I bring back the maximum v19_dat_uid_start with a subselect in the WHERE clause (something like WHERE v19_dat_uid_start = [subselect to find maximum with replicated conditions]) rather than doing a GROUP BY + ORDER BY DESC + LIMIT 1.
Ok understood. Did you try to put the coalesce in the group by? Select coaleasce(a,b) as field1 from table group by coaleasce(a,b) If this works, try the sum in the selection and use this as a subselect. select sum(field1), field2 from ( Select coaleasce(a,b) as field1, field2 from table group by coaleasce(a,b) , field2) group by field2 
I would guess that their OCR software is looking for a set number of characters, and won't just take an arbitrary length string. Funny idea though.
no prob, i misread shit all the time too :)
I don't have experience with progreSQL either. Does it support Cross apply's? Cross apply's saved me from tons of headaches. 
Haha, this is possible if you the database admin forgets to sanitize inputs. [Relevant](http://xkcd.com/327/)
Or, more usefully: http://bobby-tables.com Sanitizing inputs is the sloppy way. Bind parameters are the way to go.
99.9% sure not possible, but funny as fuck though.
And that assumes that there's any OCR going on at all. The image is probably flagged when there's a violation and reviewed by a human. Also, you would have to know the name of the table to make this work even if the OCR database user had the rights to drop a table. Its still a funny idea though.
yep, you are good, you should be able to use the wizard in management studio to bring in the tables, using the wizard you can also save the dts packages and then use scheduler to import the tables nightly.
sweeet. I just noticed I have the Business Data Visual Studio thing.
And it looks like Visual Studio does not allow OLE DB for ODBC... Screwed again, thanks fishbowl
Shazaam! 32bit Dll was the key! I got Visual Studio to connect, I'm sure I will have a dozen or so questions...
I just realized that I didn't submit the additional note I had typed in. I went on to explain that Visual Studio integrates with almost all databases that are OLE-DB compliant(pretty much everyone he listed). Eventually with a goal to remove / migrate the obscure DBMS in lieu of more common and well supported versions. Replacing an IDE across the board is also a great opportunity to move old storage methods to new. P.S. - LINQ is awesome.
Rapid SQL is a great one size fits all, but they have made a mess of the thing in recent years. I've heard TOAD is awesome for many years, but they cost even more than Rapid SQL. There is also some concern about them being bought up by Dell who have also slipped a lot in recent years. I'm hopeful that Dell's move back to a private company might solve their quality issues, but that's just optimism.
At our company the DBMSes we use are a dependency of the vendor software that uses them, so we don't have the option of just putting everyone into the system we like. However, I am having our DW and custom applications use our MSSQL 2008 R2 DB. I was aware that VS had some SQL development in it, but I always found the environment to be unfriendly. I know that MS has a tendency to act shadey when integrating with competitor software as well, but maybe it is much better since the last time I tried it. I'll have to play with it and get back to you. Does VS use the TNS for Oracle connections, or does it insist on ODBC or OLE connections to get through.
TOAD
No doubt. It is fun at first to work with so many disparate and mostly broken systems, but after you have been in charge for a period of time, you are consumed with the idea of consolidation and standardization for the sake of maintenance. 
Awesome 
I really like Database .NET www.fishcodelib.com Commercial license is $19 bucks, support is great when I've emailed. 
This looks very promising. Thanks.
Vendor supplied admin tool for DBA stuff (SSEM, PgAdmin, OEM), and DbVisualizer for schema navigation and coding. User DBArtisan in the past, but no longer on a Windows machine, and would not justify the cost today anyway.
We use Toad Data Point for our automation and for large data sets, but not for developing code. Why? Because TDP - like TDA before it - has some severe memory and performance issues the longer a query file becomes. It's also got some rather retarded problems with copy &amp; paste as well as find &amp; replace - all of which are completely unacceptable in a text IDE. Now TDP IS cross-platform, and I do use it to query Oracle and MS SQL all day, and it does have some really awesome tools, but if you're like me and you code by writing, it sucks. What I've taken to doing is using SublimeText to develop my code, then dump it into TDP/SSMS to run. Works great for me.
That is a bit strange but as you say, it's designed behaviour when you think about it. The FROM statement isn't being ignored completely - your WHERE statement still has to evaluate as true for the delete to work. If you do : DELETE @Table FROM (SELECT *, row_number() OVER (PARTITION BY Item ORDER BY Item, OtherItem) AS RowNumber FROM @Table) A WHERE A.RowNumber = 0 -- or any other condition which is false Then it won't delete anything. You could put a warning I suppose, but in this case your where clause is referencing the subquery, even if you're deleting from another table. I agree that it does mean you can do some slightly counter intuitive things without realising, e.g. DELETE @Table FROM @Table2; I suppose if you're going to use the FROM statement in DELETE statements you should probably try to always use an alias explicitly.
yeah! I'm on a roll. My issue now is how to best Synch the tables. Most of the commands seem to be better suited to databases on the same Server (or similar database types). for example I found this: SELECT * FROM tableA WHERE NOT EXISTS( SELECT * FROM tableB WHERE pkey.tableA = pkey.TableB) but I am having trouble figuring out where in the Import process to run the command. If I do it in the Source connection, then I can't get the Destination data and vice versa. A few work around suggest pulling the entire table down to compare, but if I'm going to do that I might as well just use Drop Table and recreate them, which is how i have it setup now. http://i.imgur.com/Gd13jPJ.jpg The obvious glaring flaw in this workflow is if the Source server is down for some reason, the workflow is going to drop the Tables form SQLSVR prior to attempting the connection so I would not only not get the update but lose the current stuff as well :) 
Good catch on the true/false, so it seems it interprets the FROM in this situation as an EXISTS.
Not sure what your specific question is here. But in your original query - you are deleting FROM @Table and never really referencing which rows you want to delete. So SQL is just saying "Well, there are records in his FROM, and there are no conditions on what to delete in @Table so lets just delete everything". In your first query, if you changed the subquery to say "WHERE Item &gt; 12", then it should delete zero records from @Table. Does that make sense?
Yes. I worked that out after submitting and playing with the samples some more. Then in another comment we worked out in this case the FROM is being treated like an EXISTS. Which is a little weird. 
Indeed. In my @Table / @Tabl2 example, if @Table2 is empty then 0 rows will be deleted.
D'OH! in the Import wizard there is an "Append Rows in Destination table" checkbox. I suppose that might Append Rows in Destination table for me... 
Ya that is what I was trying to say - I was just not wording it correctly. Nowhere in your query are you telling it what to delete from @Table, so it just uses the FROM as an EXISTS.
if dates are going to be used heavily in your database then you might want to consider time ranges (effective start and end date). Also it would be worth it create a date table for any date math you may have to code for. That task is easily done within Business Intelligence. You set up a cube connected to your database and use two related tables to create dimensions. Once that is complete you can create a canned time dimension from a start date to an end date. It will spit out a table in your database for each day between the start and end date.
Kind of a deal breaker on the MSSQL until they get it though.
This is listed as Spyware.Exploit.Misc.MU.fishcodelib.com at my place of business.
I'm reading the features on this thing and keep going, "Ooooooo" aloud. My programmers and analysts are getting curious.
not sure what you intend on doing but i use sql for reporting only. i drop the tables and re-import them nightly. works great.
Try this: ;WITH FirstSales (CustomerID, AppointmentDate, SalesRepID) AS ( SELECT DISTINCT A.CustomerID , MIN(AppointmentDate) , A.SalesRepID FROM ResultedSales RS JOIN Appointments A ON A.AppointmentID = RS.AppointmentID AND A.CustomerID = RS.CustomerID WHERE StatusID = 'Resulted' GROUP BY A.CustomerID, A.SalesRepID ) SELECT DISTINCT A.AppointmentID FROM Appointments A JOIN ResultedSales RS ON RS.CustomerID = A.CustomerID JOIN FirstSales FS ON FS.CustomerID = A.CustomerID WHERE A.StatusID = 'Resulted' AND A.AppointmentDate &gt; FS.AppointmentDate AND A.SalesRepID = FS.SalesRepID
Dang, that was fast. Added on AND A.AppointmentID Not in (select AppointmentID from ResultedSales) to the second query and I'm pretty sure this is what I was looking for (Still testing). Thanks very much for the help! Is there a more efficient way to exclude appointment records that are in ResultedSales?
Well damn.. get that corrected?
I think you can use sum(runtime) as your third column seeing as you have already grouped the data.
You should watch some of the videos on it, or download it and try it out. It's not the cheapest thing out there (cheaper than Rapid SQL, though) but if you really need to do a lot of work with SQL on a daily basis, I haven't found its equal. 
I downloaded it today, and I already find it superior to Rapid SQL in many ways. I'm going to use it for the week to be sure.
Interesting. I'll email the author and let him know. ESET says I am A-OK. 
SELECT AlbumName, COUNT(AlbumName) AS NumberOfTracks, sum(Runtime) from (SELECT AlbumName,cast(substring(runtime,1, charindex('.',runtime,1)-1) as int) * 60 + cast(substring(runtime,charindex('.',runtime,1)+1, len(runtime)) as int) as Runtime FROM Tracklists) b group by AlbumName The trick is to treat the field as a string. You have to convert the minutes to seconds (*60) + seconds. once you have that then you can group it within an outer query.
Here's what I came up with to get your formatting right. I assumed that the [Runtime] data was minutes.seconds, since the largest number after the . was 56, and not something that would cross the minute boundary. I threw it into a table variable to make it easier for me to play along. DECLARE @Tracklists TABLE (AlbumName NVARCHAR(128), Tracks NVARCHAR(128), Runtime NVARCHAR(10)) INSERT @Tracklists (AlbumName, Tracks, Runtime) VALUES ('Bat Out of Hell II: Back Into Hell','I''d Do Anything for Love (But I Won''t Do That)','12.00'), ('Bat Out of Hell II: Back Into Hell','Life is a Lemon and I Want my Money Back','8.00'), ('Bat Out of Hell II: Back Into Hell','Rock and Roll Dreams Come Through','5.50'), ('Bat Out of Hell II: Back Into Hell','I Just Won''t Quit','7.21'), ('Feel the Steel','Eyes of a Panther','3.37'), ('My World 2.0','Baby','3.34'), ('Sounds of a Playground Fading','Sounds of a Playground Fading','4.44'), ('Soundtrack to your Escape','Superhero of the Computer Rage','4.01'), ('Soundtrack to your Escape','My Sweet Shadow','4.39'), ('Soundtrack to your Escape','Borders and Shading','4.22'), ('The Razors Edge','Thunderstruck','4.52'), ('The Razors Edge','Money Talks','3.48'), ('The Razors Edge','Let''s Make It','3.32'), ('The Razors Edge','Shot of Love','3.56'), ('The Razors Edge','Fire Your Guns','2.53'), ('The Razors Edge','If You Dare','3.08') SELECT ts.AlbumName ,ts.NumberOfTracks ,CONVERT(NVARCHAR(2), ts.AlbumRuntime / 60) + ':' + CONVERT(NVARCHAR(2), ts.AlbumRuntime % 60) as [AlbumRuntime] FROM ( SELECT tl.AlbumName ,COUNT(tl.Tracks) as [NumberOfTracks] ,SUM(CASE WHEN CHARINDEX('.', tl.Runtime) &gt; 0 --Just a little wrapper to ensure we only try to convert things that actually have the faux decimal to integers THEN CONVERT(INT, RIGHT(tl.Runtime, LEN(tl.Runtime) - CHARINDEX('.', tl.Runtime))) --Seconds + CONVERT(INT, LEFT(tl.Runtime, CHARINDEX('.', tl.Runtime) - 1)) * 60 --Minutes converted to seconds ELSE 0 END ) AS [AlbumRuntime] FROM @Tracklists tl GROUP BY tl.AlbumName ) as ts [Results](http://i.imgur.com/yeFYLiz.png) You might want to look into using [Replicate](http://msdn.microsoft.com/en-us/library/ms174383.aspx) to pad the zeros in the minutes or seconds. *edit:added note about Replicate
That's all I'm after as well. Do you have a safeguard against dropping the table then having the connection unavailable for some reason? i.e. Source Server Down etc.?
I think it's worth noting that many of the features described here only apply to SQL Server 2012 and not earlier versions.
See above. It's in SQL and it doesn't contain any of my attempted modifications. Sorry, I just looked at the side bar and saw there's a difference between the different SQL's. This is MS SQL.
**You need to show us the table and index definitions.** Diagnosing slow queries requires full table and index definitions, not just a description or paraphrase. Maybe your tables are defined poorly. Maybe the indexes aren't created correctly. Maybe you don't have an index on that column you thought you did. Without seeing the table and index definitions, we can't tell. If you know how to do an `EXPLAIN` or get an execution plan, put the results in the question as well.
I just pasted it at the end and didn't get anything different. Should I replace: SELECT DBO.a_invoice.totalship_amount Or something like that????
what is the complete syntax you have? Is this sql server?
I'm in Excel 2007 connecting to SQL server.
Static. 15 miles must be the number
can you show me the query as you have it? Did you try just running my query separately?
You might better off this being a table where LocationA is every zipcode in the UK, LocationB is the Cinemas Zipcode, and Distance between is the your distance code. Then you just query the data based on the 15 mile radius. The nice thing is you can build this whenever you want using SQL Agent. You also can control the time the query takes by doing that. And your not stuck just doing 15 miles, you can make that a variable and pass it to a stored procedure.
This is currently kind of what I have Table A Table B Query Takes table A and works out the distance to everything in Table B only returning values less than 15 miles. I am not quite sure how SQL Agent could help me out here? I feel a bit out of my depth to be honest, and I would say I'm definitely an intermediate skilled user of SQL.
ok, that won't work. Which part of the Union is it going into, top or bottom? Is it supposed to mimic "-DBO.a_credit_memo.credit_amount+DBO.a_credit_memo.sales_tax_amount" in the bottom union where "DBO.a_invoice.totalship_amount" is?
Are these all temp tables? I would think querying the source tables with indexes on the geography problems would be better.
Yeah, that looks good. The main idea behind your article is completely applicable to earlier versions, it was just some of the *garnish* that threw me, as it were.
But it would need to be run at least once to achieve this, right?
Will give this a try:)
so try this: SELECT cm.credit_memo_number , cm.date_issued , -cm.credit_amount+cm.sales_tax_amount , cm.invoice_number, i.project_no, i.job_no, i.approved, sr.name, pjn.budget_name, pjn.program_name FROM DBO.a_credit_memo cm INNER JOIN DBO.a_invoice i ON cm.invoice_number = DBO.i.invoice_number INNER JOIN DBO.a_proj_setup_new pjn ON i.project_no = pjn.project_no INNER JOIN sr ON a_proj_setup_new.sr_proj_mgr = a_salesrep.salesrep_no WHERE (((cm.date_issued)&gt;'12/31/2012') AND (cm.date_issued)&lt;'01/01/2014') UNION SELECT i.invoice_number , i.invoice_date , i.totalship_amount - at.cr_amount , i.invoice_number, i.project_no, i.job_no, i.approved, sr.name, pjn.budget_name, pjn.program_name FROM DBO.a_invoice i INNER JOIN dbo.a_acct_trans at on i.invoice_number = at.main_key INNER JOIN DBO.a_proj_setup_new pjn ON i.project_no = pjn.project_no INNER JOIN DBO.a_salesrep sr ON pjn.sr_proj_mgr = sr.salesrep_no WHERE (((i.invoice_date)&gt;'12/31/2012') AND (i.invoice_date)&lt;'01/01/2014');
Cleaned up OP's SQL so people with time can actually read it: SELECT DBO.a_credit_memo.credit_memo_number, DBO.a_credit_memo.date_issued, (0 - DBO.a_credit_memo.credit_amount) + DBO.a_credit_memo.sales_tax_amount, DBO.a_credit_memo.invoice_number, DBO.a_invoice.project_no, DBO.a_invoice.job_no, DBO.a_invoice.approved, DBO.a_salesrep.name, DBO.a_proj_setup_new.budget_name, DBO.a_proj_setup_new.program_name FROM DBO.a_credit_memo INNER JOIN DBO.a_invoice ON DBO.a_credit_memo.invoice_number = DBO.a_invoice.invoice_number INNER JOIN DBO.a_proj_setup_new ON DBO.a_invoice.project_no = DBO.a_proj_setup_new.project_no INNER JOIN DBO.a_salesrep ON a_proj_setup_new.sr_proj_mgr = a_salesrep.salesrep_no WHERE DBO.a_credit_memo.date_issued &gt;'12/31/2012' AND DBO.a_credit_memo.date_issued &lt;'01/01/2014' UNION SELECT DBO.a_invoice.invoice_number, DBO.a_invoice.invoice_date, DBO.a_invoice.totalship_amount, DBO.a_invoice.invoice_number, DBO.a_invoice.project_no, DBO.a_invoice.job_no, DBO.a_invoice.approved, DBO.a_salesrep.name, DBO.a_proj_setup_new.budget_name, DBO.a_proj_setup_new.program_name FROM DBO.a_invoice INNER JOIN DBO.a_proj_setup_new ON DBO.a_invoice.project_no = DBO.a_proj_setup_new.project_no INNER JOIN DBO.a_salesrep ON a_proj_setup_new.sr_proj_mgr = a_salesrep.salesrep_no WHERE DBO.a_invoice.invoice_date &gt; '12/31/2012' AND DBO.a_invoice.invoice_date &lt;'01/01/2014';
Your query results in a cross join full table scan of all of the data in both tables, rather than an index based lookup - which is a huge task for the RDBMS. However, you also mentioned in one of your replies that you don't have indexes on the columns, which is why you have issues. You need to index things. A better approach would be to use spatial indexes or have a computed column based on geoloc and create an index on that. Then write your query based on that. 
error reporting. my providex database never goes down (knock on wood) but i have had slight schema changes that have broken my import.
Well the geoloc columns are unique so i can build an index. Do i need clustered or non clustered?
Will check this link out.
Sorry- isn't all the processing time coming from MAKING the distances? Surely the filter is cutting down the amount of results returned.... Your way would only make it longer. I will post a more clear example tomorrow.
Will see if I can get this to you when I'm back in work tomorrow. I have seen people online getting this working down to a few minute run time, so there's definitely some optimization to be run. I have included an execution plan and times are going over 10 hours, how does this affect performance? Negatively I assume?
Ahhhh yes. To be honest i have the user bit sorted. Its just getting the brunt work done as it needs to be updqted quarterly
Thanks everyone for your help, I really grateful :) 
I don't suppose you'd be familiar enough with FirebirdDB to know how I can get the Schema "exported" or what ever it is you do to explain your schema to SQL Server. I think one of the reasons that ODBC was partially abandoned in Visual Studio was a weakness with the amount of info it was able to pass, but then I don't understand what a schema is or does short of it's some sort of framework definition
&gt; Will see if I can get this to you when I'm back in work tomorrow. Don't do it for me. I probably won't be able to look at it. I'm just pointing out that we can't effectively do optimization without knowing what tables/indexes are being operated on.
It was quite a bit more difficult than the practice exams or the practice questions in the Exam Prep book.
so you set up the query to run quarterly using SQL Agent. That way you table will regenerate when you need it too without you even knowing.
I like Navicat, it's pricey, but it works! I use a Mac so I needed something that would run natively and do multiple dbs. Now if the would add support for DB2 I could get rid of RazorSQL. 
It was tougher than practice tests. And, most questions were "pick any correct answer" which I find tougher. I'd check out a dump online and make sure you understand the format of most questions.
Ok thanks
Very tough. good luck. It's very important to understand all the different types of joins. Read and understand the Concepts Guide. Good luck!
I looked at flamerobin previously, but I don;t think it helped with what I need to do, which is just extract data. We are a medium sized Manufacturing Company and we only use Firebird because it is the embedded database FishBowl Inventory uses, and the only reason we use FihsBowl Inventory is because we use QuickBooks, and the only reason we use QuickBooks is.... I'm not sure why we use quickbooks to be honest, I suspect there is no real reason.
i know why, it was what you started on, it sorta worked and was cheap, so you kept using it, you grew and needed better inventory management or BOM/workflow control so you looked for a bolt on to qb... just curious. it probably sucks to maintain manage from an IT perspective but gets the job done at a relatively low cost. so the thought of replacing it is mostly a pipe dream
yup, I got here looooong after those decisions were made. In my first year I walked them all the way to the dotted line on a full blown Epicor implementation but they balked at the $xxx,xxx.53... It was a good move actually, you're right, fishbowl is sucky to maintain, but it is fairly "reliable" as far as repeating the same set of issues with respect to slowdowns, crashing on wide open report requests, etc. But it's manageable but very hand-holdy. Which is why I'm doing what I'm doing from a reporting end. It seems to handle transactional stuff at a decent rate, and their mobile client works well enough. So if i can move report off of FishBowls work load and use some real tools I can integrate it into SharePoint, automate Reports wish JasperServer etc... I've go much better ways to spend $XXX,XXXX dollars.
A .sql file is just a plain text file. You can create it with any text editor.
As an additional note, it is good practice to add *SET NOCOUNT OFF* at the end of any query that uses the above, especially those that might be used somewhere/by someone else; just to prevent the potential for confusion.
At a glance, the command looks right. However, as someone who's been hit by it before, have you tried replacing your tic marks? If you've copied from somewhere like Outlook/Word then often times it's wrong and you have to manually replace them. The C:\... text should be in red if you've got the proper tics. ' vs. ’
Twas the ' marks, I did copy then from a message board, changing them seems to have worked =D
If you already have a query to count the records add pivot to the bottom like so. Select * from dbo.mytable PIVOT ( SUM([Count]) For [type] in ('show','noshow','canceled') ) as ptable Also, try in the future to avoid using type and count as column names as it can get annoying to have to put them in brackets to make them literals. 
Dirty manner: SELECT id, canceled = SUM(CASE WHEN type = "canceled" THEN count ELSE 0 END), no show =... FROM yourTable GROUP BY id Or look at PIVOT. I hate doing that.
Not necessarily, it,s got local session scope not global scope so unless you need it back on again ... Unlikely ...this doesn't really apply.
What is your query? 
Always check for nulls when concatenating fields. Use Coalesce instead of nested isnull. Format consistently to make it easier to find syntax errors. SELECT p.ProductCode AS id , p.ProductName AS [stripHTML-title] , Coalesce(p.Google_Product_Type, pp.Google_Product_Type , 'n/a') AS product_type , Coalesce(p.Google_Product_Category, pp.Google_Product_Category, 'n/a') AS google_product_category , IsNull(p.SalePrice,p.ProductPrice) AS price , IsNull(p.ProductManufacturer,'n/a') AS brand , IsNull(p.ProductCondition,'n/a') AS condition , CONVERT(VARCHAR(10), (GETDATE() + 29),120) AS expiration_date , p.ProductDescription AS [stripHTML-description] , 'Config_FullStoreURLConfig_ProductPhotosFolder/' + isnull(p.ProductCode, '') + '-2T.jpg' AS image_link , 'Config_FullStoreURLProductDetails.asp?ProductCode=' + isnull(p.ProductCode, '') + '&amp;click=2' AS link , IsNull(p.Google_Age_Group, pp.Google_Age_Group) as age_group , IsNull(p.Google_Availability, pp.Google_Availability) as availability , IsNull(p.Google_Color, pp.Google_Color) as color , IsNull(p.Google_Gender, pp.Google_Gender) as gender , IsNull(p.Google_Material, pp.Google_Material) as material , IsNull(p.Google_Pattern, pp.Google_Pattern) as pattern , IsNull(p.Google_Size, pp.Google_Size) as size , Coalesce(p.UPC_Code, pp.UPC_Code, p.Book_ISBN, pp.Book_ISBN, p.EAN, pp.EAN)as gtin , IsNull(p.Vendor_PartNo, pp.Vendor_PartNo) as mpn , p.IsChildOfProductCode as item_group_id FROM vMerchant.Products_Joined p LEFT OUTER JOIN vMerchant.Products_Joined pp ON p.IsChildOfProductCode = pp.ProductCode WHERE ISNULL(p.EnableMultiChildAddToCart, 'N') = 'N' AND ISNULL(p.EnableOptions_InventoryControl, 'N') = 'N' AND (ISNULL(p.HideProduct,'N') &lt;&gt; 'Y' OR ISNULL(p.IsChildOfProductCode,'') &lt;&gt; '') AND (ISNULL(p.Google_Product_Category, '') &lt;&gt; '' OR ISNULL(pp.Google_Product_Category, '') &lt;&gt; '') ORDER BY p.ProductCode
Put up the layout of your three tables.
SELECT p.ProductCode AS id , p.ProductName AS [stripHTML-title] , IsNull(IsNull(p.Google_Product_Type, pp.Google_Product_Type), 'n/a') AS product_type , IsNull(IsNull(p.Google_Product_Category, pp.Google_Product_Category), 'n/a') AS google_product_category , IsNull(p.SalePrice,p.ProductPrice) AS price , IsNull(p.ProductManufacturer,'n/a') AS brand , IsNull(p.ProductCondition,'n/a') AS condition , CONVERT(VARCHAR(10), (GETDATE() + 29),120) AS expiration_date , p.ProductDescription AS [stripHTML-description] , 'Config_FullStoreURLConfig_ProductPhotosFolder/' + p.ProductCode + '-2T.jpg' AS image_link , 'Config_FullStoreURLProductDetails.asp?ProductCode=' + p.ProductCode + '&amp;click=2' AS link , IsNull(p.Google_Age_Group, pp.Google_Age_Group) as age_group , IsNull(p.Google_Availability, pp.Google_Availability) as availability , IsNull(p.Google_Color, pp.Google_Color) as color , IsNull(p.Google_Gender, pp.Google_Gender) as gender , IsNull(p.Google_Material, pp.Google_Material) as material , IsNull(p.Google_Pattern, pp.Google_Pattern) as pattern , IsNull(p.Google_Size, pp.Google_Size) as size , IsNull(IsNull(IsNull(p.UPC_Code, pp.UPC_Code), IsNull(p.Book_ISBN, pp.Book_ISBN)), IsNull(p.EAN, pp.EAN))as gtin , IsNull(p.Vendor_PartNo, pp.Vendor_PartNo) as mpn , p.IsChildOfProductCode as item_group_id FROM vMerchant.Products_Joined p LEFT OUTER JOIN vMerchant.Products_Joined pp ON p.IsChildOfProductCode = pp.ProductCode WHERE ISNULL(p.EnableMultiChildAddToCart, 'N') = 'N' AND ISNULL(p.EnableOptions_InventoryControl, 'N') = 'N' AND (ISNULL(p.HideProduct,'N') &lt;&gt; 'Y' OR ISNULL(p.IsChildOfProductCode,'') &lt;&gt; '') AND (ISNULL(p.Google_Product_Category, '') &lt;&gt; '' OR ISNULL(pp.Google_Product_Category, '') &lt;&gt; '') ORDER BY p.ProductCode
sorry not great at posting that much information. If you click on the link, the information is right near the top.
What keys are these tables joined by? Your description indicates the Enemies table will map to items, but unless I missed it in my quick glance, I don't see itemId on the Enemies table. Are the columns like "enemyDropNum" the item Id's (if so, that naming convention is a bit unintuitive). Having said that, I'm also a little confused about your desired output. Are you looking to roll up all data into a single row per item (comma separated list of enemies or something to that effect) or are you looking for one row for every item/enemy combination (so if item X is dropped by 10 enemies you'll have 10 rows of output data).
Just as a general suggestion: It would really help people address SQL questions like these if you provide some basic SQL to create the tables in question and populate them with a few example rows, then also provide your desired output as you would like it to appear based on this (hopefully not entirely trivial) sample data set.
Yeah, you're completely right. I suck at anything over extremely basic SQL statements. I think I'm going to start from scratch and try and do it correctly, but that may be a while.
Sit down and think about how the data in these tables relate to each other. Consider whether or not the tables have a 1:1, 1:many, or many:many type relationship and use this to determine which tables need to reference the ids of the others. Additionally, depending on your storage engine, consider and implement foreign key relationships to preserve data integrity. You're on the right path, by the way, normalizing this data across several tables, you just need to think a bit about the relationships. With the proper relationships in place, the type of query you're asking about will be downright trivial.
Thanks!!! it worked!
Thanks!!!
I've used something like this in the past, in its current form it will work with stored values in the count field, but could be modified to count values or with a sub query. SELECT dbo.id.id, Max(t1.count) AS 'Canceled', Max(t2.count) AS 'NoShow', Max(t3.count) AS 'Show' FROM dbo.id INNER JOIN dbo.id t1 ON t1.id = dbo.id.id AND t1.type = 'Canceled' INNER JOIN dbo.id t2 ON t2.id = dbo.id.id AND t2.type = 'No Show' INNER JOIN dbo.id t3 ON t3.id = dbo.id.id AND t3.type = 'Show' GROUP BY dbo.id.id 
Wild guess, giving your vendor the benefit of the doubt. There is a MS quote or punctuation and you're copying with formatting. Try pasting it into vim or notepad or something first. I've run into quite a few issues where something that looked like a quote, was actually something else. There are quite a few characters that apps like word/outlook/other super rich text editors use in place of things like single quotes or hyphens.
This is the best article/explanation I've run into about WHY you get that error when you're using GROUP BY clauses. http://weblogs.sqlteam.com/jeffs/archive/2007/07/20/but-why-must-that-column-be-contained-in-an-aggregate.aspx 
Basically, your error is telling you that you cannot get all of the columns from a table and have a count in the same query, without telling SQL how to group together the other columns. What you are going to need to do it list the columns from tblModules in the SELECT and then add a GROUP BY clause at the end of the query (after the FROM clause). Something akin to: SELECT DISTINCT -- List all of the columns you want to see m.[module_id], m.[module_name], m.[module_description], m.[module_OtherAtribute], m.[GroupNo], -- You can get away with a * in the COUNT function COUNT(e.*) FROM --Notice the extra 'm' below, this is a table alias and lets me use the shorthand 'm' to refer to that table -- I do the same for tblEnrollments with an 'e' [tblModules] m INNER JOIN [tblEnrollments] e ON m.[GroupNo] = e.[GroupNo] GROUP BY m.[module_id], m.[module_name], m.[module_description], m.[module_OtherAtribute], m.[GroupNo] This will take each distinct module in [tblModules] and provide a COUNT of the entries in [tblEnrollments] which have that [GroupNo]. What this isn't getting you is [tblEnrolments].[StudentNo]; though, I think you aren't interested in seeing each student number. 
Your code above will only return values where the "studentid" is in both tables. You may want to create a table that has all the student id's from table s and only the matched Id's from table n. SELECT s.studentid,s.fName,s.lName,n.ContactTelNo FROM student s left join nextofkin n on s.studentid = n.studentid WHERE s.sex = 'm' Order By s.lName, s.fName; Hope that helps. 
I'll try that tomorrow. Thank you
What version of Oracle are you using? What version of SQL*Plus are you using?
Thats a really good question. I dont know why anyone would tell you not to use joins. Joins are a necessity in relational databases. "Joins" are treed from the root table. Using joins you can attach tables using outer, right, left, inner joins and joins which specify whether to join the table regardless of whether it has matching data or not. "Where" clauses can be nested, ie do whats in parenthesis first and work your way out (similar to the logic in mathematics). This could be extremely cumbersome in very normalized databases. I'm not sure how you would do an outer join using your example where clause, you would only be able to retrieve the data that exists in both tables. Lastly, performance wise it seems like it would be the same but I could so some analysis to see if there are more or longer scans with either method. A teacher that says joins are bad seems like a bad teacher imo. The most misused things in SQL are cursors and non-clustered indexes.
&gt; Currently in my second year degree at Uni and we have been explicitly told to not use joins this is, not to put too fine a point on it, bollocks i appreciate that it might be too difficult to change universities, but you're being severely shortchanged
Unless I'm completely misunderstanding you, that's not an Oracle join. Oracle joins use (+) symbols to differential left vs right joins. Or rather, they used to. Oracle switched to SQL-99 quite a while ago.
You are already doing joins. You're just using an antiquated syntax. Now try righting a query with 10+ tables and tell me how much you like this syntax. And joining 10+ tables in a single query can be a real world scenario (esp. in data warehousing). Anyways, it makes for much easier tell what is an inner versus left outer join. Embrace it.
Get ready to have your mind blown. That code example you posted *is* using a join, it's just an implicit join ( or a SQL-89, an early SQL standard, join). If your teacher is telling you to do that because of server load you should be seriously questioning their credibility. They have no performance difference. People will some times use implicit joins for a simple statement, but generally you should make your code as explicit as possible. It's a matter of taste, but the newer standard is generally preferred. The older is more prone to error and often considered harder to read. http://en.wikipedia.org/wiki/Join_(SQL)#Inner_join shows the difference, briefly. 
I may have developed a very loose definition of what an Oracle join is. I thought using joins without explicitly declaring them was of the oracle style.
It does have to do with server load. The way you are doing it right now forces the DBMS to perform a nested loop join, which could cause performance hits. Using a JOIN can actually give hints to the optimization engine to use the indices provided in the DB to help with the processing. Also, if you're being told to not use joins, then your profs need to be out of the industry. That has been out of date since 1990 ANSI SQL Standard. Horrifying...