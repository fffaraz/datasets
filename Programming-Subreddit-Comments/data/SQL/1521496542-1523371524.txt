My point still stands. You should at a minimum be making twice your salary.
As a developer, I use SQL every day to either develop our core product or troubleshoot issues. I also write a lot of reports using our database for users.
I use it in 3 areas. My first serious exposure to it was an application that utilized workflows that you built to accomplish whatever you wanted to do. This application allowed PowerShell scripting which I am heavily into. Add to that Invoke-SqlCommand and I was dropping in PowerShell/SQL queries to accomplish many things. About the same time I was using SQL more and more to make one off reports and changes for users. Often I would dive into the database, figure out where the data resides, build a query to get the data the user needs and then drop that it to an Excel spreadsheet so they could run the query whenever they wanted it. My current major role is using it for data warehousing. We use Oracle ODI to load data from a variety of accounting systems, Excel documents, text files, and web based API’s into a staging area. That data is then denormalized, and transformed into several reports. ODI simplifies the setup of this but having a good handle on SQL certainly helps.
I love you brother.
I usually put PL/SQL (ORACLE) and T-SQL (SQL Server) against each other to see which one can accomplish the task most efficiently and with fewest issues. I like to pretend I’m a Roman emperor watching from my throne as they battle it out gladiator-style.
Your SQL server must be set to GMT. You’re in central time I assume?
How do I change that? I thought I ran through all of the potential settings pretty thoroughly.
Stop being obtuse. He is saying he does your job (whatever it is you do) and that he has the same skillset that you have, but he makes $60/hr, or 124k/year. That is nearly three times what your salary is. What he is saying is that you should (could) be making that much with your current responsibilities/work load. Don't get lost in the details. You have a valuable skillset and you want to learn how to market it. Forget what "things" he does. He does the same "things" you do, and either of you could do the same "things" the the other one does. That is what he's saying. What he's saying is that you are DRASTICALLY underpaid. 
A job is all about the money my friend. The question is whether the money is worth it or not when it comes to added stress, etc. Your workload isn't that much less than everyone else here. You do the same shit we all do, but you get paid way less. It **is** all about the money. And we're going to help you make that money. Stop undervaluing yourself. It **isn't** OK to be underpaid, and that **is** in your control. You have a wife and kids? You need to provide for them. Go over to /r/personalfinance and ask this same question. Tell them you make 1/2 of what someone who is 10 years younger than you does with less experience and see what kind of responses you get. They'll tell you to fucking move. They'll tell you to fucking change. They'll tell you how to go out there and make money. Because that is what you're asking about. You aren't asking about SQL. You are asking about getting paid.
SELECT * FROM TABLE :) 
Ah shoot. Right... you said that like 2 times right above. Think I finally got that through my head now!
It'd help if you told us which platform you're using.
Running reports for users, monitoring client usage of our software, and checking to make sure nightly data dumps have been received. When I get bored I play with aliases in the internal reports and count how many "cows" have "socks" or how many blerghs are in poop. One of these days I'm going to accidentally send one of those to a client.
I build desktop apps and models, and use SQLite for the datastore. I spend a lot of of time entertaining myself by implementing stuff using CTEs, for fun, that would be really boring to implement in Java.
I’m a financial analyst so I use it to pull financial transaction data to see trends.
I’ve been trying out a few different ways to practice working with databases. I started out trying out SQLite to play around with a Pokemon database I found on Github, but realized pretty quickly that I couldn’t practice a lot of Oracle concepts in it. Then, I installed the demonstration schemas (HR and OE) for 12C on my computer, and accessing it through SQL*Plus (I couldn’t get SQL Developer to connect properly). I’ve been using the Oracle LIVE SQL website to practice on the HR schema when I’m not home as well. I'm not sure how much actual experience that adds up to. Making practice questions by section sounds like a great idea! I was definitely reliant on the practice exam set that came with my textbook for 061 (and it wasn't nearly enough), so I’m at least planning to buy the Kaplan/Transcender exam prep after I finish going through the 071 textbook. I'll give As for which exam to focus on, I don’t think I know enough about either path to decide on a specialization yet. For my purposes, I’d like to get an entry-level job first, and then learn more about specialization on the job/on the side. I’m definitely interested in the blogs and Twitter resources you’ve mentioned, as well as being a study buddy for 071. I’m still working my way through the textbook though, so I’m not sure how much help I will be for now! Thanks so much for your help!
I like crazy. Can you elaborate architecturally what your idea looks like? Are you saying using the shell to "send a query" and then if it responds with the desired output to use a specific command that would "write the query" to an existing Excel file. That's an interesting concept. You could just cycle back to the last ran statement add a prefix or syntax to write to the file... Tell me more about your crazy.
you forgot WHERE FIELD = SOMETHING
His application or table defaults could also be using `GETUTCDATE()`. Or wherever he's using to query the DB prefers UTC and his tables are `DATETIMEOFFSET`.
I mostly use it to make my boss think I'm smarter than I actually am. 
I work for an e-commerce website. I run reports for brands or see how a site change has affected traffic. I mostly look up my coworkers and see how many of them abuse the company discount. Or I see if my exes browse the site. Or I see how many users have the word '%sex%' in their username or email address. Or if one department says "we had a great promo!" then I like to go see how much it cost and how much we made and contradict them in meetings. 
Yeah that is classic. Managers that have no SQL or technical knowledge in general. Then after you provide some report they look at you like you are Houdini.
Well, you can use invoke-sqlquery (https://dbatools.io/functions/) to query the database and each column can be a object in powershell. From there you can manipulate or measure the outcome, or even run another query based on the outcome. Once you have your dataset the way you want it, then it is just a matter of exporting the data to excel. (https://blogs.technet.microsoft.com/heyscriptingguy/2014/01/10/powershell-and-excel-fast-safe-and-reliable/)
[Use a WHERE clause dummy](https://www.brentozar.com/archive/2017/12/can-prevent-deletes-inserts-without-clause-running/) 
 WHERE Name=ALEX Should this be: WHERE Name='ALEX' In your case, the while line should read: SET @Command = 'USE ' + @DB_Name + '; UPDATE ' + @table + 'SET Name=hc.alexander WHERE Name=''ALEX'');' It also looks like you have an unpaired open closed bracket after Alex, unless I missed something. 
Is ALEX a column name? If not, it will need to be properly quoted. Also, it looks like you are missing a space before the 'SET Name=' part, but it might just look that way because I'm on a phone.
Both hc.alexander and ALEX are values in the table. I want to change any value of "ALEX" in column "Name" to "hc.alexander" After making the space before "SET" I got a new error message: Incorrect syntax near ')'. I've tried ''hc.alexander'' and ''ALEX'', with no change to the result.
SQLite
You need single quotes, not double quotes, for any values. When I'm stuck using dynamic SQL, I will usually write the command out once, manually, and make sure it works before "making it dynamic", if you get my meaning
I helped build a web tool for an offshore team to tag images that were scraped from competitor websites. It was supposed to take their team a month to complete, but after a few days, users reported that they were getting an error where no images were presented to tag. I looked in the database and all of the images had been tagged. My PM and tech lead (boss) where convinced there was some backend bug, but really the team just worked faster than we expected.
How many dbas do you have in your team?
2 full time DBAs. Our environment is pretty relaxed but is global. Our largest database is 2tb and is an infosec database storing proxy logs / web activity. Our SharePoint farm is about 5tb of total space. So it's very manageable for us.
I'll check this out. Do you know if it is possible to Export it to Excel in very specific ways (i.e. to a specific cell, etc.?)
I would suggest getting an account on AWS. They have a free tier which will be able to offer you some pre configure managed database instances for mysql or postgres that should be more than enough for you to have a bit of a practice on. 
Timezones we're a real hassle for me when I was getting started, especially with daylight savings time here in the US
You will learn a huge amount of stuff from failure. Switch to MSSQL or Postegres or something like that. &gt;Anyway, I’m wondering what the easiest way to create a database and query it is- I am looking to practice making scripts to query databases and essentially clean data- likely option data from Option Metrics that I will filter by multiple... variables? I approve of your method of trying to learn SQL. It isn't the easiest way, and you will fail a lot, but you will internalize SQL instead of trying to memorize it. This will make sure you useful.
Use MSSQL on Windows. Login is your windows username and password. So you don't even need to worry about setting up an account on the database. And for a lightweight alternative to SSMS, you can get Linqpad. The free version is great but lacks intellisense.
You definitely can do that with Powershell.
&gt; I am looking to practice making scripts to query databases and essentially clean data Put a simple dataset of interest in an SQLite database and go from there. SQLite databases are file-based, so you don't have to deal with sysadmin and dbadmin issues. CSV -&gt; SQLite: https://www.sqlite.org/cvstrac/wiki?p=ImportingFiles 
I would read about 3NF and database normalization techniques in general... even if you're just going to be querying the data and not building a database from scratch yourself, it gives you a good understanding of how most data is going to be stored and the relationship between that data. https://www.essentialsql.com/get-ready-to-learn-sql-11-database-third-normal-form-explained-in-simple-english/
I haven't worked with xml datatype enough to clearly explain the hows and whys of these functions, but I was able to get the data out with the queries included below. Hope it helps you extrapolate to figure out your issue. /** Option 1 **/ ;WITH XMLNAMESPACES ( default 'http://www.jumbawumba.com/namespaces/API/reporting' ) select lsid = @xmldata.value('(/*/@lsid)[1]', 'VARCHAR(500)') ,start_time = @xmldata.value('(/session/start_time)[1]','datetime') ,end_time = @xmldata.value('(/session/end_time)[1]','datetime') ,duration = @xmldata.value('(/session/duration)[1]','nvarchar(50)') ,file_transfer_count = @xmldata.value('(/session/file_transfer_count)[1]','int') ,@xmldata /** Option 2 **/ ;WITH XMLNAMESPACES ( 'http://www.jumbawumba.com/namespaces/API/reporting' as ns , default 'http://www.jumbawumba.com/namespaces/API/reporting' ) SELECT lsid = @xmldata.value('(/*/@lsid)[1]', 'VARCHAR(500)') , Tbl.E.value('start_time[1]','datetime') AS start_time , Tbl.E.value('end_time[1]','datetime') AS end_time , Tbl.E.value('duration[1]','nvarchar(50)') AS duration , Tbl.E.value('file_transfer_count[1]','int') AS file_transfer_count , @xmldata FROM @xmldata.nodes('/session') Tbl(E)
You already have a commented out Print @command line... Go ahead and print @command instead of trying to execute it and review the script you are generating. Would you run that update command on it's own as it is printed?
Also, remember that the XML code is case sensitive.
If you have Linux I can get you started in like 5 minutes. Only takes 2 minutes to redeploy a clean database with all the table you want too. Containers are really awesome
It's inherited from the os. You don't set a time zone on sql install. 
Even the .value() method is case sensitive.
Thanks. I just found one called Vertabelo that looks good too. 
I kinda jumped the gun asking this question. In about 15 minutes, I've found a number of different tools. Hope this list can help someone else. DbSchema MySQL SQLEditor for Mac Navicat Data Modeler Vertabelo TeamSQL TablePlus DBeaver
So all of your data looks like this: | Input | | :--- | | 0 | | 100 | | 200 | | 300 | | 400 | | 500 | | 600 | | 700 | | 800 | | 900 | | 1000 | | 1100 | | 1200 | | 1200 | | 1400 | | 1500 | | 1600 | | 1700 | | 1800 | | 1900 | | 2000 | | 2100 | | 2200 | | 2300 | So the left most digit(s) represent the hour and the right most digits represent the minutes? This should help you: https://stackoverflow.com/questions/28870283/how-can-i-merge-two-datetime-parts-hour-minute-second-in-sql?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa Basically you're going to need to take the left values and push them together with the two values on the right and then hard code the seconds for 00s. You might find it easier to do something like this: select datepart(hh, left(col, 2)) + datepart(mi, right(col, 2)) from ( select case when len(col) = 3 then '0' + cast(col as varchar) when len(col) = 1 then '000' + cast(col as varchar) else cast(col as varchar) end as col from table ) x Have to fix that syntax but it should give you a place to start.
I break the database then I fix the database. It’s a möbius loop of employment.
You can try parsing it as TIME and then converting to VARCHAR in the desired format. Here's an example: DECLARE @N VARCHAR(50) = '350'; SELECT CONVERT(VARCHAR(50),CONVERT(TIME(0),LEFT(RIGHT('000' + @N,4),2) + ':' + RIGHT('000' + @N,2)),9)
 SELECT timeInt, Time=CONVERT(TIME(0),STUFF(STUFF(RIGHT('00000'+CONVERT(VARCHAR,TimeInt),6),3,0,':'),6,0,':')) FROM ( VALUES (0 ),(100 ),(200 ),(300 ) ,(400 ),(500 ),(600 ),(700 ) ,(800 ),(900 ),(1000),(1100) ,(1200),(1200),(1400),(1500) ,(1600),(1700),(1800),(1900) ,(2000),(2100),(2200),(2300) )a(timeInt)
while I was at uni we were taught using MySQL which I personally hated. I've heard that SQLEditor and DbWrench are ok. Personally I just run a windows VM on my mac and use SSMS.
I'm trying out dbeaver right now and it's pretty cool, plus it's free and easy to install: brew cask install dbeaver-community; brew-cask-install java.
Try SQLite. The easiest thing to set up and you can start playing around with queries in less than 2 minutes. follow this steps on this site: http://www.sqlitetutorial.net/download-install-sqlite/ and you'll be set up.
Maybe look at Open Modelsphere. Takes some getting used to but is pretty complete for a free dB modelling tool. 
Your VARCHAR data types need a length IE VARCHAR(30)
check out this page: [Creating Tables](https://dev.mysql.com/doc/refman/5.5/en/creating-tables.html)
Intellisense, my weakness.
SSMS is the GUI client/admin tool for MS SQL Server.
Haven't forgotten you, just haven't had any time in the last day or two to sit down and strip out personal info. Should be coming along tomorrow.
Try_convert
Thanks for the help!
Dm would be fine for some questions
Your varchars need a length. Also, consider making the following changes CONSULTANT_FIRST_NAME VARCHAR(30) NOT NULL, CONSULTANT_LAST_NAME VARCHAR(30) NOT NULL, CONSULTANT_ID INT, PROGRAM VARCHAR(20), AWARD_DT DATE, RELEASE_DT DATE, AWARD_TYPE VARCHAR(20), PRIMARY KEY (CONSULTANT_ID)); The primary key constraint will enforce that all of your consultant IDs uniquely identify each of your tuples and will prevent them from being null.
Thanks for that, I'm new to this community but i already know I'm gonna be spending a lot of time here haha 
Now it is like [this](https://i.imgur.com/MCEpYAi.png). However, I get syntax SQL error (1008) when I run the code.
Wouldn't you recommend to also add "NOT NULL AUTO_INCREMENT" to the "CONSULTANT_ID INT" since it is a primary key? 
No worries 
I'm not saying OP should start by studying normalization, but it's a much more interesting and important topic than you're making it out to be. And its importance is not just or even primarily related to server performance. If OP learns to conceptualize clean database design, they'll be way ahead of all the folks here who know all kinds of MS-SQL tricks but can't think intuitively about the relationships in their data and how to build a database that appropriately reflects them. 
No problem. Also, you should make "Awarded" into its own table: each entry should include the attributes of the award (date etc) as well as a foreign key that lets you join the table with the consultant table (the consultant id being my suggestion.) As is, your table will produce redundant consultant tuples whenever they win an award, and the tuples for consultants that have not won awards will have a lot of null fields. If its not clear what I'm trying to get across, I can explain further when I have computer access. I'm on my phone right now.
I think you'd have to hard code the list in the CHECK condition: CHECK (gender IN ('M','F','U','O'))
I would argue that by definition for a new user coming from Excel it isn't, simply because the constraints of Excel tell me that the size of his or her data is not significant enough for normalization to matter one bit. &gt; they'll be way ahead of all the folks here who know all kinds of MS-SQL tricks but can't think intuitively about the relationships in their data and how to build a database that appropriately reflects them. This I do agree with, and I do agree that normalization is an important concept, but I don't think it is one that is immediately relevant until after you can at least start navigating a database, selecting from tables, importing/exporting Excel files, and other basic tasks. Then you start going down that road. Just my .02.
First off, you don't need the timestamps unless you specifically need the timestamps secondly what will be passing the variables? because usually the simplest way to do this is to do a swap, either in the calling code or in the T-SQL. (Pseudo code) Define Date 1 and 2 Define DateStart and DateEnd If Date 1 &gt; Date 2, Datestart = Date 2, DateEnd=Date 1 Else Datestart = Date 1, DateEnd = Date 2 
Also adding a primary key will also create a clustered index on the key (in SQL server at least) improving performance on joins and lookups!
You can do something like this to work out which date falls first: DECLARE @date1 datetime2; DECLARE @date2 datetime2; SET @date1 = '2016-01-12 00:00:00.000'; SET @date2 = '2015-12-23 00:00:00.000'; SELECT @date1; SELECT @date2; If @date1 &lt;= @date2 BEGIN SELECT 'date 1 before or same as date 2'; END Else BEGIN SELECT 'date 1 after date 2'; END
&gt; simply because the constraints of Excel tell me that the size of his or her data is not significant enough for normalization to matter one bit. I don't know dude, have you seen what some organizations do with spreadsheets? Now that I remember what I dealt with as an intern at a nonprofit that used spreadsheets as their contact database, I think OP might love the normalization potential of a SQL database more than anything.
My organization is the worst at this, the files are behemoths with sometimes a hundred columns or more. Don't matter. In SQL I work/ed with tables in the hundreds of millions sin terms of order of magnitude. Anything you can physically open in Excel is so tiny by comparison to what a database can do that normalization is totally irrelevant (unless you're using their stupid SQL bullshit, which is stupid.) 
You could use the following: (date BETWEEN date1 AND date2) OR (date BETWEEN date2 AND date1)
Wow I really appreciate the effort you put into this. I'll give it a shot!
No problem at all; its second nature to me at this point.
I refer to it as wizardry.
These are called joins, they're more flexible than VLOOKUP. https://www.tutorialspoint.com/ms_access/ms_access_joins.htm
You need to JOIN the two tables to do what you are wanting. 
Nope, definitely -5 although it’s really -4 for CDT right now. 
Can you ELI5 for me please? I'm still trying to make sense of the basics..
Does the query work if you run it would out the date_shipped but keep the part like '41011801%'....?
Yes. Strange isn't it?
what is the error message? 
Thanks a lot for the source. I somewhat get and almost have the results I need. I am a noob at this since this is my first post college job where I ever had to work with a DB in real life so excuse the ignorance. Can these joins only be done by queries? I can somewhat get this but only if I use a combo box which is inefficient because after you add a lot more account records you have to scroll down to the correct name. The data from the account table is being inserted via a form. 
"Expression Evaluation Error" Might be peculiarities of Pervasive SQL and I can talk to IT about that. But posting here in case it's a fundamental SQL issue.
That is a bulk insert..? If you want to bulk insert **PL/SQL arrays** then there's this example: http://www.dba-oracle.com/t_bulk_insert.htm -- **************************************** -- create a test table for the bulk insert -- **************************************** drop table test_objects; create table test_objects tablespace users as select object_name, object_type from dba_objects; -- **************************************************** -- Populate the table into a array using bulk collect -- **************************************************** DECLARE TYPE t_tab IS TABLE OF test_objects%ROWTYPE; objects_tab t_tab := t_tab(); start_time number; end_time number; BEGIN -- Populate a collection SELECT * BULK COLLECT INTO objects_tab FROM test_objects; -- **************************************************** -- Time the population of the table with a bulk insert -- **************************************************** EXECUTE IMMEDIATE 'TRUNCATE TABLE test_objects'; Start_time := DBMS_UTILITY.get_time; FORALL i in objects_tab.first .. objects_tab.last INSERT INTO test_objects VALUES objects_tab(i); end_time := DBMS_UTILITY.get_time; DBMS_OUTPUT.PUT_LINE('Bulk Insert: '||to_char(end_time-start_time)); -- **************************************************** -- Populate the table without a bulk insert -- **************************************************** EXECUTE IMMEDIATE 'TRUNCATE TABLE test_objects'; Start_time := DBMS_UTILITY.get_time; FOR i in objects_tab.first .. objects_tab.last LOOP INSERT INTO test_objects (object_name, object_type) VALUES (objects_tab(i).object_name, objects_tab(i).object_type); END LOOP; end_time := DBMS_UTILITY.get_time; DBMS_OUTPUT.PUT_LINE('Conventional Insert: '||to_char(end_time-start_time)); COMMIT; END; / 
Is that select on a view? If so, can you extract the select in the view? That could give you an idea why it's not working.
Are dates stored a similar format? I've seen things where the dates are stored as text... So select from date where &gt; getdate()¬10 would error out... I essentially cast the varchar to date.. Then did a check to make sure whatever came back was a date.. Filtering those that had a length not match date.. Or where 1900 would show up because the column had null dates... Yes, the situation im recalling was dirty data. That's what happens when you allow people to enter text into a field and don't require it to be a certain format. The front end application is over 30+ years old... So yeah. 
you could always sign up for CS50 on edx https://www.edx.org/course/cs50s-introduction-computer-science-harvardx-cs50x CS50 is Harvard University's Introduction to Computer Science they've put all the lectures and assignments on edx and you can take the course for free the first 6 weeks is C programming, then python, flask, html, sql and SQLite there are assignments throughout the course and I found it very helpful and specific tasks and goals building this website s the final homework assignment http://mashup.cs50.net/ click on a map marker to see more this is also a good series https://www.youtube.com/watch?v=F5AlxQvya5A&amp;list=PL3B743E17BCF7B958 he has playlists on MySQL , MS SQL Server and other good stuff fhttps://www.youtube.com/user/CalebTheVideoMaker2/playlists
I think you came to the wrong sub... try r/MSAccess/ or r/excel/
Not sure how to do it in Access, but this would work in Excel... =IF(COUNTIF(H$2:H2, H2) =1, 1, 0) Put that in J2 and copy it to the other Column J rows. It counts the number of times each row's H column value appears in the rows above it and outputs a 1 when the current row contains the first instance.
I know how to do it in excel but when you have thousands of records formulas slow everything down, so I figured I could write a query and export in excel as values. 
This is an excel answer. In cell J2 =IF(H2=H1,"",1)
There is actually a timestamp as well, excel just displayed it as shot date by default and I forgot to change it. However instead of using timestamp can't we just use PKID, in a recordset of 3, the one with the lowest PKID will be the first record therefore that one has 1 and the other two are NULL. 
Then use ROWNUMBER and OVER(PARTITION BY
Table Thread: ThreadID|TopicStarter|DateTime Table Post: PostId|AuthorID|DateTime|ParentID|Content And honestly, that's about it. Your comment would be the same as a post, but with a different parent. If you want a different comment table: CommentID|AuthorID|DateTime|Content
They're backwards compatible but not forwards, so the stuff built for 2008 will work on 2012 but not vice versa. It's worth mentioning that opening a package in a newer version of Visual Studio will upgrade the packages rendering them unusable on older installs.
I'm not too knowledgeable on Access. However, I know Access can be pretty flexible, and I'm not sure exactly how you have things up, but you can probably get it how you want it in a number of different ways. You can probably add an event to wherever you are typing in the account, so that it runs the query, looking up the account, and filling in the first and last name into a "name" label that you put on the form. Each time you type in an account number, this event auto fires, which will auto lookup this data, and you just need to set it to change the labels value to the name. Again, depending on how you're actually doing things, this may or may not work as a solution.
Brilliant. To confirm my understanding - I could go to the integration services in a later version of SSMS and use that to setup a SSIS package created in VS 2008 on a 2008 R2 SQL Server?
I've only used SSMS to deploy SSIS packages in jobs through file system, I can confirm that you can use a newer version of SSMS to deploy against older versions of SQL Server, though i've never tried it with packages stored within the server.
Yes you can. You may need to update some, and upgrade all of them. I just posted a link where we moved a bunch like that, first 2008 to 2008 R2 and then 2008R2 to 2012.
Unless something is very changed with Oracle, having 'AND doctype.id = rep.dataid, 'AND orgFunc.entrynum = 1', etc. in your where clause changes the semantics of your joins with MV_TABLE to inner (by requiring that the fields are not null in the result of the join). So my first guess would be that you'd want to move these conditions into the join conditions. Secondly, this appears to be a simple pivot of your attribute/value table and pivots can be achieved in the number of ways - you can try group by/case, for example. And thirdly - you want to look at the execution plan to see what potential/perceived issues there are and react accordingly. As you change the query, you can compare the resulting execution plans to validate that your assumptions/insights/solutions are moving you in the right direction.
I feel like this is a result of a SQL ninja trying to avoid using stored procedures. You might very well benefit from simplifying it into multiple statements inside a stored procedure. Using multiple sub-queries in your update, even when they're using indices, is going to slow you down. For the update itself, you might loop over the select with a cursor and perform smaller updates without the sub-queries.
The lesson here is don’t use oracle
&gt; We are moving to Windows 10 which AFAIK doesn't support SSMS 2008. You mean BIDS? You can use SSMS 17, as far as I know. Neither SSIS nor BIDS care what version of SSMS you're using. As far as I'm aware, my boss is running BIDS for SQL Server 2008 R2 and he's on Windows 10.
or, after implementing your rowcount, just case rowcount_col when 1 then 1 else null end as ColJHeaderName. obviously we're dealing with Access so things that make sense anywhere else may or may not apply and I'm fairly certain cross apply doesn't exist in Access... I'm happy to be wrong though.
Actually for some reason my 2016 SSIS can only be accessed by a SSMS 16 and below.
Oh, *that*. Yes, with that you generally need to have the exact same version of SSMS as the version of the instance you're connecting to. However, I don't often need to connect directly to Integration Services with SSMS, so I rarely think about it. Our SSIS packages all run via SQL Agent and the jobs are all set up with proxy accounts that have the necessary permissions. The only reason to connect is to remove a package, really. 
T-SQL has the ROW_NUMBER function, but Access does not. Check this link for potential work arounds: https://stackoverflow.com/questions/17279320/row-numbers-in-query-result-using-microsoft-access If you can get row mumbers segmented by the IDs you want, then you should be able to just remove where &lt;&gt;0. 
The built-in way is to use `ROLLUP()` in the `GROUP BY`: SELECT namegroup, SUM(tranamt) FROM rmledg WHERE ( descrptn &lt;&gt; 'CreditApply' OR descrptn IS NULL ) AND rmpropid = '067' AND rmledg.posted = 'Y' AND namegroup IN ( SELECT namegroup FROM name WHERE rmpropid = '067' AND type = 'R' AND STATUS = 'C' ) GROUP BY ROLLUP(namegroup) HAVING SUM(tranamt) &gt; 0 When `namegroup` is null, that's the overall total. You could use `COALESCE(namegroup, 'Total')` if `namegroup` is a character field. If the total has to be the last record, you can add `ORDER BY CASE WHEN namegroup IS NULL THEN 2 ELSE 1 END`. 
I don't know why you need the group by and having statement, removing both will only change things if tranamt can be negative. That said, 2 quick and easy ways to get what you want, drop the GROUP BY, make it a PARTITION BY in the SUM (HAVING SUM(tranamt) OVER (Partition by namegroup)). The other, is the do nested queries, basically do a sum on your results: select sum(t.tranamt) from ( SELECT SUM(tranamt) as tranamt FROM rmledg WHERE (descrptn&lt;&gt;'CreditApply' OR descrptn IS NULL) AND rmpropid='067' AND rmledg.posted='Y' AND namegroup in (Select namegroup from name where rmpropid='067' and type='R' AND status='C') GROUP BY namegroup HAVING SUM(tranamt)&gt;0 ) as t
You can use a select statement to do it. No need for a loop. 
Ugh. I feel stupid now. Thanks a lot for your help!
In this case, "indexing the hell out of the table" looks like a unique index on mytable's defid/attrid/id/vernum/defvern. If so, seems like it would go reasonable no matter how many rows mytable has. (Having typed that, the order of the columns in the key would depend on the cardinailty, and if one of those is secretly a _long_ or something that would be scary, and how those "blob" tables are structured at the end...)
INSERT INTO TableB SELECT DATEPART(DAY,a.DateTime) ,DATEPART(MONTH,a.DateTime) ,DATEPART(YEAR,a.DateTime) FROM TableA a
Thanks man! This means a lot. :)
Thanks for the additional input! Appreciate it! :)
 CASE WHEN Region = 'Canada' THEN FieldA/FieldB ELSE NULL END
INSERT INTO table (column_x) VALUES (SELECT COUNT(col_y) FROM table2 WHERE .... ); So you create an insert statement with as value a sub SQL statement.
I keep getting missing parenthesis error at value(. create table math( ak_oh int not null, indiana int not null, primary key(ak_oh) ); INSERT INTO math(ak_oh) VALUES(SELECT COUNT(*) FROM numbers WHERE name='b'|| name='c'); What did I do wrong? 
Try this INSERT INTO math(ak_oh) VALUES((SELECT COUNT(*) FROM Businesses WHERE state='OH'OR state='AK'));
you shouldn't need to use VALUES(). create table math( ak_oh int not null , indiana int not null , primary key(ak_oh) ); INSERT INTO math(ak_oh) SELECT COUNT(*) FROM Businesses WHERE state='OH' OR state='AK';
&gt;The operation "Index Seek (NonClustered)" on the price table accounts for almost 25% of the total execution load. Something that it took me a while to finally realize: **something** has to be the most expensive operation in your query plan. An index seek being your biggest operation isn't so bad IMHO. Is performance acceptable right now? I'm guessing not, since you're posting here. Can you post the full execution plan (use http://pastetheplan.com) and a condensed version of the output of `statistics io`? Are you spilling to TempDB a lot (we'll see this in the execution plan)? Index space being 5x your data space is a bit of a worry but it may also be OK, depending. Have you considered Column Store indexes?
Splitting the data into multiple, normalized tables, in my opinion, is your best bet. The dates, in particular, need to be separated and tied to the corresponding price. Joining tables on indexed keys should take less overhead than searching through 600 million rows and multiple indexes
I used to work as a Business Analyst mainly running reports for clients and making sure the data we're importing is not terribly unreadable by our system, and that our system does what it's supposed to. I now work as a systems analyst configuring said system. SQL is the main part of my day to day job and right now integrating some python in as well.
To get some specific instruction it would be a good idea to give what type of database you're using with what language/framework you're trying to connect to it with.
Yes sorry, its all somewhat new to me. We are creating BIDS packages for an SSIS job - is that correct? Thank you for the information though, that helps. I'm currently installing everything required on a Win 10 laptop to prove the concept.
Thank you!
The packages aren't backwards compatible so I wouldn't be upgrade the packages if that's what is meant? I'm currently installing a newer version of SSMS and Visual Studio 2008 to see if they can still be maintained. It isn't desirable but what can be done until the SQL server is upgraded?
Start with eliminating the "key lookups" by including the columns referenced in that node in the preceding index.
Yes, you need to provide, at a minimum, the type of database you are referring to.
 DECLARE @desc VARCHAR(MAX) = ''; -- Set required desc here WITH p AS ( SELECT t.Id, t.Desc FROM dbo.Table AS t GROUP BY t.ID HAVING CHARINDEX(@desc, t.Desc) &gt; 0 ) SELECT t.* FROM dbo.Table AS t JOIN p ON t.Id = p.Id
Thanks for the response! My goal is to get this into VBA so I'll try to format it correctly.
Nearly all web languages have support for multiple database connection. Your friend should be able to look up references on how this is handled for your case. 
Oh, DECLARE isn't supported by Access :(
This is what I was doing already in VBA, but I need to extract all of the records for each ID that meets this criteria. So how would you tie that into xeroskiller's code above? 
if you don't want duplicates, add keyword distinct (select distinct id from...)
I figured it out with this code. Thanks for your help! SELECT * FROM (SELECT Distinct [AHC] FROM tblClin WHERE [CLIN DESC] like '*ETH*') a Left Outer Join (SELECT * FROM tblClin) b ON a.[AHC] = b.[AHC]
I'm somewhat of a beginner, so I'm not familiar with that. I'm willing to learn if you're willing to teach :)
Just use the correct connection string in the DB connectivity API.
Dumb question but have you tried selecting what you need from your big table into a #table and then joining to the rest of your process? Does that improve anything?
As a very quick answer, you need to get an understanding of how web tech works. General structure looks something like this: Database - data service - web site The database that you created already will contain the data for the application. For authentication, I would suggest using a 3rd party provider. The data service, aka data access layer or server code, is what the website will use to request data in the form of http requests, get put post delete. The website will then contain all the user interface code along with code to request data from the data service. Now, these 3 distinct pieces may end up being very intertwined depending on the technology you use. For example, a project I've worked on used MySQL for the database, Java spring for the data access layer, and angular for the website. There are a bunch more libraries and technologies involved, but those are the main ones we used for the specific site. If you are new to web dev, I'd suggest looking into the MEAN (or MERN) stack. This stands for mongo, express, angular, node as the tech stack for a website. Otherwise, good luck! There are tons of resources out there to help!
PHP
Because I don't only want distinct. I want ALL the records that match the AHCs that have that text string.
so remove the distinct??
If I do that then I'd only get the records where the description contains the text. I want all the records for each one of those AHCs.
&gt; int: ID of the currency of the price (4 currencies covered) The entries in this table seem to be actual bookings, which means they probably booked it at a given price. Since exchange rates change daily, the price tomorrow could (likely will) be different than today. Due to this, they'd have to store the exchange rate at the time of booking, at which point they may as well just include the currency instead.
This should work for Access 2013 SELECT * FROM [tbl] WHERE EXISTS ( SELECT DISTINCT [AHC] FROM [tblClin] WHERE [tbl].[ahc] = [tblClin].[ahc] [tblClin].[CLIN DESC] LIKE '%ETH%' );
what is [tbl]? I only have 1 table I'm using.
sorry I forgot to alias, i've fixed it now
So this is more efficient than my self-join above? I mean, it works fine and I managed to make it work in VBA so unless there's a serious performance improvement I'll just stick with it.
My bad. Just move that string into the CHARINDEX function, and you should be good.
What the fuck, who downvoted this? I use PHP to make webz that talks with SQL and with PDO you can talk with most any kind of database. And that link may not be the _only_ proper tutorial but it is definitely friggin great if you want to learn how to do it without opening a bunch of holes for exploitation. Sure, there are a ton of alternatives, both to PHP and to PDO but the question was "How do you connect your data base to a website?" and this is how I do it. I also use Ruby but PHP is a lot easier to pick up when you are "starting to design web pages".
Kind of hard to interpret without relative weight between the statements...
 with ro as ( select table.* ,rownumber(over(partition by personID order by date asc)) as rownum from table ) select * from ro where rownum = 1
Yeah sure work although if your using variables it would be worth adding @startdate &gt; @enddate and the reverse into the brackets so it can ignore the table check if the variable check is false. 
That works beautifully. Thank you so much.
Actually it isn't a table with bookings, it's a table with prices to rent a certain house at a given date, in a given currency. The reason we need to store all currencies is because of "rounding rules". Each of our leasing partners can set whether they want their prices rounded to nearest 1, 5 or 10, and whether to round up or down. And they can set this for each currency, if they want Euros to round to 1's, and Swedish Kroner to round to 10's, for example.
Haha, he was actually super skilled, a real database guru. Worked in his off time and on holidays because he loved database work. Whether this design is the perfect fit is of course up to debate, but the fundamentals were written like 6-7 years ago, and you know, you can't refactor everything.... have to keep patching with chewing gum as the business demands change.
What error are you getting?
Looks like you're using smart quotes (opening/closing). Make sure to use single quotes. ' instead of ‘ or ’ 
Thank you! I would have never caught that but you can bet I'll never forget it.
Good to see that you had your issue corrected. As a side note, the following code will suffice for insertions: INSERT INTO customers VALUES ('Munch', 'Rodney', '123 Easy St', NULL , 'Highland', 'TX', '78723', '8003287448', NULL, NULL); SQL will know to assign values in order.
Django, Ruby on Rails, node.js, php, etc.
More specifically, you need to store the pbkdf hash of the password, and set it to use 100,000 iterations or so. Also, get a gigantic password list and make sure they don't use any common passwords.
You wouldn't need to release the data, just the formatting of your questions. Heck, my prod database is made up of tables with columns named 'Db_Name', 'DB_Enc_Pass', 'DB_Port', 'DB_User' and all sorts of other phenomenally stupidly obviously non-sharable data, but I could easily ask the question about how I link all groups, to user group name and command_id without having to worry about data security. 
Apologies. You are worried that your data is proprietary. That is normal, generally folk would never share their **real** own data with anyone. &gt;my prod database When I said this, I meant that this is my 'Production' data - used in real world systems, happily churning away servicing thousands of users per day. I would use the word *Prod* to differentiate between test data. You don't need to share the *data* with people. When phrasing a question, I would use test examples. &gt;I want to search for all users who are called John, but their transactions fall between August 4th and Sept 4th, and were purchased in Asia. &gt;My tables are 'Customer', 'Location' and 'Transactions' In this case you need not give away any data - but you can ask the question - even telling folk the layout of the tables. In this example, there are presumed data columns but you would be able to expand by describing the column contents of your tables. &gt;Customer table holds 'Forenames', 'Surname', 'Customer_Id', 'Location' Still at no point have you needed to share your data. 
Which one usually wins?
Do you know how to design a database? Because SQL is just syntax, which is honestly trivial in the grand schema of things.
well thanks. users that know what a group_concat is, is most definitely what it was targeted for genius! 
How not to teach SQL
I think this is a homework question. The answer is the GET method.
Thank you very much for the detailed reply and code! I like your idea; few tables and columnstore indexes. The tricky part is the job/scripts that harvest data from the master database and put them in the search database, but that can be managed, and is not so performance/real-time sensitive. I will start working from your suggestion around May where we have time for the project. Thanks again.
This question sounds like its' from a text generation bot... More details!
How do you suggest it be done better? Changing the title to where to start? I'm just curious, because so far the two comments I've had have been negative, and maybe it's because this is a more advanced group with SQL? Because I'm not getting the same feedback on Linkedin and other avenues I posted this in, so I'm just curious. Because what I showed in the video is how I learned how to go from ZERO to comfortably doing joins and using SQL in less than 5 hours/5 days time. Hence I'm a little confused by the feedback in this reddit, but maybe it wasn't the appropriate place or title is why I ask @PhenomeVon
i probably should be able to but i'm afraid i'd have no clue how to, or even what 'structure' really means here :(
Sorry I wasn't very clear, can you share what the table looks like which stores the hierarchy data? The column names and some rows of data (obfuscated if necessary) Or even the full SQL statement, i.e.: SELECT r.SeniorManager FROM myTable r WHERE r.SeniorManager IN ( 'value1', 'value2', 'value3' );
i'll give that try: thanks ! :)
Can you not just read the CSV's in dated order and insert them into the database? Just make sure to have an autoincrement column and that'll let you keep order on your inserts.
If you’re logging, you should have a date time field. Once you move over to a database, if you have an index on the date time field, you can retrieve any information in order regardless of what order it was inserted in. There’s a difference between storing the data in order (csv capability) and reading the data in order (database capabilities). 
It sounds like you're already populating a search database which probably results in a massive amount of writes if you've got a 600M row table. My proposed normalized schema will drastically reduce writes, load times, space, and contention. Your final total space used would be in the order of 100 MB, not counting any LOBs.
Keep in mind that in SQL, you do not receive data in any particular order unless you ask for it. Without an ORDER BY clause in your query, you cannot depend on the result set to be "in order". The best solution is to store some kind of timestamp in your table and order by the timestamp when you query.
A relational table, by definition, is an *unordered* set of records. This extends into results sets, too, so unless you specify an ORDER BY with your SELECT query, then the order of results is however the database decides to return them. It may or may not be the order of insertion. If you need to maintain an order of your records, you will have to do it by adding a field that you can use to sequence the table. That may be a date time records or by using something like an [AUTOINCREMENT column](https://sqlite.org/autoinc.html). However, you never want to make a date time field a unique column. Trust me, that's a tremendous nightmare because things do happen simultaneously or below the clock's precision. Keep in mind that in most RDBMSs, AUTOINCREMENT columns are not guaranteed to never reuse a number, so if you delete records from the end of the table you may reuse them. Additionally, AUTOINCREMENT doesn't guarantee that there will never be gaps, as certain operations can consume a number but never actually materialize as rows in the table (e.g., failed inserts). In other words, AUTOINCREMENT isn't going to give you a perfect sequence of items with no gaps. Personally, for SQLite I would use an AUTOINCREMENT as the primary key for a log table, and then also include a date time field that has the timestamp of the event that is as precise as you can make it. Then, if you need to recover when things happen, you can ORDER BY &lt;timestamp field&gt;, &lt;autoincrement field&gt; and that will be correct in essentially all cases. 
Was there something I missed that clued you into his SQL being Oracle?
My guess is that the front ends use JAva with Sql built in.. I'm assuming they want you to know JAva for development purpose and or ability to read the t-sql that's just in the code.. that's just my two cents, but good luck either way!!!!
SQL doesn't require knowledge in Java but you can programmatically modify databases though Java. That may be what they are discussing.
Couple of points: a) As previously mentioned, this whole query lends itself very well to a PIVOT approach b) Try to MERGE instead of updating especially when you're handling large datasets like this. Re-run multiple tests using both methods and pick the best one at the end. c) I'd definitely try to add some query/dml parallelism to this operation but there's a few things to be mindful of so always check with the DBA that it's OK first. d) All of your OUTER joins are really INNER joins since the join conditions appear in the WHERE clause. e) Here's a rough idea of what the modified query might look like if you use a MERGE statement and a simple CASE WHEN pivot. Might require a bit of tinkering to get it to work right: merge into report rep using ( select mv.id , max(case when mv.attrid = 4 then mv.valstr else null end) as department , max(case when mv.attrid = 6 then mv.valstr else null end) as documenttype , max(case when mv.attrid = 2 and mv.entrynum = 1 then mv.valstr else null end) as orgfunction , max(case when mv.attrid = 20 then mv.valstr else null end) as risklevel , max(case when mv.attrid = 24 then mv.valstr else null end) as assetcriticality , max(case when mv.attrid = 23 then mv.valstr else null end) as cft from mv_table mv inner join MV_LLATTRBLOBDATA_VERNNUM_V1 mlv on mv.id = mlv.id and mv.vernum = mlv.vernum inner join MV_LLATTRDATA_MAX_VERSIONS_V1 mlmv on mlmv.id = mv.id and mv.defvern = mlmv.max_defvern where mv.defid = 3070055 and mlmv.attrid = 4 group by mv.id ) v on (v.id = rep.dataid) when matched then update set rep.department = v.department , rep.documenttype = v.documenttype , rep.orgfunction = v.orgfunction , rep.risklevel = v.risklevel , rep.assetcriticality = v.assetcriticality , rep.cft = v.cft; 
You can embed Java [inside Oracle databases](https://docs.oracle.com/cd/B28359_01/java.111/b31225/chone.htm#BABFICAE)
&gt; It's like a C# Dictionary, Hashtable, or HashSet, if you're familiar with those, .NET has [ordered collections](https://docs.microsoft.com/en-us/dotnet/standard/collections/sorted-collection-types) now, including [`OrderedDictionary`](https://msdn.microsoft.com/en-us/library/system.collections.specialized.ordereddictionary\(v=vs.110\).aspx)
ZOOP ☜(ﾟヮﾟ☜) I scroll past the “Java” tab in Toad daily. You can indeed embed it. OP does the company use oracle?
you mean select [...] from ITEMS a left join ORDER b on a.item=b.item left join INVOICE c on a.item=item...? but then i'll get the problem that if an item has both an order and an invoice then it will be only one row with both the orderno and invoiceno columns filled with values, right? i want them on separate rows like shown above. or did i misinterpret you
Try a union. Select from items left join table 1, union, left join table 2. The columns selected have to match with a union so you can select ' ' as invoiceno.
What are you trying to achieve with the table? If you are doing transactional tables and do a sum on inventory, you just tripled 56AA inventory. There is an easy way to get all the stuff in the table, but why do it like this?
It's just dummy values and columns, wanted to make it easy to understand 
You can left join the first two tables and then union a left joined table of #1 and #3. 
I work with source data larger than 600m rows all the time. Your queries are slow because you're interrogating the entire table in order to extract some kind of nuanced subset. Begin your queries by pushing that source data into a CTE, @table variable or #temp table. If you go with the latter, be sure to take advantage of indexing. Anyway, once you've got your subset, then do all your expensive LEFT JOINs or SUMs or whatever.
You should be aware that memory is handled differently in newer versions so some packages might run significantly slower after the upgrade. But yes they will work
In your amphibious table if you have a key that points to the row in the animals table that you are referring to then you shouldn't need color or legs in the amphibious table. You would just use a JOIN to get the data. But the term inherits is more a OO programming term and not a Relational Database term - FYI.
Right, but I'm providing an example of an unordered collection because a relational table is also an unordered collection. The unordered version of an OrderedDictionary is a [Dictionary](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.dictionary-2?view=netframework-4.7.1). Most people tend to think that collections are all ordered, because the simplest one (arrays) preserve order, and the next simplest ones (Lists, Queues, Stacks) also preserve order. Because tables are defined as unordered, I wanted to provide an analog to something OP might be more familiar with. 
&gt; amphibious table &gt;But the term inherits is more a OO programming term and not a Relational Database term - FYI. What RDBMS is this? 
 Well Mysql isn't ACID complaint even though it claims it is. What your asking for is the difference in isolation levels though. https://dev.mysql.com/doc/refman/5.6/en/innodb-transaction-isolation-levels.html 
&gt; http://pastetheplan.com TIL this exists. Sweet.
That is already fundamentally the way the search database works. You query the database by executing a (massive) SP, which performs around 6 or 7 major selects into progressive temp tables (each select from the previous), and finally outputs the matching houses, their price and such. Each select into a temp table does perform some fairly heavy JOINs and such, so there is definitely room for improvement. It's just a huge mouthful to refactor, and not helped by the fact that business logic and special rules are intertwined with the raw data fetching. :(
Thanks for the breakdown, that was very helpful!
You can but shouldn't. 
Hey MaveDustaine, the reason why I asked is that I work for Codecademy and we are writing a new SQL course that have three variants: Business, Marketing and Analysts and I was wondering if I can ask you a few questions - via DM or email. Thanks! Let me know!
You are good. Delete, deletes the rows in table you specified. not the relational data.
You are looking at the logical design, but the physical implementation might make more sense. In relational modelling this would be a one-to-one relationship. There may also be other subtypes. Having no other primary key evident, we create a surrogate "ID". Animal( ID, Legs, Color ) Amphibious ( ID, HasStickySkin ) So for your example we would adding an arbitrary primary key to ID, producing something like: Animal( 1234567, 4, red) Amphibious ( 1234557, yes ) This preserves 3rd normal form in the implementation.
Change payment.sum(amount) to sum(payment.amount)
Thanks! I corrected that and now I’m getting an error saying “customer.first_name” must appear in a group by clause or be used in an aggregate function. 
Add the first and last name columns to your group by clause.
Also write your select items as a list instead of a long string. Makes reading it much easier and to copy/paste into the group by. Alias your tables too if you have covered that one n class yet. Customer as C 
Didn’t realize I could group by more than one thing. Thanks for your help. 
Makes total sense. I’m not in any classes, just teaching myself as much as I can and trying the Udemy route. You guys have been extremely helpful!
task (taskID, createdByUserId, adminUserId, taskName, taskDescription... ) user(userID, userName,...) taskUser (taskID, userID, canEdit) 
It's not an alias. It's column name. You can name it whatever you want as long as you understand what they represent. I like using table name before common column names as it's easier to understand and maintain. In the above design you can have multiple users assigned to the same task with different permissions. 
can you just translate this to SSRS where your tables are joined, and your eu just puts in their other parameters?
Evidently, if your primary key is a composite one, you have to reference the entire composite key in your foreign key constraint. ;)
We use GIT.
My first thought was exactly what you have already thought of... create a view for every table and hard code the filter. Enforce usage by only allowing users access to the views. Your other option would be to simply train all individuals who write sql to use the filter on all of their queries... good luck. I guess a third option would be to offload those deletes to separate "deleted" tables with the same table schema. And then you could implement a unioned view of the tables combined if you ever need to stick them back together. You may also want to explore row-level security... though I have no experience here but it may be worth some investigation. Do your own due diligence here... I don't know if this would even work or if it would be a completely inappropriate application of the feature.
Visual Studio Team Services. Small team, so it's free and it's nice to be able to associate a check in to a project.
Just to be clear with your ask: You have a report server where you host all of your RDL files and you will keep your most current versions here, "in production," but what you want to do is track changes over time. If I am hearing you correctly the desire here is to track the changes in SSRS, not necessarily in SQL? Could be both but, for example, if someone adds a new column to SSRS without touching the SQL, you would want to have a new RDL file saved somewhere in order to archive it? Check this out: http://www.sqlservercentral.com/articles/Reporting+Services+(SSRS)/94119/
task (taskID, taskInfo, ...) user (userID, userName, ...) taskResponsibilies (taskID, userID, respID, dateStarted, dateEnded, ...) responsibiliesList (respID, respDesc, sortOrder, use (y/n), ...) Or, you could do it like this where you have a many-to-many relationship where you can keep track of multiple person's responsibilities and maybe even when they had those responsibilities. This also has the benefit of allowing more than a single person having a set responsibility simultaneously. 
In my shop, we do soft deletes as well and we do exactly what you describe - a view for every table, with `where deleteddate is null` so that only records that haven't been soft-deleted are returned. Adding onto this, I create most of my indexes on the tables as filtered indexes with that same `where` clause. Since the applications don't care about records that have been deleted 95% of the time, it helps. Performance warning: don't nest your views. Beyond a certain depth (which is fuzzy), the optimizer gets bad row estimates and you get bad query plans.
I think the stupid thing here is that both auto_id and product_name are unique, so it barely make any logical sense to have these two columns. Perhaps it would be smarter to just make an index computer from the unique product_name, and insert the full product_name in the foreign key column in the ORDERS table.
I know this is going to sound pedantic. I think that user is suggesting create the 3 table with those column names. No aliases at this point
I was thinking of doing something like this: INSERT into ORDERS (product_name) VALUES (select auto_id from PRODUCT where "product_name = testproduct"); And it seems to work, but is that efficient? I'll have lots of inserts per seconds... 
I'm not familiar with Firebird SQL, but most other dbms can return the ID they automatically assign on insert. [PostgreSQL example](https://www.postgresql.org/docs/9.1/static/sql-insert.html) [Firebird seems to support it to some extent](https://firebirdsql.org/refdocs/langrefupd21-insert.html#langrefupd21-insert-returning).
&gt; am I doing it completely wrong? sort of the FK in ORDERS should reference auto_id in PRODUCT since product_name is unique, just do a SELECT on it to fetch the auto_id value for the product that you want inserted into an order
Thanks, yes that's what I ended up doing. I'm just worried about performance. 
Thanks. At first I didn't think that answered my questions (because I didn't formulate my questions clearly, sorry about that, so confused), however, the "returning" keyword ended up helping me in this case. Had to pass the result into a variable to translate from one datatype to another.
you've already decided to put an index on the varchar (by making it unique) so really, retrieving one row by its varchar value is not at all inefficient
Team Foundation works for this but you would only be able to revert to a prior revision of the RDL file. It will not necessarily show differences in the RDL file but you can revert to a previous version of the file.
SVN - mainly for recovery purposes, not for versioning. This or GIT can handle your req's above.
Yes, its part of the Ui. Thanks for your help, the poster below seems to have solved it, there is a second installation of tools that I havn't been doing. 
Done!!! Thats what I was looking for :-) I didn't know about the second installer.
Yes my query has only deal_id in group by... And no... If you add status_id to the select/group by clause, the max value will be calculated at two levels... deal_id and status_id.... I need the query to only calculate this at deal_id level..
I got this to run and it seems a lot faster which is great but the number of rows merged vs rows updated when I run the original update is slightly off 
Thanks, I appreciate the help and advice. I think I will try to go this route for future procedures.
I am not familiar with row level security, but I am glad you’ve mentioned it, and I will look into it. Ha, people are surprisingly hard to train when it comes to this stuff. I like making things as stupid simple and repetitive for other developers as possible, and it seems like making a view for every table, and making fun of the people who don’t use the views, might be the way to go.
Yeah, I meant on the “human” side of the equation. It’s error prone in the sense that it’s still easy for people to forget to use the views. I was hoping there would be a way to do it where all future queries would magically have filters applied...but I can also imagine why a feature like that would not exist.
Possibly like this? SELECT a.deal_id, b.status_id, a.ts FROM ( SELECT deal_id, MAX (ts) ts FROM blah GROUP BY deal_id) a, (SELECT status_id, deal_id, ts FROM blah) b WHERE a.ts = b.ts; (oracle shop here)
Do I get anything for taking the time to email you? 
This is more for my own curiosity more than anything: Are you looking for actual "data analysts" or just people skilled in SQL in general?
Ah yes. Forgot about that. Maybe I can put on my resume "Featured on CodeAcademy Website One Time". 
I can answer that question...for money.
&gt; it’s still easy for people to forget to use the views i don't see how, if that's all they have access to 
∅
Do you mean group by ID *and* by week? I'm not sure I understand the what the query is, but grouping is pretty trivial. SELECT id, week, SUM(revenue) FROM table GROUP BY id, week
I would think you'd join the user table multiple times (I'm sorry- that's probably what you meant by alias). Once for createUserId, and once for adminUserId, etc. 
That’s actually a good point. I’m no DBA, and I’ve never considered using permissions to block access to the tables themselves....or I’m assuming that’s what you’re getting at? Either way, I didn’t consider looking into that option before, so thank you.
Visual studio online. It’s free for 5 users 
Okay. Thanks for the feedback, that is what I have been doing, just wanted to probe to make sure I wasn't missing some magic that someone else was doing.
After you run the original update statement, are there any rows in the report table where department is NULL?
Try posting the code formatted. I unfortunately can't make anything out on my phone so maybe it's just me. I did not see if you tried it and failed and if not, why don't you give it a go ?
I was unhappy with CodeAcademy data analysis program. I thought I was signing up for a specific SQL course, instead it had a lot more random topics. 
You can also use [dbbrowser](http://sqlitebrowser.org) for a GUI, if i remember correctly you can review the code used for tables and such. Very easy to use tool.
Hmm.. one other thought, especially since you have a potential of an undefined number of users on the task and to piggyback on the canEdit from u/unpronouncedable... TaskUsers (TaskUsersUID,TaskId,UserId,CanEdit, IsCreatedBy, IsAdmin). In that scenario, there is no userid at all on the task table, and it is a 1:M on TaskId.
&gt; What is a concept in SQL that's essential to marketing mixing english with SQL is not good for marketing
Data scientists/analysts/engineers all work!
sorry to hear that. i wrote the majority of the the sql portion on IDA. we are currently building a purely sql intensive with 3 lessons, 3 code challenges and 9 projects. some with real datasets from real companies. i can see if i can get u it for free. i’ll dm u within 2 weeks.
&gt; I guess a third option would be to offload those deletes to separate "deleted" tables with the same table schema. Temporal tables, without the soft delete (do a real delete) would be another way to do that. The deleted records will be preserved in the somewhat-hidden system table that stores the history and the data can be retrieved by querying the table using the `FOR` clause (Or querying the "hidden" history table directly).
SELECT ACCOUNT, DATEPART(WEEK,DATE COLUMN),SUM(AMOUNT) GROUP BY ACCOUNT,DATEPART(WEEK,DATE COLUMN)
So, there are a lot of great options (see the sidebar for some). I actually like WiseOwl on YouTube. His beginners series is awesome. For syntax, id go with MySQL or PostgreSQL. They are really the most popular.
Forget about the table you have for a second and write pseudo code on what you are expecting in hypothetical tables. The word SAME in the question points you to where the join will be. Then re-look at your one table available to see how you can reference it using the pseudo code logic. I'm curious what you are learning in this chapter-I have a feeling that will also give you a clue as to what you need to do. 
Which part are you asking about? The amphibious table part or the inheriting part?
&gt;We have no other tables that contain any of the relevant information for this question The table you're provided has all of the relevant information. Hint #1: You can join a table to itself. Hint #2: You don't always have to join a field to another field. You can also join a field on a string literal. But your instructor might not be going for that here.
&gt; I appreciate you trying to help me see the solution for myself somehow but I guarantee it won't work haha With an attitude like this, you won't get terribly far in school or in a job. &gt;Oh well, will have to score the points somewhere else. Giving up already?
I have the exam in thirty minutes, and I am not going to go into Database Management. I will find a way to pass the exam one way or another, and I'll never really look back, so this inquiry is mostly to satisfy my own curiosity. I wanted to understand what I was missing, but the hints aren't getting me there and I don't see the need to desperately plead or beg you to make something clear to me if you aren't willing to already. I just wanted to be thankful for you giving it a moment of your time, but I suppose that is not appreciated nor necessary considering you've stooped to condescension instead.
Whichever part applies to an RDBMS. 
I was saying the term "inherits" isn't really a RDB term, it's a term used in object oriented (OO) programming. I wasn't talking about a specific relational database.
Sorry I did post it formated, but for some reason on mobile it doesn't show up as formatted. I did try to use it by doing the command sqlite3 project.db &lt; Schema.sql but that gives me an error saying that &lt; is reserved for future uses (not sure what that means, I'm on windows if that helps at all) 
Something to keep in mind, I'd aim for the SQL platform most relevant to my industry. Do some homework and find out who is using what software and what the need / availability of work is for the type of platform in the area you are looking for a job. Go to meetups and network, find out more about what's going on. Pretty much every major SQL platform has a developer instance you can use to learn on. You can also learn the smaller ones or open source types. I'd just be consistent with what you learn in the beginning and definitely plan out where you want to land. Base your training on that to get you to that point.
Love that idea.
 select blah from tablea join tablea on id = id where something = somethingelse or select * from tablea a join ( select * from tablea where something = somethingelse) b on b.field = a.field 
Thanks for the suggestions. No SSIS capabilities but I'll take a look into that. I was attempting to try pyodbc but I kept get a connection error message. I'll give that another shot though along with the win32 client. 
codecademy.com and take Learn SQL - it's free!
Nope, but I was and I entirely agree. Good luck with your exam.
That would be so great! The content was good, I just thought the original program was strictly SQL. 
Oh jeez, what a cancerous shit community. Go fuck yourself.
Bo problem, dude. Hope you had a nice exam/test/whatever.
&gt; Are there any rows in the report table where department is NULL? Yea there's quite a few.
That's because you do not have a WHERE clause on the original UPDATE statement so it will run through the entire table and where it doesn't find a match it will nullify the fields in the update list. A merge will onlybdo an update when it finds a match and leave the rest of the rows untouched.
Do you have to convert the date strings? Can you convert or format the value / parameter you are comparing it to instead? It is unable to use the index and is not saragable. Analyze the numbers of rows the where clause is returning and each of the segments. You may be able to create a temp table of the results, then filter the results by the remaining criteria. What's the query plan? Use Redgate plan analyzer for a more user friendly approach.
This is the code as I inherited it. No, I don't need to convert anything. The parent #tables in this example can have any indexing I need.
I see so it's running as intended, that's awesome. I'm looking at it though and I admit I don't understand what's going on. Could you explain it a little? 
Yes, I noticed that as well, and quite frankly I can simply prevent null's from being in the #set in a previous step. I am still more looking at getting rid of the OR's than anything else.
The diagram raises some questions from me... * You have no real way currently of telling how long a customer was subscribed to your service. This is because you aren't tracking their subscriptions, just the orders. * Realistically, the managers reporting would just be queries against the database by some external tool/process. * To me, I read this as Drivers can be Consultants but not all Consultants are Drivers (hence the optionally they can show up with nothing case). This would mean you should perhaps break out the consulting services from the delivery service via some additional tables.
Very interesting insight and very much appreciated. So I think I'm going to add a subscription table then that connect to customer and has subscription begin date subscription end date. Is that appropriate? Drivers can be Consultants but not all Consultants are drivers. That is completely true. However I don't understand the part about breaking out the consulting services from delivery. Does that mean I should have a separate Consulting table and perhaps an attribute is driver which is Boolean with yes or no?
I suspect `convert(DATE, t.FIELD_DT) &lt;='01/01/1999'` should really be `convert(DATE, t.FIELD_DT) &lt; '01/01/1999'`. If so, the `CLIENT2` logic can be cleaned up a bit. AND NOT (t.FIELD4 = 'CLIENT2' AND o.FIELD1 = 'ZZZZ' AND t.FIELD_DT &lt; '01/01/1999' --better than converting first AND (t.FIELD3 = 'XY' OR (t.FIELD3 = 'YY' AND SUBSTRING(t.FIELD_ID,3,2) &gt;= '42') ) ) Indexes better at supporting "specific criteria" than "everything but some specific criteria", but depending on how rare the exclusions are it's possible NOT EXISTS might be a good substitute for the where clause. I kinda doubt it, but it might be something to try.
They are quite rare I suspect and I utilize not exists in some of the parent processes. I already have the bulk of the process optimized by about 25% of the original execution so I don't know how much more I can get it over the line by tackling this where. In the original process the where was much more costly because it wasn't using #tables to aggregate the data in steps, but now it might just be worth keeping as is. The way it is written still annoys me.
Others are probably more qualified than I am to make this statement. What flavor of SQL you want to learn is probably going to heavily depend on what type of data analyst you are trying to be. For example if it is marketing data, you probably want to learn something like MS SQL. On the other hand with larger financial data, transactional databases, etc., you might find much more marketability learning Oracle. Others will know better than me, but I believe an Oracle DBA is more in demand, and pays more than a MS SQL DBA. So food for thought. If you want to get into analytics then MS SQL seems quite standard, but you might find applications with Postgres, MySql, etc. I say MS SQL because for quite a lot of companies MS is the gold standard, and in many of their environments you will find large Oracle databases for many things... and then MS databases for marketing/analytics type of information.
Awesome, I wish there were more places like this one
Exactly what I’ve been looking for. Thanks OP. 
That's what I'm thinking but I'm not sure if MVs support that. Also given the pattern of the predicates, I'm not really sure how I'd structure it into a look up table, other than storing it as string then converting it somehow? 
spitballin' here: /* WHERE -- prod (D.DEFID = 3070056 AND D.ATTRID IN (2, 3, 4)) OR (D.DEFID = 3070055 AND D.ATTRID IN (2, 3, 4, 6, 20, 23, 24, 30, 31)) OR (D.DEFID = 3071379 AND D.ATTRID IN (3, 5, 8)) OR (D.defid = 3072256 AND D.attrid = 5) */ create table config (configname varchar2(32), configval varchar2(32)); insert into config values ('PRODUCTION DEF 1', '3070056'); insert into config values ('PRODUCTION DEF 2', '3070055'); insert into config values ('PRODUCTION ATTR 1', '2'); insert into config values ('PRODUCTION ATTR 2', '3'); commit; SELECT blah FROM blah WHERE -- prod ( D.DEFID = (SELECT TO_NUMBER (configval) FROM config WHERE configname = 'PRODUCTION DEF 1') AND D.ATTRID IN ( (SELECT TO_NUMBER (configval) FROM config WHERE configname = 'PRODUCTION ATTR 1'), (SELECT TO_NUMBER (configval) FROM config WHERE configname = 'PRODUCTION ATTR 2'))) );
The simplest and most used way is to simply use '1990-05-30' (with quotes).
Thank you very much! I did not know that quotes were needed. Thank you for your helpfulness and your quick response :)
Are you asking how to do this or rather someone to do it for you?
np: I was just checking this "browse by new" thing, and stumbled upon your question by chance ;) ps: you should try [stackoverflow](https://stackoverflow.com) for that kind of questions, you will have a better change to get a fast and decent answer.
It's not really an SQL question, you could do this with some vlookups and a pivot or two in Excel.
I've started applying for them, but I just have no confidence when it comes to the job market.
Networking such as meetup for something in your area. Got to learn, ask questions - find out how people got started. 
Hey.....I have an SCM degree and now work as a database developer. I would suggest you create a project of some sort that you can showcase. Keep it simple but presentable like a report. Try to challenge yourself to learn something new about data analysis and/or database querying specifically. If it was me I'd probably try and setup a free Azure account and put some data out there that I could connect to with PowerBI. You will gain confidence in yourself and be a step ahead of most entry level data "professionals". Or you could take the boring path and just take the MSSQL certification exams. This is more of the "pay to sort of win" approach. 
I shot you a DM, just to describe the data and things better. I might not be doing a good job of describing it here.
Thanks for clarifying 
Keep up an attitude of curiosity and being willing to learn. The right job may not jump out at you. But eventually you'll find one where you can improve your sql skills.
The only differences in "flavors" of SQL pertain to the different dialects used by different database management systems. There's ANSI SQL which is supposed to be the standard, but is really only used as a guide. T-SQL is what Microsoft SQL Server uses. PL/SQL is used by Oracle Etc. If you know sql from one system, you can usually read other dialects once you can pick out the differences, such as different functions. Figure out what RDBMS you're most likely to use and go from there. 
I recommend looking into Powershell. It has the Invoke-SqlCmd cmdlet which greatly simplifies throwing table results into a variable and piping that to something like Export-Csv.
https://i.imgur.com/CDqD1KV_d.jpg
alright look, I'm not trying to complain, but I have spent over a week to address this issue which is a pretty make/break life situation. After all that search, I did my best on my own to do better. It didn't help and my deadline is tight, so I am pretty much going in desperate mode. No one owes me anything, but this could really improve my life and would be a huge favor me. IDK what to do from here. I tried on my own, at least give me some credit for that.
ALL HAIL WINDOWING FUNCTIONS! 
It's because the data is sensitive and to go on a create new data and make it relevant/accurate would talk some time. I am not a student. This isn't for homework. I have no SQL or advanced Excel skills. This is for job interview process where they know I have zero SQL skills but told me do whatever I can to get some kind of reasonable answer. Its a complete and utter tecchnicality. The hiring manager said its my job but his manager is a bit of a jerk and is forcing all candidates in our field to do this, even though it isn't anything that we would ever use in our roles (as ive been told by the hiring managers). I can't tell you exactly whats been doing because its been a lot in ineffective queries with lots of errors. I watched tons of youtube videos, read tons of blogs. Im getting to the point where I would consider it desperation. I know for people who actually know SQL this is likely a 15-20 minute thing. No one owes me anything, but I am just trying to outreach as much as I can to get help.
From the sidebar -&gt; A common question is how to learn SQL. Please [view the Wiki](http://www.reddit.com/r/SQL/wiki/index) for online resources.
Sorry about that, I'll go check it out.
Hey I did a similar switch from Accounting to tech. Feel free to to PM me to talk. But overall I'd say meeting some recruiters and looking into contract work really helped me get a foot in the door as far as proving I could actually use these skills. Look into the good staffing agencies in your area. They may be able find something that can leverage your scm knowledge into something that also needs sql. 
I got my job as a data analyst by putting on my resume that I was "pursuing Oracle certification." The Udemy course I bought on it hasn't been touched, but they don't care. It gets you past the HR filter just mentioning it on there. 
I’m pretty new to this sub. Would the above be an effective way of gaining employment? Currently learning SQL and unemployed.
BULK INSERT will allow you to import the CSV files into SQL server tables, then you can just do your Sums there. That or the import data wizard.
I can second what he said would get you a ticket into a number of companies even more so if you're near a big city. source: my industry
Support engineer
Apply for jobs usually works
Where do you live
Supplement SQL with something else. Being able to retrieve data is nice; being able to make something useful of it is even better. Brush up on Excel, or jump into Tableau/Power BI, or SAS, or something along those lines. It takes time. Try and pick up some industry habits. Peruse interview questions related to these applications, and try to get good at the key items. One neat thing to this kind of work is that "best practices" frequently come in the form of simple formula/function use. Can you chart a cohort? Can you do rollups in SQL? Use INDEX-MATCH in Excel? Define classes? People don't typically learn these things out of boredom but instead out of necessity. Mastering some best practices translates into untold amount of tedious work saved and generally much fewer errors. That's the kinda stuff recruiters like to hear.
Why do you need to do this with a database/SQL in the first place? You can write a c# program or even a PowerShell script to import the data into `datatable` objects and work with them that way, or even do it all in Excel like /u/fauxmosexual suggested. If you're hellbent on doing it with SQL, you can use the Data Import Wizard (which is SSIS underneath) to import your data into tables in a database and go from there.
You came to people asking for help. You were offered suggestions. You don't get to dictate how those suggestions are formulated. You aren't in a position to say "no, I won't accept *that* help, I want it *this* way" because if you could make that determination, you'd have already solved the problem yourself. &gt; I have no SQL or advanced Excel skills So how do you know that one method is superior to another? And if you have none of these skills, how are you going to make use of *any* help you get here? You won't be able to apply the information you're provided. &gt;This is for job interview process where they know I have zero SQL skills but told me do whatever I can to get some kind of reasonable answer. You say it's not homework, but it may as well be if this is why you're looking for the answer. Do it whatever way **you** can come up with. Because if you get this job by faking your way through this interview and using a solution given to you here, you're going to have a bad time on the job.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/U1Uz5pG.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dwf0yq0) 
I would recommend you look at this survey: https://www.brentozar.com/archive/2018/01/2018-data-professionals-salary-survey-results/ And I'd recommend to do some googling. The survey will be biased to SQL Server, but there are other flavors listed inside of it. More importantly, it will show you job titles, locations, pay averages, and types of SQL used. That should be helpful with what you are trying to figure out. Afterwards I'd essentially google the question you are asking on different SQL jobs. I'd then hunt for those jobs as real job postings and look through them, see what people are looking for and then base your decisions off of that. I wouldn't get too hung up on flavors of SQL just yet. You are more at the figure out the GUI and nuances of SQL point. I imagine you are still figuring out basic architecture design, joins, proper where clause setup, etc. The basics apply to pretty much each flavor until you dig down real deep into how each optimizer handles each query differently based on architecture and design.
So all of the rows beneath Order_ID (which are blank) you want to put the values from the cells Jurisdiction Level &amp; Jurisdiction Name? First thing I'd do is write some kind of Excel equation (new column) that copies the ID from the row above if the cell to the right is blank, then once you do that you can import the data and develop a query to pull those values up. Could do it a variety of ways, probably need to know what the max number of values is going to be because each one is going to become a column, then you could just self join to the table multiple times and pull the data together into a single row.
Why not pass the parameters as DATE type and be done with it. If the end date parameter was '2018-03-27' would it not show all records for that day before midnight of the following day? 
Please, do not, under any circumstances and ever, post scalar UDFs to blogs. Maybe you're a well known dba, maybe you've written books, you know the limitations, but people who visit a blog and see something like that and copy it over to their database can accidentally murder their shit without ever knowing why. DO. NOT. USE. SCALAR. UDFS. Why? Scalar UDFs force a serial plan. Why? Scalar UDFs have no cost to the query planner. It's not that painful to use `APPLY` on an inline table-valued function. 
[Here](https://imgur.com/7nxUaUU) is the desired result. 
Yeah, so first step is you need to get that ID copied down into all those blank fields, once you do that it should be a fairly simple process to get the results you want. I have no idea how to do it within PowerQuery or Excel natively, but if you copy the ID and import it into SQL then the query should be rather simple, such as: select a.blah , b.jurisdiction_name as district1 , c.jurisdiction_name as district2 , d.jurisdiction_name as district3 from importtable a join importtable b on b.id = a.id and b.jurisdiction_level = 'value' join importtable c on c.id = a.id and c.jurisdiction_level = 'value' join importtable d on d.id = a.id and d.jurisdiction_level = 'value' If the values are unknown you could make this a bit more dynamic but if the dataset is fairly standard then you should be all set. 
Alright, I'm trying this out now. Thank you. 
&gt; Yeah, so first step is you need to get that ID copied down into all those blank fields, &amp;nbsp; You can use INDIRECT() to get the value of the same column in the row above. =INDIRECT("A" &amp; ROW() - 1) Change "A" to the column where Order_ID is in.
Oh, I didn't know that, nice! I did this once before and I think I developed a short IF() that looked to the right, and if blank went to the cell above it.
I started out in software support at a tech company, daily use of SQL to identify/trace issues. Writing one-off scripts to retrieve the information I needed to find and moving into resolving any of the SQL related issues myself by updating the stored procedures and functions. Moved into a DBA role within the company. With your customer facing role experience and a willingness to learn I don't see why you couldn't take a similar route. 
Thanks for all the comments guys. I know trying to incorporate SQL or these other tools recommended in some possible way in to my job would help, but I work for a fortune 10 company, so I have to deal with a lot bureaucracy to even go to the bathroom. Are there ways around this?
Something isn't right about your question. There cannot be distinct values you are losing in a group by unless there is something weird you aren't showing. Can you give a specific case and show what the raw data is, and then how the data looks after you run a query? Also share the query.
The values are duplicate orders in the original report, they are exact duplicates and not distinct so they are being lost in this process but I would need to include them in my report. I'll get some screenshots to illustrate what I mean shortly. 
Dude. That's like, the stupidest way of doing that
Not op but I’d like to do something similar. Can you tell me your path from accounting to present-day? I’m a cost accountant but am currently enrolled in an info systems program. I have a year or so left as I’m going at it part time and haven’t given a whole lot of thought to what comes next.
Death to scalar functions! 
To add to this does anyone have any recommendations for an SQL learner? I've been through some courses and have learned the basics, I'm kind of looking for something which gives an interactive run through of SQL uses/ DB management
Thank you! This helps a ton. You’re spot on, i’m nowhere near getting into the thick of things yet just starting off. I was told that between the “flavors” there’s only about a 10% difference and i’ll be able to pick up on that difference quickly once I fully grasp one. 
Syntax wise, that's completely true. For a beginner or someone learning, I wouldn't worry about much else other than best practices which is pretty consistent among the flavors and syntax. Once you get past best practices and syntax, the differences between flavors is staggering. I wouldn't put any attention to that for now. Once you are about 75% through [intermediate](https://softwareengineering.stackexchange.com/a/181657/256008), then I'd say you want to definitely pick a flavor and stick with it. 
ANSI SQL or a specific flavor?
Whichever
I would suggest T-SQL Fundamentals then. One of my favorite intro books. Itzik's later book Querying is also great but advanced.
Use the DateDiff function.
SELECT DATEDIFF(DAY,enddate,startdate) [W3 is a great tool](https://www.w3schools.com/sql/func_sqlserver_datediff.asp) for learning.
If it's dates in two columns and the project is on one row then datediff will be what you need. Otherwise you're going to have to provide some more detail about how the data is structured.
Start with `SELECT`, `FROM`, `WHERE` and probably some kind of date function.
Are there multiple vendors that can service a row in table A, or 1 to 1 cardinality? Is there currently a way to join table A and B? 
Ok, so if I'm hearing you correctly you need to update a list and change all of the values at the same time based on some sort of criteria? That seems fairly straight forward. I have to be honest, I don't really understand anything else you've said. Could you provide some sample data of the table(s) involved in this process? Enumeration is not really a term you hear in SQL often... it's more something I'm familiar with in terms of statistics. So whatever you are trying to do should be fairly straightforward. Maybe.
Also look into joining a SQL meetup group if you happen to be near a big city. Great opportunity to make connections and learn things.
&gt; select days_put_into &gt; from projects &gt; where project_id='???' &gt; ; Seriously tho we need more info.
You can load your results into temp tables #temp01 and #temp02 or create table variables and load them there or use CTE (common table expressions) in a chain. First option would be easier for a beginner but I would probably use a pair of CTEs. Hope that helps.
I work with naux, the issue is we're attempting to add a drop down menu for editing values of various systems support providers, drop down menu would have three different support providers, (current hacky version has two), original column was not added to a table under the tables of the main database, but was added as an ENUM column as opposed to a table being created with some VARCHAR columns under it; this original column is used as a reference for queries when checking to see who provides support for the equipment, it was never meant to be adjusted, but due to contract changes, like a few thousand pieces of equipment are changing support provider. I like naux am pretty new to the whole any of this, and we're trying to figure out a way to replace the ENUM column with an actual table and columns that won't break everything in the worst way possible, or involve us manually headbutting info into it. We're pretty sure that if we were to just delete the ENUM column and replace it with a table with three columns under it, that we'd have to go through a few hundred .phps to ensure that all references to the original column were changed to the correct table.column; concerning providing samples, we're kinda boned on that side, DB and everything are on an air-gapped network, and the only way we'd be able to get samples of data across are via hand jamming it across to here, we can try to put up a sanitized version of it if we need to, it'd just take a bit.
https://sourceafis.machinezoo.com
Yea. I've been looking into all in one solutions like m2sys. Their quote was about 7,000$ for 3 months of using their fingerprinting software license. That is a cost I can stomach if it means 300,000 people get crypto. But I think there has to be a cheaper and simpler solution. Building a database that just holds and checks the fingerprints like was described [here](https://sourceafis.machinezoo.com) shouldn't bee to hard. I understand Java, but have never used SQL. You're right that I'm probably in the wrong sub but im not sure where the best place to post would be. 
CTEs won't have any more or less performance impact than a subquery with the same definition; they are just significantly more legible. If anything requiring temp storage needs an index, you shouldn't be using a table variable. You should be using a temp table in TempDB. 
https://dba.stackexchange.com/questions/12991/ready-to-use-database-models-example/23831#23831
So far, doesn't seem like there's any need to delete the table in question. Need to identify cardinality and joinability to the first table. 
Temp tables === drop table if exists #tmp1 select a = t.alpha , b = t.bravo , c = c.harlie into #tmp1 from table1 t left join table2 c on t.id = c.id drop table if exists #tmp2 select d = t.delta , e = t.echo , f = f.oxtrot into #tmp2 from table1 t left join table2 f on t.id = f.id --select * from #tmp1 --select * from #tmp2 select * from Main M join #tmp1 t1 on M.id = t1.id join #tmp2 t2 on M.id = t2.id CTE === ;with M as ( select * from Main ), tmp1 as ( select a = t.alpha , b = t.bravo , c = c.harlie from table1 t left join table2 c on t.id = c.id ), tmp2 as ( select d = t.delta , e = t.echo , f = f.oxtrot from table1 t left join table2 f on t.id = f.id ) select * from M join tmp1 t1 on M.id = t1.id join tmp2 t2 on M.id = t2.id
Do you have a model of these tables? Not sure what your foreign keys would be.. 
everything that I have to do is posted up ahead. I think it means in a sql statement fashion. Like theres a table called sku_data and inventory
Alright, let's make a few assumptions. sku_data is your item master, with a primary key of item. inventory is your qty onhand, and item is the foreign key. You would have a separate inventory record for each warehouse warehouse is all your warehouses. question 1. select * from sku_data sd join inventory inv on sd.item = inv.item question 2. select * from warehouse w join inventory inv on w.warehouse_id = inv.warehouse_id question 3. select * from sku_data sd left outer join inventory inv on sd.item_id = inv.item_id --This will return every item whether inventory exists or not
Question 4 wouldn't really make sense unless you were aggregating your inventory. select w.warehouse_id, sum(inv.quantity) from warehouse w left outer join inventory inv on w.warehouse_id = inv.warehouse_id group by w.warehouse_id If you wanted just a straight outer join without aggregation: select * from warehouse w left outer join inventory inv on w.warehouse_id = inv.warehouse_id
This isn't about disaster relief, it's about trying to prop up the value of whatever currency is involved.
thank you so much man.
You're right, that probably would've been cleaner. You could make an argument that you'd want to know how much inventory you have in each warehouse, but since you're looking at the whole item master, it would make more sense to sum it and see how much inventory you have total: select sd.item_name, sum(inv.quantity) total_qty from sku_data sd left outer join inventory inv on sd.item_id = inv.item_id group by sd.item_name 
I didn't know that. I'm not too familiar with the other services that come installed
I don't believe there is anyplace you can get SSIS experience online. I do a decent amount of SSIS. Most of what I do is write SQL and put that into the "Execute SQL task" task. Get good at writing SQL, SSIS is an extension of that IMO.
On mobile so I can't test at the moment. But this looks very promising. And the cheaper the better, no matter how ugly. There is a fairly small number of action codes (7 or 8 I believe) and I'd say less than 1% have a code outside of the A/O/D vals you see in the screenshots. Question: what's that REPLICATE(COUNT) combo doing in the subquery? It's possible for a student with two G awards to A(ccept) one and D(ecline) another, in which case the string should say "A, D" (or "D, A", makes no difference at this stage). Thanks for the help.
On mobile so I can't test, but this looks very promising. I'll give it a shot tomorrow. We have 7 or 8 action codes, with the three you see in the screenshots making up about 99% of all cases. What's that REPLICATE(COUNT) combo doing? Students with two multiple awards in a single category could take independent action on them, like A(ccept) one and D(ecline) the other. The actions would then ideally be "A, D" (or "D, A"--wouldn't make much of a difference at this point.
so what you want is string_agg (new with 2016 i think) along with a window function... something along the lines of select studentid, sum(whatever), string_agg(action) over (partition by studentid, order by timestamp) from table group by studentid
Sadly STRING_AGG is only available starting in 2017, and we're on 2016. Not that I didn't try...!
I'll give it a shot tomorrow. At first glance REPLICATE doesn't look like it'll handle different actions for multiple instances of the same award for the same student, like StuID #5. 
Maybe try to see what kind of books are available for Integration Services out on Amazon? A lot of them will usually have some step by step hands on guides that involve pulling flat files into a sql database, creating calculated columns, checkpoints, etc. 
Thanks for the pointer. I actually already used CTEs in the queries so the only thing I had to do was to work out how to use multiple 'with' clause which wasnt difficult.
Thanks for the replies. I'm already pretty ok with writing SQL but my new job will need me to develop SSIS packages.
C# is also very useful in SSIS, as it’s essential for Script tasks, which allow you to do the actual transformation of the data before it even gets to the SQL server environment. It also allows you to manipulate the file system a lot easier (if you’re importing many files at once, and of different file types).
For basics, I’d suggest the following exercises: 1) CSV to SQL Table (import) 2) Excel to SQL table (import) 3) Table to CSV (export) 4) Table to Excel (export) Then add steps to truncate the tables before you import them Then add “For each file” loops Then add file manipulation (moving, archiving, deleting) Then add transformation scripts (Using C# Code yo validate data and stage it for the SQL Table), and vice versa. That’s should get you started at least.
[removed]
I can grasp basic C# syntax, but that really discourages me from practicing the scripting task and component. 
It’s not as complicated as you may think. I walked my team member though her first script task (she had about 1.5 years of T-SQL And pl/sql experience st the time and it was a breeze. You don’t break anything, just give it a shot (in a test environment of course). ETL AND SSIS make you a lot more marketable. You don’t really have anything to lose.
Sure thing friend! Please feel free to PM me for any related questions or tips. FYI: Andy Leonard is an AWESOME resource for SSIS related learning. https://andyleonard.blog/
[Table alias](https://www.w3schools.com/sql/sql_alias.asp)
After a table or column you can use the “AS” keyword, which basically allows you to use another name. Commonly used to shorten a lengthy table name or rename a calculated field in a select statement. 
That's very good advice. From everything I've been told, it's a fairly large operation that had been all db2 and are ramping up their sql server operations. Most of my disaster recovery experience has been with mirroring with at least one failover, but in the last few months, i have been getting experience with always on, although I'll be be the first to admit it's limited at best. 
Since you're saying you're new, just a few helpful remarks (yeah trivia!): The RDBMS you mention goes by many names, but 'postgre' is not one of them. It started out as Ingres, and got re-implemented as 'Post-Ingres', shortened to 'POSTGRES', less screaming-cap-ishly styled 'Postgres'. At that time it used the 'POSTQUEL' query language but later switched to SQL, at what time they renamed it to 'PostgreSQL' /ˈpoʊstɡrɛs ˌkjuː ˈɛl/. Today, 'PostgreSQL' and 'Postgres' are common ways of referring to this software. Internally, to avoid naming collisions, the `pg_` prefix is used in many instances, and the (un)popular tool, pg_Admin, is named like that; likewise, in insider talks, 'PG' is readily understood. The single piece of the DBMS that many users will encounter on command line is, confusingly, called `psql` (not 'pg' something). Worse, there's a now-deprecated tongue-in-cheek alias for `postgres`, `postmaster`, which will make everyone think of mail, not a DB server. BTW just in case you haven't heard of it and have something to take away from this comment, there's a tremendously useful piece of software called (again somewhat confusingly) [`pspg`](https://github.com/okbob/pspg) by *P*avel *S*těhule (a genius C programmer). It's a pager for `psql` outputs that is columnar-format-aware and allows to pin the *n* leftmost columns in Postgres query outputs similar to what you can do with spreadsheets. Check it out! Second, the screenshot you posted is just textual data. You should *really* try and find out how to copy such data as text and paste it to applications (like this forum) in text format. Basically, you can't very well develop stuff in a DB when all you do is copypasting images. I can assure that you *can* do a lot of stuff just copypasting *text*, tho ;-)
These are table aliases and are very useful, you should use them. Imagine two tables, like customer and order, both containing id columns, then you make a join like that: SELECT -- I want to have a customer id here , -- and an order id here FROM customer JOIN order ON order.customer_id = customer.id You can't just say `id` because it's ambiguous, so you use `customer.id` and `order.id`. Furthermore, you should always use table names or aliases in joined queries because otherwise you have no idea where the column is from. SELECT name , number FROM customer JOIN order ON order.customer_id = customer.id Where do `name` and `number` come from, customer or order? You have no idea. So you use `customer.name` and `order.number`, right? Now onto aliases. Imagine an app with fucked up names, like `BRXACHJ0`, someone might have no idea what this is. So in your queries or views, you could use a meaningful alias like SELECT account.number FROM BRXACHJ0 AS account Similarly, imagine an app that has very long table names like, `app.crm_mtm_customer_order_line_to_shipping_address`. Following advice from my first point, you should use table names in your select queries so that you know where a column comes from, but who in their right mind will type all this shit every time? SELECT -- whatever FROM app.crm_order order JOIN app.crm_order_line line ON line.order_id = order.id JOIN app.crm_mtm_customer_order_line_to_shipping_address line2address ON line2address.order_line_id = line.id JOIN app.crm_customer_address shipping_address ON shipping_address.id = line.customer_address_id This is much more readable than just repeating yourself all the time. You can (have to) use aliases too when joining the same table twice. Imagine a table with invoices and each invoice can have an address the invoice should be send to AND an address the delivery should be made to, and they can be different: SELECT * FROM invoice JOIN address ON address.id = invoice.invoice_address_id JOIN address ON address.id = invoice.delivery_address_id This won't work, obviously. So you have to alias them: SELECT * FROM invoice JOIN address AS invoice_address ON invoice_address.id = invoice.invoice_address_id JOIN address AS delivery_address ON delivery_address.id = invoice.delivery_address_id Keep in mind the `AS` keyword is optional for both table and column aliases. Let me know if you need any more help. I'm Polish too, in case you have any trouble with English. Btw, using column aliases with spaces in them is not good practice (also using nonascii characters like ś, ć, ł, ń etc.) for a number of reasons. I'd avoid that. 
He's not wrong, but in this production setting I really only have authorization to query the database. I'm new at this and I hear that our DBA, who works at a different site, is kind of a grouch. Not sure how seriously he would take any suggestions from the new guy on adding new objects to the DB.
But I have 16gb ddr4 :( And like I said it happens every few seconds 
Looks great. I'll report back this morning once I'm at work. Without being able to see an execution plan (and without the stats due to all the stacked CTEs), time to execute is really the only metric I have right now. :)
Very true. I like scripting in C# to create pop up windows that show me my variables. It's nice to validate what's in there.
You could try Udemy. Their courses are priced per course. Usually $10-20. Jose Portilla has one that looks good. I took the one by "The Lazy Programmer" and it was good but he does it all from the command line. 
I don't think the optimizer handles CTEs as well as it would with a more conventional query. In the case of temp tables, the creation of the temp tables are done as separate queries whereas the entire query (with CTEs substituted) is optimized and executed as one step. They allow developers to write a query in separate logical pieces, but have it all executed at once, so I see some with 3, 4 stacked expressions that could have been done as one query and they definitely perform differently.
&gt; I don't think the optimizer handles CTEs as well as it would with a more conventional query They produce the same execution plan behind the scenes as if you took the CTE definition and sub-query it into the main query. TempDB/table optimization is almost always a 'it depends' scenario. There are general rules that should be followed, but there are just as many factor that are based on the structure of the data. 
So should I start with MySQL cause of how simple it is or should I start with Postgre so I don’t build habits? 
I'm learning it with codecademy, but as a novice I can't vouch for how comprehensive their course is.
ANSI or a particular flavor?
REPLICATE is only handling multiples of one action. ISNULL(MAX(CASE WHEN Action = 'A')) + ISNULL( ... is combining different actions.
What kind of level are you at? Sqlzoo is basic, but free, well written and self-paced. Requires you to enter code. Oracle offers a free 12-week online course that also requires you to actually enter code. It provides a certificate of completion. Which is nice. https://blogs.oracle.com/developers/learn-sql-with-this-free-online-12-week-course I've taken courses from both VTC (Virtual Training Company) and Udemy and I've thought that they were both high quality. There's also websites that aggregate coupons for Udemy, Coursera courses as well, so in all honesty, I'd probably just look for some free ones and take those.
I’m a beginner but am eventually trying to get to the advance level of SQL 
Take 3 tables, Order, LineItem, and Item. Join them in every possible order, using every type of join algorithm, and measure reads, cpu, memory usage, and execution time. Discuss the best and worst performers. There are 54 permutations. WITH cteTable AS (SELECT * FROM (VALUES('Order'),('LineItem'),('Item')) t(TableName)) , cteJoin AS (SELECT * FROM (VALUES('Merge'),('Hash'),('NestedLoop')) j(JoinAlgorithm)) SELECT t1.TableName , j1.JoinAlgorithm , t2.TableName , j2.JoinAlgorithm , t3.TableName FROM cteTable AS t1 INNER JOIN cteTable AS t2 ON t2.TableName &lt;&gt; t1.TableName INNER JOIN cteTable AS t3 ON t3.TableName &lt;&gt; t1.TableName AND t3.TableName &lt;&gt; t2.TableName CROSS JOIN cteJoin AS j1 CROSS JOIN cteJoin AS j2;
Thanks for the Oracle 12 week class tip.
Make your own crash course. Get the free SQL express download from Microsoft, download the adventureworks database and play. Setting up the environment and learning how to connect to the database is great practice and is just some Google searches on how to do. Once you're setup look at the tables and see what they hold. Try and think if you were running a business what type of information would you want, then try and build it. The hand held courses are great for learning some syntax and basic concepts but if you really want to learn you just have to practice real world queries. Once you get comfortable try and build a database on something you're interested in and create reports out of it.
Seconded for him, he's a great guy and also incredibly smart.
If you're a very beginner then start with MySQL - there are many simple useful things like "upsert" - INSERT ... ON DUPLICATE KEY UPDATE. In Postgre it will not so easy to do this. 
Hot damn dude, this worked beautifully. All told my suite of CTEs are collectively running in about a minute now, which is perfectly fine. I'm going to check out the other solutions too, but thank you kindly for your help with this.
Anyone know about in person traning? Leaning towards Azure and overall cloud. 
I would be willing to work on something like this for totally free, and I suspect you can find others similarly willing. What is the time frame for your deliverable? This is something that I would expect some students to JUMP at the opportunity to get involved with. Real nice resume entry (experience) and some valuable portfolio experience. PM me if you like.
A CTE will absolutely produce a less efficient set than a sub-query in many examples that I have personally tested but that goes back to using 2008R2. Subqueries are great but the more you get the worse SQL executes them and #tables are a great way to force SQL to execute chunks of code in serial, and the ability to index them can be extremely useful.
mysql
+1 for Udemy. People have their gripes about them, but the well known teachers offer good stuff. There are plenty of free resources, such a code academy or SQLzoo, and they are definitely good. But sometimes hearing someone talk you through something in a video with examples just helps that little bit extra, at least for me. Really, for that price, it's not bad to look into, even if it's not your primary source or if you just use it for reference. That's like 3 coffee from Dunkin. 
So it should be high cardinality, as the three rows are unique I think but need to be accessible from multiple tables; the biggest issue is that as an ENUM column it has pretty crappy joinablity if I'm understanding correctly, because ENUM columns can't be re-used for multiple tables, you have to make one for each table with identical info, then you have to keep up on updating all of those individually, which involves a silly amount of DB breaking/hacking.
It works as expected with the geometry data type. I'm curious now.
This did the trick too so thanks much! Sadly we can't use it as-is because the max is 4 *for this term* and it could change, in this or any other term. If we could pull the joins dynamically with indexes based on a changing max value that might work, but that's probably moving into looping logic and/or a sproc, which I'd like to avoid if possible (but please correct me if there's another way!).
+1 for Udemy and Jose Portilla’s course. I’m going through it right now. I started having never seen SQL queries before and feel confident in the basics of navigating a database. I also have extremely little previous coding experience. 
why 'klient' but not 'kar'?
I totally agree. I'm a computer engineering student myself and some of the things we are working on will have way more impact and real world use case than many of the internships I've had. I sent you a Pm, but for future readers, and anyone who is interested in helping with this project, below is the discord link. https://discord.gg/aB5YawV Here is the website (work in progress): http://www.cryptocucuta.com/ 
Are you running command line? Are you sure you're currently accessing the DB using the `postgres` user? Try running `chmod 777 &lt;filename&gt;` If that doesn't work you may try using `\COPY` instead of `COPY` as seen [here](https://wiki.postgresql.org/wiki/COPY) 
&gt; A CTE will absolutely produce a less efficient set than a sub-query in many examples No they won't. For example: *** ;with cte AS ( SELECT idSale, idStore, idProduct, saleDate, saleValue FROM sales ) SELECT idStore, idProduct, SUM(saleValue), AVG(saleValue) FROM cte GROUP BY idStore, idProduct *** Will produce exactly the same execution plan as... *** SELECT idStore, idProduct, SUM(saleValue), AVG(saleValue) FROM (SELECT idSale, idStore, idProduct, saleDate, saleValue FROM sales) GROUP BY idStore, idProduct *** Which will produce a similar execution plan as (assuming no statistics are generated/etc)... *** CREATE VIEW salesView AS SELECT idSale, idStore, idProduct, saleDate, saleValue FROM sales GO SELECT idStore, idProduct, SUM(saleValue), AVG(saleValue) FROM salesView GROUP BY idStore, idProduct *** The performance for the above will be the same assuming you clear statistics and plan caches between tests. 
Microsoft Virtual Academy have some good starter videos - https://mva.microsoft.com/product-training/sql-server https://mva.microsoft.com/en-US/training-courses/sql-database-fundamentals-16944?
Even then, there will be no differences in the approach the engine attempts to optimize with, if the plan doesn't achieve full optimization they will be the same.
What you are saying is counter to what I have personally tested when it came to complex predictive models. In either even, though, the best approach is to use #tables and to force the optimizer to do what you tell it to do, when you tell it to do it, and execute things in serial without letting it achieve optimization at all.
A lot of the tricks with SQL are dialect specific. I had about 15 years of Oracle, and then we switched to SQL Server. There are lots if tricks regarding procedures, triggers, session settings (collation, nulls, exception handling and case sensitivity) API handling of binds etc. To really be of use to an employer, you'll need to master all that.
&gt; What you are saying is counter to what I have personally tested when it came to complex predictive models. Show me some full execution plans, in the vein of the above examples, where they are different. Quite a few SQL blogs/posts squarely agree with me on this one. 
For me it helped some but now that I'm using access and digging into real world data I find that there were a lot of things he left out. But maybe they just weren't beginner level. Idk. 
Why not just include a join on a lookup table in the MV? And if the numeric IDs that are different in your various environments can be obtained from the same query in all environments, then just use that query instead of the lookup table. 
I can't, this goes back to a previous job I had but I can tell you what type of operations I was testing. Are you familiar with the method of writing a completion model... taking a set of given data such as with a as() and then in subsequent b as() which will come up with running totals, grand totals, joining and rejoining to different steps in order to run calculations and derive values which are then in a final step used to project results across time (i.e. joining to a new set of data not referenced initially) ... it just blew up the CTEs. I vaguely remember when I made the post and was testing that someone mentioned it was an issue with 2008R2, but I could be misremembering that. Even still, the best option is to simply use #tables when you are working with data on that level. Subqueries fall apart and destroy execution times when you are trying to run complex calculations and model data in specific ways.
That sounds pretty much like standard windowed functions. Something 2008R2 handled exceptionally poorly and rarely came up with a reasonable/full execution plan. Thankfully, the introduction of significantly improved windowed functions starting in 2012, the requirement to use temp tables or cte's in a fair number of these cases became negligible or obsolete. 
I am currently on 2014 and can tell you that #tables will outperform subqueries or CTEs in many, many, many cases. In fact almost any complex case where you have big tables and are joining to multiple tables using a variety of logic, cases, complex where statements, etc.
I would need to know more about what you are asking, but if you mean what application is the best for displaying data which you prepare in SQL then to me it really comes down to two options: 1. Tableau - Looks sexy as shit, is expensive, great for establishing client "portals" where users can log in and interact with dashboards, but horrible at integrating into PowerPoint and other MS Office products. A pain in the ass to work with if you are a developer... kind of learning learning how to use an Apple computer. They call it, "drawing with numbers." 2. SSRS - Free if you have SQL server. Is a pain in the ass to work with to make anything look sexy, but much more SQL friendly than Tableau. Extremely easy to integrate into PowerPoint and other products. 
Are you running the database locally? Note that `COPY` is executed on the **server**, and as such any file paths need to be accessible on the server's file system at the path specified. As the hint says, if you want to read a file client-side for the copy, use `psql`'s `\copy` If it's a relative path issue, ensure that you're in the correct working directory for the path to be found.
If you're looking at SSRS, might as well tack Power BI on there too. Some crossover with SSRS but is much more friendly on the data viz front. Uses Power Query for relatively easy extracting/transforming. Need a subscription for web publishing though.
Seems to be unnecessary to me, the ‘%’ symbol is the wildcard symbol. I’m guessing it’s supposed to ignore rows with ‘,author,’ anywhere in the string permalink, but I can’t think of why it’s wrapped in the concat function. 
Honestly my experience with PowerBI is limited to the point where I just don't really like it, and have had a bad experience with it. I like SSRS because it is pure SQL.
&gt; concatenating %'s around a column does the same but omits all valuesinnthat column Can you explain this a little differently?
I actually took a lot at Power BI today. Got it set up and working, but is there a web portal where you can push reports? 
It should be added that sometimes table aliases are mandated, usually for subqueries. Also, it saves lives when you're trying to join 10 tables. Though, reconsider your schema when you're joining 10 tables.
You might want to consider looking into windowed function and columnstore indexes. Between them and the new cardinality estimator, I've managed to get rid of a sizeable number of temp tables in my stored procedures. SQL Server 2016 does an even better job with BatchMode processing. Going so far as I replaced a handful of analytics reports that were limited to running once a week at night because they all took 1.5-2hours each to run. Now they can be done live and run in under 10 seconds. 
I'm using pgAdmin4, not the command line. On pgAdmin, when i go to my employee test database and open the Query Tool it says "employee on postgres@testDB" so I think I'm logged into the DB as postgres. And I have made sure to give read/write/execute permissions for the dataset to postgres. I tried using \COPY but it seems like pgAdmin isn't recognizing it as a valid thing? I just get an error saying the \ is creating a syntax error.
God do I fucking hate Tableau - the second anybody starts using calculated fields in it your data is basically dead to ETL.
If serial execution gives you the performance you need, then there definitely isn't a need to move away from it. Granted, that is becoming less and less common in larger environments. Being able to build processes that parallelize nicely and utilize the hardware available to it; 24 core / 128GB memory isn't that uncommon anymore. It's a tremendous shame when I see serial analytic solution/sproc/etc that operate on multi-million/billion rows; taking minutes/hours to return the result and hammering the SAN the entire time. Especially when I've often replaced it with a recursive windowed CTE which utilizes 20 threads and returns the results orders of magnitude faster and did most of it in memory. 
Often times the types of processes I am running need to go through every row, aggregate where appropriate, join to other sections of other aggregation, etc., and then output a new row. In fact, often times my results are much greater than the source tables. The current project I am attached to outputs a table for front end consumption that is larger than any of the parent data sets it works with. When the project is fully complete the table will be ~5x greater in size than any other table in our database. Subqueries won't work.
Check out [W3 schools](https://www.w3schools.com/sql/) it's pretty useful and thorough when it comes to all the different dialects of SQL
You seem to be ignoring what I'm saying. I am not talking about 'reports.' Yes, I understand what you are saying with huge tables and running a simple query... BUT, if you want to run that simple select query you're talking about and join it to another simple select query, and join that to another one, and join that to another one, and then aggregate them, and then join them all back together... you best put that shit in a #table.
&gt; #table Why would you do that when you can pull the information live or in an OLTP table? There is a good reason the running joke about SAN admins hating SQL admins; because that #table is going straight to the storage. With all the network and associated latency that brings with it. 
So if the author column had the name John Smith in one of the rows, any row in the permalink column that had the string John Smith in it I.e. ' this permalink is for John smith' would be omitted. Does that make sense? Apologies I would post a code example but my computer is not with me and I'm on my phone 
Excel macro or a combo/turn-key table? 
Or formula, if you don't need this to be a full on "tool" for re- use
Really I should have written a script for this at this point, but my process to do this quickly tends to look lie this: 1) paste values into TextPad 2) Find/Replace ^ with ' 3) Find/Replace $ with ', 4) clean up the last line 5) control A (select all), control-j (collapse separate lines into one) Many tools out there can do the regex find/replace needed for the brunt of the work, I just don't know the ctrl-j equivalent in other apps.
&gt;Find/Replace ^ with ' &gt;Find/Replace $ with ', I tried a find/replace with those characters - but it appears to be looking for them, and they don't exist. Am I missing something?
&gt; dude if you let me run a model on an OLTP Funny, because I do this for a large telemetry data set, and don't have a single problem. Not all of it is done there, obviously. If you're doing a large amount of transforms/aggregation/etc, do them where it's fast then put the result for processing in a temp table when it's done. Use the right tools for the right job. 
Might need to enable Regex. What editor are you using? 
&gt;Funny, because I do this for a large telemetry data set, and don't have a single problem. Not all of it is done there, obviously. I honestly doubt you are running predictive models on a large set of data in a transactional environment. Our scoring would take the entire load of the server more than half a day to run. &gt;f you're doing a large amount of transforms/aggregation/etc, do them where it's fast then put the result for processing in a temp table That is precisely what I am saying to do. 
Asking someone with experience, is it true if i learn and get skilled at one “flavor” of SQL then it’s usually easy to pick up on the differences of the others?
Hi there. This is an ADBMS project, we need to implement query processing and optimisation in a particular area rather than a research based project. Any code, suggestion or resource can be very helpful.
&gt; honestly doubt you are running predictive models No, it's used to build the trees that are used for the models, because it can do so in fractions of the time. 
Thanks a lot mate, you have any codes or resources by any luck. It'll be very helpful.
Oook... so not at all what I was commenting about.
Except it is, if you did this all in temp tables, it is unnecessarily adding large overhead for no reason. The results are what matters, if you can do all the aggregation and tree generation there, you're loosing performance and it will be slower. If you're doing smaller data modeling and scoring, in-memory will destroy temp tables. Where hybrid temp/in-memory models start to win out is in large/huge data sets. Which brings me back to what I was saying, if you can break the problem down into a parallelizable solution, you can take advantage of all your hardware working on smaller problems; then let merge up the results. This allows it to have partial logarithmic scaling for something that is traditionally linear or exponential. 
Fair enough! The hip thing these days is to try and conflate the two, since SSRS was the closest thing to a true BI solution by MS before Power BI came out. I find uses for both.
Metabase, Redash, Superset are all web app BI tools that are in a similar space to Mode. I'm more familiar with Metabase, which has the advantage of a point and click UI as an alternative to SQL, but both methods have the same visualisation options. Percentage change since last query would require that somewhere, somehow, the most recent execution time is recorded. I think Metabase might do that but the challenge would be to get it into your query, since it'd be written in the app database, not your content's database. Alternatively, if the query period is known (e.g. every hour) then write the SQL to diff on the last hour?
As the new guy, it's all about how you approach the DBA. Maybe he's already got something you can use... Start a conversation with him. Usually you can't go wrong. 
Yes. There are three basic "modes": desktop, workspace, and public. Desktop is free but your sharing is limited basically to whoever can access the .pbix file. You do most of the build/design here. Workspaces are the collaborative portals on the web where you can share reports. This is also where you build your dashboards. Fully integrated with 365/SharePoint too. But this is the subscription service. I think you can get 60-day trials. Public is basically a workspace that you've published to the world. Anyone with the URL can view it. Also paid.
So I work in an industrial environment and have struggled with this for years since we have multiple systems across generations and sites plus complications of scadas and historians. We stumbled across one called DreamReports from ocean data systems that looks promising and relatively cheap for what it claims to do. We'll be rolling it out in the next few months as a test and I can report out on it.
Not sure what RDMS platform you are working with but the client took for SQL Server has a nice trick. In SSMS you can hold the alt key and drag your cursor down a column, effectively putting the cursor on all of those rows. Now all you have to type is two keys, comma and apostrophe, and it will add it to all 50 of your rows. 
So whenever you have an aggregate function (sum, count, min, max, etc) you have to use “group by” on everything else. So after your where clause put “group by ID, beginning_balance, Sum(balance)”
that gave me an error saying that i couldn't put the sum(balance) in the group_by. so i got rid of that, and it worked! thanks! now my problem is that orders also has a state column, and i only want orders that has a state of "complete" to be counted. any ideas on how to do that? i promise, i'm done once that works. :-) 
&gt;if you do all the aggregation and tree generation there That isn't at all what I said. What I said was if you don't break it down and you overly complicate subqueries/ctes then you will see a huge performance boost using #tables. &gt;Where hybrid temp/in-memory models start to win out is in large/huge data sets If you're modeling on small datasets then I'm ignoring your model.
Perhaps try a sum(case when order=complete then 1 else 0 end) as ‘column_alias’, I’m your select statement
Been using this table value function on SQL Server for some time. Http://Stackoverflow.com/a/512300/22194
Why subqueries? Just learn to write complex queries. All a subquery is is a query within a query. Just get good at writing queries. Also complex subqueries aren't efficient. There is usually a better way to get the data you want like loading it into a #Table.
Some days, this is my life.
If I understand your question...you can nest a select statement into your WHERE clause. Select * from table1 where ID in (select ID from table2 WHERE condition1='stuff' and etc...)
We looked at Dream Reports and had a license for it but didn't get to really use it. It looked really good though. OSIsoft has some really good software but it isn't cheap. OSI has an Excel add-in to pull process data and a newer product called PI Vision which lets users create process displays that run in a browser. Not a good tool for reporting though. 
Ok so for example if I wanted to pull wildcards on a list of different values and then find only the highest number from those wildcards wouldn't you use a subquery to accomplish this? I tried max() in the select with order by and it wouldn't work
author is probably a variable so '%author%' probably wouldn't work.
I wrote some VBA code to do it automatically for me and made a spreadsheet with a macro button on it to accomplish this. Would you like it? You basically just paste the values in A1 click a button and it spits it out with ('value1',value2'...etc)
No it's an Eclipse based IDE
&gt; complicate subqueries/ctes No different then if you use overly complicated temp tables. &gt; If you're modeling on small datasets then I'm ignoring your model. Good research/business has been done on breaking down large dataset problems into parallelizable smaller sets; precisely because others realize that brute force serializing results is not feasible or sustainable for what most business need. 
[Here's some code.](https://pastebin.com/c0zMCQCp) [And here's a resource.](https://www.whoishostingthis.com/resources/sql/)
I’ll check out Tableau. I need things to look sexy, as I’m the programmer and the one distributing the data to my reps and CEO lol
You are at this point talking yourself in circles, you just said a moment ago you use #tables. &gt;Good research/development has been done on breaking down large dataset problems Using #tables! And a moment ago you said you used a large OLTP to do modeling and not small sets... 
&gt; ago you use #tables When needed and suites the situation yes, not everywhere. I don't actively go out of my way to use them over alternatives. &gt; Using #tables Using recursion, windowed functions, and in-memory parallelism. Temp tables, doubly so for global temp, by structure are absolutely atrocious for parallel tasks as the storage allocation used by it is single threaded and it's response time is in *ms*, over the memory *ns*. 
Thanks. Much appreciated.
Okay this sounds like it’s for me. I use share point a lot. 
From what I can tell after playing with it for a couple of weeks, Power BI is the shut. 
[Periscope Data](https://www.periscopedata.com/) does SQL based charting, as well as Python and R. Awesome tool.
[Periscope Data](https://www.periscopedata.com/) does SQL based charting, as well as Python and R. Awesome tool.
[Periscope Data](https://www.periscopedata.com/) does SQL based charting, as well as Python and R. Awesome tool.
 create table owners(id integer ...); create table pets(id integer, owner_id integer, ...); select id from pets join owners on pets.owner_id = owners.id ... ;
It doesn't, honestly, make a big difference. They're phrased vaguely differently, so if you have a reason to expect to be using one or the other soon, pick that one.
datastudio.google.com Web-based, straightforward Google UI that you can easily pick up if familiar with the G Suite workflow. Datastudio allows importing data from diverse sources (Google sheets, internal flat DBs, etc.) and is good with calculated fields It will only get better
Check out [Blazer](https://github.com/ankane/blazer). It is not as powerful as e.g. Redash for dashboards but it gives you extremely easy way to share single query/chart and it also can be configured to be used with [PgHero](https://github.com/ankane/pghero) - which you really want to use if you have PostgreSQL database.
Thanks for sharing this. :-) This is why we have Notepad++ or some other editor on the side. 
I wouldn't go less than 8GB of RAM and 256GB storage. If you want to experiment with learning on [the full Stack Overflow database](https://www.brentozar.com/archive/2018/01/updated-and-smaller-stack-overflow-demo-databases/), for example, it's 137GB on its own.
Here you are a popular course on SQL. For free. With examples and online exercises. http://www.studybyyourself.com/seminar/sql/course/?lang=eng.
Thank you very much /u/jc4hokies. Can you provide the source of your findings? I would like to do some additional reading on this.
https://en.wikipedia.org/wiki/Well-known_text Basically, defining polygons counter clockwise makes them inclusive (space inside is included). Defining polygons clockwise makes them exclusive (space inside is excluded). You had inadvertently defined your geography as the entire Earth except for what's in those coordinates.
I have a surface pro... Personally I like it overall for general use especially note taking, but not for db work and coding. It's personal preference of course, but the screen for me is too small, the keyboard too flimsy and the stand too annoying to adjust on the fly as a serious laptop replacement for work efficiency.
I find it more efficient both in writing - splitting complex stuff into smaller parts, and performance - ability to create and drop indexes on the go.
Thank you, this may have just turned me to save for a mac book pro. 
I'm a professional DBA and I use a Surface with these specs on a daily basis, we're a start-up company so they bought all the new people these, I've been begging for an upgrade it's "fine" and gets the job done but I would recommend something with at least 8GB of RAM or more especially if you're going to be running SQL Server on it (I just connect to remote servers from it).
Please make sure to name the applicable flavor of SQL or RDBMS you're working with in the post. If this is for MSSQL or MySQL, you can do something with the INFORMATION_SCHEMA to find tables with the same column names. select * from information_schema where table_schema='my_schema' and table_name='my_table' compare the data from there.
As others have said, they're table aliases which are named in the FROM and JOIN clauses. A more verbose to declare an alias is to use the AS keyword ... FROM app.service_ticket_tab AS st JOIN app.ticket_tab AS t on (t.ticket_id = st.ticket_id) In your example, they left AS out which is fine. The alias declaration is implied without the AS keyword. Also, note that this is standard SQL and isn't specific to PostgreSQL as you asked in your post title.
DAY - "Returns an integer representing the day (day of the month) of the specified date." So DAY (GETDATE()) will return 1 for the first day of the month, 28/29/30/31 for the last. DATEADD supplied with a negative value will subtract that number of days. Take away today's day of the month from the current date and you're on the last day of the previous month. 2018-04-01 - 1 DAY = 2018-03-31
`DAY()` obtains the day of the month of any date. So `DAY(GETDATE())` is today's day number, 30 in this case. `DATEADD(what, howmany, date)` adds *howmany* *what*s to *date*. So `DATEADD(day, -30, today)` would substract 30 days from today. and of course today is calculated with `GETDATE()`. 
Well let's take a second and think about what we're doing Check out [DATEADD](https://www.w3schools.com/sql/func_sqlserver_dateadd.asp) if you're not familiar with it. Super handy tool. Okay, now we know the biggest part of what we're doing. Adding negative DAY(GETDATE()) to the current date. Okay, try SELECT DAY(GETDATE()) That gets us the current calendar day. What happens if we subtract 30 days from March 30th? We get Feb 28th. Just take a couple seconds and try to think through your problem, beginning with the first step. Seems like a whole lot is going on in that select, but it's really only 2 steps. Hope that helped!
Okay Cool I just noticed you can create code boxes as you described, so I reformatted the above statement. &amp;nbsp; This is Mssql, I have a join on EMAIL, I (currently) have two questions. &amp;nbsp; 1) I joined on the Email of two different databases, now I want to filter this result, where table "TERMINIATION_DATE" is not null. SELECT TOP 100 * FROM [GOSALESdw11].[gosalesdw].[EMP_EMPLOYEE_DIM] ed FULL join [GOSALES11].[gosaleshr].[EMPLOYEE] e on ed.email=e.email WHERE e.EMAIL is not NULL; &amp;nbsp; When I do this it says TERMINIATAION_DATE is ambiguous, however it is a Field, but I need to create it as a name? 
Thank you all! I saw this code snippet from my colleague as well, SELECT DATEADD(m, DATEDIFF(m, 0, GETDATE()) -1, 0). I'm not sure what the -1 part is doing.
AHHH okay, so once I set the alias from the beginning, everything thereafter I can refer to as e.&lt;Field name&gt; if I specified the table as e.&lt;table&gt;
&gt; SELECT DATEADD(m, DATEDIFF(m, 0, GETDATE()) -1, 0) Well again, let's look one step at a time. Still using good ol' DATEADD, we know that one now. On the w3 page I linked earlier it shows that m as the first parameter means month. okay, we're talking months. I like to know where I'm starting first, so let's look at the last parameter...0. That's kinda weird, but SQL Server sets 0 as 1900-01-01 00:00:00.000 (formatting depending on location settings of SQL Server, I think?). Okay, on to the heavier 2nd parameter. DATEDIFF(m, 0, GETDATE()) -1 A new function, [DATEDIFF](https://www.w3schools.com/sql/func_sqlserver_datediff.asp). Alright, so we're getting the difference in months between 0 (again...1900-01-01 00:00:00.000) and the current date. **Then we're subtracting one.** So we have the total number of months since January 1st, 1900, and subtracting one month. So....we add the total number of months since January 1st, 1900 minus one **to** January 1st, 1900. So it looks like it's getting us the first day of the last month. Seems pretty round-a-bout, to me, but I'll admit the way I would do it off the top of my head would have way more nested functions, this is much simpler to unwind and read when you need to. Hope it helped! Try just taking a step back and reading it one part a time.
FULL join is also going to bring back everything. Not sure about your requirements, but you may just want to use am inner join. 
ROW_NUMBER() = 1 partitioning on the countdown value in a subquery. If there are more than 1, the function will return higher and exclude them. 
Yes and no. In your example, e is an alias for Employee table and you use "e" in place of "employee" when using a table.column notation in select and where clauses.
This is why I habitually create a unique surrogate key populated by a trigger or auto number. You need a way to randomly pick one row having a value that matches in one column. I think so anyhow. You need more description here, especially the flavor of SQL.
You are making a myriad of assumptions, including that we know what groklearning is, and the specific question you are referring to. Can you show us the question and the answer you gave, and we can go from there.
Understood yeah I see... SO I think Im stuck and freaking out here a little and its because the data Im using is all over the map trying to find links to connect region by branch to order and returns... &amp;nbsp; Im looking to make a template that will show: 3-4 tables joining, with a group by and sort by with a aggregate calculation. If someone has time as a tutor tomorrow for a few hours I can pay for a webex session to review these as Im a little stressed and would like to be helped with a proper joining and elaborating on grouping and aggregate summaries. &amp;NBSP; PM ME and we can sort out a time tomorrow 
aaaaaaannd.... [deleted]
Never seen it done this way before... I always use eomonth(). Interesting. Thanks for sharing.
I use a surface pro 4 with 16GB ram and a 256GB SSD, and run 2017 Developer edition of SQL Server just fine. if I need larger storage, I use external SSD hard drives. The only issue I run into is that SSMS and BIDS aren't optimized for high DPI screens, so sometimes the UI can look weird. 
I do this with the regex replace in SQL Server Management Studio all the time.
That's why I added a function to my Excel called sqlq (sql quote). Saved thousands of hours for me.
I’m in the same exact boat as you and very interested in any answers on this!
Googling "SQL tricks" took me to https://blog.jooq.org/2016/04/25/10-sql-tricks-that-you-didnt-think-were-possible/ which made some interesting stuff. Explainextended.com has a variety of extremely clever ways of using SQL (print out a Mandelbrot set?) . There's another that provides a solver for a Sudoku, entirely in SQL. All of these will change the way you view data, SQL and how you can use it to extract meaning.
Learn about database administration. I started by learning how to interpret a query plan, super useful for a number of reasons: - Teaches you the intricacies of merge, loop and hash joins. - Learn to speed up queries without changing code (or going straight to an index) - Teaches you about health checking SQL Server. Start by watching [Brent Ozar tune queries.](https://sqlbits.com/Sessions/Event12/Watch_Brent_Tune_Queries)
Knowing languages like T-SQL, c#, python would probably help. SQL is great but working with data at a more granular level requires other approaches outside of set logic to gain efficiences.
Learn merge, hash, and nested loop joins. Learn to read an execution plan. Learn how indexes work. Essentially, of your SQL knowledge is plateauing, learn database things, and you'll find you can apply SQL more effectively.
I would recommend learning how to analyse your data in Power BI/SSRS, that would also prompt you to develop your SQL skills further.
https://www.essentialsql.com/get-ready-to-learn-sql-server-19-introduction-to-sub-queries/ Check around on this site. Lots of concise articles about SQL concepts such as subqueries, CTEs, etc.
Use DISTINCT in a subquery? I'm not sure what your schema looks like, or which SQL syntax you're using, but something like this is what I would be thinking: SELECT athletes.Name FROM Athletes JOIN ( SELECT DISTINCT Competes.Athlete_Id FROM Competes JOIN Events ON Competes.Event_Id = Events.Id HAVING COUNT(DISTINCT Competes.MedalWon) &gt; 1 ) MultipleMedalAthletes ON MultipleMedalAthletes.Athlete_Id = Athletes.Id
That's aliases, the basic structure of the query is ```sql select field1, field2, ... fn(...) as field3, ... from table1 as t1 inner join table2 as t2 on ( t1.x = t2.y ) where true and t1.q in ( ... ); ``` Using aliases you can (re-)name fields (e.g. those that the result of a function call) and tables (handy when sorting out which field is in which table and when formulating the join clause).
&gt; How to Count things without aggregate answer -- you can't
E and H are table aliases, and you really really should attach them to the columns in the SELECT clause too SELECT Code , FirstName , LastName , MONTH(BirthDate) AS BirthMonth , DAY(BirthDate) AS BirthDay FROM dbods.dbo.tDimEmployeeHierarchy AS H INNER JOIN dbGod.dbo.tSAPHREmployee AS E ON E.Code = H.EmployeeNumber WHERE H.RegionEmployeeName in ( 'smith john' , 'smith Jane' ) AND H.EndDate &gt;= '2999-12-31' ORDER BY BirthMonth , BirthDay
I avoid single letter aliases or the dreaded t1, t2 t3 etc. I use acronym aliases. As an example a query that uses 4 tables named: stu_acad_rec, id_rec, stu_finaid_rec and prog_enroll_rec Alias as: sar, idr, sfar, per
for instance I have a question regarding the syntax of http://www.dofactory.com/sql/join where he is using SELECT O.OrderNumber, CONVERT(date,O.OrderDate) AS Date, P.ProductName, I.Quantity, I.UnitPrice FROM [Order] O JOIN OrderItem I ON O.Id = I.OrderId JOIN Product P ON P.Id = I.ProductId ORDER BY O.OrderNumber &amp;nbsp; however Orderitem o is not in any table, is he just creating a alias with order id and orderitem is the alias? I dont see it in the tables provided 
I think you could do a self-left-join and use an `on` clause a la `on( a.country = b.country and a.year != b.year )`, combine that with `b.anyfield != null`. The left join makes all records from the left table stay in the result, and all columns from the same table aliased as `b` will be set to `null` in case there were no matches.
OrderItem in this case in the table name he is joining on, not a column. 
You're working with mssql right? 
I can be on around 1:30 PM EST
1:30 PM EDT happens when this comment is 50 minutes old. You can find the live countdown here: https://countle.com/168642wrIU --- I'm a bot, if you want to send feedback, please comment below or send a PM.
I'd try something with a subquery that utilized ROW_NUMBER and the main query filtered out anybody with a ROW_NUMBER &lt;= 2. You'll have to join that subquery to your other tables to actually pull the data you want to see.
sending you a PM! 
Yup pm Sent! Thank you! 
you have a `count()` in your query which is an aggregating function
What do you know and what do you want to do in the future? There are a lot of different paths you could take. BI/Report Dev? Data Analyst/Scientist? Big Data Analyst? Browsing the [MS Certifications](https://www.microsoft.com/en-us/learning/browse-all-certifications.aspx) (pick 'Data' in the middle filter) is a good place to start. You can also look at job listings that interest you and see what skills they require that you're currently lacking.
Challenge accepted. SELECT GroupByField , [Count] FROM (SELECT GroupByField , [Count] , ROW_NUMBER() OVER (PARTITION BY GroupByField ORDER BY [Count] DESC) AS RowNumber FROM (SELECT GroupByField , ROW_NUMBER() OVER (PARTITION BY GroupByField ORDER BY (SELECT NULL)) AS [Count] FROM table) AS x) AS x WHERE RowNumber = 1; 
Additionally, check out [Sentry Plan Explorer](https://www.sentryone.com/plan-explorer). I think it's better than SSMS Query Execution Plan. 
&gt; MVNAME VARCHAR(999) := 'MV_WWORK_V1'; &gt; MV := 'CREATE MATERIALIZED VIEW' || MVNAME || You're missing whitespace after the word "VIEW" 
I fixed that but now I get: *Cause: identifiers may not start with any ASCII character other than letters and numbers. $#_ are also allowed after the first character. Identifiers enclosed by doublequotes may contain any character other than a doublequote. Alternative quotes (q'#...#') cannot use spaces, tabs, or carriage returns as delimiters. For all other contexts, consult the SQL Language Reference Manual. *Action:
So dump out the value of what you’re trying to execute. I don’t recall the Oracle syntax to do that - I’ve not worked with Oracle for about ten years. 
I've tried but it doesn't work. I've used: dbms_output.put_line( MV || 'SELECT /*+ PARALLEL */ WW.WORK_STATUS, WW.WORK_DATEINITIATED, WW.WORK_OWNERID, WW.WORK_WORKID FROM WWORK WW WHERE WW.WORK_STATUS &gt; 0;'); and the result is: 'PL/SQL procedure successfully completed.' Which doesn't tell me anything 
The question specifically is to find all nationalities that won at least two gold medals in a specific year. The condition is "Do not use aggregation (GROUP BY)"
Thanks guys, I will try out the things you mentioned.
I figured it out, https://stackoverflow.com/questions/7887413/printing-the-value-of-a-variable-in-sql-developer You need to turn on dbms_output capabilities.
The minimum is 4 core licence. So two of the 2 pack. Of course you need to licence all the core of the server. If you are on a vm then just give it 4 cores.
1) If you don't mind me asking, why are you trying to create a materialized view from inside an anonymous block in the first place? In a real environment this would raise all sorts of flags for bad practice. 2) Secondly, you should remove the last semi-colon before the apostrophe in your select statement. 3) [Please user VARCHAR2 instead of VARCHAR when you declare your variables](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/sqlrf/Data-Types.html#GUID-DF7E10FC-A461-4325-A295-3FD4D150809E) 3) To see the output of DBMS_OUTPUT.PUT_LINE you may need to issue this command first before executing the block SET SERVEROUTPUT ON But depending on which IDE you're using (e.g. SQL*Developer, PLSQL Developer, TOAD etc.) there's usually a tab called Output that you can switch to. 
Do you mean SELECT name FROM users WHERE name NOT LIKE "John Doe";
Do you mean `select ingredientname from ingredients where ingredient like "%chicken breast%";`?
Probably looking for a spin on the IN operator https://www.w3schools.com/sql/sql_in.asp
It's not necessarily a bad thing if you only run them once on each instance, although in principle dynamic DDLs should be avoided whenever possible. One other thing to consider would be if you have any sort of versioning control system (VCS) put into place then IMO you may want to have different files for each MV per environment for historical purposes instead of 1 script that handles everything.
You would have to use the catalog tables. Like dbA_columns in Oracle 
Started as a data analyst and worked my way to BI analysis. I've thought about venturing towards the DBA route but I feel more suited with wrangling data sets ,reporting and that side of things .. My goal direction is data science.. but still need to beef up with statistics and most likely R once Im ready. ets
Reporting is what I do now and am gearing up towards R once I feel more comfortable with advanced stat (predictive) Tabalau seems cool too but not sure of the learning curve. Do you know if it's as steep as R or easier to get the ball rolling ..
Sounds like what you need is some kind of fuzzy matching algorithm. LIKE will not work for this. It is possible in SQL, but not widely supported. Some DBs have SOUNDEX function, which has drawbacks. If you Google for fuzzy matching, you should find a lot of example code where people have rolled their own functions or stored procedure.
It just looks like this https://i.imgur.com/gSbXqkM.png where as the sql file just looks normal. 
Adding all of that to my goals this year. I like that perspective and feel that will definitely help me out. Do you have any books or sites that helped you ? I'm looking at Itzik Ben-Gan T-SQL Fundamentals or Querying books
Thanks for that! I'll definitely be diving into that
I don't think this would work for the example he gave, but yes its obviously possible without a COUNT(). I think OP would need something like select distinct country from ( select a.country , row_number() over (partition by a.athlete order by a.athlete) as 'rn' from athletes a inner join events b on b.id = a.id inner join competes c on c.id = a.id where b.year = '1901' and constraint in ('Gold', 'Silver', 'Bronze') ) where rn &gt; 1
I would start by rethinking the way you are doing things, locks happen because two things are trying to do the same thing at the same time to the same data. When that is user based; two people trying to work on the same record it's understandable though can still be worked around. When it's part of the process however one must look at the sanity of the dev.
There is no one answer here. For one thing there are many types of locking, and various approaches are used based on what the characteristics of you business logic are. Do you really need to update the exact same rows you are reading? For example, what if every row had a surrogate key, timestamp and IP address field? You fetch the most recent row for a given IP address and possibly other parameters, create one if none exists, then after modifying your data you save a new row? You have redundant data, but at least you are not writing to a locked record. I'm not saying that's your answer, I am pointing out that certain types of transactions [OLTP systems, versus OLAP](http://datawarehouse4u.info/OLTP-vs-OLAP.html) or write heavy versus read heavy dictate how to treat your data. In very write-centric situations you may want to just append a file. The approach depends on context. 
 Do you get a specific error message? Most sql engines will block until they can acquire the lock they needed. This includes sqlite. However with sqlite make sure you have a connection on each thread and make sure they are not shared between them.
I suggest building a table of ingredient names you encounter in the wild and mapping them to the ingredient names in your database.
It is easy to get started and hard to master. Also everything feels like a hack. I love it but remember it is a visualization tool. 
&gt; I have a websocket connection pulling in significant amounts of data, essentially at a constant rate (several times per second). Are you talking 10s of txns per second? That's not a lot. You could try using a DBMS that supports multiple writers (Postgres, MySQL).
&gt; An EXCLUSIVE lock is needed in order to write to the database file. Only one EXCLUSIVE lock is allowed on the file and no other locks of any kind are allowed to coexist with an EXCLUSIVE lock. [SQLite locking](https://www.sqlite.org/lockingv3.html) That's part of SQLite architecture. You can't change that. (For all practical purposes.) The usual way of dealing with this kind of thing is to start with a client/server dbms, like PostgreSQL. You might also need to change your application architecture. Maybe use something like thread-safe queues. 
As a more cost effective option, maybe look at Azure SQL and Power BI?
so, not very familiar w/ SQLite... but from the comments it sounds like a file-oriented DB for single-threaded use. you *could* just cycle files around... have an empty template database... create a copy, fill it up, rename it... use the renamed file for batch processing into a final resting location. not ideal... a far better solution involves a DB that better handles concurrency and locking... but perhaps an easy interim solution.
&gt; Any time the db gets locked, I end up losing data. Are you using transactions (BEGIN TRANSACTION)? Are you doing COMMIT at the end of your transactions (either `connection.commit()` or `cursor.exec('COMMIT;') ? If you are losing data, it sounds to me that you are using transactions but that you get into a deadlock (two transactions waiting for each other), and since no commit is done, those pieces of data never get written. Also, if you are using several processes / threads to write to a DB, I'd strongly suggest you use a DB server such as PostgreSQL. You can do it with SQLite, but it wasn't meant for this, and necessitate some work (setting time-outs, ensuring you are on a filesystem that allows file locking etc...). Even a default Postgres install will do at least as well as SQLite, the only added burden will be to do proper backup (just copying files won't be good enough any more).
Technically `DISTINCT` is an aggregate.
Maybe write ahead logging. http://www.sqlite.org/wal.html 
SELECT * FROM color_table co, crayon_table cy WHERE co.color1 = 'GREEN' AND cy.pencil = 'GREEN';
Cheers mate! But what purpose do these aliases serve and when should I be using them? And when not to?
This has definitely been a learning experience, I wouldn't say I'm insane though :P
I haven't measured the exact volume of txns but its probably 10-30 messages per second. There are two types of messages, and one needs to be processed before it's ready to be written to file, which is where my dilemma comes in. I used threads because I needed this second type of data to be aggregated at exact 1-second intervals, and the thread was the only way I saw to do that with any reliability. Would it be bad design to just keep them in separate db files and join them when I'm ready to analyze the data?
So `where rn &gt; 1 and rn &lt; 3`? 
I think this works for OP. SELECT a.Country FROM (SELECT a.Country , a.[Count] , ROW_NUMBER() OVER (PARTITION BY a.Country ORDER BY x.[Count] DESC) AS RowNumber FROM (SELECT a.Country , ROW_NUMBER() OVER (PARTITION BY a.Country ORDER BY (SELECT NULL)) AS [Count] FROM Athletes AS a INNER JOIN Competes AS c ON c.AthleteID = a.AthleteID WHERE c.Medal IN ('Gold','Silver','Bronze') AND c.Year = 2008) AS a) AS a WHERE a.RowNumber = 1 AND a.[Count] &gt;= 2;
I'm a little confused by what the columns are in color_table and crayon_table. What columns do you want in the final table? Instead of select *, you could do something like: select ID, Hex, RGB, value, color1, '' as pencil from color_table UNION select ID, Hex, RGB, value, '' as color1, pencil from crayon_table 
queries are easier to write... and a lot easier to read
Well, lets suppose there are different columns in those 2 tables, and only a BRAND column in both tables. But, the one I left as an example doesn't have a join and a where, which has the result I need, then I want that result to be combined into 1 single table. The problem is that if I use UNION, for instance it would mix everything from both selects into BRAND for example. For instance I need something like this: Result1 from color_table: |Brand| Crayola Stadler Result2 from crayon_table: |Brand| Demo_crayon Unbranded If I make a union with those two results it will result like this: |BRAND| Crayola Stadler Demo_crayon Unbranded Meanwhile, what I need is something like this: |Result1_BRAND| Result2_BRAND| Crayola |Demo_crayon Stadler |Unbranded Perhaps that's more clear than my example?. Thanks!.
So if you do something like this: Select color_table.brand as brand1 null as brand 2 UNION select NULL as brand 1 crayon_table.brand as brand2 You will get something like this Result | Brand1 | Brand2 ---|---|---- 1 | Crayola| NULL 2 | Stadler| NULL 3 | NULL | demo_crayon 4 | NULL | unbranded 
I've had five beers and am nowhere near a pc but I think this is roughly what you want? with crayon1 as ( select rownumber as rn, * from colour_table ) , crayon2 as ( select rownumber, as rn * from crayon_table ) select a.*, b.* from crayontable1 a, crayontable2 b where a.rn = b.rn 
Yup. Thanks pal!. that worked. Cheers!. Have another one! :D
Hi, I just had to write a similar query, hopefully it's not too late to be helpful for you. DECLARE @XmlData XML = '&lt;data&gt; &lt;employees&gt; &lt;employee&gt; &lt;id&gt;10&lt;/id&gt; &lt;jobcodes&gt; &lt;jobcode&gt; &lt;jobcodeid&gt;8&lt;/jobcodeid&gt; &lt;payrate&gt;0.0000&lt;/payrate&gt; &lt;/jobcode&gt; &lt;/jobcodes&gt; &lt;/employee&gt; &lt;employee&gt; &lt;id&gt;14&lt;/id&gt; &lt;jobcodes&gt; &lt;jobcode&gt; &lt;jobcodeid&gt;300&lt;/jobcodeid&gt; &lt;payrate&gt;0.0000&lt;/payrate&gt; &lt;/jobcode&gt; &lt;jobcode&gt; &lt;jobcodeid&gt;400&lt;/jobcodeid&gt; &lt;payrate&gt;11.0000&lt;/payrate&gt; &lt;/jobcode&gt; &lt;/jobcodes&gt; &lt;/employee&gt; &lt;/employees&gt; &lt;/data&gt;' SELECT employee.value('(/employee/id)[1]', 'INT'), jobCode.value('(/jobcode/jobcodeid)[1]', 'INT'), jobCode.value('(/jobcode/payrate)[1]', 'FLOAT') FROM ( SELECT employeeData.query('.') FROM @XmlData.nodes('data/employees/employee') AS XmlEmployees(employeeData) ) AS Employees(employee) OUTER APPLY ( SELECT jobCodeData.query('.') FROM employee.nodes('employee/jobcodes/jobcode') as XmlJobCodes(jobCodeData) ) AS JobCodes(jobCode) Doing this breaks it out into a more queryable format. Let me know if you have any further questions
What have you tried so far? You can definitely write "where ingredient like '%chicken breast%'" . . . why not? 
What do they say they’re looking for?
Do you have a job or are you looking? Like, if you already have a role, what does it entail?
I imagine you would be running queries and not being a DBA. I’m just a beginner but I think YouTube courses would suffice.
Start with codeacademy
If you already have an offer than there's no need for a course, certifications are useless. You'll most likely do a lot of search queries, just watch some tutorials on youtube. Once you have a grasp on relational databases and how to do joins, you're good, learn the rest while you're working.
Dont have a job but it says in the description theyre looking for someone whos familiar with sql and handling large sets of data. Im proficient in excel and vba but yeah. And yes im looking for a new job.
No offer but its a requirement for a lot of the jobs im looking into.
Just a guess, but some SQL implements don't allow billable foreign keys
My job required me to know SQL, I did not know any. You can learn SQL on the fly. The toughest part isn't SQL, it is understanding your database.
So I should try making them "NOT NULL" instead?
But how do i tell them that i could learn it if i dont have any prior experience? I know i have no provlem learning.
I would learn enough to talk about it in the interview. Learn your joins. Inner, outer,full Do you have any knowledge about database?
Shouldn't you create Location and Category before referencing then in inventory and discarded? 
Yeah I realized that could be an issue, but I moved them and still go the same errors.
I’m a business analyst in the financial sector and my company paid for me to do a pl/sql course before starting the job. I can tell you it’s quite easy to learn the basics of sql queries (if you have any programming experience), all you need is some practice with a mock database, I’m sure there are some decent free online courses that will do. As others have said on this post, the hard part is understanding the database you are going to work with..! Good luck and don’t drop any tables!
I have a lot of experience with Chart.io, Periscope, and Mode. I think Chart.io is generally the best for dash boarding. If you have specific questions about the 3 I can answer.
You sound very much like me. Was proficient with Excel and VBA, very little awareness of SQL. W3schools is a useful online resource, and when you've got your basics nailed down, use StackOverflow (or here!) for the more complex questions. Google is always your friend! (Good luck with the job, btw).
Select/ From/ Where. Learn that, and all relational dbs are yours.
The mysololear app has a sql course. I used it to get started. After that, you can google the rest. Like most have said, learning your database is the biggest part.
Close! Select USERID, SUM(CASE WHEN TransType = 'A' THEN Quantity ELSE 0 END) AS [A], SUM(CASE WHEN TransType = 'B' THEN Quantity ELSE 0 END) AS [B] Or you could use PIVOT SELECT * FROM (SELECT UserID , TransType , SUM(Quantity) AS Quantity FROM Transactions GROUP BY UserID , TransType) AS t PIVOT (SUM(Quantity) FOR TransType IN ([A],[B])) AS p;
http://www.sql-ex.com/ Has some good exercises you can practice. Also, sqlzoo.com
You can hold Alt+Shift in SSMS to apply a cursor to multiple lines. If you have say 1000 rows, place your cursor at the very top next to the first string in your list, scroll to the very bottom of the list, hold alt + shift and click the end of the last string; A line while appear and when you type it will enter it on each line.
It’s dialect dependent, if you are onto MS SQL, then you have T-SQL, Oracle – PL/SQL and so on and so on.
2 packs of two-core licence + Power BI. However it's too expensive.
Not for a financial analyst. An FA will probably just be running select queries. I wouldn't let an FA anywhere near SSIS. 
I agree, the MS SQL cert isn't a bad idea as OP would theoretically learn the material studying for it. The 70-761 and 70-762 would be good ones to test on. I personally feel it would be insanely difficult to pass with 0 experience however, I'd foresee a ~1 year study plan to ~2 years to go from 0 experience to passing both comfortably and actually understanding the material. Maybe I'm wrong on the timeline, but I feel like I need a good 2-3 month study session at 30 min to 1 hr a day to pass each one. 
I've found in larger queries that I'd rather have a union. It's faster than using case statements. SELECT USERID, SUM(Quanity) AS Quantiity, TransType FROM Transactions WHERE TransType = 'A' GROUP BY USERID, TransType UNION ALL SELECT USERID, SUM(Quanity) AS Quantiity, TransType FROM Transactions WHERE TransType = 'B' GROUP BY USERID, TransType
How much does Postgres cost?
Unless you want to be a DBA, then certificates are not too common to need in the US for a database or BI developer role. You might try to get a "database analyst" job at a junior level and express interest in learning ETL on the job.
Union concatenates two rowsets and OP wants to have results for each transaction type in columns. You can either use `CASE` (a scalar op) or join two rowsets, `UNION` is of no help here I think. &gt; sorry, I suck at formatting. Select all the code you want to format, click on the fourth button above the comment box, the one with `&lt;&gt;` icon, it will indent for you.
Case statements aren't slow. I hear it quite a bit, but it's almost never the case.^^1 However without a perfect index, reading a table 3 times instead of once would be slow. --- *1:* In out environment we have a single case statement I consider to considerably slow down a query. Well, it's more the string manipulation on a character map (like a bitmap but with letters) inside the case statement that's slow. On the other hand, [this monster case statement](https://pastebin.com/WZvmrWjc) is way faster than alternatives to deduct holidays from a datediff.
Thanks for your reply, I was looking at jobs like that but they require me to have experience I don't have. Is it your impression that hiring managers understand that Jr data analysts can learn on the job?
Or full text search, either of those are probably alright. I'm trying to think of ideas that use indexes and can scale appropriately. Pre-calculated SOUNDEX can be good.
Some hiring managers are terrible and are very rigid in their requirements. Some are more reasonable - if a candidate is an expert SQL writer, it's not a far stretch to realize that he or she will be able to learn ETL. That said, Microsoft SQL Server Developer Edition is now free. I would suggest that you download it, and work through a few projects using SSIS (its built in ETL tool). Post some sample code up to github. Sign up for a Tableau public account, and create some data visualizations off of the data you produce. Use it to list in your resume as a beginner level skills. No candidate ever has all requirements listed for anything.
Awesome suggestions, thanks for the pointers.
&gt;Take that, MS crowd!
I _think_ that GROUP BY GROUPING SETS would get what you want: SELECT TransType, SUM(Quantity) FROM [table] GROUP BY GROUPING SETS ( ( TransType ), () )
&gt; Songs.ID IN ( SELECT CAST(SUBSTR(MAX(SUBSTR('00000000000000000000' || CAST(RANDOM() AS VARCHAR(20)),-20) || CAST(ID AS VARCHAR(20))),21) AS INT) AS ID FROM Songs GROUP BY Artist) It says "Mask is not valid." I think it's very picky about what terms are used since it's a simple plugin for Media Monkey. [Here is the instruction manual with a quick explanation of how it works](http://west-penwith.org.uk/misc/MagicNodes.pdf)
You're going to have to debug your implementation, but [the code works in SQLite.](http://www.sqlfiddle.com/#!5/da66d/5)
I still think any kind of cert is way over the top for a basic analyst who just needs a job with 'some SQL skills'. 
You don't need to have formal coursework or a certification. Learn however much you need to so that you feel comfortable honestly putting it on your resume in some way. Even if this means that your resume includes something along the lines of 'light SQL experience'. If you get into an interview make sure you set the right expectation about any skills listed on your resume like this. From the mouth of a VP - "I know this job won't require (insert random skill here). I put that on there so that I have an HR friendly reason to reject any candidate." All that being said, check out this book: https://www.amazon.com/SQL-Minutes-Sams-Teach-Yourself/dp/0672336073 It should get you a good enough understanding to be able to talk about SQL in an interview (assuming you level set with them correctly).
Learn the basics querying like Select statement, Where clause, inner join, left join and then use Power BI.
I'm afraid I cannot figure it out :(
Well I can speak to one thing that you said. Some people might not agree with this but I believe that if you can write a complex query in SQL, you can do pretty much anything in SQL. You mention that you don’t know how to transform data etc. by the hardest part about that is choosing the data to transform while protecting the data integrity and relational logic. The way you do that is through query logic. 
 SELECT 100.0 * SUM(CASE WHEN Alive = 'True' THEN 1 ELSE 0 END) / COUNT(*) AS percentage FROM Table
Would you be able to share how to.solved the problem? I remember looking into it a while back and couldn't find a solution.
So you have to pay one of there consultants to pass on the trade secrets. 
yeah, it isn't too bad, but here is what you do. I"m doing this off the top of my head, but it should make sense once you are looking at everything 1. Click on data driven subscription 2. select data source and keep hitting next 3. The most important step is where you can enter a query. A. Enter your query B. Get rid of everything in the select statement and replace with who you are sending the email to. C. For example, 'Bob@123.com; Jack@2434.com' To_Email D. Keep following prompts and it should make take you to the screen to schedule the subscription. I was utterly dumbfounded that I could not find this anywhere at all.
very true. Luckily, I think my co worker learned this in another role somewhere else so he was able to pass this on to me. 
I'm not sure if it's still the case, but data driven subscriptions were only available with the Enterprise version of SQL Server 2008, so their usefulness might be restricted to some people.
The more you use Data Driven Subscriptions, the more you'll hate them. Generating 1000 items in one subscription? 998 of 1000 succeeded. Oh you want to know which 2 failed? LOL go look in the massive log files and sort it out. Got to the point where we use C# to generate in batches out of SSRS just so we can track the errors and automatically sort them out or retry them.
Why is it necessary to put a 100 * in front of the aggregation functions? I just tried something very similar. When I do: SELECT COUNT(DISTINCT(table1.owner_id)) / (SELECT COUNT(table2.id) FROM table2) FROM table1; I'm returned a 0. Both of these queries when ran alone will provide correct numbers. When I put them together with an operator of '/' then the output is zero. If I put in a '100 *' right before the first count, then it works correctly: SELECT 100* COUNT(DISTINCT(table1.owner_id)) / (SELECT COUNT(table2.id) FROM table2) FROM table1; Does this have something to do with the fact that it's initially returning an integer or something that is less than 1 so it rounds down? In this specific case, the result should be 80, but before multiplying by 100 it's only .8 obviously. Any help on this? I think it tripped me up before.
counts are integers and dividing two integers gives you an integer 45 divided by 73 is 0 if you use integers multiplying by 100 gives a percentage, not a fraction but the real secret is multiplying by 100.0 because you are now using decimal arithmetic, and decimal results are not rounded to the nearest integer
I've used some custom change tracking which does something similar. The change tracking only stores the values which changed; everything else is null (and stored as sparse columns). Rebuilding the state of a record at a particular time would recursively coalesce any change records up to the current record, picking up values in chronological order. Nulls were handled with a bitmap. I would consider changing the recursive logic to an aggregate if designing it again today, but I was very happy with the storage efficiencies.
I've used delta tables that show column, CUD operation and before/after values, if applicable. This was common in pre-RDBMS days with quasi dbs like DBase. Now I just capture everything with sql trace and app-specific logs.
Thanks so much! &lt;3
 SELECT * FROM table1 WHERE EXISTS (SELECT * FROM table2 WHERE table1.name LIKE '%' || table2.name || '%');
I meant to thank you for this; it was super-helpful. Thanks so much for taking the time!
&gt; If I made Product_ID and Ref_Period a composite primary key, would I still be able to make PRODUCT_GROUPS.Product_ID (and wherelse Product_ID appears) a foreign key to PRODUCT.Product_ID? only if you include Ref_Period in PRODUCT_GROUPS
no, you could use a [surrogate key](https://en.wikipedia.org/wiki/Surrogate_key) instead
**Surrogate key** A surrogate key (or synthetic key, entity identifier, system-generated key, database sequence number, factless key, technical key, or arbitrary unique identifier) in a database is a unique identifier for either an entity in the modeled world or an object in the database. The surrogate key is not derived from application data, unlike a natural (or business) key which is derived from application data. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
String concatenation.
I was just caught up in general implementation and instead of quantity have duplicate rows of order_id and iteM_id and then you get the quantity using count(*)
So the only data in the relationship table is the relationship itself. 
Hm, here I'd probably call it `OrderDetail` instead of `Orders_Items`, as it's much more strongly linked to `Order` than `Item`. For example, I would also probably include an item price per unit in the `OrderDetail` table because the price of an Item can change after the Order has been placed or completed and you probably need that history to generate invoices. By default, your primary key should be a composite key on `OrderID, ItemID` because that is your natural candidate key. However, some people would also favor a surrogate key in the `OrderDetail` table, especially if your business rules might require the same item to be on an order multiple times. There would be other ways to cover this, so it might depend on your business rules. 
I think the classic example of many-to-many is students in classes. A class can have multiple students, and a student can have multiple classes. Then you have a relationship table (something like Enrollment) that stores a student ID and a class ID. You wouldn't have multiple entries of the same student in the same class. That way you can have student A, B, and C in class 1, and student C and D in class 2. I personally always really liked that example and felt that it made many-to-many relationships very clear and succinct. I think I misunderstood your question, before (then somehow managed to delete my post while trying to edit it accordingly). I think the industry standard is usually an "OrderDetail" table, that would store both an Order ID, an Item ID, and a quantity. I would think there should never be a need for multiple lines in a relationship table of the same compound ID (not unusual to even see a compound primary key for the OrderDetail table of the Order ID and a Item ID, but that's a different topic). Sorry for any confusion!
The two queries are equivalent, and will run against the database with the same execution plan. SQL is an interpretive language, and any sensible database engine will deduce that the queries are doing the same thing and execute them in the same way. Note, it is possible to write two queries that return the same results, yet are interpreted differently. However, that is not the case here.
The answer depends on what database you are using; Oracle, Postgres, MSSQL, Sqlite, etc...
the idea is to match multiple records from order to multiple records from item. (like: an order has many items, and an item can be ordered in many orders) the name "item" might not be the best here, because usually an "order item" refers to a single record. what you really mean might be a "product". regarding your question: the theory is that the Orders_Item table contains a combined PK which's fields are ItemID (FK to item) and OrderID (ID to order). so the PK is (ItemID, OrderID) so if you set your table up correctly, you would even not be allowed to insert multiple records with the same IDs, because you cannot insert multiple records with the same PK! to get around this, you'd need to create a surrogate key. but still, i'd go with the approach to add a quantity column, because the "convention" is that each record represents an identifyable thing. if you'd have equal records, you'd not be able to clearly identify a single record (because the surrogate key is artificial, with no semantic meaning). the only pro for the scenario without a quantity column i can think of would be: performance reasons. imagine if you have trillions of Orders_Items with quantity 1, and just a couple of Orders_Items with a higher quantity. then you could save some disk space if you get rid of the quantity column and use the "duplicate record" approach. but really, this must be a very special scenario.... that being said: you'd HAVE TO add multiple records, if you'd have additional (identifying) attributes in your Orders_Item. for example, if your products/items come in different colors, and you want to store that in your DB, you might have the following three colums in your relationship table: (ItemID, OrderID, Color). The primary key would be a combined key of ALL THREE colums - or you can of course create a surrogate key (because it's easier to handle 1 key column than to handle 3 ... think of joins, etc.) one last thing: generally it's a good idea to design your tables defensively. that means: restrict as much as you can. if you create a surrogate key, then create a UNIQUE CONSTRAINT on the other colums (if your RDBMS supports that)
Sorry! Using MSSQL. Also if it's easier, something I just thought of. I don't necessarily need it in datetime format. WOuld it be possible to simply parse the data as needed? Adding dashes and colons after x amounts of digits will properly format this column the way I need it.
Looks like pl/sql to me...not sure why you're using varchar instead of varchar2. Do you want someone to code this up for you or do you just want some example that helps get you going?
Here's my long-winded approach. I'm sure someone more experienced has a better way. declare @x varchar(max)= 20180403121800.000000 select convert(smalldatetime, concat(convert(date, left(@x, 8)),' ', convert(varchar(5), substring(@x, 9, 2), 8)+':'+ convert(varchar(5), substring(@x, 11, 2), 8)))
Either way is fine, really. My brain is 100% blocked atm. Need to kickstart it with either the solution or any example :)
 DECLARE v_person_num NUMBER := 1; v_flight_number NUMBER; v_seat_number VARCHAR2 (512); PROCEDURE check_flight (in_person_num IN NUMBER, out_flight_number OUT NUMBER, out_seat_number OUT VARCHAR2) IS BEGIN SELECT 377 INTO out_flight_number FROM DUAL; SELECT '12A' INTO out_seat_number FROM DUAL; EXCEPTION WHEN OTHERS THEN RAISE; END; BEGIN check_flight (v_person_num, v_flight_number, v_seat_number); DBMS_OUTPUT.put_line (v_flight_number || ', ' || v_seat_number); END; 
Awesome, thanks :)
Are the web server and SQL server on the same machine? I'm a little confused about your setup... Are you able to connect to the new SQL server with Management Studio? Are you getting errors from the new Web Server, if so what are these errors?
Do you mean sub-query refactoring? I believe that is correct, the data types are derived from the base tables unless you cast/convert in your sql statement. I think I get what you're asking.
Yes, the resulting data types in a CTE are going to be the same as would be returned by the same query run by itself - generally this means the data types will be inherited from the table. Incidentally this is the only way of doing a CTE. You can't do a CREATE TABLE or similar to declare a CTE's data types, only a select query.
Appreciate you're input, though you can declare a CTE table and define the data types, as well as constraints. Example: DECLARE @TEST_TABLE TABLE (ColumnA INT NOT NULL) INSERT @TEST_TABLE VALUES(1) SELECT * FROM @TEST_TABLE
I do believe so. Thank you. Question came up when using a CASE statement for a column in the select where the result could come from two different columns with different data types. 
SELECT ISDATE('your string') before doing to prevent a crash.
That's not a CTE, that's just a table-valued variable. All CTEs are declared in the WITH block at the start of a query.
What a lifesaver, thank you!
The compiler will work out which data type to use. It will look at all of the types that the case can possibly output, then pick the one with the [highest precedence](https://docs.microsoft.com/en-us/sql/t-sql/data-types/data-type-precedence-transact-sql). 
'create index index_name_goes_here on customer (customer_id)' Creating your indices on columns in your join clause for each of the tables listed usually yields the best performance gains. 
u/fozzie33 had a much more elegant solution but you could also have used substring functions to only compare the part of author prior to the comma to %a% WHERE left(author, charindex(author, ‘,’)) not like %a%
Oh never mind; I might have misread your tables. Sorry man its been a long day.
&gt; I need to create an INDEX ( create index on ... ) but not sure how to go about it since there are so many clauses. Where do i start? With the execution plan.
And ANSI 92 joins.
What would be the proper syntax? Just create Index for every parameter?
FROM CUSTOMER, PURCHASE, PRODUCT WHERE CUSTOMER.CUSTOMER_ID = PURCHASE.CUSTOMER_ID AND PURCHASE.PRODUCT_ID = PRODUCT.PRODUCT_ID AND PRICE &gt; 1000 —————— FROM CUSTOMER INNER JOIN PURCHASE ON CUSTOMER.CUSTOMER ID = PURCHASE. CUSTOMER ID INNER JOIN PRODUCT ON PURCHASE.PRODUCT ID = PRODUCT.PRODUCT ID WHERE PRICE &gt;1000 I replaced your underscores with spaces since I’m on mobile and it tries to format.
This won't explain your problem, but why not simply name your new server the same thing as your old server, i.e. clone it? Should solve your problem. We did this once before for a similar reason when we couldn't figure out why a new server wouldn't work with existing technologies.
https://i.imgur.com/eCdY6m7.png Axi its the link to the assignment that I have and what Im trying to solve.
 WHERE tab_col IN ( SELECT tab_col...
This maybe helpful http://www.bkent.net/Doc/simple5.htm
Yeah just slap indexes on the columns in the joins as said above: CREATE INDEX *IndexNane* ON PURCHASE (Purchase_ID) TABLESPACE *TableSpaceHere* ; https://docs.oracle.com/cd/B28359_01/server.111/b28310/indexes003.htm
 DECLARE DocTypeID NUMBER; BEGIN SELECT I’d INTO DocTypeID FROM id_table WHERE mnemo = ‘A’; INSERT INTO new_table (id, name) SELECT 10, name FROM anotherTable WHERE doc_type_id = ( SELECT tab_col FROM attr_tab WHERE doc_type_id = DocTypeId); END; As stated, use IN instead of = if you think the subquery will return more than one result.
Read our blog:- https://www.gyansetu.in/why-microsoft-excel-is-important/ 
No problem! You had me worried at first. The end goal of this database is to have it feed into a Power BI application. I'm not the one in charge of the Power BI side but it's my understand that with defined relationships Power BI can put pretty much everything together on its own. So, I don't expect to be doing very much (if any) querying of the DB itself. With that in mind, would I still be worsening my design?
Thanks for the link.
To be honest I'm not sure why we didn't, that decision was made by the more senior members of my office
As a side note, there are no columns that I can use as a secondary target for the query - making this much more difficult.
Use a `CASE`, only the first matched `WHEN` rule will be applied, so something like that maybe? I'm not sure I understand what you need. UPDATE MaintenanceTasks SET ParentMarker = CASE WHEN ParentMarker LIKE 'CT-1A%' THEN REPLACE(ParentMarker, 'CT-1A', 'CT-ROOF-01') WHEN ParentMarker LIKE 'CT-1B%' THEN REPLACE(ParentMarker, 'CT-1B', 'CT-ROOF-02') -- other CT-1s haven't matched WHEN ParentMarker LIKE 'CT-1' THEN REPLACE(ParentMarker, 'CT-ROOF-XYZ') -- Do not do anything for shit that doesn't match ELSE ParentParker END Better way would be to actually have a mapping table and update using a join but I understand you can't do that.
Okay, I'll Try that and report back. Thanks!
Not native to SSMS. You can use a free tool like Apex or Poor man's T-SQL that can help you capitalize and style your code. 
Could you replace 'CT-1-' with 'CT-ROOF-01-' ? That would avoid matching the CT-1 that's present in CT-1A. 
Thank you for finding occam's razor for me. I cannot tell you how much I appreciate it, as this simple recommendation perfectly fixes all of my issues with the automated script I created.
Sorry man, I was super tired last night and wasn't thinking straight. I shouldn't have been trying to answer your questions because I was clearly not all there. To go back to your original question: No, foreign key cannot reference a partial primary key. Since that doesn't work, can you improve your schema? Well, you can probably brute force it but the problem is that the only functional dependencies I can see here are: * Product_ID, Ref_Period --&gt; Product_Name * Group_ID --&gt; Group_Name, Group_Location The Product_Group table has no *functional* dependencies. I'm sure you've run into this issue: you cannot define a key for it, correct? It just holds arbitrary data for the purposes of joining two tables. In other words, the relation between GROUPS and PRODUCTS is *data-dependent*; not functionally dependent. If the data changes, so does the joining of PRODUCT, PRODUCT_GROUP, and GROUP. What does this mean? Basically, unless you are holding back some critical information, you cannot mathematically "improve" your schema. However, normally this wouldn't matter because querying it is trivial: SELECT Product_Name, Group_Name, Group_Loc FROM Product NATURAL JOIN Product_Group NATURAL JOIN Group And (unless I made some simple error) should return the results you want. Now, as far as Power BI is concerned, I have no idea. I have never used it before, so I can't answer any questions about it.
It is still Enterprise only.
Here is one that I wrote a few years ago. You should probably be able to deconstruct it for your purposes. DECLARE @DynamicPivotQuery NVARCHAR(MAX) DECLARE @ColumnName NVARCHAR(MAX) IF (SELECT object_id('tempdb..#temp')) IS NOT NULL DROP TABLE #temp SELECT @ColumnName = ISNULL(@ColumnName + ',','') + QUOTENAME([IssueState]) FROM ( SELECT DISTINCT [IssueState] FROM [BI_REPORTING_RO].Production.PremiumProduction WHERE [LeadSourceCode] LIKE '%X' ) A ORDER BY [IssueState] SELECT B.[LeadSourceCode] , B.[IssueState] , A.[Production] INTO #temp FROM ( SELECT [PolicyNo] , SUM([SetProduction]) AS 'Production' FROM [BI_REPORTING_RO].Production.PremiumProduction WHERE [LeadSourceCode] LIKE '%X' GROUP BY [PolicyNo] HAVING SUM([SetCount]) &lt;&gt; 0 ) A LEFT JOIN [BI_REPORTING_RO].LSP.Contracts B ON B.[PolicyNo] = A.[PolicyNo] WHERE B.[LeadSourceCode] IS NOT NULL AND B.[IssueState] IS NOT NULL SET @DynamicPivotQuery = ' WITH X AS ( SELECT [LeadSourceCode] , ' + @ColumnName + ' FROM #temp PIVOT(SUM([Production]) FOR [IssueState] IN (' + @ColumnName + ')) AS X ), Y AS ( SELECT [LeadSourceCode] , SUM([Production]) AS LSC_Total FROM #temp Group by [LeadSourceCode] ), Z AS ( SELECT ' + @ColumnName + ' FROM ( SELECT [IssueState] , SUM([Production]) AS [State_Total] FROM #temp GROUP BY [IssueState] ) X PIVOT(SUM([State_Total]) FOR [IssueState] IN (' + @ColumnName + ')) AS X ) SELECT X.* , Y.[LSC_Total] AS Total FROM X INNER JOIN Y ON Y.[LeadSourceCode] = X.[LeadSourceCode] UNION SELECT ''Total'' AS LeadSourceCode , Z.* , (SELECT SUM([LSC_Total]) FROM Y) AS Grand_Total FROM Z ' EXEC sp_executesql @DynamicPivotQuery 
I'm guessing this is a table with a date somewhere in each record, and the date can be any date and can repeat, right? If so, you can partition off of that date and get the latest date with respect to year and month. Here is an example that you can run demonstrating how that might work. with mytable as( SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL union all SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL union all [ and so on, for as many dates as you want to generate, in my test example I had 250 dates] SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL union all SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL union all SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL union all SELECT TO_DATE(TRUNC(DBMS_RANDOM.VALUE(TO_CHAR(DATE '2017-01-01','J'),TO_CHAR(DATE '2018-04-30','J'))),'J') my_dt FROM DUAL), mytable2 as ( select to_date(my_dt, 'dd-mon-yy') my_dt, row_number() over (partition by substr(to_char(my_dt,'dd-mon-yy'),3,100) order by my_dt desc) r from mytable) select my_dt from mytable2 where r=1 ; Result: my_dt --------- 30-Apr-17 28-Apr-18 30-Aug-17 30-Dec-17 27-Feb-17 28-Feb-18 28-Jan-17 27-Jan-18 30-Jul-17 27-Jun-17 31-Mar-17 31-Mar-18 29-May-17 30-Nov-17 30-Oct-17 28-Sep-17 In my test data the dates didn't necessarily hit the last day of the month every time. But it's looking at whats available and then selecting the highest date with respect to month and year. 
unless I am missing something, I think you are asking about the last_day function. This predicate should work for what you are asking for. select * from table where trunc(date_column) = trunc(last_day(date_column))
this works, but the current month will not have a last day. for instance my last day for april is the 3rd, not the 30th, where as last_day() will return the 30th
Even when I interchange the variables it still doesn’t work.
Thanks for your help! The food example I used wasn't actually what I'm working with. The rule makes more sense when it's not in this food example context. I appreciate you taking your time to help me out!
No problem! Happy to. 
Thanks for helping pointing me in the right direction! I had a feeling the query would involve a group by clause but I couldn't get it right. I appreciate you taking your time to help me out!
Thanks for the reply. I think I did some rubber-duck debugging here while trying to make my own reply. (I was wondering if the data-dependency was because the mapping between product-group could change but I realized that a product may change description enough to justify it being in another group) I learned today that the PRODUCT_GROUPS table isn't fixed as I thought and so I will have to include a date column. PRODUCT_GROUPS +----------+------------+------------+ | Group_ID | Ref_Period | Product_ID | +----------+------------+------------+ | 3500000 | 201803 | 1000000 | | 3500000 | 201803 | 1000001 | | 3500001 | 201803 | 2000000 | | 3500001 | 201803 | 2000001 | -------------------------------------- | 3500002 | 201804 | 1000000 | | 3500000 | 201804 | 1000001 | | 3500001 | 201804 | 2000000 | | 3500001 | 201804 | 2000001 | +----------+------------+------------+ I expect this is the case but can I make two composite foreign keys here with the Ref_Period column being used in both? 
yeah it works. Here is an example: https://livesql.oracle.com/apex/livesql/file/content_GH6J7B1R4NRUGEALMQNSE6OKR.html
There should be an ItemID column on the InventoryItem table.
I might not have enough information based on the data sample you gave me, but it looks like Product_ID night be a candidate key for Product_Groups. If you include Ref_Period then your candidate key becomes { Product_ID, Ref_Period }, which is the same key that PRODUCT has. If thats the case, you may as well make Group_ID an attribute of PRODUCT and drop PRODUCT_GROUP altogether.
Your query is kind of messy (why are you bringing in CUSTOMER? Newest to oldest would be desc), but I'll only address your question. I'm assuming AcctRepNo is also the EmpNo. and CUSTOMER.AcctRepNo in ( SELECT EMPLOYEE.EmpNo FROM EMPLOYEE WHERE LastName = 'Smith' ) For question 2, have you tried single quotes? 'Clerk' not "Clerk"?
If you're using SQL Server, check out [checksum](http://docs.microsoft.com/en-us/sql/t-sql/functions/checksum-transact-sql) and [rowversion](https://docs.microsoft.com/en-us/sql/t-sql/data-types/rowversion-transact-sql). A database-agnostic approach is to hash all of the values in the row and save it as an additional column. This hash value can then serve as your "watermark" for use when comparing against other versions of the row.
I forgot to mention: If you do this and (Product_ID, Ref_Period) is not a candidate key you have denormalized your data. Like I said, this is trivial from a querying standpoint. I doubt you have to force the schema this much.
CUSTOMER is needed for the AcctRepNo column, no? Regardless, adding in your query worked and the result is looking much better. Thanks! As for the 'Clerk' part, I completely overlooked that. SELECT Department.DeptNo AS 'Dept. No.', DeptName AS 'Department Name', AVG(Salary) AS 'Average Salary' FROM Department INNER JOIN Employee ON Department.DeptNo = Employee.DeptNo WHERE JobTitle &lt;&gt; 'Clerk' HAVING AVG(Salary) &gt; (SELECT AVG(Salary) FROM Employee WHERE JobTitle &lt;&gt; 'Clerk' AND Salary &gt; 5000); Getting an error stating the 'Department.DeptNo' column is invalid because it's not within an aggregate function/GROUP BY clause. Should that subquery be within WHERE as well? Sorry for all the questions!
This looks like it works, but I'm not sure if you have the ability to dynamically generate the SQL statement like this. DECLARE @ColumnList AS NVARCHAR(MAX), @SQL AS NVARCHAR(MAX) SELECT @ColumnList = STUFF ( ( SELECT ',' + QUOTENAME(PropertyKey) FROM item_properties GROUP BY PropertyKey FOR XML PATH(''), TYPE ).value('.', 'NVARCHAR(MAX)') ,1,1,'' ) SET @SQL = 'SELECT ItemNumber, ' + @ColumnList + ' FROM ( SELECT ItemNumber, PropertyValue, PropertyKey FROM item_properties ) a PIVOT ( MAX(PropertyValue) FOR PropertyKey IN (' + @ColumnList + ') ) b ' exec sp_executesql @SQL;
IT WORKS! Thank you. Im not familiar with the joins and am going to learn them now.
I'm glad it worked! If you haven't tried out these 2 sites, you should check them out. I didn't know what SQL was a year ago and I've learned so much with these 2: https://sqlzoo.net/ https://www.w3schools.com/SQL/ 
I don't have an Oracle installation to test this, but you could try: select * from mytable M1 where date = (select max(M2.date) from mytable M2 where month(M2.date) = month(M1.date) and year(M2.date) = year(M1.date))
This is perfect, thanks
I'm not sure if I completely follow what you are needing, but if you want to remove duplicates results maybe try using the GROUP BY clause and list all the columns from the select statement again.
 SELECT * FROM ( SELECT * , ROW_NUMBER() OVER (PARTITION BY RelationID, YEAR(ModifiedDate) ORDER BY YEAR(ModifiedDate) DESC) AS RN ) WHERE RN = 1
Or DISTINCT clause
Go for a practice oriented tutorial. SQL is not that difficult. You do not to actually express how to get stuff but only what you want to get. We call that a **declarative** programming language. In SQL data are stored in tables. You can get data from a table with a simple query using SELECT the_colums_you_want FROM table WHERE your_condition Table hare linked to other table what we call relationship. A relationship is done with a table referring (f**oreign key**) to another table (**primary key**). If you want to get data from two different table, you may use a join statement. SELECT the_colums_you_want FROM table1 INNER JOIN table2 ON table2.fk=table1.id The following resource may help you http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well.
Thank you for your reply! I have tried this code including the WHERE RN = 1 in the past but I always get an error when I try it.
Without knowing exactly what you're trying to do or the entire table structure, it seems you're getting two rows because the join has two matches in the second table. You need to GROUP BY or add additional conditions that filter the duplicate from the join.
You’ve probably run into an error because you’ve tried it without the outer Select. The code that’s posted here by /r/notasqlstar should work. What error do you get when you run it?
For the 2nd query, you are filtering employees who have a salary less than 5k and aggregating the total employee population into one avg salary value (which would definitely be over 5k because you filtered off all of those employees). Also, unless it's my mobile view, you don't have anything filtering off Job Title. It just says "WHERE JobTitle". Anyways, just doing this in my head, it should look something closer to this: SELECT Department.DeptNo AS 'Dept No.', Department.DeptName AS 'Dept Name', AVG(Employee.Salary) AS 'Avg Salary' FROM Department INNER JOIN Employee ON Department.DeptNo = Employee.DeptNo WHERE JobTitle &lt;&gt; 'Clerk' GROUP BY Department.DeptNo, Department.DeptName HAVING AVG(Salary) &gt; 5000
OK, so you have about one years' experience with formal ETL, at Healthcare company. You have another years' SQL experience at Insurance company. From there, the three years' experience at Investment company looks mostly unrelated to development or IT. However, you've structured that job duties description as the largest. I would do a couple of things: - Decrease the text for Investments company. Find a way to list out all three job titles and add three or so bullet points describing your entire career there - With the space saved above, add a summary section and say something along the lines of "Data / ETL professional with over two years' experience in developing data products." - Contact an outside recruiting firm (for example, someone like Robert Half Technology). They will add you to their database, go over what you're looking for, and submit your resume for clients' they are working for, provided you agree. 
I think your resume is pretty good, the other comment had some good tips that I would follow. I would say that the 2 years of experience + degree not in a technical field are the biggest drawbacks. You may have to struggle through your current job for another year or so to show that you're dedicated and have perfected your craft. I wouldn't worry too much about your technology stack, but I would expand a bit more to show that in the open space you'll free up from taking the other comment's advice.
MYSQL is not for me but - https://stackoverflow.com/questions/1895110/row-number-in-mysql You can just copy the methodology in the comments section.
listen to this. yes, a summary section should've been included. I would also recommend some time consulting - you're fairly early in your career, so a few years will help immensely... it'll broaden your industry experience, expose you to numerous types of environments (smaller, larger, MS and non MS, etc)... and it'll provide you contacts that can help your career in the long run. RHT is a good suggestion for quickly finding something... they're focused on high volume / low cost/margin... it may or may not be much pay difference, but you'll have something fairly quickly. There are also plenty of other consulting companies - some of which are more focused on lots of smaller projects, or on longer term "seat filler" staff augmentation roles.
Oh right, I overlooked that part of CUSTOMER.AcctRepNo- I was just looking at your original query. Glad it helped :) 
yeah that works because its essentially just using a variable counter and having that be the Row IDs however, mine doesn't work because I have duplicate rows and then use "Group by" to get rid of them. However the counter works by counting all the rows AND THEN it gets rid of duplicates. so the counter output is "1, 5, 9, 10" etc rather than "1,2,3,4"
Well yes you have to USE GROUP by on every column in this case. The "right way" is context-dependent. This may or may not be the only way to do it. Without knowing the whole table structure and the entire problem we can't say. 
Got you, well when you do the SELECT @rownum := @rownum+1 as RN FROM (SELECT @rownum := 0) as r, can you not do it again on the subset of data? Like use a CTE? Apologies, but my MYSQL experience is limited.
So, both the old and new databases have the same connection string; but the old one has http turned off, shouldn't that prevent the connecting of the forms to the old database?
Thanks, you are absolutely correct that I should cut down the finance experience section. I will use this extra space to highlight my IT experience and add a summary.
You raise some very good points. So the reason for the HTML/CSS is that I use this to visualize reports in emails (sending data as a table). Do I need to explain this better in the resume or is it not considered relevant experience? Also I am working on learning tableau, so this will be added to my resume once I have a better understanding of it. Your question about what I want to do confuses me. I want to be a SQL/ETL developer. So I thought having scripting/programming skills outside of just databases would be relevant to this. This is the reason I list things like powershell and Java. At work we use scripting languages to interact with the databases in an automated way. Could you explain what you mean here?
Did you mean HIPAA? Learn more about [HIPAA!](https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act)
Fair enough. good bot
this is it. had to use subset select statements
You always have an oracle instance to test with. https://livesql.oracle.com
Not OP, you make good points and so do they. I think the biggest issue is just the organization of that information. I have one short page with the following information: https://imgur.com/R8nJO8t My resume is a cover letter tailored for the position, two pages of work experience, and my skills page. The skills page will also contain references if it's mentioned or requested in the job description. The two pages of experience are 5 things related to the job posting for the most recent two jobs, the rest of the jobs I pick three things that are different between them but still relatable to the job I'm looking at. I keep a long list of all accomplishments of each job now. I also keep various paragraphs and wordings for my cover letters. So most of the time when I apply now, it's mostly a customized and plug and play resume situation. 
Most big clients have free datasets you can use and most of them also have free versions or developer versions of their tools you can use for free. Use and practice the flavor you are going to use professionally or plan on using professionally. Don't learn mongodb if no one in your area uses it and you have no interest. Don't learn SQL Server because it's the most popular and you loathe working in it. 
No. It increased from 23% to 33%, which is a 43% increase in popularity. But who am I to judge?
You can create a free oracle account and use the hosted resources at https://livesql.oracle.com/ which gives you a shell and has other tools to learn the product. You can create tables and populate them and query, etc. In addition Oracle publishes many "virtual appliances" that have an OS and Oracle built in already. You just boot it up and start using it however you want (for non commercial purposes of course). If I were doing MS I'd probably download the 180 day trial of MS SQL server and run that inside a virtual environment. https://www.microsoft.com/en-us/sql-server/sql-server-downloads Any other DB platform I'm sure you could use the same approach of creating a virtual machine and installing the product but I am not familiar with other platforms and YMMV on licensing for educational purposes. For data, there are plenty of sample databases out there. If you're wanting to get into ERP, most companies will have a demo which includes the underlying database and fictitious company example. Think Microsoft's classic "Northwinds" or "Adventureworks", or Oracle's "HR" or "Vision" demos. If you want to start messing with your own data, check out US census data. HR data is always good in my opinion. It's something we can easily relate to and straightforward. Sometimes examining ERP data is not easy unless you know a lot about things like supply chain management, inventory, and general ledgers. 
I like downloading interesting flat datasets and modeling them myself. I get more practice that way. My goto sources are: https://www.kaggle.com/datasets https://aws.amazon.com/datasets https://cloud.google.com/bigquery/public-data
The first query you're cross joining salespeople. Then second query you're not.
oh! i get it thanks buddy! 
fair enough... i was looking at the % who love Postgres, not popularity... but still not 62%
Regarding HTML/CSS... sure now and then an email visualization happens... and I would expect that anyone in IT can muddle through an occasional out-of-skillset request... personally, on hearing this, MY first question was why you even did it at all instead of using other tools (was SSRS available? if so, why not use that instead) Regarding languages, I would consider PowerShell (and Bash) worthwhile inclusions. Scripts are common in the data world. But we try very hard *not* to use programming *except* where needed - the skillsets (and tendencies/patterns) of development versus database are sadly very far apart, so for a BI team to support applications is difficult and costly. Sure, we have a few exceptions, but even earlier today as I was looking at how we might import EBCDIC files, my preference is to leverage features in our existing toolset, rather than build "yet another app". (and I'll also add that we've been shutting down a number of custom-apps-that-didnt-need-to-be-custom-apps as much and as quickly as possible for the same reasons) Additionally, the more skills you list, the more I plan to validate... tell me about the Java factory classes... when was the last time you created abstract classes, and why... when in reality, these skills aren't significant in the industry toolset used for BI/ETL. I'd rather hear about using SSIS to import a difficult file type (maybe each line is its own format? I've seen/done that) and how it was accomplished. 
Your resume isn't particularly pretty. [My resume](https://www.reddit.com/r/SQL/comments/85digu/linkedinresume_for_sqlbi_developer/dvym5p9/0 has a very similar look and feel, and I'm probably bias because it's mine... but it looks nicer. Also, are you applying for entry level or mid level jobs? Apply for higher up jobs... because less people apply for them, and it makes it easier to get interviews. 
They both find the same discrepancies. The first returns claims which have duplicate procedures. The second returns the specific duplicate procedures, and possibly returns multiple results for a single claim.
So, first off you’ll need to define a priority order for the subjects. That’s doesn’t have to to be written in to a table (though it may help). Once you’ve done this, you can write out the top student for the highest priority subject to a table with the subject name for reference. Let’s call this table top_student. Then it’s a case of writing out the top student for each subject in to top_student, making sure the new student doesn’t already exist in top_student. 
There's a lot of good ideas in these replies, I think the key is understanding your audience, who is going to be reading the resumes first. As the lead db dev of an ETL team, I could kind of read between the lines and maybe get a better idea of your experience than an HR person could. I would also kind of expect the resume to be a little generic in some areas, I would have more specific questions during an interview, the detailed answers can happen there instead trying to fit it in on the resume. Actually, it's a shame you're not in Chicago, we might have an opening soon on our ETL team, what I see on your resume would be good enough to get an initial phone interview...
No problem at all!
Ok I am not worried about the look of it. I come from finance so mine is more of that style of Resume. This is all subjective, but I have kept my resume in this style because I dislike the typical resume template that I see from most software people. Again, this is all subjective and not really that important.
Ok I am not worried about the look of it. I come from finance so mine is more of that style of Resume. This is all subjective, but I have kept my resume in this style because I dislike the typical resume template that I see from most software people. Again, this is all subjective and not really that important.
I think you're being a little harsh here. You may have a point that I need to be a little more focused, but just because a bullet doesn't literally say ETL does not mean it hasn't nothing to do with that topic. Also, I am applying to SQL developer jobs as well, not just ETL (as it say it the OP and title).
Whether or not HTTP is enabled has nothing to do with what database the server connects to behind the scenes. I'm still a little fuzzy on your setup, do you have two separate both with SQL and IIS, one old and one new? And the new machine is submitted data to the database on the old machine? If the connection strings are the same are they to localhost or 127.0.0.1?? If not then that makes perfect sense they they would both be connecting to the same SQL server.
turn the subquery into a **derived table** and then join it to your other tables SELECT ... , it.sub_amount + SUM(cit.LINEAMOUNT) AS Margin FROM CUSTTABLE ct INNER JOIN CUSTINVOICEJOUR cij ON ... INNER JOIN CUSTINVOICETRANS cit ON ... INNER JOIN ( SELECT INVOICEID , ITEMID , DATAAREAID , SUM(COSTAMOUNTPOSTED + COSTAMOUNTADJUSTMENT) AS sub_amount FROM INVENTTRANS GROUP BY INVOICEID , ITEMID , DATAAREAID ) it ON it.INVOICEID = cit.INVOICEID AND it.ITEMID = cit.ITEMID AND it.DATAAREAID = cit.DATAAREAID 
I'll recommend using Postgre SQL and PG admin for the interface. It's an increasingly popular SQL variant, and is getting a lot of attention from the open source community.
My first impression of your resume was "whoa, that's a *lot* of words...". People reading your resume should not feel overwhelmed when first seeing it, imo. Maybe keeping the same formatting but just spacing it out would be a good compromise. I think you need more whitespace. **See, doesn't this entire paragraph just sort of blend together into a nice, big blob of stuff?** If you're a hiring manager and you've been sifting through tens or hundreds of resumes for days, you will not want to read a resume formatted like yours. Visual fatigue is a thing. And there are a lot of words there on your resume. I assume you are applying for positions where applicants have similar/comparable skills and experience. One thing you can do is to lay out your resume in a way that makes it easy to read. (Again, I'm writing this paragraph in one huge chunk on purpose). First impressions are important. The only thing you are to a hiring manager is a piece of paper. Yes, content is important, but if the manager skips over your resume because they are just trying to skim as fast as possible and don't want to deal with a wall of text, they are not going to see the value you will bring to their company. --- Paragraphs are nice: My first impression of your resume was "whoa, that's a *lot* of words...". People reading your resume should not feel overwhelmed when first seeing it, imo. Maybe keeping the same formatting but just spacing it out would be a good compromise. I think you need more whitespace. If you're a hiring manager and you've been sifting through tens or hundreds of resumes for days, you will not want to read a resume formatted like yours. Visual fatigue is a thing. And there are a lot of words there on your resume. I assume you are applying for positions where applicants have similar/comparable skills and experience. One thing you can do is to lay out your resume in a way that makes it easy to read. First impressions are important. The only thing you are to a hiring manager is a piece of paper. Yes, content is important, but if the manager skips over your resume because they are just trying to skim as fast as possible and don't want to deal with a wall of text, they are not going to see the value you will bring to their company. 
I thought the title meaning you were applying for SQL-based ETL jobs, not two different positions. I'd suggest having two different resumes: one for the more ETL-focused position, one that's more for an OLTP SQL development position. For the ETL one: I would swap in things like "Improved performance of ETL processes by analyzing query design..." instead of 'database processes". A bullet point about what data sources you've loaded from (other databases, flat files, excel files, etc.). A bullet point about any visualization you've done. Data modeling/data warehouse design experience -i.e. specify STARR/Snowflake schema if you use it. 
There's a number of possible reasons, have you tried subscribing yourself?
Does it populate their username into the email field? If so, it should work just fine for subscribing themselves to reports. If you want them to be able to edit the field or if it's not delivering the emails with the alise, I think you'll have to set the `SendEmailToUserAlias` flag to false in the RSReportServer.config file... assuming you have setup the SMTP parameters already... https://docs.microsoft.com/en-us/sql/reporting-services/install-windows/e-mail-settings-reporting-services-native-mode-configuration-manager
SQL Server Express + SSMS + Adventureworks or other sample db
So what would happen if student 1 got 93 in both math and English?
in that case, the student could be selected for any one of them, randomly or otherwise
That worked beautifully! Thank you so much :-D Now I just how to get the same values a second time for a different time period :)
That looks very neat! I'll have to study up on it, because that article is a bit like Greek to me XD
A general tip for your future - never allow random in your code unless it’s completely intended. 
Getting certified is a thing but for me it's something experienced people do so they can move into consultancy and charge those nice daily rates. If your still learning then experience speaks volumes over pieces of paper. I've had candidates certified in SQL that were unable to write basic join queries. You'd be amazed what you can learn yourself if you've got the determination!
I’m a beginner and curious too, but it looks like Oracle offers certifications depending on what you want: https://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=50 
Thanks for the great feedback. You are right, I need to remove things and space it out.
Thanks for clarifying what you meant. This is a great idea.
Hi, my resume sucks and I can't get any interviews. Someone help! You should do this... Ha, nope, I do it my way because I like mot getting interviews. OK, do whatever you want. I get lots of interviews. You're arrogant.
You give no constructive feedback. You just say my resume is "ugly" with no explanation. You act like the only purpose of a resume is to look nice. You think experience and skills have nothing to do with getting an interview? You are completely ridiculous and close minded. Oh yea and your advice to just apply for higher up jobs... so genius. Wow, no wonder your other advice was so great!!! You got it all figured out.
I gave you an entire template which is literally identical to yours. .. but it isn't ugly. I also used to have a really hard time until I started applying to senior jobs, then I got more interviews than I knew what to do with. You just don't want to listen.
Having switched from Oracle to SQL Server at work, there are many gotchas that I think are valuable to know. We keep an internal wiki about collation, SQL# functions, definer rights, common error codes for `set ANSI_NULLS` etc.
I am a DBA for SQL Sever and Postgres. When hiring someone I am most concerned with their understanding of Database Design, a broad understanding of the SQL, and set theory so much more than a cert for any one sql syntax. However, a more in depth understanding of the internals of an RDBMS would definitely have me interested. 
The MIN() function does not take multiple arguments and return the minimum one. It takes a single column (which can be a calculation) and returns the minimum value from that column. If other columns are specified, they also need to be aggregate functions like MIN() or be included in the GROUP BY clause of the query. If you would like to give the structure of the table that contains the [Area] column, I would be glad to help you write the query. Without it I would be shooting in the dark.
Its subjective... I only have 2 years of experience. Senior jobs are not an option. 
https://imgur.com/a/cx3ij Took a bunch of text out and spaced things. Look better or do you still think that its too much text?
Great points, thanks for this.
Wow, that's cool. Thanks!
I appreciate that response. I have to start somewhere so hoping to get some exercises in and an opportunity to apply them.
Solved! On another site someone kindly posted a suggestion and it worked: ;with CTE AS (SELECT ID, Name, RelationID, RelationName, RelationDescription, [Year], ModifiedDate, ModifiedBy ,ROW_NUMBER() OVER (Partition by RelationID ORDER BY ModifiedDate DESC) RN FROM Table1 INNER JOIN Table2 ON Table1.RelationID = Table2.RelationID ) Select * from CTE where RN = 1
Four years ago I was an entry level SQL analyst, and today I am a senior manager, senior analyst in a F500 company. Call me a dummy if you want, but your resume sucks. I look at resumes when we hire new people and I'd toss yours in a stack. &gt;You just don't want to listen. I'm not here looking for advice. PS, I don't have a college degree.
I'm confused on the part you meant of my second where clause being tableB
Getting certified is kind of 90s. To me it feels like a way for the corps to cash in. Personally i dont know anyone who has got a cert or is going to get one. Also, in the jobmarket i would not hire because of some cert. 
Do you understand I'm not trying to get in a pissing contest with you? Your qualifications and experience are OK, and because you're here on Reddit you interest me... if you know how to interview (and I can tell you don't) then I might favor you and want to have you hired. You realize that **literally** I am the person you are pitching. I am the person who has the ability to A.) give you an interview, or B.) opine that we should hire you. You also realize, I hope, that despite my arrogance that I am working through these issues with you to try and make you understand where you are failing? My criticism is specific but you are choosing not to hear it. If you came across my desk I would simply throw you away and not be bothered again. But it is 3:25am EST and I am here responding to your punk ass bullshit cunt messages to try and break through to you. I'm not doing it because it helps me. Listen to me. Fucking open your ears. PS, I applied for senior jobs after only 2 years experience. Read my original link and the comments in that thread and learn to market yourself! Stop being a pussy!
I would recommend asking someone from the business to explain how it all works. Normally an invoice represents a request for payment, and the money is transferred via one or more payments. So it's useful to link a payment or part of a payment the invoice it invoice item that it pays for. If you just need the balance then maybe you can ignore the link. On the other hand, combining amounts from both invoices and payments could lead to double-counting. It certainly would where I work... 
So if student 1 was selected as top of English then who would be top of Maths?
I think the test I took was around $150
Huh, this might work. We should be able to safely draw any payment for an item on record back to a CustID without having to go all the way up the right side. It'll be a few days before I can test it out but this looks promising. Thanks!
Ok you win. You are the superior human. You are perfect in every way. You are better than me at everything, especially bragging to strangers on the internet. I bow down to your awesomeness.
That is the setup, correct. They both have a LocalSqlServer entry that was already there, and a secondary entry that was input 
If new the server is ~~posting~~ send it's data to the old server's database then turn off the old server and see where the error happens when the new server can't connect to the DB, that is the quickest way to find what you are looking for.
Apologies for not providing input, but I just wanted to say thanks for the suggestion of SQLdbm. That is a pretty slick tool.
No problem - hope it works :) and let me know the outcome
You shouldn't need to connect these 2 branches to get your desired output - starting from the customer, do a left join to a subquery for the left side that brings you at most 1 line per customer with amt_billed, amt_credited and do a left join to a subquery for the right side that also brings you one record per customer with amt_paid, amt_debited. 
What are the best ways to make sure the input is “safe” Edit: what is the best practice?
I would give this a read if you have time. This guy only has a couple articles on his site, but they are excellent: http://www.sommarskog.se/dynamic_sql.html
$165 at MSRP. Right now there is a deal where if you schedule it within enough time and it's close, you get 25% off your 2nd cert, and 50% off each cert after that. If you want to be full certified for 2016, you need to take 6 exams. If you want to be certified for 2012-2014, then it's 3 exams. 
Sure! The relationships feature needs a little work (hence why I drew the lines myself), but all in all a very good tool.
I was thinking of fashioning them in two CTEs and using a third for that final JOIN, so thanks. There are some simple business rules not reflected here that I would need the right branch for, but you're right that trying to join the whole honkin thing is more than I really need.
No, I just have a nicer resume than you.
You can alter the master key ALTER MASTER KEY ADD ENCRYPTION BY PASSWORD = 'newPassword' Then move the database, and set it up using OPEN MASTER KEY DECRYPTION BY PASSWORD = 'newPassword'; ALTER MASTER KEY ADD ENCRYPTION BY SERVICE MASTER KEY
Thank you for your response. That makes a lot of sense.
Thank you all for your responses and input. This has been helpful. I hope to get better and contribute to this sub in the future. 
You owe this guy gold op 
I don't think you'll need DMK. Try certificate backup it should work. Honestly speaking I have never restored DMK yet. And our environment has heavy tde implementation
I don't do gold. But I'm going to reply to it that I'll donate $25 to his favorite charity. 
$25 to the charity if your choice sir. 
What a kind thing to do. How about the Boys and Girls Club? 
https://i.imgur.com/WqMC19P.png 
Thank you sir. Hope everything works out well for you! 
Wow you are so cool! Bragging about your resume to strangers on the internet! 
Just a little hint: you are applying for technical positions. Put your technical skills front and center. 
The term sync status is probably going to throw people. That's a term used in replication to determine if a slave database is in sync with the master. What you are wanting to do is return to the last record you processed and continue processing from there. The best way to do that is to use a helper table that stores the unique identifier of the last record you processed.
You must get laid all the time!!
1
Is the full one free?
If you have an opportunity to attend the PASS Summit (annual technical conference, usually in Seattle), I would really recommend it. That was the turning point for me. If not, join PASS and try to attend one of their FREE, local SQL Saturday events. Free training from people who know way more than us, and they all have blogs which are awesome. Some of my favorites are Brent Ozar, Tom LaRock, and Paul Randal. Get plugged into the PASS community one way or another - they are great people who have been where you are, and like to help people. 
The expiry date won't be used for any query; I just need to store them. I'm using the actual credit card number to run a query of credit card numbers that are not used in my other tables. I hope that makes sense.
ok... so as a beginner, who needs to learn SQL in the next month for a job.... and i was told to put in adventureworks to learn it.... WHAT do i install?? express and tools and thats it?
Is there a name for that?
Can you access this manual? https://dsedteam.slack.com/files/U42RY94CA/F6X5WS0BT/sql_funadamentals_2016.pdf
All express versions are free. You don't want the full server, its a real resource hog.
what do I use that image for? what am i comparing that to?
Thats exactly why I Was trying to avoid it, hoping the express would get the job done!
The retail version is Mega expensive. But to Download the one I suggested is FREE. ExpressAdv 64BIT\SQLEXPRADV_x64_ENU.exe 1.1 GB https://www.microsoft.com/en-us/download/details.aspx?id=42299 
Everything is possible, but you'll need to post some example data and the expected value returned, right now there are many open questions. Imagine this: id | timestamp --|--------- 1 | 2018-04-06 5:30 pm 2 | 2018-04-06 9:30 pm 3 | 2017-04-07 8:30 pm What would you expect here? 1 &amp; 2 to be grouped together and then 2 &amp; 3 together as well? With "24 hours of each other", a single row can match multiple other rows. If that's what you expect, you can simply join the table with itself with a condition on the difference between the two timestamps: SELECT * FROM reddit_8ag7p6 match1 LEFT OUTER JOIN reddit_8ag7p6 match2 ON datediff(hour, match1.ts, match2.ts) &lt; 24 AND match1.id &lt; match2.id Produces: id | ts | id | ts --|--|--|----- 1 | 2018-04-06 17:30 | 2 | 2018-04-06 21:30 2 | 2018-04-06 21:30 | 3 | 2018-04-07 20:30 3 | 2018-04-07 20:30 | NULL | NULL [Working example](http://sqlfiddle.com/#!18/82d08/1). Also, you haven't mentioned which RDBMS are you using (MySQL, Postgres, SQL Server, Oracle, others?).
select count(*), country from customer group by country
Credit card expiry dates are usually MM/YY. Keep the column as it is and when inserting data just set as the first of the month trunc(my_expiry_date_var, 'MM'). When you are pulling data: select to_char(expirydate, 'MM/RR') from creditcard Replace MM with DD, if you want to store the day rather than the month. (Oracle Format Models)[https://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements004.htm#i34510] I don't have an Oracle instance in front of me to check all this but it "should" work. :D
Then don't include the count in your select. Literally. Or maybe you want distinct? What are you trying to achieve?
Helper table, control table, process management table, about a million other things
Maybe don't use SELECT TOP(1000) but SELECT ... ORDER BY... LIMIT 1000
Either wrong database or where clause is wrong. Maybe there’s a trailing space or something. Try running it without the where clause.
WHERE Category LIKE 'Drawing Status'
it turns out it was a trailing space in the data
Just select the columns you want. You can still use the information not pulled from the same table.
My comment isn't going to be helpful, but I always start out using select * from whatever, and slowly add new new qualifiers one-by-one until it looks broken to find the exact source of the issue. But that's my favorite part of using SQL... finding and fixing errors and issues and whatnot. 😊
If I have a query that works without the where clause and I suspect an issue with spaces I can't see, I will usually add a pipe | before and after the field in a select. That way it stands out and is easy to see: SELECT '|' + category + '|' FROM ... Syntax depends on the database, but this helps a lot. Or as Mooglecharm said, just trim it (I'd trim it in the sql first to verify). I work with a 3rd-party database so I never know what to expect in their data and see this a lot. 
I've never run into problems with case sensitivity with a SELECT statement. MS SQL, Oracle, DB2, Teradata, Access.... they have all handled it just fine.
If you think a space is bad, we have one field that has issues with trailing NULLS. It took me about a day to figure out what was going on. Then, when I wrote everything up and sent it to the data quality folks, they said A) I was wrong, those are spaces and B) It didn't matter anyway because it wouldn't affect query results. 
I installed that. Thank you
OK, I have express installed. What's next? Install adventureworks 2014? Is that straightforward?
Thanks so much for your answer. I am using MySQL. And my intent is indeed to group everything together, (assuming the data is already sorted by date), as long as the next timestamp is within one day of the oldest or newest timestamp within a group that they would be grouped together. 
It depends how the database collation is setup. Everywhere I’ve worked so far makes it case insensitive, so you are correct, most people probably haven’t had an issue with case sensitivity most of the time.
This is an area of high demand, with no formal training course. I've been in the space for nearly 20 years (well before it was called BIE) and I can say that this is a field where the most important skill you can have is the ability to learn new topics and concepts. You will need to be analytical at all times. Being strong in SQL will be a big component. Knowing data modeling and how to manage and wrangle data all are important. The biggest thing the best BIE's do is learn the business and understand what drives business decisions. Just because something is meaningful to you as an analyst, does not mean anything to a business person. Make sure you become great at telling a story with data where you can clearly see the hero and the villain. Watch out for the data scientists who think the job is to achieve statistically significant predictions. When you can, do it, but always provide the business with a direction or recommendation. A null hypothesis is only meaning full to chemical engineers and the health industry. Everyone else is looking for information that can help them decide where they should go next. Be careful with how you deliver messages and data. You will uncover things that people are doing that they may not want others to know about. If it's criminal, dot your i's and cross your t's before you tell anyone about it. When you talk to someone, that person needs to be at or above the person who you have suspected of possibly doing something wrong. Good luck!
Ok what kind of output you're expecting? I'm having a hard time imagining what you really want, but as I said, if you can present a test case with the expected result, I'm sure we can forge something that would work.
Let's say I have a table as such: http://sqlfiddle.com/#!18/51758/1 And I'd like my output to be a table with a single column (timestamp). I'd like to group the input table by timestamps within 1 minute of each other. Which means 29:31, 30:30 and 31:07 gets grouped together since the endpoints are within 1 minute of another timestamp. Then I would like to return the latest timestamp of each group. On the above group then, I would return 31:07. Hope that helps explain, sorry if my wording was not clear. 
I am not sure how adventureworks comes down. But if its zipped, unzip it, stick it in a folder somewhere, then right click on the database folder, and choose Attach. If you can't find the folder, then restart express in Admin mode. The alternative is to find the data file for your installation with will be a version of this C:\Program Files\Microsoft SQL Server\MSSQL12.SQLEXPRESS\MSSQL\DATA and stick it in there
This is the first time I’ve heard the term BIE. I Googled it but couldn’t find an answer either. What’s a BIE?
Hm, on SQL Server I'd just use a recursive CTE, but these aren't supported on MySQL, at least not until 8.0 apparently. Are procedures fine or do you need a single SELECT query? 
Great questions. Use the right tool for the right job is my recommendation. Knowing when to use tool A or tool B is something that comes with experience. If you only know how to use a hammer, every job gets done with a hammer. Java dev and PostgreSQL are great languages for building front ends up from the ground up. This will give you the absolute most amount of flexibility in your end product. This can be good. This can be bad. Think through how much you want to spend supporting the solution in the long run. If this is something for your customers and drives the revenue of the business through the tech, then the more you invest in tech, the more money the company will make. Then go 100% custom dev. This environment is perfect for java front ends with PostgreSQL. My contention is that most companies are NOT tech companies, no matter what they say. Those companies need to balance their 100% custom dev with a far larger % of solutions that come from larger "box" solutions. The flexibility will not be as great, but the speed of development, and the lower long run development costs will greatly reduce the ROI (return on investment) that the non-tech company gets from the technology. Microsoft's database and business intelligence full stack development uses, MS SQL, SSIS, SSRS, SSAS and PowerBI for the variety of tasks necessary or data analytics. These tools are directly integrated across the entire Microsoft product line and across everything in Azure. This integration is something that would take billions of dollars for someone to do with 100% custom development. In fact, the last number I saw, was that Microsoft had spent over 12 billion dollars integrating these tools across their entire platform. Do not underestimate the effort it takes to integrate everything. It's frankly something that cannot realistically be done with the pace of change of these platforms. 
Can't agree more. There are many data cooks in my companies kitchen but they just churn out graphs and charts. I believe you must have a clear understanding of what the business wants to see, understand it yourself as much as possible so that when you build a solution you can speak confidently on the figures. What differentiates me over the others is they are all pulling from the same limited front end report suite whereas I have the strong SQL skills that allow me build bespoke solutions quickly. I really enjoy the fact I can develop a solution from business requirements thru to technical implementation. 
A single SELECT query would be preferable. Actually, what would be the recursive CTE implementation in server? 
Start learning how to use Redshift, know how to manipulate SQL (not just write basic/advanced SQL, but actually understanding how different operations work). Technically, it's not all that much, there's a number of degrees you could pursue that would prepare you to become a BIE. Management Information Systems is a good one, it is a business degree, but from what I understand, most schools with a good program get pretty in depth into the programming side of things. Beyond that, SRA, and CS would show a certain aptitude for programming that most likely surpasses that of an MIS graduate, especially upon graduation. Self-taught knowledge is not bad knowledge, and it certainly won't hurt you. What higher education does is help you learn more about thinking critically, and involving all the knowledge at your disposal to accomplish a given task, it is also just straight up proof that you can deal with a topic with a more advanced, more academic/intellectual state of mind. In terms of tips or resources, build shit. Break it, twist it, find things that your building process is bad at, try and accomplish things with your software/db that you just simply do not know how to do. An example that I used was adding some sort of email notification system to my MySQL db, just sat there for a while and tried to figure it out. The things that you learn in the process of just doing things, and the way that KNOWING those things makes you better at learning about them in the future, is the most valuable thing you can gain from your position right now. And the lessons you take away will be invaluable in the future.
Yeah, that's right. I've heard the abbreviation a bunch but maybe its not universally used. 
Thanks for all of the insight. I'm actually coming from an MBA -&gt; general management/strategy background, which I enjoy and am good at, but have been doing so much additional data work to support my job and for others that I have an opportunity to transition. Have you seen others do this? if so (or just your best guess) what gaps or pitfalls should I be aware of? Any resources you'd recommend to ensure that I cover the fundamentals?
Thanks for the advice. I guess I didn't explain it all that clearly, but I am not looking for a degree to pursue. I am more curious what the typical background for the role looks like. I have an opportunity to transition into this type of role and am trying to understand what the normal expectations actually look like in the real world as opposed to in job descriptions. Also, since I wasn't actively pursuing this type of role, I am not totally prepared to take it on and any resources you recommend would be helpful. Definitely hear you that learning through experimentation and practice are the most important - that is what has gotten me to this point, but I'd like to ensure that I have a sound grasp of fundamentals as well. 
Whoops yea forgot the query. I appreciate any help you can provide! SELECT CreateDate as time, sum(number) FROM dbo.Table WHERE CreateDate between '04/04/2018 10:00' and '04/04/2018 11:00' group by CreateDate order by time
If you're just going across simple timezones (where the time changes are consistent, no going back a day) and you're not worried about data maybe being an hour off between 3-4am on dates where the time changes (for daylight savings)... Then I'd suggest just using a CTE to represent the whole table in another timezone. Note: this does not work as well if your SQL server is in UTC, because UTC does not change for daylight savings. For that you have to write a bunch of IF statements around the UTC time DST takes effect, and change your time offset for each date range between DSTs. Lets say your SQL server is in EST, and you want to look at data in PST. That's 3 hours back from EST, so you would do: WITH MyTablePST AS ( SELECT DATEADD(hh,-3,CreatedDate) AS [CreatedDate] ,Number FROM MyTable ) SELECT * FROM MyTablePST The long and short of it though, is doing any cross-timezone datetime math in SQL sucks. I'd recommend you use a layer on top of it that has libraries for doing that kind of stuff (C#, python, something of the like).
Holy fuk! You are a god. Ive been racking my brain for the last 24 hours! Thanks a bunch!
I considered "I'm sure there's a setting for that" but then I thought.... Only maniacs would set that to "Case Sensitive", right? 
Based on your own example, have a look [here](http://www.sqlfiddle.com/#!4/dcf73/6/0) and see if the output matches your expectations. This solution is written for Oracle but if it's correct, then I don't think it should be too hard to re-write in MySQL.
Every BI engineer I know including myself was a CS major. All are strong with SQL, Excel, and are able to learn and use all sorts of ETL and data related tools like Splunk, Tableau, Informatica, ServiceNow, Graphite, etc.
Probably more requirements driven.
SELECT x.Country FROM ( SELECT COUNT (ID) AS CustCount , Country FROM Customer GROUP BY Country ) ORDER BY CustCount DESC
Sorry, I'm very heavily hungover, so I have no idea if this is the best way, but I think it works: [sqlfiddle](http://sqlfiddle.com/#!18/51758/6).
Nice, window functions, haven't thought of that. I think it's better than [my approach with a recursive CTE](https://www.reddit.com/r/SQL/comments/8ag7p6/is_there_a_way_to_group_timestamps_into_all/dx09p3l/). But MySQL doesn't support either yours or mine solution until its 8.0 release.
You could create a persisted computed column.
Thanks, I think it's pretty cool that both techniques are basically slightly scaled-down versions of interval packing queries, which is what I'd be more likely to find in real life scenarios. It's also fortunate that in Oracle 12c there's also something called a MODEL clause which lets you do regular expression-like pattern matching across entire datasets so it'd be perfect for this example too. But if OP doesn't have access to any of these functionalities yet or doesn't feel comfortable enough using them then depending on the size of the dataset then they might as well take the procedural route.
This is awesome. You will find your understanding of the business side helps you move forward rapidly. Business people are going to look at you as a savior. Here is where you will face challenges. The tech community, for the most part, does not really seem to understand where this position fits within the organization. In tech only the best solutions move forward. Tech people have long drawn out conversations around what tech is the best and what really small technical detail is 'critical' to a platform. Check out the Silicon Valley episode where they talk about tabs vs spaces in formatting code. Tech ppl will presume you have a very deep knowledge of technology and any short comings you have will be used against you as you work on solutions and ideas. My recommendation would be to NOT challenge the tech area, but phrase everything with verbiage like 'how can we partner to...'.
All of these jobs seem to get new titles every 12-24 months. I've only started to hear this term in the last 8 months.
To build on this chain- Data- Find ways to build, twist, combine, pivot and shift data in a data model. Learn what works. How you can pull it all together. Learn what does NOT work. Make sure you test out the data that you are using. Ensure you maintain true to Counts and Amounts (make sure you are not duplicating or eliminating data incorrectly). Make sure you learn how to visualize the data and move through it both visually and in the back-end. Check out Garner's top BI tools. If you are going to start anywhere, start with PowerBI and Tableau. These two are the hottest guys out there right now. Learn how to use both. Learn why both are exceedingly popular and knocking out competitors. There is a strong use case for both. 
A side note- that looks like a stupid example... in 99.99% of all situations, sales is always going to be a historic measure. IE- you cannot know how many hot dogs you will sell tomorrow. Do exceptions exist, sure- but rarely do sales tracking systems EVER allow for a future sale to be counted or listed as a sale until the transaction actually occurs. There are many accounting practices and laws that prevent this... I think 'The Office' did something like this when Ryan was running the website... someone here will know more about that episode. But I hope you understand the concept I have listed above. Better Limit for Sales in the past 30 days: Select sales, region From GeoSales Where cast(SaleDate as date) &gt;= cast( GetDate() as date) -30
Okay, and when I insert and update the aggregate data to a separate table, is adding the data to a (serialized) array in a VARCHAR column the way to go? Another concern is that the data is updated at different frequencies at the moment, more often during the daytime than at night, and in the future it will be updated based on notification events every time the betting pool changes which means it could be every couple of seconds closer to the actual race, and it doesn't make sense to have data points more often than let's say every fifteen minutes or something and to look back on a couple of hours. I guess that's a programming problem but it means that I have to query the data in the array to check if it's time to insert another data point.
I would just write an if in the trigger to check on the latest timestamp in the trend table. If less than 15 minutes, do nothing. Also, what would the purpose of an array be as opposed to just column values? Say, use the same primary keys as the base table with the addition of a timestamp. Then you have that timestamp to adjust the query to see things like change this month, this year, season or what have you. 
From W3Schools: ` ALTER TABLE Orders ADD CONSTRAINT FK_PersonOrder FOREIGN KEY (PersonID) REFERENCES Persons(PersonID); `
I didn't know that the column isn't created automatically. That solved the problem. Thanks for the tip :)
Okay, I understand what you mean now, so I should use the same primary keys together with the timestamp as well as a primary key in a separate table? I thought it wasn't necessary to store it as column values since I didn't see the need to query the stake percentage of a single point of time but probably always wanted to retrieve all of it at once for a single horse in a race. Wouldn't a table like that grow quickly in size though? Let's say it's a "win7" betting pool with stake percentages from seven races and a dozen horses per race that is updated every fifteen minutes for a week. That's around 12*7 horses updated every 15 minutes 24/7 one week before the race day so that's around 50 000 rows for one betting pool with stake percentages in for example DECIMAL(4,2). How many bytes of data is that, is it four bytes per row as in 200kb in this example? Excuse me if these questions are stupid but I have a difficult time to wrap my head around this.
Creative name
What does the source table look like? 
use outer join on the other table
Store and process everything as UTC; make conversion the application developer's problem. I'm only half-joking.
yes select table a as a left join table b as b on a.X=b.X
Did you try the sidebar? A common question is how to learn SQL. Please [view the Wiki](http://www.reddit.com/r/SQL/wiki/index) for online resources.
The source table? The data is coming from a SOAP information server and fetched with cron jobs. 
You said it's stored in a database... 
Great idea, something like this would be very useful if you exposed a generic ODBC connection, unfortunately we don't use a very well known SQL platform (Sybase SQL Anywhere). I'm surprised nothing open source exists for this tbh.
Example 1, name1 Example 1, name 2 Example 1, name 3 Example 2, name 1 Something like this (hope it will format right)
The data is inserted and updated into a table called starts with race_date, race_number and horse_name as a group of columns as unique keys with a lot of other columns relevant to the individual start and the stake percentage for each betting pool is inserted and updated into another table with race_date, race_track, race_number, horse_start_number as a group of unique keys and a separate column for each of the stake percentages like win7, win6 and so on.
OK, then make a new table filled by triggers that creates the aggregate data... I guess it might be important to know to whom and how the trend data is presented? Is there no reporting software. This could really be solved with a stored procedure to query the data. Then, storage is not an issue. 
Do you have primary and foreign keys in each table to do the table joins? 
Could we get a free pass to test it out?
Update: can confirm this works. [Screenshot](https://image.ibb.co/cNiHXH/Capture.jpg)
I second PostgreSQL + pgAdmin. It's a bit of a pain to set up and figure out at first, but once you do, it's really rewarding and fun. It's a pain because nobody ever told me you have to run: postgres -D "path to data folder" For it to work properly. I used to always get errors telling me pgAdmin couldn't connect.
Yes. I used to be a heavy user of that feature in Excel and missed it in Sheets, so I built this. One added bonus is very easy automation. We always had trouble getting SQL queries in Excel to update on a schedule, but SeekWell does a good job at this.
Thanks! Most people have to say the name aloud before it makes sense.
Excel already has pretty good ODBC support, were you thinking it'd be good for the automation aspect?
I'm already working on open sourcing the side bar / add-on. Most of the value for companies is from the automation. Not having an analyst manually updating reports can save a lot of time and money.
I will try it :) Tnx
Hey I'll try to explain I'm learning to become BI developer and today I'm a sales &amp;marketing analyst so I don't have a permission to use DCL. my TCL is ok I know to do most of the regular things like join or row number etc but when I need to do something complex I get lost like multiple joins with cases and to add to that specific data with a condition and casting and... but to each one alone it's ok so I want to know how to "think" and how to add parts of code together and to improve my abilities Thank you
Unfortunately the problem solving aspect is something that can't be taught in the same way that specifics of the language can. It takes practice, time, and lots of Google. Take things one step at a time, and pull out each individual part of the problem.
Review many to many relationships. Since there are many students with the same class and each student has many classes you need a table that sits in between to link them. That way, the master information for each student is still kept within only one row. 
Wouldn’t this be a 1:many relationship? Since, in theory, there would only be one record per student in the Student table?
No. A student can take zero to many courses. And a course can have zero to many students in it.
So, just to understand, in this scenario, the student table wouldn’t just contain one record per student, but multiple records per student (one for each course in which that student is enrolled)? I guess I am just not understanding how the data is housed in the Students and Courses tables in the instance. In my mind, I’d have one master table of all students, one per record. Then I’d have another table that contains all courses each student was enrolled in (one course per record, but multiple records per student)... with a foreign key back to the Student table. And on top of that, I’d likely gave a metadata type of table with Course definitions, again, one per record. I’m curious if that is inefficient. 
the student table would be unique. the Bridge table would have one record per each valid student/course combination. the Bridge table could be only two columns. StudentId and CourseId. Both would be foreign keys to their respective tables. Hope this helps. 
you would have 3 tables. one course table, one student table, and the "bridging" table (I call it an associative table). example course table columns: id, title, description student take: id, name course_student table: course_id, student_id so if student 1 was taking courses that had IDs of 1, 2 and 3, your associative table has these records: course_id - student_id 1. 1 2. 1 3. 1 (ignore the periods, my phone won't stop putting them back) this keeps you from having nonsense columns in the student and courses table and allows you to avoid redundant data. once you implement this schema, you'd pull all the students in a course like this SELECT s.name FROM course_student cs JOIN student s ON cs.student_id = s.id WHERE cs.course_id = (SELECT id FROM course WHERE name = "Philosophy 1300") I apologize if that SQL isn't 100% accurate but I'm in bed using my phone to answer. this should give you the general idea though
Thank you so much! This helps me a lot :) 
My query had zero items selected. I just figured out that I needed to make 'left joins' instead of a normal join. But thank you for taking time to look at my problem :)
&gt;manipulating with an R script to join with other data files, add columns (like weeknumber), filtering on various strings etc, summarizing etc. Yes, you can do this with SQL (technically, since some flavors of SQL are Turing-complete, you can do "anything" with it). As tables are the base data structure that SQL operates with, you need direct access to your colleague's source tables, or a copy of them. Whether it's *better* to do your manipulation in R/python or SQL...it depends upon what those manipulations are. SQL is generally not very good for string manipulation and has limited math functions built-in, but if you're manipulating the actual data structure, combining related tables, performing aggregations, filtering based upon criteria, etc. it can do quite well. The current hotness in the SQL Server world is to take a hybrid approach; store your data in tables, use T-SQL to manipulate it into what you need to process, then pass the dataset off to R or Python (which have been built into the product, so you can hook out to a script directly from your queries) to do statistical analysis (ML, predictive analytics, etc.)
&gt; Lets say your SQL server is in EST, and you want to look at data in PST. That's 3 hours back from EST, so you would do: Except for two 3-hour windows per year (EDT/EST switchover), and portions of Indiana [prior to 2006](https://www.timeanddate.com/time/us/indiana-time.html) if you're dealing with historical data and...well, timezones and DST both suck, like you said.
I'm a MS SQL guy, so these may not work. Try [brackets] SELECT FirstName as [NickName] Try = SELECT NickName = FirstName
SELECT `first_name` AS 'nickname' Backtick your columns. Which engine are you using to store your data in MySQL? 
all things are possible through code 
I tried this but unfortunately it didn't work
I tried ' ' but it doesn't work, no error but the column headings in the results don't get changed. I'm using standalone MYSQL server. I connect to the same database with different software and as nickname/'nickname'/"nickname" all works. It's just SQL Developer that's being a pain 
&gt; users timezone as a UTC offset Might be better to just store their actual timezone, not the offset. The offset changes for many timezones twice a year. For example (server located in New York): SELECT GETDATE(), GETUTCDATE() AT TIME ZONE 'Eastern Standard Time', DATEADD(month, -2, GETUTCDATE()) AT TIME ZONE 'Eastern Standard Time'; Yields: Field1| Field2 | Field3 ---|----|---- 2018-04-09 09:30:06.883 | 2018-04-09 13:30:06.883 -04:00 | 2018-02-09 13:30:06.883 -05:00
You both got me thinking that maybe Oracle and Microsoft just aren't playing nice so.....I put my test database on Oracle 11g Express and connect to it with SQL Developer. SELECT first_name AS nickname works now :). MySQL+HeidiSQL - A ok Oracle11g+SQL Devleloper - A ok Mixing the two has caused some minor yet infuriating issues. Anyways it works, thanks to you both for nudging me in the right direction.
Hi! Quick question if you don't mind. Tried looking at stackoverflow for this but couldn't get the exact answer. Does the FK constraint actually "enforce" the keys between the tables? 
Yes, if you don't have the value in the table it references, it will not let you insert or update.
What is the thing you are trying to accomplish? &gt; For example: SELECT * FROM table_name LIMIT 1000000; If you have a clustered index, it will scan the index up to 1000000. If you have no clustered index, it will scan the table up to 1000000. It's not using the indexes at all. Indexes are used for when you need to filter data for selectivity, the example you give shows no selectivity. If you want to increase the speed of a select *, you should be looking at disks and memory, not indexes.
&gt; If you want to increase the speed of a select *, you should be looking at disks and memory, not indexes. completely disagree exhaust all possibilities of optimizing your queries via indexes before looking at the hardware
&gt; how would these index interact when selecting across multiple indexes? it doesn't work that way MySQL will only ever use a single index in retrieving rows from a table, and it will pick the best one based on how you have defined your indexes and what the sequence of columns in them are if the query isn't providing search values for the left most columns in any index, then no index will be used
Yeah I mean, you have 2 of the 3 table names spelled wrong employees !+ employee and employee person info != eployees_personal_info I'm surprised you're not seeing errors when you execute this query. 
a mysql query can only use one index **on a single table** neither the stackoverflow page you linked to, nor the mysql manual page that it links to, contradict this
in this scenario, where all columns are required to be returned, i'm going to guess you'll get a table scan try it yourself -- do an EXPLAIN
&gt; a mysql query can only use one index on a single table A few answers down: https://openquery.com.au/blog/mysql-50-index-merge-using-multiple-indexes &gt; Did you know that… MySQL 5.0 is able to use multiple indexes for a single table? The simplest example of where this comes in useful is ... WHERE a=10 OR b=20 https://dev.mysql.com/doc/refman/5.7/en/mysql-indexes.html &gt; If a multiple-column index exists on col1 and col2, the appropriate rows can be fetched directly. If separate single-column indexes exist on col1 and col2, the optimizer will attempt to use the Index Merge optimization (see Section 8.3.1.4, “Index Merge Optimization”), or attempt to find the most restrictive index by deciding which index finds fewer rows and using that index to fetch the rows. A link I did not share is: https://dev.mysql.com/doc/refman/5.7/en/index-merge-optimization.html &gt; The Index Merge access method retrieves rows with multiple range scans and merges their results into one. This access method merges index scans from a single table; it does not merge scans across multiple tables. The merge can produce unions, intersections, or unions-of-intersections of its underlying scans. 
This was not my real dataset, I changed the data and table names but used the same structure. So I didn't really checked the spelling in my post, sorry. I don't really know why my query wasn't working I think I just did not waited long enough. Because it is indeed working. Thank you for the time to check it :) 
This was not my real dataset, I changed the data and table names but used the same structure. So I didn't really checked the spelling in my post, sorry. I don't really know why my query wasn't working I think I just did not waited long enough. Because it is indeed working. Thank you for the time to check it :) 
&gt; in this scenario, where all columns are required to be returned, i'm going to guess you'll get a table scan That is exactly what I see. The requirements I listed is essentially how I interpreted OP's problem. I can add a clustered index and it will scan the index, but that's almost the same thing. I don't see how we can add an index to this table for the query as is to increase performance. If OP wanted to filter or even order the data returned, indexes now have a role. In a situation like this, I feel a HEAP is just as good or better than anything else and it's a hardware issue at some level. Any new requirements can easily change that, an order by, having less columns to select, a where clause, business logic, etc. That can alter the performance and gives us new options, but it seems like for a SELECT * with no filters or requirements, it's hardware / network based then.
Great! Glad it's working
I apologize but I dont know how to apply source formatting to the code. When I click on edit, it is formatted properly but then loses it when I submit. Can anyone advise for that as well? Thanks
&gt; Can anyone advise for that as well? edit to insert 4 spaces in front of each line of code, this this -- edit to insert 4 spaces in front of... 
look up the table that you're using the evil "select star" on that would be `spraddr` and `spriden` then simply replace the asterisk with the list of all the columns
great, in-depth explanation however, i trust you are equally able to see how this statement is dangerous if taken at face value by a noob -- &gt; If you want to increase the speed of a select *, you should be looking at disks and memory, not indexes. 
&gt; however, i trust you are equally able to see how this statement is dangerous if taken at face value by a noob -- Valid concern, I'll edit the post with a caveat. 
What about: Select first_name as nickname Works for me in SQL Developer using Oracle 12c.
Utterly evil isn't it.
Dont ever feed user supplied data into the query. Above is fine but @sql = 'select ' + user. Firstname... would be bad. 
i would always use a join instead of a correlated subquery p.s. please learn explicit join syntax, i.e. INNER JOIN
&gt; I dont get the logic of which columns to pare out for the select. okay, replace the asterisk with only those columns that you need 
i edited my reply to show you how
what data type is CustomerGroupCount? what error does running the query give you?
It's useful conceptually, but I'm not sure it gets me closer to where I need to be. What becomes of the spriden fields in my select statement? Where do I fit in my conditions?
Ok but this is what I dont understand. The columns I "need" are the ones in the original select statement. I dont understand what I am pulling from the others. I have always used * so I've never had to think through the logic and I have no clue what I "need" on the second two selects. I have tried to use columns listed throughout the query but just get errors like for instance if i enter spraddr_address_line2 in the select from spraddr, it throws an error saying spriden.pidm is an invalid identifier. 
https://www.mssqltips.com/sqlservertip/4741/exam-material-for-the-microsoft-70767--implementing-a-sql-data-warehouse/
Thanks! 
CustomerGroupCount is an Integer value and the error message reads: ‘Operation must use an updateable query.’
take the length of each column value (each long string) replace all the commas with nothing, and subtract the new length from the original length the difference will be the number of commas you replaced SELECT DATALENGTH(klas2) - DATALENGTH(REPLACE(klas2,',','')) AS diff , COUNT(*) FROM yourtable GROUP BY diff 
hey, maybe you and /u/FirstFiveQs can get together to work on [what looks like the same problem](https://www.reddit.com/r/SQL/comments/8b019h/oracle_inner_joins_and_subqueries/)
hey, maybe you and /u/tuscabam can get together to work on [what looks like the same problem](https://www.reddit.com/r/SQL/comments/8azjqq/sql_can_someone_help_with_removing_from_selects/)
Does this help? UPDATE Table1 SET CustomerGroupCount = subquery.countz from (SELECT COUNT (*) as countz FROM Table1 WHERE CustomerGroup = 'North_America' AND CustomerCountry = 'US' AND CustomerLanguage = 'English' ) as subquery WHERE CustomerGroup = 'North_America' AND CustomerCountry = 'US' AND CustomerLanguage = 'English'; 
try this: SELECT COUNT (*) AS CustCount INTO #Temp FROM Table1 WHERE CustomerGroup = ‘North_America’ AND CustomerCountry = ‘US’ AND CustomerLanguage = ‘English’ UPDATE Table1 SET CustomerGroupCount = CustCount FROM #Temp WHERE Table1.CustomerGroup = #Temp.CustomerGroup AND Table1.CustomerCountry = #Temp.CustomerCountry AND Table1.CustomerLanguage = #Temp.CustomerLanguage
thankyou very much!
this may work better: SELECT COUNT (*) AS CustCount INTO #Temp FROM Table1 WHERE CustomerGroup = ‘North_America’ AND CustomerCountry = ‘US’ AND CustomerLanguage = ‘English’ UPDATE Table1 SET CustomerGroupCount = CustCount FROM #Temp WHERE CustomerGroup = ‘North_America’ AND CustomerCountry = ‘US’ AND CustomerLanguage = ‘English’
For MySQL it's tick marks (`), not brackets.
I kinda came here to find someone that knows more about this than me and can help resolve it, not team up with another newbie to spin wheels with lol.
Thank you! This was genuinely helpful! select a.pidm_key from as_student_enrollment_summary a where a.pidm_key = c.spriden_pidm and a.enrolled_ind = 'Y' and a.registered_ind = 'Y' and a.term_code_key = '201810' and a.levl_code &lt;&gt; 'VT') as Current_Term_Ind got me exactly the output I needed! thank you!
I’m doing that course, is very interesting, with a good pace, however it lacks in the exercises parts. I’m looking a different site to complement the Stanford lessons
They should really rename this test "data integration with SQL server". The skills i learned studying for this cert I continue to use on an almost daily basis, and very little of it is actual data warehousing. 
What "OR" part? Anyway glad you got it.
I get that feeling too. There are other exams on reporting and another one that will probably complete some parts of data warehousing. 
Yeah I made a mistake on my own part. Forgot to remove 1 line of my old code when I tested this
Ahh... OK cool.
Pretty sure those are not single tics around the date format. Delete them and manually add '
Thank you, I didn't realize it was something that simple.
Welcome to 90% of software bugs. :)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/learnsql] [\[crosspost\] Learned MySQL basics the past week to get a 1.7k€ job, one day left. What is the best I can do right now?](https://www.reddit.com/r/learnSQL/comments/8b59at/crosspost_learned_mysql_basics_the_past_week_to/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Thank you a thousand times! &lt;3 I'll get me some cans of red bull and some 24/7 lofi youtube livestream, LET'S DO IT.
- indexing (you can't learn all in one day, but you should know at least why indexing is important, and how to create one, unique vs non-unique indexes) - left/inner joins, you should read more about them, it's not the end of the world if you don't use the right one right away, but you should know the differences - indexing - indexing
&gt; because any type of join uses an exact match by definition sorry, this is completely false
 FROM table1 INNER JOIN table2 ON table2.column3 LIKE '%' || table1.column4 || '%'
[https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html](https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html) 
I do not, no. I have access to the database which allows me to pull from it but it is a one way access unfortunately. The query is on my computer and linked to the database but it’s only read privileges 