Is it a possibility to display the number of sales, median price and average price instead of recent 10 sales prices?
well, i hope it supports subqueries SELECT A , B , C , D FROM daTable AS t WHERE ( SELECT COUNT(*) FROM daTable WHERE A = t.A AND B = t.B AND D &gt; t.D ) &lt; 10
a pithy answer. thank you!!
so, it works okay?
MS SQL uses Row_Number() to facilitate this kind of thing.
I'm afraid so will be the solution. Cursor is the worst of these, but I don't see a way to at least not do a correlated subquery. May I ask WHY are you updating all of these timestamps to be untrue?
SELECT tn.ID, tn.[Time], (SELECT top 1 tn2.[Time] FROM TableName tn2 WHERE tn2.Val=2 AND tn2.ID&lt;tn.ID ORDER BY tn2.ID DESC) as [TImeShouldBe] FROM TableName tn WHERE tn.Val=1
If it's truly a one-time report, do yourself a favor and kill that column! Nobody ever thinks of data cleanup...
I'm working off just a snapshot of the production database that I refresh when needed. 
This is how brute force would get you there SELECT City, Person FROM TableName ORDER BY CASE (City) WHEN 'Durban' THEN 1 WHEN 'Johannesburg' THEN 2 WHEN 'Cape Town' THEN 3 END, Person 
A better way would be to create a tiny lookup table for the Cities ID, Rank, CityName
It sounds like a Orders hierarchy. OrderID, OrderItemID. Except in this situation, you only have OrderItem and have to essentially *derive* OrderID. Now, since the system is read-only, you're kinda up shit creek for creating a new column, but boy... deriving OrderID every stinking time seems pretty expensive. About the beta thing, good! That means you can change it before it gets bigger. Trying to scale on that cursor is going to bring a server to its knees! &gt;There's a parent key but because the data is chained 3 levels down there's nothing associating the data in the first place. ...huh? A PK-FK relationship is what associates data between Parent and Child tables. &gt;There's about 4 separate pieces of data that we're trying to link together in 1 table It just... it gives me heartburn to think that you're having to *assume* that all of the rows between Val = 2 records are one contiguous order. So many things can occur that could make this NOT the case (one example I can think of is MERGE replication, in which not even IDENTITY values can guarantee ORDER or CONTINUITY).
lol. I know what you mean about the CURSOR. To give an example of where I've come. I was originally running that cursor statement against about 5million rows. I'm now running it against 15,000. So the refinement is coming. To try and explain this - top level - ID 123 - contains job info xyz middle level - ID 235 - contains no job info except that it's associated with 123 bottom level - ID multiple - contains job info abcdef and only shows association with 235 the easy part was linking the bottom level with the top level. The hard part now is taking that new table using the top level info and then chaining in various data from the bottom level without making a huge wall of SQL to get it. I think right now I'm sitting at about 100+ lines of code and about 1/2 way there. The next hard part, I have a string value in my temp table that I now need to associate with another completely unrelated ID elsewhere in the database. tl'dr - the application db that I've been handed is a piece of shit and I'm working on re-writing the application from the ground up to correct this. Long term is I'm working on creating triggers so that when this data appears it can automatically parse the data on the fly instead of having to do all this complex bullshit after the fact. I really need to be paid more.
Regarding triggers... If you plan on taking LOTS of rows, triggers do not scale well (compared to jobs). IF you take 1,000 rows a minute, your trigger runs 1,000 times a minute. NOT VERY EFFICIENT. If, however, your job is set to run once, each minute, than you'll only run that job 1440 times a day, no matter how many transactions you've processed.
Thanks very much, works perfectly for me and I see how it is happening now!
Hey question for you concerning triggers. Is it only internal triggers you want to limit? I'm looking at designing out an application and one of the things I want to do is have it trigger to a noSQL db similar to [this](http://stackoverflow.com/questions/15696511/hybrid-sql-and-mongodb-solutions) Is there any harm if you have potentially thousands of triggers a minute running if it's not writing inside the DB? I'm just trying to think of the best way to approach this. I could easily just write it in the db and do a once a day dump from the SQL to noSQL but to keep the data current would be better to write directly to the noSQL. I would like to steer away from XML though. Option 2 using the stored procedure is more ideal but really how many times do I want to be calling that stored procedure?
&gt;internal triggers I'm not sure I'm tracking here. What is an "external trigger"? &gt;Is there any harm if you have potentially thousands of triggers a minute running if it's not writing inside the DB? Oh, ok, I think I understand the question here. No, it is ALL triggers you'd want to limit (for scaleability). What you're talking about doing might be a good candidate for Transaction Replication.
Awesome! Thanks a bunch. I had no Idea it was that simple. I got USE [master] GO ALTER LOGIN [sa] WITH PASSWORD=N'NewSAPassword' GO by scripting change to new query window. So yeah I'll just have to create a Server Group. P.S. Do I have to change link tag to [SOLVED] or something?
are you sure there are actually some matches? try this SELECT -- SELECT COUNT(*) FROM CLOB AS TC INNER JOIN Books AS TB on TC.BookID = TB.bookID 
I don't know what the beef is with w3school's material in particular as there are no specific examples on that page, just a whine about it not being a wiki and allegedly being out of date. But for the basics of SQL its fine and you are being an arse for downvoting my helpful suggestion.
What changes did you make? The reason the service failed to start will be in the Windows Event Viewer under the Application Log. It will have a lovely red X next to it. The event will tell you enough about the problem to go from there.
Nope.
you don't want to grade 6 queries or no one wants to grade 6 queries?
Yes.
you're a mean old man!
Post them or PM me. Not saying I'll definitely look at them in a timely manner but depending on when I get them I'll take a look. 
i'll make a few google docs and link you to them (unless you have another preference).
i'll make a few google docs and link you to them (unless you have another preference).
PM with them. I'm decent with Oracle SQL. Let's see what you did, and maybe could explain why you got a low offer.
There are 2 types of databases: transactional (relational or OLTP) and analytical (OLAP or data warehouse, but smaller data warehouses are called data marts). Generally data marts are star schemas, and data warehouses can be star or snowflake schemas. OLTP design is different, what we call normalized (3NF or BCNF if you want to get fancy). I am just going to type the feedback here as I review it just to save effort, and edit as I go. 3) Instead of E.DimProductKey = 230, use E.DimProductValue = 'BS Company Profile' because that was the value you were given, and you don't need to go through the extra step of finding what the key is. 4) Select statement should be E.DimProductValue (instead of key), and the 2 counts. Group by E.DimProductValue as well. 5) Because you have F.DimHeadingValue and F.DimHeadingName in the select statement, you must do GROUP BY F.DimHeadingValue, F.DimHeadingName, not just F.DimHeadingValue. 6) Same reason as 3. Also, you mispelled Origin (Origint). WHERE E.DimProductValue = 'BS Company Profile' AND G.DimOriginValue = 'Company Profile Locations' 7) Instead of using H.DimTrafficSourceKey in the where statement, use H.DimTrafficSourceName = 'Organic Search'. 8) Very knit picky, but make sure that you list H.DimTrafficSourceName first in the SELECT, so that it is the first column to show. By the way, it really bothers me that only FactVisit has a schema. Overall: If it was a right or wrong type of interview, then I am sorry to say you got all of them wrong. However, only 5 does not run because of technical problems, and 6 does not run because of a misspelling. Overall, not bad as you managed to grasp the right approach for all of them, which to me, is far more important than silly syntax errors.
I'm interested in how difficult they might be, since I graduated myself into a SQL job. I want to know whether I could pass. Mind sending me a link to the doc?
I know what you mean about the schema thing because nowhere in the pdf did they have the name of that schema, so i'm not sure how i would have figured it out without 1 and 2 done for me.
What does the application log say? Did you move the master or other system databases? Do the volumes that have the transaction logs have disk space? Have you tried starting in [single user mode](http://technet.microsoft.com/en-us/library/ms188236.aspx)? Did you apply updates or service packs and not restart the server?
I am using access. I will give this a try right now. Thanks!
 ON Mid(tableB.titles,1,5) = Mid(tableA.titles,1,5) or ON Left(tableB.titles,5) = Left(tableA.titles,5) 
depends.... what did you put in the SELECT list?
This might be inefficent but you can do SELECT... FROM TableA a Inner Join TableB b ON a.Title Like b.Titles+'%'
How do you plan on handling a scenario like this: Table A The Princess Bride Table B The Prince of Persia Or maybe I don't understand what you are trying to do and that is acceptable.
Basically I would like to see the query Title---------------Code-----------Title----------------Code The Princess Bride -9878------------The prince of persia-7485 Space Jam---------****------------Space Jam----------1234 This way I can say hey space jam in tableA doesn't have that code, I need to put "1234" for it in tableA because tableB has it. 
IsNull(a.Code,b.Code) Code
Heres a pic of what I am looking for http://imgur.com/ZH0XWxq
Just as a heads up it is likely your output is going to look like this: Title Code Title Code The Princess Bride 9878 The prince of persia 7485 The Princess Bride 9878 The Princess Bride 9878 The prince of persia 7485 The Princess Bride 9878 The prince of persia 7485 The prince of persia 7485 Space Jam **** Space Jam 1234 Anything that has a similar first 5 characters will join to everything else that has a similar first 5 characters and you will get a mini cartesian. with two or 3 it wont be a big problem, but if you hit 5 or 6 similar titles it could be a little confusing.
I have attached a pic http://imgur.com/ZH0XWxq
Or you can join on a longer wildcard than 5.
you can also ride a mule down the grand canyon, it is also not a requirement
I understand. What I am saying is there are many titles that have the same first 5 characters, depending on how large this database is you are likely to have joins across titles that are not similar. Given that this is probably a one time manual process and not a huge database, I am making a big assumption because you are using Access, this might be acceptable. another way would be to have a likeness rating and a threshold that would deem two records join worthy. *Edit* The picture above conveniently ignores that possibility in the sample data.
sounds like you want a **full outer join** msaccess does not support it explicitly, so you have to UNION a RIGHT OUTER JOIN with a LEFT OUTER JOIN 
&gt; I'm trying to find RecordIDs that have RequirementIDs according to what you posted, RecordIDs don't have RequirementIDs, they have HeaderIDs please post accurate table layouts
 select a.RecordID, a.HeaderID, a.RequirementName ,b.RequirementStatusID from table a join table b on b.HeaderID = a.HeaderID where b.RequirementStatusID between 1 and 6 CAUTION I AM A NEWBIE SO THIS IS A NEWBIE GUESS!
Your newbie guess looks right to me. Though that's probably because I don't understand the question. :(
I think we'll need a bit more info to give a complete answer, i.e. the query you're using now and what it returns would be a big help. As well as a couple of lines of sample data. But, that being said, I'll give it shot. NB: I'm assuming MySQL since you didn't mention what DB you're running. SELECT t1.RecID, t1.HeaderID, t2.ReqStatusID, t1.ReqName FROM t1 JOIN t2 ON t1.HeaderID=t2.HeaderID WHERE t1.RecID &lt; 6 AND t2.ReqStatusID &lt; 6 Should get you what it seems like you want, but without additional info it's pretty hard to say. 
Yea its mySQL. The tables join each other via the HeaderIDs. The record IDs are basically the end results I'd like to see. I know I used 1-5 as the values, but they are actually different numbers. Each RecordID might have 2 requirements. There are about 100 requirements total, but I only want specifically (2,12,33,58). Those requirement IDs are used in several other tables hence theyre assigned these values, but I only need those specific numbers. From those specific requirements, I need the ones that are in StatusID (3,17). The status IDs refer to a closed or completed status. The problem I was experiencing by only using the IN statement was I would get a Record that had 2 of the requirements from the list, one was in status 3 and the other in status 5 (which is not what im looking for -- I only need the Records that could possibly have 1 or more, but all of those requirements need to be in 3 or 17). Ill have to post my SQL tomorrow because its on a different PC. 
See my reply to u/rijalati. I've tried using the IN function, which I assumed is similar to what you used, but no success.
Look at my response to rijalati, hopefully that might clarify my issue a bit more.
Hi when I make a connectionI'm not able to connect to the registered servers. I'm only able to connect a query to 1 of the registered servers 
You have to group by (or partition by) in this case. Just this by itself: SELECT MAX(courses_eff_year) will just give you the max year for that whole table. You want the max year for each course. You have to pick the unique identifier for the courses - I assume either sub or numb - and group by that to get the max for each course. You could probably join on a subquery: (select c.sub, c.numb, max(c.eff_year) as max_eff_year from courses c group by c.sub, c.numb) max_yr And then join courses.sub=max_yr.sub, courses.numb=max_yr.numb, and courses.eff_year=max_yr.max_eff_year
Is it possible for me to send an e-mail to of the results of this query? As in lets say i have a set of 5 servers called "1"...."5" Can i either output a .txt file or a .CSV file in the following format? 1,O_pass_1,N_pass_1 2,O_pass_2,N_pass_2 &gt;&gt;&gt; 5,O_pass_5,N_pass_5 **ERROR** server 4 did not update password because server 4 is currently shut down
Thank you. Yes I am using access, and dealing with to many titles that change. I will look into both your advice here. Thank you for your time and patience. EDIT This is actually perfect! thank you! 
&gt; 0, 10, 20, 30 but needs to be 0,1,2,3 (stored as a number) SELECT danumber/10 as storednumber 
when you say "group them together" you actually mean ORDER BY you can get pretty decent results with just ORDER BY (based on the sample values you posted) without having to resort to soundex
[sakila](http://dev.mysql.com/doc/sakila/en/), one of several [sample mysql databases](http://dev.mysql.com/doc/index-other.html)
http://imdbpy.sourceforge.net/docs/README.sqldb.txt
by default a sequence uses NOMAXVALUE which has a maximum value of 10^27. But lets get serious for a second, if you specified 1 billion do you really think you will hit that number? http://docs.oracle.com/cd/E11882_01/server.112/e41084/statements_6015.htm#sthref4920
http://code.google.com/p/worlddb/
Really poor articles on sql, they are so trivial and basic as to be useless. You'd be much better just reading the sql reference. As an example: http://www.w3schools.com/sql/sql_create_table.asp Anyone using this as a reference will have no idea that character sets are important aswell as collation. Where is the discussion about identity fields, the significance of NULL'able or NOT NULL fields, and the important of keeping primary keys compact? Defaulting varchars to 255 characters for no good reason is just a throwback from the early 90's. Who has a firstname of 255 characters? This is probably why this website exists... http://www.w3fools.com/
Do something like this: insert into LogTable (Time, CPU) values Substring(&lt;string variable as read from your text stream&gt;, 1, 8), Convert(Real, Substring(&lt;string&gt;, 10, 99)) Substring is slightly different in oracle, but there is a corresponding function. Real might not be an appropriate data type, but the insert statement structure is pretty universal.
Can you add a newline character in your bash script perhaps?
I did that but it did not work :(
It did not work :(
Excellent! Thank you exactly what I'm looking for!
 SELECT Points FROM [table] WHERE username = 'some_user' I may have misunderstood your post but is this what you want?
I want to do something like this: currentPoints = new MySqlCommand(" select Points FROM employees where username = " + currentusername_txt.Text).ToString; where currentPoints is an integer I declared earlier. I sort of want to load the "points" to that integer. I KNOW this is wrong. Just blattered something out so you would get a hang on what I am trying to do.
Assuming that the CSV contains all the columns in the SQL table in the appropriate order and that there are no commas in the actual data : BULK INSERT #tablename# FROM 'C:\whatever\path\to\file.csv' WITH( FIELDTERMINATOR = ',' ,ROWTERMINATOR = '\n') This command isn't great when importing data containing commas since it doesn't allow for quotes around the actual data. There are ways to do this without using the BULK INSERT command but I'd need to know more about how your data is structured to come up with an appropriate solution.
There are problems all over that... first things first... did you setup your SQL connection properly including the SqlConnection, SqlCommand and SqlDataReader? Then you need to set your command text, assign a datareader to the command, and then interrogate your datareader. Here: http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqldatareader.read.aspx
I would create a parameter and not concat a string into your select statement as that opens you up to injection attacks. SELECT Points FROM employees where username = @textboxusername Read more about parameters here: http://www.csharp-station.com/Tutorials/AdoDotNet/Lesson06.aspx AND http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlparameter.aspx 
What DBMS are you using? This is very easy to do in MSSQL as you can create a SSIS package.
Are the characters consistently the same for the first block of numbers ? IE in your example it is xx:xx:xx (8 characters)?
Yep. And if you want it to load the data every day, you can set up a job to execute the package on that schedule.
MS SQL
check out ssis and bids. ssis packages are quite good for bulk data loading. 
Set it up so you don't append data that is already in your destination table. This usually done by comparing your new source data with a prior version of the source data, or comparing new source data with destination table. This comparison enables you to select only the data that truly new for insertion into destination table.
for inner joins, it doesn't really matter for left joins, the condition should be in the ON clause, because if it isn't, then you will only get inner results
To add to r3pr0b8's answer, here's a relevant article. [What’s the difference between a filter on the ON and the WHERE clause?](http://www.jimmcleod.net/blog/index.php/2011/04/11/whats-the-difference-between-a-filter-on-the-on-and-the-where-clause/)
You have to try to make some sense. Is it your website (as in you administer the code) or just a site you are visiting (like amazon)? What's Admin Panel, is it part of a browser or part of a web design toolkit? What is the purpose of the CSV? If you have the customer info in a relational database, then you probably need SQL to get it. But that's the easy part. I think you need to check some web design forums before you ask any SQL questions.
I have total access to the site it's our small business website its ecommerce. So I asked the company who made our website to do this and he said $65 and I just did some research and I just have to maybe put in a query and export out to csv, but don't want to mess up my website. The website will be going down at the end of the week and I'm opening up a new one and all I need is csv file to just transfer all my old customers. *edit I'm looking from admin panel and it says mysql manager .01?
Okay how about this. I'm looking at a back up from 2 years ago of the SQL which is many pages long. I find a customer and all my customer is in there in a weird jumbled up csv format. Is there anyway to organize it in a more efficient way? I'm dealing with a few hundred accounts. 
Do you have Excel? Excel is really good for opening and editing CSV then saving it back out (as CSV). The alternative is to just focus on the MySQL database and write the SQL to get what you want. SQL is pretty simple. You may just want a "select * from mycustomers" then your query tool should have some options to save or copy the results. If it's copy, then just paste in Excel and save as CSV.
w3schools has a (deserved) [poor reputation](http://w3fools.com) for its dodgy html/css stuff, and its sql stuff is just plain too trivial to be very useful
Where is a good site to learn SQL past the basic select statement that has examples/its own practice area?
if i knew of a better one than sqlzoo, i would've said
Do I have to use the MySQL program or can I use the SQL in the admin panel of the website? Thanks a lot for your help.
Thanks! I've generally found that putting the limiters in the ON clause speeds up the query, but I hadn't seen it from this perspective yet.
Seems so in a basic way? So I know which database I have to click on then I get to where it says Query, Query from file, Select, Create Table, Create Database. I'm guessing I have to put the code in Query. I researched some stuff on the code to put but is it going to mess with my whole website?
I don't think it will change your website at all. It looks just like a query tool for testing. Use the one in the middle "Select * from customers" then the middle execute button. Then it should return your data in a format you can save as CSV or copy into Excel (then save as CSV).
That's what my experimenting turned out, but I wasn't sure why. Is there any reason that it doesn't matter for inners? My first impression is that the condition on the join would only return those results, whereas putting it in the where clause would return the whole set, then apply the condition. However, since my execution times don't change, I'm going to assume that the compilation of the procedure moves it up for me?
&gt; putting it in the where clause would return the whole set, then apply the condition it's okay to think about it that way, but that's not actually how it gets executed database optimizers are pretty smart 
Sorry for making the [MySQL] a post-fix
It makes it hard to keep your love a secret when you do such wonderful things. :)
Under services, get the properties of the MSSQLServer entry, you should see the path to the executable (service process); copy that entire value into a command prompt and run SQL Server out of that. It will spit out any errors it encounters. Some common ones I run into is the master database's data files got nailed after a dirty shutdown and it's suspect; hopefully you won't have that. :)
Please read the flipping side post: &gt; When requesting help or asking questions please prefix your title with the SQL variant/platform you are using within square brackets like so: [MySQL] [Oracle] [MS SQL] [PostgreSQL] etc Second, what the hell do you mean by `give me this output`? What is `this`? &gt; I am just so stuck on how to compare one set zip codes...in a column against another set of zip codes, both in regions is regions a table? I have not heard of it before you mentioned it, and that makes this seem like it doesn't make sense. Instead of approaching it the way you are, please rephrase: &gt; I have a table called `table_a` with columns `id`, `fooid`, `derp`, `herp`, `fudge`. I have another table called `foo`, with columns `id`, `name`, `key`, `whatever`. `foo`.`id` is keyed to `table_a`.`fooid`; many records can exist in `table_a` that refer to the same column in `foo`. We can't guess what the hell you're trying to say, and it's really frustrating. If you don't know the answer, you are most likely going to ask the wrong question, so instead, can you tell us exactly what you have, and exactly what you need? I think you want a selection of rows out of some table (or tables being joined) where two values (origin zip code and destination zip code), but there's no organization to your thought process; please try to use proper grammar. Read what you're saying out loud and make sure it makes sense. Yes, I'm picky, but it's only because computers are pickier. 
first query (unchanged except for formatting) -- SELECT * FROM table WHERE date='today' AND column1='1' AND column2='2' OR date='today' AND column1='3' AND column2='4' second query just negates the first -- SELECT * FROM table WHERE NOT ( date='today' AND column1='1' AND column2='2' ) AND NOT ( date='today' AND column1='3' AND column2='4' ) edit: copy/paste errors
I actually figured it out with the help of my coworker today. I will post the SQL if anyone wants it.
I don't really understand what you're trying to do but what about: SELECT * FROM table WHERE 1 = 1 AND date = 'today' AND ((column1 &lt;&gt; '1' AND column2 &lt;&gt; '2') OR (column1 &lt;&gt; '3' AND column2 &lt;&gt; '4') )
It may help others, but your problem is not really that type of problem; there was no error message or generic issue that would bring a googler here, so I wouldn't worry about it. It's always appreciated to edit the original message saying it's resolved though, with the resolution if you think appropriate 
This seems to be getting the same results.
Yes I can see how it would, any ideas for how I can do it correctly?
It's a little hard w/o more info... SELECT * FROM table WHERE ((date='today' AND column1='1' AND column2='2') OR (date='today' AND column1='3' AND column2='4')) union SELECT * FROM table WHERE ((date='today' AND column1&lt;&gt;'1' AND column2&lt;&gt;'2') OR (date='today' AND column1&lt;&gt;'3' AND column2&lt;&gt;'4')) and id not in --primary key from table (SELECT id FROM table WHERE ((date='today' AND column1='1' AND column2='2') OR (date='today' AND column1='3' AND column2='4'))) ?
do what correctly? did you try my query?
I'm going to recommend something tried and true: PluralSight was a pretty good investment for me with Business Intelligence, and the stuff I've seen for SQL is solid. There is a free version AND a paid version - the difference is you get all the class files with the paid version, but it's easy enough to follow along for free. Teaches you the terminology (big help) and more importantly, good habits.
 Select * from mytable where id not in (Select Id from mytable where category in (Select category from mycatagories where categorytype = 1) The "Inception" of queries, you don't need to know any more. I'd go deeper but that query may cause your database to hang for eternity but to the data, it would seem to be just a moment.
[Here's the link for the AdventureWorks database](http://msftdbprodsamples.codeplex.com/downloads/get/478214)
Can you see if this works? I am assuming the DB is Oracle. delete from table a where a.event in (select event from table b where a.event &lt;&gt; b.event and a.start between b.start and b.end - 1); (not tested) And in your example why is &gt;J 22 27 (keep - this starts a new set because it’s the first record after G with start &gt;= G) 22 falls between 17 and 27.
The thing that I'm struggling with here is that all the logic is dependent on the very first row. The fact that I'm starting with A sets a precedent of keeping or excluding all other rows based initially on A's start and end time, because the next row to 'keep' is the first tow that starts on or after A's end time. So in my example, based on the dependency established by A, it's keeping E, G, and J. J (start = 22) is kept because it's the first row on or after G's (end = 22) end time. I is excluded because it falls between G's start and end time.
Do you really want to make it in pure SQL ?
You. Are. Awesome!
The platform will eventually be Hive/HiveQL, but I'm currently analyzing the it and working out the code on Teradata.
I don't really see a way of getting around this that doesn't involve RBAR (row by agonizing row)... meaning... a loop. Since you aren't able to just compare rowID N against rowID N-1, you won't be able to do this set-based. :-(
yeah, after playing around with it for a couple hours, I couldn't get it to work with self-joins, but it seems like it might work with SQL recursion. I'm not that much of an expert with SQL, so thought maybe someone would have done something like this with simple SQL.
So, it doesn't mean a hill of beans difference to an INNER join, however a LEFT would benefit from having the filter in the ON clause rather than doing an WHERE (foo is null OR foo = condition). It's because of the OR.
Yea, the thing is, you kinda have to hold the last TRUE row's values in memory until such time as they're surpassed. Since that isn't always "compare this row against the row directly above it" you can't really do recursive join ON ID=(ID-1)
I blew it away and started from scratch, works fine now. Which means it will work fine forever! Huzzah!
Blobs
The Schemaverse :D Learn SQL and play a game at the same time http://schemaverse.com
`mi` is not one of the valid interval codes try `n` instead
Just tried that, didn't work. Still got a pop-up asking me to enter a parameter value after running the query.
*after* running the query? what exactly does the popup say?
 SELECT DateDiff('n',TimeStart,TimeEnd) AS Duration FROM [Activity Log];
That worked, thanks!
Try this http://sol.gfxile.net/galaxql.html 
It wouldn't hurt to download postgreSQL or MySQL and make some local databases either. 
I feel retarded but I was actually really confused by this game...
It has a steep learning curve but once you get into it it can be really addictive and equally helpful. If you are interested in playing but you had some difficulties, drop by the #schemaverse channel on irc.freenode.net. We are always happy to help answer your game/sql questions. 
 All of the above. I use DB2 on z/OS at work, previously used Oracle and MySQL in school, and PostgreSQL and MangoDB for personal projects.
I use it to store non-binary data in a relational manner. Usually to assist web pages.
But not to retrieve?
It's awfully hard to get motivated to help out someone whose username is "Because fuck you that's why".
Nope, I just like to hoard data in poorly indexed tables with comma separated varchar fields and watch the report writers heads spin as they try to make sense of it.
Databases are *highly* optimized for fast storage and retrieval. It is not writing the whole database for each transaction. Don't worry how they work with storage (at least at first), just learn proper coding patterns for using a database (eg parameterized stored procs). The real answer: they hold the file open the whole time and just read/write the parts needed to service a query. You can store binary blobs (eg JPGs) in a DB, but often just paths/pointers/meta data actually goes in. You can also store XML in a database. Your first go-round might be to see if you can simply use a RDBMS for storing your existing XML instead of the file system. Same XML, just reading/writing it DB instead of files in folders. Yes, you should use a relational database. Definitely. It is amazing you got that far with XML and the file system, but RDBMS's are popular for many good reasons.
SELECT * FROM Table_A INNER JOIN Table_B ON Table_A.a_number = (Table_B.b_number - 5) Yes. You can do math with joins. Just make sure you're storing them as integers or floats. If they are character/stringy/blobby type of storage, you'd have to do casts, which dramatically impact performance.
Reporting for big data. Not BI reporting like you get with tools like cognos &amp; business objects, but storage engines to quickly query big dimensional data. Salesforce bought Edgespring to bridge this gap. A guy who co-wrote Googles dremel tool quit Google to start a stealth company (I'm guessing it's to commercialize dremel). Plus I think too many companies focus on data warehousing and too few on how to deliver the data to users.
&gt; I laughed out loud. Oh sure... laugh all you want. :-p I'm confident SQL can handle lists quite well. But I must have watched over a dozen "Intro to SQL" type videos and not a single one even mentions lists. The only way from what I've been shown so far to make a list is to make a new table for that list. And that would be thousands of tables just for the lists, and that doesn't seem right at all.
&gt; The only way from what I've been shown so far to make a list is to make a new table for that list. not thousands of tables, no one table for each *type* of list, so one table for all bands, one table for all users, and one table for all band-user relationships, three tables in all
note the parentheses are not ( required )
Are there any good resources for database design? I could ask you question after follow-up question, and I'm sure you don't have the time for this bag of hurt :-) But just to set the tone, say there are 5,000 bands and 5,000 users. Does that mean a 5,000x5,000 table with a check-mark or something in the field if the user favorited that band? How else would you do it? Also, each band will have a list of the members. And each member would have the instrument(s) they play, and a user id if they're on the site (linking to their profile page and giving them editing permissions for the band profile if that user is logged in). Would that be another table just for band members? What about the list of their genres, would that be another table? What about the list of their albums, would that be another table? If yes, this database is going to have about 100 tables by time I'm done designing it.
&gt; Does that mean a 5,000x5,000 table with a check-mark or something in the field if the user favorited that band? not exactly the relationship table would have one row for each user/band say on average users favourite 15 bands, then there would be 75,000 rows in this table it's a very narrow table, though -- only two columns, the userid and the bandid do a search for "many-to-many relationship design"
Awesome, thanks. I just watched [this video](http://www.youtube.com/watch?v=P_nhBKs25DQ), and it seemed to explain how that would work. I assume all the SQL varieties will be super good/fast at adding/removing items in middle of the table. Will it always keep it sorted by the first column? Do IDs in SQL always need to be an integer? I only ask because I already have a 4-alphanumeric-character ID associated with each band. I guess could just add that to each band info row, but it seems a little redundant for each band to have two unique identifiers. Should there be much caching of things? Like say.. I have 5,000 bands and depending on the application, I want to access them in alphabetic order, or by ranking, or by join date, etc. In my XML system, I just cached pre-sorted versions all of them regularly so I had the data in the format that's best for the system ready to go. If it's a sort that an application requests a lot for the same results, wouldn't it make sense to cache them? Or does the database do something funky there and remember the last sort as long as no data has changed or something?
&gt; Will it always keep it sorted by the first column? tables, in general, are unsorted... do a search on indexes &gt; Do IDs in SQL always need to be an integer? nope, your 4-char identifier will work just as well &gt; Should there be much caching of things? no... you can do it if you want to, but in general you don't need to
Sounds good. It feels like the past few weeks I've just been watching SQL YouTube videos non-stop :-p What should I search for to research dynamic values. Say... in the bands table, there's a column called "score", and it is equal to the sum of all the scores/karma of all the users who favorited them (and I'd have that band-users bridge table in there). So if any of the user's "karma" changed, the value given from the band's "score" field would also change.
Untested select i.DEPOTNUMBER, S.MONTH, sum(case when i.TYPE_OF_TRANSACTION = "SELL" then 1 else 0 end) as CNT_SELL, sum(case when i.TYPE_OF_TRANSACTION = "BUYI" then 1 else 0 end) as CNT_BUY from IMPORT I join (select S.*, datename(month, DATE_BOOKING_DEPOT) as "MONTH" from STATISTIC S where S.DATE_BOOKING_DEPOT &gt;= "2013-04-01" and S.DATE_BOOKING_DEPOT &lt;= "2013-08-30") S on S.STATISTIC_ID = I.STATISTIC_ID where I.PAFILL = "N" group by i.DEPOTNUMBER, S.MONTH
i haven't used SQLServer in a while... when did it start to allow the use of the doublequote to delimit strings, e.g. PAFILL = "N" i thought doublequotes are standard sql to delimit identifiers
This is on a Sybase Adapative Server 11.9 and it supports both types of quotes: single and doubleqoutes. You can even mix them up like this: select "This's what i'm talking about" select 'This"s what i"m talking about' just make sure to end the string with the quote-type you started it with
hmmm...that join gives me an syntax-error...but I understand the way you want to solve it. thanks for your input.
I don't have a T-SQL DB, but post the syntax error and I'll fix it.
thanks, that's very kind of you select i.DEPOTNUMBER, S.MONTH, sum(case when i.TYPE_OF_TRANSACTION = "SELL" then 1 else 0 end) as CNT_SELL, sum(case when i.TYPE_OF_TRANSACTION = "BUYI" then 1 else 0 end) as CNT_BUY from IMPORT I join (select S.*, datename(month, DATE_BOOKING_DEPOT) as "MONTH" from STATISTIC S where S.DATE_BOOKING_DEPOT &gt;= "2013-04-01" and S.DATE_BOOKING_DEPOT &lt;= "2013-08-30") S on S.STATISTIC_ID = I.STATISTIC_ID where I.PAFILL = "N" group by i.DEPOTNUMBER, S.MONTH go gives me: Msg 102, Level 15, State 1: Server 'XYZ', Line 6: Incorrect syntax near 'join'. Msg 102, Level 15, State 1: Server 'XYZ', Line 8: Incorrect syntax near 'S'. 
This executes without syntax error on SQL Server 2008, T-SQL (don't have your dataset, so haven't checked operation) : select i.DEPOTNUMBER, S.MONTH, sum(case when i.TYPE_OF_TRANSACTION = 'SELL' then 1 else 0 end) as CNT_SELL, sum(case when i.TYPE_OF_TRANSACTION = 'BUYI' then 1 else 0 end) as CNT_BUY from IMPORT I join (select S.*, datename(month, DATE_BOOKING_DEPOT) as "MONTH" from STATISTIC S where S.DATE_BOOKING_DEPOT &gt;= convert(datetime, '20130401', 112) and S.DATE_BOOKING_DEPOT &lt;= convert(datetime, '20130830', 112)) S on S.STATISTIC_ID = I.STATISTIC_ID where I.PAFILL = 'N' group by i.DEPOTNUMBER, S.MONTH
i would try including the calculation like this: and (amount-amountSpent) between 150 and 1000
HAVING keyword will get you there i.e. SELECT foo, count(*) FROM tablename GROUP BY foo HAVING count(*)&gt;1 or NESTING will also get you there SELECT *, calcVal + 1 as CalcValPlusPlus ( select foo, calc_val1 + calc_val2 as calcVal FROM tablename ) t 
I can't seem to get either of these to work. I am likely just misunderstanding, but I don't think the HAVING works because I'm needing to subtract one column from another, resulting in a value represented in a *new* column which I then want to find values on between x and y. I'm not seeing where the math is done on two fields, creating a new one. The NESTING, when I turn that into the actual column names, it says "must specify table to select from" as well as "invalid column name 'balance.'" I have a giftCode field, an amount field, an amountSpent field. Those are the 3 actual fields I have to work with. I'm in table dbo.Discounts and I want to subtract amountSpent from amount, and get a new value called balance, then limit the results of the query by balance between 150 and 1000. If NESTING would still work for this, could you give me the actual example? I might just be failing to link each value to what's in your example...
You create an inline view and select from it. select m.* from (select ....., AMOUNT - AMOUNTSPENT as BALANCE from MYTABLE where ....) m where m.BALANCE between 150 and 1000 or if your RDBMS supports it, you can use a common table expression (CTE). with qry as (select ....., AMOUNT - AMOUNTSPENT as BALANCE from MYTABLE where ....) select * from qry where BALANCE between 150 and 1000
Use the expression more than once: select amount-amountSpent as balance where amount-amountSpent between 150 and 1000
I'm definitely going into SQL as a complete newbie, and over the past week I've been trying to learn as much as possible to see if it's something I think I can do as I'm in middle of the project and I don't want to put the whole thing on hold as I try to learn SQL. I'm not sure what you mean by "Playlists table". The word "playlists" hasn't been used anywhere else here.
&gt;It's a music site, and each user can have a list of their favorite bands
Oh you mean favorite band list? I think of songs when I hear "playlist". I think [r3pr0b8's comment](http://www.reddit.com/r/SQL/comments/1lb8mf/thinking_about_moving_from_xml_to_sqlsqlite/cbxksq3) helped me figure out how to make a bridge table for favorited bands. I'm not 100% sure what you mean by "you probably only have a dozen or three types of lists" though. A dozen or three sounds pretty specific but random. I'm sure I will likely have about 20-30 of these bridge tables though. 
If I were you I'd use MySQL... mainly because I'm really familiar with it so I may be biased. Go download MySQL and start playing with it. Create a database, create a few tables, create primary and foreign keys on these tables, insert data, run complicated select queries, run some joins, triggers, stored procedures, etc. Start googling those keywords as you're actually using SQL and you'll pick it up pretty quickly. I think you're crazy doing thousands of xml files.
Thanks!! That's exactly what I'm looking for
Similar to the notion that "The best camera is the one you have on you", I think it's true that "the best programming language/data-storing schema is the one that you know". I've coded dozens of sites using Perl/XML which in retrospect would have been far better suited in PHP/SQL. But I didn't know PHP or SQL... so Perl/XML was the right choice for me because I knew them so well and knew how to get the job done with them. I think it's a similar reason that you're suggesting MySQL. I'm choosing SQLite because I think it offers the relational database features that I can't quite emulate with my XML set up, but it does keep the data in a file I can access at any time, which I really like. It's not quite as easy as XML, but it's a start. I have made a SQLite database, and entered some data into it through phpLiteAdmin, and successfully pulled that data out through PHP for a select menu. So that's a start. And I've spent about 30 hours so far studying videos and such, and have a long ways to go. My eventual database I'm guessing will have dozens of really complex tables, just about every one having several columns of foreign keys. I still haven't even seen a hint as to how to get dynamic values, meaning to have a value in one table be calculated by doing lots of math from values of other tables, whose values are also dynamic. 
&gt; You can either set the primary key to the band name, or just an id number you auto increment. I would actually use the bandID (a unique 4-letter code I already have for each band). Some bands have the same name. Similarly for users, I'd make their unique username be the primary key. Here's a question though- I'd keep the username primary keys as all lowercase, but they can format the name with any capitalization they want... I guess that would have to be a separate column in the database? One un all lowercase, and one un user-preferred case? And nice, someone else did teach me about the bridge table (UserBands as you call it) before, but this is the first I'm seeing the "join" command used, it's especially helpful in the exact context I'd be using it, so thanks for that. Now if I was to throw another wrench into this mess... say a user has a choice of two levels of favoriting a band (say... "like" or "love"). Would that be a third column in the UserBands? If so, what would be the effective way to store that? There's only two levels so it could be binary. And how would that effect the results from the Select query that you showed me?
CTE WITH SourceQuery AS ( SELECT amount-amountSpent as balance ) SELECT Balance FROM SourceQuery WHERE Balance between 150 and 1000 
You have to alias a subselect select t.* from (select foo, bar, baz FROM Tablename) t 
Ok guys, I did it but I made it with a loop and a temp-table to store my results and display them at the end: begin transaction create table #out (OUT_ID numeric(38) identity not null, DEPOTNUMBER varchar(13) , MONTH varchar(15) , CNT_SELL int , CNT_BUY int ) declare @month_int int --to save the current month to inspect select @month_int = 4 --start with april while ( @month_int &lt;= 8 -- run 'till August and @month_int &lt;= 12 ) --there is no 13th Month... begin insert into #out (DEPOTNUMBER, MONTH , CNT_SELL, CNT_BUY ) select DEPOTNUMBER , case when @month_int = 1 then "JANUARY" when @month_int = 2 then "FEBRUARY" when @month_int = 3 then "MARCH" when @month_int = 4 then "APRIL" when @month_int = 5 then "MAY" when @month_int = 6 then "JUNE" when @month_int = 7 then "JULY" when @month_int = 8 then "AUGUST" when @month_int = 9 then "SEPTEMBER" when @month_int = 10 then "OKTOBER" when @month_int = 11 then "NOVEMBER" when @month_int = 12 then "DECEMBER" else "ERROR" end as MONTH , sum(case when TYPE_OF_TRANSACTION = "SELL" then 1 else 0 end) as CNT_SELL, sum(case when TYPE_OF_TRANSACTION = "BUYI" then 1 else 0 end) as CNT_BUY from IMPORT where STATISTIC_ID in ( select STATISTIC_ID from STATISTIC where DATE_BOOKING_DEPOT &gt;= convert(datetime,"01." + case when @month_int &lt; 10 then "0" + convert(varchar,@month_int) else convert(varchar,@month_int) end + ".2013",104) and DATE_BOOKING_DEPOT &lt;= convert(datetime,"30." + case when @month_int &lt; 10 then "0" + convert(varchar,@month_int) else convert(varchar,@month_int) end + ".2013",104) ) and PAFILL = "N" group by DEPOTNUMBER select @month_int = @month_int + 1 end select DEPOTNUMBER, MONTH , CNT_SELL, CNT_BUY from #out commit transaction go
sorry, I still get an Error. Anyways, check my own comment for my solution. I constructed a loop and used a temporary table. thanks for your help
Select ...., Amount - spent as balance, ... From ... Where amount - spent between 150 and 1000
It all depends on the way your data is modeled. If you have a lot of collection and relation type data (there are a 1000 versions of this object/data point, each of which has 0-100 'children' that are identical), then move to SQL. If your data is amorphous, or has very loose relations, then use a graph database: http://en.wikipedia.org/wiki/Graph_database
I guess SQL is a better choice for my data. My big concern is that it will be a really big database, and I want to make sure to do it right, and I'm going into it knowing next to nothing about SQL. I have learned quite a bit the past week, but not to the point where I'm comfortable feeling I know the best way to design the database or read/write from it.
I also want to point out that another concern of mine is a server crash of sorts. A friend of mine just had her server crash, corrupting the MySQL database to the point it's not recoverable. At least with SQLite, I feel more confident with my daily back-ups when all the data is saved to disc instead of memory. I know there are ways to back up MySQL databases, but this makes more sense to me.
First, I think you're misunderstanding what SQL even is. You're not going to access anything in a SQL database by opening a file, you're going to run a query against the database to get the data you need. SQLite and MySQL will both be the same if MySQL is setup using innodb as your storage engine. The point of a database isn't to add complexity. The system you had before sounds really complex, with thousands of xml files as you say. 
With the complexity you're looking for you should then start moving into more of an ORM or start using a php framework (yii activerecord would work nicely). Otherwise, you could do it as a third column in the Userbands table. There shouldn't be any change to the query since you're selecting all from the table (which is what the asterik means). 
 select DEPOTNUMBER , datename (M, DATE_BOOKING_DEPOT) AS Month , sum(case when TYPE_OF_TRANSACTION = "SELL" then 1 else 0 end) as CNT_SELL ,sum(case when TYPE_OF_TRANSACTION = "BUYI" then 1 else 0 end) as CNT_BUY from IMPORT as i Inner join STATISTIC as s on i. STATISTIC_ID = s.STATISTIC_ID where PAFILL = 'N' and DATE_BOOKING_DEPOT between '20130401' and '20130830' group by DEPOTNUMBER , datename (m, DATE_BOOKING_DEPOT) sorry about format, just written this on my phone, should work though.
She's not a technical person by any means, it would be her hosting company if anyone who messed up. Her facebook post read "just suffered a catastrophic website issue. My host moved things around and somehow, my database got "lost". I am currently in the process of downloading all my files and will be moving things to a new host. it was the database for WordPress. The server died, The backup didn't have a copy of it it and it's just a mess."
MySQL still adds an extra layer of complexity with usernames/passwords that I really don't want to mess with. I feel like I'm settling in ok with SQLite. The final database will probably have about 200,000 fields or so in all (very rough number) at an average of maybe 20 bytes each spread over a few dozen tables. So actually the data size may be under 10MB then. Maybe?
I have read [this article about SQLite corruption](http://www.sqlite.org/howtocorrupt.html) and it seems pretty safe. Though that is one thing I definitely liked about XML files. One file messing up wouldn't mess them all up, and I can open up the corrupted file and just fix it by hand.
More data does NOT mean it will be more complex. It's how you structure the data that matters.
Sounds like she's just paying for hosting from a company. If she knew how to setup a lamp stack herself a VPS would be a better option and that way she'd be in charge of everything. You really get what you pay for when it comes to hosting. I just don't understand why SQLite over MySQL. They are both the same thing.
I didn't mean to imply that more data means it is more complex, if that's how you read it. Just saying that not only will it be complex.. but big and complex.
She's not a computer person at all... just a radio DJ with a popular (now completely deleted) blog. You can't expect everyone in the world to know this stuff. I have no idea who she was using for hosting, but she's obviously changing hosts now. But still... if you have a hosting company that runs a MySQL server, that implies you at least have an idea of how things work... and they still managed to lose everything. SQLite, being file-based, is closer to my area of expertise, and I'm more comfortable with it. So that's why I chose SQLite. You can read an [article comparing MySQL and SQLite here](http://www.sobbayi.com/blog/software-development/decide-sqlite-mysql/). While they are both based on SQL, there still are plenty of differences.
Just do it and you'll learn on the way. Just FYI, your database doesn't sound big at all. For my job I'm constantly working with 500GB+ tables, most databases consisting of over 1TB of data. I have a smaller MySQL database running at home on my server for political data, it's over 3GB in size which is well over 20,000,000 records total. No problems.
You must be reading a different post than I. To get that from what I said is like if I said "That person is tall, with dark hair" and think I mean dark hair means a person is tall.
There seems to be a lot of ignorance about SQLite by those who don't use it. You don't need to do any kind of dump to back it up. It's one file that sits next to your web files (scripts, css, images, etc). And SQLite's own documentation says [any site that gets fewer than 100K hits/day should work fine with SQLite](http://www.sqlite.org/whentouse.html). That's "site", not database. I also recommend reading [venzen's post on stack overflow](http://stackoverflow.com/questions/14217249/sqlite-and-concurrency/17132843#17132843). And [here's an article from five years ago when SQLite was still in its infancy](http://www.bookgoldmine.com/Blog) talking about how well it worked for their website.
Yes, you do need to do a dump to back it up. You can't just copy the file and place it somewhere else as a backup. Please don't come in here with no SQL experience and then try and tell those that have years experience how it works. You are the ignorant one in this instance, not anybody here that I've seen. You can't back it up at a file level by just copying it because it's a database that's in use. With the amount of time it would take to copy the file changes could be happening on the file. In fact, the way you're suggesting backing it up would cause data corruption. Look, quit being so stubborn, *you* don't understand how these systems work, not us. Switch to MySQL or postgresql, you will wish you did in the future. I can almost guarantee that the websites these people are talking about are heavy on reads to the database. Your website sounds like there will be quite a few writes, considering that users can pick and choose which bands they like, this will bog SQLite down a TON. Keep thinking you're right though, I only have a combined 8 years experience in database administration across MANY different database systems (SQL Server, MySQL, Access {yuck}).
The simple fact is the only people suggesting not to use SQLite for websites are those who don't. Those who do use it for websites say it's great. I value the opinion of those who use a product more than those who refuse to.
Would add brackets for clarity, but otherwise this is how I'd do it. It's definitely better to split it up in to two clauses as opposed to a BETWEEN because it becomes inherently easier to understand exactly what the bottom and top numbers allowed are.
I can highly recommend the SQL cookbook - it splits up SQL into tasks and gives you the syntax in different databases. (http://www.amazon.co.uk/Cookbook-Cookbooks-OReilly-Anthony-Molinaro/dp/0596009763) For removing cursors (which I would suggest you do unless you are doing stuff that needs a row by row calc) there is a great article on SQL Server Central: (http://www.sqlservercentral.com/articles/T-SQL/66097/)
You don't even understand how the software works, yet you're completely convinced in using it. I don't understand why you're so invested in a technology you haven't even used yet. SQLite is NOT going to scale. It is mainly for reads since it locks the whole database to write to it. The main thing I want to clear up with you is that you can't just make a copy of the database file and consider that a backup. That is NOT a backup. If you make a copy of the database as a backup, it will be a corrupted copy. It seems you're mainly sold on SQLite because it's a single file, there are NO benefits to being a single file. EDIT: Logically what you're saying here doesn't make sense... of course the people that primarily use it will recommend it and those that don't will probably not recommend it. 
thanks, I appreciate the recommendation!
If you really thing SQLite won't scale... you think I'm ok sticking with the XML database system I'm using now? Because that works for me.
SQL server drives our EMC Centera data stores (90tb), as well as our corporate messaging surveillance solution (DataMinder). Big data, yay. 
No, I think you need to move to a good web database platform. That being either MySQL or PostgreSQL.
I understand where you're coming from, but seeing as how SQLite is far faster and a big upgrade from my already-working XML database system, I think I'll be ok. And as I said before, it looks relatively easy to upgrade the database to MySQL down the road if I really feel SQLite isn't cutting it.
That is weird. You could add a temporary string column to MyTempDB.dbo.contracts and update it as well with all the fields in all the where predicates? Maybe you're missing something. 
In my early days, when I realised I could use case statements in joins my mind was blown. 
Hey, me again. Sorry to bug you, but you seem pretty smart and helpful with these things. What about a relationship between like things with a varying degree? Say... a collection of zip codes that each has a distance value to each other zip code. So zipX to zipY is always equal to zipY to zipX. I assume there's a clever database-way of doing it without storing each distance twice, right?
not really sure what "like things with a varying degree" might mean, but the idea of not storing duplicate pairs of values is easy -- just always store the lower one first
I guess what I meant by "varying degree" is that it wasn't binary. Like a distance isn't restricted to 1 or 0. I thought about storing the lower one first, so there are three columns... lowerZip, upperZip, and distance. But I would often do queries like "Show me all zip codes within a 50 mile radius of zipX", and the lower/upper method didn't seem to make the most sense for that. Or is there a good fast query (knowing that there are some 50,000 unique zips) that would work?
You can use a case statement or decode (if you're on oracle). I'm assuming here that you've already joined table 1 (t1) and table 2 (t2) in a way that makes sense: case when t1.code_value = 20 then t2.A when t1.code_value = 30 then t2.B end decode(t1.code_value,20,t2.A,30,t2.B)
That was the hardest problem I have ever tried to work on and it was awesome. I spent 4 hours and came to the conclusion that it is impossible from my skill standpoint. I was able to work with temp tables and variables to try and create some way to solve this problem. The best I came up with and in hindsight was way off the mark was: declare @EID int = 0 declare @S int = 0 declare @E int = 0 select case when EventID &gt; @EID then @EID + EventID + 1 End as V_EID , case when Start &gt; @S then @S + Start end as V_S, Case when [End] &gt; @E then @E + [End] end as V_E into #SpeedTest from dbo.SpeedTest select * into #Final from dbo.SpeedTest a left join #SpeedTest b on b.V_EID = a.EventID declare @start int = 0 declare @end int = 0 select * from #Final This is the table it created: http://i.imgur.com/ZKlWeOm.png The idea was to relate one set to another. I wanted to something like if start &gt;= V_E(variable End) then insert that column into a table. Because SQL works with sets i think its impossible to code like that. 
Thanks for taking a look at the problem! I also worked on it for hours, and wasn't able to figure out a way to get it to work exactly like I wanted. I think I'm going to have to try a different approach. 
There seems to be a data structure issue at the root of this. How much control do you have over the tables?
Also, what platform are you on? Does table 2 have a column for every possible value like 020_USD, 030_USD, 020_GBP, etc?
Use not exists instead of not in. NOT EXISTS (SELECT DISTINCT 1 FROM lqa_exp_file_rqst a WHERE a.dttm_exp_file_rqstd &gt; TO_DATE (sysdate - ${NDays}) AND a.id_lqa_exp_file = LEFR .id_lqa_exp_file )
1. How large is lqa_exp_file_rqst? what % of the rows have "cd_exp_file_status != 'Purged'". not equal will kill any index and sometimes it can be better to select where "cd_exp_file_status in('every', 'other', 'available', 'value') 2. does dttm_exp_file_rqstd have an index? this would help the query sing if it did seeing how you are pinging the table twice based on that date column. 3. to_date(&lt;a date&gt;) is a silly thing to do, there is no need. date - &lt;numeric&gt; is still a date *EDIT* 4. that not in query is a total waste, that date range is already filtered by "LEFR .dttm_exp_file_rqstd &lt;= TO_DATE(sysdate - ${NDays})" seeing how it is the same table and you are just getting distinct keys. if you added this not in to improve performance then I am almost certain you do not have an index on that date column and it is killing you. 5. also verify that a left join is actually needed, I find that I run into queries that use left joins when it is not actually needed and it just takes a dump on the optimizer.
Zero, and I agree it is ugly.
Table 2 has a column for everything it could be, but not every possible combination. (i.e say there is no such thing as 020_GBP or 030_MXN, there is no column)
But what is the database? If you had SQL Server you could use unPivot in a nested select to transpose the column pairs into rows). 
This is, as the good Doctor said, a design issue. There are no simple ways to do it that won't require updating the code periodically. That being said, if you are on SQL Server you could use the unpivot function in a common table expression to create a derived table of values and then join that onto your other table. declare @Table1 table ( KeyColumn int, ValueColumnName varchar(100) ) declare @Table2 table ( KeyColumn int, ValueColumn1 varchar(100), ValueColumn2 varchar(100), ValueColumn3 varchar(100), ValueColumn4 varchar(100), ValueColumn5 varchar(100), ValueColumn6 varchar(100) ) insert into @Table2 ( KeyColumn, ValueColumn1, ValueColumn2, ValueColumn3, ValueColumn4, ValueColumn5, ValueColumn6 ) values ( 1, '1:ValueCol1', '1:ValueCol2', '1:ValueCol3', '1:ValueCol4','1:ValueCol5', '1:ValueCol6' ), ( 2, '2:ValueCol1', '2:ValueCol2', '2:ValueCol3', '2:ValueCol4','2:ValueCol5', '2:ValueCol6' ), ( 3, '3:ValueCol1', '3:ValueCol2', '3:ValueCol3', '3:ValueCol4','3:ValueCol5', '3:ValueCol6' ) insert into @Table1 ( KeyColumn, ValueColumnName ) values ( 1, 'ValueColumn1'), ( 2, 'ValueColumn4'), ( 3, 'ValueColumn3') ;with DerivedTable as ( select KeyColumn, ValueColumnName, ValueColumnValue from ( select KeyColumn, ValueColumn1, ValueColumn2, ValueColumn3, ValueColumn4, ValueColumn5, ValueColumn6 from @Table2 ) a unpivot ( ValueColumnValue for ValueColumnName in (ValueColumn1, ValueColumn2, ValueColumn3, ValueColumn4, ValueColumn5, ValueColumn6 ) ) p ) select * from @Table1 t inner join DerivedTable d on d.KeyColumn = t.KeyColumn and d.ValueColumnName = t.ValueColumnName KeyColumn ValueColumnName KeyColumn ValueColumnName ValueColumnValue ----------- -------------------- ----------- ---------------- -------------------- 1 ValueColumn1 1 ValueColumn1 1:ValueCol1 2 ValueColumn4 2 ValueColumn4 2:ValueCol4 3 ValueColumn3 3 ValueColumn3 3:ValueCol3 
ick...thanks though..legacy systems make easy problems difficult
Yuck, sorry. I think the unpivot-style solution suggested by THLycanthrope and mental405 is the way to go. Hopefully you're on a platform that supports it.
If you want us to help optimize a query, **you need to show us the table and index definitions**, as well as row counts for each of the tables. Maybe your tables are defined poorly. Maybe the indexes aren't created correctly. Maybe you don't have an index on that column you thought you did. Without seeing the table and index definitions, we can't tell. We also need row counts because that can affect query optimization greatly. If you know how to do an `EXPLAIN` or get an execution plan, put the results in the question as well.
What's the left join for? It doesn't look like anything from LEF is being used.
I have to agree. I know guys who can ace any cert exam but have proven to be terrible programmers or admins. The cert shows me that you took the time to study and get it, which is a good sign, but I always look at job experience and more specifically, what have you done *lately*. This sucks if you are fresh out of school and don't have job experience and hope to bolster your resume with a cert. I like to ask real world questions to applicants just to see if they've got basic understanding. Such as "if you've got a table called EMP and it's full of stuff like each employee's first name, last name, department code, manager_name, how would you find a list of managers who have more than 10 employees reporting to them." You either get a knowing smile and a quick answer or you get the deer in the headlights. A nice way to weed out half of your applicants, cert or not. :) 
Why are doing a left outer join with lqa_exp_file? It's not necessary, you don't do anything with it. What about the following (and put an index on dttm_exp_file_rqstd if one doesn't exist). select distinct LEFR.id_lqa_exp_file from lqa_exp_file_rqst LEFR where LEFR.cd_exp_file_status != 'Purged' and LEFR.dttm_exp_file_rqstd &lt;= sysdate - ${NDays}
SQL basics havent changed since then. go for it. i got my basics from the 2nd edition i believe of this book: http://www.amazon.com/Sams-Teach-Yourself-SQL-Hours/dp/0672335417/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1378246335&amp;sr=1-1&amp;keywords=sams+teach+yourself+sql+in+24+hours 
I went to a training class for a billing system my company was using about 3 years ago. The company we were using required someone to go to training before they would allow them to have read only access to their reporting views. They taught everything with ANSI 89 joins. A little piece of me died that week. 
SQL Fundamentals are the same no matter what you are using. I would buy another book that focuses on the more specific aspects of the system you are using for stuff like DateTime conversions, string manipulation.. etc. I will also stress that Sybase and Informix are not very popular anymore. (any before you people start chiming in, yes, i realize there are many systems out there still using them, but honestly when was the last time you said... "Hey we need an Informix server in here!") 
Oh, I knew what you meant. I was just sharing.
 SELECT manager_name AS Manager, COUNT(*) AS Peons FROM EMP GROUP BY manager_name HAVING COUNT(*) &gt; 10 Will you hire me? Will you? Will you? In all seriousness, can you give a few other questions you might ask?
I wouldn't. That's an awful lot for such an out dated book. If you did get it, you would end up buying a second book to cover all the new stuff youre missing out on. While it's true that the syntax for SQL hasn't changed, there are new functions and way to join/aggregate data. Plus, there are some new features like debugging and optimizing you might not learn about.
Some databases let you join tables by just listing join conditions in the Where clause: Select * from A,B where A.Id = B.A_ID That is a non-ansi join. Ansi requires something like A Inner Join B on A.Id = B.A_ID
Sybase also getting killed in the workplace because its license feeis too high vs its competitors. 
SQL was standardized by ANSI in 1986, and the basics haven't really changed since then. That being said, the standard has been amended 6 times since then, most recently in 2011. The newer versions of the standard haven't changed any of the basics, but they have added some new features that you may very well find useful. In addition, pretty much every RDBMS (MySQL, SQL Server, DB2, Oracle, etc) has its own "dialect" of SQL that introduces some non-standard features on top of the ANSI standard. Once you have learned ANSI SQL you will want to dive into the particular RDBMS that you plan to work with. Personally, I would recommend looking for a newer book.
Not interactive, but Phillip Greenspun wrote a great series of free books on sql and web programming. Sql for web nerds and other books he write. 
It won't include analytic functions (sometimes called window functions). In my opinion the most important and useful enhancement to SQL in a several decades. It also might not include CASE statements, which allow conditional control in SQL.
... I would rather just download (I am assuming MSSQL) the express version of 2008 or 2012 and start working on it. The Management tools give you auto completion and help libraries (And are fantastic to work with), and you can then download the ~~NORTHWIND~~ ADVENTUREWORKS database that most sql tutorials use as a base for showing examples. The Express versions are great, less cpu and ram usage allowed but that should not matter, as you get all the functionality as you get with the full versions. Base SQL you will understand in a very short time, and then be bored with it. So start off with a good platform, and get to know the tools as well. As with anything if you stick with the simple things you will get bored, get the basics down and then hit the intermediate and then advanced stuff with the right tools in place. The basic stuff is easy and will give you a false impression of "Is that all there is?". No, a thousand times no, there is so much to learn after the basics that it can keep you interested for a very long time. 
That book has been updated for the SQL-92 standard, which I believe includes CTEs (Common Table Expressions) In all honesty, almost no new fundamental concepts have been added since then. I'd buy it.
for a join to produce a single record the join has to be between two tables that have a 1:1 relationship on the columns you are joining. In many cases there is a need to have a 1:N(0 or many) relationship which means table_1 has a single header record and table_2 has many detail records, think order(1) and order_items(N). So we cannot really answer your question until we know 1. the relationship between the tables in this query 2. the output you are looking for I can see that you are referencing a column from every table and I am take a shot in the dark but i bet VW_LOGSA_MMDFLOGS has multiple records for each comb.lin becuase it looks like it could be a log table. Also, are you working on the Redstone Arsenal in the Sparkmen Center? I was a consultant there for a while and all of this information looks very familiar. I am sure not obfuscating this is some sort of [DIACAP](http://www.diacap.org/) violation
Nope, working in Rock Island. mmdf table only has one entry for each comb.lin I'll put up a diagram of where I think the issue is. 
an easy way to figure where the duplication is, is to just do a select * and keep scrolling to the right of whatever developer tool you are using and look for the repeating column values. whatever table is holding those columns is causing the duplication and you need to asses the relationship you want to use from that table. it could be as simple as joining to a (select distinct &lt;columns&gt; from table) or look to see if a table does soft updates and deletes and all of the rows are versioned. you would then have to see how the current version of a record is defined, usually there will be something like end_date is null, or end_date is some date way off in the future.
Just try commenting out joins, then adding fields and joins from one table at a time until you find the ones that add the dupes.
Well, one cheap, quick, but ugly solution is to slap a DISTINCT on the query. That will eliminate all true duplicates, but it will force a sort. I'd do that anyway as a test - if you put a DISTINCT on a query and the number of rows returned changes, then you've got duplicate rows. Many developers (and DBAs, too, I fear) use DISTINCT too freely - it covers up a multitude of sins. However, if you have a poorly designed schema, it may be necessary.
Ha! good. See? this is a nice way to find out if the guy understands select, aggregates, group by and having! All in one little package. As for sample questions.. hmmm.. maybe others can join in.. but... I usually ask stuff like.. This query is taking really long to finish, what do you look for? (hopefully they will answer indexes, statistics and improper joins) The query just returned 10,000 rows but you were expecting 100 rows, why did that happen. (and vice versa) 
try following code and see if you can narrow it down to the point where you will understand which join you have missed. Normally you get multiple records when you joined two tables on wrong column. SELECT comb.lin, mmdf.lin, ass.division, comb.division, uic.uic, ass.unit_uic FROM mgt0.vw_comb_div_readiness comb INNER JOIN mgt0.VW_LOGSA_MMDFLOGS mmdf ON mmdf.lin = comb.lin INNER JOIN mgt0.vw_comb_auth_assets ass ON ass.division = comb.division INNER JOIN mgt0.D_UIC uic ON uic.uic = ass.unit_uic I am also new here. Hope I have helped. :)
shhh, we are trying to get good sql at the end of this thread
the 10,000 row result set used a cross join instead of an inner/outer join? Still wrapping my head around cross join. 
I know - I hesitated to mention it. But there are times when it is useful. 
What do I do after I find the join which adds all the dupes?
Is this possible? DISTINCT(comb.division || comb.lin) I couldn't get it to work.
&gt; I am sure not obfuscating this is some sort of [1] DIACAP violation I'm skeptical, but edited the table names anyway.
DISTINCT would be my very last resort, this can usually be better handled by understanding the table relationships and getting a much better execution plan as a result. DISTINCT can hide bugs in the joins.
then you have to see what record in the duplicating table you want. This is usually some sort of MAX(ID) sub query or hopefully there is something like a partitioned current_version ='Y' column in that table.
Here's a link: http://philip.greenspun.com/sql/ 
SQLZoo.net helped me at the launch of my career. I couldn't recommend a better "interactive" resource for learning fundamental SQL.
&gt; the 10,000 row result set used a cross join instead of an inner/outer join? Still wrapping my head around cross join. Cross Join = every record on the left matches every record on the right. I didn't know they even existed until I found a situation where I needed it, and initially solved it with this fun (and functional) join predicate: FROM TableA JOIN TableB ON 1 = 1 This *is* a cross join. Every record in TableA matches every record in TableB. Your output rowcount will be the rowcount of TableA multiplied by the rowcount of TableB. Does that help?
&gt; Many developers (and DBAs, too, I fear) use DISTINCT too freely or use it when they don't really understand how it works not to pick on OP, but i see this all the time -- SELECT DISTINCT(comb.division || comb.lin), othercolumns... DISTINCT is ~not~ a function, it just doesn't work like that i have a feeling that OP is just seeing one-to-many results where the "one" columns are repeating but the "many columns" are different in other words, there aren't actually any duplicate rows 
You need to use CASE statements. Even if you want it to be "dynamic" (that is, you want to use a lookup table instead of having to maintain a SQL query) then you are going to end up using procedural SQL to build the CASE statement for you. Example: SELECT CASE WHEN ColumnWithValue = '020' THEN ColumnFor020 WHEN ColumnWithValue = '030' THEN ColumnFor030 ELSE 'Need More Whens' END FROM TableA Note: This can run you into some trouble if the columns have different data types. You may need to CAST them into compatible data types.
You should definitely use SQL. It's not as flexible as XML - it is picky about structure, data types and consistency - but it is *way* faster and your data doesn't have to be parsed and validated every time you go to retrieve it. In your case, I'd highly suggest SQLite. My buddy has done some volunteer work for local organizations and built some pretty cool websites using it. We're used to heavyweight enterprise engines like SQL Server and DB2/400 but ever since he tried SQLite he can't stop talking about how awesome and impressive it is for such a tiny and lightweight engine. And no, SQLite (nor any other database) is not going to re-write all 50MB of file because you put some data somewhere in the middle of it. EDIT: As a bonus, if you really like your XML, you can actually store it inside the database. While that's not always a good idea, sometimes it can be useful if you want to use XQuery or XPath to run additional queries against at the application layer.
I like the term "explicit joins". It differentiates and makes it sound naughty at the same time.
database can be written in almost any language. Doubly linked lists in C++ is a basic starting point.
What is the most preferred language in which databases are written today?
Why?
I don't have a lot of experience in this field, but want to enter it. So the reason why i asked you this question was so that i can learn the language that has been used the most in the past and try making databases.
&gt;Ironically that's helped identify an assumption I made with the initial query. Or, not so ironically. That's why I, with my damn near 20 years of SQL experience, will draw a quick sketch when things get a little confusing.
Literal answer: C/C++ is usually used to write DB engines, though LISP is very close to a DB to begin with and often fills the same roll by being embedded into an application. However, it would be polite to provide more information than what you've provided. State your use case, for example (the db would be used for generic data storage retrieved in a declarative manner to provide data to programs of no given specification. the db would be used to store connections between entities. the db would be used to save stateful information of a program's running state to be reinstated later. etc.) Or state the type of database (relational db, graph db, key-value store,etc). Or state the metrics most considered important (speed, compression, safety) Maybe list 10 different general questions. The really aggravating thing is this line though: &gt; need a little bit of your help answering some questions i have regarding the languages used to write databases Since you're new to reddit, I'm not being an asshole, but dude, don't beg for questions. If you have some questions, JUST ASK THEM. Also, this isn't really the subreddit for it. This isn't a subreddit for databases, this is a subreddit for using and writing SQL, which only incidentally works with databases. 99% of the readers in this subreddit couldn't build a DB; they just use an SQL flavored one. The set of 'databases' is much larger than and completely includes the set of 'SQL engines'. You may want to goto /r/programming.
I have used this pattern before for a similar problem: WHERE col1 = coalesce(@variable1, col1) Coalesce returns the first non-null value in the list given. It has been a while and I don't remember if it has an impact on query optimization, though.
I really appreciate your help badjuice, and thank you for directing me to the right sub reddit. Being a new guy to reddit, you've helped me out in a great way. Thank you 
Nope, that screws up the optimizer. 
I'm not sure I understand how this would fit into my solution. So to give more detail around the reason the query is structured the way it is... they're passing in these parameters and they want to basically say "if a value is not given for parameter x, ignore it and return all the results instead of using the value from that parameter. Otherwise, constrain the query using the parameter value." So we end up having a bunch of ...AND ( @var1 IS NULL OR col1 = @var1 ) AND ( @var2 IS NULL OR col2 = @var2 ) ... etc It's ugly. I came up with a work-around using dynamic SQL but I'd rather not use dynamic SQL if I can avoid it. So I'm trying to figure out how your 1=1 statement ties into this logic and if it's something I can use.
okay thank you for your help :)
nepobot's solution is the traditional one. However, dynamic SQL can cause some performance problems itself. To avoid that I would use dynamic SQL to generate the stored procedure code itself, then compile that code. I'd leave a comment with the metadata and code so that if someone needs to modify it they can re-generate the stored procedure. All performance problems will be avoided this way. Here's an example of how that would work: if object_id('tempdb..#Perm') is not null drop table #Perm; --create all possible combinations of three nullable parameters with b as ( select 'null' as parameter union all select 'not null') select b1.parameter as parameter1 ,b2.parameter as parameter2 ,b3.parameter as parameter3 into #Perm from b b1 cross join b b2 cross join b b3; --return results to text and you've got your dynamic SQL for --all permutations of input parameter nullability select ' --code automatically generated if @parameter1 is ' + parameter1 + ' and @parameter2 is ' + parameter2 + ' and @Parameter3 is ' + parameter3 + ' begin select foo from table where 1 = 1' + case parameter1 when 'Not Null' then ' and Column1 = @Parameter1' else '' end + case parameter2 when 'Not Null' then ' and Column2 = @Parameter2' else '' end + case parameter3 when 'Not Null' then ' and Column3 = @Parameter3' else '' end + ' end ' AS ProcedureCode from #Perm order by parameter1, parameter2, parameter3; 
Cool thanks. Do you know if it's possible to run an Xpath inside of a SQL query?
Look up derived tables, too
At least you didn't suggest grouping every field...
C/C++ are the languages that are most appropriate for implementing algorithms that make heavy use of optimized memory and I/O. Oracle, SQL Server, mysql, Postgres, Ingres, and nearly ever major db engine is implemented in some C dialect. That said, if you are asking thus question, I suspect you need to study up on tree structures, algorithms, and language implementation. You can't implement a database without knowing what a btree is. You can't implement a database without understanding how to write a query language.
Function wrapping will prevent indexes from being used on columns. 
That's an interesting solution. Thank you for that
Learn something new every day. I didn't realize a subquery used as table is called a derived table, I always just called it a subquery.
Where do babies come from?
That has to be the worst mobile site I've seen in a long time. If the tiny text stretching from one edge of the screen to the next wasn't enough, they added a fixed tab on the left of the screen to make sure they obscured any chance of seeing the entirety of the page at any given time.
I figured this out. I tried every way I could figure out and compared the costs in the execution plan. Instead of doing an `OR`, do an `AND` `WHERE ( @variable1 IS NOT NULL AND col1 = @variable1 )` This used the indexes for me and since its using an `AND` it might short circuit the column lookup. EDIT: Hold on. That doesn't give desired results. One other thing I want to point out. When I do a `WHERE ( @variable1 IS NULL OR col1 = @variable1 )` when there is an index, it is not doing a table scan, but actually doing a index scan. Not as good as a index Seek, but still better then a table scan. 
Looking into this a little deeper. it looks like stored procedures can execute differently then what the execution plan says because the database auto tunes the stored procedures. You might also want to look into OPTION(RECOMPILE). It is a hint use in sprocs for "param sniffing"
no worries. It's hard to understand because it's code that writes code, but unfortunately the histograms generated by nullable parameters aren't easily handled by the optimizer. /u/nepobot had it right, this is the sort of code that would implement that solution. I just tend to use these as pragmatic programming techniques. I've found inline code generators to be less than reliable. Compiling the resulting code takes all uncertainty out of it. PS: Change your "results to text" settings in SSMS to a value higher than 256 otherwise you won't see the actual output. 
I use the same pattern frequently and that pattern definitely causes a table scan on the underlying table. I know of a great whitepaper that works through various methods to deal with this and convert the scan into a seek. Unfortunately I haven't been able to locate it. I'll update this post if I find it. The most common method that I use to solve this is to add OPTION (RECOMPILE) to the end of the statement. This causes SQL to compile the statement on every run. It effectively allows the optimizer to come up with a one time use. Since the optimizer knows that it is a one time use plan, it can do some additional optimizations that it wouldn't be able to do if it was trying come up with a generic plan that would work with any combination of optional variables. If @variable is NULL, the optimizer can just eliminate that filter since it knows the plan won't be cached. If it had to cache the plan, it would still have to handle that filter since in a future execution, @variable might not be NULL. Obviously forcing a recompile on every execution of the statement is a trade off. I always use profiler to check the impact, but it is usually quite minimal. In most cases it adds a trivial amount of CPU time but eliminates a large amount of reads, especially on large underlying tables. I'll gladly make that trade any time. The second method that I use is really just an extension of the first method. I use it when I have a stored procedure that has several optional parameters and I know that my application uses the same parameters 99% of the time. For example, consider a stored procedure with the following statement in it: SELECT * FROM TBL WHERE (@VARIABLE1 IS NULL OR COLUMN1 = @VARIABLE1) AND (@VARIABLE2 IS NULL OR COLUMN2 = @VARIABLE2) AND (@VARIABLE3 IS NULL OR COLUMN3 = @VARIABLE3) OPTION (RECOMPILE) Obviously @variable1, @variable2, and @variable3 are optional and I'm handling it with my method above. But I know that my application almost always passes a value for @variable1 and @variable2 while @variable3 is rarely used. In this case I would love to not recompile every time and be able to cache an optimal plan, however, I can't since I need to handle @variable3 if we ever pass it. I solve this by creating two statements and checking the 3 variables in an if statement to decide which statement to run. I write the first statement to be optimized for that specific variable pattern I see the majority of the time and don't force a recompile and then I use the full query above for any other variable patterns. It looks something like this: IF (@VARIABLE1 IS NOT NULL AND @VARIABLE2 IS NOT NULL AND @VARIABLE3 IS NULL) BEGIN SELECT * FROM TBL WHERE COLUMN1 = @VARIABLE1 AND COLUMN2 = @VARIABLE2 END ELSE BEGIN SELECT * FROM TBL WHERE (@VARIABLE1 IS NULL OR COLUMN1 = @VARIABLE1) AND (@VARIABLE2 IS NULL OR COLUMN2 = @VARIABLE2) AND (@VARIABLE3 IS NULL OR COLUMN3 = @VARIABLE3) OPTION (RECOMPILE) END So know there are two statements. The first select is very specific to the variable pattern I see most of the time and doesn't have the @VARIABLE IS NULL OR pattern that causes the table scan. I allow that statement to be cached since it results in an optimal plan. In all other cases, I force the full optional parameter statement with a recompile every time. This won't interfere with my cached plan for the first statement since the statements don't match. This gives me a the best of both worlds - I can allow for an optimal plan and have it cached for the majority of my calls while still allowing for the efficient execution of the rare edge cases that don't match the normal call. The trade off here is that we trade some CPU time on the edge cases to eliminate the scan and we add a bit of complexity to the maintenance of the query. If we wanted to update the query we will need to make sure we handle both statements. The last method that I would consider, although I don't personally use it, is a dynamic SQL solution. There is a lot of complexity in writing a bug free query that and dynamic SQL has it's own set of best practices that you need to deal with. It looks like someone else went into more detail on this option so I won't speak much to it. With this method, though, you would end up with a dynamic query that would execute custom tailored query for the particular variable pattern you pass and each of those custom tailored queries would be optimal and cached for future use. The trade off here is complexity in your solution. The only other thing that I can think of at the moment is that the OPTION (RECOMPILE) keyword requires a specific version of SQL to work properly due to a bug. Without being able to reference the article, i think that it requires SQL 2008 R2 and the bug was fixed in cumulative update somewhere around CU4 - CU5. I could be way off on the CU number, though. I also believe that SQL 2012 was released with the bug as well and it was fixed in a CU but I have no clue what the CU number is. I hope this helps. EDIT: [I found the whitepaper!](http://www.sommarskog.se/dyn-search-2008.html) 
"what's your biggest weakness" hard to answer that truthfully, appear not to lie, and not get kicked out the door
SimpleDB, which is used at Amazon WS, also Riak and CouchDB are written in Erlang.
Here is the source for mangoDB. https://github.com/mongodb/mongo
[They do](http://technet.microsoft.com/en-us/library/ms189463.aspx). They just use a different key word. (Top) &gt; Limits the rows returned in a query result set to a specified number of rows or percentage of rows in SQL Server 2012. When TOP is used in conjunction with the ORDER BY clause, the result set is limited to the first N number of ordered rows; otherwise, it returns the first N number of rows in an undefined order. Use this clause to specify the number of rows returned from a SELECT statement or affected by an INSERT, UPDATE, MERGE, or DELETE statement. LIMIT isn't part of standard SQL. You might find it instructive to research how other database management systems implement similar behavior.
Oracle doesn't use LIMIT either (uses rownum &lt;= X instead).
Sorry, I should have been more clear. I meant to say using LIMIT + OFFSET in tandem (that's why I was referencing ROW_NUMBER in my post). To my knowledge, that symtax works in PostgreSQL, MySQL, SQLite and is a clear and concise was to accomplish pagination. I understand LIMIT/OFFSET may not be part of the SQL standard, but clearly MS SQL does many *other* things that are not part of the standard as well, right?
Thanks for pointing this out! It will be a while before we're on 2012, but it gives me something to be optimistic about. 
I always heard a good way to answer this is to admit something that's genuinely a weakness of yours, but also mention specific things you have done to improve in that area.
&gt; To my knowledge, that symtax works in PostgreSQL, MySQL, SQLite But not in Oracle, DB2, Sybase, Interbase, Teradata, Mimer SQL, etc. &gt; but clearly MS SQL does many other things that are not part of the standard as well, right? While that's true, I'm more comfortable asking why particular platforms don't support SQL standards. 
Upvote for use of grok. Its an underused word for when you actually 'get' a concept. I try to avoid DISTINCT as it kills my query performance, if you're working with legacy datasets then sometimes it the easiest way to cope with the shitpile you're faced with. 
Work. To relate currently stored relationship contact data with incoming preference data.
Where you work now should not determine whether to learn SQL or not. If the prospect of working with databases does not sound daunting and excruciatingly boring to you, or even more, if it excites you - go for it. I would be reluctant to pay for the course, though, as there are plenty of free materials on the internet, especially beginner level. A thorough book will cost less. All you need is commitment. If you don't have commitment - videos won't help much.
Yes you should learn SQL even if you're not going to use it at your current job. A few other free resources * http://sql.learncodethehardway.org/ * http://tech.pro/tutorial/1555/10-easy-steps-to-a-complete-understanding-of-sql Having the base knowledge of what SQL can do is very important. 
Not sure if you can do this on your platform but I've had to do a load of this kind of thing. I find Metaphone encoding (there are free sp's that do this) to be better than Soundex as it compares more characters. If you can use Jaro-Winkler distance between the converted datapoints being compared too (also has free sp's to implement) then it is more accurate. This is not all that quick but, in practice, its helped me. Would post links but on the train. Edit: Sorry, not really for Access. Soundex and difference probably your best bet. 
I believe that expanding you knowledge as a developer is always useful. If you find yourself more easily learning from books rather than doing a course, a couple of my recommendations are: The Manga Guide to Databases (If you don't mind the cartoony style, it's actually a great tutorial/true novice book) The Visual Quickstart Guide to SQL (This novice level book will get you started, with good coverage of the topic.) SQL Cookbook (Will take you from a beginner to intermediate level of SQL knowledge.) Good luck in your pursuit!
1. Do not pay to learn. Take a free course. 2. If it connects with you then keep going, it could lead somewhere. 
As far as I know, there isn't any built in table that records this, unless you are profiling at the time or you have some sort of custom trigger enabled.
The bar to entry for SQL and database basics is relatively low (unless you understand database basics, you don't really "know" SQL.) Relational databases are widely used, so any effort you put in is unlikely to be wasted. Don't pay to learn this stuff. There are too many excellent free resources available out there. Also, I'd recommend using a resource that teaches the SQL standard rather than one specific platform (oracle, mysql, etc.) You can learn the individual quirks of the platforms later as you have to actually work with them. 
In SQL Server land it really depends on if it's late-binding or not. Procs are late-binding, views parse first. Anything in SSRS parses first, so dynamic SQL is right out for RS. A challenge with dynamic SQL is execution plan and index utilization.
&gt; How many transactions per-minute in a high-volume environment to a single table however, that is indeed very important. MSSQL has a hard limit of ONE transaction per table per .003 seconds. Any faster than that and you pretty much have to look into scaling your DB horizontally. Thats pretty much what im looking into - wanted to see what i could pick up in my current environment outside of a SQL Perfmon counter. 
That would be rough, I'd imagine the performance hit while just tracking transactions and trans/sec wouldnt be too bad.
&gt; Is it worth it for me to learn ...... Yes. I don't care how you end that sentence, the answer is yes.
I don't think that the second link is very good to teach the basics of SQL. It's a great article but not good for beginners. 
fair enough, just wanted to point out that there are a lot of free resources to use first, if they don't work, then pay for something.
LOL, I am currently working on my MBA. I finish next december. 
Short answer: Yes, but don't pay for it yet. I have only spent less than four days in actual SQL training, and kind of put all of my set-theory math learning into cold storage for a decade between High School before accidentally getting involved with Government IT six years ago. These days I lead a team of Enterprise Data Warehouse analysts and developers. Careers are funny things. Actual certfication &amp; training is frequently easily beaten out by experience, interest, and a willingness to adapt (and sometimes to undercut someone else's rate in order to get hired on an expectation of upskilling quickly, but only do that if you know your own hourly value in other areas is solid enough to leverage off in the short-term)... That said, don't spend any money on the basic courses, there are more than enough resources available for free to get you started on basics, and a huge developer community to do the the indermediate stuff. IF you want expertise, either get an employer who values your development, or self-fund your indermediate learning into expertise, then charge it back on the future work. Everybody dealing with an application with an underlying database should have some SQL knowledge as its used widely across a number of platforms (be it MS T-SQL, Oracle, DB2, etc...) in parallel, everybody should grasp some understanding of normalisation and data modelling, as it reflects into a number of core business areas. Also, people should have an understanding of their chosen RDMS tool and how it differs from other tools, with a view to understanding _why_ they prefer a certain tool. Personally, SQLZoo is a good start if you have a basic understanding of coding and that data is available in many places in an RDBMS model, and it should should give\build on some basics through their interactive modules.
Do Not Pay To Learn. If you are incapable of learning on your own, you will never advance, as self-education is key. If your job has Microsoft Outlook and Access, you have a handy large source of data to learn with. Access can import an outlook data file, and the Global Address List is one of them. Once you have copied the GAL into your Access database file, you can use it to practice the skills you acquire in your free online courses. It is ripe for querying, data clean up, and splitting into multiple, related tables. Good Luck!
Assuming EB and EB_PART are the same thing, this... SELECT CType, Part, SMS, EB FROM part WHERE (c_type = @c_type) AND ((part LIKE '%' + @search + '%') OR (eb_part LIKE '%' + @search + '%') OR (sms LIKE '%' + @search + '%') ); ...should do what you want.
They are, I shortened the column names to make it easier and missed that one. Let me give it a shot.
No go. :(
Here's a couple screenshots of it in action. You'll notice '50' exists in the "Match String" column (aka SMS), and '22' exists in the Part Number column. Yet 50 returns a record, 22 does not. [imgur album](http://imgur.com/a/7EV7Z) The same code works in the dataset designer until I build the project. I go back in and "Preview Data" and it functions the same way as the front end. I'm stumped.
What are the data types of columns PART, SMS &amp; EB? (I'm straying into SQL Server implementation here, and this is not my area of expertise - just running a hunch). If they are CHAR(x) try casting them all to VARCHAR(x).
They are all indeed char(x), unfortunately this table is part of a large production system and I can't just change the data types. What are you thinking is going on?
I came across something odd once where "LIKE" did not play nice with CHAR(x) fields when the search text was at the end of the column being searched. I found that CASTing the columns in question to VARCHAR(x) (as below) in my code made the search work. I stress that this was not on SQL Server, so YMMV. SELECT CType, Part, SMS, EB FROM part WHERE (c_type = @c_type) AND ((CAST(part AS VARCHAR(m)) LIKE '%' + @search + '%') OR (CAST(eb AS VARCHAR(n)) LIKE '%' + @search + '%') OR (CAST(sms AS VARCHAR(o)) LIKE '%' + @search + '%') ); (m, n &amp; o should be substituted for the actual column length in each case, naturally). Apols in advance if it does not help... 
I'm giving it a shot now. I really appreciate the hand you're giving me with this. I'm at my wits end with it.
You sexy bastard. [Look what you did!](http://i.imgur.com/EDQNbqQ.png) I combined my original code with your modification to recast the data types. We have success. Sweet 8lb 6oz baby jesus.
I thought the problem waas with your search for "50", which had values at the end of the column, and failed? Does that work now, too?
Yup. Everything I throw at it returns the correct results.
Notice searching for "50" now actually returns 2 results instead of just 1. Meaning it's searching multiple columns successfully. [Screenshot](http://i.imgur.com/zEkHs5L.png)
Thanks again man. It's not much, but enjoy the reddit gold.
Sweet! Thanks!
&gt;I'll start below... *Doesn't start below* &amp;#3232;_&amp;#3232; I use it for work: adhoc reporting &amp; fixing user mistakes aka solving tickets
I want to add that don't pay until you know that is what you want to do. I personally felt sql daunting and scary without knowing anything about it. I think it can be difficult to grasp as a beginner because of the sheer amount involved can be overwhelming. I'm finally at a point where I understand conceptually most of the novice to intermediate stuff in sql so the things that scare me have shrunk(still a lot scares me). I enjoy working with ms sql, bids, ssis in practice and I would love to get the experience to work on this stuff as a full time employee. 
Leave it to a government worker to find the only sql tutorial on the internet you have to pay a hundred dollars for. /cheapshot
i would do this if you do not have functional indexes on those substrings: AND REGEXP_LIKE(SUBSTR(status1,1,1)||SUBSTR(status2,1,1)||SUBSTR(status20,1,1), '8|Y' ) 
SQL looks at your statements at essentially the same time, unless they're in a block, in which case the blocks are evaluated first. AND TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'MON-YY') &gt;= '2012-10-OCT' AND (SUBSTR(status1,1,1) IN ('8', 'Y')) OR (SUBSTR(status2,1,1) IN ('8', 'Y')) OR (SUBSTR(status20,1,1) IN ('8', 'Y')) Your statement evaluates as true as long as either of your OR statements return true, since this entire thing is treated as the same block. Use parenthesis to delimit your blocks and control how your statements interact. AND TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'MON-YY') &gt;= '2012-10-OCT' AND ( (SUBSTR(status1,1,1) IN ('8', 'Y')) OR (SUBSTR(status2,1,1) IN ('8', 'Y')) OR (SUBSTR(status20,1,1) IN ('8', 'Y')) ) Is essentially saying AND TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'MON-YY') &gt;= '2012-10-OCT' AND ( [Whatever Boolean value this block returns] ) Remember SQL always needs to get your Where result down to a single Boolean value, telling it that this record should be included or should not be included. Think of Blocks like your Order of Operations in math class. Whatever's deepest in the parenthesis gets evaluated first.
I tried putting the whole thing in parenthesis as you suggest here. I'm still getting records that are outside the date range in the previous condition. Do I perhaps have something wrong elsewhere?
I like this solution, but I'm getting an 'missing right parenthesis' error on this. AND REGEXP_LIKE (INSTR (SUBSTR(status1,1,1) IN ('8', 'Y')||SUBSTR(status2,1,1) IN ('8', 'Y')||SUBSTR(status19,1,1) IN ('8', 'Y')||SUBSTR(status20,1,1) IN ('8', 'Y'), '8|Y') )
Out of curiosity, what does the rest of your `Where` clause look like?
You don't want the whole set in parenthesis, just the or statements: select count(*) from tableA where TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'MON-YY') &gt;= '2012-10-OCT' AND (status1 = 'Y' or status2 = 'Y' or status3 = 'Y') First part limits to just the date, then the next part has to have one part true
whoops, drop the "INSTR(". I was testing a few different things. also remove every in statement, that is what the regexp_like is doing for you... AND REGEXP_LIKE(SUBSTR(status1,1,1)||SUBSTR(status2,1,1)||SUBSTR(status20,1,1), '8|Y' ) 
 SELECT dol_uic, instl, prj, TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'YYYY-MM-MON') AS month_complete, COUNT(dol_wo) won_count FROM mgt0.dcw WHERE dol_uic IN ('W6XG1B', 'W6XGAA', 'W6XHAA') AND TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'MON-YY') &gt;= '2012-10-OCT' AND ( (SUBSTR(status1,1,1) IN ('8', 'Y')) OR (SUBSTR(status2,1,1) IN ('8', 'Y')) OR (SUBSTR(status19,1,1) IN ('8', 'Y')) OR (SUBSTR(status20,1,1) IN ('8', 'Y')) ) GROUP BY dol_uic, instl, prj, TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'YYYY-MM-MON') ORDER BY dol_uic, instl, prj, TO_CHAR(TO_DATE(datecompl, 'YYDDD'), 'YYYY-MM-MON') 
to explain what is going on in the code I provided. You are build a string of the first character of every needed status field so you will get something like '8Y8'. regexp_like('8Y8','8|Y') the above code is saying, return the record if the string '8Y8' contains an 8 or a Y
I work in Logistics, and our system has a SQL Plus function, I use it to provide numbers for how many shipments we've done, the cost, value, etc. No formal training, a friend sent me a few, and I hacked pieces of different queries together, eventually I started to get some of the basics figured out, then came across sqlcourse.com and sqlcourse2. I'm interested in doing more, but I really have nothing else I can do in my current line of work.
Try changing join to left outer join. That should fix it. 
unfortunately it didnt, but it did put me on the right track and when I added 'group by p.code' it worked just perfect.
I normally use window functions for stuff like this. Eg: select * from ( select p.code, pc.semester, row_number()over(partition by p.code order by pc.semester desc) rownum from FROM `uke_program` p LEFT OUTER JOIN `uke_programcourse` pc on p.code = pc.programid) a where rownum=1
I havn't been able to find an English speaking video of this powerpoint presentation but slide 17 says its all outdated
it looks like they have connection issues. "500 Can't connect to 146.176.166.7:81 (connect: No route to host) "
A foreign key is a constraint on a column (aka an attribute). It says "in order for a piece of data to appear here, it has to exist in this other place". An example might be the author of a book. You have a table of authors: id | name --------------------------- 1 | Virginia Woolf 2 | Edith Wharton And a table of books: title | author ------------------------------- To the Lighthouse | 1 Ethan Frome | 2 A Room of One's Own | 1 We're saying every book has one and only one author, so it's pretty reasonable to put a foreign key constraint on the author column of the books table referencing the id column of the author table. Your DB will yell at you if you try to insert something into the books table with a nonsensical value in the author column (namely, an id that doesn't exist in the author column). Similarly, you'll get yelled at if you try to remove an author or alter an author's id if that author has a book in the books table (read: that authors id is somewhere in the 'author' column). This is a feature of relational databases, it's protecting the integrity of your data. Not sure I understand your questions: 1) You can insert whatever you want in table 1 as long as it matches the schema. Table1 has no foreign key constraints, so go nuts. 2) When inserting into table 2, the data you're inserting into the column with the foreign key constraint must have a matching entry in table 1 to satisfy the foreign key constraint. In the example above, we couldn't put a row into the books table with an `author` of `3` since there's no such entry in the authors table. If we need to add a book with an author that's not in our authors table, we need to create an entry for that author and then reference that author by their id. As for your final question, if you're updating the column with the foreign key constraint, then as usual whatever you're setting that value to must also exist in the table it's referencing. Always, that's exactly what a foreign key does. It sounds like your primary key on table1 is not ideal - primary keys should be things which do not need to be updated frequently, specifically for this reason. This is why you'll often hear the advice to use an "artifical key", such as a sequential id number, as a primary key. It has no real-world significance itself, so it should never need to be updated. This isn't always necessarily the best course of action, but if you're just starting with databases it's probably safe to err on that side.
You need to enter the data in the other table only in your second scenario. You say you are just messing around, you don't have an application. Usually you will pull the pk of the other table from that table during some previous operations to use in your inserts to your Fk table, so its not so tedious. I'm on tablet right now so let me know if u needs clarification.
You should be using entity framework if you want a data modeling/mapping tool. Or linq to SQL.
Try using a group by clause. Also I recommend not using this uke_ prefix. It makes code hard to read and kills auto complete tools ability to help you.
Definitely worth learning. I highly recommend the Stanford database winter 2013 course. Its free with a real prof doing videos and lots of quizzes and interactive exercises.
Thank you for your reply:) So in general is to have PKs that are incremental numerical values, just an example. Yes? However let us say that we have 100 new hires in a company and their Employee IDs are used as a PK. That means we are going to have to update all the appropriate tables the EmployeeID exists in. Yes? Lets say that 50 employees got fired, then the only way to delete their records is to delete their entries from the table where the EmployeeID is used as a PK. Yes?
Thank you for your reply :) So in other words, before I enter a value in the table with the FK the same value must first exist in the PK of the table it references. Yes?
Nailed it!
Great :-) I am happy with entity, but thought linq to SQL was also outdated. Thanks
Awesome. The cascading option for delete makes sense when a record needs to be deleted :) Thank you !!
It is, but not as bad as table adapters and is still mostly c# side.
You can write your own linq to SQL generators for EF and there are quite a few quality drivers for .net db connectivity. Check out dotconnect Edit: I should note that I wouldn't do this myself, and didn't because i had the option to start the project fresh and chose SQL server once I hit this problem, but it is doable.
&gt; how does that specific information gets populated in the other columns (the FK columns) of the other tables TABLE2 That's not the point of FKs. TABLE1 101 Recipriversexcluson 102 GromitRacer Now, when YOU populate TABLE2, say its for phone numbers... 101 800-555-1212 101 409-555-1212 102 888-999-1212 ...it's on you to make the currently selected TABLE1 PK the FK in TABLE2. And it's best to make it fail on null. 
So that means when a user enters information to TABLE1 for the PK1 column, I have to have code that will populate that information to the FK column of TABLE2, yes?
Yes. The point of a foreign key is to be able to tie to data in the first table. After you insert the data in TABLE1, you take PK1 and then create a record in TABLE2 and TABLE3 inserting PK1 into the foreign key field. You can accomplish this via a trigger, code in your application, however you'd like.
Just for a moment, try to forget about the PK/FK terminology and functionality. If these did not exist, try to think of how would you design your app to make sure all the data was added 'correctly' to all of your tables (1, 2 and 3), so that there were no stray records anywhere, no missing rows in any of your tables and all necessary identifying links between your tables were in place. When you can visualize that, my guess is that the nature of the PK/FK relationship will fall into place more readily.
The way you are phrasing it sounds backwards. TABLE2 and its FK is *dependent on* TABLE1 and its PK. You have to have a parent record before you add a child record. 
Thanks for that. I was getting caught too much in the nuts and bolts with out having an actual design of an application. What you just said made it more easy to understand. Make the tools work for you and not the other way around. This along with commandline are of the best subreddits :)
Depends on when the PK/FK relationship is created. It's entirely possible to create data first, and create relationships second. :)
It's been a long time since I've used any "learning" databases provided by Microsoft so tell me if I'm wrong, but isn't AdventureWorks the free database most tutorials use, not Northwind?
Using functions like ISNULL or COALESCE forces the query optimizer to ignore indexes on that field. Ideally you should avoid using them when possible.
Being able to help yourself improve is certainly something to be proud of too.
We've all been in your position before. It's time for you to switch sides and help others. It just turns out that teaching others help you learn just as much.
Just so you're aware, there are some limitations with the free Express edition of SQL Server. For example, it will only ever use a maximum of 1GB RAM for a running query. It can only have a maximum database size of 10GB as well. It doesn't contain SQL Agent, meaning you won't be able to schedule jobs to automate things. If you're messing with replication in any way, note that Express can only be a subscriber, it cannot publish articles. Also it doesn't install SSRS, SSIS or SSAS components, so if you're interested in learning those you'll need to purchase the Standard edition. While you're learning query writing and the SQL language in general, none of these things are critical, but I just want to point it out if you're wondering why certain bits and pieces you might read on a blog are missing. In the workforce most organisations will be using the Standard or Enterprise editions, but they're expensive.
I had no idea this was allowed, thanks!
Application is everything when it comes to SQL. You'll learn most from writing SQL to solve actual data requirements and / or attempting to solve other people's problems either on here or at during you work. Great news on your progress!
Yup was telling my wife last night came into work yesterday and something clicked. Although I "got" how to do it before now I fully understand specifically in relation to querying with inner joins to pull from multiple results.
I never would have thought in my 10+ years in IT I would enjoy SQL but I do seemingly enjoy it :)
Agreed I look forward to helping some people on here now :) As for work, ya I'm doing some serious data mining for our Data Management team and it's so much info they want I've been trying to figure out how to simplify it as much as possible so I don't have to run so many different queries to get the data. I'm still not there, but I'm very close :)
 * Create the new column as nullable (allow nulls) * Join the table by joining it to itself and looking for a null / non-null match on a serial number using the WHERE clause * Update the table from the join with YES or NO
http://sqlfiddle.com/#!3/87f33/2 Here, I set the NewField to 0 initially (NO), then set it to 1 (YES) when it meets your criteria. Sorry for the ugly SQL formatting. 
Tried your links (do you really think I didn't google first?). I don't have the files that these uninstall guides ask me to find. They don't exist. I suppose I might not have another version of postgres causing problems, but that's what all the posts I found on my actual issues seemed to suggest. EDIT: Just want to say even after I do a brew uninstall --force I still can use psql, however postgresql doesnt work after that. Ideas? EDIT 2: Not sure if it matters, but version number is 9.1.5. There are no postgres folders that I can seem to find. EDIT 3: It seems to have come with xcode, can I remove it separately?
Sweet thanks for the help!
I do not, actually. I see files, captured model, converted models, find DB Object and Versioning Navigator. I wonder why connections aren't there?
that's really weird. where did you DL sqldeveloper from? Try a re-dl? I'm using v3.2.20.09 got directly from Oracle.
What is the exact command you ran to get that error? Also, what command did you use to start the server? Your answer is a bit confusing because psql is not a server, so there is no "default psql server". psql is the official Postgres command line client. &gt;Unfortunately it uses PG This is actually very fortunate, because you'll get to learn Postgres. You'll be glad you did. :)
Oh perhaps I misunderstand what postgres is...I thought it was a database server? That error occurs when I even run `psql`
PostgreSQL is an ORDBMS. The `psql` command is a client that connects to a `postgres` server. When you run `psql` without any arguments, it attempts to connect to the default `postgres` server and the default database (usually the current user name). My guess is that you have installed Postgres, but you are not yet running a server instance. If you installed with Brew, you probably need to run `brew info postgres` and follow the instructions.
Actually I've had the processes running on my machine...Let me reinstall the homebrew postgress and get it going again, then I'll post my newest errors.
That probably isn't necessary, but I'll check back later to see what you come up with.
This is what i get from `brew info`: postgresql: stable 9.2.4 http://www.postgresql.org/ Conflicts with: postgres-xc /usr/local/Cellar/postgresql/9.2.4 (2842 files, 39M) * Built from source From: https://github.com/mxcl/homebrew/commits/master/Library/ Formula/postgresql.rb ==&gt; Dependencies Required: readline Recommended: ossp-uuid ==&gt; Options --32-bit Build 32-bit only --enable-dtrace Build with DTrace support --no-perl Build without Perl support --no-tcl Build without Tcl support --without-ossp-uuid Build without ossp-uuid support --without-python Build without python support ==&gt; Caveats initdb /usr/local/var/postgres -E utf8 # create a database postgres -D /usr/local/var/postgres # serve that database PGDATA=/usr/local/var/postgres postgres # …alternatively uilds of PostgreSQL 9 are failing and you have version 8.x installed, may need to remove the previous version first. See: tps://github.com/mxcl/homebrew/issues/issue/2510 igrate existing data from a previous major version (pre-9.2) of PostgreSQL, see: tp://www.postgresql.org/docs/9.2/static/upgrading.html machines may require provisioning of shared memory: tp://www.postgresql.org/docs/9.2/static/kernel- resources.html#SYSVIPC When installing the postgres gem, including ARCHFLAGS is recommended: ARCHFLAGS="-arch x86_64" gem install pg To install gems without sudo, see the Homebrew wiki. To reload postgresql after an upgrade: launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist When I type `postgres` it tells me it cant find the config file. 
The `postgres` command on its own will fail. You need to tell it what to do. The `pg_ctl` command is more useful for controlling a server. Here's what I use to start the server: `pg_ctl start -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log` You can find out whether a server is running with this: `pg_ctl status -D /usr/local/var/postgres` Assuming your brew set things up the same as mine. I'm leaving work but I'll check back in after I'm home.
 select sum(QTY), UPC, Order, (select top 1 name from test as t2 where t.UPC = t2.UPC)name from test as t group by Order, UPC
&gt; I don't care what name is in the name column ... SELECT sum(QTY) , UPC , Order , MIN(Name) AS some_name FROM test GROUP BY UPC , Order 
does oracle now support microsoft's TOP?
Yep, this is the solution under the conditions given by OP. I would ask OP though: why include the name field if it doesn't matter what it is? You are producing bad data by labeling the row with a random(-ish) name from the group. I'd either omit the name colume or rename it to something like "example_name" or "sample_name".
what you're asking about -- the periods -- is a standard sql way of qualifying a column name with its table name this is necessary whenever referencing a column name that exists in more than one table in the query (otherwise it's optional)
Thank you very much, I was just starting to grasp it when your reply popped up and you confirmed &amp; finalized my thoughts.
You just couldn't go a week without a bear could you?
I have to ask, what is your previous programming experience, and what is the impetus behind your new found quest for knowledge? I can tell with about 99.9% certainty you've not programmed in another language yet due to the . command being equivalent to the same semantic idea in another few dozen programming languages. And because of this, I'm genuinely curious as to why someone is wanting to learn SQL out of the blue. Not that this is a bad thing by any means, nor am I trying to discourage you, I'm simply curious.
Thanks to everyone who helped me out! I got it working once I finally understood the difference between psql and postgresql! :)
LIMIT is not universal at all. It's pretty much limited to the 3 you mentioned and some DBs not so common, such as SQL Anywhere, Vertica, and Netezza. It's definitely not part of the ANSI SQL standard. OFFSET is even less widely supported. [This page gives you more information on what the standard is, and how various RDBMS' implement it.](http://troels.arvin.dk/db/rdbms/#select-limit) Also - see the [Wikipedia page on the matter](http://en.wikipedia.org/wiki/Select_%28SQL%29#Result_limits).
Several ways of approaching this (I can think about 4-5 ways), but here's a couple of examples. I've only included pseudo code, so you still have to do your homework! 1. You look for exceptions which break the rule, i.e. where a project is not used in the city. ...where not exists (select full list of projects MINUS list of projects in the city) 2. You count all the projects and compare to the count of projects in the city. ... where (select count(full list of projects)) = (select count(projects in the city)) 
I don't understand how the count of the full list of projects can ever equal the count of projects in the city. In my case, the count of the full number of projects is 7 and the number of projects in the city is 2. How can this ever be equal? I am missing the point. Thanks for trying to help. Unfortunately I can't follow this logic in either case. For some reason this stuff really doesn't make any sense to me at all.
Look into using INNER JOIN instead of EXISTS. 1. SELECT a.field 1. FROM A 1. INNER JOIN B 1. ON A.field2 = B.field2
Another Level would be 1. SELECT a.field 1. FROM A 1. INNER JOIN B 1. ON A.field2 = B.field2 1. INNER JOIN C 1. ON A.field2 = C.field2
Thanks, I used inner join as follows: select pid from mpj inner join j on j.jid=mpj.jid where j.city='TheCity'; This gets me close as I end up with p3 p3 p6 p5 p5 I guess I need to eliminate p6 because there is only one occurrence of it. I can see that this relates to the fact that there are 2 different projects in 'TheCity' but I don't know how use this knowledge. 
So, what do you think about that: -- easy case - all pids for all jid in city TheCity -- SELECT pid FROM mpj WHERE mpj.jid IN (SELECT jid FROM j WHERE j.city = 'TheCity'); -- hard case - where pids exists for all jids in TheCity at one time -- -- j4c = all-projects-for-city view -- /* we compare size of all-jids-for-city with (all-projects-for-city divided by all-mpj-jids-for city) */ WITH j4c AS (SELECT jid FROM j WHERE city = 'TheCity') SELECT pid FROM mpj WHERE count(j4c) = count( j4c INTERSECTS (SELECT jid FROM mpj WHERE mpj.jid IN j4c) ); P.S. table-column names confuse and get time wasted. 
Your scenario isn't completely clear, but having read your other replies I think you mean something like this : select pid from ( select m.pid, count(*) from m join j on j.jid = m.jid where j.city = 'TheCity' group by m.pid having count(*) = (select count(*) from j where j.city = 'TheCity')) 
 left outer join table2 t2 on t1.field1 = t2.field1 and t1.field2 = t2.field1 Should be left outer join table2 t2 on t1.field1 = t2.field1 and t1.field2 = t2.field2
the reason you are not getting all the rows returned in your left outer join is because you specifically eliminate the unmatched rows by having non-null conditions on t2 and t3 in your WHERE clause move these conditions -- and t2.field1 = 'criteria2' and t3.field1 = 'criteria3' into the ON clauses of their respective joins 
This is working exactly how I'd wish it to work. Thank you very much for pointing this out, i didn't realise you could specify filter criteria within joins.
Please be aware that w3schools is full of extremely broken content. I know you're trying to help with reference, but this one particular site is going to cause more problems than it solves. http://w3fools.com
sql
Thank you for being honest that you're doing homework. That means we can help you without taking the work / learning experience away from you. Two points and one upboat. ---- Step 1: foundations So what they're trying to get you through here is a join. Joins are central to the SQL model. There are four basic kinds of join, though in practice you rarely use more than two of them, and almost never the rare ones. A join is how you weld two tables together. The end result will be a temporary table - thrown away after your query is over - containing the welded result. You may, and often will, chain these. I'm going to assume you're using MySQL for this example, because the various SQL dialects aren't actually the same. The only MySQL specific thing I do is auto_increment; in other dialects that might need to be serial, or a sequence, or whatever else instead. If you don't use MySQL, what auto_increment does is to stick the next integer in each time, so that you automatically get a series of increasing numbers, like you often want for an id. (Some databases may want to replace varchar with text, too.) Since you don't know joins yet, I'm going to assume you're fairly new, and I'm going to explain everything. Please understand that I'm just not sure which things you already know. `:)` Consider the simple case of a table of people and a table of CDs they own. First, let's create a way to track the people. create table people( id integer not null auto_increment primary key, name varchar(255) ); Simple enough: we have a table that's just an ID# and a name; the ID isn't allowed to be empty (not null,) and the database is told that the ID# is the important thing for fast sorting (primary key.) Next, let's populate our table. insert into people(name) values ('Charles Darwin'), ('Thomas Henry Huxley'), ('Gregor Mendel'); First, notice that I'm skipping the ID column entirely. That's because the column is auto_increment, meaning the database is expected to fill that number in on my behalf. If it wasn't, they'd go in as null, and since the column is marked not null, the insert would get rejected. Next, let's track some CDs. We'll improve this CD table later. create table discs( id integer not null auto_increment primary key, title varchar(255) ); Pretty straightforward. Same general idea. Let's also put some data in there. insert into discs(title) values('The White Album'), ('Aenima'), ('Sailing the Seas of Cheese'), ('Cosmogramma'); Now, we can't just add an owner column to that table, because chances are more than one person owns each of those albums. I have Cosmogramma; so does my friend Forest. And, obviously, we can't just add a bunch of columns, because tens of millions of people own The White Album. So what we do is we make a "bridge table" - not special tech, just a concept - whose purpose is to point at a user, a cd, and say "this is a match." This time, I'm not going to bother to give an ID column, because the concept of one ownership doesn't really need to be indexed; all we care about is "is it there or isn't it?" At first, to keep it simple, I'm going to do a lousy job. create table discs_owned( person_id integer, disc_id integer ); For reference, to look someone's ID up by name, we just do this: select id from people where name like 'Charles Darwin'; So, cool. Now we can just do this: insert into discs_owned(person_id, disc_id) values ( (select id from people where name like 'Charles Darwin'), (select id from discs where title like 'Aenima') ), ( (select id from people where name like 'Gregor Mendel'), (select id from discs where title like 'Cosmogramma') ), ( (select id from people where name like 'Thomas Henry Huxley'), (select id from discs where title like 'Aenima') ), ( (select id from people where name like 'Thomas Henry Huxley'), (select id from discs where title like 'Sailing the Seas of Cheese') ); Awesome. Each scientist owns at least one album, and THH owns two, because he's a badass. One album isn't owned; one twice; the others once each. ---- To check: mysql&gt; create table people( -&gt; id integer not null auto_increment primary key, -&gt; name varchar(255) -&gt; ); Query OK, 0 rows affected (0.13 sec) mysql&gt; insert into people(name) values ('Charles Darwin'), ('Thomas Henry Huxley'), ('Gregor Mendel'); Query OK, 3 rows affected (0.03 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql&gt; create table discs( -&gt; id integer not null auto_increment primary key, -&gt; title varchar(255) -&gt; ); Query OK, 0 rows affected (0.25 sec) mysql&gt; insert into discs(title) values('The White Album'), ('Aenima'), ('Sailing the Seas of Cheese'), ('Cosmogramma'); Query OK, 4 rows affected (0.04 sec) Records: 4 Duplicates: 0 Warnings: 0 mysql&gt; create table discs_owned( -&gt; person_id integer, -&gt; disc_id integer -&gt; ); Query OK, 0 rows affected (0.11 sec) mysql&gt; insert into discs_owned(person_id, disc_id) values -&gt; ( (select id from people where name like 'Charles Darwin'), (select id from discs where title like 'Aenima') ), -&gt; ( (select id from people where name like 'Gregor Mendel'), (select id from discs where title like 'Cosmogramma') ), -&gt; ( (select id from people where name like 'Thomas Henry Huxley'), (select id from discs where title like 'Aenima') ), -&gt; ( (select id from people where name like 'Thomas Henry Huxley'), (select id from discs where title like 'Sailing the Seas of Cheese') ); Query OK, 4 rows affected (0.06 sec) Records: 4 Duplicates: 0 Warnings: 0 mysql&gt; select * from discs_owned; +-----------+---------+ | person_id | disc_id | +-----------+---------+ | 1 | 2 | | 3 | 4 | | 2 | 2 | | 2 | 3 | +-----------+---------+ 4 rows in set (0.00 sec) ---- continued in step 2 because of comment length limits
 Step 2: getting the work done So, great, now we have our data laid out. How do we work with it? Well, let's suppose we want all scientists who own Aenima. We need SQL to put our data together in a sensible way, filter it, and come back with answers. Enter "join" and "where". First, let's take a look at the discs, to remind us what the data looks like. mysql&gt; select * from discs; +----+----------------------------+ | id | title | +----+----------------------------+ | 1 | The White Album | | 2 | Aenima | | 3 | Sailing the Seas of Cheese | | 4 | Cosmogramma | +----+----------------------------+ 4 rows in set (0.00 sec) And similarly, let's take a look at discs_owned. mysql&gt; select * from discs_owned; +-----------+---------+ | person_id | disc_id | +-----------+---------+ | 1 | 2 | | 3 | 4 | | 2 | 2 | | 2 | 3 | +-----------+---------+ 4 rows in set (0.00 sec) Okay cool. Let's get to work. We want all scientists who own Aenima. Let's begin by selecting all information about all scientists. mysql&gt; select * from people; +----+---------------------+ | id | name | +----+---------------------+ | 1 | Charles Darwin | | 2 | Thomas Henry Huxley | | 3 | Gregor Mendel | +----+---------------------+ 3 rows in set (0.00 sec) Next, let's "join" them to the discs_owned table, such that any discs_owned row whose person id matches a person by id gets extrapolated into the temp table. Notice that now, Huxley shows up twice, because he owns two albums. mysql&gt; select * from people join discs_owned on people.id = discs_owned.person_id; +----+---------------------+-----------+---------+ | id | name | person_id | disc_id | +----+---------------------+-----------+---------+ | 1 | Charles Darwin | 1 | 2 | | 3 | Gregor Mendel | 3 | 4 | | 2 | Thomas Henry Huxley | 2 | 2 | | 2 | Thomas Henry Huxley | 2 | 3 | +----+---------------------+-----------+---------+ 4 rows in set (0.00 sec) Next, let's also join the discs in, according to where the discs_owned's disc ID matches a disc's ID. Notice that The White Album doesn't show up, because none of the owned rows match it. Because these three scientists are some haters, I guess. mysql&gt; select * from people join discs_owned on people.id = discs_owned.person_id join discs on discs_owned.disc_id = discs.id; +----+---------------------+-----------+---------+----+----------------------------+ | id | name | person_id | disc_id | id | title | +----+---------------------+-----------+---------+----+----------------------------+ | 1 | Charles Darwin | 1 | 2 | 2 | Aenima | | 3 | Gregor Mendel | 3 | 4 | 4 | Cosmogramma | | 2 | Thomas Henry Huxley | 2 | 2 | 2 | Aenima | | 2 | Thomas Henry Huxley | 2 | 3 | 3 | Sailing the Seas of Cheese | +----+---------------------+-----------+---------+----+----------------------------+ 4 rows in set (0.00 sec) Next, let's say "but only albums called Aenima." mysql&gt; select * from people join discs_owned on people.id = discs_owned.person_id join discs on discs_owned.disc_id = discs.id where discs.title like 'Aenima'; +----+---------------------+-----------+---------+----+--------+ | id | name | person_id | disc_id | id | title | +----+---------------------+-----------+---------+----+--------+ | 1 | Charles Darwin | 1 | 2 | 2 | Aenima | | 2 | Thomas Henry Huxley | 2 | 2 | 2 | Aenima | +----+---------------------+-----------+---------+----+--------+ 2 rows in set (0.00 sec) And finally, since we actually only wanted the names of the people, let's throw away all the other information. mysql&gt; select people.name from people join discs_owned on people.id = discs_owned.person_id join discs on discs_owned.disc_id = discs.id where discs.title like 'Aenima'; +---------------------+ | name | +---------------------+ | Charles Darwin | | Thomas Henry Huxley | +---------------------+ 2 rows in set (0.00 sec) You have now joined three tables together, getting information from the leftmost while filtering on the rightmost, and leaving the middle one pretty much alone. The task your teacher is asking for is relatively similar. Hope this helps. ---- Footnote: I said I did a shitty job on the join table, which I did. Right now, you could insert (88,99) in for the person and album ID, and it would not complain that neither did there exist a user number 88 nor an album number 99; those are what "foreign keys" prevent. This stuff makes no effort to be fast; there should be keys saying "keep track of this column for speed's sake". Et cetera. Just wanted to get the idea across. `:)`
LOL. I had no clue. Interesting read. I'll make note. Thanks!
There are two other ways you can solve this problem, both of which I find to be more readable. 1) Create a JOIN to a sub-query: SELECT p.code, p.name, pc.semester FROM `uke_program` p LEFT JOIN (SELECT programid, max(semester) as semester FROM `uke_programcourse GROUP BY programid) pc ON p.code = pc.programid 2) Using CTEs (my favorite): WITH MaxSemester AS ( SELECT programid, MAX(semester) AS semester FROM `uke_programcourse` GROUP BY programid ) SELECT p.code, p.name, MaxSemester.semester FROM `uke_program` p LEFT JOIN MaxSemester ON p.code = MaxSemester.programid I greatly prefer using CTEs when trying to aggregate data because I find it easier to debug when something goes wrong. Especially so if you are trying to use multiple aggregates or multiple tables.
Thanks for trying to help but I'm not there yet. I think my problem lies in the fact that I do not have a table which contains only completed assignments like in your example. I need product ids from products, shipments from product_shipments and projects in 'TheCity' from projects. The projects table has projects in more places than just 'TheCity'. 
I think I follow the logic that you are trying to use i.e. keep track of the number of projects that are in 'TheCity' and only pick products which are supplied to both but I can't adapt the code you have to my situation. Is there a reason you used the manufacturer table, this doesn't contain pid?
Awesome description, very thorough and completely explains joins. I used: select p.pid from p join mpj on p.pid = mpj.pid join j on j.jid = mpj.jid where j.city='TheCity'; However, this returns me pid ----- P3 P3 P5 P5 P6 (5 rows) The problem here is that P6 is supplied to only one of the projects in 'TheCity', not not both of them. 
Replace m with mpj, mistake due to rubbish table naming.
THANKS - You are an absolute life saver!!!! I eventually got it with this: select mpj.pid, count(*) from mpj, j where j.city='TheCity' and j.jid = mpj.jid group by mpj.pid having count(*) = (select count(*) from j where j.city = 'TheCity'); 
Your current design would just cost you an extra, pointless, join. Just put the TeamID in the Players table.
use GROUP BY on the movies id, and a HAVING clause with a conditional count, i.e. HAVING 0 = COUNT(CASE WHEN format='bluray' THEN 'uhoh' END)
SELECT Movies.Title, Movies.YearReleased FROM Movies INNER JOIN Copies ON Movies.Movienum = Copies.Movienum GROUP BY Movies.Movienum HAVING 0 = COUNT(CASE WHEN Copies.Format='Blu-ray' THEN 'uhoh' END) Still displaying an error?
I would select from the movies query then do a where exists in which you check the other table for a dvd.
Issue with that (I think) is that it will return every Movie that has a DVD copy. I need to return Movies that ONLY have DVD copies.
MINUS is unknown?
What database are you using? I believe MINUS is ANSI standard, but maybe not. You could also rewrite the query with a NOT IN or NOT EXISTS if you can't get minus working. Basically I wouldn't use aggregates here. 
I am actually not sure what Database this is using. It's in SqlFire by QUT.
 select Title, YearReleased from Movies m where not exists (select null from Copies c where m.id = c.id and c.format &lt;&gt; 'DVD') and exists (select null from Copies c where m.id = c.id and c.format = 'DVD')
You should get into the habit of using ANSI 92 onwards join syntax (join...on...), rather than your ANSI 89 syntax. It's more flexible (especially in outer joins) and is the standard in DBs now.
&gt; Still displaying an error? any chance you could share what the error message is? 
This solution follows my earlier hint, where the following substitutions occur in preparation: 1. The "Writing_Assignment" table is replaced by a J table filtered by “TheCity” (all relevant projects - instead of "all writing assignments") 2. The Completed Assignments table is replaced by table MPJ, projecting column PID (or columns PID and JID as required) (this will provide all PIDs currently in use in all projects - instead of "all assignments completed by all students"). I used the following data (I used the needed columns only - all the rest is just 'fluff'): CREATE TABLE m (mid INTEGER NOT NULL); CREATE TABLE p (pid INTEGER NOT NULL); CREATE TABLE j (jid INTEGER NOT NULL,city CHAR(20)); CREATE TABLE mpj (mid INTEGER NOT NULL,pid INTEGER NOT NULL,jid INTEGER NOT NULL); /* 3 manufacturers */ INSERT m VALUES (1000);INSERT m VALUES (2000);INSERT m VALUES (3000); /* 9 products */ INSERT p VALUES (1001);INSERT p VALUES (2001);INSERT p VALUES (3001); INSERT p VALUES (1002);INSERT p VALUES (2002);INSERT p VALUES (3002); INSERT p VALUES (1003);INSERT p VALUES (2003);INSERT p VALUES (3003); /* 4 projects – 2 in TheCity */ INSERT j VALUES (1,'TheCity');INSERT j VALUES (2,'TheCity'); INSERT j VALUES (3,'TheOtherPlace');INSERT j VALUES (4,'TheOtherPlace'); /* 24 links between projects and products */ INSERT mpj VALUES (1000,1001,1);INSERT mpj VALUES (1000,1002,1);INSERT mpj VALUES (1000,1003,1); INSERT mpj VALUES (2000,2001,1);INSERT mpj VALUES (2000,2002,1);INSERT mpj VALUES (2000,2003,1); INSERT mpj VALUES (3000,3001,1);INSERT mpj VALUES (3000,3002,1);INSERT mpj VALUES (3000,3003,1); INSERT mpj VALUES (1000,1001,2);INSERT mpj VALUES (1000,1002,2);INSERT mpj VALUES (1000,1003,2); INSERT mpj VALUES (1000,1001,3);INSERT mpj VALUES (2000,2002,3);INSERT mpj VALUES (3000,3003,3); INSERT mpj VALUES (1000,1001,4);INSERT mpj VALUES (1000,1002,4);INSERT mpj VALUES (1000,1003,4); INSERT mpj VALUES (2000,2001,4);INSERT mpj VALUES (2000,2002,4);INSERT mpj VALUES (2000,2003,4); INSERT mpj VALUES (3000,3001,4);INSERT mpj VALUES (3000,3002,4);INSERT mpj VALUES (3000,3003,4); That makes: Three manufacturers (unused table) Nine distinct products Four projects: two in TheCity, two in TheOtherPlace; three products are used in all projects in TheCity (pid=1001, 1002 &amp; 1003) Twenty-four mpj linkage rows to reflect the above project characteristics. So, we end up with: /* get only the products used in all projects in 'TheCity' */ SELECT DISTINCT a.pid FROM ( SELECT mpj.pid,mpj.jid FROM mpj ) A WHERE NOT EXISTS ( SELECT * FROM j WHERE CITY = 'TheCity' AND NOT EXISTS ( SELECT 1 FROM mpj WHERE mpj.pid = A.pid AND mpj.jid = j.jid ) ) ; For your constraint (i.e. "Get the product id's (pid) for products that are supplied to all projects in TheCity"), only projects 1 &amp; 2 qualify: in those, only products 1001, 1002 and 1003 are used in both. 
It doesn't display an actual error, just some stupid message from SqlFire that doesn't seem to help much.
Not the best but how about: select title, year from movies where not exists (select 'true' from copies where movies.id = copies.id and format = 'bluray') 
That's generally not a good idea as some players can play for multiple teams (eg. club vs national, or fixed vs loan).
EMPLOYEE (EMPLOYEE_ID [PK], FIRSTNAME, LASTNAME, ....) TEAM (TEAM_ID [PK], TEAM_NAME, ....) POSITION (POS_ID [PK], POSITION_NAME) TEAM_EMPLOYEE(TEAM_ID [PK], EMPLOYEE_ID [PK], TEAM_EMPLOYEE_ID [AK]) TEAM_EMPLOYEE_HISTORY(HISTORY_ID [PK], TEAM_EMPLOYEE_ID, POS_ID, DATE_STARTED, DATE_ENDED, ....) EVENT (EVENT_ID[PK], DESCRIPTION, WINNING_TEAM_ID, RUNNERS_UP_TEAM_ID) GAME (GAME_ID [PK], EVENT_ID, GAME_DATE, HOME_TEAM_ID, AWAY_TEAM_ID, HOME_SCORE, AWAY_SCORE) GAME_EMPLOYEE(GAME_ID [PK], TEAM_EMPLOYEE_ID[PK], PLAYER_NUMBER) Notes : PK = Primary Key AK = Alternate Key I'm using Employee rather than player, so your model is more flexible and could add manager / coach, trainers, physio. etc. The design is just a rough guide but quite flexible, and you could jazz it up further. 
Option B. if I'm reading this right, you've got a table for each line of support in option A. Seems like too much overhead, and the potential for reporting nightmares later on. I'd just keep it all in one table. 
Maybe I worded it badly, but in each case there are only two tables (Roster and Employee). Option A: Roster(Date, FirstLineSupport, SecondLineSupport, EscalationOne, EscalationTwo) Option B: Roster(Date, EmployeeID, SupportRole) Using option A, each of the support role columns would contain an employee ID. For option B you could add a table that describes the support role if the meaning is not clear.
B. B can handle changes to your support model or expansion to cover other support models easily. For example, in the future you may want to use more flexible staffing where you have employees distributed as 70% front line, 20% second line, 10% escalation. Or you might want to add a new role, etc.
1. Get a DEV edition. 2. You don't necessarily need VS to be on the same VM but it can be nice. SQL Server actually optionally installs with it's own VS that used to be called BIDS, now called Data Tools. It looks like VS but is used for SSAS, SSRS, SSIS creation. 3. No, you don't necessarily need a Server version of your OS for your laptop. 4. I took a Microsoft Collection for developer that was the equivalent of Couch-to-5k. W3Schools may be a good free way to start. Bloggers I love include Brent Ozar, Pinal Dave, and Kim Tripp. 
As you all can I see, I've failed at a dozen different ways to try to do this. :(
Maybe I'm missing something (is this Oracle?), but try adding count(distinct Pro.DeviceIDO) (you don't necessarily need the distinct clause) in the select statement and then group by *everything* else. If the rows are indeed identical but for the one value, that should give you the count.
Whats your goal? To develop? Read one of CJ Dates introductions to SQL books followed by SQL for smarties. To administer? Follow the MS curriculum 
Thanks, I think I'll but them all on the same VM so. Its gonna be a toss up between windows 8 and server 2008, we'll see how it goes. I'll have a looks at W3Schools.
Thanks, though I'm along way off creating my own scenario (i think) but I'll keep it in mind
I got it working, I needed to put my count statement within its own Join. SELECT distinct sys.Name0 AS 'Server Name', PRO.*,sys.Name0 AS 'Server', sys.AD_Site_Name0 AS 'AD Site', BIOS.Manufacturer0 AS 'Manufacturer', CS.Model0 AS 'Model', BIOS.SerialNumber0 AS 'Serial Number', OS.Caption0 AS 'OS Version', OS.CSDVersion0 AS 'Service Pack Level', OS.Version0 AS 'OS Version Number', OS.InstallDate0 AS 'OS Install Date', CPU.Name0 AS 'Processor Name', CPU.Version0 AS 'CPU Internal', sys2.SystemRole0 AS 'System Role', sys.Creation_Date0 AS 'Creation Date', WS.LastHWScan AS 'Last Hardware Scan', BIOS.ReleaseDate0 AS 'BIOS Release Date' FROM v_R_System as SYS LEFT OUTER JOIN v_GS_PC_BIOS AS BIOS ON sys.ResourceID = BIOS.ResourceID LEFT OUTER JOIN v_GS_OPERATING_SYSTEM AS OS ON sys.ResourceID = OS.ResourceID LEFT OUTER JOIN v_GS_COMPUTER_SYSTEM AS CS ON sys.ResourceID = CS.ResourceID LEFT OUTER JOIN v_GS_PROCESSOR AS CPU ON sys.ResourceID = CPU.ResourceID LEFT OUTER JOIN v_RA_System_SystemOUName AS OU ON sys.ResourceID = OU.ResourceID LEFT OUTER JOIN v_GS_SYSTEM AS sys2 ON sys.ResourceID = sys2.ResourceID LEFT OUTER JOIN v_GS_WORKSTATION_STATUS AS WS ON sys.ResourceID = WS.ResourceID LEFT OUTER JOIN v_Network_DATA_Serialized AS IP ON sys.ResourceID = IP.ResourceID LEFT OUTER JOIN dbo.v_HS_COMPUTER_SYSTEM as Comp ON sys.ResourceID = Comp.ResourceID left join (select ResourceId, Count(DeviceID0) 'Number CPUs' from v_GS_Processor group by ResourceId) AS Pro ON sys.ResourceID = Pro.ResourceID WHERE (OS.Caption0 LIKE '%server%') 
 select all from InvoiceTable where PaymentMethod = 'Cash' and InvoiceNo in ('1234' , '5678', '9313' , '1328') if that errors, try this (same thing but with invoiceno as number instead of char): select all from InvoiceTable where PaymentMethod = 'Cash' and InvoiceNo in (1234 , 5678, 9313 , 1328) 
I could kiss you. Thank you so much!
 update #STATS s set s.CommonStart = d.CommonStart, s.CommonEnd = (select MIN(t.CommonStart) from #TEMP t where t.CommonStart &gt; s.StagingEnd) from #DMS_TEMP d where ...
I COULD KISS YOU! So close this seems to be it - update #STATS set #STATS.CommonStart = (select MIN(#TEMP.CommonStart) from #TEMP where #TEMP.CommonStart &gt; #STATS.StagingEnd), #STATS.CommonEnd = (select MIN(#TEMP.CommonEnd) from #TEMP where #TEMP.CommonStart &gt; #STATS.StagingEnd) from #TEMP
IN is cool when it's a SMALL discrete number of values. However, if it becomes larger than a breadbox, consider using a subquery - WHERE foo in (select foo from tablename where stuff=1)
Do not like this one bit - Roster(Date, FirstLineSupport, SecondLineSupport, EscalationOne, EscalationTwo) How are you going to query that? Better to do this Employee (ID, Name) --lookup table of Employees Support (ID, Descrip) --lookup table of Support types EmpSupport (ID, EmployeeID, SupportID, Priority int) EmpSupportEscalation (ID, EmployeeID, EscalationEmployeeID, Priority int) This way it's N3 and you don't have to do unpivot-y stuff or UNION ALL stuff.
I think you mean WHERE MovieID NOT IN (SELECT MovieID FROM MovieCopies WHERE Format='BluRay')
SQL Server 2012 Express is what you should get, 64 bit if your machine is 64 bit, with the Management Studio Express tool. SQL is not a big language, it can be very complex though. I suggest the mother-fucker tutorial: http://programming-motherfucker.com/become.html I've linked you to the overall page, there's an SQL link there. Most of the tutorials there are good, and straight to the point.
It's not that hard to query - you write a join for each of the support columns. This makes it easier to see the results in a spreadsheet format, but makes the queries unwieldy. Alternatively, you can create a sub-select for each role.
I think i'll start reading one at work. Thanks for the suggestion.
You seem to be joining to a table named "ITEM" in your subquery, but aren't including that table in your FROM clause. 
I would honestly invest money in a book like Oracle SQL by Example. Study it and do all the labs (it has all the answers too). It's what I used in graduate school. I'm an SQL ninja now. The problem with free resources are sometimes not having enough solid examples. Just spend some money, but don't pay for an online course
Re: point A), if the cost of joining is due to the number of records in the tables, it might be worth looking at partitioning or materialized views. Re: point B) I guess it must be happening quite frequently to be thinking about overhauling the whole data model. What you've described is pretty much an entity-attribute-value model. I think the main thing to weigh up with EAV is whether avoiding schema updates is worth the somewhat increased difficulty in maintaining the integrity of the data and extracting it (e.g. needing to pivot or case to get the data into one column per variable). An alternative could be to keep your core property tables as they are, and make a 1:1 sparse table for each entity level (A, B, C, etc) where you store all the other non-core properties. JSON (or hstore) could be another option but it doesn't look like MS have added that (yet).
An event is like a cup competition, a league competition, etc. Call it competition instead if it makes it easier. It has many games, that make up its schedule. At the end of it, it has a winner and runner up, which you fill in, when known. That aspect is de-normalised, to save you having to calculate from competition games table. Fairly standard practice. In every game you have many employees (players, referees, managers, linesman etc.). It's a one to many situation. The unique id is a combination of the two. Just because someone belongs to a team, doesn't mean to say they play in the game. Sport stats always record who took part. When you talk about having to add 22 employees entries per game, it really is nothing to a RDBMS system. It's useful information. I work on systems that have hundreds of millions of entries in some tables! 
entity-attribute-value (EAV) schemes have a very deserved reputation for being extremely difficult to draw meaningful information from google EAV and you'll find a bunch of horror stories joining your existing tables should not be a problem if they are indexed properly
&gt; Each entity had its own table, with its properties stored in columns and an ID pointing to the parent. Imagine an application that stores information about cemetery plots. section block plot other_columns -- North A 13 ... North B 7 ... East A 6 ... East B 1 ... South A 1 ... A lot of database designers will do this (below), thinking that their database is now a) "normalized", and b) "easier" to query, because each join is based on a single column. (You don't have to write joins on compound keys.) Table: sections section_id section -- 1 North 2 East 3 South 4 West Table: blocks block_id block -- 1 A 2 B Table: section_blocks -- section_block_id section_id block 1 1 1 2 1 2 3 2 1 4 2 2 5 3 1 Table: plots section_block_id plot -- 1 13 2 7 ... To recover the first table, you'll need a bunch of joins. But in fact, assuming an obvious primary key of {section, block, plot}, and assuming the set of columns hidden by my lazy "other_columns" doesn't introduce any new dependencies, that first table is probably in 5NF. Adding id numbers all over the place doesn't normalize the table, it just splits and disperses the key. Before you go all EAV, make sure your problems aren't due to bad relational design.
Thank you! A great link. It seems that the major issues in those slides are that it's difficult to maintain data consistency/integriy, and it's difficult to construct a row with a column for each attribute. All of the data injection is strictly controlled by us, which makes maintaining data integrity slightly less of a concern (although I'm sure plenty of people say that before royally screwing up their data :)). A naive question (which I asked another commentor): why do we need to get the data into one column per variable? Can't we do any processing we'll do (with C# or what have you) with all of the variables still in their own rows?
Our database was similar - we had the section_id and block_id tables, but not the section_block_id table. sectionID blockID plot other_columns -- 1 1 13 ... 1 2 7 ... 2 1 6 ... 2 2 1 ... 3 1 1 ... Are you saying we should avoid this setup, or are you just saying we should avoid further breaking things down, as in your example? Excuse me while I go read up on 5NF :)
As I understand it, joining a table with M rows to a table with N rows (on an indexed property of N), will still take M number of index seeks on N, or M*log(n) time. Is that accurate?
This is poorly written and has very little to do with SQL
Just wanted to say thanks for such a great explanation. It really helped me out. &gt; The answer is, perhaps, counter-intuitive. It certainly was for me. I found it difficult to reason about the final SQL statement. [Here's how I was finally able to digest it](http://i.imgur.com/hjbu70U.png) (in case it helps anybody else). edit: I should also add that the solution v0rl0n provided is more generalized than the following : select mpj.pid, count(*) from mpj, j where j.city='TheCity' and j.jid = mpj.jid group by mpj.pid having count(*) = (select count(*) from j where j.city = 'TheCity'); because it allows for more than one shipment row per product/project. In other words, if you added another line in mpj for product 1002 going to project 1, it would no longer work, as the main query count would be 3 and the in-line query count would still be two. You could change the comparison to a &gt;=, but that still wouldn't solve the problem, because you could have two shipments to project 1 and no shipments to project 2 and still have that product returned (even though the requirement is that you need products that are shipped to ALL projects in TheCity). Thanks again v0rl0n for the excellent replies. 
I guess the first thing I'd do is "denormalize" it (without actually doing denormalization) into a View which might very well look like the pivoted example you threw out. Because it IS easier to read and presents the data much better for human consumption. But Views can change with business rules. Base ERD is much much more difficult to alter once you've made those decisions.
 Nothing is required :) However if you want data consistency it will prevent you deleting rows in the foreign table if there are references to them. There are also performance benefits.
I people that OP is asking why the first CREATE TABLE does not have "FOREIGN KEY" and the second CREATE TABLE does, and if there's a difference between the two.
According to [this](http://stackoverflow.com/questions/1629537/sql-server-foreign-key-constraint-benefits), there aren't any performance benefits
 Mayby look for the 2nd link instead of the first one? http://stackoverflow.com/questions/507179/does-foreign-key-improve-query-performance Its also mentioned somewhere in the msdn doc's. Seriously just because its written on stackoverflow does not make it correct! 
Are you referring to this link? http://www.experts-exchange.com/Microsoft/Development/MS-SQL-Server/A_4293-Can-Foreign-key-improve-performance.html If so that's fair enough. I'm just going by what the majority are saying / agreeing with. I've been proven wrong many a time though when it comes to SQL!
SQL Server and SQLite are two different things.
In SQLite, there's no difference. I think MySQL will parse but ignore the first version. Correct syntax . . . note the parens. CREATE TABLE child(x, y, FOREIGN KEY (y) REFERENCES parent(id)); 
Based on the [CREATE TABLE documentation](http://www.sqlite.org/lang_createtable.html), the first syntax looks like a column constraint. The second is a table constraint. I'm not sure what differences those have, however. My guess is that table constraints can be more complex and reference multiple columns.
The purpose of a foreign key is keeping the data intact and correct. With a foreign key in place, you are prevented from putting a value in which doesn't exist in the other table (like if you have 100 movies listed, you won't be allowed to point to movie #350,) and you get the opportunity to specify what happens when you delete something that something else depends on (like if you say bob has movie 415, then you delete movie 415, should it refuse, should it delete the record about bob owning that, et cetera.) Are they needed? No, but they're a good way to prevent mistakes in your data, so you should have them.
So you're saying the command that links one column in one table to a column in the other table is not a foreign key?
Over the years in my workplace we found that is better to not use database side constrains and rather use them on the application side, this allows a lot more flexibility when cleaning a database or in case of a design change that involves a constrained table
But what does that have to do with the question?
Thanks graciously accepted. Glad it helped (if that kind of madness ever helps anyone... :-) ). ...wanders off...
Okay, but I suppose my quesiton is more: why would you need to retrieve the data with one variable per column? I can see why you'd probably need to do that if you're doing data manipulation in SQL, but if you're doing the processing elsewhere, couldn't you just dump everything with a bunch of rows and process it that way? Like I said, I'm fairly new at this :)
Both of them are standards-compliant (sql-1992), though you forgot the column types and the parentheses around the (y) after "FOREIGN KEY". I would go with the first as it involves less typing. The reason for the second format is to allow for a foreign key that references multiple columns. 
If I were you, I would break this up into smaller queries, no need to do all the joins at once. Go ahead and make them nested. So start with the easy: select author.name, book.title, book.cateogry from author inner join book on author.isbn = book.isbn ; ^that should give you all the books with author's names attached. Then you can add on the next piece to this table with an alias for the table you just created: select * from (select author.name, book.title, book.cateogry from author inner join book on author.isbn = book.isbn) as tempTable left join tempTable.isbn = award.isbn ; ^^That will give you all the results from the temporary table we created above, and because you use a left join, any book that does not have a prize will show null. Let me know if the syntax is wrong or if you still have problems. 
Given any owner could have multiple pets, and given the pet DOB is possibly null, and given there is no pet ID, I'd go with Owner Email, Pet Name Why? Because this allows multiple pet owners to be in the same house (with the same number) and for owners to have multiple pets. I do make the assumption each owner has only one pet of the same name though.
Looks like your original installation corrupted and you are stuck in the installation limbo. If you have any user database in your server/PC, try copying database files (.mdf/.ldf) into safe place. Try uninstall completely, if you can't remove them using other clean up tools, remove the leftover files/folders and also remove registry keys. Restart your computer after everything removed. Run the installer and try to fresh install again. NOTE : Your mdf/ldf files should be able to re-attach with your newly installed SQL Server.
&gt;Crazy cat lady: "Meet Kitty &amp; Kitty, they're twins!"
Based on the sample data, OwnerPhone, PetName is also a candidate key.
through my childhood we had atleast 5 cats, none of them had names... sooo ...
I think you probably answered your own question, but to use your case maybe you want a table showing which entities have which attributes, with the entities in rows and attributes in columns. As you say, its less of a problem if you have an application to help with / do that. Getting back to your original post, your idea is doable, it just requires more thought on managing the data than a traditional SQL data model.
The only sane PK there is the nonexistant petID key. Any attempts at making a composite PK from this, even though they work for this exact data set, will force really weird usage constraints. Otherwise you can go with: (petName, petType, petBreed, petDOB, ownerLastName, ownerFirstName, ownerPhone) or (petName, petType, petBreed, petDOB, ownerLastName, ownerFirstName, ownerEmail) In whatever order works best for your seek queries. But when you're putting so many keys into your PK you know you're doing something wrong. Furthermore this table is silly, there should be a reference table that contains the information about the owners. Table should really be (petName, petType, petDOB, ownerID) because at this point it's evident that you'd be constraining people to never name pets that were born at the same time the same thing. Sure it's a silly thing to do but why would you put that artificial constraint on your customers just because you can't be arsed to add a petID? So: pets (petID, petName, petType, ownerID) owners (ownerID, ownerLastName, ownerFirstName, ownerTelephone, ownerEmail) And you've got something sane to work with.
Sounds like a CLR problem. Try to look for help in a larger subreddit focused on algorithms or techsupport.
i wrote exactly this "select * from (select author.name, book.title, book.cateogry from author inner join book on author.isbn = book.isbn) as tempTable left join tempTable.isbn = award.isbn;" and its giving me an error **EDIT: i tried this "SELECT author.name, book.title, book.category, award.prize FROM award RIGHT JOIN book ON book.isbn=award.isbn INNER JOIN author ON author.isbn=book.isbn;" and it appears to be working and names are not null. not sure if this is correct though
This problem sounded pretty interesting, here is a solution I made for Oracle. here is the gist of it. first it checks if the strings are the same and just returns one of them if it is true without doing any work. then it builds up two arrays, 1 for each string and for every combination of letters(abc -&gt; a,b,c,ab,bc,abc). it then loops backwards over the first array (starting from the biggest chunk) and compares each element to that of the second array (also going in reverse, starting from the largest chunk). once it finds a match then it returns the matching value. Since we are looping backwards this is or is tied for the largest matching chunk. your question did not explain how to handle multiple matching sequences of the same length. *EDIT* I misinterpreted the question to be longest substring. I am working on longest sequence now... added comments to function and a check for if either param is null to return null; removed unnecessary to_chars and fixed some comment verbiage function longest_substring(p_s1 varchar2, p_s2 varchar2) return varchar2 is -- holds string slices type str_t is table of varchar2(100) index by varchar2(100); str_1 str_t; str_2 str_t; -- holds the respective current slice -- when traversing backwards over the arrays of slices cur_s1 varchar2(100); cur_s2 varchar2(100); -- used to not have duplicate code(i-offset-1) z pls_integer; s1_length pls_integer; s2_length pls_integer; loop_length pls_integer; offset pls_integer := 0; begin -- if both strings match then this is the longest sequence if p_s1 = p_s2 then return p_s1; end if; -- if either inputs are null then dont do any work if( p_s1 is null or p_s2 is null) then return null; end if; s1_length := length(p_s1); s2_length := length(p_s2); -- loop as many times as the longest string loop_length := greatest(s1_length, s2_length); -- traverse over each string in incrementing chunk sizes -- ending with the largest string size for i in 1..loop_length loop offset := 1; z := i+offset -1; -- advance the starting position of the chunk and send if the chunk -- will exceed the length of the longest string while( z &lt;= loop_length) loop -- if we can slice the current chunk without -- exceeding the length of the string then do so if z &lt;= s1_length then str_1(i||':' || offset) := substr(p_s1,offset,i); end if; -- if we can slice the current chunk without -- exceeding the length of the string then do so if z &lt;= s2_length then str_2(i||':' || offset) := substr(p_s2,offset,i); end if; offset := offset + 1; z := i+ offset -1; end loop; end loop; --start with the last element of the first array cur_s1 := str_1.last; while cur_s1 is not null loop -- start with the last element of the second array cur_s2 := str_2.last; while cur_s2 is not null loop -- if the elements contain the same value then return it if str_1(cur_s1) = str_2(cur_s2) then return str_1(cur_s1); end if; cur_s2 := str_2.prior(cur_s2); end loop; cur_s1 := str_1.prior(cur_s1); end loop; -- return null if no matching values were found return null; end longest_substring;
When google doesn't have an answer, thank god for reddit. It looks like this is returning the largest common string rather than the largest common subsequence, but this is the closest I've seen in SQL. For the values "thisisatest" and "testing123testing", the LCS would be "tsitest". 
aww lame, sorry for the confusion. Give me a minute :P
any chance this might come back? i just finished learning t-sql and was wanting to get into SSRS. Thanks.
 The isbn thing confuses me. What's the difference if I did author.isbn=award.isbn instead of author.isbn=book.isbn?
Gold if you do it, no pressure.
consultant life took over. I still have an editor open on this, I will work on it again in a bit.
just curious, what is this useful for? or is this just an exercise like [the traveling salesman](http://en.wikipedia.org/wiki/Travelling_salesman_problem)?
Given the domain, Kitty 1 and Kitty 2 would actually be a good solution. If the owner doesnt give you enough information to uniquely ID the animal, then you are fucked either way, SQL aside. Think of it as you were going to the shelf to get a file (this is also a good way to think about indexing in most cases). First the owner (email is good, but I might use NAME not first and last). Then once in the file you are looking for the specific pet. They say you have kitty in front of you, but you don't see a kitty. You see only Kitty 1 and Kitty 2. so you Ask. If they don't know then you have a reality problem not a SQL problem. How would they keep track of that real life instance on Animal? Description? Nickname? Color? its a crap shoot. Better off going with mgdmw's answer and moving on. This is the most likely answer and all options are equally wrong and or complicating, IMO. 
This is all true, but sometimes we go to war with the army we have. I assume this is a learning exercise, not a real world pet DB. 
Because you are not even dealing with the award table at that point of the query. The idea is this: you want to join Table A, Table B and Table C. You are going to accomplish that by joining Table A and Table B together - giving you Table Temp. Then join Table temp and Table C. You are trying to Join table A, B, and C at once and that is causing you the error. 
I have tried to identify which Objects are being missed, but it seems inconsistent. I have also tried slightly different versions of the queries, like: SELECT * FROM Objects WHERE NOT EXISTS ( SELECT * FROM ObjectFlags WHERE (ObjectFlags.vnum = Objects.vnum) AND (ObjectFlags.flag = 'GLOW') ); But I get similarly inconsistent results with this syntax.
Good tips, thank you! For the purpose of this exercise I decided to normalize it to BCNF like this: PET (Petname (PK), OwnerEmail (FK), PetType, PetBreed, PetDOB) OWNER (OwnerEmail (PK) , OwnerLastName, OwnerFirstName, OwnerPhone)
That's what I'm trying to do now. It does NOT seem to fail in the same place each time, it seems to just randomly choose to leave out or skip over Objects. Continuing to debug though (while drinking). Trust me if Google could help I would be fine; that's what the first 6 hours went to haha. I really don't love Access, but it was such a small/simple project just to brush up on things and teach myself some C#, and the DB is never going to really grow or change much (it's a 10-year-old game), so I figured "why not?" I guess I have my answer...
change those queries to select count(*) and see if it changes each time. What you may be seeing is really just a bug in the UI and not an actual difference in the results. Also, if you are using Access 2007, I can tell you from painful experience that it is full of stupid little bugs like that.
It would be very helpful in finding the issue if you posted the table structure, some sample data, and the SQL statement that you're having an issue with. Otherwise, I don't think anyone has enough information to give a hand. Thanks.
More upvotes to you! That is exactly what I want!
NOT Galaxy S4. Disappoint.
Probably color or Crazy-Cat-Lady-sense. So a crazy cat lady naming her cats the same is a reality problem, but there are people who name their pet after a previous pet. That's not so unheard of. In this way people will still have pets with the same name. My recommendation would be to make the DOB a datetime field, if you don't know it, it's the day and time it came to the shelter (maybe minus the years you think the animal is old). No problems with twins or "senior and junior" 
Obviously, but surely a more functional example doesn't hurt anybody's understanding :)
I couldn't agree more. Your info was solid
Without any more details people can only guess at what's wrong. Are you perhaps using the wrong aggregate function? If you accidentally went for MIN / MAX instead of SUM and Monday is either the minimum or maximum value, that could explain things. But this is just a completely blind 0800 AM on-my-first-cup-of-coffee stab in the dark. And now that I think of it it has little to do with running tally. And what the hell am I clicking links about running tally's for so early in the morning?
Before you make it a PK, check for nulls and then check for dupes select * from Locations where LocationID is null select locationID, count(*) FROM Locations group by LocationID having count(*) &gt;1 Once you have these two things in place, there's no problem with making your PK different from your Identity (auto incrementing column).
Why are you trying to use a varchar as a Primary Key? It would most likely be more prudent to convert it to an integer/long **then** make it a PK. Integers handles auto-increment and PK optimizations much better than varchars and generally take much less time to execute in queries. You also have to worry about sanitizing the values for the column, varchar is much more forgiving than integer data type; PK restraints should be as tight as you can make them. *Edit - It should be noted that there is valid uses for varchar PKs, just that it doesn't look like it is required in this case. Even if you do use them, it should a set length not varchar, such as a guid.*
Try using joins to see if the results are still inconsistent SELECT * FROM Objects INNER JOIN ObjectFlags (ObjectFlags.vnum = Objects.vum) WHERE (ObjectFlags.flag = 'GLOW'); and for the inverse.. SELECT * FROM Objects LEFT JOIN ObjectFlags on (ObjectFlags.vnum = Objects.vum and ObjectFlags.flag = 'GLOW') WHERE ObjectFlags.vnum IS NULL; 
Hi, please see updated info, thanks!
Hi, please see updated info, thanks!
People name their pets the same thing sometimes, so Pet Name is out. There are no reliable Candidate keys.
with an optional SoryLevel column
The assignment has already been submitted and he did get them all working. That query is brilliant tho, thank you :)
An easy thing to check would be to see if left outer joins are necessary.
I have personally found that it's WAY easier to tweak a query when it's formatted well. I didn't do much except space and tab here and there, but here's what I came up with (and I really hope this formatting works, because it's the first time I've tried this!). Edit: I also wanted to let you know that I like to put the commas at the beginning of the line instead of the end because it makes for slightly easier commenting. And it just looks nice, imho. Declare @LocalDay int = ‘20130918’ Select count(*) as Calls_Counted , COUNT(distinct C1.AcctName) + COUNT(distinct C2.AcctName) as Accounts_Reviewed , SUM(R.TalkDuration)/60 as Call_Time , SUM(R.RingDuration) as Ring_Time , case when CHAmex.ChAcct is not null then CHAmex.chacct -- These tables don't appear in your "From", fyi. when CHCiti.chacct is not null then CHCiti.chacct else 0 end as Cubs_No , case when R.CallerType = 7 then R.TargetName else R.CallerName end as To_From , R.LocalDay , Case when R.CallerType = 7 then 'inbound' Else 'outbound' end as Inbound_Outbound , Sum( case when C1.AcctName is not null then 1 When C2.AcctName is not null then 1 else 0 end) as CallSuccess -- If a call is related to a client account, it is summed up. , Case when C1.SomeCode ='R' then 1 when C2.SomeCode ='R' then 1 else ' ' end -- Converts ‘R’ into 1 to (later?) sum up number of Rs From Database3..RecordedCalls as R Left outer join Database1..CallHistory as C1 -- you likely don't need the "outer" here - left join = left outer join on R.SessionID = C1.ChSessionId left outer join Database2..CallHistory as C2 -- same as above on R.SessionID = C2.ChSessionId where R.TrunkCall = 2 and R.LocalDay = @LocalDay group by case when R.CallerType = 7 then R.TargetName else R.CallerName end , R.LocalDay , Case when R.CallerType = 7 then 'inbound' Else 'outbound' end , case when C1.AcctName is not null then C1.AcctName when C2.AcctName is not null then C2.AcctName else 0 end , Isnull(C1.AcctName,'') , Isnull(C2.AcctName,'') , Case when C1.SomeCode ='R' then 1 when C2.SomeCode ='R' then 1 else ' ' end -- Do you really want this as a space, or do you want it blank? 
+1 for formatting tips, I hate blocked code, but tab it out and it becomes something very organic
 SET IDENTITY_INSERT table ON do stuff to table GO SET IDENTITY_INSERT table OFF Is a better pattern :) The potential error produced by doing stuff will skip to the next GO and thus execute the OFF switch, leaving the DB in a safe state. Leaving the flag ON will then cause any further attempts at setting IDENTITY_INSERT, on any table, to error. 
What's wrong with isnull in this context? Doesn't it achieve the same result as coalesce?
One tip I learnt, is use count(1) instead of (*), because it doesn't need to read from columns, just the row count.. Depending on your indexing and how many other columns you do/don't read from in that query, can be a major difference in logical reads.. It's certainly never worse..
upvote for "leading comma" convention, sadly underrated by most developers
Check your query plan, count(1) AMD count(*) produce the same results, optimizer is smart like that
get your original text, put it in something like notepad++. run the replace and get that text and also put that in notepad++. run a diff on the text and see what is really going on. When i say run the replace, I mean do something like "select REPLACE(templateDocument, @BookmarkOldText, @BookmarkNewText) ..."
It would help if you post the exact error message How big is @BookmarkOldText ? It cannot be bigger than a page, so usually about 8k I think (not used MSSQL in some time) 
Sounds like a lot of work. Good luck.
This was the problem, thank you! Not really sure how to get around it .. I was going to try to piece together something using substring sand charindexes however charindex has the same limitation...
Thanks for the reply, bboyjrad nailed it though!
I would try replacing portions at a time, such as: REPLACE(templateDocument, @BookmarkOldText1, @BookmarkNewText1) REPLACE(templateDocument, @BookmarkOldText2, @BookmarkNewText2) REPLACE(templateDocument, @BookmarkOldText3, @BookmarkNewText3) whereas each variable represents a portion of the replacement text.
Yep, but also just to prevent anyone getting the wrong idea.. count(field) is not equivalent to count(1) if field is NULLable, since count ignores NULLs
&gt; has tasked me to improve an existing query Improve it in what way? Make it more maintainable, or are you talking about performance? I don't see any obvious places the query could be rewritten for performance reasons, so any perf gains are likely to come from examining the indices currently in place 
Thanks for the response. I tried that also. Because we're grouping by the room number, and most rooms have different room numbers, it returns with every unique room number and that room's capacity.
Alright I figured it out with some effort. I didn't expect to have to use subqueries in such a simple example but that's what I ended up doing. If someone has a more simple way I'd still appreciate the help. select rooms.b,rooms.n,rooms.c from rooms, (select b, max(c) as max from rooms group by b) as sub where rooms.b = sub.b and rooms.c = sub.max;
You'll have to use a derived table/subquery one way or another. Not sure which platform you're on, but if you are on SQL Server 2005 +, rank is pretty nifty for it: CREATE TABLE #R1 (B Varchar(255), N INT, C INT) INSERT INTO #R1 SELECT 'Building1', 123, 30 UNION SELECT 'Building1', 124, 15 UNION SELECT 'Building2', 123, 28 UNION SELECT 'Building2', 126, 20 SELECT * FROM (SELECT B ,N ,C ,RANK() OVER (Partition BY B ORDER BY C DESC) [RANK] FROM #R1) D1 WHERE [RANK] = 1 
Thanks for your reply. I'm using postgreSQL 9.x.x but more importantly I'm just using generic SQL functions/operators (not your fault, I should have specified). If anything you helped me realize doing *anything* remotely complicated is gonna use subqueries.
yarr apologies captaaaain. This ship be a nice lass named PostgreSQL 9.2.4. And they be more like guidelines than rules arrrggggh!
Agreed.
That's a derived table, not a subquery
What database did/does the old system run on? What database is the new system? Generally, once you have got the data out, it is almost trivial to load in into most new d/bs (all d/bs want data, so vendors bend over backwards to make loading easy). I would start by looking at the old database's capability to export its data to a common format (such as CSV). If you can get the data out in CSV, it's a fair bet that the new d/b will be able to import it using one of it's bulk loader tools. If the old d/b does not permit export to a common format, your options are more limited, and (possibly) more technically demanding. In this case, I would look for high-level language (such as a Python or Ruby) bindings packages for your old d/b's API. You will more than likely be able to get the data out that way. 
Can't get it to CSV atm. I just checked, and the old database uses: MySQL - 5.0.77. The new one uses 5.0.96. Would it be possible without the CSV file?
Just Googled the source and destination products. Looks like you're in luck - both are backed by MySQL at some version or other, so this should be a straightforward port.
&gt; Would it be possible without the CSV file? yes, definitely INSERT INTO new ( ticketID , dept_id , priority_id , topic_id , staff_id , email , name , subject , helptopic , phone , phone_ext , ip_address , status , source , isoverdue , isanswered , duedate , reopened , closed , lastmessage , lastresponse , created , updated ) SELECT id , NULL -- this one may need extra attention , priority , NULL , email , user , short , description , phone , NULL , NULL , status , NULL , 0 , 1 , NULL , NULL , NULL , NULL , NULL , FROM_UNIXTIME(create_date) , FROM_UNIXTIME(last_update) FROM old 
By any chance does the new system offer any sort of API for creating tickets? That way you can feed the API or import function with your existing tickets and they get the same business rules applied. Generally it's bad practice to do inserts into dbs like this without really understanding how the business rules are applied.
Should be more efficient though.
what the hell is a "bread-first search" ??
ORM has only increased support issues and bugs in my experience, except in the most trivial use cases.
Definitely, I have been "that guy" that always used the wrong terms, so I like I make sure people can find out the difference 
What's the difference between the two? Is it because the result set from the inner query is in the from clause with an alias of the outer query? I'm learning myself so I hope I'm asking this question correctly. :-)
ColdChaos I agree, Unfortunately I inherited this demo app which is tied into a few other demo apps and it is already using a varchar. 
He is having you run one select statement. That statement is using sub queries to find the unused records in the tables you'll want to update/delete records from. He's going about it by using the union operator (I think that's the right term) to combine the results from the sub queries. That will generate one result set which the outer query on the country table will use and then display the results of which records are not using the unique id. The result of the whole statement will be a list of records you'll want to remove. However as was noted, you should back up all your tables before making any changes in case your changes have an unexpected result or negative impact. Personally I'd backup the db and also do all of this in a test environment with a back up copy of the database. This way you can test your sql and view the results in the app in your test environment. Does that help?
I wonder if the call times will be off. If the record is in minutes and you're dividing by 60, assuming data type on the column is int you're going to return a rounded number. If you need/want a decimal you should make an adjustment. A way to cheat is to change the 60 to 60.0. Others can tell you how to properly change the query; I think one can do it with a cast or convert. 
It's where you look for the bread before you look for the butter
Well, the short answer I have read is that a subquery runs in the "where" statement, and derived tables are in the select/joins
Oops?
Functional dependencies answer the question, "Given one value for X, do I know one and only one value for Y?" So you might proceed by asking yourself, "Given one value for PropertyName, do I know one and only one value for Type?" (Yes.) The functional dependency would be expressed like this. PropertyName-&gt;Type Repeat for every combination. Expect that to take a while. For this to work with sample data, you need *representative* sample data. But you don't have that. Your sample data says that City-&gt;ZIP, but that's not true in the real world.
Thanks for the reply! So can a determinant be a combination of 2 or more columns? I am wondering if PropertyName and Description -&gt; Amount?
Yes, a determinant can have more than one column. The FD PropertyName, Description -&gt; Amount seems to be true in your sample data, and it sounds like it might be true in the real world. (Think about whether the amount to mow a lawn might change over time if a property paved over most of their lawn.)
There should be 5 tables: ZIP City StreetAddress(city, zip) Property(type, streetAddress) Order(property, serviceDate, description, amount) ZIP and City are not actually related, though they definitely seem so. One is a geopolitical boundary and another is a USPS boundary. A street could cross city/zip boundaries, but I highly doubt a street address would. A property could have more than one type. For example a building in my city is a hotel on the bottom and apartments on the top. Amount is *sort of* related to the service performed, but is usually the result of some rules and calculations (hours * hourly rate, plus taxes, minus discounts, plus surcharges, etc)
Pretty much. If your clients' developers don't understand their DBs, ORM will only help them make broken architectures, in which case, you can come in and charge $200/hr to do what they should have spent half as much time doing themselves =) And, yet, they still won't understand the DB. You can fix a broken program, but it's f***ing impossible to fix a broken programmer.
It's so weird to work with programmers, who I'll freely admit are better programmers than I will ever be, who turn into little Nancies (no offense to any Nancy out there) at the mention of a DB. But it's also so comforting job security wise...
I'm somewhat new to SQL and am creating my first database, a listing of doctors and I'm wondering if your suggestions apply to my database as well. Should I make a separate table for StreetAddress if about 99% of them are unique? That seems to make things more complex. If a doctor moves, I can't just change their address in the main table, I'd have to check the StreetAddress table to see if the new address is in it, if it is not, I'd have to add it, then get the new ID number, and finally make the change in the main table. It just seems a lot more complex and like a lot more work, and for what benefit?
append this -- AND 2 &gt; ( SELECT COUNT(*) FROM interval AS t WHERE t.schedule_id = schedule_id AND t.start &gt; start ) 
I was going to suggest ROW_NUMBER, PARTITION OVER etc
Reading through the linked file, the script should create four tables, with varying levels of primary and foreign key constraints. Assuming this is the desired result, then I see no major problems. However, from your description, it seems that the script has already been run - you should be able to see the tables in your database (although you may need to refresh the Tables view in the Object Explorer, if you're using SQL Server Management Studio). In order to remove the tables (in order to run the script again), you'll need to run something very like the following: DROP TABLE ASSIGNMENT; DROP TABLE PROJECT; DROP TABLE EMPLOYEE; DROP TABLE DEPARTMENT; As you may notice, these are in the reverse order to their creation; attempting a different order should cause the DROP TABLE to fail due to the foreign key constraint. PS: On reflection, it may also be that a previous exercise has created a DEPARTMENT table in your database. You could either drop it, or create a new database to hold the tables from this assignment.
Either way, you didn't answer the question. I know what foreign keys are. I asked what the difference was between two ways of declaring them, and you went on a tangent about what foreign keys are, which is irrelevant to what was asked.
If multiple doctors occupy the same address, then rather than enter the information on each line, you would instead add a reference to a single address table entry. An example of when this would be better would be if you have multiple doctors working at a single address - like, for example, a clinic. Example: Doctors Pierce, McIntyre and Burns work from a single address, "1 The Swamp, MASH 4077, Korea". Type 1 refers to a single flat table, Type 2 refers to multiple tables linked by foreign keys. Type 1 (1 Table): *MAIN* ID, Name, Address 1, 'Pierce', '1 The Swamp, MASH 4077, Korea' 2, 'McIntyre', '1 The Swamp, MASH 4077, Korea' 3, 'Burns', '1 The Swamp, MASH 4077, Korea' Type 2 (2 Tables): *MAIN* ID, Name, AddressID 1, 'Pierce', 1 2, 'McIntyre', 1 3, 'Burns', 1 *ADDRESS* ID, Address 1, '1 The Swamp, MASH 4077, Korea' Should the entire clinic relocate (say, because nearby shelling caused them to bug out) then you would update a single entry in the address table to move all of that clinic's associated doctors, rather than having to edit multiple rows in the main table. Type 1 (1 Table): *MAIN* ID, Name, Address 1, 'Pierce', '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'McIntyre', '1 The Swamp, MASH 4077, Somewhere else in Korea' 3, 'Burns', '1 The Swamp, MASH 4077, Somewhere else in Korea' Type 2 (2 Tables): *MAIN* ID, Name, AddressID 1, 'Pierce', 1 2, 'McIntyre', 1 3, 'Burns', 1 *ADDRESS* ID, Address 1, '1 The Swamp, MASH 4077, Somewhere else in Korea' Should a doctor leave for a new clinic (as far as your address table is concerned), then you generate a new address reference with their new details, which is mostly data that would need to be entered anyway. Type 1 (1 Table): *MAIN* ID, Name, Address 1, 'Pierce', '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'McIntyre', 'Home, USA' 3, 'Burns', '1 The Swamp, MASH 4077, Somewhere else in Korea' Type 2 (2 Tables): *MAIN* ID, Name, AddressID 1, 'Pierce', 1 2, 'McIntyre', 2 3, 'Burns', 1 *ADDRESS* ID, Address 1, '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'Home, USA' Should a new doctor join an existing clinic, then you need only add the address reference. Type 1 (1 Table): *MAIN* ID, Name, Address 1, 'Pierce', '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'McIntyre', 'Home, USA' 3, 'Burns', '1 The Swamp, MASH 4077, Somewhere else in Korea' 4, 'Hunnicut', '1 The Swamp, MASH 4077, Somewhere else in Korea' Type 2 (2 Tables): *MAIN* ID, Name, AddressID 1, 'Pierce', 1 2, 'McIntyre', 2 3, 'Burns', 1 4, 'Hunnicut', 1 *ADDRESS* ID, Address 1, '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'Home, USA' Should a doctor leave that clinic for another clinic (the address of which is already in your table), then you only need to update that doctor's address reference, rather than re-enter all of the information. Type 1 (1 Table): *MAIN* ID, Name, Address 1, 'Pierce', '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'McIntyre', 'Home, USA' 3, 'Burns', 'Home, USA' 4, 'Hunnicut', '1 The Swamp, MASH 4077, Somewhere else in Korea' Type 2 (2 Tables): *MAIN* ID, Name, AddressID 1, 'Pierce', 1 2, 'McIntyre', 2 3, 'Burns', 2 4, 'Hunnicut', 1 *ADDRESS* ID, Address 1, '1 The Swamp, MASH 4077, Somewhere else in Korea' 2, 'Home, USA' **TL/DR:** Yes, it might be more work now, but it should be less work later. Also, MASH references.
I agree it makes sense with your data where most doctors share an office with at least one other doctor, but my question was specifically asking if it made sense to make a different table when almost every doctor has their own private practice with a unique address, like with the data I'm working with. 
In report manager, each folder and report has it's own security settings that you can modify for user access. 
But there is no way to just deny a group
You can only give access to specific users/groups. If you want to allow access to everyone except for a small group, you will need to add every user except for those you don't want to have access. If you have a well-organised Active Directory structure with groups, this could be as easy as adding all of the groups except for the ones you don't want.
Well, the error is pretty clear - you're trying to create something that already exists. SQL Server won't let you do that. You have three options: 1. add a line that checks for existence, and only creates the table if it does not already exist (or, alternatively, drops it if it does exist) 2. manually drop all the tables before you start your script 3. Don't worry about it, because your script has already run successfully. I'd make sure you have 4 tables with the right names and columns before you reach that conclusion though.
No, you need to Allow everyone except for the small group access to the report.
she's a no work in mysql
TOP, she's a no work in mysql
Really? What is the alternative in MySQL? I use this all the time in T-SQL
please do a SHOW CREATE TABLE for the table your query will use a GROUP BY but in order to write it, i'd need to see the actual columns
The table is missing the 2 french columns, which I didn't intend on using anyways, but besides that it is the same as the CSV data I showed. CREATE TABLE `pollingresults` ( `uid` int(11) NOT NULL AUTO_INCREMENT, `electoral_district_number` varchar(255) DEFAULT NULL, `electoral_district_name` varchar(255) DEFAULT NULL, `polling_station_number` varchar(255) DEFAULT NULL, `polling_station_name` varchar(255) DEFAULT NULL, `void_poll_indicator` varchar(255) DEFAULT NULL, `no_poll_held_indicator` varchar(255) DEFAULT NULL, `merge_with` varchar(255) DEFAULT NULL, `rejected_ballots` int(11) DEFAULT NULL, `number_of_electors` int(11) DEFAULT NULL, `candidate_lastname` varchar(255) DEFAULT NULL, `candidate_middlename` varchar(255) DEFAULT NULL, `candidate_firstname` varchar(255) DEFAULT NULL, `political_party` varchar(255) DEFAULT NULL, `incumbent_indicator` varchar(255) DEFAULT NULL, `elected_candidate_indicator` varchar(255) DEFAULT NULL, `candidate_vote_count` int(11) DEFAULT NULL, PRIMARY KEY (`uid`) ) ENGINE=InnoDB AUTO_INCREMENT=382364 DEFAULT CHARSET=utf8 COMMENT='SELECT '
 SELECT t.electoral_district_number , t.electoral_district_name , t.candidate_lastname , t.candidate_middlename , t.candidate_firstname , t.political_party , SUM(t.candidate_vote_count) AS candidate_votes , r.riding_votes , SUM(t.candidate_vote_count) * 100.0 / r.riding_votes AS candidate_percent FROM pollingresults AS t INNER JOIN ( SELECT electoral_district_number , electoral_district_name , SUM(candidate_vote_count) AS riding_votes FROM pollingresults GROUP BY electoral_district_number , electoral_district_name ) AS r ON r.electoral_district_number = t.electoral_district_number AND r.electoral_district_name = t.electoral_district_name GROUP BY t.electoral_district_number , t.electoral_district_name , t.candidate_lastname , t.candidate_middlename , t.candidate_firstname , t.political_party 
so the query works okay? tip: this sub is generic sql, next time you could also use the r/mysql sub
Yes, worked perfectly. Thanks again.
If I'm reading it right, you don't care where each vote was cast, you just want to know the total votes each candidate received? And it's just something you're going to run once? Is that right? If so, you certainly don't need to put it into a database to get the info.
Nope, there is no performance difference between having differing ratios of virtual socket and virtual cores, it's all about the total number of vcpu's assigned to the machine. Licensing is another story of course.
So, Will it be ok if I set 1 virtual socket, 8 - numer of cores and then set cpuid.coresPerSocket to 8?
&gt; cpuid.coresPerSocket Are you on esxi 4?
No, it is ESXi 5.5 -- edit: now I see. This feature was in ESX 4 series.
Just set it in the GUI in the virtual machine settings, nothing more to it. It's only there for licensing purposes, like I said, it doesn't hurt performance to put all vcpus on one socket, or a socket per vcpu.
If you had more than one CPU, then you'd want to think about NUMA. I see you just have one CPU-- so it doesn't really matter. I'd keep the CPU count at 1, and tweak the cores. The OS may be able do some optimizations, depending on the CPU. The cores probably share L3 caches like [this](http://i.imgur.com/f5WTYIf.jpg) or [this](http://i.imgur.com/Id0p2KI.jpg) this is the cache is completely shared. If its AMD, it may have [8 integer cores, but only 4 modules](http://i.imgur.com/FpMmEl8.png), which may have a subtle effect, depending on your load. How this affects performance ultimately depends on your system load. In any case, I wouldn't consider a core a full CPU/Socket-- but each core should have less overhead than a socket. You can see that the CPU has quite a bit of stuff that isn't in the cores themselves. tl;dr Stick with cores until running into NUMA or when balancing cores across multiple CPUs. Of course, there may be non-technical use cases, like legacy software or licensing
I don't know either. It's an intro to databases class and I'm using a program they wrote that is "a seamless low profile GUI-based multi-purpose develop environment for experimenting with multiple command-based systems. It can be used to connect to multiple database servers and our own database prototypes and use them simultaneously with great ease. Batches of commands from multiple systems can be interactively developed, executed and repeated in future." The only feedback it says is failed to execute "code" and a general 'syntax error' statement with no help. I did copy/paste what you posted and that didn't work either.
They were; the only thing that fixed the problem consistently was to add an "ORDER BY" clause... thanks Access.
I'm confused on the order of your from and joins. You're doing the right thing by iterating through this one join at a time but your first step should look more like: select Instructor.Salary from Instructor inner join Student on Instructor.InstructorID = Student.MentorID where Student.Classification='freshman'; This would lead you to try: select Instructor.Salary, Person.Name from Instructor inner join Person on Instructor.InstructorID = Person.ID; So, now you have two results sets that you know work correctly, right? This should lead you to something like this: select Instructor.Salary, Person.Name from Instructor inner join Person on Instructor.InstructorID = Person.ID inner join Student on Instructor.InstructorID = Student.MentorID where Student.Classification='freshman'; If this doesn't work, you have to start breaking things apart. Example. What record set is returned when I: select * from Student where Classification='freshman';
This... I'll add to it though that you might need to look at outer joins because from the table definition you listed a Student doesn't necessarily have a Mentor... **I** wouldn't setup the tables that way, but I'm assuming that's how they were given to you for the assignment.
It is failing to execute the second query you gave me: select Instructor.Salary, Person.Name from Instructor inner join Person on Instructor.InstructorID = Person.ID Edit: my thought process for order was, grab students because we know we need to impose a restriction on this where student is a freshman. I also knew that I needed MentorID of student, so I could grab that value and match it with instructorID and then match that with person.ID.
What's the error?
Honestly, this is all I'm told: $MySQL:&gt;select Instructor.Salary, Person.Name from Instructor inner join Person on Instructor.InstructorID = Person.ID; Failed to execute the query "select Instructor.Salary, Person.Name from Instructor inner join Person on Instructor.InstructorID = Person.ID" SQLException: com.mysql.jdbc.exceptions.MySQLSyntaxErrorExceptio
I would verify your data. select distinct CONCAT('"',classification,'"') from student what do you get?
That line of code fails to execute as well.
The output is: CONCAT('"',Classification,'"') "Sophomore" "Senior" "Junior" "Freshman"
so capitalize the F in freshman in your query 'freshman' != 'Freshman'
I ran the following in sqlFiddle and your sql works fine: create table Student ( StudentID char (9) not null references Person (ID), Classification char (10), GPA double, MentorID char (9) references Instructor (InstructorID), CreditHours int, primary key (StudentID)); create table Instructor ( InstructorID char (9) not null references Person (ID), Rank char (12), Salary int, primary key (InstructorID)); create table Person ( Name char (20), ID char (9) not null, Address char (30), DOB date, primary key (ID)); insert into person values('The Dude', '1', '1234 Street St.', STR_TO_DATE('01,5,2013','%d,%m,%Y')); insert into person values('Mr. Belding', '2', '500 Smiths Ln', STR_TO_DATE('01,5,2013','%d,%m,%Y')); insert into person values('Dufus', '3', '100 Main St.', STR_TO_DATE('01,5,2013','%d,%m,%Y')); insert into instructor values('2', 'OVER_5000', 100); insert into student values('1', 'Freshman', 4.0, '2', 100); insert into student values('3', 'Senior', 2.0, '2', 900); ----------------------------------------------------------- select Instructor.Salary, Person.Name from Student inner join Instructor on Student.MentorID=Instructor.InstructorID inner join Person on Instructor.InstructorID=Person.ID where Student.Classification='Freshman'; SALARY NAME 100 Mr. Belding 
If you are using Oracle 11g+ then use pivot. http://www.oracle-base.com/articles/11g/pivot-and-unpivot-operators-11gr1.php *EDIT* just realized this will not work because you need to create an artificial id for each set for the pivot to work properly and there is nothing safe to sort by so some sort of lagg magic is unreliable
I might be late to the party, but I will tell you how we've done this, and see if maybe the same idea can be applied to yours. -We made a stored procedure, to push users into a table, giving them levels, based on company hierarchy. -Another stored procedure to filter the users' level out, based on their A/D id, which can be snagged from their machine, and tested against the table of users. -Then another stored procedure to pull records based on their level, and parameters selected. 
Here is a hack and a half that will work under 1 condition. the rows coming in from your export table are in order. CREATE SEQUENCE TEST_DATA_SEQ MINVALUE 1 MAXVALUE 9999999999999999999999999999 INCREMENT BY 1 START WITH 1 NOCACHE NOORDER NOCYCLE ; create or replace function get_test_id( p_type varchar2 default 'NEXT' ) return number is begin if p_type = 'NEXT' then return test_data_seq.nextval; else return test_data_seq.currval; end if; end get_test_id; with test_data(pref_key, pref_value) as( -- generate a fake export table select 1,'TRANSACTION' from dual union all select 2,'17' from dual union all select 100,'1' from dual union all select 200,'000011122233' from dual union all select 207,'999888777' from dual union all select 208,'0123456' from dual union all select 210,'222333444555' from dual union all select 217,'808080808080' from dual union all select 220,'1111111^4' from dual union all select 221,'2222222^4' from dual union all select 230,'03/18/13' from dual union all select 240,'77777777' from dual union all select 290,'1717171717' from dual union all select 400,'574.10' from dual union all select 400,'574.00' from dual union all select 400,'278.02' from dual union all select 400,'V85.25' from dual union all select 610,'1' from dual union all select 700,'3333333^4' from dual union all select 710,'2' from dual union all select 2001,'1' from dual union all select 2002,'2' from dual union all select 2003,'2' from dual union all select 2051,'07:00 AM' from dual union all select 2052,'03/18/13' from dual union all select 2053,'09:00 AM' from dual union all select 2054,'03/18/13' from dual union all select 2101,'00790' from dual union all select 2104,'1,2,3,4' from dual union all select 2106,'7' from dual union all select 2107,'YES' from dual union all select 2403,'07:00 AM' from dual union all select 2404,'03/18/13' from dual union all select 2405,'09:00 AM' from dual union all select 2406,'03/18/13' from dual ) select * from ( select case when pref_key = 1 then get_test_id() else get_test_id('CURRENT') end record_group_id, -- add an artificial record group id to related keys test_data.* from test_data ) PIVOT ( -- concatenates all strings of similar keys (400 is the only repeated key in this list) listagg(pref_value,',') within group(order by pref_value) for(pref_key) IN ( 1 , -- you can also provide a meaningful name here by using as 2 as secret_id, -- 100 as create_date, 200 , 207 , 208 , 210 , 217 , 220 , 221 , 230 , 240 , 290 , 400 , 610 , 700 , 710 , 2001, 2002, 2003, 2051, 2052, 2053, 2054, 2101, 2104, 2106, 2107, 2403, 2404, 2405, 2406) ) ; 
There are 100 ways to skin this cat, but this should do: BEGIN TRANSACTION DELETE FROM [DOC_HEADER] WHERE DATEPART(YEAR, [DOC_HEADER].[DOC_DATE]) IN ( '2009', '2010' ) ROLLBACK TRANSACTION When you're sure it's doing what you want it to do, change "ROLLBACK TRANSACTION" to "COMMIT TRANSACTION"
Almost. Your dates (2009-01-01 and 2010-12-31) need to be encapsulated in single-quotes. (Like this: '2009-01-01' and '2010-12-31')
Are you updating the table? Or did you want to replace it with a select statement? If you're updating the table: UPDATE table SET column = "y" WHERE column = "x" If you're replacing from a SELECT statement: SELECT CASE WHEN column = "x" THEN "y" END AS col FROM table
If you are not able to update the source table then define a view with the replace logic in it, then just use the view in your queries instead of the base table. Basically make the view resemble /u/flipstables second query.
I've used [Learning Tree](http://www.learningtree.com/) and had a great experience. It wasn't for SQL, but an intro to .NET... It's focused on "corporate training" I think they even sent my mgr a certificate of completion. 
I would *not* recommend Training Camp. I paid a significant amount of money for a course in Dallas, which they canceled about a week out. They by that time already had my money. I had to cancel my flight to Dallas, reschedule for their next course... in Pennsylvania. I had to stay in a pretty mediocre timeshare in the Poconos for a week to attend a class that was subpar on just about every level. The instructor was done teaching by 2 or 3 in the afternoon every day, and we spent the rest of the time studying as a group. The only real benefit to the course was that I had a group of like-minded people to study with and quiz each other on the material. 
I love that you discuss your professional life while posting with the handle "Bonghitter." :)
rates prefix carrier ... country 002379 Cameroon Mob 00237 009251 Pak Islamabad 0092 00923 Pak Mobile 0092 009242 Pak Mobile 0092 0088 Bdesh 0088 008802 BDesh Fixed Dhaka 0088 00880821 BDesh Fixed Sylhet 0088
Are they all 4 or 5 characters? You could run 2 updates. Update 1 sets all country code to 5 digit. Update 2 finds non-joins with country table and sets those to 4 digit. (Update 3 could find all non-joins and set to null).
Too nasty. Prefix can be 4-7 chars, always starts 00. I know I can SELECT query with **prefix like CONCAT(country,"%")** but how to do a single update query is foxing me. 
the best I can do is offer the course for $9, you can get it for that by using the discount code PSQL. Link: https://www.udemy.com/practical-sql-reporting-with-ssrs/?couponCode=PSQL
Tried this on Postgresql, don't have any MySQL available, but it worked. Also, MySQL supports UPDATE...FROM, so it should work for you too. update rates set country = c.code from (select code from country) as c where prefix like c.code || '%' #concat 
Yeah, you just have multiple sources and then either use a Merge Join or a Union All task to bring those things together. Conversely, if you want the same data source to be used 3-5 times, pull the Data Source into a Multicast and then spin off your tasks from there.
It's a little clunky but I think this should work based on your comment saying the length would be between 4 and 7. In the case statement it will go from the greatest length of 7 down to the lowest of 4 and pick the matching code along the way. I wasn't able to actually try this and I'm not sure of MySQL syntax, but hopefully this helps. UPDATE rates SET country = t.CountryCode FROM rates INNER JOIN ( SELECT Carrier, CASE WHEN c7.Name IS NOT NULL THEN c7.Prefix WHEN c6.Name IS NOT NULL THEN c6.Prefix WHEN c5.Name IS NOT NULL THEN c5.Prefix WHEN c4.Name IS NOT NULL THEN c4.Prefix END as CountryCode FROM ( SELECT Carrier, LEFT(prefix, 4) as prefix4, LEFT(prefix, 5) as prefix5, LEFT(prefix, 6) as prefix6, LEFT(prefix, 7) as prefix7 FROM rates) as r LEFT OUTER JOIN country c4 on r.prefix4 = c4.Code LEFT OUTER JOIN country c5 on r.prefix5 = c5.Code LEFT OUTER JOIN country c6 on r.prefix6 = c6.Code LEFT OUTER JOIN country c7 on r.prefix7 = c7.Code ) t ON t.carrier = rates.carrier 
Well, that's my rigid, formal version, but if you're sure it's not a problem, go ahead :)
I guess my question, as a beginner still learning how to design databases, is this - where is the line drawn where it becomes a problem? I'm designing databases that I have to live with for years, and I want to make sure I do them in the way that is the most "correct" and easiest to maintain, so I'm not kicking myself down the road.
Hi, Please paste the SQL into the www.snowflakejoins.com query visualization tool. You need to replace two dots to one in **Database3..RecordedCalls -&gt; Database3.RecordedCalls** . There are SUMs and COUNTs from more than one table / alias. This looks as a potential **fan trap**, meaning that rows are multiplied and results are incorrect. Just Google for **fan trap** or **chasm trap** and see if it fits your case, and how to fix it. -Alburnus 
But if prefix was 002379, then the LIKE would match 0023 and 00237. What tells postgres to prefer the longer code here? 
I'm assuming that you made a typo in your field names, where `cnumber` and `snumber` are actually `cno` and `sno`, respectively. Also, `R. Smith` is actually being ignored, because your `where` clauses have `J. Brown` as the name they're looking for. Other than that, your SQL is valid. [I tested it here in SQL Fiddle](http://www.sqlfiddle.com/#!2/1e6f42/5), and it works great. Edit: For your own edification (and if you're bored), if you use a different RDBMS (like Postgres), you can solve this using [window functions](http://www.postgresql.org/docs/9.1/static/tutorial-window.html), which make this query much easier. Just one of those things that's good to know that they're out there, although you don't have to know anything too specific, yet.
Yes - cnumber and snumber are supposed to be cno and sno - will edit. I used the code I attached because I know what it should return. So assuming that R. Smith said J. Brown ... this code still doesn't return correctly, although it does run. I can't figure out why it isn't returning correctly though because in my head it should work .... Edit: I just tried it in the link you sent - I see that it is working correctly there.... I wonder why, then when I run this I get the message "Your SQL query has been executed successfully" however I don't see anything. In the past when I have run things that should return null I get the message "empty set".
The following select statement with the join criteria might help you though. select prefix, carrier, country.code from rates inner join country on code = left(prefix, len(code)) Combining that with /u/HapkidoJosh's code might produce pretty clean results. 
Well, as you saw in the SQL Fiddle, it *does* return correctly, so what's "not correctly" for you? What are you expecting, and what's it returning?
Could you explain then what should happen if I change the = sign in the last line to a &lt; sign. I think maybe that is why I am confused. Also I am confused about the message I received still and why it does not just tell me that it is empty set.
If you changed the `=` to a `&lt;`, that would return all of the people that have only taken courses with R. Smith, but not *all* of R. Smith's classes. So, for example, if Smith took CS101, CS102, and CS103, and Debbie only took CS101 and CS102, `&lt;` would return Debbie, but `=` would not, since Debbie didn't take `CS103` with R. Smith. Now, this also ignores the real-world complexity that a course will be given many times in one semester, and then repeated each semester, but this appears to be a very simplified type of database.
Okay - I see. It works much better in that link you sent. Do you think just the program I am using for some reason will not show empty set on this one. In the past it has given me empty set instead of just saying that is was "successfully executed"
Ah, I dunno. What program are you using?
multiple subselects inside a coalesce, like so... select coalesce( (select top 1 code from country where code = left(prefix,7)), (select top 1 code from country where code = left(prefix,6)), ())...
Gotta be careful of those rigth joins, though.
it's called wamp server but it runs MySQL.
You should see if there is a [Longest Common Subsequence (LCS)] (http://en.wikipedia.org/wiki/Longest_common_subsequence_problem) function for MySQL. It is primarily used for fuzzy string matching like this. Here is an example I made for Oracle. You can compare the length of the result to that of the length of your columns and set some sort of threshold for matching.~~ http://www.reddit.com/r/SQL/comments/1mkk44/largest_common_subsequence_function/ccavsbc lcs('bangladesh','bdesh fixed sylhet') = bdesh lcs('bangladesh','bdesh fixed dhaka') = bdesh lcs('bangladesh','bdesh') = bdesh lcs('cameroon','cameroon mob') = cameroon lcs('pakistan','pak islamabad') = pakisa lcs('pakistan','pak mobile') = paki *EDIT* wow i really spaced out when I read your problem. longsest common substring instead of subsequence could still be something to use when matching the codes.
Do you happen to know why STUDENT.SNO intersection RESULT.SCORE would give an error? Why can' you do that?
so using the drop function works, but the professor says i am doing something wrong. She sent out a text file exactly like mine and i have to use that. I will investigate some more, and try a clean sql install on my desktop. Thanks for the tips though they did work, just not to the professors liking :(
thank you this solved my problem, in the end i just needed to create a script to flush those out then i could run the scripts i made and form the professor. Maybe the tables were there from a previous failed attempt to run the script, but thanks again I appreciate it!
Maybe where prefix starts with c.code ?
Performance wise, they are usually going to be the same. But if you aren't aggregating, you can use distinct. 
Thank you, that makes a ton of sense! What's a good way to check query plans?
Awesome, thank you so much!
Both hit TempDB. The GROUP BY way may put you in a better place to UNDERSTAND your data (IMHO distinct is often used as a cop-out for truly understanding WHY duplicates are being returned).
&gt; IMHO distinct is often used as a cop-out for truly understanding WHY duplicates are being returned That's what I was thinking, hence my asking if it was bad practice. What I might do from now on is just use DISTINCT when I want distinct records, and GROUP BY only when I want to aggregate something... that way, when I'm glancing at a query months later, I'll know immediately if I'm aggregating, or just getting distinct values. Thanks for your insight. 
When you are using DISTINCT, you have to ask yourself "Is my data normalized properly? Is this a JOIN and I am forming a Cartesian product somehow?" I find that I /almost never/ need to use DISTINCT for anything...unless I am using it in a COUNT(DISTINCT col) aggregate. And even then, I question that.
As someone who is trying to learn this, these kinds of small typos in teaching material actually make me really paranoid that what I'm reading could be wrong.
Right - well, in a perfect world, I'd never have to use DISTINCT, either... but the databases that I'm having to code against are severely non-normalized. It's almost a necessity 99% of the time, and I have no power to change the schema. However, I'm going to start using execution plans, now that I have an idea of how to get started with them... so with any luck, I'll be able to better tell if I'm doing my joins wrong or not.
I **might** have forgiven one, in the text. But three? And one in the heading? Anyway, good luck in your SQL journey. Tip: Read everything you can by Kimberly Tripp and Paul Randall.
Thanks for the pointer -- I'll be sure to check them out! Is there anything that you would say is a decent starting point? I get the basics pretty well, but I'm looking to improve those skills into something a bit more advanced. 
If you want to count how many unique occurrences of a thing there is. Say I have an order table with 35 different products (and I don't know that) with like 50,000 orders. It is an easy way for me to just see how many distinct products there are (assuming there isn't a like dbo.products table)
I'm going to be contrary here and say don't use DISTINCT. It's lazy and it doesn't help reenforce a clear understanding of your data. GROUP BY forces you to be specific about where you expect duplicates to occur. It helps you (and others reading your work) to understand JOINs better. It will make query errors easier to spot. And finally, you get to use aggregate functions, which can be the most useful of them all :)
Set 4 or 8 virtual cores, always on one virtual socket. In some instances, cores can share cache, so prefer them over sockets. Setting your cores too low will limit an instance from using all available resources. An instance that is maxed out on CPU won't be able to access any more cores than you've set, and could be denied CPU threads that might not be doing any other work. Set your cores too high, and a single instance that hits 100% threaded CPU usage will have a negative impact on the entire server. A word of caution: SQL Server doesn't always adapt correctly when you change the number of v-cores. I've seen instances that were installed with 2 virtual cores. When we bumped the v-core count to 4, SQL Server continued to operate using only 2 threads at a time. If you do change core count on a VM, run some tests to verify all of them are used properly. Good luck!
Obviously we all learn differently, and there's approximately 500 metric shit-tons of info available "out there". Rather than point to one or 2 sources (although I like SQLServerCentral.com), I'd point out that for me what's consistently worked the best is having a "goal", even if it's a made up one. Like, I read about Common Table Expressions a few times and thought I "sorta" understood it, but it wasn't until I had an actual need to use it to solve a problem that it "clicked". Also, the resources could vary depending on which area you wanted to get into (development, administration, BI, etc.) Hope this helps.
You are looking for a recursive common table expression or a hierarchical query. Google those with you DB of choice to help find syntax. Examples you will see will be like a employee to supervisor table or a bill of material components table. Also format your data better: ID ParentID 34 23 54 23 65 23 234 54 235 54 1234 235 5323 235
Sounds like what you need is a recursive query. Without knowing the database you're using we can't provide an exact solution. In SQL Server you would do it with a CTE.
I think what you are looking for is: HAVING COUNT(DISTINCT O.EmployeeID) &gt; 2. This would mean, within your grouping level (OwnerType) it is going to count the number of DISTINCT (unique) Employee #s. It's not entirely clear what you are after however, and seeing the other tables might help.
Sorry I should have mentioned it's MSSQL. I'll do some googling on recursive queries :)
You have no join criteria which links Owner table to Employee table. At a guess, employee table will have some owner Id field in it You may need to add that to your where clause, similar to how you are linking Employee to Service 
If you have a view that calls View A, which calls View B, which calls View C, which calls View D, why don't you just write a view that calls View D? Yes, you might have to learn what they all do and how they relate to your actual view, but it could potentially improve performance by quite a bit.
The query parser will have more work to do as the various levels of the view hierarchy get traversed. Also, from the data security perspective, the user's access rights will need to be interrogated against each view to make sure that user can access everything in the 'top-level' view (by checking user access rights against every child view (and ultimately) base schema/table/column). This task is significantly simplified if security is set up to operate at schema/database or table levels. Data access should not be significantly impacted by nested views IF the query parser in question can resolve the various levels of the hierarchy away at parse time (i.e., as long as the intermediate views do not all need to be physically materialized to resolve the top-level view). Having said all the above, I have had one experience in which an unconstrained SELECT * query from a certain top-level view in a VERY complex view hierarchy failed to return any rows (query failed to complete), but a constrained query (SELECT * ...WHERE...) from the same view worked correctly, and returned some data. This suggests that nested views are not an ideal mechanism, but do serve a valuable purpose sometimes (restricting data visibility being possibly one example). Multiple nested views can be nightmarish to manage. I have worked at locations with such arrangements; without adequate metadata (lineage graphs etc), tracing data through the hierarchies can be a tortuous task. There is no 'silver bullet' here; one either duplicates code, keeping views comparatively simple, or nests the existing views (to leverage the good work of others), making maintenance and traceability difficult. Take your pick... ;-) 
You should delimit those identifiers in your query, or you could have performance issues. (ie: put brackets around *OWNER* and *SERVICE* - [Owner] as O, ... , [Service] as S) http://technet.microsoft.com/en-us/library/aa238507(v=SQL.80).aspx
Views built on views are an real pain to debug.
I think the best answer to this is, "it depends". If performance is fine and multiple nested views are helping with maintenance then sure knock yourself out. If you start to see serious performance degradation then maybe it is time rethink the policy. SQL is not similar to OOP in that it is not easy to encapsulate logic and nest objects and still have the response time needed.
&gt; Yes, you might have to learn what they all do the pain of doing this is always well worth the effort 
This is really not necessary, and it for parsing purposes only, rather than anything to do with performance. 
Sounds like a bad idea. Are the rows actually changing or are you just accumulating new rows? If you truly need to work that way you need to find someway to parallelise the data. For example, if the data was about customers and you had birth date data, you could issue 12 parallel queries to get people born in each month. You would have to ensure that each query did not return more than 5000 results. You are using the service inappropriately. If they catch on to what you are doing, they will likely react negatively.
Are you sure the query execution plan didn't change with the last join or conditions that were used to produce the "outermost" view? Perhaps the database thinks it's more efficient to use a different join order, or the table may even be missing an index or stats. I currently work in an data warehousing environment where we have multiple nested views that query millions of rows. They don't seem to be any less performant than a query against the base table.
What Database platform are you using? Nested views is not something I'd want to do in MySQL, but MSSQL offers some tools to help In many cases you can rewrite one of the views as a set based UDF, and join onto the UDF instead of the view. You can also materialise the view by adding an index, which would at least aid with performance 
 create table MY_TABLE ( ID int, PARENT_ID int); insert into MY_TABLE(ID, PARENT_ID) values (23, null); insert into MY_TABLE(ID, PARENT_ID) values (34, 23); insert into MY_TABLE(ID, PARENT_ID) values (54, 23); insert into MY_TABLE(ID, PARENT_ID) values (65, 23); insert into MY_TABLE(ID, PARENT_ID) values (234, 54); insert into MY_TABLE(ID, PARENT_ID) values (235, 54); insert into MY_TABLE(ID, PARENT_ID) values (1234, 235); insert into MY_TABLE(ID, PARENT_ID) values (5323, 235); with qry as ( select ID, PARENT_ID, cast(ID as varchar(100)) as PATH, 1 as DEPTH from MY_TABLE where PARENT_ID is null union all select t.ID, t.PARENT_ID, cast(c.PATH + '/' + cast(t.ID as varchar(100)) as varchar(100)), c.DEPTH + 1 from MY_TABLE t join qry c on t.PARENT_ID = c.ID) select * from qry; | ID | PARENT_ID | PATH | DEPTH | --------|--------------|-------------------|----------|-- | 23 | (null) | 23 | 1 | | 34 | 23 | 23/34 | 2 | | 54 | 23 | 23/54 | 2 | | 65 | 23 | 23/65 | 2 | | 234 | 54 | 23/54/234 | 3 | | 235 | 54 | 23/54/235 | 3 | | 1234 | 235 | 23/54/235/1234 | 4 | | 5323 | 235 | 23/54/235/5323 | 4 | Alter the query according to any filtering criteria you deem relevant.
As a stored procedure, you'd effectively have to "re-write" the proceeding Views that he's currently trying to utilize into the new stored proc, which seems to be what he's trying to avoid from the get-go. Since he's creating views that *apparently* don't need to be parameterized, if he were going this route he might as well just rewrite a comprehensive View. A stored proc isn't necessary and falls outside the scope of what he's trying to accomplish. 
Perhaps for one's own comprehension, but not always feasible considering scope, access, or as a best use of an employees workday. 
Nested views aren't necessarily a problem - it depends on how (and why) they were created. A bad case of nested views is where a system has been developed over time with a number of incremental adjustments with no process to reactor the resulting code. I've worked on problems that after a lot of effort got solved, only to realise that the solution that was (upon review) far too complicated. Such cases can often be simplified dramatically with all (or most) logic being moved to a single query. The other possible (bad) cause of nesting comes from the "One True View" which lots of systems seem to develop in an attempt to provide most of the useful attributes for a particularly entity (often for reporting). This is so useful that a user creates a query on that view which adds in some additional information (sometimes joining back to tables already in the view). This in turn is so useful that another view is constructed on top of it which in turn is used....well you get the idea. When you look at the end result you realise the same table is being joined 5-6+ times without any real reason. Even if this isn't a performance problem (it often won't be) for maintenance reasons you may want to rewrite some of these). On the other hand, sometimes a process is complex and views can make each individual segment more manageable. For example, I once produced a system which managed project costs. Due to the way the contract was setup there was a number of intermediary views of the data - so (simplifying massively): Base Costs (Base Table) ----Costs+Overheads (View) --------Costs+Overheads+Tax (View) ------------Project Summary of C+O+T (View) ----------------Budget Summary of all Projects (View) This did mean that that the budget summary was a view of a view of a view, etc...but it also meant that individual steps could be tested (and profiled) independently and also that business logic wasn't repeated endlessly (without a gigantic single view which would have been horrific to debug). The end result performed well (although it took a lot of work to get to that point and included materialising at least one of the views). In short, what you're seeing might benefit from refactoring, but I wouldn't automatically assume that. I would say that this sort of nested object structure really requires decent documentation (even if it's just implicit documentation through decent organisation and naming conventions). I'm sure many of us have had horrible experiences trying to understand someone else's procedural code where a function calls a function which calls a function which calls a....which takes an age to investigate should anything go wrong. On the subject of interchangeability, while this is a good principle it's careful not to get too carried away. There are differences. Tables can always be updated directly - views sometimes cannot. Tables can be indexed - Views can *sometimes* be materialised but certainly on SQL Server the restrictions on Indexed Views are numerous (and frustrating).
&gt; and for the two tables below to be joined on current date and store_no i see only one table, table1 i also see an erroneous s2 table alias that was never assigned if the second query produces the current date from the yyyymmdd column, i don't understand how adding 364 days gives you last year's date this problem is messed up perhaps all you want is a UNION? 
Sorry , my first post was unclear. I editted out most of the query that was irrelevant hence the unassigned s2. I want the information from the same table but unsure how to get it in one query. Essentially for each date, I need the current sales figure, and then the sales figure from 364 days ago. The route I took was probably horrible inefficient and long winded but the only way I knew how. Apologies again for the misunderstanding!
&gt; Essentially for each date, I need the current sales figure, and then the sales figure from 364 days ago. that's a bit more clear :) SELECT thisyear.store_no , CAST(thisyear.yyyymmdd AS DATE) AS currendate , thisyear.sales AS thisyear_sales , lastyear.sales AS lastyear_sales FROM table1 AS thisyear LEFT OUTER JOIN table1 AS lastyear ON lastyear.store_no = thisyear.store_no AND CAST(lastyear.yyyymmdd AS DATE) = CAST(thisyear.yyyymmdd AS DATE) - INTERVAL 364 DAY why CAST? what's the actual datatype of the yyyymmdd column? 
It is text format. I have found using cast as quite a reliable way to convert it into datetime (tried convert but wasn't quite working for me). I will give this a crack at work tomorrow, but have a great feeling about this. Thank you! 
&gt; It is text format. do you mean CHAR or VARCHAR or TEXT? (TEXT would be horrible) why the heck would you not use DATE? 
Also, it is good practice to always alias all columns pulled out of a SELECT statement when that statement joins to multiple tables, even if there is only one instance of a given column name across those multiple tables.
It may be best to create a real table which contains every possible Prefix and the Country (int). That way it doesn't end up looking like the CASE statement from hell. Everything looked fine until you got down to BDesh. You're not going to be able to create a great substring algorigm for this one I'm afraid.
In each example I am seeing you appear to be referring to only one table &gt; table1 Joining a table back upon itself is called a recursive join and is not a very common method for what appears to be a much simpler report you are trying to build. Perhaps you could being with a simple list of the table and data you are trying to work with. Something like: Table1 Data: DateOfSale, CurrentDate, InventoryItem, etc Table***2*** Data: StoreID, ClerkID, etc ...and then maybe something like the report headers you expect to see InventoryItem, DateOfSale, ReportDate, ItemCost, SalesPerson, StoreName
Please tell us your familiarity with computer science and software engineering. If you have none, you have about a year or two of minimum knowledge to cram. Dedicated, you could cram it into a couple months. &gt;It seems like a simple enough thing that I'll be able to teach myself. This is because you do not have the knowledge to know that it is not a simple thing. TBH, here's what you need to do, assuming you know jack shit: 1) Learn how to use Linux (Trust me, just trust me, do not run your web server on Windows). 2) Learn how to use the Linux command line 3) Learn how to write HTML, CSS, and Javascript 4) Now learn how to use JQuery ( a Javascript library) because fuck if you can ever get away from using that. 5) Now you need to pick a server side language. I suggest PHP, because learning anything else or starting on a low level language (i.e. what some would call the right way) adds another 2 years. 6) Now re code your webpage to be served by PHP. Also, learn Apache webserver so you can properly manage PHP -&gt; web delivery. 7) NOW you can start learning SQL. MySQL is the engine you'll want to start on. 8) Once you've learned SQL, scrap your entire website again to take advantage of knowing how to use a database. 9) Now change vocations to "web developer", pull 75k a year, and produce code that I'll have to clean up after years later.
Not sure about SQL 2000 Server, but I would use senddate &lt;= trunc(sysdate) on Oracle SE. So, if you can find your equivalent of the trunc() function, that should work for you. [This StackOverflow discussion](http://stackoverflow.com/questions/923295/how-can-i-truncate-a-datetime-in-sql-server) seems to say that your way is better, though.
&lt;=getdate() - 1
this is perfect, equally helpful and dream-destroying
It's been a while since I've worked on Sybase or MSSQL, but I'm pretty sure what you're asking is as simple as this: Select stuff from mytable where datediff(dd, datecol, getdate())&gt;0; or if you actually need everything with a date after close of business yesterday: where datediff(hh, datecol, getdate())&gt;(datepart(hh,getdate()) + 7); where '7' represents hours between close of business yesterday and midnight *datediff(hh, '01/01/2013 12:00', '01/02/2013 14:00') = 26, for example 
On a mobile so bare with me. Use a convert function to get just the date out of a getdate(). &lt;= convert(date, getdate() - 1) This would return anything less than or equal to yesterday's calendar date (no timestamp). Let me know if this isn't what you're looking for. Or go for just a: &lt; convert(date, getdate()) Anything less than midnight today. 
That would be perfect if it wasn't a SQL 2000 box. I figured it out though. Check the edit, it's overly complicated.
you should let him use Microsoft Frontpage and IIS server, let him wrote some HTML and ASP ... and then ended up hiring a real programmer to do the job.
It's horses for courses. Normalization is generally the way to go, but if you have no particular use for the data other than displaying it to embellish something else (like doctor name in your case), then it may not be worthwhile. If, however, these titles have some meaning that can be usefully interrogated (such as "how many of our doctors on-site know dentistry?", and this can be answered by looking at &amp; counting the occurrences of a specific title value ("BDS" in the UK), then the normalization may have more value. If you don't think it's worth the effort that's great, but why bring it up at all? I have no skin in this game, if you want to append the titles to the doctor's name and maintain that, then by all means go for it.
Well, the original spreadsheet given to me has the title as a separate column and I was trying to stick with the format he gave me. And I'm still trying to learn where normalization should be done. I noticed a lot of repeated titles so I thought maybe that should be normalized, then I saw a few of the doctors had multiple titles, that kinda made it seem less worth it. One of the other columns is "category" (dentistry, radiology, primary care, etc), so I'd probably use that category rather than the title for searches. I'm also trying to figure out things like... should addresses be normalized? Most doctors have their own address, but a few do share an office. But then what if the two of the same addresses have different capitalization, or one has a double space instead of a single space... wouldn't that count as a different address as far as SQL is concerned? Is it worth "normalizing" if there can be discrepancies in the normalization? These are the questions that keep me up at night :-)
Maybe you need to convince yourself how far or how serious to take normalization. v0rl0n's spot on thou it doesn't seem definite to you. Have you looked at reference schemas from mature databases? If you were on either Oracle or Microsoft, you could look at the Sample Schemas (Oracle) or Northwind database (Microsoft) (or similar). If this is keeping you up at night, put the time to make a case for you. Cheers.
In a relational system, normalization is very often 'worth it'. To keep a database sane, normalization of key domain participants is, in my view, essential. I am of course assuming that you intend to use a normalised schema (in the style of Inmon) as opposed to a star or snowflake design (as in the style of Kimball)... On the subject of normalizing addresses, again I'd say 'do it'; there may be cases where doctors offer multiple addresses (home, work, correspondence, other...). The slight variations in fields due to 'loose' data entry / data capture rules is a perennial problem, and people tackle it in different ways, depending on the volume of addresses being considered. For example, if you have millions of addresses that you need to resolve overnight, then you're possibly looking at a product called Trillium; if you've only got a handful, then maybe a little shell script to 'normalize' and de-duplicate). This is a technique I know as 'address match/merge'; others probably use different terms but it all amounts to the same thing - keeping a single copy of each address in your d/b wherever possible. A simple rule of thumb is to ask this question "Will there be more of these things "Y" (e.g. doctor titles) for this thing "X" (e.g. doctor) as time goes on? If the answer to that is "yes" (or even if the answer is "maybe") then normalize it - it will save you time further down the line. Honest. It's all part of the fun... :-(
We can't guess your schema and You haven't provided enough information to establish the relationships between the tables. But just looking at your query, I don't think you've grasped the concept of joins at all. Generally joins should be linking the tables according to referential integrity - i.e. column names, not values, some of your joins are bizarre : e.g. JOIN countries AS d ON c.countries_code = "DK"???? . Filtering criteria "05db8426-a030-11e2-a9a5-5404a6dce322" and "7ea21f5b-5c75-11e1-8809-4061869766cd" and "DK" should be in the where clause. Otherwise relationships are effectively un-joined and become filtered cross joins, so indexes may not get used. I'm making some assumptions as you haven't shown your full schema, but something like this : SELECT a.id, a.title, c.name, b.city FROM ads AS a JOIN categories AS e ON e.id = a.categories_id JOIN locations AS b ON b.id = a.locations_id JOIN areas AS c ON c.id = b.area_id JOIN countries AS d ON d.countries_code = c.countries_code WHERE e.parent_id = "05db8426-a030-11e2-a9a5-5404a6dce322" and a.status = "active" and c.id = "7ea21f5b-5c75-11e1-8809-4061869766cd" and d.countries_code = "DK" LIMIT 0, 12; 
Here is a dump off the needed tables [dump](http://textdump.net/read/3098/)
Thanks allot man this works like a charm guess i should try read the documentation about joins again didn't know you shouldn't put values in the joins 
http://msftdbprodsamples.codeplex.com/
 select DATEADD(DD,-1,CAST(CONVERT(VARCHAR(10),GETDATE(), 101) AS DATETIME))
I'm not sure if "serious" is the right word. As I do take it very serious for situations where the benefits are obvious, say for example... it lessens the number of columns in the main table. I'm mostly wondering if there are times when it's better not to normalize, even if a column can be normalized. I just found [this article](http://databases.about.com/od/specificproducts/a/Should-I-Normalize-My-Database.htm), which seems to say "Yes, there are times when it's a good idea not to normalize."
This is the first I heard of Inmon and Kimball. Do you have a recommended link where I can read more about them? I'm still in the database-creation step, taking the guy's tab-delimited spreadsheet, and trying to figure out how to best represent it in a database. For the addresses, they're all quite-varied one string like "Associates in Radiology of CityName, CityName Medical Center 75 Main Street" or "17 Third St". With those two addresses, "17 Third St" only appears once, the first address appears maybe a dozen times. The "CityName Medical Center" appears maybe two dozen times, all with the same Main St address. I have no idea how I'd go about parsing that. I feel like strict normalization would have the address column split into officeName,addressName,and streetAddress tables. There's also some addresses that are shared except for suite numbers, with no names attached. Some have PO Box's listed. Some say "Suite 404" and others "Suite #404". I have no idea how I'd go about "properly" handling this. I may just use the whole string as a unique identifier and be ok with "Associates in Radiology of CityName, CityName Medical Center 75 Main Street" being unrelated to "CityName Medical Center 75 Main Street".
&gt; I don't know if I should have a single table hold all of the information about the request, or if I should have multiple smaller tables for each request type. both :) do a search on **supertype/subtype**
I already had a request table that stored basic information about the request (assigned appraiser, status, created date, etc.), but I was just unsure how to handle the actual request details since there's three types of requests. Like follier said, it was probably best to separate out each request type.
Agreed, the use case would determine the choice made. Good luck.
As I've been researching, I've been finding a lot of articles and discussion why you should denormalize your data - [Normalization isn't normal](http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html), [Normalization is for sissies](http://blogs.msdn.com/b/pathelland/archive/2007/07/23/normalization-is-for-sissies.aspx), [When not to normalize](http://www.reddit.com/r/programming/comments/2c89m/when_not_to_normalize_your_sql_database). I haven't had a chance to read them all yet, and I don't yet understand the intricacies of SQL enough yet to form my own opinion. I just feel the "Always normalize" rule that I hear a lot when asked feels a bit totalitarian, and my coming from an XML background, I see data differently and want to do what makes the most sense for me, the data storage, and the data usage.
If they don't want to set up replication, another option is to create an AFTER INSERT, UPDATE trigger.
I do it, as I think it makes it easier to read (for me anyway). 
Separating out each request type may be good, but suppose in the future you want to add more request types, or perhaps you want to collect more data, you might want to consider an EAV model. Of course, if you are going to do that, you'd want to question why you want a SQL database and consider using a document, object or Key-Value store. 
I work on a commercial app, we have doctor profiles in the app. I would recommend you stick the titles in their own table, and in the doctors table, reference a specific title. Those doctors who have multiple titles can simply have multiple titles in the titles table. For example: Titles: 1. DR 2. DDO 3. DR, DDO, OBGYN 4. OBGYN 5. DDS 6. DDS, OBGYN (that's not going to happen) Chances are, there are only a few who have those specialized multiple titles, so I feel it's okay to just put all the ones with multiple titles in their single mapping. 
I'd say do it the way that they want it to be done. It's a view, so if they want you to rewrite it for performance later, it can be done. That's the beauty of a view, you can change how it works behind the scenes and the application is none the wiser.
Do you see any advantage to this other than to maybe save a few bytes of data?
I couldn't get any TRIGGER statements to work across servers.
this should do it... UPDATE employee SET ExperienceLevel = CASE WHEN ExperienceLevel ='Junior' THEN 'Senior' WHEN ExperienceLevel ='Senior' THEN 'Junior' ELSE ExperienceLevel END now you have to decide whether you want to hand that in as your answer ;)
It is for school, but it isn't an assignment. I am trying to get ahead by going through the book and doing the exercises that aren't our assignments as well. Thank you. =)
Domain tables are nice for doing batch updates but no there is not a clear advantage in this case.
You have to group by OwnerType and not ExperienceLevel. I really dislike the way access creates its joins. 
Thanks for the advice. I should have stated that I tried that, and when I did the query would come up blank even though the where clause and the count requirement were met. Would you have any other suggestions? Thanks again =)
Did you change the Having count(experiencelevel) to OwnerType? Which table does the HoursWorked field come from? Maybe try starting over and building this one table at a time. First remove the group by Select OwnerType from Owner then add in the join to the next table. Eventually you'll stop getting records and hopefully be able to figure out where you're going wrong. Try figuring out how many types have more than 3 members. This is the best way to check your queries for accuracy. Just because your query returns data doesn't mean it's what you want.
Have you given any though to SQL Express?
where Senddate &amp;lt; cast (getdate() as Date) This should return results from everything that was from before today. Edit: Never mind, failed to realize that this would not work in SQL Server 2000
Hi, you can paste the SQL into www.snowflakejoins.com and see an approximation of the schema. -Alburnus
Leave it to MS to diverge from the standard in the most annoying possible way.
Thanks for the suggestion. Unfortunately with this being for school I cannot do that, however.
What you are describing is database normalization. There is a best way (currently known) and there are algorithms that will help you structure your data. Research '4th Normal Form' and Boyce-Codd Normal Form. It's a little difficult to wrap your head around at first but things like multi-valued dependencies are quite intuitive by nature.
Did you figure it out? When you say "exclude any type with less than three members", what do you mean by members? Employees? If that's the case, with the query above, if you had an OwnerType with only 1 employee who worked 5 services it would still show up. If you were working with "real" SQL you'd be able to say HAVING COUNT(DISTINCT EmployeeID) &gt; 2, but since you can't do that you want to first get it down to one row per combination of OwnerType/Employee, and then group *that* by OwnerType. Something like: SELECT OwnerType , Sum(EmpHoursWorked) AS TotalHoursWorked FROM (SELECT OwnerType , EmployeeID , Sum(HoursWorked) AS EmpHoursWorked FROM (OWNER INNER JOIN PROPERTY ON OWNER.OwnerID = PROPERTY.OwnerID) INNER JOIN (EMPLOYEE INNER JOIN SERVICE ON EMPLOYEE.EmployeeID = SERVICE.EmployeeID) ON PROPERTY.PropertyID = SERVICE.PropertyID WHERE EMPLOYEE.ExperienceLevel &lt;&gt; 'Junior' GROUP BY OwnerType, EmployeeID) GROUP BY OwnerType HAVING COUNT(EmployeeID) &gt; 2
Thanks so much for your help; I got it working. I realized that last night after I got frustrated that I changed OwnerType values back to what they were before, meaning I didn't have enough of each type for it to show. This worked perfectly.
&gt; Is there an online 'interpreter' that allows me to upload a DB and run queries on it http://www.sqlfiddle.com/
[Here's another...](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)
I would suggest avoiding MySQL entirely and going with Postgres. MySQL is awful comparatively.
..coming from Python . Why don't you try Sqlite ? It is already installed because of Python's "batteries included" philosophy, gives you standard SQL without the hassle of installing and administering a "real database". There is a standalone executable file for Windows / Linux / Mac if you don't want to work from Python, see http://www.sqlite.org/sqlite.html .
Just did a: select * from v$version And returned: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 Will I need to use XMLAGG like THLycanthrope said?
I get to practice at work with an online version, wouldn't be possible without.
yes you can, there is more overhead as you are unnecessarily going to and from XML but its a quick win if you do not want to implement the STRAGG custom ODCI.
A little off-topic, but I just got the [Teach Yourself SQL in 10 Minutes](http://amazon.rkstar.com/0672336073) book (where each chapter is supposed to be 10 minutes). I just started learning SQL and this $20 book is pretty handy.
You are missing a join clause. Presumably your orders table contains the customernumber so you would establish the join on that column: select cust.customername, cust.customernumber, ord.ordernumber from orders ord inner join customers cust on ord.customernumber = cust.customernumber where ord.status = 'Shipped' order by cust.customernumber 
Yeah, the book I got was Teach Yourself SQL in 24 hours. It's really good so far, except for the fact that it may be outdated; I have the 2003, 3rd edition).
Thanks for the heads up on XMLAGG, it took some playing with as you have to do some wonky replace and substr functions to get it looking correct. But, at the end of the day it is working! Thanks again for everyone's help!
Exactly what I was looking for. Thanks!
This is a good solution, ~~but it's generally bad practice to alias tables unless necessary (ex. if you have more than one copy of the same table)~~ edit: this may be more a matter of style.
It's an inline 'group by' applied to a single field instead of the entire select list. Easiest way I can think to explain it. It's useful if you want to get aggregates grouped by different criteria within the same select instead of needing multiple selects. For example, a running total and a query total each as separate columns in the same select.
Doesn't have to be a single field. You're allowed to do a group of columns which, by their combination, is the "composite" you're looking for. I use the heck out of it with RowNumber and Rank.
Guess I was unclear on that. You can use the over window function on any number of fields in a select.
so there are multiple records with the same id but different first_name values?
If you just want the name then this will work. Note that if there is a tie the lower alphabetical name is selected. If you want them both then you would "rank() over(partition by name_cnt order by first_name) rn" instead of row_number. This smells like homework, can't think or a practical use for this query other than teaching. with test_data(id, first_name) as ( select 1, 'Blossom' from dual union all select 2, 'Blossom' from dual union all select 3, 'Bubbles' from dual union all select 4, 'Buttercup' from dual union all select 5, 'Dexter' from dual union all select 6, 'Deedee' from dual union all select 7, 'Deedee' from dual ) select first_name from( select first_name, row_number() over(order by name_cnt desc, first_name) rn from( select first_name, count(*) name_cnt from test_data group by first_name ) ) where rn = 1;
i see. 
[try these options out](http://www.makeuseof.com/answers/open-db-file/)
What DB are you moving to? How are you trying to export? SQL Server could do a Linked Server. Then you can write queries like "insert into newtable (columns) select columns from SQLAny...oldtable" There are also several Google hits for tools and techniques to go from Sybase to another DB.
our new program has a database in our SQL 2k8 instance.
whoisearth posted a thread of suggestions. I think opening the .db with Excel, then importing the Excel files into SQL Server might be more efficient. Just be sure to double check any column datatype changes.
XMLAGG will have much higher overheads than LISTAGG and be considerably slower.
Utter crap. It's a good idea to alias tables, it's more readable and in fact some RDBMS optimizers performs slightly better if you do.
For future reference, you should post what "flavor" of SQL you're using. If you're using SQL Server, look at the datepart or datediff functions. If you're using MySQL, take a look at the year function.
"SQL Server" is just as vague imho. You're referring to MS SQL I presume.
SQL Server is literally the product name of Microsoft SQL Server. I do not believe that referring to it as such is vague.
No, it's not. It's a specific product name.
MySQL is still SQL. It has a server. Calling a MySQL Server a "SQL server" is not incorrect. When you're hosting both varieties and working on projects with both you tend to specify if it is Microsoft or "My". Microsoft didn't create the first "SQL Server" so trying to take claim of that name, albeit clever, is a dick move. 
Yes, literally a product name. But Microsoft doesn't own all things SQL. Until they do, their SQL server is just another SQL server. I suppose you work exclusively with MS SQL?
Good stuff man. Dont forget to include the poor use of large data objects (blobs). I had a developer who wrote a webpage that used Select * from mytable where [recurring] is null Recurring is a nvarchar(max). This was fine in small testing but a client had 90 million rows (30 gb) "mytable". Needless to say, it blew up. Avoid blobs unless you are using them for their intended purpose. 
having a table that marks when recurring is not null would have been faster than that, nice.
Hanging out on various technical forums, groups and mailing lists for almost a decade, I feel obligated to point out one more thing for future reference: When you ask for help in homework, you should show other users that you have made an effort to solve it by yourself. Post code you have tried and show at least basic understanding of task that has been assigned to you. You should ask for guidance, not full-baked answer. It is always good idea to read (and preferably learn by heart) "[How To Ask Questions The Smart Way](http://www.catb.org/esr/faqs/smart-questions.html)" article written by esr. Also, one guy that I know happen to teach computer science at university. The other day, he said something like that: "I would feel personally offended if I find out that my students ask for answers to their homework at random websites instead of visiting me at my office hours".
http://en.wikipedia.org/wiki/SQL_Server
Sorry, but I think you're being rather pedantic about this!!! I assure you that it professionals know what SQL server is without having to prefix it with 'microsoft' :)
Thanks for the update. I would ask my instructor, but I am pretty sure he is learning SQL with us. He is pretty bad. Anyways thanks for the info again.
Thank you! I had everything right except the DATEDIFF
Or just parenthesise for clarity [...] AND (g.grade LIKE 'A' OR g.grade LIKE 'B') It's worth noting that I you might mean to have a '%' on the end of 'A' and 'B' [ie, 'A%' and 'B%'], since you're using LIKE I assume you're trying to match 'A+', 'A-' as well as 'A'?
Thank you for taking the time to submit this. It helped greatly. I tried what felt like everything except for the parenthesis.
Hey just to help you understand this a bit more, because the way he's doing it is more 'proper' it might help you to understand it like this WHERE s.stno = g.student_number AND g.section_id = se.section_id AND se.course_num LIKE 'ACCT%' AND g.grade LIKE 'A' OR s.stno = g.student_number AND g.section_id = se.section_id AND se.course_num LIKE 'ACCT%' AND g.grade LIKE 'A' and g.grade LIKE 'B'
been using SQL server for 4+ years, TIL i am an idiot.
 1 SELECT COUNT(*) 2 FROM stu s, gradRep g, sect se 3 WHERE s.stno = g.student_number --Joining stu and gradRep tables using student number 4 AND g.section_id = se.section_id --Joining gradRep and sect tables using section_id 5 AND se.course_num LIKE 'ACCT%' 6 AND (g.grade LIKE 'A%' OR g.grade LIKE 'B%') You want to evaluate your grade predicate as a single item, so parenthesise, otherwise the whole set of predicates are evaluated and OR'd with the final g.grade LIKE 'B'... due to operator precedence. Also, when using LIKE, you generally need to use wildcards (no wildcards means exact match). **Wildcards** **%** = any combination of zero or more characters **_** = any single character. Note, if you are searching for something that contains a percent or an underscore in text, you need to escape the wildcard. select * from MY_TABLE where COLUMN like 'This budget contains a%\%%' escape '\' Would return something like... 'This budget contains a 50% pay increase'
Yeah, OP is using ANSI 89 join syntax, whereas ANSI 92 onwards has pretty much taken over and 92 is certainly what he should be trying to use going forward. There are a few cases where you still need to use 89 (normally relating to scoping, casting collections into tables for use in SQL and joining to a table within the scope), but this is outside of most people's general usage.
I believe it works as long as the field is EXACTLY the same.
I wouldn't use autoshrink ever, causes more trouble than it solves most of the time. This thread has some interesting tips for this kind of problem: http://stackoverflow.com/questions/251480/how-to-shrink-a-sql-server-log-file-with-mirroring-enabled From what I can gather the auto shrink doesn't mirror as you might assume. Then again I'm not really very good with file handling scenarios so don't take my word for anything :)
If you have a lot of databases you need to approach this with a different mindset. Whatever solution you end up implementing you must make sure it scales. Setting standards and enforcing them will save you from turning into a fire fighter at your job. In your case you probably don't have timely log backup jobs running. I set all my servers to dump log backups every 15 minutes. This will ensure they don't fill up past their _planned_ capacity. In this case I would disable auto-grow and make sure your DR pairs are built 1:1 spec wise and work with database owners to find that sweet spot between log size and backup interval so that you have to spend zero time thinking about this again and concentrate on value-adding work. 
I'm not clear on what you're trying to do. the subqueries, SELECT id FROM tableb WHERE id = 1, just return a 1 (assuming there is tabled record with id=1), so what it really comes down to is SELECT col1, col2 FROM tablea WHERE col1 = 1 OR col2 = 1 (again, assuming there is tabled record with id=1). Could you provide some more information?
The value 1 is a place holder for ? in a preparedstatement query in java. Basically table a is a list of two way paths between the nodes in table b.
Nice! I wasn't sure if I could use an JOIN on an OR and still use it on the WHERE to select the id. The new version looks like SELECT col1, col2 FROM tablea JOIN tableb ON col1 = tableb.id OR col2 = tableb.id WHERE tableb.id = ?;
After reading the original query again, you're correct. More conditions are needed. It's however still ok to include the extra condition in the ON condition, like ON (a.col1 = b.id OR a.col2 = b.id) AND b.id = 1.
Basically, his query comes down to this pseudocode: IF ? in tableb.id: SELECT * from tablea WHERE col1 = ? OR col2 = ? ELSE NULL SET; The fastest way to do this then is: SELECT col1, col2 FROM tablea WHERE (col1 = ? OR col2 = ?) AND EXISTS(SELECT 1 FROM tableb WHERE id = ?) No joins, one filter in the where clause. Nice, fast, simple, and clear.
He doesn't need a join. He just needs to make sure there is a tableb.id=1 and then filter tablea directly. SELECT col1, col2 FROM tablea WHERE (col1 = ? OR col2 = ?) AND EXISTS(SELECT 1 FROM tableb WHERE id = ?) will do the job nicely.
I should have mentioned theres two tables, Im getting stuff back but its not accurate &gt;SELECT FName, LName &gt; &gt; FROM People, ID &gt; &gt; WHERE People_num = ID_num &gt; &gt; GROUP BY FNAME, LName HAVING COUNT(ID)=1;
a person can have multiple IDs and multiple people can have the same ID, but I want to only select people who only have 1 ID
 SELECT FName, LName, count(*) FROM People JOIN ID ON ID_Num = People_num GROUP BY FName, LName HAVING count(*) = 1 
Turns out that the join is less expensive on PostgreSQL. With schema like this: &gt; create table tablea (id serial primary key, col1 int, col2 int); &gt; create table tableb (id serial primary key); &gt; create index test_idx on tablea (col1, col2); And data like this: &gt; insert into tablea (col1, col2) values (1, 2), (2, 3), (3, 4), (4, 5); &gt; insert into tableb (id) values (1), (2), (3); The queries explain like follows: explain select a.col1, a.col2 from tablea a join tableb b on (a.col1 = b.id or a.col2 = b.id) and b.id = 2; &gt; QUERY PLAN &gt; --------------------------------------------------------------------------------------- &gt; Nested Loop (**cost=0.16..9.27** rows=1 width=8) &gt; Join Filter: ((a.col1 = b.id) OR (a.col2 = b.id)) &gt; -&gt; Index Only Scan using tableb_pkey on tableb b (cost=0.16..8.17 rows=1 width=4) &gt; Index Cond: (id = 2) &gt; -&gt; Seq Scan on tablea a (cost=0.00..1.04 rows=4 width=8) &gt; (5 rows) explain select a.col1, a.col2 from tablea a where (a.col1 = 2 or a.col2 = 2) and exists (select 1 from tableb where id = 2); &gt; QUERY PLAN &gt; --------------------------------------------------------------------------------------- &gt; Result (**cost=8.17..9.23** rows=2 width=8) &gt; One-Time Filter: $0 &gt; InitPlan 1 (returns $0) &gt; -&gt; Index Only Scan using tableb_pkey on tableb (cost=0.16..8.17 rows=1 width=0) &gt; Index Cond: (id = 2) &gt; -&gt; Seq Scan on tablea a (cost=0.00..1.06 rows=2 width=8) &gt; Filter: ((col1 = 2) OR (col2 = 2)) &gt; (7 rows) Edit: Formatting.
 SELECT a.name FROM people AS a JOIN ids AS b ON a.id = b.id GROUP BY a.name HAVING count(a.id) = 1;
Thanks!
In MS SQL you can do this: having count(distinct [ID])=1
yes, I agree with the table aliases, but, for the example I was trading off readability for brevity. 
Thanks guys, I have configured my server to back up transactional logs every 20 minutes. I will see if it helps. Thanks a lot.
Ah, I was assuming Postgres reporting the costs as best/worst case scenarios instead of that start-up cost vs. total cost. The number of rows is indeed reported wrong, while the actual result still yields the correct amount of rows. using EXPLAIN ANALYSE gets better output: test1=# explain analyse select a.col1, a.col2 from tablea a where (a.col1 = 2 or a.col2 = 2) and exists (select 1 from tableb where id = 2); QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Result (cost=8.17..9.23 rows=2 width=8) (actual time=0.026..0.028 rows=2 loops=1) One-Time Filter: $0 InitPlan 1 (returns $0) -&gt; Index Only Scan using tableb_pkey on tableb (cost=0.16..8.17 rows=1 width=0) (actual time=0.016..0.016 rows=1 loops=1) Index Cond: (id = 2) Heap Fetches: 1 -&gt; Seq Scan on tablea a (cost=0.00..1.06 rows=2 width=8) (actual time=0.007..0.009 rows=2 loops=1) Filter: ((col1 = 2) OR (col2 = 2)) Rows Removed by Filter: 2 Total runtime: 0.073 ms (10 rows) test1=# explain analyse select a.col1, a.col2 from tablea a join tableb b on (a.col1 = b.id or a.col2 = b.id) and b.id = 2; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Nested Loop (cost=0.16..9.27 rows=1 width=8) (actual time=0.030..0.033 rows=2 loops=1) Join Filter: ((a.col1 = b.id) OR (a.col2 = b.id)) Rows Removed by Join Filter: 2 -&gt; Index Only Scan using tableb_pkey on tableb b (cost=0.16..8.17 rows=1 width=4) (actual time=0.016..0.017 rows=1 loops=1) Index Cond: (id = 2) Heap Fetches: 1 -&gt; Seq Scan on tablea a (cost=0.00..1.04 rows=4 width=8) (actual time=0.003..0.003 rows=4 loops=1) Total runtime: 0.080 ms (8 rows) 
The unsurprising answer is: it depends. SAS can be used simply as a mechanism to access various DBMS (and therefore SQL), but can also be used for a ton of other tasks that could require more knowledge of how SAS works. I don't think it is too difficult to learn base SAS if you have any functional knowledge of data. And there is a lot of documentation available - I am always looking up various bit of syntax that I have forgotten since I don't use SAS frequently enough to have everything I need committed to memory.
I went the other way (SAS-&gt;SQL)and found the transition wasn't too painful. Most data manipulation tasks I do in SAS can be done either via SQL commands or DATA/PROC steps. Once you get comfortable with the base SAS you start to develop preferences. The more hardcore statistical functions in SAS are actually pretty simple from a coding standpoint - the real trick there is properly structuring the data for analysis and interpreting the output of the SAS procedure.
 &gt;I don't think it is too difficult to learn base SAS if you have any functional knowledge of data. And there is a lot of documentation available - I am always looking up various bit of syntax that I have forgotten since I don't use SAS frequently enough to have everything I need committed to memory. I do this with everything on the basis you may find others have found better ways to do stuff 
Also there's this thing called spool that i don't get it.. Do i type it before i start my script on the SQL?
I have to make a table called bookstore and 3 more tables within it or something, called publisher, book, chapter. It has something to do with constraint.. and i have to send him this sum with a spool, which i don't know what it is ..
you need to use spool &lt;name&gt; before, and spool off after your query has run. 
have you tried FIELDS ESCAPED BY '\' ?
Can you use hex row terminators? 0x0A I believe is \n 0x0D I believe is \r
I don't think anyone is following them. Concise, clear, comprehensible submissions get answers.
Try putting the IIf in the sum: =Sum(IIF(Fields!status.Value ="Active", Fields!payoff.Value, 0))
I got it solved. I think I tried the way you suggested earlier on but didn't get it to work. What's the difference between having the Sum first and IIF first?
It has to do with the way aggregate functions work. Sum() evaluates the expression passed to it, for each row in the dataset. So Sum(Fields!payoff.Value) will always return the sum of payoff.Value taken over the *whole* data set. By putting the IIf condition inside the Sum(), you're saying, "Give me the sum of the values of this function call for each row in the dataset. When the current row's status.value = 'Active', then the IIf function returns the value of payoff. Otherwise it returns 0. The way you had it in the example, you were actually saying, "If status = 'Active' in the current row, then return the Sum of the whole data set; otherwise, return 0." 
How would that change it on a row by row basis? Doesn't it calculate independently of other rows? 
Sum and other aggregate functions do, but when you reference a data field inside a non-aggregate function like IIf, it returns the value of the field in the current row. 
I had to add fields of different rows, so I'm not sure if it would work inline.
Yeah, sometimes it's just not worth messing with if you have another solution. 
No worries. Thanks for the help! 
I'd say SQL is slightly easier just because its so structured. SAS has a LOT of different procedures that follow similar syntactical patterns, but I consistently have to look back at old code or the web to remember the details.
Personally, I would never delete multiple records based on some non enforceable table level or column constraint (business rule), triggered on an update. It's wrong on so many levels : non transparent, confusing, is fraught with risks, not to mention goes against ACID (Atomicity, Consistency, Isolation, Durability) rules and goes against general DB DML so is extremely bad practise. You've issued an update and you lose two records? Think about it. I would enforce my business rule on the data without touching it, by using a view. create table MY_TABLE ( ID INTEGER, PARENT_ID INTEGER, LETTER CHAR(1), constraint PK_MY_TABLE primary key (ID), foreign key (PARENT_ID) references MY_TABLE (ID) ); insert into MY_TABLE(ID, PARENT_ID, LETTER) values (1, null, 'E'); insert into MY_TABLE(ID, PARENT_ID, LETTER) values (2, 1, 'D'); insert into MY_TABLE(ID, PARENT_ID, LETTER) values (3, 2, 'C'); insert into MY_TABLE(ID, PARENT_ID, LETTER) values (4, 3, 'B'); create or replace view V_MY_TABLE_RULE as select t.ID, t.PARENT_ID, t.LETTER, t2.LETTER as PARENT_LETTER, case when t.LETTER &lt; t2.LETTER or t2.LETTER is null then 'Y' else 'N' end as VALID from MY_TABLE t left outer join MY_TABLE t2 on t2.ID = t.PARENT_ID / select ID, PARENT_ID, LETTER, PARENT_LETTER from V_MY_TABLE_RULE where VALID = 'Y' / /* ID PARENT_ID LETTER PARENT_LETTER ---------- ---------- ------ ------------- 2 1 D E 3 2 C D 4 3 B C 1 E */ The view is key preserved (key preserved means the row from the base table will appear AT MOST ONCE in the output view on that table), so will allow DML statements like update and delete on it. update V_MY_TABLE_RULE set LETTER = 'A' where ID = 2 / select ID, PARENT_ID, LETTER, PARENT_LETTER from V_MY_TABLE_RULE where VALID = 'Y' / /* ID PARENT_ID LETTER PARENT_LETTER ---------- ---------- ------ ------------- 2 1 A E 4 3 B C 1 E */ If you wish to delete invalid rows you can do this also, but you'd need to do it starting from the invalid parent to prevent "child record found" integrity issues, simple enough anyway.
I agree with /u/ziptime. Don't use triggers to delete records. It's just asking for huge problems down the line. One thing I would do if your RDBMS supports it (as it really should; I'm looking at you MySQL) is using a CHECK constraint to reject any update that violates your condition.
Oracle supports field and table (record level) check constraints, but the OP's check requirement was dependent on both child and parent record information (i.e. across multiple records). As such, this is a business rule and as far as I know not generally supported by RDBMSs.
&gt; My teacher is pretty bad at explaining... That in particular, made me laugh.
I do think you need a subselect to find the latest date with each probe. The a case statement based on the difference between now and the timestamp. If it's less than 30 seconds show the value, if not 0. Something like this: SELECT b.probe, CASE WHEN DateDiff(second, timestamp, now()) &lt;= 30 THEN value else 0 end as value FROM BuildingDraft b INNER JOIN (SELECT probe, Max(timestamp) as ts FROM BuildingDraft GROUP BY probe) subB ON subB.probe = b.probe AND subB.ts = b.timestamp ORDER BY b.probe I think this should work. I wasn't able to actually try it, but this is the path I would go down. 
You could do something like this: SELECT *--make it look right here FROM (SELECT rownum = ROW_NUMBER() OVER ( PARTITION BY probe ORDER BY timestamp), a.probe AS pid, a.timestamp AS ts, a.data as runtime FROM probe_data AS a WHERE DATEADD(ss, -30, GETDATE()) &lt; timestamp) AS t GROUP BY t.pid HAVING rownum = 1
I think the only problem with this is it wouldn't return all probes. But, I like the use of ROW_NUMBER.
If all the probes are required you can just add a CASE to null the timestamp instead of the WHERE DATEADD to put a null if the date is too old.
Try shaving the time off the date to better catagorize your date range. Select DiscDate, DiscClosedDate, CAST(DiscDate as DATE) as DateRange FROM YourTable (example: Select GETDATE() AS [DATETIME], CAST(GETDATE() AS DATE) AS DATE) This will truncate your DiscDate to days and you should be able to better display the counts. Edit: I assume stuff is plotted over datetime, if its just Date then this won't work and you should be using a bar chart. Hope this helps
 Data flow and how your databases work in your company is usually company based. There are best practices for how data should be modeled and used and so on but its all about what you need to do with it and how much freedom you have for changes and so on. If your really needing to understand all of it, do the SQL Server courses from Microsoft at a training centre (I forget the course ID's) its basically SQL Server Implementation, T-SQL (Transact), and Administration (DBA) They are obviously focused on SQL Server but they are a lot more beginner friendly than going to the Oracle equivalent and you will get a good over viewing knowledge of how DB's function and all their super magic trickery! or just ask your DBA's wtf they are on about and explain it in begginer terms for you.
They gotta eat. Where would you prefer that they get money from, corporations?
PROC SQL is like a really, really bad SQL tool. The basics are the same, but some things throw me because they are... well just SAS. The good news is, however, that PROC SQL is the PROC used the most to do your data transformations. If you have a grasp of SQL, this is easily translated to SAS. For outputting large amounts of data in a pretty format SAS is OK. For getting end users the data they want without you having to worry about it, SAS is OK. I would suggest getting some SAS guys to show you their work, and breaking it down to the various PROCs. The nice thing about Enterprise Guide is that it generates the code for you. I have produced some passable SAS code, mostly by using EG and then editing the code to fit my needs. SAS Macros are a pain in the bum-bum, but once you get your head around the code, it can be quite useful. You can throw a SAS Macro variable to a PROC SQL, and have a nice little bit of Object Oriented code interacting with SQL, and get to where you need to be in a crafty kind of way. www.lexjansen.com is a really nice source for all things SAS. Any SAS Forum, training, whatever info is there, and indexed rather nicely. I use lexjansen more often than the SAS site for finding answers to my coding problems. The SAS-L Google group is also great, but since this used to be an USENET mail group, it is populated with old-school SAS programmers that like to do things in the 1970's way. Great source of information, they just are a little too 733t about their code. If your code is just waaay too buggy, you can always submit it to the SAS help desk
you're making it harder than it really is... SELECT Col1 , Col2 , Col3 , Col4 , Col5 , Col6 FROM [Table] GROUP BY Col1 , Col2 , Col3 , Col4 , Col5 , Col6 HAVING COUNT(*) &gt; 1 
I'd rather have it there than a pop up every time I open the program.
Have you considered creating a unique index on the table? 
How will you handle the PK dupes once they are found? Is a PK dupe valid in the source tables? 
For now I'm using SQLite. When I didn't see it as a feature of SQLite, I figured it must just only be in the non-lite DBMS's, but I couldn't find it there either. Since I'm just learning, I'm trying to get a feel for what the capabilities/differences of each DBMS are, but I'm focusing on SQLite. Which ones would let me do it, and how would it be done?
I'm not using MS SQL, but I am interested in learning the capabilities of various DBs. That article does imply that "computed columns" does close to what I'm looking for, but doesn't really go into detail of how to define a computed column and how to use it. I assume you'd just use it like a normal column, but I'm finding a lot of SQL to not be intuitive so I try not to assume anything.
Something like this: CREATE TABLE Orders( [Price] money NULL, [Quantity] int NULL, [Total] AS [Price] * [Quantity] ) 
sql server computed columns like this -- CREATE TABLE orders ( id INTEGER IDENTITY NOT NULL , CONSTRAINT orders_id PRIMARY KEY CLUSTERED ( id ) , cust_id INTEGER NOT NULL , price DECIMAL(9,2) NOT NULL , qty SMALLINT NOT NULL , total AS price * qty ) me, i prefer using a **view** 
And which SQL variants does that work with? Is there a way that works with SQLite?
Cool thanks. Do you know which SQL variants allow computed columns? Do they all support views? This is the first I'm hearing about views, so I'm reading about them now. What would you say are the pros/cons to using views vs computed columns?
From what I gather, SQLite does not support computed columns. However, you can create a view that will do what you're looking for. 
yes, all databases support views, even sqlite pros: views are standard sql cons: can't think of any
You can do this with a view in just about any RDBMS.
I agree with the view route. Wouldn't it be better to have the computed column in a view just in case the business rules change? I.e. Maybe the total column needs to include tax
What are your keys and relationship between the salesperson, customer, and contact tables?
Performance is a con when you compare standard view performance to a computed column in the table. Views based on large numbers of rows will almost always underperform a computed column. Materialized views are different.
SELECT a.Nickname, a.LastName, a.FirstName, b.CustomerID, b.LastName, b.FirstName, b.Date, b.Type, b.Remarks from salesperson a inner join customer b on (a.lastname = b.lastname) and (a.firstname = b.firstname) --It is usually a bad habit to join on non-unique keys, but it seems it's what you are looking for. Also, repeating columns with same values is mehhhhh. I left the join as inner, but you might be looking for an outer or full join. Also, what SQL flavor?
SALESPERSON (**NickName**, LastName, FirstName, HireDate, WageRate, CommissionRate, Phone, Email) CUSTOMER (**CustomerID**, LastName, FirstName, Address, City, State, ZIP, Phone, Fax, Email, NickName) CONTACT (**ContactID**, CustomerID, Date, Type, Remarks) The referential integrity constraints are: NickName in CUSTOMER must exist in NickName in SALESPERSON CustomerID in CONTACT must exist in CustomerID in CUSTOMER 
Fix my dodgy queries... Or bodge around some dodgy source data.
So you're saying once a computed column is made, it can't be changed?
I guess I was asking how much harder it would be to make the computed column include tax, vs making a view include tax. It sounds like the difficulty between the two is about the same.
How do [src_ProjectedFromRate] and [project_AgainstRate] change over time? This matters, please let us know. Thanks.
You can grant exec on individual stored procedures (rather than say using a blanket database role). 
hey v0rl0n, Currently, they don't change over time. It's just for the initial data set to work with I had these columns to keep track of which scenario the data was generated from. Originally, I took all information from 2014, and projected against n+1 rates, which gave me my final output in 2014(src_ProjectedFromRate). Then I wanted to show how the figures would change from 2015-2018 for (n+1) project_AgainstRates. Final product for the user would be something like, two sets of data being able to be manipulated/viewed: 1.) 2014 at various rates 2.)2015-2018 at various rates (based on what 2014 was projected at) Please let me know if this added clarity. Thanks!
Hey evilalive, Firstly, thanks so much for the reply. I just discovered sqlfiddle, I'm going to try to give a better example of what I did so far when I get home and look over in detail what you replied with. 
TLDR; jump to the TSQL grant syntax example towards the end. EDITS for clarity on 10/13/14 am. You're not creating user stored procs in the sys schema are you? Surely not. If you're on 2008+ and your database isn't set to 80 compatibility mode just grant execute on the dbo schema. But I'm thinking your real problem here is your are looking to those antiquated fixed database roles and thinking they're the only option to manage user security. Since 2005 there's been a much better way; schemas. The benefit of granting on a schema level is it covers execute on all procedures currently in the schema AND any that may be added latter on. If a procedure is dropped and recreated instead of altered when updated schema level grants mean the User entity will retain execute. Disclaimer; sometimes you may want users to only have execute on a limited number of procedures and not all current and future ones as well. In that case direct object grants would be better. From the wording of your post schema level grants will work nicely. Just the same, if you may want to discuss this with the application's systems and programming manager if unsure. Back to the how to. For a novice the fastest way to do this is open SSMS as a member of sysadmin ("sa" is such an account) &gt; drill down on the database name in the object explorer pane ( it's on the left side of the SSMS window by default) &gt; in the databse's sub-folders find the security folder and under that look for the schema folder. Click to open schemas and once open find scroll through the schemas to find the dbo (or whichever one holds your procedures) schema and right-click. In the menu that appears find properties and left click to open the properties dialog box. In the properties for the schema you will see the permissions option on the left side. Click to activate. This will open a list of entities (users, group, roles) in the top pane and permissions on the bottom right pane. In the top pane click and highlight your user, windows group or role. If you don't see then add them first, click to highlight the account. Go to the permissions pane, scroll to find the execute permission and check the box for GRANT. Now you are done. You can press OK to complete the grant, or better yet press the "Script to" button in the upper left corner to output the TSQL commands to a new query window and THEN press OK to apply them. If you take this extra step now, the next time you need to grant execute on a schema this you can just run the script. Of course you can GRANT EXECUTE on the dbo schema instantly by typing this in Query Analyzer while logged in as a member of the sysadmin fixed server role: GRANT EXECUTE ON SCHEMA::dbo TO &lt;entity&gt; ; (Where &amp;amp;lt;entity&gt; is the user, group or role MINUS the &lt;&gt; brackets) Speaking of roles. Unless you want to repeat these steps for each user you should really create a "user defined role", add all your users to it and grant IT execute. Google or search BOL for "user defined role" to see how. Basically the same drill down steps but instead of schemas right click on roles to be presented with an option to create a new one.) Hope this helps. I apologize in advance for any readability issues. I wrote this all from memory on my phone. 
For those that don't know about it: http://www.sqlpass.org/summit/2013/Home.aspx
What is the size of the data in the datafiles (cumalative size if multiple datafiles) that these logfiles belong too? It could be time to add more SAN/HDD storage to the volumes housing those datafiles. The following is a general rule of thumb (a "you got to start SOMEWHERE" rule so to speak), your results may vary and there are plenty of exceptions to this rule amongst the 600+ instances I manage, but here it is. At least initially I size my transaction logfiles at 20% of the cumulative size of their database datafiles. So if your datafiles are 5 GB your logfile (singular) should be 1 GB. But that rule is really for initial sizing or resizing after a "one time event" (dataload for example) has autoextended your logfile beyond its normal size by a factor of 3 or more. In such cases where you feel the space consumed by the logs will be wasted going forward, yes shrink the logfile (ONLY the logfile). Should you find the logfiles autogrew along with the datafiles and are currently around 20%-30% of their size leave the logs alone and get more space on their volume. If they grew to that size through the normal course of business you should know FIND OUT why. Was it an application job performing Cartesian Joins or your index rebuild job. If the former work with your developers to fix the issue and if the latter look into more intelligent index maintenance (hint; an index reorg, if followed by a stats update, will suffice the majority of the time). UNTIL YOU find out why and fix it this is just how big they have to be. But how to tell when it grew? The default SSMS "Disk Usage" report for the database is the easiest way. In the middle there's the autogrow events drilldown. Try to match those times to your own scheduled jobs. If they match then look into what those jobs do and how improve or fix them. If not then take the times to the app team ask them what they are doing. 
The two other DBAs where I work are going, but as the recently hired Jr DBA I'll have to wait until next year.
My boss is there, and a CoWorker is there presenting 2 sessions. [1](http://www.sqlpass.org/summit/2013/Sessions/SessionDetails.aspx?sid=5118), [2](http://www.sqlpass.org/summit/2013/Sessions/SessionDetails.aspx?sid=4616) I'm a contractor, and paying for conferences out of your own pocket gets quite expensive.
I will be there! This is my 2nd time, Seattle's was AWESOME! Highly recommended.
This is my first time going. My coworker has been a couple of times before, he's the one that recommended it to me. 
Never heard about it until now. It seems like it's worth the money. I have no gripes about asking for a donation.
my company still uses 2008 ... ice age here
Drop the 2012 requirement and you shouldn't have too much trouble.
Agreed, that combo shouldn't be that hard to find. Just don't toss someone out for not specifically having 2012.
You want this: select s.NickName, s.LastName as SalesLastName, s.FirstName as SalesFirstName, cu.CustomerID, cu.LastName, cu.FirstName, co.Date, co.Type, co.Remarks from Contact co inner join Customer cu on co.CustomerID = cu.CustomerID inner join Salesperson s on cu.NickName = s.NickName You can join multiple tables together. Use `join` syntax, rather than comma-delimited tables. It's just good practice. Also, why in the hell are you using a *nickname* as a key? There should be a `SalesPersonID` field that behaves as your primary key for the `SalesPerson` table. Anyway, hope that helps.
I work for a large consulting firm in their analytics group. As was said before, drop the 2012 and you should be fine. Further, recruit from large companies as they usually have the $$ to upgrade to the current versions the fastest. Also, you'll have someone who's seen it used in several different capacities. Hope this helps. Good luck!!
Substring to split it and then case. 
Yes, nothing is easy.
pfft, we've been going from 2k5 to 2k8r2 just in the past 6 months
Hi, Probably you have data with different formats in two different tables, and intend to join on common values. How about dropping extra zeros from "desired data format" ex "1.02.09", storing it in a mapping table with two columns ("1.02.09", "1.2.9"), and mapping your "incorrect data" with this? In case there are multiple reverse mappings, take the longest . Ex "1.2.9" matches "1.2.09" and "1.02.09" and "01.02.09" This solution is not only easier to implement, but probably will discover the undocumented naming rules that you wanted to implement as a formula in the first place. -Alexandru Toth www.snowflakejoins.com 
That's a good approach, I think I can make this work, thank you.
Make ID, ID_A and ID_B "not null" columns. Make your referential integrity "on delete cascade". Not null ensures a value always exists in those fields which, coupled with your existing referential integrity, means ID_A and ID_B must be valid values. On delete cascade means deleting a parent record from A or B deletes the relation. **Several points :** I don't think you need the extra ID column on relations, I think a PK of (ID_A, ID_B) should suffice. Try to give your tables meaningful names! As for ensuring all records in A and B are used, this is a business rule. You can only achieve this with code such as a trigger, or better still, create a view to show all combinations, not sure why you need to have a table. create or replace view RELATIONS as select A.ID_A, B.ID_B from A cross join B **Missing Data:** If you do want a relations table, find missing data based on a relations PK of (ID_A, ID_B) select A.ID_A, B.ID_B from A cross join B minus select ID_A, ID_B from RELATIONS 
Show us how you need the results presented or formatted. I'm thinking a UDF might be your best bet. You could do it with straight T-SQL, possibly using a CTE, but my guess is that it would be an ungodly thing readability-wise.
Multiple ways of doing this; frankly most of them would be better implemented in code. Easiest way if you understand recursion is to create a stored procedure that takes a string and a replacement word finds the first instance of both '(' and ')' using charindex. Replace them, then pass the remaining string into the same stored procedure. If charindex returns -1 return just the string.
string1 string2 string3 would be an alright way to display these. I don't know what a UDF is but the practice problem just says use T-SQL
I may have been thinking too big-picture. Are you wanting this to be part of a SELECT statement (where the sentence is a column in a table), or do you just need T-SQL code to find the () strings from a given single sentence? If the latter, then as Coldchaos pointed out it's just a matter of using recursion OR a WHILE loop. 
/u/alexandru_toth is right, as far as using it for joining purposes. Otherwise, if you need to format the data the way you originally wanted to (for presentation purposes, or so they will sort in the correct order), you could do something like create a table-valued function to split the string on the "."s, format all the strings to 2 digits, and then use the XML method to put them back together into one row, [like so](http://sqlfiddle.com/#!3/c2535/2/0)...
I don't understand what you mean by replacing the words?
I'm there. Just checked in and got my newbie kit. 
Replace the () with [] or something. Then run the same procedure again looking for (). Since [] is not (), the procedure will skip it and find the second ().
UDF: User Defined Function I think.
Something like this (not fully tested).... CREATE FUNCTION dbo.tokenizestring ( @stringToSplit VARCHAR(MAX) ) RETURNS @returnList TABLE ([Name] [NVARCHAR] (500)) AS BEGIN DECLARE @name NVARCHAR(500) DECLARE @pos INT DECLARE @endpos INT WHILE LEN(@stringToSplit) &gt; 0 BEGIN SELECT @pos = CHARINDEX('(', @stringToSplit) SELECT @endpos = CHARINDEX(')', @stringToSplit) if @pos &lt; @endpos and @pos &gt; 0 BEGIN SELECT @name = SUBSTRING(@stringToSplit, @pos + 1, @endpos - @pos - 1) INSERT INTO @returnList SELECT @name END if @endpos &lt; @pos -- In case single ( or ) SELECT @endpos = @pos SELECT @stringToSplit = SUBSTRING(@stringToSplit, ISNULL(NULLIF(@endpos, 0), LEN(@stringToSplit))+1, LEN(@stringToSplit)-@endpos) END RETURN END Then.... select * from dbo.tokenizestring('I need to go to the (string1) and then (string2) afterwards. I can give you a (string3) if you help me.') | NAME | |---------| | string1 | | string2 | | string3 |
oh ok. So this will give me indexes for each bracket correct? Could you write out the shell of what a while loop with replacement looks like? I'm not looking for the answer i just don't know the format 
Just wanted to try an approach without loops. Doesn't quite work with nested parens, but I don't think that's in your problem scope anyway DECLARE @S VARCHAR(MAX) = 'I need to go to the (string1) and (test1) then (string2) afterwards. I can give you a (st(test2)ring3) if you help (testing space) me.' DECLARE @SQL VARCHAR(MAX) = 'SELECT [R] = ''' + REPLACE(REPLACE(@S, '(', ''' UNION SELECT [R] = ''('), ')', ')'' UNION SELECT [R] = ''') + '''' PRINT @SQL DECLARE @T TABLE( S VARCHAR(MAX) ) INSERT INTO @T EXEC(@SQL) SELECT REPLACE(REPLACE([S], '(', ''), ')','') FROM @T WHERE [S] LIKE '(%)' Results: string1 string2 test1 test2 testing space
ziptime beat me to the actual code. If you don't want to use a UDF, you can use the code inside the UDF he defined. If you want something more "basic", this is off the top of my head so syntax may be off a bit DECLARE @Sentence VARCHAR(4000) SET @Sentence = 'I need to go to the (string1) and then (string2) afterwards. I can give you a (string3) if you help me' + ' ' --added space so @Sentence reassignment doesn't fail if @Sentence ends in ")" DECLARE @LeftParen INT, @RightParen INT DECLARE @String VARCHAR(4000) WHILE 1 = 1 BEGIN SET @LeftParen = CHARINDEX('(',@Sentence) SET @RightParen = CHARINDEX('(',@Sentence) IF @LeftParen = 0 OR @RightParen = 0 -- no "(" remaining, or "(" with no matching ")" BREAK SET @String = SUBSTRING(@Sentence,@LeftParen + 1, @RightParen - @LeftParen - 1) PRINT @String -- or SELECT @String SET @Sentence = SUBSTRING(@Sentence,@RightParen + 1,4000) END 
I've used [W3Schools](http://www.w3schools.com/sql/) to learn all the syntax and play around with it. I've got experience in C++ but my job needed me to learn SQL asap. Pro: Easy to understand, Free, online Con: No certification, no teacher or other trainees. Even if you don't consider this "training" it's still a handy resource to tuck in your back pocket. 
Thank you, that helped so much!
Declare a variable and set its value to 1 or 2 using a simple SELECT. Then use an IF ELSE block to run either the 1 row or the 2 row SELECT. 
Cheers, wasnt sure a IF ELSE would do it so I went for a CASE :)
Is this a different set of columns then on the output? That's going to cause heartburn later when you use it if the number (and name) of the columns is inconsistent between the two code blocks. Better to set edge2 to null on the output that just wants edge1 so that both columns exist in all outputs. Late evaluation on a proc will allow you to do it I think, but you're going to have bad times if you do.
I am actually about to start my next presentation. This is my first time, and it's more fun then I anticipated. 
Writing on phone. Do the entries need to be indexed like this or ordered on request? For simple sorting just replace the first four characters with nothing if they are 'the ' You could add a computed column with the same expression you use for that sort and name it alternatename and persist+index that and use that column for order by clauses
pivot / crosstab
Follow up question: Which version is more normalized? Correct me if I'm wrong, but it seems to me that the first table is in 1st normal form: each field contains one and only one value. If I were creating this database I would split this into two parts. First a table of EmployeeName and IsEmployed. Second, a table of EmployeeName and IsFullTime. As a tip to /u/MarinertheRaccoon I've been reading about [Database Normalization](http://en.wikipedia.org/wiki/Database_normalization) which allows you to manage data with fewer errors. 
Those examples are extremely shortened from the actual table I'm dealing with, which has dozens of columns and thousands of rows; I was just trying to get down to the real grit of what I'm trying to achieve in my example. Those aren't actually even the real column and field names, as what I'm working on is a bit more privacy sensitive, but is generic enough to what I'm doing to work as an example. The reformatted table will only be an output to a report, not being put into a new table. Basically I need people with counts per type of "true" field, but currently counting the data I have is tedious and involves a lot of manipulation in Excel. 
since you mentioned tinyint(1), and mysql is to my knowledge the only dbms which has that syntax, could you please confirm that mysql is the dbms you are using? (please see sidebar; also, fyi you can also post in /r/mysql and then you won't have to identify your dbms) quadman's suggestion of a computed column will not, of course, work in mysql here's my mysql code for this -- SELECT title /* title is the original title */ , CASE WHEN SUBSTRING_INDEX(title,' ',1) IN ('A','An','The') THEN CONCAT( SUBSTRING(title,INSTR(title,' ')+1) ,', ' ,SUBSTRING_INDEX(title,' ',1) ) ELSE title END AS title2 /* title2 is what you sort on */ FROM daTable ORDER BY title2 *edited to add code
`SUBSTRING([ADDRESS_BLOCK],patindex('%[ABCDEFGHIJKLMNOPQRSTUVWXYZ]%', right([ADDRESS_BLOCK],len([ADDRESS_BLOCK])-1) COLLATE SQL_Latin1_General_Cp1_CS_AS)+1,len([ADDRESS_BLOCK]))` that seems to get the second part... first part would just have the start at zero, end at the patindex. No idea why A-Z wont' work but ABCDEFGHIJKLMNOPQRSTUVWXYZ will. *edit - thanks, that was fun and informative.
My specific use of tinyint(1) *does* derive specifically from MySQL experience - my first bit of coding was changing a script from writing to a Google Doc to writing to a MySQL database. Mostly the coding was done by a friend of mine, but I learned a ton. Right now, I'm using SQLite, because I'm having trouble making Django work with MySQL - pretty sure it's got to do with my misuse of virtualenv, and I'm all in a muddle about fixing it right now, so I've put it aside for the moment. I will, however, copy down that code, because it can't be THAT hard to get MySQL working with django. :&gt; Cheers!
you want to avoid doing something like this...? select EmployeeName , Field_1_Status as IsEmployed, isnull((select True from employeeTable et where et.EmployeeName = emp.EmployeeName and et.Field_1='IsFullTime' and et.Field_1_Status=True),False) as IsFullTime, isnull((select True from employeeTable et where et.EmployeeName = emp.EmployeeName and et.Field_1='IsPartlTime' and et.Field_1_Status=True),False) as IsPartlTime from employeeTable emp where field_1="IsEmployed"
p.s. &gt; Fortunately, this list won't include any songs by The The. my code will render this as The, The :) 
well, you could easily turn the code i wrote into dynamic...
Wow, thanks for the quick response. So if I wanted to use this in a select statement, how would I do that? Would I place my string name in place of address block? The Column name that has the values is Sys.AD_Site_Name0. 
so you would have this create strings... the strings would look like so: `"isnull((select True from employeeTable et where et.EmployeeName = emp.EmployeeName and et.Field_1='" + fieldname +' and et.Field_1_Status=True),False) as"+ fieldname +","` then append... `'Yahoo!' from employeeTable emp where field_1="IsEmployed"` So how would you go about creating those strings? How about this? `select distinct "isnull((select True from employeeTable et where et.EmployeeName = emp.EmployeeName and et.Field_1='" + fieldname +' and et.Field_1_Status=True),False) as"+ fieldname +"," from employeeTable` Now you have the strings...as rows... you probably want to join them somehow... make it a single row... http://www.sqlteam.com/article/using-coalesce-to-build-comma-delimited-string then you'd call 'exec' on the string you built... and you'd get your result set. 
that would be it.
Yes, because you need patindex of your column, not of the pattern you want to match. you did this: `patindex(0 ,'%[ABCDEFGHIJKLMNOPQRSTUVWXYZ]%')` So you told patindex to look for `0` within `'%[ABCDEFGHIJKLMNOPQRSTUVWXYZ]%'`. Of course it's not going to find anything. If you think the `right([ADDRESS_BLOCK],len([ADDRESS_BLOCK])-1) COLLATE SQL_Latin1_General_Cp1_CS_AS)+1` was just gibberish I assure you it's not.
I was about to beg you for the answer but I figured it out! I see what was happening at the end now, taking the length and adding one to make the last parameter of the substring command. Thanks for all of your help!!!
You have a separate sort field for the artist's name that gets created by a trigger. You'll want the same thing for any album titles, too.
The problem with the `order by title2` solution above is that it can't use any indexes. If you want your indexes to work, you need to either create a function that creates a sortkey, or you need to have a separate column that you index.
That answer you wrote down there definitely looks more sophisticated. Although I've known the recursive capabilities of CTEs, I never really had the time to come up with such solutions. I guess I am just used to cursors, because I know I can make them work and I know how to write them fast enough to move on to other tasks. Learn something everyday!
can you breifly explain the use case please
Table A and Table B are being written to at the same time. When both are complete, there is an entry in Table C to say so. On receipt of the entry in Table C, I would like a trigger to fire and an export of View D, as a .csv/.xml file, to be produced in a directory. View D is composed of data from Table A and Table B.
Why use a trigger when you can put it in as an additional link(step) in the chain? Edit - Or base it off the completion / success state of what populates Table A and B? Timestamp in Table C? 
Sort of where I'm going with it. Table C will hold the states of whether Table A and B are complete or not. Only when Table A and B are both complete then trigger the next step, which would be to extract the combined data into one file. 
I would schedule chained jobs. http://www.oracle-base.com/articles/10g/scheduler-enhancements-10gr2.php#job_chains
I'm just spitballing here: Select account_number from blah group by account_number having sum(case when code = 'useful' then 1 else 0 end) = 0 and sum(case when code = 'not found' then 1 else 0 end) &gt; 0
Create the test table: CREATE TABLE #Temp (Acct_Num INT, L_Name VARCHAR(50), F_Name VARCHAR(50), Bill_Date SMALLDATETIME, Code VARCHAR(25)) INSERT INTO #Temp SELECT '100001', 'Doe', 'John', '6/1/2013', 'useful' UNION ALL SELECT '100001', 'Doe', 'John', '6/1/2013', 'useful' UNION ALL SELECT '100001', 'Doe', 'John', '6/1/2013', 'not found' UNION ALL SELECT '100001', 'Doe', 'John', '6/1/2013', 'useful' UNION ALL SELECT '100034', 'Jane', 'Mary', '7/12/2013', 'notfound' UNION ALL SELECT '100034', 'Jane', 'Mary', '7/12/2013', 'not found' UNION ALL SELECT '100034', 'Jane', 'Mary', '7/12/2013', 'error' UNION ALL SELECT '100034', 'Jane', 'Mary', '7/12/2013', 'error' UNION ALL SELECT '100021', 'Smith', 'Mark', '1/11/2013', 'error' UNION ALL SELECT '100021', 'Smith', 'Mark', '1/11/2013', 'error' UNION ALL SELECT '100021', 'Smith', 'Mark', '1/11/2013', 'error' UNION ALL SELECT '100021', 'Smith', 'Mark', '1/11/2013', 'error' UNION ALL SELECT '10009', 'Queen', 'Mark', '1/5/2013', 'error' UNION ALL SELECT '10009', 'Queen', 'Mark', '1/5/2013', 'useful' UNION ALL SELECT '10009', 'Queen', 'Mark', '1/5/2013', 'error' UNION ALL SELECT '10009', 'Queen', 'Mark', '1/5/2013', 'error' --I'm not familiar with postgreSQL, but this is how I'd do it in MS SQL: --For the first query, I'm assuming you only want one count per account number/date/name, so the first thing is to dedupe the record set (John Doe has 3 usefuls), and then get the counts: select COUNT(*) FROM (SELECT DISTINCT * FROM #Temp WHERE Code = 'Useful') D1 For the second query, it's pretty much the same as the first, but with a not exists: select COUNT(*) FROM (SELECT DISTINCT * FROM #temp T1 WHERE Code = 'not found' AND NOT EXISTS (SELECT * FROM #temp T2 WHERE CODE = 'USEFUL' AND T1.Acct_Num = T2.Acct_Num AND T1.Bill_Date = T2.Bill_Date --join back to first derived table ) ) D1 The third count is the same as second one, but change the code &lt;&gt; error select COUNT(*) FROM (SELECT DISTINCT * FROM #temp T1 WHERE Code = 'error' AND NOT EXISTS (SELECT * FROM #temp T2 WHERE CODE &lt;&gt; 'ERROR' AND T1.Acct_Num = T2.Acct_Num AND T1.Bill_Date = T2.Bill_Date --join back to first derived table ) ) D1 
Dude, is that a table with no unique keys? Slap a serial on there.
I used to work for a software manufacturer and we used to technically interview guys, the questions used to get increasingly more difficult to the point where you wouldnt know the answer unless you were internal because knowing the answer would require internal knowledge this ridiculous line of questioning stopped when we got people who said they didnt know, you'll be surprised at the amount of bullshit people come out with at interviews because they believe they are expected to have an answer. You aren't, noone expects you to know it all and no-one can know it all, but it's important to tell the truth rather than make shit up, we were dealing with companies who purchased the software and support at great expense we couldnt have people bluffing customers we had to have honest people working with customers. 
haha that's actually hilarous. Makes sense though, I guess a lot of companies are just looking for smart guys who can adapt and not necessarily your encyclopedia type.
Especially if they're heavily Hive-focused, be comfortable with UDFs. In my experience UDFs are how work gets done with Hive. Your description sounds like (among, admittedly, a lot of other places) Facebook. If it is, feel free to message me—I used to be a data analyst there.
Yep, encyclopedia guys are good too, every IT team has them.
http://msftdbprodsamples.codeplex.com/releases/view/55330 Also, Google "Learn It First" he does great youtube tutorials. 
Although each flavor of SQL has a lot of similarities, it will be pretty rare that one script in a singular form will work across different vendors. You'll need to work with something intended for MSSQL - most examples I usually see when doing research reference a Northwind database that I believe that you should be able to find somewhere online, though I've never looked for it or dealt with it first hand.
This is what I was working towards. Much easier to manage reporting and handle additional conditions / use cases in the future.
Nope, I'm looking for '20130103', because the codes for 20130103 and 20130104 are the same, and I want the most recent date of a code change.
No, I decided to do something else entirely with my life—I'm working on becoming a high school teacher.
[American National Standards Institute](http://ansi.org)
Yeah, I figured that was gonna be the answer. But don't source control sites live...you know, on the web? I am nowhere near production, it's true. The app is for tracking status in an organization I am a part of. You get points for doing things for the club or donating to charity, etc. A user logs in, enters what they did, and then later it gets approved by a supervisor. I am using [web.py](http://webpy.org). I don't know if it had this schema ability you mentioned; it's pretty minimalist. I should also mention that this is a learning experience for me, and it's volunteer thing. I have heard that github had a steep learning curve, and I didn't think it was necessary; does it not have a lot of overhead? And how amenable is it to me just sitting down with a spare hour and working on it?
How is it related to flags in Microsoft SQL Server?
Cram for the interview and able to answer moderate questions like how to use and write a subquery, joins, self joins, etc.)
The setting values have an impact on compliance with behavior defined in SQL standards (SQL-92 is one of the more prominent revisions). These standards are partly reviewed and published by the International Organization for Standardization (ISO), of which ANSI is a US affiliate. Note that the ANSI_NULLS setting is deprecated in SQL Server 2012. In future versions (not sure about 2014 CTP), it will always be set ON. tl;dr It's just a prefix Microsoft picked for the setting name.
lag for the win. with code_table(id, code_date, code) as ( select 1, 20130101, 'A' from dual union all select 1, 20130102, 'B' from dual union all select 1, 20130103, 'A' from dual union all select 1, 20130104, 'A' from dual ) select max(code_date) from ( select ct.code_date, ct.code, lag(code,1) over (partition by id order by code_date) prev_code from code_table ct ) where code != prev_code; 
Source control repositories live on the web; but, you always have a local copy. SCM gives you the following (and more): * Ability to work in parallel with a team * Know when (and by whom) a change was made * Ability to roll back to a previous state * Ability to branch code (to try implementing a new feature, for example) I prefer Git and find it rather simple to use. An hour should be plenty of time to learn how to use it. Here are a few tutorials: * https://www.atlassian.com/git/tutorial * http://try.github.io/levels/1/challenges/1 * http://stackoverflow.com/questions/315911/git-for-beginners-the-definitive-practical-guide/ The overhead is minimal and, if you hand the project off to someone else, they can easily track the evolution of the project. My company uses Bitbucket because it's cheaper. I use Assembla for personal projects because my last company used them and private repositories are free. Github is quite popular, but you have to pay for private repositories. Since you are just starting, try Bitbucket. If you change your mind, you can easily switch to a different "origin" repository by changing a setting (but don't worry about this now). P.S. web.py does not seem to support migrations. However, submit a post at /r/Python. Someone there may have some recommendations.
You were close. The problem you were having is that there is no way for your last update to know which rows to update which sums, so it is grouping by departmentcode, but your result is only one column. you needed to add one more column to that subquery so that you can associate each department with their Sum. Try this out (bold is my changes) UPDATE #TempLinearFT SET #TempLinearFT.TotSales = STS.Totalsales FROM ( Select **s.departmentcode,** SUM(S.RetailSold) as Totalsales FROM tblSales as S INNER JOIN #TempLinearFT as TLF ON S.Store = TLF.Store AND S.DepartmentCode = TLF.DepCode WHERE (S.Saledate &gt;= TLF.StartDate AND S.Saledate &lt;= TLF.EndDate) AND (S.Saledate &gt;= @SDate AND S.Saledate &lt;= @EDate) AND S.DepartmentCode = TLF.DepCode GROUP BY **s.departmentcode --made this the sales department code for continuity** ) as STS **WHERE #TempLinearFT.DepCode = STS.DepartmentCode** **--This is the biggest missing piece. without it, the query doesn't know which rows relate to which sums. And you get your funky data** I hope this helped
[I selected a minimum amount of columns, but I think you will get the idea.](http://www.sqlfiddle.com/#!3/3abbc/52) Edit: the way I understood the problem is that there are 2 different sets of sums. It could potentially mean that there is only one sum that falls within the range of both dates, in which case the answers is [this.](http://www.sqlfiddle.com/#!3/3abbc/56)
Man! Thank you! This is exactly what I was wondering about and even more :) Cheers,
Eh, I prefer fossil. http://www.fossil-scm.org/index.html/doc/tip/www/index.wiki It's similar to Git, but you have a built-in wiki, and ticketing system. 
It's even more convoluted than that sadly. This will feed a report that shows the a store its stats concerning sales vs Linear feet per department so they can adjust how many feet they assign to them. So lets say(hypothetically); * Store XYZ opens it's doors for the first time on January 1st 2013 * The owner of Store XYZ was too lazy to enter their linear Feet prior to the store opening. * 3 weeks later they check the report and get no data, frustrated that the system is OBVIOUSLY not working they call and scream at me for a good half hour until I can finally put in a word edgewise and tell them "you didn't enter your stores linear feet per department, go do that and try again in a few days/weeks." * 3 Months later they check from opening day until the day before and get results that says "You have way too much Baby stuff and Party Stuff is flying off the shelves, reassign your shit and change your Merch." * They change the linear feet for those two departments in the store and this time miraculously they also remember update the footage in the database. * Today they check the report again they choose the opening day up until now (technically yesterday) In this situation there are 3 things that should occur; * Week 1-3 should be completely ignored because there is no way of knowing if the Linear Feet for those 3 weeks were correct, the store could have modified things after opening just because they felt like it ("my niece is having a baby, we should put more baby stuff on the shelves because people love babies" or some other dumb idea like that.) * For Baby and Party it should show 2 results; Week 4 - The day in Month 3 where they updated the LF or those departments AND The day in Month 3 - Yesterday. * For the rest it should show from Week 4 - Now Gets more convoluted the more changes happen.
Yep!
Thank you
Can you ping the server from your laptop?
Yes everything seems fine: Ping statistics for XXX.XXX.XXX.XX: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 21ms, Maximum = 145ms, Average = 52ms
Are you connecting by IP addresses or host name? 
use telnet to verify an open sql port to the ip/hostname. 
Having a background in SQL Server, I would do the following: SELECT name, [placeholder1] FROM employees WHERE datediff(yy, [placeholder1], getdate()) in (18,21)
That seems so much simpler than what I was attempting to do. Thank you very much. I'm really enjoying what I've been able to do so far in the tutorial, can you recommend any other ways to learn more about SQL?
For me, I got started by learning the basics at [W3Schools.com](http://www.w3schools.com/sql/). From there, I learned from my colleagues' SQL scripts which pertains to the data structure of the company I work for. Google is also a good resource for when you encounter an error or need a reference for function parameters.
it might help if you could say a few words about what all that code is attempting to accomplish
Are you basically taking a table of exponents and a table of numbers, and gettings all the number^exponent combinations, and their results?
If that's the case, then it's pretty easy. Do an inner join on 1 = 1, and you'll join the rows from each table together, and you can do a power() on that. You also don't need any of those variables really. --- /* declare tables */ declare @numbers table (num int) declare @exponents table (expon int) /* insert data into tables */ --do data insert here /* Don't really need a result table, especially if you're just going to output it */ SELECT N.num as Number, E.expon as Exponent, POWER(N.num, E.expon) as Result FROM @numbers N INNER JOIN @exponents E ON N.num % E.expon = 0 
are you trying to get the result set that represents the value of each number in NUMBERS raised to the power of each number in Exponents? if so you can just join them i suppose. insert into Result SELECT a.num, b.expon, power(a.num, b.expon) as res from numbers a, expons b ; you don't need an explicit join condition, or the word JOIN for that matter, because you are pulling one row for each combination of rows int he two source tables. 
Note: if you did this with a table of 100 numbers and 100 exponents, you'd end up with a 10,000 result set. So this kind of full non-conditional join is usually frowned upon unless you REALLY need it. Most guys do this kind of join *by accident*. :) 
INNER JOIN ... ON 1=1 is CROSS JOIN. They both would get you a Cartesian product.
I learned this recently in this subreddit, but I believe you can use recursive CTEs to accomplish what you want. You will have to pair it up with a UNION statement.
Thank you sir! You saved my brain from a lot of hard-ache the past week. Enjoy some gold
Thanks! Will definitely look into it.
I dunno! but I will read your links and take a look. thank you.
Pretty much all looping in SQL is considered bad practice. They are most likely trying to get you to see how to use SQL's strengths (i.e. joining relational tables) instead of using loops and cursors (I think they are trying to do that anyway). So, looping has to go through each number and exponent combination and check if (@num % @expon = 0), well, we can easily do that with an INNER JOIN ON N.num % E.expon = 0, or as /u/evilalive said, a CROSS JOIN with a WHERE clause, as it does all of that calculation on a row by row basis, instead of doing it one at a time like looping does! SQL is much more efficient when you do it this way, and its also easier to read as you don't have to decipher what the hell all those loops and if statements are doing.
ok ok makes sense. So I can have one SELECT statement and use the joins in the FROM statement...I see that in the original code it wants to insert all of this into a results table....I wouldnt have to do that anymore since id be running one string right? btw..thank you for your help
&gt; SELECT N.num as Number, &gt; E.expon as Exponent, &gt; POWER(N.num, E.expon) as Result &gt; FROM @numbers N &gt; INNER JOIN @exponents E &gt; ON N.num % E.expon = 0 So all of the code in my title can be shrunk to just this? 
[W3Schools](http://www.w3schools.com/sql/) is great. Also [technet.microsoft.com](http://technet.microsoft.com/) has the nitty-gritty syntax which is handy for more technical people, or if you're getting strange / unexpected results or errors. Search for a Key word and SQL 
hi and thank you for your help. Im really gonna need my hand held on this one...so I keep all my declare variables but I dont need them in my SELECT statement? I can simplify this whole thing to just. SELECT N.num as Number, E.expon as Exponent, POWER(N.num, E.expon) as Result FROM @numbers N INNER JOIN @exponents E ON N.num % E.expon = 0 
How would i get the total from these three cases combined? When I do a query search for 'useful', 'error', and 'not found' it doesn't return the correct number. Would i have to union each one of these cases to get the correct sum?
so would this be the right query? DECLARE @exponents table (expon int, rowid int identity(1,1)), DECLARE @numbers table (num int, rowid int identity(1,1)) ------------- INSERT INTO @numbers(num) values (3),(6),(9), (12), (15), (18), (21) SELECT @maxNum = SCOPE_IDENTITY() INSERT INTO @exponents(expon) values (2),(4),(6) SELECT @maxExp = SCOPE_IDENTITY() -------------- SELECT N.num as Number, E.expon as Exponent, POWER(N.num, E.expon) as Result FROM @numbers N INNER JOIN @exponents E ON N.num % E.expon = 0 ORDER BY 1, 2 
Yep, all the loop is doing is looping through the combinations of numbers (3, 6, 9, 12, 15, 18, 21) and exponents (2, 4, 6), and checks if the remainder of (number/exponent) equals 0, if it does, it inserts the row into the results table. After the while loop, it just outputs the results table. You don't need a results table, because that would be like doing this (in java, for instance). int number = 1; int one = number; return one; You could pretty much just do this, as it uses less memory, and is much more simple. return 1; Also, in SQL, there's 4 major forms of queries. SELECT, UPDATE, INSERT, and DELETE. (When I say data, I mean rows within that table.) 1. SELECT gets the data. It retrieves it from a table(s). 2. UPDATE modifies existing data on a table. 3. INSERT inserts data into a table. (just being thorough) 4. DELETE deletes data from a table. I'll reply to your other post as well, will need some time to type it up though!
and if you want them to LOOK the same, add "ORDER BY num, expon" at the end
The only declared variables you need are the @numbers table variable and the @exponents table variable. Every other variable you only need if you were to do it the 'looping' way. Most of those variables keep track of the current 'index' number or exponent, and the others (the @results table variable) holds the results, but you don't need that variable either. You would still need to declare the @numbers and @exponents table variables though, as well as the inserting of data into the variables. You can find the full query (with some comments and examples) [here](http://pastebin.com/7zXEzPqb). It works with Microsoft SQL Server versions of SQL, but not sure about other SQL versions.
You don't need the row id columns in the @exponents or @numbers table. You don't need to keep track of that anymore. You don't need the @maxNum or the @maxExp either. The rest is fine.
Thanks for your help. Again! really appreciated!
Nope nothing like that, truncate is the only bulk type you're going to get, but that trashes the entire table, so I wouldnt recommend that for what you described.