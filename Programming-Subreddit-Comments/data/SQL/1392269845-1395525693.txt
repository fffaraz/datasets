I was actually trying to recommend MS SQL express, which is easy to install on Windows. PostgreSQL is fairly easy on Linux, maybe not so much on Windows (haven't tried it there ;)
It sounds like a lot of your problem isn't necessarily in understanding SQL so much as it is in inheriting a database(s) which is poorly named and poorly documented. That's an entirely different problem. That is a somewhat different problem. Your first goal is to just understand how your application works. I don't have any insight into your situation, but I would suggest (if you can) tracing the business logic from the front end down to the database to understand what it's trying to do. Honestly, I don't think a greater understanding of SQL is going to help you with this. I think you need to spend time tracing requests and data flows from that. 
First of all, If you need to transpose data, joining the table repetitiously is not best practices. Second of all, You are using left joins first in your original statement and while in the second statement you are applying your inner join first in the second statement. You always include your inner join/join first to your statement. Inner joins reduce the amount of data that must be further joined. Lets say #main has 1,000,000 records and lets say #details has 5,000,000 records. On average there is 5 records in #details to 1 #main. You join #details 6 times to #main. That returns 30 million rows. Then you apply your inner join which filters the records down to 503. In your second query, you join the inner join first limiting it to 100 rows in #main. You then start joining your #details table to main. You end up with a total of 503 rows. Last of all, Best practices are to use proper transposition techniques. SELECT * FROM #Main M LEFT JOIN #DETAILS D on ON M.Key = D.Key PIVOT ( MAX(d.info) for d.rownum in ([1],[2],[3],[4],[6]) ) as P OR the old way SELECT M.*, CASE WHEN D.Rownum = 1 THEN D.INFO ELSE NULL END AS [D1.INFO], CASE WHEN D.Rownum = 2 THEN D.INFO ELSE NULL END AS [D2.INFO], CASE WHEN D.Rownum = 3 THEN D.INFO ELSE NULL END AS [D3.INFO], CASE WHEN D.Rownum = 4 THEN D.INFO ELSE NULL END AS [D4.INFO], CASE WHEN D.Rownum = 5 THEN D.INFO ELSE NULL END AS [D5.INFO], CASE WHEN D.Rownum = 6 THEN D.INFO ELSE NULL END AS [D6.INFO] FROM #Main M LEFT JOIN #DETAILS D on ON M.Key = D.Key
&gt;You always include your inner join/join first to your statement. Inner joins reduce the amount of data that must be further joined. This makes perfect sense. The application of the JOIN to Pg was added last partly because the multiple page function was given to me as an afterthought ("oh by the way, this should handle multiple pages too") and mostly because I didn't know better. I'll keep it in mind moving forward. Regarding the PIVOT statement (also again, obviously the right choice...), I simplified the query for this post but it's actually much larger. Does the PIVOT statement incur any overhead compared to the CASE method? 
here's a good one for you -- which team won the most games?
WHERE Table_ItemSN IN (PART4,PART5) 
haha nice one. i am building it for more of a personal use. not so much keeping track of league stats. just something my friends and I can use to talk smack to one another :)
Ive gotten sqlite up and running on windows pretty easily. Have no idea about postgres though haha.
what DBMS are we talking about? While I would strongly suggest you rethinking your data structure, for MSSQL you can do this : CREATE FUNCTION [dbo].[funStringListToTable] ( @strStringList varchar(8000) ,@strDelimiter char(1) = ',' ) RETURNS @tblList Table(strString varchar(200) collate database_default, intFieldPosition INT) AS BEGIN ;WITH E1(N) AS ( SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1), E2(N) AS (SELECT 1 FROM E1 a, E1 b), E4(N) AS (SELECT 1 FROM E2 a, E2 b), E42(N) AS (SELECT 1 FROM E4 a, E2 b), cteTally(N) AS ( SELECT 0 UNION ALL SELECT TOP (DATALENGTH(ISNULL(@strStringList,1))) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) FROM E42 ) ,cteStart(N1) AS ( SELECT t.N+1 FROM cteTally t WHERE (SUBSTRING(@strStringList,t.N,1) = @strDelimiter OR t.N = 0) ) INSERT INTO @tblList SELECT strFieldValue = SUBSTRING(@strStringList, s.N1, ISNULL(NULLIF(CHARINDEX(@strDelimiter,@strStringList,s.N1),0)-s.N1,8000)) ,fieldNr = ROW_NUMBER()OVER(ORDER BY s.n1) FROM cteStart s return END GO DECLARE @exampleData TABLE (id INT IDENTITY, stringList VARCHAR(255)) INSERT INTO @exampleData SELECT 'value1|value2|value3' union SELECT 'value1|value2' SELECT * FROM @exampleData d OUTER APPLY dbo.funStringListToTable(d.stringList,'|') 
This question comes up frequently enough that someone actually created a nice guide. http://www.connectionstrings.com/ There you go! I hope this helps.
Well, yes, but it's combined with not necessarily understanding the SQL. I do map it from the front end (or should say that I've begun to), but this literally comes down to every little thing... then figuring it out, discovering some weird anomaly... and researching it until it's traced back. &gt;Honestly, I don't think a greater understanding of SQL is going to help you with this. I think you need to spend time tracing requests and data flows from that. Yes. It will come with time.
Thank you this site looks promising but I can't find any guide about Word Templates, I'm going to look around a little bit
Sorry, I really don't understand your replies. "You are passing a varchar to CONVERT which throws that error" Isn't passing a varchar to CONVERT exactly how one would convert it from a date-like string to a DATE datatype? That or cast should work just fine. "I believe you could just change @DATEVAR to be a datetime if it isn't." It was originally a DATETIME type, and that didn't help. I changed it to a DATE type because the time portion wasn't needed for this calculation, and the AZDAT field doesn't contain it anyways. "Otherwise you might have an implicit conversion to a varchar that is throwing the error and some of the function that work are just overloaded nicely." If I have a CONVERT function, doesn't that make it an explicit conversion, not implicit? I added a calculated column that said an ISDATE(dateval) to the underlying view just to see if any string was not date-like and none of the rows returned a 0. Every string in the underlying table should have no problem being converted to a date using the CONVERT function.
You don't need player name in the stats table, too. Also, do you need walks and hit by pitch to correctly calculate an OBP or would you just count those as singles?
True did not think about the Access part, on Windows I would recommend SQLite.
Working with pivots can be gruesome if the rest of your data is pretty complex. So if you simplified your query for this post I would go with the second query. Generally, if I use the pivot method. I would prepull the values, pivot them and then add them to my main query. You are welcome to PM me the query and I can show how you can take those joins and using the aggregate method, add them as an inline query in your from statement. Multiple joins will work but two years down the road, what took 3 seconds then takes 5 minutes and one day it takes an hour. As the tables grow sql tries new execution plans that are more or less effective. Also being there are # in front of the table tells me that they temp tables correct? They probably are heaps and have no clustered indexes. Thus joining them on themselves causes some memory contention because the heap table must be read into memory when being joined. 
You want to query the Word doc directly by opening an ODBC database connection to a .doc file and issuing SELECTs against it? I know you can do it with .xls and .txt files, but I'm not aware of a general purpose ODBC connection string for Word docs. Perhaps you would be better off just bulk importing the data into a table before attempting to query it. 
How would you store your tree structure, assuming it could be infinitely deep? I'm not being snarky, just curious.
I'm going to steal some of your logic here for a problem I've been putting off here for a while. We have Member Status History tables that look something like this: ID Status Date 123 Active 1/1/2011 123 Expired 1/31/2012 123 Active 2/14/2012 etc - and we need to figure out the number of members at a specific point in time, so this member wouldn't return on 2/1/2012, but would on 1/31/2012. I need to iterate through and grab contiguous values and get date ranges for each time they became active. I don't think there's a much better (if any better) way than what you did. And it runs relatively quickly, so you can't beat that.
I would most likely use regular expression to parse this, but since I not entirely sure what output you want I cannot provide a query. If you can do not store data like this in the DB.
A sub query which uses regular expressions should work too. I would probably turn it into a view if I am not allowed to change the table structure to get rid off these clobs.
of course you could but you do not want that ugly query running every time. Have it run once to transform the data and then have a nice clean "select from where" that actually gets the data. This introduces a synchronization issue but i would gladly sign up for that than use a regex view on a clob **EDIT** Its the same thing as writing a calendar type application. Sure you can define a repeating event on a single line and programatically build those records every time the user queries their calendar. However that is a terrible design because you are beating up your database. Instead just limit it to 100 occurrences and insert those records, the adjustment/insert happens once, the query will happen hundreds and hundreds of times.
It seems vaguely worded, but maybe the question just wants you to ORDER BY p.fruit and u.deviceOS to show users with the same fruit and device together in your result set.
What is the aggregate relationship between users and fruits? 1:1 or 1:N? If 1:N, is it considered a match if two people have one fruit in common, or does it have to be the exact same number and combination of fruits? The answer largely depends on the answers to those questions.
worked exactly as I'd hoped. Thanks again :)
It will, his argument will outweigh yours, even though you may be right. 
Here's a good question: WHY doesn't it work? Any reason you state to answer this is a clear indication towards the fragility of your production server setup/software stack - there is not a single reason that emulated hardware shouldn't be able to do what hardware does (albeit, at a cost to performance). In other words, why is your set up platform dependent? You should know better. It's going to happen. It's already happening everywhere. I pushed so hard against VM's a half a decade ago, now they are all that I use, because I find that if I develop for the VM, not only does it work, but it works everywhere because I've abstracted away all platform specific features and code. My software fails less, it runs on a wider variety of systems, and should it need extra performance, I just ask the VM provider for extra memory/processor power rather than migrate the whole damn stack. Please illuminate me as to how you have any defensible position in this matter outside of "I built a glass cannon that breaks on the slightest modification to the environment" or "I write software for super computing systems that requires the performance of native hardware" (which you don't, as something like that would never be considered for VM/VPS systems). Also... Keep bullshit memes out of /r/sql please. This isn't /r/adviceanimals. This isn't for fun and games. This is for people to ask questions and post relevant and serious subject matter.
That's pretty much what I said. I would just use query with regexps instead of function to do the transformation. Turning that query into a view would be my last option if I am not allowed to change the schema.
I'll bite - because they refuse to give the virtualized SQL server the same resources as the hardware had previously - i.e. 16 cores and 64 GB of RAM and over 2 TB of dedicated, non-shared storage. As a result, ALL performance tests have sucked total balls. And it's not the server - all it does is server up SSRS reports and do ETL work every couple of hours. Sure, it's a poor VM implementation - but that's every reason why to fight it. Especially when the machines in question are less than two years old, and it's only a political fight because another group wants to steal the hardware for their SQL Server cluster instead of fighting for budget to buy their own. I wish I was joking.
You don't need a recursive CTE for that, buddy-guy! SELECT A.ID, A.Date as [Start_Date], isnull((select min(E.Date) from Statuses E where E.ID = A.ID and E.Status = 'Expired' and E.DATE &gt;= A.DATE),'12/31/9999') as [End_Date] FROM Statuses A WHERE A.Status = 'Active' Then you can take the specific date in question and just return the count(0) from this subquery where Membership_Date BETWEEN Start_Date and End_Date SELECT count(0) FROM ( SELECT A.ID, A.Date as [Start_Date], isnull((select min(E.Date) from Statuses E where E.ID = A.ID and E.Status = 'Expired' and E.DATE &gt;= A.DATE),'12/31/9999') as [End_Date] FROM Statuses A WHERE A.Status = 'Active') ActivePeriods WHERE GETDATE() BETWEEN ActivePeriod.[Start_Date] AND ActivePeriods.[End_Date] EDIT: I mean, you technically never said you were going to use a recursive CTE. But hey, maybe I still saved you a few minutes.
The biggest issue I had with Virtualization is performance. My internal testing of VMWare: Player,Workstation,(Defunct) Server,ESXi all suffer from I/O contention/drops when compared to VM against bare metal. Each successive version works better, and new releases get closer to bare metal. SSDs improve the hit, but physical disk drives under raid suffer much more. I noticed this under HyperV as well. One thing I noticed when I was going to publish data was that in the EULA for VMWare they prevent you from publishing benchmark data. Something weird I remember from researching this a year ago. I've been able to argue against VM systems by simply providing the relevant performance data to Sysadmins. Its not helpful to simply state "NO VMs!!!" but to state, "Our transaction speeds on average go from 25ms to 176ms when we virtualized is that acceptable to the business?" more often than not its no. However, cloud applications (Amazon AWS EC2 etc,.) are different, you know what you are getting, and the expectations should be different. Your application should be designed for HTTP latency rather than nanosecond latency by leveraging caches throughout the stack for high throughput.
This a hundred times.
I'm genuinely curious, what would you see as a benefit to be gained by visualizing a dedicated SQL Server? If you were using native recovery methods such as log shipping or mirroring, and had two identical boxes, why would still prefer the virtualized server over a dedicated one? Baring fiscal reasons, because you could easily re-purpose this hardware at a later time. I've had similar conversations with my network admin, and the main reason I'm hesitant, is that I can't find a good reason to do so if we didn't have to.
&gt; Wait, you need 64 GB of RAM and 16 cores to do reporting and data transformation? Think of big ERP systems, like Microsoft Dynamics, SAGE, SAP, etc,. Those systems are heavy hitting, and i've found are query crazy. It is not uncommon to see DB sizes around 25GB or more on disk. 64GB of ram may seem like a lot, but in ERP world its not. Each core is helpful so that (depending on your SQL Server) a query can use multiple-threads (ala cores) or each user can use a core and their work can complete quicker. ERP Systems can have more than 16 users connected at once, running sales data, manufacturing reports, importing sales orders, checking customer account balances, executing B2B EDI transactions, a lot can happen on a single database. It wouldnt surprise me. Also, some of the queries you cannot modify, they are part of the larger ERP system, and therefore locked away from a DBA's hand. All you can do is observe what the app is doing and hope that the app already put an index where it needs and add an index if you think it could use one.
Because if you need to upgrade, you don't have to touch hardware. Because setting up failure recovery is infinitely easier on VPS/VM. Because it costs less (usually). Because running a VPS cluster is more effecient, due to a dedicated server having 'down time' where it's doing nothing with it's processors/memory, whereas a VPS/VM generally has a much higher chance of finding something to do with the spare processor/memory. Because migrations from host to host are so much easier. Because you can take hardware XYZ, virtualize system ABC to act like hardware DEF, run it successfully on XYZ, and then move it over to a different VPS/VM of different hardware and it still works. Because you can decentralize your assets and your processing. Because you can cluster many servers to run one VPS/VM (scalability) easier than you can write software to use the hardware of many servers. TL:DR; Because it's easier. A lot easier.
But I have yet to hear from OP that his dataset that this is the case; you're talking about a minority of the cases. And of course each core is helpful; SQL mostly does set math which inherently benefits from concurrent processing, even if it's just one user. And no, it wouldn't surprise me either that the system uses that much, but it would surprise me to find that the db stack or the software stack accessing it is poorly optimized, which is why I raised the questions I did and specifically mentioned checking the indexes and fkeys.
oh wow that groupon deal is perfect. I was looking into taking a course, and this is pretty inexpensive. Thanks so much my friend. Huge help!!!
Agreed entirely. The most annoying thing is that it's cyclical in my experience. You can go from VMs to real servers and back to VMs again as the budgets and buzzwords revolve over time. I had an environment where we had, say, three dozen VMs running SQL Server 2000. Each did their own thing, and couldn't initially be merged due to the fact that they were all running some custom middleware which could only accommodate one instance per server. I was uncomfortable about it but it's just the way it was, despite occasional jeerings from the system folks to get it all off the VMs and onto real iron. Over the course of some years, I was able to gradually replace the middleware junk with DTS or SSIS tasks that ran PHP I developed, and 30+ VMs were consolidated to a couple of moderately-beefy servers hosting one instance apiece. VMware wasn't stressed anymore, I'd wiped out a couple dozen SQL license requirements, life was better. Suddenly our VMware implementation got some budget dollars, ESX found a new pimped-out behemoth of a cluster on which to run, and our production SQL 2005 cluster machines were finally upgraded to something reasonable (Windows Server 2003, 48GB apiece, and this was 2012..). Then the pressure starts to begin virtualizing things all over again, because now our ESX cluster kicks ass. Let's put a domain controller on there, let's put our public DNS on there, let's get rid of those two SQL staging machines you have and turn 'em into VMs. "What about all these CXPACKET waits..." "What about all of this IO contention..." What about 'em, VMware is rockin'! I wouldn't be surprised if by now they've virtualized the production SQL cluster with all nodes on the same ESX host, repurposed the servers that used to run that cluster for something else, and are now scrambling to get new hardware to build a new one. Don't know and don't care, but sort of glad to be out of there. tl;dr - SQL Server is always going to perform better on its own real hardware, with its own real cores, and as much of its own RAM as you can afford. [edit] PS: Why is a Network Admin trying to virtualize OP's server? He ought to be more concerned about the TDS packet volume than where the instance lives. Must be yet another case of occupational consolidation :(
You should alter database, not database files. How about just ALTER DATABASE ExampleDB
How are they connected to this DB? Directly via VPN or something? Or is it through an external client (web browser or whatever)? If connected directly via VPN I would check to see if their connection is consistently up, if it dips out for a moment then the SQL server wouldn't know who to send the results back to.
To answer your question: The error is right, that particular database does not exist because you referenced the .mdf. Try it again but without '_DATA' appended. From a permissions point of view, you may not have ALTER DATABASE permissions, check with your DBA, if it's you, make sure your account is sysadmin.
Oh, I feel dumb now, that worked. Out of all the databases this is the only database where the data file is named differently from the database name, and that is why the query worked for all the others and not this database. Thanks again!
it is through a VPN - I had that thought as well I will check on the stability - thanks update: kept a ping going while i did a few "large" searches, 703 packets sent, 5 lost, so stability does seem to be a problem
What flavor and version of sql?
Whoops, forgot to include that: MS Sql 2008r2
Full nightly backups of everything. These databases are fairly important. Could you expound regarding recovery modes and the log chain or link me where I could do some more reading about this stuff?
SELECT TOP 7 PERCENT * FROM YourTableName;
depends on the level of the role, for entry level basic question you will probably be asked to tell the difference between joins LEFT INNER FULL etc. you might get also asked a group by statement, simple update, insert and select statement as-well. if its a mid level developer I usual ask a recursion problem or I intentionally write sloppy code and ask them to fix it. just make sure you don't do a SELECT * or something that makes DBA's cringe 
Oracle 11g. The table structure is for a staging table that matches the loaded 3rd party log files, so we can't ask them to change their source data. I will look into this code, thank you very much!
Yeah, we are not allowed to change the schema since this is how the log files are received from a 3rd party. It was designed to be flexible, like in a sales transaction, you can just use cash, or you can use both cash and a gift check to pay for an item. It's a single transaction so it's just one row, but they wanted it to be flexible to contain any number of payment methods within that transaction. I will look into the reg exps though. Thanks for the replies, guys.
I mentioned in another reply that this is how we receive the data, and indeed we will extract it into another schema where they aren't clobs anymore, but for we that we need to parse the data first. As for the output, in the example above, I would like to be able to get : data2seq1 (from row 1) data2seq1 (from row 2) data2seq2 (from row 2) data2seq1 (from row 3) data2seq2 (from row 3) data2seq3 (from row 3) and be flexible enough to get any unspecified no. of these seqN values from column 2, so that I can perform some more logic on them. I will look into the regular expression, but can it handle this kind of flexibility in the source data? 
check this out for joins: http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html know basic operations: insert, update, select. know difference between drop and delete. yeah, for select statements, to impress people: SELECT TOP 100 * FROM table1 a WITH (NOLOCK) INNER JOIN table2 b WITH (NOLOCK) ON a.uniqueid=b.uniqueid WHERE a.blah IN ('blah1','blah2') Also, if you do an aggregate, showing the HAVING is a nice plus. know the difference between clustered and nonclustered indecies: http://technet.microsoft.com/en-us/library/ms190457.aspx know the fundamental principle behind T-SQL (if this is done in ms sql), and that transactions are your best friend ummm...what else. xml tricks are neat but i doubt they would ask you about that OH when you do WHERE make sure to ask them what are the indecies on the table. if your table (say a purchase log) is indexed by trade date and item type, make sure to have at least one or the other in your WHERE statement, otherwise it will show to the dba that you will be running extremely expensive queries, because filtering by nonindexed first, means it will have to scan everything. subqueries are good some times and bad others. there are some tricks you can do by joining a table on itself but thats really usually not needed. If you need to work with a large query, always dump it into a temp table first and then do work with it. Preferably within transactions. drop temp tables are you are done, and check for their existance if needed at the top. thats mostly for writing stored procs. Umm....thats all i can think for now. if you got questions throw them up, if i think of something else, ill add. otherwise its just mathematics: set operations on bags (multisets) and you can reason anything about it. its mostly syntax so if you can show you understand relational databases principles, thats worth more than you forgetting the exact syntax for a cursor.
In a single table. :D This was just bad because it wasn't infinitely deep, so didn't require a single table. It held multiple different types of nodes, and didn't have a depth column.
Yeah that could be it very likely. good luck!
Don't forget to ORDER BY ASC/DESC!
While others have pointed out the syntax error, the bigger question is WHY are you doing this. You mention in the comments that this is a fairly important database, then you DEFINITELY should not be doing this. For a quick lesson, a log chain is all the backups since your last full backup. For instance you take a full backup on Sunday night, then differentials every night, then maybe transaction log backups every hour or so. Lets say, you have a failure at 3pm. You would restore the full backup, then last nights differential, then every single transaction log since last nights differential. The transaction logs have starting and ending LSN (log sequence numbers) if the starting LSN from the 10am log doesn't follow the ending LSN from the 9am log backup, you have a broken chain and will only be able to recover up until the 9am transaction log backup, and you've now lost 6 hours worth of data. In your case, if you *MUST* do this, take a new full backup before you truncate (for safety) and then [again after reverting to FULL recovery to reestablish the backup chain](http://technet.microsoft.com/en-us/library/ms189272.aspx), otherwise you're still vulnerable until your next full backup. I can only assume you are trying to manage disk space by shrinking the log file. If you are backing up your transaction log at a regular basis, it will won't grow that big in the first place. What is your current backup schedule look like? Get that to an acceptable schedule and this problem goes away. Here's two articles, the first is straight from Microsoft, and the second is from the guy responsible for SQL 2008 and his wife (also quite impressive SQL person) about breaking the chain. [Working with Transaction Logs](http://technet.microsoft.com/en-us/library/ms190440%28v=sql.105%29.aspx) [Breaking the Log Chain](http://sqlmag.com/blog/breaking-chain)
An OR will always cause a SCAN, an IN will try to perform a SEEK unless the count of the IN is high, which it would then do a SCAN, ie. the IN clause included a sub-query on a table for instance, but in this case the IN would be a SEEK since its defined criteria.
When you say a recursion problem - I assume you mean in a language besides SQL? As far as I'm aware, recursive SQL really only became possible with the introduction of CTE's relatively recently. 
What kind of consultancy position is it? If it's a good company, they'll be more interested in training you for the role, assuming you're the right person. Don't get too caught up with different joins, cursors, CTEs, etc.. Just be able to deliver a solid SELECT statement given a set of tables and entities.
I would rather expect to be questioned on the difference between LOOP HASH and MERGE joins... If I'm told SQL is key for a position, I would expect to be expected to actually know SQL. And knowing SQL is not just having the syntax down.
If I was conducting an interview with you, my very first counter question would be, SELECT TOP 100 * FROM table1 a WITH (NOLOCK) INNER JOIN table2 b WITH (NOLOCK) ON a.uniqueid=b.uniqueid WHERE a.blah IN ('blah1','blah2') What is with(nolock) and why do you use it? If you could not give me a very very good answer to that one, the interview would be over. (I am impressed by that example query, but not in a good way) &gt;filtering by nonclustered first, means it will have to scan everything. Wait, what ?? 
My advice to you would be honesty. If they want to train you, they will teach you. If they already say that SQL is key to the role, I would assume that they are looking for someone that as actual experiance in SQL. And that would entail at the very least some performance optimization, reading execution plans, database design and indexing strategies. That assumption can be wrong, but if you set expectations of you having in depth knowledge on SQL programming, trust me, it will take 2 minutes to test that. So I would go in, tell them that you know the very basics, and would welcome the chance to learn more about it.
The question is not to clear perhaps you could show some sample data. Show data in the tables and show the result you would like to get.
Its an entry position. I would just be installing software, they're aware that I haven't done sql in a long time but still want me in for an interview. Training will be provided to the successful candidate. Thanks for the advice!
Entry level consultancy position. I would be installing software on client sites. Very minimal SQL, its just to ensure the software is working so i've been told it could be the odd SQL statement here and there. Thanks for the advice! I'll keep that in mind.
Thanks for the advice!
Yeah sorry, this is the Pick table: http://imgur.com/BSicpHY This is the User table: http://imgur.com/WsBljk7 And this is the result of my above query that joins these two tables together showing which users, with deviceOS 4 or 4.2, have picked which fruits. I've underlined the two results I need back: http://imgur.com/FUQFvDU The question is asking for 2 things: 1. To return the users who are using deviceOS '4' or '4.1' (done) 2. Of those users, to return those who had the same device type AND picked the same fruit All that should be returned is Marcia A. Walsh, who picked a banana with deviceType 'iPod touch 2G', and Priscilla I. Wilkerson who also picked a banana with deviceType 'iPod touch 2G'. Hope this helps 
This is the Pick table: http://imgur.com/BSicpHY This is the User table: http://imgur.com/WsBljk7 And this is the result of my above query that joins these two tables together showing which users, with deviceOS 4 or 4.2, have picked which fruits. I've underlined the two results I need back: http://imgur.com/FUQFvDU The question is asking for 2 things: 1. To return the users who are using deviceOS '4' or '4.1' (done) 2. Of those users, to return those who had the same device type AND picked the same fruit All that should be returned is Marcia A. Walsh, who picked a banana with deviceType 'iPod touch 2G', and Priscilla I. Wilkerson who also picked a banana with deviceType 'iPod touch 2G'. Does that help at all. The question is a bit ambiguous but I think that is what it is asking
That is still relatively recent as far as I am concerned - especially when you consider that in many companies updates to SQL server versions take years to ensure stability. I wouldn't expect a mid level SQL dev to be familiar with recursion in SQL. It just doesn't come up that much. 
Example please.
I respectfully disagree. Using `WITH (NOLOCK)` or setting the isolation to `READ UNCOMMITED` will actually “break” your database. SQL Server’s implementation of ACID compliance is using data locks and optionally MVCC. A `WITH (NOLOCK)` violates ACID compliance by allowing so-called “dirty reads”. In my mind, this is breaking the database. Imagine a highly transactional billing system that is highly dynamic. It becomes very important to the software and front-end user that data that is accurately read from the database even if the underlying tables have concurrent updates/inserts/deletes. In these situations, even the default isolation level might not cut it. And `READ UNCOMMITTED`, I argue, is out of the question. Even on a reporting database, you may argue that there is more leniency for “dirty data”, but even this might not be desirable. Reporting servers also have more leniency for large, sweeping updates/inserts that are wrapped in 1 transaction. Examples include ETL jobs and calculating aggregates. These transactions must be atomic because the data would not make any sense in the middle of these calculations/jobs. Imagine doing a very long, involved math problem and someone picked up your work before you completed and declared that the last line was the answer. That wouldn’t make any sense. If ACID compliance is not important to your data, there are some very good database alternatives out there that work much better. 
Only speaking MSSQL here. You are perfectly right in that the SQL Server will grab all memory it is permitted to use and does support. It will grow over time, as more data gets cached. In theory, if the OS needs more memory, the SQL Server should release some memory to not starve the OS. It will also not take blantly everything, it will leave enough for the OS to work with normally. In practice, usually one does limit the SQL Server memory to garantee some GB for the OS and other processes that might (but should not) run on the same Box (or VM in your case). As I'm talking to a sysadmin right now.... please never put database files on raid 5 ;)
that's almost 10 years ago.... there are very few shops still actively using sql server 2000 as a primary data source. last time I had a developer try to use a cursor I wanted to bitch slap them. 
And you are correct for anything functional. If he is doing simple selects on tables, just to see say, if there are results -- not to generate a precise report (thats where SSRS/BI comes in with a preconfigured query and scheduled deliverable) you would definitely want to use nolock. The goal at that point, is to figure out your query while you are developing it. LIke, I see what you mean, and any time you write procs, functions or any code that calls dynamic SQL, for the love of god you should never be using this. but for direct database querying, for select statements that have on them 3-4 joins and a few filters, where his goal is to find some forgotten static piece of information, i would still say you want to use nolock. because he is not running against the really dynamic data, but this way he prevents the remote possibility of something breaking while he is trying to find that one row with some data that was inserted last week.
I agree - cursors are a no-no. But usually there can be a good set-based solution to a problem that an under-experienced developer turns to cursors to solve. I've encountered very few that needed recursive queries.
Yes, i am aware about the dirty reads. And I use the nolocks for when i am querying the database while developing a proc, or trying to iron out some complex nested selects here and there. Also, if i know that some tables are not as dynamic as others, i will throw nolock on there as i dont anticipate that data changing generally speaking, but i want to protect myself against locking a table (in a small chance) at the same time as say, someone tries to insert a user, while i am out there searching for a subset of users. some applications are built with a sql timeout response, so if my query is locking that for a bit too long, the app might error out even though sql will eventually get to that tran in the queue and execute it. Yes. nolock is to never be used, really, in production code. the transaction queue is there for a reason. but for someone in his position, who will probably be running ad-hoc sql queries trying to just get some fairly static data out there (they wont let him query against super dynamic stuff most likely anway), it doesnt hurt, i think, to throw it on there so you minimise your chance of impact. also, didnt know about the snapshot isolation level. thanks! will check that one out. I was a devops engineer working for the company architect when he pretty much handed me the ms sql clusters and told me they were now my babies. so a lot of what i learned was from googling.
please read svtr's and my discussion on nolock. he brings up some valid points about it that i overlooked and did not explain.
from a performance aspect using recursive CTE statements is almost always faster for the simple reason that the data is usually processed in memory. but it also greatly depends on what your db engine is. 
Fair enough,and you didnt. I was enjoying your respone and our discussion! I learned some things from this, and it made me better at this at the end of the day :) but yeah. you're right. I PMd the OP to read your and mine response more carefuly to make sure he understands the nolocks. 
i think i only had one time where a cursor was actually the only way of doing something...dont ask. archaic software and extremely poorly setup database.
I'm not entirely sure if this is what you're exactly looking for, but why not try the following. SELECT u.name, u.deviceType, u.deviceOS, p.fruit INTO #temp FROM user JOIN Pick P ON u.uID = p.uID WHERE u.deviceOS in ('4', '4.1') The above will put that into a table. Now query said new table. Select t.name, t.deviceType, t.deviceOS, t.fruit from #temp t where t.deviceType = 'IPod touch 2G' and t.fruit = 'Banana' drop table #temp That's how I would tackle it. But then again I mostly work with SQL Reporting. not sure what you're using to query. Regardless it should show up. 
Thank you very much!
&gt; I wouldn't expect a mid level SQL dev to be familiar with recursion in SQL. It just doesn't come up that much. I disagree here. A mid level SQL dev should know the features of the most current version, and the most glaring "that doesn't work below @version". In this example, if you don't know about recursive CTE's you are going to resort to some dirty dirty hack. If you know about recursive CTE's, you are going to try to use a recursive CTE, and then bite down hard on your keyboard when you find out that you are running SQL2000, and can't use them. Better know, and go to the good solution first, before falling back to the dirty hack. 
a recursive cte is still a set based approach. You are essentially self joining for "deepest level of recursion" times. It is not a treat each row as its own. (at least in mssql, I don't know the implementation in other DBMS's)
&gt;Assuming a default install of Oracle, no memory settings changed, if you have an application being hosted on the server as well, is it safe to assume that application is being starved of memory? Not necessarily - ideally, applications should be on separate servers from databases in order to prevent resource conflict, however, that's not always possible. That said - on one of my boxes, I have Server 2008 R2 with 32 GB of RAM and Oracle 11.2.0.4.0 64-bit EE, and Oracle is only using 9 GB of the available RAM, and I haven't seen it spike above 12 GB yet. Obviously YMMV, but it seems fine so far.
Put single quotes around each of the parameters: exec sp_RENAME '#tmp_stg3.metric_cnt' , 'primary_offense_cnt', 'COLUMN' I'm not sure if this procedure works with temp tables though. Edit: Just checked, does not work with a temp table on 2008 R2 or 2012.
what he said... It should work thou, its still a table even in tempdb, so while there is an object id, it should work I'd say *scratching my head* why do you want to rename a column on a session local temp table? Just out of curiosity //edit What might still work, would be doing a select to tempdb.sys.tables where object_id= object_id('tempdb..#tempTable'), to get the actual name of the table as it resides in tempDB. Its a bit to fucked up to be done, as I still can't think of ANY reason to rename a column in a local temp table
&lt;size 9000&gt; MUCH &lt;/size 9000&gt; faster
I cannot get an object_id for a temp table. I seem to recall you have to look at tempdb and it ends up with some crazy name. Edit: CREATE TABLE #Test (testID int) SELECT * FROM tempdb.sys.objects WHERE type = 'U' ORDER BY create_date DESC First row returned had this in the name column: #Test_______________________________________________________________________________________________________________000000000013
On this, you might wanna have some [syntax highlighting](http://data.stackexchange.com/stackoverflow/query/167643/example-query-for-dynamic-pivoting) set nocount on /* This be simple static example */ create table #someTestData ( id int identity primary key clustered , vendor varchar(50) , articleID int , articleGroup int ) insert into #someTestData Values ('vendor1', 1,1) ,('vendor1', 2,1) ,('vendor1', 3,2) ,('vendor1', 4,2) ,('vendor1', 5,3) ,('vendor2', 1,1) ,('vendor2', 2,2) ,('vendor2', 3,1) SELECT * FROM ( SELECT Vendor, ArticleID , articleGroup from #someTestData )data pivot ( COUNT(articleID) FOR Vendor in ([vendor1], [vendor2]) )pvt /* this be I hate doing stuff like this, dynamic way */ declare @dynSQL nvarchar(max) declare @VendorsToPivot nvarchar(max) select @VendorsToPivot = isnull(@VendorsToPivot +',','') + '[' + t.Vendor +']' FROM ( SELECT DISTINCT vendor FROM #someTestData t )t SELECT @dynSQL = 'SELECT * FROM ( SELECT Vendor, ArticleID , articleGroup from #someTestData )data pivot ( COUNT(articleID) FOR Vendor in (' + @VendorsToPivot + ') )pvt' exec (@dynSQL) The idea behind this is, you need to supply the list of columns the pivot is supposed to aggregate data to. Thats the part of COUNT(articleID) FOR Vendor in ([vendor1], [vendor2]) If you do not know what values there are for the vendor, you write the list to a variable, and concatinate that to the dynamic sql, then execute the dynamicly created sql. This is not something that will run especially fast mind you, so well, you've been warned A general thought on dynamic sql, use sp_executesql @dynSQL sp_executesql will execute @dynSQL as a parameterized query, which will at least produce a reusable execution plan. This is a bad example, since there are no parameters, but well, exec (@sql) is usually not what you should use. And yes, the above query could be used for sql injection. So, be careful with stuff like that boys and (who am i kidding) girls
Thanks for giving me something to keep busy with over the weekend. :)
On complex queries you are right in that the query might be run multiple times. On easy queries, you should get lucky in that the optimizer *might* optimize it out in the join ordering. The more complex the query, the higher the chance of that not happening, so I wouldn't count on that on anything thats a bit more complex and is reusing a cte multiple times (and that in itself speaks for "complex"). Same goes for subqueries thou, on easy queries, they should get completly inlined, on more complex queries, you can end up with the subselect being executet on its own, and then joined to the outer recordset at some point during execution. So you are absolutly right in that there is (in theory) no difference in execution, and also in that temp tables can be a better, much better option, sometimes. 
&gt;Microsoft has indicated that in future versions of SQL server, all queries will need to be terminated with semicolons. I remember having that seen it on the depricated list for I think 2014, might even have been 2012. I'm not sure if they are going to go trough with it anytime soon thou. That will break a metric fuckload of code on migration, all over the world.
god damn workaholics .... :P
I wouldn't to be honest. Global temp tables are iffy to say it nicely. Using them, it dosn't take very much to have multiple sessions trying to access the same temp table .
&gt; No need to wait. it's a disaster already. The 1 actually triggers resets in the software on about 75% of the lines. And that's AFTER we came in. I don't want to talk about the dark times... if that is the "better" situation, i really don't want to ask / know. I feel for you friend ;)
Have you checked out [Sabermetrics](http://en.wikipedia.org/wiki/Sabermetrics)?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Sabermetrics**](http://en.wikipedia.org/wiki/Sabermetrics): [](#sfw) --- &gt; &gt;**Sabermetrics** is the term for the empirical analysis of [baseball](http://en.wikipedia.org/wiki/Baseball), especially [baseball statistics](http://en.wikipedia.org/wiki/Baseball_statistics) that measure in-game activity. The term is derived from the [acronym](http://en.wikipedia.org/wiki/Acronym) SABR, which stands for the [Society for American Baseball Research](http://en.wikipedia.org/wiki/Society_for_American_Baseball_Research). It was coined by [Bill James](http://en.wikipedia.org/wiki/Bill_James), who is one of its pioneers and is often considered its most prominent advocate and public face. &gt; --- ^Interesting: [^NERD ^\(sabermetrics)](http://en.wikipedia.org/wiki/NERD_\(sabermetrics\)) ^| [^Bill ^James](http://en.wikipedia.org/wiki/Bill_James) ^| [^Baseball ^Prospectus](http://en.wikipedia.org/wiki/Baseball_Prospectus) ^| [^Pete ^Palmer](http://en.wikipedia.org/wiki/Pete_Palmer) *^\/u/zombieOMG ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cffgjbg) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cffgjbg)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less.* ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 57192:%0Ahttp://www.reddit.com/r/SQL/comments/1xsarr/best_insightful_queries_for_a_baseball_leaderboard/cffgj72)
you want to look at the actual execution plan for both queries in such cases. It might be that it actually is the same, it might be that there is not enough data to give you a noticeable time difference in your test. The Query optimizer does do a LOT of optimizing, so it could actually be the same execution plan. Anyway, if you want to know what is happening below the hood, look at the execution plan.
Thank you for this information. Like I mentioned earlier I am no DBA and Google for stuff when I need help. You are correct in your assumption that we are trying to free disk space by shrinking the log files and only way that we found to shrink the logs is to change the recovery mode and then shrink the log file. From the information you have given me and what I have read, since the database isn't backed up using the SQL service on a schedule (I talked with the other sysadmin who normally takes care of SQL servers and he does full backups independent of the SQL service) the log files will continue to grow. 
&gt; Ah, I knew there was a way to get the object_id of a temp table, I just couldn't remember it (and didn't bother to look it up). I haven't used that one in a long time. when ever I use temp tables, i usually drop em if they exist, just cause I hate, really hate, debugging a stored procedure, that does not include the droping. After 5 iterations of #someDamnTemp table already exists, I usually need a coffee or a smoke
Thank you for your explanation. The databases I am talking about certainly are not that important where we would be loosing thousands of dollars if we had to rollback to the last full backup. I think the databases that have the most active transaction logs are for antivirus, a document management system and the IT service desk ticketing system. I replied [here](http://www.reddit.com/r/SQL/comments/1xtr55/ms_sql_need_help_understanding_query_error_user/cffgr09) to /u/CrossWired with some more information about our setup. The backups are taken independently of the SQL service and from what I am understanding, the logs will continue to grow since the log is not truncated and space re-used. 
Well, no. There are two different but related backups at work here. One is BACKUP DATABASE blah. That is your nightly full. That does NOT truncate the log. To back up the transaction log, you need to BACKUP LOG databasename TO location. Once you have done that, SQL Server can reuse inactive portions of the log. So, if you are in SIMPLE mode, BACKUP DATABASE is all you need. However if you are in FULL, you need to do two kinds of backups: BACKUP DATABASE - gives your log backups a starting state BACKUP LOG - backs up inactive (committed) transactions from the log, allowing the space to be reused.
oh sure, i just thought maybe he was getting the error because it was out of scope. otherwise i agree.
Just put the word DISTINCT in your select statement. SELECT DISTINCT u.name, u.deviceType, u.deviceOS, p.fruit FROM User u JOIN Pick p ON u.uID = p.uID WHERE u.deviceOS IN ("4", "4.1") 
A default Oracle DB creation on a *nix system will allocate 40% of system memory split between the PGA and SGA. Can't say the same is true for Windows, but I would hazard that it is.
The shorter one is easier to read and understand its intention. It will most likely perform as well or better than second option as your table grows. The second option might take more of a hit as the table scales up but it is more flexible if you ever need to do things like look for a row where some value changed from the previous row (2012 makes this even easier with the 'lag' and 'lead' keywords). I would probably go with option 1 and keep option 2 in the back of my head in case I ever need to do more complicated arranging.
well, if you are only doing a quick select just to see what is in the table, the simple way to go is: select top 10 rxsc.StatusCodeDate AS StatusCodeDate , rxsc.RXID FROM RXStatusCodes AS rxsc WHERE rxsc.Active = 1 order by rxsc.StatusCodeDate desc However, if you are trying to join this in (especially on a large dataset) i'd suggest going the route of the apply statement. select * from baseTable a cross apply ( select top 1 rxsc.StatusCodeDate AS StatusCodeDate , rxsc.RXID from RXStatusCodes as rxsc where rxsc.Active = 1 and rxsc.pkid = a.referenceID order by rxsc.StatusCodeDate desc ) b I used to be a fan of the row_number() partition by() statements until I found out about the cross apply and outer apply statements. Changed my life. :-) 
on complex queries, I wouldn't be so fast in throwing out the row_number. The cross apply can only be executed as a loop join, just try it with applying a join hint. That can severly bite you in the ass, and/or change your life yet again, on occations. I almost never give a definitive answer on which of semanticly equal queries one should use. It does always depend on the data and schema. Hence my advice to op, to repeat myself, look at the execution plan. If you don't know how, learn it. THAT will change your life
You can try a with to prepull and pivot the results using WITH. This benefits from parallelism on the SQL server. ;WITH details as ( SELECT m.key, [1] as [D1_Info], [2] as [D2_Info], [3] as [D3_Info], [4] as [D4_Info], [5] as [D5_Info], [6] as [D6_Info] FROM (SELECT m.key, d.rownum, d.info from #Main M LEFT JOIN #DETAILS D ON M.Key = D.Key WHERE convert(decimal,d.RowNum + 5)/6 = (d.RowNum + 5)/6 ) as src PIVOT ( MAX(info) for rownum in ([1],[2],[3],[4],[6]) ) as P ) SELECT m.column1, m.column2, m.column3, m.column4, D1_Info, D2_Info, D3_Info, D4_Info, D5_Info, D6_Info From #main m inner join details d on m.key = d.key
yeah that's cool. i'm currently looking into it now. thanks for your help; you should be on commission!
I can't recommend it enough. There is no other way to easily design and deploy this stuff. Watch the youtube videos. Good luck with it. :)
This is basically what I did (on your advice to pivot properly)... just without the CTE: SELECT * ,DetailLine18 = [constant data] ,Pages = lcount/@Lines + 1 ,Continued = CASE WHEN lcount/@Lines + 1 = PageNum THEN '' ELSE 'Continued on Next Page' END ,PageLabel = 'Page ' + convert(varchar,PageNum) + ' of ' + CONVERT(varchar,lcount/@Lines + 1) FROM #Main M JOIN (SELECT --this is a pivoted table from #details, this will work MULTI and SINGLE page items Key ,PageNum ,DetailLine1 = [1] ,DetailLine2 = [2] ,DetailLine3 = [3] ,DetailLine4 = [4] ,DetailLine5 = [5] ,DetailLine6 = [6] ,DetailLine7 = [7] ,DetailLine8 = [8] ,DetailLine9 = [9] ,DetailLine10 = [10] ,DetailLine11 = [11] ,DetailLine12 = [12] ,DetailLine13 = [13] ,DetailLine14 = [14] ,DetailLine15 = [15] ,DetailLine16 = [16] ,DetailLine17 = [17] FROM (SELECT Key ,PageNum = (Line - 1) / @Lines + 1 ,PLine = Line - @Lines * ((Line - 1) / @Lines) ,FLine = [consolidation of data into single column FROM #Details WHERE Key NOT IN (SELECT Key FROM #Errors)) B PIVOT (MAX(FLine) FOR PLine IN ([1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17]) ) P) PDetails ON M.Key= PDetails.Key I realized we had made several changes to spec and UI from when I first drafted this (18 months ago?) that allowed the PIVOT statement to be used without too much fuss. I'm glad I tackled it this way too, I feel like something clicked in my brain about how these statements work.
Cool, glad it worked out for you.
In PostgreSQL it is simple, but I do not have an Oracle database installed to check how to do this there. I am not familiar enough with Oracle to even know if it can be done in a similar way there. SELECT unnest(string_to_array(substring(data from '\|([^|]+)'), '$')) FROM test; Edit: In the code above `data` is the field with the data and `test` the table. The code above first extracts the interesting field (e.g. `data2seq1$data2seq2`) with substring and a regexp, then splits them into arrays on $, and finally unnests all the arrays into one result column. The result is: unnest ----------- data2seq1 data2seq1 data2seq2 data2seq1 data2seq2 data2seq3 (6 rows) Edit 2: Actually a simpler solution in PostgreSQL would be to rely entirely on arrays. SELECT unnest(string_to_array((string_to_array(data, '|'))[2], '$')) FROM test; Edit 3: It does not seem like it is this simple in Oracle but I found a link here about splitting strings: http://nuijten.blogspot.se/2009/07/splitting-comma-delimited-string-regexp.html
Thanks again, I really appreciate the input.
You should should check out http://ola.hallengren.com. He has developed a few scripts that help with backup and maintenance, which I assume you aren't doing regularly either. You can grab his [Maintenance Solutions](http://ola.hallengren.com/scripts/MaintenanceSolution.sql) and run it. There are 4 settings at line 34, you only should modify the backup directory, other than that, leave it alone and run it. Once it runs, you will have 4 agent jobs. None of them will have schedules, since you don't have any automate backups in place now, I'm going to recommend a schedule. I would create another job to run alot of these new jobs in sequence. This is your main weekly maintenance and backup. * Add both Database Integrity jobs, first, order doesn't matter for these two. * Add IndexOptimize * Add both history cleanup jobs * Add both cleanup jobs * Next add both of the FULL backup jobs. * Run this sometime Friday or Saturday night Next, schedule the one called DIFFS, schedule it for every night, other than the night you run your fulls. Finally, ask your self, how much data loss is acceptable? If you can lose up to an hour of data and be OK, you need hourly log backups, now thats not to say you will lose that, just worst case, your log backups are an hour old and you can restore to that point. Once you figure out this time frame, schedule the LOG backup job to run on repeat every X hours. Do not worry about the LOG jobs overlapping the backups, they'll figure it out. Now since we're talking about disk space, you likely want to push these backups OFF the server. Depending on network speed you might want to take the backup locally then copy it over the network. If this is the case, simply add a step to all of these jobs to do just that. If anything, ensure that these backups are not on the same drive as the data, otherwise this whole exercise is useless. Edit: i wrote all of this from memory, sorry if the names of the jobs aren't exact, but the concepts are on and the names are close enough you can get it done.
I just dumped a whole reply on Ola's scripts below, so that should take care of the HOW. I would agree with what you've said above, the SIMPLE would be best for your AV. We have our service desk software set to FULL as folks get kind of pissy if you lose a ticket. Great writeup above BTW
Go with GROUP BY for readability/maintainability. In the second query you're doing the same thing as GROUP BY but probably less efficiently. Why not use the internal aggregate function which is well optimized?
my pleasure, glad I could help! good luck to you :)
I live near London, UK. I am an IT consultant running my own consulting company (for the past 20 years), and do database development as about a 40% slice of my work. Servoy accounts for ALL of my development work. I used to be FileMaker Pro based, but that is a truly awful development experience, and always feels like playing with simple toys rather than working with proper databases and proper development. With Servoy, my biggest deployment uses about 100GB of database back end. Couldn't ever do that with Filemaker..
Ah, I'm in london too working for a data consultancy... I wish you luck with your business :) Thanks again for your time.
You're very welcome. It is a pity that my original post was down voted to zero, which means that fewer would have seen it. However if I have introduced Servoy to you, and it works for you, then that was the point of the post. Good luck with your endeavours too. If you need advice, or for me to put you in touch with the Servoy community, PM me. It would also be worth signing up to the forum. Introducing yourself as a new user, and asking anything that initially seems tricky or confusing. It is worth learning it (I know this from lots of clients paying me to do this work) Good luck with it.
Use the management studio to rename the column, look at the change script it generates. http://tinypic.com/view.php?pic=33e7ite&amp;s=5#.Uv99T_ldXfI additional reference: http://stackoverflow.com/questions/2431901/forcing-management-studio-to-use-alter-table-instead-of-drop-create (Note: Understand what this does because if your table has silly triggers/lots of data/can't be offline, etc etc etc)
I disagree and use row number all the time. It is not less readable/maintainable unless you're working in a mixed shop.
That is not correct and redundant. If harddrive and manufacturer is in the same table and is denormalized. There is no need for the subquery. Select manufacturers From computers Having Min (harddrive) &gt; 256 Group by manufacturers 
If you know your schema and your schema is good, "NOT IN" is great. My schema is legacy and quirky so there's always a risk of unexpected NULL values on either side, and NOT IN does not behave the way I expect. EXISTS and NOT EXISTS is more reliable for me. OP, your question indicates that you're not realizing that these are subqueries exactly. Think of your subqueries as if they are their own queries and make sure that you understand them first. Use parens. SELECT manufacturer FROM computers WHERE NOT EXISTS --this will always be true ( --this subquery returns no rows SELECT harddrive FROM computers WHERE harddrive&gt;256 AND NOT EXISTS --this will always be false ( --this subquery returns rows SELECT harddrive FROM computers WHERE harddrive &lt;=256 ) ) The deepest subquery definitely returns rows. That means NOT EXISTS will be FALSE, always. That means your middle subquery will return zero rows, always. That means that your top level NOT EXISTS will always be true and you'll just return every row from computers always. You clearly want the results of those subqueries to be different depending on which manufacturer you're talking about. That means you must specify their relationship with aliases. It's a little strange that you don't have a "manufacturers" and a "harddrive" table to work with here, but this is where we'd go with one big denormalized table: SELECT manufacturer FROM computers c1 WHERE EXISTS ( SELECT harddrive FROM computers c2 WHERE harddrive&gt;256 AND c2.manufacturer = c1.manufacturer ) AND NOT EXISTS ( SELECT harddrive FROM computers c3 WHERE harddrive &lt;=256 AND c3.manufacturer = c1.manufacturer ) GROUP BY c1.manufacturer Ya dig?
Explain please
explain what? first normal form? is teh google broken for you?
IF you are still not clear about CTE... go through the below link.... it contains very basic explanation of CTE.... http://knowitbasic.blogspot.com/2013/09/cte-in-sql-2005.html
I'd have thought workbooks&lt;&gt;sheets&lt;&gt;cells would translate pretty well into a database schema; what do you mean this is not what SQL is for?
The original Excel binary format is an object stream, one object per cell (not storing the sheet like an array). If you had to model this in SQL, it would be no different. One row per cell, with meta data about the formula and perhaps another table to store the 1-&gt;M relationship of cells its referencing. You'd use this references table to resolve the formula when something changes, and also identify circular references. You could possibly use the 'nested sets' concept, or some databases provide features specifically for recursion or hierarchical data types. This would make it easy to see which cells need recalcing, since they'd be modelled as a tree of child references from the cell being updated. 
Step one is figuring out what the original query does before changing it. If we remove `DISTINCT` the query would return the `CategoryName` for every product in the database. So adding `DISTINCT` we would get the list of categories with at least one product in them. So let us write a query which does that. The query to get all categories is. SELECT CategoryName FROM Categories ORDER BY CategoryName Now we just need the add a `WHERE` clause which filters out all categories without any product in them. The below query gets the category IDs of all products. SELECT CategoryID FROM Products We could add `DISTINCT` to remove duplicates but we do not need it. Instead we will combine the two queries. SELECT CategoryName FROM Categories WHERE CategoryID IN (SELECT CategoryID FROM Products) ORDER BY CategoryName
that won't do it, sorry, it has to be based on categoryid instead SELECT categoryname FROM categories WHERE categoryid IN ( SELECT categoryid FROM products ) ORDER BY categoryname 
Oh. I thought OP needed to make another query that used the list of category names generated by this query. In that case, I think this looks like a homework problem. OP?
oh, no doubt it is homework, the clue was "asks me to turn [query] which is a join statement, into a sub query" 
I *probably* wouldn't. I'd also hesitate to model a Word-like word processor or a Powerpoint-like slideshow as a SQL schema.
I can't help you with your SQL question - and I know that criticizing spelling on the Internet makes me an ass but so be it, I'm an ass. Since your question was specifically about making yourself marketable, I will recommend looking into the difference between compliment and complement. It might someday mean the difference between getting the job or missing out. Edit: I totally accept any downvotes. Keep 'em coming.
&gt; I will recommend looking into the difference between compliment and complement that's worth an upvote, no sarcasm intended 
If you're going to try to learn database interactivity in Java, I'd recommend MySQL. You'll learn how to use JDBC and a common database that's used by multiple languages, not just Java. Learning Oracle is valuable, but it doesn't have to be THE database you learn how to use.
When it comes to databases, the stack is the least important. You really need to learn SQL the language, and underneath that, relational algebra and tuple calculus to understand the way in which data and queries are represented. Once you know how databases and SQL work, then you can learn the intricacies of different stacks. However, learning a new stack is a lot quicker of a learning curve than learning databases themselves. I hate to say it, but you're far, far away from becoming a "database developer / analyst", it especially won't be doable ASAP. You need several courses in databases, you need to work with them yourself, you need to intern as a junior database administrator / developer, and after a few years, you might look into a job as a developer / analyst. 
Dear Java, You're awesome! Love, SQL Ok, serious answer. Doing straight up JDBC from Java is a pretty outdated approach. The trend now, if you're not ditching SQL altogether for NoSQL, is to use JPA (Java Persistence API). You use annotations in your code to describe your entities and this gets mapped to an RDBMS. Before JPA, people used a Hibernate or iBatis to accomplish the same. Not sure if there's an equivalent approach in C#.
Agreed. You can learn the specific nuances of Oracle syntax later. You can learn the basics with pretty much any database.
The problem is that if this person skips learning how to write basic SQL and skips straight to ORM, they'll be missing some key fundamentals. ORM is good for abstracting away the details of the underlying data store, but shouldn't be used to gloss over how databases work.
ORMs are in general pretty crappy for complex queries or large queries.
That's a good idea... I only asked because in both cases, there are millions of records coming back. I wasn't sure if one way was inherently better than the other. Thanks for your input!
... which unfortunately, I am. :(
Good point. I just felt like I was only aggregating on the max of one field, so maybe there was a better way to do it - I just wanted to make sure.
&gt; The shorter one is easier to read and understand its intention. This is probably the most important feature. Intent was the only reason I second-guessed myself in the first place.
I've never used a CROSS APPLY before - I think I'll give it a shot and see how it works!
Hi, here's a neat little example for you, using the package DBMS_METADATA_UTIL that might help you achieve what you need: http://www.dbaxchange.com/3_4_longdatatypes.htm
Excellent, looks perfect. I'll check it out in the morning. [You help so many...](http://images.onlyfatrabbit.com/uploads/2014/01/funny-lolcats-cats-kitten-depression-red-dot-photo-pic-humor-joke-meme.jpg) I hope you realise you're invaluable.
Thank you kindly :D
I mean, that's what Google Spreadsheet is and does, basically. My question was therefore more about how they do it from the database side than whether it is a good idea at all. Typically, for Google it *probably* was a good idea to buy/make such a service after all.
There are actually a lot of good red dot pics and gifs. I didn't do well enough there... apologies. Next time maybe.
&gt; that's what Google Spreadsheet is and does Google's spreadsheet uses a SQL database internally? I doubt it. Show me.
NUMBER is not a valid mysql datatype -- [pick something else](http://dev.mysql.com/doc/refman/5.0/en/data-types.html) 
Just to add a little to this very good explanation, you will want to be careful with one more thing regarding the table in the WHERE clause: if it is possible for the category field in the products table to be null then you will be better off using an EXISTS clause instead of IN. Either that or use NVL in your WHERE...IN sub-query.
Well, easy way, but slow since if the number is indexed, the convert will make the index useless : declare @tableFullValue table (id int identity, number bigint) declare @tableToJoin table(id int identity, numberPart varchar(14)) insert into @tableFullValue values (12345678901234), (12222222222222), (12000000000000) insert into @tableToJoin values ('12') select * from @tableFullValue t1 inner join @tableToJoin t2 ON convert(varchar(14), t1.number) like t2.numberPart +'%' 
here is what you can do, if you have the column indexed in the table containing the number as bigint. select * from @tableFullValue t1 inner join @tableToJoin t2 ON t1.number between convert(bigint, t2.numberpart + replicate('0',14 -len(t2.numberPart))) AND convert(bigint, t2.numberpart + replicate('9',14 -len(t2.numberPart))) select lowerLimit = convert(bigint, t2.numberpart + replicate('0',14 -len(t2.numberPart))) ,upperlimit = convert(bigint, t2.numberpart + replicate('9',14 -len(t2.numberPart))) from @tableToJoin t2 If you have an index on what in this example would be @tableFullValue, a convert to varchar will make the join predicate a "non sarg-able". Which means that it will not be executed as a SEEK, but as a SCAN. A Scan will run trough all values contained in the index / the table, so, thats not what you want generally speaking. By converting the "to be searched" partial number to a two bigints, lower and upper limit for a BETWEEN, the index can be used in a SEEK, thous it should be a lot faster. It does however kind of lives on the assumption that all the numbers you are searching are 14 digits. You could make the replicate('0',**14** -len(t2.numberPart)) variable, but I doubt you will see much benefit then. I hope that was somewhat understandable.... /edit: bit of explaining....
Ive a fairly new sql user...so I might be missing something...I sucessfully created my two tables...inserted what I wanted in there...and on the SELECT all query it is giving me the error " + use not valid "...I am assuming its having an issue of using + in the join any ideas?
which DBMS are you using? (postgree, mysql, mssql, oracle, sqlLite etc)
thank you so much!
looks like you are running DB2. What you can do on DB2 would be Query 1 : ON LEFT(convert(varchar(14), t1.number),length(t2.numberPart) = t2.numberPart Query 2 : ON t1.number between convert(bigint, t2.numberpart + REPEAT('0',14 -length(t2.numberPart))) AND convert(bigint, t2.numberpart + REPEAT('9',14 -length(t2.numberPart))) I don't have a DB2 test system, so well, not much more I can do.
you have really helped so much! thank you for taking the time to work with me!
Hey with you help I was able to figure this out!! thank you so much for taking the time to help me!!
you're welcome ;)
I would suggest adding a new column that holds the first two digits. You can fill that with a trigger on update/insert. Much much faster then anything you do to those numbers to get the first two digits. The other option is to use a varchar for the 14 digit column (depending on what you do with the data) and simply join on left(WILDCARD,2). In both cases you get the full use of indexes.
Is everything 1 - 2 - 3 like that? Storing these as varchars if they're actually just a few options from a small, predetermined set that will generally not change isn't optimal. If your system supports enum types (recent versions of most of the big names all do), maybe consider having those columns enum types? It'll also help with consistency (an errant space in a string in application code would be spotted by your DB using enums, varchars will put it right in and consider it a fine, valid "new" data), but you could maybe use the enumeration index (in postgres you'd join on pg_enum) of the users selection as it's score and call whatever summing you want to do on that (you just want the numeric sum of all these choices for each row? or something you'll aggregate into an average per employee over a date period maybe?). Depending on the size of your data and how often you want to do this aggregation, though (occasionally for reporting versus a common real-time value you'll be looking up and using elsewhere), calculating it on the fly might be impractical, and if you want more complex values ("very satisfied" is an 8, "satisfied" is a 4, "unsatisfied" is a -2) you'll have to do something more sophisticated like response-to-value lookup tables or potentially unweildy case statements or column-specific math in your queries. In that case, you'll want to think about some precalculated storage like an aggregates table or something in your application code that periodically requests this and chaches/manages it itself. So, tough to say without more info. EDIT: I just saw you said "I have a table which stores all pre-determined scores and its meaning to the database.", maybe you're talking about a repsonse-to-value lookup like I mentioned above?
Thanks, it's is always nice to read a someone else's take on SQL subjects. Broadens then knowledge. I would be keen to see a follow up post showing how to do an UPDATE statement using CTE.
The reason the predicted cost is off is because the predicted returned rows from the join is off. This estimate is off in both plans. I generally look at predicted vs actual rows first because that is the usual reason for the planner to chose a suboptimal plan. Hash Join (cost=107.06..443578.92 rows=*3362075* width=8) (actual time=2521.275..248683.459 rows=*1934* loops=1) Nested Loop (cost=0.00..493645.84 rows=*3362085* width=8) (actual time=151.505..11587.339 rows=*1934* loops=1) PostgreSQL for some reason expects the 1934 rows from alarm_event_probe to match with 3362075 of the 7772408 rows in the partitioned table, and that is what you need to fix. I am unfamiliar with partitioned tables and currently too tired to figure out how to do this, but I hope this insight is useful anyway.
nvl(fieldname,'Other') 
COALESCE(nameA,'Other')
you can also use ISNULL(nameA, 'Other')
Thank you everyone. I've successfully been writing my own queries and it's AWESOME. Gotta try that game next. :)
This is more applicable to NOT IN. However, it still might be better to do EXISTS if the outer query is more restrictive than the inner query. 
SQL alias just lets your client machine connect to a SQL Server by a name other than host\instance (server1\inst1) or host,port (server1,5555). The main usage I've seen is with legacy or some 3rd party apps that have a hard time parsing a SQL server with an instance/port. One thing that trips a lot of people (and vendors) up is that the SQL Alias goes on the *client machine (the one connecting to SQL)*, not the SQL host itself.
Hmm, you just gave me a wonderful (or evil) idea. Thanks! 
I think you mean asymmetric route not asynchronous. 
Again, 5 packets lost wouldn't cause an issue. MSSQL communicates over TCP, so any lost packets would be re-transmitted. If it were communicating over UDP, you'd have the answer as to why the packet loss caused an issue, but TCP can handle packet loss.
Why not just use a DNS record?
I'm not sure you understand how DNS works? If i have a SQL server named db1 and a DNS cname record for pinkfluffybunny whether i try to attempt to connect to db1 or pinkfluffybunny the port it tries to connect to will be the same regardless.
thats...what i am saying. sorry maybe i wasnt clear.
What I meant to say, DNS entry, in this case, will let you set "pinkfluffybunny" in the sharepoint installation, BUT if the SQL instance on that server is sitting on port 2000 instead of 1433, sharepoint, by default because microsoft is super smart, will call pinkfluffybunny at 1433 and will get rejected. the alias set up that way, bundles the two together. so you can setup pinkfluffybunny=db1,2000 and have the local box treat it as a dns entry but use the non-default port (the one specified in the alias) edit: its a hacky workaround needed to help microsoft with its own sharepoint app. like i said its the only app ive encountered so far when this was needed.
My fault, First rededit post for me. Can I edit it? 
no, you can't, but don't worry, we understand now :)
One good thing about using an alias is that the server admin can configure as needed, does not require domain admin, and you have knowledge of each server that is using the alias. One thing that can get annoying is having to make sure that all of the servers are using the same alias (at least with SharePoint and SQL) and some outages can occur while in the process of updating. The good thing with the a DNS record is that it is centrally managed but any server can use the alias without knowledge of which ones and you cannot reference a specific instance of the SQL server like you can with a SQL alias. 
Our word press site that is hosting my link is all of a sudden hosed...hopefully it will be back up shortly sorry :(
It is back up! Thanks for all the feedback!
Sorry, I think I made it too difficult. here's what i ended up with select E.employee_id AS 'Employee ID', E.emp_firstname as 'First Name', E.emp_lastname as 'Last Name', M.employee_id as 'Manager ID', M.emp_firstname as 'Manager First Name', M.emp_lastname as 'Manager Last Name' From hs_hr_employee E, hs_hr_employee M, hs_hr_emp_reportto R where E.employee_id = R.erep_sub_emp_number and M.employee_id = R.erep_sup_emp_number and R.erep_reporting_mode = 1
Yes, and to illustrate the differences between databases for things like XML here is how I would do it in PostgreSQL. SELECT sum(replace(amount::text, '$', '')::numeric) FROM unnest(xpath('/root/*/text()', '&lt;root&gt;&lt;node1&gt;$10&lt;/node1&gt;&lt;node2&gt;$20&lt;/node2&gt;&lt;/root&gt;')) AS amount;
I include a section about how to set up SQL alias in all my install guides.
I wanted to make him guess what syntax i was using tbh.... mine was t-sql
Haha, I actually thought you wrote that in your post. Maybe I am just too used to coding SQL.
we should have a beer sometime, tell the horror storries arround the camp fire thingy :P
I'm no SQL expert, but I believe MS SQL and Oracle SQL both use alias'.
&gt;sorry i dont know how to display code properly on reddit. press spacebar 4 times to begin code Double enter for a paragraph
Well, obviously. It would need the schema (or possibly the query plan).
Think of the diagram as a pseudo code for SQL. Use case is when you get a statements from someone else with as many as 10-20 tables joined together and you need to see what the other person is doing. Laying out the 10-20 tables in a diagram helps with the understanding and plan changes.
this was already setup on the system, i'm not much of a SQL guy. EXEC msdb.dbo.sp_send_dbmail @profile_name = 'TEST', @recipients = @HTMLEmailRecipient, @subject = 'SUBJECT NAME', @body_format = 'HTML', @body = @emailhtml if i create another variable and add more html code could i add that varible to the @body and put them together ? tried the below it does work @body = @emailhtml @emailhtml2
It should be @body = @emailhtml+@emailhtml2 However you should be aware that the maximum limit on the body is i believe 8000 characters.
Is there now way to get around that limit ?
You can try googling around but I don't think there is. 
First thing you need to know is that in SQL: there is no order in a set. So saying "I want the first 7 percent" does not compute, first 7 percent of *what*? You need to order by a certain column, maybe a DateInserted column if you have that. If you don't specify an ORDER BY, any ordering you see is an artifact of the query optimizer's strategy So this would become: SELECT TOP 7 PERCENT * FROM YourTableName ORDER BY DateInserted ASC/DESC;
why don't you filter the data to be pivoted already in the subquery? SELECT * FROM ( SELECT Year(ind) AS [ayear], LEFT(Datename(month, ind), 3)AS [amonth], Isnull(tgvh1, 0) AS Amount FROM _d WHERE (cug = @CompanyUserGUID OR @CompanyUserGUID IS NULL AND cus = @CompanySupplierGUID OR @CompanySupplierGUID IS NULL AND ug = @UserGUID OR @UserGUID IS NULL AND cg = @CompanyGUID OR @CompanyGUID IS NULL) AND &lt;insert additional filters here&gt; )s PIVOT ( ....
I couldn't get this to work on my workstation. The graph would never come up when I hit generate. Is there a newer version? Also, it says I need an account for more than 800 character statements. Is this free? Why do they require an account? Looks useful, I just want to know if it's really worth spending the time to try to get it to work.
Okay I'll give the online version a shot then! :)
Hardcode a x12? That doesn't get you around Holidays though. It may be time to stub out some lookup tables.
I'm normally asked to calculate working days (not hours) but what I tend to do (using SQL Server) is: 1. Create a calendar table. This holds a row per time unit (days in my case) with a column called something like is_working which is either 0 or 1. Populate it from 1980 to 2070 (or whatever) and put in the stuff like Easters, Christmas, etc. 2. I then create a UDF called something like udf_WorkingDaysBetween which takes two parameters - "Start" and "Finish" (both datetimes). All that does is return the result of this sort of query: SELECT SUM(C1.is_working) FROM dbo.Calendar AS C1 WHERE C1.the_date BETWEEN @StartDate AND @FinishDate I can then calculate working days on demand by just using the udf. It isn't the most efficient way of handling it but it's fast and reliable. If performance is a concern (for most reports I find it isn't) then just inline the query and dump the function. I love calendar tables because it makes a lot of date calculations so much easier and allows you to easily include/exclude date or times on request. For example, a couple of years ago we had a public holiday in the UK because some aristocrats got married (aka the Royal Wedding. Utterly impossible to calculate that sort of event since it's the whimsy of politicians, but we needed to exclude that day from any of our "working days" calculations. Now, whether this approach is viable for you depends on how accurate you need to be. You can easily create a row for every hour, but doing it for every single minute would feel weird.
/u/svtr is right. I'd either use a CTE or subquery and filter inside of there, prior to hitting the PIVOT step in the outer clause.
the downstream application belongs to the gov-ment. ;D 
That's right - I accomplished this by creating separate SELECT segments for each of two root node elements, giving them the name what would be the element, and forcing them to the bottom. Then, I artificially added the root names at the end with, "EXPLICIT, ROOT('Submission')". So it worked, but it feels dirty.
It might be the only solution. 
As SaintTimothy mentioned, a lookup is probably going to be the final answer. If it comes down to them absolutely requiring it, I'll be going this route. Thank you!
Ya, it is dirty that this is even an issue. The only cleaner way I could see would be to add a recursive level counter to the selects and sort by it as required; with root elements having a value of zero 0. Edit - This is assuming you generate it with a recursive query.
Trick becomes maintenance. Good news is... look up tables work super quick when compared to relying on date functions. http://sqlblog.com/blogs/adam_machanic/archive/2006/07/12/you-require-a-numbers-table.aspx http://dba.stackexchange.com/questions/11506/why-are-numbers-tables-invaluable
Thanks for the feedback - I wanted to make sure that I wasn't missing something inherent in the function. I think I tried what you're suggesting - sorting the values in very specific ways, but the for xml function was too particular on the universal header structure. 
export the excel sheet to a csv file import the csv file to a table (which you will drop later) do a joined update to your permanent table
so your saying you did this... SELECT * FROM (SELECT Year(ind) AS [ayear], LEFT(Datename(month, ind), 3)AS [amonth], Isnull(tgvh1, 0) AS Amount, **&lt;insert GUID field&gt;** as USERGUID FROM _d WHERE cug = @CompanyUserGUID OR @CompanyUserGUID IS NULL AND cus = @CompanySupplierGUID OR @CompanySupplierGUID IS NULL AND ug = @UserGUID OR @UserGUID IS NULL AND cg = @CompanyGUID OR @CompanyGUID IS NULL)s PIVOT ( Sum(amount) And it didn't break down the pivot per user?
Cleanse the number formats using excel functions if they are not already Use excel concat function to wrap with single quotes and add a trailing comma Copy newly formated data into SSMS wrapping it with a set of () amd ise our handy in function Select * from table where phone in (new list pasted here)
I really don't like job listings that give no salary range whatsoever. I am fully aware that everything is negotiable but if someone is looking for 115k and your budget is 55k and that doesn't come up until several interviews later, then everyone's time is wasted. Give a wide range if you must, but preferably a max.
&gt;The employee is occasionally required to stand; walk and stoop, kneel, crouch, or crawl. Nice.
A page could have only one article but article can hold multiple images. Basically I'm making a canvas page, which has like 2 text views to write/edit data and a picture view to import pictures. A user can have multiple pages but each page contains one article which has images and text. So, I'm trying to parse content in Article and Image tables using newspaperID since it's unique for each page. Do you believe you could help me if it kinda make sense..? 
Yea but I'm not totally clear on how you want the results presented. Can it be two separate queries? If you want a single query, can you accept having multiple rows with the same Page and Article, if there are multiple images?
So, I believe single query will make more sense as you mentioned. Thank you :)
Here's a result for the latter, making a few assumptions about the table relationships. If there's atleast a Page and Article, this will return atleast one result regardless of whether there's any images. If there's multiple images, the Page/Article will appear multiple times. SELECT * FROM dbo.NewspaperPage p INNER JOIN dbo.Article a ON p.newspaperID = a.newspaperID AND p.pageNum = a.newspaperPageID LEFT OUTER JOIN dbo.Image i ON p.newspaperID = i.newspaperID AND p.pageNum = i.newspaperPageID WHERE ... [Pastebin](http://pastebin.com/D3Xc31fz) for easy copying/reading
They take data mining literally? 
Might be easier for us if you explain what that line is ...
sorry the ----- separate the columns. - The date is my primary key. - For each record I need to calculate the % value change from the previous recorded date. * I hope that is somewhat clear.
Just commenting to make sure you saw my [reply](http://www.reddit.com/r/SQL/comments/1yejyj/sql_server_query/cfjtufc)
hmmm... just checking but, for the error "on the last line" do you mean the WHERE clause that you filled in? Make sure you specify the table alias for whatever column you are filtering on. In this code, the Image table data is the only data that should possibly NOT show up, I don't know what you mean about it being the only contents... did you change the SELECT portion?
Yes, I searched by id - WHERE id = 1. Although, I noticed as I was rewriting the query, its not showing the available options like different tables. I might have to restart PC. I might have misunderstood you, I was thinking that I would be able to able to see everything in Article and Image table using newspaperID. I'm thinking in C# I would be able to use newspaperID and parse every item using that id. I really appreciate for your help :) 
The reason you are getting an error is because you don't have a column named 'id'. You have one named newspaperid and another named newspaperpageid. Since you are interested in everything for a specific page number, I think your where clause should be written as below: WHERE p.pagenum = 1
No problem... and yes, you should be able to see everything in the Article and Image tables based on the newspaperID or pageNum or whatever the field is that you are filtering on.
Pretty sure any DBA who can also BI and Web could telecommute for a firm in DC and make north of $125k/yr. That's a rare breed of super duper right there.
This means to assign a value of 1 if the column is null or 0 is it isn't. Imagine an extra column in your date that reads 1 if the value of MyDate is null or 0 if is has a value. Then you sort by that value. 0 is lower than 1, so alls the 0s come first. All the 1s (I.e. nulls ) come last. Edit: see here http://technet.microsoft.com/en-us/library/ms181765.aspx
`case when MyDate is null then 1 else 0 end` produces a single value for each row of either 0 or 1, depending on if `MyDate` is null or not. So you're sorting by this value first, which puts anything that's null behind anything that's not, then you sort the non-null rows by the actual value of `MyDate`. The row with nulls won't be in any particular order, but they'll all be behind the non-null rows.
All you need is the close apostrophe on the date literal and you're good. Use it all the time. 
Heyo, there's no DISTINCT in there? Do you have a good index on caller_id_phone_num?
Thank you.
The reason it does not work is because the csv file will be written by the database user. I would recommend either finding a location where the server is allowed to write or finding a way to generate the CSV client side. I have no idea how that is best done since I am neither a MySQL guy nor a Windows guy.
IsNull(), Coalesce(), and the case-statement will do very much the same thing. Unfortunately you have to use one of these functions that conditionally replace the null value because aggregation and boolean operations are not valid on null (or unknown) values. It's worth mentioning that in a larger table it's worth your while to figure out a way to avoid using functions like this in the where clause itself (or joins) because the function will force a table-scan...
You might want to use a dense rank, so you don't miss any.
Solution: stop printing your reports
I'm assuming you mean tables since schema in PostgreSQL means something different. [Here's the tables in an easy to read form.](http://labouseur.com/courses/db/cap2.pdf) 
something like this: select c.name c.city from customers c where c.city = ( select top 1 p.city from products p group by p.city order by sum(p.quantity) asc )
Sorry to bother but "top 1" is giving me a syntax error at the "1"
i don't use postgre, but mssql. try google.
That's because its T-SQL syntax. You want to do something like: Select value, count(value) val_cnt From some table Group by value Order by val_CNT desc I 
All of them are generic. 'ab1.5', 'ab3', 'ab4' etc
what happens if you do amt*substring(commission_code,3,len(commission_code) ? If the prefix is ab as you said, or change 3 to how many chars in front + 1. You might have to cast/convert the substring part to float. 
Personally I'd just use a null replace function to chop off the 'ab' bit if that's going to be consistent. =Replace([ColumnName],'ab','')
First of all assuming that only active worker-employees tuples are listed in the WORKS_ON table, your inner join between the tables EMPLOYEE and WORKS_ON seems to also filter out those employees who are not assigned on any project. Does your task require you to select only those employees who are assigned to at least one project? Secondly, your WHERE conditions filters out by project number, it seems that you actually want to filter out by ssn. So it should go like SELECT DISTINCT e.ssn, e.lname FROM employee e, works_on w WHERE e.ssn = w.essn AND w.essn NOT IN (SELECT w1.essn FROM project p, works_on w1 WHERE p.plocation = 'Houston' and p.pnumber =w1.pno) ORDER BY lname; Note: Please for the love of holy, please always use aliases :-)
You can download SQL Server Management Studio (SSMS) [here](http://www.microsoft.com/en-us/download/details.aspx?id=29062). If you have that installed, you must attach the adventure works database (once you unzipped it). If the [official link](http://msftdbprodsamples.codeplex.com/downloads/get/478214) doesn't work, maybe it works from my [dropbox](https://dl.dropboxusercontent.com/u/25900407/AdventureWorks2012_Database.zip). If you're looking to better yourself with SQL stick around this subreddit and check the sidebar. The comments on various threads are filled with information by all sorts of SQL devs: If someone asks for a script there will be replies with queries and then people will reply to those with even better queries! There's a lot to learn. Oh yea: "select ' your text'" 4 space for text to get code
The order by clause orders by two different values. The primary ordinal is either a one or a zero. The CASE defines any record with a NULL MyDate key to be ordered by the constant 1, otherwise it will be ordered by the constant 0. The default is an ascending order and as 1 is larger than 0 then the primary ordinal immediately sperates NULL records from non-NULL records, placing the NULL ones after the non-NULLs. The secondary ordinal then orders the records within the ordering defined by the primary ordinal (NULLS after non-NULLS), which orders the non-NULL records according to their MyDate value, in an ascending order. At no point is the value of 1 or 0 used for anything other than seperating NULL and non-NULL records.
Thanks for replying again. Not going to be possible to install anything on the work laptop, they're pretty heavily locked down. If i can get permission to connect to *any* server internally, will I then be able to attach and mess around with adventureworks etc? Alternatively, is there anywhere I can download a few sdf files I can load via a compact edition server type?
&gt; If i can get permission to connect to any server internally, will I then be able to attach and mess around with adventureworks etc? No, you need certain SQL security roles to detach/attach and mess around. If you can't install software on your laptop or the IT department doesn't via a ticket, chances are you will also be denied those permissions. [This file](http://go.microsoft.com/fwlink/?LinkId=212219) should install the .sdf Northwind database.
I liked the blog post. Thanks.
this was an excellent tutorial, with writing that is far ahead of some of the stuff we've seen posted on /r/sql recently providing the complete script at the end was a nice touch more will be welcome oh, and by the way, for mysql users, the same functionality as t-sql's MERGE is available in the ON DUPLICATE KEY UPDATE option of the INSERT statement
Thanks for reading it!
Thanks a lot, I appreciate it! I have no shortage of things to write about, so I'll just keep 'em coming as I have the time (and motivation). &gt; providing the complete script at the end was a nice touch Yeah, one of my biggest gripes about most SQL tutorials is that you have to do a bunch of ground work to get the example code to run, so I've decided to do something about it. Good looking out with the information about upserting in mysql, by the way.
I would first look at the data that is going in that specific row, there might something wrong with the data for that row specifically. Another thing I would do change this setting. [Row setting](http://i.imgur.com/WVm4uID.png) Turn off height increase and see if that makes a difference
Thank you, this was exactly what I was looking for edit: I would use aliases but my instructor told us not to for easier grading
MERGE is all well and good, but remember that not everyone is on the same MSSQL version as you. This command only works in 2008 and above. That might be something you want to put near the top of your article.
Ah thanks, good catch!
Your welcome.
Stolen from here: http://stackoverflow.com/questions/1498648/sql-how-to-make-null-values-come-last-when-sorting-ascending In standard SQL (and most modern DBMS like Oracle, PostgreSQL, DB2, Firebird, SQL Server, Apache Derby, HSQLDB and H2) you can specify NULLS LAST or NULLS FIRST: Use NULLS LAST to sort them to the end: select * from some_table order by some_column DESC NULLS LAST
Thanks!
Not sure how much effort you are willing to put into it and privacy issue consideration but you could use one of the cloud databases for example Microsoft Azure and Google Cloud.
I am assuming this is MS SQL. Can you use hints to trigger parallelism? Can you look at the execution plan for the query? Both estimated and actual? You should see parallelism in there as long as your MaxDOP settings and threshold are set correctly. http://technet.microsoft.com/en-us/library/ms181714.aspx http://technet.microsoft.com/en-us/library/ms190322.aspx
A functional dependency answers the question, "Given one value for *this* set of columns, do I find one and only one value for *that* set of columns?" It's fundamental to normalization. (And, by extension, to denormalization.) So, in your User table, it *looks* like id -&gt; email Read that out loud as "id determines email". It means that if I know one value in the "id" column, I find one and only one value in the "email" column. It also looks like email -&gt; id You've also said &gt; . . . an Address can belong to one user and one user only which would imply {street, city, state} -&gt; id {street, city, state} -&gt; email Your first job is to list all the functional dependencies that you believe are supposed to hold, given your current understanding.
&gt; And I'd like to denormalize the address into it's own table. why!!!??? i've seen this more times than i care to remember, and it drives me nuts invariably, address does ~not~ need "normalization" the rule of thumb for me is whether you care about an address *regardless of whether there's anyone at that address* -- if the answer is no, and you're only interested in addresses that belong to people in your database, then address should ~not~ be a separate table, address should simply be attributes of whatever person table you have
This was actually a contrived example. The tables aren't User and Address, but I used User and Address to help illustrate the dependent relationship better, since it's a more familiar situation. The actual tables are Facility and Census (a facility has a single census, and a census can only belong to a single facility). I appreciate the response though, and it's helpful information. I suppose it's even applicable in my case. 
Ahh, that makes a lot of sense, thanks for explaining it so clearly. 
&gt; This was actually a contrived example. i friggin ~hate~ it when people do that more often than not, it causes others who might wish to help you to waste a lot of time on a scenario that isn't what you're really doing please, next time, tell us your real scenario 
You might want to use Row_Number instead of Rank depending on what you are expecting. Rank will give the same rank number for duplicate rows where as Row_Number will not. If you are expecting no more than 10 rows ever, you'd be safer using Row_Number. if Object_ID('NumberTest') is not null drop table NumberTest create table NumberTest ( Number Integer not null, LastAccessDate DateTime not null ) go insert into NumberTest select top 10000 Abs(Checksum(NewID())) % 100 Number, Convert( DateTime, Convert(VarChar(10), DateAdd(Day, Abs(Checksum(NewID())) % 65530, 0), 101) --Date + ' ' + Convert(VarChar(2), Abs(Checksum(NewID())) % 24) --Hour + ':' + Convert(VarChar(2), Abs(Checksum(NewID())) % 60) --Minute + ':' + Convert(VarChar(2), Abs(Checksum(NewID())) % 60) --Second + '.' + Convert(VarChar(3), Abs(Checksum(NewID())) % 1000) --MilliSecond ) LastAccessed from master..spt_values T1 cross join master..spt_values T2 go select A.Number, A.LastAccessDate, A.RowNum from ( select sA.Number, sA.LastAccessDate, Row_Number() over (partition by sA.Number order by sA.LastAccessDate desc) RowNum from NumberTest sA ) A where A.RowNum &lt;= 10 
Maybe example is the wrong word. This is the real scenario, with table names changed, which doesn't affect the answer but can help people not familiar with my models You know what I hate? When people answer the question they think you should have asked, not the question you asked. The question was about structuring the foreign keys. Not whether I should denormalize something. But if it helps, I'd be happy to /s/User/Facility and s/Address/Census 
You need to join on the definition, not the alias. Like this: LEFT JOIN CmsUserRate ON CmsUserRate.USERID = CASE WHEN InvoicesHdr.OrderUser = '' THEN InvoicesHdr.InvoiceUser ELSE InvoicesHdr.OrderUser END
I have narrowed it down to the merged columns on the left where it says Family, mom/child. I've decreased the font, change to column name to a shorter name, turned off the height increase and have even tried moving it. It appears it is always the last row associated with those to two columns which expand. The data doesn't take up much space for Family or mom/child so I don't know why it keeps on printing like this. If I remove those columns completely it prints fine. However, I need the columns spaced like that for the reviewer. Any ideas where to go from here?
http://en.wikipedia.org/wiki/Database_schema more generally it refers to the structure / relations of the database
It populates with "Family" for the first column on the left and then on the right "mom" or "child" depending on the client.
what does your expression look like for the field? Could it be possible that the field is not wide enough for the values? Especially if the family's name could be large? 
Awesome - worked great! Thanks!
The expressions are Fields!Family.Value and =Fields!Label.Value. It's pretty simple and just calls the value from the stored procedure which are either Family, mom, child. There is more than enough space for the data values. 
The optimizer is like calculating the efficiency of using 4 CPUs and figuring it would be quicker to run on 1 CPU. Just because you have 4 available doesn't mean it is smartest to use it. What does your timing stats look like on 4 CPU vs 6 CPU? 
Can't you grab the min and max in a subselect and then use those values to do the comparison? 
I am not sure I understand what you wish to accomplish. Why would one want to remove the middle value of a set? What if there are an even number of elements in the set of rows? Can you give me an example of the output you expect? The solution probably involves window functions though. 
 --First create the table create table Billing( ID number, Dates date, Status varchar2(100), CostID number ); -- Insertion of data begin insert into Billing values(1,'01-Jan-2014','Drop',1); insert into Billing values(1,'01-Jan-2014','Bill',2); insert into Billing values(1,'02-Jan-2014','Hold',3); insert into Billing values(1,'02-Feb-2014','Hold',4); insert into Billing values(1,'02-Oct-2014','Bill',5); insert into Billing values(1,'02-Nov-2014','Hold',6); insert into Billing values(1,'02-Nov-2014','Bill',7); insert into Billing values(1,'02-Dec-2014','Closed',8); end; --This Query gets the ROWID of every record with Status= HOLD that is preceeded by another HOLD record -- in this case we get the ROWID of record 4 because it is preceeded by record 3 which is also with STATUS = HOLD select b1.ROWID as row_id, B1.COSTID, B1.DATES, B1.ID,B1.STATUS from billing b1 inner join billing b2 on b1.CostID = b2.CostID+1 where b1.status = 'Hold' and B2.STATUS = 'Hold'; -- Than just remove that ROWID when selecting select B.* from billing b where rowid not in ( select b1.rowid as row_id from billing b1 inner join billing b2 on b1.CostID = b2.CostID+1 where b1.status = 'Hold' and B2.STATUS = 'Hold' ) order by 4; drop table BILLING;
If I understand your problem correctly, I think you can achieve the result you need with the help of analytical functions. ROW_NUMBER, DENSE_RANK, LAG, take your pick. One method (using LAG) is like this: select m.id, m.date, m.status, m.costid from ( select l.id, l.date, l.status, l.costid, lag(l.status, 1, 0) over (partition by l.id order by l.date) as last_status from my_table l ) m where m.status &lt;&gt; nvl(m.last_status, 'aaa') or nvl(m.last_status, 'aaa') &lt;&gt; 'Hold' Basically what this does is exclude all the rows (within the same ID, which I assume is a transactional representation, like an order) where the status = 'Hold' and the previous status is 'Hold' as well. Also, that date field ought to contain an hour and minute element so that you can be sure that you have the events in the correct sequence. Mind you there are always at least a couple of obvious methods for solving something, so keep exploring.
Yeah I know, thanks. The real CASE statement is much longer and more complex and I was typing it out on my phone and wanted to be brief.
I'm glad I was able to help.
Great post! My question is, what is the difference between putting multiple columns in a single index, vs making an index for each individual column? are they the same thing?
They are not the same thing. Putting multiple columns in a single index maps out the location of rows in the table in the order the columns are listed. Let's say you had an index on (dept, emp_name) on an employees table. If you read the index in order- these are example values- you would start with the row containing (Accounting, Andrew: row_id 5), go on to (Accounting, Bob: row_id 100), (Accounting, Cindy: row_id 2) ... (Accounting, Zack: row_id 231) and then onto (Business Intelligence, Anna)... etc. An index on (name) alone would list the rows sorted by alphabetical order of name (indexes by default are in ascending order), but the order of the departments amongst those employees would be essentially arbitrary. Same deal with an index on (dept.) alone. Thus if you have access predicates: select * from emp_table where name = 'Bob' and dept = 'Accounting'; Having the multicolumn index (dept, name) would let you find the row location quite easily. If (dept, name) was a key to emp_table, this is an index unique scan. However, having separate indexes on (dept) and (name) would not be nearly the same thing. If you took (name), you would find the locations of all names 'Bob', but you would have to look through *all* of them to find ('Bob', 'Accounting'). This is an index range scan, and would be a less efficient access path than using an index on (dept, name). 
I don't understand. You have 7 million distinct phone # producing 61 million rows. That means each phone number corresponds to, on average, 9 lookup timestamps. Selecting "only the 10 most recent lookups for every number" seems like it would select almost the entire table (minus some rows for phone #s that exceed the average value of 9 lookups, depending on how skewed the data is). 
I totally agree with you , thanks for such wonderful explanation
Thanks so much, that makes a lot more sense! So with that in mind, it seems like it is best to look at the most frequently utilized querys on a table, and then just turn the columns used in their where clauses into single/multi column indexes. But of course give priority to sets of columns that are used more often.
That's exactly right. One thing to note is that in multicolumn indexes, the order of leading columns is quite important. For example, the index on (dept, name) is not particularly efficient at dealing with: select * from emp_table where name = 'Cindy'; It has to do what's called a skip scan in order to find the rows with the name 'Cindy'. However, it's quite efficient at dealing with: select * from emp_table where dept = 'Information Security'; since (dept) is the leading column. 
performance depends so very very much on which platform you're running next time, please see sidebar for instructions on how to let us know which platform you're running
I don't know much about SQL Server, but I'd assume it can easily handle imports from CSV. Assuming you have a table like the following, CREATE TABLE items (item_id text, item_cost double[]); and assuming you have only seven occurrences within the array, I would do the following to get a CSV in psql \copy (select item_id, item_cost[1], item_cost[2], item_cost[3], item_cost[4], item_cost[5], item_cost[6], item_cost[7] from items) to /some/file/location with csv
does the free trial option give me enough info to pass the exam?
You'll probably want to normalize that data - anytime you have column names ending in numbers, that's generally de-normalized data. 
Erm, this might be a bit to over simplified. A non clustered index is not quite a map for the "location" inside the table. The data structure of an index is a balanced tree. The columns you include in the index are basically a touple that make up the branch / leaf nodes of the tree. The order of these columns is also important. If you have 3 values (3 columns) that make up these touples, lets say (a,b,c), you can search for (a,b) and make full use of the index. If you however only search for (c), that will not be enough to navigate the tree structure, and you will end up with an index/table scan. The value getting looked up inside the index btw, is the row identifier, that can either a reference a leaf node in the clustered index, of in case of a Heap, the row identifier. That's where Key and RID lookups come from. Those happen, if you want to return data, that is not included in a non-clustered index. 
To me the blog post is a bit shallow. Key words (and explanation of them) I'm missing would be "B-tree", "Heap", "covering index", "key lookups", "selectivity", "fragmentation", "Width"
The column name is ITEM COST. The array contains the values that are ; separated. I want to import the data into seven columns "ITEM-COST1; ITEM-COST2; ITEM-COST3; ITEM-COST4; ITEM-COST5; ITEM-COST6; ITEM-COST7" 
 SELECT * FROM tableA a INNER JOIN tableB b on a.something = b.something WHERE a.forename = 'Brian' AND a.surname = 'Templeton' 
If you want us to do your homework (which I won't by the way, I'll only point some keywords), start by listing your tables and fields so we can give you a proper answer. Anyway, you can filter records from a query by using the WHERE predicate. Look it up, it's a really basic SQL instruction.
try this : /edit : maybe I should be more clear... try this in the sense of this will actually work. You can of course write the function directly as the apply clause in the query, I posted the function definition (its inline anyway), since this is a nice thing to have in ones toolbox, and its not exactly easy to read. Its also a LOT faster than doing the splitting with a cursor or while loop. I really don't quite get why others are suggesting "open in text editor/excel", but well, each to his own SET QUOTED_IDENTIFIER ON SET ANSI_NULLS ON GO /* ------------------------------------------------------------- Purpose: Convert a list of string values, delimited by ',', or the supplied delimiter, to a recordset. The list may have empty tags and blank inbetween the numbers. Sample : Sample Input: 'abc,xxx,12g' Parameters: @strDelimiter = strDelimiter token, default = ',' Returns: Table (strString varchar, intFieldPosition int) ------------------------------------------------------------- */ CREATE FUNCTION [dbo].[funiStringListToTable] ( @strStringList varchar(8000) ,@strDelimiter char(1) = ',' ) RETURNS TABLE AS RETURN ( WITH E1(N) AS ( SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1 UNION ALL SELECT 1), E2(N) AS (SELECT 1 FROM E1 a, E1 b), E4(N) AS (SELECT 1 FROM E2 a, E2 b), E42(N) AS (SELECT 1 FROM E4 a, E2 b), cteTally(N) AS ( SELECT 0 UNION ALL SELECT TOP (DATALENGTH(ISNULL(@strStringList,1))) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) FROM E42 ) ,cteStart(N1) AS ( SELECT t.N+1 FROM cteTally t WHERE (SUBSTRING(@strStringList,t.N,1) = @strDelimiter OR t.N = 0) ) SELECT strFieldValue = SUBSTRING(@strStringList, s.N1, ISNULL(NULLIF(CHARINDEX(@strDelimiter,@strStringList,s.N1),0)-s.N1,8000)) ,fieldNr = ROW_NUMBER()OVER(ORDER BY s.n1) FROM cteStart s ) GO SELECT * INTO #example FROM ( SELECT id = 1, stringList = 'abc;123;foo' UNION SELECT id = 2, stringList = 'qwerty;567;bar' )sq SELECT * FROM #example E OUTER APPLY dbo.funiStringListToTable(E.stringList,';') FSLTT
Please give us the schema of your tables for requests like this.
couldn't you just load the data into a TXT editor, do a Find/Replace on ITEM-COST1; ITEM-COST2; ITEM-COST3; ITEM-COST4; ITEM-COST5; ITEM-COST6; ITEM-COST7 and change it to nothing, or whatever... OR import data into Excel, Parse that section into columns based on semi-colon, and remove columns you don't need? 
[SQL Antipatterns](http://pragprog.com/book/bksqla/sql-antipatterns) by Bill Karwin is a great read.
My first suggestion would be to check to see if row / table locks are preventing the job from running as expected. It could just be the job fails/hangs once because of a lock conflict but isn't releasing them until you rebuild it. You can check what locks exist with... SELECT * FROM sys.dm_tran_locks ...and cross-reference database_ID and GUID from there. *** Edit - Also look for something that maybe only ones a few times a year or has an exceptionally long run time. [This Article by Thomas Larock](http://thomaslarock.com/2012/10/how-to-find-currently-running-long-sql-agent-jobs/) is worth looking at to see which job is taking the longest to run when the problem occurs.
Awesome thanks, I'll check this out. I don't recall anything that runs that would mess it up, but I'll keep an eye out. 
I think I sorta understand, but I'm not quite sure enough to answer. Can you provide a handful of stupidly simple example rows, something like CountyID|dataProviderChainID|pairedDataProviderID 123|A| | 456|A|B| ...and so on? I'm particularly confused about what it would look like where there are 2 rows for a county. 
You can produce nicer looking tables in Reddit comments like so: Test|Test1|Test3 \:------|:------|:------ Steve|Williams| produces: Test|Test1|Test3 :------|:------|:------ Steve|Williams| 
Yes they are different. The dataProviderChainID is the PK for each row. I was trying to see if there was a way to group by the pairedDataProviderID but I keep getting the annoying "dataProviderChainID is invalid in the select because it is not contained in the aggregate function or the group by"... message.
I figured out the issue. I had originally unchecked allow height to increase and allow height to decrease; height to decrease had to be left on in order to solve the issue.
Its a read from a Progress database. It will need to be imported into an SQL table at least monthly. This is just one example of an array the database uses. some of the arrays contain up to 78 elements and the tables have multiple columns of arrays. 
OK, almost got it. Couple of questions: 1) Why 162207 and not 30707? (that is, for each pair of rows with the same pairedDataProviderID, how do you select the row to return/display?) 2) Is countyID ok to use by itself for differentiation, or do I need to use StateID + countyID? 3) To be sure I understand fully: Isn't the Provider something of a red herring in this problem? That is, Provider doesn't seem to play a role at all (unless it's part of the answer to #1) 
So the answer to #1 might sound a little studid. As I stated, I am not the DBA, but one of our DBA's wants all the dataProviderChainID's that need to be deleted. These are all instances where provider B exists where Provider A also exists for the same CountyID and same pairedDataProviderID. #2: CountyID is a FK, same as stateID. You should be able to ignore stateID completely - two different tables they refer too. #3 I think this is answered in #1, but the provider is the key to what I am trying to isolate. I want all instances of provider B, where there is a concurrent instance of provider A when countyID &amp; pairedDataProviderID match. Hope that clears it up. I do not have delete ability, so my DBA wants the list so he can just delete.
While correct, I didn't want to get into all the details of how B tree indexes are structured. Also, you CAN use indexes without the leading columns, though it depends on the RDBMS and which column it is. For example, if you have an index on (a,b) and you do a predicate search on b, the optimizer *could* choose to do a skip-scan on index (a,b) or it could choose to do a full table scan. 
If you love PL/SQL, you're going to hate T-SQL. If you use the built-in RegEx functions of Oracle, you're going to hate MS SQL - they don't exist at all without a CLR package installed, which you either have to write yourself or purchase. I could go on. Frankly, you'll find more work as an Oracle DBA - if you're good at what you do - than you will as an MS SQL DBA, plus your salary potential will be higher. That's nothing against MS SQL DBA's, there's just far more of them.
I am not sure of what Case does, but my initial table of over 10300 returns just over 930, so that looks good. There are a few -1 values though. I think this could be very good. Thanks for your assistance. Wish I had gold to give, but my gratitude and admiration, and upvote will need to suffice for now.
Why are you using regular expression in SQL?
The fundamental SQL is the same, but there are substantial differences between PL/SQL and TSQL. Spend some time on the peripheral systems such as SSIS (ETL), SSRS (Reporting), and SSAS (OLAP). Also, look at it this way, it's just a different tool. The craft is the same. I've got certs in both, but I'm primarily doing SQL Server right now.
Because some data is in a free-form field and we need to pattern match. The tables aren't designed by us - they are a replicated extract from the billing vendor.
Sounds like something that should be transformed at the integration level, ie: biztalk etc
&gt;Sounds like something that should be transformed at the integration level, ie: biztalk etc Not when the entire purpose is as a historical data store for the data being replicated directly from the billing system in real-time. The ultimate issue is MS's refusal to implement something that SQL Server users have been asking for for years (just do a quick search on MSDN/TechNet if you don't believe me) that could be done quite easily seeing as .Net has RegEx built in. Instead, you have to roll your own. Oracle, on the other hand, has had built-in and highly optimized RegEx functions for quite some time, they work beautifully and haven't slowed down a single query I've used them with.
I've gotten gold a few times, and like a dog who catches the car, I'm never sure what to do with it, so... no worries. FYI: A CASE is like a encapsulated "IF...THEN" logic value. Basically, it goes like CASE &lt;something&gt; WHEN &lt;compare1&gt; THEN &lt;value1&gt; WHEN &lt;compare2&gt; THEN &lt;value2&gt; WHEN &lt;compare3&gt; THEN &lt;value3&gt; . . WHEN &lt;compareX&gt; THEN &lt;valueX&gt; ELSE &lt;defaultvalue&gt; END That compares &lt;something&gt; to each of the &lt;compare&gt; expressions and "returns" the 1st corresponding &lt;value&gt; value. So a stupid simple example is SET @MyVariable = CASE MONTH(GETDATE()) WHEN 5 THEN 'Happy May!' WHEN 9 'It is September' ELSE 'It is not May nor December' END It can all be on one line, I just put formatted it that way for readability. There is another, similar but different, format for CASE that I'll not get into, but it operates along the same lines. RE your -1 results. That tells me you have pairs which both have 'A' as the provider. Or, more accurately, both have some value other than 'B'. Finally: I recommend AllExperts.com as another source for help in matters such as these (plus everything else from carpentry to legal issues) 
Our company has installed SQL Server on many client servers, we always put the Check Database Integrity on it's own plan, completely separate of everything else. Ever since SQL2005 the Integrity check has always failed at some point, and prevented the other items from executing.
&gt; How do I call or retrieve other fields from the joined table in my output? Right now, I can only retrieve the field you named AS field2 on my output. If I put b.field1 in my first SELECT statement it says it is invalid. I had a typo in my original query (which I've fixed now) where I referenced `b.` instead of `b_max.`. Anyways, to reference fields in the joined table use the alias that I've created `b_max.FIELD_NAME` such as `b_max.field1` &gt; What exactly is that b_max notation doing? It is an alias for the results of the subquery. While it is not required I almost always alias subqueries for readability and clarity. &gt; If I join other fields on these two tables based on the max setup, do I have to use the b_max notation on all fields? I don't fully understand the question but you don't have to always use the alias on fields but if you have fields from both tables that have the same name you will have to use the alias notation so the query processing engine knows which field to return. In my opinion it's a best practice to always fully qualify your fields for readability and clarity. &gt; PS - Can I buy you gold or something for your trouble? No need for gold (I won't even use it!). Please just pass along the knowledge gained to someone else when the time comes :) If you have any questions please let me know.
Again, Thank you very much.
As someone who came from Oracle originally (8i), and started with MSSQL 2005, I can say it was a pretty easy transition. I didn't do much server admin, but aside from minor differences the SQL scripting was nearly identical.
Sounds like something that should be transformed at the integration level, ie: biztalk etc
I made the change a bit over a year ago. SQL Server is all around simpler to admin, but is less powerful. There's just less going on in physical architecture, memory management, backups etc., but that translates into less functionality. As another user noted, there's less work and it pays a bit less. I made the change because I got tired of working for the kind of organizations that use Oracle and I really liked SQL Server's community. Just keep in mind that it is a sacrifice to be considered.
Depending on your SQL flavor of choice, T-SQL Fundamentals by Itzik Ben-Gan is a good T-SQL book
Check out [Brent Ozar's Website](http://www.brentozar.com), he has a good number of free videos in his blog. There are also a lot of paid videos (usually $30 for a 90 min lesson), so if you (or preferably a company you work for) is willing to spend a little extra, it's probably worthwhile.
I don't know, I never used the free trial.
When I attempt to add fields, other than the one in the AS statement I receive the following error: **ORA-00904: "B_MAX"."PERCODE_AA_CODE": invalid identifier** What I was really asking on that last question was about conditions of the joined table. In addition to the max date, can I setup other conditions on the joined table too and not just a max. If so, where would those be placed? I've provided my code below. This is the code that works but when I replace or add on to my selected data at the top from the joined table, I receive the error above. SELECT a.personal_id , a.personal_fname , a.personal_lname , code_field FROM personal a LEFT JOIN ( SELECT b.percode_id , MAX(b.percode_action_date) AS code_field FROM percode b WHERE b.percode_year = '2014' GROUP BY b.percode_id ) b_max ON b_max.percode_id = a.personal_id WHERE a.personal_enlisted = 'Y'
This article proposes several ways of doing this. http://www.codeproject.com/Articles/300785/Calculating-simple-running-totals-in-SQL-Server I caution against doing it in SQL because of RBAR (row by agonizing row). Instead, if there is a presentation layer, I would do this calculation here. Very rarely in my experience does a Running Total have any real value because the ORDER of the data becomes very important. Rather, it is often the case that the end-user wants a Group Total for some subgrouping.
Simple question, simple answer : DECLARE @maxid INT = (SELECT MAX(orderid) FROM Sales.Orders); happens to me too when I switch from language to language a lot
Thank you! Looks like there's an error in the book I'm reading.
Why are we tied to a function here? Is there a possibility of re-use in multiple scripts? Would a View be better? You probably wouldn't be able to re-use a function because the function would have to include a correlated subquery and know which table it's looking at. If it's only getting used in one script, maybe a CTE would be best.
It doesn't matter much in a case like this. The scalar function will be a "one query per row". A cross apply will also essentially end up at "one query per row", bit less overhead since its not a function call, but won't be much of a difference I'd venture to say (blindly, I'd want to have test scripts and execution plans for a definitive answer). Joining directly and using an aggregate function and a group by, will proberbly be very slightly faster. It will still be essentially the same number of reads, a holy crap rowcount, and depending on the join order you end up with, could even be slower by a large margin. Anyway you approach this, it will be slow as hell. In 2012, you have better options that won't melt the server, but in 2008 you are screwed Well, there is always CLR's, but if you were to ask me for an "example" of a table valued function implementation in c# or .net to give back a running "something", in order to import it as an assembly into sql server, I'd respond with sending you my hourly rate. 
I tried defining additional fields in my subquery and alas I received duplicate information. I'm not sure what you mean or need when you request CREATE TABLE and INSERT statements. I do not use those as I am querying data that already exists. I am what I would call a novice at SQL so I want to apologize in advance if I seem rather dumb.
I'm guessing something like this? after update as set nocount on UPDATE Database2.dbo.TableA SET TableA.Column1 = l1.datetime1, TableA.Column2 = l1.datetime2, TableA.Column3 = l1.integer, TAbleA.refreshtime = GETDATE() FROM TableB AS l1 inner join TableB l2 on l2.id = l1.parentid inner join TableB l3 on l3.id = l2.parentid inner join inserted i on i.id = l3.id where TableA.id = i.jobrun_id
Maybe, for a larger recordset, and depending on whether the data is sequential, I might consider persisting the running total into that same or a separate helper table. In this way, you aren't really doing running total on the entire set each time but rather, only evaluating once and then doing a simple set-based select on the bulk of the output. 
I think we could keep discussing this for days or weeks without even disagreeing ;)
I suspect you may be overthinking things a bit. If you aren't manipulating the data in some manner, then you would essentially treat the dataset that you would create the temp table from as if it were a table (look up Common Table Expressions, although you might not need to get that complex, even), and do your update using that. I'm also not sure that it's really worth worrying about whether or not you use one statement or many. Use whatever is appropriate to do what you need to do (which we don't really have enough details on to sink our teeth into and really help with) in a reasonable amount of time. There is enough missing from the initial question that I suspect you're not terribly experienced in SQL (although I could be misreading things and be way off - please forgive me if I am), and could probably benefit from getting someone to look at the problem who has a little more experience under their belt. 
Understood. The code runs but I have one issue with it. Here is a sample of what my code looks like. http://imgur.com/eeoZBqz As you can see, the AA_CODE associated with the latest (max) date is 99. When I put this field on my output I don't receive the latest code, I'll get 22 or something else listed in the table. I do get the latest date under code_field however. Am I using the wrong function for what I want. Also, I feel like I am bothering you with all of these questions. Is there a book or other reference material you'd like to point me to or recommend? Thanks again for all of your help.
&gt; As you can see, the AA_CODE associated with the latest (max) date is 99. When I put this field on my output I don't receive the latest code, I'll get 22 or something else listed in the table. I do get the latest date under code_field however. What you'll likely need to do is apply another `MAX` analytical function in the subquery. Something like this maybe: , MAX(b.aa_code) OVER (PARTITION BY b.percode_id) AS aa_code So the final query would be like: SELECT a.personal_id , a.personal_fname , a.personal_lname , b_max.code_field , b_max.&lt;other fields&gt; FROM personal a LEFT JOIN ( SELECT percode_id , code_field , &lt;other fields&gt;... FROM ( SELECT b.percode_id , &lt;other fields&gt;... , MAX(b.percode_action_date) OVER (PARTITION BY b.percode_id) AS code_field , MAX(b.aa_code) OVER (PARTITION BY b.percode_id) AS aa_code , ROW_NUMBER() OVER (PARTITION BY b.percode_id ORDER BY b.percode_id) AS rn FROM percode b WHERE b.percode_year = '2014' ) WHERE rn = 1 ) b_max ON b_max.percode_id = a.personal_id WHERE a.personal_enlisted = 'Y' ; &gt; Also, I feel like I am bothering you with all of these questions. Not at all! &gt; Is there a book or other reference material you'd like to point me to or recommend? Oracle's documentation is really good. Here are a few examples: [Oracle® Database 2 Day Developer's Guide 11g Release 2 (11.2)](http://docs.oracle.com/cd/E11882_01/appdev.112/e10766/toc.htm) Check out chapter 4. [Oracle® Database SQL Language Reference 11g Release 2 (11.2)](http://docs.oracle.com/cd/E11882_01/server.112/e41084/toc.htm) Check out chapters 3 - 7. [Oracle® Database SQL Language Quick Reference 11g Release 2 (11.2)](http://docs.oracle.com/cd/E11882_01/server.112/e41085/toc.htm) Simplified version of the entire SQL Reference linked above.
&gt; The below sql is my attempt and it doesn't seem to work. **It returns users** who have logged in within the last 90 days as well. Are you sure about that? FROM t LEFT OUTER JOIN l -- l as subquery. Abstracted for clarity. ON l.subject = t.name WHERE l.subject IS NULL AND l.logontime IS NOT NULL This should return 0 rows. You're joining to `l.subject`, so if it's null all columns pulled from `l` will be null. You can't have `l.subject IS NULL AND l.logontime IS NOT NULL` at the same time. If you want to see the last `logontime` for the users who haven't logged on in the last 90 days, you're going to have to pull them in from `l`. remove the `HAVING` clause from it. Keep the `LEFT OUTER` (you might need it in case the name isn't on `l` at all, meaning they've never logged it.) Change the `WHERE` of the main select to WHERE l.logontime &lt; trunc(sysdate)-90 OR l.logontime IS NULL 
I should've included those, I'm familiar and use all those regularly
I'll look into that. I appreciate the recommendation
dont use temp tables, and especially dont use a cursor windowing functions and recursive queries work far better. depending on your platform if you have SQL Server 2012 throw a columnstore index on the larger tables and your jaw will drop at the speed improvement. 
One thing missing from your book that I believe is absolutely essential is the order of execution for `SELECT` statements. This explains why you can't reference aliases in your `SELECT` clause in the `WHERE` clause like so: SELECT col1 AS a FROM tbl WHERE a = 100; 
Thank you for the feedback, this seems like a useful addition. I'll try to include this in the next edition of the text. If you wish, please PM me your real name for inclusion in the contributor list. This is completely optional of course.
got as far as section 1.2 in the intro and found this -- &gt; The formal term for a column in a database is a **field** and a row is known as a **record.** sorry, mate, it's the other way around!! you use this incorrect terminology throughout the book, and it's quite annoying &gt; the id field effectively tells you nothing about the programming language by itself, other than its sequential position in the table. rows do ~not~ have a "sequential postition" in tables i briefly scanned the rest of the document, and it looks fine i liked the atomicity section where you explained that “Kemeny, Kurtz” or “Kurtz, Kemeny” or even “Kemeny &amp; Kurtz” would be problematic values for a WHERE clause search, but to be completely accurate, those values are in fact atomic in the strict context of VARCHAR -- however, the example is fine to illustrate the problem of normalization 
Um, a column *is* a field and a row *is* a record. Been that way for decades.
I don't have 2012 just 08R2. In terms of not using temp tables how would you aggregate data across n number of databases inside a stored procedure? Of course I wish this wasn't the db design I was up against but compliance forces outside of my control dictate the design.
The only general advice I can give you is reading the execution plan, and looking at the STATISTICS IO and TIME. There is no general "make it faster" button. Btw, those cost percentages you see in the execution plan are just based on the estimates, they are not a true reflection of the cost of a batch or operation. They can be very far off if you run into bad estimates. Depending on what you find in the execution plan, you might want to include columns to prevent lookups, rewrite non-sargable expressions into sargable and make sure you end up with a good join order. Without having the actual execution plan there is not much I can tell you.
Wow - yes, that was it. It worked perfectly well with your suggested changes. Yes - I agree on your point about the join. Not sure how, but it was returning users &amp; records, many had null. Now if t.name is also NULL then l.subject can be NULL, I thought it was a side effect of using a LEFT JOIN. Thank you very very much!
Yeah, I realized that the % from the execution plan is based on the estimate. In fact it was showing 98% on the insert, when I think the distribution is probably greater across the query than just the insert into the temp table. I have been using the output from the stats to determine the duration of different steps of my stored procedure. I had a few ideas after sleeping on it, so I am going to give those a shot and see where I stand. I'll post up the execution plan if I can't get it any better. Thanks for the suggestions.
Thank you.
I'm more curious about how many got the 'good' answer over the 'bad' answer in those percentages. It's also not surprising that the Microsoft group got the lowest; a large portion of C# developers use the Entity framework, built into VS, letting it handle the legwork. 
MySQL got the lowest — not MSSQL ;)
if its using multiple data sources I wouldn't even use tsql. I would bring the data I need in via SSIS then filter from there once its in a more flat format. 
you will need to give a bit more detail about the problem you're trying to solve you understand why the condition after the OR is causing the error, right?
I am far from an SQL expert, I'm just reading a book about general SQL now. I chose MySQL because, hey- why not. Every answer was just an education not-at-all-confident guess… and I still managed to get 4/5. But then the premise of this quiz goes against what people tell me when I've asked for SQL help before, where they often just say "trust the database to always do what's fastest".
I'm on my phone so i cant help research the issue but could the temp table already exist from a previous query on the same connection? Try checking if the table exists in the sproc then dropping the table before creating the temp table.
I am assuming you are attempting an operation like this: SELECT x, SUM(number) AS sum_number FROM table GROUP BY x HAVING SUM(number) = 1 OR number = 1; That is not allowed. The having clause applies to the result set and not the rows scanned.
union perhaps?
I'd solve this by using a windowed aggregate and a subquery. I would bet that it won't be significantly slower, having is not the fastest operation in the book either after all. SELECT * FROM ( select something , number , sum_number = sum(number) over(partition by x) from table )sq WHERE sq.number = 123 OR sq.sum_number = 123
What I meant was that it is confirmed that it is not the other way around -a column is not a record, a row is not a field. Though yes, I accept that when we distinguish between the logical and physical models a row cannot exactly fit into the definition of a record or vice versa.
On this one, why was the answer not to include ID in the index? It said no improvement was possible. CREATE INDEX tbl_idx ON tbl (a, date_column); SELECT TOP 1 id, date_column FROM tbl WHERE a = @a ORDER BY date_column DESC
&gt; a column is not a record, a row is not a field i am certain that we can all agree on this however, you stated "the formal term for a column in a database is a field and a row is known as a record' by the other way around, i meant that **the formal terms are column and row**, whereas field and record are the informal, e.g. would not be used by database professionals 
Ah ok, good to have that confusion sorted out. Thanks for the clarification, I will try to distinguish the two in a clearer fashion in the future.
Ah yes. Forgot about the LAMP stack.
I don't think you're showing the line that's causing the problem. I can't see how the CREATE TABLE statment could possibly produce that error. We would need to see more of your stored proc.
If you just want a running total over all months and not separated by years, a simple self-join should be sufficient: SELECT t1.ProductID, t1.Month, SUM(t2.Sales) RunningTotal FROM SampleTable t1 INNER JOIN SampleTable t2 ON t1.ProductID = t2.ProductID AND t2.Month &lt;= t1.Month GROUP BY t1.ProductID, t1.Month [ORDER BY 1, 2]; **EDIT:** Sorry, that doesn't sort the years correctly. If you have more than one year each, you have to replace the month comparison with SUBSTRING(t2.Month, 3, 4) || SUBSTRING(t2.Month, 1, 2) &lt;= SUBSTRING(t1.Month, 3, 4) || SUBSTRING(t1.Month, 1, 2)
Thanks! That worked.. now I have some added complexity: Suppose the last month of my fiscal year is January, and I want to keep a running total for just the first fiscal year the product was sold, and then also track a 3 month running sales total regardless of fiscal year... so my desired result set would look like: * ProductID , Month , RunningTotalFY , RunningTotal3Months * 1 , 01-2013 , 10 , 10 * 1 , 02-2013 , 0 , 15 * 1 , 03-2013 , 0 , 22 * 1 , 04-2013 , 0 , 0 * 2 , 09-2012 , 9 , 9 * 2 , 10-2012 , 11 , 11 * 2 , 11-2012 , 12 , 12 * 3 , 02-2013 , 4 , 4 * 3 , 03-2013 , 10 , 10 * 3 , 04-2013 , 13 , 13 * 3 , 05-2013 , 20 , 0
I think this will work, at least it does when I imagine it. select cash_in.customer, sum(cash_in) - sum(cash_applied) from cash_in left outer join cash_applied on cash_in.customer = cash_applied.customer group by cash_in.customer having (sum(cash_in) - sum(cash_applied)) &gt; 0 *edited join to left
I'd say it should be a LEFT JOIN between the 2 tables. Otherwise you will not get customers with rows in the cash_in table, but no rows in the cash_applied table. Otherwise it should work I agree
Good thinking, I'll edit it.
Sadly, it really is - the proc is completely valid but research thus far seems to point to an "issue"? with the odbc driver of the application making the call. If you perform a trace(including exceptions) of a similar proc that references a # temp table you will see error Error: 208, Severity: 16, State: 0 thrown before it ever enters the proc. This seems to fall into a "deferred name resolution" function of SQL where since the object doesn't exist at invocation, a false error is thrown on the validation performed before it ever steps into the procedure. One of my other apps makes reference to procs that use this format and doesn't react to the same error, although their methods of database communication are different. Here's a full sample proc if you'd like to see it: CREATE PROCEDURE [dbo].[temp_test] AS SET NOCOUNT ON; CREATE TABLE #tmp([val1] VARCHAR(100)); INSERT INTO #tmp ([val1]) SELECT [status] FROM master..sysprocesses; SELECT * FROM #tmp; DROP TABLE #tmp; Changing all the #tmp references to a table variable completely removes the errors. However, it's opened up to performance issues as the data set grows.
Yep that did it, perfect! Thank you so much for the help!
No problem.
just curios, have you looked into partitioning instead of using multiple tables? When you have a column that is hard to index due to low cardinality it can be helpful to use a partition. http://docs.oracle.com/cd/E18283_01/server.112/e16541/part_admin001.htm#insertedID4
The reason data is split up is more due to how the data comes in. I compile data from like 20 agencies over two weeks sporadically. Then at the end of every month I get adjusted numbers. Then after 6 months I get finalized numbers. I figured having separate tables for each agencies will just make it easier to navigate.
Ugh, what a headache. This isn't going make you feel any better but we've been trying for a couple years now track down the source of these random timeouts when running certain procedures from our application. The same procedures run in less than a second from management studio. When you hook the profiler, you can see it's the identical command with and identical plan and all of that, but sometimes it just decides that it's going to run forever via ODBC. Then a a couple days later it works fine again. We had Microsoft involved for a while but eventually they decided it must be something in the hardware. That makes no sense at all though because you can run the same command side by side in management studio and the app and the management studio call works fine while the app is still broken. Clearly they just didn't want to be bothered by it anymore. The problem still pops up for a while every now and then. The only thing we can figure is there's something different inside SQL Server related to the connection end point. It doesn't seem to be anything we can do anything about though. It's very frustrating.
Ahh I see what you mean. The issue is that every transaction in 1 table is joining to every other transaction so it has that multiplying effect (also known as the Cartesian product or cross join). In that case let's do it via subquery which is much uglier but should work. Ideally I would make a function called SumCashInByCustomer which would return the sum and it would be a lot prettier and more efficient. But if you want to just get something that works quickly I think this may do the trick. It sums cash_in and cash_applied in a subquery and then subtracts those. Since we're not joining those 2 tables we remove the multiplicative effect. select cash_in.customer, (select sum(cash_in) from cash_in ci where ci.customer = cash_in.customer group by ci.customer) - (select sum(cash_applied) from cash_applied ca where ca.customer = cash_in.customer group by ca.customer) from cash_in where (select sum(cash_in) from cash_in ci where ci.customer = cash_in.customer group by ci.customer) - (select sum(cash_applied) from cash_applied ca where ca.customer = cash_in.customer group by ca.customer) &gt; 0
You've probably already tried this, but I had the exact same issue that took weeks of Googling to finally come across. If you're on 2k5 and the performance jumps with service restarts this may be worthwhile: [http://support.microsoft.com/kb/927396/en-us](http://support.microsoft.com/kb/927396/en-us) TLDR; SELECT SUM(single_pages_kb + multi_pages_kb) AS "CurrentSizeOfTokenCache(kb)" FROM sys.dm_os_memory_clerks WHERE name = 'TokenAndPermUserStore' If the cache size is bigger than the threshold that you observed, run the following command: DBCC FREESYSTEMCACHE ('TokenAndPermUserStore')
Try using a common table expression: with cte as (SELECT a.CustID, COUNT(a.OrderID) AS OrderCount FROM TableName as a GROUP BY a.CustID) SELECT cte.CustID, cte.OrderCount FROM cte WHERE cte.OrderCount = (SELECT MAX(cte.OrderCount) FROM cte); 
Not an expert, but try this: select d.* from (select custid, count(orderid) nbr_orders from sales.orders group by custid) a, (select max(b.nbr_orders) max_nbr_orders from (select custid, count(orderid) nbr_orders from sales.orders group by custid) b) c, sales.orders d where a.nbr_orders = c.max_nbr_orders and a.cust_id = d.cust_id
also you get that error because you're basically selecting... hi.custid, max(numoccurences) which you need to group by hi.custid, if you add in the group by on hi.custid then you should see the error go away
i think you want SELECT a.field1, a.field2, b.filed1, b.field2 FROM a LEFT JOIN b on a.id = b.id WHERE a.field1 in ('foo', 'hello','world') and b.field2 = 'foo' i may be missing some things from your statement so you might have to tinker with what you want. 
We are currently on clustered 2012 servers but this problem existed on 2008 and 2008 R2 as well. The problem persists through fail overs too which is what led MS to decide that it must be something to do with our SAN (which we're pretty sure it doesn't). 
If I group by hi.custid, then I get the EXACT same query result as this query: select custid, count(orderid) as numoccurences from sales.orders group by custid;
That worked....Could you please explain to me what's going on in your query?
a.field1 will never equal 'hello' or 'world'. Just look at my statement again. 
Gahh my bad. I'm trying to juggle too many things at once... Let's do it properly. I'm assuming here that a customer will always have an advance value but may not necessarily have any applied. If that's not true then we'll have to flip it. select customer_no, sum(advance_amount) as advance_sum into #temp_advance_sum from a_cash_advance group by customer_no select customer_no, sum(applied_amount) as applied_sum into #temp_applied_sum from a_cash_advance_applied group by customer_no select a.customer_no, (advance_sum - isnull(applied_sum,0) ) as balance from #temp_advance_sum a left outer join #temp_applied_sum b on a.customer_no = b.customer_no group by a.customer_no having (advance_sum - isnull(applied_sum,0) ) &gt; 0 
I know your pain - we have a similar situation with one of our databases. What we did was have the separate tables like you, but once our data has been finalized, we do a compilation and move them into a master table that's indexed and partitioned based on agency and date. Makes it much easier for final reporting.
i think it just might be your mix of ANDs and ORs that's messing you up pretty hard to tell, with all those fake names floating around :) SELECT a.field1 , a.field2 , b.field1 , b.field2 FROm a INNER JOIn b On b.id = a.id WHERE 'foo' IN (a.field1,a.field2) OR ( b.field1 in ('hello','world') AND b.field2 = 'foo' ) 
oh, wait, that won't do it maybe this? SELECT date , product_id , SUM(total_purchasers) AS total_total FROM daTable WHERE date &gt;= CURRENT_DATE - INTERVAL '7 DAYS' GROUP BY date , product_id ORDER BY date DESC 
that's not clear at all, sorry perhaps you could show some sample data, including rows that you do want returned and some you don't
You're right, it's not clear. Basically you have table b which has two rows: field1 field2 id 'hello' 'foo' 5 field1 field2 id 'world' 'bar' 5 And if one of the 'field2' columns equals 'foo' or one of the columns in table a equals 'foo', then both rows in table b must be returned. Does this make more sense now? Oh, and the id column in table b must match the id column in table a. 
&gt; if one of the 'field2' columns equals 'foo' or one of the columns in table a equals 'foo', then both rows in table b must be returned. suppose one of the columns in table a equals 'foo' -- how do i know which rows of table b (assuming matching ids) are to be returned? all of them? or only those with 'hello' or 'world'? or only if both are present? i'm still getting dizzy with all these fake names
 SELECT id FROM b WHERE field1 IN ('hello','world') GROUP BY id HAVING COUNT(*)=2 put this in a subquery, and join it to the a-b join that way, you know which b rows to return when those other conditions are met
in my tired and drunken state, sql agent job on local instance, run the queries trough linked servers and write all into a local temp table, last step, select * from temp table. Powershell should be an option too. I'm quite sure there are better options, but well it would work
Hi! Any chance you could share the table schema, at least for [Server1].Archive.dbo.Requests? Is ResultCode a numeric (tinyint?) field? The aggregates are going to cost you regardless but if anything in there happens to be invoking implicit conversion from a character type, holy moly. SUM(Case When her.ResultCode in (7,9,11) Then 0 else 1 end) Uptime, SUM(Case When her.ResultCode in (7,9,11) Then 1 else 0 end) downtime, What's this up to, it's the same calculation/aggregate twice in a row, for no benefit that I can tell. Are [Server1] and [Server2] both running the same version of SQL Server, if so / if not, which versions? Is one of the servers beefier from a CPU and RAM perspective?
what indexes do these tables have? 
I would be interested in collecting this data (only 6 MS SQL 2008 R2 servers and a few dozen production databases, but growing soon), can you post your query? I would probably do what svtr suggested or a package in SSRS scheduled to run using sql agent on the local machine.
Resultcode is nvarchar, so it seems that by getting rid of the implicit conversion I can save a bit of time there. (tested with a select top 1000 query on just the table and by putting the numbers as strings it runs almost instantly, rather than 18 seconds for the numbers as int) The code you quoted is to determine how many requests errored out. So the idea is that if the resultcode is 7, 9, or 11 there was an error with the server handling the return, it is considered downtime. So if the resultcode is not 7, 9, or 11 there was no error and it is considered uptime. Server1 is running 2005 and Server2 is running 2008. I do not know the server specs unfortunately. 
They both have PK clustered indexes on a unique ID column, Server1 also has nonclustered indexes on the rest of the columns. Server2 only has the PK clustered index.
Are PayorCode and connectorId numeric? Do they have indexes?
I'm not sure. I'll let you know once I am back in my office tomorrow morning. 
Ewwwwww temp tables: Cause sometimes you just feel like gangbanging your RAM.
It's not. There are methods to be able to select from multiple different databases in the same query (stackoverflow.com/questions/1144051/selecting-data-from-two-different-servers-in-sql-server) you shouldn't need to use a temp table. Temp tables are rather cumbersome/intensive, but they can make the code simpler, so there is that, I suppose.
I would start off by inserting both datasets into a temp table, or at least the larger one, I'd play with it all three ways actually, it's not always the silver bullet. Then create the necessary indexes. Also what does the execution plan look like? May I suggest sqlSentry for viewing execution plans. I love this tool http://www.sqlsentry.net/plan-explorer/sql-server-query-view.asp
You need entities for everything you will need to count. I see that you'll need to count Showings, Listings, Buyers, Sellers, Agents, and Offers/Contracts. 
See that's what confuses me. How do I know to list or what Showings, Listing, and Offers/Contracts? Is it something that you make up when you read it? That's what I'm confused about.
so, when is your assignment due? This looks alot linke chapter 4/5 of Kroenke's Database Concepts/Design book. Write out some more work, as it seems like we are helping out on a homework assignment. 
what about SELECT date , product_id , SUM(total_purchasers) AS total_total FROM daTable WHERE datediff(day, date, current_timestamp &lt;= 7) GROUP BY date , product_id ORDER BY date DESC 
Connectorid is numeric and does not have an index, payorcode is a varchar and is indexed.
First, up the ram. Depending on your server edition, it should support at least 64GB. Memory is dirt cheap as far as hardware for performance goes, and that is something you will feel performance wise. Then I would seriously look into setting up a dedicated reporting Server, but well, up the RAM. On the CPU, well i7 I would throw into a workstation, but not a database server. You would be better served with a XENON e5 or e7 I'd say. 
Yup. I agree totally, but I don't make the hardware decisions.
That tool looks like it will be extremely useful. Thank you. In regards to the execution plan, I am not able to view it due to some restrictions the DBA's put on the database. I am discovering that the biggest reason this query takes as long is it does is due to the shear amount of data in the tables and the amount of work it is having to do with that data. On a smaller dataset in another environment it only took 30 minutes to run as opposed to three hours.
I am running the query on Server1 as it has the larger dataset. The table dump is a good idea and I will look into that. Thank you. I think the biggest cost is just the large amount of data that has to be sifted through and then worked on. I ran this query in another environment that has a smaller dataset and it ran in 30 minutes as opposed to 3 hours.
Thank you to everyone. You gave me good starting places to make this run better.
Its due today. I understand how to do the MySQL I just didmt understand how to take what is being said and translating that into the actual database. But the other user helped me understand a lot more.
I would start with writing out the requirements in an easier to read format. (remember you can read each of these backwards as well. ( EG: Property has 1 to Many Showing can also be Many (or each) showings has only 1 property) Here is what I can take out of text for requirements and I pulled these from the text Agent can sell 1 to many Properties (Each agent can sell many properties) Property has 1 to Many Showing (A single for-sale property can be shown to potential buyers by many different agents on different days and times.) Agent can have 1 to many Showings (A single for-sale property can be shown to potential buyers by many different agents on different days and times.) Buyer can have 1 to many Showings (A single for-sale property can be shown to potential buyers by many different agents on different days and times.) Home Owner (Seller) can list 1 to Many Properties (property is listed by one seller ... and one seller can sell many properties ) Buyer can buy 0 to many Properties (purchased by one buyer. However, one buyer can buy many properties ) So I would add Agent, Properties and Showings Hope this helps 
Go to the largest table, pull out everything you need and insert it to a temp table, then do your manipulation. Doing that alone will probably speed up your query a lot. 
In design view, right click on the item number group header, and choose properties. In the format tab, look for 'Force New Page' and make sure it's set to None. Edited to add: probably better to post this over in /r/MSAccess, since it has to do with Access and not really SQL.
Would it be too difficult to set this up as an automatic SSRS report?
I would probably use the 'partition by' and 'over' keywords for this kind of thing.
none that i know of, but check out toadworld.com for their optimization products. look at the DMVs here... http://sqlserverplanet.com/category/dmvs and here... http://use-the-index-luke.com/blog/2014-01/unreasonable-defaults-primary-key-clustering-key might want to remove your clustering key and replace it with an unclustered index... 
I've got a lot of the secret weapons--just looking for something, even commercial that I could use to either show people that aren't very good with optimizing database designs how to use. I figure there has to be something out there, no sense in reinventing the wheel. ie: take schema validate relationships exist or don't exist check existing views for bad things, ie: 20 tables joined, temp tables, if statements changing pattern, too many columns, blah. 
What does 'gigantic' mean in this case ? 10 million? 100? I agree with qtyapa, the problem is probably you're running it in 2 servers Another thing you may try is to do the aggregation first, and the join later ... Maybe with a WITH ...
Unfortunately that thing was used to create one or two of our worst performing databases we host. I'll have to continue using home grown until I find something that searches stored procedures/views for "How many instances of count/other-functions, how many instances of FROM, how many instances of JOIN, how many instances of functions on columns instead of predicates, how many isnulls. Some of the queries I scan have over 58 FROM clauses, like seriously! Basically trying to make a report card for databases with things that aren't necessary show stoppers, but the ability to baseline what it was before by complexity, and after.
The classic approach is to use `INTERSECT`. Use 2 queries here that brings you 2 results. One that will return items with only flour. The second will return only items with sugar. Then put the `INTERSECT` keyword between them.
You can solve this with a self-join. SELECT item FROM table AS flour JOIN table AS sugar ON (flour.item = sugar.item) WHERE flour.ingredient = 'Flour' AND sugar.ingredient = 'Sugar'
Having 20 joins in a query isn't necessarily a sign of something bad. The problem is that the depth of a schema and the complexity of the design can at once be a design decision trade-off. A shallow schema will have fewer tables, but more nullable fields. A deeper schema will have more tables and joins, but fewer nullable fields. It depends entirely on the data you are storing the the database. That's the core problem. Any generic solution for database optimization would be unable to account for the needs of the application (and thus, the needs of the users). Even the difference between an OLAP and an OLTP database will create vastly different types of queries and joins with potentially very similar schemas. How do you abstract that to analyze it for a general case? My guess is that you *don't*. My guess is that performance is based on how the database is used just as much as how the database is designed, and there's no way to describe that. Realistically, all you can do is monitor performance and make changes from there. I would use DMVs to look for long-running querys, missing indices, long-running stored procedures, etc. I would look at the famous [Hallengren scripts](http://ola.hallengren.com/). If you're wanting to look at commercial solutions, I've heard good things about Ignite for SQL Server, SQL Sentry, and SQL Diagnostic Manager. That said, most people I've worked with just stick with the tools built in to SQL Server. They're really quite good, and most of the commercial solutions just pull data from the same sources. 
 SELECT item FROM a_table_like_so WHERE ingredient IN ('flour','sugar') GROUP BY item HAVING COUNT(*) = 2 
More than 20 tables is almost always a sign of something bad. It's the rare case when it isn't, opposed to the other way around.
Ah, yes, I should preface this, a properly normalized database it makes sense. But a non-normalized database joining tables containing 160 columns to tables containing 180 columns etc, is what I am dealing with constantly. No Indexes either!
the subquery in the where clause is referencing the outer query. For each row in the world table, the subquery gets executed. consider this sample data (and yes I am to lazy to look up the values) europe, Germany, 2222222 europe, England, 1111111 Asia , China , 9999999 Asia , Japan , 1111111 When you run the query, for all 4 rows this filtering condition will be evaluated : "Is the area of the current row, equal or larger than anything that gets returned by the subquery". The subquery takes the continet of the current row, and returns all areas of all countries in that continent. So in the query result you only get the one row that has as area, the max(area) of any country in that continent, for each continent. That would not be my way of writing such a query I have to add thou
Ah, yeah, see, I'm joining against clustered indexes all the time. :-D I think the biggest tables we have have less than 30 columns. The system does have some one-to-one table relationships, but the data are sorted between the tables. Like there's the main registration table with basic student info, but also a personal table (potentially sensitive information like homelessness and free meal status) and an academic table (expected year of graduation, graduation requirements). They *could* be a single table, but it makes sense for them not to be. There's only 20,000 students in the system at any time anyhow so the tables are small, and they join with the student ID. 
In this query you only need one of the aliases. The reason to use two is probably because the author thought it would be more readable that way (and so do I). If no aliases were used the database would have no way of telling that `y.continent` is form the main query while `x.continent` is from the sub query in `WHERE y.continent=x.continent` since both refer to the same table. The last line makes sure that when the sub query is run for every row in `world` in the main query we only care about countries on the same continent.
interesting approach
&gt; I apologize for asking dumb questions There is no dumb question with SQL. Its part of the learning process. At least you knew you needed to ask the question.
not really, if it does not generate a syntax error, which it will on a great many DBMS, it would be like the dreadful ansi 89 join syntax.
Rather than write it all out for you, here's what I suspect is actually tripping you up; You can add the users table to your query twice. From Users joined to Students, and from users again, but this time joined to teachers. As an example: select c.subject, teachername = u2.name, studentname = u1.name, cr.grade, cr.absent_number from classes c join class_registration cr on c.class_id = cr.class_id join teachers t on c.teacher_id = t.teacher_id join students s on cr.student_id = s.student_id join users u1 on s.[student_id] = u1.[user_id] join users u2 on t.[teacher_id] = u2.[user_id] 
I've seen syntax like this all over my prod systems (no comment on the vendor but it's a big company). It does make queries extremely difficult to read. I'd love to know if there is any real reason to write code this way. 
Is "modules" declared as a proc or function ?
I do not have Oracle installed but are you not just forgetting the `DECLARE` statement first in the function? CREATE OR REPLACE PROCEDURE get_modules_info_by_student_id (student_id NUMBER) AS DECLARE v_name modules.name%TYPE; v_created_date modules.created_date%TYPE; BEGIN SELECT name, created_date [...] end; /
"AS" = "DECLARE" when defining a procedure
You're right, I completely missed that.
Microsoft Access would build SQL statements like that albeit they would have parenthesis to indicate the scope. This is potentially from an upgrade/migration of an Access project.
Does query editor parse it?
Sorry, I was away for the weekend. [I pinched some of this from around the net, and other bits i put together myself.](http://pastebin.com/Bhbe3Jy9) 
Yup, thats how I'm currently carrying out the checks manually. I'd like to automate that procedure. 
Thanks, I look forward to checking this out on Tuesday when I get back in the office.
check out [this site](http://www.databaseanswers.org) for a huge # of examples for all types of data models.
Please post DDL and DML so we can help you better 
Please post DDL, a few DML to test and the SQL statement in question so we can help better.
OP, this is the correct answer.
For mysql do this: https://dev.mysql.com/doc/refman/5.5/en/load-xml.html In MS SQL, you can use bulk insert. 
What platform are you using specifically? MSSQL? MySQL? Oracle?
Yes. It transforms the FROM clause into: FROM A INNER JOIN B INNER JOIN C ON C.ID = B.ID ON B.ID = A.ID 
I'm using MySQL, using DBI library in perl.
&gt; chiastic join SQL Thanks! Googled and from what I gathered chiastic join syntax is used to control the order of the joins. For example, SELECT * FROM A INNER JOIN B INNER JOIN C INNER JOIN D ON D.ID = C.ID ON C.ID = B.ID ON B.ID = A.ID It would first join D to C, then C to B, and then B to A. One can use indentation to make it more readable: SELECT * FROM A INNER JOIN B INNER JOIN C INNER JOIN D ON C.ID = D.ID ON B.ID = C.ID ON A.ID = B.ID 
Is A.ID and B.ID actually constrained to be NOT NULL?
I don't know.
1. What are the hardware specs of the machine? 2. Does anything else also run on the same machine? 3. How are the tables indexed and what types of queries are you running? 4. Have you confirmed it is the database running slowly or just blindly guessing?
I'm learning SQL too :) I came across a reason for doing this today -- I had to do two aggregate functions on the same table in a query, and of course you can't do two aggregations in the same select statement -- I had to do it in a subselect. In the subselect, I aliased the table I had to do two things to, and just referenced the outer query after naming my subselect. It meant I could find the "Latest activity" and the "Number of activities" from one table , looking at the same column :) 
Straight SQL yes not sure what you mean about a filestream. Basically we have an SQL DB which we're trying to deconstruct - taking out all of the files from it, and saving them to a folder with their relevant filename (which is another field). I've then got further work to do on them but it's this first stage of getting the files out of the db that's stumping me :| (I have no SQL training whatsoever so I've been somewhat thrown in at the deep end here!) 
Oddly enough it's set as image but contains file data - a lot of these files will be word docs, excel spreadsheets, some will be pictures of post offices (don't ask!) etc. I need to run them through a swiss file knife after extracting them
the image is just a depricated datatype. Its the same with text and varchar(max). The image column should contain binary data. To export those files to the filesystem, I would write a very very simple application, getting a dataset from the DB, SELECT filename = Column1, binaryData = convert(varbinary(max),column2) FROM yourTable and then just loop over the recordset in your application and write the binary data to disk using the filename of the DB. Should be quite easy. 
That's a pretty low spec for a server. What you're describing doesn't sound right at all though, with the gradual slowdown. I don't know a lot about adimining mysql but a Web server and a dB on the same box with only 8 gb is automatically suspect. 
Any chance of getting replication set up on a separate server and running all queries (except the ones done by the web server) off of that?
depending on what type of database you're querying (i.e. highly transactional or not), put this at the top: "SET TRANSACTION ISOLATION LEVEL SNAPSHOT" or, "SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED". If your database is not highly transactional, it's basically like putting on a global "with (nolock)" which normally you would have to apply to every table. I work on a low-transactional db, and it's pretty much at the top of every one of my queries, purely for speed -- and it gives a significant increase.
This is a great tip! On a previous project this feature really saved us (snapshot isolation). However it is one of those features you must consider carefully. It adds extra tempdb activity (if I recall correctly).
&gt; After I get all locations within x miles, I have to query the Offers table for each location This sounds like a design flaw. Just query them all in one query, using a join Whats in SHOW PROCESSLIST when the server is under stress? You say the sql process is responsible for using the resources, all the queries have completed, yet the cpu is still at 99%. Its very unlikely all three of these things are true.
But you dont even know how much usage its getting, and whether its even optimised. Recommending enterprise SSD drives based on how little we know about his system, is REALLY premature.
It is possible that MODULES may be also a declared procedure/function/package as mak4you suggested. Also, please watch out for this line right here: WHERE student_id = student_id; I would rename the parameter name something else other than the exact column name in order to avoid any confusion, like: CREATE OR REPLACE PROCEDURE get_modules_info_by_student_id (p_student_id NUMBER) 
This is correct. OP need to dig deeper to find out what process is maxing out the CPU usage. Sounds to me like something is stuck in a processing loop, which is likely bad coding on the web site/script rather than MySQL.
yeah, that system can run way more than it needs to with a database that small. Assuming mysql has at least 1GB of ram, it should work fine.
It works here on 2012. Try removing the AS? DECLARE @empid INT = 3;
I've tried using a join, but I've been unsuccessful. I've actually asked for advice on this in this sub reddit, but I was unsuccessful in implementing it. There are 14 processes with the command 'sleep', with a blank state and blank info. There are 7 processes with the command 'query', a state of 'sending data', and the info is a select query, but the query is very simple on a very small table. This is on a godaddy dedicated server. I contacted support and they told me to that it is indeed the sql database using 99% of resources. I didn't believe it either, but I thought I'd look into it at least first. The queries I had trouble joining are here: "SELECT LocationID, LocationName, BrandID, ( 3959 * acos( cos( radians($Latitude) ) * cos( radians( Latitude ) ) * cos( radians( Longitude ) - radians($Longitude) ) + sin( radians($Latitude) ) * sin( radians( Latitude ) ) ) ) AS distance FROM Locations HAVING distance &lt; 10 ORDER BY distance" AND "SELECT OfferID, CategoryID, Description FROM Offers WHERE LocationID = 'Locations.LocationID' AND StartDate &lt;= '$todaysDate' AND EndDate &gt;= '$todaysDate'" I know this isn't optimal, but I don't think this would bog down the server this much either.
I just killed all 14 of the sleep processes and the server is back at full speed. So I think this means I'm not ending my database connection properly?
The only time I've seen a variable be assigned a default value is when it is being defined as a parameter for a stored procedure. You will have to the declare the datatype and set the value separately. DECLARE @empid INT; SET @empid = 3
This is correct. You cannot declare and set a variable in the same statement in sql server, except for sp parameters as mentioned. 
This should work: DECLARE @empid INT=(3)
&gt; Note every one of these are off by a factor the counts of other tables. classic example of **cross-join effects** one-to-many, then one-to-many, so the resulting rows are many*many to solve this, you need subqueries to do the counting/summing
Properties of the database, not the server.
Since SQL Server 2008 you've been able to declare the initial value of local variables. The syntax is exactly as OP stated.
This is not correct. You can do that from 2008 upwards. From [MSDN](http://technet.microsoft.com/en-us/library/ms188927%28v=sql.100%29.aspx): DECLARE { {{ @local_variable [AS] data_type } | [ =value ] } | { @cursor_variable_name CURSOR } } [,...n] | { @table_variable_name [AS] &lt;table_type_definition&gt; | &lt;user-defined table type&gt; }
Look at the error message, that tells you exactly what is wrong. You were trying to insert a character string into a number field. Your insert statement seems to be the problem. The course_id column probably expects a number type and not a VARCHAR2, which is what the name is. Same for module_id. 
In both my creates of module_id and course_id they are VARCHAR2
At the top of the procedure course_id is declared as a number type. 
Works for me Declare @MyValue int = 1 Command(s) completed successfully. SELECT SERVERPROPERTY('productversion'), SERVERPROPERTY ('productlevel'), SERVERPROPERTY ('edition') productversion|productlevel|edition :----|:---|:--- 10.50.4000.0|SP2|Enterprise Edition (64-bit) SELECT @@VERSION Microsoft SQL Server 2008 R2 (SP2) - 10.50.4000.0 (X64) Jun 28 2012 08:36:30 Copyright (c) Microsoft Corporation Enterprise Edition (64-bit) on Windows NT 6.1 &lt;X64&gt; (Build 7601: Service Pack 1) (Hypervisor) 
Not working. I get "Incorrect syntax near '('."
Sorry I just noticed I submitted my older version which its down as a number and not varchar2. I'
&gt; course_id := 9999; Try putting 9999 between apostrophes if you're using a varchar datatype. Secondly the course_id varchar2 declaration that you changed at the top needs a string length (such as varchar2(8)). Thirdly are you sure that you want to insert the NAME value into the COURSE_ID column of the course_modules table and the COURSE_ID value into the MODULE_ID column? It doesn't seem exactly right.
This is the error I'm getting now. LINE/COL ERROR -------- ----------------------------------------------------------------- 2/5 PLS-00103: Encountered the symbol "NAME" when expecting one of the following: ( ; is with authid as cluster compress order using compiled wrapped external deterministic parallel_enable pipelined &gt;Thirdly are you sure that you want to insert the NAME value into the COURSE_ID column of the course_modules table and the COURSE_ID value into the MODULE_ID column? It doesn't seem exactly right. Yeah. I just want it to fill in empty course ids and then display an error message. Do you think I'm far off fixing this? Because if its too complex I'll just try something easier.
Try to run it like this and examine the small changes and tell me if you understand them: CREATE OR REPLACE PROCEDURE upda_course (p_name VARCHAR2) IS v_course_id VARCHAR2(10); cursor c1 is SELECT course_id FROM courses WHERE name = p_name; BEGIN open c1; fetch c1 into v_course_id; if c1%notfound then v_course_id := '9999'; end if; INSERT INTO course_modules ( course_id, module_id) VALUES ( p_name, v_course_id ); commit; close c1; EXCEPTION WHEN OTHERS THEN raise_application_error(-20001,'An error was encountered - '||SQLCODE||' -ERROR- '||SQLERRM); END upda_course; /
This the error now when I execute ERROR at line 1: ORA-20001: An error was encountered - -12899 -ERROR- ORA-12899: value too large for column "INS2014_106"."COURSE_MODULES"."COURSE_ID" (actual: 26, maximum: 8) ORA-06512: at "INS2014_106.UPDA_COURSE", line 27 ORA-06512: at line 1 
This is what I get when I type in SELECT @@version Microsoft SQL Server 2005 - 9.00.3042.00 (Intel X86) Feb 9 2007 22:47:07 Copyright (c) 1988-2005 Microsoft Corporation Express Edition on Windows NT 6.1 (Build 7601: Service Pack 1) I checked the database's compatibility level and it says SQL Server 2005 (90). I checked the drop down menu and I get no option to change it to SQL Server 2008. 
&gt; "COURSE_MODULES"."COURSE_ID" (actual: 26, maximum: 8) There you go, just what I was saying. You want to insert the NAME value which is a 26 character string into the COURSE_ID column which is an 8 character string (as per your table definition) and of course you cannot. I would suggest going back and re-thinking a little what you actually need this procedure to do.
Do you know why it is an SQL Server 2008 instance despite me using SQL Server 2008 R2?
Yeah, that is weird. Are you sure this is SQL 2008? The behavior you are referencing is exactly what I get when running these on SQL 2005.
The SQL 2008 client can connect to SQL 2005, 2008, 2012, 2000, 7, maybe 6.5 (who has those these days?). It's not your client that determines if this works, it's the server version.
It's not a SQL Server 2008 instance, it's 200**5**. This is determined by which version is installed on the server. What you have on your client machine doesn't matter as long as it's the same or higher than the server version.
&gt; So I think this means I'm not ending my database connection properly? Possibly yes But this really wouldn't explain 99% cpu usage. A connection in sleep state is just waiting to timeout from inactivity (or for more commands). Whilst it does use up one slot in mysql's 'max_connections', it uses only a small amount of ram, and negligible cpu.
&gt; There are 7 processes with the command 'query', a state of 'sending data' This may be where your cpu problem lies. Sending data doesn't actually mean simply transmitting data, but more like reading data from disk and processing, essentially performing the meat of the query Find what queries are executing here (you can get full text using SHOW FULL PROCESSLIST) I they are SELECT queries, then dump out an execution plan using EXPLAIN SELECT etc etc You can use this to confirm whether it is using the indices you want it to (if any!)
Well, I look at SQL for most of my day, and I can honestly say, I have not the slightest idea what you are trying to do there, but it does not look like something that should be done. Please give us the table structures, and what you want to achieve. I fear the code you posted is ... not helpful... 
Surely you weren't given a table with that structure:(Homework1..Homework11) It may not be the problem, but learning to write SQL for it is wasted time.
Alright, so I saved those two views then wrote this and it worked! SELECT RR."Schedule""Schedule", RR."Orig/Dest""Orig/Dest", MAX(RR."Coupon Miles")"Run Miles", COUNT(RR."Schedule")"Pcount", SUM(RR."Coupon Miles")"Pmiles", SUM(RR."Price")"Total Revenue", RR."Month""Month", RA."ADA""ADA", RA."Express""Express", RM."Runs This Month""Runs This Month", RM."Cancelled Runs""Cancelled Runs" FROM "Relational - Run Revenue" RR LEFT JOIN "Relational - Monthly ADA/Express" RA ON RR."Schedule"=RA."Schedule" AND RR."Month"=RA."Month" LEFT JOIN "Relational - Monthly Run Count" RM ON RR."Schedule"=RM."Schedule" AND RR."Month"=RM."Month" GROUP BY RR."Schedule", RR."Orig/Dest", RR."Month", RA."ADA", RA."Express", RM."Runs This Month", RM."Cancelled Runs" Is there a simple way of nesting the code within this one so I don't have to have two tables which I'll never use? Thanks!
I prefer the former, strictly based on the size of the query. On a smaller query, I'd implement the latter style. I prefer my predicates grouped together, but if there's 2 dozen of them you've got to break it up in some way to make it readable.
I'll quote myself from a few weeks ago with a sample query (i really am tired of typing it out). This is how you can work arround the lack of a window string concatination function in MSSQL, and it will be fast and it will scale : IF object_id ('tempdb.dbo.#shirts') is NOT NULL BEGIN DROP TABLE #shirts END CREATE TABLE #shirts(articlename VARCHAR(20), color VARCHAR(20), SIZE CHAR(2)) INSERT INTO #shirts SELECT 'foo', 'black', 'S' UNION SELECT 'foo', 'black', 'X' UNION SELECT 'foo', 'White', 'S' UNION SELECT 'foo', 'White', 'M' UNION SELECT 'foo', 'White', 'XL' UNION SELECT 'bar', 'Black', 'S' SELECT s.articlename ,s.color ,fun.stringAggregated FROM #shirts s CROSS APPLY ( SELECT STUFF(sq.colorAsXml,1,1,'') FROM ( SELECT '|' + s2.size FROM #shirts s2 WHERE s2.articlename = s.articlename AND s.color = s2.color FOR XML path('') )sq (colorAsXml) )fun(stringAggregated) should be easy enough to adjust to your needs and table structure I hope, if not I'll be happy to explain how that hack is working in a bit more detail
It's a "SELECT * WHERE Name = '$var'" on a small table that only has 1 field. I don't have this table indexed at all actually, because I figured since it's so small and simple it didn't have to be. Thinking back this is probably really dumb. Could just not having a table properly indexed make it get that bad?
Right, it's just strange though that it will take 35 seconds to load a basic web page, but if i kill all of the sleep processes everything is full speed again. I'm going to try killing the sending data processes and see if the sleep processes still make everything slow by themselves.
just accept it man, the majority of database professionals will prefer your colleague's syntax. 
I still struggle to see the similarity between example 1 and ansi89 syntax. In ansi89, you have a list of tables (from-clause) then a list of predicates (where-clause), so the predicates are separated from the tables they relate to. In example 1 it's at the other end of the spectrum; predicates related to a table are as close as possible to the introduction of that table. The need for the distinct is domain-specific...
All 7 of those queries in process list, are they all just this "select * where name = " query? If theres no index, it will do a table scan. Table scans are always bad, but would 7 concurrent table scans over 400k rows cause 99% cpu, even then I feel its unlikely (although possible). Either way, you almost certainly want an index on the Name column, at which point these simple queries will be very fast. What storage engine are you using (MyISAM, InnoDB etc) ? If you don't know, run SHOW TABLE STATUS and check the engine column for each table
to the first point, in ansi 89 all the logic is bunched up in the where clause. In your preferd code convention, everything is bunched up in the from clause. To me, its the other side of the same coin. To your second point, Its a bad sign in the sense of, either you have a problem with your query or you have a problem with your schema. Needing a distinct is not a good sign, period. You might need it, no question about that, but you *should* not need it /edit: on a reporting query, its a different matter, but this is very clearly not a report. I don't need to know the details to see that this is production code
http://en.wikipedia.org/wiki/Argumentum_ad_populum
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Argumentum ad populum**](http://en.wikipedia.org/wiki/Argumentum%20ad%20populum): [](#sfw) --- &gt; &gt;In [argumentation theory](http://en.wikipedia.org/wiki/Argumentation_theory), an **argumentum ad populum** ([Latin](http://en.wikipedia.org/wiki/Latin) for "**appeal to the people**") is a [fallacious argument](http://en.wikipedia.org/wiki/Fallacy) that concludes a [proposition](http://en.wikipedia.org/wiki/Proposition) to be true because many or most people believe it. In other words, the basic idea of the argument is: "If many believe so, it is so." &gt;This type of argument is known by several names, including **appeal to the masses**, **appeal to belief**, **appeal to the majority**, **appeal to democracy**, **appeal to popularity**, **argument by consensus**, **consensus fallacy**, **authority of the many**, and **bandwagon fallacy**, and in [Latin](http://en.wikipedia.org/wiki/Latin) as **argumentum ad numerum** ("appeal to the number"), and **consensus gentium** ("agreement of the clans"). It is also the basis of a number of social phenomena, including [communal reinforcement](http://en.wikipedia.org/wiki/Communal_reinforcement) and the [bandwagon effect](http://en.wikipedia.org/wiki/Bandwagon_effect). The Chinese [proverb](http://en.wikipedia.org/wiki/Proverb) "[three men make a tiger](http://en.wikipedia.org/wiki/Three_men_make_a_tiger)" concerns the same idea. &gt; --- ^Interesting: [^The ^Wisdom ^of ^Crowds](http://en.wikipedia.org/wiki/The_Wisdom_of_Crowds) ^| [^Conventional ^wisdom](http://en.wikipedia.org/wiki/Conventional_wisdom) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfto2kz) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfto2kz)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
granted. In your previouse post you where given many reasons thou, and I foresee you given reason yet again. I'm not calling your approach wrong, you just won't get the majority to agree with you. As luck has it, we don't work in the same department / company, so we don't have to fight this one out.
Well look at that. Our main ERP system is on 2005 so I haven't had a chance to do that, but I definitely will when we finish upgrading. Thanks for the correction.
Yes, they are all just a basic select query. The table they are querying isn't big at all, it's only 4k records. I'm using MyISAM. After indexing that table my process list shows only 2 instances of that query, but the states are now statistics and opening tables. The rest are gone. Site is still super slow though, and there are 16 sleep processes.
when you saved those two queries, did you save the data or did you save the query? big difference if it's the latter, you're fine when you reference those two queries like you did in this query, they simply bring in their totals at run time, there aren't actually any tables of data created nice job, by the way, understanding what i suggested
I prefer to come across your colleague's style in projects. I find it easier to digest, and easier to tweak since usually that's all that's needed. On the other hand, if I'm constructing such a query from scratch, I'd probably write it using your syntax due to the ease with which you can comment out entire tables to isolate and troubleshoot issues. When I had solved whatever errant issue cropped up, I would probably reformat it to the other style since I know I shouldn't need to mess with the joins and their predicates again, and it's solely filter from then on out.
why not try /r/SQLServer - might get more joy there.
Here's a simple way if you just want all the comments for a single "No_": DECLARE @Comments VARCHAR(8000) SELECT @Comments = COALESCE(@Comments + ', ', '') + Comment FROM &lt;yourtablename,,&gt; WHERE No_ = 'CT0008863' SELECT @Comments
I'm having no luck with this. Apologies, I am a complete newcomer to SQL. Additionally, I've thought about this a bit more and have need this to do one extra step - to iterate: Got three columns, A has a unique number, B has a filename, and C has the data. Any way to create a folder named after A, and inside for it to have the data from C saved with the filename from the B column I've had a look at import/export in Visual Studio and I thought I had come up with a working solution, but no joy :( any assistance would to greatly appreciated! an example of the format of the data - looks like someone has at one point uploaded a microsoft logo - this is mslogo20.jpg in column B and the data column contains: 0xFFD8FFE000104A4649460001010100C800C... etc etc
&gt; Site is still super slow though, and there are 16 sleep processes. Find a page on your website which is slow. Find out how many queries its actually running to render the page (including unravelling any loops etc). Its possible you are running lots and lots of short queries, which are making the page slow, but don't necessarily show up in SHOW PROCESSLIST 
This is unlikely to be the problem. Multiple table scans on a single small table will read from the block buffer pool and for a table with 4K records, index row_id access is unlikely to be much faster. However, it certainly doesn't hurt to add an index, especially if the rows are very wide and the 4K records span an unusually large number of blocks. You definitely should have a primary key (which will define an index, at least in Oracle) for the simple reason that this is good database design and also allows your table to scale better. 
I don't know if its worth it in most cases but its probably a good idea to replace FOR XML path('')) with FOR XML path, TYPE).value('.[1]', 'nvarchar(max)') That way you actually end up with the column text rather than the encoded column text. It probably doesn't matter for most cases but as soon as you have a few special characters in the text that you are joining you are going to run into trouble. ie I would do something like: IF object_id ('tempdb.dbo.#shirts') is NOT NULL BEGIN DROP TABLE #shirts END CREATE TABLE #shirts(articlename VARCHAR(20), color VARCHAR(20), SIZE VARCHAR(4)) INSERT INTO #shirts SELECT 'foo', 'black', '&lt;S&gt;' UNION SELECT 'foo', 'black', '&lt;X&gt;' UNION SELECT 'foo', 'White', '&lt;S&gt;' UNION SELECT 'foo', 'White', '&lt;M&gt;' UNION SELECT 'foo', 'White', '&lt;XL&gt;' UNION SELECT 'bar', 'Black', '&lt;S&gt;' SELECT s.articlename ,s.color , STUFF((SELECT '|' + s2.size FROM #shirts s2 WHERE s2.articlename = s.articlename AND s.color = s2.color FOR XML path, TYPE).value('.[1]', 'nvarchar(max)'),1,1,'') stringAggregated FROM #shirts s
&gt; His argument is that filter predicates go in the where-clause, join predicates go in the join clause, and never the twain shall meet. he is an idiot the first query is by far better and of course for outer joins, the first query also also gives correct results, the second doesn't
You won't have problems with special chars in there. The xml gets converted implicitly anyway, so there is no need to reparse the xml with xquery. 
Or now that I have gone this far :) IF object_id ('tempdb.dbo.#diaryNotes') is NOT NULL BEGIN DROP TABLE #diaryNotes END CREATE TABLE #diaryNotes(timestamp, [No_]VARCHAR(9), [Line No_] int, [Link No_] int, [Comment]varchar(max)) INSERT INTO #diaryNotes([No_], [Line No_] , [Link No_] , [Comment]) Select 'CT0008863', -10000, 12387 , 'Comment 1' Union Select 'CT0008863', -20000 , 12387 , 'Comment 2' Union Select 'CT0008863', -30000 , 12387 , 'Comment 3' Union Select 'CT0008863', -40000 , 12387 , 'Comment 4' SELECT [No_] ,[Link No_] , STUFF((SELECT char(13)+char(10)+CAST([Line No_] as varchar(10))+': ' + s2.[Comment] FROM #diaryNotes s2 WHERE s2.[Link No_] = s.[Link No_] ORDER BY [Line No_] Desc FOR XML path, TYPE).value('.[1]', 'nvarchar(max)'),1,2,'') FullComment FROM #diaryNotes s GROUP BY [No_],[Link No_] 
When you use path('') you are returning the raw XML as text, just with no tags. So since the text you want would be the contents of the tags, it will be returned as encoded. Try your example with the '&lt;S&gt;' data from my example, you will see that the return is still encoded. ie bar Black &amp;lt;S&amp;gt; foo black &amp;lt;S&amp;gt;|&amp;lt;X&amp;gt; foo black &amp;lt;S&amp;gt;|&amp;lt;X&amp;gt; foo White &amp;lt;M&amp;gt;|&amp;lt;S&amp;gt;|&amp;lt;XL&amp;gt; foo White &amp;lt;M&amp;gt;|&amp;lt;S&amp;gt;|&amp;lt;XL&amp;gt; foo White &amp;lt;M&amp;gt;|&amp;lt;S&amp;gt;|&amp;lt;XL&amp;gt; You need to extract the element with xquery to get the decoded text. Like I said for 99% of cases it won't matter , but I think for comments you can be pretty sure that someone will use a special character at some point. 
Jesus, OP, not you again! When are you going to accept that your style is not correct and no matter how many times you ask, very few people (if any) are going to agree with your ANSI 89 bundled predicate approach using ANSI 92 syntax style?!!! Once again, let me spell it out : * ANSI 92 joins were introduced to make it clearer in describing the relationships between tables (FK -&gt; PK, FK -&gt; AK, or implied relationships) used in queries. * This allowed "where" clauses to be distinct filtering clauses, i.e. predicates which act as a filter on the results of those joins. * ANSI 89 join syntax suffered from this lack of segregation, which made comprehension more difficult and had limitations in facilitating complex join scenarios (outer join on multiple tables and full outer join). What you are doing is effectively recreating ANSI 89 syntax in ANSI 92 form, so you've removed any benefit in the ANSI SQL committee's work! Learn from your colleague and people on here and just accept your style is non standard. Sometimes you may need to perform predicated outer joins, but that is then obvious, because it is an exception to the rule, not the rule. 
alright, I wrote something quick and dirty in vb.net for you. Adjust the connection string, the datatypes if necessary, and depending on the amount of data, you might want to do it in batches. At the very least you should get the idea from this. I know its not "nice" code, and there is no error handling, like I said quick and dirty... Imports System.IO Module Module1 Sub Main() Dim strCon As String Dim dbCon As SqlClient.SqlConnection Dim dbCmd As SqlClient.SqlCommand Dim dr As SqlClient.SqlDataReader Dim dt As New DataTable("files") dt.Columns.Add("fileID", GetType(Int32)) dt.Columns.Add("fileName", GetType(String)) dt.Columns.Add("binaryData", GetType(Byte())) strCon = "server=(local)\sql2008;database=master;trusted_connection = true" dbCon = New SqlClient.SqlConnection(strCon) dbCon.Open() dbCmd = New SqlClient.SqlCommand("SELECT fileID = 1234, fileName = 'test.txt', binaryData = 0x61626364656667", dbCon) dr = dbCmd.ExecuteReader() While dr.Read dt.Rows.Add(dr.GetValue(0), dr.GetValue(1), dr.GetValue(2)) End While dbCon.close Dim strSaveRootDir As String = "c:\test\" Dim strDirectoryFullPath As String Dim strFullPath As String = "" Dim bw As System.IO.BinaryWriter If Not Directory.Exists(strSaveRootDir) Then Directory.CreateDirectory(strSaveRootDir) End If For Each row As DataRow In dt.Rows strDirectoryFullPath = strSaveRootDir &amp; row("fileID").ToString &amp; "\" If Not Directory.Exists(strDirectoryFullPath) Then Directory.CreateDirectory(strDirectoryFullPath) End If strFullPath = strDirectoryFullPath &amp; row("fileName") bw = New BinaryWriter(File.Open(strFullPath, FileMode.Create)) bw.Write(row("binaryData")) bw.Flush() Next End Sub End Module 
I prefer the former for logical compartmentalisation. So its definitely how I would write ad hoc queries. It helps me think about what I am doing and why I am doing it. But for consistencies sake the later is much better. Mostly because the FROM clause can't follow the same pattern. So for all the joined tables the filters are associated with the table but in contrast for the FROM clause the filters are way way at the end of the query away from the table that they are referring to. So once I have worked out a query using the former if it was a query that was going to see some use rather than just a once off I would probably reformat it to the later style for ease of maintenance and consistency. Occasionally if I had a query that had parameters I might leave all the filters not associated with the parameters in the joins and have only filters associated with the parameters in the where clause. Thats just my style though. Like you say everyone is going to have their own preferences.
Sorry I'm a little late getting back to this, but here goes: I do a couple of subqueries. What I refer to as "A" is a list of custids and their number of orders. "B" is the same query as "A", and "C" is merely the maximum number of orders from "B". Then "D" is your ORDERS table. I join C to A on nbr_orders since C is the max and this grabs the custid(s) with the most orders. Then since I have the custid(s), I join to ORDERS to pull all the rows and then we're done.
Except that when consistency is important or beneficial, popularity is actually a valid argument.
[This presentation](http://www.cs.arizona.edu/~mccann/research/divpresentation.pdf) should get you started in the right direction.
Will do. Thanks.
Well, 16GB memory is not much, its a database server, not a workstation. Spinny disks for tempDB is meeeh too. Anyway, what kind of latches do you face? Do you have many recomputations of execution plans, whats the size of your log file (are you doing the log backups in a timely fashion). I would not look at the locks as a primary source, if you have queries taking a long time to execute, you will have locks. Locks are a good thing. The latches are bad, but without knowing what type of latches you experiance, it could litteraly be anything.
I may be able to juice up the memory. As for spinny disk, thats my only option right now, SSD may be in future budget plans. Logs are backedup in 15 minute intervals and logshipped to my 2nd office as a warm standby. Right now the log is 10GB with 98% free space. As for what kind of latches - this is where the sysadmin in me and the "I'm not a DBA" cross. Can you help me determine this information? Thanks 
have a look at this blog post, http://www.sqlskills.com/blogs/paul/wait-statistics-or-please-tell-me-where-it-hurts/ . Its better written than I ever could. It can be a bit cryptic to interpret the wait stats, but it will give you great insight on where your bottleneck exists. Additionally, I would have a look in sys.dm_exec_query_stats. This mangement view contains infromation of currently cached execution plans. One thing you can look up there is how often these execution plans have been executed. This can often be a problem with "sub optimal" applications. /edit: The problem you could have with the execution plans would be a large number of single use plans, plans that only get used once. That would cause a large number of plan recompilations, and its not uncommon to have complex queries take up seconds to get compiled, and that would be pure destilled cpu pressure right there. 
thanks. Looking up these things now. 
keep us in the loop, chances are we can give you some pointers once its a bit more clear where the problems lie.
I personally write my code like number 2
Any chance this EMR app is NextGen? I know they include a stored procedure (ng_checklist) to verify you have the database configured correctly for the application, perhaps your vendor does too. You can also try out [sp_askbrent](http://www.brentozar.com/askbrent/). Its a free stored procedure that you can use to help diagnose problems, just download the script and run it (at the end of the script there is an example of how to use it). They have some other handy scripts and some good blog posts that you might find useful. 
A recursive CTE would work too: with SourceT as( select no_= 'CT0008863',line_no = -10000, link_no = 12387, txt = cast('Comment 1 ' as varchar(1000)) union all select 'CT0008863' ,-20000, 12387 , ' Comment 2 ' union all select 'CT0008863' ,-30000, 12387 , ' Comment 3 ' union all select 'CT0008863' ,-40000 , 12387 , ' Comment 4 ' ), RecurseT(no_ , line_no, link_no, txt) as( select no_, line_no = min( line_no), link_no, max(txt) from ( select no_, line_no, link_no, txt = first_value( txt) over( partition by no_, link_no order by line_no asc) from SourceT )t group by no_, link_no union all select SourceT.no_, SourceT.line_no, SourceT.link_no, cast( concat( SourceT.txt, RecurseT.txt) as varchar(1000)) from RecurseT join SourceT on SourceT.link_no = RecurseT.link_no and SourceT.line_no = RecurseT.line_no + 10000 ) select * from RecurseT where line_no = -10000
The result of the query might not be a set. After adding 'distinct', it could be written something like this: Pi{PName,Lname}( Sigma{Plocation = 'Houston'}( (Employee |x|{SSN=Mgr_SSN} Department) |x|{Dnum=Dnumber}Project ) )
 CREATE TABLE User (UserID, [other stuff]) CREATE TABLE Widget (WidgetID, [other stuff]) CREATE TABLE UserWidget(UserID, WidgetID, Order) CREATE TABLE UserWidgetSetting (UserID, WidgetID, SettingKey, SettingValue) INSERT INTO User VALUES (0, [other stuff]) --User to hold default widgets INSERT INTO Widget VALUES (1, [other stuff]), (2, [other stuff]) --Some widgets INSERT INTO UserWidget VALUES (0, 1, 1), (0, 2, 2) --Default widgets for default user INSERT INTO UserWidgetSetting VALUES (0, 1, 'SomeWidgetSettingKey', 'TheValue') --Optional widget setting for default user Whenever you create a new user, you copy the stuff from the default user with new UserID. This was a really open ended and poorly defined question but I hope this gives you some ideas at least. 
yep, so you know which software I'm running.. :(
Thanks, I did find the askbrent script yesterday and it was very helpful. And no, not NextGen, something much crappier.
I think i figured it out ... kind of. Definitely running out of disk space. It looks like the .TRN files have been stored since the beginning of time and the log folder is nearly a TB. I found where you can kill the .bak files because it was already there. How do i add something to delete old .TRN files?
Do you have a set number of different Tags? If so, you could have columns with your posts referring to each different tag and a bit operator as a value to declare if that tag is associated with that post.
Nevermind, got it. Created a subplan to delete TRN files. Problem solved. Thank you.
Should be the same thing - look in the maintenance plan for the Transaction Log Backup task. There will be a "Backups will expire..." option, pick whatever day range you feel is appropriate based on business needs / storage space. I tend towards 14, but that's me.
 SELECT p1.pid, p2.pid FROM posts p1 CROSS JOIN posts p2 WHERE NOT EXISTS (SELECT 1 from taggings t1 WHERE t1.pid = p1.pid AND NOT EXISTS (SELECT 1 from taggings t2 WHERE t2.pid = p2.pid AND t2.tid = t1.tid)) AND NOT EXISTS (SELECT 1 from taggings t1 WHERE t1.pid = p2.pid AND NOT EXISTS (SELECT 1 from taggings t2 WHERE t2.pid = p1.pid AND t2.tid = t1.tid)) AND p2.pid &gt; p1.pid So the cross join will give us every possible set of pairs of posts - comibned with the last AND clause - requiring p2 to be strictly greater than p1 will eliminate duplicates (a,b will be included and b,a will not) and self-pairs (a,a will not be included). The two NOT EXISTS clauses are mirrors of each other, and what they basically say is "make sure there is not a tagging for the first PID in the pair that does not exist for the second PID in the pair" and then "make sure there is not a tagging for the second PID in the pair that does not exist for the first PID in the pair" I haven't tested it - so start there and see how it goes. Comment back if you run into issues. EDIT: Was missing parens. EDIT2: I gave it a quick test and I think it works. Not sure how well it will scale but it gets the job done. 
Do you mean you want to know all of the branchNo's that are duplicated? That can be done with a simple GROUP BY and HAVING: SELECT branchNo FROM whateverthistableiscalled GROUP BY branchNo HAVING COUNT(*) &gt; 1 
ok so, in looking at waits.. select session_id,wait_duration_ms,wait_type from sys.dm_os_waiting_tasks order by wait_duration_ms DESC session_id wait_duration_ms wait_type 12 3785779189 SP_SERVER_DIAGNOSTICS_SLEEP 14 3785753819 ONDEMAND_TASK_QUEUE 4 3785750272 KSOURCE_WAKEUP 27 3519595993 FT_IFTSHC_MUTEX 22 3511623407 BROKER_TRANSMITTER 23 3511622322 BROKER_TRANSMITTER NULL 1011400 CLR_AUTO_EVENT NULL 1011400 CLR_AUTO_EVENT 31 522686 FT_IFTS_SCHEDULER_IDLE_WAIT 15 331294 CHECKPOINT_QUEUE 7 109413 XE_DISPATCHER_WAIT 21 37682 FT_IFTS_SCHEDULER_IDLE_WAIT NULL 35682 FT_IFTS_SCHEDULER_IDLE_WAIT 92 14888 BROKER_RECEIVE_WAITFOR 20 5580 BROKER_EVENTHANDLER 34 4797 BROKER_TASK_STOP 8 3216 XE_TIMER_EVENT 11 2102 SQLTRACE_INCREMENTAL_FLUSH_SLEEP 5 1833 REQUEST_FOR_DEADLOCK_SEARCH 3 992 LAZYWRITER_SLEEP 24 924 BROKER_TO_FLUSH 1 104 LOGMGR_QUEUE 2 20 DIRTY_PAGE_POLL 25 16 HADR_FILESTREAM_IOMGR_IOCOMPLETION Looking up the big ones on top seem to be normal, as thats just sleepy processes with nothing to do. I'm not sure really how to interepret the rest, but in looking up each wait_type none seem to be too ridiculous? I could be totally wrong.
So select * from the table where branch no is in the set of branch numbers returned by the group / having example you have?
 object_name counter_name cntr_value SQLServer:Buffer Manager Page life expectancy 12074 No idea what to make of that. It seems to be stable at that number
Doing this from my phone, so formatting will suck. Select * From whatever this table is where branchno in ( select branchno From whatever table this is Group by branchno Having count(*) &gt; 1 )
Then you want to follow the advice of [dublos' comment](http://www.reddit.com/r/SQL/comments/1zmpag/what_operatorstatementwhatever_its_called_should/cfv3sof)
thank you for clarifying. In my google searches it almost seemed my number was out of the ballpark too high and something was wrong there.
I'll try to post some code here. This is Paul Randal's wait query, and he's eliminated the ones we don't need to worry about. Run this, it should give you a better idea. WITH [Waits] AS (SELECT [wait_type], [wait_time_ms] / 1000.0 AS [WaitS], ([wait_time_ms] - [signal_wait_time_ms]) / 1000.0 AS [ResourceS], [signal_wait_time_ms] / 1000.0 AS [SignalS], [waiting_tasks_count] AS [WaitCount], 100.0 * [wait_time_ms] / SUM ([wait_time_ms]) OVER() AS [Percentage], ROW_NUMBER() OVER(ORDER BY [wait_time_ms] DESC) AS [RowNum] FROM sys.dm_os_wait_stats WHERE [wait_type] NOT IN ( N'BROKER_EVENTHANDLER', N'BROKER_RECEIVE_WAITFOR', N'BROKER_TASK_STOP', N'BROKER_TO_FLUSH', N'BROKER_TRANSMITTER', N'CHECKPOINT_QUEUE', N'CHKPT', N'CLR_AUTO_EVENT', N'CLR_MANUAL_EVENT', N'CLR_SEMAPHORE', N'DBMIRROR_DBM_EVENT', N'DBMIRROR_EVENTS_QUEUE', N'DBMIRROR_WORKER_QUEUE', N'DBMIRRORING_CMD', N'DIRTY_PAGE_POLL', N'DISPATCHER_QUEUE_SEMAPHORE', N'EXECSYNC', N'FSAGENT', N'FT_IFTS_SCHEDULER_IDLE_WAIT', N'FT_IFTSHC_MUTEX', N'HADR_CLUSAPI_CALL', N'HADR_FILESTREAM_IOMGR_IOCOMPLETION', N'HADR_LOGCAPTURE_WAIT', N'HADR_NOTIFICATION_DEQUEUE', N'HADR_TIMER_TASK', N'HADR_WORK_QUEUE', N'KSOURCE_WAKEUP', N'LAZYWRITER_SLEEP', N'LOGMGR_QUEUE', N'ONDEMAND_TASK_QUEUE', N'PWAIT_ALL_COMPONENTS_INITIALIZED', N'QDS_PERSIST_TASK_MAIN_LOOP_SLEEP', N'QDS_CLEANUP_STALE_QUERIES_TASK_MAIN_LOOP_SLEEP', N'REQUEST_FOR_DEADLOCK_SEARCH', N'RESOURCE_QUEUE', N'SERVER_IDLE_CHECK', N'SLEEP_BPOOL_FLUSH', N'SLEEP_DBSTARTUP', N'SLEEP_DCOMSTARTUP', N'SLEEP_MASTERDBREADY', N'SLEEP_MASTERMDREADY', N'SLEEP_MASTERUPGRADED', N'SLEEP_MSDBSTARTUP', N'SLEEP_SYSTEMTASK', N'SLEEP_TASK', N'SLEEP_TEMPDBSTARTUP', N'SNI_HTTP_ACCEPT', N'SP_SERVER_DIAGNOSTICS_SLEEP', N'SQLTRACE_BUFFER_FLUSH', N'SQLTRACE_INCREMENTAL_FLUSH_SLEEP', N'SQLTRACE_WAIT_ENTRIES', N'WAIT_FOR_RESULTS', N'WAITFOR', N'WAITFOR_TASKSHUTDOWN', N'WAIT_XTP_HOST_WAIT', N'WAIT_XTP_OFFLINE_CKPT_NEW_LOG', N'WAIT_XTP_CKPT_CLOSE', N'XE_DISPATCHER_JOIN', N'XE_DISPATCHER_WAIT', N'XE_TIMER_EVENT') ) SELECT [W1].[wait_type] AS [WaitType], CAST ([W1].[WaitS] AS DECIMAL (16, 2)) AS [Wait_S], CAST ([W1].[ResourceS] AS DECIMAL (16, 2)) AS [Resource_S], CAST ([W1].[SignalS] AS DECIMAL (16, 2)) AS [Signal_S], [W1].[WaitCount] AS [WaitCount], CAST ([W1].[Percentage] AS DECIMAL (5, 2)) AS [Percentage], CAST (([W1].[WaitS] / [W1].[WaitCount]) AS DECIMAL (16, 4)) AS [AvgWait_S], CAST (([W1].[ResourceS] / [W1].[WaitCount]) AS DECIMAL (16, 4)) AS [AvgRes_S], CAST (([W1].[SignalS] / [W1].[WaitCount]) AS DECIMAL (16, 4)) AS [AvgSig_S] FROM [Waits] AS [W1] INNER JOIN [Waits] AS [W2] ON [W2].[RowNum] &lt;= [W1].[RowNum] GROUP BY [W1].[RowNum], [W1].[wait_type], [W1].[WaitS], [W1].[ResourceS], [W1].[SignalS], [W1].[WaitCount], [W1].[Percentage] HAVING SUM ([W2].[Percentage]) - [W1].[Percentage] &lt; 95; -- percentage threshold GO 
How do you know something is wrong? Is there an error message or something or is the row just not being updated? The first `[` after the `SET` looks superfluous though.
Im trying to assign Bracket to EmailAddress so we can keep track of them. 
Is this a .NET SQLDataSource object?? I have a little experience in this area.... First, you have a parameter "Bracket" which is not listed in your query. Second you have a parameter in your query (@id) which is not referenced in your parameter list. This will cause problems as @id has no way of being filled in and the parameter "Bracket" will never be used. Your submit command has an extra '[' at the beginning I am pretty sure. I do not see a query which updates the EmailAddress field which is the one I think you want to update?
I was able to clean it up a bit... &lt;addForm&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt; &lt;label for="uplFile" text="Upload File" style="font-weight: bold;"/&gt; &lt;fileupload id="uplFile" path="~/MarchMania/" extensions="gif,jpg,zip,tif" datafield="uplFile" datatype="string" style="margin-left: 35px;"/&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;SelectCommand CommandText="SELECT [EmailAddress] FROM [dnn_ahno_HondaMarchMadness] WHERE [EmailAddress]=@Bracket"&gt; &lt;parameter name="Bracket" value=[[Form:EmailAddress]] /&gt; &lt;/SelectCommand&gt; &lt;SubmitCommand CommandText="UPDATE [dnn_ahno_HondaMarchMadness] SET [signature]=@signature WHERE [EmailAddress]=@EmailAddress" /&gt; dfsdfdfc [[EmailAddress]] &lt;/addForm&gt; But I think you are right, Im missing the query which would update the EmailAddress. 
could I use the IpAddress field I have in there as a good way to track against the email? UPDATE [dnn_ahno_HondaMarchMadness] SET [EmailAddress] = @EmailAddress WHERE [IpAddress] = @IpAddress ?
DNN
I'm not sure I understand your question very well, but have you thought about using Temp Tables, or CTEs? When I am using multiple queries like you're doing, and then joining data at the end, I normally prefer using CTEs to keep my data separated, and clean. Again, I could be misunderstanding you entirely here.
My mistake, I guess I missed some information in my question; I've been looking at this for so long I forget this isn't what everyone else in the world is focused on! What I'm trying to do in the final table is index all related data by month. After posting [here](http://www.reddit.com/r/SQL/comments/1zfz9e/joining_three_tables_aggregate_columns_aggregate/) I learned I was accidently doing cross-joins, causing all my data to be counted multiple times. From there I created multiple queries, each one indexing a single table, then joining those together using the final table. Since I have many more tables to work with I was thinking this is going to get quite cumbersome. I was hoping there was a way to nest the top three queries into the bottom, but I haven't been able to successfully do so (very new at SQL!) Does that make sense?
A common problem I've seen more than a few different times is someone writes a stored procedure to calculate some hard to calculate variable for an account. The original intent of the procedure was to display this information on an account level screen, where only one account number would ever be fed to the SP, so the .1 second process time is very livable. Someone else comes along and writes a whole-system view and includes that SP in the view to get at that one hard to get statistic. Now it's .1 seconds times a few million rows of data and everyone is shocked when the whole server is slow due to several of these jobs just slowing crunching along in the background.
This is looking much better. Is the value for the @EmailAddress in the update statement being set somewhere? The select command seems fine. I'm not sure how this front-end code works coming mainly from a database background myself. What you probably need to do is something like this, and I have no idea if this will work with your system: &lt;SubmitCommand CommandText="UPDATE [dnn_ahno_HondaMarchMadness] SET [signature]=@Bracket WHERE [EmailAddress]=@EmailAddress"&gt; &lt;parameter name="Bracket" value=[[Form:Bracket]]" /&gt; &lt;parameter name="EmailAddress" value=[[Form:EmailAddress]] /&gt; &lt;/SubmitCommand&gt; This assumes that "signature" is the name of the field you want to store the bracket value into. 
Yea I figured that out right as I was leaving work. Basically, they upload their jpg of the bracket, and I need to assign something to it, an identifying element so we can know who is who on the server. I dont have a field "signature" in my table, since I dont have that on my form. If that makes sense. Im going to try this tomorrow and see. Thanks for your help!
ANSI 92 attempted to segregate the relationships between sets of data involved (JOIN - preprocessing), from the removal of set members we don't want (WHERE - post processing). But ANSI 92 is flexible though it allows you to add filtering predication to pre-filtering where you require. In any case all modern RDBMS optimizers are intelligent enough to restructure and push down WHERE to JOIN if the explain plan would benefit. So query structure is largely irrelevant other than for clarity, which brings me back to one of the most important parts of the ANSI 92 SQL changes : increase clarity of SQL statements. To aid readability, comprehension, maintainability and debugging. In other words instead of having a huge predicate list in a WHERE clause (ANSI 89) which no-one could follow, we have something that is structured and implicitly more self descriptive. OK, so let's all start using your style, because you are so convinced you are correct. You write a query where a column is filtered in a JOIN. You give it to a third party or a newbie, but he doesn't have a physical data model of the table relationships, or permissions to interrogate the data dictionary (which trust me, is very common in production environments). How would he know as a developer / customer that that filter was not part of the foreign key -&gt; Primary Key referential integrity? He wouldn't - so straight away there is a problem. He could get an incorrect perception of the relationships between the tables. He could start writing new queries assuming that column was part of a primary key. He's made an implied relationship because **you** haven't followed good practices. Good SQL developers follow the segregation of JOIN and WHERE predicates, because it is clear differentiation between how tables relate to each other and how datasets are filtered, rather than for compartmentalisation like object oriented class encapsulation as you are interpreting it. Unfortunately, no amount of reasoning with you is going to make you change your mind, because you think you're correct and all of your posts are in seeking approval and arguing of justification of this. When I have more time, I'll see if I can find those references at some point, but in the meantime I'll let you carry on with your pointless quest for approval of your approach.
You just need to filter your results using a where clause. SELECT B.street, B.postcode, B.city FROM branch B INNER JOIN staff S on B.branchNo = S.branchNo WHERE S.fname = 'John' and S.lname = 'White' (edited code for clarity - thanks sidebar!)
What is this B you are using?
B is the alias I've given to table Branch ("from Branch B") - it saves me having to type the full table name each time. I did the same for Staff ("staff S"). Anyways, that's not the important bit - the where clause is what does the bit you needed.
Oh I see now, thank you man :)
Look into adding a [**Bitmap index**](http://www.dba-oracle.com/oracle_tips_bitmapped_indexes.htm) - these are ideal indexes for low cardinality scenarios. Partitions are another option you might consider. 
Sure, add a 'ran date' to the table, select the one with the min(ran date). Update the ran date at the end of the stored procedure.
Well, if you are the DBA, I'd say you certainly can and should adjust the stored procedures if they are not written as you would like. Obviously don't change the inputs or outputs, but it's your data, it's your database. The indexes, statistics, triggers, and clustered keys are yours to choose and modify. That being said, your software vendor is not in the business of tweaking performance as new versions of sql come along. They want to make the minimal changes as possible, because all changes have to be validated and reviewed, and that slows down the release process. I'd suggest you start talking to product support for your particular product and see if they have any ideas. Source: I work for a medical technology company, and I just pray one of our customers will call and say "I'm dropping all your indexes and making my own! I'm also enabling row versioning. And your triggers cause database bloat!" I'd love to be able to tell you to do those things.
Yes, this is a simple [**modulo**](http://en.wikipedia.org/wiki/Modulo_operation) operation. You create a table which stores a row for each shift. The ordinal position (PK) with the shift number. You work out the number of days passed from a date epoch. This epoch date is static but refers to the first shift in ordinal order. Ordinal position of next shift = ((Number of days since epoch) modulo (Number of shifts in shift table)) + 1 This approach allows extensibility, you can change the epoch, the shift orders and add new shifts.
Smart solution, thanks!
I love this answer and as rogue and oldschool of a sysadmin as I am, even I don't have the balls to f- with this database. 
Is there anyway for me to connect to a SQL 2008 server? I connected to the default SQLEXPRESS server (.\sqlexpress) and I assumed it would connect to the SQL 2008 server..[Picture](http://i.imgur.com/wSDVV9g.jpg)
Well, unless two people upload their bracket from the same computer, that could cause some problems.
yea, so I decided against that and wanna associate it with their email, since its rare that two people will have the same email address. 
But the data / intervals are quite skewed. The avg interval length is around 7. The standard deviation around 200000, though! Do you think that bitmap indexes still make sense?
You would need to install or upgrade the server to 2008+. The 9.0.. in your screenshot means SQL 2005. While you're at it, you can upgrade to 2012. Download that from here. http://www.microsoft.com/en-us/sqlserver/editions/2012-editions/express.aspx
Not sure why the original question specifies lower selectivity for higher values of the parameter. Given that the ranges never overlap, the selectivity for the index would be always 1. I haven't worked with Oracle for a while, but it appears that the A_PK is not being treated as a covering index (there's a table access after that). Please make sure that you have a unique index on Col1, with included Col2. If higher values of the parameter give you trouble, try reversing the order for Col1 in the index (i.e. Col1 DESC). You wouldn't need RowNum condtition and the hint in that case, I believe.
Hi. You mentioned that the table is at least a few million records big. Is partitioning an available option for your database? 
&gt; Given that the ranges never overlap, the selectivity for the index would be always 1. Yes, but Oracle doesn't know that. If I could express a constraint that could be picked up by the CBO, then this query would be much faster. &gt; but it appears that the A_PK is not being treated as a covering index (there's a table access after that). True, the query is simplified. Once the predicate matches, more columns are selected. That shouldn't have any performance impact, though. &gt; If higher values of the parameter give you trouble, try reversing the order for Col1 in the index (i.e. Col1 DESC). On average, the results will be the same. If the order is reversed, then high values will produce results quickly, and low values will produce results slowly. &gt; You wouldn't need RowNum condtition and the hint in that case, I believe. Right now, I'm afraid that these hints are necessary. Otherwise, the same query takes up to 8 seconds...
It is, yes. I'd like to avoid it, though, as the "partitions" (intervals) change quite often and are quite skewed. Unless I'm missing a particular feature here?
I believe so. Put a bitmap index (BI) on both COL1 and one on COL2, analyse them and run your examples again. In this situation the CBO can perform a BITMAP MERGE (which is fast) and I believe you should notice a huge improvement for these sorts of low cardinality, high deviation situations. Come back with your results!
When you say they change often, do you mean you perform a lot of UPDATE statements on both of these columns? How about INSERTs and DELETEs, how often do these happen? If indeed you update a lot all the time then maybe bitmap indexes aren't really suited for this situation. How about statistics, how often are those gathered and how? Have you tried running an SQL Tuning Advisor task on your (original) statement and did you learn anything new from there? How often is this table accessed, especially concurrent access? Perhaps the table is fragmented, do you perform any scheduled shrink operations on it? How about the index clustering factor, how is that compared to the total number of data blocks and row nums in the table?
You are not going to find many supporters in this sub. When writing SQL it is extremly important to the performance of a query, to keep details like the indexing on the relevant tables in mind. You can also have huge differences in the execution time, when writing a certain logic in different ways. By proposing a even higher level of abstraction (i would say SQL is already a very high level language), you are effectifly preventing the programmer to write scalable SQL. That might be alright for the simplest queries, but we already have ORM for that, and a huge number of database professionals hate ORM with a passion in any case (cause it leads to bad practices). As for this quote : &gt; a tremendous amount of implementation detail is left invisible to the programmer I can tell my SQL Server, to give me the execution plan (which gets compiled, so SQL already gets compiled into something else), showing me EXACTLY what happens below the hood. What more do you want? I don't really see the point to introducing an additional level of abstraction into database programming. And I would fight to the last drop of blood, preventing it from being introduced in my environment I have to say.
Move the Where clause below the group by and change Where to Having and that should work. Must admit I work with Teradata SQL so MySQL might be different. I'm sure some more knowledgeable types can confirm...
Thanks :)
Thanks :)
Have you created a table type for the this table? Here is a good article on this subject http://www.techrepublic.com/blog/the-enterprise-cloud/passing-table-valued-parameters-in-sql-server-2008/ 
Fair enough. Perhaps it might be that programmers need to learn more about query tuning and performance? 
[Image](http://imgs.xkcd.com/comics/exploits_of_a_mom.png) **Title:** Exploits of a Mom **Title-text:** Her daughter is named Help I'm trapped in a driver's license factory. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=327#Explanation) **Stats:** This comic has been referenced 140 time(s), representing 1.1656% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)
You should be able to just join them something like this. Sorta ghetto, but it should work. SELECT * FROM table1 AS ta INNER JOIN table2 AS tb ON ta.Username LIKE '%' + tb.username + '%'
It depends on what you are doing with the database, and if you have access to a database dev / admin. If you have a professional sitting in the room down the corridor, well, just go and ask him for his input or help on non trivial or performance critical stuff. Thats also a great way to learn. If you don't have a resource like that, well yes, frontend developers should know at least the basics about how a database works I'd say. I would call stuff like sargable vs non sargable the bare minimum. 
Could you provide the names of both tables and all the columns from both tables, if you do this we can essentially write the query for you. Edit:or even screenshots.
I believe Africa should be below Asia (DESC, right?). In any case, do things change if you ORDER BY LTRIM(Continent) DESC?
A temp table (declared as "`CREATE TABLE #TableName (columns...)`") can't be passed as a parameter. Are you talking about a table variable (declared as "`DECLARE @TableName TABLE (columns...)`")? Temp tables can't be passed as parameters, though it would be visible to anything else running on the same connection. Table variables can be but their type must be a user defined table type.
yes, ta is an alias for in this case table1. After the like, he is concatanating the column tb.username, with the wildcards '%'. In essence, while that query will be kind of the worst thing you could ever do performance wise (its an example, please don't do shit like that in production code), that join is pretty much equivilant to this : SELECT * FROM table1 as ta WHERE userName like '%user1%' OR userName like '%user2%' OR userName like '%user3%' ........ For any username present in that hypothetical table2. 
Here is what I am talking about: SELECT * INTO #TEMP FROM dbo.Example EXEC [dbo].[Example_proc] @QUERY = 'SELECT ID FROM tempdb.#TEMP', @FILENAME = 'ExampleFile' Thats what i am doing with it. 
Yeah you are right and that did work :) However I am curious about why **ORDER BY Continent DESC** won't work... Can you check out my other thread aswell? Thank you in advance! http://www.reddit.com/r/SQL/comments/1zqwce/mysql_show_total_population_based_on_two/
Try this: `@QUERY = 'SELECT ID FROM #TEMP',`
I'm guessing the reason it didn't work is that the record for Africa actually contains " Africa" (with a leading space) in the Continent column. For me, I'd say about 9 times out of 10 if a character based column isn't sorting properly, it's due to extraneous space characters.
This is the same error I always get: &gt;SQLState = S0002, NativeError = 208 &gt;Error = [Microsoft][SQL Server Native Client 10.0] &gt;[SQL Server]Invalid object name '#TEMP'.
How are you executing this query in the procedure?
Thanks, I think for the most part this is probably the correct approach. I appreciate your input.
 DECLARE @sql varchar(8000) SELECT @sql = @QUERY 
Oh, I see what LTRIM is doing now. :P But how come I can't see the extra space?
So, what have you got so far?
Yea thank you, but what is the 0 doing?
-- Insert data into the tables INSERT INTO categories (category_id, category_name) VALUES (1, 'Guitars'), (2, 'Basses'), (3, 'Drums'), (4, 'Keyboards'); INSERT INTO products (product_id, category_id, product_code, product_name, description, list_price, discount_percent, date_added) VALUES (1, 1, 'strat', 'Fender Stratocaster', 'The Fender Stratocaster is the electric guitar design that changed the world. New features include a tinted neck, parchment pickguard and control knobs, and a ''70s-style logo. Includes select alder body, 21-fret maple neck with your choice of a rosewood or maple fretboard, 3 single-coil pickups, vintage-style tremolo, and die-cast tuning keys. This guitar features a thicker bridge block for increased sustain and a more stable point of contact with the strings. At this low price, why play anything but the real thing?\r\n\r\nFeatures:\r\n\r\n* New features:\r\n* Thicker bridge block\r\n* 3-ply parchment pick guard\r\n* Tinted neck', '699.00', '30.00', '2011-10-30 09:32:40'), (2, 1, 'les_paul', 'Gibson Les Paul', 'This Les Paul guitar offers a carved top and humbucking pickups. It has a simple yet elegant design. Cutting-yet-rich tone?the hallmark of the Les Paul?pours out of the 490R and 498T Alnico II magnet humbucker pickups, which are mounted on a carved maple top with a mahogany back. The faded finish models are equipped with BurstBucker Pro pickups and a mahogany top. This guitar includes a Gibson hardshell case (Faded and satin finish models come with a gig bag) and a limited lifetime warranty.\r\n\r\nFeatures:\r\n\r\n* Carved maple top and mahogany back (Mahogany top on faded finish models)\r\n* Mahogany neck, ''59 Rounded Les Paul\r\n* Rosewood fingerboard (Ebony on Alpine white)\r\n* Tune-O-Matic bridge with stopbar\r\n* Chrome or gold hardware\r\n* 490R and 498T Alnico 2 magnet humbucker pickups (BurstBucker Pro on faded finish models)\r\n* 2 volume and 2 tone knobs, 3-way switch', '1199.00', '30.00', '2011-12-05 16:33:13'), (3, 1, 'sg', 'Gibson SG', 'This Gibson SG electric guitar takes the best of the ''62 original and adds the longer and sturdier neck joint of the late ''60s models. All the classic features you''d expect from a historic guitar. Hot humbuckers go from rich, sweet lightning to warm, tingling waves of sustain. A silky-fast rosewood fretboard plays like a dream. The original-style beveled mahogany body looks like a million bucks. Plus, Tune-O-Matic bridge and chrome hardware. Limited lifetime warranty. Includes hardshell case.\r\n\r\nFeatures:\r\n\r\n* Double-cutaway beveled mahogany body\r\n* Set mahogany neck with rounded ''50s profile\r\n* Bound rosewood fingerboard with trapezoid inlays\r\n* Tune-O-Matic bridge with stopbar tailpiece\r\n* Chrome hardware\r\n* 490R humbucker in the neck position\r\n* 498T humbucker in the bridge position\r\n* 2 volume knobs, 2 tone knobs, 3-way switch\r\n* 24-3/4" scale', '2517.00', '52.00', '2012-02-04 11:04:31'), (4, 1, 'fg700s', 'Yamaha FG700S', 'The Yamaha FG700S solid top acoustic guitar has the ultimate combo for projection and pure tone. The expertly braced spruce top speaks clearly atop the rosewood body. It has a rosewood fingerboard, rosewood bridge, die-cast tuners, body and neck binding, and a tortoise pickguard.\r\n\r\nFeatures:\r\n\r\n* Solid Sitka spruce top\r\n* Rosewood back and sides\r\n* Rosewood fingerboard\r\n* Rosewood bridge\r\n* White/black body and neck binding\r\n* Die-cast tuners\r\n* Tortoise pickguard\r\n* Limited lifetime warranty', '489.99', '38.00', '2012-06-01 11:12:59'), (5, 1, 'washburn', 'Washburn D10S', 'The Washburn D10S acoustic guitar is superbly crafted with a solid spruce top and mahogany back and sides for exceptional tone. A mahogany neck and rosewood fingerboard make fretwork a breeze, while chrome Grover-style machines keep you perfectly tuned. The Washburn D10S comes with a limited lifetime warranty.\r\n\r\nFeatures:\r\n\r\n * Spruce top\r\n * Mahogany back, sides\r\n * Mahogany neck Rosewood fingerboard\r\n * Chrome Grover-style machines', '299.00', '0.00', '2012-07-30 13:58:35'), (6, 1, 'rodriguez', 'Rodriguez Caballero 11', 'Featuring a carefully chosen, solid Canadian cedar top and laminated bubinga back and sides, the Caballero 11 classical guitar is a beauty to behold and play. The headstock and fretboard are of Indian rosewood. Nickel-plated tuners and Silver-plated frets are installed to last a lifetime. The body binding and wood rosette are exquisite.\r\n\r\nThe Rodriguez Guitar is hand crafted and glued to create precise balances. From the invisible careful sanding, even inside the body, that ensures the finished instrument''s purity of tone, to the beautifully unique rosette inlays around the soundhole and on the back of the neck, each guitar is a credit to its luthier and worthy of being handed down from one generation to another.\r\n\r\nThe tone, resonance and beauty of fine guitars are all dependent upon the wood from which they are made. The wood used in the construction of Rodriguez guitars is carefully chosen and aged to guarantee the highest quality. No wood is purchased before the tree has been cut down, and at least 2 years must elapse before the tree is turned into lumber. The wood has to be well cut from the log. The grain must be close and absolutely vertical. The shop is totally free from humidity.', '415.00', '39.00', '2012-07-30 14:12:41'), (7, 2, 'precision', 'Fender Precision', 'The Fender Precision bass guitar delivers the sound, look, and feel today''s bass players demand. This bass features that classic P-Bass old-school design. Each Precision bass boasts contemporary features and refinements that make it an excellent value. Featuring an alder body and a split single-coil pickup, this classic electric bass guitar lives up to its Fender legacy.\r\n\r\nFeatures:\r\n\r\n* Body: Alder\r\n* Neck: Maple, modern C shape, tinted satin urethane finish\r\n* Fingerboard: Rosewood or maple (depending on color)\r\n* 9-1/2" Radius (241 mm)\r\n* Frets: 20 Medium-jumbo frets\r\n* Pickups: 1 Standard Precision Bass split single-coil pickup (Mid)\r\n* Controls: Volume, Tone\r\n* Bridge: Standard vintage style with single groove saddles\r\n* Machine heads: Standard\r\n* Hardware: Chrome\r\n* Pickguard: 3-Ply Parchment\r\n* Scale Length: 34" (864 mm)\r\n* Width at Nut: 1-5/8" (41.3 mm)\r\n* Unique features: Knurled chrome P Bass knobs, Fender transition logo', '799.99', '30.00', '2012-06-01 11:29:35'), (8, 2, 'hofner', 'Hofner Icon', 'With authentic details inspired by the original, the Hofner Icon makes the legendary violin bass available to the rest of us. Don''t get the idea that this a just a "nowhere man" look-alike. This quality instrument features a real spruce top and beautiful flamed maple back and sides. The semi-hollow body and set neck will give you the warm, round tone you expect from the violin bass.\r\n\r\nFeatures:\r\n\r\n* Authentic details inspired by the original\r\n* Spruce top\r\n* Flamed maple back and sides\r\n* Set neck\r\n* Rosewood fretboard\r\n* 30" scale\r\n* 22 frets\r\n* Dot inlay', '499.99', '25.00', '2012-07-30 14:18:33'), (9, 3, 'ludwig', 'Ludwig 5-piece Drum Set with Cymbals', 'This product includes a Ludwig 5-piece drum set and a Zildjian starter cymbal pack.\r\n\r\nWith the Ludwig drum set, you get famous Ludwig quality. This set features a bass drum, two toms, a floor tom, and a snare?each with a wrapped finish. Drum hardware includes LA214FP bass pedal, snare stand, cymbal stand, hi-hat stand, and a throne.\r\n\r\nWith the Zildjian cymbal pack, you get a 14" crash, 18" crash/ride, and a pair of 13" hi-hats. Sound grooves and round hammer strikes in a simple circular pattern on the top surface of these cymbals magnify the basic sound of the distinctive alloy.\r\n\r\nFeatures:\r\n\r\n* Famous Ludwig quality\r\n* Wrapped finishes\r\n* 22" x 16" kick drum\r\n* 12" x 10" and 13" x 11" toms\r\n* 16" x 16" floor tom\r\n* 14" x 6-1/2" snare drum kick pedal\r\n* Snare stand\r\n* Straight cymbal stand hi-hat stand\r\n* FREE throne', '699.99', '30.00', '2012-07-30 12:46:40'), (10, 3, 'tama', 'Tama 5-Piece Drum Set with Cymbals', 'The Tama 5-piece Drum Set is the most affordable Tama drum kit ever to incorporate so many high-end features.\r\n\r\nWith over 40 years of experience, Tama knows what drummers really want. Which is why, no matter how long you''ve been playing the drums, no matter what budget you have to work with, Tama has the set you need, want, and can afford. Every aspect of the modern drum kit was exhaustively examined and reexamined and then improved before it was accepted as part of the Tama design. Which is why, if you start playing Tama now as a beginner, you''ll still enjoy playing it when you''ve achieved pro-status. That''s how good these groundbreaking new drums are.\r\n\r\nOnly Tama comes with a complete set of genuine Meinl HCS cymbals. These high-quality brass cymbals are made in Germany and are sonically matched so they sound great together. They are even lathed for a more refined tonal character. The set includes 14" hi-hats, 16" crash cymbal, and a 20" ride cymbal.\r\n\r\nFeatures:\r\n\r\n* 100% poplar 6-ply/7.5mm shells\r\n* Precise bearing edges\r\n* 100% glued finishes\r\n* Original small lugs\r\n* Drum heads\r\n* Accu-tune bass drum hoops\r\n* Spur brackets\r\n* Tom holder\r\n* Tom brackets', '799.99', '15.00', '2012-07-30 13:14:15'); 
OK, so, the IF statement takes 3 parameters - a test, a result if the test is true, and a result if the test is false. In this case, we are testing if the region in the current row matches the one we care about - if it does, the IF returns the value of the population column - otherwise it returns zero. To get the total population of all rows that match that region, we SUM the IFs.
I would have rather emailed the sql file. But I copied and pasted. TBH looking to get answers.
Agree, you've just posted the scripts to setup the sample data to work on. Have you tried to do any of the exercises yet?
I have not tried anything yet, no clue what I'm doing. Homework=midterm
I suggest you start here so: http://www.w3schools.com/sql/sql_syntax.asp If you really have no understanding, then we can't help you here - it would take a very long time to go through all thoses exercises line-by-line over reddit. EDIT: you first need to import the SQL file you already have into a MySQL database somewhere first to setup your example database, fill it with data, and setup a user to access it. Then you'll have something to work against.
It was not intended by any means for performance. I hope nobody would run that on a large scale production server, at least not one with more than a couple hundred rows. Wildcard concat / like join is pretty much one of the standard ammunition in the stress test arsenal.(Edit - *as it has to do a full page scan with string operations*)
Yeah that's the weird thing. I can't see the space before LTRIM...
oh i didn't think it was, I just wanted to make it clear to op, so he doesn't do something horrible. 
You can probably do a SELECT LEFT(columName,1) to return the first character of each cell in that column just to make sure. I'm lazy and usually just use TRIM but maybe there is another space that LTRIM is removing that isn't in the beginning? I doubt it though, solely based on the fact that it's named left trim. You can always just UPDATE table SET columnName = TRIM(columnName) and call it a day. Just backup your data first. 
You deserve the failing grade you will surely get. Idiot.
Try a global temp table (##Temp)
You can replace your original join on R5OBJECTS to JOIN R5OBJECTS CH_AND_P on evt_facility = ch_and_p.obj_facility JOIN ( SELECT OBJ_FACILITY from R5OBJECTS where OBJ_FACILITY in (#PROMPTMANY ('Park')#)) p on p.obj_facility = ch_and_p.obj_faciltiy or p.obj_facility = ch_and_p.obj_parent Make sure you prefix all existing columns that were coming from R5OBJECTS with "CH_AND_P." The newly joined subquery under alias "P." will give you parent facilities for both 'Park's and underlying children.
I am with you on this. Thank you for typing this all out so I didn't have to.
I'm trying to understand how this accomplishes what I need, but I will have to take a closer look at it tomorrow, when I can put this into practice. Thanks for the answer!
or a SELECT ':' + columnName + ':' FROM DirtySocks
This is pretty much straight up retarded, I even used that word correct. aka: DAE not like declaring variable types and think the computer should just know?
&gt; no amount of reasoning with you is going to make you change your mind, because you think you're correct Yeeeaaaah... I think you're reading too much into my willingness to question why someone prefers a particular style. I'm interested in pros and cons of both approaches. I have a preference, and I will argue its merits as I see them, but that's not saying I'm ignoring other arguments. I haven't actually had a lot of good responses along the lines of "your style is inferior because ...". Not wanting to rag on frogurttt's response (for example) too much, but just saying "I use style 2" doesn't add much to the discussion. &gt; You give it to a third party or a newbie, but he doesn't have a physical data model This seems quite contrived to me. Why would I allow someone to create queries against a schema they could not understand? How do you decide which predicates are join and which are filter? Are join predicates only FK-&gt;PK equijoins? Lets suppose I'm joining to a temporal table, and I only want the current data (see e.g. circuitStatus in the queries above). Should my filter on the start/end date columns go in the join, or in the where? I would prefer it to go in the join, because the temporal interval is effectively part of the PK. In the examples above there are 20 filter predicates: 5 are for parameters, 14 are for temporal restrictions (current rows only), and 1 is an exclusion filter for when we join to the same table twice (don't want the same row again). I agree that the parameter predicates are better placed in the where-clause, but the temporal restrictions are effectively part of the FK-&gt;PK relationship, and so I would prefer to see them in the join clauses.
&gt; How do you decide which predicates are join and which are filter? In terms of join predicates, they needn't always be equijoins, they could be non-equijoin, but the join is describing the relationship between the tables. I've always followed these tenets in regards to join predicates: * Foreign Key -&gt; Primary Key, * Foreign Key -&gt; Alternate Key, * Implied relationships. For example if materialized views are being used in a satellite system refreshed from a master system, so referential integrity mightn't exist, but would if they were tables etc. &gt; Lets suppose I'm joining to a temporal table, and I only want the current data (see e.g. circuitStatus in the queries above). Should my filter on the start/end date columns go in the join, or in the where? I would prefer it to go in the join, because the temporal interval is effectively part of the PK. I'm not familiar with the intricacies of your data model, but if the temporal interval is part of the PK then a join predicate seems valid. However, if you are filtering on the resultant datasets then personally I would make it a where predicate. Interesting debate nevertheless.
See the pseudo SQL in better format here: * http://imgur.com/AwCApsd * http://imgur.com/iJwz4WM * http://imgur.com/OI0C4lY Please don't be concerned about database structure or correct SQL syntax too much, I just like to be given an idea of approach to use in SQL. Thanks in advance! 
 SELECT KNA1~KUNNR, KNA1~NAME1, KNA1~KDGRP, KNVV~ZZGROUP_CODE1, KNVV~ZZGROUP_CODE2, KNVV~ZZGROUP_CODE3, If (select * from knb1 where kunnr = CUSTOMER and bukrs &lt;&gt; ‘P300’) is initial then false, else true KNVV~VKBUR, KNVV~VKGRP, KNA1~SORTL, KNA1~STRAS, KNA1~ORTO1, KNA1~REGIO, KNA1~PSTLZ, KNA1~PFACH, KNA1~PSTL2, KNA1~TELF1, KNA1~TELF2, KNA1~TELFX, ADR6~SMTP_ADDR, KNA1~SPERR, KNB1~SPERR, KNA1~AUFSD, KNVV~AUFSD, KNA1~LIFSD, KNVV~LIFSD, KNA1~FAKSD, KNVV~FAKSD, KNA1~CASSD, KNVV~CASSD, KNA1~LOEVM, KNB1~LOEVM, KNA1~NODEL, KNB1~NODEL, FROM ADR6 INNER JOIN KNA1 ON ADR6~ADDRNUMBER = KNA1~ADRNR INNER JOIN KNVP ON KNA1~KUNNR = KNVP~KUNN2 INNER JOIN KNVV ON KNVP~KUNN2 = KNVV~KUNNR INNER JOIN KNB1 ON KNVV~KUNNR = KNB1~KUNNR WHERE KNVP~PARVW=’SP’ AND KNVV`VKORG=’P300’ AND KNVP~VKORG = ‘P300’ That's the first one. I have expended all the effort I had this early in the morning, I can't do the other one.
I'll make you a deal: 1. You go and create a script for tables, using readable column names, sufficient to serve as an example. 2. Then you write insert statements for those tables, so there is some test data. 3. Then you tell us what you want returned by your sample data. As in how should the record set look like, and what data should get returned My side of the deal : Then I'll be happy to help you to figure it out.
Hi, Thank you! That is what I call Customer 1. Customer 2 to N is exactly the same except for the last line: WHERE KNVP~PARVW=’***SH***’ AND KNVV~VKORG=’P300’ AND KNVP~VKORG = ‘P300’ Where SH &amp; SP are different *types* of customers. A real world example would be: Head Office (customer 1) has multiple store locations (customer 2 to many). For every head office customer, I want to display alongside it its stores. http://imgur.com/AwCApsd
Well if I gave you a list of stuff to get for me and you have no idea where to get it from, that would be awkward right? So what if I first tell you FROM which supermarket your should get your groceries, (WHERE)WHICH the aisles to go to and then to SELECT the right product, that would be more logical right? SQL works like that, it first wants to know FROM which tables (ON) it needs to get information, WHERE restrictions apply ect and selecting is one of the last things that happen (at least in SQL Server). So if you're writing the SELECT now it does not auto-predict the columns in SSMS until after you write the FROM clause.
If whatever SQL platform you are working with allows to use sub queries in the "from" clause, do this: Select C1.name, C2.name From (mega query 1)c1 Join (mega query2) c2 on C2.parent_id = c1.id
You are looking at the execution order only. The Query optimizer will however FIRST want to know, what you want to get returned. If you have unneeded tables in your From clause, they will get optimized out. If you just want to get a count(*), the optimizer will not do any lookups on the CI in case it can satisfy the filtering on a NCI and so on. Even the index selection is very dependent on the field list, lookups are costly, so you can get CI scans even if there are indexes, depending on how many lookups would have to be done. So for the logic on what happens when I press F5 in Managment studio, the field list is just as important as the FROM clause, since it has a major impact on the compiled execution plan. Plus, in natural language, I would tell someone "get me a carton of milk, from the store down the road". So at least to me, the select feels more logical before the from.
No, ABAS is an Object Oriented Database. You'd have to start by mapping your objects out to tables, and then transform those objects into tables. You'd need a database mapper or transform to do that. However, I'd have to ask why did you want to convert it to SQL? What's wrong with leaving it as it is? Is it the cost? There are free Object Oriented Databases available such as ZODB, ObjectDB and Wakanda (among others). You could try Pervasive's Data Integrator, which is a database to database ETL mapping tool.
Well, I work for a software vendor and there are a lot of people who call themselves 'DBA' and out of the hundreds of DBA's that I've worked with, very few know much sql or keep performance logs. I'd say that your best preparation for going into this job, and to be successful is to take some training on 'Administering SQL Server'. Try to get training that goes in line with the Microsoft tests. One thing to keep in mind is that this is completely different than any websites you've ever created. You are generally not going to be in control of the database design. Now, once you start, the first thing you need to do is make sure the databases have a good backup system in place. Get a good backup and setup a test server to test out the backups, and make sure you completely understand how to backup and restore databases. Get that backup running smoothly without interventions. Good backups are key to making sure you don't run out of transaction log space. Next, you need to work on maintenance. You should have a maintenance job scheduled to update the stats (please don't rely on 'auto updating statistics') and also the indexes (check if the indexes have a fill factor. Most tables wouldn't benefit from a fill factor lower than 90). If there aren't any indexes (common among people who have home grown databases) then use the microsoft tools to suggest indexes (Database Engine Tuning Advisor). To do that, you'll need to profile the daily operations, so you'll get good suggestions. Don't accept all suggestions, but review what has been suggested and choose what would make the most impact. Before you do all that though, you are going to do something that will help your career and propel you to the top of your class. You are going to capture performance logs. Once you have your baseline performance, then you can implement your changes and capture another benchmark so you can show your manager that you just improved the speed of the database by 500% (I just threw out a number...). Once you have solid backups and maintenance, then your role will be a bit different. You are going to work on security. Verify that everyone that has access to the database actually needs access to the database. Make sure you don't assign too many rights to users. You'll be best served by having some test accounts setup, and using those to verify you know what you are doing when it comes to security. It helps to have a regular security audit so you can make sure people who are no longer working there, or have changed roles have their security changed to reflect the new status. You should be in the loop with HR so that these audits don't turn up things, but either way, you need to have audits. By this time, you are handling your duties as DBA pretty well. You need to implement regularly scheduled downtimes, so that when or if something needs to be done to a database which causes contention, or requires a restart, you can do it. You don't always need to have a downtime, and most times you can send out a message saying 'the regularly scheduled downtime has been cancelled'. However, this is your window to do things you couldn't normally do because the database is always online. Once you have the schedule in place, defend it. Don't let someone dictate 'hey you cant take it down' - remind them that without scheduled maintenance, their cars would just suddenly stop working, no to mention airplanes. Use it for upgrades and moving files around to different drives, installing hotfixes, etc. Have some tests in place so that when you do bring the system back online, you are confident letting people back into the system. After that, your next step is probably going to be training. Get some more training, read up on the sql blogs, seek out ways to improve performance, work on analytics and reporting, hire a junior dba, etc...
&gt; When you say they change often, do you mean you perform a lot of UPDATE statements on both of these columns? How about INSERTs and DELETEs, how often do these happen? INSERT, UPDATE, DELETE. All of it. &gt; How about statistics, how often are those gathered and how? Have you tried running an SQL Tuning Advisor task on your (original) statement and did you learn anything new from there? The issues arrive at development time. I.e. the frequent re-loading of the contents is not yet live. &gt; How often is this table accessed, especially concurrent access? Right now only by developers. Afterwards, there will be around 50k reads per minute, I'd say. Not a lot of writes. &gt; Perhaps the table is fragmented, do you perform any scheduled shrink operations on it? See above. Right now, we're developing this. &gt; How about the index clustering factor, how is that compared to the total number of data blocks and row nums in the table? Would have to check first... (I'll have access again next week only, unfortunately)
&gt; Once you add 'UNIQUE' to the index Oracle does know that selectivity of the index is 1. No. A simple example: Record1 = { 2, 4 } Record2 = { 3, 5 } Those are sufficiently UNIQUE, but Oracle doesn't know that they - Are intervals - Don't overlap &gt; Not sure about the 'average' theory - did you test that already? Would have to double-check
&gt; Come back with your results! Will give it a shot next week, thanks
Having a data flow diagram is incredibly helpful, but this is going about it the wrong way. A DFD is a tool to help you determine how to design and build a database. Ideally, you'd make these *before* you start writing your SQL. I can see this being of some use if your reverse engineering in design for an upgrade to an existing system with no/poor documentation, but in that case I'd still probably analyze it by hand. Better that way to catch any lingering defects in the old code.
I think you're talking about something else other than selectivity then? Oracle wouldn't (and shouldn't) care for your data representing intervals, overlaps, etc. A properly balanced btree will hang from the median of your data range, anyways. One way to really really implement 'one leaf node hit' it is to write one hash function that for any value within an interval of yours resolves it to a value X and any value outside of the interval to something else than X. A user-function index can be built on those. Seems way more trouble than its's worth to me. A decent clustered index (aka IOT for oracle) on first coordinate of your interval should do the trick, IMO. If not, this needs to be analyzed in detail.
tell you what, to test that the index lookup works ok: 1. make sure you have a unique index on Col1 with Col2 included, and re-build the index, just in case 2. run like a few million lookups for just Col1 and Col2 (nothing else) for a random parameter in the 'low range' (whatever it is) and the same number of lookups in the 'high range' (whatever 'high' is). See if the #2 gives you different times, then we can start blaming the index, Oracle and Larry.
&gt; I can tell my SQL Server, to give me the execution plan (which gets compiled, so SQL already gets compiled into something else), showing me EXACTLY what happens below the hood. What more do you want? There doesn't seem to be anything preventing the ability to generate explain plans for a given BQL query, since all the IDE would have to do is generate the explain plan for the SQL the BQL compiles to. There's also simply lots and lots of cases where the db is simply fast enough that readability of the SQL is going to trump performance concerns. I don't think OP's proposal is perfect but I like a lot of it since it compresses down a lot of the boilerplate that goes into all the small routine bits of SQL that go into an application. I could see myself using it for smaller things but resorting to pure SQL for the performance critical bits.
Thanks for the feedback! Could you see it being useful for a data analyst?
ah beat me to it, yeah (##Temp) global temp tables. FTW, upvotes!
Yeah, I can see why you would like it that way, I think it's definitely an improvement on readability, if nothing else. In the beginning, I had to re-order queries mentally like this too, but now it just kind of seems natural. DrTrunks and svtr both made good points though, just wanted to throw my 2 cents in. Is there an equivalent to profiler and/or show execution plan in mysql,postgresql, etc? Curious about something now, lol. Sorry to mini-hijack/OT.
interesting.. if it were used in a presentation.. who would benefit most from it?
Unsure, as an analyst, i normally only care about the outcome and end data, not the sql itself.... 
Love the column view. Seems like a quick and easy tool for our large team of analysts. The monthly pricing model would be a deal breaker. Though, there may be a better licensing structure there somewhere, I didn't dig into it.
Frequently, I am asked to document something I've written; it would be helpful for quickly creating diagrams of dependencies for auditing purposes. However, selling this would depend on the company and the need. In my opinion, a great way to quickly illustrate database complexities to non-developers!
Their previous DBA was fired from the company about 2 years ago for not really accomplishing anything. Since then one of their more "tech savvy" employees has been managing the system. I used to babysit the "tech savvy" guy's children when I was younger, and even at the age of 13-14, I had a relatively more robust experience with application development, database implementations, etc. From the sound of things, currently half of their databases are broken, they're maintaining a few JET databases shared across many users that are very prone to corruption. Their MS SQL server only has a few remaining tables that are fully operational. I have two weeks remaining at my current job, and this'll be my first job that I actually get to put my degree to use, so I'd like to be fully prepared. For the time being, I'll be taking on projects from the current employee that has been attempting to manage the databases. I'll be honest, aside from your basic SQL commands, and basic database management, I'm pretty novice at enterprise level database administration. I'd like to get started strong, and really do the best of my abilities. I have the benefit of being an incredibly quick learner, and being a very good problem solver. It would be a lie to say that I'm not absolutely terrified. I'm aware that I'm more qualified than any of their current employees, but I don't want to be a flop.
It's an electrical contracting company. The site that I'm going to be working at is the site of their largest contractor, one of the largest data centers in the united states. The company is responsible for supplying power to all the different server wracks, mainframes, etc. They have to be incredibly precise with their power as to not run too much to one stack. One of the databases I have seen manages the location of every single server, the amount of power required, the maximum load, etc. And from I've been told that's one of their more important ones.
This looks very very useful for a customer of mine, who uses Oracle and heavily nests views. It is sometimes a bit hard to understand what database columns back a calculated view column after 10 layers of nesting. For complex refactorings, this could thus be very useful. Also, as others have said, the usual ERD reverse-engineering tools generate huge graphs that are hard to read and contain a lot of unnecessary information. Yours looks a bit neater. Of course, I don't know how this tool performs with a database of 500 tables and 2000 views...
See [item #2 in this article here](http://tech.pro/tutorial/1555/10-easy-steps-to-a-complete-understanding-of-sql)
&gt; The Query optimizer will however FIRST want to know, what you want to get returned I think that any optimiser not written by an intern will be able to operate on a SQL AST, not on the lexical order of SELECT clauses :-)
What you say certainly doesn't hold for: - Inversing the order of SELECT and the table expression - Projections (which are just syntax sugar for views) - Implicit joins (which are just syntax sugar for equi-joins) I see nothing wrong with the above...
thanks!
Thanks to everyone who checked this out (and continues to check it out). SQLdep is in beta - and your feedback is very helpful!
Note, have you considered working together with the guys from Gudu Software? They already have a powerful SQL Parser that works with a lot of SQL dialects: http://www.sqlparser.com. If you could build upon their parser and add visualisation on top of it, you might be able to speed up your beta phase...
In any case its just syntax, the question is what the more logical order would be, and to me its more logical to first give the column list to be returned, than the tables from where those columns are to be fetched. 
If your corporate office: A- uses MS SQL Enterprise edition, and B- has connectivity to your MySQL box, then... SQL server integration services is a great ETL tool to robustly move and convert it to any table structure needed. I'd recommend it as its already paid for. If that's not an option, then I don't recommend trying to backup to "ms SQL" file format or a backup and trying ship those files across, it's not really meant for data transfer. If they change their SQL version, it won't work, etc. If a live connection isn't an option, I'd recommend exporting the data you need to .csv or XML, zipping and transferring, using a batch cron job. 
First off, the TSQL is sent as is to the server from SSMS Then the server does the most important step of creating a query execution plan. That plan is can be different for the same exact TSQL text because the server uses statistics about the tables and indexes to come up with a lower level set of instructions. Akin to a compiler turning .net code into MSIL. You can see those steps if you view the plan in SSMS, which a very important tool to maximize performance. After that its not ever really turned into "binary" per se, the server then runs that query plan, and that's a fixed part of the SQL server code. The server already has the binary code to execute each query plan step. 
Thank you!! I appreciate your help! That is what I will do. It's perfect! I don't know what version of MS SQL corporate has (I've only spoken to company reps, and no one in the IT department), but will find out! Thank you again!! 
Standard edition also comes with SSIS.
Oh that's great! Perfect! This integration will be much less problematic that I was expecting!! Thank you!! 
It's not my project :) As I understand the queries are too slow (a matter of minutes for a complex query). I don't know specific details, but it's a program wrote in the 90's. Thanks!
If an upvote is the only option, then that's all you need. If you want to be able to up or down vote, you need: post_id, username, up_or_down (int or bit or byte) The key to making it efficient is a index on post_id and (if applicable), including up_or_down as a non-indexed column. You don't need to add username to this index. Unfortunately, I'm not familiar with GQL so I don't know how involved you get to be in indexes and if you'd have these options. Once an index exists, when you call: SELECT COUNT(*) FROM VOTE_TABLE WHERE post_id = '1234' or SELECT SUM(up_or_down) FROM VOTE_TABLE WHERE post_id = '1234' Then the actual table is never queried or scanned. It hits the index which already has a count of the rows for that post_id and bam there's your answer. In the second case it has to do a bit more work but it's pretty trivial. Now, I'm making a couple assumptions about your use case; number of queries, ratio of adding votes to asking about votes, etc.. You could use a bit/byte column for up_or_down instead of int, but I'd check the performance, sometimes an int can actually be faster depending on how the SQL engine implements it. To use a bit column do SUM(2 * up_or_down - 1) to get -1/+1 from a 0/1 answer. You -could- save some space by substituting username with user_id, and create a user_id "unique ID" column in your username table [usually an int] so that you don't have to store the string in your tables, only the integer ID. That would save some space and likely increase speed, at the cost of complexity or creating/managing this extra column. And finally, there are a ton of ways to scale up from this if your website gets to reddit-size... partitioning tables, archived/active posts table, pre-summarized tables updated using triggers, etc, but I suspect the above will carry you through and keeps things pretty simple. You have to keep track of the user's vote individually, so there's no way around a table similar to the above.
and don't enforce referential integrity with the user_id, those checks are useless on these tables.
Sorry for not responding. I finished this last week. I added a custom code function CommCode(Comm_Code as string) as string select case Comm_Code case "ER1.5" return ".015" case "ER3" return ".03" case "ER6" return ".06" case "ER10" return ".1" case else return CommCode end select end function Then for the expression I used the CommCode to figure out the price. Thanks for the help.
Thanks. yes as I was starting to build a test structure I quickly realized it made a lot of sense to also have a count running within the POSTS record for that post. Also, I don't think GQL allows for a COUNT or SUM function in queries (GQL seems to have a ton of limitations vs. real SQL) -- any downsides to doing this?
There are 2 insert/update operations any time the vote changes and, of course, the referential integrity is enforced by the application (not that you have much choice with Datastore). Nothing obvious, as long as all setters (ideally, only one) are wrapped in transactions. My brain freezes trying to imagine possibilities in a transactional system with eventual consistency.
[UPDATE] Couldn't get it to work... pulled teeth whole next day manually cutting up CLOB into manageable segments. It wasn't too bad as the contents of the CLOBs were maybe 10k or 20k characters long. Now, however, I've got [a new problem](http://www.reddit.com/r/SQL/comments/2054cq/oracle_cutting_a_string_into_x_characters_for/) in the form of CLOB files that are nearer to 1,000,000 characters. Something I have absolutely no interest in manually cutting up. Have you any thoughts on how to cut these up into chunks of less than 4,000 characters? I'm assuming I'll need to use a combination of INSTR and SUBSTR but my brain doesn't seem to wanna grasp it. Basically I wanna create some kind of loop, maybe knock up a pl/sql script, that will allow me to wrap the update statement around 3999 character segments so that it comes out something like... update table set clob_column = 'first 3999 characters' where name='NAME'; update table set clob_column = clob_column||'next 3999 characters' where name='NAME'; update table set_clob_column = clob_column||'you get the picture...' where name='NAME'; Any ideas? Sorry to come peck your head again...
Basically I wanna create some kind of loop, maybe knock up a pl/sql script, that will allow me to wrap the update statement around 3999 character segments so that it comes out something like... update table set clob_column = 'first 3999 characters' where name='NAME'; update table set clob_column = clob_column||'next 3999 characters' where name='NAME'; update table set_clob_column = clob_column||'you get the picture...' where name='NAME';
That's not the right way of doing this. There's more about clever java stuff in the docs. http://docs.oracle.com/cd/B28359_01/java.111/b31227/typesupp.htm#sthref351
What do you mean by "my companies database" ? If it's an ERP or something like that, you can ask for some indicators to compute, that would be a good exercise. For instance, the total amount of sales for your products, by state, year, month, and summed up by product categories and subcategories - given that your wompany would be selling some products ofc.
We are a property management company. And i mean the entire SQL database that has every bit of data regarding the properties they manage. 
My first thought is that you should master the SELECT statement. Anything that I do with a delete or update starts as a select statement in some form. As for what to do with the data in the DB you have access to, try to locate the data in the database and relate it to what you see in the application. Can you write a select statement that give you most or all of the info on the screen for a particular property in the application? What about a list of properties from a specific seller or clients of a particular piece of property?
So find a count of properties by street, maybe an average count of tenants per property per street. Find the average rental price per tenant by house size, break that up by street. I can think of a million little things that might be interesting to look at with that data, especially if I lived in the area.
Your best bet is to get a book and do all the examples you can (short of your restrictions). My personal favorite is [SQL Essentials](http://www.amazon.com/SQL-Essentials-CDROM-Gary-Randolph/dp/1590280296/ref=sr_1_1?ie=UTF8&amp;qid=1394563447&amp;sr=8-1&amp;keywords=sql+essentials). By all means but it **used**. Others may have better choices but find one that is good for you and experiment. 
Group by post/zip code. Find the averages and medians for each area. Group by tenancy type. Find averages and medians. Combine the two, see if you get anything useful from them
it's probably sending it as a hex-ascii encoded string.
In Postgres, you can create functional indexes, and if you use that function in a query, it will use the functional index.
SELECT UserLogin, ServerURL, UserAddress COUNT(Logtime) as 'totalLogins' FROM dbo.RequestUsage WHERE ..... GROUP BY UserLogin, ServerURL, UserAddress I wouldn't look for distinct login times as more than likely you each row is different. 
It seems to not like COUNT(LogTime). Any ideas?
Nevermind I got it. Was missing a comma after UserAddress. Thanks!
1.Don't use SELECT * 2.Don't use Union $query = "SELECT TOP 5 name, description from (SELECT j.name,j.description, MAX(CASE WHEN city='$item-&gt;city' THEN 1 ELSE 0 END) as MATCH FROM #__ja_jobs j WHERE j.cat_id='$item-&gt;cat_id' AND j.location_id='$item-&gt;location_id' GROUP BY j.name,j.description) ORDER BY MATCH DESC"; $db-&gt;setQuery($query); $result = $db-&gt;loadObject(); If you want to use multiple criteria to increase the rank. Add other other criteria to your CASE WHEN statement. So if city &amp; state &amp; otherfield match THEN 2. This will rank your results. You could use rank but I didn't feel like writing it.
I am asking about how to best structure data. People that subscribe to this subreddit probably have some thoughts on that. Plus in the future someone may be interested in the same question but using MySQL or whatever. There is nothing GQL specific in my question. God knows there are plenty of head scratching things about GQL I could have asked. I would also point out that this got decent number of upvotes compared to many other"front page" posts for this subreddit and generated decent discussion so it doesn't seem like it's not of interest to other users. 
Quick Install instructions for the SQL management studio tool
It's probably the difference between varchar and nvarchar
This. But consider that storing files in SQL Server (varbinary, image (deprecated)) isn't the best plan, if you can avoid it.
Get MicroStrategy Desktop (it's free), connect to your DB, see if you can glean something interesting from the data. Well, it could be another BI/Visualization tool, but I'm partial to MicroStrategy. 
True, but using a sargable predicate will save you from needing a special-purpose index, saving space and preventing a write penalty. 
Big surprise, one software program will not satisfy all use cases. Hadoop, noSQL, and relational all have areas where they excel. MongoDB is a stupid choice for an accounting system that needs fast response times, and ACID compliance, and quick aggregations.
I admit that he is right in saying relational databases are dated. Object relational databases are the business standard now.
concur... exactly double the size...
sorry I typed it up right before I left work, good luck!
1. I'm 90% sure mirroring is enterprise-only to begin with. 2. If you're going to install the enterprise version of reporting services (which there's no point in doing - enterprise features are things like PowerView in Excel, etc. Not standard SSRS reports), you still need to pay for a full enterprise license. The license covers the database, SSRS, SSIS, SSAS, etc.
I really appreciate this! Unfortunately, it did not work for me because as /u/r3pr0b8 noticed below, I didn't mention I was using MySQL. I completely understand if you would rather not re-look at my issue, but would appreciate it if you wouldn't mind.
thriven gave you MS SQL syntax you're obviously using MySQL big difference sidebar suggests you should identify your platform when posting in this platform-neutral subreddit
Sorry, first time posting here. 
Install some type of reporting software (like SSRS) and start practicing writing queries and writing reports for that. Data is the future!
Indeed, just because you can, it does not mean you should. Always go for reusability with indexes.
Aggregate and windowing functions.
no problem use thriven's MAX(CASE...) idea, put LIMIT 5 on the ORDER BY, and remove TOP 5 
Thanks guys! I do believe this is the issue. I don't need to fix the issue right now, just need a reason as to why it's happening since a client has noticed this and inquired as to why it would be.
My thought would be to use: INSTR( table2.posttype, "_") to get the index of the "_" then use SUBSTRING( table2.post_type, INDEX_OF_UNDERSCORE ); to trim it down to just the number. [instr doc](http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_instr) [substring doc](http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substring) Your other option is to concatenate on the ID you have as a number. Something like SELECT [CONCAT("resource_", table1.id)](http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_concat) as ID_WITH_TEXT. This option would probably be easiest if you know all the strings that can occupy the "resource_" part, just do one for each and make the WHERE look like: WHERE table2.post_type = ID_WITH_TEXT1 OR table2.post_type = ID_WITH_TEXT2 ... PS: if table2 isn't built-in, you really should [normalize it](http://en.wikipedia.org/wiki/Database_normalization).
Try LIKE '%[0-9]/[0-9]%' 
That's great. I wasn't sure what sql it was and I rarely write MySQL code. Limit 5 is same as Top 5. I learned something too :)
I need a dynamic solution because I can't know all the strings in advance, and table2 is built in (it's the WordPress posts table) so normalization is not an option.
~~Oh, I'm maybe over-thinking this your other option is to use `LIKE`.~~ ~~WHERE table2.posttype LIKE '%' + table1.id + '%';~~ ~~I'm not sure if mysql supports + or if you have to use `CONCAT(%, table1.id, %)`~~ The problem with LIKE is that you might get more than you want if there are other numbers anywhere [or you'll get "1" and "10" when you want "1"]. Actually parsing the string is probably better, ~~but if you know there'll be no numbers, LIKE should be fine.~~ This would also be a good time to consider making a VIEW on the table that is a normalized version.
Perfect, that worked. thank you. I was overthinking it.
I really appreciate that, but I still can't get it to work. I tried with the code you gave as is, but after that didn't seem to work, I assumed I was supposed to replace the fields you had defaulted (name, description). Unfortunately, that left me with the same result... I tried taking out the GROUP, as some jobs have the same title, so I thought that may be giving a problem, but no... Here is the version I tried out: $query = "SELECT title from (SELECT j.title, MAX(CASE WHEN city='$item-&gt;city' THEN 1 ELSE 0 END) as MATCH FROM #__ja_jobs j WHERE j.cat_id='$item-&gt;cat_id' AND j.location_id='$item-&gt;location_id' GROUP BY j.title) ORDER BY MATCH DESC LIMIT 5"; I know you don't usually write MySQL, so if you don't want to mess with it anymore, no problem :)
If you run this query (change city,catagory id, location id) against the server the following does it work? SELECT t.title from (SELECT j.title, MAX(CASE WHEN city='phoenix' THEN 1 ELSE 0 END) as MATCH FROM #__ja_jobs j WHERE j.cat_id=1 AND j.location_id=2 GROUP BY j.title) t ORDER BY MATCH DESC LIMIT 5 I noticed as writing it out that I didn't name the subselect. I just named it as t. You could also try $query = "SELECT t.title from (SELECT j.title, MAX(CASE WHEN city='$item-&gt;city' THEN 1 ELSE 0 END) as MATCH FROM #__ja_jobs j WHERE j.cat_id='$item-&gt;cat_id' AND j.location_id='$item-&gt;location_id' GROUP BY j.title) t ORDER BY MATCH DESC LIMIT 5"; 
Apparently, you can't use derived columns in WHERE/JOIN clauses, so you'd have to do this: SELECT * FROM ( SELECT t1.id as id1, t1.username, t2.post_type, SUBSTRING( t2.post_type, INSTR(t2.post_type, "_")+1 ) as t2jid FROM t1, t2 ) dt WHERE dt.t2jid = dt.id1; This will make a full cross-product table "dt", then filter out only the records you want. It also won't have the weird LIKE behavior where "1" is LIKE "10", "11", "12", [...], "21", etc. To make it a view: CREATE VIEW t2_better AS SELECT id, post_type, SUBSTRING( t2.post_type, INSTR(t2.post_type, "_")+1 ) as user_id FROM t2; Then in the future, you can just use t2_better like a table.
I understand you reasoning, but take for example the idea of having a votes table, and columns 'songID' and 'voter'. Since you said you only need upvotes... in SQL, you could just issue a count of voters per songID. However, with GQL as I understand it, you are limited to 1000 rows per select, so if the count is 1000, you have to query again to make sure it's not over 1000 votes. That would be inefficient, so it may be better to have a songID and votecount and rely on the eventual consistency to keep that votecount up to date. Occasionally it may be out of date during heavy loads, but it would eventually show the correct number. That would be the best way to structure the data for GQL, but for SQL it would be better to do the other way, have all votes in the one table. You'd still need to retain the individual votes, so you can disallow people voting twice, but with GQL and it's limitations, it wouldn't be optimal to query the individual votes table to get a tally.
you don't need to do the aggegated subquery crap here anyway. Just take the select to the table, filtered for your where clause, and add ORDER BY CASE WHEN j.city = '$item-&gt;city' THEN 1 ELSE 0 END DESC LIMIT 5
I think that did it! I will test this afternoon. Thanks.
Instead of screwing around with business data, come play the Schemaverse :D https://schemaverse.com Learn SQL while playing a space battle game. 
Hmm I wonder if he's biased
Depends on what the data is and how it's encoded. Do you know that the data is text? Last time I was faced with that and didn't know what the data was, I wrote it all out as individual files in a separate directory, and ran file(1) against them. If all you want is a copy-pastable representation, look at [hex()](http://www.sqlite.org/lang_corefunc.html): &gt; The hex() function interprets its argument as a BLOB and returns a string which is the upper-case hexadecimal rendering of the content of that blob
 from table1 join table2 on table2.post_type = concat( 'resource_', table1.ID) If "resource" prefix is somewhat rare in the "post_type" column and if the column is indexed, I'd throw in where table2.post_type like 'resource%'. 
Just checked it and the ordering was a bit off - wasn't ordering descending by id, so I added one more element at the end, which seemed to fix it. Thanks again!! $query = "SELECT id,title,cat_id,location_id,city FROM #__ja_jobs j WHERE j.cat_id='$item-&gt;cat_id' AND j.location_id='$item-&gt;location_id' AND j.status='Approved' AND j.id&lt;&gt;'$item-&gt;id' ORDER BY CASE WHEN j.city = '$item-&gt;city' THEN 1 ELSE 0 END DESC, id DESC LIMIT 5";
do those columns exist in tbl_users? It looks like you're using an unquoted string.
Yes, the 2nd picture is of the tbl_users. I've tried single and double quotes already... Everything I've seen online I feel like I have tried! 
Swear to god I've tried that multiple times as well, however I just attempted it again and it went through. You sir saved my night haha 
Query the [information schema](http://www.postgresql.org/docs/9.3/static/information-schema.html). You'll need to figure out exactly what you need from these views. * information_schema.constraint_table_usage * information_schema.constraint_column_usage * information_schema.check_constraints * information_schema.check_constraint_routine_usage * information_schema.domain_constraints * information_schema.referential_constraints * information_schema.table_constraints Pay attention to the note at the end of that reference.
you forgot to explain what "data lineage" is
SQL Server Only: http://bit.ly/1cC4dDw 
Thanks - just to clarify, do you mean left join T1 to T2, and left join T2 to T3?
 SELECT t3.inventory_id FROM customers t1 ( nolock ) LEFT OUTER JOIN customer_orders t2 ( nolock ) ON t1.customer_id = t2.customer_id LEFT OUTER JOIN customer_order_details t3 ( nolock ) ON t2.customer_order = t3.customer_order 
Take a look at willhaney's example below. Although I wouldn't use the nolocks for anything I work on. You can just google "sql nolock" for a pretty good explanation of what that's doing.
Because in your example you're _only_ after inventory_id, there is no need to join all those tables. select inventory_id from customer_order_details; &gt;Table 2 might have the fields I want, depending on whether or not a customer has placed any orders. ... but you're not returning anything from t2? 
The OP asked about PostgreSQL; your response is about SQL Server. 
dammit. I swear he said SQL Server.
Thanks. This makes sense and I appreciate you writing it out. Worked for my query just fine.
You're right, I could have clarified that by saying "t1.customer_id, t3.inventory_id" - was just trying to portray that I wanted data from t3 using t1 as a base. T2 is just a "bridge" table. However, willhaney's solution will work for me. Thanks for replying.
The simplest way is just to query all the tables that hold a foreign key reference to see whether any rows in those tables reference any rows displayed to the user. (Assuming that you haven't declared ON DELETE CASCADE for any of them.) There's a reason nobody does this. You'll undoubtedly discover it.
The only kind of constraints which can prevent a delete is foreign key constraints (triggers can also prevent a delete, but they are not constraints). I would solve this by just manually joining with all those tables to see if any row there refer to our row. I do not think a generic automatic solution would be worth it.
Fair enough, but the last post on the app engine subreddit was 8 days ago. Within hours of posting here I got some really good answers that helped me a lot. I guess i don't see what's so wrong about that. Sometimes reddit is so overcurated. There's thousands of subreddits, not all of them (in fact most) have a lot of activity 
select X.client, X.id, X.status, X.entry_date, X.status_value from table_name X where status_value=(select min(status_value) from table_name Y where X.client=Y.client) -&gt; SQL Filddle: http://sqlfiddle.com/#!2/911ca/2
Pretty cool, thanks!!! Can you tell me how it works and what the a name of this function is? I would like to learn in order to leverage this if I ever need it in the future.
It works by selecting every client row whose status_value is the lowest. Pretty much like your question said! The trick is to use the subquery to figure out what the lowest status_value actually is. I also use 'X' and 'Y' as aliases in order to distinguish between the inner and outer queries. Hope that makes sense. 
Huh, so it's a sub query. I've never seen one like this before. I'm not capturing how it's working, but I think it's because I'm tired. I'll try to figure that out in the morning. Thanks again, and that website is pretty cool. I was looking for something like that for a while.
One more question. I didn't account for duplicates in the Status_Value column. What can I add to the query to drop the second duplicate value? [This is what I now have](http://sqlfiddle.com/#!2/b53e8/1) I don't care which of two records I keep.
I always add an ID column that increments with every record - then you can choose the record that matches max(testdata_id).
It's basically: select * from X where c1=MAX_C2. where MAX_C2 is a subquery that returns a single value. I just found that website myself because I was looking for something like jsfiddle and lo and behold, the internet always delivers. Good luck with understand it - feel free to ask questions - strangely, I find SQL kinda fun.
As do I. I'm very new at this. I need to start reading up on this.
You can do an explicit join like this: SELECT t3.inventory_id FROM customers t1 LEFT JOIN ( customer_orders t2 JOIN customer_order_details t3 ON t2.customer_order = t3.customer_order ) ON t1.customer_id = t2.customer_id What this does is that it always returns the customers, no matter what. It will also return the customer_orders for each customer BUT ONLY if that customer_order has a corresponding customer_order_detail(s). You basically do A left join (B join C) I can be useful in some cases.
The most efficient way (one table scan) is to use analytic function: select CLIENT, ID, STATUS, ENTRY_DATE, STATUS_VALUE from ( select t.*, row_number() over (partition by t.CLIENT order by t.STATUS_VALUE) as RN from TABLE_NAME t) where RN = 1 This approach will outperform any other solutions including the one suggested using MIN in a subquery (multiple full table scans with nested loops). It will also handle duplicate situations involving STATUS_VALUE.
If your version of SQL Server supports analytic functions ( I think 2012) onwards, then something like this should achieve what you want with excellent performance (untested as I don't have SQL Server here). select [TICK_ID], [BID], lead([TICK_ID], 1, null) over (order by [TICK_ID],[BID] range between [BID] + 0.004 following and unbounded following)) as HELLOWORLD from [dbo].[SQLdatabase_GBPUSD] 
Thanks for the reply. I tried your code out, and it works really fast, but it returns multiple results for each Tick_ID. I would like to have it return just the first result where the a2.Bid &gt;= (a1.Bid + 0.004) occurs. Any suggestions?
Thanks for helping. I tried your query and I thought it was going to work great, but it won't get past the (range between Bid+0.004 following and unbounded following). My guess is because a column reference is not allowed in the range statement.
Awesome! Two birds, one stone. I know have two ways of tackling this. Can you recommend any books that will help me learn more about SQL?
I suggest reading up on FOR XML PATH, I don't do much work with XML, but this looks like the easiest way to format XML output. http://technet.microsoft.com/en-us/library/ms189885.aspx
Instead of FOR XML AUTO. Try FOR XML PATH('VendorInventory'),TYPE
It's dense reading. I think the examples page is the most helpful part. 
Have you tried switching from a MIN(Tick_ID) to a TOP 1 Tick_ID ... ORDER BY Tick_ID ASC format? I would try that, leaving it as a subquery, and then retry different index combos, see if you can get from an index scan to a seek. An index on Tick_ID (ordered ASC) with Bid being an included column would be the first I'd try. I assume Tick_ID is a primary key?
I haven't tested this but i think it should do what you're asking. Note that the query you provided in comments should have an 'and' operator instead of 'or'. Below is for mssql: Where datepart(month,mydate) &gt;= datepart(month, start_date) and datepart(year, mydate) &gt;= datepart(year,start_date) and Datepart(month, mydate) &lt;= datepart(month,end_date) and Datepart(year,my date) &lt;= datepart(year,end_date)
I'd normally delegate presentation work to the application layer.
You could hint to SQL Server that it should use hash, with LEFT HASH JOIN But its almost always the wrong thing to do (a hint that works now could degrade massively over time unless you monitor it). SQL Server does a pretty decent (if pessimistic) job of estimating the best strategy, so more likely there is a missing index it could utilise, or your table statistics are out of date (see UPDATE STATISTICS). Try posting your schema and the current execution plan, and Ill see if theres anything obvious. I've not worked with SQL Server for some time though..
For which database? It is done differently in different databases.
It should be: myDate &gt;= TURNC(start_date, 'MONTH') AND myDate &lt;= LAST_DAY(end_date, 'MONTH') which can also be written as: myDate BETWEEN TURNC(start_date, 'MONTH') AND LAST_DAY(end_date, 'MONTH')
On Oracle? Best book SQL / PL/SQL book I've ever bought : Excuse the lack of hyperlink, on my phone. http://www.amazon.co.uk/gp/aw/d/1449324452/ref=mp_s_a_1_1?qid=1394853719&amp;sr=1-1&amp;pi=AC_SX110_SY165
If the dates have a time component, then the "inclusive" last day of the month will miss all times except for midnight. Even though I [posted a solution](http://www.reddit.com/r/SQL/comments/20fr3l/sql_date_compare_using_only_mont_year/cg2ufk0) for SQL Server before OP alluded to the db platform, the method is the more correct method.
Let's say you only care about results within June 2003 and February 2004. The following syntax is supported by most database servers. It doesn't matter the specific date type of the column as long as it's an actual date (not an int representing a unix timestamp for example). SELECT * FROM tbl WHERE datecol &gt;= '2003-06-01' AND datecol &lt; '2004-03-01' This is the recommended method to handle this type of query. Notice that the `WHERE` clause doesn't have functions, and it doesn't use `BETWEEN`. The `BETWEEN` keyword is highly discouraged for date ranges. This will make optimal use of indexes on `datecol`. Look up **sargable** for an explanation why. ---- **Edit**: In my above post, it seems that I conveyed that `BETWEEN` is a function, not a keyword. Let me be clear, `BETWEEN` is not a function. It can be optimized for by most (all?) database optimizers to use indexes correctly. The reason why `BETWEEN` is discouraged is explained in my post [here](http://www.reddit.com/r/SQL/comments/20fr3l/sql_date_compare_using_only_mont_year/cg3c2gb). Functions in the `WHERE` clause do not necessarily make a query non-sargeable. I'm not going to go into details of what makes a query sargeable. For the most part, however, if you use a function on a column, then the query becomes non-sargeable. Finally, I discourage functions in SQL unless necessary or aids in readability (except in cases where the function becomes non-sargeable). Using date functions here is unnecessary. The query I wrote in the original post is easier to read and possibly faster in performance then queries that use date functions.
Yes Sir.
I tested out my idea and it's just the same as what you've already got, None of the others were faster either on a (smaller, 100k) data set. Tried an indexed view but wasn't able to. It's a good problem. Your execution plan should show a small effort for a clustered index scan and 95%+ of the effort into a clustered index seek. 
I looked up sargable. BETWEEN is a keyword not a function, and it is sargable. 
Not as good as ziptime's, but I thought I'd give it a try anyway. SELECT CT.* FROM (SELECT DISTINCT Client FROM ClientTable) C OUTER APPLY ( SELECT TOP 1 * FROM ClientTable WHERE ClientTable.Client = C.Client ORDER BY Status_Value ) CT
where's your code? This looks like your homework. I will help you, but not writing it all from scratch for you.
Show some code, brother!
What's a TURNC?
Isn't it TRUNC? 
Between isn't a function it is a keyword. Otherwise this is the best example. Also for some databases it doesn't matter since they don't use indexes at all.
Also you just put readability in the toilet by over complicating the query text.
Yeah, I'm not sure who is discouraging using between for dates; it works just fine. Most optimizers will treat a range as a range regardless of syntax. 
I didn't say `BETWEEN` was a function. I said it's discouraged for date ranges.
The reason why `BETWEEN`is discouraged is because it's inclusive. Nothing to do with performance. If I had used `BETWEEN`, I could possibly get the wrong result. SELECT * FROM tbl WHERE datecol BETWEEN '2003-06-01' AND '2004-03-01' This query will INCLUDE rows with March 1, 2004. So if I wanted to use `BETWEEN`, I have to remember that February is a leap year and re-write my query like so: SELECT * FROM tbl WHERE datecol BETWEEN '2003-06-01' AND '2004-02-29' But wait! What if `datecol` is a `datetime` datatype and not `date`? Then this query still won't work because it will only include results up to February 29, 2004 midnight! It will not include results from the rest of the day. So I have to re-write my query AGAIN! This is why the `BETWEEN` operator is discouraged for date range lookups. To simplify queries and logic, use the original syntax I provided, and it will always work for these types of queries.
I don't think that description is accurate. There is no INT column when the COALESCE statement is resolved. The column is made up of both empty strings and integers. Then when the column is inserted into the temp table the empty strings get silently converted to zeroes. For example SELECT * FROM ( VALUES (''), (1) ) A(B) returns B ---- 0 1 
It's not `COALESCE()`. It appears to be how SQL Server casts strings which are empty or only contain spaces to integers. `select cast(char(32) as int)`, `select cast(space(10) as int)`, `select cast(' ' as int)` and `select cast('' as int)` each return `0`. I doubt the SQL standard even defines what's supposed to happen here. My guess is that it's behavior from Sybase era that's never been eliminated. &gt; Not what I intended at all! You're implicitly typecasting a non-numeric string to a numeric value. You deserve to get unexpected results. I can't even begin to imagine what results you expected here.
im not a real dev but that is what i had to do... dusted off my c# skills
&gt; The column is made up of both empty strings and integers. No that's wrong. A column is made up of 1 data type. It can't be a mix. In your example, there is an implicit conversion because you are mixing data types in your `FROM` clause. In a SELECT statement, all columns must have the same datatype or SQL Server will convert it implicitly OR returns an error if it can't be resolved. Implicit conversion is a good thing in SQL Server. It allows easy conversion between `INT` and `BIT` types, or `VARCHAR` and `NVARCHAR` types. Implicit conversion between a string and int is dangerous, as you experienced. Try this: DECLARE @num int = NULL; SELECT COALESCE(@int, 'banana'); -- You should get a conversion error. SELECT COALESCE(NULL, 'banana'); -- You should get "banana" since this NULL has no datatype. -- It gets converted to a character type. Now try this: DECLARE @num int; SET @num = ''; print @num; -- You should get 0.
Most RDBMSs do return an error doing that. As I said, it's probably behavior from Sybase days, by which I mean 1980s and early 1990s. The behavior probably still exists because nobody ever does this or runs in to it, so it was never fixed. Feel free to post it on connect.microsoft.com so they can argue that it's desirable behavior and mark another bug WONTFIX. INTs getting cast into strings would not happen. Primarily because T-SQL `COALESCE()` requires that all fields be the same type, but also because the table you're creating has the same basic field definitions as the source fields. The DB engine determines the field definitions, creates the new table, and then inserts the data. `SELECT ... INTO` is just a vendor-specific shortcut for doing that (other vendors often use the syntax `CREATE TABLE ... AS SELECT ...`). In any case, it doesn't analyze the column data. It just takes the column definition. 
Thanks, that makes a lot more sense.
I see. Thanks for explaining.
&gt; Feel free to post it on connect.microsoft.com so they can argue that it's desirable behavior and mark another bug WONTFIX. So true.
Much appreciated! I am quite new at this and I'll take all the help I can get.
Thanks! I will be getting this pretty soon. I play on the dev database at work and I am liking this. I would like to get proficient and find a job doing this in the future.
Thank you author for this. Hopefully this can help a newbie like me learn some sql
There's probably some text utility you could download to do this. Or it wouldn't be too tough to write it in your preferred language. I'd probably have to use /r/excel VBA, myself... 
`GO`? (or whatever you have as your batch separator).
ah I was unaware that you had to drop the GO in there thnx. It will save our DBA a lot of time. :)
Can you then tell and sql file to do an EXEC of all the other sql files by specifying their location?
This may be possible but not feasible. In theory, you can load a file into a variable and use sp_executesql or something like that. The problem is that you may have to escape out some characters in the sql files and that can become more trouble than its worth. OH, and definately, respond to /u/r3pr0b8 's [comment](http://www.reddit.com/r/SQL/comments/20jzfb/how_do_you_chain_multiple_commands_into_a_single/cg43sz6) about what DBMS this is for so that we can give you a more accurate answer.
yes I forgot. MSSQL
The preferred language is SQL (this is the SQL subreddit after all) and commands are separated by GO. http://technet.microsoft.com/en-us/library/ms188037.aspx edit: who the fuck downvotes me for correcting incorrect information? This subreddit is fucking shit cya guys.
My bad; I thought OP was trying to concatenate a bunch of different .sql files. 
How long will these arrays be? Does it vary from case to case, or are all of them e.g. three elements?
Well you could look at SQL Analysis Services which allows you to run models and things like that. 
I've never used SQL to do the analysis, what I've found as common is to use SQL to pull the data, then pump it into SAS (or build the SQL query into SAS). However, it is more likely that they mean using SQL to get the data and then analyze it using excel or common sense. The only other interpretation I can think of is using SSAS in addition. Any of these options should be specified in the description, so if you don't see SAS or SSAS then all you need is your wits
Actually someone else pointed out what I felt was the best solution and I commented on that and upvoted them. I would write this: select * from &lt;table&gt; where &lt;date_field&gt; &gt;= start_date and &lt;date_field&gt; &lt;= end_date I tested that on a timestamp field and it correctly returned every record from midnight on the start day to midnight on the end date. It also happens to be the same response that another user posted above.
I saw your edit, makes sense now and I learned something new. I thought you were talking about functions didn't realize the way it interpreted the between statement.
Also, how are you storing the data, are you using PHP or something? You could store it in a session as well. I would suggest parsing your object and storing the individual elements as data in your db... you can actually report against it that way.
This is what we do. SQL is the tool we use to pull the data into another system that we then use to analyze the data. We could do everything in SQL I guess but it would be pretty inefficient when we have such a good way to setup business rules.
In Netezza indexes don't exist so several of these tips aren't an issue for us. The big problem our DBA has is folks setting up queries so that they have 15 or 20 running concurrently and it slows the whole system down.
Efficiency is definitely the keyword there.
Thanks for replying! Well, generally, these job postings only go as far as to say "SQL Experience" as either an requirement or as a plus. They don't specify anything else. Maybe this is a dumb question, but what is the program you use to pull data from the database? So all I currently know is the syntax of SQL queries and some rudimentary relational DB design (normalization etc.). However, what programs are being used to access and write queries into genuine databases, I have no clue.
Well, maybe a dumb question, but what is the program you write these queries into or you use to access these databases? 
They probably import the data into a statistical package called R or some proprietary variant of it. R is an open source package for data analysis and is extremely good at generating charts and other data science type things. It has many plugins and hooks to different data sources such as MySQL, SQL Server, etc... [Here's a link to the R project](http://www.r-project.org/)
Personally I would use tables, hands down. Besides it being just more convenient, I can think of too many problems that may eventually arise from having strings of serialized data. You may run into problems if you ever need to do any kind of restructuring, number crunching, analysis, searching or reporting. And what if you switch languages one day? Will the new language be able to read the serialized data or will you have to write a custom unserializer? Your performance concerns may be premature. The system you may find will perform just fine. And if not, there are a multitude of ways to address it that are probably better than big serialized strings.
Yes, I'm using PHP. I am not allowed to access the SOAP server every time the visitor requests it so I would need to find some way to store it. If I store the individual elements as data in the database, I don't really understand how it's done. I could store the combination odds concerning one horse under him I guess but that would mean that the same data would be present on different spots.
Thank you for your response, and yes, I'm a bit worried about the future since this is MySQL which might not have such a bright future. I just don't quite understand how to store the data in a way that makes sense since it's always a different number of keys and elements depending on the betting pool characteristics and the number of horses in each race. Should I make a different table for each pool and store up to 225 rows for the number of possible combinations for example and keep it separate from a main table with pool specific data like turnover and quantity? It seems a bit inefficient.
Which flavor of SQL are you using? As far as actually running the queries, it varies depending on the platform. If you're using Microsoft SQL Server/T-SQL, you'll most likely use [SQL Server Management Studio](http://www.microsoft.com/en-us/download/details.aspx?id=7593). If you're using Oracle/PL-SQL, you'll probably use [Oracle's SQL Developer](http://www.oracle.com/technetwork/developer-tools/sql-developer/overview/index-097090.html) tool. I haven't worked much with MySQL before but I know [MySQL Workbench](http://www.mysql.com/products/workbench/) is an option. You will generally use one of those tools to run queries on a day-to-day basis, and then feed the data into another application (like R or Excel) for actual analysis or formatting. Obviously if you're writing an app with embedded SQL queries you'll use the API for whatever programming language you're using, so you would just use your text editor or development environment of choice.
Not being familiar with your application, I would suggest a table like event_id horse_id (or whatever your array indices correspond to) odds scratched startNrLeg1 startNrLeg2 Make the first two columns the PK, and then you can just `SELECT * etc. WHERE event_id = &lt;value&gt;` and rebuild your data structure.
Thank you! I'll look into those. No flavors really since I've only self taught the querying syntax. I believe most of those have been in MySQL or Postgre but personally I'd like to maximize the chances of it being the same that the employers are using.
Awesome, just what I was looking for. Thank you! I'm finalizing my masters in economics so I have the statistics and math in good order and mostly programming as well but SQL in particular is causing me some headaches since I'm not sure how it's being used in practice at these companies and whether just knowing querying instead of for example a particular software is what they are looking for. 
Well SQL is an ISO standard, so knowledge of SQL transfers well between different implementations even if the specifics are a bit different (oracle, mysql, MS SQL, etc). The basics I mentioned should be nearly identical between the implementations. Also fair warning that a few companies are moving away from sql in pursuit of Big data solutions like Hadoop. Long story short SQL has scaling problems, and No-SQL solutions like Hadoop are seen as the answer. With your background they may offer to train you on the specifics, but they are very different animals from SQL. Also slightly off topic, but with an economics background and a firm grounding in statistics have you thought of applying for Netflix or Amazon, they both have stupidly large data sets and the desire to better understand them, something that might be right up your alley. Anyway, Good luck!
Yeah, I've noticed a couple places which wished for Hadoop or NoSQL experience but they seemed out of my league anyway for the time being. I will definitely look into those once I've mastered everything else. Also I'd love to apply to those companies but I'm from Finland so not possible right now.
Okay, well, it's just one pool with two legs and then we have trifecta or perfecta (picking the first, second and third or first and second place finishers respectively) or twin bets where you predict the first two finishers regardless of who's first and second, and it's all for only one race. Do you think that I should make a separate table for each pool? When the betting pool is more than two legs (Win 6 for six legs for example) I store the pool information as stake percentage for that pool only for the invididual horse in a separate table together with quantity and possible payouts and so on. But with two or less legs I need to store the combination odds.
If your table is static long-term and you can verify the autonumbered IDs are accurately sequenced, using them might be the way to go. Another option for Access/Jet with a fixed given date could be something like: SELECT MAX(TradeDate) FROM ( SELECT TOP 5 TradeDate FROM DateTable WHERE TradeDate&gt;GivenDate ORDER BY TradeDate ) AS T
The autonumbers are fine for now but when an intern adds the trade dates for 2016 or whatever, I'm afraid it will break it. Your idea looks like a good one but my logic was flawed. I think I need to throw out the list of trading days and go 7 calendar days and then go to the 8th if there is are no results from the 7th (holiday) or something like that. Thank you for your help.
Aggregates aren't going to do it, you'll need a self join or you'll need a different table layout.
You can never bet on more than a trifecta for a single race so that shouldn't become a concern. Every betting pool has slightly different parameters but most of them are overlapping and fetched in the same way which encourages to try on-the-fly formulas. There might be a couple different arranging race tracks on a single day and every race track might have half a dozen different type of betting pools and perfecta/twin bets for more or less every single race. With one new row for every single combination odds, so let's say maybe a few hundred rows for each pool and a few thousand rows for each race track and race day, is that really gonna be an efficient way to store this information (rather than to store it as for example serialized strings with just one row per race track and pool)? I guess that I have to try my way performance-wise but I just wanna make a good approximation before I get going. English is not my first language by the way and I'm not a great programmer and self-taught when it comes to SQL if my questions seems a bit obtuse.
Do you have a sample of how your data looks? You could do a blob, though the data becomes worthless for analytics... Maybe a non SQL db would be good for you...
Do you mean the data in the arrays? I gave an example in another post here and it's like that with between 100 and 225 keys in each array and a few thousand characters in total if I stuff the whole array as served straight into the database. It's similar for all combination odds but maybe finishing position 1, 2 and 3 instead of startNrLeg 1 and 2 for trifecta for example. [DoubleOdds] =&gt; Array ( [0] =&gt; stdClass Object ( [odds] =&gt; stdClass Object ( [odds] =&gt; 69 [scratched] =&gt; ) [startNrLeg1] =&gt; 4 [startNrLeg2] =&gt; 2 ) [1] =&gt; stdClass Object ( [odds] =&gt; stdClass Object ( [odds] =&gt; 112 [scratched] =&gt; ) [startNrLeg1] =&gt; 4 [startNrLeg2] =&gt; 4 )
How is $39 off $238 a 50% discount?
I'm an investigative analyst, but very heavy in the data analysis side, in fact, it's the name of my group. We use SQL every day, and at multiple points of our data analysis. Our primary tool for SQL is TOAD for oracle or TOAD Data Point, more often we use TOAD Data Point now, because we can query all of our systems, not just our Oracle warehouse. As for SQL in the analysis, it's used primarily for gathering the data, then we analyze/fold/process the data in Data Point or Excel or other program. But more often then not, after we've done some sort of transformation of the data, we run additional queries to get more useful data. 
There are a couple ways to do this. Here's one way using [ROW_NUMBER()](http://technet.microsoft.com/en-us/library/ms186734.aspx): WITH Ordered AS ( SELECT Widget, TransDate, Qty, ROW_NUMBER() OVER (PARTITION BY Widget ORDER BY TransDate DESC, InvHistoryID DESC) AS N FROM tblInventory WHERE Widget = @Widget AND TransDate &lt;= @Date ) SELECT Widget, TransDate, Qty FROM Ordered WHERE N = 1 ^Edit: ^removed ^superfluous ^line
I think /u/matthra ia on point for what to know in SQL. For me, I need to know: Joins, DISTINCT, Counts, Sum, etc... For programs, we use Toad DataPoint, Toad, SQL Management Studio, and some extensions in Excel that allow us to query straight from databases. Toad Datapoint is awesome because it allows us to run queries against our Oracle Data warehouse, our SQL Server DAtabase, Excel files and Access in one query, so we could join each of the databases/files and make it one nice query. 
Beautiful! I'll definitely have to read up a bit more on ROW_NUMBER() as I can see it being immensely helpful in the future. Thank you very much!
TOAD, seems to be popping up. Ill have to check it. Thanks!
I use rownumber all. the. time. RANK can also be useful, but RowNum especially is.
ROW_NUMBER comes under a group called **analytic functions** which was one of the most important additions to modern SQL in its history. Microsoft sub-categorises some of these as ranking functions (as they are used in ranking processes). Due to their power and flexibility, I believe every DB developer using SQL nowadays should have a grasp of analytic functions. [MS SQL Analytic functions reference](http://technet.microsoft.com/en-us/library/hh213234.aspx) [MS SQL Ranking functions reference](http://technet.microsoft.com/en-us/library/ms189798.aspx)
Agree with all... once you get row number over partion by down you'll wonder where it's been all your life. Then you'll find a dozen Immediate uses for it.
Hmm. How long have you been using it for? Also, have you ever thought that since your getting so many alerts that is a problem. :)
http://documentation.red-gate.com/display/SM4/Setting+up+email+notification &gt; An alert level increases &gt; Some alerts can be configured with multiple thresholds: Low, Medium and High. These alerts are raised with a status of Active and can escalate automatically to a higher level when another threshold is passed. Select An alert level increases to specify that if this happens, a subsequent email should be sent. &gt; &gt; &gt; No emails will be sent if: &gt; the level of an Active alert is automatically downgraded (for example, drops from High to Medium) &gt; an Active alert is downgraded and then subsequently escalates to its previous higher level
Exactly, alerts are raised, but no emails are sent except the first threshold reached until the threshold falls below the first threshold again.
It seems to say here, that Low &gt; Medium &gt; High will receive alerts, but if it goes back down to Medium/Low, it won't alert if it goes back to high. But if you actioned the alert--it'd be cleared out, so if it happened again it would go off.
Have you tried Spotlight?
I should clarify my experience; alerts will show as raised on the alerts page, but it only actually sends them by email when the first one is raised.
Although i expect it to be too expensive, I'll give the trial a shot. Thanks http://www.quest.com/spotlight-on-sql-server-enterprise/ 
so expensive if you're like our setup where you have 10 instances running across 4 machines. Redgate is dirt cheap at it's per server licenses. For quest, it was per instance, ouch.
So would you say redgate is probably the best bet on the cheap end? We might stick with them, part of me is just doing due diligence. 
Yeah, and with the big custom metrics repository http://sqlmonitormetrics.red-gate.com/ you can usually find other things to monitor. 
you can create a scalar function and create a check constraint something similar to this : ALTER TABLE yourTable WITH CHECK ADD CONSTRAINT [CK_songprice] CHECK ([dbo].[songPriceOK](albumID, price)=1) 
We're in a particular industry that has it's own discount code--but with 150+ instances, it's still expensive.
Don't actually do this. Scalar functions should not to be used to fetch data from tables, you'll end up using bad execution plans since albumid and price will be ever changing.
no problem. We have a large SCOM implementation (not 2012), been trying hard to convince people to let us use a proper SQL monitoring solution. I used SQL monitor at my last job and loved it for our 5 server farm.
Awesome! the where/not exists part was where I was getting confused. This is perfect. Thank you! 
Regular expressions. On my phone so I can't look up right now.
Which SQL platform are you working from?
im using Oracle
I'm not sure what syntax you're working in so I won't bother scripting but the basic logic you'll want to apply I think follows: first find the position of the first space character (assumes no leading spaces, if that's a possibility you'll need to trim first). Return the substring if everything to the right of that position (ie everything after the first space) and if that substring like 'sales %' , return it. Using the right built in functions, it should be pretty easy to fit this into a single where clause. If you tell me the tool/DB your using I could point you in the right syntactical direction.
In that case, I'm useless unfortunately. In the future, be sure to include your platform in the submission.
thanks! im using oracle 
Thank you i will try it out!
You could look into defining an oracle function in PL/SQL that returns the 2nd word of a given string, something like they do on http://stackoverflow.com/questions/2333849/parsing-second-word-in-string-in-oracle Then use that function in your query.
This is probably the cleanest and most understandable way... There are some regex methods that could prove faster, yet wouldn't make much sense unless you are a pro with regex and many extension functions that are included in SQL techs. Oracle has a regex_substr function for this, use level and connect bys .. 
I know it's not an answer, but I have to ask... Are you using this query for a one-time data transfer out of a badly designed legacy system? If not, you really shouldnt build anything that relies on substrings like this as a view. I guarantee you will end up with 'General Sales Accounting Manager' in that field eventually, that contact will be in accounting (not sales), your view will pull them in, and sadness will ensue. Get another table going for 'Department' or something like that and add a field to Contact for the departmentID foreign key.
I haven't tested yet but I believe using GO will solve the problem.
You *met with*? I think you mean the one you work at...
Also, $10/month just to visualize SQL queries? You do realize that there are a ton of free tools, right?
Just adapt your question so its not sensitive, then post it here 
Are all the fields you want to test on in the same table? If so, your example should work, if I understand what you want. SELECT * FROM your_table WHERE email = 'test@test.com' OR dti_id IN (47,51) OR username = 'test'; 
Right, but I don't know the 47,51, or 'test' ahead of time. Those would be retrieved from the columns where email = 'test@test.com'. Does that make sense?
&gt; There probably a more efficient way, but this will get the job done. That's our motto here.
it's not hard, and it's homework ...
It's when you seek to identify the tables, columns, and transformations that have an impact on a selected table or column. It's part of Impact Analysis.
Thank you I will give this a try and get back to you
That wont work if his date is an actual timestamp with a greater granularity than the month. But replacing ADATE (all instances) with columnAlias = DATEADD(day, 1 - DATEPART(day, ADATE), ADATE) should do the trick.
Hmm.. I'm getting : Operand data type bit is invalid for sum operator.
sum(convert(int,sent)) + sum(convert(int,received)) ?? That would probably work
Thanks, that worked.. but i've completely got this backwards.. I need to re-think this as i cannot produce the chart i Need with the data in this format.. grrr
Thanks to those that have helped me.. What you've given me works perfectly in Excel.. however in this stupid wallboard system it does not.. Ive created an image to try and explain what i need http://imgur.com/5Q1qHrM
erm... SQL Server 2010
Something like this: select * from ( select ..., 'Sent' AS Direction, sum(sent) as Total union all select ..., 'Received' AS Direction, sum(received) as Total ) Report ORDER BY Report.Month should do it, obviously this is not going to work as written but I think you will get the idea, you need to select a count for all "SENT" and union that with a select of the count of "RECEIVED"
I'll give this a go, it's sometihng i kind of had working previously but i was missing the column that defined whether the data was "sent" or "received" as that doesn't actually exist in the source..
This has got me the data in the right format.. all i'm missing is a column that identifies whether this row is showing me the "SENT" or the "Received" count : SELECT Month1,Month2,COUNT(*) as Count FROM (Select Month(ADDEDDATE) As Month1, DateName(Month,ADDEDDATE) As Month2 FROM Sunrise.Incidents Where INCIDENTS.SURVEYSENT = '1' and DATENAME(Year,ADDEDDATE) = year(getdate())) As Table1 GROUP BY Month1, Month2 UNION ALL Select Month1,Month2,COUNT(*) as count FROM (Select Month(ADDEDDATE) As Month1, DateName(Month,ADDEDDATE) As Month2 FROM Sunrise.Incidents Where INCIDENTS.CUSTSURV = '1' and DATENAME(Year,ADDEDDATE) = year(getdate())) As Table1 GROUP BY Month1, Month2 Order By Month1
Well if that is all you are missing just add constants 'SENT' AS Direction to the top select statement so line 1 would become: SELECT Month1,Month2,COUNT(*) as Count, 'Sent' AS Direction FROM then add something similar to the beginning of the second select statement and you are golden
One day, 3 years from now you'll be like "dammit, finally I've run into such an odd situation involving some jerk using multiple join tables in scalar functions", or sooner if you've ever use Project Server 2007.
oh don't worry, I don't have to wait 3 years for that. I deal with shitty code all the time. Filtering on scalar functions is my favourite. 
commenting to see replies. I have no idea how to approach this but would love to see someone who does. *Edit - is your dictionary table open sourced? or where did you find it?
SQL loops are not recommended, as they use inefficient to other methods. A few questions: 1. Regarding the 400 more lines of code, is that SQL code, or is that another language invoking the query? 2. Why are you converting your dates to text? SQL is much faster if you search by the datetime rather than a text string. (I'll assume that the column data is a text string, rather than a datetime but if thats true, why is it stored as that?) The best solution would be to have the language invoking the query increment the @Date variable and just run the query again, then do stuff with the results, and keep looping.
SQL is not a good tool to solve this problem. I'm sure it's possible, but it's going to be nasty. You'd be better off with a general purpose language.
Many thanks!
I guess I'll just post everything I'm doing, which is a little more complex than the question asked, but maybe I missed something in my attempt to simplify the problem. Everything here is exactly as written, except I'm going to change the username and email address for the user's privacy. So in the auth_users table, there are two rows with these values for username, dti_id, and email: * myuser1, 71328, myuser@test.com * myuser2, NULL, myuser@test.com The username column is unique, so there will never be any duplicates there. But what I want is searches for username, dti_id, or email to return both of those records, because their emails match. Similarly, it should return both if the dti_id's match. If I search by email "myuser@test.com", it of course returns both. But if I search for username "myuser1" or dti_id "71328", it only returns the one record. I am using Perl DBI to do it. Here is my code, with the first two lines just to define variables. my $key = &lt;"username" or "dti_id" or "email"&gt;; my $value = &lt;the entered search term&gt;; my $sql = "SELECT * FROM auth_users WHERE dti_id IN (SELECT dti_id FROM auth_users WHERE LOWER($key) = LOWER(?)) OR email IN (SELECT email FROM auth_users WHERE LOWER($key) = LOWER(?))"; my $sth = $dbh-&gt;prepare($sql); $sth-&gt;execute($value,$value); But if I run that code with $key="username" and $value="myuser1", it only returns the one record.
I would say you should review your business requirements first, because once you place 'no discount' in that field, you'll not be able to sum it anymore. Nonetheless, since you can't convert the text to be numeric, you can convert the number to varchar. coalesce(cast(Discount as varchar(25)),'no discount') You'd be better off setting it to 0. coalesce(Discount,0) Good luck. 
In this situation, I think *NULL* actually provides a lot of value. There's a difference between a discount of 0 and a discount that doesn't exist. *NULL* is the right way to express that without forcing a different data type. Ghostlistener, it really depends on whether you're trying to calculate something (don't cast to VARCHAR!) or if you're trying to display something to an end user. Even then, there's generally a separation between the SQL and the presentation layer that can present *NULL* values with a more informative meaning. But if you are trying to simply present the data and raw SQL is your only tool, then have at it with COALESCE or even a CASE statement. 
&gt; does the database know when there's a single "cell" (i.e., result where rows = 1 and columns = 1) that you can use it as the value of a conditional? yes, because if it were more than one value, you'd get a syntax error it's called a **scalar subquery**
Thanks, good point, Z is thankfully unique. 
I was testing things and found out that's what it's called via an error. Good to know this is the proper way to do it. Thanks for the help!
I agree it does feel weird, but the only other (sane) way to do it would be finding the maximum value of z in a separate query then using it as a variable in a prepared statement. However, I'd only do that if you were going to reuse the maximum value of z in multiple places. You do do mention that Z is unique, but you want to be sure that it is enforced on input, either making Z a unique constrained field or prevent people from inputting a value for z that already exists.
Of course, then he could just change "=" to "IN" and CRY HAVOC AND LOOSE THE DATA OF WAR!
I would use CTE: With foo as ( Select email, username, dti_id from table where email = test@test.com) Select * from table t Inner join foo f on t.username = f. Username or t.dti_id = f.dti_id Sorry about formatting I'm on my phone.
Sure. Pm me. I won't be able to look at it till tomorrow. Its 9:51 PDT here. If that works I'd be happy to help.
I forgot to add the join condition Left join Categoriestable categories On foreign key in beer table = primary key in categories table Left join always returns every row of left table and only matching rows in right table.
Parameters are in the form :PARAMNAME. I'm assuming DTI_ID is numeric. select * from AUTH_USERS where EMAIL in (select EMAIL from AUTH_USERS where (:EMAIL is null or lower(EMAIL) = lower(:EMAIL)) and (:DTI_ID is null or DTI_ID = :DTI_ID) and (:USERNAME is null or lower(USERNAME) = lower(:USERNAME))
Ok. So here is what it would look like given what you've posted Select be.beername, categories.style From be Left join categories On be.beername = categories.beername I don't understand the hint suggesting outer/inner joins. Inner joins only return rows from both tables matching. Outer does the opposite. Left gives you every beer AND every style that has a matching field. Without knowing what fields in your categories tables are, I'm just guessing on what to use for the join. 
can you do a select top 10 * from surveyssent ?
here: https://lh4.googleusercontent.com/-4ISOrjVC5T4/Trv8vcHTQHI/AAAAAAAAABM/zUpj8AUA-Z8/w800-h800/Visual_SQL_JOINS_orig.jpg
Oh I'm not really trying to accomplish anything, just curious how it would be done. So I'm just displaying it. 
You're seriously posting a cell phone picture of your screen showing a bunch of completely useless crap in Excel and ask for help on a query that's not even in the shitty picture? No.
oh, then u/incrediblemouse gave good advice ;) but i would just set it to 0 knowing that 0 is no discount
If you have more questions, then there are more ways to pivot the data in SQL. However with just 8 questions, this is a simple way to get your metrics. Select A.answer ,(SELECT COUNT(*) FROM surveyssent WHERE Q1 = A.Answer) Q1 ,(SELECT COUNT(*) FROM surveyssent WHERE Q2 = A.Answer) Q2 ,(SELECT COUNT(*) FROM surveyssent WHERE Q3 = A.Answer) Q3 ,(SELECT COUNT(*) FROM surveyssent WHERE Q4 = A.Answer) Q4 ,(SELECT COUNT(*) FROM surveyssent WHERE Q5 = A.Answer) Q5 ,(SELECT COUNT(*) FROM surveyssent WHERE Q6 = A.Answer) Q6 ,(SELECT COUNT(*) FROM surveyssent WHERE Q7 = A.Answer) Q7 ,(SELECT COUNT(*) FROM surveyssent WHERE Q8 = A.Answer) Q8 From (Select distinct answer from (select q1 answer from surveyssent union select q2 answer from surveyssent union select q3 answer from surveyssent union select q4 answer from surveyssent union select q5 answer from surveyssent union select q6 answer from surveyssent union select q7 answer from surveyssent union select q8 answer from surveyssent ) ) A
It's all about the kharma, baby!
I figured it out: SELECT 'Q1'as Question, sum(case when Q1=1 then 1 else 0 end) '1' ,sum(case when Q1=2 then 1 else 0 end) '2' ,sum(case when Q1=3 then 1 else 0 end) '3' ,sum(case when Q1=4 then 1 else 0 end) '4' ,sum(case when Q1=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q2'as Question, sum(case when Q2=1 then 1 else 0 end) '1' ,sum(case when Q2=2 then 1 else 0 end) '2' ,sum(case when Q2=3 then 1 else 0 end) '3' ,sum(case when Q2=4 then 1 else 0 end) '4' ,sum(case when Q2=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q3'as Question, sum(case when Q3=1 then 1 else 0 end) '1' ,sum(case when Q3=2 then 1 else 0 end) '2' ,sum(case when Q3=3 then 1 else 0 end) '3' ,sum(case when Q3=4 then 1 else 0 end) '4' ,sum(case when Q3=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q4'as Question, sum(case when Q4=1 then 1 else 0 end) '1' ,sum(case when Q4=2 then 1 else 0 end) '2' ,sum(case when Q4=3 then 1 else 0 end) '3' ,sum(case when Q4=4 then 1 else 0 end) '4' ,sum(case when Q4=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q5'as Question, sum(case when Q5=1 then 1 else 0 end) '1' ,sum(case when Q5=2 then 1 else 0 end) '2' ,sum(case when Q5=3 then 1 else 0 end) '3' ,sum(case when Q5=4 then 1 else 0 end) '4' ,sum(case when Q5=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q6'as Question, sum(case when Q6=1 then 1 else 0 end) '1' ,sum(case when Q6=2 then 1 else 0 end) '2' ,sum(case when Q6=3 then 1 else 0 end) '3' ,sum(case when Q6=4 then 1 else 0 end) '4' ,sum(case when Q6=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q7'as Question, sum(case when Q7=1 then 1 else 0 end) '1' ,sum(case when Q7=2 then 1 else 0 end) '2' ,sum(case when Q7=3 then 1 else 0 end) '3' ,sum(case when Q7=4 then 1 else 0 end) '4' ,sum(case when Q7=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent] UNION ALL SELECT 'Q8'as Question, sum(case when Q8=1 then 1 else 0 end) '1' ,sum(case when Q8=2 then 1 else 0 end) '2' ,sum(case when Q8=3 then 1 else 0 end) '3' ,sum(case when Q8=4 then 1 else 0 end) '4' ,sum(case when Q8=5 then 1 else 0 end) '5' FROM [Surveys].[dbo].[SurveysSent]
Thank you, it figured it out with a case statement but this is much more dynamic. Thank you!
It was not a homework assignment but a work on. I got figured out shortly after posting. I posted my answer in the original comment. Sorry for wasting your time, honestly. 
It's usually a good idea to avoid subqueries whenever possible. In this case you can do the same thing with an ORDER BY: SELECT TOP 1 * FROM table ORDER BY z DESC
Per /u/nrmjba's suggestion of using a common table expression, this should work for all keys as long as the input value isn't null: WITH foo AS (SELECT email, username, dti_id FROM auth_users WHERE LOWER($key) = LOWER(?)) SELECT distinct u.* FROM auth_users u INNER JOIN foo f ON u.username = f.username OR u.dti_id = f.dti_id OR u.email = f.email;
&gt; SELECT TOP 1 * FROM table ORDER BY z DESC Yeah, I just tested and although that query worked in H2 it doesn't work in PostgeSQL, unfortunately. If I ever need to optimize more I'll consider your suggestion, thanks! 
This matching (aka joining) is usually automatically done with higher database operations that are not really available in excel. I don't know Access deep enough to know how to do it in there, but I do know you will be better off in any form of database application. Access is probably more convenient to you though, as it comes with MS Office. You might have more luck in /r/MSAccess/ for more in-depth help.
thank you
It's not a lady...
I think what you're saying helps with the $sth-&gt;execute($value,$value);, so I don't have to repeat the $value, though it looks like a really different SQL statement from what I'm trying to do.
I get "ERROR: syntax error at or near "WITH" at character 1" when I try this.
Nope. I just didn't expect any help without offering something in return.
What version of Postgres is it? WITH needs 8.4 or higher, I believe.
I ask questions all the time between /r/SQL and /r/SQLServer. Always got help, never had to degrade myself to get it. The world isn't filled with as many perverts as you've been lead to believe.
Ah.. PostgreSQL 8.1.9.
I knew a woman like this a few dozen years ago. Fucked her way right up to a SVP postion in Marketing Information for a large insurance company before anyone realized she didn't know anything. By then her looks had gone so she was of little use. Last I heard she was a waitress for a Denny's in downstate Illinois somewhere. So sweetie, for the sake of yourself please do your own work. It is not very hard and much more rewarding.
I rarely work in MSSQL and syntax varies (especially with dates), but this should put you on the right track. What you're missing is selecting the "orders taken" column, and you'll want to sum that as opposed to counting the OrderDate. Something like: SELECT sum(orders_taken), Orderdate FROM Orders WHERE .... GROUP BY OrderDate 
So the final write is to Oracle? And I'm assuming that you don't have write access to the other DBs so that you could just do what you explain in paragraph 4, then select * from that and write that to Oracle? Or maybe I'm not understanding you. Would WITH work for you?
This article goes over this subject rather well: http://www.itdeveloperzone.com/2012/11/generate-list-of-dates-in-sql-server.html
If he's querying the Orders table isn't it a fair assumption that it's one row per order? A count the way he's done it seems fine to me.
Can you provide more details about your setup? Is this a local machine? Windows/Linux/Mac? A remote webhost? Was is the ".sql file"? Was it a dump of a single table, or a single database with many tables, or many databases and many tables?
If you have administraion over the phpMyAdmin install, check the php.ini file and change the `upload_max_filesize` setting. Otherwise, you'll need to use the mysql command line tool.
Put the vaiables into the select after the insert statement. Don't use a values statement. This inserts the whole result set from the select statement into the table. insert into a (col1, col2, col3, col4, ect...) select b.var1, 'blah', 123, b.var2, 2, 2, 3, NULL, 3 from b For your update use a join if you can depending on what the values are returned by your select result set. If you can't use a join you'll have to use a subquery (or a cross apply). update a set a.col1 = b.var1 from table1 a join table2 b on a.bkey = b.key update a set a.col1 = (select SUM(sb.var1) from table2 sb where sb.xyz = a.col5 group by sb.col2) from table1 a
You could use a self join in this case to find out what you need. Let me know if this is what you were looking for. I created a table in MYSQL (don't have access to my Postgres instance) called Auth_Users: CREATE TABLE auth_users ( username varchar(100), dti_id int, email varchar(100) ); I then inserted the following 5 records: ('myuser1',71328,'myuser@test.com') ('myuser2',NULL,'myuser@test.com') ('myuser3',71328,'not_email@test.com') ('myuser4',75555,'not_email@test.com') ('myuser5',5,'bob@test.com') The first 2 have a shared email, the second 2 share an email, the 1st and 3rd share an dti_id, and the 5th record is not matching anyone If I run the below query: SELECT DISTINCT au1.* FROM auth_users au1 JOIN auth_users au2 ON au1.email = au2.email OR au1.dti_id =au2.dti_id WHERE au2.email = 'myuser@test.com'; I get the following results. We get the first two users (user1 and user2) because we searched for that e-mail address. However we also pulled in user3 because they share a dti_with user1: +----------+--------+--------------------+ | username | dti_id | email | +----------+--------+--------------------+ | myuser1 | 71328 | myuser@test.com | | myuser2 | NULL | myuser@test.com | | myuser3 | 71328 | not_email@test.com | +----------+--------+--------------------+ If I run the following query: SELECT DISTINCT au1.* FROM auth_users au1 JOIN auth_users au2 ON au1.email = au2.email OR au1.dti_id =au2.dti_id WHERE au2.dti_id = 71328; I get the following results. We get the user1 and user3 because we searched because of their dti_id. Then we get user2 because their email matches user1. We also get user4 because they match the email of user3. +----------+--------+--------------------+ | username | dti_id | email | +----------+--------+--------------------+ | myuser1 | 71328 | myuser@test.com | | myuser2 | NULL | myuser@test.com | | myuser3 | 71328 | not_email@test.com | | myuser4 | 75555 | not_email@test.com | +----------+--------+--------------------+ And last query to match what you gave as an example above (searched by a username and only returned the 1 record). Query: SELECT DISTINCT au1.* FROM auth_users au1 JOIN auth_users au2 ON au1.email = au2.email OR au1.dti_id =au2.dti_id WHERE au2.username = 'myuser1'; I get the following results. User1, obvious, We get user2 because of the email match to user1, and user3 because of the matching dti_id to user 1 also. User 2 does not get returned because he doesn't match any of the criteria of user4. If you want a query that matches user 4 we would have to take a different approach. +----------+--------+--------------------+ | username | dti_id | email | +----------+--------+--------------------+ | myuser1 | 71328 | myuser@test.com | | myuser2 | NULL | myuser@test.com | | myuser3 | 71328 | not_email@test.com | +----------+--------+--------------------+ I believe this is the info you wanted: to find all users that share a duplicate property of the user you searched for. Hopefully I didn't go into too much detail (or talked like you don't know what your doing) as I just like to be thorough.
Do you have shell access? FTP the file up and use the command line tool to import it..
Glad to see that it helped :). We shouldn't see the joins matching on NULL because NULL is not really a value. It doesn't actually represent anything so it cannot match anything (including other NULLS). This is why have special statements like IS NULL and IS NOT NULL (WHERE dti_id IS NULL/IS NOT NULL). We have to specify that we are looking for cells whose values do not exist/have never been instantiated (which is different from having a empty value like a blank string). But to make sure lets I will illustrate below. I have added a 6th user with another dti_id that is null, this would make user6 and user2 the only two records with NULL for dti_id: ('myuser6',NULL,'Ninja@test.com') If the Null matched Null then we would expect to see following record joins: user2 matches user1 because of EMAIL; user2 matches user2 because of EMAIL; user2 matches user6 because of NULL dti_id; -- We don't want this to happen user6 matches user2 because of NULL dti_id; -- We don't want this to happen user6 matches user6 because of EMAIL; I have done another query which should return all the records in the join: SELECT * FROM auth_users au1 JOIN auth_users au2 ON au1.email = au2.email OR au1.dti_id =au2.dti_id; And here we have the result set. As we can see we don't get a match on the NULL values and only see the three records we expected above. So we don't have to worry about the joins happening on a null value as the database doesn't seem them as values. +----------+--------+--------------------+----------+--------+--------------------+ | username | dti_id | email | username | dti_id | email | +----------+--------+--------------------+----------+--------+--------------------+ | myuser1 | 71328 | myuser@test.com | myuser1 | 71328 | myuser@test.com | | myuser2 | NULL | myuser@test.com | myuser1 | 71328 | myuser@test.com | -First Expected(Email) | myuser3 | 71328 | not_email@test.com | myuser1 | 71328 | myuser@test.com | | myuser1 | 71328 | myuser@test.com | myuser2 | NULL | myuser@test.com | | myuser2 | NULL | myuser@test.com | myuser2 | NULL | myuser@test.com | -Second Expected(Email) | myuser1 | 71328 | myuser@test.com | myuser3 | 71328 | not_email@test.com | | myuser3 | 71328 | not_email@test.com | myuser3 | 71328 | not_email@test.com | | myuser4 | 75555 | not_email@test.com | myuser3 | 71328 | not_email@test.com | | myuser3 | 71328 | not_email@test.com | myuser4 | 75555 | not_email@test.com | | myuser4 | 75555 | not_email@test.com | myuser4 | 75555 | not_email@test.com | | myuser5 | 5 | bob@test.com | myuser5 | 5 | bob@test.com | | myuser6 | NULL | Ninja@test.com | myuser6 | NULL | Ninja@test.com | -Third Expected(Email) +----------+--------+--------------------+----------+--------+--------------------+ Hopefully all your concerns have been alleviated by this. Let me know how it goes with the query when you get a chance to run it. 
*sigh*… I just tried it, copying your query exactly and only making the necessary change to pull in the search variables and it doesn't work. It always just returns the exact matches, none of the duplicates. No idea why. It really looked like your code would work.
 update a set a.col1 = sb.var1 from table1 a inner join table2 sb on sb.xyz = a.col5
To see the data which an `INSERT` added you can use the `RETURNING` keyword. For example like this: INSERT INTO Doctor VALUES (2, 'Dr House', '0400 453 211') RETURNING * To see an entire table run this command: SELECT * FROM Doctor
I never knew about RETURNING *. That is pretty awesome.
Send the dump to the server admin saying that you would be greatly appreciative if they would import it for you because it's too much data for how php is setup. Give as much info as possible as far as server name, database name etc. Also if the file is larger than a few megs it might be better to work with the Admin on getting the data over. This would be better than emailing a large file. If for some reason you have a "critically important" database and no admin, then lol hahaha at your situation. Also see the comment about updating the php settings to allow larger uploads. Someone has to have access to the server.
Interesting... Can you post your perl code so we can see what that is doing? *edit: I just installed Postgres 8.1.14 and ran the query examples I had above again. They returned the expected sets. I think there might be an issue with the way you are calling the query in your code. 
It is very useful especially when combined with [writable CTEs](http://www.postgresql.org/docs/9.3/static/queries-with.html#QUERIES-WITH-MODIFYING). Inserting the result of a DELETE into another table without rrace conditions is neat.
Untested, but something like this should get you the counts :- SELECT Count(CASE WHEN servicetype LIKE '%ADSL%' THEN 1 ELSE NULL END) AS ADSL, Count(CASE WHEN servicetype LIKE '%VDSL%' THEN 1 ELSE NULL END) AS VDSL, Count(CASE WHEN servicetype LIKE '%FTTH%' THEN 1 ELSE NULL END) AS FTTH FROM tablename 
Something like this is usually done by using a numbers table, or by creating a numbers recordset on the fly. This should give you an idea : declare @orders table (id int identity, orderDate datetime, something varchar(255)) declare @startDate datetime = '20140101' declare @endDate datetime = '20140131' declare @diff int = datediff(day, @startdate, @enddate) ;with n1 as (select n = 0 union select n = 1) , n2 as (select n1.n from n1 n1 cross join n1 n2) , n4 as (select n1.n from n2 n1 cross join n2 n2) , n8 as (select n1.n from n4 n1 cross join n4 n2) , n16 as (select n1.n from n8 n1 cross join n8 n2) , n32 as (select n1.n from n16 n1 cross join n16 n2) , dates as ( select top(@diff) dateOfOrder = dateadd(day, row_number() over(order by (select null)) -1, @startdate) from n32 ) select d.dateOfOrder , count(o.id) from dates d LEFT JOIN @orders o on d.dateOfOrder = o.orderDate group by d.dateOfOrder order by d.dateOfOrder asc