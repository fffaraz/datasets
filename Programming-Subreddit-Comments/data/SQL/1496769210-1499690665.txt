Paratwa's solution above worked, but I'm going to try yours as wellas I've never used a CROSS APPLY before. You guys amaze me how you can just churn this stuff out, while I'm over here pulling my hair out for hours trying to even think of where to begin!
Happy to help, you could have also used max but with the order by there you can chose various combinations if you need a different response from the data.
 Select 'Paid' as 'Paid Status', * from payable union select 'Unpaid', * from unpayable this puts everything into one column (as paid or unpaid) If you want two different columns, just add another comma with a blank
I have so much to learn. I never have to do much outside of some joins and wheres...so I don't get to practice fancier stuff often. Then by the time I need it again, I've forgotten the new tricks I learned =p.
&gt; Select 'Payable' as 'Pay Column, * from payable &gt; union &gt; select 'Unpayable', * from unpayable Thank you! Just adjusted it a tiny bit and it worked great!
It just comes with time man. I know some amazing engineers who I've had the pleasure of working with, and I'd say this even with 15 years of experience working in huge data on a scale of 1-10 on knowledge I'd say I am maybe a 5 or 6, I've *seen* one guy I'd say was an 8 or 9 once but he was barely functioning as a normal human and was unable to communicate worth a damn so he was gone in a year or two. 
I think I'll be okay being in the 5 or 6 range, then =). I just want to get to the point where when I run into issues like this, I at least have an idea of things to try instead of just having to google it. I know it comes with times, but I'm always frustrated that I don't learn faster =). 
Yup! One of the things I do with my developers (whether its sql, c, perl, vb, whatever) is get them to do overviews with the rest of the team, that's *after* a review, which includes two people, one guy who knows the same subject area and a secondary person who knows nothing about that dataset. This spreads the knowledge throughout the team in a spectacular way.
When I add the declare the variables it still gives me an error. When I run the code above it returns this error message: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'DECLARE @CTime DATETIME' at line 1
Yes, you can run the four set commands just fine. When I try to run my code I get this error: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'WHILE (@LCV &lt; 25) DO SET @Salt = @Salt + CHAR(ROUND((RAND(@Seed) * 94.0) + 32' at line 1 
See if this helps. &gt; You can't do a for loop in an SQL editor without a stored procedure. I use MySQL Workbench to make this. &gt; A quick stored procedure should do the job: [Source](https://dba.stackexchange.com/questions/62812/how-to-do-while-loops) https://dev.mysql.com/doc/refman/5.7/en/while.html So try this: Create Procedure dowhile() Begin SET @CTime = NOW(); SET @Seed = (HOUR(@CTime) * 10000000) + (MINUTE(@CTime) * 100000) + (SECOND(@CTime) * 1000) + ((SECOND(@CTime) * 1000) * 1000); SET @LCV = 1; SET @Salt = CHAR(ROUND((RAND(@Seed) * 94.0) + 32),3); WHILE (@LCV &lt; 25) DO SET @Salt = @Salt + CHAR(ROUND((RAND(@Seed) * 94.0) + 32),3); SET @LCV = @LCV + 1; END WHILE; SELECT @Salt; END; 
Just as a pointer, it's considered best practice to declare your columns vs select all. There are times to use select *, but they are very specific. As a side trick, put your cursor to the right of the * and hit tab, it will auto populate every column in order for you.
 DROP TABLE A DROP TABLE B DROP TABLE C ...
Thanks for your reply! I had seen talk of ignore dupe, but I don't have much access to the database, so I wasn't sure if I could alter the index, plus I was a little hesitent to change something like that until I'm a little more experienced. =)
ooh....um, i think in MSSQL you may be able to navigate to the tempDB folder and delete them all. 
Thank you for your response. I actually had tried that previously, but it still gives me an error when I try creating the procedure. Running this code still produces an error when I try to run it through workbench: Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '' at line 4 When I hover over line 4 ('SET @CTime = NOW();') in the editor it says "syntax error: missing 'semicolon'"
We're going to want the RDBMS / version and create statement. 
It still seemed to produce the same error when I changed it.
I wouldn't consider it most efficient, but it's the advice I always give. Pick a data set that is interesting to you. Sports box scores, game info, poker stats, census data, weather, whatever. Find a source for that data. Download it. Put it into tables. Start asking questions of the data, writing queries, creating views. Make some new tables to organize the data differently. Try to design an index that will make a query faster, even if it's from 1 second to milleseconds. Basically, find some data that can keep your attention and play with it.
Thanks for the advice/help everyone! [This what i ended up going with ;P](https://imgur.com/a/obG3k) Feeling like i kinda dodged the real solution here but my homework is also due soon, so. I did learn from you guys suggestions and advice though :) 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/aiWIRI4.png ^[Source](https://github.com/AUTplayed/imguralbumbot) ^| ^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^| ^[Creator](https://np.reddit.com/user/AUTplayed/) ^| ^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^| ^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dijr2kk) 
not in a sane/safe way. BUT who am I to judge??? :D :D :D So to just answer the question and not question your sanity. ******** I would just grab all the tables you want to drop into excel then create a concatenated formula to drop each table. **OR** create a query that pulls all of the tables you want &gt; &gt; use databaseNameYouHate &gt; &gt; select &gt; ist.*, &gt; &gt; 'whatevercommand you want drop table ' + ist.table_catalog + '.' + ist.table_schema + '.' + ist.table_name + ';' as madness &gt; &gt; from &gt; information_schema.tables ist &gt; &gt; where ist.table_name is like '%thevarioussimilarnames%' then copy that madness output into mssql and run it. PLEASE for the love of god make sure its not the master database or contains anything that you vaguely may be interested in keeping.
either run a query on informationschema or sys to get the drop command of all tables you want as: select 'drop table ' + t.name ' + ';' FROM informationschema.tables as t WHERE &lt;filter&gt; remember to perhaps get the schema names too unless you are one of those people who put everything in your default schema (please don't) or run something like this if you want every single table removed: EXEC sp_MSforeachtable "DROP TABLE ?" 
&gt; Find a source for that data. Download it. Put it into tables. This is where I'm at right now. I'm a javascript guy trying to get better at sql. I built a web scraper and pulled some data that I want to play with, but right now it's all in JSON. Trying to convert this json into tabular data has been very challenging. There are some cases where I need to create or update a certain row, but the source data doesn't provide a unique key that I can use to check for the existence of the row, so I'm needing to write queries on the surrounding data to figure out what to do. Then I have to figure out how to tear the json apart into rows that I can insert into the proper tables. As a learning exercise, I'm trying to do this from inside postgres, initially using jsonb, and without the aid of an imperative language. Coming from javascript and nosql dbs, sql syntax feels quite foreign, and I'm having trouble internalizing some data structures and how to work with them.
This would be super easy with MSSQL, you could create a recursive CTE to populate a in-line view of all dates. Because My SQL doesn't support CTEs or, I think, recursive inline views you would want to actually create a table with all your possible dates, and left join your results to that table. He's a procedure I've stolen from Stack Overflow that will create your dates table: DROP PROCEDURE IF EXISTS filldates; DELIMITER | CREATE PROCEDURE filldates(dateStart DATE, dateEnd DATE) BEGIN WHILE dateStart &lt;= dateEnd DO INSERT INTO tablename (datename) VALUES (dateStart); SET dateStart = date_add(dateStart, INTERVAL 1 DAY); END WHILE; END; | DELIMITER ; CALL filldates('2011-01-01','2011-12-31'); Change the dates in the last line to the range you want to appear in your report and the datename/tablename. Then you can change your current query to: SELECT id, datename, DAYNAME(datename) FROM tablename LEFT JOIN users ON users.activity_date = tablename.datename WHERE datename &lt; '2017-01-01' ORDER BY DAYOFWEEK (activity_date) 
Interesting where can I get sample data sets to work on in SQL server?
Pk = primary key Fk = foreign key Sk = surrogate key, typically used in Kimball(-ish) schemas for OLAP. Best example is probably a T2 SCD, where each row version gets a unique Sk in the dimension to be used for OLAP (because it's more efficient to process than the composite of Pk + Start + End). And, if you run across it, a Bk is a business key, usually the Pk from a source system. If anyone tells me their data model "blows a lot of people's minds", I assume it sucks. Would love to see an ERD and use case/benchmarks but can't think of any good reason to separate records from their Pks.
https://www.kaggle.com/datasets is a great place to start. If there's something you want to know more about, just start googling and usually you can find some sort of online api you can tap into.
Almost anywhere, you can import .csv or .txt files pretty easily via SQL (BULK INSERT) or SSMS (right click DB in object explorer, tasks, import data. The wizard is old and kinda janky but usually works). Microsoft also produce full sample databases. The one for 2016 is https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers
I'd be looking to either Build a table holding all intervals, join to that using BETWEEN. But BETWEEN is slow as shit, no good if you have large volumes. Transform the timestamps directly by rounding to the closest 15 or 30 minute interval (in MSSQL, DATETIME2FROMPARTS using some kind of rounding on the minutes).
Thanks for your answer however I would like to not create a table b/c I also want to be able to add other consecutive rows such as month. If I were to use your solution I would also have to create a table with possible month values. 
Surely you'd just add month to the same table, if your base data was at per-day level?
Typically, I'll want to perform some transformations of the source data, and want to conditionally insert/update records based on if I have already received them. I prefer to use native SQL (batch mode) than compiled languages (RBAR) to perform such transformations and conditional logic. It's partly personal preference, but I've had a lot of success getting external data into a table as simply as possible, THEN worrying about how messy it might be. It's so much easier to trouble shoot bad data when you can query it vs having to debug code.
I was hoping for a more dynamic way. B/c what if I want to sort by hour, or by every other week, or by month and year? I was hoping to use PHP to build out mysql statement. So there's no sort of command where i could say something like: SELECT id, date_act FROM (INSERT INTO this (id, date_act) VALUES (2, 'xxxx/xx/xx'), (3, 'xxxx/xx/xx') ) And perform some sort of UNION with my original data? 
Just an update if you're curious. It ended up being that they just prefix the column names with PK or FK or SK or whatever. And that the keys don't always match up in naming convention (obviously). So he definitely misspoke to me on the phone and made it sound more weird. Lol. But anyway he said I passed the test. Thanks for helping out.
Do you know of any example dbs for postgres or MySQL that are on par with World Wide Importers?
I'm afraid not, I've only worked with MSSQL thus far.
Great to hear, thanks for the update! Honestly, it seems to me that having your column names reflect what type of key it is, is just additional work in the long run. Think of all the reporting queries or even procedural queries that run SELECT/UPDATE/DELETE. If your keys change, suddenly you have a LOT of work to do. I get adding a PK prefix to the primary key constraint/index, but the column name is just asking for trouble. FK, that's a bit more understandable, but still not really recommended as those may change too. Hope you get it! Keep me posted?
First, I have no idea how this is a SQL related question at all. Secondly, it sounds like they are able to make a content change (words in the page) as opposed to things that are in the URLs, and page names which might have technical impact. It seems like if you are ok with the word "influencer" remaining in the URL, but just changing in the content you could save some money. While this isn't ideal, depending on how things were designed, I could understand technically how this could be the case. Instead of reaching out to strangers on the internet, why not ask him to clearly explain the situation to you and provide a detailed estimate of cost along with the the impact of what you are asking for so you can fully understand it. It seems less likely that he is trying to rip you off and more likely that you don't understand the impact of the request and that the problem is a communication problem between you and your technical team. 
Single word changes can have far reaching affects. Ask him for an explanation. It sounds like the issue is renaming the page name in the address/URL is the problem, which makes sense. They need to make sure that links and other stuff other than just the word you see in the text are not broken. If you just care what is displayed then, as he said, no problem. If you need more done it's understandable that more review needs to be done so see what can happen. Honestly, you need to work on your communication skills. You should have asked him for more details, maybe a phone call if his e-mail wasn't clear. You're jumping to "ripped off" is because you don't understand that something that looks simple can have a lot going on in the background. That's understandable, but you should be asking your vendor questions.
THIS IS EXACTLY WHAT I WAS LOOKING FOR. Thank you!!
interesting, I did not know this. Thanks
Thanks! Do I need to do any cleanup after I'm done to purge @ExcList from RAM?
Postgresql and json. Google it, learn how to create SQL to get the info you like, put it on github and when you apply for a job put your github stuff on your CV. MongoDB uses Postgresql for analytics BTW. 
Same advice I give out. Find something you know about and build a database around it, then once done, start asking complex questions of the data. Like what was the ERA for batters in the American League who were born outside of America? Oh shit.. you don't have that data? Time to figure out how to get it. Or if you do have the data, what other questions can you ask? What if I want to see it quarterly? What is the median ERA (do it in SQL)? How about if I want to see percents of totals, pivoted data, etc.? As an analyst Excel is your best friend and your biggest crutch. Your goal is to (usually) write queries which return data in a format that can be easily consumed by Excel (or SSRS, or Tableau) and then used in meetings, PowerPoints, etc. Doing a percent of total is very easy in Excel... but learning to do it in SQL is how you up your skills.
I primarily use MSSQL, but I believe the usage is the same. CASE statements are not like cast in usage. You could write the following: ORDER BY CASE WHEN [column] = 'a1' THEN 1 WHEN [Column] = 'd4' THEN 2 ELSE 3 END This will convert the values to the numbers and use them to order by instead of the values. You can do a lot of interesting logic with CASE statements. I can give better examples when not on mobile if you like. https://www.techonthenet.com/sql_server/functions/case.php
Awesome, thanks so much!
You could use CASE, but I'd be more inclined to join to a ranking table (e.g. ('5A', 1), ('8A', 2) and use those values. The best place to start reading more about a MSSQL function is MS docs https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql
It's hard for us to speculate without at least knowing what CMS you're running and what sort of costs we're talking about. On the one hand, it could be entirely legitimate. On the other, tech illiterate people can be very easy $$$.
Thank you so much for the info!
This. Using a CASE statement to do a sort is the sort of a thing a non-database developer would do. You're better off setting things up so that you just change the data for a different outcome, and not have to touch the code/query you wrote.
Exactly, table valued function or just an actual table you can reference. With an actual table, you can index it as it grows, and put constraints on it so you don't have duplicate data.
No.
Thanks 
Speaking as a SQL Server DBA: please jesus, no. Download the Express Edition (probably 2014 or 2016), and use management studio 
To add to this where possible use the latest, the SSMS clients are backwards compatible. 17.1 is the most current
We had some storage arrays become spare which have come into a mid term plan to replace the storage on that unit. The corporate focus isn't on that platform so any major investment in time or effort is lost considering the rate of pace in developing new platforms. The long term goal is consolidation of that platform elsewhere and the short term plan is to keep it limping along with the constraints we have.
Yeah if you're seeking 100% comparability, but that's not what someone who's trying to learn needs to worry about. 
No. I learned using Access, but it's better to just get SQL Server. Plus, it's easier to set up these days, and there are so many resources online now that it's probably easier to find an answer or example for SQL Server than Access.
Once you've gone beyond baby's first table join and select, Access's SQL gets really weird - lots of missing functionality, its own set of functions that are closer to Excel's than real SQL, weird oddities in syntax, etc etc. If you get too far learning SQL with access you pick up a lot of bad habits to unlearn later.
Hey man. As someone who works in the real world of software where I'm debugging our devs' PL/SQL code (for the world's most used software of its kind), I understand where you're coming from. Though I know you were intentionally trying to being condescending, I commend you on your inability to understand why beginner tools can be useful. Cheers. 
or give me a good resource to optimising queries?
I wouldn't call Access a beginner's tool, but more a platform on which one can build feature rich, data intensive, single user applications. When your aim is to learn 'sec' SQL, I would stick to readily available, (more or less) standards compliant tools, like SQLite, PostgreSQL, MySQL, or SQL Server Express Edition. They are all free to use. 
That would be embarrassingy blowhard even if your super l33t experience was pedagogically relevant. 
It would be useful if you could provide your platform. Formatted code would be nice too. Without either of those all I can suggest is looking into transaction isolation levels for one that is appropriate to your needs. 
Formatted: Select o.Vehicle as [No Unit] , o.Customer , o.CustomerDisplayName as Name , o.Product as Material , P.ProductName as Product , o.Depot as Works , a.Distance , o.CustTravelTime as [Assumed TravelTime] , o.RequiredDateTime as [Required DateTime] , o.TimeWindowOrder as [To Window Provided] , ToWindowDateTime as [Window DateTime] , o.OrderRef as [Ref No] , o.OrderEntryDateTime as [Orderred DateTime] , o.EnteredBy , o.Instructions , o.Warnings , o.Planning , o.AllocationState as Status , o.LoadDocket as Docket , o.LoadingDateTime as [Commenced Loading] , o.IntransitDateTime as [Completed Loading] , o.ArrivedDateTime as [Arrived Customer] , o.ArrivedDateTime as [Commenced Unloading] , o.UnloadedDateTime as [Completed Unloading] , '?' as [DMS] , o.CustomerOrder as CustPO , o.Renegotiated , o.TurnAround , o.StockOut , o.LateDelivery , o.LateOrder , o.StandingOrder , o.NightShift , o.modifier as [Actual Modifier] , t2.Net , t2.Trantype , o.PlannedDateTime , o.VehAssignedDateTime as [Published Date] , t2.DriverName , o.CancelledDateTime from Orders o left outer join Product P on p.Product = o.Product left outer join AuthPlantCust A on a.Customer = o.Customer and a.Plant = o.Plant --left join trans T on O.orderref = t.orderref outer apply ( SELECT Top 1 Docket, Trantype, DriverName, Net FROM Trans t where o.Orderref = t.Orderref AND TranType &lt;&gt; 'SRT' ORDER BY DespatchdateTime Desc ) T2 where o.RequiredDateTime &gt;= (@DespatchFrom) and o.RequiredDateTime &lt; (@DespatchTo) and o.Company = 'C' order by o.RequiredDateTime , o.OrderRef 1. I know nothing about your indexes, but I see 2 parameters: @despatchfrom &amp; @despatchto https://www.brentozar.com/archive/2013/06/the-elephant-and-the-mouse-or-parameter-sniffing-in-sql-server/ https://blogs.technet.microsoft.com/mdegre/2011/11/06/what-is-parameter-sniffing/ 2. Check your tables if you have an indexes with these columns * o.RequiredDateTime (this one should be first*) * o.company (this one should be first* and possibly filtered) * o.Product * o.Customer * o.Plant * a.Plant * a.Customer * p.Product *totally depends on the density 3. Check if a filtered index with company = 'c' is possible or feasible. 4. Don't ORDER BY in SQL it costs time! Sort in your (reporting) software / client side.
Any help as to what exactly what needs to be downloaded? I tried this a couple years ago and it didn't end well. Had a ton of things I wasn't sure if I needed downloaded and never got it to install properly. Any help would be appreciated. 
Same here. Major US bank hired me for a BI role. Got in and the entire division I was working for was supported by Access/VBA. Was totally swindled during the interview. Noped the heck out of that job within 3 weeks. 
You should do your homework yourself. Having said that, there are 2 ways to copy a database. You can either backup and restore or script out the generation of the schema and data.
Agreed 100%. If asking for help, it's always a good idea to show what you've tried so far. Here are some good resources to get you going; [Docs.microsoft.com/sql](https://docs.microsoft.com/sql) [Dba.stackexchange.com](Http://DBA.stackexchange.com) 
I did the backup and restore database one, i actually finish it but when i gave to my teacher for grading he told me that was not the method he wanted us to use so im searching for another and the one mentioned in the post is the only one i've found so far
If you can get an actual execution plan, I'll take a look and let you know for sure. Query optimization very much depends on the environment and not just how the SQL is written. Without any additional information, these are my thoughts. * Without a perfect index, the `OUTER APPLY` could single handedly kill the query. * Remove order bys when troubleshooting performance * You can try `OPTION (OPTIMIZE FOR (@DespatchFrom = '2017-02-01', @DespatchTo = '2017-02-02'))` for whatever a typical date range might be. `OPTION (RECOMPILE)` is another possibility if the query isn't executed frequently.
I'm sorry to say this but it sounds like your teacher is exercising some very poor judgement. Your teacher should be praising a student for coming up with a solution that they were not thinking of but still worked. Unless this assignment is calling back to a specific lesson they taught, backing up and restoring a database via T-SQL is a completely valid answer.
I prefer having separate primary keys and natural or "business" keys (and yes I come from Oracle) and your primary key should (almost) never change. Even with "on update cascade" isn't there the potential for locking and concurrency issues? I look at it from the other side, whats the downside to having a separate primary key? (One more column in your select isn't a real good reason imo). The storage issue of one extra numeric key per table isn't that big, and if you are using something like email address as your key, may actually save space and make your FK indexes more efficient (speculation). I believe searching numeric indexes is faster than text ones, though I don't know much about mysql indexes under the hood.
Oh I feel you, I may or may not have been working for one of the largest hotel chains in the world and calling it a shitshow was a gross understatement. 
As u/AquietMan stated, having a primary key that is not stable, is not the end of the world for a small database, but can have catastrophic issues for very large databases, especially when the system you have does not allow for references to cascade upon an update. You won't find natural keys that are tied to personally identifiable information (email, SSN, First/LastName, Birthdate, etc) in a production environment due to the lack of uniqueness, stability, or for security concerns. Some sort of Member/CustomerID is definitely possible and worth looking into. Some examples of natural keys that would be acceptable: dbo.State StateAbbr | State ---|--- AK | Alaska AZ | Arizona dbo.Zip ZipCode | StateAbbr | ---|---|---- 99501 | AK | 99502 | AK | Your most common key you'll come across is a surrogate key that has no direct business meaning at all. This is used the most because as the database designer, you have minimized security concerns by completely abstracting any meaning to the value. And you have complete control over it's uniqueness and thus its stability. Hope that helps.
This, and [this](https://www.amazon.com/T-SQL-Fundamentals-3rd-Itzik-Ben-Gan/dp/150930200X/ref=sr_1_1?ie=UTF8&amp;qid=1496936546&amp;sr=8-1&amp;keywords=Itzik+Ben-gan).
I would question the use of an email address as a PK, not just because it could change, but it's also an inefficient data type when repeated in other tables as foreign keys.
It works but adds students who have logged in within that period.
To use your example, what if the business decided that a user can now have multiple email addresses for their account? You would probably create an "email" table but now your primary key doesn't make as much sense. Or the business will stop collecting email addr and only use twitter handles. I know those are not a likely scenario in real life, but the point is that using a dedicated primary key helps future proof for structural changes to the database. 
Make it `INNER JOIN ClockPunches B`
* Why real over decimal: Real is 4 bytes and Decimal(5,4) is 5 bytes. I only have 4 significant figures usually and real supports up to 7. * Why DateTime2 over DateTime: DateTime2(2) is 5 bytes and DateTime is 8 bytes, more importantly temporal tables require datetime2 data type. 
&gt; Some examples of natural keys that would be acceptable: why do so few "surrogate keys on every table" understand this
upvote for **leading comma convention**
For starters, your table seems setup backwards. This would be better. ID | Year | Revenue | Margin | Sales ---|---|----|----|---- A | 2011| 111 | 4.0 | 12 A | 2012 | 123 | 3.2 | 57 A | 2013 | 234 | 2.1 | 45 B | 2011| 555 | 5.3 | 43 B | 2012 | 432 | 5.1 | 54 B | 2013 | 634 | 4.3 | 55 Notice how you can `SUM(Revenue)` or `SUM(Sales)`, where before `SUM(2011)` or `SUM(2013)` is meaningless. Also, for the questions you are asking of the data (filter things of the same ID) it feels like everything should be in one table instead of split out in 20-30 different places.
The column set idea is interesting, I can use *for json auto* to convert the xml to json easily while still being able to quickly scaffolded changes from the database into the mvc application; of course, this would require someone to maintain the db and application if changes to the model were desired.
Adding a sparse column to the end of a table is minimal work, since null values don't have any row allocation. I'd only be concerned for the most active tables without maintenance windows. The SQL insert code doesn't need to change if using the column set. Just add a sparse column, and you can insert Xml elements with the new name, just like earlier columns. CREATE TABLE dbo.SparseTest ( ID int NOT NULL IDENTITY(1,1), ColumnSet xml COLUMN_SET FOR ALL_SPARSE_COLUMNS, Sparse1 varchar(50) SPARSE NULL ); GO INSERT INTO dbo.SparseTest (ColumnSet) VALUES ('&lt;Sparse1&gt;Test&lt;/Sparse1&gt;'); SELECT ID, ColumnSet, Sparse1 FROM dbo.SparseTest; GO ALTER TABLE dbo.SparseTest ADD Sparse2 varchar(50) SPARSE NULL; GO INSERT INTO dbo.SparseTest (ColumnSet) VALUES ('&lt;Sparse1&gt;Test&lt;/Sparse1&gt;'),('&lt;Sparse2&gt;Test&lt;/Sparse2&gt;'); SELECT ID, ColumnSet, Sparse1, Sparse2 FROM dbo.SparseTest; There are valid reasons not to do this, but if it's suitable the database footprint will be much better.
Access never ceases to amaze me with its ability to choke on the excess parentheses its own SQL editor adds.
People using access at my company has cut years off my life in trying to troubleshoot and re create some of the apps that are created with that program, horrible.
You're a special kind of idiot. 
I think you'll need to explain where you want to use it a bit further. IFNA returns a value if the argument is #NA, so the closes to this function to my mind is ISNULL or COALESCE, which allow you to replace a null with another value, or return the first non-null argument, respectively. Neither of these having anything to do with searching multiple tables for a value.
My eyes are now literally burning in pain ...
Unfortunately I can write queries but not much else.. I'm thinking of adding no lock, creating a view with indexing.. but ask me to create an execution plan I have no clue Its very obvious my outer apply is what has caused me issues with huge cpu usage, a lock on the primary transaction table takes 45 minutes before it lets go.
Thank you, I have some googling and learning to do!
Based on the below comment, I used an execution plan that advised to use the following: USE [DMS] GO CREATE NONCLUSTERED INDEX [IX_Trans_OrderRef_TranType] ON [dbo].[Trans] ([OrderRef],[TranType]) GO Is this enough or should I create more indexes?
Maybe try and split 3 letter segments into columns first, then unpivot and aggregate.
So for "Johnson", it could be "Joh", "ohn", "hns", etc? Try this: with NAME_LIST as ( --30 names pulled from my ass select 1 id, 'MICHAEL' FIRST_NAME from dual union all select 2 id, 'ROBERT' FIRST_NAME from dual union all select 3 id, 'WILLIAM' FIRST_NAME from dual union all select 4 id, 'RICHARD' FIRST_NAME from dual union all select 5 id, 'JENNIFER' FIRST_NAME from dual union all select 6 id, 'THOMAS' FIRST_NAME from dual union all select 7 id, 'JOSEPH' FIRST_NAME from dual union all select 8 id, 'STEVEN' FIRST_NAME from dual union all select 9 id, 'DANIEL' FIRST_NAME from dual union all select 10 id, 'JEFFREY' FIRST_NAME from dual union all select 11 id, 'CHARLES' FIRST_NAME from dual union all select 12 id, 'STEPHEN' FIRST_NAME from dual union all select 13 id, 'MICHELLE' FIRST_NAME from dual union all select 14 id, 'JESSICA' FIRST_NAME from dual union all select 15 id, 'CHRISTOPHER' FIRST_NAME from dual union all select 16 id, 'ANDREW' FIRST_NAME from dual union all select 17 id, 'MELISSA' FIRST_NAME from dual union all select 18 id, 'AMANDA' FIRST_NAME from dual union all select 29 id, 'RONALD' FIRST_NAME from dual union all select 20 id, 'ELIZABETH' FIRST_NAME from dual union all select 21 id, 'STEPHANIE' FIRST_NAME from dual union all select 22 id, 'MATTHEW' FIRST_NAME from dual union all select 23 id, 'KENNETH' FIRST_NAME from dual union all select 24 id, 'GEORGE' FIRST_NAME from dual union all select 25 id, 'ASHLEY' FIRST_NAME from dual union all select 26 id, 'DONALD' FIRST_NAME from dual union all select 27 id, 'EDWARD' FIRST_NAME from dual union all select 28 id, 'HEATHER' FIRST_NAME from dual union all select 29 id, 'KIMBERLY' FIRST_NAME from dual union all select 30 id, 'GREGORY' FIRST_NAME from dual ), SUB_NAMES as ( --for each row, get a row with substr(name, N, 3) up to N= length(name)-2 --Adapted from this method of parsing out a delimited field into rows: -- https://stackoverflow.com/questions/14328621/splitting-string-into-multiple-rows-in-oracle select ID, FIRST_NAME, substr(FIRST_NAME, levels.COLUMN_VALUE, 3) as SUB_NAME, LEVELS.COLUMN_VALUE as START_INDEX from NAME_LIST, table(cast(multiset(select level from dual connect by level &lt;= length(FIRST_NAME)-2) as sys.OdciNumberList)) levels -- ^^^ wtf is this witchcraft? ) --The above is the hard part. From here, it's just a matter of choosing how to do the outputs. I'll just LISTAGG them as an example. select SUB_NAME --If "BAR" was the most popular substring, BARBARA would appear twice in this list. Do another subquery with distinct if that's not desired. , listagg(FIRST_NAME, ', ') within group (order by id) ALL_NAMES , count(1) OCCURENCES from SUB_NAMES group by SUB_NAME order by count(1) desc fetch first 5 rows only ; SUB_NAME | ALL_NAMES | OCCURENCES | :--- | :--- | :--- | CHA | MICHAEL, RICHARD, CHARLES | 3 | STE | STEVEN, STEPHEN, STEPHANIE | 3 | ICH | MICHAEL, RICHARD, MICHELLE | 3 | EPH | JOSEPH, STEPHEN, STEPHANIE | 3 | ALD | DONALD, RONALD | 2 | 
Do you think It's a good buy? 50$ on Amazon, I'm considering to buy it. It would be the first SQL book I'll have.
This might be exactly what I'm looking for! Thanks!
I might have framed my question poorly. I have a table full of houses and what we charged to do our portion of work on the house. I need to find our cost of goods sold, which could be in one of three different other tables (working on getting all the info on one place but it is on ITs backlog). Where the cost of goods sold comes from depends on if the price is a base price, an add-on, or an odd job. Even worse the key I would use to look each up is different for each cell. For base prices, I have to use a concatenate for the job code and another cell, for add-ons I have to use the job code plus a different valye, and there is a third value where it is denoted that the job is an odd job. 
Thanks! I will look into this function today! Thanks!
it's an advanced book Celko is definitely an acquired taste is there no IT library near you that you can visit?
You would need to use a trigger that runs after each insert and update. The values in the inserted tables would need to be checked against the orignial table for the specific duplicates and rollback the transaction if they are found. That said, I do not suggest going with that approach if you can because triggers slow down writes to the table when checking is not required. A better approach would be to use a stored procedure..
https://www.simple-talk.com/sql/t-sql-programming/the-performance-of-the-t-sql-window-functions/ https://sqlperformance.com/2013/03/t-sql-queries/the-problem-with-window-functions-and-views https://www.simple-talk.com/sql/t-sql-programming/window-functions-in-sql/ All those links have good info. But I still use windows functions, sometimes I prefer easily read code and easier to write code vs performance. It all depends. For a high availability and performing OLTP DB, I'd use whatever makes it run efficiently without errors or consistency problems. For a periodic task that takes 1-4 seconds of computation, I'll just use whatever I want. (Within reason)
this will return the result you are looking for when it comes to the data you provided select id, COUNT(distinct date) from dt group by id but that will not work if in the same date a customer can have 2 codes. If you will have that case than one way to do it would be select id, COUNT(distinct CONCAT(date, code)) from dt group by id there are other ways of doing this like using a CTE to get the distinct values and then do a count(code) 
Worked great. I was thinking backwards. I was trying to count the codes, not the dates. Thanks!
You *could* do it with a unique index on an expression. Not sure about performance though. Example: create table PERSONS ( PERSON_ID number , HOUSEHOLD_ID number , NAME varchar2(30) , HEAD_OF_HH number , constraint PEOPLE_PK primary key (PERSON_ID) , constraint HEAD_OF_HH check (HEAD_OF_HH in (1,0)) ); insert into PERSONS values (1, 1, 'Al' , 1); insert into PERSONS values (2, 1, 'Bob' , 0); insert into PERSONS values (3, 1, 'Carl' , 0); insert into PERSONS values (4, 1, 'Don' , 0); insert into PERSONS values (5, 2, 'Earl' , 1); insert into PERSONS values (6, 2, 'Fred' , 0); insert into PERSONS values (7, 2, 'George', 0); insert into PERSONS values (8, 3, 'Henry' , 1); **PERSONS** PERSON_ID | HOUSEHOLD_ID | NAME | HEAD_OF_HH | :--- | :--- | :--- | :--- | 1 | 1 | Al | 1 | 2 | 1 | Bob | 0 | 3 | 1 | Carl | 0 | 4 | 1 | Don | 0 | 5 | 2 | Earl | 1 | 6 | 2 | Fred | 0 | 7 | 2 | George | 0 | 8 | 3 | Henry | 1 | If you want to enforce that a household has only one HEAD_OF_HH, you can create an index like so: create unique index PERSONS_SINGLEHEAD on PERSONS ( HOUSEHOLD_ID||'^'||decode(HEAD_OF_HH, 1, '1', 'x'||PERSON_ID) ); Then when you try to insert or update to a new HEAD_OF_HH, you'd get an error: insert into PERSONS values (9, 1, 'Iago', 1); --or update PERSONS set HEAD_OF_HH = 1 where PERSON_ID = 2; --both return: ORA-00001: unique constraint (WAREHOUSE.PERSONS_SINGLEHEAD) violated Of course, the "correct" answer is to normalize: **PERSONS** PERSON_ID | HOUSEHOLD_ID | NAME | :--- | :--- | :--- | 1 | 1 | Al | 2 | 1 | Bob | 3 | 1 | Carl | 4 | 1 | Don | 5 | 2 | Earl | 6 | 2 | Fred | 7 | 2 | George | 8 | 3 | Henry | **HOUSEHOLDS** HOUSEHOLD_ID | HEAD_PERSON_ID | :--- | :--- | 1 | 1 | 2 | 5 | 3 | 8 | 
Your example code looks like different databases on the same server. Is it different databases or different servers?
Good point, I added another dot. They're completely separate physical servers. Each server is in a different office location.
Thanks, I'll give this a shot.
Pluralsight has some great ones and you can get three months free with a MS Dev account without CC info. There's a few good resources here: https://www.cathrinewilhelmsen.net/2015/01/28/preparing-for-and-taking-exam-70-461-querying-microsoft-sql-server-2012/
What is this sorcery?
I've also done this several times. You can take it a step further and hold your remote server names in a table, then pull them in to a dataset.
I think I originally picked it up from Adam Machanic's sp_whoisactive.
This is a perfect situation for a datamart. Perhaps you could look into that?
Is that similar to (or part of) a data warehouse? I've thought about doing something like that but I don't know how to get real-time data in them without hogging resources. I've heard SQL server 2016 made that easier, but we're still on 2012 standard ed.
Thanks, I really like this approach!
SQLServerCentral Stairway Series http://www.sqlservercentral.com/stairway/75773/
I think /u/AXISMGT has you covered: Join your job/house table to the three tables where the cost fields live on the three different keys, and in the select statement COALESCE(table1.goodscost, table2.goodscost, table3.goodscost), and that will return the first of those that isn't null.
grouping by id and code gives a different result than grouping just by id
That's fair
RIGHT?! It is absolutely bananas when you find out about things that have been right in front of you. Like the day you were struggling with trying to get the PARTITION BY in a ROW_NUMBER() right and discovered DENSE_RANK() was a thing. Or you can't figure out why dollar amounts in a report weren't adding up and you found out that the MONEY data type doesn't round and just tells precision digits to fuck off. Or that you can skip rows in a result set by using OFFSET in your ORDER BY clause. Or you got a trial of Redgate SQL Toolbox and then realized that there are some things you don't give a flying fuck about cost and would sell your kidney to have a license forever.
CodeSchool.com has a fantastic intro course where you get to try examples after the videos. The more advanced course will cost you $29 a month. 
SQL Server Developer Edition is a better choice than Express nowadays. Otherwise yeah, SQL Server and SSMS is ideal as you just install both and run Management Studio. MySQL or PostgreSQL are great too, but IMO their admin tools/clients are a little less intuitive to a beginner and learning stored procedures can itself make more sense as a process
Thousands of tables? I really, really, hope you're exaggerating. Although if you're not, I kind of admire the persistence of that developer. In my case, we have one with about 100 queries that when you start digging in you realize could be reduced to about 25. But you don't dare change anything because it's damn near impossible to follow the data flow. Seriously. We were trying to track down a problem and three of us spent 4 to 8 hours each digging around and ended up more confused about how it worked at the end then we were at the beginning. I tried to figure out where one field got set and never was able to figure it out. And this is used for a business critical function at a Fortune 25 company. Supposedly it's going away next year. Supposedly.
DevGym.Oracle.com is oracles free academy and comes with your own Oracle sandbox in the cloud. Docs.microsoft.com/sql has all resources you need to get a free sql server instance running on your machine and start coding. https://www.brentozar.com/training/ is also a great resource, and they podcast/webcast is fun to listen to.
What is your question/problem? Are you asking how to make two fields a primary key?
I'm of the opinion that if your project necessitates a real RDBMS you've already moved beyond the Access usage sweet spot. Access is a way for non-technical people to make a very quick ad-hoc internal database with a simple front-end/reporting. That's where it should be used. It's got so much extra stuff beyond being a GUI for running queries on RDBMSes, and performance contraints arising from running queries over ODBC, that it's really not very lightweight at all. It doesn't even do GUI query design amazingly well either, there are better free tools for that. So I mean lets all shit on those millions of times Access gets used where it shouldn't, but it does actually have a small niche that it fills really well, and it's not the niche you're talking about.
That's because you can't have two primary keys. What you want to do is create one primary key made up of two fields. CREATE TABLE empproj (proj_id INT FOREIGN KEY NOT NULL REFERENCES project(proj_id) , emp_id INT FOREIGN KEY NOT NULL REFERENCES employee(emp_id) ,PRIMARY KEY (proj_id, epi_id) );
Thanks!! So is that like PRIMARY KEY CLUSTERED?
But what if there is a cost on two the three tables. Doesn't sound like you want a COALESCE, but rather a SUM() and a GROUP BY?
Sounds like in that scenario, you'd want to determine what the hierarchy is. If found in 2 tables, and I only want the highest (or lowest) one, I'd probably make identical record for them and do a ROW_NUMBER ordering by cost DESC and then use that RN =1 in my where clause.
I may or may not know of a place that has an access database that has a macro that pulls in a fixed width text file, and creates about 50 text files out of it. The macro is supposedly called from a simple C# console application when the text file appears in a certain location. On counsel‚Äôs advice, I invoke my right under the Fifth Amendment not to answer any questions regarding this matter, on the grounds I may incriminate myself.
I learnt on codecademy. Was enough to get me a senior analyst job.
Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. It is kinda what you re looking for. Free, for beginners, well structured, with online exercises.
Thanks for sharing üëç
I found you can do window functions on aggregates the other day. Completely blew my mind! 
You can use a recursive CTE to achieve this quite easily. http://stevestedman.com/2013/06/recursive-cte-for-dates-in-a-year/
You want to create a "blank" dataset to supplement your business data and fill in the gaps. If you have a Calendar table hanging out (I always have a Calendar table within reach) you can use that to create the blank dataset. Otherwise you can create one on the fly. This is my prefered method for doing that. WITH cteDateRange AS ( SELECT MIN(bd.BusinessDate) AS MinDate, MAX(bd.BusinessDate) AS MaxDate, DATEDIFF(DAY,MIN(bd.BusinessDate),MAX(bd.BusinessDate)) AS DayCount FROM dbo.BusinessData AS bd ), cte10 AS ( SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank UNION ALL SELECT NULL AS Blank , cte100 AS ( SELECT NULL AS Blank FROM cte10 AS a CROSS JOIN cte10 AS b ), cteDefaultDates AS ( SELECT DATEADD(DAY,n.N-1,dr.MinDate) AS DefaultDate, CONVERT(money,0) AS DefaultValue FROM cteDateRange AS dr CROSS APPLY (SELECT TOP(dr.DayCount) ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS N FROM cte100 AS a CROSS JOIN cte100 AS b) AS n ), cteUnion AS ( SELECT bd.BusinessDate, bd.BusinessValue FROM dbo.BudinessData AS bd UNION ALL SELECT dd.DefaultDate, dd.DefaultValue FROM cteDefaultDates AS dd ) SELECT u.BusinessDate, SUM(u.BusinessValue) AS BusinessValue FROM cteUnion AS u GROUP BY u.BusinessDate;
I would imagine it probably doesn't even take very large tables to do that if you screw up the join bad enough. 
For sure. They surely might be split line items that belong to a purchase order for all we know.
&gt; I prefer having separate primary keys and natural or "business" keys This *separate primary key* is what Chris Date calls a *surrogate key*. Speaking only for myself, I prefer to science the shit out of database design and maintenance. &gt; Even with "on update cascade" isn't there the potential for locking and concurrency issues? That *potential* is always there. But I've been doing this since the 1980s, and my systems don't have trouble with locking or with concurrency using foreign key references to natural keys. This is a function of a) quality-of-implementation on the part of the dbms vendors, and b) the load on the dbms server or servers. You can *test* this during design and QA. You don't have to guess. &gt; The storage issue of one extra numeric key per table isn't that big But that's not the whole story. Depending on the dbms, that key might be added to *every* index on that table. (SQL Server does that for clustered primary keys.) That makes every index wider and slower to maintain. Most people only think about how an index might speed up SELECT statements. On big databases, indexes slow down inserts, updates, and deletes. (Because the index has to be changed along with the data.) DBAs like myself are paid to consider the big picture, not just SELECT statements. (Or, more commonly, not just *one* SELECT statement.) One downside is that using surrogate keys *always* require a join to get the data you want. But you'd be surprised, probably, to find out how often the data you want is a candidate key. A natural candidate key doesn't require a join. I've worked on systems that had queries involving several dozen joins; sensible use of natural keys reduced most of then to less than 10; all of them to less than 20. They were faster, too. &gt; I believe searching numeric indexes is faster than text ones, though I don't know much about mysql indexes under the hood. It usually depends on the width of the column. You can measure it. I think you'd be surprised at a) how fast indexed searches are nowadays on text columns, and b) how little difference there is for "slower" text searches. (*Slower than integer* and *too slow for my users* are two utterly different things.)
Your results should include occupied and unoccupied rooms.
You could use global temporary tables. Populate the temp table for each list. This is actually what Oracle recommends as well to avoid hitting the 1000 element limit for in lists.
Honestly it was REALLY hard to follow what you were asking here. can you provide some examples.
Why not dump it all to one file then use split to split the file into 100 line chunks?
Glad I could help!
Long dynamic lists for IN (...) is a bad idea, cause you can potentially get ORA-01795. Use global temporary tables populated with values.
Several queries combined with union?
I would love to but realistically the data set I'm dealing with is far too big to make every case mutually exclusive 
Nested case statements? Not pretty though.
This is all the information I've been given.
&gt; Let's say a specific case has 2 passengers, then it is part of several groups (e.g. &lt;3 &lt;4 &lt;5 etc) tackling this from the "mutually exclusive" angle is wrong -- you'll end up with a shitload of code instead, if you want to report a single case in multiple groups, you need to create a groups table CREATE TABLE groups ( name VARCHAR(13) PRIMARY KEY , lo INTEGER , hi VARCHAR(9) ); INSERT INTO groups VALUES ('passengers &lt; 7' , 1 , 6 ) ,('passengers &lt; 6' , 1 , 5 ) ,('passengers &lt; 5' , 1 , 4 ) ,('passengers &lt; 4' , 1 , 3 ) ,('passengers &lt; 3' , 1 , 2 ) ,('passengers &lt; 2' , 1 , 1 ) ; then join your table like this -- FROM yourtable INNER JOIN groups ON yourtable.passengers BETWEN groups.lo AND groups.hi vwalah, the case with 2 passengers belongs to 5 of the above groups 
And your work expects you to build queries based on an incomplete schema and no access to the tables? That seems ridiculous to me.
This is a pretty simple usage of Group By. select quote, rep, date from table group by quote, rep, date
Our date records are stored to the individual second. The previous process was to just copy all of it to excel, separate the date into date and time columns, delete the time column, then remove dupes. Can this be done in some one-step fashion?
Cast your datetime as a date then group.
Not an idiot. There's a ton to learn when it comes to SQL. Can you post your code once it's finished?
okay, that's pretty messed up i'll walk you through a rewrite can you please give me the names of all the tables that should be involved in this query
Keep in mind that what you're actually doing is learning new ways to think through data relationships and then translate them to code syntax to get the job done, so that's not always an effortless process. Keep at it.
Op ignore this comment
Op ignore this comment
I did it. Got my MCP in 2000. Proceeded to work retail at best buy, and circuit city. Got a decent gig with a hospital as an IT guy, general computer everything. Quit that for reasons, and went to work at a call center. Moved up the chain in a year, and ended up in my current role. Doing data analytics, DBA, etc. My world is DBs now. Get in somewhere on the ground floor and try to do anything and everything you can to work your way up. Call centers suck, but you can usually move your way into some kind of analytical role in a company that has call centers. Telecom, and insurance are big ones. MICROSOFT CERT WAS EXPENSIVE AND DID NOTHING FOR ME. Caps sorry.
Okay. This is going to be a weird one, and there may be some bugs with less convenient datasets. I'll try to explain what's going on. 1. For each main record, we select the best candidate filler record that could precede or follow the main record. 2. We override some dates, since the filler records can't overlap the main record, and setup some sorting so everything gets in the right sequence. 3. We split out the main, preceding, and following records using CROSS APPLY + UNION ALL. It works just like UNPIVOT, but you can do multiple columns at once. 4. We do a ROW_NUMBER() - ROW_NUMBER() trick to identify cases where there are multiples of the same record in a row. 5. We aggregate to collapse these duplicated rows, and pick the correct narrowed time spans. --- SELECT c.ID, c.[Priority], MAX(c.StartTime) AS StartTime, MIN(c.EndTime) AS EndTime FROM (SELECT x.ID, x.[Priority], x.StartTime, x.EndTime, ROW_NUMBER() OVER (ORDER BY x.SortTime, x.EndTime, x.StartTime) - ROW_NUMBER() OVER (PARTITION BY x.[Priority] ORDER BY x.SortTime, x.EndTime, x.StartTime) AS [Sequence] FROM dbo.Times AS t OUTER APPLY (SELECT TOP (1) t2.* FROM dbo.Times AS t2 WHERE t.[Priority] &lt;= t2.[Priority] AND t.StartTime &gt; t2.StartTime AND t.StartTime &lt; t2.EndTime ORDER BY t2.[Priority]) AS s OUTER APPLY (SELECT TOP (1) t3.* FROM dbo.Times AS t3 WHERE t.[Priority] &lt;= t3.[Priority] AND t.StartTime &gt; t3.StartTime AND t.StartTime &lt; t3.EndTime ORDER BY t3.[Priority]) AS e CROSS APPLY (SELECT t.ID, t.[Priority], t.StartTime, t.EndTime, t.EndTime AS SortTime UNION ALL SELECT s.ID, s.[Priority], s.StartTime, t.StartTime AS EndTime, t.StartTime AS SortTime WHERE s.ID IS NOT NULL UNION ALL SELECT e.ID, e.[Priority], t.EndTime AS StartTime, e.EndTime, t.EndTime AS SortTime WHERE e.ID IS NOT NULL) AS x) AS c GROUP BY c.[Sequence], c.[Priority], c.ID HAVING MAX(c.StartTime) &lt; MIN(c.EndTime) ORDER BY MAX(c.StartTime);
&gt; Also your joins a super weird, you're mixing two styles (Listing your tables broken apart by commas, and the INNER JOIN style) and I'm not sure if or how that would work. You should have each table after the first prefixed by how you want to join it. Don't list your tables with a comma. It's the old inefficient way of doing things. Do: INNER JOIN Table AS a ON a.Field = o.Field OR INNER JOIN Table ON Table.Field = AnotherTable.Field The top one I see more regularly. The word "AS" is optional (in SQL Server at least).
Notepad, with Ctrl/H? :) Ok, maybe this? I've never used it and can't recommend it, it was just the first Google search result for "Sybase to MSSQL SQL converter" -&gt; http://www.sqlines.com/sybase-to-sql-server Give it a free, quick, simple, online test here -&gt; http://www.sqlines.com/online
Sorry to post once again, but in ELI5 terms, could you briefly explain how you use analytics and what kind of SQL (if you use if at all for your position) you utilize in your position? Because I have no work experience, despite having the degree I'm honestly a little fuzzy on what kind of analytics goes down in a real world setting.
You can download MS SQL Server Express for free. The reason I suggest a datamart as a project is 1. It is relevant to analyst roles and 2. it will thoroughly excersize your SQL skills. Find some interesting data (https://www.kaggle.com/datasets has some good stuff), and turn it into a star schema with Fact and Dimension tables. [Kimball](http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/) is considered an authority on the subject, but honestly [wikipedia](https://en.wikipedia.org/wiki/Star_schema) is enough to get you started. You can then attach it to Excel as a pivot table, and answer a variety of questions about the data. A good portfolio piece for an analyst might be screen shot of Excel, showing a interesting result, with the dimensions and measure displayed on the side.
Since you didn't specify a database system I'm going to go with something that works in MS SQL Server (it's not fast and it's not pretty but it returns a result): SELECT T.NAME AS [TABLE NAME] , C.NAME AS [COLUMN NAME] , ( SELECT COUNT(*) FROM INFORMATION_SCHEMA.ROUTINES WHERE ROUTINE_DEFINITION LIKE '%' + T.NAME + '%' AND ROUTINE_DEFINITION LIKE '%' + C.NAME + '%' AND ROUTINE_TYPE='PROCEDURE') FROM SYS.OBJECTS AS T JOIN SYS.COLUMNS AS C ON T.OBJECT_ID=C.OBJECT_ID JOIN SYS.TYPES AS P ON C.SYSTEM_TYPE_ID=P.SYSTEM_TYPE_ID WHERE T.TYPE_DESC='USER_TABLE';
Thank you I have just got back to the office this morning and this produces the correct results. I did in my first iteration use the same general method with a recursive CTE to generate all the minutes and then assign them to each of the time periods. Unfortunately when plugging this method into the rest of my code I end up with some results that wouldn't scale too well :( The process this will be driving at peak times is called around 10k times per minute and unfortunately the results cant be cached as it is dependant upon the caller Run Number | Duration | CPU | Read | Write ---------------|------------|-------|-------|------- 1 | 237 | 235 | 120571 | 0 2 | 236 | 219 | 124723 | 0 3 | 236 | 234 | 120565 | 0
The above was cobbled together from existing queries I have used in the past to list all fields and tables and to find procedures that use a text string. The caveat exists where the above query will give you a count how many times the table name and field name are used in stored procedures (not a count of how much times that field on that table are used, that is a far more difficult question to answer due to aliasing).
The course is kind of useless if you do not want to be a DBA. An MCSE certification is kind of useless if you end up going to a site with another DB like Oracle. As others say, learn basic selects, learn joins, CTEs and CASE/WHENs. If you are modifying, learn transactions. the typical interview questions revolve around joins and such. The advantage of this approach is that it is mostly portable across databases with some minor gotchas. However, it is easier to start by learning one relational database well and then going to other dialects.
okay, here ya go -- FROM Customer AS c INNER JOIN Invoice AS i ON i.CustID = c.CustID INNER JOIN InvoiceLineItem AS l ON l.DonutOrderID = i.DonutOrderID INNER JOIN Donut AS d ON d.DonutID = l.DonutID 
thanks for the update ;o)
I like your solution much better. I think it takes to long as an interview question (I spent a couple hours also), but maybe if it was given to applicants a day or two before the interview. It is a thought provoking task.
You are not replying to deny, you are replying to your own post. New to reddit? Just hit the reply link underneath the comment you are responding to. The database you are using is important. For example Oracle can automatically track column usage and provide reports on it. You can also capture the actual queries that are executed to analyze them.
I thought about resolving it at less than every 1 minute, the issue however is the general system configuration allows this to be set at any given minute in a day and there are already values present which do not line up with any more rational boundaries and could be changed at any time. Having looked through the data I was unable to find a scenario where there was more than 9 boundaries involved in the calculation. (As an idea this calculation is being used for working out how many minutes an item will be at a certain price for e.g. off peak and peak). In the query the outer apply is causing there to be a select from vals once for each row in times and so most of the reads are being caused by this. My code had the vals being calculated in a CTE which meant the CTE was calculated 1440 times hence the very high read values. Placing the results of vals into a temp table reduces the reads to 17616 and the duration to 87ms. With the current outer apply I am not sure that it would be possible to tune this much further. I do however very much appreciate the feedback and the different ways of solving the same problem.
I've worked with Sybase but more heavily with SQL Server. You guys are going to need a lot of development and administration help with the conversion and there's going to be a lot of manual lifting.
Or you were too close to the picture to see it all. A lot of times I think / get stuck on SQL and I have to back away, take a day or two break, then look at the problem again. When I see it, I usually get a solution immediately when before I'd bash my head against it for hours.
I will look into this, thank you very much!
A ton of great information here, thank you! After doing some research, I think I am having some difficulty with starting this project. I feel like I am lacking the direction and an objective (it feels to open ended in a sense). If you were to be doing this project, what sort of questions would you be asking yourself about the data? What kind of objective or direction would you set for yourself to get started with it?
Thank you for your reply! Yes, it seems like my best option right now is to find a ground/entry level position and work my way up. Everyone seems to be stating that relevant experience will always be more valuable than any certification. I think that I need to shift gears in job searching and rather than start right off as an analyst, maybe I need to look into data entry positions, and work my way up. Thank you!
Methinks OP is asking for a script/program to extract the referenced tables and/or fields of arbitrary sql statements in a number of text files, and thus any metadata provided by a database are not that much helpful. Unfortunately, no such program is known to me, but I'd suggest to use a sql formatter to normalize the scripts and then a good text editor with macro/scripting and /or a spreadsheet, apply some logic and elbow grease. Also the output from EXPLAIN PLAN might come handy.
That is what everyone seems to be saying in this thread! Unfortunately, I have no work experience right now, so its kind of a double negative whammy to my credentials :( . However the consensus is that I should try to teach SQL to myself and be familiar with it rather than get a certification and be generally more competent in the software.
You need an additional join to the role table, something like this: SELECT a.per_FirstName AS Nome, pc.c1 AS Profissao, a.per_cls_ID AS Membro, r.MemberRole FROM person_per AS a LEFT JOIN person_custom pc ON a.per_id = pc.per_ID LEFT JOIN list_lst AS r ON r.per_cls_ID = a.per_cls_ID ORDER BY pc.c1, a.per_FirstName;
Could always use the queries in the scripts to create views or more simply one big package with lots of cursors, and then look at dependencies. The dependencies would tell you which objects they rely on. Now there is both table and column level dependency tracking in Oracle.
I'll just sneak right in and put [something similar for the desktop version of Excel here](https://querystorm.com).
#1 Go to SQL User Group meetings and introduce yourself around #2 Get the certs #3 Go to SQL PASS if you can afford it
Yeah, I got to thinking and looked back at my code and made some more changes, which were significant enough that I figured I'd delete the comment and post a new one so that you'd notice it, if you'd seen it already. It sounds like that's the best you can do for now. Do tell if you end up finding something that works better- I'd be interested to know what that is. Good luck, and cheers! :)
Since you've put in a left join to person_custom I'm making the join to list_lst a left join as well. If you've *always* got a row in list_lst for every row of person you can change it to an inner join. I also made up field names for those you didn't provide so you will need to update them appropriately. Most everyone else who's answered has given a similar answer so I'm just adding some explanation to see if that helps give you some "why" as well as "what". SELECT Pessoa.per_FirstName AS Nome , pc.c1 as Profissao , Pessoa.per_cls_ID as Membro , Funcao.role_name -- Guessing the field name since you didn't give it. FROM person_per AS Pessoa -- if you're going to alias, make them meaningful LEFT JOIN person_custom pc ON Pessoa.per_id = pc.per_ID LEFT JOIN list_lst Funcao ON Pessoa.per_cls_ID = Funcao.role_id -- guessing role field name ORDER Funcao BY pc.c1 , Pessoa.per_FirstName I left the person class id column in and added the name, so you can remove that if everything matches up properly. You can join as many tables as you like in a single statement, so if you can join two tables you can join many more just following the same process. 
i have a similiar problem now. I need to list all people and their maritial status. I have the same person_per table, that has a per_ID as PK. a table called person_properties has columns per_ID and mariage (int between 1 and 4) a table called maritial_status with columns 'magiage_int' and a string for each status. I want to print NAME , STATUS 
THANK YOU! i have a similiar problem now. I need to list all people and their maritial status. I have the same person_per table, that has a per_ID as PK. a table called person_properties has columns per_ID and mariage (int between 1 and 4) a table called maritial_status with columns 'magiage_int' and a string for each status. I want to print NAME , STATUS 
&gt; THANK YOU! &gt; i have a similiar problem now. I need to list all people and their maritial status. I have the same person_per table, that has a per_ID as PK. a table called person_properties has columns per_ID and mariage (int between 1 and 4) a table called maritial_status with columns 'magiage_int' and a string for each status. I want to print NAME , STATUS 
[`IDENTITY`](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql-identity-property)
Hi - I don't think you should avoid UNION. I wrote the article as a puzzle, to get people thinking about SQL and that there is more than one way to get the the same result. :)
To append to this. If your tmpTable had an identity column that was an incremental value automatically, you can just add that number vs rownumber. To make it unique indefinitely, load your users to a staging table, then delete them out afterwards re-using the table next time. It will continually increment for you. You can also use another table to keep track of where that unique increment last left off instead if you prefer to truncate or drop the table afterwards. Then your proc would reseed the last value logged in the table and update the table afterwards. Example. Joe smoe gets imported, his ID is 7. End of proc. Proc updates another table and says, last ID used is 7. Next day, Jane doe needs to be imported. Table is either re-created or it was just truncated yesterday. Your proc looks up the table it wrote the number 7 to yesterday, then it reseeds that table Jane is about to be imported to, so it starts at 8 instead of 1. Then she is imported, her ID is 8, then the proc writes to the table to say, hey, the last ID was 8. 
Getdate()-1 is shorthand for the full notation. It's fine for one-off stuff, but definitely not what you want to have to get reviewed by a DBA. 
If you enjoy Healthcare related stuff, then go for it, otherwise pick something you find interesting (so interesting you obsessively want to work with it). Pivots, they are pretty straight forwards /r/excel has load of people willing to lend a hand if you get overwhelmed.
Let me get this straight, you have 1 NULL per SLA with a number as some sort of default value? That's odd. select z1.Client, z1.slatype, isnull(z2.[MeasureQuantity], z3.[MeasureQuantity]) FROM [SLA_Dashboard] Z1 LEFT JOIN dbo.[SLA_GoalByClient] Z2 ON (Z2.SLAType = 'XYZ' AND Z2.Client = Z1.Client) inner join (select slatype, [MeasureQuantity] from SLA_GoalByClient where client is null) z3 on z3.slatype= z1.slatype This query will fuck right up when you have a second null for that same SLAType though.
,Row_Number() Over (Partition By Employee, Location Order by NewID()) as RowNum Use the above as a column in your query, then return the results where RowNum = 1. 
I figured it out, thank you. I was unfamiliar with the quotename function. I took what you put and instead of the number I put double quotes inside single quotes and it wrapped the whole column in double quotes. When I had it automatically save as a csv and I opened it everything was in one column. Thank you again. Since getting my company to change the data in the database would have probably been a lot harder to convince to do since they are company names.
Do you know off the top of your head if it's possible to configure a table in such a way that it will only allow 1 row where Client = NULL per distinct SLAType? EDIT: Just going to define NULL as 'Default', change the SPROC and then create a PK on Client + SLAType and be done with this bastard.
This is perfect thanks!
No problem. Yea, I get why you're doing it. It just seems error prone to me if someone would ever enter another NULL row. My query will just scan the second table again once (not rbar), shouldn't impact performance that much. If it does you could create a filtered index on SLA_GoalByClient with only the client ASC and include the measurequantity column where client is null. An other option would be creating a isnull(measurequantity, case when slatype = x then 400 ect. But I wont recommend putting hard literals in your query for a little performance gain and extra administration.
Good shout with the window function, much simpler than my roundabout step by step way. I hate that Reddit doesn't notify you of new posts while your writing a reply, 15mins wasted for someone to best my answer before I finished typing it ;).
Yes, I see your point. I edited my other response. I think I'm going to define NULL and then generate a PK based on Client + SLAType so that I can never have more than 2 combinations in the same table.
I feel like I'm doing your homework for you.. can you take my last answer and use that as a template to see if you can build the answer, then I'll try and point you the right direction if you stray?
It wasn't working for me. The whole column was coming up as null Edit: never mind haha it works. Not sure what I did before.
And else you can use SELECT * ,COUNT(LogMessage) over (partition by Client, Logmessage) Count FROM test1.dbo.logs_test (haunting mazor with windows functions)
The really annoying thing is I use the buggers all the time, I just never think to use them as the FIRST thing.
i work for a large telecom, most people would call it the empire, as the deathstar is the logo. as far as hints and tips to getting a job... you have no experience, and no one cares about certs. The masters degree will get you far though. Especially if you want to work at a large corporation, which is honestly the best place for you if you are interested in analytics work, or data warehousing, etc. As i said earlier, i started as a call taking rep at a call center, that was 13 years ago, and back then our logo was an orange dude. I worked on the phones as a rep for about 9 months, i volunteered for anything they would throw at me. From going through returned mail, to training new hires during one of the mergers. after 9 months i ended up training new hires. When that was done i went on to be a quality analyst (listening and scoring calls taken by reps). at about 2.5 years into my job there i got REALLY lucky and landed a reporting position, the position was remote, so i could work at the call center where i was located, and report to someone on the east coast. I had gotten a little excel and sharepoint under my belt while i was working as a quality analyst, this is what set me apart from the rest of the applicants. I was able to take a long list and scrub a random set of data from the list (using access). While i was working as a quality analyst i took it upon myself to build out trackers for the team so we would know who to observe next, and of course track scores and calibration sessions. Anyway, move onto the next position, i was a reporting analyst, just building reports on different things like sales data, customer satisfaction data, different call center metrics, compensation reports, etc. etc. the good thing about the call center environment for a database person is that there is an ungodly amount of data to track and report on. this of course led me to start using a DBMS (database management system), which is MS SQL. Excel just didnt cut it with the amount of data i needed to report on things. So now im in the same position 11 years later, but i have obviously grown, gotten promoted, etc., but im still doing the same stuff overall. speaking of which, i have no college degree, i only have my MCP cert, which did absolutely nothing in the way of getting me a job. i have some college, but i never finished as i put my wife through school. Anyway. I have hired people to do what i do. I have worked with college grads, people with master's degrees and just highschool grads. To me, experience is what counts, a portfolio maybe, something that tells me i can let you loose and you will be able to navigate the datawarehouses, build ETL processes to move the data around, and be useful in the role. what do i do as an analyst? access data warehouses and query tables to get data that makes sense to the task i need to accomplish. Many datawarehouses have tons and tons of metadata, being able to find the data, read it, and query it to get results for a report is the most useful skill you can have. We have MANY datawarehouses, and i use some flavor of SQL for all of them, MS SQL, teradata, Oracle, Vertica, ASTER. there is PostgreSQL like ASTER which is basically the same as SQL but with more analytical functions built in. if you can do a select, with joins, and aggregations, and some minor functions then you should be good to step into an analytical role. I also do ETL (extract, transform, load), or data movement. I have my own server, so in some cases i need to pull in a result set from one warehouse and another from another warehouse, then ill pull it into my server and run whatever i need to run to make a report or an aggregate dataset. the movement and scheduling of that movement is called ETL. there are many tools and schedulers around. For MS SQL i use SSIS (sql server integration services) to build my instruction on what to pull from where, and MS SQL agent to schedule the runtime of those packages. Linux has a scheduler called cron, and there is also open source software called pentahoe that does the same stuff. i also act as a data architect. meaning that i will design tables and schema that work best for whatever project i am working on. Build and design a database with a star schema. using PKs and FKs, fact and dimension tables, etc. Another job i do is admin a set of servers that do text analytics. i use many of the skills i have already spoken about to move data to this tool we manage, as well as managing the physical server software. the servers are maintained by IT, and are located in a data center. and then there is the task of being the SME (subject matter expert) on whatever project you are working on. I am the data SME for a survey project we do. After retail visits and calls to customer care, we send out text message surveys. I take all of the survey data as it comes back and move it, aggregate it, give it meaning, pass it through the text analytics tool for the whole of the company. we get about 1 million survey responses per month. Those responses are used to build metrics to measure EVERYONE and EVERYTHING. we use them to pay out incentives to reps and we also use the text analytics tool to find issues and make suggestions to leadership as to what can or should be changes in our processes to make customer experience better. hope that helps. i know it isnt a ELI5, but that is the gist of it. i always called myself a data pimp. if you have a lot of data and cant move it around and manipulate it then call me, that is what i do. the reason i suggested telco or insurance, is because there is an easy barrier to cross to get yourself in the door. they both have call centers, and they both measure literally EVERYTHING that is done by the rep for the customer during a call or visit, if you have the gumption, you can work a shitty call center job for a year or so and start to branch out into analytics or reporting. 
There was a recent thread on that subject: https://www.reddit.com/r/SQL/comments/6gb2eh/can_someone_suggest_a_good_source_to_learn_sql/ [And I'd also add SQLZoo](https://sqlzoo.net/) to those suggestions too. Good luck, and don't be discouraged: just a little bit of SQL is all that you should need to round off what should be a really high-demand skill set.
sorry my friend. it is not actually homework. I am trying to adapt an opensource CRM for my church, and these are some reports that they asked me to get. can you help me with this one? 
In Excel, the result of a query can be placed into a table - and a table is different from your typical rows and columns, because it's a specific named range that's tied back to something - like your query. The rest is just Excel macros and VBA.
I went with rewriting things and going with a value of Default instead of NULL just to make it more intuitive to any poor bastard who inherits this growing monster. I'm still debating whether to change from using an OR to a third join or not and then using an ISNULL() in the select, not so much for performance purposes but again to make it more intuitive. Maybe I'll put it on my future to do list. 
I'll look into some Youtube tutorials and refer to the fine folks over there if need be as well. Thank you!
How does that JOIN without specifying that you're joining, or giving it any conditions?
It was just the very basics of search querying, but even then I found that I was able to pick up the "language" quite quickly and it just made sense in a very logical manner. I hope that continues to stick.
I didn't know that was a thing.... Time to hit google. 
Good thing to learn. You can just dump a stored procedure into a table (INSERT INTO) which runs at whatever interval you want, and then either have the SP_ send it by email when it runs, connect the Excel workbook directly to the table, or use a program like SSRS to make it look pretty and deliver it by Excel or CSV format by email. Using SSRS (free) you could also give your users a link to go to and some parameters for them to pick, so that they can only interact with a dashboard to pull only the data they want to see. I would recommend writing any calculations you need right in the SQL and not doing them in Excel, SSRS, etc. 
Is that identical to '"' + REPLACE(fieldname,'"','""') + '"' or CHAR(34) + REPLACE(fieldname,CHAR(34),CHAR(34) + CHAR(34)) + CHAR(34) ?
Yes but, I think quotename might have a limit on characters returned, scratch that it does varchar(128) or nvarchar(256)... Well that sucks, Im surprised I've never hit the limit. [1] I've been using SSIS for most of my exports, so I guess I shouldn't be too surprised I've not bumped into that limit before.
You might need to look at maunaloona's way of doing it if your strings are &gt; varchar(128) or nvarchar(256). Sorry it totally slipped my mind last night that there was some stupid limit on that function.
I prefer COUNT(*) for readibility but it's a matter of style.
Yes you are right on the dot, I used a lot of statistics and the software that I used was JMP, which is the mac version of SAS. My program also dealt a lot more with getting students up to snuff with healthcare terminology and the industry. It helped me out a lot because my undergrad degree was in Business, and I knew nothing about the healthcare industry before hand. Pivot tables are still something I need to learn as I only had 1 instance of exposure of it during my curriculum, but it is nice to hear that it is easier to learn!
 REPLACE(fname, '''John''', '"John"') Or more generally REPLACE(fname, '''', '"')
Pretty sure the field of mathematics will be transformed in the next 5-10 years through machine learning in the same way the job of a computer (person who computes) was taken over by computers in the middle of 20th century. If computers can beat the world champion at Go, mathematics won't be far behind. Besides, getting a degree just so you can teach at university seems dubious as those teaching positions are very limited. If for some reason you can't get a teaching job, what's your Plan B?
In your opinion what career choice would be best?
&gt;Do you like your job? Yes, very much so. For me it's a good balance between dealing with real world business problems / issues and working with systems. &gt;Is it stressful? In general, no. Where it is stressful it's very rarely the work (directly), it's much more likely to be either something inherent in the work environment (e.g. not enough resources, poor work culture, etc) or people related (i.e. someone, often someone rather senior, is being a prick). SQL in general and MS SQL in particular is pretty damn reliable. Things tend not to go wrong if they're configured properly (obviously not counting things like total hardware failure) and if you're regularly being called at 2am then something is wrong, either with the way your tech is setup or with the way you're resourced (this is simplistic I realise). But the answer to both your questions really depends on where you're working (and with who). The key thing here is : Do you enjoy working with databases / database systems? Forget about careers and economic viability and all that jazz - fundamentally, minute by minute are you going to actually enjoy the work you're doing? If not then find something else. 
Code Academy has a basic course available for free. I believe it would only take a couple of hours to go through it. I don't think you need any prior computer science knowledge to learn SQL but a good understanding of boolean math will help. Examples: TRUE AND FALSE = FALSE TRUE OR FALSE = TRUE
I started with [Head First SQL](https://www.amazon.com/gp/aw/d/B006QNDJZI/ref=mp_s_a_1_1?ie=UTF8&amp;qid=1497468172&amp;sr=8-1&amp;pi=AC_SX236_SY340_FMwebp_QL65&amp;keywords=head+first+sql) several years ago. I thought it was pretty good at the time, but I haven't revisited it now that I'm more familiar with SQL. I did the codecedemy course a year or so ago to see if it was something I'd recommend to my coworkers, and I wasn't all that impressed by it. Regardless of the material you start with, be sure to set up (or get access to) a server with data. You'll learn much faster by practicing and seeing result sets (...and error messages).
no harm in being new, and also it's good to try to help others but in this case, you're just wrong IN is used for a list of items, where the number of items in the list must be greater than zero so `WHERE fname NOT IN ('fred')` is perfectly okay
for **databases that support standard sql**, use double pipes FTFY the plus sign is evil proprietary microsoft sql syntax 
It seemed magical the first time I learned it was possible to do it that way. I know my query would produce the results he was looking for.
Truthfully, most DBA's don't even have bullholes anymore. We just have a schedule job that runs: TRUNCATE body.colon;
SQL injection occurs when you are doing something like a string concatenation to generate SQL queries based on a user input. Take this simplified example. Say you have a field (Variable1) that is used to update the FirstName field in the Users table. `UPDATE Users SET FirstName = " + Variable1 + "` But if you enter some SQL code as the input (Variable1) from your site, such as: `''; UPDATE Users SET EmailAddress = 'h4x3d@Gmail.com'` The result is: `UPDATE Users SET FirstName = ''; UPDATE Users SET EmailAddress = 'h4x3d@Gmail.com'` Now you have injected your own code into the website and made a change that was unintended through the UI. Of course you will need to replace the code with your actual field and table names but hopefully this gives you an idea. Here's a good site to use for practices to prevent injection: http://bobby-tables.com/php
Oh god yes. Otherwise it's not even worth making a proc. Good point. That's why I said "takes in" implying Params.
Here is a serious question. Do you enjoy writing and reading SQL, making backups and restores of databases, reading execution plans, making tables and scripts and functions, tuning settings, partitioning or any combination of these things? If so, SQL career is for you. If not, you won't like it. SQL often involves a decent amount of math so keep that in mind as well.
This site is really good: https://sqlzoo.net/ Check it out. Some really basic queries with a simple, interactive GUI and then quizzes. 
That isn't feasible. I need each SLA to have (1) row per client, and then (1) row for default or null. 
And that's bad design because it forces you to needlessly join twice to the same table. If you have the 10 million rows you mentioned, that's a pretty big hit to performance.
But it doesn't need to be joined to twice if I use the OR statement. At the moment the meta process is coming in at 15 seconds, so I'm not terribly worried about it. Each incremental sub-process I write might add a few seconds in execution time, but the process only will run once a month, so I would much rather save space than time.
I don't think you understand what I'm saying. Your way doesn't save space. There are no upsides to the way you're doing it.
You can't be serious, or I am completely misunderstanding you. My front end table looks like this: | Client | DateRange | SLAType | Country | Region | Category | Numerator | Denominator | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | ABC | 2017-01-01 | XYZ | USA | AMER | 123 | 5 | 10 | | ABC | 2017-02-01 | XYZ | USA | AMER | 123 | 2 | 6 | | ABC | 2017-03-01 | XYZ | USA | AMER | 123 | 7 | 9 | | ABC | 2017-04-01 | XYZ | USA | AMER | 123 | 11 | 13 | | ABC | 2017-05-01 | XYZ | USA | AMER | 123 | 1 | 7 | You're proposing that I add an additional column to this which has the specific measure values for XYZ, XYZ1, etc.?
Hmmm... now I'm seeing what you're saying. Keep the defaults in a specific table, and the specific measures in a separate table, and then in addition to simply joining twice to the same table revising the code to joining to two individual tables and using an ISNULL() or COALESCE() to mitigate the join conditions. That does sound sexy, but I can't imagine the performance is any different than two joins to the same table, and keeping the schema simplified with one table as opposed to two. 
Hmm. What about this? SELECT a.Invoice_ID ,a.Docket_ID ,b.Invoice_ID FROM dbo.tbl a LEFT OUTER JOIN dbo.tbl b ON a.Docket_ID = b.Docket_ID AND a.Invoice_ID &lt;&gt; b.Invoice_ID WHERE b.Invoice_ID IS NOT NULL I don't have a great deal of experience with self joins, but I think the logic is sound.
 SELECT docket_ID FROM yourtable GROUP BY docket_ID HAVING count(invoice_ID) &gt; 1
I figure I could write something like this: SELECT PROJECT_ID, (COLUMN_2 * ( SELECT TASK_PERCENTAGE FROM TASK_DEFINITION WHERE TASK_ID='1' AND FIELD_NAME='COLUMN_2')) AS COLUMN2 FROM TASK_1 WHERE PROJECT_ID='5654' but that is just plain ugly considering I have tasks with eleven subtasks :(
I used this: SELECT Invoice_ID, Docket_ID, Invoice_Date, Docket_Date, Plant_ID, Text, Invoice_Amount, FROM Truck_Invoice WHERE Docket_ID IN ( SELECT Docket_ID FROM Truck_Invoice GROUP BY Docket_ID HAVING COUNT(*) &gt; 1 ) The issue I now face is that Docket_ID has multiple entries on an Invoice_ID due to multiple Items on the docket. So now I am hoping to find a way that I can run an extra line that could produce a result that would only produce a result of Docket_IDs where they match multiple unique Invoice_IDs. 
&amp; is an illegal character in XML Check this thread: https://stackoverflow.com/questions/730133/invalid-characters-in-xml and this one https://stackoverflow.com/questions/1328538/how-do-i-escape-ampersands-in-xml-so-they-are-rendered-as-entities-in-html
and i have no hesitation calling those proprietary functions evil too don't be so fragile 
I think he's implying that the field of mathematics will be mostly automated. Database Administration is a career that shows no signs of slowing. In fact, we automate processes. So theoretically you could be writing code to replace mathematicians in a few years. (I am writing code to replace data entry people on this very day). Databases aren't going anywhere. They're yuge right now and for the foreseeable future.
On the other hand, I think a completely standardized language would be too limiting (maybe even impossible to design) across operating systems and platforms.
Thanks. Besides my MCSA SQL certification, are there any other things I should do to make myself employable in this field? Is working from home a realistic prospect with the cloud?
Unpivot and union all TASK_X tables together to (Project_Id, Task_ID, Field_Name, Field_Value) (this would have been a better way to keep this EAV storage to begin with). From there it is a straightforward join back to the task definition table and to get your output percentages.
thx a lot !
check out DataCleaner. It does this, and is FOSS https://datacleaner.org/get_datacleaner_ce
Your question isn't exactly clear, but going by your example table and expected results, you want the last *row* per STUDENT_ID, is that correct? Are you using an `ORDER BY` somewhere? STUDENT_ID | TEST_ID | TEST_SCORE | TEST_DATE | | :--- | :--- | :--- | :--- | :--- | 1 | A100 | 70 | 5/23/2017 | | 1 | A200 | 90 | 5/19/2017 | | 1 | A200 | 99 | 5/20/2017 | &lt;-- return this | 2 | A100 | 54 | 5/19/2017 | | 2 | A100 | 80 | 5/20/2017 | | 2 | A200 | 50 | 5/20/2017 | &lt;-- return this | 3 | A100 | 100 | 5/20/2017 | &lt;-- return this | 4 | A200 | 40 | 4/27/2017 | &lt;-- return this | 5 | A100 | 80 | 4/20/2017 | | 5 | A200 | 85 | 6/1/2017 | &lt;-- return this | 6 | A100 | 70 | 5/23/2017 | | 6 | A200 | 90 | 5/19/2017 | | 6 | A200 | 90 | 5/20/2017 | &lt;-- return this | 
No it isn't! I'm trying to simplify the data because it gets into another subject that is just way too confusing :) So -- one record per student that is showing solely the Highest Test (Oracle allows me to do a MAX() on alphanumerics luckily...the actual codes are similar to what I've used as an example). -- and its highest score. And if there are two with the same TestID and Score I need the highest date. This is all to analyze a series of tests that students can actually take multiple times -- and if they get a high enough score, they can take the next level test (which despite the score means that the previous test was mastered! So a score of 65 on A200 is actually higher than 99 in A100). Clearer? Probably not! Which was was trying to keep it more generic! --- EDIT For some reason it didn't show the table you have below when I was responding. But yes this is what I'm looking for. The code I have is ordered by STUDENT_ID, TEST_ID DESC, TEST_SCORE DESC, TEST_DATE DESC I really only want the first row for each student. I have code to do this, but damn...its like 30 lines long. Before I was doing SQL I'd throw this sort of things into PERL (or other languages) and clean it...and it was like 2 lines :-) Thank you very much for responding! 
Ah... what you want is an [analytic function](https://docs.oracle.com/cd/E11882_01/server.112/e41084/functions004.htm#SQLRF06174). Basically, you select all the rows, and then you create a "window" (via `partition by`) or group of records, and within that window do an `order by` to rank the records. select STUDENT_ID, TEST_ID, TEST_SCORE, TEST_DATE from ( select T.* , row_number() over ( partition by STUDENT_ID order by TEST_ID desc nulls last ,TEST_SCORE desc nulls last ,TEST_DATE desc nulls last ) RNK from TABLE_NAME T ) where RNK = 1 ; So if you run the subquery by itself, you'll see that each record has been ranked per STUDENT_ID: STUDENT_ID | TEST_ID | TEST_SCORE | TEST_DATE | RNK | :--- | :--- | :--- | :--- | :--- | 1 | A100 | 70 | 5/23/2017 | 3 | 1 | A200 | 90 | 5/19/2017 | 2 | 1 | A200 | 99 | 5/20/2017 | 1 | 2 | A100 | 54 | 5/19/2017 | 3 | 2 | A100 | 80 | 5/20/2017 | 2 | 2 | A200 | 50 | 5/20/2017 | 1 | 3 | A100 | 100 | 5/20/2017 | 1 | 4 | A200 | 40 | 4/27/2017 | 1 | 5 | A100 | 80 | 4/20/2017 | 2 | 5 | A200 | 85 | 6/1/2017 | 1 | 6 | A100 | 70 | 5/23/2017 | 3 | 6 | A200 | 90 | 5/19/2017 | 2 | 6 | A200 | 90 | 5/20/2017 | 1 | From there, the outer query just filters to records `where RNK = 1` In this case, you'll probably want `row_number`, but read through the link for when use `rank` or `dense_rank`. It all depends on how you want to handle ties.
Interesting! I've never used any analytic functions! I can live with ONE subquery, but the way I was doing things was out into spaghetti code land. I am going to try this right now. I'll write back either way because this may clean up a LOT of recent work I've done. Most of what I'm doing really is just cleaning the data before I can get to analyzing it. Again, thanks!
You want the row that has the highest test score per test id, and you want it to show with the earliest or most recent date? I.e. if the same student takes the same test ten times on ten different days and scores 100 each time, which row do you want?
Take a look at what BeeFourThree posted -- he got it! I was a little unclear in my request. Thank you for responding, however I think my answer is above -- I just had to use the analytic functions!
Oh you absolutely do need to use a row_number, I'm just trying to make sure I (and you) understand what you want exactly. As an outsider it would seem more intuitive that you'd list the oldest date with the highest score instead of the newest date (which is what you're getting) -- just wanted to ask the question so you don't run into a problem downstream
Oh then you're good. Carry on.
This is a really good site, thanks!
Comma's may not always be the best delimiter. Using pipe ( | ) or other printable but rarely occurring character as your delimiter will solve this in the future Biggest problem is when you are dealing with descriptions.
At the end of the day, the key data would be - Invoice_ID, Invoice_Data, Docket_ID, Docket_Date, Plant_ID Where if a Docket_ID exists in in multiple Invoice_ID it displays. Hope that helps! 
&gt; This can fail the attribute can be null or 0. Throw something in your query that count nulls and non nulls in that case. &gt;But, if I just check the number of records, I wouldn't know that the right records were inserted. I thought you were inserting all records in your temp table? If not, you could query this quite easily. You have a select statement you're inserting records with, modify this so it's just returning the key - does it return the same record count when you join the key to the inserted table? &gt;Are you talking about inner joining the destination table and the temporary table? I meant similar to before - you've got a query with a select statement that you're inserting with. Modify this so it returns the key of the records to be inserted, count that. Join that statement to the inserted records, check you've still got the same count. Then join the destination table to the other tables in the database that it normally joins with, check the counts (being careful to make it distinct on the key field on the inserted table) - this is a check of referential integrity with the rest of the DB.
Go with a combo. Put a sql db connection into excell. Then query the data from the db, then create pivot tables etc. If possible, use PowerQuery/ powerpivot. Have fun!
Except for the very basics IDK that Excel is much for viz. My understanding is that the more biz logic you can do in the database the better performance wise.
Sorry, I'm on mobile right now so it's a little difficult to explain. I'll post a quick sample table tomorrow AM. Basically if there are 5 records returned by the query then one of the columns in each record needs to be populated with 5.
I like this 
No partition should work too: count(*) over ()
let me walk you through an overhaul of your code step one, add indentation SELECT "SteveTest$Prod_ Order Line".Status , "SteveTest Live$Prod_ Order Line"."Prod_ Order No_" , "SteveTest Live$Prod_ Order Line"."Item No_" , "SteveTest Live$Prod_ Order Line".Description , "SteveTest Live$Prod_ Order Line".Quantity , "SteveTest Live$Prod_ Order Line"."Due Date" , "SteveTest Live$Prod_ Order Line"."Description 2" , "SteveTest Live$Prod_ Order Line"."Unit of Measure Code" FROM "Steve-Prod".dbo."SteveTest Live$Prod_ Order Line" "SteveTest Live$Prod_ Order Line" WHERE ( "SteveTest Live$Prod_ Order Line".Status=1 ) OR ( "SteveTest Live$Prod_ Order Line".Status=1 ) OR ( "SteveTest Live$Prod_ Order Line".Status=1 ) OR ( "SteveTest Live$Prod_ Order Line".Status=1 ) ORDER BY "SteveTest Live$Prod_ Order Line".Status step two, use a better table alias SELECT "SteveTest$Prod_ Order Line".Status , t."Prod_ Order No_" , t."Item No_" , t.Description , t.Quantity , t."Due Date" , t."Description 2" , t."Unit of Measure Code" FROM "Steve-Prod".dbo."SteveTest Live$Prod_ Order Line" AS t WHERE ( t.Status=1 ) OR ( t.Status=1 ) OR ( t.Status=1 ) OR ( t.Status=1 ) ORDER BY t.Status step three, fix typo on first line SELECT t.Status , t."Prod_ Order No_" , t."Item No_" , t.Description , t.Quantity , t."Due Date" , t."Description 2" , t."Unit of Measure Code" FROM "Steve-Prod".dbo."SteveTest Live$Prod_ Order Line" AS t WHERE ( t.Status=1 ) OR ( t.Status=1 ) OR ( t.Status=1 ) OR ( t.Status=1 ) ORDER BY t.Status step four, remove redundant WHERE conditions, unnecessary parentheses, unnecessary ORDER BY Clause SELECT t.Status , t."Prod_ Order No_" , t."Item No_" , t.Description , t.Quantity , t."Due Date" , t."Description 2" , t."Unit of Measure Code" FROM "Steve-Prod".dbo."SteveTest Live$Prod_ Order Line" AS t WHERE t.Status=1 now, we are ready to tackle your problem did you say you wanted to SUM() something? or perhaps you meant COUNT()? 
In terms of impossible with SQL (although not really analysis), maybe MS Office automation. You can extract and manipulate a data set (via, say, a site you access and send input with IE), chart it, convert that chart into an image, create a PowerPoint file, paste the image, attach the pptx to an email and send it. It's doable but slow and if you try handing it off to a typical office worker they'll break it in 20 minutes, at least if you make goofy crap the way I do.
Anything you want to make re-usable should live in SQL, because you can easily make views that allow re-use of the logic. It's much harder to re-use logic that lives in an Excel spreadsheet. If you copy the formulas to a new workbook, you then have version control issues if you want to make changes. So: Use SQL if you are going to need to re-use the logic, especially if you're defining KPI's and so on. Also, it helps to keep as much of the business logic as possible in one place. That way you have one version of the truth, and things like debugging, and back up are simpler. But for putting together what-if analyses and giving other people in the company data to play with, Excel is great. So, any definitions that you use more than once should be pushed up to the server as a field defined in a view, but when you're exploring the data and trying out a variety formulas to answer a business question, use whatever is your favourite tool. Avoid putting logic in perl scripts etc. if you can do it in SQL, because otherwise you have an extra learning curve for someone who joins your team to be able to fix or maintain it. If your decision is finely balanced do it in SQL, because then your team mate who doesn't know Excel so well can understand the logic too. One thing to be wary of when working fast and loose in a small team, is making sure you keep good backups and documentation for your "one-time only" analyses - SQL *or* Excel. You'll be amazed how often you'll be asked to repeat or justify a one-time only decision a year or two later. EDIT: More commas
I'm a huge fan of using tab as a delimiter instead of comma. In fact, by default, that's what Excel will expect as the delimiter if you copy/paste. When I can't use tab, I use ; because fuck commas and double quotes and all that shit.
"Saltillo Van" does not contain the string "sil" next question?
Going along with this, Just entering "1" brings up nothing as well. 
put the entire LIKE string into your SELECT clause to see how it's being evaluated you will learn something very interesting about the Replace function
Don't use SELECT * in views. Not only do you not get the benefit that you were expecting, but by not selecting the columns you wish to include in the view you may end up with duplicated column names.
OK and if I already have a gazillion select * views? How to find and recompile them all? And how else to make a view basically an alias for a table, fully transparent?
Refactor time. Look into CTE's they are probably what you are looking for.
Eh I cannot justify the time to spend on refactoring queries used every day and potential breaking them. Is there a way to recompile them all?
This is what I was looking for! Thanks.
Yeah, no partition is exactly what i'm needing. Thanks.
I stopped at "layers of views." Nested views will eventually wreak havoc on your execution plan with bad estimates and your performance will go in the toilet. Not to mention the confusion of where your data is really coming from. /u/sqlbek [wrote a sproc](https://sqlbek.wordpress.com/2015/03/03/debuting-sp_helpexpandview/) to unwind them. More reading: * http://www.sqlservercentral.com/blogs/2cents/2010/04/05/nested-views-causing-query-performance-woes/ * https://dba.stackexchange.com/a/5491/35474 * http://sqlmag.com/sql-server/what-are-your-nested-views-doing
Listen to /u/Cal1gula. Start fixing these things now before the inevitable performance problem happens and you're under the gun trying to fix them in production *right now*. If they're not causing problems today, you can work through them, test properly, and replace incrementally as you get better things implemented.
It is a mess. I want to only group the same items and same date. So there shouldn't be a differing unit of measure code because a new Item would be a new line. Does that sound correct thinking wise though?
All right but what is a proper fix? I don't know what fields will be needed in the future, and then I will have the same exact problem of not knowing in how many views to add a new field from the tables
Sorry you are correct the formatting is nutty. Just need to it consolidate so I don't get three lines of the same item and same date but rather just one row of that item and date. I would show an example but I can't get it to format right here. 
I cannot give half a shit about performance. I simply want a maintainable code base - see how I am already having maintenance problems, and I am using the same logic as you in normal programming when you try to ensure good structure and code reuse by having small functions and long chains of calling them. Really I must have at least a layer of abstraction over the tables, as the tables could be renamed any day: in the ERP software we are using the company name is part of the table name, the user says well it would be nicer to call it Fuckwit Limited instead of Fuckwit Ltd. and bam all tables are automatically renamed. Then have a layer of abstraction turning the shitty data format into something human readable, it should be ‚Ç¨12,5 not ‚Ç¨12000000.0000.05 or whatever the fuck the idiots who designed the ERP app came up with, same for dates, it is not 1701, it is '' (empty) in a proper report, and isnulling all the nulls as they are also ugly in a report and so on. The standard data format for everything but texts is horrible in this system. Then on top of this I can make the views that are actual reports And then a stored procedure so I can just have the data for the last month and not every data forever (Another thing making my life hard is utter lack of functions like start_last_month end_last_month in both SQL and the PowerShell I call it from as if it would not be utterly obvious that is how people report stuff like financial results I ended up making these myself) Where was I? Ah, performance. Thankfully I am not a DBA we are not big enough to have a DBA I just make reports. Performance is the dick measuring contest of DBA's the same way how tableless CSS is the dick measuring contest of web designers. In reality they actually do matter in some really big and expensive projects, mine is not so if the query runs fifteen seconds longer I don't care. I schedule it to run at night anyway. Thanks for the links I will check them out.
The "proper fix" is to write focused code for each of your different scenarios, rather than trying to write general catch-all code. There are numerous other consequences to the current approach, such as inhibiting SQL Server's ability to make use of indexes effectively.
So much anger born from a lack of understanding. I'm sorry but I can't help you understand why this isn't going to work well if this is how you're going to approach the problem and the people attempting to help you. &gt; and I am using the same logic as you in normal programming when you try to ensure good structure and code reuse by having small functions and long chains of calling them And that's going to get you into a heap of trouble, because SQL is not like "normal programming languages."
Good to know, thanks! I was remembering how some functions require an order by in the partition, and incorrectly assumed the partition was always required.
Every time I see a select * view I want to throw my computer at someone. What's the point? If tables are renamed? Why are production tables being renamed? 
&gt; Really I must have at least a layer of abstraction over the tables, as the tables could be renamed any day: in the ERP software we are using the company name is part of the table name, the user says well it would be nicer to call it Fuckwit Limited instead of Fuckwit Ltd. and bam all tables are automatically renamed. &gt; This is what he said in a different post. I'd kind of like to know what the ERP application is to be honest so I can avoid it, forever.
http://www.dpriver.com/pp/sqlformat.htm SELECT "stevetest$prod_ order line".status, "stevetest live$prod_ order line"."Prod_ Order No_", "stevetest live$prod_ order line"."Item No_", "stevetest live$prod_ order line".description, "stevetest live$prod_ order line".quantity, "stevetest live$prod_ order line"."due date", "stevetest live$prod_ order line"."Description 2", "stevetest live$prod_ order line"."Unit of Measure Code" FROM "Steve-Prod".dbo."stevetest live$prod_ order line" "SteveTest Live$Prod_ Order Line" WHERE ( "stevetest live$prod_ order line".status = 1 ) OR ( "stevetest live$prod_ order line".status = 1 ) OR ( "stevetest live$prod_ order line".status = 1 ) OR ( "stevetest live$prod_ order line".status = 1 ) ORDER BY "stevetest live$prod_ order line".status Took &lt;2 seconds to make your code readable.
Yeah fuck that. At a minimum then I would be sucking the data into production tables of my own, probably in an entirely separate database on the server. That sounds hellish.
What I would do considering the ERP shit show you're involved with... is suck all of the data out of those tables which can be renamed, and store them in a permanent production database where nothing changes without a change request. You could probably create a mapping table where you store the permanent home and the ERP naming schema in another column, then write a sproc to run at any interval of your choosing to move the data over, then base all reports on the permanent tables.
So let me get this straight. 1. You have reports based on tables joined together. 1. The way these joins occur is static for any given report. 1. The number of columns in any given table is dynamic in your report in the sense that you expect that if a column is added or dropped the report will show that column 1. You accept that the foreign key columns and columns that are referenced by the stored procedure parameters remain. 1. You have a background in other programming languages and are not a strict SSRS guy. One possible solution is to write a C# or powershell program that regenerates these views periodically. Just interrogate `Information_Schema.columns`. That ISO catalog view has the information you want to generate a `DROP VIEW/CREATE VIEW . . .` statement. Then just have a sql agent job run that every day or hour or whatever you need. While I take issue with /u/SQLBek's statement that DRY doesn't apply well to procedural languages in general, I agree its true in the case of T-SQL Views, UDFs and stored procedures, and in the general case of these objects in RDBMS systems. In the world of app-dev we get around this problem by writing application code to generate SQL, or using an ORM or query generation libray to write our SQL for us. We can then have functions inside of function, and pass around query generator objects in a way that works. Since you don't care about performance, then any arguments about the plan caching of adhoc SQL are moot here.
Ok, here is what I have so far: SELECT "stevetest live$prod_ order line".status, "stevetest live$prod_ order line"."prod_ order no_", "stevetest live$prod_ order line"."item no_", "stevetest live$prod_ order line".description, Sum ("stevetest live$prod_ order line".quantity) AS TOTAL_QTY, "stevetest live$prod_ order line"."due date" AS 'PRODUCTION_DATE', "stevetest live$prod_ order line"."description 2" AS 'SHIP_DATE', "stevetest live$prod_ order line"."unit of measure code" FROM "Steve-Prod".dbo."stevetest live$prod_ order line" "SteveTest Live$Prod_ Order Line" WHERE ( "stevetest live$prod_ order line".status = 1 ) GROUP BY "stevetest live$prod_ order line"."due date", "stevetest live$prod_ order line"."prod_ order no_", "stevetest live$prod_ order line".status, "stevetest live$prod_ order line"."item no_", "stevetest live$prod_ order line".description, "stevetest live$prod_ order line"."description 2", "stevetest live$prod_ order line"."unit of measure code", "stevetest live$prod_ order line"."quantity" ORDER BY "stevetest live$prod_ order line".status It probably makes no sense. I just need it to GROUP BY the "DUE_DATE", so any item with the same date just gives the total quantity vs 2 or 3 lines of the same item on the same date. Fingers crossed my formatting is better also... 
There are a lot of ways to accomplish this. I've had a hard time resolving what you're looking for by using a set based query and typically resort to correlated subquery. The correlated subquery is a subquery that references the primary query and joins on it. The result is the subquery is executed and analyzed for each row in the main query. Here's an example of one. https://stackoverflow.com/questions/879111/t-sql-subquery-maxdate-and-joins There's also other methods but I feel correlated subquery is my favorite minus figuring out a good way to execute in a set. 
Use windowing/analytic functions to find first and last date. There are a few ways to do it based on whatever data type your date column and other stuff column is. This assumes date and other stuff all line up row-wise. The basic idea is something like this: SELECT PERSON, MIN(DATE) OVER (PARTITION BY PERSON) as FIRST_DATE, MIN(OTHER_STUFF) OVER (PARTITION BY PERSON) as FIRST_OTHER_STUFF, MAX(DATE) OVER (PARTITION BY PERSON) as LAST_DATE, MAX(OTHER_STUFF) OVER (PARTITION BY PERSON) as LAST_OTHER_STUFF, FROM YOUR_TABLE; Or use FIRST/LAST VALUE: SELECT PERSON, FIRST_VALUE(DATE) OVER (PARTITION BY PERSON ORDER BY DATE asc) as FIRST_DATE, FIRST_VALUE(OTHER_STUFF) OVER (PARTITION BY PERSON ORDER BY DATE asc) as FIRST_OTHER_STUFF, LAST_VALUE(DATE) OVER (PARTITION BY PERSON ORDER BY DATE asc) as LAST_DATE, LAST_VALUE(OTHER_STUFF) OVER (PARTITION BY PERSON ORDER BY DATE asc) as LAST_OTHER_STUFF, FROM YOUR_TABLE; Double check the asc/desc ordering to make sure it's correct and that FIRST and LAST pull the correct dates. EDIT: Sorry - MIN/MAX, FIRST/LAST will probably pull OTHER_STUFF alphabetically/numerically depending on datatype. Do a subquery to get first and last dates, then query your main table and join on your subquery, where dates = first date and last date to get first and last other stuff.
Close. Just take the quantity column out of your GROUP BY clause. You're not grouping by it, you're summing it :)
Thank you for the suggestion, I had been playing around with the analytic functions a bunch and couldn't get anything to work. Unfortunately my Other Stuff fields are categorical and not numerical or dates, so using only analytic functions isn't possible. I tried combining the analytic functions with the correlated queries in my reply to /u/Rehd.
My first answer could have been better. You could just use group by. Find the first and last dates first, then query. This is pretty clunky but just a quick draft for the basic idea: SELECT a.PERSON, "First Date", first_other_stuff, "last stuff", last_other_stuff from table a inner join (SELECT fs.PERSON, fd."First Date", fs.other_stuff as first_other_stuff from table fs inner join (SELECT person MIN (date) as "First Date" FROM table GROUP BY person) fd on fs.date=fd."First Date") b on a.person=b.person inner join (SELECT ls.PERSON, fd."last Date", ls.other_stuff as last_other_stuff from table ls inner join (SELECT person MIN (date) as "last Date" FROM table GROUP BY person) ld on ls.date=ld."last Date") c on a.person=c.person
This is my pseudo code attempt at a set based option. Insert the subquery into a temp or table variable (@FirstTable) (SELECT MAX (tab.date) OVER (PARTITION BY tab.person) as "FirstDate", MIN (tab.date) OVER (PARTITION BY tab.person) as "LastDate", Person from table ) SELECT a.PERSON, "First Date", first_other_stuff, "last stuff", last_other_stuff from table a inner join @FirstTable on A.Person = @FirstTable = Person and table.date = @FirstTable.FirstDate inner join @FirstTable on A.Person = @FirstTable = Person and table.date = @FirstTable.LastDate /u/TheQuantifiedOther 
Did you mean to respond to the OP here? I don't see where /u/alinroc mentioned repetition or anything. 
You've got that backwards. The same row of A can't be joined to both the first date and the last date (unless the person has only one record), so that would return two rows per person. Instead, create a driver^\* table/subquery with one record per PERSON and both the first and last dates, and join back to the table *twice* select DRIVER.PERSON , FST.DT as FST_DT , FST.OTHER as FST_OTHER , LST.DT as LST_DT , LST.OTHER as LST_OTHER from (select PERSON, min(DT) FST_DT, max(DT) LST_DT from TBL group by PERSON ) DRIVER inner join TBL FST on DRIVER.PERSON = FST.PERSON and DRIVER.FST_DT = FST.DT inner join TBL LST on DRIVER.PERSON = LST.PERSON and DRIVER.LST_DT = LST.DT ; \* I don't know if there's a proper term for it, but I've always considered any subquery that gathers the needed keys and then joins to the same/other tables to pick up attributes to be a *DRIVER*.
Converting string to date is usually bad practice but if that's all you've got... use this: INSERT INTO REPORTING ("CitationDate", "CitationID", "DatePaid", "Location", ) SELECT CAST(LEFT(CITATIONDATE, 10) AS DATE), CITATIONID, DATEPAID, LOCATION, FROM STAGING; That will take the left 10 characters, which are hopefully a valid date, and cast them as a date to insert into your (I assume) date column. Make sure your format comes out OK because SQL could be doing some automatic date formatting based on your collation. If it is then I can give you a query that will handle that too.
Thanks!
Wouldn't you need to also use trunc(b.sent_date,'DD') to get the day to group on?
What do you get for create_date, modify_date, is_cycling, current_value, is_exhausted, etc. from SELECT * FROM sys.sequences?
NTEXT is deprecated. Try utilizing NVARCHAR(MAX) or VARCHAR(MAX). SQL Server stores them differently than NTEXT too. 
Can you use a staging table on the SQL Server side? Bring it in into an NTEXT stage table then do whatever transformations you need to do? 
username checks out
A couple of things: * In Teradata, you can `select type(&lt;column&gt;)` to see what the datatype is * Apparently the maximum character limit in Teradata is 64,000 * SQL Server's `varchar(max)` and `nvarchar(max)` allow for up to 2gb of text [(Documentation)](https://docs.microsoft.com/en-us/sql/t-sql/data-types/nchar-and-nvarchar-transact-sql) * Everything in Teradata is `nvarchar` but is simply called `varchar` * I know from experience that you'll want to set up SQL Server as `nvarchar()` to read a `varchar()` from Teradata * I've never dealt with such a long string, but I suspect this "mismatch" of datatypes will solve your issue
What is the granularity of the records in sales_sent, and what is in current_members? 
Current_members is just a number. sales_sent basicly contains this. sent_date - data type is date example 06/05/2017 Sales rep name - text Product type - text current_members - number Customer_number - number Now 1 customer can have multiple rows from the a sales rep with different or the same amount of current_member for each product type. I only want to sum the highest current member and ignore the others. The month to date version gets the right number breaking it down to by day gets me a different amount when I try and match it to the month to date version. 
Great info, I'll give these a try.
I am attempting to bring these into a staging table (from there I was going to clean them up).
&gt;be sure to set up (or get access to) a server with data Is this possible without being employed in the business? Currently work in TV, and looking to work in data analysis. My company offers free training in SQL, but I don't think they provide a server or a database. 
For question 1, as long as `Stock_ID` is indexed (and the ID fields are the key/indexed), it should be pretty low cost query (I don't have a lot of exposure to PostgreSQL, but hopefully my T-SQL stuff will translate without too much trouble): with Newest_Entry as ( select stock_id , max(ID) as max_id from Stock_Ticks group by stock_id ) select s.name as Stock_Name , s.symbol as Stock_Symbol , st.price , st.updated_at from Newest_Entry as ne inner join Stock_Ticks as st on ne.max_id = st.ID inner join Stocks as s on st.stock_id = s.ID; For question 2, I've got a couple of ideas, but they'd probably have terrible performance. I'll edit my post if I come up with a reasonable solution.
Thanks for the thorough answer. I failed to mention that I am using PostgreSQL which from what I see doesn't support virtual columns. So this leaves me with views or functions. Probably functions according to these links https://www.postgresql.org/docs/current/static/indexes-expressional.html http://bernardoamc.github.io/sql/2015/05/11/postgres-virtual-columns/ Also what concerns me is speed over space. That complex query should be run every time someone queries by zodiac. Unfortunately I don't know much about databases so maybe my concerns are unfounded and indexing will solve the problem. Thanks again for the help
Postgres does basically support persisted computed columns in the form of [indexes on expressions](https://www.postgresql.org/docs/current/static/indexes-expressional.html). I suggest [writing a function](https://www.postgresql.org/docs/current/static/sql-createfunction.html), perhaps `zodiac(date)`, and then creating an expression index that uses it. Postgres can then do a simple index lookup when you run `SELECT * FROM users WHERE zodiac(birth_date) = 'virgo';` Postgres notices that you have an index on `zodiac(birth_date)` and substitutes a lookup for the calculation. This is much faster than a table scan (because the DB doesn't need to look at every row, not because calculating zodiac signs takes very long).
Ok, what is it?! 
I'll admit, I stopped using SQLFiddle 6-8 months ago because it was always overloaded. Historically I'd used it quite a bit off StackOverflow. When the site choked on running something as simple as `SELECT 1`, though, I gave up on it. I know you're running into the problem of traffic issues, but if it doesn't work I stop using it. I'll try to remember to use it again. It's a fantastic service that provides a lot of value to people, and you should be proud of the impact you've made regardless of the problems that have occurred.
If you plan on doing any datediff or date add to the min Or max dates be sure to cast as date time because it returns an integer 
Thanks. I will do it this way. 
Making queries non-SARGable (as your co-worker pointed out) and generally ignoring (or not having!) indexes is one of the biggest performance killers. I like the [SolarWinds 12-step SQL Server query tuning infographic](http://launch.solarwinds.com/rs/solarwindsworldwide/images/12_Step_Query_Tuning_SQLS_IG.pdf) (PDF warning). There's an [Oracle version](http://www.solarwinds.com/resources/infographics/oracle-query-performance-tuning.aspx) too. Specific to SQL Server: * Are you using table variables? Unless it's only for a handful of records, it's probably wrecking your query plan with bad statistics/cardinality estimates. * Are you nesting views? Unwind them. At some point, nested views make statistics useless. * Are you using the same CTE twice in one query? Make it a temp table instead * Are you using a cursor to loop over records to do an update? Make it a set-based operation instead * If you're running the same query multiple times but with different criteria, can you parameterize it instead of dynamically constructing the query as a string? * Are you using `IN` with a subquery? Try `EXISTS` or a `JOIN` instead.
That depends on what your application is and what it's supposed to do with the data. But unless it's a front-end for heavy-duty analytics, you probably shouldn't do this.
Hi, I'm kind of a new learner of SQL; could you explain this one a little bit more? I'm just curious.
You really only need to know SQL to be able to pull the data properly. Depending on the level of work you do and size of the data, you may be able to get away with doing the manipulation once in Python. I started in SQL and am making my way through pandas now and found this page extremely helpful: https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html Good luck!
[PHB](https://en.wikipedia.org/wiki/Pointy-haired_Boss)
Couldn't agree more. Logical reads are the final arbiter on a system where you don't have the freedom to flush buffers, etc. 
Sure thing! Let's do us some learning! I should have specified that in general, its poor form to put a function anywhere in your where clause. The first thing to remember, however, is indexes. Indexes are everything in SQL Server. Good indexes means a job well done and you don't lose sleep over it at nights. Seriously, learn indexes!! Imagine if someone asked you for some information, say history of christmas, and you had to *scan* through 500 pages instead of just looking in the index in the back of book under "Christmas, history of... page X". It would take forever right? The person asking would be pissed off. An index would be a pretty great idea! Its the same way with SQL. *But*, when you put functions on your where clause, you are effectively telling the query analyzer to *ignore the index*! Imagine if every time you went to check the index in the back of the history book, some butt head slapped your hand with a ruler and said soup-nazi style "No index for you!" Take this, I am using SQL Server 2016 and AdventureWorks2016 database. Do you know how to set those up? SELECT [EmailAddress] FROM [Person].[EmailAddress] WHERE left( [EmailAddress] , 2 ) = 'As' There is an index placed on the EmailAddress column, and if you run this query and select the "Include execution plan" button, you will see it does an index scan, which is bad. That is the equivalent of looking through every page of the book. This is because the analyzer is having to evaluate *every* row in the database before anything is returned. Imagine if you had a table with millions and millions of rows, and yes you probably WILL encounter this scenario at some point in your SQL career, I guarantee it. This begs the question, why is it evaluating every row first? The answer is because of the difference in the lexical and logical ordering of a queries analyzers order of operations. [I suggest giving this a read](https://dzone.com/articles/a-beginners-guide-to-the-true-order-of-sqlnbspoper). Dont feel bad if you don't totally understand all of it at first. Suffice to say that the returned set of data is unpredictable to the query analyzer. Anyways, back to the question at hand. If we rewrite the above query as SELECT [EmailAddress] FROM [Person].[EmailAddress] WHERE [EmailAddress] LIKE 'AS%' Now, the query analyzer can use the index that was created on the column, which is much much much much faster. The CPU will thank you, the network will thank you, your database administrator will thank you, the end users will thank you, *and* most importantly, your future self will thank you! Here is another example, there is an index on the ModifiedDate columns: SELECT [SalesOrderID] , [SalesOrderDetailID] , [ModifiedDate] FROM [Sales].[SalesOrderDetail] WHERE DATEDIFF( YEAR , ModifiedDate , GETDATE() ) &lt; GETDATE() You will notice this does not use the created index. Lets change it up: SELECT [SalesOrderID] , [SalesOrderDetailID] , [ModifiedDate] FROM [Sales].[SalesOrderDetail] WHERE ModifiedDate &gt; GETDATE() Now, the analyzer can use the index properly. Helpful link: [http://use-the-index-luke.com/](http://use-the-index-luke.com/) Hope this helps! Good luck! 
I prefer DMX for querying thug life modules because I know he gon give it to me
Not to nitpick, but it's `PL/SQL` for Oracle.
Update Statistics. If a query anticipates zero records but encounters 1 million, it will suffer a slow and horrible death.
Thanks. About the "app dev world" it is really weird IMHO how the strenghts or SQL are not used, it is such a great language for generating reports, this is why it got reinvented as LINQ. It is fucking great to just sum up all the relevant looking numerical fields, group by all the important looking codes (think item or customer code) and datepart the transaction date into months or something and group by it too, throw the result into an Excel Pivot table and let people figure out their own reports out of it. I don't even have to waste time on defining requirements or use cases this way, one guy use the same pivot for sales global the other per item etc. basically this way I reinvented "business intelligence" and "OLAP cubes" and "data warehousing" without all the thick bullshit bingo language and complexity those stuff tend to require. I once wasted a week on figuring out what the heck OLAP is and it turned out their "measures" are mostly this "sum up everything important looking numeric" and their "dimension" are this "group by important codes" so all that overly complicated and obscure crap can be reinvented in a simple SQL view...
What is new clients? It is in house for one company, why do you assume clients? And what is not declarative about views?
It is not too hellish. It would be if I was working for a consulting company and had many customers like this. But I am in house, one company group, I can deal with one rename per two years tops.
Not customer: company. Your own company, customers do not get into the picture. You can have multiple ones because in real life people often set up every business stream as a different company yet the same people do the accounting for them. And there is no company table, because every table needs to be duplicated for every company. I think the reason it is so that they want to discourge SQL reporting, they want to sell their own reporting tools.
If you are working on a Linux environment, I think that learning Bash and Python will take you a long way forward. Many DBAs I know are writing automations and scripts with those two.
Possibly some (basic) Unix as well
*Understand SARG-ability* will take you further than "use indexes". An index isn't useful unless your predicate is sargable.
&gt; Is there a good method out there for organizing such tools? My organization method may not work for you. Find something that works and use it consistently.
Couldn't agree more. Last year I interviewed people for a database-related internship and asked a real basic "how do you approach performance tuning" question. **Every** answer was "indexes". That was it. Just create indexes and all of your problems go away, I guess?
+100 - even for a 1 man team, a consistent encoding scheme &amp; solid documentation will serve you well. Even if you're the only one around, documentation will help you because as you said yourself, we all forget those "once in a blue moon" things we code.
Since you're asking about MS SQL, PowerShell is really about it. Microsoft is embracing PowerShell in a big way and there are things like the dbatools.io project that make a DBA's life much easier. I've been working with SQL Server for 15 years, as both an operational DBA &amp; a T-SQL developer, and I haven't had to make use of anything else. Now, if you want to dive into ETL &amp; BI, then SSIS, MDX or DAX will serve you well. If you want to get into machine learning &amp; data science, R &amp; Python. But these are specializations that you do not have to get into if you do not wish to.
Lots of excellent advice here already. What I'll add to this, is to not write single, gigantic, T-SQL statements that try to do everything all at once. Why? Because of a critical nuance of the Query Optimizer - that it has a limited amount of time, to come up with a "good enough execution plan quickly." The QO has the same time slice to generate an optimal execution plan for a simple query with 3-4 JOINs vs a complex query with 2 dozen JOINs, 2 sub-queries, and a nested view or two, etc. Think of it this way - you have a 10 second window to come up with your game plan to do something simple - move boxes out of a garage into a truck, vs say, move the entirety of your household into a truck. TL;DR - break up massive, monolithic T-SQL queries into smaller, more focused queries. It may be more steps/queries, but generally speaking it'll be more efficient.
Oops, missed the part where the specific database (MSSQL) was specified in the question. Sorry about that, and thanks for the clarification.
But apparently the select * is not expanded but saved at compile time so it ignores later fields? BTW why would that expansion be a bad thing? Think about this. To get the current inventory on hand, every single stock transaction ever needs to be read, the whole table, and groupes and summed. I suppose the QO is smart enough to slurp that table only once at execution, taking a few minutes. Just because in one view the fields are presented in a more readable format, in another view they are left join with master data so that we can actually figure out what product it is etc. the transactions are still just read once? And here is something else as well. When I got into this business in 2002 I had to do some SQL level optimization because the ERP had shit performance and the big lesson we learned it is all about optimizing disk reads. For example the reason we want indexes on where conditions is is that indexes are stored sequentially on a disk so quickly read, lacking this it will read the whole table from the disk and then filter in memory. My point is twofolds, in most reports I will need the whole table anyway, or maybe I have a date from to where conditions, and I hope the QO can figure that through views. The second is that I am not 100% sure about the hardware but I think we are using solid state disks. Those are fast. Pretty soon indexes will be an outdated thing as disk I/O performance won't be a bottleneck. At that means forget performance issues because it has always been those that were the major ones, not CPU or RAM.
But DMX does have problems impersonating federal agents though.
Break the query out.
Hi! That's going to depend heavily on what they want you to do with it. But I suppose either way you're going to need to start from the beginning. W3schools.com is a reasonable start for get to to grips with basic querying and then take it from there.
I said the same thing. I've sped up so many applications by simply breaking out querys into steps, indexing the common id on the tables that are made and then outputting it all.
Great video!
My usual product agnostic beginner recommendations follow: * Gentle Introduction * The Manga Guide to Databases - Takahashi * Intermediate Treatment (begin here if the above is not to your taste) * SQL Queries for Mere Mortals - Viescas, Hernandez * Database Design for Mere Mortals - Hernandez * Advanced (return to this later) * SQL and Relational Theory: How to Write Accurate SQL Code - C. J. Date You'll quickly discover that every database engine (Oracle, Access, MySQL, SQL Server, PostgreSQL, etc) has slight syntactic differences, and various functions/features unique to that engine. I strongly suggest starting with one engine and sticking with it until you're comfortable. You should not chose Access as your first product. It is a uniquely terrible dialect of SQL.
Hahaha ahhh this is too good. SELECT [SQL Jokes] FROM REDDIT WHERE [Losers] IS NOT NULL
I'm in similar boat. Can anyone suggest a good IDE to start with and perhaps some guides for that IDE (preferably in text/interactive code format)?
Personally prefer MSSQL and Management Studio.
also, what are you using to move the data? SSIS, import/export wizard? if using varchar(MAX) over NVARCHAR(MAX) doesnt work then it i have come across cases where the first X rows are checked to "create" the table schema. have you built your table already to mimic the source table? so if you let the wizard build your landing table them it could put that column as too narrow and thus truncation error. 
You can't do DDL in a CASE afaik (at least not on MS SQL but I doubt in MySQL also). You will need to use a different method (sproc, if/else, etc.).
SELECT MIN(DateColumn)? Or do you actually want an average of the timestamp? Or two rows? It's not really clear what you're asking.
I personally recommend to focus solely on your MSCA to start. As others stated, Powershell / batch is very good to learn next. Python / C# / R are the next things you can learn, pending on what you need or want to do. I would recommend learning enough basic Linux to do things without looking up cmds frequently.
I am using the import/export wizard. I can certainly try pre-building the landing data. That sounds like a good avenue to try. However, the error is reported if I do just a basic select * against the openquery statement (without any import/export going on). --Example: select * from OPENQUERY(TDsource, 'select * from TD_Source_Table_With_Large_Varchars') The following will return the error despite running casts on the local side. --Example: select cast(col1 as nvarchar(8000)) from OPENQUERY(TDsource, 'select col1 from TD_Source_Table_With_Large_Varchars') The following works correctly (as expected) --Example: select * from OPENQUERY(TDsource, 'select cast(col1 as nvarchar(8000)) from TD_Source_Table_With_Large_Varchars') However, this will lead to truncation that I don't want. My thought is that the openquery command is the issue here? 
 with t(date, value) as ( select * from (values('2011-01-01'::date, 125), ('2012-01-01'::date, 150)) t ) select date - (2011 + extract(year from date) || ' years')::interval, sum(value) from t group by 1 
OP mentioned using Oracle....
Trying not to bog myself down in the details! (Also, good point) -1 for reading comprehension.
If they said "indexes", and followed that with "then run it again and look at the execution plan", I might give an intern credit for that. But above entry level, they need to be able to tell me *why* a query isn't using an index. 
Don't promote w3schools. Their info is often outdated, or just wrong. 
I would just do SELECT TO_CHAR(date,'MM-DD') as date_at, SUM(x) FROM table GROUP BY 1 and then you know the dates are aggregated together, across years. 
Interesting. I definitely have some cases of multiple self joins like the first example on what could be, for some clients, a largeish table. I will have to run some performance tests with your method. 
Analyst here that has no SQL developers in house. I'm happy to say that my database looks MUCH worse than that. You would puke just looking at it. That said, i've set up some awesome stuff and it's a key business driver. Maybe if we make millions we'll hire someone like yourself to put it all together.
I'm very new to SQL as well, but I see a few things that don't make sense. For example why have allowed (yes/no) and denied (yes/no) when you could have approval (allowed/denied)...same with a bunch of columns that seem to have to do with payment method. Also, I'm not sure, but something seems off about the data types. I honestly don't know if I'm on the right track either though
One thing that sticks out to me: objects with spaces and/or non-alphanumeric characters break lots of stuff.
&gt; the columns that contain names with /'s also contain 2 values. &gt; The query being run was substringing the second value out of the column. Now I'm triggered.
&gt; What am I missing? your GROUP BY clause is wrong how many rows does each name have? that's what you're getting when you say `GROUP BY name`
 SELECT CREATION ,SUM(CASE WHEN NAME LIKE 'A%' THEN 1 ELSE 0 END) AS 'A NAMES' ,SUM(CASE WHEN NAME LIKE 'B%' THEN 1 ELSE 0 END) AS 'B NAMES' FROM TABLE WHERE NAME NOT IN ('FRESH', 'OUTDATED') GROUP BY CREATION have to have the parenthesis after the end as the end statement is needed for the case to close. 
There is absolutely no training material available whatsoever for one of the most common languages on earth. You must learn through blood, sweat and tears (and trial and error). Good luck. edit: But seriously though almost any college that has any IT program will have classes where you will most likely learn at least basic SQL. Or there are (probably) trillions of resources available online to learn on your own.
No, PHP is completely unrelated to SQL except that it makes SQL injections really easy. You don't need to learn one to learn the other. They serve completely different purposes. MySQL is a variation of the SQL language. It is an engine that runs the database and uses standard SQL and MySQL specific functions. There are minor variations in each SQL RDBMS (MySQL, SQL Server, Oracle, PostgresSQL, etc.) but they are very similar and mostly interchangeable. I would recommend starting here: https://www.brentozar.com/learn-query-sql-server-stackoverflow-database/ 
I think I can if it's not in information_schema it should be tied to the object.
Heh sounds like you probably contain a single row in each page.
Are you actually using MySQL? Like you can install SQL Management Studio and connect to any SQL server (if the SQL server is installed) Microsoft has a "test" database called AdventureWorks that they provide a lot of test code for. I can give you links to all of this stuff. If you are using MySQL you want to install one of their IDE (MySQL Workbench etc) programs that use a MySQL db or else you will need to use a command prompt or something silly to access it. Like so: http://www.wikihow.com/Create-a-Database-in-MySQL
That's probably one of the best things to do to handle that ~~problem~~ schema design choice.
Hey man, many thanks, that sounds great. I'm not using MySQL, I just chose it on the vague assumption that it was the most popular. Open to anything.
I need the date, actually I just want the date, not the time, the time was there as dumb me in a hurry on my lunch hour overlooked my small java program and once I ran it it was already done, now on my spare time on the work, I'm studying SQL a bit, it's going pretty well except for that, I can't bloody figure out how to make it return only one date, since all the dates have the same day, month and year, only thing different is the time, which I'm not even calling... I do understand what Distinct does, but I do not find any other way to return a single data, and then give the flat count in the column as I ask..
No problem! Glad I could help.
What these analysts do is get file from state, load it by hand. Massage it, extract a bunch of foreign linked servers, match of the data via it's description values, Massage the data. Sit there will it takes 2 hours to query about 120mb of data because it's a couple 100 thousand rows in heaps. Complain about the server being slow. Go home. Watch some terrible Netflix show like iBoy and come in next day to do the same thing. Edit: I'm not making it up. One of the analysts dropped the "this is my existence". I feel for them and I try to steer the group but it's a lot of work to fix the underlying issue rather than keep bailing water out of the boat everyday.
I've done that plenty of times in the past. The thing it taught me is to ask for help, and usually people are more than happy to give it. So here I am.
Table 3 is the crux of the matter right. I am not sure I am visualizing how that would be laid out. Like how is the information stored in that table? I basically need a boolean value for every user...for every zip. Or a linked list of included zips. Problem is I don't to store and iterate over a linked list. I guess I could just store a linked list as a Python object, and store a reference to it in the DB? I just don't think that is good for a lot of reasons, normalization among them.
Has anybody used any of these before? If so, which would you recommend? Thanks!
Table 3 is just User ID and Zip ID, no other columns needed. There's one row in that table for every zip code that is selected for that user. the unique key on Table 3 is the combination of the two foreign keys.
I can vouch for Metabase, but have to qualify that it's not your typical BI tool, and it's relatively new to the scene. Re:Dash is another great BI tool that is similar to Metabase.
W3schools is a great resource for learning SQL, still use it when I get stuck. 
Colleges often teach SQL with stuff that you don't really need (relational algebra and such). If you've got a motor, you can learn using numerous tools that are freely available - free online courses, YouTube videos, local library, etc. 
Love it
Code Academy has a course here you can try: https://www.codecademy.com/learn/learn-sql 
This is a really bizzare list... some of the items included are much more advanced analytics tools than one would normally expect in a BI tool. I know the lines between BI and analytics are getting continuously more blurry but some of these are ridiculous. KNIME and RapidMiner are definitely not in the same category as things like SpagoBI, Pentaho, or the ELK stack. KNIME and RapidMiner are designed to run statistical analyses and machine learning algorithms. They're much more comparable to something like SAS, SPSS, or Azure ML. The ELK stack itself makes no sense being here- as far as I'm aware it is used for log analysis and/or search and few people use it for generic BI projects, though Kibana (the K in ELK) is certainly what I would call a BI reporting/visualization tool.
 (SELECT * FROM WORKS_ON WHERE ESSN = EMPLOYEE.SSN AND PNO = PROJECT.PNO); What the above does is grab from works_on, everything, and that everything must contain a ESSN equal to your employee ssn and your PNO must equal your project pno. The issue here is, this must be executed for the other two queries. So let's just note what that piece was doing and move on. (SELECT * FROM PROJECT WHERE NOT EXISTS So now we are grabbing everything from PROJECT, but it must meet a criteria. Works_on is now going to act like a reverse inner join. It's going to find all the records that don't match the project number and all the records that do match based on the works_on, but it will only return the ones that don't match. SELECT * FROM EMPLOYEE WHERE NOT EXISTS And here we are grabbing everything from employee. Now the criteria here is that it has to match on the SSN back in the works_on, essentially acting as an inner join. But it will look for records that don't exist from the above, which is looking for things that don't exist with a pno. So what you're getting in the final select are employees with a project number where the ssn is matched in the works on table. The most inner subquery is being evaluated row by row for each outer query, and the 2nd inner subquery is likely being evaluated row by row by the most outer query. Ideally you would do things in sets rather than having everything analyzed row by row. What you have created is essentially a correlated subquery. https://en.wikipedia.org/wiki/Correlated_subquery Your question was: Display the details of employees who work on ALL projects. I would want to keep this dynamic, I'd like to make sure if any new projects are added, we can account for this. You have a dimension table that keeps track of them. I would either look at total rows or a count of the number of projects. Next I would do a join on the project and works table, but the where criteria would be based on how many records exist in the works_on table comparing to the number of projects. If the number of projects is equal to the rows or greater in the works_on based on the distinct count of PNO for that user in the projects table, then I would want them to return. Then I would just do an inner join on the employee from there to return their information. 
That's a great tip thank you. However, I am still confused as how can the outermost query ever be reached as there should always be an output for the middle query and therefore, could not make it to the outer query. This is my main confusion because if you SELECT * from a table, should there not always be an output?
&gt; With my double negation query, am I right in saying that it will loop through many times using each PNO This is true through the mechanics of correlated subqueries, it must analyze each row in the outer query and inner query to meet the criteria of the where clause. &gt; so it will look if PNO 4 has output from the WORKSON and then if it does not have any, it selects PNO 4. Is that on track or close? More like, it will select all results from workson where the SSN is matched and also where the pno is matched. So if PNO 4 exists in works on: SELECT * FROM PROJECT WHERE NOT EXISTS The above will grab the results where PNO 4 does not exist. SELECT * FROM EMPLOYEE WHERE NOT EXISTS This will then select everything from employees that did not exist in project that is matched in workson. Your query structure to solve the problem is the equivalent of spaghetti code. Spaghetti code is exactly as it sounds. Its unstructured, not uniform, hard to follow, and hard to resolve later. You can solve this problem in a much easier fashion than how you are now or at least in a less confusing manner. Keep your goals in mind. &gt; Display the details of employees who work on ALL projects. So the employee will need to be listed each time in workson and it will correspond directly to the projects table. Once you have your projects and workedon, the last join is easy as it's just returning reference data for that person. So try this: - Get the distinct # of projects total - Get the distinct count of projects each person has worked The first one is a subquery and it should create your where clause. The second one is your informative select query, this output should be joined to your employees table. Remember, you have to return all people who have worked on all projects. That means they have to have every PNO possible associated to their SSN. Your current setup will evaluate For a correlated version, I think this is close to what you want. (MySQL is not my first language and I have no workbench or instance installed to test this.) select WORKS_ON.essn, employee.fname from works_on inner join employee on employee.ssn = works_on.essn WHERE WORKS_ON.PNO in (SELECT PNO FROM Project WHERE PROJECT.PNO = PNO)
Not always, since the WHERE clause is evaluated first, you could have no results. But again, in your query you are referencing the outermost query in the innermost query. So the SQL Engine builds the entire data set by looping through each row. Something most likely similar to this: 1) Scan the entire WORKS_ON table, order the data 2) Scan the entire PROJECTS table 3) Join the result of 1 &amp; 2 together in a Nested Loop (do step 1 &amp; 2 for each rows) 4) Scan the entire EMPLOYEES table to match the SSN for each row 5) Join the result of 4 to the result from step 3, exclude the result (again by looping through each row to evaluate) 6) Return the data from the selected columns (* in this case) I think you are getting hung up in SELECT *. This means "choose all columns" not "choose all rows". The rows are chosen through your WHERE and NOT EXISTS clause. Not through your SELECT *. You are building 3 separate sets of data and then SQL does a comparison between the data that does or does not exist. This might be less confusing if you start writing out your columns and not using SELECT *. SELECT * is a bad practice to get into as it kills efficiency of a lot of queries since the engine has a tough time using indexes to lookup, sort and aggregate your data quickly.
Thank you so much. Your explanation pretty much just made it click for me. I am fully aware that my query isn't great but that is what the lecturer has me doing so I had to try and make sense of it. Thank you so much again you really saved me there. Also I understand the way you redid the query and I would approach this problem a similar way, if not told I had to use double negation. Thanks!!
Pretty sure the OP left out the fact that they don't want to aggregate or group the other columns, hence why they can't figure out how to add them. Or maybe not, since they didn't provide any information at all...
I want to be able to group customeR_key by c.SITE_ID, p.PRODUCT_CATEGORY_DESC
I wouldn't qualify myself as an expert SQL slinger, just someone who's used databases for nearly 20 years. So apply salt to this answer. &gt; Q1) For every department located in Stafford, list the name of the department, last name of the department manager, and the name of the project this is controlled by this department. So I'd start with the Departments in Sheffield. We join the departments to their locations using the `dnumber` column: select department.dname from department inner join dept_locations.dnumber = department.dnumber where dlocation = 'Sheffield'; Now I have the department names in Sheffield. Now let's find the department manager. A deparment manager seems to be stored in the `mgr_ssn` column for a department. Since I'm looking for the employee last name, let's join the employee table on it's `ssn` column: select department.dname , employee.lname from department inner join dept_locations on dept_locations.dnumber = department.dnumber inner join employee on employee.ssn = department.mgr_ssn where dlocation = 'Sheffield'; Okay, so next they asked for the name of the project(s?) controlled by this department. That's hiding in the project table where each project has a `dnum`. Join it to the department table on the `dnumber` column: select department.dname , employee.lname , project.pname from department inner join dept_locations on dept_locations.dnumber = department.dnumber inner join employee on employee.ssn = department.mgr_ssn inner join project on project.dnum = department.dnumber where dept_locations.dlocation = 'Sheffield'; As /u/Dctcheng said, these are Ansi joins. If you've not been taught those, or are supposed to do those joins in the where clause, you'll have to re-write these. --- Edit: The takeaway here should be how to go about building up your query. Don't take a single stab at it and hope for the best. Build up to it.
Could you link those articles? Nothing immediately comes to my mind as to why HR would need SQL. Maybe generating reports or calculating aggregates, but those require only basic SQL knowledge. In regards to learning SQL, there is a section on the sidebar dedicated to it. I personally recommend sql-tutorial.ru and sql-ex.ru + documentation for a DBMS of your choice. All in all couldn't hurt to learn something useful.
If you do an aggregate then you must group by every column in the SELECT list that is not part of the aggregate function itself. This means that if you only want to GROUP BY cp.BILL_TYPE_KEY and c.CUSTOMER_CATEGORY_DESC then you can only have COUNT(DISTINCT c.CUSTOMER_KEY), cp.BILL_TYPE_KEY and c.CUSTOMER_CATEGORY_DESC in the SELECT list. There are ways to display additional columns but it would require you to make this entire query into a derived table.
Hi and thank you for your response, geometrix!! I like a lot of your ideas and was playing around with them but ran into a couple issues. I will add that PG acts very differently from SQL Server and I am not opposed to changing things up at all. The issue I am seeing right now is I am not only comparing to see if names are the same of different but other types of discrepancies like First/Last swap or a hyphenated named. And then if it falls into one of those other categories I need to be able to update the "Diff Type" to either 'First Last Swap' or 'Hyphenated Name'. This was accomplished through PG like below: if (last_p = first_s or first_p = last_s) then isldiff = true; isfdiff = true; ldifftype = 'Last First Swap'; fdifftype = 'Last First Swap'; I am not sure that same logic can be accomplished in a CASE statement with how the query is setup. The rest of the discrepancy comparisons I think can be accomplished but the swaps I am not sure about. Thanks again for the response you have already been a huge help!
@ symbols in table names is a big WutFace from me
The "@sbedw.world" in the table reference indicates that it is a linked database.
Yeah, Metabase can do calculated columns, but probably doesn't support as many expressions as Excel. It's also not as good at complex charting as Tableau. If something in the middle is what you're looking for, and you know SQL, then Metabase is a good choice.
Every single time on this subreddit I see that syntax, it's for university. I guess professors haven't had to use SQL in the real world in the last 20 years.
Knowing SQL is fine and dandy; but just knowing it casually and tooling around in production is just playing with fire. Especially when you've gone out of your way to exclude IT from managing the database for whatever reason. Sensitive data? Regulations only care about who is authorized and their need to be, not *who*. IT is on equal footing as far as need-to-know goes as everyone else. Do you guys have a standardized process for changes--are they audited? What about backups? Do you audit user access? Etc. An IT guy doesn't simply know SQL, any idiot can watch ten YouTube videos and query a database; but can you say without hesitation that you're doing it professionally?
I hope your business/boss knows what they're asking... There's a lot more to it than just - *Hey could you go create a DataWarehouse?* Starting with your dimension, DimCompany - You've got your 'Natural Key' - as you said, it's a composite key of the 'Group' and the 'Company'. So I'm going to continue on with the assumption that the combination of those is unique. It is your job to put **all of the descriptive columns about the unique combination in this table**. Things like GroupName, GroupAddress, CompanyName, Company Address seem to fit the bill. When you start adding things like 'LocationID', you're now talking about potentially a separate Dimension table. If you were to just have a 'LocationAddress' - I would argue you just tack it on to your existing dimension. But having things like a 'LocationID' makes me think that there are specific things about that ID that you can describe - which makes me lean towards another dimension. Your dimensions should drive your fact tables. I've made an ad-hoc example really quick: http://imgur.com/a/AtP3i Your Fact table should have it's own identity column, and then all of your measures can be inserted per CompanyKey/LocationKey/DateKey, whatever other dimensions make up your fact. 
I Appreciate this. I'm going to do it peace by peace as a practice so I can get it down. Then I'll do it as a whole query. It's just frustrating to go from PowerPoints then "here make a query for credit".
DBAs can give permission to view execution plans without requiring any additional permissions. Make a case why execution plans are important to your work. Also, understand and address any specific concerns the DBAs have. Otherwise, removing sections of a query until it is fast can be effective in figuring out what parts to work on.
I know more now than before thanks to everyone here! Now I need to remove [Duplicates](http://imgur.com/XX5pKpb). 
As above, you have to group any answers that aren't aggregated if the select column includes an aggregate. If you would only expect one answer for that column then either grouping it or adding a fake aggregate can get around this
I think there are a couple of things you can focus on even without having the exact plan. First, you should monitor the frequency of the query being executed - make sure the bottleneck is the performance and not something misconfigured or behaving in a way you don't expect. Next piece of advice: do you have any access to system tables? If you do, you can derive some of the pain points yourself by looking at the table for missing indexes. Lastly, as others are saying, getting that permission shouldn't be too hard but if your DBAs are very cautious they should consider making sure the dev server you use (if you have one) has the permission.
Solution: start hitting the servers with the shittiest sql you can muster. When people start complaining that the server is slow as balls maybe they will change their tune.
Hi, Under my research, I got to know that we should know before DBA. Below are mentioned points. 1. Basic understanding of databases 2. Working knowledge of RDBMS concepts &amp; SQL 3. Working knowledge of Oracle 9i or 10g To know more you can Read oracle.com, Koenig Solutions &amp; much more.
Use a windowed function: ,COUNT(DISTINCT c.CUSTOMER_KEY) OVER (PARTITION BYcp.BILL_TYPE_KEY, by c.CUSTOMER_CATEGORY_DESC)
That is true, you are correct, but I really didn't want to go into all of the ways the engine could evaluate a query with someone who was struggling to understand NOT EXISTS and correlation. I oversimplified a bit.
&gt; If a DBA is intentionally preventing you from doing something that makes THEIR job easier, there's some power trip issues there. That or there's some boneheaded "security" policies that the DBAs can't even get fixed. Usually put in place by people who have zero understanding of the tech.
Since the other poster also said this will be processed row-by-agonizing-row, I don't want OP to think IN or EXIST = bad writing.
 insert into table1 (key, x1, x2...x11) select key, x1, x2...x11 from table2 where key = 68 I mean, are you just asking how to insert a row where the key is 68? Or any 1 row? Or are you asking how to *UPDATE* table1 with the values from table2 where the key matches? The question is not very clear.
&gt; EDW.CUSTOMER_DIM@sbedw.world What kind of maniac names a table that? Also, how does this query have anything to do with what OP asked? 
I was trying to add my query. I am OP
Ah yes, you can use VALUES if you want to specify the literal values. Or else you can pass variables, or a select statement, etc. to match the insert.
That's fair. My way only works when you have constant text/number values through the records and aren't trying to group them. 
Honestly there is nothing like real world experience. After learning the basics, I got better by struggling through writing more complex queries to fulfill the demands of the business. If you get stuck, google hard for solutions, and then add those new tools to your SQL arsenal. I work with finance people. These days I can guess what they are going to want. Here are some sample questions that may challenge you for queries above the basics. Key metrics? QTD, MTD, YTD values? Can you sum these rows together? (aggregation? group by? Having? Date tables? Unions?) Can you make the report look like this? (format SQL to work better with your output options. Do you need flags/case statements? My super complex reports are done in crystal) Can you improve performance? (CTEs? indexes? temp tables? ) If you don't have something like that, work with AdventureWorks or whatever database you can query against. 
I recently discovered https://www.codewars.com/ It isn't directly a learning site, but it is definitely a good site to challenge yourself and to view how others have solved the challenges - picking up on nuances that will sharpen your skills.
A few thoughts. I suspect the views are more than simple abstractions of tables, and contain multiple joins and other logic. If possible, changing INNER JOINS to OUTER JOINS can have the benefit of not performing joins if a particular query doesn't require columns from the referenced table. Check that statistics are updated and accurate. A common problem is that statistics will estimate few or no records returned for a filter and choose join algorithms with slow per record speed but minimal overhead. But when the query encounters many records, the joins perform poorly. Examining execution plans can help identify these scenarios. sp_executesql, stored procedures, temp tables, parameterization, and explicit filters can each perform well or poorly. There's no general best practice that universally helps slow queries. If you can share some actual execution plans, I can probably point out specific things that could be problems.
SECTION | CONTENT :--|:-- Title | How Modern SQL Databases Come up with Algorithms that You Would Have Never Dreamed Of by Lukas Eder Description | SQL is the only ever successful, mainstream, and general-purpose 4GL (Fourth Generation Programming Language) and it is awesome! With modern cost based optimisation, relational databases like Oracle, SQL Server, PostgreSQL finally keep up to the promise of a powerful declarative programming model by adapting to ever changing productive data without performance penalties. Thousand-line-long, complex SQL statements can be run in far below a millisecond against billion-row strong tables if databas... Length | 0:50:31 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
SQL for Smarties by Joe Celko
This. Make your DBAs pull plans for you until they get so irritated that they just give you the explicit permissions needed, so you can pull your own plans. :-p
Its not the particular fields, its the concept of how to relate the zips to users. There seems to be a consensus between here and stackoverflow that something like what you have said is best. Thanks again.
Thanks for the advice!
Are you looking for something like this https://pgexercises.com/ ? Online exercises with their answers.
https://explainextended.com/ has some really fun posts. Drawing Mandelbrot s, solving sudoku, all sorts of weird stuff
Pretty much. As a dba myself, I would love if developers asked for permissions to execution plans to fix their poorly performing queries. Less head aches for me later on. 
This helps so much! Thank you!!
Thank you! I wasn't too sure about the HAVING part either haha. 
Nestled queries sounds so cozy! I may start using that.
SQL: Snuggle Query Language
Senior .NET dev (6th year) here working on these kinds of systems from time to time. Could you look at moving the performance problem away from the user and into a periodic process? Instead of trying to pull a dynamic query together, then retrieve that data, why not remove that process from the user, make it happen overnight, or hourly, or whatever suits your data &amp; users? You can place the summarised data in a separate schema, optimized for querying. The viability of this depends on: A) that your users don't require accurate to the minute (or second) data B) that the number of possible scenarios is low enough that you can run them all in the time you need to do it. If this is a problem then could you summarise the data to a higher level and optimize the table for querying using the dynamic SQL you have? If this is not possible, then you could try and do it with a temp table as you've suggested. This may or may not be a good idea depending on how much data you have. I wouldn't recommend using Linq to do this. In my experience, there's no magic bullet for these things, but you can move performance around if that option is open to you. Out of interest, you say you don't have time to re-engineer it, are you replacing it soon, or is a rewrite on the cards in the next couple of years? Because that is some technical debt you really don't need. EDIT: Removed some stupidity. 
Not a MySQL guy but maybe https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_substring-index ? SELECT SUBSTRING_INDEX('xyz@abc@123@meow@mix', '@', 3); http://rextester.com/CFBXND61305
Try WHERE NOT EXISTS. https://www.techonthenet.com/postgresql/exists.php
Any chance these articles were written by SQL training providers? It *could* be useful, but it's not likely HR would be allowed to run queries against a production environment.
&gt; "SUBSTRING_INDEX" doesn't seem to be a recognized function then you're likely not running MySQL as your thread title suggests
I'd start [here](https://www.codecademy.com/learn/learn-sql). If you can get access to a copy of a database to practice on at work (with your user restricted to only use that copy, or perhaps to have read-only access of a staging/testing database maybe) that would make learning easier. Like others have said, you should stick to `select` statements at first.
check some of the most used queries and see what the execution plan what they are doing. Most probably adding indexes or making small changes on the query to better use the existing indexes will solve most of the problems. If you are on SQL Server 2016 look at memory optimized tables Try to make the views such that they will take advantage of join elimination If you have calculated values on which you are filtering make them persisted calculated columns and index them 
It's dying on the data types. Sorry, I should have clarified in the post, I had to run to a meeting. &gt; Msg 4824, Level 16, State 1, Line 1 &gt; Cannot bulk load. Invalid data type for column number 1 in the format file "X:\LedgerFormatFile.fmt". 
I was afraid it was that but I couldn't find the table to reference. I don't quite understand why the data types are defined differently. 
Much appretiated! Thank you, it did indeed work Select Substring( potato, -10, 10) as 'DATE' From test1
Simplify that monster: "Explain the difference between a fact and dimension?". I can tell in 4 seconds if you know what you're doing.
&gt; calculated values One aspect of these views that seems to cause the most overhead is a max-date value attached to several different "statuses" within each row of the final data. I did move these to a static lookup table (inserted and updated by triggers) rather than having it joined on a MAX(*dateTime*) from joined views. That really sped things up, but it's still overall pretty bad. I'm going to take a look at a similar approach to more of the lower-level views.
Can't import arbitrary CSVs with BULK INSERT, even with a format file. Can you use a different separator and remove double quotes around fields?
Hopefully this is a good lesson in database design on why keeping data in columns and not strings is useful! :)
Eheh I'm testing out stuff, I have to learn with my mistakes, given enough time I shall understand this better.
In my experience, experience is the only thing that matters. And by experience I mean how much your boss likes you at the interview. Source: Have a high school diploma, no college degree, no certifications, consulted for professional sports teams, presidential candidates... etc.
I hire people quite often and can confirm the statement above. At least as long as they know what we are interviewing about and can answer several coding questions, then it's all about their attitude and well they would click with the rest of the team.
I would like to agree with /u/Cal1gula and /u/Paratwa - as someone who has interviewed for jobs and interviewed others to hire... the certs might get you an interview, but it's personality and base skills that make the most decisions. If you want to add the certificate - go ahead. Will it matter? It might, but it won't be the end-all-reason someone offers you a job. When companies go to hire, they have a few boxes to check off... and then they size you up with the gut. "Will this guy make my life hell?" "Is this guy going to get along with XYZ?" "Will *I* get in trouble for bringing this guy into our company?" Those are the questions they are actually wanting answers too.
I manage a team of analysts and I'm not even sure what that cert is. Much more concerned with the work someone has done, the environment they've worked in, projects they've led, and how I think they'll get along with others and support the business. If it's something easy to do, then I don't see the downside, but I wouldn't spend time on it if it's going to take away from something else. 
depends on how bad it is if your server loses power
In-memory tables are [durable](https://blogs.technet.microsoft.com/dataplatforminsider/2013/10/11/in-memory-oltp-how-durability-is-achieved-for-memory-optimized-tables/) - don't spread misinformation.
Alternatively, you can simply up the max allocated server memory as well. This will allow for longer retention in the buffer cache, thus giving you your desired results. 
Even when you're "testing out stuff", you need to use the correct data types. Read up on the types available in your RDBMS before doing anything. Another thing to learn: it's much easier/cheaper to fix a design problem early than when it hits production.
I don't see what that has to do with anything?
Nono I mean, I'm trying to learn what can and can't be done. This is done with fake data generated by a a program of mine, it's not to hit production nor do I work with SQL on a daily basis, but would like to learn, and understand the do and donts of SQL. I usually learn by messing up with stuff in a way I find fun, just so I don't get bored, this helps a bit with that.
You're right, not saying the opposite, but consider this, if one day I'm handed a DB, and they ask me to organize something by date for example, and instead of a datetime, it's a string with multiple information, I'll still have to do that job right? They won't allow me to filter their DB and alter the rows.
Is MySql query optimizer that bad?
Actually `UNION` is correct in that scenario. `OR` conditions don't produce duplicates, so `UNION` is equivalent. This is primarily useful if separate indexes can service each condition, because indexes can't satisfy `OR` conditions efficiently. edit: [Here's a demo.](http://sqlfiddle.com/#!6/2fd30/5)
So, this article states &gt;In SQL, wildcard is provided for us with '%' symbol. Using wildcard will definitely slow down your query especially for table that are really huge. We can optimize our query with wildcard by doing a postfix wildcard instead of pre or full wildcard. Which is true, however the solution was don't ask this question of the database if you need a pre or full wildcard. But say in a theoretical I need to find everyone who has a manager job title. The titles could range from "District Manager" to "District Manager IV" or "Manager of...". Wouldn't the only way to solve this be something along the lines of SELECT * FROM dbo.Employee WHERE JobTitle LIKE '%Manager%' and to optimise it you would index the column so that the query doesn't do a table scan? Or am I thinking of this type of scenario wrong?
 CASE WHEN Entry IN ('the sale was made','sold') THEN 'sold' ELSE Entry END AS Entry 
Slow down and do the join right? We can see what you're doing on your screen. Not much else we can do unless you post some code.
Schema: &gt;Table Name: person Field Type Notes person_id int(8) ÔÇ∑ Primary key ÔÇ∑ Auto-increment value ÔÇ∑ Required first_name varchar(25) ÔÇ∑ Required last_name varchar(25) ÔÇ∑ Required Table Name: building Field Type Notes building_id int(8) ÔÇ∑ Primary key ÔÇ∑ Auto-increment value ÔÇ∑ Required building_name varchar(50) ÔÇ∑ Required &gt;Table Name: room Field Type Notes room_id int(8) ÔÇ∑ Primary key ÔÇ∑ Auto-increment value ÔÇ∑ Required room_number varchar(10) ÔÇ∑ Required building_id int(8) ÔÇ∑ Required capacity int(8) ÔÇ∑ Required Table Name: meeting Field Type Notes meeting_id int(8) ÔÇ∑ Primary key ÔÇ∑ Auto-increment value ÔÇ∑ Required room_id int(8) ÔÇ∑ Required meeting_start datetime ÔÇ∑ Required meeting_end datetime ÔÇ∑ Required Query &amp; error: mysql&gt; select person.first_name, person.last_name, building.building_name, room.room_number, meeting.meeting _start, meeting.meeting_end -&gt; from person, meeting -&gt; join building on person_id = building_id -&gt; join room on -&gt; room_id = meeting_id -&gt; where person_id = 1 -&gt; order by meeting.meeting_start; ERROR 1054 (42S22): Unknown column 'person_id' in 'on clause'
Dude first thing about writing code is that it's literally a trillion and a half times easier to debug if you can actually read it: SELECT person.first_name, person.last_name, building.building_name, room.room_number, meeting.meeting _start, meeting.meeting_end -&gt; FROM person, meeting -&gt; JOIN building ON person_id = building_id -&gt; JOIN room ON -&gt; room_id = meeting_id -&gt; WHERE person_id = 1 -&gt; ORDER BY meeting.meeting_start; Now since it's formatted, can you see your errors? By the way, I used this site to format: http://www.dpriver.com/pp/sqlformat.htm
Yes I meant rewriting it. From what I can interpret from the OP, they have more than just the two values 'the sale was made' and 'sold'. I suppose they can make a big list for the `IN` clause if they run a distinct query on the column first. Heck they may be able to use `IS NOT NULL`
I've tried and can't get it to work on my home computer. We're learning mssms 17.1, so I downloaded that and the wideworld import database. I also downloaded the sql server, but can't get them to work together. I had a friend who is actually an IT guy work on it for a few hours and he couldn't get it to work either, so I've given up on getting it on my home computer.
There's also Dev essentials where you get free azure credit. You could spin up a VM, do some training at home
Hmm, without knowing what errors/troubles you were having I couldn't say what the problem is. What version of SQL? Is the SQL instance installed and available in services.msc? What OS do you use? What errors are you getting and where are you getting them. I genuinely feel getting access at home will do wonders for your learning, it is well worth getting to the bottom of your issues.
When I need to deal with complex date issues, I thank my lucky stars I spent half a day building a date table.
Basically a table with a bunch of date values that serves as a reference or look-up? Is what I'm describing something that SQL can do?
**Knapsack problem** The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items. The problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography, applied mathematics, and daily fantasy sports. The knapsack problem has been studied for more than a century, with early works dating as far back as 1897. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^] ^Downvote ^to ^remove ^| ^v0.23
Good bot.
Here's what I would do (not knowing how many rows you have - this could be a little slow if you have many rows because it uses recursion). This solution doesn't require creating a scenario table. This assumes a MS SQL only solution will suffice. I have no idea if this is possible in MySQL or other variants. Find when each distributor submitted their last invoice using query5 and your invoice snapshot structure (Query6): &gt; SELECT &gt; Query5.DistID, &gt; Query5.ratio, &gt; MAX(CAST(invoice_snapshot.invoicedate AS DATE) AS MaxInvoiceDate &gt; FROM &gt; invoice_snapshot &gt; inner join Query5 &gt; ON invoice_snapshot.DistID = Query5.DistID &gt; WHERE &gt; invoicedate &gt;= '2016-04-01' &gt; AND invoicedate &lt;= '2017-03-31' &gt; GROUP BY &gt; DistID, &gt; ratio Add a row number so we can identify each row sequentially - this is important - (Query7). &gt; SELECT &gt; DistID, &gt; ratio, &gt; MaxInvoiceDate, &gt; ROW_NUMBER() OVER (PARTITION BY DistID ORDER BY MaxInvoiceDate, ratio DESC) AS LastInvoiceOrder &gt; FROM &gt; Query6 Use a [recursive CTE or other solution found here](https://stackoverflow.com/questions/11310877/calculate-running-total-running-balance) to find the running total of the sum of the ratio going backwards through time: &gt; ;WITH MaxInvoiceCTE AS &gt; ( &gt; SELECT &gt; DistID, &gt; MaxInvoiceDate, &gt; ratio, &gt; LastInvoiceOrder, &gt; RunningRatioTotal = ratio &gt; FROM &gt; Query7 &gt; WHERE &gt; LastInvoiceOrder = 1 &gt; &gt; UNION ALL &gt; &gt; SELECT &gt; Query7.DistID, &gt; Query7.MaxInvoiceDate, &gt; Query7.ratio, &gt; Query7.LastInvoiceOrder, &gt; MaxInvoiceCTE.RunningRatioTotal + Query7.ratio &gt; FROM &gt; MaxInvoiceCTE &gt; INNER JOIN Query7 &gt; ON MaxInvoiceCTE.LastInvoiceOrder = Query7.LastIncoiveOrder + 1 &gt;) The data from the CTE will look like this: DistID|MaxInvoiceDate|ratio|RunningRatioTotal :--|:-- |:--|:-- 1|1-Apr-17|0.22|0.22 2|1-Apr-17 |0.1|0.32 3|31-Mar-17|.5|0.82 4|20-Mar-17|.16|0.98 5|18-Mar-17|.01|0.99 We want 20-Mar-17, so grab the max MaxInvoiceDate date. &gt; SELECT &gt; MAX(MaxInvoiceDate) &gt; FROM &gt; MaxInvoiceCTE &gt;WHERE &gt; RunningRatioTotal &gt;= .975 As for your dates possibly changing - you can use variables. &gt; DECLARE @TargetDateRangeEnd DATETIME = '2017-4-1' &gt; &gt; DECLARE @TargetDateRangeStart DATETIME &gt; &gt; SET @TargetDateRangeStart = DATEADD(YEAR, -1, @TargetDateRangeEnd) &gt; &gt; SELECT * FROM invoice_snapshot WHERE invoicedate BETWEEN @TargetDateRangeStart AND @TargetDateRangeEnd Edit: reread and realised I missed the LastInvoiceOrder as a column in the CTE. 
&gt; Basically a table with a bunch of date values that serves as a reference or look-up? [20 minutes well spent](https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/)
I actually sell through Amazon, and I don't think there's an exact number for visibility but I know it's important to have positive, consistent feedback (especially early on) in order to move up the chain and get that traction. Likely those reviews jumped them up the SQL search results, so I'd say it's pretty obvious at this point that they were inflating their brand to try and get more sales. Now it totally sounds like a scam to me. 
 SELECT DOCNR, ITEMNR, MIN(ROWNR) AS ROWNR FROM ORACLE_TABLE_QUESTIONMARK GROUP BY DOCNR, ITEMNR, CASE WHEN ROWNR IN (1,1001) THEN 1 ELSE ROWNR END
In a bubble, this might be relevant. But if you look at the *actual* review in question, none of this applies. I mean it's lovely to do some analysis of theoretical situations that don't exist in this context, but let's focus on reality. The reviews are fake.
Could you just create the temp table then do an insert with a where clause? That would eliminate the need for a case statement
I think you missed the point of my thread.
&gt; In a bubble, this might be relevant. But if you look at the actual review in question, none of this applies. Based upon what? I've taken a look and all say "verified purchase". This seems like OP just wants to bash a book they shouldn't have bought, a bunch of others are jumping in on the poor English of reviewers. It's buyers remorse, maybe some unconscious bias. There is nothing to suggest readers must have good English grammar skills, or pass an arbitrary review of other users. All I need to see are the words "Verified purchase" and I either trust Amazon, or I don't use their site. It's that simple.
&gt; This book composing is truly well and you can undoubtedly see every last thing about it. diverse sorts and other helpful things additionally talked about in this book. I am truly awed when I read this book. The writer made a fantastic showing with regards to and furthermore composed extremely well. This book contains demonstrated strides and methodologies on the most proficient method to utilize SQL to get to information in your social database. On the planet today SQL is the most generally actualized dialect. I am truly awed when I read this book. The writer made a fantastic showing with regards to and furthermore composed exceptionally well. I trust on the off chance that you read this book you can gain more from this. I would exceptionally prescribed to peruse this book everybody. You trust whatever you like buddy. This review is a load of fluff and clearly there to pad the stats. This one just like almost all of the others. They all read the same. I bet if you did an analysis of the writing style they are even from the same person. How many times can you state that you're "truly awed" when you read a book on SQL. Only someone fluffing their reviews would write this crap. Did you write this book or something? You seem to be really interested in defending this. edit: Also really wild how there are 5 reviews in the first 2 days, all from similar "non english speakers" then not a single review for 5 months and suddenly the same shit appears again! It's almost like someone is trying to game the system. Either you're too trusting, or intentionally obtuse.
Got you. I actually ran the product through [a site that tests reviews](https://reviewmeta.com/amazon/1540795233) to see if they're real or fake, and they said approx 30 of the reviews are fake. For example they noted that many of the reviewers also reviewed a lot of the same products - eg 10 of the reviewers also reviewed a random book on meal prep, half of those also reviewed a book on twitter marketing.
&gt; This seems like OP just wants to bash a book they shouldn't have bought I didn't buy the book? I don't know why you're attempting to bash me here, I'm just trying to warn people. &gt; All I need to see are the words "Verified purchase" The product owner can make a 100% discount code for the product and give this to the company who does the fake reviews in order for them to be "verified purchases". This is an old and very common tactic for boosting review scores. [There are numerous threads on seller central about it](https://sellercentral.amazon.com/forums/thread.jspa?messageID=3455440). The ebook is also free through kindle unlimited, so again no money has to change hands for a company that provides fake reviews to appear as "verified purchase". As I said above, [a highly respected site that analyses reviews](https://reviewmeta.com/amazon/1540795233) found over half the reviews on this product are likely fake. For christ sake, [this five star review](https://www.amazon.com/gp/customer-reviews/R1IMN1T4QMWUPB/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&amp;ASIN=1540795233) actually lifts some sentences (critical of the book) from [this negative review](https://www.amazon.com/gp/customer-reviews/RGTY676NQS583/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&amp;ASIN=1540795233) and puts them in the middle of the vague but positive word salad review. Likely at some point Amazon will realize this product is using fake reviews, but as yet they haven't and it's highly rated on Amazon, which is why i came on here to warn others.
It's you who needs to broaden your thinking with a more critical view. You should approach something like this with suspicion. Not trying to equivocate it to random data pieces you found elsewhere on the internet like... how many theoretical English speaking users could review a book on SQL. While ignoring the data showing the reviews appear to be faked. edit: Downvoting everyone else isn't going to make you any less wrong here.
&gt; suddenly the same shit appears again Yep, four to five reviews in garbled english a day for about two weeks out of nowhere. Totally not fake!
`SELECT FROM comments WHERE text LIKE %ü§ñ%` For those who want to check for themselves
Learning how to set up this up is pretty important growing pains. If you end up taking your career down this path, you may have to do this a lot depending on the size of your shop, smaller companies you are expected to wear multiple job hats and be able to do a lot of different roles based on what is needed . That being said, try to download sql server express and get that up an running before importing your database. That way if you are having specific issues with that part you'll know where you are at with the install and be able to get a specific question ( or show a specific error ) so people on here could you help you with that. 
So if I understand you correctly, you would suggest importing say, all the SysInfo.csv files from each machine into the same table and database, and then creating a child table based on fileID for more information about specific comparisons?
 As in-- Parent table: Comp1.csv has ID of 1. Comp2.csv has ID of 2. Child Table: ID:1, SysName: c1; Usernames: a;b;c; , IP: xxx.... ID:2, SysName: c2, Usernames: d;e;f; , IP: xxx.... So I could see all the info from the .csv in the child table, and refer to the parent table to find where it came from, more or less? Could this work for CSVs that have more than one entry per machine? Like a CSV that describes 3 columns (say Username, LastLogin, Domain) that have N number of rows, instead of just one like SysInfo? I'm imagining like Parent Table: ID:1, MachineName: XYZ ID:2, MachineName: ABC Child Table: ID: 1, User: a, LastLogin: yr, Domain:X ID: 1, User: b, LastLogin: yr, Domain:Y ID: 1, User: c, LastLogin: yr, Domain:Z ID: 2, User: a, LastLogin: yr, Domain:X ID: 2, User: b, LastLogin: yr, Domain:Y ID: 2, User: c, LastLogin: yr, Domain:Z So I could then select to display only ID: 2 if desired, or see if ID: 2 user 'a' has the same data as ID: 1? 
Yep. There are tools to help spot fake Amazon reviews: https://reviewmeta.com/amazon/1540795233 Interestingly, though this book has a ton of fake reviews, this book's adjusted rating (with authentic reviews) is roughly the same.
Thanks, that actually helps me quite a bit-- it also straighted a misunderstanding I had in my head. Really appreciate it
wowa... imma try this in the office tomorrow 
If any of you are looking for a good book, I can recomend [Beginning Microsoft SQL Server 2012 Programming 1st Edition by Paul Atkinson](https://www.amazon.com/gp/product/1118102282/ref=oh_aui_detailpage_o00_s01?ie=UTF8&amp;psc=1). My school used this book (and still does). Even though it focuses on Microsoft SQL Server, ANSI-SQL concepts are on nearly every modern SQL platform. I also think this book goes a lot more in depth than the one OP posted. This book is 833 pages and the one posted is only 88 pages. It covers DML, DDL, and DCL
Checked that as well and it isn't when selecting the row from the @tmp table. It only returns NULL when selecting within a UNION or UNION ALL.
I had suspected that also and double checked to be sure. The dev environment column ordinal structure mirrors the prd environment. Table structure is entirely identical.
If you don't group by then each record is still individual. Add group by productname and it should be clearer. 
Ok, thanks. So how would I go about returning one result then? Just the highest price product?
That's what max does. It returns the highest price per product you group by. If you wanted the max price for one specific product when you would need to specify that in the where clause. E.g. where productname = 'wood' 
&gt;I'm playing around in the famous NorthWind database to build up my knowledge. Question: in order to find the min or max unit price of something, along with what its called, why can't I simply query: &gt; &gt;SELECT productname, max(unitprice) FROM products; &gt; &gt;without getting an error. From what I gather it seems to want me to use GROUP BY, but I don't understand why. &gt; &gt;Thanks This would return every product name, but the max unit price of them all which would all be the same number. If you add in group by for product name then do a max it will do what you want. It seperate them by product name then takes the max of each product. You can also add in an order by price to make sure the most expensive one is on top, and then use Top 1 to get only the first result. The simplest thing is probably SELECT TOP 1 product, price FROM table name ORDER BY price desc 
So you want the highest price of any product and then to find out that that product is? 
This would best accomplished with `TOP(1)` and `ORDER BY`. SELECT TOP(1) ProductName, UnitPrice FROM Products ORDER BY UnitPrice DESC; It is possible to get the same thing with `MAX` it is more complicated and messy.
It would help if you could describe all steps that you took (maybe take them again, you might have omitted something by mistake) and provide output that you have received after every step.
No problem :)
Yes and yes, all contained in the "Viewpoint" database. 
Same result when using a temp table rather than a table variable. I'm not able to do a select into as it's across environments and don't have the ability to add a linked server unfortunately.
That's cool. I understand. Try identifying the column from the Concatenation and track it back to the Compute Scalars to see if there's any funny looking implicit converts or something. This reminds me of a query that would return 4 records with `SELECT *` but the value 2 with `SELECT COUNT(*)`. We figured out why, and like that I suspect there's a technical reason here and not a Microsoft bug.
You've created the service //Viewpoint/EventNotificationService but you've sent the notification to //Viewpoint/EventNotificationService**e**. If you send to //Viewpoint/EventNotificationService, does it show up in sys.event_notifications?
How exactly are you doing this if you have no linked servers? I don't understand how you are able to do an INSERT INTO if you can't do a SELECT INTO.
Sorry, that typo came when I was writing up my post. I've gone over everything and notification is in //Viewpoint/EventNotificationService and the Event Notification shows up correctly in sys.event_notifications. But when I drop a user, nothing shows up in the EventNotifcationQueue
I like the image at the bottom of the page [here](https://www.codeproject.com/Articles/33052/Visual-Representation-of-SQL-Joins).
I'm not sure i'm understanding correctly. I tried this and didn't get what I was looking for: select* from #test1 a INNER JOIN #test2 b ON b.ID BETWEEN 1 AND a.count the result had 0 rows
Numbers (aka #test2) should just be a table of consecutive integers, like this: ID| -:| 1| 2| 3| 4| 5| ...| The idea is if your count is one, you join to record 1. If your count is 4 you join to records 1, 2, 3, and 4, duplicating your record 4 times.
It worked! Thank you!
Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Free, for beginners, well structured, keep things simple, with online exercises.
UNPIVOT is the way to go here I think: CREATE TABLE #Products (Product int, Material money, Labour money, Overhead money) INSERT INTO #Products VALUES (1, 10, 5.5, 6.5) INSERT INTO #Products VALUES (2, 15, 6.25, 7.8) SELECT * FROM #Products SELECT Product, Cost, Cost_head FROM ( SELECT Product, Material, Labour, Overhead FROM #Products ) AS prod UNPIVOT ( Cost FOR Cost_Head IN (Material, Labour, Overhead) ) AS up; RESULTS: Product Cost_Head Cost 1 Material 10.00 1 Labour 5.50 1 Overhead 6.50 2 Material 15.00 2 Labour 6.25 2 Overhead 7.80
That seems to be exactly what I need. I'll give it a try. Thanks for your help.
I was going to do something complex with PIVOTs and dynamic descriptors, but then I realized your formatting was wonky, and it's actually a simple group by. SELECT FruitName, FruitDescription, COUNT(*) FROM Fruit GROUP BY FruitName, FruitDescription
Thanks !
Not sure this worked. I end up with count as 1 because each name and description is a unique combination. Wanted to see in a row how many descriptions exist for a name.. not sure if that makes sense. Back to the thinking board
I got it to work by creating two queries and referencing the first query's filtered results to the original table. Just wish there was a way to consolidate it down to one query 
this article is not very inspiring. It screams of look at me I just learned SQL.
My Teradata is very rusty. Here's my educated guess: SELECT SUBSTRING(CAST(&lt;timestamp_col&gt; AS VARCHAR(64),1,POSITION('-' IN CAST(&lt;timestamp_col&gt; AS VARCHAR(64))) edit: Can you tell I'm writing too much bash? educated-guess -&gt; educated guess
What is an untranslatable character?
The only time I've seen a database give errors for invalid characters are when trying to create xml or something. You could try accent insensitive (AI) collation. SELECT CONVERT(char(1),N'„´ï') AS [?] -- results in ? but no error SELECT a.Test, b.Test -- "AI" collation considers the two strings equal FROM (SELECT 'TEST' COLLATE SQL_Latin1_General_CP1_CI_AI AS Test) AS a FULL OUTER JOIN (SELECT 't√©st' COLLATE SQL_Latin1_General_CP1_CI_AI AS Test) AS b ON b.Test = a.Test
Word on the street is that people who use spaces get paid more than people who use tabs. I'm converting. (/s btw)
Can i combine this with other part? or maybe have it For example i want to change a parameter date AND FDATE BETWEEN '07/01/2015' AND '06/30/2016' rather than updating these every year i would love to have it as '07/01/(@current year-2)' AND '06/30/(@current year-1)' something like set @x int = RIGHT(YEAR(Creation_Date), 2) YY; SELECT * from table where date&gt; '07-FEB-@x'
This is how I typically do it, http://tomaslind.net/2013/12/26/export-data-excel-to-sql-server/ Recommend creating DSN connections in ODBC, makes the connections strings testable and easier to understand. Excel is typically 32 bit, so in Win7 the shortucut is C:\Windows\SysWOW64\odbcad32.exe 
What data types are the fields you are joining on?
what value does the primary key bring? if its already the clustered key and is unique (say an identity column) then there is no value of it being a PK (it already is in everything but name). If its not the clustered key, is that all you need from it? Would a non clustered index be enough? Or do you just need to make sure its unique? All of this is just a long way of saying, you sure you need to do this? I would say backfilling the table could be the best way to go about it, probably just need to tune your backfill. But I would say the biggest question here is understanding the statement "The field that should be the primary key"
[Image](https://imgs.xkcd.com/comics/compiling.png) [Mobile](https://m.xkcd.com/303/) **Title:** Compiling **Title-text:** 'Are you stealing those LCDs?' 'Yeah, but I'm doing it while my code compiles\.' [Comic Explanation](https://www.explainxkcd.com/wiki/index.php/303#Explanation) **Stats:** This comic has been referenced 924 times, representing 0.5717% of referenced xkcds. --- ^[xkcd.com](https://www.xkcd.com) ^| ^[xkcd sub](https://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](https://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](https://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_djj3cyd)
It's a heap table, no indexes or anything.
850 GB heap. you either inherited a big problem or there are many things wrong here. 
If the table is by some miracle partitioned, you can perform operations one partition at a time (with some caveats). Also, you can paralellize the SSIS (though not efficiently because it's a heap) to save time.
So like: SELECT FruitName, COUNT(DISTINCT FruitDescription) FROM Fruit GROUP BY FruitName ? If not could you try reformatting your example output so we can see what exactly you're looking for?
Knowing which RDBMS it is might be useful too.
Converting zip to tar does seem like a step that might have gone wrong. How did you do it? 
Most of these seem reasonable but this &gt;You would think that by now most people would understand that having four monitors isn‚Äôt helping. is idiocy. Having to tab back and forth between different windows to copy and paste is idiotic. There is multitasking how most people do it (using multiple apps for a single task) and multitasking that the "studies" refer to (actually doing multiple tasks at the same time). Most people with multiple monitors are not doing multiple tasks, they are just using multiple tools for a single task. End Rant.
you could also make some special views for the HR people and have their spreadsheets hookup directly to the backend ;)
do OBDC connections hog more data than say..a regular user using a python script or SQL gui tool to grab data? im not a DBA, but do use SQL at work quite a bit in addition to writing code but I feel like DBAs are their own breed of people 
They don't take full advantage of the optimizers, they're unreliable, and mostly unpredictable. Regular users (developers) of my databases do not get to query directly the production database (at least not via their applications). They ask for data, I give them a stored procedure to call. They ask for DML, I give them a stored procedure to call. I'm a development/application DBA, so I actually am more code/optimization focused rather than hardware/backup focused. I'm all for letting developers and users do what they want, as long as it's not a database I have to support. If they can support it on their own, then I'll help them set it up and they can go to town. However when I have to support it, and have to take PCI, PII, RTO, and RPO into consideration, they're getting what I deem is acceptable. I was a developer myself for a number years (both .net and database). After seeing what some of my co-workers did, I'm fully confident that my reservations are justified.
Real time is not a real word. You're talking about an interval or schedule, which is not real time. Do you need the data by the millisecond? The minute? The hour? Also, you should be having the data exported to a database (of yours) -- where your model lives. Not doing it in Excel. You will have many problems in the future.
I consider my greatest attribute is that I make more mistakes than my peers. As a result of having personally encountering so many problems, solutions come very easily. There is a co-worker that is particularly timid/careful who I am trying to break of the habit. I point out all my mistakes when helping him. Something like this: "Let's comment out this join to see if that's the problem." *F5* *error* "Now all these columns are invalid, so we'll comment those out." *F5* *error* "Ok. The outer query also had some columns reference." *F5* *running* "That's taking to long." *cancel* "Ahh, This join condition is referencing the wrong table." *F5* "That's better, but I expected more rows than that. If we order by the date we can see if the date range is wrong." *F5* "Yep. We're missing an entire day because our filter is midnight, but our date has a time component." "Let's add a day and subtract 3 milliseconds." *F5* "If we only subtract 1 millisecond, it will round up to the next day." *completes* "Now to add the join back in. If we group the sub query we don't have to group the main query." *F5* *error* "Oops. Forgot to take out the aggregates; don't need these anymore." *F5* "See how much faster that is. Let's add in the other columns we removed." *F5* *error* "Oh right. We need to add these two columns into the subquery, too." *F5* *error* "I expected that. They're not part of the group by, but it's best to just wrap them in a MAX." *F5* "The reason is we want the group by to match the join condition so it doesn't expand the data set." *completes* "And we're done. You see, I must have made 10 errors in 5 minutes. Stop second guessing yourself and just press F5."
I'm not sure what you're attempting to do here: INSERT INTO admin_user_session (id, session_id, user_id, status, created_at, updated_at, ip) VALUES CREATE TABLE authorization_role ( role_id int(10) UNSIGNED NOT NULL COMMENT 'Role ID', etc etc Normally an insert statement is to add data to a table, so you would go something like INSERT INTO admin_user_session (id, session_id, user_id, status, created_at, updated_at, ip) VALUES (1,2,3,'Status', '2017-06-29', '2017-06-29', '127.0.0.1') It doesn't make sense to say "insert this into the table: create a table" which is what you're attempting to do. What are you trying to put into admin_user_session ?
Thanks but not working :( 
One other option I have is the following: SELECT ur.lang, count(DISTINCT ur.id) as count_of_reviews, eh.toolqueue as toolqueue, date_trunc('hour', ur.tssubmitted) as month_of_review, EXTRACT(DAY FROM ur.tssubmitted) as weekday, FROM t_userreview ur JOIN t_userreviewedithistory eh ON ur.id = eh.userreviewid and eh.toolqueue in (1,2, 10,11,12,21,39,45,57,58,70,71) where ur.tssubmitted BETWEEN '2017-05-01' and '2017-05-02' and ur.pagetype in (1,2,3) and ur.lang in ('fr','it','es') GROUP BY 1,3,4,5 ORDER BY 1,3 What i need is to add a CASE function, which I thought would be straight forward but it isn't :/ 
 select p.EMPLOYEE_CODE ,p.EMPLOYEE_NAME ,p.OFFC [TK Office] ,sum(t.BASE_HRS) [Base Hours] ,datename(mm, t.TRAN_DATE) from blah blah blah group by datename(mm, t.TRAN_DATE) Be aware this is going to ignore years: if your dataset includes more than one year then you're getting all of the e.g., Januaries from all years added together, which you will need to fix either by excluding other years in the WHERE clause or adding Year(t.TRAN_DATE) to the SELECT and GROUP BY clauses.
oh dear
Ok, got it. concat(DATEPART(YYYY,t.tran_date),substring(convert(char(10),t.TRAN_DATE,101),1,2)) Now my values are like 201701, 201702, 201703 etc. Datepart was not giving a leading zero for the single digit months and....had to track it down. 
That was the ticket. Ended up with: concat(DATEPART(YYYY,t.tran_date),substring(convert(char(10),t.TRAN_DATE,101),1,2)) To get the year and month with leading zero, then concatenate the columns
How would you write that?
 SELECT SUM(Column1 * Column2) as TOTAL ,(SUM(Column1 * Column2) * SUM(Column3)) as TOTAL2 FROM Numbers http://sqlfiddle.com/#!6/c06da/11
Newer versions of SQL Server, I think 2008 or 2012 add `eomonth()` function, which returns the last day of the month for a given date. If the year matters, this would be a better fit than `datepart()`. Update: This is a 2012 feature.
 to_char(yourdatefield, 'day')
Might want this to happen when DaysOut is &gt;= 90. Not just = 90. Also, you are adding 10 to the fee, not setting it to 10. Big difference. Finally you are setting yourself up for errors with that update statement. Try something like the (untested) trigger below. Just assign the :new value. Don't write an update statement. CREATE OR REPLACE TRIGGER late_fee_trg BEFORE INSERT OR UPDATE ON BOOKS FOR EACH ROW DECLARE BEGIN If :new.DaysOut &gt;= 90 then :new.fee := 10; End if; END; 
Could you elaborate more on the optimzers? apparently there is more to a SQL query than just "SELECT * FROM myTABLE" lol...
Dont know much about SQL server but i can't seem to find a distinct clause in your query. Try to use DISTINCT in A.[yearweek]. My apologies if I am wrong.
Is your database set to allow openrowset? Is the account setup to just read? Perhaps it needs write access . Are you trying to copy with a delimiter so excel knows whats a column name and what's the data? Look into the sqlcmd bc ... i forget the actual command. Im sleeeepy sorry Im used to tsql so hope this gets you in the right direction.
Look into running the output clause in your query for testing. Find what the errors are if u can... maybe turn off error checking . I think by default its set to ten errors and then the query will fail.. 
I'm learning with the purpose to further develop my knowledge of Django, and web developing, this could become a good asset in the future. I have no idea if something like this could happen in the future, but meh I messed up coding my program, it ended up generating strings out of files I had of my server, and instead of deleting the DB, I decided why not try and find some solution to the problem, to filter information out of a string like it was a int, or a datetime variable.
There isn't a `BOMONTH()` function, so if you want/need the first day of the month, you can just take `EOMONTH()`, add a day (to roll over to the first of the next month), and then subtract a month. select dateadd(month, -1, dateadd(day, 1, eomonth(order_date))) as order_month Or you could do it the old, pre-SQL Server 2012 way: Determine how many months have elapsed since day 0, and add that number of months to 0: select dateadd(month, datediff(month, 0, order_date), 0) as order_month
Hey Ram, Thanks for the reply. It is just this table. I tested other tables and they worked. So it is just this table. I have write rights. What do you mean by copy with a delimiter and look into sqlcmd? PS im in oracle
I don't think so.
Yes, I can agree that using 4 monitors for one task is different. I'd like to know where you get your data that "Most people with multiple monitors are not doing multiple tasks". What I've witnessed is that the 4 monitors are used for things like email, monitoring software, coding, and social stuff like twitter/facebook. Those are different tasks, IMO. 
Well, that seems fair. Thanks for the comment, and for not being "that guy".
Yes, that's a decent iterative approach. I find iterations like this, where you make a small change and measure the output, are far more effective than making a lot of changes at once and trying to debug from there. 
I've learned that common sense isn't so common, thus the reason for me sharing the list, Thanks for the comment. 
What do you mean that kind of helped? I linked you to the exact function you need to use... did you even click it? We're all asked to do things every day that isn't part of our normal job. That's how you get new skills at most jobs. Before we had the internet it was a lot more difficult to learn things. Now you have all the information in the world available to you in a few searches. Spend more time reading/researching and less time asking for other people to write code for you on reddit...
That's great to hear. What industry are you in (no need to say the company name if you don't want). I'm just curious to know how the use of multiple monitors might vary by job role as well as industry. I assume it does, but now want some data to see if it is true. Thanks again for the comment. 
This may not be the answer you are looking for, but why not do it all in a script task with c# or vb? You have query the table and parse through any conversions or validations. Then you can run the approprate commands to create the database yourself. 
In my SSIS experience, the issue was that the first error was the "true" error and the second was that the max number of permitted errors was exceeded when the first exception was thrown. If that's the case in your situation, I think there is a "MaxAllowedErrors" property of a package that you can set to -1 to allow all errors. 
Point taken, I could have worded it better. I'll keep it in mind should I reuse the content at some point. Thanks for the feedback. 
Ok, I think I see the mistake (i've finally looked at the page that you've linked), you are supposed to extract the zip (the tar is inside the zip archive), not convert it to tar. 
I had a rummage around the properties of the package and a data flow task. Whilst there were error count properties they didn't do anything to the number of issues after changing the mapping to something that causes issues. I think I'm just going to have to live with this.
It's just a shortcut name for a table. When you are joining multiple tables that might have the same column names, it needs to know which you are referring to. A lot of the time it is just saving typing instead of using the whole table name, but sometimes it mandatory, like when you join to the same table twice, so now you have to distinguish between the two.
It is an alias, so you don't have to type out the whole table name. Say table 1 and 2 have a column with the same name, you can alias the tables, table1 as p and table2 as s, then call the column using that shorthand: P.column and S.column
Ohhhhhhhh..... So when they put "FROM HumanResources.Employee AS e" they are specifying not only they are taking data from that table, but also that the table specified will be denoted e for this query?
It's called an alias and it's a name you can give tables, columns or views. It's really helpful with long table names etc. https://en.wikipedia.org/wiki/Alias_(SQL)
Yes, the two are interchangeable for that query
It makes things a lot easier to read. For example: SELECT [AdventureWorks2014].[HumanResources].[Employee].[BusinessEntityID] ,[AdventureWorks2014].[HumanResources].[Employee].[NationalIDNumber] ,[AdventureWorks2014].[HumanResources].[Employee].[LoginID] ,[AdventureWorks2014].[HumanResources].[Employee].[OrganizationNode] ,[AdventureWorks2014].[HumanResources].[Employee].[OrganizationLevel] ,[AdventureWorks2014].[HumanResources].[Employee].[JobTitle] ,[AdventureWorks2014].[HumanResources].[Employee].[BirthDate] ,[AdventureWorks2014].[HumanResources].[Employee].[MaritalStatus] ,[AdventureWorks2014].[HumanResources].[Employee].[Gender] ,[AdventureWorks2014].[HumanResources].[Employee].[HireDate] ,[AdventureWorks2014].[HumanResources].[Employee].[SalariedFlag] ,[AdventureWorks2014].[HumanResources].[Employee].[VacationHours] ,[AdventureWorks2014].[HumanResources].[Employee].[SickLeaveHours] ,[AdventureWorks2014].[HumanResources].[Employee].[CurrentFlag] ,[AdventureWorks2014].[HumanResources].[Employee].[rowguid] ,[AdventureWorks2014].[HumanResources].[Employee].[ModifiedDate] ,[AdventureWorks2014].[HumanResources].[EmployeePayHistory].[BusinessEntityID] ,[AdventureWorks2014].[HumanResources].[EmployeePayHistory].[RateChangeDate] ,[AdventureWorks2014].[HumanResources].[EmployeePayHistory].[Rate] ,[AdventureWorks2014].[HumanResources].[EmployeePayHistory].[PayFrequency] ,[AdventureWorks2014].[HumanResources].[EmployeePayHistory].[ModifiedDate] FROM [AdventureWorks2014].[HumanResources].[Employee] JOIN [AdventureWorks2014].[HumanResources].[EmployeePayHistory] ON [AdventureWorks2014].[HumanResources].[EmployeePayHistory].[BusinessEntityID] = [AdventureWorks2014].[HumanResources].[Employee].[BusinessEntityID] And this is just a select statement! It can be rewritten: SELECT e.[BusinessEntityID] ,e.[NationalIDNumber] ,e.[LoginID] ,e.[OrganizationNode] ,e.[OrganizationLevel] ,e.[JobTitle] ,e.[BirthDate] ,e.[MaritalStatus] ,e.[Gender] ,e.[HireDate] ,e.[SalariedFlag] ,e.[VacationHours] ,e.[SickLeaveHours] ,e.[CurrentFlag] ,e.[rowguid] ,e.[ModifiedDate] ,h.[BusinessEntityID] ,h.[RateChangeDate] ,h.[Rate] ,h.[PayFrequency] ,h.[ModifiedDate] FROM [AdventureWorks2014].[HumanResources].[Employee] e JOIN [AdventureWorks2014].[HumanResources].[EmployeePayHistory] h ON h.[BusinessEntityID] = e.[BusinessEntityID] Not only is it much easier to read, you can make intelligent aliases for tables that are otherwise funky. Take a (fake) example from the MS Dynamics GP database: SELECT acct.actdescr AS 'Account Description' ,vendor.vendname AS 'Vendor Name' ,order.sopnumbe AS 'Sales Order Number' ,checkbook.chekbkid AS 'Checkbook Number' FROM SOP10100 order JOIN RM00101 customer ON order.custnmbr = customer.custnmbr JOIN GL00100 acct ON acct.actindex = order.actindex JOIN CM00100 checkbook ON checkbook.chekbkid = vendor.checkbkid If you have table names that are not representative of their data, you can alias them so the query is readable by giving those tables actual names. In the above example, the aliases make an otherwise completely unreadable query (at least to someone who doesn't know the schema) into something that pretty much anyone can understand. The first time you have like 100 columns you need in a query and you start typing out or copying and pasting all the table names you will realize that a) Intellisense is one of the best things Microsoft ever invented and b) table aliases will save you hours of typing the same shit over and over.
If you redirect the error rows to a table with really generous data column types , you can capture the bad rows and then manually go through and try to figure out where you need more validation
Sure, just use two queries is the simplest way.
In your first example, did you convert to e and h in that code or somewhere else? I couldn't spot where you put the AS unless it's indexed elsewhere? Also, maybe a stupid novice question, but I literally just started studying SQL yesterday so pardon this question... Is a Query actually creating/altering anything? Like is it creating a brand new table or set of data? or is it just returning what you're looking for? I suppose I'm confused on that as well. 
Alrighty, thanks a bunch! :) Edit: Changed my code up, works like a charm. Cheers!
You can use AS, or not use AS, for table aliases in most (all?) SQL languages. It's the same thing. Yes, to answer your question, for that query SQL will recognize those names as their underlying table names. Just like when I log into reddit it recognizes my alias Cal1gula and not my real name.
Basically you are doing the opposite of everyone else and making it easier for yourself but no one else that has to support you. Would not want to have to read your code... How could it be easier to understand that addresses = x, profiles=y and credit history=z? Instead of "add", "prof" and "cred"?
What do you do if you have to join to the table 10 times? ch1, ch2, ch3, etc? What if you have sub-queries within sub-queries of repetitive ch joins? ch11? ch12? ch13? Using sequential schema which changes (i.e. ABC inside, XYZ outside) lets you immediately see where something is coming from so you can investigate the table for a new column / condition to join on, etc. It makes debugging code for someone who is not intimately familiar with the tables so much easier. 
Unless you have some sort of internal naming scheme going on then it doesn't matter. How do you know that the first cte in your proc is b1 and not just b or a or z or x or y? You don't, you have to look at it every time you go back to that code. Or you could name it cte1. Then you know exactly what it is from the alias. Just seems like a waste of time to me, but again, to each their own.
I can see what you are saying with your point about it being easier when working with so many different tables. But how exactly do you define aliases? I'm not seeing that anywhere. In my example that I posted, they define the alias right in the beginning. But in others code, I'm not seeing it to find anywhere I'm just saying that it is simply abbreviated. 
Then I would make it something descriptive like... chsums, chtotals, chyear, chmonth... so that way the next time I read it and understand "oh I made this subquery for the credit history totals, this one for yearly, this one for monthly". Instead of having to read the entire thing and go "ok, a is my summed data, b is my yearly, c is my monthly, and that joins to x which is my credit info cte and..." etc. I don't like relearning my own work.
It is arbitrary, sometimes I go back and re-edit code. For examples lets take a simple: select * from ( select * from a ) x left join ( select * from b ) y left join ( select * from c ) z I might go back and change these if I have to add an additional sub-query, such as: select * from ( select * from a ) w left join ( select * from b ) x left join ( select * from c ) y left join ( select * from d ) z I tend to write code roughly at first so that I can give it a chance to mature, then I go back and retype it in CAPS in various places and get it ready for production. That is usually when I decide what will be what, because I can understand how things relate... so I might have a complex function I name as z1, and it's always going to be z1... and then I notate it in the code so the next person to come along can understand.
&gt; FROM [AdventureWorks2014].[HumanResources].[Employee] e &gt; JOIN [AdventureWorks2014].[HumanResources].[EmployeePayHistory] h He skipped the `AS` on these lines, at the end. With the `AS`: &gt; FROM [AdventureWorks2014].[HumanResources].[Employee] AS e &gt; JOIN [AdventureWorks2014].[HumanResources].[EmployeePayHistory] AS h I should denote that either practice is completely acceptable. They function identically.
So wait... you don't HAVE to have the AS? SQL just knows if you put a letter after a huge thing like [AdventureWorks2014].[HumanResources].[Employee] that you intend to alias it as that letter? 
Yep! SQL is lenient on some things. SQL is not case sensitive, nor is it space or line sensitive. So this code: SELECT * FROM TABLE Is also the same as: SELECT * FROM TABLE And also select * from table and SelECt * FroM TablE You get the picture... So this syntax: select * from table as t also functions the same way as this: select * from table t The SQL engine understands that you referenced the table, and then the text immediately afterwards is the alias, whether you use AS or not. Some arguments are optional and some are required. Also, there are a lot of minor variations between database engines too. Oracle SQL has some functions that MS SQL does not. Same with MySQL and PostgreSQL.
You've got a strong opinion about everything, don't you?
I tend to find myself apart from DBAs/IT in what I ask for because my role involves analytics, modeling, etc. In addition I try to annotate my code with the concept that it will be inherited by someone once I'm gone and have moved on to a new role, and that those people may not understand the tables as well as I do. This is how I prefer to inherit code, so this is how I code.
Presumably you've got a query pulling this data. Just add something to the query along the lines of WHERE eventdate &gt;= getdate() The getdate() will vary depending on your platform - e.g., now() for MySQL, CURRENT_TIMESTAMP for Postgresql, etc.
thank you! my other question is the date format is automatically YYYY-MM-DD, is there a way to change it to DD-MM-YYYY?
As everyone said, it is a table alias, but please do everyone else a favor who might have to read your code - don't use single character aliases. Gotta have at least something to make it clear what it is.
Yes but the detail depends on which platform you're using. 
Dates (when stored properly) are not "formatted" in the database at all - they're a binary format. Formatting the date for the user is done at the presentation layer. If you must do it in your query (which in most cases like this is not necessary), look up your RDBMS's date-formatting functions.
I thought it was that, too. I did unzip the .zip file and: http://imgur.com/a/JWtkj ...no .tar file in there :(. I tried restoring from that restore.sql file and that didn't work, either.
&gt;:( [Here is a picture of a kitten to cheer you up](https://www.vets4pets.com/_resources/assets/inline/full/0/236990.jpg)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/L9S3gZ3.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[state_of_imgur](https://np.reddit.com/r/u_imguralbumbot/comments/6i1huv/imgur_has_gone_to_shit) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20djn6xve) 
After thinking about it I am more confused. I selected TYPE in my database as DATE for column event_date. It works but is formatted YYYY-MM-DD which just looks awkward to me. I want to sort the concert list by date. Yesterdays concerts won't be there or will maybe go in to an archive. Today's concerts will display and below the tomorrow and the next day and so on. I tried sorting by date with it at YYYY-MM-DD but that didn't seem to work right so now I wake up wondering 1. How can I handle the date problem. I just want it to not look like YYYY-MM-DD and I want the date sorted the way I had above. 
&gt; which in most cases like this is not necessary upvote
Idk, when I download the zip, the tar is inside... perhaps your unzip program is trying to be too clever and since it sees another archive inside the zip, it continues and untars it?
Depends on the platform. Tell us which platform you're using (MS SQL/Oracle/MS SQL/MySQL/PostgreSQL/etc) and we can answer the question. Also helps to post the query you've already got.
Good luck i dont know oracle. 
You could try making a charindex / count the lenght of characters till you get to a number then minus the len of those numbers to get a start of what should be a date. Then tale the left 10 characters assuming the date like 10/10/2010 ect. 
Agree but if you want to play with it have it setup views for u. You can then access the views with read only permissions and not havevyo worry about all the sensitive data assuming they try to mask and limit your access.... ass u me ing
I did something similiar I believe, I created a substring, and filtered the last 10 characters, Select Substring( potato, -10, 10) as 'DATE' From test1 This solved the problem!
I'm using php for my form and to display data and MySQL for my database. Sorry. 
Why not use php to format the dates?
I'm learning still so I don't know how. People enter the date on the form of the event and then it's sorted. 
Be extremely careful with situations like this. They may not even have the intention of ever bringing you on board and may be looking to exploit for free work. I'd ask for at least some sort of Compensation 
I think they are trying to exploit free work. But even if that's the case, won't it be good on a resume?? It's experience still, right? I don't think they have an intention to hire. I think they just want free work. But I need a foot in the door :P
It might look good, but there are a ton of entry level analyst positions that are paid. 
That is how I broke into analytics coming from an IT background. I did small side consulting gigs for about 2 years while working another job, some of them paid, some of them unpaid, then I landed my first gig.
In my experience, this has not been the case. I live in Florida. Most positions marked entry-level expect at least two years of experience‚Ä¶ The requirements are more stiff here because of it being a retirement state. I've applied to several "entry level" IT rolls, and even had one of the recruiters contact me telling me that I didn't have enough experience for what they were looking for because they want someone who has medium to advanced SQL knowledge. For some reason it seems like down here, they tend to put the requirements of a mid-level analyst onto an entry-level job posting so they pay you as if you are entry-level but expect you to have the knowledge skills and abilities of someone who is mid to advanced level. So coming right out of college, you are at an immediate disadvantage
Here is the code (just simple trying to figure out the date format) People will fill out the date on a form which will be entered in to the database.I used DATE as the function. https://pastebin.com/VTpasnSz is the code http://hilifellc.com/pheer/testing.php is the outcome. I would really like it to be the opposite. DD-MM-YYYY 
Hey if you don't mind and want to get the experience and have the time to work for free, I can't argue with that. Do what you feel you should do. Experience is experience and it may be a great opportunity for you to get your hands in a real life production environment.
Honestly my only alternative is to work in retail at some company like Staples or Office Depot. Those are the only jobs I've been qualified for and been offered. But those don't help me get a position as a data analyst or any of that sort of thing. Unfortunately they require a bachelors degrees at least and then some experience in programming and quantitative areas after. But no, I definitely don't mind. I know the owner of the small business because we went to high school. This would be a purely experiential learning experience that I have opted in for. It's not like he's taking advantage of me. I just want to help out and gain valuable experience. As an aside, I do owe him a little bit because he totally served as a reference for me several times and gave one of the most stellar reference as ever and help me secure several jobs in the past. So having him as a reference and helping him out would definitely be beneficial. Because for my first serious data analysis job, he would be a stellar reference for it. 
Really, it's how you present yourself. I've seen the most bottom level employees with terrible work ethic sell themselves like gold, and land a great job. Similarly, I've seen very hardworking professionals sell themselves short, and end up with far less than they deserve. For example... If you're as forthright in an interview as you are here, it won't look good. You should never say things like "my previous minimum wage office job." It only serves to degrade your reputation before they know a thing about you. If you think the pay is less than what you're worth, negotiate for it. Here is how the conversation can go: "I've self taught myself SQL and have been working with it for over a year. Is there any flexibility in the pay for this position? I feel that my skills warrant a little bit more pay for my knowledge and expertise." Lastly, any other IT skills you should pick up on are the ones SPECIFICALLY stated in the job posting. If they say they prefer someone with SAS, Minitab, SSRS, R programming... Immediately get started on those skills. It shows that you are willing to go the extra step to land the position. This is my two cents. 
edit: see below comment, much more accurate.
T-SQL or Transact SQL is the dialect of SQL shared by Sybase and Microsoft. It was born out of joint development done between both companies in building what would become Microsoft SQL Server and Sybase Adaptive Server Enterprise (ASE). After deciding to go their separate ways Microsoft kept the Microsoft versions of the product for SQL Server and has continued to refine and expand. While they share a lot of the same features, Sybase and Microsoft's versions have deviated as Microsoft has completely rewritten MSSQL. Another way to look at if is every major DBMS has its own implementation of SQL and it's extensions to the SQL language to deal with certain internal structures of the respective SQL engines. They all are the same language but with different dialects. For example, the UPDATE clause for SQL Server when updating multiple columns does not follow the convention of either the SQL standard nor some of the other DBMSS, also you don't really see FROM clause joins in other DBMS UPDATE statements. The major dialects are really T-SQL, PL/SQL - Oracle, ANSI SQL (the ANSI standard). Each DBMS has their own dialect but these are the three major. Also, many of the DBMS try to implement the others. For example, DB2 has a mode that runs something like 99% of all PL/SQL code to make it easy to port apps over.
It's not the default SQL. ANSI SQL is the "default" SQL, if you could say that. T-SQL is the SQL dialect for Microsoft SQL Server and Sybase. PL/SQL is the SQL dialect for Oracle. OP: There is ANSI SQL, and then each vendor creates their own, extended version. T-SQL is one of these. You can choose to write ANSI-compliant code, or use the specific extensions to T-SQL, PL/SQL, etc.
Thanks, much better reponse. Deleted mine so people can see yours.
&gt; I'm learning still so I don't know how. So look it up. I'm sure you can find an answer with Google quicker than you'll get an answer from people here.
Try changing $sql = "SELECT * FROM events"; to $sql = "SELECT DATE_FORMAT(event_date, '%d %m %y') as event_date, bands, venue FROM events WHERE event_date &gt; now() ORDER BY event_date asc";
&gt; So look it up. I'm sure you can find an answer with Google quicker than you'll get an answer from people here. I appreciate the advice but like I said, I am learning and I did Google it but some of the things I found on Google were confusing. When I request help here people don't have to answer if they don't want to (or reply for that matter) and a lot of the info I found on Google was old. I truly appreciate the knowledge I gain from the members here and the other subs I am a member of but nobody has to help if they don't want. Pretty much any question asked could get the response "google it" but this is more personal and I have met a few nice people who were willing to walk me through or teach me because I want to learn so I will understand why things work the way they do .
Thank you very much! That did the trick! I couldn't find anything on Google that would work. I really appreciate your help and learned something about formatting the date. Thanks again!
Don't really know your database but a left join on transaction_type and month_id doesn't seem right. 
That was it!!! Thanks a lot. It was the join, I shouldn't have joined my first part of the query, I should have done this (no join necessary): WITH monthly_volume AS( SELECT month_id AS mv_month_id, COUNT(*) AS mv, transaction_type FROM test_data GROUP BY month_id, transaction_type ORDER BY month_id, transaction_type ASC ) SELECT monthly_volume.transaction_type, REGR_SLOPE(mv, mv_month_id)::FLOAT AS mv_slope, REGR_INTERCEPT(mv, mv_month_id) AS mv_y_intercept, CASE WHEN REGR_SLOPE(mv, mv_month_id) &lt; 0 THEN 'Negative' ELSE 'Positive' END AS mv_trxn_volume_trend_all_months, (REGR_INTERCEPT(mv, mv_month_id))+((REGR_SLOPE(mv, mv_month_id)*((MAX(mv_month_id))+6))) AS mv_model_6_month_forecast FROM monthly_volume GROUP BY monthly_volume.transaction_type;
I've been working on software development for a year with no relevant career experience and no degree. I wanted to be an Android Developer but now I think I want to be a DBA. I'm studying for the Oracle Associate 11g test and assuming I pass it, I'll have: *Oracle Database 11g Administrator Certified Associate Certification *Google Associate Android Developer Certification *Advanced Google Analytics Certification Do you have any advice on what I should do to get my first position in the industry?
Can you post your query?
DESC Employees; I'm literally just carbon copying what the guys doing in the video and it's not working. Which is leading me to believe that he is using my SQL. I mean it does look very different than Microsoft SQL Server but he never really says what he is using...
What he's doing is looking up the columns of the table. DESC as in describe. This is specific to MySQL. You can search for "list the column definitions of a table" for whatever database you are using to find the equivalent feature. The standard use of DESC is to order a query in descending order. `ORDER BY MyColumn DESC`
That's an Oracle method. You probably want to use EXEC sp_columns.
sp_help &lt;tableName&gt; in MSSQL
Ohhhhhh no wonder half the stuff I learn there doesn't work xD it's MySQL playlist :P 
You don‚Äôt have a salary value. Unless it‚Äôs 55000, which looks like a zip code part of the address since you are missing a ‚Äò after street and before 55000.
You're missing a closing ' on your address in the values clause. Its taking the coma and 55000 as part of the address. 
Heh, you're quick. That was an accident. That wasn't the error I was getting. I fixed it now. It's that MSG 102 thing that's actually getting me. Hm I think I fixed it actually, even the MSG 102... Tricky part is all of these asterisks that have to go around strings. Also, why is there a number column that goes 1, 2, 3, and then an ID column? Do I need both a number column and ID? 
Now your ‚Äò is on the wrong side of your comma. 
In your screenshot you have a ‚Äò after 55000 you don‚Äôt need. 
http://imgur.com/a/8zljv WOOO I got it. Now to fix that null thing in the ID column. Again, so confused why I have the furthest left column counting down, and then ID. Do I have to have ID? 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/iKEvLPX.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[state_of_imgur](https://np.reddit.com/r/u_imguralbumbot/comments/6i1huv/imgur_has_gone_to_shit) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20djpepxj) 
You have a single quote after the salary, but not before it. If it's a string, you need one before the `5`; if it's numeric, you need to lose the quote after `0`.
You don‚Äôt have to, but it is highly recommend, especially if you have a chance of duplicates being loaded into your table. It‚Äôs likely null because you have indentity set to false on that column and it should be true. 
Good point! So how do I remove that NULL from ID and get it to put a number in there? Do I just Insert? 
Remove those two other rows then set identity to true on your table. 
How do I set it to true? Do I have to remake the entire table?
Nope, I can‚Äôt write the code for you, but you should be able to google it pretty easily. 
&gt; I think they are trying to exploit free work. Probably. If you're in the US and they're deriving business value from your work, the DoL will most likely (90% chance or better) consider this unpaid internship illegal. Find a paying entry-level job at a company that isn't so blatant about exploiting people like you.
set identity_insert YourTable ON I found this and will try this. :)
I‚Äôm sure the code will require referring to the ID column specifically as well. 
For improving at SQL, I suggest downloading data that interests you and play with it. I find I naturally encounter real world lessons during personal projects. For pursuing certifications, there's a bunch of online or in person courses, or you could try to find a book or other resource.
You might try `SET SHOWPLAN_TEXT ON;`.
Stanford has a free online class for relational databases and SQL. It's as good a place as any to start. 
Wanted to second this, I improved very quickly when I had real data and problems to solve. That being said I found sqlzoo.com to be a great resource to learn the basics so I would start there and find some data
It really depends how far along you are. I'd suggest [Hacker Rank](https://www.hackerrank.com/domains/sql/select). They have great tutorials, and exercises for you to practice with. If they're too tough, there are discussions where people will walk you throw the example. Great self paced learning. Once you get a firm hold on the basics, I think you should practice yourself. Download data from Kaggle or look online, and pull the data you need. You'll have to find a way to hook up to a database with a MSSQL or MySQL client.
[SQLZoo](http://www.sqlzoo.net) makes an excellent starter resource, with interactive examples and self-quizzes. Starts easy, gets harder. He seems to have changed examples back to easier forms since that time I once made my grad cry out for help in frustration.
Kahn academy 
If you wish to use a book, I would definitely recommend *A Visual Introduction to SQL* by Chappell and Trimble. You should be able to get a used copy pretty cheaply and the book explains SQL usage quite well.
You can reference a table by the method: [database].[schema].[table] (you can actually go another level deeper with linked servers, but lets not get out of hand!) As you said tho, that's cumbersome. So SQL lets you set a context. Your logon typically has a context, or "default database" .. In SSMS, you can change this on the fly with the command 'using'. So you can do: USING DATABASE SELECT * FROM TABLE So if you only have one database, you don't really have an issue using the shorthand syntax. It gets more fun when you want to join across databases. SELECT * FROM table INNER JOIN database2.dbo.table2 on &lt;blah&gt; You can of course use aliases to make things neater again: SELECT * FROM table t INNER JOIN database2.dbo.table2 t2 on &lt;blah&gt; WHERE t2.column = t.column
Pressing ALT+F1 while selecting the tablename also runs sp_help (if you want to save yourself from pressing 5 more keys).
Just to add to what VIDGuide has said, a reason why you'd explicitly list table names in a query is to handle ambiguity. Say for example in your solutionpros database you have the employees table you mentioned and also a customers table. If you want to list the names of customers served by employees you might pull out fields firstname and lastname from both tables. Yet, if these field names have the same name SQL doesn't know which specific field you are referring to, hence the table.field notation for clarity - e.g. employees.firstname, employees.lastname, customer.firstname, customer.lastname.
Microsoft's Querying with Transact-SQL course on edX
First pick a full SQL dialect, it could be Oracle, SQL Server or it could be PostgresQL. They differ subtly so it useful to learn one fairly well and then to use it as a bridge into others. The only criteria should be the richness of the implementation, the documentation and the availability of free software for a single user. PostgresSQL is totally free and the others have free implementations for a single user.
I learnt through codecademy and was enough to land me a senior analyst role.
Oh nice. Been using SSMS for over 10 years and didn't know that trick.
So what I gathered from your response is this‚Ä¶ You don't actually need the brackets, but what the heck is a Schema? My employees table does say dbo.Employees however so I can see how that relates to your example of dbo.table2 
The brackets are like quotes in file paths and work around names that would be invalid if left unqualified. If I have a column, database or table name that, for example, contains a space, starts with a numeral, is a SQL keyword you would need the brackets, otherwise they're optional. I'll let someone else more qualified discuss the purpose of schemas, but they are logical separators for tables. Like other items here, if not needed to disambiguate, they can be left out. You can either type database.schema.table or database..table (for when there only exists one schema that contains that db and table combo) or just table. Generally I fully qualify then alias since I never know what the future holds. 
Could you possibly link to the specific courses that got you ready?
Dbo is database owner and is default user schema in ms sql. If you create a role for yourself then you could have database.yourname.table. the brackets are needed if you have spaces or other types of characters like an exclamation point in the naming convention. So a database named. My database wouldn't work in the query unless surrounded by brackets. [My database]. same for user schema and tables and columns. Essentially all naming conventions in ms sql. 
https://i.imgur.com/4Veh9Se.png Take this for example. FROM Production.Product Where is the Production coming from? Is that the username? The database is adventure works and this production portion. I'm assuming that product and productreview are the tables. But wtf is Production? He literally doesn't explain that at all. So I'm confused
Yes, it is probably the user schema. On the left hand side, expand object explorer. That will show you the layout. Expand the database named adventure works. Then expand the tables object. Once that is expanded you will probably see the products.product table. Products is the role given to access and perform specific functions withing those tables that have that schema. Users will be tired to roles. Dbo is a role, DBA is a role. Your user account will have that role assigned to it. Etc. You can see the layout for all of those in the object explorer as well under security. edit and disclaimer: i am probably wrong about how the user schemas are set up. This is how i understand it though. adventureworks is a prebuilt DB, in my experience any DB you create yourself will usually default to dbo.
When a table is created you can then assign that role. Default is usually dbo. So create table adventureworks.products.product (column1 int null, column2 varchar (10) null)
http://imgur.com/a/rWO3i my database named VOC has users, dbo is one of those users. my personal account is stored at the server instance level, my account is then mapped to the dbo account on each database. you can see in the table list that all of the tables are prefixed with dbo. if i wanted to create a new one with a different user schema then i could also do guest, or clarabridge. or INFORMATION_SCHEMA. whoever is mapped to those DB users can then access the tables with whatever level of permissions are granted to that DBO role. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/GgInpxm.png ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[state_of_imgur](https://np.reddit.com/r/u_imguralbumbot/comments/6i1huv/imgur_has_gone_to_shit) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20djqavoz) 
But why would they name a user 'Production?' I guess that is the part that is really mystifying to me. Edit: Production not products
actually it isnt products. looking at your previous message it is production. so it is probably separated out as production and maybe development or something like that. so lets say you have a production and a development environment working off of one database, then whatever apps are editing and working in production can see those production tables and others can only work on development tables. just another way to breakout what work can be done by who and permissions. 
So it is common then to have each department be a user? I corrected my statement. Production* is a user in this case?* since I only have one database and one user, would that mean that I don't even have to include the schema? I would assume that very small start ups don't even have multiple users because they wouldn't have a need for it. But I could be wrong. Correct me if I am
so not a user, but a user schema. a user is an object in the server, a schema is an object in the DB, a user is applied to a schema, that schema allows a user to do specific tasks as directed by that schema. you do not need to use the schema, no, unless you are using the fully qualified name. so you have some USE statements in your queries. if you USE adventureworks, then the query window understands that you are already using that specific database. all you need is a table name in your query. USE adventureworks select * from table or select * from adventureworks.production.table both work the same way. now let's say you have multiple databases db1 and db2 USE db1 select tbl1.col1, db2.dbo.tbl1.col1 from tbl1 outer join db2.dbo.tbl1 on tbl1.sid = db2.dbo.tbl1.sid or without the USE statement then you may need to explicitly state where you are getting your tables and columns from. so you may see the following select db1.dbo.tbl1.col1, db2.dbo.tbl1.col1 from db1.dbo.tbl1 outer join db2.dbo.tbl1 on db1.dbo.tbl1.sid = db2.dbo.tbl1.sid or you can do stuff with aliasing. select T1.col1, T2.col1 from db1.dbo.tbl1 AS T1 outer join db2.dbo.tbl1 AS T2 on T1.sid = T2.sid Just know that depending on where you are in your query window, you may have to define where you need to get your data. 
Thanks! Super super super helpful. So far I have only encountered the DBO schema. I guess that's why I'm confused about the Production one. Makes it difficult for new users to SQL I guess when you are seeing something completely different than what they are showing you
so the thing about multiple users and multiple schemas is just based on what you want in your setup. that is a very broad question with an answer that i always hated to hear. "it depends" for instance i have a DB with a couple of hundred users, the DB is built using the dbo user schema. it works. i have another db on my server that has multiple user schemas in a db. it is for a team that wanted to share a DB, but they wanted their tables ordered by the schema. so each user has a corresponding schema so that when you open up object explorer they can create tables that will be grouped together by schema instead of by alphabetical order. there are other reasons to do different restrictions and permissions, but it will depend. if i were you though, i wouldnt get too hung up on this part right now. just know that your queries will need to follow the naming convention of [server].[database name].[schema].[tablename].[column name]. in many cases all you need is [table].[column name] for most of your query work. If you want to get REALLY specific, you can.
someone else above also mentioned ambiguity. you may see errors such as "abiguous object referenced" in your results. take the following select T1.col1, T2.col1 from db1.dbo.tbl1 AS T1 outer join db2.dbo.tbl1 AS T2 on T1.sid = T2.sid this query is fine, but the following is ambiguous select col1 from db1.dbo.tbl1 AS T1 outer join db2.dbo.tbl1 AS T2 on T1.sid = T2.sid essentially, Col1 exists in both tables. so it needs to be defined. SQL is saying, "yo, which table do i need to get this from, col1 exists in both". so you can see i took both of the tables in the query, and i aliased them. so using the following [alias].[column] works. but remove [alias] and it doesnt. 
something like this should do it. SELECT dateColumn FROM tbl where datepart(dd,dateColumn) between 16 and 31 good point made by cruyff8, this solution will work for MS SQL but may differ in other RDBMDS
check any tutorials site like w3schools or can go for any course on udemy or any other similar site. Rest, all depends on your practice.
Which database are you using? 
i liken schemas to classes in OO code. Schemas are containers (logical addressing, structure) for database objects. There more than just tables in the schema; users are there too along with a bunch of other objects.
MSSQL? would just be - &gt;INSERT INTO migrated (id, data, game_id) &gt; &gt;SELECT id, data, game_id FROM old_table;
I am working with MSSQL
I am working with MSSQL
MySQL. I am tring to insert the data I get from SELECT to the table migrated.
Yet it is still the correct answer, also for MySQL: https://dev.mysql.com/doc/refman/5.7/en/insert-select.html
It seems to work, my bad.
It works for some reason.
Person.BusinessEntity looks like your main table which links to everything else. First you'll notice that HumanResources.Employee has BusinessEntityID, so you can link on that. Now consider the possibilities that an employee can have more than one address. Their home address is obvious, but maybe they currently live overseas on a long term assignment. Let's call that their shipping address. Also, if someone's home address changes half way through the year, you still need to know both for tax purposes. For other types of business contacts (customers, vendors, etc) you could need to record many other types of addresses (home, work, shipping, billing, corporate, ...). Considerations like this are why AdventureWorks might chose to organize Person.Address and other things as separate tables. From Person.BusinessEntity you can get to Person.BusinessEntityAddress (where you pick which address you want) and Person.Address. You'll also probably go from Person.BusinessEntity to Person.BusinessEntityContact (emergency contact might be here, so pick the correct contact) and Person.Person for name, and Person.PersonPhone (again several phones to pick from). All of this is called normalization, and this examples is highly normalized. More normalization typical results in more flexibility but queries are more complicated. There are also some concrete scenarios where inadequate normalization leads to significant problems.
Ahhh yes, the term used was normalization. So it's not 'normal' in the traditional sense of how excel does it, stuffing everything into one single table? I guess that's the toughest thing for me to wrap my head around in regards to this concept of 'normalization.' Because if I were using excel, I would put EmployeeID, FirstName, LastName, Phone, Address1, Address2, Email, etc all under one sheet and populate it with every entry possible. But going to SQL fresh and new, it's kinda like "whoa.. Everything is all separated into its own table (or sheet if you think like excel I guess). Get what I mean? :O But thanks for your response. Definitely helps 
[I think this site does a decent job of explaining normalization.](https://www.essentialsql.com/get-ready-to-learn-sql-database-normalization-explained-in-simple-english/) Also, normalization isn't a bunch of rules that must be followed or you're wrong. It's a sliding scale of `SIMPLICITY &lt;---&gt; FLEXIBILIY`. It's usually best to stop normalizing at the point that flexibility is no longer useful.
You want to read up on [normalization](https://en.wikipedia.org/wiki/Database_normalization) and a few of the Normal Forms. [This MSDN article](https://msdn.microsoft.com/en-us/library/aa291817\(v=vs.71\).aspx) may be more digestable. TL;DR: Databases are normalized (in part) to: * Help ensure data integrity * Improve performance (most of the time) * Reduce duplicated data Attempting to learn SQL without at least a little bit of a primer on set and relational theory (just a couple hours' reading, you don't have to become an expert) will leave you with a few big open questions like this.
As far as I remember there were two modules for SQL. An introduction then a specialised business analytics focused one. The best thing to do is what everyone suggests, practise every day on real datasets and you will get better. I was overwhelmed with some of the complex queries I faced when I started my new role but now I write them from scratch. Combining SQL and vba is very powerful when automating spreadsheet/database processes. 
&gt;:) I am happy that you are happy. Spread the happiness around. [This doggo demands it.](https://i0.wp.com/s4.favim.com/orig/50/boo-cute-dog-pomeranian-Favim.com-452643.jpg)
Whilst a normalised database is good for an application, people like me don't like them. Queries have many joins and can be complex to write. The format of the data in the tables may not be useful for reporting. I'm a BI developer, and in my world, we denormalise the structure of our application database to make a faster reporting database, commonly known as a data mart. Using your example of an employee, you'll have joins for getting each individual piece of data. In a datamart, this will all be put in to one table for easier, faster querying, very similar to your Excel spreadsheet example. 
It looks like you've already got the table to insert into, by for future reference it's handy to know that you can create the table as part of the select: SELECT id, data, game_id INTO migrated FROM old_table
&gt; I wished we used an ORM, but we don't, so I need to use this prehistoric method I highly doubt your ORM would be easier than such a simple SQL statement like this. Even if you are using an ORM you should not be oblivious to the database you are using.
another point to add to what is being said here, to aid in the simplicity as you develop, you can use Views. Basically these are like saved query templates (and a bit more complex than that, but you get the idea) -- So if you have 3-4 tables defining your person, but find that you need a fairly common base subset of this information regularly, you can create a few for it, then query the view in your pages/application. When you need to add in extra data, or execute quick queries without the extra data, work directly with the needed tables, but fall back on the view(s) to quickly code in where needed. Example: TABLE: PERSON, Address, Company Data, etc. VIEW: VP_PERSON, contains a set of the above that is commonly used. SELECT * FROM VP_PERSON where needed SELECT ID FROM PERSON WHERE PERSONNAME = 'Michael' -- Query the table directly for a faster response if you just need a core component. 
It was more of a big deal back in the day. To keep a database normalized meant faster read and write on limited resources. Now with massive servers and great memory, pulling an unnormalized set of data can be pretty quick. When tuning now a days brings you from a second to fractions of seconds... unless you really need it that quick (financial for example).. unnormalized databases don't hurt that bad and are simple to query. Now don't tell my dba chums I said that. Those complex joins and normalized designs are part of our bread and butter. Edit... I still remember the time I heard of data lake like Mongo db. ... I was like "you can do that?!?"
Something like: WHERE SUBSTRING(date_field, 9, 2) BETWEEN '16' AND '31'
So this 'datamart' form of table is better/more efficient for Business Intelligence? Also I could think this would be very advantageous for a small business that doesn't have tens of thousands of employee records to manage :P 
Depending on your scenario, you'll have two groups of databases. Your live systems and your data warehouse. The live systems will be heavily developed for running the transactional systems. You won't be running much, if any, reporting on these to prevent causing performance issues. Your data warehouse will store the data that is brought in from the various live systems. Here you normalize the data and design it so you can relate all the data from the live systems in a meaningful form. You can leverage this data for reporting, analysis, etc. since the loads and changes should happen on a schedule instead of constantly updating. The data here would be pretty normalized for speed and other reasons. You can track changes in say addresses, positions, salary, etc. If you have one table for everything, you would have to duplicate everything to indicate the change. Normalizing reduces the amount of information and gives you better methods of gaining the result you need. Different parts of the organization likely need to view the data on a regular basis in a particular format to perform their function. Instead of always needing to write majorly complex queries, you create a datamart. This datamart will place the data in ways that the business uses from the data warehouse that makes them operate efficiently. This may result in some denormalization in order to make the data simpler to use. The datamart may include new tables, views, and other alterations to the data.
I took your advice and am currently downloading my own data. What certs do you think should be the first for me to pursue? 
Right so suppose you have a number of different tables. You have employee, department, address, email, etc. How would you link all of these tables together? Does the ID of each employee directly match across all tables? I would imagine that having 15 tables, entering any new employee manually would be a nightmare‚Ä¶ I couldn't imagine how you could possibly do it. How you could possibly add a new employee into employee, department, address, email, phone, etc. unless of course you used a select statement that compared something say ID in all of these tables against one another and then use aliases with a laundry list of table names in order to insert data into each table. I guess I just don't understand that part. With this data normalization, how do you insert a new entry into each table to correlate to one new employee?
Weather. That's BRILLIANT. 
Look up SCOPE_IDENTITY(). You can leverage the primary identity key inserted into one table to insert into multiple tables that depend on that key id.
I don't have any certs nor do I value them much for candidates. Perhaps they can serve as a substitute for a technical degree, but after you land your first job I don't think they matter much.
I shall look that up. But is that the primary way people do it with normalization? I'm just asking how they do it across multiple tables
In database systems I would be really shocked if you were hand entering entries the way you describe across 15 tables, that's got an insanely high chance of having mistakes. You would be using a program to gather the data or take a user input and that would most likely be referencing a stored procedure/transaction to take the contents of the entire employee onboarding form and populate the appropriate fields. I feel like your teacher must not have gotten to discussing primary keys and foreign keys, because once you cover those I think it will clarify some of the questions you are wrestling with in terms of correlating data across tables. 
I believe it is.SCOPE_IDENTITY() returns the last identity value generated for any table in the current session and the current scope. Like this: INSERT INTO employees (Name) VALUES (Fred) INSERT INTO Addresses (EmployeeId, Address) VALUES (SCOPE_IDENTITY(), 'Address') (Double check my syntax because I did this from memory on my phone) Assuming the employee table has an auto incrementing primary key column. SCOPE in this sense is the last identity value created within the current batch, stored procedure etc. If you were to do another insert later in a separate query the scope_identity would be different. https://dba.stackexchange.com/questions/124847/best-way-to-get-last-identity-inserted-in-a-table
Very useful, seems way simpler than parsing NOAA xml like we recently did. Is there a more detailed explanation of this use case? I've never really used clr.
I'm suspect the CLR implementation is as complicated if not more so. CLR (in this context) is .Net code compiled into .dll files that are installed into a MSSQL and become database objects (stored procedures, scalar functions, aggregate functions, table valued functions, or custom data types). So whatever you had to do to parse NOAA you'd probably still have to do, but it could be implemented as a TVF instead of as a scheduled task or whatever. This is what it looks like to make a CLR function. [Microsoft.SqlServer.Server.SqlFunction( FillRowMethodName = "&lt;TableValuedFunctionName&gt;_FillRow", TableDefinition = "&lt;SQL code that defines your columns&gt;")] public static IEnumerable &lt;TableValuedFunctionName&gt;(&lt;Parameters you'll be able to pass into the function&gt;) { &lt;.Net code that accesses the webservice&gt; return &lt;object that iterates one row at a time&gt;; } public static void &lt;TableValuedFunctionName&gt;_FillRow(Object obj, &lt;variables that match your column definition&gt;) { &lt;.Net code that matches your row object to the column variables&gt; }
You likely wouldn't have the employee id on many tables but rather they keys from email, address, phone, etc. Regarding inputting the data, you wouldn't by hand mostly. You would have entry forms, triggers, processing scripts, etc. These things would check if say an email exists. If it does, get that email's id and then use that when inserting I to the employee table or whatever it is doing. If not, insert the email to the email table and then use the new email id when inserting it for the employee. The number of tables can get pretty large and complex depending on what you want to do. But you do this to reduce the size of the data and easier to maintain things. Let's use an address as an example. Let's assume an address has a max of 50 characters for address and city, 2 for state code, and 5 for zip code. The address id is stored as integer (4 bytes) that's a max of 107 characters (107 bytes) with 4 bytes for the so 111 bytes. There are 1000 address you store in an address table. This results in 111 KB of storage. If we have 10,000 employees, the additional storage usage is only 40 KB on the employee table. Total address storage comes out to 141 KB. Now if we didn't use an address table, we would have to store 107 bytes of data for each employee. This uses 1.07 MB of storage instead. This is a large difference relatively. We also do not have a way of storing changes efficiently resulting in poor scaling. Now if we attached addresses to customers, suppliers, business units, shipping addresses, etc. The scaling and maintenance gets out of hand without consolidating our addresses into their own table. What if we wanted to flag an email where a customer says stop sending me marketing offers but several users use the same email? When they change their preference, you would have to update all the records everywhere if not normalized. When you have an email table to maintain this, you can update the one location.
&gt; &gt; { &gt; &lt;.Net code that accesses the webservice&gt; &gt; &gt; return &lt;object that iterates one row at a time&gt;; &gt; &gt; } &gt; Do you have an example to share for this portion? 
I've used them to give me access to the regex C# functions. mostly for pattern matching or string manipulation like removing all non alpha characters. But mostly I don't use them.
Research database normalisation. Normalisation (breaking tables up) makes sense for application databases where data integrity is the most important factor. De-normalisation (merging tables together) makes sense where you have a need to optimise read operations such as a data warehouse but the degree to which you do this depends on what pattern of data warehouse you are implementing. Some people here are saying what they prefer based on what queries they are writing. This is a weird perspective for me. The pattern of database you develop should be purely based on what type of performance/operations you require from it. 
Here in Brazil we have "eletronic invoces", so every invoice is actually a XML sent to the goverment webservice. So, there are several goverment webservice to check, cancel, validade received invoices and I do most of those making CLRs and joining in my queries. 
Ahhhhh. So while learning SQL, I am in putting all of the stuff by hand, but if I'm working for a company, most of the stuff is being input automatically through forms on their website and stuff? Like address, phone number, etc.? I think I understand now. So there are these forms that make database entries in their respective tables and what I am trying to learn is supposed to be how to look up and maintain all of that information? Jesus. Here I was under the impression that everyone was in putting all 10,000 records in all of their tables themselves by hand
&gt;I want to become an expert That will take years. The experts I know have been at it for a decade or more and they're *still* only "experts" on a few aspects of SQL Server at best. It's such a huge platform that *no one* can master it all. What's equally if not more important is that they're humble, know what they don't know *and* know who does know those things so they can ask *them*. That's one of my favorite parts of conferences/meetups - the "oh, you're working with X feature/product/tech? You *have* to talk to that person over there, they had a great blog post last week/were just asking me for help with that over lunch" conversations. &gt; I want to be that annoying ass guy that shows up to the interview, and they know without a doubt, he knows wayyyyyyy too much shit. If you're that guy, I'm going to pass on you. [Don't get cocky, kid.](https://rarlindseysmash.com/images/stupid-programmer-tricks-and-star-wars-gifs/star_wars.gif)
We have a string splitter than returns a table with an index column and the value column. It works as well as you would expect (very poor performance for anything beyond a few dozen rows).
This is the best solution (in MS SQL). If the dateColumn is indexed, you can still use that index. Other solutions involving substring or other implicit conversions result in having to do a Table or Clustered Index scan.
Even learning, you can get [websites](https://www.mockaroo.com) to generate fake data for you to use and load that. It might not have all of the constraints you need but you could still process them in bulk.
Most of my examples are hundreds of lines of code, split across several files. Over the years I've developed custom Json, Csv, and Xml parser classes (popular libraries use System.Serialization which is not allowed in SQL CLR) which have some confusing and unnecessary features for a simple example. I did throw together an example for this webservice: http://www.abs.gov.au/api/demography/populationprojection [Microsoft.SqlServer.Server.SqlFunction( FillRowMethodName = "PopulationClock_FillRow", TableDefinition = "JsonKey nvarchar(50), JsonValue nvarchar(250)")] public static IEnumerable PopulationClock() { List&lt;KeyValuePair&lt;String, String&gt;&gt; result = new List&lt;KeyValuePair&lt;String, String&gt;&gt; { }; WebRequest request = WebRequest.Create("http://www.abs.gov.au/api/demography/populationprojection"); using (WebResponse response = request.GetResponse()) using (TextReader reader = new StreamReader(response.GetResponseStream())) foreach (String s in reader.ReadToEnd().Split(new String[] { "\",\"" }, StringSplitOptions.None)) result.Add(new KeyValuePair&lt;String, String&gt;( s.Split(new Char[] { ':' }, 2)[0].Replace("\"", "").Replace("{", "").Replace("}", ""), s.Split(new Char[] { ':' }, 2)[1].Replace("\"", "").Replace("{", "").Replace("}", ""))); return result; } public static void PopulationClock_FillRow(Object o, out SqlString JsonKey, out SqlString JsonValue) { KeyValuePair&lt;String, String&gt; kvp = (KeyValuePair&lt;String, String&gt;)o; JsonKey = kvp.Key; JsonValue = kvp.Value; } --- SELECT * FROM dbo.PopulationClock(); --- JsonKey|JsonValue :-|:- attribution|Australian Bureau of Statistics popNow|24577667 timeStamp|05 Jul 2017 01:33:53 AEST projectionStartDate|31 December 2016 birthRate|1 minute and 41 seconds deathRate|3 minutes and 19 seconds overseasMigrationRate|2 minutes and 19 seconds growthRate|1 minute and 23 seconds rateSecond|83.28381622418177928837 source|Australian Demographic Statistics, December Quarter 2016 (cat. no. 3101.0) sourceURL|http://www.abs.gov.au/ausstats/abs@.nsf/mf/3101.0 copyRight|Copyright Commonwealth of Australia
I'm surprised. CLR string splitters I've used had very good performance, and could parse 100k results in a second.
Thank you for the example, it is very interesting concept.
Thanks for the resource! Already have adventure works, will add another database on for fun. Are these extra databases good for reporting services as well? Edit: Holy shit. That website is amazing. Such awesome data. 
Thanks I'll look into this more. And, Go Hokies.
Can you post the SQL for the view?
If i comment out all the lines calling the functions it completes almost instantly, so i'm like 99.9% sure it's because of the functions...I tried using an existing stored procedure we have that does this but couldn't figure out how to pass multiple records into the SP and then return those values in the query results...I ended up going for the function since it was the only way i was able to take all the different revenue calculation exceptions we have at our company... select o.ord_hdrnumber as [ord_number], o.mov_number as [mov_number], o.ord_status as [ord_status], CASE WHEN o.ord_invoicestatus='PPD' then 'Y' else 'N' END as [invoiced], convert(date, o.ord_startdate) as [ship_date], convert(date, o.ord_completiondate) as [del_date], o.ord_billto as [billto_id], c.cmp_name as [billto_name], ct1.cty_name as [origin_city], ct1.cty_state as [originstate], ct2.cty_name as [dest_city], ct2.cty_state as [deststate], o.ord_revtype3 as [office], l.name as [sales_rep], o.ord_subcompany as [sales_rep_code], o.ord_bookedby as [book_rep], CONVERT(DECIMAL(10,2),dbo.fn_exchangerateRevUS(o.ord_hdrnumber)) as [revenue], CONVERT(DECIMAL(10,2),ISNULL(dbo.fn_USExchangeRateCost(o.ord_hdrnumber),0)) as [cost], CONVERT(DECIMAL(10,2),dbo.fn_exchangerateRevUS(o.ord_hdrnumber) - ISNULL(dbo.fn_USExchangeRateCost(o.ord_hdrnumber),0)) as [gross_profit], ISNULL((SELECT top 1 CONVERT(varchar(max),not_text_large)+' - '+CONVERT(VARCHAR,last_updatedatetime,120) from Notes n where n.ntb_table='orderheader' and (n.not_Type='NONE' or not_type is null) and n.nre_tablekey=convert(varchar(10),o.ord_hdrnumber) order by last_updatedatetime desc),'') as [latest_notes], CASE WHEN (SELECT top 1 not_text from Notes n where n.ntb_table='orderheader' and n.nre_tablekey=convert(varchar(10),o.ord_hdrnumber) and (n.not_Type='NONE' or not_type is null) order by last_updatedatetime desc) is null THEN '&amp;nbsp;' else 'More' end as [more_notes] from ORDERHEADER o inner join company c on o.ord_billto = c.cmp_id inner join CITY ct1 on o.ord_origincity = ct1.cty_code inner join CITY ct2 on o.ord_destcity = ct2.cty_code JOIN labelfile l on o.ord_subcompany=l.abbr and l.labeldefinition='Company' GROUP BY o.ord_hdrnumber, o.mov_number, o.ord_status, o.ord_invoicestatus, o.ord_startdate, o.ord_completiondate, o.ord_billto, c.cmp_name, ct1.cty_name, ct1.cty_state,ct2.cty_name,ct2.cty_state, l.name, o.ord_revtype3, o.ord_subcompany, o.ord_bookedby, o.ord_currency, o.ord_totalcharge MY report is basically selecting from this view and it passes parameters in the where clause for the SalesID, Start and End date ( Using the ord_startdate columnID) and order status.
* Why are you doing all the overhead of the table variable? * Don't do string comparisons for the join condition if you can avoid it * There is an probably an implicit cast somewhere * Why not sum the PPD, LRD, FUEL, and ACC charges directly * You don't cover the NULL case (semi-important nitpick) * The joins and table names are inconsistent, while this doesn't cause a syntax problem, it makes it very difficult to trace (nitpick) *** CREATE FUNCTION [dbo].[fn_exchangerateRevUS] ( @OrderNum int ) RETURNS MONEY BEGIN DECLARE @TOTAL MONEY SELECT ORDERHEADER.ord_hdrnumber, @TOTAL = CASE ORDERHEADER.ord_invoicestatus WHEN 'PPD' THEN SUM(CASE WHEN cht_typeofcharge = 'LRD' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) ELSE ORDERHEADER.ord_charge END + SUM(CASE WHEN cht_typeofcharge = 'FUEL' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) + SUM(CASE WHEN cht_typeofcharge = 'ACC' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) / MAX(CASE WHEN orderheader.ord_currency LIKE '%US%' THEN 1 ELSE e.rate END) FROM dbo.ORDERHEADER LEFT JOIN dbo.INVOICEDETAIL ON ORDERHEADER.ord_hdrnumber = INVOICEDETAIL.ord_hdrnumber LEFT JOIN CHARGETYPE ON CHARGETYPE.cht_itemcode=INVOICEDETAIL.cht_itemcode LEFT JOIN exchangerates e ON DATEPART(YEAR, orderheader.ord_Startdate) = e.year AND DATEPART(MONTH, orderheader.ord_Startdate) = e.month WHERE orderheader.ord_hdrnumber=@OrderNum GROUP BY orderheader.ord_currency, orderheader.ord_hdrnumber SET @TOTAL = ISNULL(@TOTAL,-1) -- If function returns -1, there was an error or it couldn't find the order ID RETURN @TOTAL END *** **Disclaimer**: I wrote this with notepad, syntax checking is out the window. Edit - **Are you doing this in a cursor? Because if you are looping through all the order ids, stahp**
-I see what you mean about the variable table; that makes perfect sense now lol..not sure why my brain decided that was ok. -The only string comparison is the the View's subquery; with the nature of that table ( Notes) i don't have another option, but that doesn't seem to affect performance at all. You're right to sum the values there, will try it out. For the Nulls, do you mean in the Function's query, I.E use ISNULL around each case statement? I'm going to test out your suggestions and see what happens, [but i read this article that basically says to avoid using scalar functions in any select statements or in the where clause since SQL will always execute the function N times for each record](http://www.databasejournal.com/features/mssql/article.php/3845381/T-SQL-Best-Practices-150-Don146t-Use-Scalar-Value-Functions-in-Column-List-or-WHERE-Clauses.htm) /EDIT: Yea, tried all your suggestions in the function but still seeing high execution times... 1571 results, 7.6s duration, 6.1s CPU time, 529687 reads, 13172 writes. I have to somehow build these functions' logic directly into the view while still taking all those cases into consideration...
&gt; For the Nulls, do you mean in the Function's query, I.E use ISNULL around each case statement? I edited a solution to this in my above, would return -1 if it's null. Ya, you definitely don't want to be using this function in a select statement. That would be a huge no-no, basically the same as doing cursor. You can actually look at the execution plan and you'll see the function executions equal to the number of rows in the select. What you should do is bypass the function entirely, since it's not needed by the sound of it: *** -- Not required, but makes maintenance or report adaptation much simpler DECLARE @orderIdsToReportOn TABLE (orderId INT); INSERT INTO @orderIdsToReportOn (orderId) SELECT 'Whatever filtering criteria for your order ids here' SELECT ORDERHEADER.ord_hdrnumber, olist.orderId, CASE ORDERHEADER.ord_invoicestatus WHEN 'PPD' THEN SUM(CASE WHEN cht_typeofcharge = 'LRD' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) ELSE ORDERHEADER.ord_charge END + SUM(CASE WHEN cht_typeofcharge = 'FUEL' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) + SUM(CASE WHEN cht_typeofcharge = 'ACC' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) / MAX(CASE WHEN orderheader.ord_currency LIKE '%US%' THEN 1 ELSE e.rate END) FROM dbo.ORDERHEADER LEFT JOIN dbo.INVOICEDETAIL ON ORDERHEADER.ord_hdrnumber = INVOICEDETAIL.ord_hdrnumber LEFT JOIN CHARGETYPE ON CHARGETYPE.cht_itemcode=INVOICEDETAIL.cht_itemcode LEFT JOIN exchangerates e ON DATEPART(YEAR, orderheader.ord_Startdate) = e.year AND DATEPART(MONTH, orderheader.ord_Startdate) = e.month LEFT JOIN @orderIdsToReportOn AS olist ON dbo.orderheader.ord_hdrnumber = olist.orderId GROUP BY orderheader.ord_hdrnumber, olist.orderId *** Note: If olist.orderId is NULL, then it was missing from the result set for some reason
Shouldn't this be more like: CREATE TABLE new_table AS SELECT * FROM old_table 
I see from OP's other answer he's using MySQL, so I think you're correct. Mine is for MS SQL.
&gt; I also need the query to run on PostgreSQL, Oracle, and Informix. you're new to date functions across DBMS platforms, aren't you what you're after is the holy grail called "standard SQL" which was invented ~decades~ ago, but good luck getting all DBMS platforms to support it...
Yeah in MSSQL the syntax is: DROP TABLE Employees; Happy to help :-)
Here's how to solve this conundrum. Find the person in the company who is insisting on using an AS400 in 2017. Offer to migrate the data to a more modern database solution. If they refuse then beat them to death with a baseball bat. Then migrate.
There is a couple reasons for this. lets use phone numbers because it demonstrates this well.. If you put phone numbers in your other tables it is very hard to report on duplicates. You might have a phone number that belongs to an employee, who is also a customer, who also has a second account opened up, who is also a contact for a family member on a third account. That information is very hard to track and report on if they are just "Fields / Columns" on various tables throughout your database. If you have a Phone numbers table however, with relationships back to your Employees table, your Customers table, your contacts table, and any other table that may require phone numbers - you can more easily spot duplicates and enforce data integrity. Depending on how far you take this this is is the Nth Normal form of the database - for most database use 3rd normal form is best practice but in some cases denormalization, or further normalization is required.
No, your query is not alright. And an ORM would probably be an unnecessary performance hit as a substitute for writing what is possibly the easiest query in history using the "prehistoric" method.
https://db.grussell.org/sql/ This is one of the best resourses (similar to sqlzoo) that I have found so far. A must do.
Get out of the habit of using UDF's (User Defined Functions). They don't work how you think they work. Try an approach like this (this is your view definition): SELECT o.ord_hdrnumber AS [ord_number], o.mov_number AS [mov_number], o.ord_status AS [ord_status], CASE WHEN o.ord_invoicestatus='PPD' then 'Y' else 'N' END AS [invoiced], convert(date, o.ord_startdate) AS [ship_date], convert(date, o.ord_completiondate) AS [del_date], o.ord_billto AS [billto_id], c.cmp_name AS [billto_name], ct1.cty_name AS [origin_city], ct1.cty_state AS [originstate], ct2.cty_name AS [dest_city], ct2.cty_state AS [deststate], o.ord_revtype3 AS [office], l.name AS [sales_rep], o.ord_subcompany AS [sales_rep_code], o.ord_bookedby AS [book_rep], CONVERT(DECIMAL(10,2),exchangerateRevUS.total) AS [revenue], CONVERT(DECIMAL(10,2),ISNULL(dbo.fn_USExchangeRateCost(o.ord_hdrnumber),0)) AS [cost], CONVERT(DECIMAL(10,2),exchangerateRevUS.total - ISNULL(exchangerateRevUS.total,0)) AS [gross_profit], ISNULL(CAST(TopNote.not_text_large AS VARCHAR(MAX)) + ' - ' + CONVERT(VARCHAR, TopNote.last_updatedatetime, 120), '') AS [latest_notes], CASE TopNote.not_text WHEN NULL THEN '&amp;nbsp;' ELSE 'More' END AS [more_notes] FROM ORDERHEADER o INNER JOIN company c ON o.ord_billto = c.cmp_id INNER JOIN CITY ct1 ON o.ord_origincity = ct1.cty_code INNER JOIN CITY ct2 ON o.ord_destcity = ct2.cty_code INNER JOIN labelfile l ON o.ord_subcompany=l.abbr and l.labeldefinition='Company' CROSS APPLY ( SELECT CASE ORDERHEADER.ord_invoicestatus WHEN 'PPD' THEN SUM(CASE WHEN cht_typeofcharge = 'LRD' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) ELSE ORDERHEADER.ord_charge END + SUM(CASE WHEN cht_typeofcharge = 'FUEL' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) + SUM(CASE WHEN cht_typeofcharge = 'ACC' THEN INVOICEDETAIL.ivd_charge ELSE 0 END) / MAX(CASE WHEN orderheader.ord_currency LIKE '%US%' THEN 1 ELSE e.rate END) AS Total FROM dbo.ORDERHEADER LEFT JOIN dbo.INVOICEDETAIL ON ORDERHEADER.ord_hdrnumber = INVOICEDETAIL.ord_hdrnumber LEFT JOIN CHARGETYPE ON CHARGETYPE.cht_itemcode=INVOICEDETAIL.cht_itemcode LEFT JOIN exchangerates e ON DATEPART(YEAR, orderheader.ord_Startdate) = e.year AND DATEPART(MONTH, orderheader.ord_Startdate) = e.month WHERE dbo.orderheader.ord_hdrnumber = o.ord_hdrnumber GROUP BY orderheader.ord_hdrnumber ) AS exchangerateRevUS CROSS APPLY ( SELECT TOP 1 not_text_large, not_text, last_updatedatetime FROM Notes n WHERE n.ntb_table = 'orderheader' AND n.nre_tablekey = CONVERT(VARCHAR(10),o.ord_hdrnumber) AND ( n.not_Type = 'NONE' OR not_type IS NULL ) ORDER BY last_updatedatetime DESC ) AS TopNote GROUP BY o.ord_hdrnumber, o.mov_number, o.ord_status, o.ord_invoicestatus, o.ord_startdate, o.ord_completiondate, o.ord_billto, c.cmp_name, ct1.cty_name, ct1.cty_state, ct2.cty_name, ct2.cty_state, l.name, o.ord_revtype3, o.ord_subcompany, o.ord_bookedby, o.ord_currency, o.ord_totalcharge **Note** - I don't have your tables/data so I can't guarantee the accuracy of this query's syntax/data, so tweak it how you need
[removed]
https://www.w3schools.com/SQL/deFault.asp W3schools could be a really good place to start off, they have examples for you to practice with. I was in a similar place as you a couple of years ago. The content here was enough to get me through a basic proficiency test with the company I am now working for. If you are willing to learn and capable make sure that shines through on your CV. In my interview I actually asked if I could use Google on my phone to help me tackle some of the harder questions and they actually took that as a positive. Good luck! 
Thank you SO much for pointing me in the right direction. This is fantastic. This looks like a great starting place. Do you have any recommendations for after I finish this material? I am assuming this is just an intro which is exactly what I was looking for :-). Perhaps any additional languages to learn? Perhaps Python? Additionally, is there a test on SQL that will give me a legitimate and reputable certification also on the site? I see that they have a PHP/SQL Test that you can take. Would you recommend I also read the PHP tutorial and take this test?
We customize retail software that uses SQL server as the backend and use CLR for any web service integration, basically: * Tax integration, call CLR with sale/ shipping info, get back tax info into SQL * Loyalty/ Gift Card web services * Zip code validation, not extremely sexy but it is a favorite of mine, trigger that if the zip code is updated will do a CLR web call to validate the city/ state/ country
If you can spare $10 udemy has plenty of courses about SQL. Just wait until they have their sales. I think they running sales now. They have monthly plenty of courses for $10 so no need to rush. Also a good intro is this course from khan academy https://www.khanacademy.org/computing/computer-programming/sql Also [this subs wiki](https://www.reddit.com/r/SQL/wiki/index) has some resources. Also, since you mentioned Excel, maybe [this course from Coursera](https://www.coursera.org/specializations/excel-mysql) is relevant. 
Those two online hands on training are the best ones I have found so far: http://sqlzoo.net and https://db.grussell.org/sql/
&gt; ly, no such program is known to me, Thanks. I did indeed do it by formatting the queries and using Notepad++ to find the rows with FROM/JOIN in it. These were basically the rows with the tables. It worked, but hoped to find a tool to do this easier :)
I'm new indeed :) Thanks for telling me :)
If you're in the business analytics field, learning to use SQL with PHP is probably not the best way to go-- that's typically a combination used for web development rather than data science. For data analysis needs, Python is probably the easiest and most versatile scripting language to learn. However, learning to use SQL &amp; scripting languages to build interactive web apps is also a fun and useful skill. Maybe something like [MCSE Data Management and Analytics]( https://www.microsoft.com/en-us/learning/mcse-data-management-analytics.aspx) would be a good certification to work towards, but from what I understand it's pretty much expert-level. 
Is there another SQL-specific certification that you recommend? I think that is a good place to start and I would love to get that on my Resume ASAP, if possible.
I can't say I'm too familiar with all the different certifications that are available, but on the whole IT certification programs tend to be pretty rigorous, often requiring months of prep. However, many online courses will give you a certificate of completion/proficiency-- not an official certification credential, but definitely appropriate to put on a resume. For example, Stanford has some [free online database classes](http://online.stanford.edu/course/databases-self-paced) that have a well-regarded name attached to them but aren't official certs.
Wonderful explanation. Now I just need to understand how primary keys work and stuff
Honestly, I think having a non-official cert is a great start. I think I will see how far just having "Knowledgeable or Proficient in SQL" on my Resume gets me. Thank you so much for your help. I think the guide you send is a great starting point. I will see how in-depth it goes. I think just saying I've used SQL on my Resume will go a long way even if I'm not an expert or have an official cert.
Glad it helped. I think the two things you've pointed at are different specialisations. 1. Python, R, and quite few others are scripting languages often used for data analysis/analytics. Both Python and R are very popular as they are completely free. I would imagine these would be useful skills for the long-term, with data science a growing field. 2. PHP I am less familiar with. But it's a language commonly used for server programming. Picking up PHP alongside SQL will lend itself more to data management, than analysis. But I know plenty of analysts who have become accidental database admins! I would personally recommend sticking with SQL, until your more confident with data structures. As it's quite simple and very useful. Python is my own preference after that. It's easy and you can do almost anything with it. Start with a basic project and expand from there. (Anaconda is a good package that will give you a development environment and full install of the language) E.g. read in a textfile and count the frequency of words and/or letters. Then look for a guide on building a wordcloud. Instead of a textile connect to the twitter api. 
https://sqlbolt.com
Awesome. Thank you so much for this information. I will start reading the SQL tutorial that you have given me. I apologize if I have already asked you this but do you recommend any specific certifications that I should obtain after I feel I have a good basic understanding of SQL? I know that official ones require months of studying and can get quite expensive but I was wondering if there was one that I can use on my Resume, even if it's not an official one. I think just saying I have experience in SQL would go a long way on my Resume and in a job interview.
&gt; Fixed Width file &gt; Trailing spaces shouldn't be there choose one
I need the trailing spaces the query generates. I don't need the ones sqlcmd is pulling out of thin air.
Have you tried -Y? I am honestly still not really sure what you are trying to do. Maybe if you have an example it would be easier to understand the issue.
There are quite a few different sites and suggestions in the comments that I've seen. If you find one with a format you enjoy, pick that and follow it through to the end. (If you have a target company or industry, you could always contact a recruiter and find their preference.) Once you've done that you'll be better equipped to fill the gaps in your knowledge, and know the next steps for you. 
Sounds good. Thanks again for your help.
I only made one library so far, but this is what it does: 1. Generate a sequential guid based on the current epoch time, this lets me have a clustered primary key on the guid 2. Extract the datetime from that sequential guid 3. Convert datetime to other timezones/adjusting for DST for various areas.
I highly suggest the following: 1st) Buy How to learn SQL in 10 minutes and do some reading. Its a basic book written with a lot of examples to get you up to speed in a short period of time. It has multiple code examples for various databases. [Sam's Amazon Link](https://www.amazon.com/SQL-Minutes-Sams-Teach-Yourself-ebook/dp/B009XDGF2C/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1499275904&amp;sr=1-1&amp;keywords=sams%27s+sql+in+10+minutes) 2nd) Install a version of SQL. I suggest [MySQL](https://www.mysql.com/downloads/) or [SQL Server Express](https://www.microsoft.com/en-us/sql-server/sql-server-editions-express) 3rd) Start playing with the example databases in both of those books and I believe SQL in 10 also includes a sample database for both of those distributions. 
Not sure what select statement you are passing to SQLcmd but you should specify your field length with a LEFT command such as: SELECT LEFT(myField, 100) from MyTable I'm assuming you are using the Wildcard (*)?
Those look like extremely helpful resources. Thank you so much for the information. I will definitely check those out :-P. What is your opinion on putting SQL experience on my Resume without an official certification? I think that just having SQL experience will go a long way for Business Analyst positions but I could be wrong. 
Using field+replicate(' ', #-len(field)) Some of them require specific characters and I did some copy/pasting all the way down then just changed the fields as well as added case statements to deal with nulls. There might have been an easier way but it was what came to mind. It works when you save results from smss or copy and paste results from there. Its just running it through sqlcmd that it chokes. One of the fields is 586 characters long, guess imma try and break that one up to see if it helps.
Volunteer your time as a data analyst for a charity, church etc. Do a google search for volunteering in your area is your best route. Certification is going to be a bit harder without hands on experience but it can be done. MySQL, Oracle, SQL Server all have established training programs (expensive) and books (cheap) that can you use to study for basic certifications. But frankly I'd rather hire someone with a year of volunteering as a data analyst than someone with just a certification. You need some kind of real life experience to have a conversation over what you can do with a client.
People are focusing a lot on the Syntax here. Perhaps familiarize yourself with Database theory: https://www.essentialsql.com/get-ready-to-learn-sql-8-database-first-normal-form-explained-in-simple-english/
I don't recommend indexed views unless you have a very good reason, not to mention this example wouldn't work as written (no OUTER JOINs, no subqueries). Indexing the underlying table is almost always preferable.
I wish I understood regex better. Regex and MDX are the two languages I struggle with.
Sounds like you have a long script. Try running SQLCMD in interactive mode and focus on the part (after select not after with) that is choking. I've had migration problems with calling data from MySQL into SQL Server and many of the legacy scripts were pages long. Only running interactively was I able to tell what the problem was. (Bad data was my answer most of the time in that example) 
The code above (minus removing text image) ran successfully on my test database. I've ran views using left joins before (not outer join) and the index on the source table is already there. 
Also, [CodeAcademy](https://www.codecademy.com/learn/learn-sql).
The view is valid, but if you tried to create an index on the view you couldn't. Look under [Additional Requirements](https://docs.microsoft.com/en-us/sql/relational-databases/views/create-indexed-views) for list of restrictions.
Okay. I see. A unique index key on the view just adds overhead. 
Where is the FOREIGN KEY text? Try to create your two tables in two separate queries. You also haven't encapsulated the table you are referencing with backticks--that may be your problem.
Interactive mode? You mean sqlcmd mode in SMSS? Just tried that. It outputs proper results. *edit* oops, found out how to do command line interactive mode. I'll try it out.
Yep. 1. An indexed view writes all the data of the view to disk, like a new table. 2. Keeps the data updated with any change in contained tables, like a trigger. I've also found there is a limit to how well they scale. We have some things that started out as indexed views, but we had to redesign them as separate tables that we keep updated because they became too slow.
I can second this suggestion. In two weeks of part-time learning, I went from zero knowledge to having a working query that saved me days of work per month. Definitely worth the investment and the course was nice and slow and explanatory.
Well, here's a partial issue that I'm dealing with. The removal of "Bob Smith" in the above example wasn't just to give my example complexity. I'm passing the table to a widely used Access database; if I was to write the query in Access SQL (not a pass through query, which is probably where I'm going); Access literally will download every table in it's entirety before removing the "Bob Smith" parts. So in actuality (and even without the index) this view is faster than running in Access. But like I said, this begs for pass through queries written in ugly VBA and passed to SQL. I'll cut the view for now.
Think I maybe have it and it was bad data like you said. It was getting stupid with some of the data types. Smss and report builder handle them without a hiccup but some of my date conversions were the problem.
&gt; You also haven't encapsulated the table you are referencing with backticks--that may be your problem. that is only ever the problem when the identifier is a reserved word or contains special characters people that actually use backticks (or allow their interface software to generate them) should be taken out back and shot 
One software program I use will allow standard sql queries against text files, but only if you use the backtick syntax. For any other SQL query it uses brackets...
works with or without backticks, everything succeeds, i run both create tables seperately than both inserts seperately no failures. foreign key text is like this ALTER TABLE `zertocrawler`.`usagedetail` ADD INDEX `site_fk_idx` (`site_id` ASC); ALTER TABLE `zertocrawler`.`usagedetail` ADD CONSTRAINT `site_fk` FOREIGN KEY (`site_id`) REFERENCES `zertocrawler`.`sites` (`id`) ON DELETE NO ACTION ON UPDATE NO ACTION; this does not execute succesfully 
SQL for smarties, 5th Edition by Joe Celko.
That was exactly my problem when I migrated the MYSQL tables to SQL Server. MySQL allowed for things like dates of "0/0/1900" that would cause SQL Server to vomit. Best of luck.
"Duplicate" rows usually means you have a JOIN, and it is returning records that you want to filter (but haven't). So you probably need something additional in the ON or WHERE clause. That's just a general rule, but you have to evaluate on a case by case basis. Could also be a bad GROUP BY or a number of other issues. It's impossible to know for sure without looking at the query (and probably having some information on your schema and data structure).
I do see a number of rows of JOINs in the sql, but I have no idea about the data structure. I know it's pulling off of many, many table containing millions and millions of rows of data. If it's doubtful that this could be resolved without knowing the data layout, then I will reserve from posting the sql and wasting everyone's time. I appreciate the info. 
The number of rows in the table isn't really relevant. It's the relationship between the table(s). Go ahead and post it, it could be something obvious.
If we could get the query and maybe an example on the duplicate results we should be able to narrow it down. It is probably on a join and it might be obvious or it might be "run these two queries on the tables alone to see the results". Of course if it is a select with sub-queries and joins on 20 separate tables with columns like x_124 than it would be a lot harder.
Im in your position as well and found modeanalytics.com by accident that enable me to play around with premade sql databases and python using your web browser only This is very perfect for me since i can continue my play around anywhere being web based :)
If they are truly exact duplicate rows, try adding DISTINCT after the select and see if that helps. To see exactly what is happening would require a peruse of the code though.
I liked [this resource by Mode Analytics.] (https://community.modeanalytics.com/sql/tutorial/introduction-to-sql/) Contains examples and their explanations with their platform to actually execute these queries. Give it a try.
Edit 2 (don't why it won't let me load a second edit to the post on mobile): the reason why I can't simply filter off the "?" rows is because sometimes I get data that only has "?" in the product spaces, so I lose that count if i don't count them. 
Are you trolling?????
If you want one row per account you'll have to apply logic to pick which product you want, or decide how to count multiple products on one row. For picking a single product, there is a similar example in the speed section. QUALIFY ROW_NUMBER() OVER (PARTITION BY srv_accs_id ORDER BY subsrptn_prd_seq_nbr DESC) =1
Thanks for the reply. I'm in a similar situation as the OP. Have you ever done any free-lance work with SQL or other languages? I think it would be cool to do a project and then show an employer during an interview. I'm a recent Econ grad and have lots of experience working with Stata. Any suggestions for free lance projects?
Here's the site I use to hire very short term (~ 1-2 weeks): https://www.upwork.com/ It's great to build your resume and get some work experience.
if you need to join it up with disparate data to do an analysis its very useful. 
From looking at the results it seems like you are seeing mostly duplicate except for the promo info. For this I would look at the "prd" view specifically from the query. Not sure what that table is supposed to show but it is looking like there are multiple rows per prd_id so that would cause multiple rows. If that is correct than it might be hard to recommend the best solution without knowing more about the data structure and what it needs to show but it is at least the right direction. I would do a single query like this to test: select * from edwviews.prd prd where prd.prd_id = [Get prd_id to lookup one] AND prd.bill_sys_geo_id = 24 If you can get a prd_id that shows up multiple times in the final query and you run this and it gives you multiple rows that might be the answer, you would then need to determine if there is only one row that is needed from prd to filter more, or the best way to group if that makes sense.
Code academy is cool how they have a virtual sql console for you to work with in the browser.
&gt;Where do I start? Mock up your table on something like www.sqlfiddle.com and we can help you build the query. &gt;Do I need to make every calculation and store that data in a separate table? Nope, you structure your data so that you can calculate everything on the fly. No need to store calculations. You'll write the query so that each time it's run, it'll pull the latest data and give you "dynamic" results.
if u wanna just be able to add sql in ur resume theres plenty of replies posted already. my advice to really learn is come up with ur own use case of a database and then just get dirty with it and start building by googling everything you dont know about. this is because you will be asked practical questions on sql, not if u completed a tutorial online, hiring managers look for that and ask questions like "what have u done with sql" rather than "how do u write this query". plus sql is worth spending the time to learn. it doesnt matter which implementation u learn either, i've been using mssql and postgres for a while then recently moved to mysql without problem.
 CREATE TABLE IF NOT EXISTS `Test` ( `resultID` int(11) NOT NULL, `Player` varchar(40) NOT NULL, `Opponent` varchar(32) NOT NULL, `G1 Result` varchar(1) NOT NULL, `G2 Result` varchar(1) NOT NULL, `G3 Result` varchar(1) NOT NULL, `Match Result` varchar(1) NOT NULL, PRIMARY KEY (`resultID`) ) INSERT INTO `Test` (`resultID`, `Player`, `Opponent`, `G1 Result`, `G2 Result`, `G3 Result`, `Match Result`) VALUES (5, 'Matt', 'Jim', 'L', 'L', '', 'L'), (4, 'Matt', 'Scott', 'W', 'L', 'W', 'W'), (3, 'Scott', 'Jim', 'W', 'L', 'W', 'W'), (2, 'Matt', 'David', 'L', 'W', 'L', 'L'), (1, 'Jim', 'David', 'W', 'W', '', 'W'); 
So this is a trimmed down table. You can see each match is comprised of 3 games. I'd like to be able to calculate percentage of games won and percentage of matches won. And I imagine ultimately, I'll need each result listed twice, so that, for instance, in a match between Matt and Jim, there's a line where Matt is the player and a line where Jim is the player, so that that match is included in both of their win calculations.
You're pretty close! But you want to track each result as it's own row, not as columns. You've got it right to store each distinct game twice in your table, one row for the winner and one row for the loser. Have the game info, name, opponent name, and two additional columns ("measures"). Those columns track whether the team won or lost. If they won then the win=1 and loss=0, if they lost then win=0 and loss=1. Do this for both rows, from opposite perspectives. That makes it trivial to track win % for each team: select player ,(sum(win)*1.0)/(sum(win) over () as win_perc ,(sum(loss)*1.0)/(sum(loss) over() as loss_perc from test group by player; The "over()" is a "windowing function" that escapes the context of the current row and sums across all rows. You can add "partition by" within the parenthesis to change the behavior of the sum. You multiply by 1.0 to change the integer to a float. This assumes there is always only two teams, and you should consider how to handle potential edge cases (tie? rained out?), but it's a fairly decent design. 
Check that http://www.studybyyourself.com/seminar/sql/course/?lang=eng. Free, for beginners, well structured, keep things simple, with online exercises.
When you have a basic understanding of SQL you can look at these articles: http://sqlmag.com/t-sql/t-sql-best-practices-part-1 http://sqlmag.com/t-sql/t-sql-foundations-thinking-sets http://www.sqltuners.net/blog/13-04-30/SET_based_processing_vs_Iterative.aspx For writing efficient SQL.
&gt; You multiply by 1.0 to change the integer to a float. I think you should multiply by 1e if you want a float. 1.0 is a decimal.
Where could I read more about what CLR are is and how to take advantage of it? 
You can grant rights to a specific symmetric key and cert. Check this post: https://www.mssqltips.com/sqlservertip/2431/sql-server-column-level-encryption-example-using-symmetric-keys/ Appears VIEW DEFINITION is all that is needed. Not CONTROL. At least for reading the encrypted data.
Ended up being a lot of other columns too. Pretty much every mixed datatype caused by a formula or concatenate puked instead of converting to a single type like SMSS does automatically. Luckily being a couple hundred lines, it's a fast query. I ended up casting everything as a varchar(max) and concatenating everything everything into one long row. The thing it's being fed into wants 1500 character long rows with no delimiters.
One thing to worry about is international characters or unicode (nvarchar) being passed to regular text (varchar). 
Cascading updates really has very few use case scenarios that I have found. If you cascade updates on the employeeid it would update employeeid. So not really great for your scenario unless phone number was an id value used in both tables. Then when you updated phone number, you would want the change to cascade to other tables that used the phone number as a key. Hopefully that makes sense. I think to answer your question specifically, no the phone numbers won't be updated unless you had a foreign key to the phone number field itself. If you cascade updates on employeeid (or phoneid), then those values would be updated when their respective foreign keys were updated. Normally you wouldn't need to update an employeeid (or phoneid) value.
I interviewed for Athena Health. The offer just was just slightly under what other people were paying.
I think I get it. So my structure would be something like a row for each match and columns with player 1, player 2, and each game (with a "1" for a player 1 win)? Are those game columns the measures you're talking about or do I need additional blank columns? As for edge cases, the plan is to have a web form where this data is captured. The reason I have match result along with game result is so that rather than calculate the match result based on the games, the player entering the result could list something like G1 - Loss, G2 - Win, G3 - (null), Match - Win, and something like "retired" in a notes field. I'm going to try to rewrite this tonight.
A quick Googling suggests float is smaller and faster, where decimal is more precise. Any further reason to use one over another? Is this that thing where 9.95 gets rounded to 9.9 because there is no exact binary equivalent for 9.95 and storing it as a float means the number stored is actually something like 9.94999999?
&gt; DECRYPTION BY CERTIFICATE: CONTROL permission on the certificate and knowledge of the password that encrypts its private key. src: https://docs.microsoft.com/en-us/sql/t-sql/statements/open-symmetric-key-transact-sql VIEW DEF on the symmetric key, but still CONTROL on the cert
Yes, that's what I meant. A foreign key to the phone numbers field. I did it with the cascade for deletion‚Ä¶ So when I delete an employee, it deletes their phone numbers. I'm just not sure how it works with cascading updates though
You're almost there. Check to see if max (purchased_order) = 1 to determine if they are new. This will not work if you are looking at a broad date range unless you don't care about the date (i.e., if you need to know for each purchase whether the customer was new, this will not work. If all you need to know is as of today, which customers are new, it will work). Alternatively, you can group by customer id then count all records for each customer. Any user with more than one record is not new. This will erroneously call users with more than one record per unit time youre looking at as existing, though. My preferred method is just to use LAG. Use lag (purchase_date, 1) over (partition by customer_id order by purchase_date asc) and check if it is null. If so, that is their first purchase. This has the advantage of allowing to to also calculate time between purchases in the same query. 
I think also a regular old IN statement would work assuming we don't actually need to rank the purchases right? 
Think of it as a Venn diagram to make two populations: one will be the population of people that go to the Ski and Ride school; the other will be a group that has been there before. (there are a thousand ways, but) Make two CTEs for both populations and join them accordingly. with cte as (select * ...) , cte2 as (select * ... ) select * from cte join cte2 on etc 
The ranking is just to find the first purchase. All others are irrelevant unless he/she needs some additional info, like time between purchases. 
Maybe you have to qualify your name column in the where clause since both tables have a name field maybe the engine is a bit confused which to compare?
This should work: ;WITH CTE (userid, planName) AS (SELECT userid, planName From Table B) UPDATE A SET DATE = GETDATE() FROM Table A JOIN CTE B ON B.userid = A.userid AND B.planName = A.planName
Looks like it works perfect. Thank you!
the primary key is the column (or columns) that uniquely identify a single row... Easiest way to do this is with an "ID" Column, sometimes it is more appropriate to have more complex keys though especially if you need to optimize searches.
What, specifically, do you think is wrong here?
Seems perfectly legit to me. (Although, only glanced at it for 30 sec.) Let's just say, I've seen worse CORRECTLY done, and INCORRECTLY done. 
Certs are usually good for those who don't have experience. Experience in my opinion will always trump certs. In my early years of IT work I noticed a lot of help desk or desktop support staff had A+, Security+ or any other entry level cert that knew next to nothing.
I doubt anyone cares about your GPA. Expect your first gig or two to be short term contracts. With any luck you'll get a contact to hire offer early. SQL people are in demand. Certs are okay, but I've never worked with anyone who's had one.
Those certs are still relevant for helpdesk, sys admin/analyst roles. I'm not sure if those are worth OPs time assuming OP wants to work with data analytics in his career. But since OP has good exoerience, he should be in good shape finding a job after school.
Relevant sure, but it's very rare that employers require it. 
They're going to be useful for you getting your first couple of jobs where anything that helps you stand out from all the other recent grads goes a long way. Whether that's worth it depends a lot on how much demand for your skills you have locally.
I know anecdotes are... anecdotes but in my experience the stereotype holds true. The few people I know - myself included - who have *the knack* never bothered to/needed to get certified. All the while, anyone I've ever known who got certified usually lacked *the knack*. I've found that young guys can have the knack but experience is what tempers them and makes them truly wieldable and effective.
Missing from this article: 1. Using PHP to make a direct SQL connection. I jest. Mostly. But really, the proper way to do SQL in PHP that utilizes user input is to utilize parameterized queries, such as pg_query_params for postgresql with variables built outside the sql while using htmlspecialchars on your inputs. I am not sure which functions do similar for mysql and mssql and such, but I know they exist - perhaps this article references them, but i don't see it doing any sanitization on the inputs either.
Certs tick boxes for HR who doesn't know better. Experience relative to the position ticks boxes for direct management. Typically I've found this to be true over the last 15-20 years. Your technical interviewer should be able to distinguish a paper tiger from someone with bona fide knowledge. As experience begins to outweigh your education, Certs can become less of a factor in your employability, but Cert'ing/continuing education AND experience shows professional development and never ever hurts as you move forward in your career. But if you gain experience and become a true SME at your subject area and do not have certifications it won't hurt you, as a typical engineer/technician/implementer, either. Certification helps in certain career path variants, like instruction and consulting. I also believe that certs build fundamentals for skills and, myself, work towards adding them to my resume if possible when training provides the opportunity. One hallmark of a great employer, and you should be [[interviewing your prospective employer]](https://www.forbes.com/sites/nextavenue/2014/06/18/10-job-interview-questions-you-should-ask/2/#608afe1b727c) [[1]](https://biginterview.com/blog/2011/08/best-questions-to-ask-end-interview.html) [[2]](https://www.themuse.com/advice/51-interview-questions-you-should-be-asking) as hard as they are interviewing you, is that they provide for training on a yearly basis in the subject areas that you would be responsible for maintaining, implementing, etc for the company.
Pluralsight.com you have to pay but worth it for anything technical you want to learn 
What an ugly looking database 
For everything you know there is twice as much you don't know you don't know. Get a book more advanced and keep poking and prodding deeper in to the technology. That's what makes you stand out in IT in my opinion. 
I'm guessing since you're using PHP, you're using MySQL? Can you explain which fields you want to get back from postmeta, and what you want to do if there's more than one postmeta record? e.g., SELECT (list your posts fields here) , group_concat(meta_id) , group_concat(post_id) , group_concat(meta_key) , group_concat(meta_value) FROM posts a JOIN postmeta b ON b.post_id = a.ID WHERE post_type='product' GROUP BY (list your posts fields here again) Will return one record per post record, and stuff all of the other data into comma delimited strings, but that's probably not what you want.
Float, or floating point, is basically stored as scientific notation in binary. So when you write 1.2e10 that is what is stored (but in binary). It can store extremely large or small quantities but not as precise. I never use it in any of the applications I've built. Decimal is used for precise quantities (not measured but either natural numbers or "made up in a computer").
There are over 100 fields in `Postmeta` so, I'm not sure if it's good procedure to have so many fields listed? In the end there won't be any duplicates because there can be only 1 `Metavalue`per `Metakey`. The suggested code won't work, unless I get really messy with PHP. The problem with it is, there are dozens of products, so it's merging all the `meta` information from every product into one string. Kinda going backwards :D From too many returns to a single return. I'm trying to find an effective method for this, so if you'd like, assume there are 10,000 products, with 1 row in Posts, and 200 Rows in MetaKeys How about this question. If I join two tables, and filter the the results, is there a way to take multiple results from table two and place it in an array? So I have a multidimensional array? Or a similar concept with objects? I mean I suppose I could break my SQL into two parts, one part for isolating the posts, open a foreach, then do another query into the metaposts. But then I'm doing a third tier query aswell, so I'm not sure if this is poor technique or not. Doesn't seem like a good idea to do it that way..
I think we're getting confused in terminology: a field is a column, you said there were four columns in metavalue, that's four fields. When I say "list your post fields", that's listing your column names for the table posts. What the posted query should return is for each product one record (i.e., row), with one meta_id column which will be the four metaid's concatenated, one post_id column with the four metaid's concatenated. Are there always and only four records with the same _featured, _weight, _length properties, or do the properties vary by product? Again, not sure that's what you're after, you need to explain what you're doing with the metapost fields. It might be you're trying to do display-layer stuff at the database level where it might be better handled in php - do one query for products and another for tags. It also sounds like another tack is to output as JSON or XML, but I don't know how or whether MySQL supports this. 
Cursors perform very badly in MSSQL. Don't expect (self-made) functions to work as well as they do in Oracle. MSSQL will always estimate 1 row to be returned from a function. Right now I'm optimizing code that has 3 functions nested in each other... Turn on (Read-Committed) Snapshot Isolation on the database if you're starting a new project, the code will probably expect this (Oracle uses this). MSSQL doesn't have packages, but you can make your git folder structure package-like for yourself. http://sqlmag.com/t-sql/t-sql-best-practices-part-1 http://sqlmag.com/t-sql/t-sql-best-practices-part-2 http://sqlmag.com/t-sql/t-sql-foundations-thinking-sets http://www.sqltuners.net/blog/13-04-30/SET_based_processing_vs_Iterative.aspx For writing efficient T-SQL.
What can I use to build my own database and mess around? MySQL? Also, is Python similar to SQL? I've been messing around with some basic Python as well, mostly to use this: http://beets.io/
sql stands for structured query language. it's used to query relational database systems. u can say it's mostly standard across all sql databases engines, but sql is sql. python is an interpreted scripting language. it's similar to ruby and it's different from sql in their purpose. to build ur own database i would recommend either postgres or mysql since they're both open source and widely used. you should have no problem installing and finding documentation and help.
What do you mean refuses to let you make 1-1 relationship because they don't match up? If you do an inner join it will only return rows that match. It's kind of the point of an inner join. You don't need to reseed. It is completely unnecessary in almost all cases for the ID values to be an unbroken sequence. I think you might be confusing what a relationship means in the database. A 1:1 relationship simply means that the record in table a will only have 1 matching record in table b. It is not always the case that table a will require a record in table b (really this is 1:0 relationship... but I digress), so that means there won't always be the same records in both tables. Unless it is a requirement for the business or application (then we use a foreign key and other constraints to enforce the logic). And theoretically you don't have a 1:1 relationship. You have a 1:N or 1-M relationship. That is, each 1 employee can have *many* phone numbers, not just 1. If you are concerned about orphaned records in the phone number table you can use "ON CASCADE DELETE" to keep it clean. Reseeding won't fix that issue. If you really want to reseed though to see how it works, you need to use this: https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-checkident-transact-sql
Gives me a nasty error: &gt; (The ALTER TABLE statement conflicted with the FOREIGN KEY constraint The conflict occurred in database) because my phone numbers table has employeeID 1-10, and the Employees table only has 1-10 with 3 missing because of the deleted record. But I'm looking for a way to reset employeeID numbering so it is 1,2,3,4,5,6,7,8,9 since there is no record 10. Is there even such a thing ? I even tried &gt; DBCC CHECKIDENT(MyTable, RESEED, 1) But it did nothing
Sounds good. Thanks so much. Additional question...is SQL similar to Visual Basic?
I am still not sure what you are trying to do. If you made a unique primary key on the identity column in both tables and created a foreign key then yes you have created a 1:1 relationship--but that's not what you actually wanted--since I'm pretty sure you want to have *many* rows in phone numbers for *each* employee. See the 2nd post down here: https://stackoverflow.com/questions/5112473/designing-11-and-1m-relationships-in-sql-server Gaps in ID fields is not a problem. It will happen and that is normal.
* Please specify SQL platform when you make posts * This looks suspiciously like homework * You'll want to use GROUP BY and COUNT to solve the problem
You can do this in a subselect with a 'having' clause. The subselect would look something like this, on mobile sorry: (Select ID from table b group by ID having count(*) &gt; 1) sub on sub.ID = tableA.ID Edit: rereading it you may have to concatenate your 'ID' column as it appears to contain two values. But it should still work out the same. Especially if you know you always want the first 3 characters
* MySQL * No, thankfully * Makes sense! Been a few years since I've written SQL. Just needed a brain boost. I got it. Thanks!
 SELECT * FROM tablea WHERE id IN( SELECT id FROM tableb GROUP BY id HAVING COUNT(*) &gt; 1 )
Ohhh. I was trying to reset the ID column and "reseed it" basically. Because the gap I guess bothers me. But if it's completely normal in data bases‚Ä¶ And extremely challenging to reset it‚Ä¶ Might as well leave it?
Yes it's completely normal. Try not to think of the ID field as a sequential numbering, just as a unique value. Sometimes databases use a GUID (which is a hex code about 38 characters long) but they are *much* less efficient to use than an integer due to their length and the fact that they can't be ordered easily.
Well when I add a value into my Employees table now, it goes 9, 10, 101 ;P So that's perfectly fine though? EDIT: OMG I figured it out. Okay so DBCC CHECKIDENT(MyTable, RESEED, 1) basically tells you what value your table will start numbering at. Like a farmer, it's the seed. So in order to replace 3 which has disappeared been deleted, you can take 3-1, to get a seed number of 2. So I set the RESEED to 2, and inserted a John G Wayne record, which became EmployeeID 3. So now it goes 1,2,3,4,5,6,7,8,9,10 again! :D WOOHOO! Then, I set seed to 9, so it will automatically pick up again after 10 and go on as normal. [But for future reference... again... I shouldn't care about that ordering of identity?](https://stackoverflow.com/questions/14642013/why-are-there-gaps-in-my-identity-column-values) &gt; Do not expect the identities to be consecutive. There are many scenarios that can leave gaps. Consider the identity like an abstract number and do not attach any business meaning to it. 
Yes. The main point of consideration when it comes to the clustered index is that it maintains a relationship with other records properly. If you change that value of 101 to 11, then all records in other tables with a reference to 101 will also have to be changed to reflect that, as they have broken relationships now. There is very minimal gain as far as I know.
This is so interesting. Your mind thinks it will be perfectly sequential, and it just feels so wrong to see a gap in the identity. Like in Excel. If you delete a row in excel, the one below automatically jumps up into it and there is no gap. But if you saw 1,2,4,6 in excel you'd be like wtf is this?! I guess that's why I'm concerned about the gap. 
A few thoughts. * I most often see similar processes related to scheduling data. * You may want to read through [this post.](https://www.reddit.com/r/SQL/comments/6gsoj7/prioritising_overlapping_time_ranges_to_produce_a/) I especially like [OP's solution.](https://www.reddit.com/r/SQL/comments/6gsoj7/prioritising_overlapping_time_ranges_to_produce_a/diu49rh/) The key point is instead of treating umbrella segments, right segments, and left segments separately, you separate all segments by the distinct boundaries of the set. * I'm not a fan of `SET @sql = @sql + ''` over and over. I prefer line breaks inside the dynamic sql like this: ---- SET @sql = 'DECLARE @overlap_updated AS INT; SET @overlap_updated = -1; WHILE (@overlap_updated != 0) BEGIN /* Stuff goes here */ END'; ---- * Instead of asking for field names as parameters, you could expect the user to populate a #temptable with a clustered index that describes the data. Expect the clustered index to contain exactly two fields of a date datatype (the first is your begin date, the second is your end date), the other clustered index columns are key columns, and everything else is non-key. This also protects against any trepidation of calling a sproc full of dynamic sql pointed at an important table. I'm not sure if all this is a particularly good idea, but it's a thought. That's all for now. I may have additional ideas later.
GETDATE() is not evaluated once per row. SELECT GETDATE() FROM sys.objects I agree with using a variable to store the date if the goal is to have the same date across multiple statements.
Data stored on the pages in the sql database is un-ordered. The identity is to give it a unique value, and allow it to be ordered (if required). It's not stored in an indexed row (unless you make an index) like an array or in excel. Different concepts.
nope. entirely different. probably the ONLY thing in common is they're both programming languages, it's all.
First, thank you. * I work in healthcare, and I see it a lot when we want to capture attributes, characteristics, behaviors, and conditions of people (or other entities). The partition is usually dynamic and dependent upon the slice of data that we want out of the table (e.g. multiple column like member identifier, program and then breaking by category of eligibility). * I'm still digesting this post. I think the key take away is that the granularity of the segmentation needs to be known ahead of time - in this case that it was by minutes. I'm going to look closer at the "CalculatedBoundaries" answer. I think my granularity is fixed to day right now. I'll have to run this approach through some tests and see if I can line it up. * Good input. I might change this. I'll definitely think about it. Might make it easier to read. The biggest challenge is the depth of the dynamic SQL, which is needed to support n-depth partitions and carry fields. * This really is to support n-depth partition fields and carry fields (fields that aren't used in the calculation but want to be carried along with the result). Using the #temptable with a fixed structure would feel more like an API and would probably reduce the need for dynamic SQL. I'm not a fan of dynamic SQL because of the difficulty in maintaining and supporting it. I was going for maximum flexibility (unknown, n-depth input) and re-usability. I have a simple axiom: "Flexibility breeds complexity... and performance problems." So, there is definitely a trade off. Again, I appreciate your feedback and time. I'm going to reflect on your comments and think about how to improve the code. 
Oh ok. I don't know much about Visual Basic but the syntax looked similar so I wasn't sure. 
Regarding the #temptable idea, I wasn't advocating for a fixed structure or eliminating dynamic sql. I was just thinking of it as an alternative to using parameters to define everything. For example: EXEC dbo.DateSegments_AlignWithinTable @tableName = 'dbo.EmployeeStatus', @keyFieldList = 'EmployeeID,CallCenterID', @nonkeyFieldList = 'StatusID', @effectiveDateFieldName = 'StartTime', @terminationDateFieldName = 'EndTime'; Is equivalent to: SELECT EmployeeID, CallCenterID, StatusID, StartTime, EndTime INTO #DateSegments FROM dbo.EmployeeStatus; CREATE UNIQUE CLUSTERED INDEX CIX ON #DateSegments (EmployeeID, CallCenterID, StartTime, EndTime); EXEC dbo.DateSegments_AlignWithinTable; Then in your sproc you could identify the type of column from the clustered index definition. SELECT c.name AS ColumnName, CASE WHEN ic.key_ordinal &gt; 0 AND ty.name LIKE '%date%' AND ROW_NUMBER() OVER (PARTITION BY CASE WHEN ty.name LIKE '%date%' THEN 1 ELSE 0 END ORDER BY ic.key_ordinal) = 1 THEN 'EffectiveDateField' WHEN ic.key_ordinal &gt; 0 AND ty.name LIKE '%date%' AND ROW_NUMBER() OVER (PARTITION BY CASE WHEN ty.name LIKE '%date%' THEN 1 ELSE 0 END ORDER BY ic.key_ordinal) = 2 THEN 'TerminationDateField' WHEN ic.key_ordinal &gt; 0 THEN 'KeyField' ELSE 'NonKeyField' END AS ColumnType FROM tempdb.sys.indexes AS i INNER JOIN tempdb.sys.columns AS c ON c.object_id = i.object_id INNER JOIN tempdb.sys.types AS ty ON ty.user_type_id = c.user_type_id LEFT OUTER JOIN tempdb.sys.index_columns AS ic ON ic.object_id = i.object_id AND ic.column_id = c.column_id WHERE i.object_id = OBJECT_ID('tempdb..#DateSegments') AND i.index_id = 1 -- clustered index ORDER BY c.column_id; --- ColumnName|ColumnType :-|:- EmployeeID|KeyField CallCenterID|KeyField StatusID|NonKeyField StartTime|EffectiveDateField EndTime|TerminationDateField
So it is intended to be unordered unless we make it an indexed row, but that is wholly unnecessary? Is that what you're saying? 
Interesting. So, use the metadata of the input table. It would change the process chain a little (unless the source already had the right clustered index). From: source table -&gt; procedure -&gt; output to source table -&gt; subset/temporary table -&gt; create clustered index -&gt; procedure -&gt; output. Since we might have multiple date fields, we would still need the effective and termination date of the segment as input, but that would take care the partition and non-key fields by looking for the clustered index as the partition (which would help performance) and then other fields as carry fields. I see the procedure as a generic transformation process as we move from a transactional, normalized form to a more de-normalized form or for prepping changing the granularity. I really like the metadata idea as it becomes self-describing. Thanks for proposing another approach. 
Assuming you're using MS SQL, given that looks like SQL Server Management Studio... You can set a column as a primary key via the GUI (right click a table, select Design or via the "New =&gt; Table" window) by just right clicking it and selecting "Set Primary Key" It gets scripted out a _little_ different via the GUI (if you select "Generate Change Script" here's what you get): CREATE TABLE dbo.Employees ( EmployeeID int NOT NULL IDENTITY (1, 1) ) ON [PRIMARY] GO ALTER TABLE dbo.Employees ADD CONSTRAINT PK_Employees_1 PRIMARY KEY CLUSTERED ( EmployeeID ) WITH( STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] GO But they're essentially the same... if you need it to be a primary key, then sure, script it out as soon as you can... no need to "wait". To answer your "is it needed" question... almost every table should have a primary key. There's very few exceptions to this. All tables do _**NOT**_ need to have an identity... so if you're _always_ creating an identity, it might not be needed. Sometimes your primary key will be a composite of two foreign keys... sometimes it will be another number or code that doesn't necessarily work as an identity. Creating an identity does not automatically create the unique key constraints... so you would want to create that if your identity is not your primary key.
We're getting deep into how the SQL engine works now, but imagine a blank white page with 8000 spaces. That's how SQL stores data. If you have a table with 2 columns, firstname and lastname, SQL does not know which "order" they go in. It simply inserts the data on the page where there is a spot open. So if you insert data like 'Paul', 'Smith' and a second row 'John', 'Doe', when you select the data 'Paul Smith' will be the first row returned *unless* you add an ORDER BY clause to guarantee ordering. In that case, SQL will order the data alphabetically. Else it will return the data as it locates it on each page it scans through. Now say we add a 3rd column ID which is a simple integer (no key). But we insert our data "backwards". (2, 'Paul', 'Smith), (1, 'John', 'Doe). Now literally on the page in the database we have this: 2, Paul, Smith 1, John, Doe If you select from this table you will get the data exactly like that, UNLESS you add ORDER BY ID. Now you can guarantee the order in which the data is returned in your query. If you ORDER BY firstname, you will get the alphabetical ordering returned. If you delete ID 2 and insert it again it will get added after. This is why data is considered "un-ordered" in the database and ordering can only be guaranteed using ORDER BY. Indexing adds a additional page of data with only the columns included in the index. Just like pages of a book. SQL does not innately know that John Doe is on page 1 or 1000 of the book so without an index it has to scan every page. You could have deleted and re-added John Doe 100 times so it may be on the first or last page. SQL doesn't know (without an index). With an index on the ID column, now SQL can go to the index and find the matching page and return John Doe *much* faster, especially when there is a lot of data in the table. So really indexes make a huge impact on performance. I don't want you to confuse that with ordering. By default, SQL does not order data. It just creates it on blank white pages and (without an index) it must scan the entire table to find the record. Ordering is a function of RETURNING the data from the database. But the data by itself resting on those pages in the database could theoretically be on *any* page in *any* order. 
&gt; Unless the source already had the right clustered index I would require the caller to create an appropriate index, document the requirements, and throw descriptive errors if requirements are not met. &gt; Since we might have multiple date fields, we would still need the effective and termination date of the segment as input. I would define the first CIX date field as the EffectiveDate, the second CIX date field as the TerminationDate, any subsequent CIX date fields as KeyFields, and any other date fields as NonKeyFields. Again, this is merely an interesting alternative and not necessarily a preferable one.
&gt; where instr(receivers, '"' || user_id || '"') &gt; 0
This may be a really stupid question but where exactly do you enter SQL commands? For example, I am using an online guide and they let you practice right in the browser. In real world applications, where am I entering these commands? In Oracle/MySQL?
This is written such that it returns 10 results per classifier.
if you only have a 1:1 relationship, add the damn columns to the base table if you have a 1:0-1 relationship, and are worried about the volume of data, create a table with [employeeid] as FK to [employee].[employeeid], but don't assume that [phonenumer].[id] will equal [employee].[id]... the fact that you have gaps is INTENTIONAL, since you're worried about data volume... otherwise you just add the damn column to the base table if you have a 1:N relationship, same applies as above, except that you NEED the separate table because you might have multiple phone numbers (home/cell)... so of COURSE they won't align ID 1 = ID 1 this is basic stuff... IDs are meaningless except for associating rows.. they provide no inherent information... ignore the fucking gap and move on to issues worthy of the time/$$ you're costing
I may have been confusing it with ordering. But I definitely understand everything you're saying. So I definitely want indexing which is why I was trying to reset the EmployeeID, which I sort of figured out with the reseed statement. It allows you to begin the index or number where are you specify minus one, so if you need it to start at three, you specify reseed 2. I figured that out the hard way :p but I guess it's really not useful to take out the gaps because there still is an identity between the ID and the record itself. I guess gaps really don't matter. Because that ID can go to an infinite number anyway. I can have 155,312 as my ID‚Ä¶ And it would probably be a bitch to re-organize and re-order all of them just so it looks nice. 
Very helpful and informative! Thanks so much. What about the identity park? Do I always need that auto number? Basically, if I have EmployeeID, do I need to specify IDENTITY(1,1) every time? Like what happens if I make a table that doesn't have the auto numbering so it creates a new number every time? I guess I'm really confused about that
Are you coming from Access? Maybe MySQL? I haven't heard the term auto number in a long time. Anyway, if you leave off the identity you'll have to specify a unique EmployeeID every time you insert into the table. So instead of INSERT dbo.Employees (FirstName, MiddleInitial, LastName) You'll have to do INSERT dbo.Employees (EmployeeID, FirstName, MiddleInitial, LastName) And make sure your inserted EmployeeID value is unique and not null, otherwise you'll get a PK constraint violation. This might be a valid strategy in some scenarios, but normally if your PK column is an integer type you'll want it to be Identity (1,1).
When specifying an identity, the seed / increment "(1,1)" are required... however, if you don't specify it, it will default to (1,1). If you make a table with `EmployeeID int` but do not set is as an identity, you will have to manually insert a number each time you add a row. If it's also a primary key, you will need to make sure it's unique before you insert... so in the EmployeeID example, it makes a lot of sense to make it an identity. Using an identity (or some other arbitrary incremental number) is called a surrogate key. This is because it's not actually part of your data, but added to your data for uniqueness. A column that is unique and already part of your data is a natural key. This would be things such as Social Security Numbers, Product/Part Numbers, etc... There are a ton of resources out there about database design, best practices, etc... there's not going to be a single "right" answer for all cases. Here's a pretty short read with some examples: http://www.agiledata.org/essays/keys.html
Yes I know, but it's not doing that, sometimes it gives fewer than 10, although there are more than 10 than coulld be pulled. 
There are a bunch of other filters and inner joins and group bys that could reduce the data set. If you could share an actual execution plan, I might be able to tell which other part is responsible.
&gt;And it would probably be a bitch to re-organize and re-order all of them just so it looks nice. Not only that, it doesn't make any sense. There was a record identity 3. It existed at one time. To insert a new record identity 3 could create issues with historical data, or data in other tables that use that 3 as its foreign key. If you then insert a "new" 3, you are linking that NEW record to potential old history. Identity fields are intended to be a unique key, not a sequential key. Now, if on output of a query, you desire to have your rows numbered sequentially, you could use a ROW_NUMBER() function in your select statement and it will give you a sequential numbered series for each row - but it wouldn't tie back to your unique identity key.
u need to get a program that connects to the database engine, either mysql or psql, they're command line application names for respectively mysql and postgres database engines. there are other ways to send queries to the database engines. using a mysql or postgres driver library for python or visual basic for example are different ways.
I may just be fundamentally confused (probably from lack of experience) on how this works, but are you saying that you use something like mysql to input the query instructions and you connect it to a separate database (like a huge excel file, microsoft access file, oracle file, etc.)?
The only real reason I can think of to reseed a field like that is if you have so much ordering fragmentation, and you're close to exceeding whatever data type's size limit you have (for int, it's over 2 billion), and it's an auto-incremented field, and it's feasible to also update every other table with a relationship to that one, and you also don't want to upgrade the data type's storage size (i.e. making it a bigint or something). Basically, I don't know of any practical reasons for it. I'm sure there are some that more knowledgeable people could point out to me, but none really off the top of my head.
&gt; Not only that, it doesn't make any sense. There was a record identity 3. It existed at one time. To insert a new record identity 3 could create issues with historical data, or data in other tables that use that 3 as its foreign key. If you then insert a "new" 3, you are linking that NEW record to potential old history. I completely get what you're saying here. That is very true! Very good advice. In order to prevent that though, I set the relationship to cascade delete. So whenever there is a record of three or anything else, whenever it is deleted, it delete all of its other associated records through other tables. The only exception to this is a future table I have set up called orders. I set that one to null in the future because I don't want orders to be deleted when an employee or customer is deleted. In order to practice good accounting of those orders. With that being said, I definitely appreciate your advice. I won't be worried next time whenever I delete a record about reseeding. I'll just leave it as is. 
It's so strange because I noticed that‚Ä¶ If you don't specify the identity when creating the table, you have to add the numbers yourself. Which can be kind of annoying. But most tutorials I was watching didn't even specify that until I googled basically why wasn't auto remembering it and how to auto number it. Definitely going to read that resource though. Thanks for that 
Coming from access and Excel actually. Since I'm brand spanking new to database management systems, auto number is basically what I've always called Excel or access's index because they always keep the numbers tidy and in order. If you delete a row in excel or access I think they automatically update and re-index. Not too sure about the access part but I know excel does. So it's a little bit strange working with a database that doesn't auto number. That you have to set yourself.
If you could give some sample data along with an example of what you'd like the results to be, that would help. You don't need to give all 100 PostMeta columns - maybe just five or six of those columns.
If you're that bothered by missing numbers in records, I dare say you probably shouldn't be a dba :) It happens extremely often and is perfectly acceptable and expected. Ordering is completely irrelevant in this scenario. 
Try using [find_in_set](https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_find-in-set). Assuming those square brackets aren't actually part of the data, the following works for me: where find_in_set(userID, replace(Receivers, '"', '')) &gt; 0 If the brackets are in there, you can always strip those out, too. Here's a similar example from [stackoverflow](https://stackoverflow.com/questions/18710921/mysql-find-in-set-with-array-string).
Thanks for replying mate. Yar, my terminology is very loose, sorry for any confusion. First off, **your original suggestion actually works, up to a point**. I poorly executed it. My apologies on the continued mis-information. I mucked up on the GROUP BY portion. &gt;SELECT * &gt; , group_concat(meta_id) &gt; , group_concat(post_id) &gt; , group_concat(meta_key) as b_meta_key &gt; , group_concat(meta_value) as b_meta_value &gt; FROM posts a &gt; JOIN postmeta b ON b.post_id = a.ID &gt; WHERE post_type='product' &gt; GROUP BY ID The only problem I had was accessing the concated data, so I assigned an alias to it. If anyone is interested in this, from here I access the string in PHP, and then use PHP's [explode](http://php.net/manual/en/function.explode.php) to create an array. By doing this with both the key's and values, I create two isolated arrays. Continuing, I then join the arrays by using php's [array_combine](http://php.net/manual/en/function.array-combine.php), and then in theory I have a completed array of the meta keys and meta values. There is a problem with this though. I'm still trying to figure out what's going on, but for some products it's saying the created key/value arrays don't have the same amount of elements, so the combine isn't working. I'm trying to sift through the concated string and cross reference it with the table to see if somehow it's missing information. It's just tedious. These are the possible issues I can see right now. - Sometimes the `Meta_Value` can be completely empty, vs a Null. I'm not sure if this is an issue but perhaps if the last item is blank, it won't be registed in the concated string. - Explode targets a delimiter, the default is `,`. So if an entry has a comma in it, for example in a paragraph, this will confuse explode into reading that there is another entry, thus creating false entries which offbalances it with the keys. -- -- To add, you can change Explodes target delimiter to whatever you want. Additionally, you can change the group_concats default separator from a comma to whatever you want. &gt; , group_concat(meta_id SEPARATOR '*-*') I can possibly get this to work, but unless I have a completely unique Separator and restrict user input, and then somehow configure it so it gives presence to an empty entry at the end, this isn't a solid way to move. Not when the data entered is open to any form of input. Hell, I don't even know if this is a security issue on advanced levels. New Meta Keys can be added/modified/removed, and not all entries will use the same meta keys. So I really need to be able to isolate the individual all entries that relate to the post_id and place them into an array/object. This surely must be possible with SQL? Concating in this instance into a string just feels very inefficient, and almost reckless. Feels more backdoor ish. Even if I get this to work, I don't feel comfortable with this method, it feels like there is too much additional access to the information which potentially makes it susceptible to errors or being compromised. (not knocking the method overall! Just saying I don't think it's the best approach in this particular case) So...... I'm wondering, is it better to do two queries? Or is it possible to do one query, where I grab a single result from one table, and then a second search where I put all the results from the other table into an object that is included with the first table? &gt;object(stdClass)#19 (3) { &gt; ( [ID] =&gt; 1651 [post_title] =&gt; Italy (Copy)2 [2]=&gt; &gt; object(stdClass)#20 (22) { [meta_key] =&gt; _width [meta_value] =&gt; 1 [meta_key] =&gt; _length [meta_value] =&gt; 5 ... OR [_width] =&gt; 1 [_length] =&gt; 5 ... Here's a visual of the meta_post table layout in anycase &gt;`Meta_id` -- `Post_id` -- `Meta_key` -- `Meta_value` &gt; 1600 ------- 1890 ----- _weight ----------- 6 &gt; 1601 ------- 1890 ----- _length ----------- 12 &gt; 1602 ------- 1890 -----_colour ----------- red,orange,blue &gt; 1603 ------- 1890 -----_price ----------- 4356 &gt; 1604 ------- 1890 -----_origin ----------- Unknown &gt; 1606 ------- 1893 -----_weight ----------- 6 &gt; 1607 ------- 1893 -----_length ----------- &gt; 1608 ------- 1893 -----_colour ----------- red,orange,green &gt; 1609 ------- 1893 -----_price ----------- 67 &gt; 1610 ------- 1893 -----_price ----------- 45 *edit on the JSON /XML route, i think again this is creating more additional routes that arn't needed for this circumstance. The more I can stay tight to SQL normal procedure the better in the end. The more routes and data modification I add in, just creates more possibilities for problems. Plus it's not so clean. I'd definitely be interested in the methods, but as far as specifically handling product information, I want a clean line that doesn't deviate away from SQL - PHP's standard methods unless I absolutely have to. And at that point, I'd rather use two queries. 
Sure. I posted this in the above conversation aswell. &gt;`Meta_id` -- `Post_id` -- `Meta_key` -- `Meta_value` &gt; 1600 ------- 1890 ----- _weight ----------- 6 &gt; 1601 ------- 1890 ----- _length ----------- 12 &gt; 1602 ------- 1890 -----_colour ----------- red,orange,blue &gt; 1603 ------- 1890 -----_price ----------- 4356 &gt; 1604 ------- 1890 -----_origin ----------- Unknown &gt; 1606 ------- 1893 -----_weight ----------- 6 &gt; 1607 ------- 1893 -----_length ----------- &gt; 1608 ------- 1893 -----_colour ----------- red,orange,green &gt; 1609 ------- 1893 -----_price ----------- 67 &gt; 1610 ------- 1893 -----_price ----------- 45 Somethings to note. Some entries can be empty, and some `Meta_Keys` may not exist for certain products, and there can even be duplicate key_entries . So it's really up in the air. The only way to do it is to grab all the entries with a unique post_id, and throw them into an array/object. A string just feels like the wrong way of doing it. I'd prefer to have a multidimensional array in all honesty. The problem is, `Post's` has one entry, `Postmeta` has over 100 generally. So the queries can't be executed in the same way. I don't know much about sql, but is there a way to execute the query for `Posts` and then execute another query that sweeps `Postmeta`and places that information all in an isolated array? The more I'm writing this, the more I'm veering towards just having two individual queries. I think it would be nicer to have one though... I'm not sure though if that's possible. Vs security, or performance aswell. 
Use an EXISTS clause instead of IN for performance. Yes this is a small example but it's better to get in the habit of using methods to maximize performance
Not to diminish what others have said about design, I've been in a situation where I had to directly manipulate the primary key column. IIRC all I had to do was turn identity insert on, do my business, and thwn turn it back off. I only reseed on rare occasions and usually if something got all fucked up and I had to start over for some reason. 
The identity column as the primary key can devolve into a bit of a religious war. The terminology you're looking for is natural key vs. surrogate key. A natural key might be something like a social security number. There are additional reasons why you wouldn't want to use a SSN as the primary key, but as an example related to natural vs. surrogate: it *should be* unique and everybody *should* have one, but fraud, people refusing to give their number (so now you have to make something up as a placeholder,) people not having one (which is possible, but rare) or perhaps having to include newborns or immigrants who don't have a SSN yet, etc. can screw up your system in a hurry. And SSNs are just the obvious example. There are lots of things that seem like a good natural key out of the gate, but can cause some heartburn later. One of the issues years ago when hard drive space was a bigger problem was that a natural key could easily require more storage space than an integer value. Consider that primary keys get repeated as foreign keys in other tables and that primary keys and foreign keys are usually also indexed (explicitly or internally) and the space considerations begin to add up. There are also performance considerations. Joining on integer values is more efficient than joining on character values. This is somewhat less of an issue now than it used to be also, but if you're dealing with large data sets, it can also add up. Cascading updates help, but managing updates to natural, primary keys can be a pain. The biggest pain is when your natural primary key is a composite key of multiple columns. If that key is a foreign key in another table, all of the composite key columns have to be duplicated in the related table (space and performance hits) and updating the primary key gets really hairy. And just writing the join statements over and over in SQL queries on composite keys... ugh. Is it required to use a surrogate key in the form of an integer identity? No. But it is recommended by Microsoft and in my twenty-something years of experience it has proven out to be the best way to do things. Your mileage may vary.
Cascade deletes can be helpful, but that is not always the database design or architecture. What I am trying to communicate is that might work fine for your specific instance, but its not something you can or should rely on across the board. Also, if you ever find yourself working in an industry where you need to be able to audit historical data, this becomes a horrible idea too. Basically, it might be ok for a personal project, or some non-consequential application use, but it is a bad habit to have in general, so generally it's best to avoid it. If you don't mind me asking, what kind of work are you doing currently? Just self exploration? Or are you working on this for a business use? Always interested to know where people are coming from.
Don't overthink it. You could use LIKE with double quotes around the userid and wildcards on both ends. WHERE receivers LIKE '%"11"%' WHERE receivers LIKE '%"1"%' Hopefully this isn't searching a ton of data because indexes will not be used with LIKE '%%' or INSTR. 
That's not what a guid is at all. A guid is just a 128 bit integer and implicit promise that you'll pick a random number somewhere in that space. It's a number that's bigger than the number of atoms in the universe so by picking a random one, you can be pretty sure no one else will ever randomly pick the same one. Sure enough that we all agree to assume it won't happen. The beauty of this is that any one, anywhere can pick a new one, and presume it to be unique without having to check first. They're great keys for distributed systems. The problem with them is that their number space is huge and their distribution completely random over that space. This is bad for clustered indexes that stored in their numerical order. You can save space by not allocating empty slots to the huge gaps between the values, but since you can't predict where in the number space the next new value will be and the records need to be stored in order of pk value, you end up having rewrite at least one whole page on every insert (or update that would change the pk value (?!)). That's bad news for performance. The solution is if you have guid pk, make the index non-clustered. Better yet, have a normal identity pk and put the guid in a column with a unique constraint. Oh, and guids are super easy to sort, because they're just a really big integer. Unless you store them as strings. But then that's on you.
&gt; you're close to exceeding whatever data type's size limit you have (for int, it's over 2 billion), This is the only time I've ever re-seeded a table. One of the tables in our application creates and deletes records every few minutes and after a couple of years, it overflows and we have to stop a service, delete the records from the table (a few hundred typically), reseed it, and then start the service again.
I think we're talking about two different things: https://docs.microsoft.com/en-us/sql/t-sql/functions/newid-transact-sql
This explains it very well. Subsequently, an identity is just an automatic type of surrogate key if you will. 
Maybe add something like this to the where clause: AND order_shop IN (SELECT id FROM mt_shop.shops WHERE name = 'Test Shop - Best Shop')
Floats are stored as three parts. `N.umber * Base ^ Exponent` This gives it the flexibility to represent a wide possibility of values, but it doesn't store the value as written. For example, 29632.345 might be stored as 1.8086148 * 2 ^ 14 = 29632.34488. Real can stores smaller numbers in each part, and represent a smaller range of values, with less precision, at the benefit of a smaller size.
Why did you decide to separate out those two tables in the first place - it looks like you'll have one record per order in each table, so you aren't benefiting by breaking into two tables. I associate cascading deletes with star-scheme type tables, so this isn't necessarily the intended use for them. I'd recommend combining the tables. But if you want to keep them separate, I think you'll need to ensure your deletes affect both tables via your code (i.e. A trigger)
You need to make OrderID of OrderInfo not and identity column. It can be a plain old int primary key, but not an identity.
I did so on the recommendation by someone else to normalize my table because one of the rules of normalization is that no column depends on anything other than the primary key. For example, order info doesn't depend on order ID. Order info depends on the product. Customer ID and order ID however are interrelated. So I guess I was trying to normalize by splitting them up. 
What you have is called a name-value table or name value pairs and you want to pivot it. There is a SQL solution to it - depending on the database it may vary so search the internet you will find several answers using above terms.
You could use a union query to do that instead.
No, we're not. Guid is an acronym for Globally Unique IDentifier. The SQL Server keyword for the guid data type is uniqueidentifier. Guids are commonly represented as 4 groups of 4 hexidecimal digits because it's more convenient than a potentially 39 digit decimal number. Those 16 hex digits completely express the 128 bit value of the guid. If you turn all your guids into strings, you're just making everything more complicated because an integer is a lot easier to do computer stuff with. The string representation is just for us dumb humans (and ETL processes that don't want to worry about bit endianess between data sources).
Look into pivot tables. There is no standard support so you need to know your final column names and tease them out with cases. Postgres has some tools to help with pivot tables, too.
I dunno man, you are saying a lot of things that pretty much go against everything I'm finding on the internet. The simple fact that GUID is 4x the size of int is enough to tell me that int will perform better in the vast majority of situations. GUID is great if you need to guarantee uniqueness of the id across servers and... not better for much else.
I'm so glad someone is asking this. It's always confused me. It sounds like you have a grasp on how it's stored, but could you explain WHY the actual value isn't stored?
Less space per value in certain cases.
Ah got it - so do you have many records in OrderInfo for every one record in OrderNumber (ie a customer buys multiple products in the same order)? If so, then splitting them up is correct. If that's the case, then what u/jc4hokies says is correct - you don't want orderId in OrderInfo to be an identity - its value will be determined when you insert into OrderNumber.
Yes, exactly. Now you understand why I split them up. I wanted to represent multiple orders for the same customer
an electronic database system like sql is a program that accepts commands and then makes modifications to data tables for you. excel or access provide u with graphical user interface to perform these actions. when you need a system that automates and performs complex combinations of tasks on data tables you use sql. so of course, you need to have access to the command input of those database programs. for mysql, you generally type mysql in a bash or other command line program. for postgres, it is psql. there are other programs or ways to write instructions to those database systems. for example you can save the desired commands into sql script files and then use another computer language such as visual basic or python to schedule running those files. there are also libraries written for other languages that abstract the operations you do in sql for u so u never need to write sql code directly. it's a very broad topic and there are tons of books and resources online. i recommend just reading everything u can find first to give u a big picture.
Floats can simultaneously store extremely big numbers (far away from zero) or extremely tiny numbers (lots of decimal points). Exact data types can be defined to do one or the other, but not both. Basically, a float can adapt to whatever scale best suits a particular number while an exact datatype is required to store every theoretical value in the range it is defined. This query demonstrates the range of values capable between decimals and floats of the same size. The substring strips off the metadata from the decimal so it's apples to apples. SELECT SUBSTRING(convert(varbinary,CONVERT(decimal(19,0), 9999999999999999999)),5,9) UNION ALL SELECT SUBSTRING(convert(varbinary,CONVERT(decimal(19,19), 0.1111111111111111111)),5,9) UNION ALL SELECT CONVERT(varbinary,CONVERT(float,3.40E38)) -- 3400000000000000000000000000000000000000 UNION ALL SELECT CONVERT(varbinary,CONVERT(float,1.18E-38))--0.0000000000000000000000000000000000000118 --- Binary| :-| 0xFFFFE7890423C78A| 0xC771C42BAB756B0F| 0x47EFF933C78CDFAD| 0x38100FB32C6204C4|
Right. Identities avoid the tedious work to generate unique keys manually in multiuser environments where multiple inserts could compete for resources used to track the values. Identities come with their own issues. You don't know the value until the insert is complete, so there's an additional communication with the database or stored procedures involved to capture and return the identity value. Most modern ORMs like Entity Framework deal with this behind the scenes so it's less of a concern. On balance, the conveniences identities offer outweigh the issues.
thanks man, i ended up stuffing all the column names into a variable and then using a dynamic pivot statement. Very cool stuff, I feel silly for not thinking of a pivot function myself! 
Simple: You need an OrderInfoId column in OrderInfo that is the identity column. I'd also recommend the same thing for the Department table. Every table should have its own Id column not dependent on another table for its identity.
Is your SSMS on the latest version?
My first thought too - often see this error if the version of Management Studio is older than the version of SQL you're connecting to.
...That's exactly what I said. A Guid is just a really big number that every one agrees to pick at random. You were saying all this stuff about being hex strings and difficult to sort. They're super easy to sort. They're very difficult (theoretically impossible) to predict what the next one will be but once you know what it is, it's trivial to sort. And yes, you don't need a 128 bit number space when you can rely a single authority to ensure that IDs are unique, like when you're working inside in your own database. That's why I said it's generally not ideal to use a guid as the PK on your tables. Guids are actually a really simple idea (a promise to pick a random number out of huge range of possibilities) that solve one specific class of problem (distributed asynchronous uniqueness). I find that a very few developers have a good understanding of what they are and what they do.
As the docs say, REAL is FLOAT(24).
The whole line of 16.x has been buggy for me. I haven't installed 17 yet, ended up going back to 2014 SSMS. 
My only thought here is that maybe the employees table does not have a location_id column. That is an oddly written query, at least to me though. I think instead I would have left/inner joined employees to departments and used the departments field directly in the CASE statement.
The example is from the Oracle SQL Fundamentals, hence why I'm trying to see why is there and if it's a practical one..
Ah, I'm really only familiar with SQL Server so hopefully somebody has a better answer than I do.
its oddly written and with large volume of data it will never return because it executes the subquery for each row returned from father query. it should be with left join and the case statment should be with null / not null. also the query provided doesnt take into consideration there might be many depaertment_id's for location_id = 1800 
You shouldn't use 1800, you should use whatever the sub query returns... presumably the departmentID and locationID aren't the same number But yeah, this seems like an odd thing to do unless the departments can move locations - and even then it would be better off as a join not a sub query (I try to limit sub queries to quick-and-dirty debug queries that will only be run manually from a console
To be honest this query was written by someone who does not understand JOIN's in SQL. If for every employee record there was a respective departments record he should have used JOIN or a LEFT JOIN if there were records missing in departments. 
Under what section is it? may be its trying to explain usage of sub-query, if it is I must admit its a poorly chosen example.
It's under Scalar Subqueries: Examples , from Oracle Database 11g: SQL Fundamentals II .
I think that explains they only wanted a query(sub query in this case) to return only one value which explains the =1800. Its just a bad example when you understand how the engine works but for someone who is new to SQL it still acts as an example to understand the concept.