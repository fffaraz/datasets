The windowing functions - LEAD and LAG. Cursor and self join killers!
You're just asking a variation on [your previous question](https://www.reddit.com/r/SQL/comments/9b1pjy/what_are_simple_alternatives_to_use_other_than/). Both versions are all over the place and make no sense.
I did create the composite index but my query is still taking long time to return. My query looks like &gt; SELECT * FROM TableA A JOIN TableB B on A.F1 = B.F1 and A.F2 = B.F2 and A.F3 = B.F3 WHERE A.F4 = 'ABC' A.F4 is indexed as well.
I did create the composite index but my query is still taking long time to return. My query looks like &gt;SELECT * FROM TableA A &gt;JOIN TableB B on A.F1 = B.F1 and A.F2 = B.F2 and A.F3 = B.F3 &gt;WHERE A.F4 = 'ABC' A.F4 is indexed as well.
Ew.
CTRL+G
Show your table schemas and the indexes.
Have you used the explain plan function to see if it using the indexes correctly? Docs: https://dev.mysql.com/doc/refman/8.0/en/explain.html This will tell you how your query is being executed, what joins are being made and if they're using indexes etc
To be fair, the documentation you linked to *does* suggest a hash of concatenated columns as a possibly-faster alternative to a composite index.
I wouldn’t say to give up now if it’s the latter. Yes, it doesn’t sound like he understands the mechanics of developing the front end of this project, but understanding the back end can go along way. I would recommend they run with the project as far as they can go. If they can’t complete it themselves, hopefully they understand the backend side, and can bring on a partner to do the front end work. 
Before updating records validate your rows with a SELECT statement. Once I’m comfortable with the results I use the exact WHERE clause in my UPDATE. Also, CURSOR. Took a bit to wrap my head around it but so useful.
That looks okay... Next step, looking at the query plan to make sure that composite index is being used.
Power BI
Thanks for your assistance. I'm not sure why but the same query is returning instantly now. Not sure if something else was affecting the performance of the server. The only thing that changed was that I added a new field to one table. After that finished everything is moving in milliseconds. &amp;#x200B; Query plan meant execution plan? Two Non-Unique Key Look coming into a nested loop. &amp;#x200B;
Regex functions and replacing sub-queries with tables. I can then run unit tests on the tables and the logic is so much ch easier to read and understand. Also, the book ‘The Clean Coder’ by Robert C. Martin. 
I'm not sure if MySQL has this function, but in Oracle if you run the same query more than once, it can cache it and it'll return really quickly.
It also caches the SQL query plan such that new indexes are not immediately used. It could be that the plan aged out and then finally took the index into account on the next calculation, creating a new query plan.
Ah yes, I didn't take that into account! Good info for this scenario.
Also using the rownumber and parition by/order by along with recursive cte has helped me soooooo much. 
I do a lot of data warehousing now and sequence objects have been indispensable.
COLLATE DATABASE_DEFAULT
My latest approach to Oracle PL/SQL's painful "exists" check. select max(f_pk_field) into v_exists from t_test where f_pk_field = value; So that you get one row back even if no row would otherwise be returned. I'm sure this isn't new but I came up with it after tiring form the common practice at my work of an empty join to dual for the same result. 
Excel. Access... And everything else people mentioned. It looked like alot of the comments left these off. 
Hey, ramborocks, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Just want to say thanks for the great explanation!
ALT + X also executes a query!
You should never rely on predicting ordering of data. If you need a specific ordering in your query, always explicitly order the data in the query.
Can you say more on this, please? I see it around but I don't know what it is? What's your experience with it?
From experience, sound logic helps. You want to be able to run semantically correct queries (I cannot emphasize this enough). Also, knowing your server helps. I once ran a DISTINCT on MySQL via workbench that produced different results each time. I later found out a setting which was on by default required that I use GROUP BY... so I would say, the ability to look for information helps. Another thing I strongly recommend is being able to communicate clearly- never be ambiguous. Actually, I feel like a huge part of being promoted or taken seriously is how you relate to your superiors and even those junior to you. I have seen highly competent but introverted individuals not get a promotion in favour of a decent DBA but an overall likeable guy. You want to be approachable but at the same time not easily swayed. You want to be firm but at the same flexible. &amp;#x200B; That's all I got. Other than that, make a habit of documenting processes and making data dumps regularly. Never underestimate the power of DML.
PowerBI is also a pretty desirable front end skill these days!
Essentially what used to be SSRS, but better
[CREATE TRIGGER](https://docs.microsoft.com/en-us/sql/t-sql/statements/create-trigger-transact-sql?view=sql-server-2017) is probably what you are looking for.
If I get it right, the CREATE_TRIGGER is used on a database-event, which is a manual event (insert, update, delete...). What I'd need is the event to be "the query send a result"
I looked it up, had no idea Power BI was an MS product. I saw a presentation at work last work and got curious but not enough to actually look it up until I saw it mentioned today
Wonderful explanation! Thanks man. Somebody give him a gold
And what do you mean by " someone is making let's say 3 transactions in 1 minute "? aren't those insert/update/delete events? &lt; What I'd need is the event to be "the query send a result" &gt; doesn't sounds very clear.
CLR functions made things a ton easier for me. At my previous job, our customer wanted to intuitively search across a large dataset of encrypted strings, but it was ungodly slow on the app side. So I whipped up a string distance and metaphone function that brought searches down from 15 seconds to sub-second times. Also allowed users to enter *anything* into the search bar and the function would use regular expressions to infer which field(s) to search. Apply weighting and ranking and boom, qualified search results.
If you use SSMS, I recommend SSMSBoost. Even the free version has good features but the paid version is a game changer. Too many things to list! SSMSBoost.com
I saw business analysis in posts above, but I see a project management experience / education mentioned in about half the job posters I see. If you don't have a lot of project hours, a CAPM from the Project Management Institute is one way you can demonstrate knowledge and desire to learn. You can use it as a stepping stone to the more widely recognized PMP. 
This is something I've never been able to quite grasp. I've never found a good example of when to and when to not use it.
Would that not result in more lines to type? My example is probably bad as it doesn't have to be PK field in the WHERE, rather just a unique field. In many cases the existing serves as a lookup for a PK as well as an exists check and my main goal is to have less to write and read.
Log the value to a table, then on each subsequent run you can check for val+1
An EXCEPTION block is standard for PL/SQL, so anyone else reading your code is going to expect it. Especially when doing a Singleton Select - you can pretty much assume each of these is going to come with a boilerplate NO_DATA_FOUND and TOO_MANY_ROWS exception handling. If this fits your goal then that's fine, but if I seen this is my own environment I'd rewrite it. It's more lines of code but it's standard, and therefore, to me - easier to read. First pass at this in a large PL/SQL package would probably have me skip over what was actually trying to be achieved. 
This man deserves a sql donut. You are so so right. 
It's called "SQL 2016 Database Development". The 761 is essentially how to write T-SQL and understand functionality in T-SQL. The 762 is understanding SQL Server's internals of T-SQL. That's my light summary on the cert, it requires two exams to achieve. https://www.microsoft.com/en-us/learning/mcsa-sql2016-database-development-certification.aspx
 LEFT OUTER JOIN ( SELECT client_id , COUNT(*) AS qt_debts , SUM(value) AS value_debts FROM debts AS dd WHERE datecolumn = ( SELECT MAX(datecolumn) FROM debts WHERE client_id = dd.client_id AND datecolumn &gt;= '2018-01-01' AND datecolumn &lt; '2018-03-01' ) GROUP BY client_id ) AS d ON d.client_id = c.id 
ctrl + r ............ I think I love you. thank you kind stranger! 
This is fine until you forget to run the rollback or commit and leave an open transaction on a production server blocking a ton of queries, and do that enough times that your whole departments prod access gets revoked
This moment. Right now. Thank you everyone.
Sometimes sp_msforeachdb (MSSQL) silently fails and I don't have as much control on it like I wish I could, and I can't stand cursors. So I've learned to use @@Rowcount for quick code I need to run in batches on all DB's, like so: declare @dbname sysname, @query nvarchar(max) select name into #todo from sys.databases -- 'seed' the initial @@ROWCOUNT select top 1 @dbname = dbname from #todo -- the better than cursor loop I prefer while (@@rowcount &gt; 0) begin set @query = 'do something with [' + @dbname +']; exec(@query) delete #todo where dbname = @dbname -- reseed for next loop select top 1 @dbname = dbname from #todo end
Window functions are the only thing I can think of when I think about the biggest change in my queries. They make *everything* so much easier. I haven't even considered using a group by since. Maybe sometimes a window function is a little more wordy - but it's always perfectly clear what your intention was and altering a table doesn't effect the query. Makes code so much more readable and intentions clearer.
Honestly, I use Applies all the time and I couldn't tell you the difference between it and a joined subselect. But the biggest benefit I've seen is anytime I need to join and use a top(x), I use an apply. Makes it so much easier to write, more efficient, and easier to read.
I think....I think that's not right. Distinct definitely has a place. But using it on huge data sets is a real efficiency killer. I agree too often DISTINCT is tossed in as a terrible band-aid, but keep an open mind. Also....I know this should be to the OP but I can't think of any English speaker/reader that would think GROUP BY is easier to read/understand than distinct...
Not sure what DB engine you're using, but you could set the query up to run automated. For instance on linux you could do this in a rudimentary way using cron. SSRS on SQL Server could also do this. If the query is small then it shouldn't be a problem to run this constantly. The other option is to use a trigger (as others have mentioned). Every time a row is inserted (a new transaction) you could make some logic that checks the last transactions of that ID and output a value to a file. There are many ways to do that, it depends on your table structure and what you have access to. 
I think you my have your own definition of "standard" like many others I've encountered in this field. I use exception blocks where they're needed. I avoid them when they're not. To argue this isn't easy to read code is a stretch, especially when in context with the rest of the code. Changing code because it skins a cat differently than you would is generally frowned up in my environment, both by Devs and especially by QA who hardly want to test cosmetic changes. Like any idiom, once you get to know it, you won't have trouble reading it. If you skip over it once does that mean its useless? 
Control-K-C for commenting out line. And Control-K-U to un-comment it But still, I find this useful - I didn't know SSMS would catch your closing block comment even if it was commented, that's pretty cool. Thanks for sharing.
This is going to have a much cleaner execution plan than your above solution. select * from accounts where (acctno,paydate) in (select acctno,max(paydate) paydate from accounts group by acctno)
Thank you!
Keep in mind we know nothing of your backend (what is creating these transactions) so we have to make some assumptions; * Every transaction has a UserID linked to it. * Every transaction is timestamped. * Every Transaction is given an auto generated transaction ID Now that that is out of the way take this into consideration. A transaction. As far as the DB is concerned is an INSERT statement. So creating a trigger that on INSERT fires off a function that goes "Hey, This User ID just made a new transaction, are there any other transactions from them? and if yes then what is the time difference between them?" Alternatively you could make the trigger write the User ID, Transaction ID and Timestamp to a tracking table then have an make an external program that sits in the tray or runs as a service and basically runs a query on that table every minute or so looking for timestamps that are too close and then emailing when it finds some. Not every SQL thing needs to be done **in** SQL
They're kind of different things but they tend to do the same thing: Reduce to a set of unique values. You can apply a GROUP BY without an aggregate function and receive the same result as DISTINCT in a shorter time. I haven't found a case where I "had to" use DISTINCT over GROUP BY.
I'd normally do this with a JOIN to an the account and a maxdate from a sub query. SELECT a.* FROM accounts a JOIN (SELECT AcctNo, MAX(PayDate) as Mdate FROM accounts GROUP BY acountnumber) md ON a.AcctNo = md.AcctNo AND a.date = md.Mdate Keep in mind that if PayDate is same on an account for multiple Reps then this will return a row for each of those reps.
Power BI consultant here. AMA. Microsoft Power BI is Microsoft's new modern reporting platform that's included with Office 365 E5, or as a standalone license for $10/month. It is very easy to pick up if you're interested in a skillset that's in high demand. I would recommend the book "The Definitive Guide to DAX" to learn the language used by Power BI to create measures and custom columns. There are lots of resources online to learn how to develop reports using the free Power BI Desktop tool. Gartner has recently rated Power BI as being a more complete BI platform than the other top platforms, Tableau and Qlik. It's data modeling capabilities are unparalleled, and when you deploy it to the cloud, can leverage some amazing Machine Learning capabilities. Look for free Dashboard In a Day workshops in your area for free training on Power BI. BTW, it's NOT a replacement for SSRS, which still exists as part of the Microsoft BI stack for on-prem SQL Servers, or Power BI Premium customers. Power BI is meant for ad-hoc analysis, whereas SSRS is best used for pixel-perfect paginate reports. 
Agreed that Power BI shouldn't be left out. Many companies are moving from Tableau and Qlik to Microsoft Power BI because it can do everything those other platforms can do, and is FAR more affordable. 
Are you filtering on the `select`, or just copying all records from the source table to the destination tables? Are you column datatypes sized appropriately (not using varchar(255) for everything, etc)?
Hey man haven’t been on reddit the past 2 days. Did you get this figured out?
It got pretty slow, so i tried a different approach: JOIN ( SELECT client_id , max(datecolumn) AS date_submitted , COUNT(*) AS qt_debts , SUM(value) AS value_debts FROM debts AS dd WHERE datecolumn &gt;= '26.11.2008' AND datecolumn &lt;= '27.11.2008' GROUP BY client_id ) AS d ON d.client_id = c.id Its working, but i noticed it will only use the most recent date submited, i am not sure on how to fetch the older date. but i dont think that should be an issue. If i wanted to sort name by alphabetical, where should i use the ORDER BY clause? 
It got pretty slow, so i tried a different approach: &gt;JOIN ( SELECT client_id , &gt; max(datecolumn) AS date_submitted , &gt; COUNT(*) AS qt_debts , &gt; SUM(value) AS value_debts &gt; FROM debts AS dd &gt; WHERE datecolumn &gt;= '26.11.2008' AND datecolumn &lt;= '27.11.2008' &gt; GROUP BY client_id ) AS d ON d.client_id = c.id Its working, but i noticed it will only use the most recent date submited, i am not sure on how to fetch the older date. but i dont think that should be an issue. If i wanted to sort name by alphabetical, where should i use the ORDER BY clause? 
It got pretty slow, so i tried a different approach: JOIN ( SELECT codcliente , max(DATAEMISSAO) AS DATA_EMISSAO , COUNT(*) AS QT_VENDA , SUM(valorpago) AS VALOR_VENDA FROM PARTICULARVENDA AS dd WHERE datapagamento is null AND DATAEMISSAO &gt;= '26.11.2008' AND DATAEMISSAO &lt;= '27.11.2008' GROUP BY dd.codcliente ) AS v ON v.CODCLIENTE = c.CODCLIENTE Its working, but i noticed it will only use the most recent date submited, i am not sure on how to fetch the older date. but i dont think that should be an issue. If i wanted to sort name by alphabetical, where should i use the ORDER BY clause?
You've lost me. Got a pseudo code example? 
How can i order clients alphabetical? 
Thank you u/Shortywz and u/IgneSapien. I suppose anything is possible, however, I've never actually seen an occurrence of that. Generally pay dates are spaced a month apart.
TSQL: WITH SkillsOrdered AS ( SELECT \* , Level\_Ordinal = ROW\_NUMBER() OVER (PARTITION BY \[Level\] ORDER BY NEWID() FROM Skills WHERE \[Level\] =&lt;Level of player&gt; ) SELECT \* FROM SkillsOrdered WHERE Level\_Ordinal &gt;=2 &amp;#x200B;
Just to be clear, I know there is a place for it. Whenever I see it in the wild I always analyze the query and see if it is even capable of creating duplicates. Usually it's not. If it is, I then see if a null or something else in the data will kill it's uniqueness. This is all because I've worked with shitty devs in the past who just throw a DISTINCT in when they have dupes, and it works in that single scenario then later on with changes to the underlying data we end up with dupes.
Ah thank you but I got it working by using the ORDER BY RAND()
There very well could be better ways to do it. I usually stick with something like this because I think it's more explicit and readable than other methods I've found, and it's performed better at least in my datasets: select c.ClientID ,c.ClientName ,so.SalesOrderID from tblClients c outer apply( select top(1) so.SalesOrderID from tblSalesOrder so where so.ClientID = c.ClientID order by so.SaleDate desc ) so just makes it quick and easy to get the most recent sales order for each client, and it's obvious that's what I was *trying* to do. 
When a Singleton Select is used and no rows are returned it’s an exception. The standard way for handling exceptions is with an EXCEPTION block. It’s going against the grain of how this should be handled, and if in a body of thousands of lines of PL/SQL package code - having to find where someone has written a creative way to handle an exception when there’s already a standard way defined to do so is in no way easier to read and screams “Hey look how cool I am”.
In SSMS, Query Designer - Change Type makes insert and update (specially for lots of column) very easy to write
I tested using a DISTINCT versus a GROUP BY for the same query, and got the same execution plan. So, like most SQL Server questions, the correct answer is "It depends."
One thing I use CROSS APPLY for a lot is if I'm doing a calculation I don't want to repeat multiple times like something where I have a multi-level deep nested SUBSTRING or similar. If you do a cross apply it becomes cleaner as in the below simplified example SELECT SUBSTRING(@string, LEN(@string) - CHARINDEX(@string, @string2), LEN(@string) AS myString FROM Table AS t Can become: SELECT SUBSTRING(@string, val.stringLength - val.startingPoint, val.stringLength) AS myString FROM Table AS t CROSS APPLY ( SELECT LEN(@string) AS stringLength ,CHARINDEX(@string, @string2) AS startingPoint --more here ) AS val &amp;#x200B;
This is a better answer because it bulletproofs against the scenario where two entries for the same account for the same date exist.
Similarly, STRING_AGG finally came out in SQL Server 2017. Amazing how easy life is now.
I mentioned that. &gt;As you can see I've ORDER BY on the depname. If my data in the table is already ordered, then you can just remove all the ORDER BY statements you see below. However, I conceded that it's disingenuous to talk in general and then toss something very specific in there without doubling back and covering what I meant there. Perhaps I should have put. &gt;As you can see I've ORDER BY on the depname. If my data was coming from a view that already orders, then you can just remove all the ORDER BY statements you see below if that order within the view agrees with the order that you are after. Same goes for any table that is explicitly built with ordering in mind like materialized views (for databases that allow materialization with a logical presentation) or script/program/service driven tables which happens in a subset of some mainframes and minicomputers (ie AS/400). Again, all of those are **very specific** circumstances and I could have just avoided all of this by saying. &gt;As you can see I've ORDER BY on the depname. You should always order your data, there are a few exceptions to that, but they are so limited that you should just go ahead and use ORDER BY anyway.
1. **Poor Man's T-Sql Formatter** plugin for Notepad++ 2. Using the **INFORMATION\_SCHEMA** or SYS views to create SQL from table and column metadata. For example, I use this query all the time to profile NULLs in a table. When the resulting text is executed, it shows the number of records in the table, and the number and percent of NULL values in each column: &amp;#8203; DECLARE @TableSchema NVARCHAR(50); DECLARE @TableName NVARCHAR(50); SET @TableSchema = 'finance'; SET @TableName = 'transaction_ledger'; SELECT -1 AS ORDINAL_POSITION ,'select sum(1) as NumberOfRecords' AS SQL_Text UNION ALL SELECT ORDINAL_POSITION ,', convert(nvarchar(50), sum(case when [' + [COLUMN_NAME] + '] is null then 1 else 0 end)) + N'' ('' + convert(nvarchar(50), (sum(case when [' + [COLUMN_NAME] + '] is null then 1.0 else 0.0 end) / sum(1.0)) * 100.0) + N''%)'' as [' + [COLUMN_NAME] + ']' FROM [INFORMATION_SCHEMA].[COLUMNS] WHERE [TABLE_SCHEMA] = @TableSchema AND TABLE_NAME = @TableName UNION ALL SELECT 4242 ,'from ' + @TableSchema + '.' + @TableName ORDER BY [ORDINAL_POSITION] &amp;#x200B;
Yes! Sequences can be so powerful but it seems like people don't know about them. They're great for getting an ID into your application before actually inserting data, without risking re-use of the ID.
Could you go into detail? 
Who says its an exception? I want a record or a null record, not an exception.
Adam Machanic presenting *SQL Server Query Plan Analysis: The 5 Culprits That Cause 95% of Your Performance Headache*: r/https://www.youtube.com/watch?v=bS0q1nBP3As My favorite video on SQL Server, query plans, query processing, and query tuning. "Hey Lazy Spool, do you have any information about this Product ID?" (at 37:10) Makes me laugh every time.
[statisticsparser.com](https://statisticsparser.com), in conjunction with SET STATISTICS IO, TIME ON for analysing page reads.
Well that was thoroughly unconvincing. I can only wish you many more years of avoiding cool guy coders like me. 
There are a bunch of different SQL IDEs that you can use. Using just a terminal is kind of a nightmare unless you're just doing very simple queries. Look up datagrip or mysql workbench. That's the easiest way to interact with a database.
Yeah the server (mysqld) is just a background service process, and clients connect to it to run queries. The included client is just a command line tool (mysql). And of course there's GUI clients like mysql workbench, dbeaver, heidisql, datagrip etc. VSCode is a text editor for programmers that don't want a full on IDE. But it doesn't have any SQL features aside from syntax highlighting when you edit .sql files (as far as I know). If you're mainly doing SQL, you're better off using something that will actually run and display the queries too. If you happen to have a jetbrains IDE (I use phpstorm) you might already have that functionality. Otherwise use dbeaver or mysql workbench (if you only need mysql).
I'm not sure I understand what you mean by it "running through" CLI. That the server is a command line program? MySQL has a basic command line shell, but you're able to interface with the server via anything that can establish a SQL connection e.g. VS Code, MySQL Workbench, Atom. I don't think you want to be executing queries directly via CLI, sounds like a pain. MySQL is just the server. You can download MySQL Workbench separately, which is the native IDE. A text editor (e.g. Notepad++, though maybe there are plugins for it) alone wouldn't let you establish a SQL connection. The definition of client vs. IDE seems a bit fuzzy, I guess an IDE is just fuller featured. Something like VS Code (or presuambly Atom) would let you connect to the server, write queries, view results, and debug. An IDE like MySQL Workbench should be fuller featured, and allow that plus a tree view of server objects, graphical representations of query plans and schemas, GUI shortcuts for e.g. backup and restore tasks etc.
You just need to store the prior result somewhere, e.g. a table.
The easy answer is not to test ad-hoc queries on prod ;-)
We use it for a call center with live updates to call status and incoming calls. In my other life outside of work, I use it on the reporting side of things. 
The order of columns really does not matter that much, but you can add the keyword FIRST to make the new column appear first, if you want. Check out https://stackoverflow.com/questions/14753321/add-auto-increment-id-to-existing-table
You keep forgetting to drop the #temp table before running the script? There you go. IF OBJECT_ID('tempdb..#Table') IS NOT NULL DROP TABLE #Table CREATE TABLE #Table (...)
MySQL Workbench is free and fine. Datagrip is not free and amazing.
 select * from table1 t1 left outer join table t2 on t1.value1 = t2.value2 and t2.date1 &gt; cast(current as date) where t2.value2 is null; Do a left outer join on the second table, and use "where anycolumn in table2 is null"
 SELECT E.PERSON_ID, E.SUBSCRIBER_ENROLLMENT_ID, E.START_DATE, E.END_DATE, E.ELIGIBILITY_ID FROM CLAIM.ELIGIBILITY E WHERE E.STATUS = 'A' AND E.PERSON_ID IN ('43821623', '38674689') AND (E.PERSON_ID, E.MAX_START) IN (SELECT PERSON_ID, MAX(START_DATE) as MAX_START FROM CLAIM_ELIGIBILITY GROUP BY PERSON_ID);
Okay this is interesting. Correct me wherever I'm wrong, trying to walk through this. So there's mysqld, or MySQL server, and then there's just MySQL, the client and simple command line tool. Now, when you say MySQL server (mysqld), you don't mean a storage server, or whatever you might call it, correct? For instance, Oracle is not storing all of the databases I create via MySQL in their servers. Those are stored locally somewhere in my directory. 'The server is a background server process...' So I need to turn this on whenever I intend to run queries. This is what, I suppose, takes my client input and makes the magic happen? Right now I'm mainly doing SQL. In time I'd like to complement SQL with an appropriate language (R, php, and python seem to come up most often in that conversation), but baby steps first lol. Assuming I'll work my way towards a text editor, what client would you say is best for beginners? Dbeaver or Workbench? Suppose I should mention I am on a Mac.
Okay that helps a lot. If I'm synthesizing all this info correctly, MySQL comes with basic command line client, but obviously nobody wants to work via CLI. I just need something, a client or an IDE, that will let me establish a connection to the MySQL server. A text editor alone won't let me do this. It seems like there are clients/IDEs that are developed solely for SQL, e.g. Workbench, Sequel Pro, dbeaver, datagrip, etc. These would allow me to run queries and view the results (and more) without the need to work in command line, so one of these is what I'm looking for. I did think Atom and VS Code were text editors though? All of that being said, do you have a recommendation for a client to choose? E.g. Workbench, Sequel Pro, dbeaver, datagrip, etc.
SELECT distinct E.PERSON_ID, first_value(E.SUBSCRIBER_ENROLLMENT_ID) OVER (PARTITION BY E.PERSON_ID order by E.START_DATE desc) , first_value(E.START_DATE ) OVER (PARTITION BY E.PERSON_ID order by E.START_DATE desc) , first_value(E.END_DATE) OVER (PARTITION BY E.PERSON_ID order by E.START_DATE desc) , first_value(E.ELIGIBILITY_ID) OVER (PARTITION BY E.PERSON_ID order by E.START_DATE desc) FROM CLAIM.ELIGIBILITY E WHERE E.STATUS = 'A' AND E.PERSON_ID IN ('43821623', '38674689');
Thanks so much for the distinction between an editor and a client. Was still scratching my head on that one. I guess, just to get a full understanding of the lay of the land, where would that put an IDE? Something like a supercharged text editor? Yes sounds like I'll go with one of the clients to start. So when I 'connect' to a database, what does that mean/entail? For example, assuming the database is local on my desktop, why would I need to connect with it? Aside from literal access to the database I suppose, but it sounds like much more is going on that that. Connecting has to do with connecting via MySQL server? Or something?
Just google what you want to do until you have it down pat. That’s how I learned SQL and I consider myself advanced at this point, approx 2 years after getting a job that used it. 
Supercharged text editor is probably a good way of thinking about it. IDEs will have more features geared toward code editing that something like notepad wouldn't have, like code folding, autocomplete, syntax highlighting, finding the close parentheses, etc. &gt; assuming the database is local on my desktop, why would I need to connect with it? Because all "server"-type programs are coded to operate that way regardless of the setup environment. They have a constantly running loop that always listens for incoming connections/requests. I agree that having it on the same computer confuses things. You could run a webserver on your laptop and you'd still need to make http GET requests to it, because that's what it's programmed to do. Your client software will have some kind of config where you'll need to enter the connection/host details. In your case, it's usually "localhost", or 127.0.0.1, plus the port number (I forget what port MySQL runs on), and then user login credentials. 
This is why I ALWAYS follow this pattern 1. highlight and run the begin statement and query. 2. Verify # of rows affected 3. highlight and run the commit statement. (note - the line is still commented out) 4. verify rows affected 5. run commit statement again 6. verify you get the error - "The COMMIT TRANSACTION request has no corresponding BEGIN TRANSACTION." No danger of leaving an open transaction, assuming I'm still breathing (normally).
Will the optimizer treat the "and t2.date1 &lt; cast(current as date)" the same in the JOIN as it would in the WHERE? I think it will, but I've never actually tested it.
This. I would say it took me about two years to get to what i would consider an intermediate or somewhat advanced level after working in my job that uses it everyday. Google is your best friend.
LEAD and LAG are amazing, as are FIRST_VALUE and LAST_VALUE. They are replacing queries that I would have chosen to write in MDX against the cube layer in the past.
About 8 years and counting. I'm still and probably will forever be learning SQL. How long until I felt really competent at my job? It varies based on the day. Some days I feel like a wizard, other days I feel like a total noob. What matters is that you know what you need to know to do your job and do it well. After that, it's all gravy to the company. I would suggest to always continue learning forcing yourself to advance. Technology is not the market where you want to stagnate yourself. Now, gun to my head, how long did it take to be alright to good? Probably 3ish years, one year was quite a bit of SQL. The 2nd year was off and on, but lots of technology learning, and the third year was heavy duty SQL with lots of good teachers around.
I only ever used it with oracle in production, mainly because it's insane how oracle makes you do an update with a join. 
Basic SQL is easy. You can learn stuff like basic SELECT stuff, filtering with the WHERE clause, joins, etc. in a few weeks, if that. This is just basic DML though. Most people probably don't learn more advanced stuff like subqueries, CTEs, window functions, etc. In addition to DML, there's other aspects of the language like DDL, DCL, etc. which most people don't learn. Probably the best quote I read was that SQL is like chess: easy to learn but difficult to master.
That's probably 60% of what I use it for. "I know there's going to be data in here later but I need the record in place now without fucking up the keys."
I am not at a master level, but I feel like I have reached a level of professional competency. I would say that took about 2 years to achieve, with hitting proficiency after maybe 4-8 months depending on your resources, environment, level of use, etc. &gt;I’m nervous if I don’t learn it fast enough they will get impatient and dump me. So I get anxious to learn everything ASAP. Keep this mentality and you should be fine. Come here and ask for help, post sample code, sample data, go to other forums. My first 6 months on the job... I think about 99% of my work was pinged through this forum here, and people were great. If you are working for a living and people are helping you develop professional skills then have the courtesy to give them Reddit Gold. It's a small little token but entirely appropriate considering how much time you can save seeking help from others. Really the thing that helped me get to where I'm at was reading other peoples posts here and trying to help them do their homework, or help them with their jobs, or reading other solutions to other peoples problems and taking the time to see what they're doing, how they're doing it, why they're doing it, etc.
&gt; When I downloaded and installed the free community version of MySQL on my desktop, what exactly did I download and install? Depending where you got it, you probably got the MySQL database server (mysqld) and the command line tools (mysql) There's no "cloud" or external component (in your current setup). The mysqld program stores your data on your local computer, reads that data, parses incoming SQL, and does whatever number crunching it needs to do You *can* host MySQL server on remote computers, or in the cloud Generally, database servers are stored on a central LAN network server computer, or on a remote cloud computer, to make it easy for multiple people to share data Oracle owns MySQL, but it is open source, therefore free 
I've been a professional developer for over 20 years. I was really good with MS Access when I started in 1997 to help a state government agency get ready for Y2K. I've worked my up since then, been a backup DBA for my current agency for 4-5 years now. I'll let you know when I feel like I've "learned it". Seriously though, since I juggle a bunch of languages/environments, I keep my scripts and check or copy them when I start new projects. I google syntax every day, and double or triple check my clauses before I change data. Attention to detail, ability to google problems well, and respect for your organization and the power you have when touching it's data directly are key. And the number one rule, don't BS people. Even if they're not techies it doesn't mean they're stupid. If you don't know something tell them "I'm not sure, let me check and I'll get back to you" and then follow through every time. You'll be their SQL guru above people with 2x your tech knowledge because you're reliable, honest, and you don't make statements/commitments without an exceedingly high confidence that you're correct.
You just need to subquery it. &gt;From Query1, Add another join (inner or left depending on if your query2 will return all 'professionals' or not) It will look something similar to: INNER JOIN ( [COPY AND PASTE QUERY 2 HERE] ) Q2 ON J3.Professional = Q2.[TK#] &gt;Then add to your select statement Q2.[PF + PR Dollars Received]
So you want to get all of the distinct ID's where the word "shots" does not appear in the narrative field? And then to see the entire dataset of those ID's?
When I say "server", I'm just referring to the background daemon/service/program on your own computer, not the actual physical computer (or virtual machine). * mysql workstation - only if you're only using mysql, might be easier * dbeaver - supports multiple databases 
About 7 years, but I'm still learning. Ask me again in 7 years and I'll say the same. But you can get functional within a few weeks if you practice. The other thing that is harder to come by is exposure to real world bullshit scenarios. Textbook examples are great at showing how things should be, and the real world is there to kick your ass because that's not really how it is. 
Hey there, you've got a lot of questions going on there and I think you are just trying to wrap your head around something that is incredibly large here. Let's start really basic here. **The Server** So I get that when you hear server, you're thinking some massive machine off in the distance, but server has another meaning (because programmers are fun like that). So on your Mac, I think that's what you said you have, you have programs like iTunes, Calendar, Calculator, and so on. A server (in the software sense, which is all I'm going to focus on) is just yet another program that's on your Mac. Plain and simple program just like every other program on it. When you open Calendar on your Mac, it's just sitting there waiting for you to do something with it. It is **waiting on input** from the user. Eventually you move your mouse, type something on your keyboard, and the program responds to that input you are giving it. Calendar is **waiting** and is **listening** for input from your keyboard and mouse in this case. Now your Mac has a lot of places it can get input, your camera on the computer (if you have one), from the microphone jack (I honestly don't own a Mac, I have no idea what ports are on them), and another source of input is your network (wi-fi or ethernet). Now MySQL, the server, also known as mysqld (I'll cover why it's called that) is just a program that's sitting on your computer. Now MySQL, the package that you downloaded, actually comes with several programs within it, but for now we're just going to focus on mysqld. At any rate, mysqld is just a program that sits on your computer and when you run it, it'll do just like every other program, **wait** and **listen** for input. Unlike other programs, it's not **listening** to the keyboard or the mouse. Instead it is **listening** to your network for input. Now when I say your network, we're just talking about everything up to your wi-fi router here. Not the entire Internet or anything like that. That brings me to... **Loopback** I won't go into a lot of networking theory here. However, I'll cover the parts that are required to get the point home. When you connect to a network, say your wi-fi router or via ethernet, you will receive an **address** that is unique to you on the network. This way, people will know how to contact you. Want to send a message to that Mac sitting over there? Well you send it to **address** 192.168.0.8 or whatever it is that is your address. Now this brings me to the **loopback** device on your system. There's a special address [127.0.0.1](https://127.0.0.1) that every machine has. This is called the **loopback** address. When you send something to that address, it just comes right back to you like a boomerang, no need to be on a wi-fi or anything. The **loopback** device works so long as your machine's OS is working. So when one program outputs to 127.0.0.1, it becomes input for maybe some other program running on your machine. This is the key here. mysqld is sitting there waiting for input on the network (which is your loopback device and/or your wi-fi/ethernet, depending on how you configure mysqld). mysqld is not **listening** for input on your keyboard or mouse, so you need a program that will listen to your keyboard and mouse as input, build some output, send it via the loopback as input to mysqld. That brings me to... **Client/Server architecture** I won't get into nitty gritty details here, but basically mysqld is a server in the software sense. It is **listening** to the network awaiting a **request** to come in via the network it is **listening** to. A **client** will take input from "something else" in this case your keyboard and mouse. The **client** takes what you type into it and then when you tell it to, will send a **request** to the indicated server. That **request** might be to "log in" or "tell me what schemas I have" or "run this query", your client knows how to take what you've given it and create a valid **request** for the server. The mysqld isn't a fool, it's only going to respect requests that are "valid". I should point out here now, we are talking about two different programs running on your machine. There's the client program and mysqld. Just like you can have iTunes and Calendar open at the same time, you can have a client and mysqld open at the same time as well. Additionally, your client and mysqld talk to each other via the **loopback** device. If I remember correctly, you can have an email come into you Mail program on Mac and if there's an invite in the email, then you can click on that and Calendar will do something with that invite. Here, Mail and Calendar are talking to each other via some other method that Apple created. Calendar can also sync with Google calendar, here Calendar and Google talk to each other via your network device. So having two programs talk to each other isn't uncommon, and they can talk to each other via a whole lot of different methods (that I won't go into). But for right now, just remember that your client and mysqld are talking to each other via the **loopback** device. **The other programs that come with MySQL** When you downloaded MySQL from the Internet that package had not only mysqld within, but some other programs as well. Three programs that came with it (besides mysqld) are: * mysqladmin * mysqlshow * mysql That last one is confusing just being called "mysql" versus "mysqld". So we now know about four different programs that come with MySQL the package. mysqld, mysql, mysqlshow, and mysqladmin. now those three that I just mentioned are **clients** that you can use to connect to mysqld. [I'll refer you to here](https://dev.mysql.com/doc/refman/8.0/en/testing-server.html) to see there usage. These programs are very, very, very basic clients. They don't do a whole lot outside of the single minded scope that they've been assigned to do. Just like Calculator for all of it's glory, still cannot schedule you something on your calendar. Calculator is focused on adding, subtracting, and whatever else your heart desires in the realm of simple arthritic and that's pretty much it. The manual page I linked for you, should give you an idea about what each of these tools do. However, as limited as they may be in scope, they are still **clients** for mysqld, they're just not really feature rich clients. A lot of folks have indicated other clients that you might be interested in downloading, like MySQL workbench or BeaverDB or whatever, that's the nice thing about client/server architecture. So long as the client knows how to **talk** to the server and give it a valid **request**, it doesn't matter which client you use. Additionally, web servers and web browsers are exactly the same thing, a client/server architecture. It doesn't matter which browser you use to get to Reddit's server, so long as your browser knows how to speak the language of the server, the server will happily fulfill your **request**. **Protocol** Now I've said a lot about a **valid request** and the language the client/server speaks. This gets into **protocol**, which I'm only going to lightly touch on. The **protocol** is the manner in which a client and server speak to each other. For web browsers this protocol is called **http** or **https**. Which is just short for **H**yper**T**ext **T**ransfer **P**rotocol or **S**ecure **H**yper**T**ext **T**ransfer **P**rotocol (it's just the s is placed at the end. Which putting the "s" at the end is just a normal naming convention which is also why there is a "d" at the end of mysqld. The final "d" stands for daemon, which why it is called that gives into a lot of history that will just bore you to tears. But yeah, it's normal to put "s" and "d" and a whole lot of other letters at the end.) mysqld has it's own **protocol** that it "speaks" and your client (whichever one you choose) will need to speak that same **protocol** in order for anything to work. The three other programs I spoke about "mysqladmin, mysqlshow, and mysql" speak native mysql protocol. However, a lot of other clients do not speak native mysql. Instead you are going to need... &amp;#x200B;
Yeah, it will apply it the same way... where it will only select records from table2 which have that criteria.
Wow this is all great, thanks again. Okay figured that was probably the open source portion of things. Yes this is quite a great rabbit hole I found myself in haha, cheers.
Wooooow this should be posted somewhere for all to see. If I had any gold it would be yours. This will take me some time to get through, but it looks like a great explainer for the whole thing. Thanks @justyb11
No problem! Glad i can help
I'm not sure I understand what you need. From what I gather, this should do it. Select column1 , column2 , column3 from tablename where columnid in (Select columnid from tablename where columnname like '%Searchterm%') 
ctrl+shift+v is awesome! Thank you!
I think they're referring to CTE's (common table expressions) but I've never heard of it referred to that way before.
RBAFR
Find a copy of The Guru's Guide to SQL Server Transact-SQL by Ken Henderson and work through chapter 6. I used it to get my first DBA job 18 years ago, and a BI friend just passed a couple of Facebook BI interviews and got her new job. 
Having had some exposure to it in college which is by no means even basic level, real on the job learning was about 6 months. But I used SQL and VB6 every day back then.
Human beings typically write SQL via an editor, not a full-blown IDE, but a SQL editor. It's not like writing a bash script where you type stuff out in a CLI - it can be done, but it's just less common in SQL. Honestly though, the most common way of executing SQL commands is via other applications making connections to databases in code (the code could be Python, Java, C++, C#, etc), and then submitting SQL commands. In your case, just use MySQL Workbench.
Full time developer for over 4 years and still learning every day. One thing I wish I had learned early on was performance tuning, indexes, and how to read a query plan. This is a great site to learn about indexing [https://use-the-index-luke.com/](https://use-the-index-luke.com/)
I'm not at work anymore, but I believe a subquery will work for what I need. So I'm pulling dispatcher call data, every time they enter a new line of narrative it's a separate row but with the same call number. I want to find any calls that use the term "Shots" (or shots fired) and pull all rows that may have been missed due to them not having that string in the narrative field. We have only just started using SQL for analysis, but I see the potential with it and I'm trying to brute my way through the learning curve. Prior to this my team was copying and pasting garbage around from 6 different excel sheets *shudder* 
The biggest reason (myself included) so many answers are in the "many years" region is because most programmers are mostly doing normal programming (in the application language) and only actually writing SQL a very small percentage of the time. If you dedicate yourself to just learning SQL, it will be much faster. So don't stress about it. It's actually much easier than regular programming. &gt; The people who hired me knew I had no background in code or SQL You should be fine then. If you're just focusing on SQL for a few weeks or months, you'll probably get better at SQL than a lot of regular programmers... many of who only learn the basics themselves. &gt; querying large sets of data. A amount of data doesn't matter too much, it's more about how complex the queries are. Having a large amount of data will actually help you learn more faster, because there will be lots of existing examples for you to look at.
Google was my best friend when I started. I never knew what SQL really was until I messed around with it at work about 1 year and a half ago. Before diving into all this, I was proficient in Excel and did a lot of VBA. I was lucky enough to have someone in my company provide me samples of his queries which allowed me to see what the end results were and to break down what each thing did, and what could be changed to receive the specific data I wanted. If you can, make some friends and don’t hesitate to ask people questions. Doing so exposed myself to others already doing this work and now they’ve handed over Ad Hoc requests from folks that are looking for specific data I excel at. If they knew you had no previous knowledge in SQL they surely believe you are capable of learning it which is an awesome opportunity to be given. Do as much searching as you can and test things out. If some reports call for pulling multiple tables, query tables down to the basics to see what each sub query is doing to get to the final result. Once you get proficient in SQL it will start to look more familiar as you go. I’m converting my Oracle queries over to PostgresSQL and it almost feels like I’m learning it again(sort of) due to different syntax used. I know I said it at the beginning but Google was a lifesaver for me in the beginning and even now as I’m trying to understand how PostgreSQL works. Congrats and good luck on this!
This doesn't quite work as neatly; if you want to comment only the last line. At the point you have a dangling "AND". &amp;#x200B; I have always wondered why I saw 1=1 in WHERE clauses over the years, now I know. 
Should've used some @temporary_tables. /s
That sounds horrendous! In that case that should do just fine, I guess the next question is, what is it that you need to do with the data once you've got it? That will depend on what you need to pull and then how you go about grouping it back together. 
How do you want to choose the one tag to be returned? Randomly? Or is there some sort of preference?
GROUP_CONCAT is what you're looking for!!! 
Why do you need to have it as a join? You'd be better off using WHERE NOT EXISTS.
I wrote my first query probably ten years ago. I used my first recursive CTE in a real setting two weeks ago.
Left join is probably what you want. Look up join charts on Google
You can use STUFF and FOR XML Path, it's not too difficult. Here's some more information: [https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server](https://stackoverflow.com/questions/31211506/how-stuff-and-for-xml-path-work-in-sql-server)
I remember when I found out about this approach after having done the same thing using a pivot and dynamic SQL. It was a happy day.
Fantastic thank you
Beautiful thank you
Have a look at the other answers
No pref
Performance will become an issue at some point (as your dataset grows) given it's extremely inefficient compared to doing explicit insert/update/delete statements. Additionally, it can be extremely buggy https://www.mssqltips.com/sqlservertip/3074/use-caution-with-sql-servers-merge-statement/
Table variables aren't bad per-say and they absolutely have very good use cases. When in doubt, test both scenarios.
Switched to this when ssmstools when to a paid model and I've never looked back. Could not work efficiently without it!
Some day I'll actually run into a use case that fulfills all of the pre-reqs for indexed views
I think you want: select left(TRANS.FROM_WORKSTATION_ID,5) as Grid, SUM(case when TRANS.CREATE_DATE Between DATEADD(HOUR,7,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) And DATEADD(HOUR,15,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) then TRANS.TRANS_QUANTITY else 0 end) as [E_Decant], SUM(case when TRANS.CREATE_DATE Between DATEADD(HOUR,15,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) And DATEADD(HOUR,23,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) then TRANS.TRANS_QUANTITY else 0 end) as [L_Decant], SUM(Case when TRANS.CREATE_DATE Between DATEADD(HOUR,23,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) And DATEADD(HOUR,07,DATEADD(day,DATEDIFF(day,0,GETDATE()),-0)) then TRANS.TRANS_QUANTITY else 0 end) as [N_Decant] from IDC.dbo.TRANS TRANS WHERE (TRANS.TRANSACTION_TYPE='PUTAWAY') Group by left(TRANS.FROM_WORKSTATION_ID,5)
This worked for me as well. Thanks for your help and giving me a solution I can use in the future for other things too.
Without putting words in the mouth of the OP, and without checking the development investment required on this, I'd have thought that it would be a good as a report prototyper. There are occasions when implementing new reports in to an existing SSRS/Power BI/Data Warehouse solution involves a not insignificant upfront development, especially when not all the data requirements existing within the current data model. Going through a couple of iterations at this level could reducing the fucking about when it comes to implementing the report. 
Solved Perfect result. Thankyou very much for your help Saeveo
Thanks for responding to my post. There are two themes behind Cirkulate: 1. There will be users who will always want to or need to consume data in spreadsheets. None of the data warehousing or PowerBI-style services I've seen make it easy for recipients to consume reports like they would consume a spreadsheet. 2. Cirkulate's aim is to enable a developer or analyst to get out of Cirkulate as quickly as possible so they can get back to working on code or actual analysis in these other solutions like PowerBI, Tableau, etc. It is really really fast to setup spreadsheets in Cirkulate and much easier to use compared to SSRS Report Designer. Data warehousing solutions are great for "analysts to perform analysis" but very disruptive when it comes to quickly getting out of the way for this spreadsheet reports use case. I am happy to screen share a [demo](https://app.cirkulate.com/demo) of the app to dig deeper into it. (Can connect privately for contact info) &amp;#x200B; Thanks again!
Hi Bungle\_bogs, You've got the core theme right. Because you can go from SQL to spreadsheet in seconds, even when a spreadsheet contains data from multiple databases, the speed really enables building out a report quickly and get back to work. If things need some change based on recipient feedback, its fast to iterate and refresh the SQL. &amp;#x200B; This all works so fast because of the purpose-built spreadsheet that takes SQL directly as input, as opposed to Excel or Google Sheets where additional plugins are required(not to mention the additional steps to schedule refresh and managing recipients for each report) Thanks!
Sure. Its always possible in SQL! It’s kind of hard for me to recommend a solution because I dont know the data and I’m assuming you’ll want to use this for more than your Jane Doe example. How many PARTCATCODEs are there? Just the two? Will all professionals always have both? For your specific example, try creating a query that just pulls the Professional and ORIG info (all of the info except the RESP data). From there just do a join onto a subquery that only pulls Professional ID and the RESP info. After you make the join you can add the RESP info to your select statement. Sorry I cant give you code specifics right now. Im on my phone at work. If you still need help I can prob do better when I get home
but looks like it doesn't apply for your platform? please always identify your platform in /r/sql (see sidebar)
Welcome to the world of ETL! &gt;Would you create a table with a few extra columns to account for characters that would accidentally count as delimiters? Nope. Cleanse the data before it hits the database. Better yet, get the third party to provide a better-formatted feed. For example, if it's a CSV file, quote the values so that the data can be parsed properly (this is the standard convention). Loading the data into a staging table (which is usually a heap) before a final insert into the real table(s) is also a common practice and may be beneficial to you. &gt;What would you designate as the primary key? Do you spot one column that has only unique values and make that your primary key, or do you create an ID INT AUTO_INCREMENT column and make that your primary key? That depends on the data. If you can identify a field in the source data that's a good candidate for a PK, use it. Otherwise, create your own.
Can I ask, what part of the language interests you more? Is it the queries and how they function or database itself and how it is built and maintained? 
I follow you. I'll give it a whirl. Just need a push in the right direction. Will follow-up!
How do I cleanse the data before it hits the database? Let's say it's a .txt file and it is delimited by |. There are values like this: UCLA | School of Business. That value counts as 2 separate values: UCLA and School of Business, instead of 1 value UCLA | School of Business. I see a good candidate for Primary Key as there are no duplicate values so far, though what if in the next month update there happens to be 2 rows that have the same value in the supposed Primary Key column?
I cannot tell you how frequently I get complaints from business users about Power BI reports not being Excel. It's infuriating, but some people just cannot wrap their heads around exporting data from Power BI if they want to do something else with it.
It depends on a lot of factors including your city and your educational background. $48k a year seems pretty reasonable to me for a data anlayst with one year of work experience. But if you're in New York City or San Francisco, or another city with a very high cost of living, perhaps it should be higher. A good way to check is to use salary.com and enter your location title, find a good match, and see what the median is. Then adjust higher or lower based on your education and experience. If you still feel that 48k is too low for your location, you can bring that information from salary.com to your HR and ask for more.
It all depends on where you live, but in my opinion 48k for only 1 year of experience isn’t too bad. 
&gt; There are values like this: UCLA | School of Business. That value counts as 2 separate values: UCLA and School of Business, instead of 1 value UCLA | School of Business. That's where quoting your values comes in. It should be `"UCLA | School of Business"|"Field 2"|"Field 3"`. This is pretty standard in the world of CSV data exchange and whoever is giving you this data should be prepared to handle it. &gt;there are no duplicate values so far "so far" being the key words here. &gt;though what if in the next month update there happens to be 2 rows that have the same value in the supposed Primary Key column? Ask the provider of the data if they have a field they can provide or suggest as your PK. If they can't, create your own. Don't work on this in a bubble, talk to whoever is providing this data to you. It's a collaboration.
What if the values aren't quoted in the source txt file? UCLA | School of Business|Tom|2000-09-10|
Ask the provider of the data to quote their values. Again, this is a standard convention in data exchange to handle the **exact** situation you're hitting and should not be a burden for the provider. *You have to talk to the data provider to have them do this*.
That's almost double your starting salary, so good for you! If you agree with that wage, I recommend taking their offer and working for another 6 months or year before trying to negotiate for more. Prove yourself with your work, and go for some training or certifications at the same time. Depending on where you live and how much they have in their budget, and how well you do, you could easily bump that income up another 20% or more.
He is right. This shouldn't be a big deal. The request should be something like this. "I am finding characters within the values of a column that are the same as the file's column delimiters. Please scrub these characters from the values or use text qualifiers when building the file." They may want to sanitize before or after, but this will tell them that either way they prefer is fine with you. From there you can either change your ETL process by looking for text qualifiers, or do nothing because the data is already clean. 
Try [glassdoor.com](https://glassdoor.com) to compare your salary. As much as this sounds like an ad, it's not. Glassdoor helped me with salary negotiations. 
Depends on where you are. Here in central ar 48k for that would not be too bad.
Very interesting, people can add multiple queries to the same sheet and track their KPIs or make a dashboard.
Hmm.. as far as I know, at least on SQL Server, using EXISTS instead of checking for COUNT(\*)&gt;0 are actually the same thing. They will generate the same execution plan. But, it will be different if you use COUNT(\*)&gt;1. `SELECT 1` `FROM msdb..sysjobs job` `WHERE EXISTS ( SELECT 1/0` `FROM msdb..sysjobsteps stp` `WHERE job.job_id = stp.job_id)` &amp;#x200B; `SELECT 1` `FROM msdb..sysjobs job` `WHERE ( SELECT COUNT(*)` `FROM msdb..sysjobsteps stp` `WHERE job.job_id = stp.job_id) &gt; 0` \--Slower query `SELECT 1` `FROM msdb..sysjobs job` `WHERE ( SELECT COUNT(*)` `FROM msdb..sysjobsteps stp` `WHERE job.job_id = stp.job_id) &gt; 1` ​ The first two queries will generate the same execution plan. And, the same idea works for the following example. ​ `IF EXISTS (SELECT * FROM [AdventureWorks].[Person].[Address])` `SELECT 'Do Something'` `IF (SELECT COUNT(*) FROM [AdventureWorks].[Person].[Address] )&gt;0` `SELECT 'Do Something'` ​
I would suggest a project where you build and design your own database for something you have some advanced knowledge of. For example, you could build a database (tables of data) to house baseball statistics. You'll need to go online find the data, possibly normalize it, import it into tables, etc. Once you have a first pass start asking your database baseball questions. Who had the most home runs in 1986? Don't have data that goes back that far? Time to get it and figure out how to add it in. Based on the last 10 years or data, which day of the week has the highest number of average home runs? How about median? What you're describing is getting a job in SQL and working with real datasets. Maybe try to find a few small businesses and offer to design a database for them for free and start working with their data / figuring out what they want to see then learning how to write the queries. 99% of the SQL I know came from someone asking me for something, in my head understanding what the final data set should look like, and then slowly writing a query and building it up, Googling and asking questions here to do things I'm unsure of, or in places where I need advice to make it more efficient / elegant. If you can't get that going for you then another great way to learn is by being an active member here and helping other people with their problems, or if you can't answer their question then looking at a solution someone else provides and internalizing it. Building some mock tables &amp; populating them with sample data so that you can run the query on your local machine. It sounds like you are at a point where you are capable of using SQL to do "basic" tasks... which sounds like you could probably get a job working with SQL depending on what market you're in. 
&gt;Then if you need to add a new number type like TollFreeOffice you'd just add a nullable column. That's poor normalization, and it's how we end up with 100-column tables. Have a separate table of phone numbers with type and number which links back to the person.
That's a good idea. No flat tables.
I don't know how much real-world experience w/o a proper job, but I can say, a lot of people (myself included) who post here have projects at work they need to complete and will ask questions. See if you can help answer their questions.
They ARE amazing...and were long overdue. Has helped me get rid or a lot of RBAR and large self joining temp tables.
 SELECT ... FROM ( /* 1st query */ ) AS one INNER JOIN ( /* 2nd query */ ) AS two ON SUBSTRING(two.column FROM 5) = one.column
I've taken it a step further, and have used one field character mappings to represent entire lookup tables. CampaignID|Medium -:|:- 1|Phone 2|Mail 3|Radio 4|Mail 5|Phone 6|Radio 7|Mail 8|Phone 9|Phone DECLARE @MediumMap varchar(max) = 'PMRMPRMPP';--This happens dynamically SELECT bt.SortedField , SUM(CASE WHEN SUBSTRING(@MediumMap,bt.CampaignID,1) = 'P' THEN 1 ELSE 0 END) AS PhoneCount , SUM(CASE WHEN SUBSTRING(@MediumMap,bt.CampaignID,1) = 'M' THEN 1 ELSE 0 END) AS MailCount , SUM(CASE WHEN SUBSTRING(@MediumMap,bt.CampaignID,1) = 'R' THEN 1 ELSE 0 END) AS RadioCount FROM BigTable AS bt GROUP BY bt.SortedField; The point being the query gets to take advantage of some sorted index, that a join to Campaign would mess up. This is at the cost of computationally expensive string operations.
Alternatively, if the stuff at the start of the 2nd column (001-, etc.) isn't fixed length, I think you could do something with the Access InStr function: WHERE InStr(1,two.column, one.column) &lt;&gt; 0 
You can unnest the queries using CTEs - aka the WITH clause. Won't change the logic but is easier to read.
...and you shouldnt have to know how to automate a report. If you know SQL or are in a dev team also tasked with sending spreadsheet reports, your time is more valuable than spending on setting up spreadsheet reports. Hence, Cirkulate. I am building this also to channel my own time into more impactful and interesting parts of my work.
I will let you know when I am done learning! LOL. Honestly, things are always improving/changing/regressing/progressing it is good practice to be always learning.
I'm all about the vendor management. If they're not conforming to specs/industry conventions, I'll call them out on it and tell them to get it fixed.
1. Alt+F1 2. Assigning my own ctrl+num shortcuts (so for example I just highlight a table name and run "select top 1000 * on it) 3. Very little known ctrl+del 4. Dynamic code + case
1/ FULL OUTER JOIN is missing 2/ if you do OUTER, you should also do EXCEPT 3/ diagram illustrating CROSS JOIN is wrong other than that, those diagrams are killer, and a great improvement over other venn diagrams i have seen
Thanks AaronPDX. I've generally found that the further away you move from tech teams, the higher chances are that spreadsheets are the one and only format in which users consume data/reports. The concentric circle generally goes like: tech-&gt;product mgrs-&gt;marketing-&gt;design-&gt;sales-&gt;compliance-&gt;legal-&gt;finance-&gt;execs. Although, some do consume via services like PowerBI/Tableau/Qlik/Looker when visualizations are needed.
in 2/, i meant to say UNION, not OUTER -- if you do UNION, you should do EXCEPT
Does it have any charting capabilities? What about kpi indicator visuals? One thing I noticed, your product is based around the delivery of spreadsheet, and in the video showing how people get the spreadsheets you had some data populated that was not set out in a standard table layout, yet your only download option was csv. Downloading the file in your demo as a cab would result in a messy file no? Would it not be better to have a download to excel option? 
Trump_wrong.gif
I would treat Glassdoor's salary info as "being in the same state where the ballpark is located:" https://www.asktheheadhunter.com/11910/glassdoor-salary-data
If you're adding a new column to the select clause of a query that contains a GROUP BY, and it's not a part of an aggregate (such as "price" in this example), it needs to be added to the GROUP BY as well. So try SELECT cust_city, cust_country, MIN(price) FROM customer GROUP BY cust_country, cust_city ;
If you want to download database with data in, then get a copy of Stack Overflow's!
Completely open to the fact I might be wrong but isn't OUTER and FULL OUTER the same thing?
You're missing Anti-joins. /u/r3pr0b8 has mentioned EXCEPT, but you can also get one (in MSSQL at least) by using WHERE NOT EXISTS.
It is technically self-reported data.
no, they aren't OUTER can be one of three types -- LEFT, RIGHT, or FULL if you leave off that keyword, INNER is assumed 
 SELECT t.CustNumber , t.FirstName , t.Surname , t.DOB FROM ( SELECT FirstName, Surname, DOB FROM Customers GROUP BY FirstName, Surname, DOB HAVING COUNT(*) &gt; 1 ) AS dupes INNER JOIN Customers AS t ON t.FirstName = dupes.FirstName AND t.Surname = dupes.Surname AND t.DOB = dupes.DOB 
Where 'phone' and 'name' are two example fields Haystack and Needle can be same table: select n.id as needle_id, h.id as haystack_id, case when n.name = h.name then 1 else 0 end + case when n.phone = h.phone then 1 else 0 end as relevance from needles n join haystack h on n.name = h.name or n.phone = h.phone order by relevance desc; What you are looking for is called probabilistic record linkage
 SELECT one.whateverThisColumnIsActuallyCalled1,two.whateverThisColumnIsActuallyCalled2 FROM ( SELECT whateverThisColumnIsActuallyCalled1 FROM District WHERE District.ParentId = -2) AS one INNER JOIN (SELECT whateverThisColumnIsActuallyCalled2 FROM District WHERE District.ParentId &lt; -2) AS two ON RIGHT(two.column,LEN(two.column)-4) = one.column
You know, I've seen that term thrown around a bit (window functions) and never realized that's what I was doing. Thanks for the learning!
To answer your question about negotiating a higher salary... * Are there other higher-paying options available? If there is competition in this job market, they'll need to up the ante. * Can you show them why you think you're worth more? * Are they looking for someone long term? If so, can you show them that you are too? An employer may be willing to invest more if they know you're going to be around.
What is database normalization and why do we do it?
wwut? &amp;#x200B;
Do you happen to know if there any good online video courses? The one course on 70-761 on Udemy is garbage... 
Why do you want to work here/What are your hobbies/What is your approach to a big project/What do you do if you can't solve the problem/Where do you see yourself in 5 years.
Which RDBMS?
Explain how a clustered index works.
Explain the difference between a left join/inner join/full join.
Recent SQL Server DBA Interview: * What is parameter sniffing? * What would you consider a good fragmentation percentage to reorganize an index? Rebuild? * What is your least favorite part about SSIS? * Which high availability technologies have you worked with? * What theoretically performs better, a LEFT OUTER JOIN with IS NULL in the WHERE clause or WHERE EXISTS? * What versions of SQL Server have you worked with? * What first steps would you take to troubleshoot a performance issue? * Do you have any experience with database modeling? At most interviews I've had, I was asked if I had experience with X or Y industry specific software that the company uses. Usually you will know that from the job listing beforehand. And the more generalized questions like "how would you handle a disagreement with a co-worker?" too. 
If we put the id into the group by, it will give us a row for each customer with what their minimum price is. I \*think\* that what the OP wants to extract is: for each city/county pair, which id in that grouping has the minimum price within that grouping. If this is the case, OP, you need to do a join of the original table against the aggregated table. Something like this: \`\`\` SELECT a.cust\_id, a.price, b.cust\_city, b.cust\_country FROM customer a JOIN (SELECT cust\_city, cust\_country, MIN(price) min\_price FROM customer GROUP BY cust\_country, cust\_city) b ON a.cust\_city = b.cust\_city AND a.cust\_country = b.cust\_country AND a.price = b.min\_price;\`\`\`
This is the perfect use case for window functions. Have a look at this wonderful explanation: https://www.reddit.com/r/SQL/comments/9azokm/what_are_your_gamechanging_discoveries_in_sql/e4zqic8?context=1
This is a simplified description but it'll get the point across. Think of it like this, you have an unordered table of first names, and ages. Assume that there is an even distribution of letters (so the number of names that starts with A is the same as starts with B, and starts with C, etc). You want to return the information for everyone who is named John. SELECT * FROM myTable where Name = 'John' So the table is scanned. First row, is his name John? No, next row, is his name John, no next row, next row, is his name John, yes, add it to the results, etc. Once it hits a John, it adds it to the list and has to continue scanning the rest of the table because it's unordered, "John"s can literally be anywhere in the list. Now you can make a clustered index on the name column. This will sort the records and make what's called a B tree structure of all the data. [https://blog.couchbase.com/wp-content/uploads/2014/12/SecondaryIndex.png](https://blog.couchbase.com/wp-content/uploads/2014/12/SecondaryIndex.png) The first thing it's going to do is look at the first letter, J. Anything that starts with J will fall into the second bucket, G-M. That means that with a single comparison, you've eliminated 75% of the searches you would otherwise need to do. Then perhaps the next level of the structure breaks out everything from G-I into one bucket (called nodes), and J-M into another node. Since we're looking at a J, we can eliminate 50% of the remaining results. So in two comparisons, we've eliminated about 88% of all data. There is a tradeoff though, the more levels, the faster your searches, but the larger the size the index becomes. Eventually, after a few levels, you won't have any more nodes and you'll just start scanning the data. You'll pass James, Jack, Joe, and eventually John. It'll read back the John's to you quickly since they're all right next to eachother. And after it passes the last John, the query can stop running. The list is in alphabetical order, so it knows that once you've passed the last John, there are no more Johns in the list. And clusered indices can be made multi-leveled. So if you want to search for everyone named John who is 50 years old, you can add age to your index. Multi-column clustered indices are more nuances than single column ones but the jist is the same. Going back to the tradeoffs, indices will make selects faster, but reduce the speed of anything that makes changes to the data. Because every time you insert/update a value, it has to recompile the index, which takes time. Hope this helps! &amp;#x200B;
Unless you're really going to a large company with a whole slew of database engineers of various types, a lot of the time the question you get is, "Tell me about joins." Because that's nearly all the interviewer knows. If there's another SQL user around, you get into like... Primary/foreign/natural keys? Inner/outer left/right joins? Cross joins? Full joins? Clustered/nonclustered indexes? Where do you start with creating indexes to speed up performance? What are the three usual forms of database normalization and why are they important? What's a view? Tell me about index fragmentation and what to do about it. Etc. 
...what IS parameter sniffing? I've never heard that term before.
Not op but here you go. https://dba.stackexchange.com/questions/121034/best-practice-between-using-left-join-or-not-exists Joining the field's then filtering is less performant because the engine has to retrieve all rows from both tables then filter out what you don't want basically caching the results of both sets. Exists/not exists or in/not in only returns the rows in table b that match table a or not. Smaller data sets in cache.
Nothing related to the work needed. literally: I ask back: "What do you need from me, vs the canned questions you ask me?" I got the job 3 times now, becoming their friends in the interview. 
 * Crappy developers. * ola scripts * M$ burying the variations in bad online updates * HA? all of them. none of new ones * Theoretically? Can i try the differences per project. * 7, til 2016 * reproduce it, check the results in a copy of prod, is it in proc or a embedded massive sql script in some shit CRUD .net app? * no. 
I didn't use a course in particular but I did watch a lot of videos. What subjects would you like a video on? I can probably find or recommend resources on that. 
&gt; Tell me about index fragmentation and what to do about it. [I tend to](https://www.brentozar.com/archive/2016/01/should-i-worry-about-index-fragmentation/) [not worry](https://www.brentozar.com/archive/category/indexing/index-maintenance/) [about it](https://www.brentozar.com/archive/2017/11/good-reasons-rebuild-reorganize-indexes/) [too much](https://groupby.org/conference-session-abstracts/why-defragmenting-your-indexes-isnt-helping/). There are usually bigger problems with the queries, indexing decisions (missing indexes or useless ones), or application design that causes significantly more pain.
&gt; Had I been in SSMS with actual tables, and not doing it on paper, I could have worked that one out This is kind of why I don't like the "write me some SQL" questions at interviews. I am terrible at doing it outside of an IDE or at least with some formatting turned on in N++. The FizzBuzz question is interesting though. In practice, I think I would prefer to utilize a "Numbers" table rather than a union or cte just because I think that's uglier code... which is another reason I'm not into the "write me some SQL" questions. Is the interviewer looking for a specific answer? Do they want to know that I can use a union, or a cte, or are they looking for some other method I didn't think of? Are they going to ding me on a missing comma or something goofy when I'm writing on a white board? I suppose if you can't have that kind of discussion at an interview then you probably won't work well with that person. Still not a fan of the whiteboard sql task ;)
The FizzBuzz was a take-home assignment before the final interview. And they asked for with and without loops because looping would be a "traditional" programming approach, while not using a loop would push the solution more toward a tally table-based approach. In this case, they were really just looking for thought process and "can you explain why you did it this way." They weren't looking for an *exact* match on the solutions, just "can you do it and explain why/how".
What would be the correct answer to that?
When was the last time production went down? How did you handle it?
It depends. One decent generic answer is you normalize a system to optimize for heavy writing systems (OLTP) and you denormalize for heavy reading systems (OLAP).
Hmm why do they want to know what my hobbies are? Why do they need to know what I do in my free time. 
okay, sure 1/ your initial query that you posted finds the duplicates, right? it produces a table of data containing one row for each set of values that is duplicated 2/ so that goes into the FROM clause as a subquery, also called a *derived table* with the name "dupes" 3/ then just join the dupes to your original table, such that each single row in the dupes table matches more than one row in the original table, and you simply show these multiple rows with their customer number 
I’m not convinced that the data could be relied on to be ordered in the final output even if the subquery referred to an ordered view.
Here's another way to do it: SELECT * FROM ( SELECT *, COUNT(*) OVER(PARTITION BY FirstName, Surname, DOB) AS Count ) AS C WHERE C.Count &gt; 1 ORDER BY C.Count DESC
this is exactly what I was thinking. The Padding suggestion is a good idea also. 
Yeah I mean, it’s what I use to compare Dates that are in VARCHAR YYYYMMDDYYHH24MMSS format. It’s less expensive to convert/compare them as INT/BIGINT than it is as DATETIME/DATETIME2.
Thanks - I appreciate your help. I’m looking for a comprehensive overview. I’m somewhat new to SQL, although i can write basic short queries for my job. Need something that is going to teach me in depth
Do you know SQL? I said no and then began studying it before the next interview.
* Do you have any indexes on the table? * Are you certain that the query is executing and not being blocked by another query/process? * Are you resource (memory) constrained, either on the server or client? * What does the execution plan for the query look like?
row by awful fuckin' row
You made the big time my dude. https://www.brentozar.com/archive/2018/08/what-were-your-game-changing-discoveries-in-sql/
Hi Coldchaos, I totally agree with you. What you speak of is the ability to make decisions and apply good judgement on when to automate and when not to. What I was trying to say is that once you've made the decision, the system should not get in your way and in fact, should handle the details of "how" the automation should happen. Cirkulate's point of view is that, for eg, when speaking of SSRS, does it have to take so many steps and constant referring to google to see exactly what knob to set next for just getting SQL to populate a spreadsheet? All these steps are the "how to automate" part.
&gt; It depends. You can answer *every* technical question this way :)
Hi Cmdr Charting does not exist just yet. Its an interesting area but just the spreadsheets take so much work for developers and analysts that I want to solve that first. KPI indicators via colored fields or alerts based on certain cells and their values is definitely on the agenda. I am glad you saw the video. Really, thanks for taking a look. I wasnt sure if the quality was good enough to get the point across. About the table layout, thats an area of improvement for sure. Also, it displays unlike a spreadsheet in say Excel or Google Sheets because the video showed how the spreadsheet table will look when sent in the email body. The idea here was to help recipients consume a snapshot of the table within the email client. Since I am building the spreadsheet to work very closely with SQL, a lot of my effort has gone into the functional parts of getting this experience correct. The aesthetics of making the table look like state-of-the-art spreadsheet layouts is coming soon. I just needed some feedback if the idea itself wasnt totally useless(which I feel I have now :-). Thank you!
I like this, saved for future use (and just done it myself as a mental exercise). Have been the interviewer a number of times for SQL/Analyst based roles, what we normally look for is the ability to do research and learn something new if/when you dont know the answer. So yes, we look for people who know how to google when they are stuck, or ask for help.
 Select unique_identifier, max(datetimestamp) from table Where datetimestamp &gt; '01/06/2018 23:00:00' Group by unique_identifier Order by unique_identifier; Toss an index on datetimestamp. Without one, you're going to have to scan the entire table to find the largest value of datetimestamp for each unique_identifier.
I work at a K-12 with about 8,000 enrolled students. Our student information system also runs on SQL Server. Duplicate students due to data entry errors are a daily concern here, and in they must be found and consolidated in order to correctly complete legally mandated data reports. That means we need to look for not just exact duplicates, but near duplicates as well because many of those are duplicates. We use a report that takes advantage of [the SOUNDEX\(\) function](https://docs.microsoft.com/en-us/sql/t-sql/functions/soundex-transact-sql?view=sql-server-2017) to look for near matches so that Foreman, Forman, and Formen are all the same. It's also remarkably common to enter slightly incorrect birthdates, so we have to take that into account as well. The drawback to this method is that you need to maintain a whitelist of false positives. However, this query does find about 99% of duplicates in the system. ;cte as ( select r1.student_id ,r1.last_name ,r1.first_name ,r1.middle_name ,r1.birthdate ,r1.current_status ,dense_rank() over (order by soundex(r1.last_name),soundex(r1.first_name)) group_id from dbo.reg r1 join dbo.reg r2 on r2.student_id &lt;&gt; r1.student_id and soundex(r2.last_name) = soundex(r1.last_name) and soundex(r2.first_name) = soundex(r1.first_name) and ( (month(r2.birthdate) = month(r1.birthdate) and day(r2.birthdate) = day(r1.birthdate)) or (month(r2.birthdate) = month(r1.birthdate) and year(r2.birthdate) = year(r1.birthdate)) or (day(r2.birthdate) = day(r1.birthdate) and year(r2.birthdate) = year(r1.birthdate)) ) where not exists ( select 1 from duplicate_whitelist l1 where l1.student_id1 = r1.student_id and l1.student_id2 = r2.student_id ) and not exists ( select 1 from duplicate_whitelist l2 where l2.student_id2 = r1.student_id and l2.student_id1 = r2.student_id ) ) select * from cte order by group_id, last_name, first_name, middle_name; 
Which indexes would matter here? Throwing a clustered one on the date field?
You can't just "throw a clustered index" on a particular field as you're only permitted one CI per table. I'm asking for *all* the indexes to see if there's one that maybe should be used but isn't.
So I'm specifically asking what if any index could possibly speed up a query where you're looking for a max(timestamp). Would a clustered one improve the speed of that query? Indexing the ID wouldn't, correct?
Would a non-clustered index help at all, or would it still need to do a full table scan for the max()?
It depends on the volume and distribution of the data. If that `datetimestamp` starts at 2017-12-15` then it'll probably still be a full scan. If it starts 10 years in the past, it may be a seek. In a vacuum which lacks details about the rest of the schema and the data itself, it's impossible to guess.
Bingo. That was my only question for one of the jobs that I've had!
Assuming MSSQL: Doing anything with DateTimes is incredibly and unbelievably inefficient. Especially comparing them with an aggregate. Ya gotta do what ya gotta do, but if you can avoid using DateTimes it will almost invariably make everything faster. Now - anything past this is far from best practices - so weak of heart my need to stop reading here. Also - without more info on the size of your dataset and whether or not this is the whole query- it could also be bad advice that makes it take longer. If this is something that you're running one-off and just need to get it to work - I'd look for an identity column in the table that's on the cusp of that datetime-range you've provided and use a &gt;= the first ID that would fit in it. This is terrible, awful advice. **Don't do this for anything that will be run more than once.** I'm fidgeting around thinking about it - but, again, sometimes ya gotta do what ya gotta do and depending on your version of SQL - identity columns are pretty reliably always increasing. Other ideas that come to mind are using DATEPART creatively - it's a lot easier for the optimizer to work with ints than datetimes. **Again, bad advice **. This is especially bad advice if you need that time restriction on only 1 date in the range - would require a case statement, creativity, and an unreasonable amount of testing. The note here is you'd really have to have a couple inserts because of the time restriction on only 1 date in the range, then a few deletes from the temp table winding it down to the max day then the max time in order to reduce datetime compares. It might take less time to run - but will probably take you longer than is reasonable to write it out and test it. 
If you have multiple matches between a single Dc_valpol row and rows in the Cs_valpol table, but only want a single Dc_valpol row to be displayed, first you need to decide how you want to handle that. Do you want to aggregate the matching rows into a single row? Do you want to pick only one of the multiple matches to display, and if so based on what criteria (sort by...).
is there no way to have the original Dc_valpol in my result query with a single Cs_valpol record next to it if it matches exactly and null for the rest of the fields if there is no match? I wan't it to work almost as if it is a vlookup in excel. If it exists in the other, display it, if it doesn't display as null
Yes you can do it, but what do you want the query to do if when there is more than one Cs_valpol match? You said in the request that "the left join will create extra rows based on multiple matches". So how do you want to handle that scenario? Which Cs row do you want it to pick? Or do you want to combine the rows that match? A vlookup will grab the first row it finds based on the sort order in the excel table. Is it OK in your scenario if it ignores all other matches? And if so, which record would you like to come first and therefore be matched?
The simplest thing out there: Be lazy and efficient - format the code so you can see the structure of the logic just like any other enterprise language - Java, C#, etc. This has saved me an enormous amount of time and headaches over the decades. Otherwise known as I work less overtime. &amp;#x200B; Extremely Simplistic Example: SELECT 'Total income is', ((OrderQty \* UnitPrice) \* (1.0 - UnitPriceDiscount)), ' for ', p.Name AS ProductName FROM Production.Product AS p INNER JOIN Sales.SalesOrderDetail AS sod ON p.ProductID = sod.ProductID ORDER BY ProductName ASC; &amp;#x200B; Now imagine some stored procedure or view that is more than 10 lines long (50?, 200?) doing it the usual way My formatting style (On the farm there was always the Right Way, the Wrong Way, and Grampa's Way of doing something): select 'Total income is' , ((S.OrderQty \* S.UnitPrice) \* (1.0 - S.UnitPriceDiscount)) , ' for ' , P.Name ProductName from Production.Product P inner join Sales.SalesOrderDetail S on P.ProductID = S.ProductID order by ProductName; &amp;#x200B; &amp;#x200B; 
Yes it would be okay to ignore all other matches in my scenario. I apologize for the confusion.
If my understanding of what you want to do is correct, you may want to try an OUTER APPLY. SELECT * FROM Dc_valpol dc OUTER APPLY ( SELECT TOP 1 * FROM Cs_valpol cs ON dc.Type = cs.Type AND dc.Policy_name = cs.Polocy_name ) cs2 This will select a maximum of one row from CS per row in DC. If you want to pick a specific record from CS, like maybe the most recent one, you can specify ORDER BY RecordDate DESC This will order the records from soonest to latest, and the TOP 1 record will be the soonest one.
Remember, left join is going to return ALL rows in your left table as well as matches in the right. You want inner join. You may also want to join the right as a subquery to aggregate down to one row, if you don't want to match on multiple rows on the right. You can do this by grouping by the fields that match between left and right. 
Everything else works up until the case when, in case that wasn't clear.
Get the job with this ONE SIMPLE TRICK. Interviewers *hate it*.
Also, wear glasses because they make you look so darn sophisticated.
&gt; And the more generalized questions like "how would you handle a disagreement with a co-worker?" too. *sweating intensifies*
Like someone else said, window functions are your friend here. For example, you can apply ROW\_NUMBER() to your query. SELECT key_val , cust_city , cust_country , price , ROW_NUMBER() OVER (PARTITION BY cust_city, cust_country ORDER BY price ASC) as r FROM customer This query will apply a row number to your data. Normally, if you don't use PARTITION BY, it will just apply an ever-increasing row number to your data sorted in whatever order you choose. However, since you've specified PARTIION BY cust\_city, cust\_country, that will reset the row number to 1 every time it hits a new combination of cust\_city, cust\_country. Since we've specified ORDER BY price ASC, that means that the lowest price is always going to be row number 1 for any combination of city and country. So to get the results you're looking for, wrap this query up in a CTE, and select only the records where the row number = 1. WITH priceData AS ( [that query] ) SELECT key_val , cust_city , cust_country , price FROM priceData where r = 1 &amp;#x200B;
Oh I definitely didn't know what values they were looking for. I also think it's a poor question. I said I wasn't sure, and they just said they use 10/30% and we moved on. Luckily it wasn't some deal breaker.
A case statement must end with an "end". [T-SQL documentation for CASE](https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql?view=sql-server-2017) FWIW, I don't think this is going to do what you expect as it will only run for AccountCode = '10521' and you'll get no results when that AccountCode isn't in one of your required reports. This will happen because it's in an AND in the where clause, and thus is required.
When I run it the SurfaceCosts number is way to high. It looks like it is including all costs from Account number 10521 not just where it exists in the same reports as the others. 
https://en.wikipedia.org/wiki/Database_normalization#Objectives 
 Select Distinct DailyCosts.WellID, DailyCosts.JobID, CasingInfo.HoleSection, CasingInfo.ShoeDepth, SUM(DailyCosts.DailyCost) OVER (Partition by DailyCosts.WellID Order by DailyCosts.WellID) As SurfaceCosts From DailyCosts JOIN CasingInfo ON DailyCosts.WellID = CasingInfo.WellID WHERE DailyCost NOT IN (0.0001 , 0.001 , 0.00 ,0.002 , 0.01 , 1) AND HoleSection = 'surf'; AND (AccountCode IN ('10670' , '10680' , '10202' , '10203') OR (AccountCode IN ('10521') AND ReportNo IN (select distinct ReportNo from DailyCosts where AccountCode in ('10670' , '10680' , '10202' , '10203') ) )) GROUP BY DailyCosts.WellID, DailyCosts.JobID, CasingInfo.HoleSection, CasingInfo.ShoeDepth; 
Cool. I’ll try that next 
UNIQUE index based on multiple columns
Yes, but I want one of those columns to be a date with a span of +- 1 day.
Anti virus is one of my first go to checks when things "suddenly start going slowly". Especially when io is involved. Been burned so many times by a sudden "threat" popping up on a scan that just turns out to be a program exe or something like that.
I've done it when needing multiple layers of aggregation, especially when needing to maintain separate COUNT operations. Typically the COUNT from an inner subquery turns into the SUM of an outer subquery. I've also done it when needing to join across aggregation keys to other tables. Engagement statistics stored in multiple tables, for example, are a lot more straightforward to GROUP BY Job ID, then JOIN ON Job ID, so you avoid Cartesian products.
I feel like I'm misunderstanding. No of the examples have a subquery in the from clause, they're tables joined via subquery. 
Sounds like you want a CHECK CONSTRAINT, something like: ALTER TABLE tab1 ADD CONSTRAINT CHK_IDandDate1 CHECK (id_1 = 1 and id_2 = 2 and id_3 = 3 and date &gt; DATE_SUB(CURDATE(),INTERVAL 1 DAY) AND date &lt; DATE_SUB(CURDATE(), INTERVAL -1 DAY) They weren't supported in MySQL previously though, not sure if that's still the case.
Gets more complicated as you layer in multiple engagement metrics, including Email Campaign Name or Revenue Attribution Channel, but this is a good example of using subqueries in the FROM clause as virtual table references: SELECT COALESCE(O.Job_ID, C.Job_ID) AS Job_ID, SUM(Subscribers_Opening) AS Subscribers_Opening, SUM(Subscribers_Clicking) AS Subscribers_Clicking FROM ( SELECT Job_ID, COUNT(DISTINCT Subscriber_ID) AS Subscribers_Opening FROM Emails_Opened GROUP BY Job_ID ) AS O FULL JOIN ( SELECT Job_ID, COUNT(DISTINCT Subscriber_ID) AS Subscribers_Clicking FROM Emails_Clicked GROUP BY Job_ID ) AS C ON O.Job_ID = C.Job_ID
I have one... strange solution. I'll explain it in T-SQL (MS Sql Server), but it should be easily convertible to MySql. At least I hope it would be. &amp;#x200B; The main goal is to create two new computed columns for left gap and right gap of one day and create unique index on them. [https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html](https://dev.mysql.com/doc/refman/8.0/en/create-table-secondary-indexes.html) &amp;#x200B; The hardest part is to write functions for that columns, so my idea is: 1. If your day of month is even, you compute "leftday" column as day + 1 and "rightday" column as day - 1. If it is odd - reverse order: leftday = day - 1, rightday = day + 1 2. You calculate "leftsum" = leftday + day and "rightsum" = rightday + day 3. Now you have 2 columns where neighbour rows for sequential days are the same in either leftsum column or rightsum column 4. You apply unique index on both of them and now you are happy. 5. I lied, this doesn't work with dates from different months and years. But we can hack it with days of unix time! And we will have straight sequence of days for all the time. Now truly happy! &amp;#x200B; Example on T-Sql with temp table: create table #tmp ( RealDate date, LeftSum int, UnixDay int, RightSum int ) insert into #tmp (RealDate) values ('20180903') ,('20180902') ,('20180901') ,('20180831') ,('20180830') ,('20180829') update #tmp set UnixDay = DATEDIFF(second,{d '1970-01-01'}, RealDate)/3600/24 -- should be smth like UNIX_TIMESTAMP(STR_TO_DATE(RealDate, '%M %d %Y %h:%i%p'))/3600/24 -- for MySql according to https://stackoverflow.com/questions/11133760/mysql-convert-datetime-to-unix-timestamp -- next statement is shorter, this one is just for explanation --update #tmp --set LeftSum = iif (UnixDay % 2 = 0, UnixDay + 1, UnixDay - 1) + UnixDay -- ,RightSum = iif (UnixDay % 2 = 1, UnixDay + 1, UnixDay - 1) + UnixDay update #tmp set LeftSum = 2*UnixDay + iif (UnixDay % 2 = 0, 1, -1) ,RightSum = 2*UnixDay + iif (UnixDay % 2 = 1, 1, -1) select * from #tmp --create unique nonclustered index IX_tmp_left on #tmp (LeftSum) --create unique nonclustered index IX_tmp_right on #tmp (RightSum) drop table #tmp If you try to uncomment indexes, you will get errors, until you comment out some dates from insert list. &amp;#x200B; So final version with computed columns in T-Sql will be: create table #tmp ( RealDate date, LeftSum as 2*(DATEDIFF(second,{d '1970-01-01'}, RealDate)/3600/24) + iif ((DATEDIFF(second,{d '1970-01-01'}, RealDate)/3600/24) % 2 = 0, 1, -1), RightSum as 2*(DATEDIFF(second,{d '1970-01-01'}, RealDate)/3600/24) + iif ((DATEDIFF(second,{d '1970-01-01'}, RealDate)/3600/24) % 2 = 1, 1, -1) ) insert into #tmp (RealDate) values ('20180903') ,('20180902') ,('20180901') ,('20180831') ,('20180830') ,('20180829') select * from #tmp --create unique nonclustered index IX_tmp_left on #tmp (LeftSum) --create unique nonclustered index IX_tmp_right on #tmp (RightSum) drop table #tmp Maybe there is any much simplier way to get same values for neighbour days, but at least this one works.
I completely understand, but the above examples from commenters do not have a subquery in the from clause. They're joining a table via subquery before the final from clause. 
Still no final from clause. 
But there's not a subquery in the from, just a joined subquery. 
I don't understand your distinction. You can construct a single subquery alone without joins inside the FROM clause, and any SELECT logic references the aliases specified in your subquery.
I mean a from clause is separate from a join. The above have subqueries as joins, but the from clause is NOT a subquery, in their examples. 
[See how joins are optional sub-parts of the from clause.](https://docs.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql?view=sql-server-2017)
This is great and worked PERFECTLY. &amp;#x200B; Thanks for your help.
Create a second table with a PK based on your three IDs and a Date column. In your main table, write a trigger. The trigger should then write three rows to your new table - using the three IDs from INSERTED, plus three dates - the date you passed in, plus one date before and one after. The trigger should raise an error if you break your logic.
https://use-the-index-luke.com/
A very basic summary of the features and nothing about its short comings. &gt;there are a couple shortcomings with SSMS &gt;No snippets False. You should know what you are talking about before attempting to blogspam it. 1/10 article. 
It's just an interface you can use to connect to a database server. It's similar to SSMS, LINQpad, etc.
A DBMS on its own typically doesn’t have a visually robust user interface, so DBeaver provides a GUI that can be used with a DMBS to help you explore the tables in the database, manage users/roles, run queries - anything you might normally want to do in your database. You can interact with the database using a command line, through whatever programming language you are using, or through a GUI like DBeaver. Many DBMS do provide their own GUIs (e.g. pgAdmin for Postgres, MSSQL Management Studio for MSSQL, Oracle SQL Developer for Oracle). DBeaver is similar to most of them with one of the biggest differences being that you can use DBeaver with multiple databases. 
Why is the first count 4? Just to clarify, you have no ids on this table or anything, just a datetime column and then alternating values in the two other columns where you want to count the nulls in between them?
What are you actually trying to count? The intervals look regular. Are you counting the number of hours between items? Will Start or End time ever be different than the Master Calendar? 
It seems to start at start time and end at end time, so 1st row start, 2nd NULL, 3rd NULL and 4th end. 
Remove the paren before the first RTRIM and after the name after the AS (I think they are not needed, though i don’t use Access) or move the closing paren to before the AS. The AS has to be outside of the parens.
That’s did it! Thank you very much! It’s amazing how such small errors throw everything off. 
See if this works for you... WITH StartGroups AS ( SELECT *,CASE WHEN StartTime IS NOT NULL THEN 1 ELSE 0 END AS StartGroup FROM YourTable ) ,StartIDs AS ( SELECT *,SUM(StartGroup) OVER (ORDER BY MasterCalendar) AS StartID FROM StartGroups ) ,RowNums as ( SELECT *,ROW_NUMBER() OVER (PARTITION BY StartID ORDER BY MasterCalendar) AS RowNum FROM StartIDs ) ,CUEs as ( SELECT StartID ,MIN(RowNum) AS CountUntilExit FROM RowNums WHERE ExitTime IS NOT NULL GROUP BY StartID ) SELECT MasterCalendar ,StartTime ,ExitTime ,CountUntilExit FROM RowNums LEFT JOIN CUEs ON RowNums.StartID = CUEs.StartID AND StartGroup = 1 
That's a cool use-case for window functions. I've "simplified" your example to use integers instead of timestamps. Notice that your `MasterCalendar` timestamps are not continuously increasing, e.g. there's usually a gap of 1h between the timestamps, except at `10:00 AM`, after which there is a 2h gap. I've modeled this by using integers `1, 2, 3, 4, 6, 7, 8, 9`. I'm using PostgreSQL / SQL standard syntax. It's easy to translate that to any other vendor. ## Starting data set We start out with this data set, where `a = MasterCalendar`, `b = StartTime` and `c = ExitTime`: ``` values (1, 1, null), (2, null, null), (3, null, null), (4, null, 4), -- Observe a gap of 2 after this value (6, null, null), (7, null, null), (8, 8, null), (9, null, 9) ``` ### Create a consecutive series of integers, skipping the 2h gaps In order to "normalise" your data set, we need an additional column that is guaranteed to be a set of consecutive integers with no gaps. The `ROW_NUMBER()` window function is perfect for this. The assumption here is that there are no two identical timestamps. So, the query is now: ``` with d(a, b, c) as ( values (1, 1, null), (2, null, null), (3, null, null), (4, null, 4), (6, null, null), (7, null, null), (8, 8, null), (9, null, 9) ), t as ( select row_number() over (order by a) as rn, a, b, c from d ) select * from t; ``` Yielding: ``` rn |a |b |c | ---|--|--|--| 1 |1 |1 | | 2 |2 | | | 3 |3 | | | 4 |4 | |4 | 5 |6 | | | 6 |7 | | | 7 |8 |8 | | 8 |9 | |9 | ``` ### Calculating the count These row numbers are now consecutive and can be used to calculate the distance between two non-null values of `b` and `c`. For example, `c = 9` is at `rn = 8` and `b = 8` is at `rn = 7`, so we can subtract the two `rn` values, fix off-by-one: `rn2 - rn1 + 1` or `8 - 7 + 1` to get the desired count value of `2` How to do it? ``` with d(a, b, c) as ( values (1, 1, null), (2, null, null), (3, null, null), (4, null, 4), (6, null, null), (7, null, null), (8, 8, null), (9, null, 9) ), t as ( select row_number() over (order by a) as rn, a, b, c from d ) select a, b, c, case when b is not null then min(case when c is not null then rn end) over (order by rn rows between 1 following and unbounded following) - rn + 1 end cnt from t; ``` [SQLFiddle here](http://sqlfiddle.com/#!17/9eecb/20400) **Explanation:** The `cnt` value is calculated only when `b is not null`. It then searches for the next `rn` value, subtracts "this" `rn` value and adds `1`. The "next `rn` value" can be found using another window function: - The window frame is ordered by the `rn` value and starts at the next row from the current row. Remember, the current row would be a row with a value `b is not null`. So, we're looking at all the values below any value of `b`. - We use the `min()` function to find the *lowest* `rn` below any given value of `b`, but only for `rn` values whose corresponding value `c is not null` The result is: ``` a |b |c |cnt | --|--|--|----| 1 |1 | |4 | 2 | | | | 3 | | | | 4 | |4 | | 6 | | | | 7 | | | | 8 |8 | |2 | 9 | |9 | | ``` This sounds fancy at first. Learn window functions and it will all make sense. [I've blogged about window functions many times on the jOOQ blog](https://blog.jooq.org/tag/window-functions/), and I will blog about this interesting example soon as well.
Correct-- the left-most column has unique values and there is no separate ID column. And, yes-- I need to count the number of rows occurring in between each StartTime column entry and its closest subsequent ExitTime column entry.
The intervals are actually not regular; there are some sporadic gaps that prevent the "MasterCalendar" column from looking identical to a 24-hour schedule. That is why I must count the rows between items. To answer your other question-- the start time will always correspond to the entry appearing on the same row of the MasterCalendar column, and the ExitTime will also always correspond to the entry appearing on the same row of the MasterCalendar column.
So I'm not trying to be an ass by asking this. I'm hoping to help you and I understand the data better... You are trying to count irregular timed gaps between two rows. You could have a 12:00 entry and 12:00 start time, and a 15:00 entry with a 15:00 end time. In the same table, you could have an entry for a different day, but 13:00 and 14:00 entries in between. The first day will show 1, the second day will show 3. How will knowing that help you accomplish what you are trying to do?
Very cool! For what it's worth u/KMichel2500, I like this better than my solution. Here it is formatted to work with your column names... WITH t AS ( SELECT *,ROW_NUMBER() OVER (ORDER BY MasterCalendar) AS rn FROM YourTable ) SELECT MasterCalendar ,StartTime ,ExitTime ,CASE WHEN StartTime IS NOT NULL THEN MIN(CASE WHEN ExitTime IS NOT NULL THEN rn END) OVER (ORDER BY rn ROWS BETWEEN 1 FOLLOWING AND UNBOUNDED FOLLOWING) - rn + 1 END CountUntilExit FROM t ORDER BY MasterCalendar
You can join tables on any common columns. You dont need foreign keys to join tables 
i regret that i have only one upvote to give for "aren't next to each other"
Man cut me some slack, this is only my second week of the class, and even knowing what SQL is. :( 
Its tough when you first start SQL. but i want you to know that Access uses SQL, but its going to really hurt when you start hitting against a proper database service like Oracle or SQL Server. anyway: rtrim is just a function. as is a alias for a fieldname. select rtrim(FieldName) as NewName, rtrim(ltrim(Fieldname2)) as NewName2, Fieldname3, fieldname4 + ', ' + fieldname5 as LastFirst stuff like that. where as MSAccess uses &amp; in lieu of the + 
It’s all good, I know he was joking haha. I meant tables that dont have a common query. I was knocking my head against the wall for about two hours until I finally stumbled upon a video that showed me how to link by simply selecting the tables when the tab pops up when you first start a new query. I’ve managed to get most of them figured out now by using that strategy, it’s so much easier than typing everything out. 
Hey someone gets me! Haha. I was able to do it by selecting the different tables from the initial menu that pops up. I managed to get most of them done using this method. 
Thank you very much! I’m slowly trying to learn, haha. Good thing this isn’t really my focus of study, it’s just a required class for my Masters. It’s interesting to get a small glimpse of programming! 
Well, found the solution and thought I'd share. I had to make the unique index deferrable and initially deferred: `ALTER TABLE abc.def` `ADD CONSTRAINT "Tree_Index_UNQ" UNIQUE (ccc)` `DEFERRABLE INITIALLY DEFERRED;`
If the table is `abc` and the column is `ccc`, then you should be doing `set ccc` not `set list_index`. update abc set ccc = CASE when id=8 then 7 when id=7 then 6 when id=6 then 5 when id=5 then 4 end where id in (8,7,6,5);
If you're using SQL Server, and greater than 2005 version.. here's a rough draft &amp;#x200B; delete x from ( select \*, rn=row\_number() over (partition by title order by title) from sl\_product ) x where rn &gt; 1; Here you can run it as a select to see what would be deleted: &amp;#x200B; select \* from ( select \*, rn=row\_number() over (partition by title order by title) from sl\_product ) x where rn &gt; 1; &amp;#x200B;
This is exactly what u was gonna say! You did it best by also speaking to using a select first 🤣
Thanks. Been doing dB development for 6 years and didn’t know this was a thing. I always drop and re add them constraint. 
SQL is a database management language, not a programming language, if that eases your mind at all. No programming skills required. General knowledge in Excel will help with envisioning the structures, so that's great. Best resource will depend on how you learn and how fancy you want to get with the data. W3Schools is usually pretty solid: https://www.w3schools.com/sql/
I appreciate your question-- makes me verify the sense of things. The reason that knowing the number of rows between entry and exit proves useful information (despite the master calendar's seeming irregularity) is that the number, which represents a duration of time, will be compared to a "competing" number derived from a different portion of the table (not shown in this Reddit question). Even though the master calendar has irregular increments, all that matters is comparison among "competing" durations across a given entry. 
Thanks for the contribution. It turns out, though, that start time and end time CAN occur on the same line, sometimes.
There's some great stuff in here. What a clever notion to make numbered groups containing every row following a particular entry timestamp. I translated your code into the actual working model and got it functional. The only drawback I see with your solution is . . . contrary to my original intention, which I did not make clear, each of your entry timestamps only gets assigned an exit timestamp if a newer entry timestamp doesn't appear first. So, you have it to where most of the entry timestamps get "orphaned" without an exit timestamp. My original intention was for every entry timestamp to lay claim to an exit timestamp. In many cases this means that the same exit timestamp gets applied to multiple entry timestamps, which is fine.
Okay-- I will try to study Lukaseder's solution, but I will have to "level up" because I know nothing of window functions. I wonder if it will successfully address the drawback I encountered with your otherwise excellent solution (I described the drawback in a reply to your solution).
Wow, talk about a thorough response! I will have to hit your blog and learn about window functions, which I am eager to do, before I can try implementing this approach. Tell me, would this approach lend itself to tweaking so that the "overlapping events" you warn about could be accounted for? You see-- and I apologize for overlooking this detail in the original post-- but it turns out that if multiple entry-timestamps occur prior to the appearance of an exit-timestamp, that same exit-timestamp should get applied to all of those preceding entry-timestamps. That sounds like the very "overlapping events" that you alluded to.
I remember that feeling in "it's all foreign so it's all the same" :) Hmm, hopefully others can chime in on the how long question. I had a pretty decent background when I first learned SQL (Advanced Excel, some Access knowledge) so don't know if this helps. Think within a week -not sure how many hours- I was comfortable with the basics and practicing using an existing database: SELECT, GROUP BY, aggregate functions like SUM, COUNT etc. The statements to create tables and the data types to use should be pretty quick for you to pick up after that. Having some Excel knowledge is where that comes in handy. Getting into JOINS (accessing multiple tables in one query) can get a little trickier so might take some time. Same with designing the database. All depends on the complexity of your data.
When I asked back about it, my manager just wanted to make sure that I had something outside of work because the person before me was burnt out from just working/going home to sleep
Stanford has an awesome course on databases, give it a shot.
simple -- don't use BETWEEN WHERE CREATE_DATE &gt;= DATEADD(HOUR,23,DATEADD(day,DATEDIFF(day,0,GETDATE()-1),-0)) AND CREATE_DATE &lt; DATEADD(HOUR,7,DATEADD(day,DATEDIFF(day,0,GETDATE()),-0))) 
Solved. That worked Perfectly. Thank you very much for your help
Like the commenter above, I also recommend w3schools for the basics. Once you have a good grasp of the basics, move onto sqlservercentral.com . It has stairway lessons that go quite in depth on topics, in addition to a great community, free eBooks, and sargable forums. 
After reading your responses I think /u/lukaseder's solution will work best for you. It does behave the way you want, in that multiple start times are exited by the next exit time. I believe the overlapping warning was for situations where visually you wanted the intervals to look like this... S E |-----------| S E |-----------| It doesn't behave like this. Both starts are instead exited by the first exit time like this... S E |-----------| S E |-----| Which I think is what you want anyway? The only change to make is because of another comment I saw where you said that the exit time could be the same row as the start time. Assuming you want that to be calculated as 1, you just need to change the lower bound for the window frame from `1 FOLLOWING` to `CURRENT ROW`.
[this will help](https://db-engines.com/en/system/Informix%3BMySQL%3BSQLite)
I get a error message before i even execute. What I'm i doing wrong? &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
I guess you missed the part where I asked "if you were using sql server, and if so..." Sorry, I don't know mysql enough to translate my answer.
Sure thing. It had me stumped for a while so I was glad to come across this.
Thanks for sharing. By the way, what IDE is that? 
You can do it three different ways: 1) At the top of the screen using SSMS there is a little drop down menu that will let you pick the database. 2) You can indicate which database to use, and join tables from multiple databases (or servers) in the FROM clause, e.g. `FROM [Server].[Database].[Table]` 3) You can use the condition `USE [DatabaseName];` at the very top of your query and it will specify where to execute from. 
If you are using SSMS or something similar you can just right click on a table and click `SELECT TOP 1000`... the sample query it will generate should include the database name (with brackets) and give you an exact working example.
It looks like you're after a union of the two tables, with a sort on the results.
I didn't show/mention an IDE in the blog post, but the syntax highlighting is the Solarized Dark theme, if that's what you're looking for.
can't you just insert yourshift event table into your shift table, then order by employeeId, starttime?
I don't know if it was 2008R2, or if it has changed between versions, but IIRC I used to have issues with bracketing the schema/table when using `[Server].[Database]`.
So....sorting is your friend 
Can't use that either. =\ That would do it though.
To clarify, we can't use the built in asc or desc functions. 
I am not. What is a CTE? If you happen to have a resource that you would recommend that might help, that would be great! Thanks for replying. 
I have some named instance and have to include the brackets around server/instance. That's actually the only time I have to use the brackets. So I guess I really do [Server/Instance].Database.Schema.Table 
Yeah, brackets seem to be necessary around the server, but not for anything else, but I've had issues in the past if I try to bracket everything. These days I don't have any linked servers, so it's not really something I deal with anymore.
Keeping it as vague as possible... A self join on the table and only using a certain comparison operator can get you to a data set that has everything BUT the max value. Can continue from there to come up with ways to find the max without any functions. Let me know if you'd like me to expand at all.
Thanks so much. Let me give it a go and check back in. 
Common Table Expression. They're kind of like local views that only exist for a single query. It's easy to do what you want with one, but if you haven't covered them yet you might not be able to use one.
Someone posted this problem awhile back, I don't remember the solutions but there were some pretty clever ones.
No. See the Combined Result Set. There are now 14 rows because the SHIFTEVENTS are now in the SHIFT "timeline". So for Row 1 Normal is from 8am to 10am. Row 2 is now a Shift Event from 10am to 10:15 am.. Row 3 is now again Normal from 10:15 to 12:00PM, etc.
Except you'd insert a normal record to intersperse between the start of the shift, each event, and the end of the shift. So if there were 3 shift events during a shift, you'd have 4 normal records for that shift. 
should I just learn SQL?
Having dealt with both a fair amount, I can honestly say that in my experience, Oracle's support sucks. The biggest differences I see is that Microsoft has a lot more tooling around it to make things easier to manage. Of course, that's also easier to do when you're supporting fewer operating systems. With Oracle, you're going to spend a *lot* of time at a command prompt of some sort. With SQL Server, you can do things that way but SSMS (SQL Server Management Studio) also gives you the ability to do quite a few things. Microsoft has branched out what they're supporting recently, where you can now run SQL Server (supported!) in containers (Windows or Linux) and on Linux. Not everything is supported on Linux right now, but I expect that to change over time. For a developer or end user, the differences are mostly syntax. For a DBA, the differences are significant.
Maybe This? Select t1.* From t t1 Left outer join t t2 On t1.id &lt; t2.id Where t2.id is null
No need to expand, got em! Thanks so much!
Right on, glad it helped! 
I've used both heavily. Oracle is awesome for "undoing" (REDO) a delete or change. MSSQL is easier for patches and upgrading I know there are books and articles that compare them, but both do enough of the same for most systems. I also believe that Mssql has much more online free blogs and support. 
We run SQL 2017 on Linux, and SQL 2012 and 2014 on Windows
Using cursors for local variables in stored procedures. It made automating and scaling our customer analytics much easier.
Learning every day.
Agree, that is why i wrote one: https://github.com/gsvigruha/cosyan
I'm starting to feel that this might just kind of happen on its own... Only learnt about PWAs recently, and now that I've been looking more into this kind of thing, I can see it getting much easier for systems that were initially built to only work in the browser will become closer to native applications on both regular desktop + mobile operating systems. https://pouchdb.com/ isn't SQL, but based on Couchdb's protocol, but it lets you creating offline applications as PWAs, or using electron and things like that to run JS programmed systems offline, and then sync data when online. https://github.com/kripken/sql.js/ is a kind of small project, but it's basically sqlite rewritting in JS from my understanding. So you can use it in Node, PWAs, electron etc. I know some people hate this kind of stuff, especially electron, but I find it intersting seeing that the distinction between: * backend/database servers * browsers * desktop applications * mobile apps ...are all slowly disappearing in some areas. There's quite a lot of desktop software that already uses SQLite... Lightroom is is an interested big name one. I imagine this kind of thing will become more and more common over time. Many more companies are realising it makes sense to use some open source libraries rather than re-invent the wheel on everything. Before webdev was a big thing, SQL was pretty much only used on "big" systems... typical desktop software would use their own kind of data storage. But with all these open source libraries making this stuff easier, and also the fact that most webdevs are used to using SQL on most projects, I see it becoming more of a thing for desktop software too.
I like the revel in Oracle, haven’t found that nor an equivalent in MS SQL Server.
Yes, that’s what I meant. Thanks. I’m still getting my head around recursive algorithms.
Lots of ways to do it. ROW_NUMBER() window functions and correlated subquery, for example.
Select c1.id, c2.id from characters c1 join characters c2 on c1.id&lt;&gt;c2.id
Add a where clause c1.id&lt;=c2.id?
No worries - I keep looking for interesting problems to think / blog about :-) In the case of overlapping events, sure that can be solved with an extra level of indirection, as long as it is very obvious how to link individual start / exit timestamps. If you could post some example data, I might be able to come up with a solution (and another blog post, of course), soon.
Indeed, I was thinking of this kind of situation: S E |-----------| S E |---| Which can still be solved if there's an obvious way to link the two timestamps that belong together.
can you not get the same result with? select * from Characters c cross join Characters c2 where c.id &lt;= c2.id
For what it's worth, you mentioned Access needing more competition. Well, this is LibreOffice Base.
So the Client table has a single row per person. You mentioned a lot of modalities of investment... would the Budget table have a single row per person? Or would you have multiple rows per person for their balance in each type of investment? If the second, then without a doubt you need a different table. The PK would be on the Client Table (ClientID or whatever), then the FK on the Budget Table. The key on the Budget Table would be a combination of ClientID and InvestmentType (or whatever). If the first, then why not add the new columns to the client table? What would make it "too crowded". I can tell you there are some ridiculously wide (as in # of columns) table designs out there, so there's not a standard rule. But it's also possible to have 1-1 relationships between two tables. Not a common design, but there can be reasons. Since modelling is obviously new for you, I would recommend just writing a couple rows down on paper, that might help get an idea how the data would lay out in each scenario. &gt; search all history to see if he has the budget to do so. And that seems dumb. BTW, this is the way Bitcoin works, and also (still) the way some mainframe based investment software works. The advantage is that a history table and the budget table could get out of sync which would be very very bad (expensive!). It all depends how much risk you are willing to take that the budget table is wrong.
I will try running examples on paper in each scenario, that seems like a good idea. It's the first scenario, i wanna have budgetA, budgetB, budgetC, one for each modality. When I say "too crowded", I'm actually extending some principles from modeling/normalization: I think that it seems weird to mix personal info with the numbers. It's just that they seem like they need to be used in different contexts... But now I see that it actually doesn't matter a lot. So, from my perspective we have 3 possibilities: a) no info of budget within client b) budget info inside client table c) budget info on a separate table. Correct me if I'm wrong, but from your answer I undestood that the 3 cases are possible. I think I'm gonna go with the b) approach just cause a) seems a bit superficial (I'm doing this for a selective process), and c), a little excessive and dangerous. Thanks for your reply!
This is the most vague question you could ever ask. Is it worth it as an will you get a job with it? Most probabaly yes. You can make good money of you know SQL well. I am not even close to good with SQL but I'm making nearly £27,000 at 22 off of it.
Yeah, if you want to do anything with relational databases. SQL is pretty easy to get the hang of but optimization can be difficult. 
So I would think carefully about having columns for budgetA, budgetB, budgetC, etc. What happens when someone wants to add a new modality after it's built? If it's by column, now you have to rebuild every single report that accesses that datatable to include new "budgetD". If it's by row (with a BudgetTypeID column to specify which Budget), then the report will continue to work "as is". In general when you have multiples of things that are nearly the same, it's a bad idea to make each one a different column.
Maybe for the better haha. I feel my company is ages behind because cognos is our backbone. Might as well handwrite everything in a book, use a hand calculator, and draw your own visuals on a chalkboard. 
Yikes. I’m in a related industry and am in the early stages of starting that process, so that’s like a nightmare to me. We currently have a bunch of legacy systems that don’t talk to each other. Can you share what went wrong? Did the requirements change or were they not spec’d right to start with? Or was it just not the right product?
God I wish. I have seen millions poured into projects long after it was determined the project would not meet our needs. Even worse, most of the time after the millions are poured in, the project goes live and involves support contracts that cost 100's of thousands a year. It is like our management doesn't understand the sunk cost fallacy.
The database is intended to be used by small firms doing investments for their own employees (google translate is telling me this is called private pension, which is probably the wrong term), so yeah, I believe it definitely fits into your example. That was really helpful, thanks! &amp;#x200B;
I make north of $100,000 USD/yr mostly doing sql stuff. If you are a senior database engineer of a fortune 500 company plan on 150+. Is that worth it? If you are already a senior developer then it might not be. Most of my brogrammers make 100-130ish. 
A few things. One, I don't think they realized how hard it is going to be to maintain this system and the skill sets required to do so are hard to come by and expensive. Second, it was being pushed as a tool to replace all reporting and we would no longer need tools like SSRS because hey, "cognos can do anything". The more my co workers and I started looking at what was built and what was going to be built as packages for reporting, we said, this is not going to work. Basically, they would have packages for different subject areas. The problem is our reporting is very complex and many times you need to join data between multiple subject areas all over the place. In order to get this working in cognos, it would be the most ass backwards convoluted thing you can imagine. If our reporting was simple and based on clearly defined metrics then I guess it would work, but not for us. I knew it would fail from the start, but here we are a year later and it is gone. 
absolutely. We can continue using SSRS, tableau and SSAS tabular models. 
I think we spent over a million on this cognos project. We could have probably 90% by going with microsoft. 
wow i gotta learn it
Oh geez. That makes sense. We’ve had a few projects fail over the years because IT understimated the complexity of the business. Seems obvious to me that business will (eventually at least) want interdepartmental reporting! Then, on the other hand, the business side seems to underestimate the cost it requires to maintain IT systems...
In a word, yes. Arguably it is one of the easiest languages to learn, but one that will take some time to master. I've been using both PL-SQL (Oracle) and T-SQL (Microsoft SQL Server) for over 18 years and there is still plenty to learn. Not to mention the Business Intelligence and Report Design/Writing aspect. Many jobs in that space to be sure. Since you know Python you might try R as a transition. However, I will say that I'm surprised you've not already had to develop against a Relational DB (RDBMS). FWIW it's also a good plan to learn how to deal with NoSQL (Couchbase, Postgres, AWS DynamoDB, etc) because it is far cheaper to use with a start-up app.
yeah, they were sold that IBM cognos is the wonderful easy to use product that requires no code and everyone can build reports off anything and our lives will be a 1000x easier. What ends up happening is it takes forever to learn this tool and when you want some complex, you can't even do it so you end going back to tools you know work like SSRS. The problem is many people making decisions are "hipsters" and just hate everything microsoft. 
A large percentage of software projects in general fail. It’s not unique to BI. 
Wow i'm gonna learn it but r inst very popular is it &amp;#x200B;
ok
No, but I've definitely seen projects which dragged out way beyond the original finish line generating tons of extra costs. So I'm not sure which one is less reasonable :)
I believe what you are looking for is a Join. Since this seems like homework I'll just provide a hint. Where the two tables "Join" is on EmployeeID. Then you'd want to sort on start time and group by employee ID. HTH
What's even worse is when a company gives up an ERP implementation. Saw a project that was worked on for 3 year's end a few months before go UAT after dropping over 12 million. System was fine, but it was a call from a higher up and no one had any say. Crazy what a handshake between CEOs can do.
&gt;It is like our management doesn't understand the sunk cost fallacy. As long as you can take credit for something, it's not a loss. By corollary, as long as you can throw your competition under the bus because something exists, it's not a loss.
Wow. Yes, and you wrote word for word why the project I was involved with had the plug pulled. I was managing a group of BAs and testers at the time and it became increasingly clear our users weren't having it. Cognos over promised, and under delivered. 
Not terribly. It's still a bit new and working to gain mainstream acceptance. However, it's close I. Some was to Python which is why I suggested it.p
Most of the failed big projects I have worked on were becuase of what you describe. The project tried to be all things to all users. The "boiling the ocean" approach always sounds good to leaderships ears and can read well on paper. In reality, however, it often leaves large segements of the user base out in the cold. Then when the burn rate of the project starts to shift into the red, leadership will then take the "stop the bleeding" approach and usually just pulls the plug. In an ideal project, the scope should be constrained to the highest impact teams with version iterations to address the lesser needs. Again, leadership doesn't usually like this because it means "you won't get to my team and their reporting needs for 1.5 years? Forget that." It is a classic disconnect between the people making the decisions and the people who are actually doing the work.
R is quite popular, particularly in certain data analysis positions.
Just $1 Million on the BI project? I thought you said it was a large project. :)
Definitely not homework. I haven't had homework in 25 years :( There's more to it than a JOIN. I probably didn't clearly state what I wanted, I was hoping the Combined table example would show what I was after. The SHIFT Events have to be inserted within the range of the Shifts but represent their own rows, and then the gaps around the shifts have to be the shift itself. 
Hardware and software licensing alone will be over $1 Million before you even pay a penny to any staff. Staffing the project requires a minimum of a PM, a functional SME, one tech lead / architect, a DB engineer, and a BI dashboard developer (and more likely more than one of those last 2). So 5-7 minimum project size is over $1 Million per year in staffing costs.
what kind of stuff did IBM promise with cognos? 
Look r3pr0b8, I respect your strong stance on issues like readability and the leading comma convention and ANSI-91 joins above all else but this shit has to stop INNER JOIN Those words belong together, the JOIN isn't a new block from INNER.
The fuck is exam 70-49?
Sorry about that. It’s the MCSA: SQL Server 2012 Certification. 
So should I stop studying for the 70-461 exam and jump straight into the 70-761? Is the material extremely different? Thanks for the prompt answer! 
It covers 2014 too, but take the 761. 2016 has some cool features 
Thanks so much! May I ask what study material you’re using for that exam! Good luck to you as well. :) 
Thanks. :) I’ll look into it straight away! 
Hate Microsoft but like IBM? That's weird.
Union
In my experience the fail of implementing of the ERP system was predetermined as the client wanted exact copy of the business process which was very convoluted and required a lot of manual operations. In fact the new system was supposed to be changing from paper data entry to electronic data entry to the electronic version of paper forms. No libraries, no helping or pre filling at all. Just because client's staff and supervisors had no experience with any ERP system AND they was afraid to change procedures AND the protect manager from outside didn't understand how the company works. It was big fail and long (and expensive) way to finally bury the body.
BI should never be sold as a project, as it will never be a complete product. It will always grow as business needs evolve. Embrace agile, iteration when building your BI product and release to your business user rapid, useful improvements. This way, you will constantly be delivering a better and better product, always meeting user expectations.
i used cognos for a few years. it is POSSIBLE to use without knowing SQL but very difficult . the visualizations suck balls. the version of SQL they do use leaves out a lot of really useful functions. yiu saved your users a lot of grief. cognos sucks ass.
That wasn’t a software failure, that sounds like a design failure. I’m not saying Cognos is the best because all systems have their quirks, but you join your tables into views in Framework Manager just like other reporting tools.
I've seen this happen three times in my career. I don't believe anything the sales team or rfp team tell me now. We'll be rolling out a new system in the next two years that is supposed to "self serve" the data so the end users can pull their own reports. It's like I'm living groundhog day. Or there is some conspiracy where the various software companies give secret kickbacks to CIO's. By the time the project fails, usually the person who gave it the green light is either up or out.
Boiling the ocean. Never heard it. Love it. Gonna steal it.
 I need a sum of daily costs which have the account numbers 10521 and 10841. I need the sum of costs from the casing date and all the cementing dates linked to the casing date when the Casing.HoleSection = Surf. There will always be one casing date but there could be multiple cement dates. The cement table contains the casing run date. I can't directly link the cement table to the cost table because there are no dates in the cost table. What I have to do is link the cost table to the daily report table since they are linked by report numbers. Once they are linked by report numbers I can use the date from daily reports. The query (below) is working except when there is more than one occurrence of an account code. For example, WellID 12-25 has two cost reports that contain the costs I need to sum. It should be a total of 47,300. Instead, I am getting a total of 33,800 because it only counts 10841 once even though it occurs twice on one of the reports. Below is the query: Select Distinct Sum(DailyCosts.DailyCost) OVER (PARTITION by DailyCosts.WellID ORDER by DailyCosts.WellID) As LaborCost, Cementjob.HoleSection,Cementjob.WellID From DailyReports JOIN DailyCosts ON DailyReports.WellID = DailyCosts.WELLID and DailyCosts.ReportNo = DailyReports.ReportNo JOIN CementJob ON DailyCosts.WellID = CementJob.WellID Where (AccountCode IN('10521','10841')) AND DailyReports.ReportDate = CementJob.CasingRunDate AND CementJob.HoleSection = 'SURF' AND DailyCosts.WellID = '12-25' order by wellID If I change DailyReports.ReportDate = CemementJob.CasingRunDate to DailyReports.ReportDate = CementJob.JobStartDate It will total correctly but will not pick up wells where the cement date did not happen on the same date as the CasingRunDate. I know the WHERE needs to be modified to pick up both these dates (CementJob.StartDate and CasingInfo.RunDate) but I can't seem to get anything to work.
Dude how about a count by employee id on the last table you’ve got up there? A count of 3 means they’re in the clear.
Mssql has access to regex via clr integration.
What you seek is called Relational Division 
Perfect, let me play with that. Thanks!
Depending on the team structure this is often something well beyond the scope of even the lead developer. A lot of bad agile implementations gave the vaugue requirements come in to be followed by clarification that really is requirement creep beyond the abilities (within budget at least) of the selected tool. 
Honestly the choice here is not about My SQL or MSSQL. It's about everything else you use. Do you use a lot of Linux and other Unix based technologies? The I would go My SQL. Do you use mostly Windows/NT technologies? Then you want MSSQL. Seriously, they both have their upsides and downsides, but consider the overall environment first and foremost. You don't pick your relational DB because of a few features, you pick it because it fits your environment. If you choose Oracle with an like Windows environment your going to spend a lot of time looking at all the Microsoft offerings wishing u could use them. If you pick MSSQL in an environment that uses a lot of non Windows based machines, you're going to have a lot of uphill issues with making your relational DB consistent across your network.
One of the best online course. For beginners and for free. With examples and online exercises. [http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). 
Try it for an hour and see what you think. It's only 4 hours long. You can smash that out in an evening 
It's just a client. When I'm on my Mac I use it because SSMS isn't available
For your use case the amount of data should be fairly low and also the need for lightning fast real time transactions and synchronization should be low. In that case I think I'd create something like: * Customer table: Primary key + name and other details CREATE TABLE [Pension].[Customer]( [CustomerId] [int] IDENTITY(1,1) NOT NULL, [Name] [nvarchar](150) NULL, [Birthday] [datetime2](7) NULL, [ExternalId] [int] NOT NULL, [CreatedDate] [datetime2](7) NOT NULL, [IsActive] [bit] NOT NULL, [DeletedDate] [datetime2](7) NULL, CONSTRAINT [PK_Customer] PRIMARY KEY CLUSTERED ( [CustomerId] ASC)) * Account table: Customer can have multiple accounts. Linked with foreign key to Customer table. Has primary key (AccountId), account name, type, balance and date for last balance update. * ActiveTransactions table: New transactions are written to this table. Foreign key to Account table. Possibly an insert trigger that checks that no new transaction can bring account balance below 0. * ArchivedTransactions table: Every night the active transactions is moved to this table and the sum is added to the balance for the account in the Account table. This is probably overkill for a school assignment but still seems very simple and fairly intuitive to me. If you have a DBMS installed on your pc you can quickly create the tables, fill them with some rows and check query performance (let me know if I can assist). If you do this make sure that your primary keys are clustered and that you have non-clustered indexes on the foreign keys - then performance should be fast. Your inserts will be fairly small and rare in this type of setup so you just want to maximize performance towards querying the system. I hope this makes sense. Let me know if I can help out with anything :)
BI has a large number of different aspects that all have to come together for success. Each piece increases the degrees of complexity and chances for failure. Happy your management was smart enough to ser the doomed project and cancel it. Suggest breaking the effort up into separat pieces.
Someone who doesn't already know SQL may not be able to figure out if it's good or not.
I’ve also accomplished this with a view or CTE that pivots on what would be your trainings table, making a T/F column for each available training
Please share the temp table structures. You are joining all tables by DatabaseID only. Are you sure you want to do that? It sounds like a cartesian explosion. Do you have a TableID and ColumnID columns that you should be joining on also?
some starting points: make sure you have proper indexing on all tables involved in the join operations look at the query plan and try to identify any problem joins if you cant access or interpret the query plan, then reduce your query down to a single join and iterate from there and find out where your query is getting bogged down 
Why do you think a Masters is going to advance your career as a DBA? What will the Masters degree be in?
I've never heard of a DBA specific masters (if that is what you mean). What is the overall direction you want you career going in, and what was your bachelors in? If you want a generalist then perhaps a masters in data science or computer science, but I would question why you need the masters in the first place. Once you have experience in the role, that seems to count for a lot more.
I'm siding with the rest. None. Universities are hopelessly behind on data science. Bachelors don't use the word at all and in Masters education it's a fancy buzzword that's occasionally thrown around. IF they're having any database specific courses (and again, I haven't seen any) then it's going to be a very small part of your education and you'll be stuck with mostly filler courses that you have to complete around it. There are private courses on DBA. They even give certificates if that's what you're after. 
You have now come across someone who is familiar with it and doesn't pronounce it as sequel. 
Masters in Database Management which is obviously slightly different than the role of a DBA, but some aspects overlap. But if not a degree should I solely rely on experience and certs?
It would be a masters in database management or possibly computer science, I would like to eventually become a senior DBA which I know I can do with experience and due time, but if the company I work for pays for a masters than why not use that advantage?
well for me Ess-Q-EL is the propper way to call it. Just the German pronounciation. Yes even in professional/enterprise context.
When you say “pays for a masters,” does that mean they will pay 100% of your tuition and fees, or they will increase your salary once you have the degree? I’ve never worked with a dba with a masters before, and am a sr dba myself. But if it were 100% free, I might do it. Otherwise, I will stick to private training and technical conferences, which my company does pay for. 
I'll die on that hill, too.
This is it fellas! Our day has come! SQL nerds will be the new jocks, we will get all the sexy people to love us!
I teach it and work with it, and use it both ways... typically say sequel with people familiar with it, and EssQel for those not.... when i teach it, i say both. 
sorry, you missed the boat, that was am hour ago... go back to your desk. 
True enough: if they are going to fund it without too many contractual shackles (check this carefully!) then go for it! I don't know any of the top of my head, but i'd advise you to look at the syllabus that any provider will cover carefully. You'll want one that covers technical topics in depth, not merely just business stuff dressed up as DBA. 
Yes, and in true analyst fashion the boss waved him off as soon as it got technical. 
NOOOOOOOOOOOkay. :(
I use ess-Q-El when talking to others without a tech background and Sequel when talking with people with a tech background 
Old people tend to say ess queue elle.
Something something ivory tower. Academia is waaaay behind on databases 
union is just going to place the data next to it, that doesn't achieve anything.
Select Distinct Sum(DailyCosts.DailyCost) OVER (PARTITION by DailyCosts.WellID ORDER by DailyCosts.WellID) As LaborCost ,Cementjob.HoleSection ,Cementjob.WellID FROM DailyReports JOIN DailyCosts ON DailyReports.WellID = DailyCosts.WELLID AND DailyCosts.ReportNo = DailyReports.ReportNo JOIN CementJob ON DailyCosts.WellID = CementJob.WellID ​WHERE (AccountCode IN('10521','10841')) AND DailyReports.ReportDate = CementJob.CasingRunDate AND CementJob.HoleSection = 'SURF' AND DailyCosts.WellID = '12-25' ORDER BY wellID Do you have to use Distinct in this query? Have you tried just doing SUM() without the partition?
My last company was in a similar position. We had a BI solution from a 3rd party (can't even remember the name, it was so bad I've struck it from memory). I was able to convince management that instead of continuing with that software we could create a UI to sit on top of the data structure we already had in place for the 3rd party software. Long story short they provided minimum support and expected maximum effort. The new BI solution worked great and was way better. I've always been an advocate for home growing your BI solution if you can. That way you can implement what you need and not have to worry about all the bells and whistles that come with 3rd party software.
So they basically created a link server and then joined a table? SORCERY!
My wife and I were watching it together and I made the same comment. That was more Jim Halpert than Jack Ryan!
exactly, which made me laugh and my wife look at me funny. 
I've never seen a BI project succeed in scope and implementation. No matter where I've been they always manage to scope creep into failure or buy some other product Midway that doesn't quite work. I once did a project that was for a large company who needed their monthly financials automated. It was taking 20 days and involved 75 developers. SQL server, Oracle, Excel, Text and DB2. I migrated it to PowerBI using Alteryx. At the end they realized that they couldn't share the data because the people who needed it weren't on the domain. So it was completely scrapped back to Excel and manual. That's a 6 million annual labor cost plus software plus delay in business data. I'm gonna assume that's a 15 million annual loss for that failure. 
Work experience&gt;Advanced degrees
I think everyone in the EU is calling it S.Q.L.; Sequel feels like an americanism.
A master DBA does not become one by doing a MBA.
From the wikipedia page: &gt; After moving to the San Jose Research Laboratory in 1973, they began work on SEQUEL.[15] The acronym SEQUEL was later changed to SQL because "SEQUEL" was a trademark of the UK-based Hawker Siddeley aircraft company [17] That's why
Been working with SQL for a little over thirty years now, have always called it Ess-Q-El, so to say. I think it was originally pronounced and spelled SEQUEL but changed to SQL and pronounced Ess-Q-El, because someone had a copyright or a trademark for SEQUEL.
Can confirm, am old.
Basically man hahaha
Ah, yes. Thanks for the feedback. We are trying to keep this to very basic types right now, but will be looking into an additional document featuring anti-join types as well.
Cool - if you have the time and like school, I don't see how a free degree could hurt you, especially if your bachelors was not CS-related. But as others have stated, that's probably not the typical path for a DBA. I don't have any recommendations for masters programs, but if you need some solid MS SQL Server blogs/people to follow, good conferences, etc, I can help!
 SELECT * FROM yourtable where value &gt;= ALL(SELECT distinct value FROM yourtable) &amp;#x200B;
Big companies pay good money to people who are good at it. And the good thing is that it isn't hard to become proficient in SQL.
A masters has not been a determining factor in any hiring or technical job assignments that I’ve ever been exposed to. Save one type: IT management Source: am a DBA, and also an IT manager, with a MSIT with emphasis in Proj Mgmt, graduated in 2005. I was already several years deep in my IT career when my prior company paid for it, 100% with a 2 year time commitment. It’s been very good at getting me prioritized for interviews and with recruiters. And no doubt landed me both a Program Manager and DBA Manager role at two different companies. Having years of real world experience under my belt... I could have taught almost every class, especially the RDBMS, programming, and proj mgmt classes. I was doing it every day. The class work was not that hard, but it was a time drain. I had no life beyond work and school. I’ve taken it upon myself to stay an SME hands-on manager, and being a jack of all trades I specifically picked SQL Server to dive deep into after working with it as a developer/jr. admin for several years. I both do, and delegate. I set direction, strategy, etc and manage projects and resources. And I make sure my skills stay relevant like any good IT person does. My completed project list, current product knowledge and real world experience trump a degree every time. But they are great to get past HR for interviews or deal with places with odd rules (for example, my 1st employer had a rule that everyone beyond a first line manager had to have a masters). But if I had to pay for it.... no way!
True story - for quite a while, I was the only one on my team with a CS degree. We even have a theater major, although he did minor in math.
TIL there are people who don't spell out the letters when saying it.
Probably just asked if he could a PDF data export every week instead of dealing with queries. 
It is now an acronym for Structured Query Language, hence Ess-Q-EL. Technically this is more correct, but I do tend to say Sequel.
The Wiki someone linked below is a good description of what it is. In my context, I was moving from a technical support role to a database-oriented role. In my answer I explained the ways in which our database is *not* normalized and why that makes it hard to work with sometimes. (It's not denormalized, it's just normalized badly.) This showed that I understood both the concept and our unique institutional structure. I got the job, so I guess it worked.
I guess cross join another table with 64 outcome numbers? 
I feel like the only way to do it would be to blow up the set and then collapse it down. Could concatenate the fields and then cross join to a table of 64... Just more a curiosity than anything else.
Funny to see me asking the question there
What would be a good use case for this? I get how it works, but I'm struggling to think of an example where it would be useful.
/r/Backronym
http://www.databaseanswers.org/data_models/
I have a goal set to become a jack of all trades so I can become a top notch DBA. My first year as a jr DBA and working for my company is this month and I’ve learned a lot, but there’s so much more to go and I don’t want to cut corners. You’ve definitely given me a lot to think about! Do you have any recommendations to materials like blogs, certs, conferences, etc. that you think would benefit me?
"sequel" would be confusing for the average viewer, while S.Q.L. sounds properly nerdy
Yes, but most of my info is at the office. I’ll update again tomorrow.
Yes, but most of my info is at my desk. I’ll PM you tomorrow.
You’re right. Would have been a better cinematic experience if they would delve into details of sub queries 
What you are seeking is called the set of permutations, so Google that. You will find a lot of people working on it and no definitively good answers. Here's one that I have used once in a while : http://michaeljswart.com/2017/02/generate-permutations-fast-using-sql/
try PostgreSQL instead. It's more standards compliant
Excellent, I will do that! Thank you! 
To start, your class is probably using mssql, aka Microsoft sql server aka t-sql. That's an implementation of sql. Not all sql implementations support the same keywords and operations. The basics are basically the same, but they often have a couple of differences. Mysql doesnt support the intersect statement. You would have to do the equivalent by doing it out long hand.
If you are set on using MySQL look how to use the “IN” operator instead. 
I'm not, someone suggested PostgreSQL, which I will explore tomorrow morning.
Union will remove dupes.
Could you give an example of what you're trying to do, the result you're getting, and the result you expect to get.
Trying to sum values for an account where the actual account name exists in one table, but the unique ids for these accounts are different in each table or don’t exist. So basically it will sum table A, but it won’t sum table b since the id is null. So I get two results. A sum of the account Id with value and a sum of the null 
Maybe someone else can decipher what you're trying to say.
 select a.col1, a.col2, sum(nvl(b.col2, 0)) as "total" from table1 a inner join table2 b on a.col1 = b.col1 group by a.col1, a.col2;
Thanks. I was mentally trying to think if it was possible to do without a LOOP, some kind of self join on row number &gt; or something clever like that.
You can also use SQL Sever Express which is the free version of MS SQL
[https://selectstarsql.com/](https://selectstarsql.com/) &amp;#x200B; This is also a really good resource
Can you post your code? Also explain what you want to achieve. Then we can give better feedback. Basically everything in the Select statement that isn’t an aggregate function needs to also be in the group by statement. Sometimes I put my aggregate functions in separate CTEs or tables and then join them to the main part of the query later on. Anyway, post your code and we can give better feedback.
Microsoft has a scipt for it on their [wiki here](https://social.technet.microsoft.com/wiki/contents/articles/2321.script-to-create-or-drop-all-primary-keys.aspx). Can I ask though why you want to drop and create them? I assume the goal must be something else like then using search and replace to change naming policy in the create script or something? If we know the goal there might be a better solution. 
I think we had this already a few days ago 
[removed]
duplicates aren't my problem. 
No. some RDBMSs use MINUS instead of EXCEPT, that is true, but INTERSECT and JOIN are completely different things. However, MySQL doesn't support any of them, and you could fake them using JOINS. For example: `SELECT id` `FROM table1` `INTERSECT` `SELECT id` `FROM table2` could be mimicked by something like `SELECT DISTINCT id` `FROM table1 INNER JOIN table2 ON` [`table1.id`](https://table1.id) `= table2.id` &amp;#x200B; &amp;#x200B;
Even if we did, I hadn't seen it before so I appreciate the post.
Passing any of the Oracle 1z0-071 Dumps Questions certification exams is a painstaking task and demands a lot of hard work. But Realbraindumps is a big support in achieving this goal, as it has built up the confidence of many candidates by providing them the best quality exam dumps to succeed in their certification exams. I am also one of them who got timely help by getting 1z0-071 exam dumps and [1z0-071 Braindumps Exam](https://www.realbraindumps.com/1z0-071-braindumps.html) Testt Engine for my exam. I prepared 1z0-071 Dumps Questions Answers very keenly and practiced a lot with 1z0-071 test engine. To my surprised when I went for my 1z0-071 Dumps exam I found so many questions similar to those present in the practice tests. Hence Realbraindumps exam dumps are the main reason for my 1z0-071 exam success. &amp;#x200B;
/u/jazpermo \- I've been thinking about this in my head and have yet to come up with a satisfactory solution yet. I can see it logically, but can't work out how to make it happen in a query. I'm not giving up on it quite yet, but I'll have to come back to it. Have you considered posting this question in SQL Server Central Forums? Lots of excellent resources there and free to join. I'd post something like this here: [https://www.sqlservercentral.com/Forums/SQL-Server-2017/SQL-Server-2017-Development](https://www.sqlservercentral.com/Forums/SQL-Server-2017/SQL-Server-2017-Development)
the comment on SQL wasn't necessarily commenting on the data shown on screen. He appeared to be reviewing actual reports/files and not running queries. Regardless, what he had up looked silly, but the show is entertaining. 
Is your current position/company going to help pay for the degree? It may be better to look into some certs or continued experience (as others have said). It's funny, I worked in IT and got my masters online in Data Analytics. My masters didn't further my position as a DBA (what I currently do) rather pointed me to it. A masters might be something to look at further down the road, whereas experience is what is best now.
A fairly good introduction to it. I've only dabbled in programming before, but I want to take it seriously this time. I know SQL is used everywhere (front and backend), and I know it's databases, but nothing else. As for what (I think) I'll need it for - webdev/frontend possibly? While backend is more interesting to me, what I'll need is a strictly 8h day (for various personal and professional reasons). I figure if the server crashes at 3am a frontend dev won't be the one who has to get outta bed. ...right? And thanks for the tutorial recommendation!
Yes they will pay for all the expenses, but I don’t currently know of any terms (such as, complete in X years) I’m currently studying for 70-764 Microsoft cert and continue up from there. But I was curious if I should use my time into a masters and if so what major would benefit me the most or stick to other certifications and such.
How do I process the data into the actual target table in chunks?
That's pretty typical (depending on the database, is this Postgres?) with so few records. Although complexity-wise an index lookup is better on paper, most SQL DBs try to estimate I/O and constant time factors. Most indexes are non-clustered and the DB stores the index in a different table under the hood sorted by the index key. So in reality you have an O(log(n)) lookup on the index table to obtain row identifiers then another set of O(log(n)) lookups in the actual table for each row ID. Often a straight O(n) table scan will be faster, even for 1000s of records depending on the query and the hardware of the physical server. Once you get into 10s to 100s of thousands of records you should see index lookups exclusively, but not always. BTW, this a gross oversimplification of the inner workings of the DB. Most use pretty sophisticated algorithms to develop the query plan based on index statistics, RAM/page pool size, and a lot of other things. But at their heart they all take in some consideration of computational complexity and I/O cost. The point is the DB does all this for you and most of the time you don't have to worry about it.
&gt;I'd like to add the numbers together but I don't even know what datatype to convert them to. The least-bad way to do this is probably to parse the strings into their constituent pieces (hours, minutes and seconds), convert it all to seconds, add them up, then (if necessary) reformat back to `HH:MM:SS`. Ain't gonna be pretty. &gt;for some reason I also have malformed data in some of these columns Because date/time data is being stored as strings. The "reason" is that there's no constraint on how the data is stored, so people are able insert whatever they want in whatever format they want.
Are the customer numbers actually unique? In the first table it doesn't appear so. Is there account number in your data? This is just a non technical off the top of my head answer. 
You can use DISTINCT to remove those duplicates
&gt; What’s worse than data silos? Data silos that invent their own query language. *Amen*
&gt; What’s worse than data silos? Data silos that invent their own query language. *Amen*
When I use SELECT DISTINCT &amp;#x200B; I get one result for each but I can't group the ones together by name &amp;#x200B; &amp;#x200B; it appears like this: |1|John|Smith|12/03/1988| |:-|:-|:-|:-| |4|Patrick|Jones|01/01/1983| |1|David|Smyth|02/07/1963| |5|John|Smith|12/03/1988| |2|Patrick|Jones|01/01/1983| &amp;#x200B; I need it to appear like this, So I can see the customers and the different customer no's next to each other: |1|John|Smith|12/03/1988| |:-|:-|:-|:-| |5|John|Smith|12/03/1988| |2|Patrick|Jones|01/01/1983| |4|Patrick|Jones|01/01/1983| |1|David|Smyth|02/07/1963| &amp;#x200B; &amp;#x200B; &amp;#x200B;
&gt; As of now I am leaning towards putting it in ssrs whenever reasonable. Not doing too many stored procedures yet. Advantages of using stored procedures: * You can run, test, and debug the query independent of SSRS * Your friendly neighborhood DBA can tweak the stored procedure for performance *without* having to redeploy the report * Nobody wants to write complex SQL inside SSRS, it's so much easier in SSMS. * Stored procedures kept in separate text files are *much* easier to maintain *and diff* in source control
That also depends on how complex the queries are. For example if we're talking simple queries that are pointed at a "cube" which is designed for that task, and you have this all sitting on its own ReportServer... then it should be fine. This will somewhat depend on how many users are requesting things concurrently. In my experience with SSRS you tend to start in SSRS and write SQL code... and if it gets to a point where you're doing too much, then you need to go down and think about creating scheduled sprocs that dump data into tables that are designed to handle those reports. So for example a lot of times I might start with a report where I do 99% of the work in a custom SQL snippet such as: select stuff from table join table join table where stuff In time, as data grows, etc, this stops being an optimal practice and the report execution takes too long. So then I will create a job that does all that work and dumps it off into a table. Whether or not you create a view pointing at that table is not really relevant because all you have to do next is go into the SSRS report and change your snippet to: select * from table And it should work just like it did before. For me the salient point to take away from this is to avoid creating columns in SSRS and using the GUI to do the calculations as opposed to writing the calculations into the SQL. If you use the GUI to do that and you decide you need to make a change... now you need to go update all of the reports individually, whereas if you do it in SQL you only need to update snippets or sprocs, and a lot of the work can be simplified by copy &amp; pasting as opposed to having to click around.
&gt; So don't create calculated fields and spend a bunch of time in the GUI when you can be writing simple code to do that for you. Flip side: You can burn a lot of money in the form of CPU utilization (per-core licensing) asking the database engine to do a lot of text parsing/formatting and simple math that could easily be done in SSRS.
&gt; That's pretty typical (depending on the database, is this Postgres?) with so few records. This. It's likely just a single disk IO to return all the data in the table, so it's easier to just start at the first row and pull everything rather than trying to seek to specific records based on the index. If your table volume increases significantly (100k+ records or more) and you update the table statistics (so the optimizer is aware of the additional volume), then it might choose an index scan instead. 
Could you elaborate. &amp;#x200B; I am currently on-prem but will be moving to Azure next quarter.
This is beautiful and definitely useful, however the solution I come up with needs to be able to run in SQL Server and Netezza. Unfortunately Netezza doesn't support Cross Apply. :( Any other ideas?
https://www.reddit.com/r/SQL/comments/9d6lws/best_practice_ssrs/e5g2en3/
So you're saying SSRS the application itself will still utilize the server resources that SQL would use but can handle specific things better than simply doing the work in SQL and bringing the columns into SSRS?
I'm not that familiar with MySQL, but what I'd do with SQL Server is write a routine to loop over the records in batches, import them to the target, and then mark them as imported in the staging table. You could also add a delay between iterations to make sure you're not holding up other queries that may want to read/write the table for too long.
Thanks for the link, this is really great. Unfortunately I need to try to find a solution that will work in Netezza as well as SQL Server. I have the same set of 100 tables on each database and I need to be able to create the primary keys in SQL Server that exist in Netezza. I guess I'll start looking at how to do cursors in Netezza.
How long would you add a delay?
To remove the duplicates based only on Name and DOB, just modify the inner query to remove CustomerNo: FROM (SELECT FirstName, LastName, DOB From temp_table GROUP BY FirstName, LastName, DOB HAVING COUNT(*) &gt; 1 ) AS dupes 
I'm using PostgreSql. Here is what my table looks like. ``` aruprakshit=# \d posts; Table "public.posts" Column | Type | Collation | Nullable | Default ------------+-----------------------+-----------+----------+----------------------------------- id | integer | | not null | nextval('posts_id_seq'::regclass) title | character varying(50) | | | body | text | | | vote_count | numeric | | not null | 0 Indexes: "posts_pkey" PRIMARY KEY, btree (id) "posts_id_uindex" UNIQUE, btree (id) "posts_vote_count_idx" btree (vote_count) Referenced by: TABLE "categories" CONSTRAINT "categories_posts_id_fk" FOREIGN KEY (post_id) REFERENCES posts(id) ```
What is the database platform you are using? I am betting this will work: &amp;#x200B; `SELECT OutputId` `FROM OutputAttribute OA` `WHERE OA.AttributeValue = CAST(11 AS NVARCHAR(255))` `and OA.AttributeValue = CAST(11 AS NVARCHAR(255))` &amp;#x200B; The problem now is, you are repeating the exact same function. This is no different than the below: `SELECT OutputId` `FROM OutputAttribute OA` `WHERE OA.AttributeValue = CAST(11 AS NVARCHAR(255))` Also, this may become a data issue if you are looking at the same column. If the column should equal X and it should equal Y, that probably won't work. &amp;#x200B; &amp;#x200B; |ID|Value| |:-|:-| |1|X| |2|Y| |3|Z| &amp;#x200B; Consider the data set above. I want all rows where Value = X. &amp;#x200B; `SELECT ID, Value` `FROM Table` `WHERE Value = 'X'` &amp;#x200B; This would return the first row, we would see 1 and X. If we did: `SELECT ID,Value` `FROM Table` `Where Value = 'X' and Value = 'Y'` There would be no results returned because it's not possible for the same row to contain both values. Now if we did this: `SELECT ID,Value` `FROM Table` `Where Value = 'X' and ID = '1'` We would get the first row back. If we did: `SELECT ID,Value` `FROM Table` `Where Value = 'X' OR ID = '2'` We would get two rows back. Rows 1 and 2. If we did: `SELECT ID,Value` `FROM Table` `Where Value = 'X' OR VALUE = 'Y'` You would again get rows 1 and 2 back because it meets the criteria.
What is the create or alter statement for the vote\_count index? Also, /u/bandit1216 is right that it will probably still use the scan vs seek because with 5 rows, it probably won't use an index. Depending on how your index is set up though, it may never use the index regardless of row size. 
It's 4 spaces at the start of the line. SELECT B.customer_Id FROM Purchases B WHERE B.customer_Id = A.customer_Id AND B.purchaseDate &lt; A.purchaseDate AND B.minutesStreamed &lt;= A.minutesStreamed would return a result for the matched customer_Id for every prior date where they streamed fewer minutes then A.purchaseDate (which I'm assuming is today or a recent date). This together with your EXISTS clause means that a customer_id would get returned into your results if the corresponding customer has *ever* streamed fewer hours than A.minutesStreamed.
Put four spaces in front of code to get it to format appropriately. Eight spaces if you want to indent the second line, etc. Easiest way to do it is to just select it all in SSMS and hit TAB, then paste it into Reddit.
When filtering on aggregates (like SUM), you use the HAVING clause: SELECT EmployeeName, SUM(EmployeeSales) AS Sales FROM Sales GROUP BY EmployeeName HAVING SUM(EmployeeSales) &gt; 0; 
Try HAVING Sales &gt; 0 after your GROUP BY.
I would probably write a small script and stream the data. Because of how streams work, you wont use all memory (RAM) because you dont need to load all the data in memory. Id probably use something like Python. I find writing these kinds of operations easier in a "real" language, than in pure SQL. 
I'm trying to wrap my head around the problem and try to think whether there's a better solution, but to answer the question you've actually asked: In an EXISTS like you have it, that will return TRUE back as soon as it finds any row customer_id that streamed fewer minutes at any date before A.purchaseDate. I think this would only exclude customers who have been monotonously streaming more minutes every date.
You have to do a self-join. select a.OutputID from OutputAttribute a inner join OutputAttribute b on a.pkcols = b.pkcols where a.AttributeValue = '11' and b.AttributeValue = '4';
Yup. `ORDER BY FirstName, LastName, DOB`
 SELECT a.customer_id FROM ( SELECT ROW_NUMBER() OVER (partition by customer_id ORDER BY purchaseDate) AS RowNum, minutesStreamed, customer_id FROM Measurements ) a INNER JOIN ( SELECT ROW_NUMBER() OVER (partition by customer_id ORDER BY minutesStreamed DESC) AS RowNum, minutesStreamed FROM Measurements ) b ON a.RowNum = b.RowNum WHERE a.customer_id = b.customer_id AND a.minutesStreamed = b.minutesStreamed I've sort of adapted this from a StackOverflow question and don't have a DB in front of me to test it, but try this. If minutesStreamed is monotonously decreasing for a customer then we should get the same order for rows if we order by purchaseDate ASC and minutesStreamed DESC.
 SELECT OutputId FROM OutputAttribute OA WHERE OA.AttributeValue IN ( CAST(11 AS NVARCHAR(255)), CAST(4 AS NVARCHAR(255)) ) GROUP BY OutputId HAVING MAX(OA.AttributeValue) = CAST(4 AS NVARCHAR(255)) AND MIN(OA.AttributeValue) = CAST(11 AS NVARCHAR(255))
thanks for explaining it.
"DSLs" is a much more commonly used acronym outside of IT.. This graphic LULZ. 
&amp;#x200B; SELECT OutputId FROM OutputAttribute OA INNER JOIN OutputAttributeDefinition OAD ON OA.AttributeID = OAD.AttributeID AND OAD.AttributeDesc = 'Claim Invoice Status' WHERE EXISTS(SELECT TOP 1 1 FROM OutputAttribute iOA WHERE iOA.OutputID = OA.OutputID AND OA.AttributeValue = CAST(11 AS NVARCHAR(255))) AND EXISTS(SELECT TOP 1 1 FROM OutputAttribute iOA WHERE iOA.OutputID = OA.OutputID AND OA.AttributeValue = CAST(4 AS NVARCHAR(255))) &amp;#x200B;
&gt; You have to do a self-join well, no, you don't ~have~ to... there are other ways
So it will theoretically go through every single b.purchasedate, compare it to every single a.purchasedate? Same for the customer_ID where clause and the minustesSTreamed clause?
 SELECT OA.OutputId FROM OutputAttribute OA INNER JOIN OutputAttributeDefinition OAD ON OAD.AttributeID = OA.AttributeID AND OAD.AttributeDesc = 'Claim Invoice Status' WHERE OA.OutputId IN ( SELECT OutputId FROM OutputAttribute WHERE AttributeValue IN ( CAST(11 AS NVARCHAR(255) , CAST(4 AS NVARCHAR(255) ) GROUP BY OutputId HAVING COUNT(DISTINCT AttributeValue) = 2 )
What about autoincrement primary keys, do I insert those before or after data loads?
&gt; SELECT OutputId &gt; FROM OutputAttribute OA &gt; INNER JOIN OutputAttributeDefinition OAD &gt; ON OA.AttributeID = OAD.AttributeID AND OAD.AttributeDesc = 'Claim Invoice Status' &gt; WHERE EXISTS &gt; ( &gt; SELECT TOP 1 1 &gt; FROM OutputAttribute iOA &gt; WHERE iOA.OutputID = OA.OutputID &gt; AND OA.AttributeValue = CAST(11 AS NVARCHAR(255)) &gt; ) &gt; AND EXISTS &gt; ( &gt; SELECT TOP 1 1 &gt; FROM OutputAttribute iOA &gt; WHERE iOA.OutputID = OA.OutputID &gt; AND OA.AttributeValue = CAST(4 AS NVARCHAR(255)) &gt; ) &gt; Thanks for this. Tried it, I'm getting no values back.
A little bit too contrarian. ElasticSearch isn't just a DSL , it has a completely different underlying storage and indexing structure, of course SQL isn't a good fit for it. Same can be said for Splunk, Cascading, Kusto, even MongoDB. The worst is when it almost looks like SQL but it's got so many custom functions and clauses that you still have to learn a new dialect.
In MSSQL as well I believe there is some optimisation where it will stop matching once it find a match for each A row. But barring that it will attempt to match every row in A to every row in B until it finds a match or determines that there are none. (This is generally true, but the DB engine might skip over records if it has relevant data, like in the case of a merge join where it knows the order of the records involved.)
Got it, thanks! If it was an 'in', you're saying it would actually do every single match. Since it is an 'exist', it stops at the first match. I just wanted to conceptually understand if it was going to (essentially) compute to the cross product between the two columns / arrays. Sounds like that is indeed what it does.
They are called identity columns, and they are created when you are setting up the destination tables. So long as they are setup you can more or less ignore their existence, you don't need to specifically call them out during inserts. The exception being if you plan on using it as a FK, in which case you'll have to lookup the value from the first table during your insert into the second/details table. 
After many hours of pulling my hair out...i managed to come up with a working solution http://sqlfiddle.com/#!18/5b48c/9/0 
The tools a lot of DBs have for bulk loading does what you're saying already. It generally doesn't just load everything into RAM, it would do it more intelligently than that. I would bet that your need to use other languages is due to lack of knowledge of SQL and the engines you have available to you. That doesn't make SQL not a 'real' language.
Nice solution! I was trying something with a CASE statement, bit kept running into syntax issues.
To be fair, shouldn't Lucene be described as a language for *searching* specific results as opposed to *querying* database?
Im sorry the term "real language" was not the right one. Indeed i have not used tools DB admins use, or do i know what they are capable of. The question seemed like this to me: - Lots of data dumped in a large csv - Need to manipulate the data quite a lot before inserting - Also need to have logic based on inserted data If this is all easily doable in some tool i dont know about, sure use it! To me it just seemed like a perfect fit for a small 20-50LOC one-off script.
That will definitely fall under my last complaint. elasticSearch criteria isn't joins and wildcards, it's more like Apollo or Hive (another too close to SQL dialect); replicating its "search" functionality - geospatial, faceting, similarity scores, tf-idf modeling, etc - with SQL will just be annoying.
I suggest parsing and storing them as seconds. So RIGHT(Time1,2) would get you seconds. LEFT(RIGHT(Time1,5),2) gets you minutes and so on. As for issue #2, what are those entries supposed to mean? You can't parse them in any meaningful way until you figure out how they should be interpreted.
The often forgotten HAVING clause is exactly what you should use here. It specifies a predication for the aggregation phase of query processing. This applies to all tuples within the aggregation in the case that you are using GROUPING SETS, ROLLUP, or CUBE. IF you need predication for a specific tuple you can use GROUPING scalar and combination logic to limit.
A repost, on Reddit? Well glory be.
And people wonder why I'm using the SPLUNK, ServiceNow, Salesforce connectors for my BI suites!
There may be. Just because "it works" with a simple view doesn't mean it can't work better with a stored procedure. Test both and see which works better. then re-evaluate every few months as your data and needs change. That isn't to say you can't use a view and then query it from a stored procedure, with the report calling the stored procedure. That way, when you *do* hit a performance issue or change in requirements/data that requires a significant change (like moving from a view to something that required more involved logic), you can alter the stored procedure instead of having to redeploy the report.
Is there any reason there application can't connect directly to the listener AG? If they point it to the Listener AG DNS regardless of failover it'll always go to primary. 
&gt;From the above pic, the first row with OutputId = 14, is the odd one out. Since it does not have both 11 and 4 as entries. The other rows below all have 11 &amp; 4, except 14. &gt; &gt;So the result I'm expecting to get from the query is all the other numbers, except 14. &amp;#x200B; You are selecting from OutputAttribute, this is your primary table. When you reference a LEFT join, you are taking the results to match from the OutputAttribute table and matching them to the OutputAttributeDefinition table. If you had a no match between them but the record exists in OutputAttribute, you will see that row. If you had no match between them but the record exists in OutputAttributeDefinition, you would not see that row. The join you are using is an inner join, so that means only records that match between both tables will be seen. In addition to this, it is also going to limit the matching to rows where the OutputAttributeDefinition equals Claim Invoice Status. This is not the same as putting it in the where clause, so you may want to try that. Your where clause is looking for the AttributeValue where it is 11 or 4. Now, let's look at this. All descriptions = Claim Invoice Status which is a filter condition. All AttributeValues are 4 or 11, which is a filter condition. All AttributeID rows exist between both tables where the AttributeDesc is also Claim Invoice Status. Based on your query, the result set is exactly what I'd expect back. I see nothing that would filter out the OutputID of 14 in your query. I understand what you need accomplished though, you would be expecting another row for Outputid 14 to have the AttributeValue of 11 as well but it isn't there, that is not a criteria I see in your query however. That is the explanation of why you are getting the result set you are. What you would need to do is one of the few suggestions below, other folks have re-written your query to match it up. /u/r3pr0b8 and /u/jc4hokies have great ideas. 
Wow, a literal screenshot. Nice.
I'm not sure, but this might help you: https://dba.stackexchange.com/questions/171331/why-does-searching-for-like-n-match-any-unicode-character-and-n-match
OMG a HARDCOPY of a SCREEN in COLOR
You might be able to use Unicode Categories for this (if your DBMS supports regex, and supports them): https://www.regular-expressions.info/unicode.html#category For example, `\p{C}` will match "invisible control characters and unused code points". These are likely the values that are showing up as question marks because they're not real unicode characters. If that isn't enough you can always add more categories to your regex.
My guess is this is what you're looking for: https://www.info.teradata.com/HTMLPubs/DB_TTU_16_00/index.html#page/SQL_Reference/B035-1143-160K/wxe1472241381672.html So I would expect something like: WHERE Field LIKE U&amp;'%#FFFD%' UESCAPE '#' But I've no idea if that's correct. I'd start with `SELECT U&amp;'%#FFFD%' UESCAPE '#'` and see what that even returns. 
What is hex value of that character? Use its hex value to filter. --validate Hexvalue FROM_BYTES(TO_BYTES('hexvalue','base 16'),'ascii') 
Yes I got their point. Nice explanation.
 SELECT Location, Product, Date, Delta FROM tab1 WHERE Date = SELECT DATEADD(month, DATEDIFF(month, 0, Date), 0) Order by Date, Location, Product
 SELECT a.Location, a.Product, a.Date, a.Delta FROM (SELECT Location, Product, Date, SUM(Delta) OVER (PARTITION BY Location, Product ORDER BY Date) as Delta FROM tab1) a WHERE a.Date = DATEADD(month, DATEDIFF(month, 0, a.Date), 0) ORDER BY Date, Location, Product Something like this? The SUM(Delta) OVER is a window function; if I've written it correctly it should give you a running total of Delta per Location and Product. Then the where gives you back just the dates that match the start of the month.
You rock dude! This works perfect, thanks!
Cool, no problem, glad it works!
Because then they'd have to log into SSMS and run the package. I'm looking for an easier way... Something like a batch script or something like that.
You could do something like create an SSIS job that looks in a directory every 5 minutes and sucks up a file.... then based on either the name of the file, or the contents of that file trigger jobs accordingly. Not exactly sure how this would look but a quick thought would be something like an Excel file with 2 columns saved to a network folder where you can limit access. Users open it up, type the name of their job and maybe some kind of "password", or their username, whatever. SSIS sucks the data up and puts it into a log table of some kind, and then you have another job that routinely scans the log table for new entries which will then trigger the possible jobs. Others might have a better solution. I currently am able to send an email to an address and have jobs run on our production server. No idea how it is set up or what technologies are leveraged.
Well you could package it up in Powershell I guess? The sp_start_job stored proc can be used to start a job. The problem is giving them access to run the jobs. You could add them to SQLAgentUserRole, but you'd have to make them the owner of the job for them to able to start it. SQLAgentOperatorRole can execute any job, but *it can start any job*, which might be too much access to give. There's a good bit on StackExchange [here](https://dba.stackexchange.com/a/72098) about creating an SP to allow a user to start a job without owning it. So you could create an SP for each of the relevant jobs, grant your users access to execute the SPs and then create a Powershell script for each job.
This is a cool article. Thanks.
Well then it's up to them to do it, and not you. If you're looking for a script to run, then just use powershell.
Most DB's have system tables that you can query that already store the metadata about relationships and column types. I know postgres does as well, though I can't quote you how to do it off the top of my head. So that eliminates the need to manage nearly all of that, you'd just have to worry about metadata that's not already in the system tables. 
Yes, I'm thinking of only storing application descriptions on the columns comment: CREATE TABLE "public"."NewTable" ( "table_id" int4 NOT NULL, "name" varchar(255) NOT NULL, "email" varchar(255) NOT NULL, "password" varchar(255) NOT NULL, "address" jsonb NOT NULL, "customField" varchar(255), "customField2" varchar(255), "etc" datetime NOT NULL, "manyToMany" varchar[] NOT NULL, PRIMARY KEY ("table_id") ) WITH (OIDS=FALSE) ; COMMENT ON COLUMN "public"."NewTable"."table_id" IS 'system'; COMMENT ON COLUMN "public"."NewTable"."name" IS 'system'; COMMENT ON COLUMN "public"."NewTable"."email" IS 'system'; COMMENT ON COLUMN "public"."NewTable"."password" IS 'system'; COMMENT ON COLUMN "public"."NewTable"."customField" IS 'resourceURL'; COMMENT ON COLUMN "public"."NewTable"."customField2" IS 'imageURL'; COMMENT ON COLUMN "public"."NewTable"."manyToMany" IS 'fk_tablename'; In this example `system` comments makes it so that the user can't edit/remove those columns, `fk_tablename` tells the application that the entries are referencing table `tablename.table_id` (must guarantee all tables have table_id as primary key), `imageURL` would hint that this column stores images, so then the application can only accept image uploads through the api, etc.
You can creat a batch file with sqlcmd. echo off sqlcmd -E -S SQLSVRBOSTON1\MyDB1 -i C:\Temp\ClearTables.sql set /p delExit=Press the ENTER key to exit...:
That's going to catch a lot of characters besides U+FFFD...
Note: keywords exist as a string in the two columns 
Almost got it.... just use `IN` instead of `CONTAINS` use OR and group with parenthesis: Select * from Table Where Column=‘Name’ AND (DestPath IN (select Keyword from PROJECT_KEYWORDS) OR SourcePath CONTAINS (select Keyword from PROJECT_KEYWORDS));
is there any chance of a new repair ticket being started before a previous ticket has been closed? &amp;#x200B; for ex. could row id 4 have started on 10/09/2017 and ended on 15/09/2017?
Depends on what your schema looks like, the above doesn't seem to have a transaction type anywhere. Assuming that transactionType is a column in the Mdtransactions table, you'd add AND tr.transactionType = 'labor' just before the group by.
Hi again, How about the location column is on a different table? It was recently removed off my usual table to a different table:(. I've been trying to figure out. The "order" remains on both tables. Hope you can help. Thanks! 
I don't know how to do nested IF's using SQL but you could break this up into multiple stored procedures such as IF THIS THEN EXEC THAT, then in that procedure you can do IF THIS THEN EXEC THAT, and so forth.
Are you saying that the keywords might be in the string along with other words? e.g. you might want to match on 'DOG', but one of the DestPath entries that you want to match on is 'DOG CAT DUCK'
Yes. I want to find all destpath values that contain any keywords in the table project keywords. They exist as a string in destpath. 
Something like this maybe? This is SQL Server, you'd be using strpos is something like postgres or, I think, instr in sqlite. SELECT * from Table WHERE Column='Name' AND (CHARINDEX((SELECT Keyword from PROJECT_KEYWORDS),DestPath)&lt;&gt;0 OR CHARINDEX((SELECT Keyword from PROJECT_KEYWORDS),SourcePath)&lt;&gt;0)
Why last row is flagged as Yes? That client doesn't hava any other errors of type 615. &amp;#x200B; And you haven't mentioned your language. With SqlServer 2012+ LAG window function does everyting you need in one row: &amp;#x200B; declare @t table ( event_date date, client_id int, error_id varchar(32) ) insert into @t (event_date, client_id, error_id) values ('2018-09-01', 12345, '232 send failure') -- No ,('2018-09-02', 12345, '232 send failure') -- Yes ,('2018-09-03', 12345, '232 send failure') -- Yes ,('2018-09-05', 12345, '232 send failure') -- No ,('2018-09-01', 12345, '508 server failure') -- No ,('2018-09-02', 12345, '615 script break') -- No ,('2018-09-01', 67890, '232 send failure') -- No ,('2018-09-02', 67890, '232 send failure') -- Yes ,('2018-09-03', 67890, '404 load failure') -- No ,('2018-09-01', 67890, '508 server failure') -- No ,('2018-09-02', 67890, '615 script break') -- Yes select * ,iif(datediff(day, lag(event_date, 1, '1900-01-01') over (partition by client_id, error_id order by event_date), -- prev event_date per client_id per error_id event_date) &lt;= 1 -- and diff between prev date and current one must be not more that one day ,'yes', 'no') flag from @t &amp;#x200B;
Oh, you'd want to move it up to the join clause then, so left join Mdtransactions as tr on wo.workordernumber = tr.WorkOrderNumber and tr.transactionType = 'labor' The wo. fields will all still be populated regardless of whether there's a match. 
Oh, there's a thought, are the keywords in your strings always separated by the same characters? As in, commas or tabs or spaces, or whatever? You could use STRING_SPLIT if you're using SQL Server 2016 or 2017. Or you could write a table-valued function for the same in an older version.
CHARINDEX is returning a non-zero value (which means its working). But it’s not telling me what those values are. How would I get it to just output all those matches that contain any of the keywords in the table PROJECT_KEYWORDS as a string in destpath? Regardless if they are separated or not. I would prefer it it outputted the whole row for the matches Destpath is a file path. I’m looking for the word DOG in “C://desktop/users/picsofmyDOG2018”
Yes yes!!! A million cookies to you thank you so much!
Assuming annual.Sales is an int, you're better off leaving the apostrophes out of the WHERE clause. And in your question you stated greater than 25000, but your query is selecting less than 25000. But the question is a little vague, is the table of individual sales, and you need to find the annual sales figures, or is the table a list of annual values and you just need to filter it out? If the table is just of individual sales, you'll need to do a group by statement.
Create a proc that executes a job. Put a button somewhere that executes that proc.
Once you set the value of that variable @Status, that value is referenced, not the query that calculated that value. So you should be good. If you want to double check, add "select @Status" after the section that inserts the row, and you should see it stays at the original value. I've written looped procedures where I'm storing min(uid) in a variable, then deleting that row when done, and I have to reset the variable to the fresh min(uid) before the next loop. 
&gt; WHERE Annual. Sales &lt; ’25000’; This is "less than 25000". Also, if your AnnualSales column is a numeric datatype, the single quotes around 25000 aren't needed; those are only for string (varchar) datatypes.
They can also replace cursors for some scenarios where you need to do row-wise processing and are generally faster in those cases. 
Needs more clarification for sure, but if I had to guess based on how a sane person would set up their db: Select user_or_customer_id_you_want_to_do_stuff_with, SUM(individual_sale_value) total_sales FROM sales WHERE sale_date BETWEEN &lt;year_start&gt; AND &lt;year_end&gt; GROUP BY 1 HAVING SUM(individual_sale_value) &gt;= 25000 Order by 2 DESC
Yep. I was just about to say instead of searching for Unicode, search for all that is not Unicode. 
Can you show your table structures? That matters quite a bit on this. a row_number() over (partition x) might work depending on your table structure.
For that use case you probably want a table valued function
Do each of the records include both employee IDs? 
As to your last point, it's a fair point. The reason I want it to be logontime is so that when I eventually subtract the two to get a session duration, I can eliminate 0 second sessions (Every time someone logs onto our software's homepage, a bunch of RSS feeds fire and log false positives in the usersession table). These RSS false positives log an entry into dbo.usersession but not dbo.useractivity, but I think leaving it Null and eliminating those probably makes the downstream data cleaner. I'll definitely check out coalesce though. My biggest problem, though, is that sometimes there is one entry in dbo.UserActivity, and its value is sometimes earlier than the logontime making my eventual sessiontime calculation a mess. What I ultimately wanted to do was include one more OR statement right after the IS NULL to eliminate these scenarios. 
Yeah, leaving it null, you can just eliminate the nulls later instead of having to calculate session duration to eliminate 0 second sessions. You want to avoid calculations (and calculated columns) in where clauses anyways. Coalesce takes values from left to right, and returns the first non-null value: coalesce(null,null,'Some value', null, 'some other value') --returns 'some value' coalesce((SELECT Max(ua.time) FROM useractivity ua WHERE ua.usersessionid = us.usersessionid), us.logontime) as logofftime Is how coalesce solved the first one, if you really wanted to get rid of the null. Adding the additional stipulation, and keeping the null: (SELECT Max(ua.time) FROM useractivity ua WHERE ua.usersessionid = us.usersessionid Having Max(ua.time) &gt;= us.logontime )as logofftime I think that should work.
Ah that works perfectly! Thanks much. coalesce is a lovely little statement. Now I need to do the intellectual work of deciding what a session is, and whether to use the actual logofftime and have drastically inflated session durations, or to use the last registered action in the system as the effective logoff time. 
SELECT * FROM Sales WHERE [Annual Sales] &gt; 25000
You know what "tri" stands for..don't you?
Unfortunatly yes. The field is user input so it could be anything. &amp;#x200B; However I will use your suggestion as it would get me closer than I'm currently.
Hey, dsvella, just a quick heads-up: **unfortunatly** is actually spelled **unfortunately**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
As someone who is relatively new to SQL - I learnt sub queries through temporary tables. The best way to learn is as you have done, start with the final table - but then figure out if you need to make any intermediate steps between the initial table and the final table. From there, start by writing the part that will eventually become the sub query and make sure it works, then just slap some brackets around it and treat it like you are selecting from any standard table. For reference - here is how I did it when I was learning: &amp;#x200B; `SELECT Location, Product, Date, SUM(Delta) OVER (PARTITION BY Location, Product ORDER BY Date) as Delta` `INTO #TEMP1` `FROM tab1` &amp;#x200B; `SELECT Location, Product, Date, Delta` `FROM #TEMP1` `WHERE Date = DATEADD(month, DATEDIFF(month, 0, a.Date), 0)` `ORDER BY Date, Location, Product` &amp;#x200B; Once you have that idea down, creating more complex sub queries becomes much easier. &amp;#x200B; &amp;#x200B;
&gt; I just want one of the two returned WHERE person1.id &lt; person2.id 
This is so cool. For anyone starting with this a huge obstacle is always having something new to play with. 
Thanks, make sense to just dice it all up into seconds, add them, and then convert to a real time format.
Solution Verified. &amp;#x200B; Thank you! 
it's not a triangle when it has four corners...
as in, tri again?
thanks for the tip :) will definitely be practicing my subqueries. Had a bit of a eureka moment with them yesterday where it finally clicked.
...and, to be more helpful, the SQL standard way to refer to this and other Unicode codepoints you don't want to type out literally is u&amp;'\fffd' (the backslash is the default in Postgres, but you can choose another escape character if you like)
I would also guess you need to link (join) this table to the customer table, odds are the sales table has a customer id, and you need to join the customer if in the sales table to the customer id on the customer table to get the customer who made these purchases. Not sure why you'd want to know which sales were over 25k otherwise.
Here is what it is as I understand it. A cursor is simply a memory space where "something" is stored. Typically the "something" is a select statement of data. The data can be selected into variables held in the memory space. Then, for each record in the cursor, you can loop through it, such as a programming for-loop, and you can manipulate the memory space, reading and writing from variables, and then typically when you're out of the loop, you might insert the manipulated result into another table or update the original table. The goal is to give you the power of a procedural language when typical set operations won't easily work or are too many steps. 
Is a cursor any different than a loop for non-integers? 
I can't show because of PII but I can try a mock if that will help?
It is a pointer of sorts that iterates row by row over query results allowing for row by row processing. I've heard bad things about performance and have never needed to use them myself if I thought hard enough. 
Im hoping to get only the last employee to record a labor hour, with the below code it creates a new entry for each labor transaction. I'm hoping to make it so that it only shows 1 entry per workordernumber. Thoughts? `select wo.locationname,wo.craft ,wo.WorkOrderNumber ,wo.RequestDate, wo.RequestDesc,wo.ActionTaken,wo.ActualHours, wo.TotalCost ,max(`[`tr.date`](https://tr.date)`) as 'Last Labor Hour', tr.TransDescription as 'Employee'` &amp;#x200B; `from mdworkorder as wo` &amp;#x200B; `left join Mdtransactions as tr` &amp;#x200B; `on wo.workordernumber = tr.WorkOrderNumber` `and tr.type = 'labor'` &amp;#x200B; `where` &amp;#x200B; `wo.Status in ('new request','work in progress')` &amp;#x200B; `AND` &amp;#x200B; `wo.Purpose in ('General Maintenance')` &amp;#x200B; `AND` &amp;#x200B; `wo.Priority in ('medium')` &amp;#x200B; `AND` &amp;#x200B; `wo.requestdate &lt;= dateadd(d,-30, getdate())` &amp;#x200B; &amp;#x200B; &amp;#x200B; `group by wo.locationname,wo.craft,wo.WorkOrderNumber ,wo.RequestDate, wo.RequestDesc,wo.ActionTaken,wo.ActualHours, wo.TotalCost, tr.TransDescription`
Your actual table structure has PII in it? 
That's great!
Don't know about the software but something that isn't obvious about CTE's is the fact they need to be the first thing in a statement. If there's anything before them in a batch you need to use a semicolon ( ; ) to explicitly terminate the last statement before you start writing the CTE. For example this won't run CREATE TABLE #test (a int) WITH a as (SELECT * FROM #Test) SELECT * FROM a But this will CREATE TABLE #test (a int); WITH a as (SELECT * FROM #Test) SELECT * FROM a So try sticking one before the WITH and see if that helps. 
it's a logicl loop, but for sql (ex. For I = 1 to 100; print i; next) for example for every record in "select * from employee where lastname = 'smith' do this update employee set lastame to lastname+firstname Next record they are frowned upon because they are inefficient and you gotta lock a lot of recs. most things you want to do with a cursor you can do with a well worded query. 
the wildcard at the front of the ILIKE search string means any index on the name column will be ignored
Not necessarily ignored. The engine may still choose to scan a narrow index and then do a key/rid lookup because it would involve fewer page reads than scanning the table itself, but an index seek would be impossible. &amp;#x200B; Sorry...picking nits. :-)
Another reason cursors are frowned on is they are resource intensive. By default, cursors can be traversed forwards and back, updated, may be global (depending on database settings), etc. This is overkill when they are usually just used to make a one way trip through a set of data and perform some actions for each one. &amp;#x200B; A lot of the time loops are used in SQL because people can't wrap their heads around how to do something using set based logic. But when you really gotta do it, use the LOCAL FAST\_FORWARD arguments when declaring the cursor to make it use as little resources as possible.
 select companies.name as company_name, shipyards.* from shipyards join companies on companies.id = shipyards.company_id where shipyards.name ilike '%lisnave%'; Your tables have less than 100 rows each.... there is no need to use an index because the entire tables can fit into memory. A single disk IO will return the entire table. ------------------------------------------------------------------ | Hash Join (cost=3.19..490.18 rows=66 width=764) (actual time=0.283..16.764 rows=53 loops=1) | Hash Cond: (shipyards.company_id = companies.id) | -&gt; Seq Scan on shipyards (cost=0.00..486.80 rows=66 width=735) (actual time=0.221..16.547 rows=53 loops=1) | Filter: ((name)::text ~~* '%lisnave%'::text) | Rows Removed by Filter: 11691 | -&gt; Hash (cost=2.53..2.53 rows=53 width=45) (actual time=0.054..0.054 rows=53 loops=1) | Buckets: 1024 Batches: 1 Memory Usage: 13kB | -&gt; Seq Scan on companies (cost=0.00..2.53 rows=53 width=45) (actual time=0.007..0.036 rows=53 loops=1) | Planning time: 0.581 ms | Execution time: 16.867 ms | 
Simply put, cursors are a [control structure](https://en.wikiversity.org/wiki/Control_structures) for database records processing. To sum up the article I linked, a control structure is something that will accept input and based on input given change the flow of execution based on the input. Databases have engines within them that attempt to optimize for a particular target, one target may be to optimize on CPU load, another might be to optimize on memory usage, and so on. Whatever, the target, the optimization engine will attempt to execute instructions within the server to meet that target. To optimize, a method that may be employed is to act upon data within the database in groups rather than rows. For example, let's say you have data that is a **customer ID**, **Date of payment**, and **Amount Paid**. Now you might want to get a count of customers by month they paid. The server might be asked to index the data by *MONTH(PaidDate)*. What the server will do is keep a logical arrangement (logical meaning that the records are not actually physically written to disk in that manner) of the records by the indicated index. That arrangement once it has been initially calculated, is then written to disk. When you insert/delete/update the indexed value, the arrangement is updated incrementally. When you execute your query to get the count of customers per month, what the server will do instead, is look at the index, and just count the number of entries underneath that particular partition. INDEX + January (size: 328 records) | --&gt; Pointer to record 1467 | --&gt; Pointer to record 3713 | --&gt; Pointer to record 3919 : : + February (size: 293 records) | --&gt; Pointer to record 1472 | --&gt; Pointer to record 2377 : : Thus when you need a count, instead of look at the actual records, the optimization engine will just simply grab the record size for each index, since it needs to keep count of how many records are within each partition. If you just need February data, the database looks at January sees 328 records and then jumps to the spot on the hard driver represented by 328 \* (whatever record size is) bytes from the beginning of the index. If you need SUM of say amount paid for March data, then the server jumps to (328 + 293) \* (Record size) bytes, reads in how many records are in March, and if processor allows it, start a thread that begins summing the first 25% of the records, another thread that begins summing the second 25% of the records, a third thread that begins summing the third 25% of the records, and finally fourth thread that begins summing the last 25% of the records. The server may even decide in a low period that it can spin up even more threads than four or there might be restrictions on your access that limits you to no more than two threads. Point being for summing the server uses a divide and conqueror strategy. That is, if the current optimization target allows it. Okay so all of that said. All of this is the server taking a given target and dictating the flow of execution. This is a server control structure. You don't get a say in it, other than creating indexes, setting optimization targets, providing hints, and so forth. But in the end, the server is in full control of how it internally processes the data. It should also be said, since I'll say something about it, once the results are complete and the server has then final results ready to send to you, a part of the server will marshall the data for transmission. That basically looks at current bandwidth available, packet length, connection properties, and so on. But basically the marshalling process may determine that it's best to send you 39.6 records per packet across the network, the server serializes (marshalles) the data in an optimizes way based on all that stuff I talked about. CURSORs are you basically taking control of all the above to an extent. Again let's say you want the SUM of customers for the first half of the year, but you are going to access that with a cursor. Depending on the type of cursor, I'll get to that in a moment, the server might not do the whole multiple threads upon the whole dataset per month. It might only calculate the first 1/3 of the month January. Why? Because you the person who created the cursor, might get to 1/6/2018 realize, you got the data that you actually needed or requirement suddenly changed and delete the cursor. The server doesn't know what you might do, so it only calculates up to a limit based on the current optimization target. Which if the query is asking the server to conserve memory or that is the default target of the server, it's going to piecemeal the results so as not to take up too much memory. As your cursor gets towards the end of the data (say you are a 100 rows from the end of the current calculated block), the server might start getting the next piece of data ready for you. Additionally, the marshalling process is totally hijacked here. CURSORs usually deal with row by row data access, so the marshalling process just serializes the row your are currently on and sends it to you. Some databases, if you use their database driver in your client, will send you a block of records, say like 20 rows, even though you are still accessing them row by row. However, that's still not completely optimized since only the marshalling process can see the total size of the results. CURSORs have direction, this means that the row that will be sent to you is in some logical order. Typically you'll see forward only cursors, that is, that once you ask for the next record that logically comes next, the row that you were on and are leaving, is forever gone from the cursor's access. Forward only cursors have the advantage that the server can figure out some ahead of time calculations. Once you scroll forward 10 rows, the server might have 50 more rows left in the buffer to send you, and thus it might start calculating 10 more rows to add to the buffer. Cursors can also be scrollable, in that you can traverse forwards and backwards. In this case, the server might buffer 100 rows, when you get to row 50, it might get another 50 ready, but keeps the 50 you just left in the buffer just in case you scroll backwards. This costs more memory which the server might instead calculate the whole thing, write it to disk in a temporary table, and **then** load the pieces you need into memory, which means you eat up disk space. So if cursors negatively impact the server's performance, why have them? That's because they can be used to implement complex logic. For example, a server may contain metadata that is actionable, for example you might want to query what indexes you have on your server, compare that to a list of indexes your team has indicated could use a rebuild, and then rebuild said index. That would be an example where a cursor would come into play since the action of rebuilding the index would be sufficiently complex enough to warrant it. Typically, you want to avoid cursors since they can break any kind of optimization that server might be able to do on its own. However, should you have a task that is too complex to have in a simple query, cursors would be one option to use. Hope that helps.
So there is no way to speed this up? :) 
At a minimum you need an index on **companies.id** and on **shipyards.company_id**. If you want to speed up the LIKE that has a leading wildcard, you can look into creating a "trigram index" on that column. Or if this specific query is used a lot, you can create a function-based index on that specific search term (it will be very fast, but such a specific index will not be useful to any other query). Can't even see your query the way you have it formatted. Here it is for everyone else: select companies.name as company_name, shipyards.* from shipyards join companies on companies.id = shipyards.company_id where shipyards.name ilike '%lisnave%'; 
Imagine a program that opened a text file but instead of reading it all at once, it read one line, and outputted that. And then it read the next line. That's a cursor A database cursor is the same idea. It opens the table, reads a row, outputs the row, then reads the next row
You can create a GIN index on 'name'. Look for pg_trgm extension
I would try a correlated sub-query, assuming the date column is a date/time and there will be 0 chance of an update happening at the exact same time on the exact same WorkOrder: select tr.WorkOrderNumber, tr.date, tr.TransDescription from MDTransactions tr where tr.Date = (select max(date) as 'Date' from MDTransactions tm where tm.WorkOrderNumber = tr.WorkOrderNumber ); order by tr.WorkOrderNumber
Can you tell me what that function will do?
 select tr.WorkOrderNumber, tr.date, tr.TransDescription from MDTransactions tr where (tr.WorkOrderNumber, tr.date) IN (select WorkOrderNumber, max(date) from MDTransactions group by WorkOrderNumber);
I am still getting the same work order # multiple times. &amp;#x200B; Just would like 1 line per work order # with the date of the latest transaction, and who put it in. &amp;#x200B;
I understand that you asked to do a function based index, but I was interested what result of the function you will be using to index. The table has a column `name`, and I am picking rows with substring match. How would you put into a discrete value to a separate column. Would you explain please that? 
Use a case statement in the index. When the column is ILIKE '%your_string%' then 1, else 0. Then change your where clause to say WHERE (same case statement...) = 1 But again, this is a round about way of doing something when you might consider a redesign like adding a new column.
You ask a question in the first sentence saying it should delete it, and deleting from B is easy enough, but not C exactly what SQL have your written for this so far? personally: i'd use foreign keys with cascade delete. 
This is how I'm deleting from the first two, not in the same query `DELETE from A WHERE aID = $1` `DELETE from B WHERE aID = $1`
Write it in powershell and throw it in a .net app would be my suggestion.
delete C from C inner join B on C.Bid = B.Bid where B.aId = $1 
I don't think inner join works with postgres on delete statements, forgot to mention I'm using postgres
Yeah, you left that part out. lol the where clause would just need the join rule instead. 
Can you use the output from the delete virtual table to load a few temporary table and then use it for your subsequent deletes? I don't know much about Postgres.
Foreign keys with cascade delete.
outsystems.com for their wonderful (/s) Sql generation. 
Relational refers to [Relational Algebra](https://en.wikipedia.org/wiki/Relational_algebra). A single table can be used relationally. It can be self-referential.
**Relational algebra** Relational algebra, first created by Edgar F. Codd while at IBM, is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/SQL/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Closest thing would be Great Plains (now called Microsoft Dynamics GP). 
It depends how you define relational when you say it's a relational database. 
It depends how you define relational when you say it's a relational database. 
Yes, is called a ternary relationship. For example think of an Employee table. Employer1 can be a manager of Employee2 and Employee3. Therefore you can have a one to many relationship with a single table. Hope that helped.
Yes, is called a ternary relationship. For example think of an Employee table. Employer1 can be a manager of Employee2 and Employee3. Therefore you can have a one to many relationship with a single table ;) Hope that helped.
Yes, is called a ternary relationship. For example think of an Employee table. Employer1 can be a manager of Employee2 and Employee3. Therefore you can have a one to many relationship with a single table ;) Hope that helped.
Yes, is called a ternary relationship. For example think of an Employee table. Employer1 can be a manager of Employee2 and Employee3. Therefore you can have a one to many relationship with a single table ;) Hope that helped.
Yes, is called a ternary relationship. For example think of an Employee table. Employer1 can be a manager of Employee2 and Employee3. Therefore you can have a one to many relationship with a single table
There is no need for a cursor at all to accomplish the code generation task you have there. 
What is the current statement?
A single table is a relation. https://en.m.wikipedia.org/wiki/Relation_(database) 
 DELETE FROM C USING B WHERE C.Bid = B.Bid AND B.aID = $1; Tip for future problems, postgres's documentation is pretty good: https://www.postgresql.org/docs/9.0/static/sql-delete.html
I can't enter your site with ad blocker enabled, and it's full of spam. as I mentioned here: https://www.reddit.com/r/bigdata/comments/9e19w0/the_best_big_data_interview_questions_and_answers/e5lqrtj/ I'm reporting you here as well, in the hopes you'll stop spamming.
If you don’t want to alter the existing structure, then create a temp table of the results to delete from A, then get the child values and store all of these in a temp table. Delete from child up where exists in the temp table!
You're asking the question kind of backwards. SSMS is a client for MS SQL Server only, so what you're *really* asking for is a QuickBooks alternative that uses MSSQL Server as the back-end.
You can use SSIS and Quickbooks with a CDATA connector. https://www.cdata.com/drivers/quickbooks/odbc/
&gt; Because relational refers to essentially a tabular structure (&amp; the relationship between columns and rows), as opposed to the idea of how the tables relate to one another Nope. Relational Databases are "relational" because data in the tables can be related to each other. 
I'm on my phone but look at this https://stackoverflow.com/questions/3913620/get-all-table-names-of-a-particular-database-by-sql-query to get the names then look up dynamic SQL and curors. You should be able to loop through all tables and move data into one table with along with inserting the original table name.
Thanks, reading it now!
There's no way to use a variable in a from clause, but you could use dynamic sql to create the statement and then execute it. Something like grab the list of tables from sys.tables and then loop through the list. Each loop can store the table name in a variable, create a statement (I'm assuming you want to merge these tables into a single table, so the statement could be an insert into), and then execute the statement.
You have a normal btree index on the name field. Several of us have already told you that if you have a leading wildcard like that, the index will not be used. The trigram index I suggested above should work.
When you have really_long_table_names_that_just_go_on, aliases will start to look nicer and nicer.
So you are grouping where transaction name = 'Installment' and TransactionID = 1? select date, name, sum(thing) from table where name = 'Installment' and nameid = 1 group by id
Insufficient data for a meaningful answer. You're missing a table or a column here. You need something that groups Installment and Installment Tax into one, like the same Id that's assigned to both. That would be the Id to group by. Without that the only way you can do it is by making assumptions about Installment and Installment Tax having sequential Transaction Ids. It can be done but I doubt that's the answer you're looking for.
I actually removed selecting transaction ID, transaction name and transaction name id and did a group by date.. doesn’t solve the issue though 
Sadly that column you are referring to does not exist in the database, trust me, I looked. 
Yep and there’s always self joins
I'm not grouping by date... you want to group by ID but you only want the Date, but you only want to group where the transaction is an installment. That's where you are getting messed up.
In addition to shortening references to tables with long names, there are cases where you need an alias. For example, when you write a correlated subquery: SELECT * FROM Employee E WHERE NOT EXISTS ( SELECT 1 FROM Employee WHERE ManagerEmployeeID = E.EmployeeID ) This query will give you all employees who don't manage another employee. The E alias is required so the inner query knows to look at EmployeeID on the _outer_ instance of the Employee table.
Ahh I see what you’re saying. That code works in MYSQL but in T-SQL you are required to group everything you are not summing. 
I think you didn't check the code or overlooked it. I used `shipyards.name ilike 'lisnave%';` **not** `shipyards.name ilike '%lisnave%';`. [link](https://gist.github.com/aruprakshit/eafdb3e64106ca01d2e2163952f96822#file-index-txt-L50)
I wasn't trying to give you working code, I was trying to explain to you what you need to do in order to get it to work for yourself :)
Unfortunately, grouping by ID won’t SUM it though. 
Can you give me an example? I can do this in a sub query, but then I run into the date problem...
MSSQL?
Let's make it simple. You want to sum all the transactions. select transactionid, sum(transactionamount) from table where transactionname = 'installment' and transactionnameid = 1 group by transactionid That will give you the correct... I see your problem. OK, so...this is going to be hacky. select x.transactionid, tsum+transactionamount as tgsum, x.transactiondate from ( select transactionid, transactiondate, sum(transactionamount) as tsum from table where transactionname = 'installment' and transactionnameid = 1 group by transactionid, transactiondate ) x left join table y on y.transactionid = x.transactionid + 1 and x.transactionname = 'Installment Tax' and TransactionID = 2 Try that.
The problem is your ID's kind of suck and your data is denormalized. You should have an ID that correlates the transactionName ID. From a data perspective there isn't anything that is stating that transaction ID 12 and 15 aren't paired together. It could be the cost of the installment for transaction 12 but 15 doesn't relate to 12 any more or less than than transaction 13. Do you have an ID field is the same between 12/13, and 14/15 that is not the same between the two pairs? Like a "SaleID" or something?
It's a little difficult to follow what you want to do from your description. I think it could be done with an inner query. E.g. Select T.transactionName ,T.TransactionDate ,(Select sum(t2.transactionamount) from transaction T2 where t2.date = t.date) as TransactionAmount From transaction t Where t.TransactionName = 'Installment'
Yeah, I actually saw that, but your original post had it the other way. Wasn't sure if this was just a typo on your part, because **this changes everything**, I hope you realize. A normal LIKE query will be able to use an index just fine without a leading wildcard. But you are seeing issues because using **ILIKE** (case insensitive LIKE) instead of **LIKE**. That's because ILIKE is like doing UPPER or LOWER on the column before doing the LIKE. Now that you have your index, try running your query using LIKE instead of ILIKE to prove what I am saying, and tell me what kind of run time difference you see. If you need to do a case-insensitive search, then make a function-based index on LOWER(shipyards.name) and then change your where clause to where LOWER(shipyards.name) LIKE 'lisnave%'; 
I’m going to give that a try.. looks good though 
I followed you, hope I have not done any mistakes. :) # Index ```sql CREATE INDEX shipyards_lower_name_idx ON public.shipyards USING btree (lower((name)::text)); explain analyze select companies.name as company_name, shipyards.* from shipyards join companies on companies.id = shipyards.company_id where lower(shipyards.name) ilike 'lisnave%'; ``` And explain plan is below. Still it is doing seqential scan. Before running the plan, I ran the query 10 times. ``` QUERY PLAN | --------------------------------------------------------------------------------------------------------------| Nested Loop (cost=0.00..519.35 rows=1 width=764) (actual time=0.302..21.847 rows=53 loops=1) | Join Filter: (shipyards.company_id = companies.id) | Rows Removed by Join Filter: 1376 | -&gt; Seq Scan on shipyards (cost=0.00..516.16 rows=1 width=735) (actual time=0.289..20.748 rows=53 loops=1) | Filter: (lower((name)::text) ~~* 'lisnave%'::text) | Rows Removed by Filter: 11691 | -&gt; Seq Scan on companies (cost=0.00..2.53 rows=53 width=45) (actual time=0.002..0.005 rows=27 loops=53) | Planning time: 0.313 ms | Execution time: 21.909 ms | ```
Question though - where are your Group by statements? 
I’ve never tried to do a join like that. 1 and 2 are the IDs for the transaction name only. And yes it’s always those values. Would you create a temp table to do this? 
dude, format that code when you post here. It's hard to read when it's all on one line that isn't even all visible on the screen. No, this does not follow my suggestion 100%. Issues: - why are you casting with ::text ? That should not be necessary. Just use lower(name) - your where clause is still using ilike. The whole point was to use LIKE with a function based index. It should be: where lower(shipyards.name) like 'lisnave%';
Something hacky that comes to mind is creating a new field for grouping that looks like "group: 1 1 2 2 3 3 4 4" and then group by that field. 
If this is going to be a one time script, the quickest simplest way is usually to do something like this: Select 'insert into centraltable (name, column1, column2) select ''' + tablename + ''', column1, column2 from ' + tablename' from sys.tables Something like that just returns you back a set of 9000 insert statements one to pull from each table. Copy/paste those into a new window, run them, and your main table is filled. It's ugly, but it doesn't involve learning to manage cursors or any loops, and it's easy to check your work since the insert statements are sitting right in front of you. Obviously this is only ever an ok idea for things that are one time quick hacks, but when you're just trying to move some stuff around, or tell data in a test environment, using a select to return constructed statements that you then run separately is super useful.
&gt; CREATE INDEX shipyards_lower_name_idx ON public.shipyards USING btree (lower((name)::text)); It is added by postgresql, not sure why. I am not writing it like that using `lower((name)::text)`. I just did `lower(name)`. Not sure why the code formatting is worse for you. Now, I am using the online links. http://dpaste.com/317HJ1X https://explain.depesz.com/s/yMK 
Yes I did run ``` docking_dev=# ANALYZE shipyards; ANALYZE ```
I’m not really speaking for or against it, that’s just the justification. However, I think that failing to consistently reference the table you’re using in one form or another (long or short hand) is a terrible practice due to the ambiguous fields/columns and generally harder-to-maintain code. 
Does that mean you're not really trying to sum over multiple transactions, just find the total of each transaction plus its associated tax? For that, something like this might work Select t.TransactionDate, t.TransactionName, t.TransactionAmount + coalesce(x.transactionAmount,0) as TotalAmt from Transaction T left join (select TransactionID, TransactionAmount from Transaction Where TransactionNameID = 2) x on t.TransactionID + 1 = x.TransactionID Where t.TransactionNameID = 1 That won't work for any case where the tax transaction is not the base transaction plus one, but I think it's about all you can do if there really isn't anything else that links the two. Once you get the answer you're looking for, slap whoever came up with that model.
The queries might run faster. Someone once gave me a query that was spinning. I replaced the table names with aliases and it ran fine. Can someone confirm this? I’m not 100% sure.
Grouping by that column would group all the ‘Transaction’ rows together, right? That’s not what op wants. Check their table at the bottom. They want each transaction+tax.
Couldn’t you group by transaction date and the case statement? That would create the requested table
This should work but it's not best practices to be assuming the previous record is always assumed to be associated. SELECT TransactionID, TransactionDate, 'InstallMent' AS TransactionName, SUM(TransactionAmount) AS TransactionAmount FROM ( SELECT MAX(CASE WHEN t.TransactionName = 'Installment' THEN t.TransactionID WHEN t.TransactionName = 'Installment Tax' THEN t.TransactionID-1 ELSE NULL END) as TransactionID, t.TransactionAmount, t.TransactionDate, MAX(t.TransactionNameID) AS TransactionItems FROM Transaction t ) GROUP BY TransactionID, TransactionDate
I don’t think it would. You’d end up with the total per day for installment type transactions and tax separately. Each transaction should have its tax added, the. That total has its own row.
This isn't really about a variable name (or column name), though. In SQL it is helpful to fully qualify columns, so you know which table each came from. However, your nice long table name then gets repeated many times in the single query, which bloats it. Aliasing the *table* helps this, and you still have your full column names. You also still have the full table name, in the FROM clause. Also, having the original table name is often less clear than an alias. Consider a join of a table like NewPurchaseOrder joined to NewPurchaseOrderHeader and NewPurchaseOrderFooter. Using just po, h, and f actually makes it easier to distinguish them within the query.
No, I've never heard of this being the case
&gt;SELECT \* FROM Sales WHERE \[Annual Sales\] &gt; 25000 Thank you so much! This is exactly what I needed,
I know you are but what am I?
Example: A developer is using some CRUD tools and has the tablename hardcoded in, and you have to move it to a new database on the same server. 
update C set fieldname = 'text' from Table1 C inner join Table2 D on C.id = D.id 
Because we're super lazy, so it's less keystrokes. 
Among other reasons, it would be amazingly painful to write multi-table joins otherwise. If you querying specific columns from a multitude of tables you are going to have a very bad time if you are typing out the full table names each time. SELECT tableName1.column1, tableName1.column2, tableName2.column3, etc FROM tableName1 LEFT [OUTER] JOIN tableName2 ON tableName1.column = tableName2.column That ends up requiring a lot more typing than if you just aliased each table to T1, T2, etc. Particularly once you get into 3+ table joins. 
Generally, it's not *necessary*, but a major convenience. It's a lot easier and readable to write SELECT , a.EmployeeID , a.EmployeeName , b.EmployeeSalary , b.EmployeeBonus FROM Employees a LEFT JOIN Salaries b ON a.EmployeeID = b.EmployeeID rather than SELECT , EmployeeGeneralInformation.EmployeeID , EmployeeGeneralInformation.EmployeeName , EmployeeSalaryInformation.EmployeeSalary , EmployeeSalaryInformation.EmployeeBonus FROM Employees LEFT JOIN Salaries ON EmployeeGeneralInformation.EmployeeID = EmployeeSalaryInformation.EmployeeID &amp;#x200B;
The way you clarify the question confused me, and makes me think you may not be understanding aliases. I use an alias for every table in every multiple table query I ever write. I regularly join several tables on a common PersonID field. It's a lot easier to join on "p.personID = hm.personID" than it is to type out "person.personID = householdmember.personID". aliases in tsql simply turn a table reference into a short name. Cuts a lot of junk out of my queries. It's required for some subquerying and aggregate functions too, but that's beside the point.
What have you got so far? 
Not much. I am terrible at writing SQL. SELECT sales.salespersonfirstname, sales.salespersonlastname, sales.annualsales salesregion2.region, salesregion2.regionsales FROM sales region 2 INNER JOIN sales This is me trying to figure out how to grab from 2 tables. I feel like I am very off, plus I need get the "west" sales region, descending annual sales etc. &amp;#x200B;
Ok, almost there. Do you know the structure of the two tables? I.e. the columns in the tables?
Select date, transactiontype, sum(transactionamount) as transaction_amt Where transaction = 'desired transaction type' Group by date, transactiontype
&gt; Would you create a temp table to do this? I think you're over complicating this. Just simply use that as the join predicate. 
Never tried it, but I doubt any GUI client is going to be very good at opening 1gb+ files. What exactly are you doing with these big files? If you're just restoring large dumps, then that would be better done on the command line. If it's not just restoring backups, is there a reason why the files are so big for day-to-day work, rather than being split into multiple files?
What your GUI program probably struggles with is opening and parsing that 1GB+ file. If you're working from the command line, a command line tool can simply take the file as an argument and read it chunk by chunk as needed, so it doesn't need to buffer up the entire file, like your GUI program is (probably) doing.
Are you loading and running the file into the editor or are you running it from Heidi? Does one of your tables have a huge record set? It might be crashing because you're trying to insert records into a table that can't have any more data in it. We've run into this issue at work with tables, we had to open the file in an editor and alter the maximum row size for the table prior to the file inserting the records into it.
Did you figure this out? You need to join the two tables on the salesperson ID, filter to the West region using a WHERE clause, and sort with an ORDER BY clause. 
As for the existing table, it's also 1+ GB. But, it doesn't appear to have trouble holding more data. In your first question, I'm not sure what you mean. Historically, I've uploaded files to the server from my computer, then used a command line to import the file to the database. (I was looking at Heidi as an alternative to using the command line, but--if it's going to get stuck on larger files--I'll just stick with command line.) &amp;#x200B;
There is nothing about the SERIALIZABLE isolation level that states you have to take a pessimistic exclusive lock on data. You are conflating concurrency control implementation with isolation semantics. This blog article is a summary of a really good paper from 1992 that explains these concepts: https://blog.acolyer.org/2016/02/24/a-critique-of-ansi-sql-isolation-levels/
I think it would depend on how the GUI client is written, but you could probably test it with smaller, 20-50MB test cases. Just some rudimentary timing will show if there'll be a speed increase. 
Hey sorry, I misunderstood what you were asking. The company I work for sometimes has to restore backups files that're mysql dump files and one of the two issue we usually run into is either the file we're restoring, whether it be using heidi or command line require the dump file be edited manually and increase the maximum row size of the table prior to the inserts being run. The other issue, max packet size comes up often. For the first question, you can load the files into the editor and then run them or instead of loading them into the editor you just run them. From the file menu there's the options of Load SQL file which will load it into the editor or Run SQL file which will just run it instead of loading it in the editor.
&gt;Thanks for the info, that helped a lot. I'm close, but still getting an error. Right now I have SELECT \[salesperson first name\], \[salesperson last name\], \[annual sales\] FROM \[sales\] &amp;#x200B; UNION &amp;#x200B; SELECT \[region\], \[region sales\] FROM \[sales regions 2\] &amp;#x200B; WHERE \[Sales region\]=West &amp;#x200B; ORDER BY \[Annual sales\] DESC; &amp;#x200B; The error I'm getting is the union doesn't match
You're getting the duplicate columns because you're asking for them. `select *` is saying "give me every column from every table in this query."
Is there a nice way to get all columns from the joined table?
Try WHERE [sales region]= 'West' Non-numeric qualifications normally require single quotes around them like above. If that doesn't work, it could be the spacing issue as you seem to be inconsistent with where the spaces are. Try the below where I aliased the table names. if that doesn't work, post a screenshot of the table layouts with exact column names. ​ SELECT s.salespersonfirstname, s.salespersonlastname, s.annualsales, sr.region, sr.regionsales FROM sales s INNER JOIN [sales regions 2] sr ON s.salespersonID=sr.salespersonID WHERE sr.[Sales region]='West' ORDER BY s.Annualsales DESC;
I want a table that has one key column along with the customer names and addresses without having to explicitly select all the columns by name. Is there a god way to do that? I think: SELECT table2.* FROM table1 LEFT JOIN table2 ON table1.customerID = table2.customerNUM would just give me all the columns from table 2, without the benefit of the Join?
&gt;When I compare the fields raw, it does not calculate accurately. If the timestamps are stored in the format you described then doing a string comparison should work. Can you give an example of where that's not working?
PIVOT then AVG
For the SUM("Sumcalculatorfield") section, you need to use DECIMAL instead of NUMERIC as NUMERIC rounds according to [https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-2017) &amp;#x200B; In the nonnullcalculator section you can utilize the ISNUMERIC() function as such: &amp;#x200B; ISNUMERIC( F1 ) + ISNUMERIC( F2 ) + ... ISNUMERIC( F6 ) + ISNUMERIC( F7 ) &amp;#x200B; ISNUMERIC() Returns 1 if the value is numeric (0 is indeed numeric), and 1 if the value is not a number or null. &amp;#x200B; ISNUMERIC() Documentation: [https://docs.microsoft.com/en-us/sql/t-sql/functions/isnumeric-transact-sql?view=sql-server-2017](https://docs.microsoft.com/en-us/sql/t-sql/functions/isnumeric-transact-sql?view=sql-server-2017) &amp;#x200B;
SELECT qkey, avg(Val) (SELECT * FROM (…) q UNPIVOT (Val FOR Field IN (Field1,Field2,…,Field9) )a GROUP BY qkey
Others are helping you out so ill just give this www.w3schools.com it helps
The standard practice is in fact to list and rename all columns, manually if you will. So you do `select ... from table1 as t1 left join table2 as t2 on ....` (where you can omit the table aliases when you're fine with the original names and don't do a self-join, when they become obligatory), and in the field list you then do `select t1.customerID as t1_customerID, t2.customerID as t2_customerID ...` and so on. Of course you'd normally omit the duplicate fields but you get the idea.
What does this do?
so the /vagrant/VagrantFiles/DB/ is just the path where the files will go. It'll then search for the latest file and set it as the filename last\_dump &amp;#x200B; t So just use the variable last\_dump or whatever you call it and use that line. It should work. 
Batch script? That's what I'm trying to do.
Take the max timestamp record for each order? SELECT a2.siteid ,customer ,a2.city ,a2.state ,a2.vendorname ,SUM(CASE WHEN originalstatus='Not Done' THEN 1 ELSE 0 END) as notdonetotal ,SUM(CASE WHEN interstatus='Miss' THEN 1 ELSE 0 END) as MissedTotals FROM a1 INNER JOIN a2 ON a2.order=a1.order AND (rec.servicetime) &gt;= (CURRENT_DATE - interval '45 day') INNER JOIN (select order, max(enterstaystatustime) as "statustime" from a1) max_entry ON a1.order = max_entry.order AND a1.enterstaystatustime = max_entry.statustime GROUP BY a2.siteid,customer,a2.city, a2.state, a2.vendorname
Thanks that has been my main hub so far, but it might help others to have it here. great resource.
The more I looked at this and could not find a wildcard shortcut the more I thought this was probably the case. This makes perfect sense thanks for clearing that up. Is it usually the case where there are not too many columns and this is just fine or do you ever find yourself needed to manually type out tons of columns? Like for a migration or something? I am very new and have no concept of how many columns tables can actually end up having. I'm just curious. thanks
Just tried this; it displays "7" as the result by default, meaning that if only one number is entered into one of the fields, it is treating the calculation as 1/7 instead of 1/1
&gt; currently joining two tables that each have fields with time in HH:MM format This isn't describing how that time is actually stored in the two tables. We need to know the column data type and if the column datatype is different between the two tables. Since comparing the fields raw isn't working, my suspicion is that the two tables have the column defined as a different data type. If that's the case you need to convert one or both of them to be the same data type.
Returns the following error: &amp;#x200B; There is an error in the formula. Please rectify it. Error 5 in OpenRS; Incorrect syntax near '...'. Records could not be fetched. \- select avg(Val) (SELECT \* FROM (...) q UNPIVOT (Val FOR Field IN (1,1,1,1,1,1,1) )a
well FWIW the PostgreSQL column limit is at 1600, but I guess you're not going to find many cases with that many columns. And yes, writing SQL means writing a lot of stuff, it's not a very terse language, but that's why we all love it. Get yourself a decent text editor like Sublime Text or similar that has multiple insertion points, syntax coloring and auto completion, and then always write your SQL neatly formatted with all column names lining up vertically; these little things go a long way to make writing SQL more enjoyable and less of an typewriter exercise.
exactly, the most important reason is when you can't achieve what you need w/o an alias. another case where you need aliases is in certain kinds of joins: select A.F, B.F from MyTable A, MyTable B where A.JoinField1 = B.JoinField2 think of each alias to the same table as of an independent **view** on the same recordset.
Your getting downvoted but you are correct. Just cause there are many ways to skin a cat does not mean you need to use a polearm.
&gt;I make a joint based on surname What happens when someone's name changes? Or you have multiple people with the same name?
Thanks a bunch!!
True, the surname was meant as an example. The joints are based on the full names (first and last name) as well as the birthday 
That's not a good key either. It's very possible for two people to have the same name and birthdate.
What is your JDBC driver version?
My what lol
Hi r/wolf2600, I've tried and it keeps on stating that relation a1 does not exist? 
Commenting because I'm also interested. 
Same 
We have a separate query which checks on duplicates, to ensure mentioned scenario does not re-occur. To specify, the database isn’t live and gets updated on a yearly basis.
How could a1 not exist? You said your two tables are named a1 and a2.
Correct, I have two tables labeled A1 and A2 (as table aliases). I've double checked to see if there were typo/errors but the error is still occurring. Here is the error message. Reason: SQL Error \[500310\] \[42P01\]: \[Amazon\](500310) Invalid operation: relation "a1" does not exist;
Is this MS SQL? I know it's kind of cliche at this point, but have you looked into CTEs? 
 INNER JOIN (select order, max(enterstaystatustime) as "statustime" from a1 group by order) max_entry ON a1.order = max_entry.order AND a1.enterstaystatustime = max_entry.statustime group by?
Yup, I've included the group by as well. It's strange?
I downloaded it just 2 weeks ago so that Mac version Queries running in different SQL editors
Sorry, I've tried the group by order but now this error is occuring SQL Error \[500310\] \[42601\]: \[Amazon\](500310) Invalid operation: syntax error at or near "order" Position: 479; &amp;#x200B; Reason: SQL Error \[500310\] \[42P01\]: \[Amazon\](500310) Invalid operation: relation "a1" does not exist; 
Yep, was doing that on dbeaver before actually. Running an update query then the query on the pg activity to check how long the update was taking, now the pg activity just goes to infinity and doesn't get registered when I look from pgadmin
I think this is a great beginners / fundamentals book. It will give you an in depth focus on the basics: &amp;#x200B; [https://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145/ref=sr\_1\_3?ie=UTF8&amp;qid=1536613857&amp;sr=8-3&amp;keywords=tsql+fundamentals](https://www.amazon.com/Microsoft-Server-Fundamentals-Developer-Reference/dp/0735658145/ref=sr_1_3?ie=UTF8&amp;qid=1536613857&amp;sr=8-3&amp;keywords=tsql+fundamentals)
How would I leverage CTE's in this case? (Sorry - I've not done something like this before, and I think I need to go to bed!)
Hi, My reasoning is that Table A2 contains the siteID, order, city and state where as A1 only contains the order and statuses. &amp;#x200B;
Ah I see. Yes, you are correct as I only want to count 1 instance per orderid. However, now I'm running on the same issue where the same error message is occurring. Tables A1 and A2 do exist as a table alias. Error message below. SQL Error \[500310\] \[42P01\]: \[Amazon\](500310) Invalid operation: relation "a1" does not exist; I really do not know what the issue is. I even tried not typing the full table schema without the Alias but the same issue is occurring. 
Start with something simple like w3schools website and find the SQL section. Some people hate using free websites but it worked for me.
Same. Or online lectures, I'll take what I could get.
Does running explain analyze on you query returns anything or it hangs too?
W3schools is more like looking at flash cards, or just a reference. I think stackoverflow is more helpful for looking up specific solutions. The best thing to do is install SQL server and mgt studio along with adventure works db and start playing around with it. There are some helpful YouTube tutorials too and something like codeacademy isn't bad either imo
Something you might consider is signing up on [the SQL Saturday website](https://www.sqlsaturday.com/default.aspx). These are more oriented around SQL Server, but SQL Saturdays are *free* one-day events where you can take a variety of sessions. On a related note, [sign up on the PASS website](https://www.pass.org/AttendanEvent/OnlineEvents/24HoursofPASS.aspx) and watch for free online events.
How about un-pivoting the former table with 3 unions (i.e. contact\_id, email\_1 from former union all contact\_id, email\_2 from former union all contact\_id, email\_3 from former), and then inner joining to the new table on both columns, and seeing if the number of rows correlate?
I'm going to have to look at it line by line again to see what's going on. I've other queries written from "A1" and had no issues running them.
If you ran an UPDATE, is there by any chance an open transaction? It seems like a lock issue.
https://use-the-index-luke.com/
I started with this book: SQL Queries for Mere Mortals: A Hands-On Guide to Data Manipulation in SQL (4th Edition) https://www.amazon.com/dp/0134858336/ref=cm_sw_r_cp_api_UIWLBbJZF1KBH It’s very through but a bit slow at the start. I go back and reference it from time to time still.
I think you're talking about Products? If so, order_items points to the products table orders -&lt; order_items &gt;- products You should use SQL Table Inheritance to allow products with different attributes 
I added the 2 tables to my original post! I was still getting an error.
I don’t think they are super important. Totally my own experience but I feel like I’ve benefited more by reading some of the Edward Tufte and Stephen Few books. I think it depends though. Do you wanna stay in BI or get more into data science?
 I think this will work for you. It's about as clean as it gets in this situation, or at least that I see without more information. I used CTEs because they're standard SQL. ID should be replaced by whatever your Business/Primary Key for the table is. Ex: Week number or date. IsNumeric returns a 0 or a 1. It shouldn't default to 7, I think you were using it wrong. See below. &amp;#x200B; WITH SUMMED(ID,SummedVal) AS ( SELECT ID ,CAST( CAST(COALESCE(FIELD1,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD2,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD3,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD4,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD5,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD6,0) AS NUMERIC(5,2)) + CAST(COALESCE(FIELD6,0) AS NUMERIC(5,2)) AS NUMERIC(5,2)) FROM WEEKNUMCALC ) ,NumFields (ID, NumVals) AS ( SELECT ID ,ISNUMERIC(Field1) + ISNUMERIC(Field2) + ISNUMERIC(Field3) + ISNUMERIC(Field4) + ISNUMERIC(Field5) + ISNUMERIC(Field6) + ISNUMERIC(Field7) AS NumVals FROM WEEKNUMCALC ) SELECT A.ID ,CAST(SummedVal / COALESCE(NumVals,1) AS NUMERIC(5,2)) AS AveragedAmt ,SummedVal ,NumVals FROM SUMMED A INNER JOIN NUMFIELDS B ON A.ID = B.ID Table: ID Field1 Field2 Field3 Field4 Field5 Field6 Field7 1 1.00 5.20 NULL NULL NULL 10.00 NULL 2 4.30 6.00 5.50 NULL 2.20 10.00 NULL 3 3.00 4.00 8.00 NULL NULL NULL 5.00 &amp;#x200B; Results: ID AveragedAmt SummedVal NumVals 1 8.73 26.20 3 2 7.60 38.00 5 3 3.75 15.00 4
Also, on the other side of the spectrum is Joe Celco's "SQL For Smarties." Great book, especially if you want highly portable code.
That's a good question. In my opinion you can know from memory the formulae for all those metrics, but if you don't know when it's appropriate to use them then it's worthless. Sometimes using percentiles makes much more sense than using a average for example. That's the kind of knowledge a BI analyst should have besides mastering the tools he'll be using everyday, such as SQL or python or tableau or whatever. I wouldn't go with the hardcore math behind all of that, but more with the use cases for all the different metrics and how to calculate them using SQL for example. Hope it helps.
same
Don't have time to write the code for you, but what you're looking for is done by generating a row_number in a temporary table (or using a qualify statement in teradata) partitioned by the workorderid and ordered by the timestamp Descending (the most recent record), you would then take only the records where they have a row number of 1. Obviously you need to make sure your temporary table/cte can be joined to the set if you don't have access to a qualify-like function. If you want an example asap, Google can do that (Google something like 'qualify sql') - otherwise, I can give you one later 
You can try "**SQL Queries for Mere Mortals” By John L. Viescas and Michael J. Hernandez** and you could even go through some wonderful guides and tutorials at StrataScratch online [https://www.stratascratch.com/sql.html](https://www.stratascratch.com/sql.html). This is a platform where you can practice your codes.
* **Beginners**: [Practical SQL Handbook](https://www.amazon.com/Practical-SQL-Handbook-Using-Variants/dp/0201703092) or [SQL Primer](https://www.amazon.com/SQL-Primer-Accelerated-Introduction-Basics/dp/1484235754) (Disclosure: I wrote the latter) * **Advanced**: [The Art of SQL](https://www.amazon.com/Art-SQL-Stephane-Faroult/dp/0596008945) and [SQL Antipatterns](https://www.amazon.com/SQL-Antipatterns-Programming-Pragmatic-Programmers/dp/1934356557) * **Reference**: [Introduction to SQL](https://www.amazon.com/Introduction-SQL-Mastering-Relational-Database/dp/0321305965) or [SQL Cookbook](https://www.amazon.com/SQL-Cookbook-Solutions-Techniques-Developers/dp/0596009763)
Commenting yo find this later, thank you 
Hey dude. This worked perfectly, thanks a lot for the help!
Maybe mention that there's a fee involved.
I used this when I first started - https://www.amazon.co.uk/Sams-Teach-Yourself-SQL-Minutes/dp/0672321289 Pretty simple and great to get the fundamentals!
Your were supposed to replace the ellipsis with your base query, e.g. SELECT * INTO #tmp FROM ( VALUES ('id1',null,null,1,1,1,1,1) ,('id2',1,2,3,4,5,6,7) ,('id3',null,null,NULL,4,5,6,7) ,('id4',0,0,1,1,1,1,1) )a(qKey,Field1,Field2,Field3,Field4,Field5,Field6,Field7) SELECT qkey, avg(Val*1.0), count(*) FROM ( SELECT * FROM #tmp q UNPIVOT ( Val FOR Field IN (Field1,Field2,Field3,Field4,Field5,Field6,Field7) )u )a GROUP BY qKey
I didn't start with books. I actually started with an app on my smartphone called SoloLearn. It had little quizlets, and was very beneficial. That gave me an understanding of the basics. From there, I went to W3Schools and CodeAcademy sites for more practical usage. Then I got involved in data analytics at work, and people introduced more advanced concepts to me like CTE queries, 'begin transaction' statements with 'commit' or 'rollback' along with the ideas behind it (pros and cons). Then I learned the rollup feature, and moved on to pivot tables (hate doing it still). Then I learned about triggers and functions. Eventually, I moved away from Development (not entirely) to Administration. My point of all this is... what do you want to do with this knowledge? It might be best to choose a path first before learning from the books.
First two ideas that come to mind are: Case statement. This one hard codes the translated values into your stored procedure. select [db_name] = [name], [file_name] = 'yyyymmdd_' + case when [name] = 'master' then 'm1' when [name] = 'tempdb' then 't1' when [name] = 'model' then 'm2' when [name] = 'msdb' then 'm3' end + '.csv' from sys.databases Translation table. In this example I've just used a CTE, but the translation table could easily be a permanent table that you edit if a name needs to change or a new database is added. This would be my preferred option. ;with DB_alt_names as ( select * from (values ('master', 'm1'), ('tempdb', 't1'), ('model', 'm2'), ('msdb', 'm3')) as a (old_name, new_name) ) select file_name_orig = 'yyyymmdd_' + DBs.name + '.csv', file_name_new = 'yyyymmdd_' + DB_alt_names.new_name + '.csv' from sys.databases as DBs inner join DB_alt_names on DBs.name = DB_alt_names.old_name &amp;#x200B;
You could try using SUBSTRING(string to search, start position, length)
Something like this should work: DECLARE @sqlCmd varchar(1000) SELECT @sqlCmd = 'IF ''?'' LIKE ''server%'' BEGIN USE ? DBCC SHRINKDATABASE (''?'', 10); END' EXEC sp_MSforeachdb @sqlCmd
PATINDEX will allow you to search a varchar(max) to get the starting point, iirc. Then you can use SUBSTRING to pull the value.
generally it's not a good idea to store totals which are comprised of other data values you sure you need to do this? because you'll have to re-update table 1 totals every time there are changes to table 2
Is this SQL Server? If so, you could use the built-in XML methods, i.e. SELECT CAST('&lt;test&gt;&lt;v&gt;blah&lt;/v&gt;&lt;/test&gt;' AS XML).value('(//v)[1]','varchar(max)') or DECLARE @x XML SET @x = '&lt;test&gt;&lt;v&gt;blah&lt;/v&gt;&lt;/test&gt;' SELECT @x.value('(//v)[1]','varchar(max)')
sub queries and joins, potentially even using optimized views. If you need this to be repeatable you can do it with functions. 
I am self taught and I use SQL regularly for my job. Data analyst/Market intelligence types roles. Type of jobs where knowing sql is a plus but not a necessary and lets you provide higher value compare to other employees. 
short answer, yes. I began my career as a call center operations manager, then moved into an analyst role where i used SQL pretty much to just pull data and then dump it into Excel for processing, aggregation, ad hoc reporting, and presentation. it was an internal lateral move but i got the job because of my business experience as well as my limited knowledge of SQL. previously i spent a summer interning for a software company writing QA SQL scripts. That was years ago, and now I'm a backend software engineer designing databases, automating data integrations, and performance tuning legacy code. I think you're on the right path.
thank you for this answer, just solidifies that im headed in the right direction my end goal is business intelligence but designing databases and automation and AI definitely peals my interest how long have you been at your current position for?
I know very few people who are DBAs/database developers who went to school for it.
Self taught but work as a data analyst not a DBA but I use both SQL and R regularly in my job
I'm completely self taught in SQL. My educational background is in supply chain management. I just stumbled into an analyst role out of college and worked my way up the technical chain. Learned MS Access, then Cognos BI tools, and then eventually mssql. I'm a app dev manager now, but I was a database app dev for a while and a Sr db app dev for a few years. Honestly, I think I lucked out with having a high performance team to bounce ideas off of. I also got my SQL Server 2012 MCSA. I think studying for that was incredibly valuable. 
The way I got in to doing data stuff was that a job came up in a call center I was working at for someone to fill in while this lady went on maternity leave. She was supporting this other dude who was running their employee bonus program - all pretty basic excel admin. Even when I was on the phones I was always trying to get involved in non-phone based stuff, and had done some basic excel tasks for my team. Once I got the job doing the excel stuff, I worked my ass off and about six months later replaced the guy she was supporting. Fast forward eight years and I write SQL queries and use them to make either Tableau dashboards or SSRS reports, and next year I should be hitting a six figure salary. Honestly, I think the trick is getting the first job. After that, while qualifications will help, you're not automatically out of the running for roles. You just have to be able to talk about the things you've done enough that they'll give you the job.
self taught starting 1980 programming, into job, into college, into career into college, now a DBA.
I have a bachelors in computer engineering but I work as a SQL DBA/Dev and .Net Dev. I guess I’m sorta self-taught as I never had to take a class on these in college.
Is A.name and B.name uniquely identifying the same people? What if there are two Bobs
Sorry if I’m misunderstanding, but If the other conditions match the records 100 percent of the time, is the join you are asking for necessary? 
There are many of us in the industry who are self taught. I work as a BI Developer at a major bank. For perspective my background is in advertising. To be fair I had to work my may around this, since I started in a very different position. If you are trying to break through I would suggest doing lots of personal projects and creating a portafolio. 
Self-taught. I have a Master's degree in Econ. Computer science kinda came naturally. Though bit flipping utility is still an odd concept to grasp as a tool. Going back to school for Data Analytics may be useful but I'd say to pick a particular field of interest. The spectrum for data analysis techniques is BROAD. I'd recommend choosing a speciality area early on.
In this example Bob will only be 1 record in both sets, but the ID for bob may not be the same. 
On average what was the slowest day to travel? What was faster, traveling to work or back to work? 
I'd say you're on the right track. Unless you have other reasons to go back to school for a computer science degree or something similar, I think you'll find a lot of university DB classes to be pretty useless. I'm self-taught with about 8 years of experience and have helped a few friends with university SQL classes that were overall pretty terrible. The one friend I know that found the courses to be easy already knew SQL. All the ones I've seen were extremely broad, the professor would focus on some programming vice that was irrelevant to the language (like always capitalizing keywords, he'd mark down points if you didn't I kid you not) or they'd just focus heavily on relational theory without going much into the applications. All the classes tried to cover way too much material, including stuff that is really DB administration like security and backups (and is often vendor-specific).
Nope. Just install a DBMS on your desktop, connect your client to it and start querying.
What sort of advanced topics? https://smile.amazon.com/Joe-Celkos-SQL-Smarties-Programming/dp/0128007613/ ??
Why does ur above code, or just the first 3 conditions plus a.name = b.name not work? Also a.id and b.id being Null is a condition that returns zero rows 
I haven't tried with A and B null, just B IS NULL AND A = B. The process takes quite a bit of time to get to the point where I can test that join condition.
That will always return no results, because if B is NULL then A = B is always false, because NULL is never equal to anything, including other NULLs.
Thank you! A little off topic: Investing time to master SQL is worth it, right? I mean the job prospectus is infinite. And, SQL knowledge can be applied anywhere.
Thanks!
So that's why the double NULL condition is required `OR (A.ID IS NULL AND B.ID IS NULL AND B.Name = A.Name)`. I get it. I kind of assumed that would solve the code but I was having a hard time seeing why.
Self-taught here as well. I had used Excel for pet projects since I was probably 12, progressively getting more skilled at it. Until recently nearly all of my work experience in my adult life had been admin, where data was pretty auxiliary to the primary functions, but I was always kind of "the Excel guy," even if that's not what I was getting paid for. About this time last year I dedicated myself to learning SQL and interfacing SQL Server with Excel and Power BI, among other things. That played a huge part in me landing a data analyst job. I'm already getting paid more and enjoying myself way more than I ever did at any of my previous jobs, and yet there is still so much to learn, and so much room for career advancement. Couldn't be happier.
is DataGrip a good IDE? 
Try where (jobtran.Uf_Resource &gt;= @MachineStarting OR @MachineStarting is null) and (jobtran.Uf_Resource &lt;= @MachineEnding OR @MachineEnding is null) &amp;#x200B;
It can connect to a lot of different dbms's. Razorsql is another. You can also just download express sql server and use ssms for education purposes
&gt;Then moved into an analyst role where i used SQL pretty much to just pull data and then dump it into Excel for processing That's where I've been at for the last few years... I'm trying to make a jump now but it seems very overwhelming. I'll get there someday though! 
Good question, I could've been more specific. Tutorials that cover partitions, joins, aggregate functions, etc.
I don't use an IDE for SQL. Just DBVisualizer to run queries.
I would say, start here: [Are these SQL concepts for beginners, intermediate, or advanced developers?](https://softwareengineering.stackexchange.com/questions/181651/are-these-sql-concepts-for-beginners-intermediate-or-advanced-developers) SQL is a lot more than joins as I'm sure you know. Start with materializing views and writing a trigger to refresh that upon updates to the underlying tables. Make some stored procedures that handle a slowly changing dimension. If you can do that you can be top 20% depending on your geography. 
It says the error is coming from the FROM clause. Even if I try to adjust it though I feel like other errors are appearing
My story is scarily similar to livinglifelazily! Started in customer service on the phone, taught myself Excel, then access, then SQL out of boredom, ended up doing custom reporting for call center, managed a team of people doing Excel and access reporting, moved out of management into a ssrs developer role with IT, now I'm Sr. ETL, but have done full stack over my 10 years now in BI over two different companies. my degree is in nothing close to IT. I was lucky to have some good mentors over my time, but it's completely possible to do self taught. Just keep learning. Happy to help if you have any questions! 
I'm self taught as a Senior Business Systems Analyst. Started as an accounting guy with no SQL skills and somewhat basic Excel skills (could do Pivot tables, basically). We hired a guy to improve our processes, and he essentially used VBA/Excel/SQL to automate a lot of processes. This intrigued the heck out of me, so after he left I started looking at his work and used online tutorials because I wanted to be able to do what he did. After I got decent at SQL, I interviewed for a "Financial Analyst" position, but it was a hybrid role that needed a lot of SQL/data skills. 2 years in and they promoted me to what I am now and shifted my focus to Business Systems from a pure finance focus.
I can't help without more detail, sorry. I think it would help you to revisit some videos on the basics of selects and joins and specifically search for the version of SQL you are using (TSQL, MySQL, etc). Good luck!
I was an analyst for 2-3 years then for the next 8 years I was a BI analyst/developer hybrid support role where I spent a lot of time converting Excel reports into SSRS ones and working on the automation and data-gathering processes to feed those reports using SSIS. I did a LOT of meetings with business groups to explain what we could and could not do for them. I left that company 2 years ago when I got laid off in a headcount exercise. I've been in my current role for about 18 months. I'm essentially doing the same thing but expanded to doing a lot more data-driven tasks. I wouldn't take away from my comment that it'll take you 10+ years to get where you want to be though. I'm sure you could get there within 5. I just got comfortable.
It's hard to tell from the education link, is this free? thanks
Self taught. DBA/Developer/Devops type. Really, when I worked at Microsoft the self-taught, non-certified people were the only ones who tended to even get an interview. 
Here. GED and most of an AA. Self taught, learned to program and started writing automations for my dept. It got around the company and I got hired into a Devops Role. I learned SQL on the job, and one class I took. 
I don't actually know anyone who went to school for a SQL specific or DBA-esque job. 
I think SQL and databases are worth the time, I have a very good career because of it. At the end of the day, it comes down to what you want to do and opportunity. Technology and roles change, nothing is likely future proof. That said, SQL and Databases will be around for a very long time and people will probably always need folks who have that skill set. If you are not in a tech hub though, you may find jobs less easy to come by. Other jobs may really like you to have SQL too as an additional skill, but this all depends greatly on what you want to do. 
Yeah, I'm working on it. I'm finally in a new position where people are not hoarding information as badly, are willing to help, and overall very supportive, so I've already added a ton of knowledge just in the month or so I've been there. Thanks for your response!! 
Data Engineer here. I'm Self-taught. Started as a Data Analyst with SQL. Now I work with Big Data and Azure using SQL and Python mostly, but know and use a bit of Java, JavaScript, C# etc.. You definitely don't need schooling. People commonly start where you're at right now. You're on the right track 👍. 
I just recently landed a BSA role at my company. Waiting for our project to get up-and-running, and still am not so sure what most of what I will be doing entails. I have a similar background with you in terms of having no SQL skills and pretty basic Excel skills, and having barely any experience in the realm of analytics. Could you provide a bit of an overview as to what your role consists of? Obviously this will vary by company, but I am intrigued based on the similarities to my situation. 
My colleagues of a similar age (mid thirties) are almost completely self taught. There just wasn't a study pathway into the BI area at all, so we all have pretty much the same career story: got into an unrelated field, found a need for data that we kludged something together from google searches, business said 'hey wow more of that pls' and the next thing we know we're data analysts and BI developers and that sort of thing. BUT these days there are study pathways (particularly in data science), and formalised entry level positions rather than people falling into it accidentally. I think it's much harder to get your foot in the door today than it was fifteen years ago when the qualifications barely existed. In your case though it sounds like you've already got a foot in the door in a place where you can start building your skills and experience, so I definitely wouldn't recommend going back to school.
Most developers don't know about indexes? Even a little?? Woah... Scary. 
I use it at work. Its very flexible it will connect to just about any database imaginable. Over all I like it. For a beginner it might be too much all at once to learn.
Could this just be simplified by isnull(@machinestarting,ufresources) &gt;= isnull(@machineend,@machinestarting) Im on my phone... Sorry for syntax. I'm not sure if the pseudo code I did is correct but I'm sure you can find a better solution then the nested case statements. You'll be glad you did if you have to troubleshoot this in a few months and have to read through it all... 
Hey! I am currently an ETL developer and I am looking into possibly moving into data engineering. Can I PM you to find out more about the field and your career path? Thanks!
I always have trouble summarizing this, but I'm essentially "jack of all trades, master of none" guy on the Finance team. Not great at Finance, not great at coding. But I'm the probably the only one in the company that could do both. It's a LOT of ad-hoc stuff, but some of the bigger things I've done recently are: * Create standardized excel templates for things like budgets, forecasts, pricing models, etc. for our team to use, which use scripts to download the information into a SQL database. Allows our Excel guys to stay in Excel, but still get the data into a database for easier reporting and manipulation. * Create tableau dashboards for our internal team. I also usually work with IT on the external dashboards that will be to employees and/or customers. Sometimes I feel like the Finance &lt;&gt; IT translator because my boss doesn't understand the Tableau stuff and IT doesn't understand the numbers. * Lots of ad-hoc stuff. Honestly 50% of what I do is ad-hoc stuff. Most of these requests typically involve querying one of our many databases (and we've just acquired 4-5 new companies that all have different data systems), and then merging things from different data sources to be able to summarize. For example, our COO might look at Q2 financials and wonder why company B's labor was 8% higher than budget. This typically means querying Company B's database, querying our budget database, normalizing the data, throwing it into Excel, creating a bunch of random pivots to understand the driver of variance, then making some charts and throwing it into a slide deck so me or my boss can explain it in the next meeting. * A lot of non-systems, finance things. Like I still do things for month-end close that doesn't require any hard software skills. * Answer a lot of random questions. Maybe like 30-40 minutes of my day is just sitting with people on the team and answering various questions about how they can pull a certain piece of data or something
Hmm. What rDBMS are you using? What aren't you saying about your table, do you have any other joins in this query? Because I took a couple of minutes and on my box (SQL Server Express 2016) I just ran this: CREATE TABLE [randy] ( AccountID int NOT NULL ,Region varchar(255) NULL ,Vendor varchar(255) NULL ,PlacementAmt Numeric(7,2) NOT NULL ); CREATE TABLE [bobandy] ( AccountID int NULL ,PayDate date NULL ); INSERT INTO RANDY (AccountID, Region, Vendor, PlacementAmt) VALUES ( 1234567 ,NULL ,'ABS' ,127.37 ); INSERT INTO BOBANDY (AccountID, PayDate) VALUES ( 1234567 ,CAST('2018-08-17' AS DATE) ); INSERT INTO BOBANDY (AccountID, PayDate) VALUES ( NULL ,CAST('2018-08-17' AS DATE) ); INSERT INTO BOBANDY (AccountID, PayDate) VALUES ( 1234567 ,NULL ); SELECT AccountID ,Region ,Vendor ,PlacementAmt ,(SELECT TOP 1 PAYDATE FROM BOBANDY b WHERE a.AccountID = b.AccountID ORDER BY PAYDATE DESC) FROM RANDY a and my resultset was: AccountID Region Vendor PlacementAmt (No column name) 1234567 NULL ABS 127.37 2018-08-17
Is there a particular flavor you are looking at or just in general? Typically each flavor treats and handles things differently, albeit similarly. 
I am a self-taught full stack web app developer. I use the LAMP stack and mostly build wordpress plugins. I've never paid for a course or boot camp. I took one semester of online college towards an IT bachelor's degree included no classes in coding which is what I wanted to do. I quit when I got my current job almost 4 years ago. Learning what I needed to for the job eventually gave me a full set of skills. Now I'm about to finish rewriting the companies Enterprise level insurance application and management software.
Sure thing
Hmm that's what I figured. Thank you, I'll look into the SQL Operations Studio - it should probably suit my basic needs, right?
I'm lazy and just look for it when I need it.
Heidi (and similar) are mostly all going to get pretty bent out of shape at a file that size. Your best performance will be command line. That being said, if I remember correctly, there is a setting for determining whether the server or the client parses the data. May/May not help your performance from Heidi.
Costs nothing to try it out. Keep using it until you hit something you can't do. Worst case, just about everything done in SSMS can be done via T-SQL. You won't have a nice GUI for it but you can still do it.
Learning your database is going to be based around how well the architecture was planned and executed. &amp;#x200B; What you are looking to do, is create an Entity Relation Diagram (ERD). This essentially shows the relationships between the objects in the database. &amp;#x200B; Hints I look for when creating an ERD: * Foreign keys * Constraints * Naming conventions * Indexes * Procedures / Triggers (Trickier, because if you don't have a tool to help illustrate what these touch, you're going to do a lot of painful digging.) There are many tools that can help you create this diagram as /u/ben_it graciously helped us evaluate. [You can see that user's amazing post here.](https://www.reddit.com/r/SQL/comments/8vuor5/76_data_modeling_tools_compared/?ref=share&amp;ref_source=link) Most of these tools use most of those items I've listed to create those diagrams. &amp;#x200B; Apart from the physical hints you can see in the database, the next piece would be evaluating the system tables or any method available to your platform to analyze SQL that is running by the application. Hopefully you have business analysts or existing database folks who can give you a tutorial rundown of the instance and the business logic of the data, but if you don't, capturing the SQL ran in your environment is probably the 2nd best option. &amp;#x200B; Something to keep in mind is that the business logic of a work place may not be very easy or intuitive to learn. Many places understand this thankfully, but it can definitely take time to learn. Don't confuse size with complexity, or even quantity. I've seen smaller compact instances with insane logic and I've seen huge environments that were incredibly easy to follow. &amp;#x200B; Another idea to keep in mind is how well you understand the industry. My first job was based in a call center where I job shadowed almost every single role in the company and was there for about six years. I knew the data, business, clients, and anything you could think of inside and out. If you understand the business, it's going to make understanding the data connections across your objects that much easier. &amp;#x200B; What you need to watch out for when looking at jobs is what architecture was in place and where they are at currently. If business logic is hidden throughout the database and the applications, you may run into troubles figuring out where is what. The same goes with the lack of physical hints as I indicated earlier, if this was not planned and executed well, you may be out of luck and have to resort to knowledge transfer from individuals, sifting through code, and capturing SQL. 
Or those odd situations where you join to another table more than once using different criteria. Aliases makes it easier to tell which join is being used on the column selection.
So I did manage to solve this problem. It turns out that one of my joins was creating some extra records. Unfortunately the fix was far more complicated than it needed to be. The database I'm using is the Intersystems Cache db. Now, had I used a common table expression, I could have literally halved the size of the code, but the Cache database conforms to SQL 92, whereas common table expressions were introduced in SQL 99. Likewise on that particular database, we have zero write access, no tables, no temp tables, zilch, nada, nothing. So everything has to be done in one query.
I have been working on my company's database for 2 years, never wrote anything down. Eventually you will get to know your tables. 
I feel sorry for you sir. No CTEs sucks. :( Good to hear you solved it. I figured after my test there was something else you needed to join to - it means Account wasn't the only PK.
I'm dumber having read this
Put it in git 
Most devs know that indexes are important for performance, but cannot do it correctly. I've done an online quiz about SQL indexing. Turns out 60% of participants fail it. https://use-the-index-luke.com/3-minute-test
[http://www.studybyyourself.com/seminar/sql/course/?lang=eng](http://www.studybyyourself.com/seminar/sql/course/?lang=eng). There you can learn basics for querying a database using SQL. The course, along with examples, is quite easy to follow. You can submit exercises as well. An **advanced** course is also available. 
I'm also self taught. I've had a similar experience to some others here - livinglifelazily and [DrysineDrone](https://www.reddit.com/user/DrysineDrone). I started with my company in the call center taking tech support calls, took the opportunities I could to learn the business (we're an ISP) and move into more technical roles, and ended up as a business/data analyst within just under 2 years. The biggest contributors for me getting my job you already have: strong Excel and passable SQL literacy, and motivation to take on the stuff other people didn't want to, which I typically just automate as much as I can (my company abuses excel, so vba has become very useful). For a data analyst job, definitely pick up some R knowledge, and maybe look into a BI platform like Tableau, Microstrategy, PowerBI, etc, which look good on resumes (Tableau has a student edition, Microstrategy you can use free as a learner/individual). I'm a high school drop out, so, if I can do it, anyone can, and I love my job.
Lots of people are self taught so it's possible. I took a $15 online course and got a job as a data analyst. I won't say it's easy to get a job but you should really leverage your current experience/expertise and find a data analyst role that finds that. What's more important is the business acumen and intuition gained from experience than actually knowing how to use SQL. Impresser the hiring manager with your current expertise and then you just need to pass the SQL screener.
Can you post the query you’ve written?
I found the answer. It can be done in such a way: `SELECT * FROM documents t1` `WHERE revisionNumber IN ( SELECT MAX(revisionNumber)` `FROM` `documents t2 WHERE t1.title=t2.title AND t1.documentForm=t2.documentForm` `GROUP BY t2.title,t2.documentForm` `)`
Could you just make all your queries stored procs. There are quite a few ways then to string search through all procs on the system. With basic comments in your code it should be enough... Another idea which I actually do run in production... I create reports for multiple departments - some one time reports others get pushed to ssrs. Usually once something is approved and ready to go live I save the project. No fancy coding just good ol fashion organization. On a secure network drive I have folders labed #dept1, #dept2 ect. Inside each department folder holds all the reports. If i ever want to look at some hidious code written two years ago I've got it...
Use VMware or whatever VM software you like and stick windows on it with SSMS installed 
Change it up maybe.. By no way might this be the best way but it should work.. In theory Select distinct accountid , region , vendor, , placementamt , cast(null as date) as paydate Into #aaa From table a Update #aaa Set #aaa. Paydate = baa. Paydate From (select max(Paydate) as paydate, accountid from tableb Group by accountid )#baa Where #baa. Accountid = #aaa.accountid ... Yeah I'm on my phone. Sorry this didn't look so ugly in my mind when I started to try helping... Goodluck 
shouldn’t it be id 6,7,10 instead of 6,7,3? since you want to return only the rows with the highest register_number
Hi, if your SQL implementation supports `DISTINCT ON` (almost all system should...), you can avoid the join in your query and simply write: SELECT DISTINCT ON (d.title, d.documentForm) d.* FROM documents AS d ORDER BY d.title, d.documentForm, d.revisionNumber DESC; Best wishes, —Torsten
Yeah it was typo, thanks :)
An outdated an inefficient way to use sql. if you think you need a cursor post for the group. Im guessing whatever the need there's a better way... 
Here ye here ye. This! 
I've created a ssrs dashboard which then depending on the button the users press different powershell / storedprocs/ database mail can be run. Pretty easy and they like it because they can just fav the page 
Join the table to it's self SELECT * FROM Prices a Inner join prices b On Where a. Source = '' And b. Source = ''
" it is not even worth starting to write any logic in sql" Who says this? It's never been said that I have ever heard. "Complex things result in complex bugs, and bugs at the level of the database are quite scary" Said the application programmer who does not understand databases. You keep talking about implementing logic in the app code and using triggers... you do realise that a lot of logic runs a lot faster if you do it in the database, you don't have to move the data to the app server to then filter down. Also triggers are in the vast vast majority of situations not needed and an unnecessary performance burden. I don't use PostgreSQL, I am T-SQL but it very much looks like you are using dynamic sql with a cursor inside a function which is at least in T-SQL land the worst performing method you could use. Stop writing articles on SQL, you do not understand it and stick to your app code cause this article is awful. Complete garbage blogspam. 
Why do blogspam people think that their garbage articles will be well received here? The majority of people who post blogspam here seem to know significantly less than the majority of the people reading them. 
I’m a data scientist/analytics manager for a Fortune 500. I started out as an analyst with a bit of Excel knowledge five years ago. I taught myself SQL, which sped up my analytics pipeline, but never wanted to get into backend development. Think about where you want to be. Going to school for SQL alone is a waste of time and money. If you want to be a backend dev/DBA, get the certs. If you want SQL as part of your toolbox to facilitate your analyses, whether that be Excel, R, Python, etc, then you can get by learning on the job + free online resources. For example, our team hires different people with different skill sets. Minimum knowledge of SQL is required, but you spend so much time in it and it’s so easy to teach that we rather hire for analytical prowess (much harder to come by) and worry about advanced SQL on the job. Only pay for school/courses that will have a return on investment in industry, unless you have a lot of time and money.
I am not 100% sure what you are asking but generally dynamic SQL and cursors are bad practice. I am not sure why there is any dynamic SQL here as it does not look like its needed. By making it a regular statement and not executing it dynamically it will not break anything and will run faster. So the stored proc is generating a results set and you essentially want to union them all together? create table #Temp (whatever columns outputted from Sproc) insert into #Temp EXEC dbo.storedproc @EpisodeId Then outside the cursor select * from #Temp You cannot execute a proc into a temp table (ie select * into #temp), you must define the columns and table definition prior to execution of the proc.
There is a population of crabs numbering 50000 at the start of the year. Every month 5% of the crabs alive at the start of the month, excluding offspring, die during the month. For the months of May/June/July 25% of the crabs alive at the start of the month pair off and pro-create. Each pair have 3 off-spring. Every month, 15% of newly born crabs over 3 months old depart the colony. Assumptions: Off spring do not pro-create Make any other assumptions you need if you are in doubt. Write code to calculate the number of crabs at the end of the year
I'm interested in learning sql so I ended up here...may I ask what you used to learn the language and to practice using it once you learned it?
Hey, option one worked like a charm. Many thanks for the help.
I don't see the big deal. Nathan re-implemented some database engine features in Julia as a learning experience. And how does posting something on Github qualify as trying to "monetize" it?
Aren't we all self taught? I'd be surprised to learn companies home-grow their tech staff, and schools don't exactly teach practical applications.
Fair enough its not monetised like most of the blogspam here but it's not a database engine at all. I just feel that if your opening sentence is admitting you don't know something then you should learn it properly before blogspamming it. If your skill level is at doing 'intro to' free online courses then you are years away from contributing anything useful to the subject and certainly nowhere near bring able to author an article that is useful. 
I used lynda.com to get the basics, stack exchange to learn answers to specific questions, this subreddit, and very patient coworkers who were willing to help
Dump the SQLite databases to CSV. Load the CSV files into Postgres.
i have the same background as you trying to make a career change, what certificates or other qualifications are most looked for by industry? have a BA in supply chain with office experience as an analyst. 
PMP certs never hurt, but that's project management. 
im at a point where i have no idea what i really want to do just want to increase chances of being hired / call backs for interviews
If you're wanting to stay technical, which I'd recommend early in your career, I'd say shoot for an MCSA for SQL server 2016. They're only 2 exams now and you can focus on either querying, administering, or bi dev. You could get that cert in 6months if you bust your butt :)
Unfortunately this is Core SQL only, meaning I'm restricted to AND, OR and NOT statements. As for efficiency I'm referring to the length of the query
If you know a scripting language like Python you could iterate through the tables in the SQLite database and insert them into the PGSql database. There are a few tricky things you'd have to look out for. The SQLite is obviously all just strings so you'd have to more-or-less manually or at least semi-manually indicate what each column was going to be in terms of data type. (You nominally have something like an integer for example in SQLite, but nothing is stopping you from putting say, a letter in a particular row in that column. So you either have to have been doing some other type of validation along the way, hope that you don't have any invalid data, or determine how to handle invalid data). Or you could copy each table as [table_name]_string, making each column a string, regardless what the original type was. Once everything is in PGSql, copy it over to a table by the original name with the correct column values. It is probably going to be fairly laborious either way though it sounds like it was for a webapp or something so if it was mostly constructed by an application you might be good to go. I wish I had a guide to help you. That said, shameless plug: I've done this before if you were looking to subcontract it out.
If you don't want your logic to be in the code, then you don't have much alternatives but stored procs. You are right to try to have the less possible logic in your db, but sometimes a SP is a nice way to handle specifics. If you don't want a sp, then the way to go would be to try to have very specific functions that achieve the interaction in the db. One to register, one to login, one to update a profile and so on. &amp;#x200B;
Not sure if I'm being dumb. Isn't it just.. ```` WHERE ((A != B) AND (A != C) AND (B != C)) ````
There are other ways to do it but I don't know that they would be simpler. (Side note: if any of your columns are nullable, don't forget to address that issue as well.)
I don't usually worry about length of the query when I'm talking about efficiency. I'm thinking more about what the SQL optimizer is going to do. But it sounds like you're trying to do this for a class? in which case We're talking theoretical anyway. I recently had a problem in SQL Server where the query plan was doing some nested loops and reading rows as if it was doing a cross join because of an OR clause. I ended up splitting it into two statements to speed up the query.
I ended up writing a Python script using native `sqlite3` (without SQLAlchemy), which is probably better than importing from CSV, anyway. https://github.com/patarapolw/pathsql/blob/master/dev/migrate.py
Yup, standard De Morgan's.
Hey there, I'm a DBA and we have Simplivity hardware as well. We have not seen this particular issue, but I would be happy to look at one of my 2017 instances tomorrow. If you're interested, shoot me a PM and we can connect.
How long is it expected of a new person to get familiar with all the tables and what columns they have inside those tables, assuming the database has 50-100 tables? Thanks for the great replies!
You should contact SQLzoo. 
Window functions! WITH a AS ( SELECT ROW_NUMBER() OVER (PARTITION BY Title ORDER BY revisionNumber DESC) AS r , * FROM yourTable) SELECT * FROM a WHERE r = 1 The Row Number function in this case will give each type of report a row number, with the highest revision number being 1. Then I just select the record where the row number is 1, ensuring that that specific row is the max revision number.
After submitting the post, I can't see the screenshot, so i'll copy the code here: &amp;#x200B; /\*7\*/ SELECT COURSE\_NO, SECTION\_ID, LOCATION FROM COURSE, SECTION WHERE COURSE\_NO BETWEEN 100 AND 199 AND LOCATION = 'L214' OR 'L509' ORDER BY COURSE\_NO; &amp;#x200B; Error starting at line : 2 in command - SELECT COURSE\_NO, SECTION\_ID, LOCATION FROM COURSE, SECTION WHERE COURSE\_NO BETWEEN 100 AND 199 AND LOCATION = 'L214' OR 'L509' ORDER BY COURSE\_NO Error at Command Line : 6 Column : 1 Error report - SQL Error: ORA-00920: invalid relational operator 00920. 00000 - "invalid relational operator" \*Cause: \*Action: &amp;#x200B;
Your query is using the = operator for multiple values. Use In. Location in (value1, value2)
You can also use (Location = value1 or location = value2) If you use this, remember that parenthesis matter. 
Run [sp_Blitz](https://github.com/BrentOzarULTD/SQL-Server-First-Responder-Kit) and pay attention to the WAIT statistics. If the issue is common, it should show up and identify a probable cause for the activity. 
That totally stinks!! Ive pretty much been an accidental DBA since I started my career... I can't remember a time I was restricted..
Try this? &amp;#x200B; SELECT COURSE\_NO, SECTION\_ID, LOCATION &amp;#x200B; FROM COURSE, SECTION &amp;#x200B; WHERE COURSE\_NO BETWEEN 100 and 199 AND LOCATION IN ('L214','L509') ORDER BY COURSE\_NO;
I assume I do, but based on my searches I should be able to accomplish what I need by listing both tables in my FROM line. The problem is no matter how I write everything else, I still end up with SQL dev taking issue with me pulling the course table, with a yellow line under that and only that. There's nothing wrong with the table itself, as this is the first query i'm having problems with.
I’m on mobile and don’t have a way of throwing the code into a program or anything, but looking at that, your From statement looks like it would be the culprit, doesn’t it? Saying SELECT (whatever you need) FROM COURSE, SECTION I’m still no expert with SQL by any means, but the syntax with the comma and then “section” after Course would be causing the error. What are the full names of the tables including database.schema.table.tablename? Is it just one table?
Haven't used redshift but I'd do something like SELECT cust.ID, Cust.Name, min(DATE(Order.Date)) OVER (PARTITION BY cust.ID, cust.name), FIRST_VALUE(Order.Item_number) OVER (PARTITION BY cust.ID, cust.name ORDER BY DATE(Order.Date) ASC) Or if redshift doesn't support first_value then SELECT ID, Name, Date, Item_number FROM (SELECT Cust.ID, Cust.Name, Order.Date, Order.ITEM_NUMBER, row_number() over (partition by Cust.Name, Order.Date order by Order.Date asc) as rowno FROM (your tables/joins go here) ) x WHERE x.rowno = 1
Thank you! Very helpful advice. &gt; specific flavor I am not sure if I understand what flavor refers to here. Do you mean Relational, NoSQL?
Thank you! That seems to have worked, although I don't quite understand why. I apologize for my newbie sql knowledge. I believe the part you edited above was to allow redshift to use all rows of data within the dataset, however how does the OVER PARTITION BY solve this issue whereas a group by or other way couldn't? Is it due to the way it is calculated in the database, at the window/row level? Let's say we have the following dataset id name date number 01 tony 01/01/2018 5 02 bob 01/02/2018 3 03 tony 01/03/2018 1 So the date selected would be 01/01/2018 and the number would be 5, however what is the process it thinks through to get to the 5 instead of the 1? Thanks again for your help! 
Sorry I made a couple of edits and now I'm not sure which version of my comment you're replying to: But to explain: OVER (partition by x order by Y) is used by window functions like FIRST_VALUE. The partition is saying "Split up the data set by the ID and name, and order it by date.". That splitting is a "window" Because it has an order (the date), FIRST_VALUE gets the first thing that the window is ordered by.
Make sure your on the correct database or type the full name and schema. An example: adventureworks.dbo.sales vs sales. Sales will work if your logged on adventure works but not if your logged into poopoo server... 
Hey, MrDarcy87, just a quick heads-up: **neccessary** is actually spelled **necessary**. You can remember it by **one c, two s’s**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Good bot
Actually, the code you had prior was correct (I was looking for the ID based on the date, not the customer). I understand now, thanks. Basically, since we are ordered by the date, logically the first order item number would also be that with the earliest date, right? Thanks!
Here are the primary flavors : SQL Server, mySQL, oracle, postgresql, db2, Sybase, and mariadb. You can see more here :https://db-engines.com/en/ranking. They all have similarities but some are more different than others. I always recommend to learn one of the top 5 as those are by far the most used and have the most resources. 
I think it's `sp_blitzFirst` that gives the wait stats, not `sp_blitz`
Full backups are independent of each other. More likely that something else is going on (for example, many things back up at the same time as the first and the SAN/Backup unit where overwhelmed).
I updated my post
Hey, sarcastagirly, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
You're correct. I was pointing to the Git repo for the full scripts and various documentation so that he can go through them. Familiarity with the scripts at large are beneficial. 
Thanks, I'll see what I can get from this. The write latency is always the same, even after share point reboots etc. 
Hope they help! Also, how many tempdb files do you have and are they of a fixed, uniform size? 
8 temp DB files ( 8 cores on this VM), they are set to auto grow.Total size of tempdb is 136MB, so virtually no use. That's what's baffling me. 
Completely self taught with Economics B.S. I was working a Credit Analyst and a need came to analyze origination data intra-department. I taught myself SQL then I used that experience to move into a Business Systems Analyst role, which turned into BI/Data Warehousing role. This all in the span of about 3 years. If you spend the time it's possible. Almost everyone I know is self taught.
PIVOT is the word you are looking for here And as long as your datetime "buckets" (9am -10am ) arent too numerous, a CASE statement will help. 
Unless I'm missing something, you're not joining c onto the employee kee on b, you're just looking between dates on a so you're doing a massive cartesian join. You could also clean up the latter part of the third join by using NVL or the SQL server equivalent rather than using an OR operator within the join
[https://stackoverflow.com/questions/20324372/splitting-a-single-column-name-into-two-forename-surname-in-sql](https://stackoverflow.com/questions/20324372/splitting-a-single-column-name-into-two-forename-surname-in-sql)
Yeah I added an edit. I’m gonna try and figure out the syntax to add the second condition on the join to c I’m an idiot. 
May be overkill but if probably make tables a and b into a subselect then you can join c onto that on employee key = employee key and date between x and y
Okay. I'm a moron. Just adding "and b.emp\_sk = c.emp\_sk" fixed everything. Thanks for your help!
Can you mock some data up and add a case or two to your example table and then another mock of what you're trying to have it return as? If I'm understanding your table well enough you can `select blah from table` and it will give you a bunch of rows in a column. Are you trying to then take those rows and `select row1, row2, row3, row4, etc. from newtable`?
Nevermind. I'm unfathomably dense. Course is a foreign key of Section. Made this infinitely more complex than it needed to be.
Someone already has the code here, but the jist of your approach should be to identify that separator between the First Name and Last Name and exploit that. So if the first and last names are separated by a space, for the first column, it'll be a substring composed of everything from the beginning of the string to whatever location the space appears. The last name will be a substring composed of everything from the space to the end of the string (excluding the space, of course).
good bot
I'll take a stab at your suggestion.... &gt; I don't really agree with the overall approach. I suspect this will become very slow very quickly. I've been concerned about this as well. My plan was to index and partition as necessary to keep the sheer table length from becoming an issue. Is there a better pattern that you can think of to track changes in a table so that don't become huge maintenance issues? The problem I'm trying to solve is that I'm loaned out as time permits for these guys, and their requests of which tables to track changes regularly. And these source views are just extraordinarily wide.
I've used a change tracking setup in the past based on sparse columns. You have the current values as a 1:1 copy of the table/view. Than you have a separate change table. The PK and AsOfDate are normal columns. Then everything else is a sparse column, and stores just the previous value. You can get some use out of [COLUMN_SETs](https://docs.microsoft.com/en-us/sql/relational-databases/tables/use-column-sets?view=sql-server-2017) also.
Amazing info, thank you! I read some on this yesterday but not knowing much about SQL this is all a lot to ingest. If the tempdb size is only 136MB wouldn't that mean that tempdb isn't really even being used? 
But what here is no space? What if his name is Randy__Bobandy? 
Hey. Sorry to bother you again. The query runs. And it runs fast. But there are some errors, and some of the dates/assignments aren’t lining up as they should. Because table c is using “nominal dates” while table a isn’t (as far as I can fell) do I need to do anything special for the date/time in a, and the nominal dates in c to “talk to each other” accurately? As you can tell, I’m quite the novice. And my guy at work that is really good at this stuff is on vacation. 
Spell check the file first?
How do I do that, with 6 million records?
&gt; You can remember it by begins with g-. same goes for .gif -- pronounced the same way 
Okay r3pr0b8, now THIS is the hill I want to die on.
Does every row have a misspelling or something? If so, that's what interns are for.
For addresses there are verification services. For everything else you use what you can. Sometimes that's showing a list of distinct words. Then spellcheck them or have humans go over it and highlight the issues. If this is a reload monthly then keep track of what words got changed or even what records got changed so you can recreated the fixes. There are lots of other things you can do, you just have to get creative. I don't know of a list of techniques but I imagine there is one somewhere on the internet.
The idea is that there has to be *some* identifying character between the names. If it's Randy\_\_Bobandy, the character is a double underscore \_\_. If there is no splitting character, then I am not sure how you would do it. If the name is in upper camel case, like JohnSmith, then you might be able to do a case sensitive search, I don't know how to do that though. If there's no distinguishable difference between first and last name, like johnsmith, then I don't know if it's possible. Of course you and I know that johnsmith is supposed to be John Smith, but to SQL, it's just a string of characters.
Not really sure, but is it possible that some of the words (maybe version) are reserved by Hibernate / JPA? I'd try putting all of the field names into backticks: insert into reminders (`created_at`, `modified_at`, `version`, `interval`, `message`, `owne`r) values ('2018-09-13 20:43:35.666', '2018-09-13 20:43:35.666', 0, 20, 'This is a test', '1234567890123456');
I manually change them. Of course you have a ton of records. Unfortunate.
A table called Users with a Primary Key that's each User's unique ID (whatever you want that to be). A table called Posts with a Primary Key that makes each Post unique. Then in that Posts table have a column called UserID that's a foreign key that references the Users table. That would be the straightforward plan in my head. Any issues with that? Or other issues that would make it more complicated?
the error message contains a hint about which word it was of course, a blunderbuss shotgun solution will work -- "let's slap backticks around everything, that'll fix it"
You manually change them? What do you do when there are file updates every month? Do you keep track of those changes?
There are some tools like Soundex() that can help with this some. Another is just to have an update script where you throw in a lot of update statements specific to particular words and add to it incrementally and run it after each load. 
2017. Thank you!
Thank you for all your help!
I ran sp blitz, [here](https://whcmllccom-my.sharepoint.com/:x:/g/personal/jeff_whcmllc_com/EfpPsj8FH85Jg3RpnhQcXv0B6x8OMKkd--Sezx9samqHFQ?e=jzUZzI) is the output. 
what if you insert the acsii value for non breaking space using the chr function? chr(255)? sorry i'm on mobile so can't mess with it atm
I just went through something similar for 27 million rows with 90 fields. The redditor above is right. Unless you use something besides sql, you're going to have to make a lot of assumptions and perform lots of updates. RBMS are only as good as the data feed to them. Data validation for some stuff should be done on the front end. 
In the past, depending on the data you're working on there will be a way of validating data cleanliness. An instance I always teach is for 'fixed value' stuff like maybe addresses (cities, countries, postal/zip codes). The 'University of California Las Angeles" I would hope is an uncommon misspelling. But you can write very basic code (and enhance it as needed for the case) to get you a count of instances in a data set. For instance if you do a distinct count on a 'university' column, sort by count descending and sift through the list and find high-count badly spelled university names and change them. This can be applied to any field with a 'fixed set' of data. My experience in this was mostly around manufacturer names and models. Applying a bit of Pareto analysis to it; if your top 80% of your list is correct, going further down is optional to how clean you want your data to be. Someone else mentioned interns, you could get them to correct in excel and do an update. For 'open value' fields like free-form text, I've loop through the rows and applied a spell check to each value and logged everything with a certain level of bad spelling. It really depends on how clean your data needs to be versus how much time you have. But generally there is always a way to get the DB to do the heavy lifting. 
You might need to qualify the location of the file more like: SET @OUT_FILE = 'C:\FolderName\' + @MACHINE_NAME + '.txt'
I tried it, but it didn't change anything. Thanks for the reply, though.
I run it through R Studio first to clean it up. You have to learn R but it's quicker, for me, and easier.
This really isn't how databases work. Put them all into a single large table and it will work much better.