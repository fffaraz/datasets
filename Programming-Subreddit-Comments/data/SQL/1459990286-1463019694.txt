Here's how I'd solve this: SELECT projectworker.project_id, worker.worker_name, worker.worker_id, worker.supervisor_id FROM worker JOIN projectworker ON worker.worker_id = projectworker.worker_id AND worker.supervisor_id = projectworker.supervisor_id The `CASE` statement /u/artmast mentioned is definitely worth knowing, but it's not really the best way to go about solving this problem. Side note: using implicit joins like you did (`FROM worker, projectworker`) is poor practice.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/sqlserver] [SSIS - loop through files and move with dynamic destination (x-post \/r\/SQL)](https://np.reddit.com/r/SQLServer/comments/4dosdi/ssis_loop_through_files_and_move_with_dynamic/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Why is that considered poor practice? My professor mentioned the same thing in a lecture but I neglected to ask her.
If the files are distinguishable like you're saying, couldn't you direct the files to the location using multiple loops? As in one looks for *ABC*.xls, the next *DEF*.xls, etc? It would only be able to move the files to the specified location if it matched what you put in the loop. As far as passing the filename that's just having a variable and setting it in the loop. Then in the loop a file system task would do the move, the source is your filename variable, destination is the folder.
Yeah, that does sound doable. There's currently 15 or so naming conventions, with more down the line which makes it sound tedious, but better than nothing! Thanks
I believe you may have to temporarily turn off FK checks before you attempt the alter. 
Yeah man glad to help. Copy paste will save you some time, but yeah I agree it's not the most elegant of solutions. Maybe someone else will come by and we'll both learn a better way.
I've ran into this before. The solution I went with was mandating all SSRS report params have to be all lowercase. All the rdl files were changed and then the app/data developers had to change names there. Some devs were smart and had a wrapper class that was used to call the reports and they just had to fix it there.
You need to use `\` since * is part of markdown. \*ABC\*.xls turns into \*ABC\*.xls
That makes sense, thanks.
It is possible to do in SQL but I don't think it's feasible. How many distinct items and customers do you have? The solution requires listing all possible combinations of items bought by each customer and then grouping by it to get the count. To get the combinations I would assign each item a bit in a bitmask (e.g. assuming 3 products a bitmask 101 would mean product 1 and 3 were bought). But this results in customer_count * 2^product_count rows to scan, and we're limited by the number of bits the biggest type can hold (64 for bigint, but that's not a realistic count of rows). For just 20 products and 20 customers, that's 20 million rows. All in all, some kind of an analytical solution would be much better suited than SQL to produce the desired results. I can produce a working concept procedure, just not sure if it's worth the time. 
Thanks for your opinion.
This is the furthest I've gotten with the query so far. I'll work on it some more later on. SELECT LEFT(list, LEN(list) - 1) + ' ' + CAST(SUM(soldCount) AS VARCHAR) AS list FROM ( SELECT id, COUNT(distinct productName) AS productCount, COUNT(productName) AS soldCount FROM ForgeRock GROUP BY id ) AS sold CROSS APPLY ( SELECT DISTINCT productName + ',' AS [text()] FROM ForgeRock fg WHERE fg.id = sold.id FOR XML PATH('') ) AS itemLists(list) WHERE productCount &gt;= 2 GROUP BY list ORDER BY LEN(list)
I would add to /u/rcgarcia's point. it is also a matter of clarity and maintainability. Knowing, at a glance, what conditions connect the tables at all times and what conditions are specific to answering a specific question makes fixing a problem with, or enhancing/borrowing a given piece of SQL *much* easier. Completely different point.. your table structure doesn't make sense to me. I don't know why you'd even have the supervisor ID in the projectworker table at all if the supervisor is that worker's supervisor and not a temporary supervisor just for that project. If it were structured that the worker's supervisor is linked in the worker table, so for this example: worker worker_name worker_id worker_sup And project work back to the example from yesterday where it's just project work project_id worker_id join_date end_date Then you'd just select SELECT project.project_id , project.project_name , worker.worker_name , ( SELECT Supwrk.worker_name FROM worker as Supwrk WHERE worker.supervisor_id = Supwrk.worker_id ) as supervisor FROM project INNER JOIN projectworker ON project.project_id = projectworker.project_id INNER JOIN worker ON projectworker.worker_id = worker.worker_id WHERE EXISTS ( SELECT 1 FROM projectworker supprj WHERE project.project_id = SupPrj.project_id -- same project as worker. AND supprj.worker_id = worker.supervisor_id -- and the worker is the supervisor of the main workder ) Otherwise you're redundantly storing the supervisor in every line of the projectworker table which seems like a waste of space. You'd only need the supervisor in the project worker table and *NOT* in the worker table is if the supervisor was just for the duration of the project, but if that was the case you'd never have the question on when the supervisor and worker both worked on the same project because the answer would be "always, unless the worker *is* the supervisor" You can introduce a whole new level of complexity if you have to track when a worker switches teams. 
Question: Is a given project *only* funded by one FundID, or can a given project be paid for by *multiple* FundIDs? If only one, I'd just add project begin/end dates to the projects table. Then you're just selecting SELECT Projects.ProjectID FROM Projects INNER JOIN Funds ON Projects.FundID = Funds.FundID WHERE Projects.EndDate &lt;= Funds.FundEndDate -- Why add the table name to the field name? If a project can be funded by one or more FundIDs, I'd make an intermediate table ProjectFund FundID(FK), ProjectID(FK) and have Projects contain all project specific information with no foreign key. SELECT Projects.ProjectID FROM Projects INNER JOIN ProjectFund ON Projects.ProjectID = ProjectFund.ProjectID INNER JOIN Funds ON ProjectFund.FundID = Funds.FundID WHERE Projects.EndDate &lt;= Funds.FundEndDate -- Why add the table name to the field name? Though then sometimes a project is "on time" according to one funding source and not completed on time according to another. That would seem more like something you'd have in the project table like having ProjectedEndDate and ActualEndDate which could be compared without regard to funding source.
It's not the complete solution, I just need to find a way to count smaller sequences that appear in larger sequences. I know it's possible as a set based solution, it's just a bit of a brain teaser
yes
Thanks!
why no powershell? Just use the same lookup method(either in a table, or CSV). Look up the folder value(in a function) based on the filename from the get-childitem cmdlet. you can create a log file and export the move data with -passthru....or export to csv.
Of course, I am in Oracle RightNow, records in right now are assigned Answer Id's. When I view an answer then is the body of file also called "Answer". To search I go to my toolbar I click "Answer" &gt; "Answer Default" then the main search comes up. In there I can filter language, access level etc. Then I can type in the following fields "answer id range", "phrases" "similar phrases", "exact search", and "complex expressions". For this search I have been typing in "phrases", but "complex expressions" gives me similar results. I have tried '%[0-9][0-9][0-9][0-9][0-9]%' and some similar expressions but I'm not seeing answers that i know contain a 5 digit extension in the body on the text. Hopefully this is the right detail. This program can be confusing to describe since some terms like "Answers" can refer to multiple things.
You have to create a variable with an object type. Google this, there are lots of walk throughs on how to do this. I have had to build these at almost every company that I have worked for. Each time in Google the solution and follow the steps they provide. 
That works! Congrats! I see you used a `foreach` so you should have been able to iterate an array since a file collection works like an array. If you post a snippet of your code, I can help you iterate through the array of strings.
For what it's worth, you can put *anything* in your JOIN conditionals, just as in a WHERE statement. In fact, if you're using INNER JOINs, there's literally no difference whatsoever between WHERE and JOIN. Understanding that can lead to a big "aha" moment in learning how to make the most out of SQL.
You could dump it into a temp table without your having clause and then run a select on that, if performance isn't a concern.
&gt; there's literally no difference whatsoever between WHERE and JOIN. There's literally no difference *to the DBMS* between WHERE and JOIN *for most DBMSs* There's a huge difference to readbility and maintainability of SQL code.
Use top 1/limit 1 SELECT top 1 d.DEPARTMENT_NAME, d.ADDRESS FROM DEPARTMENT d INNER JOIN EMPLOYEE e ON d.DEPARTMENT_ID = e.DEPARTMENT_ID WHERE d.ADDRESS NOT LIKE 'DALLAS' GROUP BY d.DEPARTMENT_NAME, d.ADDRESS ORDER BY count(*) desc; Edit: by the way the reason you get no records is that you are excluding Dallas in the main query but not the subquery in your HAVING clause.
 select top 1 dim.info, count(*) from order_pizzas ord join pizzas piz on(ord.pza_dmo_code=piz.dmo_code) join dimension dim on(dim.code=piz.dmo_code) group by dim.info order by 2 desc; edit: probably limit 1 in Oracle .... edit 2: [this](https://www.reddit.com/r/SQL/comments/4dsd2x/selecting_next_highest_max_value_while_excluding/d1txjoc) is the way to do it in Oracle.
Right off the bat, you're HAVING clause is causing your issues. It is not RELATING to your department, it is just getting MAX(COUNT()) values. You could look up CTE and add a Row number value over department Id to it, then that would let you do a select where RowNumber = 1. Simple way I see how it goes.... SELECT TOP 1 d.DEPARTMENT_NAME ,d.ADDRESS FROM dbo.DEPARTMENT AS d INNER JOIN dbo.EMPLOYEE AS e ON d.DEPARTMENT_ID = e.DEPARTMENT_ID WHERE d.ADDRESS NOT LIKE 'DALLAS' GROUP BY d.DEPARTMENT_ID ,d.DEPARTMENT_NAME ,d.ADDRESS ORDER BY COUNT(*) DESC;
you are counting them in the ORDER BY, just don't need to display it.
Which do you more think is readable/maintainable? I tend to keep as much of the logic as I can in the WHERE and just put the column-to-column conditions in the join. Example: SELECT * FROM t1 JOIN t2 ON t1.a = t2.a WHERE t2.b = 'whatever' Instead of: SELECT * FROM t1 JOIN t2 ON t1.a = t2.a AND t2.b = 'whatever'
I love your formatting and I know that's considered best practice, but it's tough to break the habit. Sounds like the way I do the JOIN/WHERE conditions are good to go :) 
I'm not sure it's considered best practice, but it's how I grew up doing it, so it's an equally hard habit for me to break. I just finally broke the comma at the end habit this year and I've been doing this stuff for 30+ years now. SOOO much easier to comment things out when debugging. 
Thank you! This makes sense to me. I really appreciate it. I'm going to dissect this to better understand it!
My sister's called Rachel.
 with dimcount as ( select dim.info as info, count(*) cnt from order_pizzas ord join pizzas piz on(ord.pza_dmo_code=piz.dmo_code) join dimension dim on(dim.code=piz.dmo_code) group by dim.info ) select info, cnt from dimcount where cnt = (select max(cnt) from dimcount)
Just "JOIN"? I was thinking it would need INNER, LEFT or RIGHT JOIN depending on the direction of the relationship. I'll give it a try in the morning though. Thanks! 
Might be 32bit odac but is driver on server 64bit? I would switch to the Oracle native client
If msforeach is missing dbs, do you have access to read those DBs? If the list of DBs doesn't change just hardcode and use a cursor to build your query dynamically. - which is to say that you have to use a cursor in this case. 
I'm tired and on my phone so correct me if I'm wrong, but you want to get a running total of the sum of each days' transactions? If that's the case, replace 'SUM(transactionvalue) OVER...' with 'SUM(SUM(transactionvalue)) OVER...'. Even though SUM is typically an aggregate function, having the "over" clause turns it into a window function instead. This is true for all aggregate functions AFAIK. Edit: looked at the data you provided, I think this is indeed the solution you're looking for
not sure what perspective you want, but either: select uniquecode from table1 where concat(tag1,tag2,tag3) like '%A%' or select uniquecode, tag1 as tag from table 1 union select uniquecode, tag2 as tag from table 1 union select uniquecode, tag3 as tag from table 1
Default join, with no keyword, is inner join which takes only those records that have a match on both sides. 
For anyone that would like to try BigQuery for free (no credit card is needed): - https://www.reddit.com/r/bigquery/comments/3dg9le/analyzing_50_billion_wikipedia_pageviews_in_5/ (it will only take 5 minutes to see the same results as Mark) (Great series of posts, Mark!)
Thanks, UNION ALL worked, you were also right about not being able to tell them apart too, so you helped me rule it out as a solution. I ended up using SWITCH to solve the problem. Sorry for taking so long to reply.
Gotta be in a faq some place by now: pivot = group/aggregates with case; unpivot = cross join selector/case.
Worked, thanks so much!!
Well you could: * Create a temporary table with a an integer auto-number column and a NVARCHAR(MAX) column * Populate that with a list of databases * Select the top 1 ID and name * Run your proc on that database * Delete that row from your temp table * If there are rows left then jump back to the select top 1. Or you could put together a C# console project to do this.
It's not working as in you're getting too many rows, not getting enough rows, or not getting *any* rows? What do you get if you select SELECT * FROM Projects WHERE FundID = 1
 SELECT storenumber , SUM(CASE WHEN type='return' THEN amount ELSE NULL END) / SUM(CASE WHEN type='purchase' THEN amount ELSE NULL END) AS ratio FROM transactions GROUP BY storenumber 
We have hundreds of DBs per server, one per client. When we are asked by account management to 'TURN OFF' a client, we simply set the DB offline and a flag in a another control database that controls the login screen. If for some reason they forget to export something from our system or whatever reason we need to turn the client back on in the next 60 or 90 days, we set back online, re-enable the login page and boom, they have access. 
Make sure you deal with null values in the denominator. You do not want to divide but null or 0.
Valid point, though one would hope that there are no stores that have one or more transactions in the transaction table but none that are purchases. 
Maybe just a slight adjustment that should be unnecessary 99.999999 percent of the time. SELECT storenumber , SUM(CASE WHEN type='return' THEN amount ELSE NULL END) / SUM(CASE WHEN type='purchase' THEN amount ELSE NULL END) AS ratio FROM transactions GROUP BY storenumber HAVING SUM(CASE WHEN type='purchase' THEN amount ELSE NULL END) &gt; 0
So I've learned a little more I know I can direct my search my typing "WHERE ANSWER CONTENT QUICK PREVIEW" this will look at the correct field in the files i'm looking in. My problem now is saying what to look for basically any extension from 10000 to 19999. So for example I've tried "WHERE ANSWER CONTENT QUICK PREVIEW '=&gt; 10000 and &lt;= 19999' However I am getting files that aren't related.
if the tags don't need to be distinct (ie you want them all, even if there are dupes), then `union all` should work better. also, if using sql server, you could use the `unpivot` operator which does this for you (and may work better, not sure tho) 
To add to this, you should also look into how you want your XML structured as you may not want everything as an attribute, but rather as an element or a mix thereof. [Linky to one such explanation/discussion](http://stackoverflow.com/questions/241819/xml-best-practices-attributes-vs-additional-elements)
I feel like a much better approach would be this: select distinct a.storenumber , case when c.purchase &lt;&gt; 0 then b.return / c.purchase else 0 end as ratio from transactions a inner join ( select storenumber , sum(amount) as return from transactions where type = return group by storenumber ) b on b.storenumber = a.storenumber inner join ( select storenumber , sum(amount) as purchase from transactions where type = purchase group by storenumber ) c on c.storenumber = a.storenumber edit - Or more simply like this: select a.storenumber , sum(b.amount) / sum(c.amount) as ratio from transactions a inner join transactions b on b.storenumber = a.storenumber and b.type = 'return' inner join transactions c on c.storenumber = a.storenumber and b.type = 'purchase' group by a.storenumber having sum(c.amount) &lt;&gt; 0 If you want to see the `NULL` data then you can make those `INNER JOIN`s into `LEFT JOIN`s and put an `ISNULL()` around the ratio expression and have it labeled as whatever you want, e.g., `'No Purchases'` or `'0'`.
sys.table_types (Transact-SQL) Applies To: SQL Server 2014, SQL Server 2016 Preview Displays properties of **user-defined** table types in SQL Server. So you would only have stuff in sys.table_types if you created a table type and generally you would only make one if you need to pass a list of things as a parameter to a stored procedure without creating a permanent or temporary table for it. Not sure why it says it applies to 2014 and 2016 since I use table types in 2008r2 
I wonder if I'm the only one that likes to keep most command words and parameters separate. SELECT A ,B ,Z FROM t1 JOIN t2 ON t1.a = t2.a WHERE t2.b = 'whatever' AND t1.A = 'Bob' I find this so much more legible.
Why not change the `INNER JOIN` to a `LEFT JOIN` or are you saying that `TABLE2` doesn't have all of the `UN_ID`s?
Again, nope, only if it actually occurs. TABLE4 (the 'other' table mentioned above) is the master for the UN_IDs, so they are all in there. 
So add: `RIGHT JOIN TABLE4 ON TABLE4.JOU_ID = TABLE1.JOU_ID` and then modify parent such as: SELECT TABLE4.UN_ID , ISNULL(COUNT(*),'DesiredResult') as Count
Team Foundation Server (TFS) from Microsoft. 
i don't like how you indiscriminately toss a DISTINCT into your "much better approach" -- apparently necessitated because you **multiply** the summed returns and purchases by every transaction!! insane!! and your "more simply" solution involves **cross-joining** each transaction for a store against every return for that store, and then cross-joining those to every purchase for that store -- which is, how shall i put this, spectacularly wrong 
Nice article, thanks for the link. I'm pretty much brand new to using SQL at my new job, so I'm constantly looking for more new stuff to learn, comprehend and be aware of. I'm currently practicing on Khan academy, using the free Stanford classes and their videos (you know, the ones with the professor that has the most soothing voice ever). My question to you all is thusly: what is the best initial certification I can obtain for SQL? Hopefully it includes training as well and costs less than $1,000 USD? Thanks everyone. 
[removed]
It isn't indiscriminate at all. You're no longer grouping and should be using a distinct. I personally avoid using case statements to derive two integers *and* do something to them whenever possible because they can be bastards for performance. The second example would probably be my preference, or using a CTE to do each count and then join the two parts together such as with x as (), y as () select a/b from x inner join y &gt; which is, how shall i put this, spectacularly wrong Spectacularly wrong until you work with a large transactional table or start trying to use that case logic within sub-queries of its own. 
http://stackoverflow.com/questions/12875368/subquery-total-performance-vs-case-sum-performance
We have a Git option and team foundation for Git as approved options and was one that we were going to actively explore. May have to go ahead and start the process to get access to this and try it out. Gotta love beaurocracy!
By the way, that isn't a cross-join. It's a self-join. https://www.youtube.com/watch?v=hOOkdwuFwWE https://www.youtube.com/watch?v=0V4SJz5BHv4
Your date is a timestamp, you need to cast it to a date to get what you're after. `... WHERE date_joined::date &gt;= 'dateyouwant';`
Agreed. Use sys.tables instead.
I came here to say TFS as well. It's literally designed to compliment SSIS, SSRS, SSAS, SSMS, and .NET
Still not following you at all. A cross join is a join without a condition, this specificially has conditions. It should work. 
Yes, you've said that. Please explain why it won't work.
Is that all within the same query? I guess I can set it separately.
There are a number of sample tests online to give you an idea of what the tests are like. I finished my masters in database systems and studied for 3 months with the Oracle study book, and I found them to be very challenging. Not just the content, but each test took me almost the entire test time. For me, I had to be certified for my job. Additionally, they are pretty expensive, so you want to be prepared so you don't have a re-take. This is not to discourage you on your path. Everyone is different about tests, test taking, and recall. With the Admin test, it's helpful to have actually done admin tasks. Recover/restore/backup, change setting, etc. 
Thanks for this. I think I understand what this is meant to be doing, but I'm struggling to integrate it. Rookie here. Should I be changing my inner join to match the UN_IDs instead? EDIT: Also, table4 doesn't have the JOU_ID
&gt; It should work. ha ha ha ha ha i invite you to test it
In the platform you're using all dates have a time component. When the time isn't explicitly given for a date, it is assumed to be 00:00:00 (midnight.) So when you do: WHERE date_joined &gt; '2015-12-20' What you're really saying is: WHERE date_joined &gt; '2015-12-20 00:00:00' Most of 2015-12-20 falls in this range (except for midnight.) This also explains why = doesn't do what you expected.
in case you missed my earlier comment that your FROM clause produces 54 rows, here's your query with the SUMs and GROUP BY removed, so that you can see the rows produced by your FROM clause -- select a.storenumber , b.amount AS pur , c.amount AS ret from transactions a inner join transactions b on b.storenumber = a.storenumber and b.type = 'ret' inner join transactions c on c.storenumber = a.storenumber and c.type = 'pur' -- note: fixed typo order by a.storenumber storenumber pur ret 22 7 7 22 7 7 22 7 7 22 7 8 22 7 8 22 7 8 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 33 3 5 
Can you explain why? And no, I haven't, I wrote this from home but I will on Monday. Can you be civil. This isn't /r/gaming, we're all just trying to learn. Stop being a cunt.
&gt; Can you explain why? can i explain why your FROM clause is blowing up? no, man, you need to figure that out for yourself just scroll back, and try to understand my posts where i mentioned how you're cross-joining purchases and returns within each store 
Exactly what have you tried so far? 
Can you show the tables and column names?
Yes, you mentioned I was cross joining and I looked it up and see that a cross join is a join without any conditions... this is an inner join (self join) which according to the link I gave you is different. So... you're kind of useless. I'll test on Monday but I don't understand what you're here for. Everyone else seems to be trying to learn.
i'm useless? you refuse to even ~try~ to understand what i've been saying everyone else **except you** is trying to learn store 22 has 3 rows, 2 purchases and 1 return -- your joins return **6** rows store 33 has 6 rows, 4 purchases and 2 returns -- your joins return **48** rows obviously the dictionary definition of a cross join as having no conditions isn't going to explain what's happening here you are getting **cross products** *within each store* okay, i can understand that you don't have a desktop server and need to test this in your office but a good sql developer can easily **desk check** a faulty join when presented with sample data of 3 and 6 rows that you continue to fail to even think about your joins being spectacularly wrong says a lot about you, far more than me calling you a cunt
Clearly I've tried enough to Google what you said and provide links which dispute what you're saying. &gt;but a good sql developer can easily desk check a faulty join when presented with sample data of 3 and 6 rows Look at my name, bro. I'm not a good sql developer. I'm here to learn. I can understand what cross products are, but if you say I'm getting them because I'm cross joining and I'm not then it confuses the shit out of me.
Yes, I believe so.
You can only ORDER BY on colums (or aliases!) in the SELECT list b/c the ordering is applied at the end. You can read more about that here: http://oracle.readthedocs.org/en/latest/sql/basics/query-processing-order.html. 
okay, let me try it again start with this sample data -- SELECT a.storenumber , a.type , a.amount FROM transactions AS a storenumber type amount 22 pur 7 22 pur 8 22 ret 7 33 pur 5 33 pur 5 33 pur 5 33 pur 5 33 ret 3 33 ret 3 next, do the first of your joins -- SELECT a.storenumber , a.type AS atype , a.amount AS aamount , b.type AS btype , b.amount AS bamount FROM transactions AS a INNER JOIN transactions AS b ON b.storenumber = a.storenumber AND b.type = 'pur' storenumber atype aamount btype bamount 22 pur 7 pur 7 22 pur 8 pur 7 22 ret 7 pur 7 22 pur 7 pur 8 22 pur 8 pur 8 22 ret 7 pur 8 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 ret 3 pur 5 33 ret 3 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 ret 3 pur 5 33 ret 3 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 ret 3 pur 5 33 ret 3 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 pur 5 pur 5 33 ret 3 pur 5 33 ret 3 pur 5 finally, extend this to your second join -- SELECT a.storenumber , a.type AS atype , a.amount AS aamount , b.type AS btype , b.amount AS bamount , c.type AS ctype , c.amount AS camount FROM transactions AS a INNER JOIN transactions AS b ON b.storenumber = a.storenumber AND b.type = 'pur' INNER JOIN transactions AS c ON c.storenumber = a.storenumber AND c.type = 'ret' storenumber atype aamount btype bamount ctype camount 22 pur 7 pur 7 ret 7 22 pur 8 pur 7 ret 7 22 ret 7 pur 7 ret 7 22 pur 7 pur 8 ret 7 22 pur 8 pur 8 ret 7 22 ret 7 pur 8 ret 7 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 pur 5 pur 5 ret 3 33 ret 3 pur 5 ret 3 33 ret 3 pur 5 ret 3 if you were now to apply a SUM function to this data, even with a GROUP BY on storenumber, those totals would be wrong you've produced cross products -- transactions X purchases X returns *within each store* it's not a complete cross join, rather, it exhibits cross join effects 
Without seeing the table structure or schemas it is hard to given concrete advice. However, looking at the questions I get the feeling this would have a lot to do with joins. If you read about how joins work, it should give you a better chance at solving the questions. While on the topic of joins, try to develop a habit of using good aliases for your tables. Such as using OD for a table called OrderDetails, or cust for a table called customers. It will make it easier to follow you code instead of using single character aliases (ie a,b,c etc). Something I like doing with my joins is comment why I need this table, which has come in handy when troubleshooting something I wrote a year or so ago. 
I'm with you, but why isn't that solved by a simple distinct, the sum on pur and ret are being done separately and should be equal for each row where storenumber = storenumber, no?
You want an outer join rather than an inner join. If this is a homework problem. Please state what you've done to get you where you are now and what hasn't worked that you tried.
You can achieve this with triggers. More specifically an [after insert trigger](http://www.techonthenet.com/oracle/triggers/after_insert.php) 
Generally speaking you do NOT want to put your production ODS (operational database system) on the same hardware as your data warehouse for a number of reasons. The biggest problem is that a poor query of the data warehouse could impact your production. Best bet is 1 server for ODS, and 1 server for data warehousing and hot backup of ODS. Define all connections to the ODS with SQL aliases, then set up a fail over to the DW server. If this is a new project, I would look at maxing out the memory on the servers, cut back on processor cores, and get the fastest processors you can. This will cut down on licensing costs, and if you use SQL 2016 you can use virtual tables to ensure performance. 
How do you mean "automatically"? In your specific example, let's say a customer record is created, how would a DB engine know what address should the customer have, unless a client application submits another record with the data? Or did you mean that the address record sent by the client application would need to be attached to the just created Customer? In the latter case, depending on the DB engine there are various ways to get to the latest identity/sequence generated number. 
Thanks for sharing. In a year or so when I have more experience under my belt I'll be hunting hard for a company like yours. What's a day to day look like for you? What skills would I need to be hired on your team?
Everyone saying SSRS here is right.
There's also Report Builder. Not as full-featured as BIDS/Visual Studio, but will get the job done and produces RDL files that you can deploy to an SSRS server all the same.
So give up trying to find this role then? Or accept the information you've been given from experienced people who have answered your question and use this in your search.
should that not be A.Date = CONVERT(varchar,B.TS,112) Also - please tell us what platform you're on!
Time to start learning then. 
Please don't cross post without telling people. You already asked elsewhere. http://reddit.com/r/sqlite/comments/4e8eg0/whenever_a_record_is_created_in_a_table_i_want_a/d1y0p6e
Uhhh.. You hiring?
I can't think of a less useful metric than school and gpa.
Well you have to meet certain standards to get your shot so, it is what it is. People like to say GPA doesnt matter, most of them probably had a bad GPA. Sure seems to show what kind of person you were in college at least. My fiance just graduated...kids with lower GPA usually werent less smart they were just less mature or just lazy, it was pretty accurate for those kinds of things.
Didn't include the RDBMS either. 
His cross-post was in /r/sqlite, if that helps.
Noticed that. :) Just saying without your keen eye it wouldn't have been available. 
If you want a row for every UN_ID whether the count is greater than 0 or not, then do something like SELECT Master.UN_ID , ( SELECT COUNT(*) FROM TABLE2 INNER JOIN TABLE1 ON TABLE2.JOU_ID = TABLE1.JOU_ID WHERE TABLE1.JOU_ID IN ( SELECT TABLE3.JOU_ID FROM TABLE3 WHERE JOU_DATE = ? AND STATUS = 'NEW' ) AND Master.UN_ID = TABLE2.UN_ID AND TABLE1.UN_ID like 'VID%' ) FROM TABLE4 AS Master GROUP BY UN_ID ORDER BY UN_ID
So sec0811_employees has a foreign key reference to sec0811_departments? Let's assume that field is named department_fk and it refers to the department_pk field in the sec0811_departments table. An update statement to the departments table that would fail would be anything where you change a department_pk that is referenced in the employees table. So, you could do: update sec0811_departments set department_pk = department_pk + 1 where department_pk in (select distinct department_fk from sec0811_employees) A delete statement you could do would be: delete from sec0811_departments where department_pk in (select distinct department_fk from sec0811_employees)
Thank you. This works. Could you explain like I'm five what "casting" does? 
So I still don't understand. Table1 contains all distinct storenumbers. Table2 is a subquery of Table1 that sums all purchases by distinct storenumber. Table3 is a subquery of Table1 that sums all returns by distinct storenumber. Shouldn't the returning `SELECT DISTINCT` provide (1) store number with the same number of purchases and returns? So by selecting distinct would it not give me the distinct from A, B, and C, where B and C always have the same sums?
i believe you may be referencing the first "much better approach") of your two queries in [this post](https://www.reddit.com/r/SQL/comments/4dx4dk/mysql_sum_and_divide_function_of_2_values_in_1_col/d1vicz7) your error there is in returning one row for every transaction (a), but only if that store has both purchases (b) and returns (c) -- as an aside, any store that has no returns will be dropped, so you should be using outer joins and since you have one row for every transaction, you need DISTINCT to collapse them hugely inefficient, but at least you get the right answer my many posts in this thread have been directed at the second ("more simply") of the two queries in your post, which is not only grossly inefficient but produces incorrect results 
Ok, cool. That makes sense. I appreciate it!
I will tomorrow at work. How is this not solved by the join condition = store number AND = type?
Ive been confused as shit this whole time thinking you were saying example 1 was wrong and example 2 was correct, when it seems you've been saying example 1 is correct and 2 is wrong. Which I can wrap my head around.
&gt; it seems you've been saying example 1 is correct and 2 is wrong. it not only seems that way, it is that way example 1 is inefficient but at least gives the right answer example 2 is grossly inefficient and wrong 
For the `create` statements, you can script out the database schema. With SQLite this is `.schema` inside the shell. In SQL Server, you can do it with a right-click on the database and select the appropriate menu item. As for all of the `insert` statements, unless you've been logging them all along, you can't really. You could script out the tables such that you get `insert` statements for all of your data; this could get huge. This seems like kind of an odd request to me; the `insert` statements don't really prove much about what you've done, and scripting out the schema doesn't tell your instructor much about *how* you arrived at the current schema (did you use SQL statements, a GUI designer, etc. - and if you're supposed to be using SQL statements, why not ask for access to your source code repository?). If they're looking for a complete copy of your database for evaluation purposes, taking a backup using your RDBMS's backup methods would be preferable over rebuilding from your queries.
You might get lucky depending on the tools you're using. If you happen to use MySQL through a Linux shell, it will usually create a `.mysql_history` file in your home directory. Barring that, check whatever interface you use and see if it has some default logging turned on. I'm pretty sure HeidiSQL logs by default. I don't think SQL Server Management Studio does any logging, SSMS Tools Pack handles that for me.
We're not a huge company, but we do process a tonne of data. Our prod was reduced to a 30 day purge for performance purposes, and the DWH sits there for historical and reporting purposes - it isn't used as a fail over. At this point, I'm more after any industry best practices relating to it. I know I can argue enough for why we shouldn't consolidate them, however I also know that being able to provide literature from those more knowledgeable than I will carry a lot more weight. 
This is my concern. We trimmed our prod to 30 days for performance increases. I just don't see it being a good idea to house our DWH off the same hardware, which will also then be doing all of our reporting, etc. 
All MS. Contractually bound. 
If this is for a project and you hand typed the inserts, you'll have to redo that work but save it. You don't provide details, but it seems like you are trying to submit SQL that will build a small database and populate it with some sample info? Is this the case or is there something you've left out?
Thanks for your advice. I know some ruby on rails and JavaScript, but it's too junior to be able to leverage it into job interviews just yet. I guess I'll develop those skills and take a look in a year or two. Shame though, I would rather be working as a dba. 
I didn't say that Position A at Company B could be had as remote. I meant that you can eventually find a remote position for what you want to do. But, it will take some patience.
Thank you so much. was able to finally finish the chapter. cheers.
&gt; My question to you all is thusly: what is the best initial certification I can obtain for SQL? IMO, you're thinking about this incorrectly - because I used to think like this. You really should be focusing on what you need to know to do what you need to do. Certification is not a substitute for experience. Unless you have a photographic memory, you're probably not going to be able to perform during a cert exam. Even with several years experience, its still a challenge to pass an exam.
Well what's the purpose of this? If you're passing this into the front-end it'll be much easier to just let them handle it.
check out the book 'Sams teach youself SQL in 10 minutes' each chapter is supposed to take you 10 minutes to read and learn a concept. its a decent read to get a nice foundation. but i think a class would be best since they would provide labs.
I agree. Patience will be the name of the game then - thanks!
I'd not bother with 'in person training', unless you're into people reading books to you for a few grand... If you want to spend a few bucks you're better off with a subscription to something like [Pluralsight](http://www.pluralsight.com/), or [Safari](https://www.safaribooksonline.com/), and there's also plenty of courses on [udemy](https://www.udemy.com/courses/search/?q=sql&amp;src=ukw&amp;lang=en) if you're looking for something more piecemeal. There's also lots of free stuff available as well: [udacity](https://www.udacity.com/course/intro-to-relational-databases--ud197) [Stanford](https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about) [EDX](https://www.edx.org/course/querying-transact-sql-microsoft-dat201x-1) I have no experience with Codeacademy Pro, though I have never heard anything bad about them. My experience with paid learning has been that so long as you're able to keep at it the online offerings are very much worth it, and it's a fraction of the price of in-class training.
The SQL exercises up on codecademy pro are extremely basic. A book would be far better. The exercises even with a pro subscription can be completed in such a short time they're good for supplementing other learning and getting some time dicking around with SQL, but they're not good as an education in SQL in and of itself.
Learn SQL The Hard Way by Zed Shaw (of Learn Python The Hard Way)
Thanks. I guess this would be goal 1b. whereas learning what I need to do at work is goal 1a. just trying to pad my resume and knowledge as part of my 3 year plan.
Thanks, that actually what I was looking for.
Are the books you refer to in A and B actual titles or generic types of books? And are there any online resources that are similar to these?
You've got to at least give it a try. We're not going to do your homework for you. 
One vital element missing. What DBMS? What have you attempted so we have a clue what your knowledge level is? After all if we use SQL that's completely outside of what you've been taught, you may get the right answer but your professor may not give you full marks because you're using the wrong method.
A trigger would indeed be the method I'd use for this. Is rent request only populated on the completion of the rental process? (Otherwise you need to build something in that reverses the update if the customer cancels the rental before it's final) Is the employee also marking the rental as fulfilled? If so I would actually make it an update trigger that updates the mater table with "Not Available" for all items in the order when the order status is updated to Fulfilled. Though walking through the process in my mind I think you actually need both triggers. First, you need the master table to have at least three order related statuses. Available, Ordered and Not Available (or Rented) When the customer places their order, an insert trigger on the Rent_Request table updates the status of each item to Ordered. When the employee gathers the order and packages it up for the customer, they update the order to fulfilled and an update trigger updates the MasterTable to Not Available or Rented. That way you can stop showing the item as Available for other customers from the moment that a given customer places their order for the item. 
Don't be so hostile, homie. I just wanted to make sure you tried first... SELECT C.CUS_AREACODE ,COUNT (C.CUS_AREACODE) as AREA_CODE_COUNT FROM CUSTOMER C WHERE C.CUS_AREACODE LIKE '615' GROUP BY C.CUS_AREACODE;
&gt; SELECT CUS_AREACODE FROM CUSTOMER WHERE CUS_AREACODE = 615 GROUP BY CUS_AREACODE SELECT CUS_AREACODE, COUNT(*) FROM CUSTOMER -- WHERE CUS_AREACODE = 615 -- You want all area codes so you don't actually have any where clause. GROUP BY CUS_AREACODE
Formatted for sanity purposes: DECLARE @Variance TABLE ( ProdID INT NOT NULL PRIMARY KEY IDENTITY, ProdDate date, PartNo INT, PartName nvarchar(50), LotNo INT, DieCastGoodQTY INT, TrimShopGoodQTY INT, Variance INT ) SELECT pd.ProdId, pd.ProdDate, p.PartNo, p.PartName, SUBSTRING((SELECT DISTINCT ' ' + ld.LotNo FROM MPP_Prod.dbo.LotData ld WHERE pd.ProdID = ld.ProdID FOR XML PATH('')), 2, 100 ) AS 'Lot Number', pd.GoodQty, SUBSTRING((SELECT DISTINCT ' ' + SUM(ld.Quantity) FROM MPP_Prod.dbo.LotData ld WHERE pd.ProdID = ld.ProdID FOR XML PATH('')), 1, 500) AS 'Trim Shop' FROM MPP_Prod.dbo.ProdData pd INNER JOIN MPP_Prod.dbo.Parts p ON pd.PartId = p.PartId INNER JOIN MPP_Prod.dbo.LotData ld ON pd.ProdID = ld.ProdID WHERE pd.GoodQty &lt;&gt; 0 AND pd.ProdDate &gt; 2015 - 09 - 01 GROUP BY pd.ProdID, pd.ProdDate, p.PartNo, p.PartName, ld.LotNo, pd.GoodQty, pd.NoGoodQty, ld.Quantity ORDER BY ProdID DESC
bds.Market and tbdov.Market should both be text entries. Those are names of regions. There might very well be bad data in the Date column. Without pulling the whole column and going through it, I wouldn't know. I've dropped the year from the GroupBy statement, and am running the query again to see if the problem is there. EDIT: Nope, the same error.
You are correct and the best solution is WHERE COALESCE(quotelog.assignedTo,-1) = COALESCE(userID,quotelog.assignedTo,-1) AND (q1.createdAt BETWEEN datebegin AND dateend) where number like -1 should be invalid userId, because I want in case of null to return all quotes for all users not with one that have null or specific user
And did that give you any errors when you ran that code? It looks like it would accomplish what you're trying to do but I don't have an instance of MSSQL handy I can try it with to seek out less obvious syntax issues. 
You can call the same table twice, you just need to alias it. I don't know your table structure but you'll want something like below; SELECT inv.Product_ID owner.Employee_ID Owner_Employee_ID sign.Employee_ID SignedOut_Employee_ID FROM inventory AS inv JOIN employee AS owner ON inv.Owner_ID = owner.Employee_ID JOIN employee AS sign ON inv.SignedOutBy_ID = sign.Employee_ID You'll obviously want to include any fields that you need but this is the way that you'll want to join the employee table twice using different fields.
If you are going to convert non-numeric fields to integer, then use explicit casts. This will not only more likely surface which one is bad, but it helps other people when reading your code. Rule of thumb - don't use implicit data type conversions.
In Oracle you can use TO_CHAR http://www.techonthenet.com/oracle/functions/to_char.php I don't know about other DB software though.
Can you elaborate a bit? Is the string you're showing how python is trying to fill out the database, or is that what you see in the database after? i.e. is python performing INSERT INTO workn00bsTable W ( ADateField) VALUES ('2016-04-11') and that's returning an error, OR is python succeeding in it's part then you're selecting from the database: SELECT ADateField FROM workn00bsTable W And that's returning '2016-04-11'?? 
Try this with parenthesis SELECT "Name", "Vorname", "PersonalNr" FROM "Angestellte" NATURAL JOIN "HaeltBetreut" WHERE "Typ" = 'Assistent' AND ("VorlesungsNr" = '121' OR "VorlesungsNr" = '123' OR "VorlesungsNr" = '127')
Smallest change: SELECT "Name", "Vorname", "PersonalNr" FROM "Angestellte" NATURAL JOIN "HaeltBetreut" WHERE "Typ" = 'Assistent' AND ("VorlesungsNr" = '121' -- without parenthesis you will get OR "VorlesungsNr" = '123' -- the extra rows you see OR "VorlesungsNr" = '127') More efficient and clear: SELECT "Name", "Vorname", "PersonalNr" FROM "Angestellte" NATURAL JOIN "HaeltBetreut" WHERE "Typ" = 'Assistent' AND "VorlesungsNr" in ( '121' , '123', '127') Edit: had an extra opening paren.
There is more than one way to "work remotely". I remember one occasion when we were on-site at client A during the day, then in the evening we were at the hotel. The boss was VPNed into client B and I was VPNed into client C. Before knocking off we both VPNed into client A. Back at the office we could VPN into any and all of our clients. During any day I could be logged into clients coast to coast. I think what you are talking about is working from home. 
As in, exactly that string ends up in the tables? If I were using TSQL I'd try doing a SUBSTRING (or two, to get the right hand quotation mark) and then CASTing it as DATETIME. That may well be wrong/horrible.
Sorry about that. And thank you for the help! 
Ok, time to really simplify and if that gets a good result change one thing at a time till it appears again. What happens if you execute SELECT tbdov.Market, COUNT(*) FROM [dbo].[TableauBigDealOfferViewsByDayByZone] AS tbdov WITH (NOLOCK) INNER JOIN BigDealSubscribers AS bds WITH (NOLOCK) ON tbdov.Market = bds.Market WHERE tbdov.Date BETWEEN '2013-01-01' AND '2015-12-31' -- can be dangerous if tbdov is actually datetime GROUP BY tbdov.Market
Probably not what you're looking for, but I'm a tech consultant. When I'm not travelling and I don't have any face to face meetings (more often than you might think, for now) I'm free to work from wherever.
Yes to both. You're selecting one field, where that field is equal to one value, and then grouping by that field. That means one result.
[Apparently so](https://www.sqlite.org/lang_createtrigger.html)
&gt; SELECT Date &gt; ,ISDATE(Date) &gt; FROM table &gt; WHERE ISDATE(Date)=0 Running it now. EDIT: No results.
It's interesting that it worked as a separate query. Can you share what that query was? The fact it worked without the join makes it seem like Market is somehow the thing being converted which seems very strange if they are both strings as you described.
DBA jobs are far less plentiful in general than developer jobs. Most companies I've worked for/with have a ratio in the neighborhood of 20:1 developers to DBAs. Many don't have any DBAs at all. From my experience, if you want to be doing more SQL work, focus on back end development.
Ahhh.. okay and did the other query still have the datepart in the where clause? (I'm figuring if you go that to work, it eliminates that as the problem)
I had a list of years and a list of months, crossing them gave me all months in all years which was useful at the time 
Did you try the second thing? Because that's about all I can think of. There shouldn't be anything wrong with BETWEEN 2013 AND 2015 because DatePart should return an integer. Possibly try casting those as ints, but they should be anyway. Or just comment out the WHERE and GROUP BY clauses if you haven't tried that already.
&gt; For example, if the default lay out of my table's columns is: lastname, firstname, and DOB, wouldn't new data need to be entered in that exact order? Yes and no. If you want to avoid specifying columns in your insert, then yes. If you specify them, then no. INSERT INTO table_name VALUES ('Doe', 'John', '1/1/2016', 'Aloysius' ) Could just as easily be INSERT INTO table_name ( lastname, firstname, DOB, middlename ) VALUES ( 'Doe', 'John', '1/1/2016', 'Aloysius') or INSERT INTO table_name ( firstname, middlename , lastname, DOB) VALUES ( 'John', 'Aloysius', 'Doe', '1/1/2016') Edit: specifying column names in your insert is a best practice so you're not screwed over with a silent error if someone adds a column in the middle of a table by doing a drop/create.
When I was populating a table of IP4 addresses where I had a hard code of the first 3 parts and wanted the full range of the last part. I made a temp table of string populated with 000 to 255 and inserted into my table using a cross join between the table that had the hard coded beginning '124.200.025.' concatenated with my temp table. That made files that could be sucked into the routers for the firewall.
It may not actually be a permissions problem. Look in the SQL log for the real error. Have you run a backup of the DB so that the logfile CAN shrink?
There's a number of operations where cross-joins come in handy: * unpivot-ing ( going from (ID,A,B,..) tuple to {(ID,A),(ID,B),...}) * base dimension data for cross-tabbing in the absence of metric data (e.g. # of orders by department by year, even if there are years a department didn't have orders or a department with no orders whatsoever) (probably /u/user_naem's example) * converting ranges to individual values (/u/D_W_Hunter example) * singleton aka single-row joins (table/result joined is guaranteed to have no more than a single row)
you can use case where case when userID is null then q1.createdAt BETWEEN datebegin AND dateend else quotelog.assignedTo = userID AND (q1.createdAt BETWEEN datebegin AND dateend) end 
Adding to this; I had to cross join a date range with a department list, then join to a sales table so I could replace nulls with 0, rather than just omitting the date/department combo. Though I admit, I was not exactly happy with the workaround, but it did work.
I use them for connecting tables. I have two tables site and project connected by project_tosite table. I sometimes get requests to insert new projects into the database that are available in all sites. I then need to use a cross join to connect all the needed keys. insert project_tosite (project_id, site_id) from project cross join site Thats the only use i have actually used them for
One that I use often is for report that needs to be copied multiple times, like a copy for the Factory, another for the Administration and another for the Customer, I have a "Copies" table and cross join it with the report base, it allows me to make small changes for specific copies and still use the same report base.
Are you logging as the Administrator of a Windows domain or Administrator of the DB? Maybe the domain admin user doesn't have the needed permission for that operation. In Security -&gt; Logins -&gt; right click the Administrator user -&gt; Properties -&gt; Server Roles: check is the sysadmin is checked
Are you **sure** Market is properly defined as a string in both tables? Edit.. try that same sql, but comment out the where. That way the *only* equality will be the join.
Let me partially take what I said back. I learned a lot by studying for certification - to the point where I started doing more and better. So, I can't say it will be all for naught. But, I still would need to go through near memory drills to remember the syntax required to pass the exam.
I would definitely try batching it into at least 4 batches of data (if not more). This is how I insert/delete most of my bulk records. If your IDENTITY column is consistent, then divide the max ID by 4 and split your INSERT using something like WHERE ID &lt; x, with X being the MAX ID of each batch. This is simple enough to do with a while loop if needed. Also, if you can hold off on adding the indexes to your destination table until you're done loading the data, this will help too. After you're done you can create the indexes and then rebuild them.
You don't need to be a programmer to learn SQL. Everything you need to get started is online. Try Google (yes, I'm that guy): http://lmgtfy.com/?q=SQL+Basics 
&gt; sqlite That's the piece we needed. [Here's an answer from stackoverflow](http://stackoverflow.com/questions/4428795/sqlite-convert-string-to-date). Edit: For the record/future reference, I found this by googling "sqlite convert string to date" -- it's the first result.
 select salary, column_2, column_3, column_4 from tablename sort by salary desc You don't need the * when specifying column names. Group by is a totally different feature that aggregates like values together. You have wrap column names with aggregate functions when using group by.
Order by, not group by
Thanks but I get an error saying there's a syntax error in the FROM clause. 
that worked, thank you. /u/gusgizmo this one worked, but thanks to you both. what is the difference between order by and sort by? since the former work where as the latter did not.
what is an aggregate function?
There is no sort by clause, only order by.
I had a brainfart, there is no sort by clause.
Elaborating a bit... Specifying column names in just about everything is best practice, not just inserts.
How big of a load? &gt; The hold up is building the indexes which uses about 75% of the execution plan(just an identity prime key column) Rebuilding non-clustered indexes isn't bad after a very large load. But an "identity prim**ary** key" rebuild throws up red flags. You can't rebuild a primary key, it can be dropped and recreated. The primary key is a unique identity. Primary key columns are usually the clustered index as well. Clustered indexes are the physical order in which the data is stored on disk. Dropping and creating the clustered index will basically rebuild the entirety of the table. Don't drop/create primary keys, don't drop/create clustered indexes. Usually when doing very large loads you will bulk load into a table without a primary key/clustered index or you'll bulk load into a table with a simple incrementing identifier that is also the clustered index as either solutions write sequentially and very fast to disk. This is the fastest way to get data into the SQL server. Your next step will be to move the data from the staging table to the destination table ordering by the clustered index on the destination table. This will ensure your initial load does not fragment the clustered index on the table (sql optimizer may order this automatically). If you are going to rebuild the indexes on the destination table after the initial load it would be faster to just drop the indexes prior to the load and recreate them after after the load finishes. Basically, if you don't, it'll be attempting to maintain the indexes and then it'll undo all that work when it rebuilds them anyways. I wouldn't do this in the incremental loads, with the incremental loads I would run a script like [Ola's scripts](https://ola.hallengren.com/sql-server-index-and-statistics-maintenance.html) for determining the fragmentation on the indexes after they load and rebuild/reorganize as necessary. &gt; Our current system has only one environment for both DEV/QA/UAT/PRD so I am concerned about the integrity of the server. It really comes down to honestly how much data you have. 
this video is from this free online lesson software found here: https://www.khanacademy.org/computing/computer-programming/sql/sql-basics/v/welcome-to-sql
My bad , finally whew formatted it :)
here you can read about SQL Server Backup and there restoration processes. http://www.sqlserverlogexplorer.com/backup-and-restore-strategy-in-sql-server/
There aren't a great many use cases (90% of joins are probably LEFT OUTER or INNER)... but if CROSS JOIN didn't exist and one of those uses cases came up, you'd probably end up spending a lot of time writing hilariously inefficient script.
 SELECT UID, COUNT(group) AS CountGroup FROM Table GROUP BY UID HAVING COUNT(group) &gt; 1 That will bring all the UID that has more than 1 group, with your 2nd example it would return: UID|CountGroup :--|:-- 100|2
when i try this I get this reply "Msg 201, Level 16, State 4, Procedure sp_report, Line 0 Procedure or function 'sp_report' expects parameter '@studID', which was not supplied."
It looks like /u/aburger already found the answer for you. DATE( '2016-04-11') should get you an SQLite date so when Python inserts that date it needs to insert using that date function to convert the string. For all future posts, make sure that you have put the DBMS in the title of your post. That way your readers are already considering the peculiarities of that particular DBMS when they read your post.
Also.. note for future posts. Post your dbms (MSAaccess) in the title and indent your code by 4 spaces and it will retain it's formatting. SELECT column_1 , column_2 , column_3 , salary FROM tablename ORDER BY salary asc;
&gt; In this scenario I want to count all of UID 100's actions as target, discounting them from control, due to the contamination. Is there a general rule for how you want to handle contaminated UIDs? Also, you describe the situation as each UID as belonging to one of you groups, 'control' and 'target', then immediately list what you expect to see that includes a group of 'exposed' which is neither. Can you add why this is the case to your explanation? Also, when posting it is helpful to include the DBMS you are working in as part to the title. Since you haven't done that (and titles cannot be edited) can you mention that now? 
Why not just add an additional JOIN for the outer FROM and have the join condition be the BETWEEN the date conditions. Utilizing cursors for this is an absolutely terrible idea as they will be slow and would force the same query to be ran repeatedly/unnecessarily. Additionally if you want to pull multiple quarters/etc it is much easier to do so. 
Just set the maximum commit size to about 1000. You are likrly reading the plan wrongly, generating identity columns is unlikely to cost much. If you have lots of secondary indexes then that might add *some* overhead. Edit: obligatory wtf regarding having no seperate prod environment! 
I had tried joining my run_dates to the outer FROM initially, but then I wasn't able to pass them down to the subqueries that actually determine the reporting periods. The solution I came up with was to join the run dates on the innermost FROM, where the period dates are determined and then add the R12_Start_Date to my outer GROUP BY statement. The final query is: SELECT customer_nkey, period.R12_Start_Date, period.R12_End_date, period.Year_C445 Year_C445, period.Quarter_C445 Quarter_C445, DATEDIFF(period.R12_End_date,max(order_date)) Recency, count(distinct oh_nkey) number_orders, count(distinct oh_nkey)/12 frequency, sum(ol_amt) ol_amt, sum(case when ol_status in (2048, 4096) then 0 else ol_amt end) net_spend, sum(ol_amt) / count(distinct oh_nkey) AOV FROM (SELECT run.rdate, DATE_ADD(max(d3.Last_Dt_In_Mer_Quarter_C445), INTERVAL 1 DAY) R12_Start_Date, d.Last_Dt_In_Mer_Quarter_C445 R12_End_date, d.Year_C445, d.Quarter_C445 FROM (SELECT distinct Actual_Date rdate FROM moda_dw.dim_date WHERE Month_Num in (4, 7, 10, 1) and Day_Of_Month = 15 and Actual_Date &lt; CAST('2016-12-01' as Date)) run LEFT JOIN moda_dw.dim_date d on (run.rdate &gt; d.Actual_Date and d.Last_Dt_In_Mer_Quarter_C445 &lt; run.rdate) LEFT JOIN moda_dw.dim_date d2 on d2.Actual_Date &gt; d.Actual_Date and (run.rdate &gt; d2.Actual_Date and d2.Last_Dt_In_Mer_Quarter_C445 &lt; run.rdate) LEFT JOIN moda_dw.dim_date d3 on d3.Quarter_C445 = d.Quarter_C445 and d3.Year_C445 = d.Year_C445 - 1 WHERE d2.Actual_Date is null GROUP BY run.rdate HAVING R12_Start_Date is not null) period LEFT JOIN moda_dw.fact_order_line fol on fol.order_date &gt; R12_Start_Date and fol.order_date &lt; R12_End_date GROUP BY customer_nkey, R12_Start_Date ;
Dumb question but whats DMBS EDIT: Data base managment system my bad
The text is your format string. So if you wanted a date like 2016-04-13 you would do to_char(date_value, 'YYYY-MM-DD')
Totally fine. Do yourself a favour though and get in the habit of wrapping OR in paretheses
Looks like this worked! Thanks bud!
Thanks for the breakdown, much appreciated :)
I found a more efficient solution to my problem but thanks for your post! Now I know how to use a cursor if I need to in future. 
As a follow up, I have another inquiry I was hoping to receive some feedback on. I'm curious to know if there is a reverse solution for the weird date formula roveo was kind enough to produce a few months back? where weird_date = chr((year(now()) // 10 - 200)::int + 65) || right(year(now())::varchar, 1) I needed to convert a CHAR B6 year to read DATE 16. Now I need to do the opposite. Take a current DATE 16 and convert it to CHAR B6. Thanks again! 
 SELECT e.* , v.* FROM ExtraTable e RIGHT JOIN ##visitors v ON ( v.policy = e.id1 or v.policy = e.id2) Potential issues... right joins can make a mess for later people maintaining the SQL because we're so used to left joins. With a right join your output will have a row for every row in V, but only a row with E when there's a match to id1 or id2 More often you'll see that written as SELECT e.*, v.* FROM ##visitors v LEFT JOIN ExtraTable e ON ( v.policy = e.id1 or v.policy = e.id2) Because we expect the main driver of the query to be listed first. Example CREATE TABLE Extratable ( id1 int , id2 int , tkey int ); CREATE TABLE visitors ( policy int , tkey int ); INSERT INTO Extratable ( id1, id2, tkey) VALUES ( 1, 1, 1); INSERT INTO Extratable ( id1, id2, tkey) VALUES ( 1, 2, 2); INSERT INTO Extratable ( id1, id2, tkey) VALUES ( 3, 4, 3); INSERT INTO visitors ( policy, tkey) VALUES ( 1, 1); INSERT INTO visitors ( policy, tkey) VALUES ( 2, 2); INSERT INTO visitors ( policy, tkey) VALUES ( 5, 3); INSERT INTO visitors ( policy, tkey) VALUES ( 6, 4); -- as currently written SELECT e.tkey AS e_tKey , v.tkey AS v_tKey FROM Extratable e RIGHT JOIN visitors v ON ( v.policy = e.id1 or v.policy = e.id2); | E key | V key | |--------|-------| | 1 | 1 | | 2 | 1 | -- if e matches to two policies, you get extra rows. | 2 | 2 | | (null) | 3 | | (null) | 4 | -- Is the same as the more common Left join SELECT e.tkey AS e_tKey , v.tkey AS v_tkey FROM visitors v LEFT JOIN Extratable e ON ( v.policy = e.id1 or v.policy = e.id2); | E key | V key | |--------|-------| | 1 | 1 | | 2 | 1 | | 2 | 2 | | (null) | 3 | | (null) | 4 | 
The first problem I am seeing here is that your table VIX_DATA has a column that's named Date and contains a Date. THAT'S BAD DESIGN. Really. It's a source of confusion that will repeatedly bite whomever has to work with it in the butt. Next.. Did you name your string that contains the date "today"? I'm not at all familiar with python, so I'm just winging it here from an example I found insert_stmt = ( "INSERT INTO INSERT INTO VIX_DATA (Date,ALERT,XIV) " "VALUES ( DATE(%s), %s, %s)" ) data = ('2016-4-13', VIX, VX1) cursor.execute(insert_stmt, data) [the example](https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor-execute.html) was using Mysql, so I switched the date function they were using for the SQLite version. If, on the other hand, they were using a python date function you might steal from them.
I'm not familiar with Oracle, but I'm going to take a shot in the dark anyway. Have you tried to create the tables without the contraints and adding them after with ALTER TABLE? E
Yeah... I guess I would try to add them after the fact? DROP TABLE DETAIL_RENTAL; DROP TABLE VIDEO; DROP TABLE MOVIE; DROP TABLE PRICE; DROP TABLE RENTAL; DROP TABLE MEMBERSHIP; CREATE TABLE MEMBERSHIP ( MEM_NUM numeric(3) not null, MEM_FNAME varchar(25) not null, MEM_LASTNAME varchar(25) not null, MEM_STREET varchar(35) not null, MEM_CITY varchar(12) not null, MEM_STATE char(2) not null, MEM_ZIP numeric(5) not null, MEM_BALANCE numeric(2) not null, CONSTRAINT membership_pk PRIMARY KEY (MEM_NUM) ); CREATE TABLE RENTAL ( RENT_NUM numeric(4) not null, RENT_DATE date not null, MEM_NUM numeric(3) not null, CONSTRAINT rental_pk PRIMARY KEY (RENT_NUM) ); CREATE TABLE PRICE ( PRICE_CODE numeric(1) not null, PRICE_DESCRIPTION varchar(25) not null, PRICE_RENTFEE numeric(2,1) not null, PRICE_DAILYLATEFEE numeric(1) not null, CONSTRAINT price_pk PRIMARY KEY (PRICE_CODE) ); CREATE TABLE MOVIE ( MOVIE_NUM numeric(4) not null, MOVIE_TITLE varchar(30) not null, MOVIE_YEAR numeric(4) not null, MOVIE_COST numeric(4,2) not null, MOVIE_GENRE varchar(15) not null, PRICE_CODE numeric(1) not null, CONSTRAINT movie_pk PRIMARY KEY (MOVIE_NUM) ); CREATE TABLE VIDEO ( VID_NUM numeric(5) not null, VID_INDATE date not null, MOVIE_NUM numeric(4) not null, CONSTRAINT video_pk PRIMARY KEY (VID_NUM), ); CREATE TABLE DETAIL_RENTAL ( RENT_NUM numeric(4) not null, VID_NUM numeric(5) not null, DETAIL_FEE numeric(2,1) not null, DETAIL_DUEDATE date not null, DETAIL_RETURNDATE date not null, DETAIL_DAILYFLATFEE numeric(1) not null, CONSTRAINT d_rental_pk PRIMARY KEY (RENT_NUM,VID_NUM) ); ALTER TABLE rental ADD CONSTRAINT rental_mem_fk FOREIGN KEY (MEM_NUM) REFERENCES MEMBERSHIP (MEM_NUM); ALTER TABLE movie ADD CONSTRAINT price_fk FOREIGN KEY (PRICE_CODE) REFERENCES PRICE (PRICE_CODE); ALTER TABLE video ADD CONSTRAINT video_movie_fk FOREIGN KEY (MOVIE_NUM) REFERENCES MOVIE (MOVIE_NUM); ALTER TABLE detail_rental ADD CONSTRAINT rental_fk FOREIGN KEY (RENT_NUM) REFERENCES RENTAL (RENT_NUM); ALTER TABLE detail_rental ADD CONSTRAINT video_fk FOREIGN KEY (VID_NUM) REFERENCES VIDEO (VID_NUM);
RedGate SQLSearch (free!)
Please listen to this man. I've literally never used a right join in production code. 
You've provided a perfectly valid MySQL create statement there, but not Oracle. Is this definitely Oracle? See http://sqlfiddle.com/#!9/9d2da2
I'm going to make a wild guess. CONSTRAINT d_rental_pk PRIMARY KEY (RENT_ID,VID_ID), CONSTRAINT rental_fk FOREIGN KEY(RENT_ID) REFERENCES RENTAL(RENT_ID), CONSTRAINT video_fk FOREIGN KEY(VID_ID) REFERENCES VIDEO(VID_ID) A field cannot be both a primary key and a foriegn key. Other basic problems. You're creating keys tablename_NUM. Don't do that. Use Key, use ID, don't use NUM I'll bite you later if you do. DROP TABLE DETAIL_RENTAL; DROP TABLE VIDEO; DROP TABLE MOVIE; DROP TABLE PRICE; DROP TABLE RENTAL; DROP TABLE MEMBERSHIP; CREATE TABLE MEMBERSHIP ( MEM_ID numeric(3) not null, MEM_FNAME varchar(25) not null, MEM_LASTNAME varchar(25) not null, MEM_STREET varchar(35) not null, MEM_CITY varchar(12) not null, MEM_STATE char(2) not null, MEM_ZIP numeric(5) not null, MEM_BALANCE numeric(2) not null, CONSTRAINT membership_pk PRIMARY KEY (MEM_ID) ); CREATE TABLE RENTAL ( RENT_ID numeric(4) not null, RENT_DATE date not null, MEM_ID numeric(3) not null, CONSTRAINT rental_pk PRIMARY KEY (RENT_ID), CONSTRAINT rental_mem_fk FOREIGN KEY (MEM_ID) REFERENCES MEMBERSHIP(MEM_ID) ); CREATE TABLE PRICE ( PRICE_CODE numeric(1) not null, PRICE_DESCRIPTION varchar(25) not null, PRICE_RENTFEE numeric(2,1) not null, PRICE_DAILYLATEFEE numeric(1) not null, CONSTRAINT price_pk PRIMARY KEY (PRICE_CODE) ); CREATE TABLE MOVIE ( MOVIE_ID numeric(4) not null, MOVIE_TITLE varchar(30) not null, MOVIE_YEAR numeric(4) not null, MOVIE_COST numeric(4,2) not null, MOVIE_GENRE varchar(15) not null, PRICE_CODE numeric(1) not null, CONSTRAINT movie_pk PRIMARY KEY (MOVIE_ID), CONSTRAINT price_fk FOREIGN KEY (PRICE_CODE) REFERENCES PRICE(PRICE_CODE) ); CREATE TABLE VIDEO ( VID_ID numeric(5) not null, VID_INDATE date not null, MOVIE_ID numeric(4) not null, CONSTRAINT video_pk PRIMARY KEY (VID_ID), CONSTRAINT video_movie_fk FOREIGN KEY (MOVIE_ID) REFERENCES MOVIE(MOVIE_ID) ); CREATE TABLE DETAIL_RENTAL ( DETAIL_RENTAL_ID numeric(6) not null, RENT_ID numeric(4) not null, VID_ID numeric(5) not null, DETAIL_FEE numeric(2,1) not null, DETAIL_DUEDATE date not null, DETAIL_RETURNDATE date not null, DETAIL_DAILYFLATFEE numeric(1) not null, CONSTRAINT d_rental_pk PRIMARY KEY (DETAIL_RENTAL_ID), CONSTRAINT rental_fk FOREIGN KEY(RENT_ID) REFERENCES RENTAL(RENT_ID), CONSTRAINT video_fk FOREIGN KEY(VID_ID) REFERENCES VIDEO(VID_ID) );
&gt; "INSERT INTO INSERT INTO VIX_DATA (Date,ALERT,XIV) " First I think you put INSERT INTO in 2x by accident, second even when I remove that I get this error OperationalError: near "%": syntax error 
I believe Oracle still throws this error without data. One of its many...uh...quirks 
Yeah, so you're right. The first example ran fine the second was producing cross products. You're responses confused the shit out of me and I realized about 2 minutes in my environment that I'd do a `select from` a params table before hitting the transaction table. Anyway, both approaches are I think faster than yours or at the very least equal. I'll reiterate that I don't like using case logic to produce a function. Would have been a lot easier if you had just pointed out that the `from` would need to be a subquery such as: from ( select distinct storenumber from transactions ) a
I use this: http://i.imgur.com/2mlaF1M.jpg
I use Toad all day. SQL developer is handy too, but Toad just makes everything easier for me. 
[removed]
Strongly recommend Brent Ozar's blitz script for DBAs: https://www.brentozar.com/blitz/ Actually his whole First Responder Kit and his 6-month DBA training email (all 100% free) are great: https://www.brentozar.com/first-aid/sql-server-downloads/
I edited my post. TLDR; Can't use row level locking. It was one of the first things the DBAs veto'd. We've had issues with it in the past where it would lock more information that it was supposed to (I don't know the specifics, I'm not a DBA) and they said no more row locking in SProcs.
Thanks man. I am following him.
Oracle + PostgreSQL * Linux workstation * vim * SQL*Plus * psql * my own database environments -- for learning/testing/developing/etc * Firefox with 'Tree Style Tab' - I use lots of tabs doing research I don't have much use for web or gui interfaces. 
Thanks, I will definetly consider making three order related statuses. I just don't know how to make an action on the website (completing shopping card order) create a row in Rent_Request. I don't think I have had to write code like this before. Would I be correct in looking at using PHP to insert data into a database?
If I ever found myself having to write one I'd assume my script was fucked and review it.
Because you're doing an INSERT rather than an UPDATE? To elaborate a little further, there's no reason for a WHERE with an INSERT the way that you're doing it. You're inputting new data, you're not changing data that is already there. If you want to edit data that is already in the table then you'd be looking for an UPDATE command.
updlock and readpast are some words I heard the DBAs saying. I'm just trying to figure out how to write a stored procedure that meets their requirements.
Thanks a lot :-) Both exercises submission and course are nice :-)
I'm going to guess that you are attempting to reference tvf1 like a table without the parenthesis after three function name. You still need to include those So RIGHT OUTER JOIN [schema].[TVF1]()
Yep that was how I was doing it. I'll make that change. Thanks
You're correct that was a cut/paste error. Did you follow the link to the example? I'm not sure that specifying the parameter with a %s is MySQL specific, are you passing a parameter in a similar manner to what you're trying here anywhere?
Do you want to update the data for contaminated UIDs, or just reflect their contamination (and being moved to the target group) in your count? * updating contaminated UIDs use UPDATE Actions SET group = 'target' WHERE group = 'control' AND EXISTS ( SELECT 1 FROM Actions InActions WHERE Actions.UID = InActions.UID AND InActions.group = 'target' ) * Just in your report use SELECT UID , CASE WHEN EXISTS ( SELECT 1 -- if even one row exists that they are target, they count as target FROM Actions InActions WHERE Actions.UID = InActions.UID AND InActions.group = 'target' ) THEN 'target' ELSE 'control' END as group , actions , time FROM Actions WHERE ... * if your actions table is huge, you may want to designate whether a UID is target or control in a temp table before you start and join them. (I'm not familiar with vertica so there may be syntactical errors/differences) use SELECT UID , CASE WHEN EXISTS ( SELECT 1 -- if even one row exists that they are target, they count as target FROM Actions InActions WHERE Actions.UID = InActions.UID AND InActions.group = 'target' ) THEN 'target' ELSE 'control' END as group INTO #UIDGroup FROM Actions Then SELECT Actions.UID , #UIDGroup.group , Actions.actions , Actions.time FROM Actions INNER JOIN #UIDGroup ON Actions.UID = #UIDGroup.UID WHERE .... 
Made me wanna pull out my hair a couple of times but worth it.
Hey! I wanted to thank you for all the replies! I did end up going with a transaction statement, and it had the desired effect, so far as I know. I'm getting with a DBA tomorrow and we're going to run some tests that should force two different threads to try to step on each others' toes to see what happens and make sure that they play nice. The locking stuff was something about locking escalation when SQL is low on resources, and some of our tables get HUGE and some of our machines get pounded, so they've had locking escalation problems in the past. Again, I'm not a DBA, and I should read up more on this to learn more of the specifics of SQL locking escalation (scenarios, causes, etc.), but that's what I was told. Again, big thanks for the help!
There are some great tools in these comments, and I would say they are all worth a look. Just a word on tools in general though, in many environments, you are expected to be able to survive without them. Yep, even a programmer's notepad. Trying to justify purchase of a tool is often very difficult, depending on the environment. Just as you succeed, you end up on a different project and they have no licenses - now you feel 'crippled' in your work because you are not so productive. If you are trying to break into the field, your first moves should be to thoroughly learn the native toolset in all its variants. 
Are you logged in as SCAUDLE1? Try qualifying your table names in all the statements e.g. CREATE TABLE SCAUDLE1.MEMBERSHIP Etc.
This might be a stupid question, but do those emails still go out? I signed up a few months ago under two different email addresses. I got the initial ebook/scripts but nothing else. EDIT: Nevermind. I signed up again and this time got the "Week 0" email so it seems to be working after all.
Problem is, SQLite [does not support](http://www.sqlite.org/omitted.html) adding foreign keys to existing tables through ALTER TABLE statements. One option is to rename a current table, then create a new identical one with foreign keys enabled, then copy the data from the original into the copy, but I don't think it will work optimally given the size of the database.
C# .NET 
Sweet, I know how to do it then. Lemme get back to my desk. I am on break right now
Sweet man, thanks for the help. Im going to try this out now. 
 SELECT ClassID, COUNT(MemberID) AS [NumberOfMembers] FROM CLASSES GROUP BY ClassID; SELECT AVG(DOB) AS [AverageAge] FROM MEMBER WHERE CLASSES.ClassID = 'Classes'; SELECT MEMBER.MemberID, MEMBER.FirstName,MEMBER.LastName ,MEMBER.Phone, MEMBER.Email, Member.DOB , CLASSES.ClassID FROM MEMBER JOIN CLASSES ON MEMBER.MemberID=[CLASSES].[MemberID] AND CLASSES.ClassID='Aqua'
Here's a query that should roughly approximate what you're looking for. SELECT * FROM title t INNER JOIN kind_type kt ON kt.id = t.kind_id INNER JOIN cast_info ci ON ci.movie_id = t.id INNER JOIN role_type rt ON rt.id = ci.role_id INNER JOIN name n ON n.id = ci.person_id LEFT JOIN aka_name aka ON aka.person_id = n.id LEFT JOIN person_info pi ON pi.person_id = n.id WHERE kt.kind = 'movie' AND t.production_year BETWEEN 2000 AND 2015 AND rt.role IN ( /* List of roles that define "crew" here */ ) /* Runtime of 40 minutes. This code assumes movies are allowed to have multiple runtimes. If any runtime is 40 minutes or longer, it's included. Movies with no runtime are excluded. */ AND EXISTS ( SELECT 1 FROM movie_info mi_rt --I have no way of knowing if I should be using movie_info_idx instead INNER JOIN info_type it_rt ON it_rt.id = mi_rt.info_type_id WHERE it_rt.info = 'runtimes' AND mi_rt.info &gt; 40 --I have no way of knowing what should be here AND mi_rt.movie_id = t.id ) /* Exclude adult. This code assumes movies are allowed to have multiple genres. If 'adult' is one of the genres, it's excluded. Movies with no genre are also excluded. */ AND NOT EXISTS ( SELECT 1 FROM movie_info mi_genre --I have no way of knowing if I should be using movie_info_idx instead INNER JOIN info_type it_genre ON it_genre.id = mi_genre.info_type_id WHERE it_genre.info = 'genres' AND mi_genre.info = 'adult' --I have no way of knowing what should be here AND mi_genre.movie_id = t.id ) I have no idea what `complete_cast` and `comp_cast_type` are and how they relate. I have no idea which fields are nullable, and therefore my guess about where INNER and OUTER joins should be is complete guesswork. I have no idea which tables are many-to-one either, so your results could have duplicates all over the place. Create an index on each field that's used as a join condition. One index for each field. Name them something like `IDX_TableName_FieldName`.
Anything by Joe Cielko.
One way - with aggregates, you might need isnull for the second count tho SELECT Ida From tableLink Group by Ida Having count(case when idb in ([mylist]) then 1 end) &gt;1 And count( case when fib not in ([mylist]) then 1 end) = 0 Another (probably better performing tho uglier) is to do 2 exists checks.
PowerBI &gt; tableau
I know that was an option, but for values that are passed through SQLParameters, SQL injection is not really an issue when directly ran to a query. Dynamic SQL does have its own issues like opening your self up to said injection. But it can easily be fixed with client and server side validation. Table valued parameters are best in this case but require a lot more setup.
 INSERT INTO bob_psp_link_redirect ( url , url_redirect ) VALUES ('http://happyhappy.com/happyhappy/','http://happyhappy.com/happyhappy/') , ('http://happyhappy.com/happyhappy/','http://happyhappy.com/happyhappy/') , ('http://happyhappy.com/happyhappy/','http://happyhappy.com/happyhappy/') 
You have a couple syntax issues going on - first off, the `tbl_name` needs to go, just use the real table name which I'm assuming is `bob_psp_link_redirect`. Also, you should only put `VALUES` once, like so: INSERT INTO bob_psp_link_redirect (url, url_redirect) VALUES ('url 1', 'redirect 1') ,('url 2', 'redirect 2') --etc... If you need separate `VALUES` clauses for whatever reason you'll need to write out the whole `INSERT` statement again as well. For what it's worth, you don't need to use double quotes anywhere in this query, and you should be using single quotes for strings. You can use double quotes for aliases and object names, but it's generally not necessary except in special circumstances, like if an object name has a space in it.
First step of debugging Access, remove all of the unnecessary parentheses that it adds.
And Linux 
You cannot update a join. You can only use 1 table. You could use exist or subquery. You could use a stored proc and cursor and go through it row by row and do what you want that way. But with just 1 SQL statement like that I dont think you can.
yess
He's not updating the join he's updating one table and simply using the join as part of his where clause which is perfectly fine. 
Ok, for the first one you want to replace MemberID in the SELECT statement with ClassID (the field that you're grouping by). The second one, you're not actually linking to the CLASSES table, you want to link that using a JOIN in order to utilise the CLASSES table, at the moment you're only using data that's in the MEMBER table. In this one you're doing a cartesian join of the MEMBER and CLASSES tables (it's going to join all members to all classes regardless of the data in there), you need to add a JOIN in here too.
If the SQL editor in Access is still a horrible piece of shit without syntax highlighting, I'd suggest pasting stuff into Notepad++ to check you don't have unclosed brackets etc.
Ok, so the issue is your WHERE clause. Using the = operator can only be used with one result. Think about this one. Can you reduce the result down to one row (and maybe do it twice) or can you use a different operator that will allow for multiple values to be passed through (replace = with something else)? If you're struggling like this and know the output that you're seeing from the sub select, try replacing this with the actual output that you're seeing (the two fields that are returned). This would only be for the purpose of error checking and may make it more obvious which operator you should use (e.g. WHERE author.authorid = ('result1','result2') )
Use IN instead of = and add a DISTINCT to the subquery. That should do it. 
Thanks guys. Worked a treat
Probably not, postgres explain and the pgadmin query tool's explain visualiser does a pretty great job of this already, for free
User to Post table is probably the only real way to do it if you need to keep track of who did what. The main focus is probably going to be efficient seeking so you don't fetch too much data too frequently. Facebook generally won't tell you all the names of the people who liked the post, it will likely just focus on the last 10 or so and the rest is going to be a straight count.
&gt; But it seems inefficient for something like Facebook where there could be many many likes. How would that be inefficient? That's one row for every unique post/user combination, now with an added "type of like" field. It's an easily indexed search, and i suspect they segment the table with the posts from the last n hours in the fastest portion and the next n hours/days in another segment and anything older than 4-5 days in another segment.
Additional notes: SELECT book.title FROM book INNER JOIN written_by -- Not every DBMS defaults to inner. specify. ON book.bookdescid = written_by.bookdescid INNER JOIN Author -- Not every DBMS defaults to inner. specify. ON Author.authorid = written_by.authorid WHERE Author.authorid = ( SELECT author.authorid -- Is this guarnteed to return you one row? Apparently not. FROM author iAuthor -- You're using the same tables in an inner select better to use an alias INNER JOIN written_by iWritten_By ON iAuthor.authorid = iWritten_By.authorid INNER JOIN iBook ON iWritten_By.bookdescid = iBook.bookdescid WHERE iBook.title = upper('Harry Potter') AND role = 'Author' ); When you write a subselect, by default the reference to the same table name refers to the version inside the subselect (which is why it worked as written), but sometimes you want to refer to the outside version so if you alias the inner version to a unique name you're able to do that, it also adds clarity for someone else coming along and reading your query. Some DBMSs (I think even most) default a JOIN that's unspecified to be an inner join, but both for clarity and that one tool/DBMS that defaults differently specify what type of join you're doing.
This should also work right? SELECT COUNT(*) AS TotalCount FROM (SELECT Field1 FROM Tab1 UNION ALL SELECT Field1 FROM Tab2) AS Base
I'm in. This looks really great and could safe us a lot of time. 
Yep.. that should also get you a count of all rows from both tables. 
dear OP, please see sidebar about identifying your platform as long as we're throwing syntax against the wall... SELECT (SELECT COUNT(*) FROM TAB1) + (SELECT COUNT(*) FROM TAB2) AS obj_count
Referenced views get collapsed into the current view, so yes, explain and pgadmin are suitable as they show the full data paths. It won't help if say, intermediate tables are created instead of views. But if these tables are getting refreshed then they should be views (generally). Also I would have thought most ETL platforms did diagrams as well as job design and job execution, so I don't get what sqldep is going for.
Thanks!
 ORDER BY 6 or ORDER BY "total"
It is efficient enough. Until you're as large as Facebook you don't really need to worry about it. 
Oh man and we are still running 2000
My apologies 
Just to clarify -- we are processing any DML statements like insert, updates, merges not just views. About ETL platforms - many companies are using hand coded stored procedures. Even when the company uses ETL platform from major vendors like Oracle, Informatica it still fails to keep track of the lineage when developer uses custom SQL statements. However let's get back to main topic. QueryScope solves the pain when you write a SQL query with 10 table joins and several nested subselects. Then 2 months after you need to edit the query. We simply help you to understand what the query does by providing a diagram. It is just a much friendlier way compared to explain plan. And explain plan gives you less info. Now we are looking for beta users to help us develop something truly useful. Cheers.
Awesome, thank you!
What does your question have to do with SQL? How can we help?
its not MSCA. Its through my employer. My "expert" level class starts at the end of the month. Im up to learning about Stored Procs 
I got a reporting position with no formal certs or training, I'm sure you'll be able to land something pretty easily having those. How long did they take you to complete? 
Yeah, this is going to be the easiest way to do it. You can't PIVOT easily because each type of fruit has different varieties. I suppose you could PIVOT the CTE query, but that's some pretty ugly looking syntax. Truthfully, you should handle this *in your application*. It's a display or presentation problem, *not* a data layer problem. That's why it feels like you're pounding a square peg into a round hole. Yes, you *really are* supposed to process multiple rows from the database even if you consider it a single item in your application. 
My table has 44k rows. This is going to take a while 
well I started teaching myself SQL last June through Khan Academy. My first official class started in October. 
It shouldn't take a while. 44k is nothing to SQL. Worked just fine on my box, make sure you got the joins correct.
Great answer. Thanks for the idea!
thanks ill take a swing at it
I hope you have something newer in parallel, but my dirty secret is that I'm still in love with DTS even now.
Only difference on my end was that I needed top 5. Code ran for an hour and then I killed it. 
I hate dts. I'm growing less of a fan of ssis over the years. New lines of business and new systems. I'm forcing a move to 2016 Azure SQL and Azure apps, SharePoint Online and Office 365. If I keep ending up in admin positions, I'm refusing to build an infrastructure I actually must maintain.
hmmp, on the bigger files it still returns extra rows. i did remove the extra rows in the text editor and the problem still persists. erg
You need indexes then. Should run in no time on 44k rows.
As suggested by someone from stackoverflow, I'm guessing the right way to go about this is to fill the whole table during the initial INSERT INTO statement. Gonna try that tomorrow.
That style of statement can work but you must correlate the subqueries with the updated table so they chose a value per updated row. 
I have been considering something like this for a while. I wrote a prototype lineage tool for mainframe-based SQL apps a year or so ago but stopped at the SQL parsing bit (time and money became blockers). Kudos for taking it on. I always felt that there were a couple of tricky bits to creating a SQL lineage tool like this: 1. View hierarchy comprehension and refresh. Do you handle view hierarchy resolution? I guess you do if this is a lineage tool. If so, cool. 2. Data dictionary integration. Do you refer back to the db's data dictionary to continually refresh your internal metadata or do you use the db's data dictionary 'live' during graph production? How do you manage the potential dictionary contention issues (or do you expect that this will not be an issue?). 3. Internal representation. I was considering retaining the lineage data in Neo4J (graph database). Is it your intention to retain the lineage info in some form? Good Luck. This is a significant undertaking - I wish you well. :)
You're a lifesaver I'm really new to this and struggling. If i had gold to give i'd give it to you.
I hate to ask for more help, i'm just trying to weed out errors not get you guys to do my work for me. How or why could i possibly have an invalid character? &amp;nbsp; INSERT INTO EMPLOYEE ( OCCUPATION ,FIRST_NAME ,LAST_NAME ,STAFF_ID ,HIRE_DATE ,GENDER ,RELATIONSHIP_STATUS ,DOB ,ADDRESS ,POSTCODE ,PHONE_NO ) VALUES ( 'Doctor' ,John ,Smith ,789ghi ,21-07-2011 ,Male ,Married ,31-May-1991 ,110 Moore St ,3000 ,0412345678 ); 
Not 100% sure but start by trying to use ' not  and  Sidenote: Thought It may seem nitpicky It's still not completely proper formatting by the way. Look at mine vs yours; The tabs have a purpose, they delimit the action or parameters contained within the command. SELECT INTO (Command) EMPLOYEE (Target) ( List of Target Fields ) Values (Command) ( List of Values ) Clarity is your best friend especially once you get into much more complex stuff. Can you imagine how much more difficult something like this would be to read if it was flat. UPDATE PriceTest SET Ignore = 'True' FROM ( Select J.RNum ,PriceTest.* FROM PriceTest JOIN ( SELECT Store ,Vendorcode ,Prodid ,EventCode ,PosRetSold ,ROW_NUMBER() OVER(PARTITION BY Store ORDER BY PosRetSold DESC) as RNum FROM PriceTest ) J ON J.Store = PriceTest.Store AND J.Vendorcode = PriceTest.Vendorcode AND J.Prodid = PriceTest.Prodid AND J.EventCode = PriceTest.EventCode AND J.RNum &lt;= @TopLimit ) AS IJ WHERE PriceTest.Store = IJ.Store AND PriceTest.Vendorcode = IJ.Vendorcode AND PriceTest.Prodid = IJ.Prodid And this isn't even THAT complex of a statement. My boss and I wrote a statement the other day that had 8 nested selects each with inner joins, the damn thing was like one of those Russian dolls with smaller dolls inside.
That actually worked in getting rid of that error, but replaced it with "missing right parenthesis" ? I have two opens and two closes..? **Edit:** I got it! Finally. Big thanks to everyone involved, I managed to create a row. Here was the eventual syntax. INSERT INTO EMPLOYEE ( OCCUPATION , FIRST_NAME , LAST_NAME , STAFF_ID , HIRE_DATE , GENDER , RELATIONSHIP_STATUS , DOB, ADDRESS , POSTCODE , PHONE_NO ) VALUES ( 'Doctor' , 'John' , 'Smith' , '789ghi' , '06-May-2011' , 'Male' , 'Married' , '31-May-1991' , '110 Moore St' , '3000' , '0412345678' ); 
Congrats, you're one step closer to not sucking at SQL :P Now go back and read my edit to my response.
I agree that modifying your INSERT INTO is the easiest way. When you join [cast_info] to [movie], you are getting one record per movie, per person, per role for just the movies you are interested in (a one to many join). If at that step, you insert not just person_id, but also movie_id and role_id, there's no need for an update. INSERT INTO crew (person_id, movie_id, role_id) SELECT cast_info.person_id, cast_info.movie_id, cast_info.role_id FROM cast_info INNER JOIN movie ON cast_info.movie_id=movie.movie_id; If it helps, think of your INNER JOIN as applying a filter on the full cast_info table to just the records for the movies of interest to you. You are inserting that subset of records into your new table.
Gah! Stupid mobile font! Is that a space in PHONE NO? Didn't spot it. If you want a space in a column name you have to quote it. An easy rule of thumb with sql parsers is to think of spaces as delimiters.
I'm interested, we use a cursor to run through a list of kpis for load into our datamart. Even using the for each component in ssis doesn't make this different from a cursor, so what's the non cursor method? 
You're quite welcome, glad I could help.
If you aren't already using it, make sure you download Notepad ++ or something similar. It makes it so much easier to track your parenthesis and doesn't give you weird conversion errors when you copy and paste it into SQL Developer or SSMS. 
So is that instead of the Kimball one? Also, I've been using MySQL because it is free, but I'm guessing it may not cut it? Any insights into data analytics? 
that book uses the kimball style. bit less verbose. covers some practical stuff. it's less daunting than kimball's so good for beginners. if you wanna stay open source / free, then try the following (MySQL is fine): Mondrian OLAP server http://mondrian.pentaho.com/ Pentaho Schema Workbench (for creating the cube schema) http://mondrian.pentaho.com/documentation/workbench.php Pentaho Aggregation Designer (for finding useful aggregations and building them) http://infocenter.pentaho.com/help/index.jsp?topic=%2Faggregation_designer_guide%2Fconcept_pad_overview.html Saiku AJAX pivot tables http://analytical-labs.com/ [Pentaho Report Designer](http://reporting.pentaho.com/report_designer.php) for designing reports. Pentaho Data Integration (Kettle) for ETL into data warehouse http://kettle.pentaho.com/ [DataCleaner](http://www.datacleaner.org/) for quality control Re advanced analytics / data mining, python is good. Lots of books out there
Don't worry, I changed this pretty early on. I was just being lazy I guess.
shuddup! that's cool. thx
It depends. Probably 1/3 are simple joins, 1/3 is very complicated, and the other 1/3 is having to create and run an SSIS package to move data from one server to another, and then running a query kof some sort against it. Linked servers aren't allowed in our environments, hence why SSIS is used.
If you can do everything in this post, then you are more qualified than 90% of contractors. Go find a contract position ASAP. The real world experience will serve you much better than many books or classes ever will.
The vast majority of the queries in work on arrival far bigger than 50 lines. In fact, I frequently work on queries that have to be altered because they are too long and cannot be parsed by the query editor.
Bit confused, so use Postgre...or OTN? 
So I looked into the book and I just bought it on Amazon! However, I'm not too sure what the other links you posted are for? Are they different options to use? 
We made a study to try to recreate it in .NET, but the cost and time needed to change the technology from an ERP that works perfectly to another one that do pretty much the same isn't worth. Also, the ERP is only used local, I work in a quite big Printing Industry (about 350 employees) so it attend our demand.
It would be scary if we sold the ERP, since we only use it in-house, I can't see a problem.
I have a 1,300 line production query that takes about 30 minutes to run daily and populates a cube which is then used to populate all sorts of other production reports. It sounds daunting but it's really a series of passes that dedupes and builds up temp tables and then joins them all together. For the most part it's just simple joins and subqueries and it only pulls together a few hundred thousand rows of data. If you want to see it I can paste it tomorrow from work. Some of my 20-50 line queries are way more 'complex' in terms of SQL use (like a dynamic pivot table) but my favorite 'complex query' is my forecasting tool. It isn't terribly long (~100 lines) and doesn't do anything too wild but I love it.
You're joining on variety, not type. That's probably why it's taking so long.
Probably about half and half at my current job. I used to work for an IBM shop and nearly every proc was simple and straightforward. They had a "do one thing and do it well" approach to data modeling. But this meant hundreds of stored proc to document and a wicked complicated nightly update process that broke a lot.
Exactly. The front end is out dated, but the back end that really matter is running in a SQL 2014 standard edition with high-safety mirror with witness (I'd like to use Always On but you need enterprise edition for that) with servers in different buildings, raid 10 of SSD disks and so on, it is very pretty sight :D Since our SQL contract includes all updates, we'll update soon to SQL 2016 and I'm making a new project to use the reports for mobile using the SSRS, mobile was pretty much our only absent area and that will no more be an issue.
Those are your workhorse business intelligence tools. Not fancy like deep learning toolkits but useful for many tasks 
To return just 1 row for your subquery try (Select Top 1 b.OrderedQuantity...
yes
It wasn't access itself, it was the add ins and libraries etc. 
Thanks for encouraging words :) Talking about our data lineage tool (not Queryscope): Ad1) we do handle view hierarchies Ad2) you just bundle your SQL statements/stored procedures/views for analysis and send it to us. We calculate the metatada once per each upload. In this scenario we do not need to have any direct connection to DB itself. Ad3) we have played around with Neo4j to retain the lineage data. However it was an overkill for our use case and we stick with relational databases (one of our customer has a batch with 750,000 columns in it and we do not have any performance issues with generating lineage charts in realtime). Thanks again and it be great to see you among our beta testers for QueryScope (=generating a diagram per single SQL statement to quickly understand table joins, sub-select structure, etc.).
Most of the business world runs on Access and Excel. It's horrifying. I don't care what big systems they have running things on the back end - **so** much stuff is decided based on data dumped into Excel and then "massaged to tell the story." Or Excel documents being the system of record for entire projects. *Massive* budgets maintained, forecast &amp; monitored via dozens of linked workbooks &amp; worksheets.
We make a web app and reporting system, and certainly some of the queries used are big, but generally anything that long is turned into a stored procedure and used from there. Still the same size query executing regularly, but the "code" isn't making it ad hoc.
I'm hearing this a lot now, access solutions are slowly rotting away with each Windows release.
The real problem with MS Access solutions was that they are so easy to setup, that people set them up who had no real business creating ERP systems. And then the popularity keeps waning, due to other options being available in 2016 for many types of needs, so there's no real incentive to provide any reasonable rescue path that is more worth it than the ol' abandon it and start again approach that we took.
I work for a company that sells a web product with a SQL back end. Whilst I don't create a lot of code any more (got developers for that) I do review a lot of code and work on optimizations. I'd say it's about half and half for the size but the big gotcha is that the smaller queries quite regularly call views that themselves are large so it can be a little deceptive. For reference, the script that i have open on my screen at the moment is 78 lines long but calls three different views and has calculated sub select views. EDIT: Can't provide example code I'm afraid, but it's basically this; SELECT FROM table1 LEFT JOIN table2 LEFT JOIN view1 LEFT JOIN view2 LEFT JOIN view3 LEFT JOIN ( SELECT FROM table3 LEFT JOIN table4 WHERE GROUP BY ) LEFT JOIN ( SELECT FROM table5 LEFT JOIN table6 WHERE GROUP BY ) LEFT JOIN ( SELECT FROM table7 LEFT JOIN table8 WHERE GROUP BY )
I used to think like this, I promised myself I'd never use a cursor in SQL ever again. That was until I was trying to automate a blocking report to show the object that was being blocked (rather than something like "OBJECT: 7:1319779859:14"). The way that I chose to do this was to extract the different elements of the code above (e.g. '7', '1319779859', '14') and run it through DBCC PAGE WITH TABLERESULTS. So that I could get the metadata information that I needed to be able to run the OBJECT_NAME command and get the object name. All of this was done to allow the slightly less technically inclined to understand what was being blocked in each instance. As the output was a table I decided the most efficient way of retrieving the data was with a cursor. Now this isn't code that is used in production but there are specific uses for cursors when absolutely needed. edit: output was a table, not a cursor
Go DB Dev/ Analyst. The role of DNA is slowly going away (it will be a long time before they are all gone, but if you have the option, go for growth positions) as companies and vendors offer more and more cloud options. DB Development is on the rise with no end in sight. Plus, companies that used to think theyou could fill those positions for $60-80k have learned to offer a better wage right at the start. There are 4-5 local companies that I have laughed at when they told me their salary range within the last 3 years that I have recently seen them advertising for the same position in the $110-120k range.
As a counter to this, any MySQL has bindings to Excel and can be used for small-business reporting, it has less cost, and the likelihood of free reporting alternatives becoming available in the future is high. Also if you switch your office-suite; I know it also has bindings to libreoffice &amp; apache openoffice. Btw if you have time, look up libre free.
It would be nice if there was something similar (and also OSS) to SSRS to use for reporting with Postgres. I just use SSRS, but we already have SQL Server licenses.
There's [SpagoBI](http://www.spagobi.org/) it's open source and free with a paid professional support model if you need it. It's pretty impressive, but again, if you already have Oracle/SQL/SAP in your shop, you wouldn't bother.
Does it have to be PHPMyAdmin? [MySQL Workbench has a feature that might give you what you're looking for.](https://dev.mysql.com/doc/workbench/en/wb-database-diff-report.html)
Sure. I have a table, lets call it table1, that houses all the data in our ticketing system. Once a week we have a ticket that we create that we want to monitor closely. There are three columns I'm specifically interested in: Case Data Date Updated ---- ---- ------------ 1000 Data Here Timestamp 1000 Data Here Timestamp Anytime there is an update, a new row is added. I want to capture these updates whenever they happen. I'm perfectly capable of running a select statement periodically and just grabbing that, but I was wondering if there is a more preferred way to go about doing this. The case number changes from week to week, so I'm not sure if a trigger or something of that nature would suit my needs. I would also like to have my updates be fairly responsive, I don't need it instantly, but shortly after an update would be nice. I guess my question is how could I go about doing this without hammering the database, or is the best way to just do a select statement every couple of minutes? Does that make sense?
The fact that the case number changes every week is troubling... In your ticketing system it sounds like ticket 1000 goes through an update so your table has Case 1000, data, 4/17/2016:8am Case 1000, data, 4/17/2016:9am ... etc. Then next week the ticketing system changes it to case 2000? That's really unfortunate if that is the situation. What a nightmare because now you're in a position where you want to update all case 1000 to 2000 where table data = ticketing system data. Sorry I am probably jumping ahead of myself. Back to your initial question. You can set a trigger on that table that inserts the basics you want whenever a record is inserted into it. I work in a database that has triggers on tables that are hit hundreds of times a day and they're resource efficient enough to not blow things up. Another method is if you don't want to mess with triggers you could write a script that inserts data. Something like: Insert into table2 (case, data, timestamp) select case, data, timestamp from table 1 where conditions (conditions such as similar data and timestamps can't exist in table 2 already... etc.) You would then take that insert statement and place it into a sql job agent job. These jobs can be set to a schedule. You can make it run every minute or every 5 minutes... anything that works for you. I'm a little confused as to why you'd even capture the case number if it's truly changing every week. That's just very odd / bad for storing data. If the "data" part of this is somewhat unique you probably have a better chance of grouping by that. I could be way off and just imaging things incorrectly I don't know. Either way hopefully one of my methods makes sense. Let me know if you have any questions. edit: &gt;I want to capture these updates whenever they happen. If this is the primary goal make a trigger for sure
Data or layout? If Data, you will likely want to make a backup of your production database, restore it onto your dev server and then use a tool (or write a script against INFORMATION_SCHEMA) to compare.
What about a cte?
Not sure if I understand your question but you can have multiple when...then statements in a Case
Trigger sample: CREATE TRIGGER [dbo].[tr_table1_INSERT] ON [dbo].[table1] FOR INSERT AS declare @case int select @case = t1.case from table1 t1 INNER JOIN inserted i ON t1.case = i.case begin insert into table2 (case, data, timestamp) select @case, data, timestamp from table1 t1 print 'Update done' end This is sql server 2012 syntax. Not sure if that lines up with your version of sql.
thanks good friend.
I would never chase the mighty dollar and the organization I am with now has been good to me with ample retirement plan and healthcare, but this is coming from intellectually growth seeking and doing something I'd love more going forward -- passion. No, it is not so bad as answering phones, which would make me jump in a heartbeat or also if the pay + benefits is much less.
If you're trying to do what I think you are then you'd be better off in /r/etl The easiest way to do this is pull the table in increments. You need a date or sequence column to work with, sounds like you have both. Now make your first read. Take a point in time, store it and select everything with date &lt;= that. Next time you select &gt; that up to your new point in time. You can repeat this as frequently as you need, fetching each increment of data as it comes into being.
Create a variable and set it as the AVG() of all products. Then make a simple select with AVG() and filter the products lower than the variable. It will be almost like what was discussed here: https://www.reddit.com/r/SQL/comments/4akaq5/sql_help_using_update_with_avg_beginner/ but you'll make a select instead of an update
Thanks for the response. In the end I ended up doing it with two separate queries and trying to simplify the data types being used. The exists method worked, but it was noticably slow and when trying to put in the parameter using the `@Param` annotation, Hibernate threw a hissy fit Ended up with a solution whereby I had the given list, and I iterated through it checking if the value is in a Set. Not the most efficient, but decent enough for the beta Thanks again
And i use this to see the sql text. I hope this helps as well You can send an alert then blocking_session_id &gt; 1 or wait_time gets too high. SELECT session_id, status, command, b.name, blocking_session_id, user_id, wait_time, cpu_time, total_elapsed_time, reads, writes, text FROM sys.dm_exec_requests a inner join sys.databases b on a.database_id = b.database_id OUTER APPLY sys.dm_exec_sql_text(a.sql_handle) c where session_id &lt;&gt; @@spid order by name
If I highlight a word, and ctr-f, it brings up a box over the top of my editor with 6 or so find/replace options. I'm pretty sure I tried esc but I can try again later...
x10hosting.com, sign up with them, you'll access to 2 dbs, you just need 1, full phpMyAdmin access to do the work. Why do you wait so long?
I'll be honest I didn't mean to. I got frustrated last week because I couldn't get mysql working on my desktop and then forgot about it. I'll give it a shot. Thanks!
&gt; Your log drive needs to be as large as the largest possible transaction for each database added together. That makes sense for the minimum size for simple recovery, but it's more complex for full recovery. Full recovery it also depends on how often you're taking log backups. &gt; TempDB... 8 sets of files or same number as CPU cores. I usually do one per core. I also usually enable trace flag 1118. I can't remember if this is default behavior now or not. You know, the biggest use of tempdb and log space in almost every system I've used is if you try to run the built-in index rebuild maintenance task. That will do an index rebuild on every index in the database. On our systems we tend to employ the Ola Hallengren maintenance scripts, but a good old database-wide index rebuild is likely to be the high end of usage for a lot of systems. If you can populate your system with the right amount of data, you can do this to determine a reasonable high water mark for the size of tempdb and transaction log. Even if you can only generate a month worth of data, this could be used to help do capacity planning. The problem with both these questions is that they both depend on your application. In other words, nobody can predict without full knowledge of the system's operations and load. That's why it feels like you're trying to put the cart before the horse. Microsoft does publish [capacity planning guides](https://technet.microsoft.com/en-us/library/ms345368\(v=sql.105\).aspx) but the reality is that they just tell you what you need to know about your system in order to plan correctly. 
There may be a more elegant solution, but it sounds to me like what you need is subqueries within the Select. My original thinking would have resulted in at least 2 rows (depending on ties), but a form of SELECT (Select...) AS val1, (SELECT...) AS val2 FROM.... would be a single row. 
Can I ask why you're hard coding nuanced money values into your code rather than using a dimension/lookup table? An inner join to the table is faster than interrogating values row by agonizing row in a case statement, and more importantly if you use a dimension/lookup table you can better manage changes to the money values. What if it becomes $1294.82 instead of $1294.81 tomorrow? Do you want to update a stored procedure or update an entry in a lookup table?
[OPENROWSET](https://msdn.microsoft.com/en-us/library/ms190312.aspx) along with the already mentioned BULK INSERT.
Ah! Thank you very much.
You should be able to do it, by granting permissions to "Authenticated Users" / "Everyone", assuming everybody needs the same level of permissions. 
Why dont you just create a table named ormmapping, and put it there ? Abusing comments as metadata, is lets say unusual.
There is too much going on here for me to do this mentally. Can you show the optimizer diagram? that could narrow down where to work on. One thing to experiment with is this function set: &gt;st_dwithin(cp.centroid_geom, ST_SETSRID(ST_MAKEPOINT(-122.19071129181152, 47.68521319786181), 4326), 1.0) Set up a variable, put that calculation/function call into it, then report back to see if there's any improvement. Also, self-joins are sometimes tricky. I'd experiment with generating a second work table and seeing if you get better results joining to it. WARNING: you might not want to put that into production, but looking at the output results might be better. 
Have you checked out pluralsight.com?
I'm assuming this question is aimed at folks whose main job function isn't database administration or development?
I think it's more for examples related to querying data. But I would love to hear if you have examples related to challenges of maintaining or developing databases. Thanks!
Task: Just about any of the above Challenge: Find an existing job, query, SSIS package, trigger, stored procedure that is remotely similar to what you want to do. If you cannot find anything similar to what you need done, Google it. Get that SQL and modify it to do what I need. If you still can't find any similar SQL script, break your task down into steps, and repeat. Industry: Multiple (medical, administrative, financial, education) Platform: MS SQL, MySQL PostGresql, Access
Do you know any C#? Things may be easier if you're not solely requiring on your DB and SQL knowledge. Try to port things over in pieces, if you can, while maintaining the functionality of the project. So take a copy of the access project, maybe you go into it, get it to start populating your SQL DB as well as the access DB. Then start rewriting the different functions, reports, etc from access over to C# and SQL. I like to use stored procedures and call them in simple c# console applications. Then you learn how to automate the access project, and call it from C#. Then finally, you port over any required legacy functionality. Hopefully, end the end, you'll have the application ported over, it will perform a whole lot better, and you'll have been able to shutdown the legacy system. The good thing is, porting VB to C# isn't that bad. Unless you go with VB.NET lol.
If it's truly a public facing server, then it already is open to everyone on the internet.
Query tasks can range from very simple to very comlex depending mostly on if you need the query to be dynamic and how familiar you are with the data but are usually pretty straight forward. For me its maintenance or import tasks that are never as straght forward as they first appear either because you have to sanatize or transform the data most of the time.
[This](http://stackoverflow.com/questions/741414/insert-update-trigger-how-to-determine-if-insert-or-update) should give you the info you need.
Are Temporal Tables going to be available in all editions, or has that not yet been decided/announced?
I expect those announcements/etc to probably come out of Ignite this year.
Task: Work on some extremely complex T-SQL in the form of stored procedures since that's where the majority of the business logic is located at, millions upon millions of lines. Manipulate, Store, and Regurgitate data every which way that business wants it. There are two concepts that are ingrained into every developer there: * Your queries will be hitting massive tables easily surpassing a million entries * Foreign Keys are not allowed, for very good reasons (Whomp... Whomp) Industry: Healthcare Platform: SQL Server 2008r2 (*Whomp... Whomp...*)
I'm sure that will be a feature that nobody will use and everyone will continue to roll their own for.
What region are you located at?
It was an architectural/business decision that the cost to maintain referential integrity is too high with high volumes of data in constant movement throughout the system. Also, the overall architecture doesn't allow for usage and in fact builds on that requirement. 
Oh, I absolutely believe that. I'm sure it's a lot better than rolling it yourself. However, I still see brand new systems that use triggers instead of foreign key constraints. I see brand new systems that use cursors in stored procedures. I see brand new systems that use SELECT DISTINCT ... FROM FULL JOIN FULL JOIN FULL JOIN FULL JOIN instead of PIVOT. I see brand new systems that have ridiculously complex stored procs that could be replaced by a MERGE statement. I see brand new systems rocking ANSI-89 comma joins. My point is that every dev's first choice is "do what you know," and never, "learn what works best." 
EA for state dept. of education Task: keep 18 year old state ODS, LDS, educator licensure and textbook management databases running while we build a new one. Challenges: decisions based on vendor advice rather than industry best practices have us working in an oracle/MySQL/MSSQL mixed environment. Every system has a subject matter expert and knowledge is concentrated in those people. Very little documentation, development environment designed for an organization 3x our size, the people that make funding/personnel decisions think the internet is a program installed on their computers. That being said, I absolutely love my job. My specialty is working with our data science team. When we get special/short term data requests, I use SQL/Pandas/R to clean the data before it goes for analysis. These requests are often 10+ gigs of data and have a delivery window of 4-8 hours. 
Telecom industry: customer experience. I have a ms sql 2008r2 sandbox. Lots of ETL in order to get different data points attached to survey data from data warehouses which include teradata, oracle, SaS, ftp, other ms sql servers. Lots of design and ETL automation making data available for analysts to build reports and presentation. I have a sandbox which is unusual so I have a mix of etl, dev, and DBA experience. I started with populating and building reports. I extract from the warehouses so that i only use my own machine to burn processors and eat up I/O. Survey data is analyzed to look for trends in call centers, and I also provide feeds to text analytics tools in order to analyze free form text, also to find trends and pain points. I'm a data gatherer/data pimp.
Our observation team (call centers) create surveys in sharepoint with upwards of 160 questions and flows within. They make us pull their data and aggregate it to report it. Each little function in the centers has their own survey link in sharepoint, that changes every month. We have to alter an ssis package each month to pull the data.... I hate sharepoint so much.....
DateTime
Not arguing with you. But I want to at least make it difficult to let people see this stuff.
Why do you assume they are the same? There is no immediately obvious relationship between the two values. I'd suggest looking at views or stored procedures in the DB to see how they're being used. My best bet is that the varchar ID is some encrypted or hashed value.
I agree with this thought as well. Double check the column definition in the table. You might have to convert/cast the string as a date time or timestamp. 
I would say *take it*. You'll get a ton of exposure to a ton of different companies, meaning new technologies and new experience every time you meet another client.
&gt;i assume they are the same!?! how do i get from one to another? What are you basing this assumption on? &gt;if these questions werent weird enough for you: all varchar ids i see in this database (of an Android app if that helps) start with an M. is there a reason for that? is there a special name for these 30length ids? You need to ask the creator of the application what their IDs mean and how they're generated. Anything else is purely random speculation.
Recently joined a similar company. It's awesome!@
What database? Different DBs do (Max,Top, limit) differently. basic steps: average all players select max average row use that player id to get all scores select gamename with max score 
You can probably do this a couple ways, but I believe this should work: WITH CTE AS( SELECT Gamename, score FROM Games join Scoring on Scoing.gameID = Games.gameID Where playerID in ( SELECT TOP 1 playerID, avg(score) FROM Scoring ORDER BY avg(score) DESC ) ) SELECT Gamename FROM CTE WHERE score in ( SELECT max(score) from CTE ) As u/THLycanthrope mentioned, you may need to make some changes on the subquery that's limiting the player to the one with the highest average depending on what DB you are using. Note: I should also add that this assumes that a player could have gotten his highest score in more than one game.
TY
https://www.reddit.com/r/RemoteJobr/
If you select the min and max in one fell swoop or interrogation than as I understand it, you'll get a row that meets both sets of criteria. So if you have A 1 1 A 1 2 A 2 2 A 2 3 You won't get an output of A 1 3 you'll get an output of A 1 2 Because A 1 2 is a row with both a low value for column 2 that also has a high value for column 3 By treating the one table like it's two .. then joining on the common value (the row identifier) you satisfy the requirement of finding both the lowest value for column 2 and highest value for column 3 *** I don't know if this explanation helped or was clear as mud. But to put it more simply, when you query a table you get results by the row. If your query is more nuanced and you need to search multiple rows for multiple column results, treat each query as its own query. Good luck 
Sounds like you want an ERP There are some open source options available, such as Odoo (formerly OpenERP) and X-Tuple, SQL-Ledger Odoo is $25 USD / user / month for hosted X-Tuple is $30 / user / month for hosted (plus some upfront fees) SQL-Ledger is self-hosted only AFAIK All are open source, so you could host yourself and not pay for software. Cost of hardware essentially depends on the number of users and how much down-time you can handle. You can get a basic server-class computer for $500, more if you have a lot of users or can't have any down-time (requiring RAID 10, lots of drives, and multiple servers) 
If you are following the Microsoft~ route: * [IT Dev Connections](http://www.itdevconnections.com/dc16/Public/Enter.aspx) * [Microsoft Ignite](https://ignite.microsoft.com/#fbid=Ng8OjF9frge) 
That's really it. Only thing in the window. I get the same error when I use a query as simple as: update Products set REGION = '0382' where CITY = '9382' 
Check if the Products table have any trigger, that may be the reason.
Show us what you got, or what you think is right or close, so far.
If you're on the MS stack, then you can't overlook: * [PASS Summit](http://www.sqlpass.org/Events/PASSSummit.aspx) * [PASS BA Conference](http://passbaconference.com/2016/Home.aspx) * [SQL Saturday](http://sqlsaturday.com/) * [Dev Intersection](https://devintersection.com/#!/SQL-Conference)
Yeah I'm an idiot , ESC fixed it. Lol
I'm using MySQL
The errors you are getting would help: But off hand: In your inner joins script, you are referencing character_actor.actor_name but are not including the Character_actor table. Multiple Joins: Referencing actor.name, but not including actor table. Left Joins: Same, referencing actor.name but not actor table. I didn't see problems with the others, but I expect you might be using incorrect table or fields maybe?
Please post your scripts
I'm clicking around. I like the videos but they lack any real oompf. I feel like you spend way too much time in the beginning explaining what SQL is and talking about it rather than showing it to us so my initial criticism is to come up with a better introduction. What I would do is build a cube of some kind (no joins) where you have lets say (1) year of production data summarized. Maybe it's all your sales, by day, by salesman, or whatever is something relevant to your business that your coworkers are going to understand right off the bat. Then do a demonstration, or an interactive demonstration. Don't tell them what SQL is, or what a query is... show them. So maybe you type out the basics for them: SELECT FROM WHERE And then go a step further to explaining how intuitive it is. For example: "So `SELECT` is what we want, right? We might want to get &lt;insert example 1&gt;, or &lt;insert example 2&gt;. We're going to `SELECT` the data we want." "Now `FROM` just tells us where we're going to get it from. If we were selecting something about baseball we would `SELECT FROM Baseball`, but for our purposes we're going to `SELECT FROM Cube`" "`WHERE` is what's going to let us pick what we want to select. For example `SELECT FROM Baseball WHERE League = 'National'` is going to give us only data from the National league and none of the data from the American league." Then give them a verbal problem: "So, how what would it look like if we wanted to get all of the sales for 2015?" At this point a student should be able to say something like this: SELECT Sales FROM Cube WHERE Year = '2015' Now if you're clever this isn't going to work. Why? Because the [Cube] table might have the [Year] column named something like [SaleYear] ---&gt; at which point you can now show them all the wonderfully intuitive named columns in your example table. This is going to let them mentally understand how to ask any basic question they want. `WHERE Salesman = 'X'`, etc. *and* show them how to look up the names of the columns (which will come in handy below when you talk about joins.) But first spend the next 20-30 minutes having them write &amp; practice very simple queries that would be intuitive for an Excel user. Show them what the data looks like when it comes out, and then start asking questions like, "If you were using Excel how would you *sum* the data." You can then quickly cover group bys (which will confuse them, so don't explain it, just tell them they have to do it whenever they sum, you can come back to explain why another day,) but what you're really doing is empowering your students so that they feel like they can use your [Cube] right off the bat to get simple answers. You can show them how to Google for names of functions such as `MAX()`, or `MIN()`, or how they can type equations right into the `SELECT` such as `SUM(Sales) * .06` (if your state tax is 6 cents, etc.) I would cover all of that verbally with a PowerPoint and maybe switching to a SQL window where you're typing in the queries as they're telling them to you (even if they're wrong. Let them see the error messages and understand how to use them... and understand they aren't breaking anything.) Then I would gear your online material around this first lesson. Start off by saying something like, "OK, now that you have mastered the basics lets go over a few terms. SQL stands for structured query language and... blah blah blah." You need to draw them in and get them thinking in SQL *before* you tell them what it is or what they can do. For example the concept of a `JOIN` is 100% intuitive if you show someone a table of baseball players, and their statistics and then ask how you would only pick players from the local team. Inevitably one of the smarter people is going to tell you that you can't do it because there is no column for [TeamName]... aha you will say, very perceptive, thats right, so then *show them* the Team table and then ask them what is common between the two tables... and hopefully then everyone will say, "duh, the player table has a team ID and the team table has a team ID..." That's when you suddenly pull the word `JOIN` out of your ass and say, 'right, yes, ok so we want to `JOIN` the tables together, right?' SELECT PlayerName FROM Players JOIN Teams ON Teams.ID = Players.TeamID And then to make sure they're paying attention run the query and show them how it gives you all the players and the name of their teams. Remind them that you're only trying to get the players from your local team and ask how to do that... aha, another bright student will tell you about `WHERE`. If I were you I would be asking a lot more questions and not making so many statements. People learn and engage when you ask them questions and you make sure you give them enough information to answer them as they listen. If you just talk and talk and talk in statements they tune out. For work I think what you've done is fine and you won't be embarrassed, but do you want to just look good or do you want to really make a difference and help your coworkers learn SQL? Personally I would rather be embarrassed during the first meeting if in the long run everyone learns SQL and looks back and says how instrumental I was to helping them than looking good on day one and everyone struggling to learn it, or finding out that those who successfully learned it did so by going to other tools besides mine. 
Get a database/SQL course textbook. It'll be filled with review questions you can work through over and over again.
SELECT character.name, character_actor.actor_name FROM character INNER JOIN character_tv_show ON character.id = character_tv_show.character_id; At first glance, you are referencing a table (character_actor) that's not used in the FROM or JOIN: **character_actor.actor_name** Your JOIN looks OK, though. Glancing through the others and they all seem to have the same problem. Are you getting an error on #27? Finally, "but I keep getting errors" is not nearly as helpful as "The errors I'm getting are _______"
http://beyondrelational.com/puzzles/default.aspx
Yikes! that's almost a form of torture. Anyway, address the table references issue I mentioned and see what happens
I could solve it, read my other reply... I feel so stupid lol.
Good catch. Very common for developers to assume an update trigger will always operate on a single row....nothing is always in sql though....
To be fair, your problem isn't JOINs..it's referencing tables that aren't used. Meaning in this construct: **SELECT ..... ABC.Column** **FROM ....** **JOIN .....** There has to be some "thing" named ABC. Usually that "thing" is a table, but it can also be a table alias: **FROM SomeLongAssTableNameTooBigToUseEverywhere AS ABC** (the "thing" can also be in the JOIN clause, of course) 
I really appreciate your feedback! I agree, I did make a lot of statement rather than trying to engage users by asking simple but thought provoking questions. The goal of doing this is to teach my co-workers how to dig through our database. Our analytics department is super small (i.e. 3 people, supporting 200 people). So my team's goal is try to teach those who are interested how to use our DB and get the data and create charts themselves. Again thank you and I really appreciate your feedback! :)
It is the double quotes. Oracle object names aren't case sensitive as long as you don't use quotes.
You could always try to convince your boss to let you go on a SQL Cruise... Although, I've heard that the total cost of attendance is actually competitive.
You might have more luck in a forum that deals will shell scripting, if that is where you think the errors are.
Yeah you're right it's thinking it's a name of a DB column instead of a string for the first error. Code looks like perl or something I haven't worked with that in a while but try something like : author=($(grep "&lt;Author&gt;" $file | sed 's/&lt;Author&gt;//g')); author = "'" + author + "'" Whatever string appending is used use that to append a single quote on either side of the value. Second error content: There's something in that string that is formatted incorrectly. Could be a quote or some sort of formatting code CRLF or something. Try copying the content to a .txt file and see if it errors the same. If it does there's some formatting character in there. If not it's a different character you'll have to replace. A quote or comma or something you'll have to replace with a pipe or use an escape sequence to include it. Depends on where it's being stored, what data type etc... Third issue with the date: What data type is the column it's going into? If it's a date time then you'll have to format that string in a way that makes sense to the DB as a date time. DB's don't know that the string "Feb 3, 2010" means that date unless it's in a very specific format something like "02/03/2010". So you can either put that value into a regular varchar type column that doesn't care or format it correctly. I don't see the SQL for the value but the data seems like it's all a single number the fact that it's saying there's a comma makes me wonder what kind of data type that column is. If it's a varchar and you don't have single quotes around it then I'm guessing it would throw an error like that. If it's numeric then the value may not be making it's way to the variable. Hope that helps a bit. 
Any recommendations?
Try this web site http://www.studybyyourself.com/seminar/sql/course/?lang=eng. There you can submit exercises and get a feedback right away. It is free and they provide the solution in case you cannot solve one exercise.
Looking over your script it seems that there are a few problems: 1. First problem you have is that you don't validate the input to the script. This could be a big issue considering the way you are using the input. If there is no parameter passed then it will start iterating from the root directory ($1/* with $1='' means /*). 2. Also with the line $1/* , you should only check for the file types you are expecting ($1/*.dat) otherwise it could run for non *.dat files and give you weird output. 3. The reason that the author/content/date line was not working is that you are not quoting the value so it thinks that you are passing in a column or identifier name. You should of wrapped your variable in single quotes ('author' --&gt; '${author[i]}') and that would fix the issue. Bash will not interpret it as a literal value since the whole string is wrapped in double quotes. 4. The reason columns 9 and 10 (value and rooms) are not working is because your grep statements are wrong. value=($(grep "&lt;Values&gt;" $file | sed 's/&lt;Value&gt;//g')); You have an extra "s" at the end of "&lt;Value&gt;" in the grep statement. It should be : value=($(grep "&lt;Value&gt;" $file | sed 's/&lt;Value&gt;//g')); The problem with rooms is that it is not Room but Rooms: rooms=($(grep "&lt;Room&gt;" $file | sed 's/&lt;Room&gt;//g')); Should be: rooms=($(grep "&lt;Rooms&gt;" $file | sed 's/&lt;Rooms&gt;//g')); 5. There is an additional issue with "content" that requires and extra replacement. Since "content" is written by a user they can use single quotes in their reviews. You must escape the single quotes inside of content values. In SQLITE you will have to make a ' into '' for it to be escaped (Pascal style as C-Style is not supported). Modify the "content" line from this: content=($(grep "&lt;Content&gt;" $file | sed 's/&lt;Content&gt;//g')); To this and it will replace the single quotes with two single quotes: content=($(grep "&lt;Content&gt;" $file | sed "s/&lt;Content&gt;//g;s/'/''/g")); Here is the modified shell script: if [ -z $1 ]; then echo "You must supply a directory containing the *.dat files you wish to proceess" exit 1; fi; IFS=$'\n' for file in $1/*.dat; do author=($(grep "&lt;Author&gt;" $file | sed 's/&lt;Author&gt;//g')); content=($(grep "&lt;Content&gt;" $file | sed "s/&lt;Content&gt;//g;s/'/''/g")); date=($(grep "&lt;Date&gt;" $file | sed 's/&lt;Date&gt;//g')); readers=($(grep "&lt;No. Reader&gt;" $file | sed 's/&lt;No. Reader&gt;//g')); helpful=($(grep "&lt;No. Helpful&gt;" $file | sed 's/&lt;No. Helpful&gt;//g')); overall=($(grep "&lt;Overall&gt;" $file | sed 's/&lt;Overall&gt;//g')); value=($(grep "&lt;Value&gt;" $file | sed 's/&lt;Value&gt;//g')); rooms=($(grep "&lt;Rooms&gt;" $file | sed 's/&lt;Rooms&gt;//g')); location=($(grep "&lt;Location&gt;" $file | sed 's/&lt;Location&gt;//g')); cleanliness=($(grep "&lt;Cleanliness&gt;" $file | sed 's/&lt;Cleanliness&gt;//g')); receptionarea=($(grep "&lt;Check in / front desk&gt;" $file | sed 's/&lt;Check in \/ front desk&gt;//g')); service=($(grep "&lt;Service&gt;" $file | sed 's/&lt;Service&gt;//g')); businessservice=($(grep "&lt;Business service&gt;" $file | sed 's/&lt;Business service&gt;//g')); length=${#author[@]} hotelID="$(echo $file | sed 's/.dat//g' | sed 's/[^0-9]*//g')"; for((i = 0; i &lt; $length; i++)); do sqlite3 test.sql "INSERT INTO HotelReviews VALUES($hotelID, $i, '${author[i]}', '${content[i]}', '${date[i]}', ${readers[i]}, ${helpful[i]}, ${overall[i]}, ${value[i]}, ${rooms[i]}, ${location[i]}, ${cleanliness[i]}, ${reception$ done done Let me know if this solves your issues. *edit to fix formatting issues.
Additionally, if you or your company is paying money for it is reasonable to ask them to get it right. 
unfortunately the column isn't at the end. we have a resolution going forward, but i still see places where it can fail going forward. ah well i dont have the authority to make them change it, but have been working with those that do... Damn Project managers.
we have, although im not completely sold on the solution. They are injecting a java script at the time that the customer presses submit to clean the quotes. Why the hell dont they do that in the ETL process that provides the CSV is beyond me. any time someone has a browser with java disabled we can have the same issues pop up.
ill give this one a try, thanks a ton for the code. It would populate a trash bin type file that we can show them, hey idiots, it isnt working. 
No, I mean: is someone forcing you to keep everything SQL because of a business decision or because this is a school assignment, or are you free to choose technology for yourself? If I were in your shoes and I was free to do whatever I wanted, I'd write a small script (or find one online) that takes the data from SQLite and puts it into something like the ELK (Elasticsearch, Logstash, Kibana) stack, which has built-in capabilities for doing those kinds of queries very efficiently and easily, like demonstrated on actual stock data here: http://blog.webkid.io/visualize-datasets-with-elk/
Thing about Project Managers, they won't do anything about it until it starts to hurt them in some personal manner. Indicate that it will take 400% of estimate to complete the work, as the incoming data quality is so below industry standards that it requires bespoke pre-processing, just to make it into a usable CSV. Once they can no longer just tick the boxes on their plan, they might actually extract a digit. 
This is Red Flag material right here. Drop them like a hot brick.
That would have been my approach as well. Build 5 min increment table with cte and aggregate data on to it
Does the column after the problematic one have a specific data format?
This is actually really easy. Just GROUP BY {five minute interval, rounded}. So, for a single day: SELECT AVG(PRICE), MIN(TIME) AS firstSample, MAX(TIME) AS lastSample, CAST((SUBSTR(TIME, 1, 2) * 60 + SUBSTR(TIME, 4, 2)) / 5 AS INTEGER) AS nthFiveMinuteInterval FROM tbl WHERE DATE=19990603 GROUP BY CAST((SUBSTR(TIME, 1, 2) * 60 + SUBSTR(TIME, 4, 2)) / 5 AS INTEGER) Obviously that time conversion is a bit crappy, so feel free to come up with something better. But the notion is there. What you called "segment", I called "nthFiveMinuteInterval"; basically it's just using a closed form calculation instead of a lookup table. If you want to do this "for all days", you can add DATE to the GROUP BY columns, instead of using a WHERE clause to get it for one day.
This may work for you: http://stackoverflow.com/questions/830792/t-sql-round-to-nearest-15-minute-interval ...though I can't be sure that it's all valid in SQLITE, as it's meant for TSQL. The same basic principle should be do-able in SQLITE. So, first get your date/time into an actual datetime field, then update it using a function like the one in that stack overflow post. Then you can group on your new field, and run your price averages in the aggregate. Edit: Here's something that should work, I only tested with a few rows though: SELECT t.SYMBOL, t.DATE, t.TIME, SUBSTR(t.DATE, 1, 4) || '-' || SUBSTR(t.DATE, 5, 2) || '-' || SUBSTR(t.DATE, 7, 2) || ' ' || TIME AS original_dt, SUBSTR(t.DATE, 1, 4) || '-' || SUBSTR(t.DATE, 5, 2) || '-' || SUBSTR(t.DATE, 7, 2) || ' ' || SUBSTR(t.TIME, 1, 2) || ':' || SUBSTR(((SUBSTR(t.TIME, 4, 2) / 5) * 5) || '00', 1, 2) || ':' || '00' AS rounded_dt FROM Test t It's basically diving the minutes by 5, which drops the remainder (and doesn't cause a divide by zero error in SQLITE), then multiply back to get the rounded number, pad the minutes with leading zeroes if necessary, and drop the seconds.
What is the schema of your database? How are your times stored? Assuming they are stored as strings (as your sample shows), how about this? SELECT avg(PRICE), DATE, min(TIME) FROM stocks GROUP BY strftime('%s', DATE || ' ' || TIME) / (5 * 60); The `GROUP BY` clause says process this query in groups of records for which the record on the right is the same. On the right, I divide the date (represented as seconds since 1970) by 5 so all records within a five second intervall map to the same value. The `avg()` and `min()` functions operate on each group of records separately. Optionally, if you want to restrict this to a certain day, add a `HAVING` clause: SELECT avg(PRICE), DATE, min(TIME) FROM stocks GROUP BY strftime('%s', DATE || ' ' || TIME) / (5 * 60) HAVING DATE=19990603; Note though that your date format is a bit strange and might need some preprocessing.
See my comment for a simpler approach.
Yep I'm trying to find the GROUP BY ... That actually works for my date format. Is there anyway to get like some REPL like in Python for SQL? 
the foreign key should not be from films to person_film... it should be the other way around
In this case, the query from above should work. Make sure you got my last change, I forgot the `* 60`, so the query took five-second averages instead of five-minute averages.
this makes sense. thanks for the help
thanks i will definitely check into that!
Maybe something like this will return ranges you look for: select t.TIME || "-" || (t.TIME+5), avg(x.PRICE) from table t, table x where t.TIME &lt;= x.TIME and x.TIME &lt; t.TIME+5 group by t.TIME; I checked it on integers and it seems to work fine, but you'll probably need to play with time formatting in order to add 5 minutes properly.
There is another free program out there called [CSVED](http://csved.sjfrancke.nl/) that can import broken files to clean up, you might also give it a try.
Thanks a lot for helping! I really made some progress these last couple of hours. My current query # "15:39:20" takes 15x60 minutes + 39/5 as nthFiveMinInterval # !! Obvious error when single digit hour, i.e. 9 !! test2 &lt;- dbGetQuery(db," SELECT DATE, AVG(PRICE), MIN(TIME) AS firstSample, MAX(TIME) AS lastSample, CAST((SUBSTR(TIME, 1, 2) * 60 + SUBSTR(TIME, 4, 2)) / 5 AS INTEGER) AS nthFiveMinuteInterval FROM currentdata WHERE DATE=19990603 GROUP BY CAST((SUBSTR(TIME, 1, 2) * 60 + SUBSTR(TIME, 4, 2)) / 5 AS INTEGER) ") Bugs out when the hour number is single. I got some really strange results and after some thinking I figured it out. Can I implement some IF statements in SQL as well? Thankfully it only matters for the first half hour from 9:30 to 10:00 but still! 
 DECLARE @TimePeriods TABLE (PeriodStart time, PeriodEnd time); WITH TimeIntervals AS ( SELECT CAST('0:00' AS Time) AS PeriodStart, CAST('0:05' AS Time) AS PeriodEnd UNION ALL SELECT DATEADD(MINUTE, 5, PeriodStart), DATEADD(MINUTE,5,PeriodEnd) FROM TimeIntervals WHERE PeriodStart &lt; PeriodEnd ) INSERT INTO @TimePeriods SELECT * FROM TimeIntervals OPTION (MAXRECURSION 0) SELECT SYMBOL, DATE, PeriodStart, PeriodEnd, xPrices.PriceAverage FROM @TimePeriods AS TP OUTER APPLY (SELECT SYMBOL, DATE, AVG(PRICE) AS PriceAverage FROM @YOURTABLE AS YOURTABLE WHERE YOURTABLE.TIME BETWEEN TP.PeriodStart AND TP.PeriodEnd GROUP BY SYMBOL, DATE) xPrices This is likely not the best solution, I'm actually fond of /u/chunkyks method. Also I have no idea if this will work in SQLite. Never used it, no way to test.
notepad++ is probably better. I've honestly never used it and only hear great things. If you're already skilled at using the find and replace options in notepad++ then it's the same logic in editpad lite. When I drop 4 columns of data into editpad lite I use the find and replace option to do something like this: **ID, Data, timestamp** **14, Text, Sept9** **16, Text2, Sept10** Then I replace carriage returns with a carriage return + some code on the bottom line. "insert into table (id, data, timestamp) select '" Then I replace the tabs with ', ' I end up with something like **insert into table (id, data, timestamp) select '14', 'Text', 'Sept9'** **insert into table (id, data, timestamp) select '16', 'Text2', 'Sept10'** Then later I'll grab my dirty data: **dirty data, 14** **dirty data2, 16** and I'll do the same logic but this time with updates. Update table set dirty_data_column = ' is the first part I'll add followed by: where id = ' Which results in **update table set dirty_data_column = dirty data where id = '14'** Sorry if this isn't helpful. As I'm typing all this out I'm realizing that you may be several skill levels ahead of me and I've possibly wasted your time. 
Why don't you use the `strftime()` from my query? It does the job fine and much simpler than this.
Probably select distinct off of a cartesian product/cross join.
 test &lt;- dbGetQuery(db," SELECT DATE, AVG(PRICE), MIN(TIME) AS firstSample, MAX(TIME) AS lastSample, FROM currentdata WHERE DATE=19990603 GROUP BY DATE, strftime('%s', TIME) / (5 * 60) ") Gives this output, which is still not really what I want https://i.imgur.com/9RgcZcv.png It still errors on single digit hours ... and there is indeed data before this https://i.imgur.com/b1XyZff.png
&gt; GROUP BY DATE, strftime('%s', (CASE WHEN TIME LIKE '_:__:__' THEN '0' || TIME ELSE TIME END)) / (5 * 60); It works! You're my hero for today. Thanks so much man it's really appreciated I would not have figured this out without you (and all those who helped as well). Thanks again! 
Yeah, a CSV would be nice. Are you allowed to preprocess the data before loading it into the database? Because you could make your life much easier if you were to convert the date and time format into something SQLite likes before loading the data into the database. Anyway, does this query work?
Okay, then you are out of luck, unless you are allowed to modify the dataset that is, but I don't think you are.
Well, then you could run a query like that to make builtin date and time functions work correctly: UPDATE currentdata SET DATE = substr(DATE,1,4) || '-' || substr(DATE,5,2) || '-' || substr(DATE,7,2)); UPDATE currentdata SET TIME = '0' || TIME WHERE TIME LIKE '_:__:__'; The first query fixes the dates, the second one fixes the times. Now you can use `DATE` and `TIME` with the builtin date and time functions. Though, you should really put date and time into a single column as the two belong together.
I will need this in the future probably. I only used one day today because I wanted to get it to work for a single day first. Would you mind if I comment on this tomorrow or in the future to ask for some help if needed? 
I think the method I used here and asked about in this thread would do it for you: https://www.reddit.com/r/SQL/comments/3qr8y3/ms_sql_how_does_this_xmlbased_aggregation_work/
If the first two columns really are just simple values like your example, then this is very simple in any programming language. - Read a line... pull the data from position 0 until the first comma. - Pull the data from the 1st comma to the 2nd comma. - Pull all remaining data. There's your 3 elements. - Clean up the 3rd element by changing the inner double quotes to two double quotes, and qualify the element in double quotes, like this: - &gt; d,2,"omg, like ""really"", i mean cmon, this is horrible" This is a format that most CSV parsers (such as good old LumenWorks) can parse successfully.
This is really close. I'm new to SQL so pardon the noob question, but how to I get rid of the XML tags in my result set?
Here's a SQL Server solution you probably can rewrite for whatever platform you're working with: /* Delete Duplicate records */ WITH CTE (COl1,Col2, DuplicateCount) AS ( SELECT COl1,Col2, ROW_NUMBER() OVER(PARTITION BY COl1,Col2 ORDER BY Col1) AS DuplicateCount FROM DuplicateRcordTable ) DELETE FROM CTE WHERE DuplicateCount &gt; 1 http://blog.sqlauthority.com/2009/06/23/sql-server-2005-2008-delete-duplicate-rows/ Though, you'll probably have to put a row count and a null count aggregate in there. Something to the extent of: Select Count(Case IsNull(Desc,'1') When '1' then 1 else 0 end)
For MSSQL: The random part is the order by newId() - It generates a random Id for each row, then orders by that. select top 10 * from software where name = 'Unbuntu' order by newid() union all select top 20 * from software where name &lt;&gt; 'Unbuntu' order by newid() 
Not looking to delete the rows, just remove them from my query results. The Partition By might do the trick though, I'll have to re-read how those work.
Select columns from table where desc IS NOT NULL
That will get rid of 6 and 7. I need null records unless desc has something in it.
What does your result set look like now? Here's fake data in my table: user_id store_number user1 123 user1 456 user1 789 user2 123 user2 789 And here's the result set I get from them: user_id stores user1 123;456;789; user2 123;789; So I was thinking you'd be close already with just switching the semicolon with a comma. 
I figured was the issue was.
First of all, write which RDBMS you are using, but I`ll guess it is Oracle. Secondly, exactly what /u/asdubya said, but I was curious about the code and I formated that for you, here: WITH CST AS ( SELECT TO_CHAR(DCC_BUSN_ENTTY_ID) Cluster_ID, DCC_BUSN_ENTTY_ID GCP_ID, DCC_BUSN_ENTTY_ID LOCATION_ID, DCC_PROD_ID PRODUCT_ID, DCC_BUSN_ENTTY_ID Application_ID, DCC_PROD_ID BIT_VECTOR_TYP_CD , EQPMT_LOC_ASSCN_ID AUDIENCE_ID , EQPMT_LOC_ASSCN_ID ATTRIBUTE_ID, SUM(PHY_CASES) PHY_CASES FROM (SELECT itc.EQPMT_LOC_ASSCN_ID, ELA.EQPMT_LOC_ASSCN_PRNT_ID, dbe.DCC_BUSN_ENTTY_ID, itc.DCC_PROD_ID, ITC.FILLED_DTM, itc.NATL_RTE_ID_VAL ROUTE_NM, itc.FILLED_CASES_QTY AS PHY_CASES a FROM SEN_DATA.SALES_HISTORY_13_MONTH itc JOIN sen_data.product prd ON prd.dcc_prod_id = itc.dcc_prod_id AND prd.ROW_DEL_FLG='N' JOIN sen_data.product_category_association pascn ON pascn.dcc_prod_id =prd.dcc_prod_id AND pascn.ROW_DEL_FLG='N' JOIN sen_data.eqpmt_loc_asscn ela ON ela.EQPMT_LOC_ASSCN_ID=itc.EQPMT_LOC_ASSCN_ID AND ela.ROW_DEL_FLG ='N' JOIN sen_data.dcc_busn_entty_eqpmt_loc dbel ON dbel.DCC_BUSN_ENTTY_EQPMT_LOC_ID=ela.DCC_BUSN_ENTTY_EQPMT_LOC_ID AND dbel.ROW_DEL_FLG ='N' JOIN sen_data.dcc_busn_entty dbe ON dbe.DCC_BUSN_ENTTY_ID=dbel.DCC_BUSN_ENTTY_ID AND dbe.ROW_DEL_FLG ='N' ) GROUP BY TO_CHAR(EQPMT_LOC_ASSCN_ID) , EQPMT_LOC_ASSCN_ID , DCC_BUSN_ENTTY_ID , DCC_PROD_ID , DCC_BUSN_ENTTY_ID , DCC_PROD_ID , EQPMT_LOC_ASSCN_ID , EQPMT_LOC_ASSCN_ID , FILLED_DTM ) , A AS ( SELECT MONTHS_BETWEEN(CURRENT_DATE, CURRENT_DATE-30) Months_of_Cases_Sold, PRODUCT_ID, Location_ID, NULL Cluster_ID, NULL BIT_VECTOR_TYP_CD, NULL GCP_ID, NULL APPLICATION_ID, NULL AUDIENCE_ID, NULL ATTRIBUTE_ID, NULL PRODUCT_NM, 1 RANK, SUM(PHY_CASES )Unit_cases FROM CST GROUP BY Location_ID, PRODUCT_ID, CLUSTER_ID, GCP_ID, APPLICATION_ID, BIT_VECTOR_TYP_CD, AUDIENCE_ID, ATTRIBUTE_ID ), B AS ( SELECT MONTHS_BETWEEN(CURRENT_DATE, CURRENT_DATE-30) Months_of_Cases_Sold, Location_ID, SUM(PHY_CASES) Location_Cases FROM CST GROUP BY Location_ID ) SELECT A.PRODUCT_ID, A.Location_ID, SUM(A.Value/B.Location_Cases) Value FROM A, B GROUP BY A.PRODUCT_ID, A.Location_ID Finally, without a sample data, we can't tell if the result is correct ...
Well the code works I know that but i was wondering if you would know mathematical how to get that "pen" 
So what do you do when you observe trends? GOod and bad..
If you just want the code, here it is: https://www.sqlteaching.com/#!multiple_joins SELECT character.name, actor.name FROM character INNER JOIN character_actor ON character.id = character_actor.character_id INNER JOIN actor ON actor.id = character_actor.actor_id https://www.sqlteaching.com/#!left_joins SELECT character.name, actor.name FROM character LEFT JOIN character_actor ON character.id = character_actor.character_id LEFT JOIN actor ON actor.id = character_actor.actor_id https://www.sqlteaching.com/#!table_alias SELECT c.name, a.name FROM character as c LEFT JOIN character_actor as ca ON c.id = ca.character_id LEFT JOIN actor as a ON a.id = ca.actor_id https://www.sqlteaching.com/#!column_alias SELECT character.name as character, actor.name as actor FROM character LEFT JOIN character_actor ON character.id = character_actor.character_id LEFT JOIN actor ON actor.id = character_actor.actor_id
Word
OH I just noticed that it is 4 links, in that case I gave the answer to link 1 and 3, give me a sec for 2 and 4
All I need is Table Alias now and i'm done :)
thank you so much for these! you are an sql god. haha
FAAAAR from that. I'll tell you, you really should understand those concepts of multiple JOINs and aliases, that is the basic of SQL and whatever IT path you take you will need it for sure or even if you are taking the analysis path you will also need it.
thanks for the tip but i don't plan on doing IT or data analysis or anything like that.
There's a tutorial on Microsoft Virtual Academy
Thought I'd use your insult list for a quick and dirty SQL version; CREATE TABLE #Insults1 (Word1 varchar(50)) CREATE TABLE #Insults2 (Word2 varchar(50)) CREATE TABLE #Insults3 (Word3 varchar(50)) CREATE TABLE #Insults4 (Word4 varchar(50)) INSERT INTO #Insults1 (Word1) VALUES ('tossing'),('bloody'),('shitting'),('wanking'),('stinky'),('raging'),('dumb'),('fucking'),('holy'),('cocking'),('ranting'),('twunting'),('hairy'),('spunking'),('slapping'),('sodding'),('blooming'),('frigging'),('sponglicking'),('cock wielding'),('failed'),('artist formally known as'),('pulsating'),('throbbing'),('lonely'),('failed'),('spastic'),('senile'),('strangely shaped'),('virgin'),('fat'),('gigantic'),('sticky'),('bald'),('bearded'),('horse-loving'),('spotty'),('spitting'),('fritzl-admiring'),('friend of a'),('indeterminable'),('overrated'),('fingerlicking'),('diaper-wearing'),('leg-humping'),('gold-digging'),('mong loving'),('trout-faced'),('cunt rotting'),('inbred'),('annoying'),('slutty'),('cross-eyed'),('scurvy looking'),('cunting'),('wrinkled old'),('dried out'),('wanktastic'),('bum-banging'),('cockmunching'),('animal-fondling'),('scruffy-looking'),('glorious'),('utter'),('complete'),('enormous'),('go suck a'),('fuckfaced'),('broadfaced'),('son of a'),('pigfaced'),('treacherous'),('retarded') INSERT INTO #Insults2 (Word2) VALUES ('cock'),('tit'),('cunt'),('wank'),('piss'),('shit'),('arse'),('sperm'),('nipple'),('anus'),('colon'),('shaft'),('dick'),('poop'),('semen'),('slut'),('suck'),('scrotum'),('cock-tip'),('tea-bag'),('jizz'),('cockstorm'),('bunghole'),('shitface'),('ass'),('nut'),('tramp'),('cum'),('sphincter'),('turd'),('cocksplurt'),('cockthistle'),('dickwhistle'),('gloryhole'),('gaylord'),('spazz'),('nutsack'),('fuck'),('spunk'),('fuckwit'),('clusterfuck'),('douchewaffle'),('retard') INSERT INTO #Insults3 (Word3) VALUES ('force'),('bottom'),('hole'),('goatse'),('testicle'),('balls'),('bucket'),('biscuit'),('stain'),('boy'),('flaps'),('erection'),('twat'),('twunt'),('mong'),('faggot'),('pirate'),('asswipe'),('minge'),('jockey'),('cockroach'),('worm'),('MILF'),('spunk-bubble'),('bollock'),('tranny'),('knob'),('nugget'),('king'),('hole'),('kid'),('trailer') INSERT INTO #Insults4 (Word4) VALUES ('licker'),('lover'),('shiner'),('blender'),('fucker'),('assjacker'),('butler'),('packer'),('rider'),('wanker'),('sucker'),('felcher'),('wiper'),('experiment'),('wiper'),('bender'),('dictator'),('basher'),('piper'),('slapper'),('fondler'),('plonker'),('bastard'),('handler'),('herder'),('fan'),('amputee'),('extractor'),('professor'),('graduate'),('voyeur') SELECT TOP 1 Word1 + ' ' + Word2 + ' ' + Word3 + ' ' + Word4 [You're a what?] FROM #Insults1 CROSS JOIN #Insults2 CROSS JOIN #Insults3 CROSS JOIN #Insults4 ORDER BY NEWID() DROP TABLE #Insults1 DROP TABLE #Insults2 DROP TABLE #Insults3 DROP TABLE #Insults4
Remember that doing a cross join gives you the product of the count of each table. 175 \* 103 \* 61 \* 30 = 32,985,750 rows, just to select one! 
That looks to be a copy/paste misfortune. That should be &gt;= 
No problem. That happens :) Happy reddit b-day
Making your life easier in the future.. follow the advice you've been given on formatting your SQL and the reddit add on, indent all code by 4 spaces so that formatting is preserved when you paste it into a message. I'm surprised no one pointed out the smart quotes in your original post (the source of your invalid characters) but thankfully you figured that one out yourself. What are you using to create/edit your SQL? My personal preference is [UltraEdit](http://www.ultraedit.com/) but others swear by [VIM](http://www.vim.org/), [TextPad](https://www.textpad.com/), and [Notepad++](https://notepad-plus-plus.org/) These will all help you avoid smart quotes as well as each offering their own suite of tools for making your SQL easier to read/edit. 
Yep, operator cost is under 400 on my system. I consider that fine on a silly script like this :) Edit: You could change the final select to something like below which drops the operator cost to 0.06 on my system; SELECT a.Word1 + ' ' + b.Word2 + ' ' + c.Word3 + ' ' + d.Word4 [You're a what?] FROM (SELECT TOP 1 Word1 FROM #Insults1 ORDER BY NEWID()) a CROSS JOIN (SELECT TOP 1 Word2 FROM #Insults2 ORDER BY NEWID()) b CROSS JOIN (SELECT TOP 1 Word3 FROM #Insults3 ORDER BY NEWID()) c CROSS JOIN (SELECT TOP 1 Word4 FROM #Insults4 ORDER BY NEWID()) d
You are actually not storing it, you are just referencing the logical set by using a common table expression. Look at the query execution plan to determine if the optimizer is efficient.
Good to know, thank you. Still learning...
I have an idea, it's wasteful, but it should pretty simple. Why not just add a ton of extra columns that are varchar maxes? (I deal in 100's of TB of DB's. Space is nothing to us. I have worked in shops where my laptop had more ram than the Server itself. So I can understand if this is not doable because of resources.) Import based on csv. What happens? Col 1 is always Col 1, Col 2 is always Col 2. Col 3 is either the full or partial of the string and Col 4 or 5 or 6 or 7, etc is the remainder. From there you can write a CTE or Cursor or Case when or even just a simple concatenate that takes all the extra columns and merges them back into one. Then you can run an insert into another table and drop this crappy huge staging table. Just wanted to give another idea I hadn't seen anyone here list. Is it dirty? Ya, just like your data. Is it scalable? Probably not. Is it fast and probably will work? Likely.
Unless there's a very good reason, don't mess with the auto-incriminating column. Having gaps in the value is just fine and trying to reset it is going to be a mess. As for how to remove the rows, how about something like DELETE FROM db_films WHERE id NOT IN ( SELECT MAX(id) FROM db_films GROUP BY NAME ,link ,YEAR )
insert into newtable select distinct... From duplicated table? *on mobile
If I understand correctly, you're trying to return a random row, correct? There's a lot of ways to do this but one way it could be accomplished completely in SQL would be to randomize the rows via GUIDs and take the top 1. Scale-able and it doesn't matter if you have to remove rows down the line. SELECT TOP 1 NAME, link, YEAR FROM ( SELECT *, UUID() AS GUID FROM db_films ) AS t1 ORDER BY GUID
I thought of that as well and literally just tried it. Gave the same not a GROUP BY expression on the SUM PARTITION OVER line 8(
No it doesn't; and it's ridiculous. You can only have 1 with statement in Teradata. At least the version I'm using. My boss mentioned that he heard more recent versions of Teradata can handle it but we are both unsure. 
Unless you're doing recursion, why use cte? A sub query would get you the same result. Less readable of course, but still functional.
So I updated my database with your commands UPDATE currentdata SET DATE = substr(DATE,1,4) || '-' || substr(DATE,5,2) || '-' || substr(DATE,7,2)); UPDATE currentdata SET TIME = '0' || TIME WHERE TIME LIKE '_:__:__'; and now the format is something like this SYMBOL DATE TIME PRICE AAPL 1999-06-02 09:59:54 46,50000 Should I just merge DATE and TIME together now and SQL will think everything's allright? 
I agree, but sometimes it is unavoidable as the logic gets more complex. my workaround is to create temp tables. 
The `||` operator concatenates strings. So `'foo' || 'bar'` is `'foobar'`. I've linked the syntax page of SQLite before, you mig want to read it.
The following table maybe? (user, badgetype, score) and then you can flair it depending on the score.
sqlitebrowser seems to implicitly put transactions around statements you type. Try to remove the `BEGIN TRANSACTION` and `END TRANSACTION` to solve this problem.
I figured something like that as well, I'm running the query (as we speak) line by line now and make some manual backups. The file Im working with is thankfully not too large
It's a pleasure to me.
Yes. Read about the `ATTACH DATABASE` statement.
You can create an index on an expression, be it a mathematical expression of some sort, or even a function, just go with `create index yourindex on yourtable ( (a-b) );`. [Example](http://sqlfiddle.com/#!15/e17cf1/4).
Sorry, my knowledge about Django is limited to "it's written in Python", so I wouldn't know, but I suspect you have to do it directly.
Sort of, it was literally a blast course of SQL and they gave us a few questions to solved and emphasized using the internet to help, so here I am. Thanks for your hints. I'll see if I can work my way around it. :) 
get it to print out oCMD.CommandText as well as the parameters right before testingConnection.ExecuteNonQuery() and post what it is actually trying to do rather than what you think it should be doing. I'm going to assume though that userNameText.Text or userNameText.Text has something is making the insert fail, which is why I'm asking for the commandtext and parameter content of the object. Also as a completely unrelated piece of advice. Build ONE single function that deals with parameter based nonqueries and toss an object at it instead of rewriting the code every time. Like this Function ExecuteParam(ByVal oCMD As SqlCommand) As Integer Dim iRet As Integer = 0 Dim oConn As New SqlConnection(sConn) oCMD.Connection = oConn oConn.Open() Try iRet = oCMD.ExecuteNonQuery() Catch ex As Exception '//LOG ErrorReporting("ExecuteParam", ex, oCMD.CommandText) Finally oConn.Close() End Try Return iRet End Function That way all you have to do is pass it an object that has your statement and your parameters. iRecordCount = ExecuteParam(testingConnection) the function handles all opening and closing of the database so it's clean and efficient. I'm using SQLcommand here but you can use OledbCommand too
You can also escape it. [password] should also have worked, if you needed to keep the column name.
This - except varchar(max) is what it says on the tin: a varchar column with no length restriction. If you save the package when the wizard prompt you to, you can open it in BIDS and mess with the File Connection's column settings. Alternatively use bulk loading where you can gain a bit more direct control over the format without having to install a lot of GUI tools: https://msdn.microsoft.com/en-us/library/ms162802.aspx 
http://www.postgresql.org/docs/8.3/static/indexes-ordering.html Yup if your index 'covers' the whole query requirement (keys, order by columns, any other column involved) then the optimiser knows it can use the index to optimise the sort.
It looks to me like you're missing the closing parenthesis after @password. 
Thanks a bunch for your help! I've noted that subtraction is an operator. Small problem, the code runs fine, but it does not return any rows. Do you know why this is? Edit: Never mind, I made a typo. Fixed it. Thanks again for your help.
All the best for your test ! Yes French is my native language :-)
Merci dude! Oh man, I always wanted to learn French. This test gave me even more will to do it! I'm gonna start searching for a French language school somewhere. I just love the language, the way it sounds and what it represents. Although German would be really useful to learn too!
ORDER BY NEWID() Have to say, I've needed to do something like this before but never thought to do it this way. From a performance standpoint, it's tempting to compare it to a ROW_NUMBER() OVER(ORDER BY SELECT NULL). At any rate, thanks, I'll keep it in the toolbox.
hi,translating my script to english asap
 SELECT p.name,s.IDStud, count(*) AS NumberOfExamedSubjects FROM exams e INNER JOIN students s ON e.IDStud=s.IDStud INNER JOIN People p ON s.IDPeople=p.IDPeople INNER JOIN discipline d ON e.IDSubject=d.IDSubject INNER JOIN StudyPath t ON s.IDStud=t.IDStud WHERE t.specialitate='STS' AND t.an='3' GROUP BY p.name, s.IDStud UNION SELECT p.name,s.IDStud,count(*) AS failed FROM exams e INNER JOIN students s ON e.IDStud=s.IDStud INNER JOIN people p ON s.IDPeople=p.IDPeople INNER JOIN Subjects d ON e.IDSubject=d.IDSubject INNER JOIN StudyPath t ON s.IDStud=t.IDStud WHERE t.specialitate='STS' AND t.Year='3' AND e.Grade&lt;5 GROUP BY p.name, s.IDStud UNION SELECT p.name,s.IDStud,count(*) AS passed FROM exams e INNER JOIN studenti s ON e.IDStud=s.IDStud INNER JOIN persoane p ON s.IDPeople=p.IDPeople INNER JOIN subjects d ON e.IDSubject=d.IDSubject INNER JOIN StudyPath t ON s.IDStud=t.IDStud WHERE t.specialitate='STS' AND t.an='3' AND e.nota&gt;=5 GROUP BY p.name, s.IDStud 
The problem is it's only giving me the name of the student,his ID and the number of exams,but I need it to give me the passed and failed exams as well
Je t'en prie man ! You definitely should. French is kinda close to English. 60% of words have a common root more or less. German also is kinda close to English. But French is more widely spoken I guess :-)
If you want to do simple, use google cloud sql. Yes it is mysql, but generation 2 has pretty good performance, and it starts at ~$5 a month just like running your own Postgresql.
Thanks! In this case it needs to be PostgreSQL, but it's good to know that's an option
I don't know what I had selected, but it was giving me a consistently insane number... I guess I made a mistake somewhere.
Lots of reasons 1. I was concerned about db security in my MVP. I have a single database that serves many customers but each one would kill for the other's data. I have security on an application level, but the row-level security available on PostgreSQL 9.5 is just amazing. On the SQL level I can assure that the user is not even able to access data outside of it's scope even within the same shared tables. 2. Failover instances (or what PG calls clusters) are pretty easy out of the box. So easy that services like AWS or Google Cloud will offer it at a marginal fee. 3. Concurrency is about as good as it gets shy of Oracle. 4. I write mostly in Python &amp; PG is the best fit for Django. I'm still not sure if I'm going with Django for the front end, but if I do, this will be a perfect fit. 
This presentation is 257 slides, and starts with at least 20 slides of guff before even getting to the 10 tricks. Did you seriously present this somewhere? Did the audience gnaw off their arms while waiting for it to get to the point?
Constructive criticism... Those looking for "tricks" are past the point of needing an intro to SQL, or at least they should be. 
I think you want to count e.IDStud, not s.IDStud. I wrote a query in my own database using the same joining, grouping, and union scheme you have and it works--sort of--when I counted from the exams tables. Each student got a line with a number for total exams, passed, and failed but, since they were all unioned in the same column, it was vague what the numbers meant. It looks like this: Name | Id | NumberOfExamedSubjects ---|---|---- John | W200034 | 22 John | W200034 | 16 John | W200034 | 6 Ivan | W200266 | 3 Ivan | W200266 | 3 However, I did not join any additional tables for subject or study path because I wanted to keep it simple. So, if the grid above is what you want it to look like, then I recommend that you first try to count on e.IDStud. If that does not work remove the other two tables and see if you can just get all the students with all the grades, all the passes, and all the fails. Then work on joining in the rest of the tables, one at a time, to see what fails. But I have another recommendation: get rid of the unions and use subqueries for the exams/passed/failed. Something like this: Select s.IDStud, (select count (e2.idstud) from exams e2 S2 where e2.idstud = e.idstud) AS NumberOfExamedSubjects, (select count (e2.idstud) from exams e2 where e2.idstud = e.idstud and e2.grade &gt;= 5) AS Passed, (select count (e2.idstud) from exams e2 where e2.idstud = e.idstud and e2.grade &lt; 5) AS Failed from exams e INNER JOIN students s on e.IDStud=s.IDStud etc. etc. This will give you results like: Name | ID | Number_of_exams | Passed | Failed ---|---|----|----|---- John | W200034 | 22 | 16 | 6 Ivan | W200266 | 3 | 3 | 0 I hope that helps. 
How?
Wow, I very surprised that so many of the MySQL columns are NO. I guess MySQL isn't so popular because of it's features ;-)
What's your desired output and what are you asking?
Loads of mistakes on this...
It will be easiest if you get your monthid format into a datetime. I would go with option b below: declare @mo int = 201512 select @mo 'format', @mo + 1 'current method', @mo/100 'year', right(@mo, 2) 'month', DATEADD(m, 1, Cast(cast(@mo/100 as varchar(4)) + '-' + right(@mo, 2) + '-01' as Datetime)) 'option a', DATEADD(m, 1, convert(datetime, cast(@mo as varchar(6)) + '01')) 'option b' 
This makes much, much more sense! It also checks out, as I'm sure you knew it would. What is the type of sql in your original post? Just curious. I've only really started my SQL querying a few weeks ago and am new to all of it.
Which db in particular are you talking about? I know one database on the list very well and from what I can see it is accurate for that database. 
Wrong again, jooq blog. I did think these we possible! Nice article though.
;-)
Awesome! Is it geared towards absolute beginners/intermediate/advanced ??
[Relevant.](http://gifyoutube.com/gif/YEZ3M2)
Different dialects of SQL seem to have different versions of a FIND function. You need a built-in function like FIND(bigstring, littlestring, startpos), which searches through bigstring, starting at the startpos, looking for the first occurence of littlestring. You also need a LEFT(bigstring, nbr) that returns the first nbr of characters in bigstring. Or you might have SUBSTRING(bigstring, startpos, nbr) that returns nbr characters from bigstring starting at the startpos position. It also might be called SUBSTR(...). Then: FIND( campaign, '_', 1) is the position of the first underscore. FIND( campaign, '_', 1) + 1 is the position of the next character after the first underscore. FIND(campaign, '_', FIND( campaign, '_', 1) + 1) is the position of the second underscore. FIND(campaign, '_', FIND( campaign, '_', 1) + 1) - 1 is the position of the last character before the second underscore. It is also the number of characters before the second underscore. LEFT(campaign, FIND(campaign, '_', FIND( campaign, '_', 1) + 1) - 1) is the characters themselves, up to but not including the second underscore. SUBSTRING(campaign, 1, FIND(campaign, '_', FIND( campaign, '_', 1) + 1) - 1) is the characters, starting with the first character, up to but not including the second underscore.
SQL server is a server software on it's own, so users are not interacting with it directly. You could write a query (using SSMS, for example) - but it seems you want to avoid this. There are some visual tools to build queries and there are some reporting/business intelligence tools (such as Tableau, Power BI, Microstrategy, Qlik, etc.) that would build query (or many queries) behind the scenes given a certain data/report layout.
But Postgres is open source, and has more features green-lit than Oracle, yet from what I've seen MySQL is still more popular. I haven't used MySQL, but if it was just about the money or features that Postgres should be more popular, but I don't think it is more popular than MySQL which leads me to think there must be some other attribute of MySQL driving its popularity. I just don't know what it is ;-)
Nice. Anywhere to sign up for news about it?
The jOOQ newsletter would be a good place. If you've never visited the website, [you should see a popup at the bottom](http://www.jooq.org/learn/). If you have, delete cookies first, and the popup will re-appear.
Intermediate with 1-2 advanced sections - the golden middle. There's enough beginner material out there already. It should be useful for people who work with SQL quite a bit, but never really got a hang of it, because of: - ORMs hiding most of it - Lack of time - Lack of use-cases (but still interested) - Lack of someone explaining things simply, systematically, and non-boringly - Lack of good references (e.g. working with awful, frustrating legacy SQL)
The talk fits in 45 minutes. Trust me!
Pretty good article. But, ugh I hate titles like that.
fuck me I should start a blog too. If clicky clicky through the god damn gui warents blogging about, I have tons of things that I actually should blog about but I think are to basic to actually do. I mean come on, you basicially are blogging about a wizzard here. Anyhow let me replace your entire screenshot collection of a wizzard with one actual line of code : bcp AdventureWorks.Sales.vSalesPerson out C:\Data\SalesPerson.txt -c T (https://msdn.microsoft.com/en-us/library/ms162802.aspx) done
What type of application is accesing the server. Is it through oledb or odbc
1. Your corporate DBA is absolutely right not giving you permissions to create server-wide objects. 2. Talk to them and/or their manager and most likely there's a combination of several things: a) A ticketing/request system exists where you could submit your request b) Such a configuration is not possible c) linked servers are against one of your enterprise IT policies d) performance implications 3) they do not trust the level of your experience 3. You could run some kind of federated data governance solution (never had any major exposure with these myself). 4. You could run ETL processes to bring required data under one "roof" (another RDBMS, a Hadoop box, a Mongo box, etc.) 5. You could use a BI tool (MicroStrategy, Tableau, etc.) that supports data source blending.
This is still considered beginner/intermediate?! I guess I have a lot more to learn.
Not trying to sound dumb, but why would it matter? I have access and username in database A and access and username in database B. They set up a connection that allows my username/password to link the two. I only have read to both anyway, what could I possibly mess up? 
How are you expecting users to interact with this data?
OK, but are you expecting your end user to need to write SQL?
Great video but I believe now I feel even more like it is not a big deal. Obviously we would create a linked connection via my windows authentication so it's really not a security issue. And then writing efficient queries is easy enough. So again still don't see why they won't except for the amount of time it would take to set just one user up and then others might want their own connections and it becomes a nightmare. I'll end up being lazy and installing the enterprise edition of TOAD and just writing cross connection queries but it drives me nuts to have to use TOADs interim Access database. Talk about slow
Subscribed. Your articles fill a nice niche of adding curiosity and enthusiasm to the otherwise very dry reference manuals we usually use on a day to day basis. 
Thanks for the feedback, I appreciate it! Well, I always say: "SQL is a device whose mystery is only exceeded by its power". It's just too crazy to be treated in a dry manner :) (like XSLT or regex, for that matter)
Heh, we do seem to understand each other. I always thought that there are two kinds of developers: 1. Data-driven ones that like data transformation languages like SQL, XSLT, regex, functional languages. 2. Model-driven ones that like stateful applications. I suspect the latter can cash in more when doing consultancy work.
ive been looking, apparently 2014 dev is $50. Do you have a link showing free anywhere? MS site is where im finding it for $50 per seat. Nevermind, i found it. Looks like it is free for dev members. looks like i need to sign up and sign in :) link if anyone else here would like it or needs it. https://myprodscussu1.app.vssubscriptions.visualstudio.com/Downloads?pid=1682
So you want users to calculate the sum of a field, any field, by simply providing the name of the field? You need to make a stored procedure and use a lot of red code. Something like this (woefully untested, written on my phone while I sip coffee) CREATE PROCEDURE proc_ShouldaDoneThisInTableau @FieldName VARCHAR (7) NOT NULL AS BEGIN DECLARE @uglyredcode VARCHAR(255) SET @uglyredcode = 'SELECT ID, SUM(' + @FieldName + ') FROM YOUR TABLE GROUP BY ID' EXEC @uglyredcode END If this works like I think it should then all your users have to do to get the sum of all GroupID or Value is just EXEC proc_ShouldaDoneThisInTableau 'GroupID' or EXEC proc_ShouldaDoneThisInTableau 'Value' give it a whirl. Don't blame me if the architects or DBAs get mad at you. Good luck
Not only are linked servers a security risk in terms of badguys doing bad things they're a security risk in terms of nice guys doing dumb things Here's an example of what a bad guy can do SELECT * FROM OPENQUERY ( linkserver, 'DROP TABLE ALL_FINANCE_DATA') The open query isn't smart enough to read what its sending, it just sends the request over the wire and hopes to get a response. But here's what a nice but dumb guy could do SELECT * FROM OPENQUERY( linkserver, 'SELECT * FROM ALL_FINANCE_DATA') A query that big could crash both the Oracle *and* the SQL Server You should be looking at ETL tools like SSIS for this kind of stuff
@mo is just a sql variable The select statement shows how you can parse the value you were working with, then change it to a date and add a month.
 SELECT t1.gender, t1.age, t1.cnt, 100*t1.cnt/ttl.cnt pct FROM ( SELECT application_job ,user_gender as gender ,CASE WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 22 THEN '16 - 21' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 31 THEN '22 - 30' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 45 THEN '31 - 45' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 65 THEN '45 - 64' ELSE '65+' END age ,count(*) as cnt FROM applications LEFT JOIN user_personal_information ON user_personal_information_user = application_user WHERE application_job = ? GROUP BY application_job ,user_gender ,CASE WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 22 THEN '16 - 21' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 31 THEN '22 - 30' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 45 THEN '31 - 45' WHEN TIMESTAMPDIFF(YEAR, user_birthdate, CURDATE()) &lt; 65 THEN '45 - 64' ELSE '65+' END ) t1 JOIN ( SELECT application_job ,count(*) as cnt FROM applications GROUP BY application_job ) ttl ON ttl.application_job = t1.application_job 
What are you using to generate/save the CSV file?
As /u/rbardy mentioned, use the "text qualifier" if you can use SSIS. If you can't do that, you can use the QUOTENAME function around each field. It can be messy if you are pulling a lot of fields, but it can work well for small sets. Also, you have to be prepared for what will happen if you already have a double quote in the data itself. You're going to want to plan for a way to deal with those. &gt; SELECT QUOTENAME(FirstName,'""') as FirstName, QUOTENAME(LastName,'""') as LastName FROM Employees &gt; --- &gt; FirstName, LastName &gt; "Mike", "Jones"
Assuming there are no issues with the pipes (network) and protocol (duplex and speed), it sounds like sql server is running out of resources. Anything in the os app and system logs? sql logs? Don't forget to look before the reported issue time. You might see something leading up to the issue. Don't forget to check the client's logs. Are snapshots or backups going on during these times? Is this a stand alone sql server or part of a cluster/ha? What resources ram, cpu, ... does the windows and sql server have? Version? Is the sql server memory limited? Is there enough ram for the os to live in? How many worker threads do you have? select max_workers_count from sys.dm_os_sys_info Run a profile trace to capture deadlocks and timeouts. sys.dm_exec_sessions may help too. Just a thought... you said the db is a back end to a web server? Are you being attacked? Is someone sucking all the data from your db via the web server? If this is sql server enterprise, turn on auditing for dml on your key tables. (i think you can on tables). If it is not the enterprise version, fire up profiler and do it from there. 
&gt; When the files are properly created lol
Nice. I am still trying to learn more SQL... It's what I struggle at most unfortunately :(
It's pure logic. Think about stacking data into piles and then how you would look at a single record from each pile and decide if the two go together. If something doesn't make sense, you probably need a different pile, or some different data in your existing pile. 
Don't do that. Your table is a transaction table. Each row represents one and only one transaction. So your earliest purchase date does not belong in this table at all. Each person could make multiple purchases. You are wanting to the earliest purchase date for a particular person. That data belongs with the list of persons and not the list of transactions made by all persons. Since that is calculated data there are lots of folks that will tell you not to store calculated data at all. I agree with that opinion.
Are you getting any error or just undesired result? Can you take a screen of the issue and post here?
All problems will be solved if my users can write SQL! I am expecting them just to enter equations like A/B, CxA/B etc. I am hoping to re-iterate through these equations and calculate automatically. 
Yea. I didn't intend to but I exaggerated a little. It's not "beginner", but I've been using sql for less than a year and I understand it.
Yeah, you'd be surprised how few long-term coders even know what a CTE is. People get used to coding a certain way and when something new comes out they either disregard it or don't even know it exists. Seriously, we interviewed nine people, all with lots of programming and SQL coding experience, and only *one* could adequately describe what a CTE is and when they would use one. 
Call 'em columns, not attributes round these here parts ;) Your DDL is correct &gt; a foreign key is an attribute that appears in another relation as a primary key Not quite. A foreign key is a constraint that says "make sure a value in this column exists in another column". That column can even be in the same table ;) A foreign key can point to any unique key. A primary key is a special type of unique key A foreign key can be on the same column as the primary key 
I'm dealing with some folks right now who can't figure this out. All sorts of double-quotes in the values of one of the fields they're sending and it breaks SSIS. They actually suggested using single quotes as the text qualifier, but there are even more of those. I'm tempted to ask them to use  as the qualifier because _ has pretty much been the default when dealing with them.
Yeah, not going to get rich off the $19 licenses:) The idea is to get traction, lower the barrier for people to use it at home so they'll want their companies to buy it as well.
I like the idea here, I've been working on moving off our report system to stored procedures called from Excel. I'm going to this out in the morning see if this could solve my need. Looks exciting regardless! 
You should be able to pick up SQL pretty quickly. The syntax is easy enough to understand what it's doing and why. I work as a BI Analyst/Dev, which under the setup I inherited; requires me to know SQL, Python and VBA/VBS. To be honest, any thing I do with SQL is far simpler than trying to interpret someone else's code to debug an automated report or figure why extract failed when I do daily ETL. 
Sorry to bother you again. This query works SELECT MIN(DATETIME), MAX(DATETIME), AVG(PRICE) FROM stocks WHERE DATETIME&lt;"1999-06-02 00:00:00" GROUP BY strftime('%s', DATETIME)/(60*5); However, it just came to my attention that I need the opening price and the closing price in each 5 minute interval as well. So how would I go about picking the first row and the last row of the group y statement? I have tried google but the answers seem really arcane to me, I was hoping you could shed some light on this. 
Look at the DATEDIFF function. It'd be something like... SELECT employee_id , employee_name , DATEDIFF(y,HireDate,GETDATE()) || ' year ' || MOD(DATEDIFF(m,HireDate,GETDATE()), 12) || ' month ' || MOD(DATEDIFF(d,HireDate,GETDATE()), 7) || ' day' AS time_employed FROM employee
Thank you, got me in the right direction. Apparently MOD doesn't exist in SQL Server 2014, so I had to modify the query a bit: SELECT EmployeeNumber ,EmployeeName , CONVERT(VARCHAR(5),DATEDIFF(yy,HireDate,GETDATE())) + ' year ' + CONVERT(VARCHAR(5),DATEDIFF(m,HireDate,GETDATE()) % 12) + ' month ' + CONVERT(VARCHAR(5),DATEDIFF(m,HireDate,GETDATE()) % 7) + ' day' AS time_employed FROM tblEmployees
this is nice because it gives an example of creating a function but the function itself is useless, for two reasons 1. it doesn't count statutory holidays 2. it doesn't count business closings
I like 'unfuckulate'. 
I was going to construct some sort of "where exists" clause. Your solutions are much better.
Thanks! That works great. What if I know want a summary with a count of how many computers has compliant = 1 and how many has compliant = 0 ? 
Is there anything in particular you're hoping to accomplish? That might make it easier to point you in the right direction. Since you're new, I recommend downloading the MySQL Workbench for your platform and start creating some databases. I'm sure you can download some sample databases as well if you do some googling.
YMMV, but datediff is a tricky function to use in MS SQL server. For example, datediff( yy, '12/31/2011', '1/1/2012') would be equal to "1" and you wouldn't say that a person has been employed for 1 year given these dates, would you?
Are all of the differential backups 2GB? They should start of smallish and get larger through the day, then get smaller again after the full backup runs. The other thing is, is there anything you are doing that modifies 2GB of days during a day? Index rebuild, maybe? What isn't good about differentials? Are you worried about the size of the differentials or the time it takes to create them?
What's your RPO &amp; RTO? What is the concern here - that your backups are too big, or the database is? I'd use transaction log backups instead of differentials, they give you point in time restore (not just to the hour) and should be a lot smaller than the diffs as they're not cumulative. Backups can also be compressed, depending on the type of data in the database and its compressability you can make from 0 to massive space savings :) If your concern is more around the size of the database - are you adding data as well as removing it? If so, make sure you have some index maintenance - depending on the type and use of your data, you can recover a lot of space in cases where it ends up only partially filling pages 
Ah really? I started some JS a while ago at learnt about the for loops so figured it would be the same. I want to find out how many entries a value has in table when it has another value. For example: select top 5 #columeA, COUNT(#columeA) where #columeB = '#valueA' Instead of having to change #valueA every time, I want to find the top 5 values for every DISTINCT(#valueA) which appears in the table. Does that even make sense? Hard to describe the process while anonymizing the names :D
OK - so your description is a little confusing, but we can work through it. If I understand correctly, you have a single table (We'll call it Table1) which has 2 columns - ColumnA and ColumnB. For each possible value in ColumnB you'd like to see the top 5 (and only the top 5?) values of ColumnA and how many times that value appears paired up with the ColumnB value we're looking at? For the sake of simplicity, let's eliminate the Top 5 requirement and get ALL values of ColumnA for each value in ColumnB and how many times it appears (top 5 per value adds complexity, but can still be done): SELECT ColumnB, ColumnA, COUNT(*) as Number_of_Times FROM Table1 GROUP BY ColumnB, ColumnA ORDER BY ColumnB, Number_of_Times DESC As I said, limiting it to 5 records per ColumnB/ColumnA combination adds some complexity. What we want to do is assign a ranking to each possible value of ColumnA within ColumnB and only include ranks 1-5. So we need a subquery like: SELECT ColumnB, ColumnA, Number_of_Times FROM (SELECT ColumnB, ColumnA, COUNT(*) as Number_of_Times, ROW_NUMBER() OVER (PARTITION BY ColumnB ORDER BY COUNT(*) DESC) as ranking FROM Table1 GROUP BY ColumnB, ColumnA) T WHERE ranking &lt;= 5 Let me know if I am misunderstanding the problem you are trying to solve. 
So the "for each distinct(city)" reuqirement that you are looking for is the concept of the "GROUP BY" in SQL. The columns that you GROUP BY will have a single record per distinct value in your result set and you can add additional columns to your SELECT clause to aggregate them (Such as a COUNT). So here is my first query adjusted for your example here (without the Top 5 requirement): SELECT city, username, COUNT(*) as Number_of_Logins FROM Table1 WHERE eventname = 'userlogin' GROUP BY city, username ORDER BY city, Number_of_Logins DESC Presumably your results will now look something like this city username number_of_logins London Mike 16 London Billy 8 London Tom 4 London Tony 3 London Ryan 2 London Peter 1 New York Phil 8 New York Ed 6 Philadelphia Joe 3 Philadelphia Chris 3 etc... That query will give us every single person from London who logged in, and how many times they logged in. Then every person from New York who logged in and their counts etc. If we want to only include the 5 most commonly logged in users from each city (in the example above, not including Peter, since he is the 6th most commonly logged in from London), then we have to adjust the query to match my second query like so: SELECT city, username, Number_of_logins FROM (SELECT city, username, COUNT(*) as Number_of_Logins, ROW_NUMBER() OVER (PARTITION BY city ORDER BY COUNT(*) DESC) as ranking FROM Table1 WHERE eventname = 'userlogin' GROUP BY City, username) T WHERE ranking &lt;= 5 The results for this query should match the first one exactly except for excluding the 6th most common user and beyond for each city. EDIT: The next question you're going to have to ask yourself is "If there is a tie for 5th place in number of logins from a city - do we want to include all of the tied users, even if that means more than 5 people per city, or simply whichever user comes first alphabetically (or by some other criteria) amongst all of the tied users?" Either way can be done - but we'll need to adjust our query to suit your requirements.
Youre a great help man, thanks
See if this is what you need: SELECT COUNT(*) FROM [database.[TASKS] WHERE REQDATE &gt;= '2016-4-20' and COMPLETED IS NOT NULL and TYPE = 'TS Desktop Support' and WOTYPE2 = 'Hardware' and WOTYPE3 = 'Desktop CPU' and TRY_CONVERT(int,WO_TEXT1) IS NULL It will bring the amount of rows where WO_TEXT1 isn't a number and attend all your criteria 
Ah, good thing. Stephen King's The Dark Tower series of books makes a running recurring theme of the number 19 as an important/special number, that was what I was referencing :)
Hi Bill. I took a look and it is very entry level information What differentiates this from the thousands of other cheatsheets that are out there? Why are you duplicating W3 schools and then linking to them? Why wouldn't I just go straight there? Basically, why is this needed? I don't want you to be offended, but I always ask this question of a business spending money on a new product. If you were Bob down the street doing it to learn, or for a school project, cool, but as part of a business model it seems like you guys are a bit late.
This works very well. I did however have to add _ORDER BY City, ranking_ at then end as it was only showing 1 entry per city but the results changed every re-run of the query. The last part of the query i haven't mentioned yet is I will also need the CityId which is located in another table so will need to join the tables. I was thinking adding a _Table1 as t, table2 as tt_ _where t.city=tt.city_ in there somewhere but I am too drained to think about that now! Adding the tied "6th" guy would be interesting but may run into problems where the 5th guy is at 1 and there are another 5 people at 1 also there the "6th" guy becomes "10th".
Just to clarify, I need the rows to be similar to other rows in every column. For example sake, lets say similar means +1 or - 1. so if row 1 is (place a, 1, 2, 3) then row 2 need to be (place a, 2, 2,2) or (place a 1,1,2) etc to consider similar
I agree... maybe use something else instead of w3schools? 
Glad to hear it. I'd wager that the query was not returning one row per city, but was returning multiple records per city scattered troughout the result set (like the most logged in user in London as the first record and them the second highest for London returned as the 104th record). Or maybe that is what you meant. In any case, adding the order by clause is the right move. You are correct about needing to join the tables. And in this case the right place for it is right where we say "from table 1" on line 7. Though I highly recommend learning the ANSI join syntax (A inner join B on a.id =b.id) as described here: http://www.orafaq.com/node/2618 Sure, they will technically work, but comma separated lists of tables for joins seem to only have disadvantages compared to ANSI Joins and I'd suggest a new SQL developer learn the more modern way. Responding to your other comment, line 6. The purpose here is to provide an actual, usable field in our query that indicates where that person falls, number of logins-wise, within all the people in his city so that we can then limit the rows per city that we return. Row_number is not a part of standard SQL. It is a built-in function of Microsoft's T-SQL language, which is used by SQL server. If you are not using SQL server I am sure whatever variant you are using will have its own version but the name and syntax might differ. The syntax is a little strange to read so I will take it 1 part at a time. Row_number() is simply the call to the function. And its syntax dictates that it MUST be followed by the word OVER. PARTITION BYindicates whether we are breaking out result set into multiple subsets for numbering purposes or whether we want to treat the whole result as 1 set. Since we specified PARTITION BY city the query knows that after it is done assigning rankings to all of the people in London it should start again at 1 when it gets up to people from New York. if we wanted people ranked simply by how many logins they had, we could leave out the words PARTITION BY city entirely. ORDER BY: This one is a bit confusing since we are used to using ORDER BY to control how the results show in the result set. But this order by has a different purpose. And that is to tell it how we want things ranked. We want to know who had the most logins - therefore we want the person whose ranking is number 1 to have the most logins and therefore we want to order our list by number of logins DESCENDING. If you wanted to know the 5 people who logged in LEAST per city you would change it to ASCENDING. Or if we wanted to get the tallest 5 people from each city (assuming we had a height field in table1) we would just change it to ORDER BY height DESC. More on Row_number() here: https://msdn.microsoft.com/en-us/library/ms186734.aspx I hope that helps clarify it. But I am happy to answer any questions you have. Sorry if there are typos. Wrote this one on my phone. 
 select distinct place from table4columns c1 where exists( select * from table4columns c2 where c2.place = c1.place and c2.length - c1.length between -1.0 and 1.0 and c2.width - c1.width between -1.0 and 1.0 and c2.height - c1.height between -1.0 and 1.0 ) 
I studied computer science both as an undergraduate and for my grad degree including coursework in databases for both and I have been working as a professional software developer for 13 years, the past 10 doing the majority of my work in SQL. So, you know, you pick it up here and there. 
Select customer from payments group by customer having min(paydate) between date1 and date2 This will return all "first payment" customers within a given date1 - date2 range.
I think this is exactly what I'm looking for. I won't be able to test for a couple days but thank you so much!
You mean the first time in the account's history, not just for the month, correct? I would do this: SELECT COUNT(PaymentId) FROM Payments Where AccountID = "" COUNT will tell you how many rows exist in the table that meet your conditions. So if the person has made one payment, that statement will return '1'.
Also I just wanted to specify that I'm learning SQL on MySQL. I've already gotten started and it seems extremely easy only because MySQL basically fills out all of the coding for you. Am I gimping myself by learning on MySQL as opposed to a server that isn't as user friendly? Like would this hurt me on potential job opportunities? Edit: sorry I'm really slow lol you already mentioned MySQL still my question remains if you believe I should venture outside of MySQL once I get comfortable or not. 
I use algebra every day in writing queries and working with data sets made learning calculus incredibly easy. Knowledge of calculus in turn helped me write even better queries and perform even better analysis. 
oh yes either minor or major, now I get it, thanks!
Here is what I came up with: --First Some Test Data WITH data AS ( SELECT * FROM ( VALUES (10000, '2016-03-20 12:50:00', 'User'), (10000, '2016-03-20 17:50:00', 'Agent'), (10000, '2016-03-20 23:50:00', 'User'), (10000, '2016-03-21 00:40:00', 'User'), (10000, '2016-03-21 06:50:00', 'Agent'), (10000, '2016-03-21 07:30:00', 'User'), (10000, '2016-03-21 10:30:00', 'Agent'), (20000, '2016-03-20 12:50:00', 'User'), (20000, '2016-03-20 17:50:00', 'Agent'), (20000, '2016-03-20 23:50:00', 'User'), (20000, '2016-03-21 00:40:00', 'User'), (20000, '2016-03-21 06:50:00', 'Agent'), (20000, '2016-03-21 07:30:00', 'User'), (20000, '2016-03-21 10:30:00', 'Agent') ) v (Ticket_Number, Created_At, Sender) ), --Then add some info from the leading record leadData AS ( SELECT *, LEAD(created_at) OVER (PARTITION BY Ticket_Number ORDER BY created_at) Next_Created_at, LEAD(Sender) OVER (PARTITION BY Ticket_Number ORDER BY created_at) Next_Sender FROM data ) --Then calculate the result based on the leading data SELECT Ticket_Number, 1.0 * COUNT( CASE WHEN next_sender = 'agent' AND DATEDIFF(MINUTE, created_at, next_created_at) &lt;= 6 * 60 THEN 1 ELSE NULL END ) / COUNT(*) Service_Level FROM leadData WHERE sender = 'user' GROUP BY Ticket_Number
Will I be able to list counts for multiple requirements. For example, this query will (hopefully, I haven't had chance to try it yet) give me one row, we'll call it Desktop CPU support. I need multiple columns for various types of TYPE, WOTYPE2, and WOTYPE3. That make sense? Lemme see if I can put it in a table on here. Desktop - Hardware - Desktop CPU | Desktop - Software - O365 | Training - Software 365 -------------------------------------------------------------- 10| 3 | 6 EDIT Ok yea I suck at making tables in reddit, but hopefully you get the point. 
sqlzoo.net 
Now why'd you want to do grouping sets for a single set while there's a simpler syntax available? This will create confusion eventually to someone. I ,for example, assumed different 'group by' sets without fully processing the syntax and was struggling to figure out why you wanted split totals - because in a rare case that grouping sets is used, this is what it is needed for.
Code Academy
I completely agree with your approach here. Often I want to reference a code base or feature that I don't often work with. I use search all the time anyway. It makes sense for me to get that simple information faster and in a less distracting environment. 
&gt; column IS NOT NULL AND column &lt;&gt; ''; But in the syntax there is no single space. It is quote-quote, not quote-space-quote. &lt;&gt; means "not equal to", usually in the land of microsoft syntax. But to me, is not null and &lt;&gt; '' seems like it is saying the exact same thing. What is the difference between something that is null and something with a value of '' ? Because '' isn't a value at all, obviously. Seems like that would be null as well. Edit: [Here](http://stackoverflow.com/questions/4802015/difference-between-null-and-empty-java-string) is a good explanation which I think makes sense. Null is not the not the same as "empty string" so this condition in your SQL checks for both cases 
khanacademy is where I am right now. I've learned a bit and it is MUCH better cadecademy. 
Less than or greater than, which is true only when the values are not equal. 
This is what I'd do with that dataset CREATE TABLE #Movies (id INTEGER PRIMARY KEY , Title TEXT, Released INTEGER, Sequel_ID INTEGER); INSERT INTO #Movies VALUES (1, 'Harry Potter and the Philosophers Stone', 2001, 2); INSERT INTO #Movies VALUES (2, 'Harry Potter and the Chamber of Secrets', 2002, 3); INSERT INTO #Movies VALUES (3, 'Harry Potter and the Prisoner of Azkaban', 2004, 4); INSERT INTO #Movies VALUES (4, 'Harry Potter and the Goblet of Fire', 2005, 5); INSERT INTO #Movies VALUES (5, 'Harry Potter and the Order of the Phoenix ', 2007, 6); INSERT INTO #Movies VALUES (6, 'Harry Potter and the Half-Blood Prince', 2009, 7); INSERT INTO #Movies VALUES (7, 'Harry Potter and the Deathly Hallows  Part 1', 2010, 8); INSERT INTO #Movies VALUES (8, 'Harry Potter and the Deathly Hallows  Part 2', 2011, NULL); SELECT M1.Title, M2.Title as Sequel FROM #Movies M1 LEFT OUTER JOIN #Movies M2 ON M2.id = M1.Sequel_ID ; Drop Table #Movies The Left Join had the ID/Sequel_ID backwards ... I had to reverse the tables in the join
Can you give some examples?
Thanks this is very helpful!
Thats interesting I'll give that a try later. Thank you 
very good explanation, thanks for it!
Although it's functionally equivalent, I'll typically code that like: SELECT * FROM Table WHERE ISNULL(column,'') = ''
* https://www.codecademy.com/learn/learn-sql and https://www.codecademy.com/learn/sql-analyzing-business-metrics Free. Completed both courses. Interactive tutorials. Good as a first start, but found this a bit too simple and not challenging enough at times. I think this makes you feel more confident in SQL than you really are. * https://sqlschool.modeanalytics.com/toc/ Free. Completed all exercises. Very good tutorials (written) with mini practice sessions throughout the tutorials to ensure thorough understanding. Quizzes were appropriate in difficulty. I learned a lot from these. This is probably my favorite. * https://play.google.com/store/apps/details?id=com.sololearn.sql&amp;hl=en Free. Quick and easy to start off with to become familiar with basic queries and syntax structure. But does not go into greater depth than the basics. Still recommend when first starting off. * https://academy.vertabelo.com/course Free. These courses were OK. Completed all three courses. Learn by doing through reading examples and putting examples into practice. Could be more in depth and since multiple queries can answer the same question, I needed to check with those who commented to make sure my queries were outputting the same result even though it was "incorrect." * https://www.khanacademy.org/computing/computer-programming/sql Another Intro SQL course like Mode or Vertabelo above. Continues to build on basics with different examples. * https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/ Free. Used to be on Coursera. I first started off with this, and having very little background in SQL I found this difficult to follow. Moreso because there are no practice sessions while watching the videos so you cannot learn what you are taught immediately. 
You can't reference an aggregate from the SELECT in the WHERE clause, i.e. Rev14, Rev15, and Rev16. You can put an outer query around what you have and use it for your WHERE clause. SELECT * FROM ( SELECT ACCT_ID , REGION , LOB , RAD_FY17 , SUB_ACCT_ID , BUY_POWER , TOT_SOW_YR_0_PCTG , TOT_RAD_YR_0_FLG , CONSOLIDATED_SIC_CODE , CONSOLIDATED_EMPLOYEE_NUM , DUNS_NUMBER , sum(REV_FY14Q1+REV_FY14Q2+ REV_FY14Q3+ REV_FY14Q4) as REV14 , sum(REV_FY15Q1+REV_FY15Q2+REV_FY15Q3+REV_FY15Q4) AS Rev15 , sum(REV_FY16Q1+REV_FY16Q2+REV_FY16Q3+REV_FY16Q4) AS Rev16 FROM DL_LAB_MKT_BI_AUTOMATION.ACCT_RAD_EXTRC_FY17Q1M2_RPTG INNER JOIN PARTY_PKG.ACCT_RAD_SMRY_VW ON ACCT_ID=SUB_ACCT_ID INNER JOIN finance.account_pivot ON ACCT_ID=PARTY_ID ) AS A WHERE (LOB = 'TTL' AND TOT_RAD_YR_0_FLG = 'I') AND (Rev16 &gt; 0 OR Rev15 &gt; 0 OR Rev14 &gt;0) GROUP BY 1,2,3,4,5,6,7,8,9,10,11 
W3Schools has more than enough tools available for you to learn quickly. If you're looking for hands-on practice, download PG Admin III if you're down for giving postgreSQL a go.
That's because there are 14 expressions in the SELECT and only 11 groups specified. It would probably work if you added 12, 13, 14 to the GROUP BY clause. Your expression below works but this would be easier to maintain since you don't have to change your aggregates in both the SELECT and WHERE clauses.
See my answer above. While this works, it would be more difficult to maintain if you ever needed to change the computation of the aggregates.
how big an online retail outlet are we talking? you can setup a simple web store for $5/month with wordpress and woo commerce there are free, open source ERPs as well such as Odoo
The drawback of doing it this way is if "column" has an index you won't be using it. If it doesn't have an index then either is fine.
&lt;...&gt; Ahem. Yeah. What /u/BellisBlueday said. What's the problem, exactly?
you should definitely use code academy. it has courses on SQL and other languages as well, and is very use friendly. https://www.codecademy.com/
HAVING will work for aggregate functions. 
turn left or right
Would something like this work? I've connected all the tables but I'm not sure I'm getting the correct information from them lol select st.name, sum(g.value*c.units)/sum(c.units) as major_gpa from courses c, students st join enrollment e on st.id = e.student_id join grades g on g.id = e.grade_id where st.major_id = c.department_id group by st.name;
Null is an undetermined value. You can't compare it to another value because it might or might not be equal. The comparison returns null (not true or false)
Free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can submit exercises. Some are easy and some very difficult. Just do it at your pace !
I'd be interested to know the thought processes that led you to posting your request for help on the SQL subreddit. Why /r/SQL instead of, say, /r/ERP or /r/NetSuite?
Are you using open table to insert the data? If so, that's likely the issue. Do the insert with the 'insert into' statement and see if that resolves your issue. If you must use open table, right click on the read only row and select 'execute sql'.
Yes, that worked. Instead of editing an open table, I started from scratch and inserted the values using INSERT TO and was able to define the parameters properly in the query.
So you want the results as XML? I have to confess, I'm not very good with XPath - just haven't had a lot of use for it.
JS is meant to handle loops. SQL is not optimized at all to handle loops.
Nope. This is a project for a course
Does your course require you to have sufficient competence to ask your homework questions on a relevant Subreddit? If so you are not off to a good start.
[removed]
https://www.youtube.com/playlist?list=PLA91GUTAXAlQlNpuePCGH3atT_6kNP7-U
SQL Cookbook is a book i've recommended again and again - can't say enough good things about it.
[sqlzoo](http://sqlzoo.net)
I base my code on my his explanation. There's multiple way to do stuff, I that was the first one that came to my mind. 
Whomever wrote this test is an idiot. 1. This isn't an SQL question, so the answer must be **D**. 2. This one is **D** because you need the OUTPUT clause to return all the inserted keys, and answer C won't work because no default value for the key has been specified (NEWID() or NEWSEQUENTIALID()). The fact that the table name changes mid-question is irritating. If there was a "none of the above" answer, you'd have to pick that. 3. This is also pretty badly written. My guess is that it has to be **B**, because of the information that's actually in sys.dm_db_missing_index_group_stats. That is the only view that has the *cost* information, so it's the only way to tell which indexes you "need" (i.e., which will be used a lot and which will save a lot of expense). That's putting kind of a strange nuance to the question, however. That said... every time I've done this I've used B, C, *and* D in my query, so I'd still consider this a bad question. Indeed, if you look at [the doc for dm_db_missing_index_group_stats](https://msdn.microsoft.com/en-us/library/ms345421.aspx) you'll see the query I started with, which is based on all three dynamic management views. 4. "Since you notice that this query takes a long time to run" What query? The CREATE TABLE is taking a long time to run? Wow, that's super bad! It should be instantaneous! Oh, wait. The question is talking about a query which isn't even shown at all. How are we supposed to answer a question to optimize a query *without even seeing the query*? Use a dart board to answer this one. Without seeing the long running query it can't really be answered, but my guess is **D** or **A**. 5. This should be **C**. Check the doc for [EXECUTE AS SELF](https://msdn.microsoft.com/en-us/library/ms188354.aspx). 6. This one is **D**. Query cost will tell you how efficient the query is and page reads tells you how much I/O the query takes. Execution time is nice, but the others might inform you more about how the query will scale.
Came to say this.
I think you may be confusing the Management studio with the server components. You can install them both from the Developer edition (or any edition).
select *, coalesce(department, company, user) as owner from "Widget" 
Yes and for free. One of the best I found http://www.studybyyourself.com/seminar/sql/course/?lang=eng. You can submit exercises. Some are easy and some very difficult. Just do it at your pace !
This can be improved slightly by using a single SUM: SELECT CustomerID, SUM(CASE WHEN Action = 'Deposit' THEN Amount WHEN Action = 'Withdrawal' THEN -Amount END) AS Net FROM mytable GROUP BY CustomerID
I'm not sure there's an absolute method, but this is the one that makes the most sense to me. I almost invariably avoid design that expects a lot of NULLs. Using the method below you can divide by as many or as few owners as you like. You could seamlessly add owners or *types* of owners without an issue. More importantly (at least for me) you can easily switch to a temporal data structure smoothly from here. So if you needed to monitor which owner owned which table *at what time*, you would be better off structuring it this way. id | ownersource | ownerID ---------|----------|----------|---------- 1 | user | 4 2 | user | 6 3 | user | 10 4 | company | 6 5 | department | 8 
Yes, when I saw question 1 I thought I was in the wrong test ahah. Question 3 and 4 are a nightmare because they are really poorly written. Nonetheless, thanks for the help! It was really insightful.
Make an SSIS package that imports from a table or server to other. You can make it either update the production tables or just insert the new data or delete all and re-insert, choose what is the best for your scenario.
What have you tried so far? If I'm reading between the lines properly, you're *way* over-complicating this with your description and it's a pretty simple two-table join.
SELECT type, SUM(personnelAssisted) FROM #temp1 GROUP BY type; 
"I'm trying to get the NAME from table1 for every type J" SELECT Table1.Name FROM Table2 INNER JOIN Table1 ON Table2.ID = Table1.ID "Then I need join that name back to table2 on table2 - name to get the ID from table2." SELECT Table1.Name, B.ID FROM Table2 A INNER JOIN Table1 ON A.ID = Table1.ID INNER JOIN Table2 B ON Table1.Name = B.Name "Then take the ID from Table 2 and join it back to ID on table1 to get the name associated with that ID." SELECT C.Name, B.ID, D.Name FROM Table2 A INNER JOIN Table1 C ON A.ID = C.ID INNER JOIN Table2 B ON C.Name = B.Name INNER JOIN Table1 D ON D.ID = B.ID I think if I saw the data it would be much less complicated. 
Exactly what is in the link that /u/notasqlstar posted. But resuming, you need to make a select that gets the headers, use that result to make a string, use that string as the command that will be executed by a "sp_executesql" command. It is pretty ugly to see and to make the 1st time, but sure works.
In that case you just do another join like the first, but on ID. SELECT T1.NAME, T1_2.NAME FROM Table1 T1 JOIN Table2 T2 on T1.NAME = T2.NAME JOIN Table1 T1_2 on T1_2.ID = T2.ID WHERE T1.TYPE = 'J';
I wasn't really wrapping my head around the ' + @column + ' stuff. 
which database brand?
select (seconds_column/60) as minutes_column from mytable?
Just join it to itself. Select T1.Name, T2.Name FROM Trees T1 JOIN Trees T2 ON T1.Tr_Loc = T2.Tr_Loc AND T1.ID &lt;&gt; T2.ID Or something like that. Sorry for the formatting as I'm on mobile. 
SELECT A.LastFirst, A.SID, B.Recorded from table_A A Left join table table_b B ON A.SID = B.SID WHERE B.SID IS NULL This will give you all records in A where it is missing from B You can additional tables as well. But if your intents is to show everyone missing in B this will work. If items can be in B but just missing data from b.recorded columns then move b.recorded is null to your where instead.
First, you've got a query that's more complicated than it needs to be. SELECT Table_A.LastFirst, Table_A.SID, Table_B.Recorded FROM Table_A LEFT JOIN Table_B ON Table_A.SID = Table_B.SID WHERE Table_B.SID is null or Table_B.SID = Table_A.SID or Table_B.Recorded Is Null You can add more tables via left joins along this line SELECT Table_A.LastFirst, Table_A.SID, Table_B.Recorded, Table_C.Recorded FROM Table_A LEFT JOIN Table_B ON Table_A.SID = Table_B.SID LEFT JOIN Table_C ON Table_A.SID = Table_C.SID WHERE Table_B.SID is null or Table_B.SID = Table_A.SID or Table_B.Recorded Is Null or Table_C.SID is null or Table_C.SID = Table_A.SID or Table_C.Recorded Is Null Or something along those lines - might need to tweak the where clause a bit to get exactly what you want.
the first two SELECTs are overkill -- the INNER JOIN will return the matched rows, and the LEFT OUTER JOIN along with the IS NULL check will return the unmatched rows you can simplify this to a single SELECT -- SELECT Table_A.LastFirst , Table_A.SID , Table_B.Recorded FROM Table_A INNER JOIN Table_B ON Table_B.SID = Table_A.SID as for adding data from other tables, you'll need to identify where they came from, and a simple technique is to place an identifying value (i like the alias "source") on all the rows thus generated -- SELECT Table_A.LastFirst , Table_A.SID , 'Table B' AS source , Table_B.Recorded FROM Table_A INNER JOIN Table_B ON Table_B.SID = Table_A.SID UNION ALL SELECT Table_A.LastFirst , Table_A.SID , 'Table C' AS source , Table_C.Recorded FROM Table_A INNER JOIN Table_C ON Table_C.SID = Table_A.SID UNION ALL SELECT Table_A.LastFirst , Table_A.SID , 'Table D' AS source , Table_D.Recorded FROM Table_A INNER JOIN Table_D ON Table_D.SID = Table_A.SID ORDER BY Table_A.LastFirst , Table_A.SID , source 
Thank you for the information. The date for these does not span over time, it is just one date. The way the question is worded I think it is ok that it returns numbers instead of dates. The assignment question i am trying to solve is this: write a query for Total minutes for each call type, grouped by week of call date SELECT CallLength, CallType, Activity_Date FROM TABLE GROUP BY datepart (wk, Activity_Date) Second part of questions asks: Modify the query so it meets the following criteria -Grouped by week beginning on monday SELECT CallLength, CallType, Activity_Date FROM TABLE SET DATEFIRST 1 GROUP BY datepart (wk, Activity_Date) 
I think one thing that would help me out would be if you could explain what the 0's are doing in the query above. I'm lacking a little context i think. 
Thanks for trying to help with this. I have trying the restructured code you provided, but it doesn't seem to work. I am getting an error in the FROM clause. I believe Access requires "( )" for this type of operation. 
Yeah, Access has some screwy formatting it wants. I've only done a little bit of Access - I mostly work with SQL Server (and some Oracle)
 SELECT [Tr_loc], COUNT(1) AS [Count] FROM [Trees] GROUP BY [Tr_Loc] This should give you a list of locations with a count of trees in each one. If a location does not have any trees then that location won't show in the list at all. Then you need do eliminate then result rows where the count is less than two. Invert the comparison in your mind and see that you need rows that ***HAVE*** a COUNT greater than one. This might be your first experience with the HAVING clause. I have trouble remembering how to use it so I always have to look it up. 
I found some C# code for taking two lat/long coordinates and coming up with a distance. Took me a couple hours but I converted that to a SQL function. Sorry that code is owned by a former employer so I can't share. As to the time you take your work order time and subtract 24 hours. This gives you a lower limit. Add 24 hours and this gives you the upper limit. Anything between those limits is within your 48 hour window.
For datediff, that is the starting week. For 00:00:00.000 that is the time portion of the datetime data , it's best practice to set all your files to midnight if you have time data but are ignoring it for something like weeks.
No. An empty string (shown as '') and NULL are ***not*** the same thing. Relax this is a bit difficult thing to get your head around at first. The '' string is something. It is a proper string that contains no characters. NULL says that the data does not exist at all.
I rarely see an employer who highly values certs over experience but obviously you can't have the latter. I'd get the 461 study guide, install Express, and learn everything back to front. That way you can demonstrate sql knowledge in interviews, plus there is value to saying you've self-studied the material in your education section of your resume - it suggests you understand the material and shows inclination for self-study and improvement. If you want to do the exam you can at that point, but put in the study first. If you understand and can use the stuff in even the first half of that book you're above the majority of experienced developers I've known who say they understand sql. If you understand the whole thing you're above most DBAs I know. If you're going to intern you'll be fine on the experience front by graduation. Between the experience and the study you note on your resume I'd be likely to interview you for an entry level position and you'll be able to demonstrate the knowledge just fine. The cert may push you higher than another resume or two, and the exam isn't super difficult if you know your stuff, though the questions can be oddly worded and the answers tricky so definitely do practice exams! 
[removed]
I'm in a reporting role on the business-side rather than the IT-side of my company. I have no certifications, and I think that's even true of most of the data team in IT. It could vary by company, but I'm definitely getting by without them. 
That is where I am currently stuck at. I've been applying for over 10 internships for the summer but I couldnt land any opportunity. I've had my resume looked at by professors, career advisor at my school and they both thought my resume looks great. So I'm guessing my downfall is due to lack of experience and certs. Thats why I want to focus on studying and building my knowledge of database and SQL over the summer so I could do better in future opportunities.
Push for it then. Hell I'd hire you just knowing that you want to learn. 
Yes I do have a two year degree for Computer Support. I actually just took two classes related two what I do now, a relational database class and a SQL class. Thinking back to my own internship and interviews, I actually think the main reason I was hired was because we talked a lot about how I would handle working with people who weren't very tech savvy, which was what the department needed help with. So it was probably just a lot of dumb luck. 
Wasn't just luck, it was your communication skills that got you into the field. Thats one other thing I will practice on.
Sorry, nowhere near. Get your knowledge and roll with it. You also communicate well which sounds patronizing but you'd be surprised how hard a lot of tech folks find it to express simple points, or to talk to both technical and non-technical people, a skill which will take you very far. 
It amazes me how much more my experience with mysql/mssql gets me than any other skill. And it's not remotely as complicated as others I have. But people are afraid of database or something. I've seen excel workbooks that were insane works of art put together by folks who turn green looking at a simple select statement. 
So how would you suggest someone get into it coming from a sysadmin role? I've been a sysadmin for about 3.5 years and a Jr sys for 1.5 before that with L1-2 tech support before that, but there really isn't an opportunity to apply for one without some piece of paper to get me in the door. I see a lot of professionals say certs are worth bunk ( and I've seen plenty of evidence to corroborate that) but should I just send in resumes without 'dba' experience even though that's certainly been in the realm of my responsibilities? Edit: I should clarify, I do have a passion for relational database work of any kind. I work and study outside of my job as well as volunteer for any task related to database work that I can. This translates to a resume that, while it says sysadmin as a job title, might read closer to dba. Maybe I just have a hard time getting through hr vetting, maybe I suck at resumes. 
ok sir , thaks for suggest ! 
I just realised what my issue was with my own query thanks to this! I had done what you have wrote down here, except for the last bit about the IDs being unequal to each other. Such a silly mistake from my part, but thanks a million for the help!
define the parameters? there are no parameters
I'll just echo everyone else - none. I have no SQL certifications and a bachelor of arts in Journalism. Work on some side projects (even just for yourself). Prove you can do it. In my mind, anyone can pass a certification exam. Doesn't mean anything if you can't apply the knowledge and understand how to use it in the real world.
I'm a SQL developer, I have no certificates. If any of my current or former co workers have certs I'm totally unaware of them. Every SQL position interview I've had has some general questions to determine your skill level. Be prepared to know the difference between the various join types.
OK, but the Java portions wouldn't be (a significant) part of the certification - a query is a query, whether it's in a Java app or executed by the user directly against the database. PL SQL is Oracle's dialect/extension of SQL.
Do you have an expression ("Expr1" etc) in your design view? Example: https://support.content.office.net/en-us/media/5b50a6d2-891d-43a0-9347-9ed32b582768.jpg
You might have to start from scratch or sell yourself differently. For resume help ask your friends, colleagues, and post it to /r/ITCareerQuestions. Do you do any programming at your current job, take any classes, etc.? The volunteering should help, build off of that. You may have to start over and look for entry level positions to get your foot in the door with a company. I started as a Network Administrator (more of a system admin on a small IT team), but now I'm a SQL programmer. I did have a schooling background where I had plenty of IT and programming classes though. A good friend of mine helped me get a job at a place because I knew COBOL, that transitioned into learning SQL on the job and going from there. As for the certs, I agree with everyone else, don't worry about them.
Really? I feel like I'm pretty much a novice at SQL, but joins rarely give me a problem, I can also distinguish between an outer or an inner join, while also understanding when and where to use each. Basically, your statement has confused me even more on where I stand with my personal level of knowledge for SQL. Then again maybe I'm selling myself short and you just pointed it out to me. 
If you have the money/resources, the Microsoft B.I stack as a whole is incredibly versatile. It is expensive but it handles so many different types of data. For front end reporting, I'd check out Tableau. It can connect to nearly any type of data. 
&gt; For front end reporting, I'd check out Tableau. It can connect to nearly any type of data. But you'll pay dearly for the privilege.
SSRS is an amazing reporting tool, but Access reports is also very flexible and allow a lot of design freedom. If your only issue is the 2gb limit from Access, then get a MSSQL express, move your data there (10gb limit and SQL has an easy time importing data from Access), link the tables to your access application and keep using your reports there.
Another good idea is to explore the various roles you can obtain without needing a ton of experience. You may not start as a SQL Dev, but you can easily get there after a couple years in a entry level role. Data Analyst is one that comes to mind, that's where I got my start. 
That is my current setup, and I do a lot of table linking across other Access database files to avoid the 2 gb limit on individual .accdb files. I'm not sure if I can keep that up indefinitely as reports grow larger, however, that could just require more imagination on my part. 
Can confirm.Am an Enterprise user with a developer license and they basically want your first born. If you want Tableau Server they also require a sacrificial offering.
Yeah, I know what PL/SQL is. Also, Java used with an Oracle DB is a bit different. You can do things like call a Java procedure from a trigger with Java Stored Procedures. Let me expand on what I meant when I mentioned the certificates, which I think is what you are hung up on. The courses and certificates I've seen were mostly DB administration courses and querying courses targeted advanced end users (like analysts) or admins. I'm sure there are stuff for developers too but where I was that wasn't really popular, among employers or employees. 
Yeah but their offices in Fremont *are* pretty nice-looking, or at least the lobby is. So someone's getting something for that price.
You can use UNION to combine the results from both queries. You should be able to combine the queries by adding the extra columns for table b like normal, and compensating for them by selecting null in their place in the query for table a: SELECT A.Emp, A.Mon, A.Tas, A.Hou, A.TPT, null, null FROM TABLE_A A UNION SELECT B.Emp, B.Mon, B.Tas, B.Hou, B.TPT, B.EXTRA_COLUMN_1, B.EXTRA_COLUMN_2 FROM TABLE_B B
worked like a charm, thanks!
http://sqlmag.com/t-sql/working-variables-t-sql-part-1
Is JavaScript your server language? Otherwise you'll need to pass the javascript variable to the backend first (e.g. by submitting a form or via AJAX). Your server side language (E.g. PHP, Ruby, Node.js using JavaScript) can then use that in your SQL query.
Ah gotcha, it's more of a barometer question. 
* don't alias to numbers, don't alias to single letters, These table names are short, just use them without aliases. * Average *what*? Average call duration? Something else? What are the data elements you have in each of these tables? If you're going to ask us to do your homework for you need to actually provide better information. Your current attempted SQL statement has * the where in the wrong spot * a second where with your join that should be an ON * you cannot say field = ( value, or value) you need to do an IN clause What DBMS is this database stored on? SELECT Calls.CallType , City , DATEPART( Week, Calls.Call_Date) as week , AVG( Calls.Duration) FROM Customers INNER JOIN Calls ON Calls.CustomerID= Customers.ID Where Customers.city IN ('San Jose', 'Boston') GROUP BY Calls.CallType , DATEPART( Week, Calls.Call_Date)
Does your workaround involve string concatenation? Because if it does, I've got some bad news for you...
You could also use a CTE (common table expression) for getting the max(transaction_date) and a unique identifier then link your main clause to that CTE: with CTE as ( select max(transaction_date) ,transaction_id from whatever_table group by transaction_id) select attributes from master m inner join CTE on m.transaction_id = CTE.transaction_id /** THis would limit your select from master to only grab the latest transaction **/ 
that's just a generic sql timeout. It's not able to reach the database, wherever it is, for one reason or another. You'll have to triage from the start. Maybe use a packet sniffer to find what machine it's trying to reach sql on? Then make sure the database is there and available for logins. If it is find what account it's failing to login with, from the database's log, and make sure it not locked from failures or something. If it should all be self contained on the same box you may just want to verify that MSSQL service is starting up on your machine without error.
It would work as written because city only exists in one of the two tables, so the database parser doesn't need to have the name fully qualified, but it's better technique to do so, therefore.... SELECT Calls.CallType , Customers.City , DATEPART( Week, Calls.Call_Date) as week , AVG( Calls.Duration) FROM Customers INNER JOIN Calls ON Calls.CustomerID= Customers.ID WHERE Customers.city IN ('San Jose', 'Boston') GROUP BY Calls.CallType , DATEPART( Week, Calls.Call_Date) And I think I covered your weeks starting on Monday in your other post.
So as far as MySQL being built for speed and postgres being slower, not at all true any more. It is still true that postgres has many more features though. MySQL's replication story is better at the moment for sure, but postgres still has totally adequate replication for 95% of use cases. Also, if you interface with other dbs a lot, FDWs will be a giant help. 
Reboots: not just for your windows anymore.
If you're on more or less recent MS SQL platform you can use 'cross apply' instead of an inner join to get to a specific record: cross apply ( select top 1 * from accountTransaction TT where tt.accountReferenceID = R.accountReferenceID and tt.AgencyID not in ('25') order by TT.CompletedDateTime desc ) T
Agreed. Unless there's a specific reason for using MySQL, I'd go with POSTGRESQL. Both are good, but PostgreSQL is more feature rich and much closer to commercial dbs than MySQL. Unless you really need SQL Server, I'd go with PostgreSQL.
Combo box on access doesn't get new data after it loads in the form open. What you can do is to add the: combobox.rowsource = "select * from table" in the combo box onEnter event, or in the Office 2010+ use the combobox.requery to update the list.
Unsure of the dbms you're using, but this is Microsoft compliant. Select datepart(wk, call_date) as week, avg(callduration)/60 as avgCallinMinutes from tasks Group by datepart(wk, call_date) The second one, I'm not sure what grouped by weeks on Monday mean. But it's the same query. Select datepart(wk, calldate) as week, avg(call_duration) /60 as avgCallinMinutes from tasks t left join customer c On t.id = c.customerid Where city in ('dc', 'boston') 
Have you looked into making your query a stored procedure?
One of the things to consider is the environment of the clients attaching to the database. If your apps are all web based then, for the purpose of the database, you only have one application. If, on the other hand, all your users are running Windows desktops then there is a hidden factor to consider. Right now all your users already have the needed database client access drivers installed and maintained. If you change to either of the others you will have this expense. Even if the drivers are free you still have the expense of having to go to each user and do upgrades. You might find that you are total dollars (if that is tour currency) ahead to get a better server and stay with express. Also look at the server machine. What ***else*** is it running. If ***anything*** then move that stuff elsewhere and reevaluate. I don't like to run even a lightweight web server on my SQL box.
Sorry for the late response. I tried to run this query in R test &lt;- dbGetQuery(db," SELECT MIN(DATETIME) AS MINDT, MAX(DATETIME) AS MAXDT, AVG(PRICE), (SELECT PRICE FROM stocks WHERE DATETIME=MINDT) AS OPENING, (SELECT PRICE FROM stocks WHERE DATETIME=MAXDT) AS CLOSING FROM stocks WHERE DATETIME &lt; '1999-06-02 00:00:00' GROUP BY strftime('%s', DATETIME)/(60*5); ") but that gives me this error Error in sqliteSendQuery(con, statement, bind.data) : error in statement: no such column: MINDT edit: I think this is due to the fact that subqueries can not acces aliases outside itself ... but i have no idea how to fix this really. 
Run pg_dump using Task Scheduler. Docs cover how to deal with passwords on Linux ; not sure about Windows. 
I'm sure you have a table that handles the user logins, you can simple have a flag on that table for whether or not they are a seller. Just pass that in as a parameter to the upload page.
`DATEPART(DAY, LoadDate) = 1` Or, if you're not using SQL Server, `DAY(LoadDate) = 1` But this could cause major performance issues as it's not [SARGable](https://en.wikipedia.org/wiki/Sargable). What happens when you have a "business month end" that *doesn't* fall on the last calendar day of the month? This is where a [date table in a utility database](https://www.brentozar.com/archive/2014/12/simply-must-date-table-video/) can come in handy. Include a column on that table that indicates which days are the month end days you want (just a yes/no), join on that table and filter on that "month end" bit column. Then you *can* use indexes.
SPIDs get reused all the time. Once a connection is broken/closed, that SPID can (and will) be re-used for a future connection.
This is application-level security, not database security. It's going to be down to how you design your application.
thank you , kind stranger, but that data base is not editable so I cannot create a date table. I don't know about date part, DATEPART(DAY, LoadDate) = 1 but to get all the data from the month of April, I need to key in May 01. For all dates to specify should be the 1st day of each month. Since the daily data on server is unique and appended each day, I am thinking DATEPART(DAY, LoadDate) = 1 would retrieve partial data from each days and repeat it in the results, like data pulled with 04-15 keyed in is data from 04-01 until 04-14, and when it gets to 04-16, It will return all data above of 04-15, and all data above as of 04-16 if this makes sense

Way too much work. Go back to my original suggestion - what you describe here is replicating what I suggested, but with a required change in the query every month.
someone downvoted you, not me. this works. thank you. 
Yes, I noticed it doesn't capture data from 2013 at all. what performance issues exactly are you referring to ?
Hey dude. Just an FYI - the '0' *is* actually a valid date value for this code - in this case it auto sets to the first Monday of the week that contains it. You can *also* set the date manually, as you did in your code, but I didn't have anything more to go off of in his original post, so I just worked with what I had ;) [Here's](http://www.sqlservercentral.com/blogs/lynnpettis/2009/3/25/some-common-date-routines/) a quick resource to check out some more on the use of 0 in this situation.
So just running this in 1 query will permanently set week starting at Monday- for all subsequent queries ran?
Do an update then insert if the update didn't find a matching key. I. E @@rowcount =0
Actually, I'm going to retract that. http://stackoverflow.com/questions/883127/sql-server-set-datefirst-scope
If you're doing it in the code behind then just handle it there with a simple if statement. Otherwise if you have to do it in SQL then try something like this... IF EXISTS(SELECT whatever FROM whatever_table WHERE whatever_condition) BEGIN update END ELSE BEGIN insert END END
so if i understand correctly - you cannot permenatly set datefirst to Monday? - You have to run that SET DATEFIRST for every session you start? 
Correct. You can also use a date and time mapping table setup. It would be worth using a function if this will have to be fixed frequently. http://www.jacek-szarapa.com/index.php?p=sql&amp;d=10
so if i understand correctly. If i run this query it will set the entire DB to datefirst as Monday? I am working in SQL Server not MySQL declare @cDate datetime; set @cDate = GetDate() select (@@DATEFIRST-1)+DATEPART(dw, @cDate)- case when (@@DATEFIRST-1)+ DATEPART(dw, @cDate)&gt;7 then 7 else 0 end 
It will only set the session you are connected in to Monday. So if you have two tabs open and run that in tab 1 and then select @@DATEFIRST in tab2, you will see a different day in both tabs. It's only session based, not instance based.
exactly I understand it is session based. However how do I write a query to permanently make it affect all sessions? OR is this not possible?
Thanks! I have on last question however regarding DATEFIRST set to Monday. IS there a way i can run a command to set the entire DB Datefirst to Monday? I want a permenant change so i dont have to Set DATEFIRST at the first of every query
Thank you very much.
I'll do some reading. Thank you for this resource.
This should do: SELECT * FROM [table] WHERE (width-height &lt;300 AND ratio &gt;=1.7) OR (width-height between 300 and 700 AND ratio BETWEEN 1.5 and 2.15) OR (width-height &gt;700 AND ratio BETWEEN 1.7 and 2.3)
Thank you!!
so, like I said, using the "0" in this code will ensure that every grouped week is grouped by the first Monday that the date is contained in. (Hence why the day Jan 15, 2015 is bucketed into the week of Jan 12, 2015 in the example I gave you before) If you are *required* to alter DATEFIRST for the assignment I would read a little about declaring variables and setting global variables - you can alter it at the beginning of your code and bingo-bango. If the assignment isn't explicit about using and altering DATEFIRST, you can go ahead with the previous code and rest assured that it will auto group the weeks by the Monday of that week.
A data comparison tool is what you want. If you told us what db you are using we might even suggest a few. 
You may as well stick in a parentpartyid in parties and model the hierarchy while you're at it.
Oh, check those two things: Is your combo box ControlSource linked to a field of your table? The BoundColumn has the right number of the column from the query in the RowSource? 
From SSMS select View-&gt;Registered Servers. Right click on Database Engine/Local Server Group. Add New Server Group and then add New Server Registrations. Right click on your new server group that has multiple servers in it and select "New Query". Put in a query that works for a single server and be amazed how SSMS executes it on every server in the group with an added column to indicate the instance name. Anyway, I'm being a bit facetious here but I just discovered this and it's damn cool. If you weren't aware of "Registered Servers" it will probably make you grin as well.
See "Registered Servers" in SSMS and "New Query" once you've got them set up in Server Groups.
I recently started using registered servers after reading an article. I love them! only downside is we have multiple testing environments and they do not use the same credentials, so i cant group all 100 servers together, but its still damn useful :)
I keep getting the same #1064 error. Here's what I got SELECT * FROM [wp_postmeta] Where [meta_value] LIKE "%www.shoes.com%";
Got an error LIMIT 0' at line 2 
Alias shorthand references to the table. FROM wp_PostMeta A This "nicknames" the wp_PostMeta as "A". It can there on out be referred to as "A" whenever mentioned.
.......................................................................................................................................
Does this look like I am on the right track? SELECT PONumber, CASE PO_ID WHEN PONumber = 'W' THEN 1 WHEN PONumber = 'SAL' THEN 2 WHEN PONumber = 'F' THEN 3 END AS locationID FROM POs;
Solved it like this - SELECT p.*, CASE WHEN p.PONumber like 'W%' THEN 'William' WHEN p.PONumber like 'SAL%' THEN 'Salford' WHEN p.PONumber like 'F%' THEN 'Farm' WHEN p.PONumber like 'LA%' THEN 'LA' WHEN p.PONumber like 'N%' THEN 'Newman' WHEN p.PONumber like 'S%' THEN 'Shed' WHEN p.PONumber like 'H%' THEN 'Home' END as "locationName" FROM POs p
Which ever is easier for you. If the target is already empty, then let it be. If you will make the target, then it is easier to just copy the source db and it will have data, in that case also let it be.
DECLARE @COLUMN VARCHAR(100); SET @COLUMN = '%STRINGTOFIND%'; SELECT T.TABLE_SCHEMA ,T.TABLE_NAME ,C.COLUMN_NAME ,C.ORDINAL_POSITION FROM INFORMATION_SCHEMA.COLUMNS C INNER JOIN INFORMATION_SCHEMA.TABLES T ON C.TABLE_SCHEMA=T.TABLE_SCHEMA AND C.TABLE_NAME=T.TABLE_NAME WHERE C.COLUMN_NAME LIKE @COLUMN
I'm no expert and haven't used sqlite but maybe FROM stocks WHERE DATETIME = MIN(DATETIME)
SELECT and FROM are inverted, should be: use [insert_your_database_name] SELECT INFORMATION_SCHEMA.COLUMNS FROM TABLE_NAME WHERE COLUMN_NAME like '%stringtofind%' Well ... a query can't be more basic that that, unless you don't even use the WHERE haha
Oh, that was my fault .. I messed up some of it before I formatted it and saw the subselect in the where I also put as next to my aliases this time around SELECT TN.[XHWVE#] AS [Wave Num] , LM.[Lists Over Max] , count(TN.[XHPKL#]) AS [Total PLs] , SUM(TN.XHTLUN) AS [Total Units] , SUM(TN.XHTLPK) AS [Total Locs] FROM TABLE_NAME as TN Left Join (SELECT TN.[XHWVE#] , count(TN.[XHPKL#]) AS [Lists Over Max] FROM TABLE_NAME as TN WHERE TN.[XHPKL#] in (select TN2.[XHPKL#] from TABLE_NAME as TN2 where TN.[XHWVE#] = TN2.[XHWVE#] and TN2.XHORG = '330' and (TN2.XHTLPK &gt;= [Max Pick Parameter] or TN2.XHTLUN &gt;= [Max Unit Parameter]) ) group by TN.[XHWVE#] ) as LM on LM.[XHWVE#] = TN.[XHWVE#] WHERE TN.XHORG = '330' and TN.[XHWVE#] between [Enter low wave value] and [Enter high wave value] GROUP BY TN.[XHWVE#] ,LM.[Lists Over Max] ORDER BY TN.[XHWVE#];
MERGE? I mean, assuming you're suggesting what I think you are, it's probably not good practice to hold duplicated data. Unless your volumes are massive then the "double dip" approach should be fine. If your PK is present in the source data (i.e. not an identity) then maybe put an index on it.
For anyone using SQL Server, Redgate's SQL search is a plugging for SSMS that does this and so much more and it's FREE!
You rock!! Thanks!
Ahhh.. that makes sense. It's not valid for this code in Sybase ASE.. which is where I tried it.
Yeah I was trying to avoid using like. If it works and your data set isn't massive then that way is fine. 
Search all objects for a string: SELECT name , type_desc , create_date , modify_date FROM sys.objects WHERE OBJECT_DEFINITION(OBJECT_ID) LIKE '%SearchString%' Unrelated to searching for column names, just thought I'd add a handy script that I use now and then.
If possible make 2 new fields in that table and the default value of those fields be SUSER_SNAME() and APP_NAME(). That way you can see who and what made the insert.
A likely scenario you'll see an application log error instead: Column name or number of supplied values does not match table definition. 
Depends how the insert was made. IF the insert uses a "SELECT *" then it may cause that error, if the the fields are described then it is fine, also if the insertion comes from a Form in any system then it will also work. EDIT: An error may be even better, you'll find where the insertion is coming and also fix the INSERT statement to not use SELECT * haha
are you asking how to make a join on columns with different names? Like this: select A.*, B.* from MyTableOne A akindof join MyTableTwo B on B.This = A.That There's no need for CTEs or anything. Please prefix all your columns with aliases, even if column names are unique.
potentially window functions are right here. first_value, last_value, etc.
Aggregate functions can't be used in the where clause.
I'd try a CTE then
Thank you all for your help. I created a combination of the CTE and the Cross Apply offered up as suggestions but unfortunately I think I need to go back to the drawing board with my issue. Seems even if I query for agencyID not in 25 I will still receive accounts that could have had a transaction performed by ID 25. The query is just keeping out the instances where a transaction was performed by ID 25. This is ultimately what I'm trying to keep out. My thinking was treating this like a one to one relationship when it is a one to many relationship. 
Are you wanting to UPDATE the IDs? Or are you want to SELECT the IDs where they are the same?
This. I can guarantee that this works since SSMS 2005. I can't see the real usefulness of that plugin, it only saves the time of actually highlight the code ...
What I like to do when i'm checking delete statements: BEGIN TRAN DELETE FROM [wp_postmeta] OUTPUT Deleted.* WHERE [meta_value] LIKE "%www.shoes.com%"; ROLLBACK TRAN This query will show you what rows are being deleted like a select statement, and also rollback any deleted rows like nothing happened. Once you are happy you can change "ROLLBACK TRAN" to a "COMMIT TRAN" and you are as good as gold.
i do too much in a day to list but common things I and others consitently work on data clean up and IT projects related to data tools. I write code and create reports for upper management with recommendations based on analysis. I don't often get to choose questions myself sadly, but sometimes. Questions usually come to me from the executive management. If the questions come from elsewhere, I put them in a 'maybe someday' pile because I'm so busy with everything else.
I'm a business analysis at a small consulting firm, but handle a lot of the data analysis stuff, especially when it gets too big for excel. A lot of time is spent cleaning and shaping, then I start exploring the data and creating quick graphs looking for anything insightful. We usually have an idea of what the outcome is going to be when we go into it, but I'm always looking for something unexpected. The last part, and really the most important, is the "so what". Being able to explain what that means and the implications it has is key. I should mention I do the majority of my work in R (just completed the Data Science Certification through John Hopkins), but also fluent in VBA and SQL, and entry level in Python. 
First, as a junior person, you won't be asked to create ideas "from scratch." The business leader and senior people in your department will have the idea, it'll be your job to provide them data sets and analysis that test their hypotheses. Your value to the team beyond pure technical know-how is you understand how to bring objectivity and rigor to their intuition. So when you hear them suggesting things like "Maybe it was bad weather that caused the dip in sales" you can analyze that data, and if whether the data says it is or isn't so, you still did your job. You can also start to impress upon them the importance of capturing data so that have something to analyze. When I was working at a shipping firm, I had them build databases to house inspection and cargo loss data because they were constantly guessing at these numbers or manually adding them up from paper reports. Important skills: * Good listening and question asking. As an analyst, your job a lot of times is turn a mental model / metaphor / intuitive concept into a concrete model. Use meetings to determine scope, tolerance for error, confounding variables, etc. Explain it back to the business; rinse; repeat. * Objectivity and honesty - if they don't trust you they won't trust your numbers. * Strong Understanding of Scientific Method (yes, the one you learned in 5th grade) * Modeling data to support more than one question (I would *strongly* recommend you pick up a book on OLAP / data warehousing concepts ) * Communication - being able to explain your analysis in layman's terms (or even better, in dollars and cents) is huge. The most important piece of advice is to stay focused on the business objectives.
You pretty much just described ETL work
UNPIVOT is a beautiful command and can be used without any worry. If your source allow you to use it then please do, it makes the work of change from a bad structured table or a data from a sheet and easily change into a structured data that you can work as a normalized table, can't be any better.
Check the Scheduler? The update program could be in there. 
You tested it, it improves performance. Ultimately, it's a select statement in a view. It should never hurt anything. As it stands a union to select from a table is reading x amount of times and sorting for distinction (union performs distinct in mssql). Unpivot is a great function to read once and transpose the data. You may have to convert some fields in your select statement so before the unpivot they are all the same data type. This is just converting the data as its being read, it does not change the table. In IT and development. It's easier to ask for forgiveness than to ask for permission because nobody at my job understands what exactly I do. Have more confidence in your choices because you were a 100% correct in using Unpivot over union.
ayy lmao
https://msdn.microsoft.com/en-us/library/ms181091.aspx https://www.wireshark.org/ You can find out EXACTLY what is causing it! Also: SELECT session_id, num_writes, st.text AS statement_text FROM sys.dm_exec_connections AS ec CROSS APPLY sys.dm_exec_sql_text(ec.most_recent_sql_handle) AS st ORDER BY num_writes DESC
Bring down the NETWORK? Only if their network is REALLY REALLY poorly designed. There is no reason local network utilization for a single machine running near max should cause problems, unless the entire network is only build to the capacity of allowing one machine to use its full connection (in which case, it is a HORRIBLY designed network).
Yea, I have queries that pull millions of rows, does maths, etc... and it MAY slow down a network for 30s-2 minutes, but nothing like bringing it down. This sounds like a really poorly designed network or horrible machines.
Very good way of looking at it. It might be time for some behavior modification on the developer's end. I agree top 2000 is the better way to go. Thanks for this.
So you need this query as a view/inline to find your min/max datetime by the group by, then join this back to the main table. You can then join to only grab the first/last record in each group, the pivot out price with sum(case ...
What are some examples of the questions? Just to get a general gauge of what I might be getting into and how I might want to cater my cover letter or resume to display an analytical mind. Do you utilize your skills in SQL and then get the data and put it into an excel spread sheet? How would you generally create reports? Tableau? Excel? Graphs? Powerpoint? Sorry if I'm asking too much. Much appreciated. 
Can I ask what you mean in some of your bullet point? &gt; Use meetings to determine scope, tolerance for error, confounding variables, etc. Explain it back to the business; rinse; repeat. I understand the important of turning data into useful information. My question is what kind of knowledge should I have in my background. Do I need a finance or business background to help me explain such information to my manager? And how would you generally describe the ways in which one would convey the information? Would it be through a graph? A spreadsheet? Powerpoint? &gt; Modeling data to support more than one question (I would strongly recommend you pick up a book on OLAP / data warehousing concepts ) Do you have any recommendations on a book to pick up? 
In all seriousness though I spend a lot of my time either in the office or at home doing research on data analysis/visualization tools. Keeping my skills current can be a huge endeavour sometimes. There's always more certs to test for, more conferences and meetups to attend, etc. As far as the day to day requests, a big part of what I do when I acquire a client is figuring out their data analysis requirements. Data collection, ETL, visualization and reporting. Finding out how the business units I deal with do their work. Then I generate the output based on what I find there and what we agree to with the stakeholders. A lot of times they just need a few new fields added to an existing report. My favorite part of the job though is running into an organization where non-functional requirements start impeding their day to day operations. When data gets too large or the requirements for real time data become an issue and we're able to go into project mode and create a robust solution using best practices. I love building data warehouses quickly and simply when possible. I also love having that much control on cleansing data so that we can hand the client a tool like Tableau so that they can perform deep dives into their data. Training them can be quite fun. I've worked for manufacturing, mining, government, higher education, business to business service companies and while each business is unique there's enough in common with each other when it comes to the requirements of business functions like HR, Finance, Inventory, etc. So there's a lot of opportunity out there to hop around in various industries. About the only thing I haven't done is retail but I hope to do so in the future. Good luck out there and have fun.
Why not just deploy and run it a serverside? The dtsdebughost doesn't really give you the best test for volume scenarios.
One last question. You're extremely informative and you gave me some really good stuff so far so I greatly appreciate the time you took to respond to my questions. &gt; A typical solution will be comprised of the client's source data (Oracle, SAP, SQL Server, other proprietary or open source solutions), an integration layer performing all the ETL using SSIS or basic jobs and stored procs in SQL Server, the data warehouse itself and some front end reporting tool (Tableau, SSRS, excel, etc.) Just to gain a better understanding: Let's say I find a job for Company ABC that is large enough to have it's own data analysis department. And they want me to do an analysis of their data of say Customer that bought Item X in month of January. I'm guessing that analysis would be considered a typical solution of the "client's source"? And in reference to integration lawyer performing ll the ETL using SSIS, do you have any good resources that I can familiarize myself with the linguistics and terms that are involved here? I've read some stuff about ETL but I don't think I know even a fraction of what there is to know here and I feel it'll be vital to me to familiarize myself much more with this area as a support beam to everything I'm trying to learn. When you say data warehouse are most companies nowadays using an out source for this? Say a small company would never have a data warehouse of it's own right? Or in such situation won't even have a complicated data storage system. Taking back to your friend's place as an example. What kind of data set did you help him build to increase their back office processes? I know all businesses have data, it's just that I don't see smaller companies finding the need for an analysis like yourself. 
fair question, its like any other career path, you have entry level, and advancements. You would be hired for an entry level job, most likely doing data entry or cleaning up excels over and over, where its then handed to someone who does futher analysis. After a few weeks/months you will learn what your doing, how your info is being used and who uses it. Eventually youll be doing more than helping, youll be contributing. This is where a business background can help
Did you learn R through coursera? While I've been focusing on SQL I feel R might be the next important language to get to know. After I gained a sufficient amount of knowledge on SQL that is... or get a job that lets me continue to grow in it. 
LocalDB
not sure a union into one field would work because I want to find all the cases where UPCcode is for example '059800990158' or '00059800700153' and all the cases where originalUPC is '059800990158' or '00059800700153' So it could potentially bring up '059800990158' | '059800990158' '00059800700153' | '00059800700153' '059800990158' | '00059800700153' '00059800700153' | '059800990158' '00059800700153' | Another completely different UPC Another completely different UPC | '00059800700153' Another completely different UPC | '059800990158' '059800990158' | Another completely different UPC And most frustratingly of all '059800990158' | Multiple different UPC's Multiple different UPC's | '059800990158' '00059800700153' | Multiple different UPC's Multiple different UPC's | '00059800700153' Which are the big, Oh shit the supplier fucked up big time ones that can send my stores into a frenzy when you wind up with a list of 10 to 1000 products that suddenly all scan as if they were a $2 item.
That would bring back the whole list instead though which can be anywhere between 150 and 1000 lines which is one of the things I want to avoid unless I made some sort of counter and limited it to where counter = n but I wouldn't be able to look at pairs though...
Thank You very much for the response. I was mostly worried because the results with this seemed too good to be true. I was worried that there was some "gotcha" that I was missing.
What exactly are you trying to do and what have you done so far? Really hard to understand without seeing what you need or how you're doing it
Just trying to align my output into columns properly. Like I have last name, first name then job status. But since the first/last names are differing lengths, I can't left align the job status properly. I've tried TRIM, LPAD, RPAD. Basically my columns go right or left and looks wobbly, I want them all aligned to the left so they each start with the first letter in the same vertical alignment. *edit* basically I want this [good result](http://i.imgur.com/8MRvYjs.jpg) but have [this](http://i.imgur.com/t0jQOxK.jpg) (these are random LPAD values, not my best effort, I forgot to save but it doesn't matter I am not getting the result I want) my formatting code - v_output := idx2.last_name || ', ' || idx2.first_name || LPAD((v_job_status_format), 14) || LPAD((v_job_type_format),16) || LPAD((idx2.hire_date), 14) || LPAD((idx2.pilot_type) , 20);
I agree, LocalDB is the way to go. If for some reason that doesn't work out maybe look into SQLite. https://system.data.sqlite.org/index.html/doc/trunk/www/index.wiki
Ask your developers "are you doing the following on a table?" SELECT * FROM [table] ", and the table has bunch and bunches of rows?" My bet is that they don't need every column and for the sake of pity they don't need that many rows across a long distance connection. Yes rogue queries can and do max out the bandwidth on even good connections. Write better queries. 
Assuming that the last &amp; first names are defined as varchar2, then it looks like you just need to *lpad* the concatenated version. Using a combined length of 45 characters (just a random example), it would be something like: v_output := LPAD(idx2.last_name || ', ' || idx2.first_name, 45) || LPAD(v_job_status_format, 14) || LPAD(v_job_type_format, 16) || LPAD(idx2.hire_date, 14) || LPAD(idx2.pilot_type, 20); I'd also suggest performing explicit conversions using *to_char()* for anything which isn't actually a text value, since that gives better control of the output (and is generally a good practice anyway).
The answer is simple. Look at the definition of CTE in books on line. You can alias the columns coming out of the CTE by putting the column names in the header of the CTE. Almost nobody does this. It might just solve your problem though.
Awesome! It ended up being RPAD, but the concatenated last/first name was the secret sauce! [Behold! I have achieved mediocrity!!!](http://i.imgur.com/pcwa92g.jpg)
Ugh I hate that interface, but more power to you if you get value out of it. The 'Progress' tree is damn awful - with SSIS's long error messages that burn 1000 chars before they get to the point, and that tree view that truncates them, it drives me up the wall. I have found the SSISDB logging and some tips from this made all the difference for me: http://sqlblog.com/blogs/jamie_thomson/archive/2011/07/16/ssis-logging-in-denali.aspx I also process and re-view the sysssislog into a better tree view, stripping out path failures, repeats and message 'guff'. Fortunately when it eventually gets to the point SSIS tends to stick the important bit in double quotes. And 2 billion a day. Wow. What is that, sensor data or something? I'm guessing you're working with appliances to stash that much data? 
thank you for your awnser, but could you give me an example?
yup and yup
I've found on numerous occasions that what VS provides during debug time of SSIS packages often has very little relation to what actually happened, row counts can be entirely wrong, and sometimes execution indicators don't update (i.e., it'll still show as running when the package has completed). I still use it, but I'd take what it presents you with a grain of salt.
I know in MSSQL you can do select top (@var) * from table. So you could grab your ratio and do the math and set it to your variable(s). Is this all in the same result set? What if the ratio is 1:2:3 and you need more rows for B than A? 
This looks like some advanced class test, so here's a first step description: figure out relevant maximum counts for names groups from Table_Y, figure out maximum multiplier for the ratios to get up to these maximums across all of your names. You never said how exactly you want to fetch the N rows from a respective Name group so there are various ways to go from there. 
Your prof sounds like a dickhead.
&gt;I've tried LocalDB. I get errors when deploying applications using LocalDB. Only works properly in development. The LocalDB file that you're using has to be part of the deployment as well, or you have to configure the deployment to create a LocalDB and associated objects if they don't already exist. I'm not at work right now, but I'm pretty sure I've saved a link on how to set that up - I'll search for it on Monday.
This is an excellent question.
I normally do this for different entities. For instance a faculty member is also a user but they both represent a single person and are a one to one relationship. Faculty members are going to have properties a regular user does not so it helps from not having a bunch of blank columns associatedto the user table. I would take a look at if they are really different entities otherwise I would add the attributes to the single table. I am not a DBA or claiming to be the best, just my 2 cents. 
Amazing! Thank you! 
First of all, I do agree with you, but for the sake of discussion... The last place one should ever look when troubleshooting a problem like this is the code, especially if it's stable code. 
&gt; One thing I've seen way too much of is the dependence on 'null' in query filters, Agreed on both counts. This practice is common and faulty. Thanks
&gt; it's all about the relationships between tables and what info you are after. Then it should come naturally and there won't be a video that teaches you 'joins' because how you link up tables is determined by the data model and the question you are trying to answer. OP, this is the TL;DR and its very important for other aspects of SQL in addition to the explicit joins in the FROM clause. You have to start thinking in sets of data and understanding how two tables related to each other. Even if they don't related to each other directly, they will probably related to each other indirectly via a series of other table joins with the interim tables serving as links in a chain between the two tables. You just have to figure out those relations first. I don't think you can learn this from a video. You have to get out there and practice it with actual data. Eventually, you'll get to the epiphany where you realize how it all works and can make it do your bidding - kind of like Neo controlling the bullets at the end of the Matrix. But, it takes time and practice.
Have you ever tried something similar to: SELECT AutoNo FROM tblSupplierProducts AS SP WHERE 1 = CASE WHEN SP.UpCcOdeIn IN ('059800990158', '00059800700153') THEN 1 WHEN SP.OriginalUpc IN ('059800990158', '00059800700153') THEN 1 ELSE 0 END;
I'm not sure I'm explaining myself right at this point, I added more info to an edit of my main post but even then I think this question may be a lost cause. -_-
I'd make that a bridge table from another table. A person is a person. A role is a role. A person can have many roles/permissions, and a role/permission can have many people. Please don't use "is*x*" in your database. Yes, it's a bit faster, but are you smart enough to predict every single possible permutation of what entities will be classified be? An entity/bridge/type schema will make your DB maintainable and flexible. An is/isnot column will make your DB inflexible, and will infuriate everyone who has to make it behave in ways you didn't foresee.
Well, you're right, but when different entities are contained in the same table, and nulls are explicitly used to differentiate them, then you no longer have normalized data. From a practical standpoint does it make a difference? Someone's yes, sometimes no. 
&gt; when different entities are contained in the same table, and nulls are explicitly used to differentiate them, then you no longer have normalized data. Agreed. If we don't have normalized data we can normalize it further. If there are fields that are dependant on the value of the is_faculty column then that must be part of a primary key in a new table in order to follow 3NF. I.e. "Every non-key attribute must provide a fact about the key, the whole key, and nothing but the key." What about this: --user information common to all users go in this table CREATE TABLE [user]( [user_id] [int] NOT NULL, [is_faculty] [bit] NULL, [name] [nvarchar](10) NULL, CONSTRAINT [PK_user] PRIMARY KEY ( [user_id] ASC )); --attributes only relevant to faculty member goes in this table. --notice that is_faculty is part of the key. CREATE TABLE [user_faculty]( [user_id] [int] NOT NULL, [is_faculty] [bit] NOT NULL, [faculty_attribute1] [nvarchar](10) NULL, CONSTRAINT [PK_user_faculty] PRIMARY KEY ( [user_id] ASC, [is_faculty] ASC )); --Add an FK constraint to the user ALTER TABLE [user] ADD CONSTRAINT [FK_user_user_faculty] FOREIGN KEY([user_id], [is_faculty]) REFERENCES [user_faculty] ([user_id], [is_faculty]); --add non faculty member. insert into [user] (user_id, is_faculty,name) values (1, null, 'Carl'); --add a faculty member. --faculty members must first insert into user_faculty table in order to satisfy the FK in the user table. insert into user_faculty (user_id, is_faculty, faculty_attribute1) values (2, 1, 'some value'); insert into [user] (user_id, is_faculty,name) values (2, 1, 'Henry'); I'm not sure if this is the correct way to model a faculty member that is also a user. I'm open to suggestions. 
Make a Weeks reference table. You can use year and week numbers as a key then have as many columns as you need for pre-determined or pre-formatted values. 
Can you elaborate on this? I have *isX* columns on a variety of tables but am not sure how a bridge table would be beneficial. For example I have a job which sends a CSV to a FTP server and upon a successful send it will update the [isSent] column from a 0 to a 1. 
Okay so I was bored (and I only have MYSQL on my home machine but I work with MSSQL at work). Here's the setup of your basic sheet. I chose to put the left set and right set all in one "UPC" column and simply designate them as such with the "typ" column. That way all you have to do is specify the "id" and it gets both left and right UPCs. create table test.reddit_example(id int,typ varchar(10),UPC varchar(30)) #--alternately you could... declare @reddit_example table(id int,UPCCode varchar(30),OriginalUPC varchar(30)); --Don't forget to change all below references to this table. truncate table test.reddit_example; insert into test.reddit_example(id,typ,UPC) values (1,'left','059800990158' ) ,(2,'left','057700215012' ) ,(3,'left','057700215067' ) ,(4,'left','041116005954' ) ,(5,'left','061900107008' ) ,(6,'left','061900000071' ) ,(7,'left','061900128409' ) ,(8,'left','061900126405' ) ,(9,'left','060100034831' ) ,(10,'left','060100035159' ) ,(11,'left','060100035364' ) ,(12,'left','061900000125' ) ,(13,'left','061900000248' ) ,(14,'left','061900000736' ) ,(15,'left','061900000750' ) ,(16,'left','061900126306' ) ,(17,'left','061900128300' ) ,(18,'left','068274000218' ) ,(19,'left','061900101150' ) ,(20,'left','061900101266' ) ,(21,'left','061900126351' ) ,(22,'left','061900129352' ) ,(23,'left','061900128355' ) ,(24,'left','061900122209' ) ,(25,'left','061900122407' ) ,(26,'left','060100035210' ) ,(27,'left','060100000102' ) ,(28,'left','060100000348' ) ,(29,'left','060100000225' ) ,(30,'left','060100009389' ) ,(31,'left','061900000286' ) ,(32,'left','061900101136' ) ,(33,'left','061900107138' ) ,(34,'left','061900107053' ) ,(35,'left','061900107121' ) ,(36,'left','061900107046' ) ,(37,'left','061900126252' ) ,(38,'left','061900128256' ) ,(39,'left','061900129253' ) ,(40,'left','060100037641' ) ,(41,'left','060100037764' ) ,(42,'left','060100038631' ) ,(43,'left','060100000164' ) ,(44,'left','060100000409' ) ,(45,'left','061900122605' ) ,(46,'left','061900122803' ) ,(47,'left','061900110602' ) ,(48,'left','061900111500' ) ,(49,'left','060100035180' ) ,(50,'left','060100034398' ) ,(51,'left','060100034367' ) ,(52,'left','011179531073' ) ,(53,'left','062823094024' ) ,(54,'left','9781770667945') ,(55,'left','9781770667952') ,(56,'left','9781770667969') ,(57,'left','9781770667976') ,(58,'left','6242746310690') ,(59,'left','6242746310768') ,(60,'left','6242746277726') ,(61,'left','062823980594' ) ,(62,'left','062823836372' ) ,(63,'left','0627027511106') ,(64,'left','884191022108' ) ,(65,'left','055577104002' ) ,(66,'left','057000243760' ) ,(67,'left','056200762163' ) ,(68,'left','057000005924' ) ,(69,'left','775749188899' ) ,(70,'left','0614156061848') ,(1,'right','00059800700153') ,(2,'right','10057700215019') ,(3,'right','10057700215064') ,(4,'right','10041116104777') ,(5,'right','10061900007008') ,(6,'right','10061900038200') ,(7,'right','10061900028409') ,(8,'right','10061900026405') ,(9,'right','060100034855') ,(10,'right','060100035173') ,(11,'right','060100035388') ,(12,'right','10061900001341') ,(13,'right','10061900001051') ,(14,'right','10061900000801') ,(15,'right','10061900000825') ,(16,'right','10061900026306') ,(17,'right','10061900028300') ,(18,'right','068274000225') ,(19,'right','10061900001150') ,(20,'right','10061900001266') ,(21,'right','10061900026351') ,(22,'right','10061900029352') ,(23,'right','10061900028355') ,(24,'right','10058924022209') ,(25,'right','10058924022407') ,(26,'right','060100035234') ,(27,'right','060100000126') ,(28,'right','060100000362') ,(29,'right','060100000249') ,(30,'right','060100009402') ,(31,'right','10061900003970') ,(32,'right','10061900001136') ,(33,'right','10061900007138') ,(34,'right','10061900007053') ,(35,'right','10061900007121') ,(36,'right','10061900007046') ,(37,'right','10061900026252') ,(38,'right','10061900028256') ,(39,'right','10061900029253') ,(40,'right','060100037665') ,(41,'right','060100037788') ,(42,'right','060100038655') ,(43,'right','060100000188') ,(44,'right','060100000423') ,(45,'right','10058924022605') ,(46,'right','10058924022803') ,(47,'right','10058924010602') ,(48,'right','10061900011500') ,(49,'right','060100035203') ,(50,'right','060100034411') ,(51,'right','060100034381') ,(52,'right','60107826132') ,(53,'right','63517099790') ,(54,'right','9781770667945') ,(55,'right','9781770667952') ,(56,'right','9781770667969') ,(57,'right','9781770667976') ,(58,'right','624274631069') ,(59,'right','624274631076') ,(60,'right','624274627772') ,(61,'right','062823071476') ,(62,'right','060107852322') ,(63,'right','6270275111006') ,(64,'right','00884191022108') ,(65,'right','10055577104092') ,(66,'right','10057000243767') ,(67,'right','10056200762160') ,(68,'right','10057000005921') ,(69,'right','775749084719') ,(70,'right','0614156061848'); ... and then here's the actual query against it. -- Here's your query, once you set up your table with the UPC combo values declare @id int set @id = 1; --Here's where you can just change the ID to whatever combo you're looking at from your static source sheet thingy. select * from test.reddit_source --change this to your table. where UPCCode in (select UPC from test.reddit_example where id = @id) or OriginalUPC in (select UPC from test.reddit_example where id = @id); Hopefully this is even remotely close to what you're looking to do. If not, let me know.
Check out the following web site. You have a picture which explains it quite well. http://studybyyourself.com/seminar/sql/course/chapter-10-review/?lang=en
That could actually work. edit: yep, this does what I need
I'm not sure exactly what the math is when you end up with more than two sources, but you could convert the values to positive/negative as appropriate and take the sum for each start/end date combination: select start_date, end_date, sum(adjustedvalue) yVal from (select case when source='combined' then value else -value end adjustedvalue, start_date, end_date from table) group by start_date, end_date; I'm also assuming that you only need the Y value, and you don't need to see each value for each source. I also may have understood your "bear in mind that there are multiple source files" statement. I'm assuming that you mean that there could be more than two sources per start/end combo. Either way, this still works, but it will make things more concise if you have multiple sources you're dealing with. If not, a simple JOIN would probably work fine.
Thank you for the help, I will try this.
I believe you're talking about different things. Your isSent column is perfectly fine. It represents state for your entity. I believe what /u/Boomer8450 means is e.g. if you have users where some users are students and some are faculty members. There probably will at some point be a user who is both a faculty member and a student. Then it makes sense to assign different roles on users. You'll need a Role table that stores the roles. And then you connect users and roles with a many to many relationship table called UserRole (convention when doing many-to-many relationships is to just concatenate the names of the tables) where you have the PK from User and Role. But you're really stuck with the same problem as before though. Different roles may have different attributes. The for users with the faculty role may need to store [HireDate] and [Manager]. Where do you store those? In the UserRole table? Then you'll have nulls in the columns for roles where those attributes aren't appplicable. The very thing you wanted to avoid and sparked the discussion in the first place. Perhaps a many to many relationship can get rid of some of the null columns, but not all. I'm open to further discussion and suggestions of course.
&gt; The faculty table would depend on the user table, not the other way around; Makes sense. Thanks. 
Oh, yes, I understand, have that same problem with some of my agent/manager tables and it drives me nuts because of the way it was architected. You're doing the lords work, carry on.
Just need basic grouping here. DECLARE @STARTDATE datetime = '2016-04-01'; DECLARE @ENDDATE datetime = '2016-04-09'; SELECT StartDate, SUM(Submits) / SUM(Leads) FROM [Digital].dbo.DSU_ForecastHistory WHERE DataType = 'Forecast' AND StartDate BETWEEN @STARTDATE AND @ENDDATE GROUP BY StartDate 
That isn't going to accomplish what I'm after.
You need to table drive your intervals so that you can join them into the filters and group by the interval number. Something like this should work: DECLARE @StartDate DATETIME = '2016-04-01'; DECLARE @EndDate DATETIME = '2016-04-09'; DECLARE @NumberOfIntervals INT = 10; DECLARE @IntervalLength INT = DATEDIFF(dd, @StartDate, @EndDate); WITH Intervals AS ( SELECT 1 AS IntervalNum ,DATEADD(DAY, -@IntervalLength - 1, @StartDate) AS IntervalStart ,DATEADD(DAY, -1, @StartDate) AS IntervalEnd UNION ALL SELECT I.IntervalNum + 1 AS IntervalNum ,DATEADD(DAY, -@IntervalLength - 1, I.IntervalStart) AS IntervalStart ,DATEADD(DAY, -1, I.IntervalStart) AS IntervalEnd FROM Intervals AS I WHERE I.IntervalNum &lt;= @IntervalLength + 1 ) --SELECT -- * --FROM -- Intervals AS I --; SELECT I.IntervalNum ,SUM(Submits) / SUM(Leads) AS Value FROM [Digital].dbo.DSU_ForecastHistory AS FH INNER JOIN Intervals AS I ON FH.StartDate BETWEEN I.IntervalStart AND I.IntervalEnd WHERE FH.DataType = 'Forecast' GROUP BY I.IntervalNum ;
Love you. Will test it after my 10 o'clock. Look for some sweet gold if it works.
When I use between for dates I usually use an expression in ssrs to add a day to whatever day is selected by the person using the report. Easier than explaining to to the user's the way it actually works, and they still get to select what looks right to them.
What the heck are you after?
The table its hitting is just date.
I'm hoping that it was a joke
Pretty much just winging it.. these should be fairly close 1. How many orders were placed in month. a SELECT COUNT(*) As OrdersThisMonth FROM Orders WHERE Order_Date between {first of month} AND {last day of month} 1. How many orders were placed by users for whom this was their first order a SELECT COUNT(*) as FirstOrdersThisMonth FROM Orders WHERE Order_Date between {first of month} AND {last day of month} AND NOT EXIST ( SELECT 1 FROM ORDERS inOrders WHERE inOrders.Order_date &lt; {first of month} AND inOrders.CustomerID = Orders.CustomerID ) * And were also new users. ( This one requires a create date, or join date in the customer table) a SELECT COUNT(*) as FirstOrdersThisMonthbyNewUsers FROM Orders WHERE Order_Date between {first of month} AND {last day of month} AND NOT EXIST ( SELECT 1 FROM ORDERS inOrders WHERE inOrders.Order_date &lt; {first of month} AND inOrders.CustomerID = Orders.CustomerID ) AND EXISTS ( SELECT 1 FROM Customer WHERE Customer.CustomerID = Orders.CustomerID AND Customer.Join_date BETWEEN {first of month} and {last day of month} ) 1. In Feb, how many of the orders were placed by people who placed their first order in Jan, and so on for every month of 2016. a SELECT DATEPART( Month, Order_Date) , Count( OrderID) CustomerAlsoPlacedorderInMonth FROM Orders WHERE Orders.Order_Date &lt; {first of month} AND EXISTS ( SELECT 1 -- They placed an order this month FROM Orders InOrders WHERE InOrders.Order_Date between {first of month} AND {last day of month} ANd InOrders.CustomerID = Orders.CustomerID ) AND NOT EXIST ( SELECT 1 -- There is no earlier order by this customer FROM ORDERS inOrders WHERE inOrders.Order_date &lt; {first of month} AND inOrders.CustomerID = Orders.CustomerID ) GROUP BY DATEPART( Month, Order_Date) 1. In Feb, how many of the orders were placed by people who also placed an order in Jan that was not a first order, and so on for every month of 2016 a SELECT DATEPART( Month, Order_Date) , Count( OrderID) CustomerAlsoPlacedorderInMonth FROM Orders WHERE Orders.Order_Date &lt; {first of month} AND EXISTS ( SELECT 1 -- They placed an order this month FROM Orders InOrders WHERE InOrders.Order_Date between {first of month} AND {last day of month} ANd InOrders.CustomerID = Orders.CustomerID ) AND EXIST ( SELECT 1 -- They *did* place an earlier order FROM ORDERS inOrders WHERE inOrders.Order_date &lt; {first of month} AND inOrders.CustomerID = Orders.CustomerID ) GROUP BY DATEPART( Month, Order_Date) 
[It's Monday](http://memesvault.com/wp-content/uploads/Facepalm-Meme-04.png), I'm tired. 
Thanks, I wanted to avoid doing this because I'm already in an IF block for the job title (turning it from an acronym into actual words), but I guess I'll have to bite the bullet and just do it
Use the LIMIT clause (after ORDER BY) or in case you're stuck on Oracle 11g or before use ROWNUM in a subquery and a WHERE ROWNUM &lt;= 10 in the outer SELECT. If you're on SQL Server there are similar constructs. Just make sure you avoid the OFFSET clause when flipping through pages of data: http://oracle.readthedocs.io/en/latest/sql/indexes/top-n-pagination.html. 
TOP? LIMIT? ROWNUM? we can't answer your question because you didn't identify which database you're using 
looks like you're using microsoft curly quotes on your string try it with single quotes -- 'R:\usb\SQL_Sample_Data_1.txt'
 job_title := job_title||message_i_need_to_append; is the syntax in pl/sql, or you can use the concat function... SQL&gt; set serveroutput on size unlimited SQL&gt; declare 2 l_str varchar2(255) := 'hello'; 3 begin 4 l_str := l_str || ' world'; 5 dbms_output.put_line( l_str ); 6 end; 7 / hello world PL/SQL procedure successfully completed. SQL&gt; declare 2 l_str varchar2(255) := 'hello'; 3 begin 4 l_str := concat( l_str, ' world' ); 5 dbms_output.put_line( l_str ); 6 end; 7 / hello world PL/SQL procedure successfully completed. 
still very hard to tell what you are trying to do since your syntax is not java, not plsql, not any language (sort of a mixture of T-SQL from MS sql server, a bit of pro*c and plsql :) ) are you trying to build a trigger? :new.title is trigger syntax.... anyway, does this do it? (please - never ever use SELECT to assign, use assignment to assign in plsql, this is not t-sql) if employee_class = 'Instructional' then if (job_title like 'Assc%') then new_title := 'Associate Professor of ' || Orgnization_title; else .... end if; 
Back it up, of its not being done. And then implement and audit trail, and monitor it for a while to see who the users are. Depending on which dbms you are running, you don't specify, then you have a varying assortment of built in tools to help you further along in terms of what is being done to the database, which is doing it and do forth 
Looks like you are using a SELECT query whose output is not stored into variables or a temporary table inside the FUNCTION which is required. Function can return only one single value. 
Thanks so much! Any ideas for fixing this? I'm kinda new to functions so I haven't really got it figured out.
i'm using visual studio 2012
2 things 1) Have you tried adding the file from; * C:\temp instead of * C:\50\subfolders\including\one\that\needs\computer\admin\rights Program files is a protected Operating system folder, anything inside it requires admin rights in order to play with it. 2) there is no file extension in there, I doubt you can add an entire folder this way(and no file name for that matter unless your image is called image and if it is I want to smack you.)
So I just tried a second test and got the same message as you this time the difference is I made a folder called images2 beforehand so i'm pretty sure you can't use a folder as your FILENAME my first attempt that worked actually created a 50 meg file called images USE master; GO ALTER DATABASE NB ADD FILE ( NAME = Images2, FILENAME = 'C:\Temp\Images2', SIZE = 50MB, MAXSIZE = 10GB, FILEGROWTH = 5MB ); GO
When you say certain pages are spilling into other pages do you mean the rows or the columns? If certain columns are then your page width isn't wide enough. Making it larger should help. Also, make sure you don't have a page break before and page break after on the separate elements. You only need 1.
Well. I'm happy that worked for you :). But, I tried that on my side and still received the same error. 
~~look at my other response.~~ This is the other response actually... Like I said in this one I got the same error as you when I already had a FOLDER called images2 in C:\Temp and in the version that did work it CREATED a file called images. Therefore you cannot link a folder in an ADD FILE in fact all the examples on MSDN used FILENAME = 'C:\somefile.ntf' FILENAME = 'C:\somefile.mdf' etc
Oh! Completely missed that. Thanks.
ok, so just use: :new.title := 'blah ' || whatever; do not "select into" to assign :) probably you could use a case statement, set the prefix in it, and make the one assignment... something like this: https://docs.oracle.com/cd/B19306_01/appdev.102/b14261/case_statement.htm
Sp_msforeachtable
What is the purpose of this "picture" of the data? A raw dump of "here's the contents of all my tables" doesn't tell me much if I'm trying to understand how a database actually works. Are you going to be scrubbing the data so that it doesn't contain PII or anything of actual business value (company secrets, account numbers, addresses, sales figures, etc.)? Would an ERD, or just scripting out the schema, be sufficient?
I know someone else gave you something, but why don't you use datepart with weeks? It looks like that's what you are returning to me. Then you can just input a date range and group by the value returned by datepart. Could get jacked up if using across multiple years, but you could pull year also and order by both. declare @StartDate DATETIME = 'Allows earlier date' declare @EndDate DATETIME = 'Whatever far out date' SELECT SUM(Submits) / SUM(Leads), datepart(ww,StartDate) FROM [Digital].dbo.DSU_ForecastHistory WHERE DataType = 'Forecast' AND StartDate BETWEEN @STARTDATE AND @ENDDATE Group by StartDate
It would be a raw dump of the contents minus PPI.
Unfortunately, you can only fit so many rows on a page. With excel your spreadsheet keeps scrolling. PDFs don't give you that luxury. In the past my leadership has expressed similar criteria as yours has. I've had to do silly things like adjust my page size to 20x35 to make it "fit" in a PDF document. You can try this route of playing with page sizes. The PDFs should automatically fit to page. I wish I could give you a better option, but I've never found one.
Thanks, your right. I actually had to shrink my table down even more. The problem is I have indicators on there and now it looks like a tiny dot that you can barely see
On mobile, I hope you get the idea.
That alone would not be helpful to me if I needed to understand how a database is set up. 
The system admin runs a backup of SQL every night. We are running Microsoft SQL Server 2012. It appears there is monitoring built in by the original individual. I know of quite a few of the apps, and access programs that use SQL backend, but part of my problem is that I don't know what data corresponds with what interface.
In very certain cases, when the status can only ever logically be a binary, it can make sense. In this situation, what happens in 6 months when The Powers That Be decide failed, resent, and pending need to be added? All of a sudden, *isSent* no longer really works. If you have a *statusID* field, and a _Status_ table, you can just add items to the status table, the *statusID* can join to them, and no changes to the schema are required. In the original question by /u/whattodo-whattodo, the same login applies. Instead of an *isstudent* and *isfaculty* flags, what happens when you need to add an administrator role, visiting faculty, graduate student, undergrad student, TA, etc.? Now two *isX* fields have multiplied to 8 *isX* columns, each requiring schema changes. It's not about nulls nearly as much as no one can predict the future, from whims of TPTB, to simple oversights, to fundamental changes in the real world. If you plan every design with a "if I can think of something else to stick in here, the end users will eventually want it or something like it" mentality, it quickly becomes apparent that *isX* columns just don't work very well in the real world.
I want/need a dynamic select though, so if I choose 2 days at the top, I want it to go back 10 intervals of 2 days. If I choose 1 month, it goes back 10 months, etc.
Backups are done nightly. I will talk to my admin about getting a test box spun up to get the DBs on so I don't screw up live data. Thanks for the advice on accidental dba. I will take a look at that. 
A little bit of everything, sounds like my job. Like others have said, just learn the setup and don't mess with anything unless you have to. You are best leaving everything as is and just maintain it. SSRS can get super complex super quick if you start moving stuff around. Tracing data can be tough even if you know the DB. So document and learn what apps do what as time permits. SSMS (SQL Server Management Studio) is what you'll need to run queries and to manage the databases. It sounds like you are off to a good start. These things take time. Aside from backups just make sure you are cognizant of permissions of any new users you need to setup. You don't want to give basic users access to the accounting database for example (if there is one on this server). From here just start learning about the data that you'll be working with. You won't be able to make/modify SSRS reports unless you understand the underlying data.
Exactly. It will give you far more flexibility down the road, and it won't be a big deal when someone asks for another status. 
UNION ALL tells the optimizer and execution engine to just concatenate the result sets. UNION will also do so, but then it sifts through the final result set and removes any duplicate rows. It does this extra dup processing no matter what. If you know the result set is already unique, like in our case, save your memory, cpu, and query time by adding ALL. 
Interesting, thanks. I really like how you used a CTE to bastardize a loop. When I first started trying to write this I tried a loop but just couldn't seem to get it to work so I just brute forced it and hoped for the best. Turned out really nice. I'll update the post tomorrow with the full code, how I got the original start/end dates using another union, and there was a slight modification I made to your code: WHERE I.IntervalNum &lt;= @NumberOfIntervals + 1 Then let the user select how many intervals they want for the output. Turned out really nice.
Solution Verified. Thank you very much. Do you know how to export data from a table to a text file? Here was my post: [https://www.reddit.com/r/SQL/comments/4igxey/mysql_outfile_command_returns_securefilepriv/](https://www.reddit.com/r/SQL/comments/4igxey/mysql_outfile_command_returns_securefilepriv/)
That's great! Thank you so so much!
Is there any way to break down the query to use multiple temp tables? 
Install Microsoft sql server on a local computer and restore a backup to it. Never make changes in production. Test first. Google "database migration scripts" for making changes. Get in the routine of making "up" and "down" scripts, with a date signifying what order the script should be in. "up" is applying the change, and "down" reversing it. This will help you a lot . 
I agree with this. This is the direction my answer was moving towards. 
Using the select into outfile syntax from the mysql command line. http://dev.mysql.com/doc/refman/5.7/en/select-into.html
If you have two tables sharing a many-to-many relationship, it is reasonable to expect there to be a junction table between them. 
would you keep the foreign key relationships updated (mainly with new inserts) with a trigger?
spool
Why would you need to update the foreign key relationships?
Thanks. I will look into that.
I get an error message saying something about not secure-file-priv
You've got floating point accumulation error somewhere. Changing the order of adding floating point numbers can change the outcome, because of rounding error. [This is a feature of IEEE 1394](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html). SQL does not, and will not, define the order of summing. Short version: you should not be using floating points for basically anything that you care about the accuracy of. This is why when someone says "and then I switched to integer types for money," experienced programmers run screaming away. Start counting cents in integer instead. . &gt; I'd expect it to be able to do floating point operations consistently This will never be the case on any machine for the rest of time. The entire purpose of floating point is to approximate fractions. The word "approximate" is your clue.
Floating point error. Switch your column type from float to decimal(12,2) or whatever precision you need. You can also cast it before the sum but a large data set will have a big impact on query performance.
So based on this, predefining an absolute ordering, should eliminate the back-and-forth switching that OP is experiencing? 
Thanks, I believe that would work except I just realized my date set is in varchar not date so I am having issues using cast now &gt;.&lt;
Does this work: Select Year(CAST([Date Of Service] as date)) as Year, Month(CAST([Date Of Service] as date)) as Month, avg(cast([Total Amount Paid] as float)) From Mydatabase Where [Date Of Service] between 01/01/2014 and 01/31/2015 GROUP BY Month(CAST([Date Of Service] as date)), Year(CAST([Date Of Service] as date))
Actually the first one worked, and I just typed it wrong lol Thanks for the help
It happens lol. My pleasure.
Floatimg point registers are 80 (not 8 as i typoed) I think?
No problem using lots of them if your model genuinely needs them. You may be using them when you don't need to. You'd have to run some examples by us.
Right click on 'Integration Services Catalog'. You can configure a bunch of retention settings.
For sure. I know the post was kind of vague in that regard. I was just curious if there is a such thing as too many junctions. In my mind, if I use too many my statements could get really messy with the amount joins required. 
As a rule of thumb: if you grab a sql result into an array, the use that array in another sql statement, then you are doing it wrong.
&gt; decimal(12,2) decimal(19,4) seems to be common for storing monetary values.
&gt; I swear the SQL server isn't a 90's era Pentium machine so I'd expect it to be able to do floating point operations consistently, but it seems random which of these 2 values i get back. It isn't a sql server issue. It is a floating point issue. You have the same kinds of problem on all database systems using floating point and all programming languages using float and all systems that implement float point numbers. https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html
I usually go with: "If you're looping with SQL, you're *probably* going down the wrong path."
thanks this is helpful! I'm looking to write the query in such a way that each month is a separate row (rather than specify start and end date) - is this possible?
Because using non-portable types is generally a lose, because I already brought the Decimal types (BCD is Binary Coded Decimal,) and because the decimal types are *sloooooooow* because they're not using on-chip math. Primitives are usually a win.
well, you need to fetch into a variable and you'll need to test that variable in the while condition. It is also entirely possible that there's no report table or there's no ID column in it (or both). It seems to be a given that you're learning SQL. Why start with cursors? 
The "Select [ID], [Name] From [Report]" works? That error is like the select of the cursor isn't correct. EDIT: If you'll use the values of the ID inside the cursor, you'll need to declare a variable and use the FETCH INTO there, like this: DECLARE @ID AS INT DECLARE @Name AS VARCHAR(100) Declare firstCursor cursor Static for Select [ID], [Name] From [Report] Open firstCursor fetch NEXT From firstCursor into @ID,@Name while left(@Name,1)&lt;&gt;'a' and @@fetch_status = 0 fetch Next from firstCursor Close firstCursor DEALLOCATE firstCursor
Sorry i just realized that the Invalid Column name is 'Name' at line 14 so it is in the cursor not in the select statement (which is functioning correctly)
Because the money type introduces imprecision when doing multiplication or division. Scary stuff. http://blog.learningtree.com/en/is-money-bad-the-money-datatype-in-sql-server/
into outfile is probably better, but if you are still having trouble you can also do mysql -h HostIP -P PortNumber -D DatabaseName -u UserName -pPassword &lt; InFile.sql &gt; OutFile.txt That produces a tab-delimited export. If you use this make sure you don't put a space after `-p`
There is an old programming aphorism attributed to Brian Kernighan: &gt; Floating point numbers are like piles of sand; every time you move them around, you lose a little sand and pick up a little dirt. 
Yes, everywhere you look people abolishes cursors, but to run tasks per register, there is no better solution.
Never heard this one before. This is great `:)`
er. yes, that `:)` sorry and thank you
You could use an alternate to BCP called SQLCMD. Here's an example from StackOverflow. sqlcmd -S . -d MyDb -E -s, -W -Q "select account,rptmonth, thename from theTable" &gt; c:\dataExport.csv [Here's the discussion](http://stackoverflow.com/questions/24497970/sqlcmd-to-generate-file-without-dashed-line-under-header-without-row-count) 
I suppose we've all seen Office Space. Makes sense. 
Your DBA changed the precision on your fact column to be 9,2. In other words he took it down to the hundredths place.
This made so much sense to me. You write in a very clear way. So if I understand right you would make a temp table of my basic joins and then update the columns afterwards with the other data? I can't wait to try this at work tomorrow. Do you have any example code or example articles where you've learned some best practices on this method? Or any tips?
It looks like you're not the first to ask this. Here's something on stackoverflow: http://stackoverflow.com/questions/1355876/export-table-to-file-with-column-headers-column-names-using-the-bcp-utility-an
The ...&gt; means it's continuing after a line break, expecting a semicolon. It's weird that you have one and it's still doing this. Edit: Side note, Ctrl-Z should exit.
1. Put the sub-sub query into a CTE or temp table 2. Put the sub-query into a CTE or temp table 3. All of your JOINs are INNER JOINs; get rid of the stuff in your WHERE clause and put it in an AND clause on the ON portion of your JOIN 4. Are you reaaaalllly sure you need all these JOINs? Try eliminating some and checking output 5. Possibly look for index hints. SQL query optimizer will point you in the right direction but I'd suggest rar.CheckAmount, rac.Void, a.Name and rar.Date
A good disaster recovery plan must take into account numerous factors: sensitivity of data, data loss tolerance, required availability, etc. The plan can be based on few a solutions:  Failover clustering  Database mirroring  Replication  Log shipping  Backup and restore Each solution has its own advantages and cost of implementing. Based on the needs, a disaster recovery plan should include on one or more available solutions or a third party [SQL Recovery tool](https://www.systoolsgroup.com/sql-recovery.html) is much preferable.
You need to write it as a function not a procedure, that way you can call it in your Select.
correction: in T-SQL, or Microsoft SQL SQL is a generic, standard language, and does not include CHARINDEX 
Can you set up a linked server?
Thinking about it, I could create a job to copy the table from server 2 to server 1 on a nightly basis I'm still a little stuck about how I compare the two tables
Thanks for the replies Thinking about this differently, I now have the data all in 1 database Table 1 contains, the event title Table 2 contains the event title (although will not match table 1 ecactly - there will be events missing) I would like the output to look like this, so essentially comparing the two tables to see if data is missing Event(1) | Event(2) magic | magic music | music singing | **MISSING** dancing | dancing balet | **MISSING** 
You need to find the first record with `NAME` starting with A? Ditch the cursor and clearly define what defines "first" (how are you ordering?). declare @FirstID int; select top 1 @FirstID = ID, Name from report where name like 'a%' order by ID; If you are using a cursor to "find" something in a single table, you are almost certainly doing something wrong. And if you care about ordering (and you seemingly do, since you're looking for the "first" of something), you're wasting your time and getting invalid results unless you use `ORDER BY` If you explain better what you're really trying to achieve with this, people can give more detailed/helpful answers
I have 2 instances in the the same server, the 2nd instance is a legacy company, simple JOINs has a small slowdown for sure, but not that significant.
My favorite use is a simple exists or not exists tests. If it exists, CHARINDEX will return &gt; 0. If the substring doesn't exist, CHARINDEX will = 0. Preferable over LIKE to me because I don't have to deal with escaping wild card characters.
It's difficult to describe with no schema or sample data to work with, but the basic version is just an anti-join like this: SELECT * FROM SendingServer.MyDB.dbo.EventTable S WHERE S.Ticked = 'Y' AND NOT EXISTS ( SELECT 1 FROM ReceivingServer.MyOtherDB.dbo.EventTable R WHERE R.Key1 = S.Key1 AND R.Key2 = S.Key2 ) Or: SELECT * FROM SendingServer.MyDB.dbo.EventTable S LEFT JOIN ReceivingServer.MyOtherDB.dbo.EventTable R ON R.Key1 = S.Key1 AND R.Key2 = S.Key2 WHERE S.Ticked = 'Y' AND R.Key1 IS NULL AND R.Key2 IS NULL If you need to check every field's contents it gets more complex, but not by much. 
From a programming standpoint I like what you're doing. From a performance standpoint, LIKE is still the better way to go.
I'm too lazy even to copy/paste/exec your whole code, but a cursory look reveals that you have spaces in the column names whereas XSD does not (it has both camel case and underscore-style naming). Make sure you are naming columns the same way XSD names elements first, then see what else falls off? PS. I see you've been toiling on this one for a month already.
Gleaming code from your query here is a start. SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; --Let's dirty read so we don't block anyone... Might get bad data though, oh well... IF OBJECT_ID('tempdb..#Results', 'U') IS NULL BEGIN DROP TABLE #Accounts; END; IF OBJECT_ID('tempdb..#AccountProviders', 'U') IS NULL BEGIN DROP TABLE #AccountProviders; END; IF OBJECT_ID('tempdb..#Results', 'U') IS NULL BEGIN DROP TABLE #Results; END; CREATE TABLE #Accounts ( AccountsId INT NOT NULL IDENTITY(1,1) PRIMARY KEY ,Account VARCHAR(100) NULL ,District VARCHAR(100) NULL ,EDIRARemittancesId INT NULL ); CREATE TABLE #Results ( ResultsId INT NOT NULL IDENTITY(1,1) PRIMARY KEY ,Account VARCHAR(100) NULL ,District VARCHAR(100) NULL ,ServiceProvider VARCHAR(100) NULL ,TotalPaid DECIMAL(18,4) NULL ); INSERT INTO #Accounts ( Account ,District ,EDIRARemittancesId ) SELECT a.Name ,sd.Name ,rar.ID FROM database1.dbo.Accounts AS a INNER JOIN database1.dbo.AccountDistricts AS ad ON a.ID = ad.AccountID INNER JOIN database1.dbo.StateDistricts AS sd ON ad.StateDistrictID = sd.ID INNER JOIN database1.dbo.EDIRARemittances rar on rar.NPI = sd.NPI WHERE rar.CheckAmount &gt; 0 AND a.Name = 'Account4' AND rar.RADate &gt;= '7/1/2015 12:00:00 AM' AND rar.RADate &lt;= '6/30/2016 12:00:00 AM' AND a.ID = IsNull(1435, a.ID);--This will never be null WTH, param replacement?? --Non-Voided Remittance must exist. DELETE a FROM #Accounts AS a WHERE EXISTS --Anti Semi-Join is faster ( SELECT * FROM database1.dbo.EDIRAClaims rac WHERE rac.EDIRARemittanceID = a.EDIRARemittanceId AND rac.void IS NULL ); ect... 
The point is for the great majority application involving money (income tax, loan, interest calculation) the rounding behavior is expected and even more mandatory and float artifact are considered as wrong 
Queries in general. The places where I had to use cursor is to ensure some business rules in a 3rd party software that makes budgets, so in a trigger I made a cursor run some selects to give custom RAISERROR for each item of the budget made. If we had access to the source code we could make those checks there, but in that case a cursor in the DB side is our only option.
The other comments cover this pretty well. You make an OUTER join between the 2 tables in the 2 databases, either LEFT/RIGHT depending on the master/slave direction, or use FULL OUTER if you want to see sync both ways, using the primary keys. Then stick a WHERE clause in for the 'compare' bit. One or the other primary key being NULL means 'missing from this side', or you can compare other columns just by putting that logic in WHERE e.g.: FROM db1.dbo.myTable FULL OUTER JOIN db2.dbo.myTable ON db1.dbo.myTable.Key = db2.dbo.myTable.Key WHERE (db1.dbo.myTable.Key IS NULL) OR (db2.dbo.myTable.Key IS NULL) OR (db1.dbo.myTable.isTicked != db2.dbo.myTable.isTicked)
Ah, this makes sense, thanks for the reply. Yeah, SQL server lacks an equivalent of Oracle/Postgress query-based FOR loop, unfortunately.
I just want to point out that it's possible to use 2 datasets with different datasources in an SSRS report and then do a join within an expression in a field. It's not very intuitive (the function is called Lookup, not Join) and is definitely more complicated than any other solution presented so far, so I wouldn't recommend it. But I've had to do it in the past, I figured I'd mention that the capability exists.
I might just bite the bullet and do SELECT DISTINCT([whatever]) LIKE '[whatever]' for each
I've already got it running a distinct select on the product name, so if 20 computers have "Adobe Reader XI (11.0.01)" installed, it's only showing as one record. My issue is that I'm getting 10+ records just for Adobe Reader XI (11.0.01-11.0.x), which when compounded across dozens of software installs makes my query results incredibly bloated.
I believe there is an Is_Numeric function that can determine what a character might be....
Imo, I think it really depends on whether you are looking for a quick answer or a long term skill here... what your are talking about it a very simple usage of a fuzzy matching algorithm. Some google searches for that term should get you moving in the right direction, here are a couple options I found. * For Access: http://allenbrowne.com/vba-Soundex.html * For SQL: http://anastasiosyal.com/POST/2009/01/11/18.ASPX * Another one for SQL. I'm not sure I really like the strategy being used here, but I do think its worth a read as the rationales behind it give context to some common complications: http://www.decisivedata.net/blog/cleaning-messy-data-sql-part-1-fuzzy-matching-names/ Those links should give some practical options for how to implement the type of functionality you are talking about, but it's really just the tip of the Data Quality iceberg.
I'm not sure they are like Access and I doubt it, but there is: Oracle SQL Developer, DbViz, SQuirreL, PL/SQL Developer,... Just look for SQL IDEs. Google has you covered. SQL Developer lets you write queries graphically in case you're into that kind of thing. 
From your description, the first ones were all for a given reporting month, do you want all varieties here to span multiple months, or just some of them?
&gt; tried a few different select TOP 1 but for some reason its not working Post your attempts and the errors. `TOP 1` is a correct way to get a single record in SQL Server.
I use something like this to search for strings in code create PROCEDURE dbo.Find_Text_in_all @StringToSearch varchar(100) AS -- Finds text in various user objects in a database. Includes procs, triggers, views, etc. -- Example of use: -- find_text_in_all 'raiserror 4' SET @StringToSearch = '%' +@StringToSearch + '%' SELECT SO.Name,so.type, [Attached to]=(select sp.name from sysobjects sp where sp.id=so.parent_obj), [Attached to type]=(select sp.type from sysobjects sp where sp.id=so.parent_obj) FROM sysobjects SO (NOLOCK) INNER JOIN syscomments SC (NOLOCK) on SO.Id = SC.ID AND SC.Text LIKE @stringtosearch ORDER BY SO.Name
Ctrl-Z is Windows for Ctrl-D.
Code without comments. I wonder what this wall of spaghetti does?
Another approach is to attach a Profiler session to the database, get the user on the phone, and ask them to go ahead and run the "Financial Summary Report" for testing purposes. You should be able to see what procs are fired off.
Function would be the best idea. Or put it into a variable Declare @mynumber int; Execute @mynumber = customproc_sp ; 
Okay?
RBAR, cursors where they don't make sense, `SET ROWCOUNT 1` with an `UPDATE` loop...
People who drop and reload tables as part of a daily job instead of doing a merge. We're talking tables that are hundreds of millions of rows
&gt; My assignment clearly asks for 2 collumns to return all customers in the DB in one collumn and the second to list of customers who have 1 or more orders Column.. only one L Column 1 is a list of all customers in the database, column 2 shouldn't be a list of customers again, that makes Zero sense. The closest sensible requirements I can come up with is a list of all customers and a second column that lists how many orders that customer has placed, 0 to however many. SELECT Customers.userid , COUNT( orders.OrderID) as Number_of_Orders FROM Customers LEFT JOIN ON Customers.userid=orders.USER_ID GROUP BY Customers.userid The way to create a literal interpretation of that specification. SELECT Customers.userid , MAX( orders.USER_ID) -- you've got multiple order rows so you need some grouping function. FROM Customers LEFT JOIN ON Customers.userid=orders.USER_ID GROUP BY Customers.userid Doesn't matter if it's max or MIN since they are all the same
Some of these are duplicates from others, but these are from a talk on database worst practices I've been giving for well over a decade (and are unfortunately, unchanged :( doesn't matter how many times they are brought up, they keep happening ) * not using bind variables * swallowing exceptions in stored procedures (when others THEN NULL in oracle speak) * totally extensible, 100% generic data models (like an entity attribute value - EAV - model). They are very secure, you can get data in - but you'll never efficiently retrieve it ever again * re-invent as many native database features as you can (eg: blow off learning the product you use) * using a string type for everything (just in case, you never know when that date field might need to hold "hello world" you know, it is more flexible). And for bonus points, using the maximum string length for everything (you know, just in case again) * committing frequently (eg: committing after each and every statement - even when it takes fifteen statements to properly construct your transaction. If you've ever wondered why your flowers didn't arrive, your hotel reservation "went missing", there are two orders instead of one, etc etc etc - it'll be because someone doesn't know what the word "transaction" means) * attempting to be database independent. Unless you are something like an SAP (which by the way work on very specific dot releases with very specific patches applied on very specific OS implementations/versions, with very specific parameters set and explicit directions on exactly how to configure the database), and want to basically build your own database on top of a database - this is going to be hard. They are different - even if they all speak "SQL". * not designing in security from day 1 - assuming "it'll be easy to add". It never, ever is - never was, never is, never will be. And if you don't design your model to be secure - trying to secure it after the fact isn't going to be pretty or fast those are the big ones ;) but bind variables and not using them properly is by far the largest.... edit, added a few minutes later - i forgot another biggie: * doing things slow by slow - row by row. Thinking only procedurally and not thinking in sets...
Can you share more info on this method? Is a profiler a 3rd party program? I have some code I run that does what you're mentioning but it will just spit out the code that is running instead of the location details. Sometimes I can grab stuff in time but some of our work spaces and reports load so quickly I can't catch them in time.
This isn't a homework answers sub. We don't do your hw for you. If you come back with a query that you've worked on, but are having some issues with, and can't figure it out, then it's something we can try to point out what to try.
SQL Server Profiler is part of the suite of client tools (like SQL Server Management Studio) in SQL Server. Its probably installed on the server with SQL Server installed. If you have the SQL Server install disk or ISO, you can install it on your workstation if you choose the correct options. You could also use extended events. But, that has somewhat of a steeper learning curve and doesn't exactly have a GUI.
Love me some DataGrip. I'm taking a DB course and learning how to use DG has been awesome. Not the easiest to just pick up but once you learn SQL foundations and know what to look for it really shines.
You may want to take a closer look at some of the procs and other objects, see if maybe they used [extended properties](https://technet.microsoft.com/en-us/library/ms186989\(v=sql.105\).aspx) for storing more metadata. ERWin did this on a schema that I used to have to work on, though it turned out in the end it was very well documented in that metadata.
Try using SOUNDEX(Softname) which will give you an alphanumeric code based on the first letter and the "sound" of the first few letters. Then sort on the Soundex value and see how well it did grouping similar titles. If you really wanted to do it better, you could split the string into individual words and get the soundex value of the first two or three (or ten) words. Then CONCAT(soundex1, soundex2) and sort/group by that field for (theoretically) double the accuracy. 
AQT has a Build Query tool.